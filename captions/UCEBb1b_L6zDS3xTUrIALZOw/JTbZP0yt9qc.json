[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "text": " PHILIPPE RIGOLLET:\nSo welcome back.",
    "start": "17880",
    "end": "23849"
  },
  {
    "text": "We're going to finish this\nchapter on maximum likelihood estimation. And last time, I briefly\nmentioned something that",
    "start": "23850",
    "end": "30830"
  },
  {
    "text": "was called Fisher information. So Fisher information,\nin general,",
    "start": "30830",
    "end": "35990"
  },
  {
    "text": "is actually a matrix when you\nhave a multivariate parameter theta.",
    "start": "35990",
    "end": "41390"
  },
  {
    "text": "So if theta, for example,\nis of dimension d, then the Fisher\ninformation matrix",
    "start": "41390",
    "end": "46430"
  },
  {
    "text": "is going to be a d by d matrix. You can see that, because\nit's the outer product.",
    "start": "46430",
    "end": "51739"
  },
  {
    "text": "So it's of the form\ngradient gradient transpose. So if it's gradient\ngradient transpose,",
    "start": "51740",
    "end": "57230"
  },
  {
    "text": "the gradient is\nthe d dimensional. And so gradient times gradient\ntranspose is a d by d matrix.",
    "start": "57230",
    "end": "63530"
  },
  {
    "text": "And so this matrix\nactually contains-- well, tells you it's called\nFisher information matrix.",
    "start": "63530",
    "end": "69560"
  },
  {
    "text": "So it's basically\ntelling you how much information about the\ntheta is in your model. So for example, if your model\nis very well-parameterized,",
    "start": "69560",
    "end": "77960"
  },
  {
    "text": "then you will have a\nlot of information. You will have a higher-- so let's think of it as\nbeing a scalar number,",
    "start": "77960",
    "end": "83600"
  },
  {
    "text": "just one number\nnow-- so you're going to have a larger information\nabout your parameter in the same probability\ndistribution.",
    "start": "83600",
    "end": "90090"
  },
  {
    "text": "But if start having a weird\nway to parameterize your model,",
    "start": "90090",
    "end": "95899"
  },
  {
    "text": "then the Fisher information\nis actually going to drop. So as a concrete example\nthink of, for example, a parameter of interest\nin a Gaussian model,",
    "start": "95900",
    "end": "104240"
  },
  {
    "text": "where the mean is\nknown to be zero. But what you're interested in\nis the variance, sigma squared. If I'm interested\nin sigma square,",
    "start": "104240",
    "end": "110390"
  },
  {
    "text": "I could parameterize my model\nby sigma, sigma squared, sigma to the fourth, sigma to 24th.",
    "start": "110390",
    "end": "115880"
  },
  {
    "text": "I could parameterize\nit by whatever I want, then I would have a\nsimple transformation. Then you could say\nthat some of them",
    "start": "115880",
    "end": "121100"
  },
  {
    "text": "are actually more\nor less informative, and you're going to\nhave different values for your Fisher information.",
    "start": "121100",
    "end": "126799"
  },
  {
    "text": "So let's just review a few\nwell-known computations. So I will focus primarily on the\none dimensional case as usual.",
    "start": "126800",
    "end": "137540"
  },
  {
    "text": "And I claim that\nthere's two definitions. So if theta is a real\nvalued parameter,",
    "start": "137540",
    "end": "144019"
  },
  {
    "text": "then there's basically\ntwo definitions that you can think of for\nyour Fisher information. One involves the\nfirst derivative",
    "start": "144020",
    "end": "150350"
  },
  {
    "text": "of your log likelihood. And the second one involves\nthe second derivative. So the log likelihood\nhere, we're",
    "start": "150350",
    "end": "156590"
  },
  {
    "text": "actually going to\ndefine it as l of theta. And what is it? Well, it's simply the likelihood\nfunction for one observation.",
    "start": "156590",
    "end": "163550"
  },
  {
    "text": "So it's l-- and I'm going to\nwrite 1 just to make sure that we all know what we're talking\nabout one observation--",
    "start": "163550",
    "end": "169010"
  },
  {
    "text": "of-- which is the order again,\nI think it's X and theta. ",
    "start": "169010",
    "end": "175170"
  },
  {
    "text": "So that's the log\nlikelihood, remember? ",
    "start": "175170",
    "end": "185290"
  },
  {
    "text": "So for example, if\nI have a density, what is it going to be? It's going to be log\nof f sub theta of X.",
    "start": "185290",
    "end": "192330"
  },
  {
    "text": "So this guy is a\nrandom variable, because it's a function\nof a random variable.",
    "start": "192330",
    "end": "197456"
  },
  {
    "text": "And that's what you see\nexpectations of this thing. It's a random function of theta. If I view this as a\nfunction of theta,",
    "start": "197456",
    "end": "203525"
  },
  {
    "text": "the function becomes\nrandom, because it depends on this random X. And so I of theta is actually\ndefined as the variance",
    "start": "203525",
    "end": "215180"
  },
  {
    "text": "of l prime of theta--",
    "start": "215180",
    "end": "220400"
  },
  {
    "text": "so the variance of the\nderivative of this function. And I also claim that it's equal\nto negative the expectation",
    "start": "220400",
    "end": "230750"
  },
  {
    "text": "of the second\nderivative of theta. ",
    "start": "230750",
    "end": "237380"
  },
  {
    "text": "And here, the expectation\nand the variance are computed, because this\nfunction, remember, is random.",
    "start": "237380",
    "end": "243120"
  },
  {
    "text": "So I need to tell you\nwhat is the distribution of the X with\nrespect to which I'm computing the expectation\nand the variance.",
    "start": "243120",
    "end": "248882"
  },
  {
    "text": "And it's the theta itself.  So typically, the\ntheta we're going",
    "start": "248882",
    "end": "255340"
  },
  {
    "text": "to be interested in--\nso there's a Fisher information for all\nvalues of the parameter,",
    "start": "255340",
    "end": "260519"
  },
  {
    "text": "but the one typically\nwe're interested in is the true\nparameter, theta star.",
    "start": "260519",
    "end": "265830"
  },
  {
    "text": "But view this as a function\nof theta right now. So now, I need to prove\nto you-- and this is not",
    "start": "265830",
    "end": "271759"
  },
  {
    "text": "a trivial statement-- the\nvariance of the derivative is equal to negative\nthe expectation of the second derivative.",
    "start": "271760",
    "end": "277777"
  },
  {
    "text": "I mean, there's really quite a\nbit that comes into this right. And it comes from the fact that\nthis is a log not of anything.",
    "start": "277777",
    "end": "284267"
  },
  {
    "text": "It's the log of a density. So let's just prove\nthat without having to bother too much\nourselves with",
    "start": "284267",
    "end": "291260"
  },
  {
    "text": "some technical assumptions. And the technical assumptions\nare the assumptions that allow me to permute\nderivative and integral.",
    "start": "291260",
    "end": "299270"
  },
  {
    "text": "Because when I compute the\nvariances and expectations, I'm actually integrating\nagainst the density.",
    "start": "299270",
    "end": "304310"
  },
  {
    "text": "And what I want to do is to make\nsure that I can always do that. So my technical assumptions\nare I can always permute",
    "start": "304310",
    "end": "313250"
  },
  {
    "text": "integral and derivatives. So let's just prove this.",
    "start": "313250",
    "end": "319350"
  },
  {
    "text": "So what I'm going to do\nis I'm going to assume that X has density f theta.",
    "start": "319350",
    "end": "332750"
  },
  {
    "text": " And I'm actually just\ngoing to write-- well, let me write it f theta right now.",
    "start": "332750",
    "end": "339030"
  },
  {
    "text": "Let me try to not be\nlazy about writing. And so the thing\nI'm going to use is the fact that the integral of\nthis density is equal to what?",
    "start": "339030",
    "end": "350540"
  },
  {
    "text": "1. And this is where I'm going\nto start doing weird things. That means that if I take\nthe derivative of this guy,",
    "start": "350540",
    "end": "356860"
  },
  {
    "text": "it's equal to 0. So that means that if I look\nat the derivative with respect",
    "start": "356860",
    "end": "363370"
  },
  {
    "text": "to theta of integral f theta\nof X dX, this is equal to 0.",
    "start": "363370",
    "end": "371840"
  },
  {
    "text": "And this is where I'm\nactually making this switch, is that I'm going to say\nthat this is actually equal to the integral\nof the derivative.",
    "start": "371840",
    "end": "379670"
  },
  {
    "start": "379670",
    "end": "387670"
  },
  {
    "text": "So that's going to be the\nfirst thing I'm going to use. And of course, if it's true\nfor the first derivative,",
    "start": "387670",
    "end": "392820"
  },
  {
    "text": "it's going to be true for\nthe second derivative. So I'm going to actually\ndo it a second time. And the second thing\nI'm going to use",
    "start": "392820",
    "end": "398220"
  },
  {
    "text": "is the fact the integral\nof the second derivative",
    "start": "398220",
    "end": "406860"
  },
  {
    "text": "is equal to 0.  So let's start from here. ",
    "start": "406860",
    "end": "419410"
  },
  {
    "text": "And let me start from,\nsay, the expectation of the second derivative\nof l prime theta.",
    "start": "419410",
    "end": "426789"
  },
  {
    "text": "So what is l prime prime theta? Well, it's the second derivative\nof log of f theta of X.",
    "start": "426790",
    "end": "441320"
  },
  {
    "text": "And we know that the\nderivative of the log-- sorry-- yeah, so the derivative\nof the log is 1 over--",
    "start": "441320",
    "end": "450780"
  },
  {
    "text": "well, it's the derivative\nof f divided by f itself. ",
    "start": "450780",
    "end": "469646"
  },
  {
    "text": "Everybody's with me?  Just log of f prime\nis f prime over f.",
    "start": "469647",
    "end": "478760"
  },
  {
    "text": "Here, it's just that f, I view\nthis as a function of theta and not as a function of X.",
    "start": "478760",
    "end": "484199"
  },
  {
    "text": "So now, I need to take another\nderivative of this thing. So that's going to be equal to--",
    "start": "484200",
    "end": "489560"
  },
  {
    "text": "well, so we all know the\nformula for the derivative of the ratio. So I pick up the second\nderivative times f theta",
    "start": "489560",
    "end": "502080"
  },
  {
    "text": "minus the first\nderivative squared",
    "start": "502080",
    "end": "510479"
  },
  {
    "text": "divided by f theta squared-- ",
    "start": "510480",
    "end": "518590"
  },
  {
    "text": "basic calculus. And now, I need to check\nthat negative the expectation of this guy is giving\nme back what I want.",
    "start": "518590",
    "end": "527580"
  },
  {
    "text": "Well what is negative the\nexpectation of l prime prime of theta?",
    "start": "527580",
    "end": "534150"
  },
  {
    "text": "Well, what we need to do\nis to do negative integral of this guy against f theta.",
    "start": "534150",
    "end": "539500"
  },
  {
    "text": "So it's minus the integral of-- ",
    "start": "539500",
    "end": "568340"
  },
  {
    "text": "That's just the definition\nof the expectation. I take an integral\nagainst f theta.",
    "start": "568340",
    "end": "574370"
  },
  {
    "text": "But here, I have something nice. What's happening is that\nthose guys are canceling. ",
    "start": "574370",
    "end": "581936"
  },
  {
    "text": "And now that those\nguys are canceling, those guys are canceling too. ",
    "start": "581937",
    "end": "591556"
  },
  {
    "text": "So what I have is\nthat the first term-- I'm going to break\nthis difference here. So I'm going to say that\nintegral of this difference",
    "start": "591556",
    "end": "598250"
  },
  {
    "text": "is the difference\nof the integrals. So the first term is\ngoing to be the integral",
    "start": "598250",
    "end": "603260"
  },
  {
    "text": "of d over d theta\nsquared of f theta.",
    "start": "603260",
    "end": "609256"
  },
  {
    "text": " And the second one, the negative\nsigns are going to cancel,",
    "start": "609256",
    "end": "617360"
  },
  {
    "text": "and I'm going to\nbe left with this.",
    "start": "617360",
    "end": "632140"
  },
  {
    "start": "632140",
    "end": "637700"
  },
  {
    "text": "Everybody's following? Anybody found the mistake? ",
    "start": "637700",
    "end": "644860"
  },
  {
    "text": "How about the other mistake? I don't know if\nthere's a mistake. I'm just trying to get you\nto check what I'm doing.",
    "start": "644860",
    "end": "650980"
  },
  {
    "text": " With me so far?",
    "start": "650980",
    "end": "656370"
  },
  {
    "text": "So this guy here is the integral\nof the second the derivative of f of X dX.",
    "start": "656370",
    "end": "662399"
  },
  {
    "text": "What is this? AUDIENCE: It's 0. PHILIPPE RIGOLLET: It's 0.",
    "start": "662400",
    "end": "668330"
  },
  {
    "text": "And that's because of this guy,\nwhich I will call frowny face.",
    "start": "668330",
    "end": "676910"
  },
  {
    "text": "So frowny face tells me this. And let's call this guy\nmonkey that hides his eyes.",
    "start": "676910",
    "end": "686480"
  },
  {
    "text": "No, let's just do\nsomething simpler. Let's call it star. And this guy, we\nwill use later on.",
    "start": "686480",
    "end": "692180"
  },
  {
    "text": " So now, I have to prove\nthat this guy, which",
    "start": "692180",
    "end": "697760"
  },
  {
    "text": "I have proved is\nequal to this, is now equal to the variance\nof l prime theta.",
    "start": "697760",
    "end": "706070"
  },
  {
    "text": "So now, let's go back\nto the other way. We're going to meet halfway. I'm going to have a series-- I want to prove that this\nguy is equal to this guy.",
    "start": "706070",
    "end": "716090"
  },
  {
    "text": "And I'm going to have\na series of equalities that I'm going to meet halfway.",
    "start": "716090",
    "end": "721740"
  },
  {
    "text": "So let's start\nfrom the other end. We started from the negative\nl prime prime theta. Let's start with\nthe variance part.",
    "start": "721740",
    "end": "726743"
  },
  {
    "text": " Variance of l prime of theta,\nso that's the variance--",
    "start": "726743",
    "end": "737394"
  },
  {
    "start": "737394",
    "end": "742519"
  },
  {
    "text": "so that's the\nexpectation of l prime",
    "start": "742520",
    "end": "749170"
  },
  {
    "text": "of theta squared minus the\nsquare of the expectation of l",
    "start": "749170",
    "end": "755230"
  },
  {
    "text": "prime of theta. ",
    "start": "755230",
    "end": "761370"
  },
  {
    "text": "Now, what is the square\nof the expectation of l prime of theta? Well, l prime of theta is equal\nto the partial with respect",
    "start": "761370",
    "end": "770750"
  },
  {
    "text": "to theta of log f theta of X,\nwhich we know from the first",
    "start": "770750",
    "end": "777461"
  },
  {
    "text": "line over there-- that's what's\nin the bracket on the second line there-- is actually equal to the\npartial over theta of f",
    "start": "777461",
    "end": "785010"
  },
  {
    "text": "theta X divided by\nf theta X. That's the derivative of the log.",
    "start": "785010",
    "end": "791389"
  },
  {
    "text": "So when I look at the\nexpectation of this guy, I'm going to have the integral\nof this against f theta.",
    "start": "791390",
    "end": "798700"
  },
  {
    "text": "And the f thetas are\ngoing to cancel again, just like I did here. So this thing is actually\nequal to the integral",
    "start": "798700",
    "end": "806110"
  },
  {
    "text": "of partial over theta\nof f theta of X dX.",
    "start": "806110",
    "end": "813649"
  },
  {
    "text": "And what does this equal to?  0, by the monkey hiding is eyes.",
    "start": "813650",
    "end": "822310"
  },
  {
    "text": "So that's star-- tells me\nthat this is equal to 0. ",
    "start": "822310",
    "end": "830090"
  },
  {
    "text": "So basically, when I compute\nthe variance, this term is not. Going to matter. I only have to\ncomplete the first one. ",
    "start": "830090",
    "end": "850630"
  },
  {
    "text": "So what is the first one? Well, the first one is the\nexpectation of l prime squared.",
    "start": "850630",
    "end": "861280"
  },
  {
    "text": " And so that guy is the integral\nof-- well, what is l prime?",
    "start": "861280",
    "end": "869770"
  },
  {
    "text": "Again, it's partial\nover partial theta f theta of X divided by f theta\nof X. Now, this time, this guy",
    "start": "869770",
    "end": "877959"
  },
  {
    "text": "is squared against the density. ",
    "start": "877960",
    "end": "884319"
  },
  {
    "text": "So one of the f thetas cancel. ",
    "start": "884320",
    "end": "907195"
  },
  {
    "text": "But now, I'm back to what\nI had before for this guy. ",
    "start": "907195",
    "end": "916420"
  },
  {
    "text": "So this guy is now\nequal to this guy. There's just the same formula.",
    "start": "916420",
    "end": "921940"
  },
  {
    "text": "So they're the same thing. And so I've moved both ways. Starting from the\nexpression that",
    "start": "921940",
    "end": "927800"
  },
  {
    "text": "involves the expectation\nof the second derivative, I've come to this guy. And starting from the\nexpression that tells me",
    "start": "927800",
    "end": "934310"
  },
  {
    "text": "about the variance of\nthe first derivative, I've come to the same guy. So that completes my proof.",
    "start": "934310",
    "end": "941397"
  },
  {
    "text": "Are there any questions\nabout the proof? ",
    "start": "941397",
    "end": "947050"
  },
  {
    "text": "We also have on our way found an\nexplicit formula for the Fisher",
    "start": "947050",
    "end": "953890"
  },
  {
    "text": "information as well. So now that I have this\nthing, I could actually add that if X has a\ndensity, for example,",
    "start": "953890",
    "end": "962200"
  },
  {
    "text": "this is also equal\nto the integral of-- ",
    "start": "962200",
    "end": "969810"
  },
  {
    "text": "well, the partial over\ntheta of f theta of X",
    "start": "969810",
    "end": "975840"
  },
  {
    "text": "squared divided by f\ntheta of X, because I just",
    "start": "975840",
    "end": "984431"
  },
  {
    "text": "proved that those two\nthings were actually equal to the same thing,\nwhich was this guy. ",
    "start": "984431",
    "end": "991230"
  },
  {
    "text": "Now in practice, this is really\ngoing to be the useful one. The other two are going\nto be useful depending on what case you're in.",
    "start": "991230",
    "end": "997810"
  },
  {
    "text": "So if I ask you to compute\nthe Fisher information, you have now three\nways to pick from.",
    "start": "997810",
    "end": "1003020"
  },
  {
    "text": "And basically,\npractice will tell you which one to choose if you\nwant to save five minutes when you're doing your computations.",
    "start": "1003020",
    "end": "1008666"
  },
  {
    "text": "Maybe you're the guy who\nlikes to take derivatives. And then you're going to go\nwith the second derivative one. Maybe you're the guy who\nlikes to extend squares,",
    "start": "1008666",
    "end": "1015000"
  },
  {
    "text": "so you're going to\ntake the one that involves the square\nof the squared prime. And maybe you're\njust a normal person,",
    "start": "1015000",
    "end": "1021042"
  },
  {
    "text": "and you want to use that guy. ",
    "start": "1021042",
    "end": "1026119"
  },
  {
    "text": "Why do I care? This is the Fisher information. And I could have defined the\n[? Hilbert ?] information",
    "start": "1026119",
    "end": "1031790"
  },
  {
    "text": "by taking the square\nroot of this guy plus the sine of this thing\nand just be super happy and have my name in textbooks.",
    "start": "1031790",
    "end": "1038369"
  },
  {
    "text": "But this thing has a\nvery particular meaning. When we're doing the maximum\nlikelihood estimation--",
    "start": "1038369",
    "end": "1044540"
  },
  {
    "text": " so remember the maximum\nlikelihood estimation is just",
    "start": "1044540",
    "end": "1052909"
  },
  {
    "text": "an empirical version of trying\nto minimize the KL divergence. So what we're trying to\ndo, maximum likelihood,",
    "start": "1052910",
    "end": "1059570"
  },
  {
    "text": "is really trying to\nminimize the KL divergence. ",
    "start": "1059570",
    "end": "1074159"
  },
  {
    "text": "And we're trying to minimize\nthis function, remember? So now what we're\ngoing to do is we're",
    "start": "1074160",
    "end": "1081033"
  },
  {
    "text": "going to plot this function. We said that, let's\nplace ourselves in cases where\nthis KL is convex,",
    "start": "1081034",
    "end": "1086440"
  },
  {
    "text": "so that the inverse is concave. So it's going to\nlook like this-- U-shaped, that's convex.",
    "start": "1086440",
    "end": "1093930"
  },
  {
    "text": "So that's the truth thing\nI'm trying to minimize. And what I said is that\nI'm going to actually try to estimate this guy.",
    "start": "1093930",
    "end": "1099215"
  },
  {
    "text": "So in practice,\nI'm going to have something that looks like\nthis, but it's not really this. ",
    "start": "1099215",
    "end": "1106425"
  },
  {
    "text": "And we're not going\nto do this, but you can show that you\ncan control this uniformly over the entire space,\nthat there is no space where",
    "start": "1106425",
    "end": "1113809"
  },
  {
    "text": "it just becomes huge. In particular, this\nis not the space where it just\nbecomes super huge, and the minimum\nof the dotted line",
    "start": "1113810",
    "end": "1119929"
  },
  {
    "text": "becomes really\nfar from this guy. So if those two functions\nare close to each other,",
    "start": "1119930",
    "end": "1125330"
  },
  {
    "text": "then this implies that the\nminimum here of the dotted line is close to the minimum\nof the solid line.",
    "start": "1125330",
    "end": "1133250"
  },
  {
    "text": "So we know that\nthis is theta star. And this is our MLE,\nestimator, theta hat ml.",
    "start": "1133250",
    "end": "1140472"
  },
  {
    "text": "So that's basically\nthe principle-- the more data we have,\nthe closer the dotted line is to the solid line.",
    "start": "1140472",
    "end": "1147019"
  },
  {
    "text": "And so the minimum is\ncloser to the minimum. But now, this is\njust one example,",
    "start": "1147020",
    "end": "1152799"
  },
  {
    "text": "where I drew a picture for you. But there could be some\nreally nasty examples. Think of this\nexample, where I have",
    "start": "1152800",
    "end": "1160330"
  },
  {
    "text": "a function, which is convex,\nbut it looks more like this. ",
    "start": "1160330",
    "end": "1170120"
  },
  {
    "text": "That's convex, it's U-shaped. It's just a professional U.",
    "start": "1170120",
    "end": "1176300"
  },
  {
    "text": "Now, I'm going to put a dotted\nline around it that has pretty",
    "start": "1176300",
    "end": "1181430"
  },
  {
    "text": "much the same fluctuations. The bend around it\nis of this size. ",
    "start": "1181430",
    "end": "1192980"
  },
  {
    "text": "So do we agree that the\ndistance between the solid line and the dotted line is\npretty much the same",
    "start": "1192980",
    "end": "1198590"
  },
  {
    "text": "in those two pictures? Now, here, depending\non how I tilt this guy,",
    "start": "1198590",
    "end": "1204650"
  },
  {
    "text": "basically, I can put the minimum\ntheta star wherever I want. And let's say that here,\nI actually put it here.",
    "start": "1204650",
    "end": "1211649"
  },
  {
    "text": "That's pretty much the\nminimum of this line. And now, the minimum of the\ndotted line is this guy. ",
    "start": "1211650",
    "end": "1220930"
  },
  {
    "text": "So they're very far. The fact that I'm very\nflat at the bottom makes my requirements\nfor being close",
    "start": "1220930",
    "end": "1228340"
  },
  {
    "text": "to the U-shaped solid\ncurve much more stringent, if I want to stay close.",
    "start": "1228340",
    "end": "1234019"
  },
  {
    "text": "And so this is the\ncanonical case. This is the annoying case.",
    "start": "1234020",
    "end": "1239720"
  },
  {
    "text": "And of course, you\nhave the awesome case-- looks like this.",
    "start": "1239720",
    "end": "1245539"
  },
  {
    "text": "And then whether\nyou deviate, you can have something\nthat moves pretty far. It doesn't matter, it's\nalways going to stay close.",
    "start": "1245540",
    "end": "1253480"
  },
  {
    "text": "Now, what is the quantity\nthat measures how curved I am at a given point--",
    "start": "1253480",
    "end": "1259700"
  },
  {
    "text": "how curved the function\nis at a given point? The secondary derivative.",
    "start": "1259700",
    "end": "1265419"
  },
  {
    "text": "And so the Fisher information is\nnegative the second derivative.",
    "start": "1265420",
    "end": "1271150"
  },
  {
    "text": "Why the negative? ",
    "start": "1271150",
    "end": "1277044"
  },
  {
    "text": "Well here-- Yeah, we're\nlooking for a minimum, and this guy is really\nthe-- you should view this as a reverted function.",
    "start": "1277044",
    "end": "1283460"
  },
  {
    "text": "This is we're trying to\nmaximize the likelihood, which is basically maximizing\nthe negative KL.",
    "start": "1283460",
    "end": "1288950"
  },
  {
    "text": "So the picture I'm showing you\nis trying to minimize the KL. So the truth picture that\nyou should see for this guy",
    "start": "1288950",
    "end": "1293990"
  },
  {
    "text": "is the same, except that\nit's just flipped over. But the curvature is the same,\nwhether I flip my sheet or not.",
    "start": "1293990",
    "end": "1300799"
  },
  {
    "text": "So it's the same thing. So apart from this\nnegative sign, which is just\ncoming from the fact that we're maximizing\ninstead of minimizing,",
    "start": "1300800",
    "end": "1307549"
  },
  {
    "text": "this is just telling me\nhow curved my likelihood is around the maximum. And therefore, it's actually\ntelling me how good,",
    "start": "1307550",
    "end": "1315080"
  },
  {
    "text": "how robust my maximum\nlikelihood estimator is. It's going to tell me how\nclose, actually, my likelihood",
    "start": "1315080",
    "end": "1321269"
  },
  {
    "text": "estimator is going to be-- maximum likelihood is going\nto be to the true parameter.",
    "start": "1321270",
    "end": "1326510"
  },
  {
    "text": "So I should be able\nto see that somewhere. There should be some\nstatement that tells me that this Fisher\ninformation will",
    "start": "1326510",
    "end": "1333230"
  },
  {
    "text": "play a role when assessing the\nprecision of this estimator. And remember, how do we\ncharacterize a good estimator?",
    "start": "1333230",
    "end": "1340279"
  },
  {
    "text": "Well, we look at its bias,\nor we look its variance. And we can combine the two\nand form the quadratic risk.",
    "start": "1340280",
    "end": "1346880"
  },
  {
    "text": "So essentially, what\nwe're going to try to say is that one of those\nguys-- either the bias or the variance or\nthe quadratic risk--",
    "start": "1346880",
    "end": "1353150"
  },
  {
    "text": "is going to be worse if\nmy function is flatter, meaning that my Fisher\ninformation is smaller. ",
    "start": "1353150",
    "end": "1360389"
  },
  {
    "text": "And this is exactly the\npoint of this last theorem. So let's look at a\ncouple of conditions.",
    "start": "1360390",
    "end": "1366270"
  },
  {
    "text": "So this is your typical\n1950s statistics",
    "start": "1366270",
    "end": "1371310"
  },
  {
    "text": "paper that has like one\npage of assumptions. And this was like that\nin the early days,",
    "start": "1371310",
    "end": "1376764"
  },
  {
    "text": "because people\nwere trying to make theories that would be valid\nfor as many models as possible. And now, people are\nsort of abusing this,",
    "start": "1376764",
    "end": "1383280"
  },
  {
    "text": "and they're just making all\nthis lists of assumptions so that their\nparticular method works for their particular\nproblem, because they just",
    "start": "1383280",
    "end": "1388628"
  },
  {
    "text": "want to take shortcuts. But really, the maximum\nlikelihood estimator is basically as old\nas modern statistics.",
    "start": "1388628",
    "end": "1395820"
  },
  {
    "text": "And so this was really\nnecessary conditions. And we'll just parse that. The model is identified.",
    "start": "1395820",
    "end": "1401610"
  },
  {
    "text": "Well, better be, because\nI'm trying to estimate theta and not P theta. So this one is good.",
    "start": "1401610",
    "end": "1406860"
  },
  {
    "text": "For all theta, the support of P\ntheta does not depend on theta.",
    "start": "1406860",
    "end": "1412630"
  },
  {
    "text": "So that's just something\nthat we need to have. Otherwise, things\nbecome really messy. And in particular,\nI'm not going to be",
    "start": "1412630",
    "end": "1418539"
  },
  {
    "text": "able to define likelihood-- Kullback-Leibler divergences. Then why can I not do that?",
    "start": "1418540",
    "end": "1424340"
  },
  {
    "text": "Well, because the\nKullback-Leibler divergence has a log of the ratio\nof two densities.",
    "start": "1424340",
    "end": "1429429"
  },
  {
    "text": "And if one of the support\nis changing with theta is it might be they have\nthe log of something that's 0 or something that's not 0.",
    "start": "1429430",
    "end": "1435820"
  },
  {
    "text": "And the log of 0 is a slightly\nannoying quantity to play with. And so we're just\nremoving that case.",
    "start": "1435820",
    "end": "1441220"
  },
  {
    "text": "Nothing depends on theta-- think of it as being\nbasically the entire real line as a support for\nGaussian, for example.",
    "start": "1441220",
    "end": "1448020"
  },
  {
    "text": "Theta star is not on\nthe boundary of theta. Can anybody tell me\nwhy this is important?",
    "start": "1448020",
    "end": "1453147"
  },
  {
    "text": " We're talking about derivatives.",
    "start": "1453147",
    "end": "1459142"
  },
  {
    "text": "So when I want to talk\nabout derivatives, I'm talking about fluctuations\naround a certain point. And if I'm at the boundary,\nit's actually really annoying.",
    "start": "1459142",
    "end": "1466166"
  },
  {
    "text": "I might have the\nderivative-- remember, I give you this example-- where the maximum likelihood is\njust obtained at the boundary,",
    "start": "1466166",
    "end": "1471720"
  },
  {
    "text": "because the function cannot\ngrow anymore at the boundary. But it does not mean\nthat the first order derivative is equal to 0.",
    "start": "1471720",
    "end": "1478050"
  },
  {
    "text": "It does not mean anything. So all this picture\nhere is valid only if I'm actually\nachieving the minimum inside.",
    "start": "1478050",
    "end": "1486720"
  },
  {
    "text": "Because if my theta space stops\nhere and it's just this guy,",
    "start": "1486720",
    "end": "1492030"
  },
  {
    "text": "I'm going to be here. And there's no questions\nabout curvature or anything that comes into play. It's completely different.",
    "start": "1492030",
    "end": "1498340"
  },
  {
    "text": "So here, it's inside. Again, think of theta as\nbeing the entire real line. Then everything is inside.",
    "start": "1498340",
    "end": "1505550"
  },
  {
    "text": "I is invertible. What does it mean for a\npositive number, a 1 by 1 matrix",
    "start": "1505550",
    "end": "1511130"
  },
  {
    "text": "to be invertible? ",
    "start": "1511130",
    "end": "1516820"
  },
  {
    "text": "Yep. AUDIENCE: It'd be equal\nto its [INAUDIBLE].. PHILIPPE RIGOLLET: A 1 by 1\nmatrix, that's a number, right?",
    "start": "1516820",
    "end": "1524167"
  },
  {
    "text": "What is a characteristic-- if I\ngive you a matrix with numbers and ask you if it's\ninvertible, what are you going to do with it?",
    "start": "1524167",
    "end": "1531658"
  },
  {
    "text": "AUDIENCE: Check if\nthe determinant is 0. PHILIPPE RIGOLLET: Check\nif the determinant is 0. What is the determinant\nof the 1 by 1 matrix?",
    "start": "1531658",
    "end": "1537600"
  },
  {
    "text": "It's just the number itself. So that's basically, you want\nto check if this number is 0 or not. So we're going to think in\nthe one dimensional case here.",
    "start": "1537600",
    "end": "1544990"
  },
  {
    "text": "And in the one dimensional\ncase, that just means that the\ncurvature is not 0.",
    "start": "1544990",
    "end": "1551480"
  },
  {
    "text": "Well, it better be not\n0, because then I'm going to have no guarantees. If I'm totally flat,\nif I have no curvature,",
    "start": "1551480",
    "end": "1556679"
  },
  {
    "text": "I'm basically totally\nflat at the bottom. And then I'm going\nto get nasty things. Now, this is not true.",
    "start": "1556680",
    "end": "1562169"
  },
  {
    "text": "I could have the curvature\nwhich grows like-- so here, it's basically-- the second\nderivative is telling me--",
    "start": "1562170",
    "end": "1568110"
  },
  {
    "text": "if I do the Taylor\nexpansion, it's telling me how I grow as a\nfunction of, say, x squared.",
    "start": "1568110",
    "end": "1573170"
  },
  {
    "text": "It's the quadratic term\nthat I'm controlling. It could be that this guy is\n0, and then the term of order,",
    "start": "1573170",
    "end": "1579170"
  },
  {
    "text": "x to the fourth, is picking up. That could be the first\none that's non-zero.  But that would mean that\nmy rate of convergence",
    "start": "1579170",
    "end": "1585270"
  },
  {
    "text": "would not be square root of n. When I'm actually playing\ncentral limit theorem, it would become n to the 1/4th.",
    "start": "1585270",
    "end": "1590820"
  },
  {
    "text": "And if I have all a bunch\nof 0 until the 16th order, I would have n to the\n1/16th, because that's really",
    "start": "1590820",
    "end": "1596460"
  },
  {
    "text": "telling me how flat I am. So we're going to\nfocus on the case where it's only quadratic\nterms, and the rates",
    "start": "1596460",
    "end": "1603160"
  },
  {
    "text": "of the central limit\ntheorems kick in. And then a few other\ntechnical conditions--",
    "start": "1603160",
    "end": "1608559"
  },
  {
    "text": "we just used a couple of them. So I permuted\nlimit and integral. And you can check that\nreally what you want",
    "start": "1608560",
    "end": "1614890"
  },
  {
    "text": "is that the integral of a\nderivative is equal to 0. Well, it just means that\nthe values at the two ends",
    "start": "1614890",
    "end": "1620370"
  },
  {
    "text": "are actually the same. So those are slightly\ndifferent things. So now, what we have is that\nthe maximum likelihood estimator",
    "start": "1620370",
    "end": "1628900"
  },
  {
    "text": "has the following\ntwo properties. The first one, if I were to\nsay that in words, what would I say, that theta hat is--",
    "start": "1628900",
    "end": "1635470"
  },
  {
    "text": " Is what?",
    "start": "1635470",
    "end": "1640789"
  },
  {
    "text": "Yeah, that's what I\nwould say when I-- that's for mathematicians. But if I'm a statistician,\nwhat am I going to say?",
    "start": "1640790",
    "end": "1647530"
  },
  {
    "text": "It's consistent. It's a consistent\nestimator of theta star. It converges in\nprobability to theta star.",
    "start": "1647530",
    "end": "1653830"
  },
  {
    "text": "And then we have this sort\nof central limit theorem statement. The central limit theorem\nstatement tells me that if this",
    "start": "1653830",
    "end": "1659320"
  },
  {
    "text": "was an average and I remove the\nexpectation of the average-- let's say it's 0, for example--",
    "start": "1659320",
    "end": "1665716"
  },
  {
    "text": "then square root of n\ntimes the average blah goes through some\nnormal distribution. This is telling me that\nthis is actually true,",
    "start": "1665717",
    "end": "1672080"
  },
  {
    "text": "even if theta hat has nothing\nto do with an average. That's remarkable. Theta hat might not\neven have a closed form,",
    "start": "1672080",
    "end": "1679640"
  },
  {
    "text": "and I'm still having\nbasically the same properties as an average that\nwould be given to me by a central limit theorem.",
    "start": "1679640",
    "end": "1688180"
  },
  {
    "text": "And what is the\nasymptotic variance? So that's the variance in the n. So here, I'm thinking of having\nthose guys being multivariate.",
    "start": "1688180",
    "end": "1695980"
  },
  {
    "text": "And so I have the inverse\nof the covariance matrix that shows up as the\nvariance-covariance matrix",
    "start": "1695980",
    "end": "1701049"
  },
  {
    "text": "asymptotically. But if you think of just being\na one dimensional parameter, it's one over this\nFisher information,",
    "start": "1701050",
    "end": "1707680"
  },
  {
    "text": "one over the curvature. So the curvature is\nreally flat, the variance becomes really big.",
    "start": "1707680",
    "end": "1713230"
  },
  {
    "text": "If the function is really\nflat, curvature is low, variance is big. If the curvature is very high,\nthe variance becomes very low.",
    "start": "1713230",
    "end": "1721383"
  },
  {
    "text": "And so that\nillustrates everything that's happening with the\npictures that we have. And if you look,\nwhat's amazing here,",
    "start": "1721384",
    "end": "1728740"
  },
  {
    "text": "there is no square root 2\npi, there's no fudge factors going on here. This is the asymptotic\nvariance, nothing else.",
    "start": "1728740",
    "end": "1736269"
  },
  {
    "text": "It's all in there,\nall in the curvature. ",
    "start": "1736270",
    "end": "1743770"
  },
  {
    "text": "Are there any\nquestions about this?  So you can see here that theta\nstar is the true parameter.",
    "start": "1743770",
    "end": "1751190"
  },
  {
    "text": "And the information matrix\nis evaluated at theta star.",
    "start": "1751190",
    "end": "1757159"
  },
  {
    "text": "That's the point that matters. When I drew this\npicture, the point that was at the very bottom\nwas always theta star.",
    "start": "1757160",
    "end": "1762420"
  },
  {
    "text": "It's the one that minimizes\nthe KL divergence, am as long as I'm identified.",
    "start": "1762420",
    "end": "1772856"
  },
  {
    "text": "Yes. AUDIENCE: So the\nhigher the curvature, the higher the inverse of\nthe Fisher information?",
    "start": "1772856",
    "end": "1778515"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nNo, the higher the Fisher information itself. So the inverse is\ngoing to be smaller.",
    "start": "1778515",
    "end": "1786310"
  },
  {
    "text": "So small variance is good. So now what it\nmeans, actually, if I look at what is\nthe quadratic risk",
    "start": "1786310",
    "end": "1791980"
  },
  {
    "text": "of this guy,\nasymptotically-- what is asymptotic quadratic risk? Well, it's 0 actually.",
    "start": "1791980",
    "end": "1797590"
  },
  {
    "text": "But if I assume that\nthis thing is true, that this thing is\npretty much Gaussian,",
    "start": "1797590",
    "end": "1803419"
  },
  {
    "text": "if I look at the\nquadratic risk, well, it's the expectation of the\nsquare of this thing. And so it's going to scale\nlike the variance divided by n.",
    "start": "1803419",
    "end": "1812312"
  },
  {
    "text": " The bias goes to\n0, just by this.",
    "start": "1812312",
    "end": "1818800"
  },
  {
    "text": "And then the quadratic\nrisk is going to scale like one over Fisher\ninformation divided by n. ",
    "start": "1818800",
    "end": "1828241"
  },
  {
    "text": "So here, the-- I'm not\nmentioning the constants. There must be constants, because\neverything is asymptotic. So for each finite\nn, I'm going to have",
    "start": "1828241",
    "end": "1833834"
  },
  {
    "text": "some constants that show up. ",
    "start": "1833834",
    "end": "1839270"
  },
  {
    "text": "Everybody just got their mind\nblown by this amazing theorem? So I mean, if you think about\nit, the MLE can be anything.",
    "start": "1839270",
    "end": "1848090"
  },
  {
    "text": "I'm sorry to say to\nyou, in many instances, the MLE is just going to be\nan average, which is just going to be slightly annoying.",
    "start": "1848090",
    "end": "1854660"
  },
  {
    "text": "But there are some\ncases where it's not. And we have to resort\nto this theorem rather than actually resorting\nto the central limit theorem",
    "start": "1854660",
    "end": "1861920"
  },
  {
    "text": "to prove this thing. And more importantly, even\nif this was an average, you don't have to even\nknow how to compute",
    "start": "1861920",
    "end": "1867710"
  },
  {
    "text": "the covariance matrix-- sorry, the variance\nof this thing to plug it into the\ncentral limit theorem.",
    "start": "1867710",
    "end": "1874490"
  },
  {
    "text": "I'm telling you, it's actually\ngiven by the Fisher information matrix. So if it's an average,\nbetween you and me,",
    "start": "1874490",
    "end": "1882070"
  },
  {
    "text": "you probably want to go the\ncentral limit theorem route if you want to prove\nthis kind of stuff. But if it's not, then\nthat's your best shot.",
    "start": "1882070",
    "end": "1888909"
  },
  {
    "text": "But you have to check\nthose conditions. I will give you for\ngranted the 0.5.",
    "start": "1888910",
    "end": "1898020"
  },
  {
    "text": "Ready? Any questions? We're going to wrap\nup this chapter four. So if you have questions,\nthat's the time.",
    "start": "1898020",
    "end": "1903440"
  },
  {
    "text": "Yes. AUDIENCE: What was the\nquadratic risk up there? PHILIPPE RIGOLLET: You\nmean the definition? AUDIENCE: No, the--\nwhat is was for this.",
    "start": "1903440",
    "end": "1909620"
  },
  {
    "text": "PHILIPPE RIGOLLET: Well,\nyou see the quadratic risk, if I think of it as\nbeing one dimensional, the quadratic risk\nis the expectation",
    "start": "1909620",
    "end": "1915272"
  },
  {
    "text": "of the square of the difference\nbetween theta hat and theta star. ",
    "start": "1915272",
    "end": "1921010"
  },
  {
    "text": "So that means that if I think\nof this as having a normal 0, 1, that's basically computing\nthe expectation of the square",
    "start": "1921010",
    "end": "1929680"
  },
  {
    "text": "of this Gaussian divided by n. I just divided by square\nroot of n on both sides.",
    "start": "1929680",
    "end": "1935759"
  },
  {
    "text": "So it's the expectation of\nthe square of this Gaussian. The Gaussian is mean 0, so\nthe expectation of the square is just a variance.",
    "start": "1935759",
    "end": "1943060"
  },
  {
    "text": "And so I'm left with 1 over\nthe Fisher information divided by n. AUDIENCE: I see. OK. ",
    "start": "1943060",
    "end": "1954084"
  },
  {
    "text": "PHILIPPE RIGOLLET: So let's\nmove on to chapter four. And this is the\nmethod of moments. So the method of moments is\nactually maybe a bit older",
    "start": "1954084",
    "end": "1962000"
  },
  {
    "text": "than maximum likelihood. And maximum likelihood is\ndated, say, early 20th century,",
    "start": "1962000",
    "end": "1968490"
  },
  {
    "text": "I mean as a systematic\nthing, because as I said, many of those guys are\ngoing to be averages. So finding an average is\nprobably a little older.",
    "start": "1968490",
    "end": "1976010"
  },
  {
    "text": "The method of moments,\nthere's some really nice uses. There's a paper by Pearson in\n1904, I believe, or maybe 1894.",
    "start": "1976010",
    "end": "1983679"
  },
  {
    "text": "I don't know.  And this paper, he was\nactually studying some species",
    "start": "1983679",
    "end": "1990860"
  },
  {
    "text": "of crab in an island,\nand he was trying to make some measurements. That's how he came up with this\nmodel of mixtures of Gaussians,",
    "start": "1990860",
    "end": "1996314"
  },
  {
    "text": "because there was actually\ntwo different populations in this populations of crab. And the way he actually\nfitted the parameters",
    "start": "1996314",
    "end": "2003580"
  },
  {
    "text": "was by doing the\nmethod of moments, except that since there\nwere a lot of parameters, he actually had to basically\nsolve six equations with six",
    "start": "2003580",
    "end": "2013580"
  },
  {
    "text": "unknowns. And that was a\ncomplete nightmare. And the guy did it by hand. And we don't know how\nhe did it actually.",
    "start": "2013580",
    "end": "2020140"
  },
  {
    "text": "But that is pretty impressive. So I want to start-- and this first part\nis a little brutal.",
    "start": "2020140",
    "end": "2028150"
  },
  {
    "text": "But this is a Course 18 class,\nand I do not want to give you-- So let's all agree that this\ncourse might be slightly more",
    "start": "2028150",
    "end": "2034510"
  },
  {
    "text": "challenging than AP statistics. And that means that it's\ngoing to be challenging just",
    "start": "2034510",
    "end": "2040539"
  },
  {
    "text": "during class. I'm not going to ask you about\nthe Weierstrass Approximation Theorem during the exams.",
    "start": "2040540",
    "end": "2045670"
  },
  {
    "text": "But what I want is to give\nyou mathematical motivations for what we're doing. And I can promise\nyou that maybe you",
    "start": "2045670",
    "end": "2052480"
  },
  {
    "text": "will have a slightly higher body\ntemperature during the lecture,",
    "start": "2052480",
    "end": "2057730"
  },
  {
    "text": "but you will come out\nsmarter of this class. And I'm trying to motivate to\nyou for using mathematical tool",
    "start": "2057730",
    "end": "2064810"
  },
  {
    "text": "and show you where interesting\nmathematical things that you might find dry elsewhere\nactually work very beautifully",
    "start": "2064810",
    "end": "2071800"
  },
  {
    "text": "in the stats literature. And one that we saw was using\nKullback-Leibler divergence out of motivation for maximum\nlikelihood estimation,",
    "start": "2071800",
    "end": "2078638"
  },
  {
    "text": "for example. So the Weierstrass\nApproximation Theorem is something that comes\nfrom pure analysis.",
    "start": "2078639",
    "end": "2085270"
  },
  {
    "text": "So maybe-- I mean, it took\nme a while before I saw that. And essentially,\nwhat it's telling you",
    "start": "2085270",
    "end": "2091239"
  },
  {
    "text": "is that if you look\nat a function that is continuous on\nan interval a, b-- on a segment a, b--",
    "start": "2091239",
    "end": "2097810"
  },
  {
    "text": "then you can actually\napproximate it uniformly well by\npolynomials as long",
    "start": "2097810",
    "end": "2105430"
  },
  {
    "text": "as you're willing\nto take the degree of this polynomials\nlarge enough. So the formal statement\nis, for any epsilon,",
    "start": "2105430",
    "end": "2111890"
  },
  {
    "text": "there exists the d that depends\non epsilon in a1 to ad-- so if you insist on having an\naccuracy which is 1/10,000,",
    "start": "2111890",
    "end": "2120400"
  },
  {
    "text": "maybe you're going to need a\npolynomial of degree 100,000, who knows. It doesn't tell you\nanything about this.",
    "start": "2120400",
    "end": "2126520"
  },
  {
    "text": "But it's telling you\nthat at least you have only a finite\nnumber of parameters to approximate those\nfunctions that typically",
    "start": "2126520",
    "end": "2131725"
  },
  {
    "text": "require an infinite number of\nparameters to be described. So that's actually quite nice. And that's the basis\nfor many things",
    "start": "2131725",
    "end": "2139510"
  },
  {
    "text": "and many polynomial\nmethods typically. And so here, it's\nuniform, so there's",
    "start": "2139510",
    "end": "2145540"
  },
  {
    "text": "this max over x that shows up\nthat's actually nice as well. That's Weierstrass\nApproximation Theorem.",
    "start": "2145540",
    "end": "2152200"
  },
  {
    "text": "Why is that useful to us? Well, in statistics, I\nhave a sample of X1 to Xn.",
    "start": "2152200",
    "end": "2158180"
  },
  {
    "text": "I have, say, a unified\nstatistical model. I'm not always\ngoing to remind you that it's identified--\nnot unified-- identified",
    "start": "2158180",
    "end": "2164200"
  },
  {
    "text": "statistical model. And I'm going to assume\nthat it has a density. You could think of\nit as having a PMF,",
    "start": "2164200",
    "end": "2170170"
  },
  {
    "text": "but think of it as having\na density for one second. Now, what I want is to\nfind the distribution.",
    "start": "2170170",
    "end": "2176769"
  },
  {
    "text": "I want to find theta. And finding theta,\nsince it's identified as equivalent to\nfinding P theta, which",
    "start": "2176770",
    "end": "2182590"
  },
  {
    "text": "is equivalent to finding f\ntheta, and knowing a function is the same--",
    "start": "2182590",
    "end": "2188410"
  },
  {
    "text": "knowing a density is the\nsame as knowing a density against any test function h. So that means that if I want\nto make sure I know a density--",
    "start": "2188410",
    "end": "2198588"
  },
  {
    "text": "if I want to check if two\ndensities are the same, all I have to do is to\ncompute their integral against all bounded\ncontinuous functions.",
    "start": "2198589",
    "end": "2206240"
  },
  {
    "text": "You already know\nthat it would be true if I checked for\nall functions h. But since f is a\ndensity, I can actually",
    "start": "2206240",
    "end": "2213170"
  },
  {
    "text": "look only at functions\nh that are bounded, say, between minus 1 and\n1, and that are continuous.",
    "start": "2213170",
    "end": "2224359"
  },
  {
    "text": "That's enough. Agreed? Well, just trust me on this. Yes, you have a question?",
    "start": "2224360",
    "end": "2231774"
  },
  {
    "text": "AUDIENCE: Why is this--\nlike, why shouldn't you just say that [INAUDIBLE]?",
    "start": "2231774",
    "end": "2237263"
  },
  {
    "text": " PHILIPPE RIGOLLET:\nYeah, I can do that. I'm just finding\na characterization",
    "start": "2237263",
    "end": "2243410"
  },
  {
    "text": "that's going to be\nuseful for me later on. I can find a bunch of them. But here, this one is\ngoing to be useful.",
    "start": "2243410",
    "end": "2248600"
  },
  {
    "text": "So all I need to say is that f\ntheta star integrated against X, h of x-- so this\nimplies that f--",
    "start": "2248600",
    "end": "2255700"
  },
  {
    "text": "if theta is equal to f\ntheta star not everywhere, but almost everywhere.",
    "start": "2255700",
    "end": "2261770"
  },
  {
    "text": "And that's only true if I\nguarantee to you that f theta and f theta stars are densities. This is not true\nfor any function.",
    "start": "2261770",
    "end": "2269180"
  },
  {
    "text": "So now, that means that, well,\nif I wanted to estimate theta hat, all I would have to do is\nto compute the average, right--",
    "start": "2269180",
    "end": "2276880"
  },
  {
    "text": "so this guy here, the integral-- let me clean up a bit my board.",
    "start": "2276880",
    "end": "2282480"
  },
  {
    "start": "2282480",
    "end": "2302590"
  },
  {
    "text": "So my goal is to find theta\nsuch that, if I look at f theta",
    "start": "2302590",
    "end": "2310350"
  },
  {
    "text": "and now I integrate it\nagainst h of x, then this gives me the same\nthing as if I were",
    "start": "2310350",
    "end": "2316540"
  },
  {
    "text": "to do it against f theta star.",
    "start": "2316540",
    "end": "2322860"
  },
  {
    "text": "And I want this for any h,\nwhich is continuous and bounded. ",
    "start": "2322860",
    "end": "2328390"
  },
  {
    "text": "So of course, I don't know\nwhat this quantity is. It depends on my\nunknown theta star. But I have theta from this.",
    "start": "2328390",
    "end": "2334600"
  },
  {
    "text": "And I'm going to do the usual-- the good old statistical\ntrick, which is, well, this I can write as\nthe expectation with respect",
    "start": "2334600",
    "end": "2341470"
  },
  {
    "text": "to P theta star of h theta of x. That's just the integral of\na function against something.",
    "start": "2341470",
    "end": "2348950"
  },
  {
    "text": "And so what I can do\nis say, well, now I don't know this guy. But my good old\ntrick from the book",
    "start": "2348950",
    "end": "2354069"
  },
  {
    "text": "is replace expectations\nby averages. And what I get-- ",
    "start": "2354070",
    "end": "2363049"
  },
  {
    "text": "And that's approximately by\nthe law of large numbers.",
    "start": "2363050",
    "end": "2369190"
  },
  {
    "text": "So if I can actually find\na function f theta such that when I integrate\nit against h",
    "start": "2369190",
    "end": "2376150"
  },
  {
    "text": "it gives me pretty much the\naverage of the evaluations of h over my data\npoints for all h,",
    "start": "2376150",
    "end": "2382890"
  },
  {
    "text": "then that should be\na good candidate.  The problem is that's a\nlot of functions to try.",
    "start": "2382890",
    "end": "2392040"
  },
  {
    "text": "Even if we reduced that\nfrom all possible functions to bounded and\ncontinuous ones, that's still a pretty large\ninfinite number of them.",
    "start": "2392040",
    "end": "2401490"
  },
  {
    "text": "And so what we can do is to use\nour Weierstrass Approximation Theorem. And it says, well, maybe I don't\nneed to test it against all h.",
    "start": "2401490",
    "end": "2409170"
  },
  {
    "text": "Maybe the polynomials\nare enough for me. So what I'm going to do is I'm\ngoing to look only at functions",
    "start": "2409170",
    "end": "2414570"
  },
  {
    "text": "h that are of the\nform sum of ak--",
    "start": "2414570",
    "end": "2420130"
  },
  {
    "text": "so h of x is sum of\nak X to the k-th for k",
    "start": "2420130",
    "end": "2429724"
  },
  {
    "text": "equals 0 to d-- only\npolynomials of degree d. So when I look at the\naverage of my h's, I'm",
    "start": "2429725",
    "end": "2437520"
  },
  {
    "text": "going to get a term\nlike the first one. So the first one here, this guy,\nbecomes 1/n sum from i equal 1",
    "start": "2437520",
    "end": "2447485"
  },
  {
    "text": "to n sum from k equal 0\nto d of ak Xi to the k-th.",
    "start": "2447485",
    "end": "2454290"
  },
  {
    "text": "That's just the average\nof the values of h of Xi. And now, what I need\nto do is to check",
    "start": "2454290",
    "end": "2460710"
  },
  {
    "text": "that it's the same\nthing when I integrate h of this form as well.",
    "start": "2460710",
    "end": "2466640"
  },
  {
    "text": "I want this to hold for all\npolynomials of degree d. That's still a lot of them.",
    "start": "2466640",
    "end": "2472109"
  },
  {
    "text": "There's still an infinite\nnumber of polynomials, because there's an infinite\nnumber of numbers a0 to ad",
    "start": "2472110",
    "end": "2477870"
  },
  {
    "text": "that describe those polynomials. But since those guys\nare polynomials,",
    "start": "2477870",
    "end": "2483410"
  },
  {
    "text": "it's actually enough for me\nto look only at the terms of the form X to the k-th--",
    "start": "2483410",
    "end": "2488420"
  },
  {
    "text": "no linear combination,\nno nothing. So actually, it's\nenough to look only at h of x, which is equal to X\nto the k-th for k equal 0 to d.",
    "start": "2488420",
    "end": "2500050"
  },
  {
    "text": " And now, how many of\nthose guys are there?",
    "start": "2500050",
    "end": "2506350"
  },
  {
    "text": "Just d plus 1, 0 to d. So that's actually a much\neasier thing for me to solve.",
    "start": "2506350",
    "end": "2511640"
  },
  {
    "text": " Now, this quantity, which is the\nintegral of f theta against X",
    "start": "2511640",
    "end": "2521970"
  },
  {
    "text": "to the k-th-- so that the\nexpectation of X to the k-th here-- it's called moment of order\nk, or k-th moment of P theta.",
    "start": "2521970",
    "end": "2532940"
  },
  {
    "text": "That's a moment. A moment is just the\nexpectation of the power. The mean is which moment?",
    "start": "2532940",
    "end": "2539780"
  },
  {
    "text": "The first moment. And variance is not\nexactly the second moment. It's the second moment minus\nthe first moment squared.",
    "start": "2539780",
    "end": "2547170"
  },
  {
    "text": " That's the variance. It's E of X squared\nminus E of X squared.",
    "start": "2547170",
    "end": "2554691"
  },
  {
    "text": "So those are things\nthat you already know. And then you can go higher. You can go to E of X\ncube, E of X blah, blah.",
    "start": "2554691",
    "end": "2560200"
  },
  {
    "text": "Here, I say go to\nE of X to the d-th. Now, as you can see,\nthis is not something you can really put\nin action right now,",
    "start": "2560200",
    "end": "2567781"
  },
  {
    "text": "because the Weierstrass\nApproximation Theorem does not tell you what d should be. Actually, we totally\nlost track of the epsilon",
    "start": "2567781",
    "end": "2574020"
  },
  {
    "text": "I was even looking for. I just said approximately\nequal, approximately equal. And so all this thing is\nreally just motivation.",
    "start": "2574020",
    "end": "2579300"
  },
  {
    "text": "But it's essentially\ntelling you that if you go to d large\nenough, technically",
    "start": "2579300",
    "end": "2585010"
  },
  {
    "text": "you should be able to identify\nexactly your distribution up to epsilon.",
    "start": "2585010",
    "end": "2591280"
  },
  {
    "text": "So I should be pretty good,\nif I go to d large enough. Now in practice, actually\nthere should be much",
    "start": "2591280",
    "end": "2599190"
  },
  {
    "text": "less than arbitrarily large d. Typically, we are going\nto need d which is 1 or 2.",
    "start": "2599190",
    "end": "2605460"
  },
  {
    "text": " So there are some limitations\nto the Weierstrass Approximation",
    "start": "2605460",
    "end": "2611720"
  },
  {
    "text": "Theorem. And there's a few. The first one is\nthat it only works for continuous functions, which\nis not so much of a problem.",
    "start": "2611720",
    "end": "2619849"
  },
  {
    "text": "That can be fixed. Well, we need bounded\ncontinuous functions. It works only on intervals.",
    "start": "2619850",
    "end": "2625961"
  },
  {
    "text": "That's annoying,\nbecause we're going to have random variables that\nare defined beyond intervals.",
    "start": "2625961",
    "end": "2631080"
  },
  {
    "text": "So we need something that just\ngoes beyond the intervals. And you can imagine that if\nyou let your functions be huge, it's going to be\nvery hard for you",
    "start": "2631080",
    "end": "2637256"
  },
  {
    "text": "to have a polynomial\napproximately [INAUDIBLE] well. Things are going to start going\nup and down at the boundary,",
    "start": "2637256",
    "end": "2642500"
  },
  {
    "text": "and it's going to be very hard. And again, as I\nsaid several times, it doesn't tell us\nwhat d should be.",
    "start": "2642500",
    "end": "2649160"
  },
  {
    "text": "And as statisticians, we're\nlooking for methods, not like principles of existence\nof a method that exists.",
    "start": "2649160",
    "end": "2655910"
  },
  {
    "text": "So if E is discrete,\nI can actually",
    "start": "2655910",
    "end": "2661839"
  },
  {
    "text": "get a handle on this d. If E is discrete and\nactually finite-- I'm going to actually\nlook at a finite E,",
    "start": "2661840",
    "end": "2669250"
  },
  {
    "text": "meaning that I have a PMF on,\nsay, r possible values, x1 and xr.",
    "start": "2669250",
    "end": "2674404"
  },
  {
    "text": "My random variable,\ncapital X, can take only r possible values. Let's think of them as being\nthe integer numbers 1 to r.",
    "start": "2674404",
    "end": "2681550"
  },
  {
    "text": "That's the number of\nsuccess out of r trials that I get, for example.",
    "start": "2681550",
    "end": "2686589"
  },
  {
    "text": "Binomial rp, that's exactly\nsomething like this.",
    "start": "2686590",
    "end": "2691640"
  },
  {
    "text": "So now, clearly this\nentire distribution is defined by the PMF, which\ngives me exactly r numbers.",
    "start": "2691640",
    "end": "2700452"
  },
  {
    "text": "So it can completely\ndescribe this distribution with r numbers. The question is, do I have an\nenormous amount of redundancy",
    "start": "2700452",
    "end": "2708290"
  },
  {
    "text": "if I try to describe this\ndistribution using moments? It might be that I need--\nsay, r is equal to 10,",
    "start": "2708290",
    "end": "2714970"
  },
  {
    "text": "maybe I have only 10 numbers\nto describe this thing, but I actually need to compute\nmoments up to the order of 100",
    "start": "2714970",
    "end": "2720980"
  },
  {
    "text": "before I actually recover\nentirely the distribution. Maybe I need to go infinite. Maybe the Weierstrass\nTheorem is the only thing",
    "start": "2720980",
    "end": "2727220"
  },
  {
    "text": "that actually saves me here. And I just cannot\nrecover it exactly. I can go to epsilon if I'm\nwilling to go to higher",
    "start": "2727220",
    "end": "2733340"
  },
  {
    "text": "and higher polynomials. Oh, by the way, in the\nWeierstrass Approximation Theorem, I can promise you\nthat as epsilon goes to 0,",
    "start": "2733340",
    "end": "2739190"
  },
  {
    "text": "d goes to infinity. So now, really I don't even\nhave actually r parameters.",
    "start": "2739190",
    "end": "2746160"
  },
  {
    "text": "I have only r minus parameter,\nbecause the last one-- because they sum up to 1.",
    "start": "2746160",
    "end": "2751500"
  },
  {
    "text": "So the last one I can\nalways get by doing 1 minus the sum of the\nfirst r minus 1 first.",
    "start": "2751500",
    "end": "2756960"
  },
  {
    "text": "Agreed? So each distribution\nr numbers is described by r minus 1 parameters.",
    "start": "2756960",
    "end": "2764700"
  },
  {
    "text": "The question is, can I\nuse only r minus moments to describe this guy? ",
    "start": "2764700",
    "end": "2772870"
  },
  {
    "text": "This is something called\nGaussian quadrature. The Gaussian quadrature\ntells you, yes, moments",
    "start": "2772870",
    "end": "2778930"
  },
  {
    "text": "are actually a good way to\nreparametrize your distribution in the sense that if\nI give you the moments",
    "start": "2778930",
    "end": "2784869"
  },
  {
    "text": "or if I give you the\nprobability mass function, I'm basically giving you\nexactly the same information. You can recover all the\nprobabilities from there.",
    "start": "2784870",
    "end": "2792770"
  },
  {
    "text": "So here, I'm going\nto denote by-- I'm going to drop the\nnotation in theta.",
    "start": "2792770",
    "end": "2797869"
  },
  {
    "text": "I don't have theta. Here, I'm talking about\nany generic distribution. And so I'm going to\ncall mk the k-th moment.",
    "start": "2797870",
    "end": "2804950"
  },
  {
    "text": " And I have a PMF, this\nis really the sum for j",
    "start": "2804950",
    "end": "2814610"
  },
  {
    "text": "equals 1 to r of xj to\nthe k-th times p of xj.",
    "start": "2814610",
    "end": "2826690"
  },
  {
    "text": "And this is the PMF. So that's my k-th moment.",
    "start": "2826690",
    "end": "2832099"
  },
  {
    "text": "So the k-th moment is a linear\ncombination of the numbers that I am interested in. ",
    "start": "2832100",
    "end": "2839750"
  },
  {
    "text": "So that's one equation. ",
    "start": "2839750",
    "end": "2845220"
  },
  {
    "text": "And I have as many\nequations as I'm actually willing to look at moments. So if I'm looking at 25\nmoments, I have 25 equations.",
    "start": "2845220",
    "end": "2854250"
  },
  {
    "text": "m1 equals blah with\nthis to the power of 1, m2 equals blah with this to\nthe power of 2, et cetera.",
    "start": "2854250",
    "end": "2860020"
  },
  {
    "text": "And then I also\nhave the equation that 1 is equal to the\nsum of the p of xj.",
    "start": "2860020",
    "end": "2871240"
  },
  {
    "text": "That's just the\ndefinition of PMF. So this is r's. They're ugly, but those are r's.",
    "start": "2871240",
    "end": "2878162"
  },
  {
    "text": " So now, this is a system\nof linear equations in p,",
    "start": "2878163",
    "end": "2884390"
  },
  {
    "text": "and I can actually write it\nin its canonical form, which is of the form a\nmatrix of those guys",
    "start": "2884390",
    "end": "2891410"
  },
  {
    "text": "times my parameters of interest\nis equal to a right hand side. The right hand side\nis the moments.",
    "start": "2891410",
    "end": "2897880"
  },
  {
    "text": "That means, if I\ndid you the moments, can you come back and\nfind what the PMF, because we know already\nfrom probability",
    "start": "2897880",
    "end": "2904869"
  },
  {
    "text": "that the PMF is all I need\nto know to fully describe my distribution. Given the moments,\nthat's unclear.",
    "start": "2904870",
    "end": "2912010"
  },
  {
    "text": "Now, here, I'm going to actually\ntake exactly r minus 1 moment",
    "start": "2912010",
    "end": "2917830"
  },
  {
    "text": "and this extra condition\nthat the sum of those guys should be 1. So that gives me r equations\nbased on r minus 1 moments.",
    "start": "2917830",
    "end": "2925240"
  },
  {
    "text": "And how many unknowns do I have? Well, I have my r\nunknown parameters",
    "start": "2925240",
    "end": "2934230"
  },
  {
    "text": "for the PMF, the r\nvalues of the PMF. Now, of course, this is\ngoing to play a huge role",
    "start": "2934230",
    "end": "2942540"
  },
  {
    "text": "in whether the are many\np's that give me the same. The goal is to find if there\nare several p's that can give me",
    "start": "2942540",
    "end": "2949619"
  },
  {
    "text": "the same moments. But if there's only one p that\ncan give me a set of moment, that means that I have a\none-to-one correspondence",
    "start": "2949620",
    "end": "2955260"
  },
  {
    "text": "between PMF and moments. And so if you give\nme the moments, I can just go back to the PMF.",
    "start": "2955260",
    "end": "2963074"
  },
  {
    "text": "Now, how do I go back? Well, by inverting this matrix. If I multiply this\nmatrix by its inverse,",
    "start": "2963074",
    "end": "2968710"
  },
  {
    "text": "I'm going to get the identity\ntimes the vector of p's equal the inverse of the\nmatrix times the m's.",
    "start": "2968710",
    "end": "2976890"
  },
  {
    "text": "So what we want to\ndo is to say that p is equal to the inverse of this\nbig matrix times the moments",
    "start": "2976890",
    "end": "2985349"
  },
  {
    "text": "that you give me. And if I can actually\ntalk about the inverse, then I have basically\na one-to-one mapping",
    "start": "2985350",
    "end": "2992410"
  },
  {
    "text": "between the m's, the\nmoments, and the matrix. So what I need to show is that\nthis matrix is invertible.",
    "start": "2992410",
    "end": "2998380"
  },
  {
    "text": "And we just decided\nthat the way to check if a matrix is invertible is\nby computing its determinant.",
    "start": "2998380",
    "end": "3005670"
  },
  {
    "text": "Who has computed a\ndeterminant before? Who was supposed to compute a\ndeterminant at least than just",
    "start": "3005670",
    "end": "3012820"
  },
  {
    "text": "to say, no, you\nknow how to do it. So you know how to\ncompute determinants. And if you've seen any\ndeterminant in class,",
    "start": "3012820",
    "end": "3019180"
  },
  {
    "text": "there's one that shows up in the\nexercises that professors love. And it's called the\nVandermonde determinant.",
    "start": "3019180",
    "end": "3025390"
  },
  {
    "text": "And it's the\ndeterminant of a matrix that have a very specific form. It looks like-- so there's\nbasically only r parameters",
    "start": "3025390",
    "end": "3031779"
  },
  {
    "text": "to this r by r matrix. The first row, or the\nfirst column-- sometimes, it's presented like that--",
    "start": "3031780",
    "end": "3037810"
  },
  {
    "text": "is this vector where each\nentry is to the power of 1. And the second one is each\nentry is to the power of 2,",
    "start": "3037810",
    "end": "3043800"
  },
  {
    "text": "and to the power of 3, and\nto the power 4, et cetera. So that's exactly what we\nhave-- x1 to the first, x2",
    "start": "3043800",
    "end": "3049410"
  },
  {
    "text": "to the first, all the\nway to xr to the first, and then same thing\nto the power of 2, all the way to the last one.",
    "start": "3049410",
    "end": "3054550"
  },
  {
    "text": "And I also need to add\nthe row of all 1's, which you can think of those guys are\nto the power of 0, if you want.",
    "start": "3054550",
    "end": "3061210"
  },
  {
    "text": "So I should really\nput it on top, if I wanted to have\na nice ordering. So that was the\nmatrix that I had.",
    "start": "3061210",
    "end": "3067290"
  },
  {
    "text": "And I'm not asking\nyou to check it. You can prove that by\ninduction actually, typically by doing the\nusual let's eliminate",
    "start": "3067290",
    "end": "3074190"
  },
  {
    "text": "some rows and columns\ntype of tricks that you do for matrices. So you basically start\nfrom the whole matrix.",
    "start": "3074190",
    "end": "3079197"
  },
  {
    "text": "And then you move onto a matrix\nthat has only one 1's and then 0's here. And then you have Vandermonde\nthat's just slightly smaller.",
    "start": "3079197",
    "end": "3085827"
  },
  {
    "text": "And then you just iterate. Yeah. AUDIENCE: I feel like there's a\nloss to either the supra index,",
    "start": "3085827",
    "end": "3091502"
  },
  {
    "text": "or the sub index should have\na k somewhere [INAUDIBLE].. ",
    "start": "3091502",
    "end": "3098119"
  },
  {
    "text": "[INAUDIBLE] the one\nI'm talking about? PHILIPPE RIGOLLET:\nYeah, I know, but I don't think the answer\nto your question is yes.",
    "start": "3098119",
    "end": "3105280"
  },
  {
    "text": "So k is the general\nindex, right? So there's no k. k does not\nexist. k just is here for me",
    "start": "3105280",
    "end": "3111180"
  },
  {
    "text": "to tell me for k equals 1 to r. So this is an r by r matrix.",
    "start": "3111180",
    "end": "3116250"
  },
  {
    "text": "And so there is no k there. So if you wanted\nthe generic term, if I wanted to put 1 in the\nmiddle on the j-th row and k-th",
    "start": "3116250",
    "end": "3123980"
  },
  {
    "text": "column, that would be x-- so j-th row would be x\nsub k to the power of j.",
    "start": "3123980",
    "end": "3133410"
  },
  {
    "text": "That would be the-- And so now, this is\nbasically the sum--",
    "start": "3133410",
    "end": "3139089"
  },
  {
    "text": "well, that should\nnot be strictly-- So that would be for j\nand k between 1 and r.",
    "start": "3139090",
    "end": "3145000"
  },
  {
    "text": "So this is the\nformula that get when you try to expand this\nVandermonde determinant. You have to do it only once when\nyou're a sophomore typically.",
    "start": "3145000",
    "end": "3152109"
  },
  {
    "text": "And then you can just go\non Wikipedia to do it. That's what I did. I actually made a\nmistake copying it. The first one should be 1\nless than or equal to j.",
    "start": "3152110",
    "end": "3159369"
  },
  {
    "text": "And the last one should be\nk less than or equal to r. And now what you\nhave is the product of the differences of xj and xk.",
    "start": "3159370",
    "end": "3165520"
  },
  {
    "text": " And for this thing\nto be non-zero, you need all the\nterms to be non-zero.",
    "start": "3165520",
    "end": "3171259"
  },
  {
    "text": "And for all the\nterms to be non-zero, you need to have no xi, xj, and\nno xj, xk that are identical.",
    "start": "3171259",
    "end": "3178412"
  },
  {
    "text": "If all those are\ndifferent numbers, then this product is going\nto be different from 0. And those are different\nnumbers, because those",
    "start": "3178412",
    "end": "3185010"
  },
  {
    "text": "are r possible values that\nyour random verbal takes. You're not going to\nsay that it takes two",
    "start": "3185010",
    "end": "3191360"
  },
  {
    "text": "with probability 1.5-- sorry, two with probability 0.5\nand two with probability 0.25.",
    "start": "3191360",
    "end": "3198170"
  },
  {
    "text": "You're going to say it takes two\nwith probability 0.75 directly. So those xj's are different.",
    "start": "3198170",
    "end": "3204350"
  },
  {
    "text": "These are the different values\nthat your random variable can take. ",
    "start": "3204350",
    "end": "3212200"
  },
  {
    "text": "Remember, xj, xk was just the\ndifferent values x1 to xr--",
    "start": "3212200",
    "end": "3217404"
  },
  {
    "text": "sorry-- was the different\nvalues that your random variable can take. Nobody in their right mind\nwould write twice the same value",
    "start": "3217404",
    "end": "3223796"
  },
  {
    "text": "in this list.  So my Vandermonde is non-zero.",
    "start": "3223796",
    "end": "3229119"
  },
  {
    "text": "So I can invert it. And I have a one-to-one\ncorrespondence between my entire PMF and\nthe first r minus 1's moments",
    "start": "3229119",
    "end": "3235970"
  },
  {
    "text": "to which I append the\nnumber 1, which is really the moment of order 0 again.",
    "start": "3235970",
    "end": "3242700"
  },
  {
    "text": "It's E of X to the\n0-th, which is 1. So good news, I only\nneed r minus 1 parameters",
    "start": "3242700",
    "end": "3250110"
  },
  {
    "text": "to describe r\nminus 1 parameters. And I can choose either\nthe values of my PMF. Or I can choose the r\nminus 1 first moments.",
    "start": "3250110",
    "end": "3256360"
  },
  {
    "text": " So the moments\ntell me something.",
    "start": "3256360",
    "end": "3262580"
  },
  {
    "text": "Here, it tells me that if I\nhave a discrete distribution with r possible\nvalues, I only need",
    "start": "3262580",
    "end": "3268160"
  },
  {
    "text": "to compute r minus 1 moments. So this is better than\nWeierstrass Approximation",
    "start": "3268160",
    "end": "3274471"
  },
  {
    "text": "Theorem. This tells me exactly how many\nmoments I need to consider. And this is for\nany distribution. This is not a\ndistribution that's",
    "start": "3274471",
    "end": "3281099"
  },
  {
    "text": "parametrized by one\nparameter, like the Poisson or the binomial\nor all this stuff.",
    "start": "3281100",
    "end": "3287210"
  },
  {
    "text": "This is for any distribution\nunder a finite number. So hopefully, if I\nreduce the family of PMFs",
    "start": "3287210",
    "end": "3293810"
  },
  {
    "text": "that I'm looking at to\na one-parameter family, I'm actually going to need\nto compute much less than r minus 1 values.",
    "start": "3293810",
    "end": "3301109"
  },
  {
    "text": "But this is actually hopeful. It tells you that\nthe method of moments is going to work for\nany distribution.",
    "start": "3301110",
    "end": "3306775"
  },
  {
    "text": "You just have to invert\na Vandermonde matrix. ",
    "start": "3306775",
    "end": "3313220"
  },
  {
    "text": "So just the conclusion--\nthe statistical conclusion-- is that moments contain\nimportant information",
    "start": "3313220",
    "end": "3320770"
  },
  {
    "text": "about the PMF and the PDF. If we can estimate these\nmoments accurately,",
    "start": "3320770",
    "end": "3326890"
  },
  {
    "text": "we can solve for the\nparameters of the distribution and recover the distribution.",
    "start": "3326890",
    "end": "3332674"
  },
  {
    "text": "And in a parametric\nsetting, where knowing P theta amounts\nto knowing theta, which is identifiability--\nthis is not innocuous--",
    "start": "3332674",
    "end": "3341270"
  },
  {
    "text": "it is often the case that\neven less moments are needed. After all, if theta is a\none dimensional parameter,",
    "start": "3341270",
    "end": "3346810"
  },
  {
    "text": "I have one parameter\nto estimate. Why would I go\nand get 25 moments to get this one parameter.",
    "start": "3346810",
    "end": "3352870"
  },
  {
    "text": "Typically, there\nis actually-- we will see that the\nmethod of moments just says if you have a\nd dimensional parameter,",
    "start": "3352870",
    "end": "3358480"
  },
  {
    "text": "just compute d\nmoments, and that's it. But this is only on\na case-by-case basis.",
    "start": "3358480",
    "end": "3364280"
  },
  {
    "text": "I mean, maybe your model will\ntotally screw up its parameters and you actually\nneed to get them.",
    "start": "3364280",
    "end": "3369950"
  },
  {
    "text": "I mean, think about it, if the\nfunction is parameterized just",
    "start": "3369950",
    "end": "3376710"
  },
  {
    "text": "by its 27th moment-- like, that's the only thing that\nmatters in this distribution, I just describe the function,\nit's just a density,",
    "start": "3376710",
    "end": "3384186"
  },
  {
    "text": "and the only thing that can\nchange from one distribution to another is this 27th moment-- well, then you're going to\nhave to go get the 27th moment.",
    "start": "3384187",
    "end": "3390900"
  },
  {
    "text": "And that probably means that\nyour modeling step was actually pretty bad. ",
    "start": "3390900",
    "end": "3397680"
  },
  {
    "text": "So the rule of thumb, if theta\nis in Rd, we need d moments. ",
    "start": "3397680",
    "end": "3406970"
  },
  {
    "text": "So what is the\nmethod of moments? ",
    "start": "3406970",
    "end": "3412800"
  },
  {
    "text": "That's just a good old trick. Replace the expectation\nby averages.",
    "start": "3412800",
    "end": "3418380"
  },
  {
    "text": "That's the beauty. The moments are expectations. So let's just replace the\nexpectations by averages",
    "start": "3418380",
    "end": "3424710"
  },
  {
    "text": "and then do it with\nthe average version, as if it was the true one.",
    "start": "3424710",
    "end": "3430200"
  },
  {
    "text": "So for example, I'm going to\ntalk about population moments, when I'm computing them\nwith the true distribution,",
    "start": "3430200",
    "end": "3436357"
  },
  {
    "text": "and I'm going to talk about\nthem empirical moments, when I talk about averages.",
    "start": "3436357",
    "end": "3442290"
  },
  {
    "text": "So those are the two\nquantities that I have. And now, what I hope\nis that there is.",
    "start": "3442290",
    "end": "3448430"
  },
  {
    "text": "So this is basically-- everything is here. That's where all the money is.",
    "start": "3448430",
    "end": "3453750"
  },
  {
    "text": "I'm going to assume there's\na function psi that maps my parameters-- let's\nsay they're in Rd--",
    "start": "3453750",
    "end": "3460119"
  },
  {
    "text": "to the set of the\nfirst d moments. ",
    "start": "3460120",
    "end": "3465490"
  },
  {
    "text": "Well, what I want to do\nis to come from this guy back to theta. So it better be that\nthis function is--",
    "start": "3465490",
    "end": "3470980"
  },
  {
    "text": " invertible. I want this function\nto be invertible.",
    "start": "3470980",
    "end": "3477385"
  },
  {
    "text": "In the Vandermonde\ncase, this function with just a linear function--\nmultiply a matrix by theta.",
    "start": "3477385",
    "end": "3483609"
  },
  {
    "text": "Then inverting a linear function\nis inverting the matrix. Then this is the same thing. So now what I'm\ngoing to assume--",
    "start": "3483610",
    "end": "3489520"
  },
  {
    "text": "and that's key for this method\nto work-- is that this theta-- so this function\npsi is one to one.",
    "start": "3489520",
    "end": "3496570"
  },
  {
    "text": "There's only one theta that\ngets only one set of moments.",
    "start": "3496570",
    "end": "3504360"
  },
  {
    "text": "And so if it's one to one, I\ncan talk about its inverse. And so now, I'm going to\nbe able to define theta as the inverse of the moments--",
    "start": "3504360",
    "end": "3512330"
  },
  {
    "text": "the reciprocal of the moments. And so now, what I get is that\nthe moment estimator is just",
    "start": "3512330",
    "end": "3517940"
  },
  {
    "text": "the thing where rather than\ntaking the true guys in there, I'm actually going to take the\nempirical moments in there.",
    "start": "3517940",
    "end": "3524779"
  },
  {
    "text": " Before we go any\nfurther, I'd like",
    "start": "3524780",
    "end": "3530530"
  },
  {
    "text": "to just go back and tell\nyou that this is not completely free.",
    "start": "3530530",
    "end": "3536380"
  },
  {
    "text": "How well-behaved\nyour function psi is going to play a huge role. ",
    "start": "3536380",
    "end": "3542490"
  },
  {
    "text": "Can somebody tell me what\nthe typical distance-- if I have a sample\nof size n, what is the typical distance between\nan average and the expectation?",
    "start": "3542490",
    "end": "3550062"
  },
  {
    "text": " What is the typical distance? What is the order of magnitude\nas a function of n between xn",
    "start": "3550062",
    "end": "3558920"
  },
  {
    "text": "bar and its expectation. AUDIENCE: 1 over\nsquare root of n.",
    "start": "3558920",
    "end": "3565000"
  },
  {
    "text": "PHILIPPE RIGOLLET: 1\nover square root n. That's what the central limit\ntheorem tells us, right? The central limit\ntheorem tells us that those things are\nbasically a Gaussian, which",
    "start": "3565000",
    "end": "3571521"
  },
  {
    "text": "is of order of 1 divided\nby its square of n. And so basically, I\nstart with something",
    "start": "3571521",
    "end": "3577670"
  },
  {
    "text": "which is 1 over square root\nof n away from the true thing. Now, if my function psi inverse\nis super steep like this--",
    "start": "3577670",
    "end": "3589730"
  },
  {
    "text": "that's psi inverse-- then\njust small fluctuations, even",
    "start": "3589730",
    "end": "3594970"
  },
  {
    "text": "if they're of order\n1 square root of n, can translate into giant\nfluctuations in the y-axis.",
    "start": "3594970",
    "end": "3604090"
  },
  {
    "text": "And that's going\nto be controlled by how steep psi inverse\nis, which is the same",
    "start": "3604090",
    "end": "3609640"
  },
  {
    "text": "as saying how flat psi is-- how flat is psi.",
    "start": "3609640",
    "end": "3615720"
  },
  {
    "text": "So if you go back to\nthis Vandermonde inverse, what it's telling you is that\nif this inverse matrix blows up",
    "start": "3615720",
    "end": "3626569"
  },
  {
    "text": "this guy a lot-- so if I start from a small\nfluctuation of this thing",
    "start": "3626570",
    "end": "3632566"
  },
  {
    "text": "and then they're\nblowing up by applying the inverse of\nthis matrix, things are not going to go well.",
    "start": "3632566",
    "end": "3637600"
  },
  {
    "text": "Anybody knows what is the number\nthat I should be looking for? So that's from, say,\nnumerical linear algebra",
    "start": "3637600",
    "end": "3645080"
  },
  {
    "text": "numerical methods. When I have a system\nof linear equations, what is the actual\nnumber I should",
    "start": "3645080",
    "end": "3650660"
  },
  {
    "text": "be looking at to\nknow how much I'm blowing up the fluctuations? Yeah. AUDIENCE: Condition number?",
    "start": "3650660",
    "end": "3655776"
  },
  {
    "text": "PHILIPPE RIGOLLET: The\ncondition number, right. So what's important here\nis the condition number of this matrix. If the condition number\nof this matrix is small,",
    "start": "3655776",
    "end": "3663714"
  },
  {
    "text": "then it's good. It's not going to blow up much. But if the condition\nnumber is very large, it's just going\nto blow up a lot.",
    "start": "3663715",
    "end": "3668720"
  },
  {
    "text": "And the condition\nnumber is the ratio of the largest and the\nsmallest eigenvalues. So you'll have to\nknow what it is.",
    "start": "3668720",
    "end": "3674720"
  },
  {
    "text": "But this is how all these\nthings get together. So the numerical\nstability translates",
    "start": "3674720",
    "end": "3681380"
  },
  {
    "text": "into statistical stability here. And numerical\nmeans just if I had",
    "start": "3681380",
    "end": "3686684"
  },
  {
    "text": "errors in measuring\nthe right hand side, how much would they translate\ninto errors on the left hand side. So the error here is intrinsic\nto statistical questions.",
    "start": "3686684",
    "end": "3693976"
  },
  {
    "text": " So that's my estimator,\nprovided that it exists.",
    "start": "3693976",
    "end": "3702490"
  },
  {
    "text": "And I said it's a one to\none, so it should exist, if I assume that\npsi is invertible.",
    "start": "3702490",
    "end": "3708520"
  },
  {
    "text": "So how good is this guy? That's going to be\ndefinitely our question-- how good is this thing.",
    "start": "3708520",
    "end": "3714859"
  },
  {
    "text": "And as I said, there's chances\nthat if psi is really steep,",
    "start": "3714860",
    "end": "3720560"
  },
  {
    "text": "then it should be\nnot very good-- if psi inverse is very steep,\nit should not be very good,",
    "start": "3720560",
    "end": "3726140"
  },
  {
    "text": "which means that it's-- well, let's just\nleave it to that.",
    "start": "3726140",
    "end": "3731480"
  },
  {
    "text": "So that means that\nI should probably see the derivative of\npsi showing up somewhere. If the derivative of psi\ninverse, say, is very large,",
    "start": "3731480",
    "end": "3739626"
  },
  {
    "text": "then I should actually\nhave a larger variance in my estimator. So hopefully, just like we\nhad a theorem that told us",
    "start": "3739626",
    "end": "3746900"
  },
  {
    "text": "that the Fisher information\nwas key in the variance of the maximum\nlikelihood estimator, we should have a\ntheorem that tells us",
    "start": "3746900",
    "end": "3752473"
  },
  {
    "text": "that the derivative\nof psi inverse is going to have a key role\nin the method of moments. So let's do it.",
    "start": "3752473",
    "end": "3758792"
  },
  {
    "start": "3758792",
    "end": "3777540"
  },
  {
    "text": "So I'm going to talk\nto you about matrices. So now, I have--",
    "start": "3777540",
    "end": "3782680"
  },
  {
    "start": "3782680",
    "end": "3790150"
  },
  {
    "text": "So since I have to manipulate\nd numbers at any given time, I'm just going to concatenate\nthem into a vector.",
    "start": "3790150",
    "end": "3797610"
  },
  {
    "text": "So I'm going to call\ncapital M theta-- so that's basically\nthe population moment.",
    "start": "3797610",
    "end": "3804569"
  },
  {
    "text": "And I have M hat, which is\njust m hat 1 to m hat d.",
    "start": "3804570",
    "end": "3811320"
  },
  {
    "text": "And that's my empirical moment. ",
    "start": "3811320",
    "end": "3819100"
  },
  {
    "text": "And what's going\nto play a role is what is the variance-covariance\nof the random vector.",
    "start": "3819100",
    "end": "3825369"
  },
  {
    "text": "So I have this vector 1-- do I have 1?",
    "start": "3825370",
    "end": "3830440"
  },
  {
    "text": "No, I don't have 1. ",
    "start": "3830440",
    "end": "3839240"
  },
  {
    "text": "So that's a d\ndimensional vector. And here, I take the\nsuccessive powers.",
    "start": "3839240",
    "end": "3844940"
  },
  {
    "text": "Remember, that looks very much\nlike a column of my Vandermonde matrix.",
    "start": "3844940",
    "end": "3850590"
  },
  {
    "text": "So now, I have\nthis random vector. It's just the successive powers\nof some random variable X. And the variance-covariance\nmatrix is the expectation--",
    "start": "3850590",
    "end": "3859480"
  },
  {
    "text": "so sigma-- of theta. The theta just means I'm\ngoing to take expectations with respect to theta.",
    "start": "3859480",
    "end": "3866310"
  },
  {
    "text": "That's the expectation\nwith respect to theta of this\nguy times this guy",
    "start": "3866310",
    "end": "3871316"
  },
  {
    "text": "transpose minus the same\nthing but with the expectation",
    "start": "3871316",
    "end": "3880575"
  },
  {
    "text": "inside.  Why do I do X, X1.",
    "start": "3880575",
    "end": "3886760"
  },
  {
    "text": "I have X, X2, X3.  X, X2, Xd times the\nexpectation of X, X2, Xd.",
    "start": "3886760",
    "end": "3904384"
  },
  {
    "text": "Everybody sees what this is? So this is a matrix where if I\nlook at the ij-th term of this",
    "start": "3904384",
    "end": "3911790"
  },
  {
    "text": "matrix-- or let's say, jk-th term,\nso on row j and column k,",
    "start": "3911790",
    "end": "3920980"
  },
  {
    "text": "I have sigma jk of theta.",
    "start": "3920980",
    "end": "3926130"
  },
  {
    "text": "And it's simply the\nexpectation of X to the j plus k-- well, Xj Xk minus\nexpectation of Xj expectation",
    "start": "3926130",
    "end": "3940541"
  },
  {
    "text": "of Xk.  So I can write this as m j plus\nk of theta minus mj of theta",
    "start": "3940541",
    "end": "3953970"
  },
  {
    "text": "times mk of theta. ",
    "start": "3953970",
    "end": "3960840"
  },
  {
    "text": "So that's my covariance matrix\nof this particular vector that I define.",
    "start": "3960840",
    "end": "3966869"
  },
  {
    "text": "And now, I'm going to\nassume that psi inverse-- well, if I want to\ntalk about the slope in an analytic fashion,\nI have to assume",
    "start": "3966870",
    "end": "3974060"
  },
  {
    "text": "that psi is differentiable. And I will talk\nabout the gradient of psi, which is, if\nit's one dimensional,",
    "start": "3974060",
    "end": "3980500"
  },
  {
    "text": "it's just the derivative. And here, that's where\nnotation becomes annoying. And I'm going to\nactually just assume",
    "start": "3980500",
    "end": "3986011"
  },
  {
    "text": "that so now I have a vector. But it's a vector\nof functions and I want to compute those functions\nat a particular value.",
    "start": "3986011",
    "end": "3992840"
  },
  {
    "text": "And the value I'm\nactually interested in is at the m of theta parameter. So psi inverse goes\nfrom the set of moments",
    "start": "3992840",
    "end": "4001600"
  },
  {
    "text": "to the set of parameters. So when I look at the\ngradient of this guy, it should be a function that\ntakes as inputs moments.",
    "start": "4001600",
    "end": "4008740"
  },
  {
    "text": "And where do I want this\nfunction to be evaluated at? At the true moment--",
    "start": "4008740",
    "end": "4014352"
  },
  {
    "text": "at the population moment vector. Just like when I computed\nmy Fisher information,",
    "start": "4014352",
    "end": "4020860"
  },
  {
    "text": "I was computing it at\nthe true parameter. So now, once they\ncompute this guy--",
    "start": "4020860",
    "end": "4028400"
  },
  {
    "text": "so now, why is this a\nd by d gradient matrix? ",
    "start": "4028400",
    "end": "4035840"
  },
  {
    "text": "So I have a gradient vector when\nI have a function from rd to r. This is the partial derivatives.",
    "start": "4035840",
    "end": "4042160"
  },
  {
    "text": "But now, I have a\nfunction from rd to rd. So I have to take the\nderivative with respect",
    "start": "4042160",
    "end": "4048210"
  },
  {
    "text": "to the arrival coordinate\nand the departure coordinate. ",
    "start": "4048210",
    "end": "4055260"
  },
  {
    "text": "And so that's the\ngradient matrix. And now, I have the\nfollowing properties.",
    "start": "4055260",
    "end": "4061359"
  },
  {
    "text": "The first one is that\nthe law of large numbers tells me that theta hat is a\nweakly or strongly consistent",
    "start": "4061360",
    "end": "4072720"
  },
  {
    "text": "estimator. So either I use the strong\nlaw of large numbers or the weak law\nof large numbers, and I get strong or\nweak consistency.",
    "start": "4072720",
    "end": "4081300"
  },
  {
    "text": "So what does that mean? Why is that true? Well, because now so I\nreally have the function--",
    "start": "4081300",
    "end": "4092470"
  },
  {
    "text": "so what is my estimator? Theta hat this psi inverse\nof m hat 1 to m hat k.",
    "start": "4092470",
    "end": "4103689"
  },
  {
    "text": "Now, by the law\nof large numbers, let's look only at the weak one.",
    "start": "4103689",
    "end": "4108889"
  },
  {
    "text": "Law of large numbers tells\nme that each of the mj hat",
    "start": "4108890",
    "end": "4115600"
  },
  {
    "text": "is going to converge\nin probability as n to infinity to the-- so\nthe empirical moments",
    "start": "4115600",
    "end": "4120970"
  },
  {
    "text": "converge to the\npopulation moments. That's what the good\nold trick is using,",
    "start": "4120970",
    "end": "4128189"
  },
  {
    "text": "the fact that the\nempirical moments are close to the true\nmoments as n becomes larger. And that's because, well,\njust because the m hat j's",
    "start": "4128189",
    "end": "4135389"
  },
  {
    "text": "are averages, and the\nlaw of large numbers works for averages. So now, plus if I look at my\ncontinuous mapping theorem,",
    "start": "4135390",
    "end": "4144930"
  },
  {
    "text": "then I have that psi inverse\nis continuously differentiable.",
    "start": "4144930",
    "end": "4150700"
  },
  {
    "text": "So it's definitely continuous. And so what I have is\nthat psi inverse of m hat",
    "start": "4150700",
    "end": "4156509"
  },
  {
    "text": "1 m hat d converges to psi\ninverse m1 to md, which",
    "start": "4156510",
    "end": "4168739"
  },
  {
    "text": "is equal to of theta star.",
    "start": "4168740",
    "end": "4173950"
  },
  {
    "text": "So that's theta star. By definition, we assumed that\nthat was the unique one that was actually doing this.",
    "start": "4173950",
    "end": "4180189"
  },
  {
    "text": "Again, this is a very\nstrong assumption. I mean, it's basically saying,\nif the method of moment works,",
    "start": "4180189",
    "end": "4186100"
  },
  {
    "text": "it works. So the fact that psi\ninverse one to one",
    "start": "4186100",
    "end": "4191710"
  },
  {
    "text": "is really the key to\nmaking this guy work. And then I also have a\ncentral limit theorem.",
    "start": "4191710",
    "end": "4197199"
  },
  {
    "text": "And the central limit\ntheorem is basically telling me that M hat\nis converging to M even",
    "start": "4197200",
    "end": "4204550"
  },
  {
    "text": "in the multivariate sense. So if I look at the vector of\nM hat and the true vector of M, then I actually make them go--",
    "start": "4204550",
    "end": "4211870"
  },
  {
    "text": "I look at the difference for\nscale by square root of n. It goes to some Gaussian. And usually, we would see--\nif it was one dimensional,",
    "start": "4211870",
    "end": "4218200"
  },
  {
    "text": "we would see the variance. Then we see the\nvariance-covariance matrix. Who has never seen the-- well,\nnobody answers this question.",
    "start": "4218200",
    "end": "4225370"
  },
  {
    "text": "Who has already seen the\nmultivariate central limit theorem? ",
    "start": "4225370",
    "end": "4231339"
  },
  {
    "text": "Who was never seen the\nmultivariate central limit theorem? So the multivariate\ncentral limit theorem",
    "start": "4231339",
    "end": "4237630"
  },
  {
    "text": "is basically just\nthe slight extension of the univariate one.",
    "start": "4237630",
    "end": "4243630"
  },
  {
    "text": "It just says that\nif I want to think-- so the univariate one would\ntell me something like this--",
    "start": "4243630",
    "end": "4251020"
  },
  {
    "start": "4251020",
    "end": "4265460"
  },
  {
    "text": "and 0. And then I would have basically\nthe variance of X to the j-th.",
    "start": "4265460",
    "end": "4278960"
  },
  {
    "text": "So that's what the central\nlimit theorem tells me. This is an average. ",
    "start": "4278960",
    "end": "4289350"
  },
  {
    "text": "So this is just for averages. The central limit\ntheorem tells me this. Just think of X to\nthe j-th as being y.",
    "start": "4289350",
    "end": "4296560"
  },
  {
    "text": "And that would be true. Everybody agrees with me? So now, this is\nactually telling me what's happening for all\nthese guys individually.",
    "start": "4296560",
    "end": "4305610"
  },
  {
    "text": "But what happens when those guys\nstart to correlate together? I'd like to know if\nthey actually correlate",
    "start": "4305610",
    "end": "4311090"
  },
  {
    "text": "the same way asymptotically. And so if I actually looked\nat the covariance matrix",
    "start": "4311090",
    "end": "4316760"
  },
  {
    "text": "of this vector-- ",
    "start": "4316760",
    "end": "4323440"
  },
  {
    "text": "so now, I need to look at\na matrix which is d by d-- then would those univariate\ncentral limit theorems",
    "start": "4323440",
    "end": "4330170"
  },
  {
    "text": "tell me-- so let me right like\nthis, double bar.",
    "start": "4330170",
    "end": "4336890"
  },
  {
    "text": "So that's just the\ncovariance matrix. This notation, V double bar is\nthe variance-covariance matrix.",
    "start": "4336890",
    "end": "4343050"
  },
  {
    "text": "So what this thing tells\nme-- so I know this thing is a matrix, d by d.",
    "start": "4343050",
    "end": "4350116"
  },
  {
    "text": "Those univariate central\nlimit theorems only give me information\nabout the diagonal terms.",
    "start": "4350117",
    "end": "4356150"
  },
  {
    "text": "But here, I have no idea where\nthe covariance matrix is. This guy is telling me, for\nexample, that this thing is",
    "start": "4356150",
    "end": "4366020"
  },
  {
    "text": "like variance of X to the j-th. But what if I want to\nfind off-diagonal elements",
    "start": "4366020",
    "end": "4371860"
  },
  {
    "text": "of this matrix? Well, I need to use a\nmultivariate central limit theorem. And really what it's telling\nme is that you can actually",
    "start": "4371860",
    "end": "4378670"
  },
  {
    "text": "replace this guy here-- ",
    "start": "4378670",
    "end": "4390450"
  },
  {
    "text": "so that goes in distribution\nto some normal mean 0, again. And now, what I\nhave is just sigma",
    "start": "4390450",
    "end": "4397080"
  },
  {
    "text": "of theta, which is just\nthe covariance matrix of this vector X, X2, X3,\nX4, all the way to Xd.",
    "start": "4397080",
    "end": "4406696"
  },
  {
    "text": "And that's it. So that's a\nmultivariate Gaussian. Who has never seen a\nmultivariate Gaussian?",
    "start": "4406696",
    "end": "4413040"
  },
  {
    "text": "Please, just go on\nWikipedia or something. There's not much\nto know about it. But I don't have time to\nredo probability here.",
    "start": "4413040",
    "end": "4420270"
  },
  {
    "text": "So we're going to\nhave to live with it. Now, to be fair,\nif your goal is not",
    "start": "4420270",
    "end": "4426230"
  },
  {
    "text": "to become a\nstatistical savant, we will stick to\nunivariate questions",
    "start": "4426230",
    "end": "4432490"
  },
  {
    "text": "in the scope of\nhomework and exams.",
    "start": "4432490",
    "end": "4441260"
  },
  {
    "text": "So now, what was the\ndelta method telling me?",
    "start": "4441260",
    "end": "4449900"
  },
  {
    "text": "It was telling me that if I had\na central limit theorem that told me that theta hat\nwas going to theta,",
    "start": "4449900",
    "end": "4456112"
  },
  {
    "text": "or square root of n\ntheta hat minus theta was going to some\nGaussian, then I could look at square root of Mg\nof theta hat minus g of theta.",
    "start": "4456112",
    "end": "4463219"
  },
  {
    "text": "And this thing was also\ngoing to a Gaussian. But what it had to\nbe is the square of the derivative of\ng in the variance.",
    "start": "4463220",
    "end": "4472699"
  },
  {
    "text": "So the delta method,\nit was just a way to go from square\nroot of n theta",
    "start": "4472700",
    "end": "4478280"
  },
  {
    "text": "hat n minus theta goes to some\nN, say 0, sigma squared, to--",
    "start": "4478280",
    "end": "4486809"
  },
  {
    "text": "so delta method was telling\nme that this was square root Ng of theta hat N\nminus g of theta",
    "start": "4486810",
    "end": "4496030"
  },
  {
    "text": "was going in distribution\nto N0 sigma squared",
    "start": "4496030",
    "end": "4501770"
  },
  {
    "text": "g prime squared of theta. ",
    "start": "4501770",
    "end": "4507210"
  },
  {
    "text": "That was the delta method. Now, here, we have a\nfunction of those guys.",
    "start": "4507210",
    "end": "4512699"
  },
  {
    "text": "The central limit theorem,\neven the multivariate one, is only guaranteeing something\nfor me regarding the moments.",
    "start": "4512700",
    "end": "4520180"
  },
  {
    "text": "But now, I need to map the\nmoments back into some theta, so I have a function\nof the moments.",
    "start": "4520180",
    "end": "4526230"
  },
  {
    "text": "And there is something\ncalled the multivariate delta",
    "start": "4526230",
    "end": "4531950"
  },
  {
    "text": "method, where derivatives\nare replaced by gradients. Like, they always are in\nmultivariate calculus.",
    "start": "4531950",
    "end": "4539310"
  },
  {
    "text": "And rather than multiplying,\nsince things do not compute, rather than choosing which\nside I want to put the square,",
    "start": "4539310",
    "end": "4546557"
  },
  {
    "text": "I'm actually just going to take\nhalf of the square on one side and the other half of the\nsquare on the other side.",
    "start": "4546557",
    "end": "4551810"
  },
  {
    "text": "So the way you\nshould view this, you should think of sigma\nsquared times g prime squared",
    "start": "4551810",
    "end": "4559160"
  },
  {
    "text": "as being g prime of\ntheta times sigma squared times g prime of theta.",
    "start": "4559160",
    "end": "4566040"
  },
  {
    "text": "And now, this is\ncompletely symmetric. And the multivariate\ndelta method",
    "start": "4566040",
    "end": "4574850"
  },
  {
    "text": "is basically telling you that\nyou get the gradient here.",
    "start": "4574850",
    "end": "4580010"
  },
  {
    "text": "So you start from\nsomething that's like that over there, a sigma-- so that's my sigma squared,\nthink of sigma squared.",
    "start": "4580010",
    "end": "4586280"
  },
  {
    "text": "And then I premultiply by\nthe gradient and postmultiply by the gradient. The first one is transposed.",
    "start": "4586280",
    "end": "4591680"
  },
  {
    "text": "The second one is not. But that's very\nstraightforward extension. You don't even have\nto understand it.",
    "start": "4591680",
    "end": "4597890"
  },
  {
    "text": "Just think of what would be\nthe natural generalization. Here, by the way,\nI wrote explicitly",
    "start": "4597890",
    "end": "4604449"
  },
  {
    "text": "what the gradient of a\nmultivariate function is. So that's a function\nthat goes from Rd to Rk.",
    "start": "4604450",
    "end": "4613930"
  },
  {
    "text": "So now, the gradient\nis a d by k matrix.  And so now, for this\nguy, we can do it",
    "start": "4613930",
    "end": "4620504"
  },
  {
    "text": "for the method or moments. And we can see that\nbasically we're going to have this\nscaling that depends on the gradient of the\nreciprocal of psi, which",
    "start": "4620504",
    "end": "4628300"
  },
  {
    "text": "is normal. Because if psi is super steep,\nif psi inverse is super steep, then the gradient\nis going to be huge,",
    "start": "4628300",
    "end": "4634720"
  },
  {
    "text": "which is going to translate\ninto having a huge variance for the method of moments. ",
    "start": "4634720",
    "end": "4641180"
  },
  {
    "text": "So this is actually the end. I would like to encourage\nyou-- and we'll probably do it",
    "start": "4641180",
    "end": "4646460"
  },
  {
    "text": "on Thursday just to start. But I encourage you do\nit in one dimension, so that you know how to\nuse the method of moments,",
    "start": "4646460",
    "end": "4655070"
  },
  {
    "text": "you know how to do\na bunch of things. Do it in one dimension and see\nhow you can check those things.",
    "start": "4655070",
    "end": "4660470"
  },
  {
    "text": "So just as a quick comparison,\nin terms of the quadratic risk, the maximum likelihood\nestimator is typically",
    "start": "4660470",
    "end": "4666050"
  },
  {
    "text": "more accurate than\nthe method of moments. What is pretty\ngood to do is, when",
    "start": "4666050",
    "end": "4671440"
  },
  {
    "text": "you have a\nnon-concave likelihood function, what\npeople like to do is to start with the method of\nmoments as an initialization",
    "start": "4671440",
    "end": "4678980"
  },
  {
    "text": "and then run some algorithm\nthat optimizes locally the likelihood starting\nfrom this point, because it's actually\nlikely to be closer.",
    "start": "4678980",
    "end": "4685985"
  },
  {
    "text": "And then the MLE is\ngoing to improve it a little bit by pushing the\nlikelihood a little better.",
    "start": "4685985",
    "end": "4692010"
  },
  {
    "text": "So of course, the\nmaximum likelihood is sometimes intractable. Whereas, computing\nmoments is fairly doable.",
    "start": "4692010",
    "end": "4698440"
  },
  {
    "text": "If the likelihood is\nconcave, as I said, we can use optimization\nalgorithms, such as interior-point\nmethods or gradient descent,",
    "start": "4698440",
    "end": "4704020"
  },
  {
    "text": "I guess, to maximize it. And if the likelihood\nis non-concave, we only have local heuristics. Risk And that's what I meant--",
    "start": "4704020",
    "end": "4709920"
  },
  {
    "text": "you have only local maxima. And one trick you can do-- so your likelihood\nlooks like this,",
    "start": "4709920",
    "end": "4717880"
  },
  {
    "text": "and it might be the case that if\nyou have a lot of those peaks, you basically have to start\nyour algorithm in each",
    "start": "4717880",
    "end": "4724810"
  },
  {
    "text": "of those peaks. But the method of\nmoments can actually start you in the right\npeak, and then you",
    "start": "4724810",
    "end": "4730510"
  },
  {
    "text": "just move up by doing\nsome local algorithm for maximum likelihood. So that's not key.",
    "start": "4730510",
    "end": "4736180"
  },
  {
    "text": "But that's just if you want\nto think about algorithmically how I would end up doing this\nand how can I combine the two.",
    "start": "4736180",
    "end": "4743470"
  },
  {
    "text": "So I'll see you on Thursday. Thank you. ",
    "start": "4743470",
    "end": "4748944"
  }
]