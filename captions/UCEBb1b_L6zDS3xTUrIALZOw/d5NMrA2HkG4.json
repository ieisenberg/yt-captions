[
  {
    "text": " The following\ncontent is provided under a Creative\nCommons license. Your support will help MIT\nOpenCourseWare continue",
    "start": "0",
    "end": "6870"
  },
  {
    "text": "to offer high quality\neducational resources for free. To make a donation, or\nview additional materials",
    "start": "6870",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "26810"
  },
  {
    "text": "PROFESSOR: Any questions from\nlast time about Gibbs Sampling? ",
    "start": "26810",
    "end": "32450"
  },
  {
    "text": "No? So at the end, we\nintroduced this concept",
    "start": "32450",
    "end": "38210"
  },
  {
    "text": "of relative entropy. So I just wanted to briefly\nreview this, and make sure it's clear to everyone.",
    "start": "38210",
    "end": "44050"
  },
  {
    "text": "So the relative\nentropy is a measure of distance between\nprobability distributions--",
    "start": "44050",
    "end": "51340"
  },
  {
    "text": "can be written different ways,\noften with this D p q notation. And as you'll see, it's\nthe mean bit-score,",
    "start": "51340",
    "end": "59949"
  },
  {
    "text": "if you're scoring a motif\nwith a foreground model, pk, and a background model,\nqk it's the average log",
    "start": "59950",
    "end": "68610"
  },
  {
    "text": "odd score under the motif model. And I asked you to show\nthat under the special case",
    "start": "68610",
    "end": "77040"
  },
  {
    "text": "where qk is 1 over\n4 to the w, that is uniform background, that the\nrelative entropy of the motif",
    "start": "77040",
    "end": "84930"
  },
  {
    "text": "ends up being simply 2w minus 8. Did anyone have a\nchance to do this?",
    "start": "84930",
    "end": "93280"
  },
  {
    "text": "It's pretty simple--\nhas anyone done this? Can anyone show this?",
    "start": "93280",
    "end": "99340"
  },
  {
    "text": "Want me to do it briefly? How many would like to\nactually see this derivation? It's very, very quick. Few people, OK.",
    "start": "99340",
    "end": "104790"
  },
  {
    "text": "So I'll just do\nthat really quick. So summation Pk log Pk over\nqk equals-- so you rewrite it",
    "start": "104790",
    "end": "116640"
  },
  {
    "text": "as a difference, the\nlog of a quotient",
    "start": "116640",
    "end": "122270"
  },
  {
    "text": "is the difference of the log. Of summation log Pk plus\nsummation Pk log qk.",
    "start": "122270",
    "end": "132850"
  },
  {
    "text": "OK, and then the special case\nthat we're dealing with here is that qk is\nequal to a quarter,",
    "start": "132850",
    "end": "138140"
  },
  {
    "text": "if we're dealing\nwith the simplest case of a one-base motif. And so you recognize that\nthat's minus H of Pp, right?",
    "start": "138140",
    "end": "148180"
  },
  {
    "text": "H of p is defined as minus\nthat, so it's minus H p. And this here, that's\njust a quarter.",
    "start": "148180",
    "end": "154730"
  },
  {
    "text": "Log 2 of a quarter is minus 2. You can take the minus\n2 outside of the sum, so you're ending up with\nminus 2-- I'm sorry.",
    "start": "154730",
    "end": "165692"
  },
  {
    "text": "How come Sally\ndidn't correct me? Usually she catches\nthese things. So that's a minus there,\nright, because we're",
    "start": "165692",
    "end": "171189"
  },
  {
    "text": "taking the difference. And so then we have a minus\n2 that we're pulling out",
    "start": "171189",
    "end": "176830"
  },
  {
    "text": "from this, and you're\nleft with summation Pk. And summation Pk, it sums to 1.",
    "start": "176830",
    "end": "182900"
  },
  {
    "text": "So that's just 1. And so this equals minus\nminus 2, or 2 minus H of p.",
    "start": "182900",
    "end": "191010"
  },
  {
    "text": "And there are many other\nresults of this type that can be shown in\ninformation theory.",
    "start": "191010",
    "end": "199422"
  },
  {
    "text": "Often there are\nsome simple results you can get simply by\nusing this by splitting it into different\nterms, and summing.",
    "start": "199422",
    "end": "206620"
  },
  {
    "text": "So another result that\nI mentioned earlier, without showing, is that if you\nhave a motif, say, of length 2,",
    "start": "206620",
    "end": "215310"
  },
  {
    "text": "that the information\ncontent of that motif model can be broken into\nthe information",
    "start": "215310",
    "end": "223230"
  },
  {
    "text": "content of each position\nif your model is such that the positions\nare independent.",
    "start": "223230",
    "end": "228700"
  },
  {
    "text": "So you would have,\nin that case-- let's just take the entropy of\na model on [? dinucleotides. ?]",
    "start": "228700",
    "end": "236740"
  },
  {
    "text": "It that would be minus\nsummation Pi Pj log Pi Pj,",
    "start": "236740",
    "end": "245520"
  },
  {
    "text": "if you have a model where\nthe two are independent, and this sum would be\ntaken over both i and j.",
    "start": "245520",
    "end": "252794"
  },
  {
    "text": "And so if you want to show\nthat this is equal to-- I claim that this is\nequal to the i--",
    "start": "252794",
    "end": "271500"
  },
  {
    "text": "Anyway, if you have different\npositions, in general-- this would be the more general\nterm-- where you have two different compositions at the\ntwo positions for the motif.",
    "start": "271500",
    "end": "279400"
  },
  {
    "text": "And then you can show that\nit's equal to basically the sum",
    "start": "279400",
    "end": "290860"
  },
  {
    "text": "the entropies at\nthe two positions.",
    "start": "290860",
    "end": "296887"
  },
  {
    "text": "OK, you do the same thing. You separate out\nthe log of the sum, in terms of the sum of\nthe logs, and then you",
    "start": "296887",
    "end": "303949"
  },
  {
    "text": "do properties of summations\nuntil you get the answer. OK, so this is your\nhomework, and obviously it",
    "start": "303950",
    "end": "310804"
  },
  {
    "text": "won't be graded. But we'll check in\nnext Thursday and see if anyone has\nquestions with that.",
    "start": "310804",
    "end": "316580"
  },
  {
    "text": "So what is the use\nof relative entropy? the main use in\nbio-informatics is",
    "start": "316580",
    "end": "321810"
  },
  {
    "text": "that it's a measure that\ntakes into account non-uniform",
    "start": "321810",
    "end": "326830"
  },
  {
    "text": "backgrounds. The standard definition\nof information basically works\nwhen the background",
    "start": "326830",
    "end": "333320"
  },
  {
    "text": "is uniform, but falls apart\nwhen it's non-uniform. So if you have a\nvery biased genome,",
    "start": "333320",
    "end": "340050"
  },
  {
    "text": "like this one shown\nhere which is 75% A T, then the information content\nusing the standard method",
    "start": "340050",
    "end": "345910"
  },
  {
    "text": "would be two bits of this\nmotif, which is P C equals 1. But then, that would\npredict, using the formula,",
    "start": "345910",
    "end": "353860"
  },
  {
    "text": "that a motif occurs 2 to\nthe information content--",
    "start": "353860",
    "end": "360189"
  },
  {
    "text": "once every 2 to the information\ncontent bases-- that would be 2 to the 2,\nwhich would be 4 bases, and that's clearly\nincorrect in this case.",
    "start": "360190",
    "end": "366949"
  },
  {
    "text": "But the relative\nentropy, if you do it, there will be four terms, but\nthree of them just have a 0.",
    "start": "366950",
    "end": "376250"
  },
  {
    "text": "And then one of them has a 1,\nso it's 1 times log 1 over 1/8,",
    "start": "376250",
    "end": "388760"
  },
  {
    "text": "in this case, and that's\nwill be equal to 3.",
    "start": "388760",
    "end": "394157"
  },
  {
    "text": "And so the relative\nentropy clearly gives you a more\nsensible version. It's a good measure for\nnon-uniform backgrounds.",
    "start": "394157",
    "end": "403190"
  },
  {
    "text": "Questions about\nrelative entropy? ",
    "start": "403190",
    "end": "408350"
  },
  {
    "text": "All right, so then\nwe said you can use a weight matrix, or a\nposition-specific probability matrix, for a motif like this\nfive-prime splice site motif,",
    "start": "408350",
    "end": "416300"
  },
  {
    "text": "assuming independence\nbetween positions. But if that's not true, then\na natural generalization",
    "start": "416300",
    "end": "422840"
  },
  {
    "text": "would be an inhomogeneous\nMarkov model.",
    "start": "422840",
    "end": "428400"
  },
  {
    "text": "So now, we're going to say\nthat the base at position k",
    "start": "428400",
    "end": "435530"
  },
  {
    "text": "depends on the base\nat position k minus 1, but not on anything before that.",
    "start": "435530",
    "end": "441140"
  },
  {
    "text": "And so, the probability\nof generating a particular sequence, S1 to S9,\nis now given by this expression",
    "start": "441140",
    "end": "449500"
  },
  {
    "text": "here, where you have for\nevery base after the first,",
    "start": "449500",
    "end": "456170"
  },
  {
    "text": "you have a conditional\nprobability. This is the\nconditional probability of seeing the base, S2,\nat position minus 2, given",
    "start": "456170",
    "end": "462354"
  },
  {
    "text": "that you saw S1 at position\nminus 3, and so forth. And again, you can take the log\nfor convenience, if you like.",
    "start": "462355",
    "end": "471160"
  },
  {
    "text": "So I actually implemented\nboth of these models. So just for thinking about it,\nif you want to implement this,",
    "start": "471160",
    "end": "481080"
  },
  {
    "text": "you have parameters-- these\nconditional probability parameters-- and you\nestimate them as shown here.",
    "start": "481080",
    "end": "488870"
  },
  {
    "text": "So remember, conditional\nprobability of A given B",
    "start": "488870",
    "end": "496340"
  },
  {
    "text": "is the joint probability\ndivided by the probability of B.",
    "start": "496340",
    "end": "501660"
  },
  {
    "text": "And so in this case, that\nwould be the joint probability of seeing C A at minus\n3, minus 2, divided",
    "start": "501660",
    "end": "508520"
  },
  {
    "text": "by the probability of\nseeing C at minus 3. You could have the ratio of the\nfrequencies, or, in this case,",
    "start": "508520",
    "end": "514924"
  },
  {
    "text": "the counts, because the\nnormalization constant will cancel. Is that clear?",
    "start": "514924",
    "end": "520770"
  },
  {
    "text": "So I actually implemented\nboth the weight matrix model and a first-order Markov model\nof five-prime splice sites, and scored some\ngenomic sequence.",
    "start": "520770",
    "end": "527750"
  },
  {
    "text": "And what you can see here, the\nunits are in 1/10th-bit units,",
    "start": "527750",
    "end": "533460"
  },
  {
    "text": "is that they both are partially\nsuccessful in separating real",
    "start": "533460",
    "end": "538790"
  },
  {
    "text": "five-prime splice sites-- shown\nin black from the background, shown in light bars--\nbut in both cases,",
    "start": "538790",
    "end": "545399"
  },
  {
    "text": "it's not a perfect separation. There's some overlap here. And if you zoom\nthere, you can see",
    "start": "545400",
    "end": "550500"
  },
  {
    "text": "that the Markov model\nis a little bit better. It has a tighter\ntail on the left.",
    "start": "550500",
    "end": "558170"
  },
  {
    "text": "So it's generally\nseparating the true from the decoys a\nlittle bit better.",
    "start": "558170",
    "end": "563256"
  },
  {
    "text": "Not dramatically better,\nbut slightly better. Yes, question? AUDIENCE: From the\nprevious slide,",
    "start": "563256",
    "end": "568365"
  },
  {
    "text": "could you clarify what the\nletter R and the letter S are?",
    "start": "568365",
    "end": "573990"
  },
  {
    "text": "PROFESSOR: Yes,\nsorry about that. R would be the odd\nratio-- so it's the ratio of the\nprobability of generating",
    "start": "573990",
    "end": "581180"
  },
  {
    "text": "that sequence under\nthe foreground model-- the plus\nmodel, we're calling it-- divided by the probability\nunder the background,",
    "start": "581180",
    "end": "589480"
  },
  {
    "text": "or minus, model. And then, I think I\npointed out last time that when you get\nproducts of probabilities,",
    "start": "589480",
    "end": "596780"
  },
  {
    "text": "they tend to get very small. This can cause\ncomputational problems. And so if you just take the\nlog, you convert it into a sum.",
    "start": "596780",
    "end": "604430"
  },
  {
    "text": "And so we'll often\nuse score, or S, for the log of the odds ratio.",
    "start": "604430",
    "end": "612440"
  },
  {
    "text": "Sorry, should have\nmarked that more clearly. ",
    "start": "612440",
    "end": "617879"
  },
  {
    "text": "So Markov models can\nimprove performance, when there is dependence,\nand when you have enough data",
    "start": "617880",
    "end": "623890"
  },
  {
    "text": "to estimate the increased\nnumber of parameters. And it doesn't just\nhave to be dependence",
    "start": "623890",
    "end": "630710"
  },
  {
    "text": "on the previous base--\nyou can have a model where the probability of\nthe next base depends",
    "start": "630710",
    "end": "637709"
  },
  {
    "text": "on the two previous bases. That would be called a\nsecond-order Markov model, or in general, a\nK-order Markov model.",
    "start": "637710",
    "end": "643340"
  },
  {
    "text": " Sometimes, these dependencies\nactually occur in practice.",
    "start": "643340",
    "end": "651257"
  },
  {
    "text": "With five-prime splice\nsites, it's a nice example, because there's probably\na couple thousand of them in the human genome, and\nwe know them very well,",
    "start": "651257",
    "end": "657550"
  },
  {
    "text": "so you can make\nquite complex models and have enough\ndata to train them.",
    "start": "657550",
    "end": "663050"
  },
  {
    "text": "But in general,\nif you're thinking about modeling a transcription\nfactor binding site, or something, often you might\nhave dozens or, at best,",
    "start": "663050",
    "end": "670399"
  },
  {
    "text": "hundreds of examples, typically. And so you might not\nhave enough to train some of the larger model.",
    "start": "670400",
    "end": "676680"
  },
  {
    "text": "So how many\nparameters do you need to fit a K-order Markov model?",
    "start": "676680",
    "end": "681700"
  },
  {
    "text": "So question first, yeah? AUDIENCE: [INAUDIBLE]\nIf you're comparing the first-order Markov models\nwith W M M, what is W M M?",
    "start": "681700",
    "end": "692000"
  },
  {
    "text": "PROFESSOR: Weight matrix, or\nposition-specific probability matrix.",
    "start": "692000",
    "end": "697400"
  },
  {
    "text": "Just a model of independence\nbetween the two. ",
    "start": "697400",
    "end": "704370"
  },
  {
    "text": "Coming back to this case. So let's suppose\nyou are thinking about making a\nK-order Markov model,",
    "start": "704370",
    "end": "709659"
  },
  {
    "text": "because you do some\nstatistical tasks and you find there's some\ndependence between sets of positions in your motif.",
    "start": "709659",
    "end": "716459"
  },
  {
    "text": "How many parameters\nwould there be? So if you have an independence\nmodel, or weight matrix",
    "start": "716460",
    "end": "722110"
  },
  {
    "text": "or position-specific\nprobability matrix, there are four parameters\nat each position,",
    "start": "722110",
    "end": "729110"
  },
  {
    "text": "the probabilities\nof the four bases. This will be only\nthree free parameters,",
    "start": "729110",
    "end": "734329"
  },
  {
    "text": "because the fourth one-- but\nlet's just think about it as four, four parameters\ntimes the width of the motif.",
    "start": "734330",
    "end": "740260"
  },
  {
    "text": "So if I now go to a\nfirst-order Markov model, now there's more\nparameters, because I",
    "start": "740260",
    "end": "745540"
  },
  {
    "text": "have these conditional\nprobabilities at each position. So how many\nparameters are there? ",
    "start": "745540",
    "end": "756840"
  },
  {
    "text": "For a first-order Markov? How many do I need to estimate?",
    "start": "756840",
    "end": "762610"
  },
  {
    "text": "Yeah, Kevin? AUDIENCE: I think it would\nbe 16 at each position. PROFESSOR: Yeah, 16\nat each position, except the first\nposition, which has four.",
    "start": "762610",
    "end": "769100"
  },
  {
    "text": "OK, and what about a\nsecond-order Markov model, where you condition on the\ntwo previous positions?",
    "start": "769100",
    "end": "774730"
  },
  {
    "text": " 64, right?",
    "start": "774730",
    "end": "780840"
  },
  {
    "text": "Because you have two possible\nbases you're conditioning on, that's 16 possibilities times 4.",
    "start": "780840",
    "end": "785960"
  },
  {
    "text": "And so in general, the\nformula is 4 to the k plus 1.",
    "start": "785960",
    "end": "792830"
  },
  {
    "text": "This is really the issue-- if\nyou have only 100 sequences,",
    "start": "792830",
    "end": "798040"
  },
  {
    "text": "and you need to estimate 64\nparameters at each position, you don't have enough\ndata to estimate those.",
    "start": "798040",
    "end": "803440"
  },
  {
    "text": "So you shouldn't use\nsuch a high order model.",
    "start": "803440",
    "end": "810250"
  },
  {
    "text": "All right, so let's\nthink about this-- what could happen if you don't\nhave enough data to estimate",
    "start": "810250",
    "end": "817600"
  },
  {
    "text": "parameters, and how can\nyou get around that? So let's just take a\nvery simple example. So suppose you were setting\na new transcription factor.",
    "start": "817600",
    "end": "824920"
  },
  {
    "text": "You had done some sort of\npull-down assay, followed",
    "start": "824920",
    "end": "830360"
  },
  {
    "text": "by, say, conventional\nsequencing, and identified 10\nsequences that bind",
    "start": "830360",
    "end": "835959"
  },
  {
    "text": "to that transcription factor. And these are the 10\nsequences, and you align them. You see there is\nsort of a pattern",
    "start": "835960",
    "end": "841600"
  },
  {
    "text": "there-- there's usually an\nA at the first position, and usually a C at the\nsecond, and so forth. And so you consider making\na weight matrix model.",
    "start": "841600",
    "end": "851680"
  },
  {
    "text": "Then you tally up-- there's\neight A's, one C, one G, and no T's at the\nfirst position.",
    "start": "851680",
    "end": "858020"
  },
  {
    "text": "So how confident can\nyou be that T is not compatible with binding of\nthis transcription factor?",
    "start": "858020",
    "end": "865660"
  },
  {
    "text": "Who thinks you can\nbe very confident? Most of you are\nshaking your head. So if you're not confident,\nwhy are you not confident?",
    "start": "865660",
    "end": "876269"
  },
  {
    "text": "I think-- wait, were\nyou shaking your head? What's the problem here?",
    "start": "876270",
    "end": "882110"
  },
  {
    "text": "It's just too small\na sample, right? Maybe T occurs rarely.",
    "start": "882110",
    "end": "887290"
  },
  {
    "text": "So suppose that T occurs\nat a frequency of 10%, what's the probability of\nthat in natural sequences?",
    "start": "887290",
    "end": "894480"
  },
  {
    "text": "And we just have a\nrandom sample of those. What's the probability\nwe wouldn't see any T's in a\nsample of size 10?",
    "start": "894480",
    "end": "902269"
  },
  {
    "text": " Anyone have an idea?",
    "start": "902270",
    "end": "907900"
  },
  {
    "text": "Anyone have a ballpark\nnumber on this? ",
    "start": "907900",
    "end": "913750"
  },
  {
    "text": "Yeah, Simona? AUDIENCE: 0.9 to the 10th. PROFESSOR: 0.9 to the 10th, OK.",
    "start": "913750",
    "end": "918790"
  },
  {
    "text": "And what is that? AUDIENCE: 0.9 is the probability\nthat you grab one, and don't see a T, and then\nyou do that 10 times.",
    "start": "918790",
    "end": "926450"
  },
  {
    "text": "PROFESSOR: Yeah, exactly. In genera it's a binomial\nthing, but it works out to be 0.9 to the 10th. And that's roughly--\nthis is like a Poisson.",
    "start": "926450",
    "end": "933790"
  },
  {
    "text": "There's a mean of 1, so it's\nroughly E to the minus 1, so about 35% chance that\nyou don't see any T's.",
    "start": "933790",
    "end": "941830"
  },
  {
    "text": "So we really shouldn't\nbe confident. T probably doesn't have\na frequency of 0.5,",
    "start": "941830",
    "end": "948320"
  },
  {
    "text": "but it could easily have a\nfrequency of 10% or even 5%, or even 15%.",
    "start": "948320",
    "end": "954910"
  },
  {
    "text": "And you might have\njust not seen it. So you don't want to assign\na probability 0 to T.",
    "start": "954910",
    "end": "960290"
  },
  {
    "text": "But what value should you assign\nfor something you haven't seen?",
    "start": "960290",
    "end": "965680"
  },
  {
    "text": "Sally?  So, it turns out there\nis a principled way",
    "start": "965680",
    "end": "973220"
  },
  {
    "text": "to do this called pseudocounts. So basically, if you use\nmaximum likelihood estimation,",
    "start": "973220",
    "end": "982430"
  },
  {
    "text": "you get-- maximum\nlikelihood, it turns out, is equal to the\nobserved frequency.",
    "start": "982430",
    "end": "988420"
  },
  {
    "text": "But if you assume that the\ntrue frequency is unknown,",
    "start": "988420",
    "end": "994910"
  },
  {
    "text": "but was sampled from\nall possible, reasonable frequencies-- so that's a\nDirichlet distribution-- then",
    "start": "994910",
    "end": "1001920"
  },
  {
    "text": "you can calculate what the\nposterior distribution is in a Bayesian framework, given\nthat you observed, for example,",
    "start": "1001920",
    "end": "1009899"
  },
  {
    "text": "zero T's, what's\nthe distribution of that parameter,\nfrequency of T?",
    "start": "1009900",
    "end": "1018649"
  },
  {
    "text": "And it turns out, it's\nequivalent to adding a single count to\neach of your bins.",
    "start": "1018650",
    "end": "1026418"
  },
  {
    "text": "I'm not going to go\nthrough the derivation, because it takes\ntime, but it is well described in the appendix\nof a book called,",
    "start": "1026419",
    "end": "1033299"
  },
  {
    "text": "Biological Sequence\nAnalysis, published about 10, 15 years ago by a number of\nleaders in the field-- Durbin,",
    "start": "1033300",
    "end": "1039730"
  },
  {
    "text": "Eddy, Krogh, and Mitchison. And there's also a derivation\nof this in the probability",
    "start": "1039730",
    "end": "1045990"
  },
  {
    "text": "and statistics primer. So basically you just do\nthis poster calculation,",
    "start": "1045990",
    "end": "1052080"
  },
  {
    "text": "and it turns out to be\nequivalent to adding 1 count. So when you add 1 count--\nand then, of course, you re-normalize,\nand then you get",
    "start": "1052080",
    "end": "1059880"
  },
  {
    "text": "a frequency-- what\neffectively it does is it will reduce the\nfrequency of things",
    "start": "1059880",
    "end": "1067120"
  },
  {
    "text": "that you observe most commonly,\nand boost up the things that you don't see, so\nthat you actually end up",
    "start": "1067120",
    "end": "1075010"
  },
  {
    "text": "assigning a probability\nof 0.07 to T. Now, if you had a\nlarger sample-- so",
    "start": "1075010",
    "end": "1081980"
  },
  {
    "text": "let's imagine instead of 8\n1 1 0, it was 80 10 10 0, you still at a single count.",
    "start": "1081980",
    "end": "1088980"
  },
  {
    "text": "So you can see in\nthat case, you're only going to be adding a very\nsmall, close to 1%, for T.",
    "start": "1088980",
    "end": "1096990"
  },
  {
    "text": "So as you get more\ndata, it converges to the maximum\nlikelihood estimate.",
    "start": "1096990",
    "end": "1102900"
  },
  {
    "text": "But it does something more\nreasonable, more open-minded, in a case where you're really\nlimited in terms of data.",
    "start": "1102900",
    "end": "1108200"
  },
  {
    "text": "So the limitation--\nyou always want to be aware when\nyou're considering going to a more complex model to\nget better predictability-- you",
    "start": "1108200",
    "end": "1115049"
  },
  {
    "text": "want to be aware of\nhow much data you have, and whether you have enough to\naccurately estimate parameters.",
    "start": "1115050",
    "end": "1121910"
  },
  {
    "text": "And if you don't, you\neither simplify the model, or if you can't\nsimplify it anymore,",
    "start": "1121910",
    "end": "1127240"
  },
  {
    "text": "consider using pseudocounts. Sometimes you'll see\nsmaller pseudocounts added-- like instead of 1\n1 1 1, you might",
    "start": "1127240",
    "end": "1134070"
  },
  {
    "text": "see a quarter, one pseudocount\ndistributed across the four bins. There's arguments pro and\ncon, which I won't go into.",
    "start": "1134070",
    "end": "1141380"
  },
  {
    "text": " So for the remainder\nof today, I want",
    "start": "1141380",
    "end": "1146880"
  },
  {
    "text": "to introduce hidden\nMarkov models. We'll talk about\nsome the terminology,",
    "start": "1146880",
    "end": "1153070"
  },
  {
    "text": "some applications, and the\nViterbi algorithm-- which is a core algorithm when\nusing HMMs to predict things--",
    "start": "1153070",
    "end": "1162070"
  },
  {
    "text": "and then we'll give\na couple examples. So we'll talk about\nthe CpG Island",
    "start": "1162070",
    "end": "1167230"
  },
  {
    "text": "HMM, which is about the\nsimplest HMM I could think of, which is good for illustrating\nthe mechanics of HMM.",
    "start": "1167230",
    "end": "1174140"
  },
  {
    "text": "And then a couple\nlater, probably coming into the next\nlecture, some examples",
    "start": "1174140",
    "end": "1181390"
  },
  {
    "text": "of real-world\nHMMs, like one that predicts transmembrane helices. So some background reading\nfor today's lecture",
    "start": "1181390",
    "end": "1188080"
  },
  {
    "text": "that's posted on\nthe course website, there's a nature\nbiotechnology primer on HMMs,",
    "start": "1188080",
    "end": "1193140"
  },
  {
    "text": "there's a little\nbit in the textbook. But really, if you want to\nunderstand the guts of HMMs,",
    "start": "1193140",
    "end": "1198620"
  },
  {
    "text": "you should read the\nRabiner tutorial, which is really\npretty well done.",
    "start": "1198620",
    "end": "1204830"
  },
  {
    "text": "For Thursday's lecture, I will\npost another of these nature biotechnology primers\non RNA folding.",
    "start": "1204830",
    "end": "1211580"
  },
  {
    "text": "This one is actually has\na little bit more content, takes a little bit\nlonger to absorb probably",
    "start": "1211580",
    "end": "1217310"
  },
  {
    "text": "than some of the\nothers, but still a good introduction\nto the topic. And then it turns out the\ntext has a pretty good section",
    "start": "1217310",
    "end": "1224990"
  },
  {
    "text": "on RNA folding, so take\na look at chapter 11. ",
    "start": "1224990",
    "end": "1233550"
  },
  {
    "text": "So hidden Markov\nmodels can be thought of as a general approach\nfor modeling sequence",
    "start": "1233550",
    "end": "1242360"
  },
  {
    "text": "labeling problems--\nyou have sequences, they might be genomic\nsequences, protein sequences, RNA sequences.",
    "start": "1242360",
    "end": "1247790"
  },
  {
    "text": "And these sequences have\nfeatures-- promoters, they may have domains,\net cetera, linear motifs.",
    "start": "1247790",
    "end": "1255040"
  },
  {
    "text": "And you want to\nlabel those features",
    "start": "1255040",
    "end": "1260320"
  },
  {
    "text": "in an unknown sequence. So a classical example\nwould be gene finding. You have a genomic sequence,\nsome parts are, say, exon,",
    "start": "1260320",
    "end": "1268720"
  },
  {
    "text": "some are introns. You want to be able to\nlabel them, it's not known. But you might have a training\nset of known exons and introns,",
    "start": "1268720",
    "end": "1275970"
  },
  {
    "text": "and you might learn what the\nsequence composition of each of those labels\nlooks like, and then",
    "start": "1275970",
    "end": "1282720"
  },
  {
    "text": "make a model that\nbuilds things together. And what they allow you\nto do, though, with HMMs is to have transition\nprobabilities",
    "start": "1282720",
    "end": "1289892"
  },
  {
    "text": "between the different states. You could model\nstates, you can model the length of different types\nof states to some extent--",
    "start": "1289892",
    "end": "1296299"
  },
  {
    "text": "as we'll see-- and\nyou can model which states need to\nfollow other states.",
    "start": "1296300",
    "end": "1303169"
  },
  {
    "text": "They're relatively\neasy to design, you can just simply\ndraw a graph.",
    "start": "1303170",
    "end": "1308850"
  },
  {
    "text": "It can even have cycles\nin it, that's OK. And they've been\ndescribed as the LEGOs",
    "start": "1308850",
    "end": "1316090"
  },
  {
    "text": "of computational\nsequence analysis. They were developed originally\nin electrical engineering",
    "start": "1316090",
    "end": "1321299"
  },
  {
    "text": "four or five decades\nago for applications in voice recognition,\nand they're still used in voice recognition.",
    "start": "1321300",
    "end": "1327220"
  },
  {
    "text": "So when you are calling up some\nlarge corporation, and instead",
    "start": "1327220",
    "end": "1332789"
  },
  {
    "text": "of a person answering\nthe phone, some computer answering the phone\nand attempting",
    "start": "1332790",
    "end": "1338050"
  },
  {
    "text": "to recognize your\nvoice, it could well be an HMM on the other end,\nwhich is either correctly",
    "start": "1338050",
    "end": "1345260"
  },
  {
    "text": "recognizing what\nyou're saying, or not. So you can thank them or\nblame them, as you wish.",
    "start": "1345260",
    "end": "1352990"
  },
  {
    "text": "All right, so Markov Model\nexample-- we did this before, imagine the genotype\nat a particular locus,",
    "start": "1352990",
    "end": "1362330"
  },
  {
    "text": "and successive generations is\nthought of as a Markov chain.",
    "start": "1362330",
    "end": "1368070"
  },
  {
    "text": "Bart's genotype depends on\nHomer's, but is conditionally independent of Grandpa\nSimpson's, given Homer's.",
    "start": "1368070",
    "end": "1375090"
  },
  {
    "text": "So now what's a\nhidden Markov model? So imagine that our\nDNA sequencer is not",
    "start": "1375090",
    "end": "1381830"
  },
  {
    "text": "working that week,\nwe can't actually go in and measure the genotype. But instead, we're going to\nobserve some phenotype that's",
    "start": "1381830",
    "end": "1390260"
  },
  {
    "text": "dependent on genotype. But it's not dependent\nin a deterministic way,",
    "start": "1390260",
    "end": "1397679"
  },
  {
    "text": "it's dependent in a more\ncomplex way, because there's an impact of environment,\nas well, let's say.",
    "start": "1397680",
    "end": "1403390"
  },
  {
    "text": "So we're imagining that your\ngenotype at the apolipoprotein",
    "start": "1403390",
    "end": "1408710"
  },
  {
    "text": "locus is correlated\nwith cholesterol, but doesn't\ncompletely predict it. So you're homozygous, you tend\nto have higher LDL cholesterol",
    "start": "1408710",
    "end": "1416690"
  },
  {
    "text": "than you are heterozygous. But there's a\ndistribution depending on how many doughnuts you\neat, or something like that.",
    "start": "1416690",
    "end": "1423690"
  },
  {
    "text": "Imagine that we\nobserve that grandpa had low cholesterol, 150,\nHomer had high cholesterol,",
    "start": "1423690",
    "end": "1431560"
  },
  {
    "text": "and Bart's cholesterol\nis intermediate. Now if we had just observed\nBart's cholesterol,",
    "start": "1431560",
    "end": "1439550"
  },
  {
    "text": "we would say, well, it\ncould go either way. It could be homozygous\nor heterozygous.",
    "start": "1439550",
    "end": "1447720"
  },
  {
    "text": "You would just look at\nthe population frequency of those two, and would\nuse that to guess.",
    "start": "1447720",
    "end": "1453450"
  },
  {
    "text": "But remember, we know his\nfather's cholesterol, which was 250, makes it\nmuch more likely",
    "start": "1453450",
    "end": "1459990"
  },
  {
    "text": "that his father was homozygous,\nand then that, in turn, biases",
    "start": "1459990",
    "end": "1465714"
  },
  {
    "text": "the distribution\n[? of it. ?] So that'll make it a little bit more\nlikely that Bart, himself, is homozygous, if you didn't know.",
    "start": "1465714",
    "end": "1472100"
  },
  {
    "text": "So this is the basic idea-- you\nhave some observable phenotype,",
    "start": "1472100",
    "end": "1477590"
  },
  {
    "text": "if you will, that depends,\nin a probabilistic way, on something hidden.",
    "start": "1477590",
    "end": "1482600"
  },
  {
    "text": "And that hidden thing has some\ndependent structure to it.",
    "start": "1482600",
    "end": "1488179"
  },
  {
    "text": "And you want to, then,\npredict those hidden states from the observable data. So we'll give some more\nexamples coming up.",
    "start": "1488180",
    "end": "1494640"
  },
  {
    "text": "And the way to think about these\nmodels, or at least a handy way to think about them, is\nas generative models.",
    "start": "1494640",
    "end": "1501110"
  },
  {
    "text": "And so this is from\nthe Rabiner tutorial-- you imagine an HMM used\nin order to generate",
    "start": "1501110",
    "end": "1508670"
  },
  {
    "text": "observable sequences. So there's these hidden\nstates-- think of them as genotypes--\nobservable-- think",
    "start": "1508670",
    "end": "1514299"
  },
  {
    "text": "of them as the\ncholesterol levels. So the way that it works is\nyou choose an initial state from one of your\npossible hidden states,",
    "start": "1514300",
    "end": "1521650"
  },
  {
    "text": "according to some\ninitial distribution, you set the time\nvariable equal to 1. In this case, it's T,\nwhich will, in our case,",
    "start": "1521650",
    "end": "1529549"
  },
  {
    "text": "often be the position\nin the sequence. And then you choose\nan observed value,",
    "start": "1529550",
    "end": "1535200"
  },
  {
    "text": "according to some\nprobability distribution, but it depends on what\nthat hidden state was.",
    "start": "1535200",
    "end": "1541240"
  },
  {
    "text": "And then you transition\nto a new state, and then you emit another one.",
    "start": "1541240",
    "end": "1547450"
  },
  {
    "text": "So we'll do an example. Let's say bacterial gene\nfinding is our application,",
    "start": "1547450",
    "end": "1556880"
  },
  {
    "text": "and we're going to model\na bacterial gene-- these are protein coding genes,\nonly it's got to have a start",
    "start": "1556880",
    "end": "1562820"
  },
  {
    "text": "coat on, it's got to have\nan open reading frame, and then it's got to\nhave a stop code on it.",
    "start": "1562820",
    "end": "1568880"
  },
  {
    "text": "So how many different states\ndo we need in our HMM?",
    "start": "1568880",
    "end": "1573960"
  },
  {
    "text": "What should our states be?  Anyone?",
    "start": "1573960",
    "end": "1579766"
  },
  {
    "text": "Do you want to make that-- Tim? AUDIENCE: Maybe you\nneed four states, because the start state, the\norf state, the stop state,",
    "start": "1579766",
    "end": "1586861"
  },
  {
    "text": "and the non-genic state. PROFESSOR: OK, start, orf,\nstop and then intergenic,",
    "start": "1586861",
    "end": "1597290"
  },
  {
    "text": "or non-genic. ",
    "start": "1597290",
    "end": "1603250"
  },
  {
    "text": "OK now, remember these\nare the hidden states, so what are they going to emit? They emit observable\ndata, what's",
    "start": "1603250",
    "end": "1610020"
  },
  {
    "text": "that observable\ndata going to be? Sequence, OK. And how many bases of sequence\nshould each of them emit?",
    "start": "1610020",
    "end": "1617070"
  },
  {
    "text": "AUDIENCE: Well I\nguess we don't know. PROFESSOR: You have a choice. You're the model builder,\nyou can do anything you want.",
    "start": "1617070",
    "end": "1624550"
  },
  {
    "text": "1, 5, 10-- any number\nof bases you want. And they can emit different\nthings, if you want.",
    "start": "1624550",
    "end": "1631335"
  },
  {
    "text": "This is generative,\nyou can do anything you want-- there will\nbe consequences later, but for now-- I'm going\nto call this-- go ahead.",
    "start": "1631335",
    "end": "1639919"
  },
  {
    "text": "AUDIENCE: You could start with\nthe start and the stop states maybe being three.",
    "start": "1639920",
    "end": "1645420"
  },
  {
    "text": "PROFESSOR: Three, OK. So this is going to\nemit three nucleotides.",
    "start": "1645420",
    "end": "1650630"
  },
  {
    "text": " How about this state? What should this emit?",
    "start": "1650630",
    "end": "1657162"
  },
  {
    "text": " AUDIENCE: Any number. PROFESSOR: Any number?",
    "start": "1657162",
    "end": "1664632"
  },
  {
    "text": "Yeah, OK, Sally? AUDIENCE: If you let\nit emit one number, and then add a self-cycle,\nthen that would work.",
    "start": "1664632",
    "end": "1671110"
  },
  {
    "text": "PROFESSOR: So Sally wants\nto have this state emit one nucleotide, but she\nwants it to have",
    "start": "1671110",
    "end": "1676960"
  },
  {
    "text": "a chance of returning to itself. So that then we can have strings\nof N's to represent intergenic.",
    "start": "1676960",
    "end": "1683170"
  },
  {
    "text": "Does that make sense? And these, I agree. Three is a good choice, here. If you had this one emit\nthree, as well, then",
    "start": "1683170",
    "end": "1690550"
  },
  {
    "text": "your genes would have to\nbe a multiple of three apart from each other,\nwhich isn't realistic. You would miss out on\nsome genes for that.",
    "start": "1690550",
    "end": "1698010"
  },
  {
    "text": "So this has to be able to\nemit arbitrary numbers. So you could either have it\nemit an arbitrary number,",
    "start": "1698010",
    "end": "1704049"
  },
  {
    "text": "but it's going to turn\nout to make the Viterbi algorithm easier if it\njust emits one and recurs,",
    "start": "1704050",
    "end": "1710010"
  },
  {
    "text": "as Sally suggested. And then we have our orf state. So how about here? What should we do here?",
    "start": "1710010",
    "end": "1716110"
  },
  {
    "text": "AUDIENCE: It can be\nthree, and then you put the circle [INAUDIBLE]. PROFESSOR: So I'm going to\nchange the name to Codon,",
    "start": "1716110",
    "end": "1722066"
  },
  {
    "text": "because it's going to emit\none codon-- three nucleotides. And then recur to itself.",
    "start": "1722066",
    "end": "1727760"
  },
  {
    "text": "And now what transitions\nshould we allow between states?  AUDIENCE: So start to four,\norf to stop, then stop to N,",
    "start": "1727760",
    "end": "1742139"
  },
  {
    "text": "and then to start. PROFESSOR: Any others? ",
    "start": "1742140",
    "end": "1754068"
  },
  {
    "text": "Yeah? AUDIENCE: N could\ngo to stop, as well. PROFESSOR: I'm sorry,\nN could go to stop? AUDIENCE: Yeah, so that\nthe gene [INAUDIBLE].",
    "start": "1754068",
    "end": "1760231"
  },
  {
    "text": "PROFESSOR: OK, so\nthat's a question. We're thinking of a\ngene on the plus strand, a gene could well be\non the opposite strand.",
    "start": "1760231",
    "end": "1767200"
  },
  {
    "text": "And so we should\nprobably make a model of where you would hit stop\non the other strand, which",
    "start": "1767200",
    "end": "1774314"
  },
  {
    "text": "would emit a triplet of the\ninverse complement of the stop code, [INAUDIBLE] et cetera. That's true, excellent point.",
    "start": "1774314",
    "end": "1780280"
  },
  {
    "text": "And then you would\ntraverse this whole circle in the opposite direction. But it wouldn't\nbe the same state.",
    "start": "1780280",
    "end": "1786190"
  },
  {
    "text": "It would be stop-- because it\nwould emit different things. So you'd have minus\nstop, stop minus strand.",
    "start": "1786190",
    "end": "1795047"
  },
  {
    "text": "And then you'd have\nsome other states there. And I'm not going to draw\nthose, but that's a point. And you could have a\nteeny one-codon gene",
    "start": "1795047",
    "end": "1801950"
  },
  {
    "text": "if you want, but\nprobably not worth it. All right, everyone have\nan idea about this HMM?",
    "start": "1801950",
    "end": "1811330"
  },
  {
    "text": "So this is a model you have\nto specify in order for this to actually generate sequence.",
    "start": "1811330",
    "end": "1817150"
  },
  {
    "text": "This model will\nactually generate annotations and sequence. You have to specify\nwhere to start,",
    "start": "1817150",
    "end": "1823350"
  },
  {
    "text": "so you have to have some\nprobability of starting, but the first base that\nyou're going to generate is going to begin intergenic,\nor start, or codon, et cetera.",
    "start": "1823350",
    "end": "1830380"
  },
  {
    "text": "And you might give it a\nhigh probability of this, and then it'll generate a label.",
    "start": "1830380",
    "end": "1836440"
  },
  {
    "text": "So for example, let's say N.\nAnd then it'll generate a base, let's say G. And then you look at\nthese probabilities,",
    "start": "1836440",
    "end": "1843414"
  },
  {
    "text": "so the transition probability\nhere, versus this-- you either generate another N, or\nyou generate a start.",
    "start": "1843415",
    "end": "1848860"
  },
  {
    "text": "And let's say you go\nto start, and then you'll generate three\nbases, so A T G.",
    "start": "1848860",
    "end": "1854400"
  },
  {
    "text": "And then you would go\nto the codon state, you would emit some other\ntriplet, and so forth.",
    "start": "1854400",
    "end": "1863666"
  },
  {
    "text": "So this is a model that will\ngenerate strings of annotations with associated bases.",
    "start": "1863666",
    "end": "1869460"
  },
  {
    "text": " Still doesn't predict\ngene structure yet,",
    "start": "1869460",
    "end": "1875039"
  },
  {
    "text": "but at least it generates\ngene structures. All right, so we are going to,\nfor the sake of illustrating",
    "start": "1875040",
    "end": "1884049"
  },
  {
    "text": "the Viterbi\nalgorithm, we're going to use a simpler HMM in that. So this one only has two\nstates, and its purpose",
    "start": "1884050",
    "end": "1891060"
  },
  {
    "text": "is to predict CPG islands\nin a vertebrate genome.",
    "start": "1891060",
    "end": "1896840"
  },
  {
    "text": "So what are CPG islands? Anyone remember? ",
    "start": "1896840",
    "end": "1904010"
  },
  {
    "text": "What is a CPG island? Anyone heard of this? I'm sure some of you. ",
    "start": "1904010",
    "end": "1913090"
  },
  {
    "text": "Well, the definition\nhere is going to be regions of high CNG\ncontent, and relatively high abundance of CPG dinucleotides,\nwhich are unmethylated.",
    "start": "1913090",
    "end": "1920900"
  },
  {
    "text": "So what is the P here? The p means that the\nCG we're talking about",
    "start": "1920900",
    "end": "1926200"
  },
  {
    "text": "is C followed by G along\nthe particular DNA strand,",
    "start": "1926200",
    "end": "1932419"
  },
  {
    "text": "just to distinguish it\nfrom C base paired with G. We're not talking\nabout a base pair here, we're talking about\nC and G following",
    "start": "1932420",
    "end": "1938679"
  },
  {
    "text": "each other along the strand. So this dinucleotide is\nrare in vertebrate genomes,",
    "start": "1938680",
    "end": "1945040"
  },
  {
    "text": "because CPG is the\nsite of a methylase, and methylation of the C\nis mutogenic-- it lends",
    "start": "1945040",
    "end": "1953392"
  },
  {
    "text": "to a much higher\nrate of mutations. so CPGs often mutate\naway, except for the ones that are necessary.",
    "start": "1953392",
    "end": "1959020"
  },
  {
    "text": "But there are certain\nregions, often near promoters, that are unmethylated,\nand therefore, CPGs",
    "start": "1959020",
    "end": "1967100"
  },
  {
    "text": "can accumulate to\nhigher frequencies. And so you can actually\nlook for these regions",
    "start": "1967100",
    "end": "1973020"
  },
  {
    "text": "and use them to predict\nwhere promoters are. That's one application. So they have higher CPG\ndinucleotide content, and also",
    "start": "1973020",
    "end": "1980250"
  },
  {
    "text": "higher C and G content. The background of the human\ngenome is about 40% C G only,",
    "start": "1980250",
    "end": "1986220"
  },
  {
    "text": "so it's a bit AT rich, and so\nyou see these patches of, say, 50% to 60% C G that are often\nassociated with promoters,",
    "start": "1986220",
    "end": "1993330"
  },
  {
    "text": "with promoters of roughly\nhalf of human genes. So we're going to-- I always\ndrop that little clicker thing.",
    "start": "1993330",
    "end": "2000500"
  },
  {
    "text": "Here it is. We're going to make\na model of these, and then run it to predict\npromoters in the gene.",
    "start": "2000500",
    "end": "2009549"
  },
  {
    "text": "So here's our model. We have two states, we\nhave a genome state-- this sort of generic\nposition in the genome--",
    "start": "2009550",
    "end": "2014710"
  },
  {
    "text": "and then we have\nan island state. We have the simplest\npossible transitions, you can go genome to\ngenome, genome to island,",
    "start": "2014710",
    "end": "2022820"
  },
  {
    "text": "island to genome,\nor island to island. So now you can generate islands\nof arbitrary size, interspersed",
    "start": "2022820",
    "end": "2029860"
  },
  {
    "text": "with genomic regions\nof arbitrary size. And then each of\nthose hidden states",
    "start": "2029860",
    "end": "2036980"
  },
  {
    "text": "is going to emit a single base. So a CPG island in\nthis model is a stretch",
    "start": "2036980",
    "end": "2042520"
  },
  {
    "text": "of I states in a row flanked\nby G states, if you will.",
    "start": "2042520",
    "end": "2048679"
  },
  {
    "text": "Everyone clear on this set up? Good. So here, in order to\nfully specify the model,",
    "start": "2048679",
    "end": "2056638"
  },
  {
    "text": "you need to say what\nall the parameters are. And there are really\nthree class of parameters.",
    "start": "2056639",
    "end": "2064658"
  },
  {
    "text": "There are initiation\nprobabilities-- so the green here is the\nnotation used in the Rabiner",
    "start": "2064659",
    "end": "2070800"
  },
  {
    "text": "tutorial, except they call them\n[? pi ?] [? j's. ?] So here, I'm going to say it's a 99%\nchance you start in the generic",
    "start": "2070800",
    "end": "2079000"
  },
  {
    "text": "genome state, and a 1% chance\nyou start in an island state, because islands are\nnot that common.",
    "start": "2079000",
    "end": "2086119"
  },
  {
    "text": "And then you need to specify\ntransition probabilities. So there's four possible\ntransitions you could make,",
    "start": "2086120",
    "end": "2092829"
  },
  {
    "text": "and you need to assign\nprobabilities to them. So if the average length of\nan island were 1,000 bases,",
    "start": "2092830",
    "end": "2100630"
  },
  {
    "text": "then a reasonable value\nfor the I to I transition would be 0.999.",
    "start": "2100630",
    "end": "2107440"
  },
  {
    "text": "You have 99.9% chance of\nmaking another island, and 0.1% chance of\nleaving that island state.",
    "start": "2107440",
    "end": "2114359"
  },
  {
    "text": "If you just run that in\nthis generative mode, it would generate a variety\nof lengths of islands,",
    "start": "2114360",
    "end": "2121370"
  },
  {
    "text": "but on average, they'd\nbe about one kb long, because the probability of\nterminating is one in 1,000.",
    "start": "2121370",
    "end": "2129260"
  },
  {
    "text": "And then if we imagine that\nthose one kb islands are interspersed with genomic\nregions that are about, say,",
    "start": "2129260",
    "end": "2135690"
  },
  {
    "text": "100 kilo bases long on average,\nthen you would get this [? five ?] [? nines ?]\nprobability for P G G,",
    "start": "2135690",
    "end": "2141890"
  },
  {
    "text": "and 10 to the minus fifth as\na probability of going from genome to islands. That would generate\nwidely spaced islands,",
    "start": "2141890",
    "end": "2150190"
  },
  {
    "text": "that are on average\n100 kb apart, that are about one kb in length. Is that making sense?",
    "start": "2150190",
    "end": "2158059"
  },
  {
    "text": "And now the third type of\nprobability we need to specify are called emission\nprobabilities,",
    "start": "2158060",
    "end": "2164130"
  },
  {
    "text": "which are the B J K\nin Rabiner notation. And this is where the predictive\npower is going to come in.",
    "start": "2164130",
    "end": "2171960"
  },
  {
    "text": "There has to be a\ndifference in the emissions if you're going to\nhave any ability to predict these\nfeatures, and so we're",
    "start": "2171960",
    "end": "2178580"
  },
  {
    "text": "going to imagine that\nthe genome is 40% C G, and islands are 60% C G.\nSo it's a base composition",
    "start": "2178580",
    "end": "2184027"
  },
  {
    "text": "that we're modeling. We're not doing the\ndinucleotides here, that would make it\nmore complicated. We're just looking for\npatches of high G C content.",
    "start": "2184027",
    "end": "2190623"
  },
  {
    "text": " So now we've fully\nspecified our model.",
    "start": "2190623",
    "end": "2196300"
  },
  {
    "start": "2196300",
    "end": "2201410"
  },
  {
    "text": "The problem here\nis that the model is written from the\nhidden generating",
    "start": "2201410",
    "end": "2207900"
  },
  {
    "text": "the observable, and the problem\nwe're faced with, in practice, is that we have the\nobservable sequence,",
    "start": "2207900",
    "end": "2214099"
  },
  {
    "text": "and we want to go\nback to the hidden. So we need to reverse\nthe conditioning that's",
    "start": "2214099",
    "end": "2219980"
  },
  {
    "text": "in the model. So when you see this\ntype of problem, how do you reverse conditioning?",
    "start": "2219980",
    "end": "2226310"
  },
  {
    "text": "In general, what's\na good way to do it? You see P A, given\nB, but the model",
    "start": "2226310",
    "end": "2233530"
  },
  {
    "text": "you have is written in P\nB, given A. What do you do? What's that? AUDIENCE: Bayes theorem.",
    "start": "2233530",
    "end": "2239399"
  },
  {
    "text": "PROFESSOR: Yeah, Bayes theorem. Right, let's do\nbase theorem here.",
    "start": "2239399",
    "end": "2245830"
  },
  {
    "text": " Remember, definition of\nconditional probability,",
    "start": "2245830",
    "end": "2253390"
  },
  {
    "text": "if we have P A given B--\nso this might be the hidden states, given the observables--\nwe want to write that in terms",
    "start": "2253390",
    "end": "2260819"
  },
  {
    "text": "of P B given A. So\nwhat we do first? How do we derive Bayes rule?",
    "start": "2260820",
    "end": "2266410"
  },
  {
    "text": "You first write the definition\nof conditional probability--",
    "start": "2266410",
    "end": "2272770"
  },
  {
    "text": "right, that's just\nthe definition. And now what do I do?",
    "start": "2272770",
    "end": "2278160"
  },
  {
    "text": "Split the top part into what? AUDIENCE: A times P of\nB given [INAUDIBLE].",
    "start": "2278160",
    "end": "2289120"
  },
  {
    "text": "PROFESSOR: P B\ngiven A. That's just another way of writing\njoint probability of P A B,",
    "start": "2289120",
    "end": "2294930"
  },
  {
    "text": "using the definition of\nconditional probability again. ",
    "start": "2294930",
    "end": "2302950"
  },
  {
    "text": "So now it's written\nthe other way. That's basically the idea.",
    "start": "2302950",
    "end": "2310529"
  },
  {
    "text": "So this is the simple form. And like I said, I\ndon't usually call it a theorem, because\nit's so simple-- it's",
    "start": "2310530",
    "end": "2317470"
  },
  {
    "text": "something you can\nderive in 30 seconds. It should be called maybe\na rule, or something.",
    "start": "2317470",
    "end": "2325030"
  },
  {
    "text": "There is a more general form. This is where you have\ntwo states, basically B",
    "start": "2325030",
    "end": "2334099"
  },
  {
    "text": "or not B that\nyou're dealing with. And there's this\nmore general form that's shown on the slide, which\nis when you have many states.",
    "start": "2334100",
    "end": "2340940"
  },
  {
    "text": "And it basically\nis the same idea, it's just that we've\nrewritten this term, this P B,",
    "start": "2340940",
    "end": "2349540"
  },
  {
    "text": "and we split it up into\nall the possible states. ",
    "start": "2349540",
    "end": "2362620"
  },
  {
    "text": "The slide starts from PB given\nA and goes the other-- anyway, so you rewrite the\nbottom term as a sum",
    "start": "2362620",
    "end": "2369530"
  },
  {
    "text": "of all the possible cases. All right, so how does\nthat apply to HMMs?",
    "start": "2369530",
    "end": "2377510"
  },
  {
    "text": "So with HMMs, we're interested\nin the joint probability",
    "start": "2377510",
    "end": "2384670"
  },
  {
    "text": "of a set of hidden states, and\na set of observable states. So H, capital H, is\ngoing to be a vector that",
    "start": "2384670",
    "end": "2392440"
  },
  {
    "text": "specifies the particular hidden\nstate-- for instance, island",
    "start": "2392440",
    "end": "2398415"
  },
  {
    "text": "or genome-- at position 1,\nthat's H1 all the way to H N. So little h's are specific\nvalues for those hidden state.",
    "start": "2398415",
    "end": "2407760"
  },
  {
    "text": "And then, big O is a\nvector that describes the different bases\nin the genome.",
    "start": "2407760",
    "end": "2414440"
  },
  {
    "text": "So O1 is the first\nbase in the genome, up to O N. One can imagine\ncomparing two H vectors,",
    "start": "2414440",
    "end": "2424309"
  },
  {
    "text": "one of which, H versus the\nH primes, what's the what's the probability of this\nhidden state versus that?",
    "start": "2424310",
    "end": "2433010"
  },
  {
    "text": "You could compare them in terms\nof their joint probabilities with this model,\nand perhaps favor",
    "start": "2433010",
    "end": "2438450"
  },
  {
    "text": "those that have\nhigher probabilities. Yeah?",
    "start": "2438450",
    "end": "2443810"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nof the two capital H's have any different notation?",
    "start": "2443810",
    "end": "2449674"
  },
  {
    "text": "Like the second one being H\nprime, or something like that? Or are they supposed to be-- PROFESSOR: With the\nH, in this case,",
    "start": "2449674",
    "end": "2455607"
  },
  {
    "text": "these are probability statements\nabout random variables. So H is a random\nvariable, which could assume any possible\nsequence of hidden states.",
    "start": "2455607",
    "end": "2462860"
  },
  {
    "text": "The little h's are\nspecific values.",
    "start": "2462860",
    "end": "2468230"
  },
  {
    "text": "So for instance,\nimagine comparing what's the probability of\nH equals genome, genome,",
    "start": "2468230",
    "end": "2479910"
  },
  {
    "text": "genome, versus the\nprobability that H equals genome, genome, island.",
    "start": "2479910",
    "end": "2486210"
  },
  {
    "text": "So the little h's, or\nthe little h primes, are specific instances. The H's is a random\nvariable, unknown.",
    "start": "2486210",
    "end": "2493010"
  },
  {
    "text": "Does that help? ",
    "start": "2493010",
    "end": "2501280"
  },
  {
    "text": "OK, so how do we\napply Bayes' rule? So what we're interested\nin here is the probability",
    "start": "2501280",
    "end": "2506920"
  },
  {
    "text": "that H, this unknown variable\nthat represents hidden states, that it equals a particular\nset of hidden states,",
    "start": "2506920",
    "end": "2513329"
  },
  {
    "text": "little h1 to h N, given\nthe observables, little o1 to little oN, which is the\nactual sequence that we see.",
    "start": "2513330",
    "end": "2521200"
  },
  {
    "text": "And we can write\nthat using definition of conditional probability\nas the joint probability",
    "start": "2521200",
    "end": "2527710"
  },
  {
    "text": "patient of H and O, over\nthe probability of O. And then Bayes' rule, we just\napply conditional probability",
    "start": "2527710",
    "end": "2533430"
  },
  {
    "text": "again. It's P H times P O,\ngiven H, over P O.",
    "start": "2533430",
    "end": "2540960"
  },
  {
    "text": "So it turns out that\nthis P O-- so what is P O equals O1 to O N in this model?",
    "start": "2540960",
    "end": "2550000"
  },
  {
    "text": "Well, the model specifies how\nto generate the hidden states, and how the observables are\ngenerated from those hidden",
    "start": "2550000",
    "end": "2557300"
  },
  {
    "text": "states. So P O is actually defined\nas the sum of P O comma H",
    "start": "2557300",
    "end": "2566310"
  },
  {
    "text": "equals the first possible\nhidden state, plus the same term",
    "start": "2566310",
    "end": "2571540"
  },
  {
    "text": "for the second. You have to sum over all\nthe possible outcomes of the hidden states,\nevery possible thing.",
    "start": "2571540",
    "end": "2578790"
  },
  {
    "text": "So if we have a sequence\nof length three, you have to sum\nover the possibility",
    "start": "2578790",
    "end": "2584119"
  },
  {
    "text": "that H might be G G G or\nG G I, or G I G, or G I I,",
    "start": "2584120",
    "end": "2591850"
  },
  {
    "text": "or I G G, et cetera. ",
    "start": "2591850",
    "end": "2599760"
  },
  {
    "text": "You have to sum over\neight possibilities here. And if the sequence\nis a million long,",
    "start": "2599760",
    "end": "2605000"
  },
  {
    "text": "you have to sum over 2 to the\none millionth possibilities. That sounds complicated\nto calculate.",
    "start": "2605000",
    "end": "2611610"
  },
  {
    "text": "So it turns out that\nthere's actually a trick, and you can calculate it. But you don't have to.",
    "start": "2611610",
    "end": "2617950"
  },
  {
    "text": "That's one of the\ngood things, is that we can just treat\nit as a constant. So notice that the denominator\nhere is independent of the H's.",
    "start": "2617950",
    "end": "2627780"
  },
  {
    "text": "So we'll just treat that as a\nconstant, an unknown constant. And what we're interested\nin, which possible value of H",
    "start": "2627780",
    "end": "2635720"
  },
  {
    "text": "has a higher probability? So we're just going to try to\nmaximize P H equals H1 to H N--",
    "start": "2635720",
    "end": "2644079"
  },
  {
    "text": "find the optimal\nsequence of hidden states that optimizes that\njoint probability,",
    "start": "2644080",
    "end": "2649310"
  },
  {
    "text": "the joint probability with\nthe observable values, O1 to O N. Is that making sense?",
    "start": "2649310",
    "end": "2659360"
  },
  {
    "text": "Basically, we want to find\nthe sequence of hidden states, we'll call it H opt.",
    "start": "2659360",
    "end": "2664540"
  },
  {
    "text": "So now H opt here is\na particular vector. Capital H, by itself,\nis a random vector. This is now a particular\nvector of hidden states,",
    "start": "2664540",
    "end": "2673109"
  },
  {
    "text": "H1 opt through H N opt. And it's defined as the\nvector of hidden states",
    "start": "2673110",
    "end": "2681349"
  },
  {
    "text": "that maximizes the joint\nprobability with O equals O1 to O N, where that's the\nobserved sequence that we're",
    "start": "2681350",
    "end": "2693220"
  },
  {
    "text": "dealing with. So now what I'm\ntelling you is if we",
    "start": "2693220",
    "end": "2699450"
  },
  {
    "text": "can find the vector\nof hidden states that maximizes the\njoint probability, then that will also maximize\nthe conditional probability of H",
    "start": "2699450",
    "end": "2708520"
  },
  {
    "text": "given O. And that's often\nthe language of linguistics",
    "start": "2708520",
    "end": "2714680"
  },
  {
    "text": "is used, and it's called the\noptimal parse of the sequence. You'll see that sometimes,\nI might say that.",
    "start": "2714680",
    "end": "2721980"
  },
  {
    "text": "So the solution is to\ndefine these variables,",
    "start": "2721980",
    "end": "2728510"
  },
  {
    "text": "R I of H, which are\ndefined as the probability",
    "start": "2728510",
    "end": "2735610"
  },
  {
    "text": "of the optimal parse of\nthe subsequence from one to I-- not the\nwhole long sequence,",
    "start": "2735610",
    "end": "2740980"
  },
  {
    "text": "but a little piece of\nit from the beginning to a particular place in the\nmiddle, that ends in state, H.",
    "start": "2740980",
    "end": "2747619"
  },
  {
    "text": "And so first. We calculate R, R 1\n1, the probability of generating the\nfirst base, given",
    "start": "2747620",
    "end": "2754320"
  },
  {
    "text": "that it ended in hidden state 1. And then we would do\nthe hidden state 2, and then we basically have to\nfigure out a way, a recursion,",
    "start": "2754320",
    "end": "2762110"
  },
  {
    "text": "for getting the probabilities\nof the optimal parses ending",
    "start": "2762110",
    "end": "2767800"
  },
  {
    "text": "at each of the\nstates at position 2, given the values at position 1. And then we go, work\nour way all the way",
    "start": "2767800",
    "end": "2774000"
  },
  {
    "text": "down to the end of the sequence. And then we'll figure\nout which is better, and then we'll\nbacktrack to figure out",
    "start": "2774000",
    "end": "2779940"
  },
  {
    "text": "what that optimal parse was. We'll do an example\non the board,",
    "start": "2779940",
    "end": "2785000"
  },
  {
    "text": "this is unlikely to be\ncompletely clear at this point.",
    "start": "2785000",
    "end": "2791180"
  },
  {
    "text": "But don't worry. So why is this called\nthe Viterbi algorithm? Well, this is the guy\nwho figured it out.",
    "start": "2791180",
    "end": "2798320"
  },
  {
    "text": "He was actually an MIT alum. He did his bachelor's\nand master's in double E,",
    "start": "2798320",
    "end": "2804122"
  },
  {
    "text": "I don't know, quite\nawhile ago, '50s or '60s. And later went on\nto found Qualcomm,",
    "start": "2804122",
    "end": "2811210"
  },
  {
    "text": "and is now big philanthropist,\nwho apparently supports USC.",
    "start": "2811210",
    "end": "2817060"
  },
  {
    "text": "I don't know why he\nlost his loyalty to MIT, but maybe he'll come back\nand give us a seminar.",
    "start": "2817060",
    "end": "2823870"
  },
  {
    "text": "I actually met him once. Let's talk about his\nalgorithm a little more. So what I want to do is I\nwant to take a particular HMM,",
    "start": "2823870",
    "end": "2834690"
  },
  {
    "text": "so we'll take our\nCPG island HMM, and then we'll go through\nthe actual Viterbi",
    "start": "2834690",
    "end": "2841369"
  },
  {
    "text": "algorithm on the board\nfor a particular sequence. And you'll see that it's\nactually pretty simple.",
    "start": "2841370",
    "end": "2849660"
  },
  {
    "text": "But then you'll\nalso see that it's not totally obvious\nwhy it works. ",
    "start": "2849660",
    "end": "2856500"
  },
  {
    "text": "The mechanics of it\nare not that bad, but the understanding\nreally how it",
    "start": "2856500",
    "end": "2861690"
  },
  {
    "text": "is able to come up with\nthe optimal parses, that's the more subtle part.",
    "start": "2861690",
    "end": "2869270"
  },
  {
    "text": "So let's suppose\nwe have a sequence, A C G. Can anyone tell\nme what the optimal parse",
    "start": "2869270",
    "end": "2878620"
  },
  {
    "text": "of this sequence is,\nwithout doing Viterbi? ",
    "start": "2878620",
    "end": "2884240"
  },
  {
    "text": "With this particular\nmodel, these initiation probabilities transitions\nand emissions?",
    "start": "2884240",
    "end": "2891740"
  },
  {
    "text": "Do you know what it's\ngoing to be in advance? Any guesses?",
    "start": "2891740",
    "end": "2897910"
  },
  {
    "text": "AUDIENCE: How about\ngenome, island, island. PROFESSOR: Genome,\nisland, island. Because, you're saying,\nthat way the emissions",
    "start": "2897910",
    "end": "2904220"
  },
  {
    "text": "will be optimized, right? Because you'll omit\nthe C's and G's. OK, that's a reasonable guess.",
    "start": "2904220",
    "end": "2909680"
  },
  {
    "text": "Sally's shaking\nher head, though. AUDIENCE: The\ntransitional probability from being in the genome\nis very, very small,",
    "start": "2909680",
    "end": "2916059"
  },
  {
    "text": "and so it's more likely\nthat it'll either only be in the genome or\nonly be in the island. PROFESSOR: So the\ntransition from going",
    "start": "2916059",
    "end": "2921585"
  },
  {
    "text": "from a genome to island or\nisland to genome is very small, and so she's saying you're\ngoing to pay a bigger penalty for making that\ntransition in there, that",
    "start": "2921585",
    "end": "2928640"
  },
  {
    "text": "may not be offset\nby the emissions. Right, is that your point? Yeah, question?",
    "start": "2928640",
    "end": "2933820"
  },
  {
    "text": "AUDIENCE: Check here-- when\nwe're talking about the optimal parse, we're saying let's\nmaximize the probability",
    "start": "2933820",
    "end": "2941700"
  },
  {
    "text": "of that letter-- PROFESSOR: The\njoint probability. AUDIENCE: Sorry, the joint\nprobability of that letter--",
    "start": "2941700",
    "end": "2948658"
  },
  {
    "text": "PROFESSOR: Of that [INAUDIBLE]\nstate and that letter. AUDIENCE: OK, so that means-- PROFESSOR: Or that set\nof [INAUDIBLE] states",
    "start": "2948658",
    "end": "2954092"
  },
  {
    "text": "and that set of bases. AUDIENCE: So when\nwe're computing across this\nthree-letter thing, we have to say the probability\nof the letter, then let's",
    "start": "2954092",
    "end": "2961846"
  },
  {
    "text": "multiply if by the\nprobability of the transition to the next letter, and\nthen multiply it again",
    "start": "2961846",
    "end": "2968622"
  },
  {
    "text": "[INAUDIBLE] and that letter. PROFESSOR: So let's do this. If A C G is our\nsequence-- I'm just",
    "start": "2968622",
    "end": "2976400"
  },
  {
    "text": "going to space it\nout a little bit. Here's our A at position one,\nhere's our C at position two,",
    "start": "2976400",
    "end": "2983970"
  },
  {
    "text": "and our G at position three.",
    "start": "2983970",
    "end": "2989099"
  },
  {
    "text": "And then we have\nour hidden states. And so we'll write genome first,\nand then we have island here.",
    "start": "2989100",
    "end": "2998099"
  },
  {
    "text": " And so what is the optimal\nparse of the sequence from base",
    "start": "2998100",
    "end": "3005700"
  },
  {
    "text": "one to base one,\nthat ends in genome?",
    "start": "3005700",
    "end": "3010920"
  },
  {
    "text": "It's just the one\nthat starts in genome, because it doesn't go--\nright, so it's just genome. And what is its probability?",
    "start": "3010920",
    "end": "3017920"
  },
  {
    "text": "That's how this thing\nis defined here. This is this R I H thing\nI was talking about.",
    "start": "3017920",
    "end": "3024869"
  },
  {
    "text": "This is H, and this is I here. ",
    "start": "3024870",
    "end": "3031130"
  },
  {
    "text": "So the probability\nof the optimal parse of the sequence, up to position\none, that ends in genome,",
    "start": "3031130",
    "end": "3038349"
  },
  {
    "text": "is just the one that\nstarts in genome, and then emits that base. So what's the probability\nof starting in genome?",
    "start": "3038350",
    "end": "3043650"
  },
  {
    "text": " It's five nines, right?",
    "start": "3043650",
    "end": "3049270"
  },
  {
    "text": "So that's the initial\nprobability-- 9 9 9 9 9. And then what's the probability\nof emitting an A, given",
    "start": "3049270",
    "end": "3058039"
  },
  {
    "text": "that we're in the genome state? 0.3. So I claim that\nthis is the value",
    "start": "3058040",
    "end": "3066359"
  },
  {
    "text": "of R1 of genome, of\nthe genome state.",
    "start": "3066360",
    "end": "3071370"
  },
  {
    "text": "OK, that's the optimal parse. There's only one\nparse, so there's nothing-- it is what it is.",
    "start": "3071370",
    "end": "3077080"
  },
  {
    "text": "You start here, there's no\ntransitions-- we started here-- and then you emit an A.",
    "start": "3077080",
    "end": "3082930"
  },
  {
    "text": "What's the probability\nof the optimal parse ending an island at position\none of the sequence?",
    "start": "3082930",
    "end": "3089149"
  },
  {
    "text": "Someone else? Yeah, question? AUDIENCE: Why are we using\nthe transition probability? PROFESSOR: This is the\ninitial-- Oh, I'm sorry.",
    "start": "3089149",
    "end": "3097171"
  },
  {
    "text": "Correct. Thank you, thank you--\nwhat was your name? AUDIENCE: Deborah. PROFESSOR: Deborah,\nOK, thanks Deborah. It should be the\ninitial probability,",
    "start": "3097171",
    "end": "3103451"
  },
  {
    "text": "which is 0.99, good. Initial probability. What about island? Deborah, you want\nto take this one?",
    "start": "3103451",
    "end": "3110062"
  },
  {
    "text": "AUDIENCE: 0.01. PROFESSOR: 0.01 to be an island. And what about the\nemission probability?",
    "start": "3110062",
    "end": "3118930"
  },
  {
    "text": "We have to start\nan island, and then emit an A, which\nis probably what? AUDIENCE: 0.2.",
    "start": "3118930",
    "end": "3124200"
  },
  {
    "text": "PROFESSOR: 0.2, yeah,\nit's up on the screen. Should be, hopefully. Yeah, 0.02.",
    "start": "3124200",
    "end": "3130900"
  },
  {
    "text": "So who's winning so far? If the sequences\nended at position one? Genome, genome's winning.",
    "start": "3130900",
    "end": "3136690"
  },
  {
    "text": "This is a lot bigger\nthan that, it's about 150 times bigger, right? Now what do when we go-- we said\nwe have to do recursion, right?",
    "start": "3136690",
    "end": "3145210"
  },
  {
    "text": "We have to figure\nout the probability of the optimal parse ending\nat position two in each",
    "start": "3145210",
    "end": "3150840"
  },
  {
    "text": "of these states, given\nthe optimal parse ending at position one.",
    "start": "3150840",
    "end": "3156280"
  },
  {
    "text": "How do we figure that out? ",
    "start": "3156280",
    "end": "3163594"
  },
  {
    "text": "What are we going to\nwrite here, or what do we have to compare to\nfigure out what to put here?",
    "start": "3163594",
    "end": "3171310"
  },
  {
    "text": "There's two possible\nparses ending in genome at position two--\nthere's the one that started in genome\nat position one,",
    "start": "3171310",
    "end": "3176769"
  },
  {
    "text": "and there's the one\nthat started at island. So you have to\ncompare this to this.",
    "start": "3176770",
    "end": "3182680"
  },
  {
    "text": "So you compare what the\nprobability of this parse was times the\ntransition probability,",
    "start": "3182680",
    "end": "3189109"
  },
  {
    "text": "and then the emission\nin that state. So what would that be? ",
    "start": "3189110",
    "end": "3196780"
  },
  {
    "text": "What would this be,\nif we stay in genome? What's the transition?",
    "start": "3196780",
    "end": "3202380"
  },
  {
    "text": "Now we've got our five\nnines, yeah, good. So five nines. And the emission is what?",
    "start": "3202380",
    "end": "3208074"
  },
  {
    "text": "AUDIENCE: Genome at 0.2. PROFESSOR: 0.2, right. And times this.",
    "start": "3208074",
    "end": "3215859"
  },
  {
    "text": "And what about this one? What are we going\nto multiply when we consider this island\nto genome transition?",
    "start": "3215860",
    "end": "3221356"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] 0.01?",
    "start": "3221356",
    "end": "3228131"
  },
  {
    "text": "Because the genome is still 0.2. PROFESSOR: It's still 0.2. So we take the maximum\nof these, right?",
    "start": "3228132",
    "end": "3234720"
  },
  {
    "text": "We're doing optimal parse,\nhighest probability. So which one of these\ntwo turns this bigger? Clearly, the top one is bigger.",
    "start": "3234720",
    "end": "3240450"
  },
  {
    "text": "This one is already\nbigger than this, and we're multiplying\nby the same data, so clearly the answer\nhere is 0.99 times",
    "start": "3240450",
    "end": "3251370"
  },
  {
    "text": "0.3 times-- my nines\nare going to have",
    "start": "3251370",
    "end": "3258530"
  },
  {
    "text": "to get really skinny\nhere-- 0.99999 and 0.2.",
    "start": "3258530",
    "end": "3264250"
  },
  {
    "text": "That's the winner. And the other thing we do,\nbesides recording this number, is we circle this arrow.",
    "start": "3264250",
    "end": "3270654"
  },
  {
    "text": "Does this ring a bell? This is sort of\nlike Needleman-Wench or Smith-Waterman,\nwhere you don't just",
    "start": "3270654",
    "end": "3276710"
  },
  {
    "text": "record what's the\nbest score, but you remember how you got there. We're going to need that later. And what about here?",
    "start": "3276710",
    "end": "3282540"
  },
  {
    "text": "What's the optimal\nparse-- what's the probability of the\noptimal parse ending",
    "start": "3282540",
    "end": "3288339"
  },
  {
    "text": "in island at position two? Or what do I have to\ndo to calculate it? ",
    "start": "3288340",
    "end": "3300314"
  },
  {
    "text": "Sorry? AUDIENCE: You have to\ncalculate the [INAUDIBLE].",
    "start": "3300314",
    "end": "3305640"
  },
  {
    "text": "PROFESSOR: Right, you consider\ngoing genome to island here, and island to island. And who's going\nto win that race?",
    "start": "3305640",
    "end": "3312509"
  },
  {
    "text": "Do you have an idea?  Genome to island\nhad a head start,",
    "start": "3312510",
    "end": "3319120"
  },
  {
    "text": "but it pays a penalty\nfor the transition. The transition is pretty small,\nthat's 10 to the minus fifth.",
    "start": "3319120",
    "end": "3327510"
  },
  {
    "text": "And what about this one? This has a much higher\ntransition probability",
    "start": "3327510",
    "end": "3333150"
  },
  {
    "text": "of 0.999. And so even though you were\nstarting from something, this is about 150 times\nsmaller than this.",
    "start": "3333150",
    "end": "3340550"
  },
  {
    "text": "But this is being multiplied\nby 10 to the minus fifth, and this is being multiplied\nby something that's around 1.",
    "start": "3340550",
    "end": "3346010"
  },
  {
    "text": "So this one will win,\nisland to island will win. Everyone agree on that? And what will that value be?",
    "start": "3346010",
    "end": "3353940"
  },
  {
    "text": "So it's whatever\nit was before times",
    "start": "3353940",
    "end": "3359150"
  },
  {
    "text": "the transition, which is what? From island to island? ",
    "start": "3359150",
    "end": "3365589"
  },
  {
    "text": "999. Times the emission which is? ",
    "start": "3365590",
    "end": "3371870"
  },
  {
    "text": "0.3. Island is more\nlikely [? to omit ?] a C. Everyone clear on that?",
    "start": "3371870",
    "end": "3377270"
  },
  {
    "text": " And then, we're not that until\nwe circle this arrow here.",
    "start": "3377270",
    "end": "3384370"
  },
  {
    "text": "That was the winner, the\nwinner was coming from island, remaining on island. And then we keep\ngoing like this.",
    "start": "3384370",
    "end": "3392292"
  },
  {
    "text": "Do you want me to\ndo one more base? How many people want\nme to do one more base, and how many people\nwant me to stop this?",
    "start": "3392292",
    "end": "3397696"
  },
  {
    "text": "I'll do one more\nbase, but you guys will have to help\nme a little bit.",
    "start": "3397696",
    "end": "3402750"
  },
  {
    "text": "Who is going to\nwin-- now we want the probability of the\noptimal parse ending in G,",
    "start": "3402750",
    "end": "3409119"
  },
  {
    "text": "ending at position three, which\nis a G, and ending in genome,",
    "start": "3409120",
    "end": "3414300"
  },
  {
    "text": "or ending in island. So for ending in genome, where\nis that one going to come from?",
    "start": "3414300",
    "end": "3422839"
  },
  {
    "text": "Which is going to win? This one, or this one?",
    "start": "3422840",
    "end": "3428530"
  },
  {
    "text": "AUDIENCE: Stay in genome. PROFESSOR: Yeah, stay in\ngenome is going to win. This one is already\nbigger than this,",
    "start": "3428530",
    "end": "3433920"
  },
  {
    "text": "and the transition\nprobability here-- this is a 10 to minus 3\ntransition probability.",
    "start": "3433920",
    "end": "3439860"
  },
  {
    "text": "And this is a probability\nthat's near one, so the transitions are\ngoing to dominate here.",
    "start": "3439860",
    "end": "3445319"
  },
  {
    "text": "And so you're going to\nhave this term-- I'm going to call that R 2 of G.\nThat's this notation here.",
    "start": "3445320",
    "end": "3458119"
  },
  {
    "text": "And times the\ntransition probability, genome to genome, which\nis all these nines here.",
    "start": "3458120",
    "end": "3464645"
  },
  {
    "text": " And then the\nemission probability of a G in the\ngenome state is 0.2.",
    "start": "3464645",
    "end": "3471540"
  },
  {
    "text": "And who's going to win here\nfor the optimal parse, ending at position three,\nin the island state?",
    "start": "3471540",
    "end": "3478035"
  },
  {
    "text": " Is it going to be this guy,\nto island, or this one,",
    "start": "3478035",
    "end": "3486640"
  },
  {
    "text": "changing from genome to island? Island to island, because,\nagain, the transmission",
    "start": "3486640",
    "end": "3491880"
  },
  {
    "text": "probability is\nprohibitive-- that's a 10 to the minus fifth\npenalty there. So you're going\nto stay in island.",
    "start": "3491880",
    "end": "3498480"
  },
  {
    "text": "So this one won here,\nand this one won here. And so this term\nhere is R 2 of island",
    "start": "3498480",
    "end": "3509609"
  },
  {
    "text": "times the transition\nprobability, island to island, which is 0.999 times the\nemission probability, which",
    "start": "3509610",
    "end": "3516810"
  },
  {
    "text": "is 0.3 of a G in\nthe island state. Everyone clear on that?",
    "start": "3516810",
    "end": "3525440"
  },
  {
    "text": "Now, if we went out another 20\nbases, what's going to happen?",
    "start": "3525440",
    "end": "3533640"
  },
  {
    "text": "Probably not a lot. Probably the same kind of\nstuff that's happening. That seems kind of\nboring, but when would",
    "start": "3533640",
    "end": "3541050"
  },
  {
    "text": "we actually get a cross? What would it take? To push you over and cause\nyou to transition from one",
    "start": "3541050",
    "end": "3548933"
  },
  {
    "text": "to the other?  AUDIENCE: Slowly stacked\nagainst you or long enough?",
    "start": "3548934",
    "end": "3556674"
  },
  {
    "text": "PROFESSOR: Yeah, that's\na good way to put it. So let me give you an example. This is the Viterbi algorithm,\nwritten out mathematically.",
    "start": "3556675",
    "end": "3567690"
  },
  {
    "text": "We can go over this in\na moment, but I just want to try to stay\nwith the intuition here.",
    "start": "3567690",
    "end": "3573020"
  },
  {
    "text": "We did that, now\nI want to do this.",
    "start": "3573020",
    "end": "3578460"
  },
  {
    "text": "Suppose your sequence is A C\nG T, repeating 10,000 times.",
    "start": "3578460",
    "end": "3585760"
  },
  {
    "text": "Can anyone figure out what the\noptimal parse of that sequence",
    "start": "3585760",
    "end": "3591310"
  },
  {
    "text": "would be, without doing\nViterbi in their head? ",
    "start": "3591310",
    "end": "3599970"
  },
  {
    "text": "Start and stay in genome. Can you explain why? AUDIENCE: Because\nit's equal to the--",
    "start": "3599970",
    "end": "3606111"
  },
  {
    "text": "what are the\n[? widths? ?] Because it's homogeneous to the\ndistributor, as opposed",
    "start": "3606111",
    "end": "3613079"
  },
  {
    "text": "to enriched for C N G, and it\njust repeats without pattern.",
    "start": "3613080",
    "end": "3619238"
  },
  {
    "text": "Or it repeats throughout without\n[? concentrating the C N Gs ?] anywhere. PROFESSOR: Right, so the unit of\nthe repeat, this A C G T unit,",
    "start": "3619238",
    "end": "3625555"
  },
  {
    "text": "is not biased for either one. So there will be 2.3\nemissions, and 2.2 emissions,",
    "start": "3625555",
    "end": "3632120"
  },
  {
    "text": "whether you go through\nthose in G G G G or in I I I I. Does\nthat make sense?",
    "start": "3632120",
    "end": "3637200"
  },
  {
    "text": "So the emissions will be the\nsame, if you're all in genome, or if you're all in island. And the initial\nprobabilities favor genome,",
    "start": "3637200",
    "end": "3646359"
  },
  {
    "text": "and the transitions also\nfavor staying in genome. Right, so all genome.",
    "start": "3646360",
    "end": "3651433"
  },
  {
    "text": "Can everyone see that?  So do you want to take\na stab at this next one?",
    "start": "3651433",
    "end": "3658377"
  },
  {
    "start": "3658377",
    "end": "3664710"
  },
  {
    "text": "This one's harder. Let me ask you, in\nthe optimal parse, what state is it\ngoing to end in?",
    "start": "3664710",
    "end": "3672729"
  },
  {
    "text": "AUDIENCE: Genome. PROFESSOR: Genome, you've got\nto run [? with a 1,000 ?] T's. And genome, the emissions\nfavor emitting T's.",
    "start": "3672729",
    "end": "3680330"
  },
  {
    "text": "So clearly, it's going\nto end in genome. And then, what about those runs\nof C's and G's in the middle",
    "start": "3680330",
    "end": "3687609"
  },
  {
    "text": "there? Are any of those long enough to\ntrigger a transition to island? What was your name again?",
    "start": "3687610",
    "end": "3694102"
  },
  {
    "text": "AUDIENCE: Daniel. PROFESSOR: Daniel, so\nyou're shaking your head. You think they're\nnot long enough. So you think the winner's\ngoing to be genome all the way?",
    "start": "3694102",
    "end": "3704230"
  },
  {
    "text": "Who thinks they're long enough? Or maybe some of them are? Go ahead, what was your name?",
    "start": "3704230",
    "end": "3709908"
  },
  {
    "text": "AUDIENCE: Michael. PROFESSOR: Michael, yeah. AUDIENCE: The ones at length\n80 and 60 are long enough,",
    "start": "3709908",
    "end": "3714946"
  },
  {
    "text": "but the one at length 20 is not. PROFESSOR: OK, and\nwhy do you say that? AUDIENCE: Just looking\nat power of 3 over 2,",
    "start": "3714946",
    "end": "3723860"
  },
  {
    "text": "3 times 10 to the 3 isn't enough\nto overcome the difference in transition probabilities\nbetween the island and genome.",
    "start": "3723860",
    "end": "3731750"
  },
  {
    "text": "But 3 times 10 to the 10\nand 1 times 10 to the 14",
    "start": "3731750",
    "end": "3737630"
  },
  {
    "text": "is, over the length\nof those sequences.",
    "start": "3737630",
    "end": "3745680"
  },
  {
    "text": "The difference in\nprobability of making that switch once at the\nbeginning [INAUDIBLE].",
    "start": "3745680",
    "end": "3751177"
  },
  {
    "text": "PROFESSOR: OK, did everyone\nget what Michael was saying? So, Michael, can you\nexplain why powers of 1.5",
    "start": "3751177",
    "end": "3756500"
  },
  {
    "text": "are relevant here? AUDIENCE: Oh, that's the\nratio of emission probability",
    "start": "3756500",
    "end": "3768619"
  },
  {
    "text": "between the C states\nand the G states, between island and genome.",
    "start": "3768620",
    "end": "3774920"
  },
  {
    "text": "So in island it's 0.3,\nand in genome it's 0.2 over [INAUDIBLE]. PROFESSOR: Right, so when you're\ngoing through a run of C's,",
    "start": "3774920",
    "end": "3781390"
  },
  {
    "text": "if you're in the island\nstate, you get a power of 1.5, in terms of emissions\nat each position.",
    "start": "3781390",
    "end": "3788520"
  },
  {
    "text": "What about the transitions? You're sort of\nglossing over those. Why is that?",
    "start": "3788520",
    "end": "3793670"
  },
  {
    "text": "AUDIENCE: Because that\nonly has to happen once at the beginning. So the ratio between the\ntransition probabilities",
    "start": "3793670",
    "end": "3806020"
  },
  {
    "text": "is really high, but as\nlong as the compounded ratio of the\nemission probability",
    "start": "3806020",
    "end": "3812345"
  },
  {
    "text": "is high enough\nover a [INAUDIBLE] of sequences, that as long\nas that compound emission",
    "start": "3812345",
    "end": "3819755"
  },
  {
    "text": "is greater than that one-off\nratio at the beginning, then the island is\nmore [INAUDIBLE].",
    "start": "3819755",
    "end": "3831790"
  },
  {
    "text": "PROFESSOR: Yeah, so if you think\nabout the transitions, I to I, or G to G, as being close to 1--\nso if you think of them as 1,",
    "start": "3831790",
    "end": "3841290"
  },
  {
    "text": "then you can ignore them,\nand only focus on the cases where it transitions\nfrom G to I, and I to G.",
    "start": "3841290",
    "end": "3846710"
  },
  {
    "text": "So you say that 60 and\n80 are long enough. So your prediction is\nat the optimal parse",
    "start": "3846710",
    "end": "3855880"
  },
  {
    "text": "is G 1,000 I 80\nG another 2,020--",
    "start": "3855880",
    "end": "3870460"
  },
  {
    "text": "you said that one wasn't going\nto-- and then I 60 G 1,000.",
    "start": "3870460",
    "end": "3877369"
  },
  {
    "text": "Michael, is that\nwhat you're saying? Can you read this? AUDIENCE: Yeah.",
    "start": "3877370",
    "end": "3883520"
  },
  {
    "text": "PROFESSOR: OK, so why do\nyou say that 10 to the 10th",
    "start": "3883520",
    "end": "3888920"
  },
  {
    "text": "is enough to flip the switch,\nand 10 to the 3rd is not? AUDIENCE: If I remember the\nnumbers from the previous slide",
    "start": "3888920",
    "end": "3896492"
  },
  {
    "text": "correctly-- PROFESSOR: A couple\nof slides back? AUDIENCE: So if you look at\nthe ratio of the probability",
    "start": "3896492",
    "end": "3905164"
  },
  {
    "text": "of staying in the genome, and\nthe probability of going from the genome to the island, it's-- PROFESSOR: 10 to the 5th.",
    "start": "3905165",
    "end": "3911520"
  },
  {
    "text": "AUDIENCE: Yeah, 10 to the 5th. So whatever happens going over\nthe next [? run ?] of sequences",
    "start": "3911520",
    "end": "3919001"
  },
  {
    "text": "has to overcome the difference\nin ratio for the switch to become more likely.",
    "start": "3919001",
    "end": "3925322"
  },
  {
    "text": "PROFESSOR: Right. So if everyone agrees that\nwe're going to start in genome,",
    "start": "3925322",
    "end": "3931820"
  },
  {
    "text": "we've got a run of 1,000 A's,\nand genome is favored anyway-- so that's clear, we're going to\nbe in genome at the beginning",
    "start": "3931820",
    "end": "3938829"
  },
  {
    "text": "for the first 1,000, and\nbe in genome at the end, then if you're going\nto go to island, you have to pay two\npenalties, basically.",
    "start": "3938830",
    "end": "3946050"
  },
  {
    "text": "You pay the penalty of\nstarting an island, which is 10 to the minus 5th-- this is\nmaybe a slightly different way",
    "start": "3946050",
    "end": "3951657"
  },
  {
    "text": "than you were thinking\nabout it, but I think it's equivalent-- 10 the\nminus 5th to switch island, and then you pay a penalty\ncoming back, 10 to the minus 3.",
    "start": "3951657",
    "end": "3959770"
  },
  {
    "text": "And all the other\ntransitions are near 1. So it's like a 10 to\nthe minus 8 penalty for going from genome\nto island and back.",
    "start": "3959770",
    "end": "3967359"
  },
  {
    "text": "And so if the emissions\nare greater than 10 to the 8th, favor island by\na gradient of 10 to the 8th,",
    "start": "3967360",
    "end": "3975523"
  },
  {
    "text": "it'll be worth doing that. Does that make sense? AUDIENCE: I forgot about\nthe penalty of [INAUDIBLE],",
    "start": "3975523",
    "end": "3981554"
  },
  {
    "text": "but it's still the [INAUDIBLE]. PROFESSOR: Yeah,\nit's still true. Everyone see that? You have to pay a\npenalty of 10 to the 8gh",
    "start": "3981554",
    "end": "3988930"
  },
  {
    "text": "to go from genome\nto island and back. But the emissions\ncan make up for that. Even though it seems small,\nit seems like 60 bases is not",
    "start": "3988930",
    "end": "3996130"
  },
  {
    "text": "enough-- it's multiplicative,\nand it adds up. Sally?",
    "start": "3996130",
    "end": "4001880"
  },
  {
    "text": "AUDIENCE: So it seems\nlike the [INAUDIBLE] to me is going to\nreturn a lagging answer,",
    "start": "4001880",
    "end": "4010380"
  },
  {
    "text": "because we're not going\nto actually switch to genome in our HMM until we\nhit the point where we should",
    "start": "4010380",
    "end": "4017380"
  },
  {
    "text": "[? tip, ?] which would be about\n60 G's into the run of 80. PROFESSOR: So you're\nsaying it's not actually",
    "start": "4017380",
    "end": "4023057"
  },
  {
    "text": "going to predict\nthe right thing? AUDIENCE: Do you have to rerun\nthe [INAUDIBLE] processing",
    "start": "4023057",
    "end": "4029388"
  },
  {
    "text": "to get it actually in\nline to the correct thing? PROFESSOR: What do\npeople think about this?",
    "start": "4029388",
    "end": "4035060"
  },
  {
    "text": "Yeah, comment? AUDIENCE: That's\nnot quite the case, because you [? pack it ?]\nor you [? stack it ?] both",
    "start": "4035060",
    "end": "4042210"
  },
  {
    "text": "in the genome and\nisland possibilities, and your transition\nis the penalty.",
    "start": "4042210",
    "end": "4047647"
  },
  {
    "text": "So it's the highest\nimpact penalty. So when you island to island\nto island in that string of 80,",
    "start": "4047647",
    "end": "4056830"
  },
  {
    "text": "the transition\nwill only be valid starting at the first one. ",
    "start": "4056830",
    "end": "4064308"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4064308",
    "end": "4070176"
  },
  {
    "text": "PROFESSOR: OK, we're\nat position 1,000. I think you're on\nthe right track here.",
    "start": "4070176",
    "end": "4075390"
  },
  {
    "text": "So I'm going to claim that\nthe Viterbi will transition at the right place,\nbecause it's actually",
    "start": "4075390",
    "end": "4080430"
  },
  {
    "text": "proven to generate\nthe optimal parse. So I'm right, but I\ntotally get your intuition.",
    "start": "4080430",
    "end": "4089030"
  },
  {
    "text": "This is the key thing--\nmost people's intuition, my intuition, everyone's\nintuition when they first hear about this is that it\nseems like you don't transition",
    "start": "4089030",
    "end": "4097089"
  },
  {
    "text": "soon enough. It seems like you have\nto look into the future to know to transition\nat that place, right? And obviously you can't\nlook into the future,",
    "start": "4097090",
    "end": "4103759"
  },
  {
    "text": "it's a recursion. How does it work? Clearly, this is going\nto be the winner.",
    "start": "4103760",
    "end": "4111680"
  },
  {
    "text": "So let's go to position\n1,001, that's the first C.",
    "start": "4111680",
    "end": "4117014"
  },
  {
    "text": "And this guy is going\nto come from here,",
    "start": "4117014",
    "end": "4123160"
  },
  {
    "text": "this guy is the winner overall--\nG 1,000 is clearly the winner. But what about this guy?",
    "start": "4123160",
    "end": "4130189"
  },
  {
    "text": "Where's it coming from? G 1,000, it's coming from there.",
    "start": "4130189",
    "end": "4137040"
  },
  {
    "text": "And in fact, the previous\nguy came from G 1,000.",
    "start": "4137040",
    "end": "4143770"
  },
  {
    "text": "I 1,000 came from G\n999, and so forth. Now, here's the\ninteresting question.",
    "start": "4143770",
    "end": "4150710"
  },
  {
    "text": "What happens at 1,002?",
    "start": "4150710",
    "end": "4156040"
  },
  {
    "text": "Sally, I want you to tell\nme what happens at 1,002.",
    "start": "4156040",
    "end": "4163729"
  },
  {
    "text": "Who wins here? AUDIENCE: Genome. PROFESSOR: Genome. Who wins here?",
    "start": "4163729",
    "end": "4168958"
  },
  {
    "text": "AUDIENCE: Island. PROFESSOR: Island. It had been transitioning,\ngenome has got a head start,",
    "start": "4168958",
    "end": "4174600"
  },
  {
    "text": "so the best way\nto beat an island is to have been genome\nas long as possible, up until position 1,000.",
    "start": "4174600",
    "end": "4181640"
  },
  {
    "text": "And that was still\ntrue at 1,001. It's no longer true after that. It was actually better to\nhave transitioned back here",
    "start": "4181640",
    "end": "4188818"
  },
  {
    "text": "to get that one extra\nemission, that one power of 1.5 from emitting that C\nin the island state.",
    "start": "4188819",
    "end": "4194120"
  },
  {
    "text": "If you're going to\nbe in island anyway-- this is much lower than\nthis, at this point.",
    "start": "4194120",
    "end": "4199780"
  },
  {
    "text": "It's about 10 to the 5th lower. But that's OK, we still keep it. It's the best that\nends in island.",
    "start": "4199780",
    "end": "4205140"
  },
  {
    "text": "Do you see what I'm saying? OK, there were all\nthese-- island always",
    "start": "4205140",
    "end": "4211840"
  },
  {
    "text": "had to come from genome at\nthe latest possible time, up until this point, and\nnow it's actually better to have made that transition\nthere, and then stay in island.",
    "start": "4211840",
    "end": "4218780"
  },
  {
    "text": "So you can see island is\ngoing to win for awhile, and then it'll flip back.",
    "start": "4218780",
    "end": "4224460"
  },
  {
    "text": "And the question is going\nto be down here at 1,060,",
    "start": "4224460",
    "end": "4230080"
  },
  {
    "text": "going to 1,061. Who's bigger here? This guy was perhaps--\nwell, we don't even",
    "start": "4230080",
    "end": "4237180"
  },
  {
    "text": "know exactly how we got here. But you can see that this\nparse here that stays in island",
    "start": "4237180",
    "end": "4247410"
  },
  {
    "text": "is going to be optimal. And the question is, would\nit be just staying in genome? And the answer is yes, because\nit gained 10 to the 10th",
    "start": "4247410",
    "end": "4254170"
  },
  {
    "text": "in emissions, overcomes\nthe 10 to the 8th penalty that it paid.",
    "start": "4254170",
    "end": "4259921"
  },
  {
    "text": "Now what do you do at the end? How do you actually find\nthe optimal parse overall? ",
    "start": "4259921",
    "end": "4267050"
  },
  {
    "text": "I go out to position\nwhatever it is, 4,160.",
    "start": "4267050",
    "end": "4281690"
  },
  {
    "text": "I've got a probability\nhere, probability here, what do I do with those?",
    "start": "4281690",
    "end": "4286730"
  },
  {
    "text": "Right, but what do I do first? You pick the bigger one,\nwhichever one's bigger. We decided that this one is\ngoing to be bigger, right?",
    "start": "4286730",
    "end": "4295810"
  },
  {
    "text": "And then remember all the\narrows that I circled? You just backtrack through\nand figure out what it was.",
    "start": "4295810",
    "end": "4302910"
  },
  {
    "text": "Does that make sense? That's the Viterbi algorithm. We'll do it a little bit\nmore on this next time,",
    "start": "4302910",
    "end": "4313349"
  },
  {
    "text": "or definitely field questions. It's a little bit tricky\nto get your head around. ",
    "start": "4313350",
    "end": "4321450"
  },
  {
    "text": "It's a dynamic programming\nalgorithm, like Needleman-Wench or Smith-Waterman, but\na little bit different.",
    "start": "4321450",
    "end": "4326710"
  },
  {
    "text": " The runtime-- what\nis the runtime,",
    "start": "4326710",
    "end": "4332230"
  },
  {
    "text": "for those who were\nsleeping and didn't notice that little thing\nI flashed up there?",
    "start": "4332230",
    "end": "4337860"
  },
  {
    "text": "Or, if you read it, can you\nexplain where it comes from? How does the runtime depend\non the number of hidden states",
    "start": "4337860",
    "end": "4344969"
  },
  {
    "text": "and the length of the sequence? ",
    "start": "4344969",
    "end": "4351139"
  },
  {
    "text": "I've got K states, sequence of\nlength L, what is the runtime?",
    "start": "4351140",
    "end": "4357590"
  },
  {
    "text": "So I'm going to\nput this up here. This might help. ",
    "start": "4357590",
    "end": "4367210"
  },
  {
    "text": "So when you look at the\nrecursion like this, when you want to think\nabout the runtime--",
    "start": "4367210",
    "end": "4373640"
  },
  {
    "text": "forget about initialization\nand termination, that's not [INAUDIBLE]. It's what you do on the\ntypical intermediate state that",
    "start": "4373640",
    "end": "4381550"
  },
  {
    "text": "determines the runtime. That's what grows\nwith sequence length. So what do you have\nto do at each--",
    "start": "4381550",
    "end": "4389460"
  },
  {
    "text": "to go from position I\nto position I plus 1? How many calculations? ",
    "start": "4389460",
    "end": "4396208"
  },
  {
    "text": "AUDIENCE: You have to do\nN calculations for 33. Is that right?",
    "start": "4396208",
    "end": "4401699"
  },
  {
    "text": "PROFESSOR: Yeah, so 33. Yeah, the notation\nis a little bit different, but how many--\nlet me ask you this,",
    "start": "4401700",
    "end": "4408460"
  },
  {
    "text": "how many transitions do\nyou have to consider? If I have an HMM\nwith K hidden states?",
    "start": "4408460",
    "end": "4414110"
  },
  {
    "text": "AUDIENCE: K squared. PROFESSOR: K squared, right? So you're going to have to\ndo K squared calculations,",
    "start": "4414110",
    "end": "4420155"
  },
  {
    "text": "basically, to go from\nposition I to I plus 1. So what is the\noverall dependence on K and L, the length\nof the sequence?",
    "start": "4420155",
    "end": "4428460"
  },
  {
    "text": " OK, it's K squared L. It's\nlinear in the sequence.",
    "start": "4428460",
    "end": "4435559"
  },
  {
    "text": "So is this good or bad? Yes, Sally? AUDIENCE: Doesn't this assume\nthat the graph is complete? And if you don't\nactually have [INAUDIBLE]",
    "start": "4435560",
    "end": "4443916"
  },
  {
    "text": "you can get a little faster? PROFESSOR: Yeah,\nit's a good point. So this is the\nworst case, or this",
    "start": "4443916",
    "end": "4449390"
  },
  {
    "text": "is in the case where you can\ntransition from any state to any other state. If you remember, the\ngene finding HMM--",
    "start": "4449390",
    "end": "4458410"
  },
  {
    "text": "I might have erased it,\nI think I erased it-- if you can see this subtle-- No, remember Tim designed\nan HMM for gene-finding",
    "start": "4458410",
    "end": "4466530"
  },
  {
    "text": "here, which only had some\nof the arrows were allowed. So if that's true, if there's\na bunch of zero probabilities",
    "start": "4466530",
    "end": "4473010"
  },
  {
    "text": "for transitions, then\nyou can ignore those, and it actually speeds it up. That's true. It's a good point.",
    "start": "4473010",
    "end": "4478680"
  },
  {
    "text": "Everyone got this? So this the worst case. K squared L-- is\nthis good or bad?",
    "start": "4478680",
    "end": "4485320"
  },
  {
    "text": "Fast or slow? Slow? I mean, it depends on the\nstructure of your HMM.",
    "start": "4485320",
    "end": "4492599"
  },
  {
    "text": "For a simple HMM, like\nthe CPG island HMM, this is like blindingly fast.",
    "start": "4492600",
    "end": "4500210"
  },
  {
    "text": "K squared is 4, right? So it takes the same\norder of magnitude as just reading the sequence.",
    "start": "4500210",
    "end": "4505510"
  },
  {
    "text": "So it'll be super, super fast. If you make a really complicated\nHMM, it can be slower.",
    "start": "4505510",
    "end": "4510750"
  },
  {
    "text": "But the point is that for\ngenomic sequence analysis, L is big.",
    "start": "4510750",
    "end": "4516250"
  },
  {
    "text": "So as long as you keep\nK small, it'll run fast. It's much better than sequence\ncomparison, where you end up",
    "start": "4516250",
    "end": "4522310"
  },
  {
    "text": "with these L squared\ntypes of things. So it's faster than\nsequence comparison. So that's really\none of the reasons",
    "start": "4522310",
    "end": "4527410"
  },
  {
    "text": "why Viterbi is so popular,\nis it's super fast. So in the last couple\nminutes, I just",
    "start": "4527410",
    "end": "4533020"
  },
  {
    "text": "want to say a few things\nabout the midterm. You guys did remember\nthere is a mid-term, right?",
    "start": "4533020",
    "end": "4538490"
  },
  {
    "text": "So the midterm is a week from\ntoday, Tuesday, March 18.",
    "start": "4538490",
    "end": "4544440"
  },
  {
    "text": "For everybody, it's\ngoing to be here, except for those\nwho are in 6874--",
    "start": "4544440",
    "end": "4550239"
  },
  {
    "text": "and those people\nshould go to 68 180. And because they're going\nto be given extra time,",
    "start": "4550240",
    "end": "4557200"
  },
  {
    "text": "you should go there early. Go there at 12:40. Everyone else,\nwho's not in 6784,",
    "start": "4557200",
    "end": "4563199"
  },
  {
    "text": "should come here to the\nregular class by 1:00 PM, just so you have a chance to\nget set up and everything.",
    "start": "4563200",
    "end": "4569500"
  },
  {
    "text": "And then the exam will\nstart promptly at 1:05, and will end at 2:25,\nan hour and 20 minutes.",
    "start": "4569500",
    "end": "4576820"
  },
  {
    "text": "It is closed book, open notes. So don't bring your textbook,\nbut you can bring up",
    "start": "4576820",
    "end": "4582420"
  },
  {
    "text": "to two pages-- they\ncan be double sided if you want-- of notes. So why do we do this? Well, we think that the act\nof going through the lectures",
    "start": "4582420",
    "end": "4592949"
  },
  {
    "text": "and textbook, or whatever\nother notes you have, and deciding what's most\nimportant, maybe helpful.",
    "start": "4592950",
    "end": "4601480"
  },
  {
    "text": "And so this, hopefully, will be\na useful studying exercise, so figure out what's most\nimportant and write it down on a piece of paper if you are\nlikely to forget it-- maybe",
    "start": "4601480",
    "end": "4609340"
  },
  {
    "text": "complicated\nequations, things like that, you might\nwant to write down. No calculators or\nother electronic aids.",
    "start": "4609340",
    "end": "4615570"
  },
  {
    "text": "But you won't need them. If you get an answer that's\ne squared over 17 factorial--",
    "start": "4615570",
    "end": "4623400"
  },
  {
    "text": "you're asked to convert\nthat into a decimal. Just leave it like that. So what should you study?",
    "start": "4623400",
    "end": "4629430"
  },
  {
    "text": "So you should study your lecture\nnotes, readings and tutorials, and past exams. Past exams have been posted\nto the course website.",
    "start": "4629430",
    "end": "4637050"
  },
  {
    "text": "p-sets as well. The midterm exams from\npast years are posted. And there's some variation\nin topics from year to year,",
    "start": "4637050",
    "end": "4645110"
  },
  {
    "text": "so if you're reading through\na midterm from a past year, and you run across an\nunfamiliar phrase or concept,",
    "start": "4645110",
    "end": "4653000"
  },
  {
    "text": "you have to ask\nyourself, was I just dozing off when\nthat was discussed? Or was that not\ndiscussed this year?",
    "start": "4653000",
    "end": "4659320"
  },
  {
    "text": "And act appropriately. The content of the midterm\nwill be all the lectures,",
    "start": "4659320",
    "end": "4664800"
  },
  {
    "text": "all the topics up through\ntoday-- hidden Markov models. And I'll do a little bit\nmore on hidden Markov models",
    "start": "4664800",
    "end": "4670960"
  },
  {
    "text": "on Thursday. That part could be on the exam,\nbut the next major topic-- RNA",
    "start": "4670960",
    "end": "4677060"
  },
  {
    "text": "secondary structure--\nwill not be on the exam. It'll be on a p-set\nin the future.",
    "start": "4677060",
    "end": "4682627"
  },
  {
    "text": "Any questions about the midterm? ",
    "start": "4682627",
    "end": "4690060"
  },
  {
    "text": "And the TAs will be doing\nsome review stuff in sections this week. ",
    "start": "4690060",
    "end": "4697670"
  },
  {
    "text": "OK, thank you. See you on Thursday. ",
    "start": "4697670",
    "end": "4705499"
  }
]