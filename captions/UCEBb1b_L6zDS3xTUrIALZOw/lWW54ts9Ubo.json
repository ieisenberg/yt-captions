[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "text": " PHILIPPE RIGOLLET:\nSo I apologize.",
    "start": "17880",
    "end": "23300"
  },
  {
    "text": "My voice is not 100%. So if you don't understand\nwhat I'm saying, please ask me.",
    "start": "23300",
    "end": "32930"
  },
  {
    "text": "So we're going to be analyzing--\nactually, not really analyzing. We described a\nsecond-order method",
    "start": "32930",
    "end": "38750"
  },
  {
    "text": "to optimize the log likelihood\nin a generalized linear model, when the parameter\nof interest was beta.",
    "start": "38750",
    "end": "45637"
  },
  {
    "text": "So here, I'm going to rewrite\nthe whole thing as a beta. So that's the equation you see. But we really have this beta.",
    "start": "45637",
    "end": "51560"
  },
  {
    "text": "And at iteration k plus 1,\nbeta is given by beta k.",
    "start": "51560",
    "end": "58160"
  },
  {
    "text": "And then I have a plus sign. And the plus, if you think of\nthe Fisher information at beta",
    "start": "58160",
    "end": "66390"
  },
  {
    "text": "k as being some number-- if you were to say\nwhether it's a positive or a negative\nnumber, it's actually",
    "start": "66390",
    "end": "72548"
  },
  {
    "text": "going to be a positive\nnumber, because it's a positive semi-definite matrix. So since we're doing\ngradient ascent,",
    "start": "72549",
    "end": "78020"
  },
  {
    "text": "we have a plus sign here. And then the\ndirection is basically gradient ln at beta k.",
    "start": "78020",
    "end": "86750"
  },
  {
    "text": "OK? So this is the iterations that\nwe're trying to implement. And we could just do this. At each iteration, we compute\nthe Fisher information,",
    "start": "86750",
    "end": "94814"
  },
  {
    "text": "and then we do it\nagain and again. All right. That's called the\nFisher-scoring algorithm. And I told you that this\nwas going to converge.",
    "start": "94814",
    "end": "101045"
  },
  {
    "text": "And what we're going to\ntry to do in this lecture is to show how we can\nre-implement this,",
    "start": "101045",
    "end": "106100"
  },
  {
    "text": "using iteratively\nre-weighted least squares, so that each step\nof this algorithm consists simply of solving a\nweighted least square problem.",
    "start": "106100",
    "end": "114270"
  },
  {
    "text": "All right. So let's go back quickly\nand remind ourselves",
    "start": "114270",
    "end": "119840"
  },
  {
    "text": "that we are in the Gaussian-- sorry, we're in the\nexponential family.",
    "start": "119840",
    "end": "127110"
  },
  {
    "text": "So if I look at the log\nlikelihood for one observation, so here it's ln--",
    "start": "127110",
    "end": "132290"
  },
  {
    "text": "sorry. This is the sum from i\nequal 1 to n of yi minus-- ",
    "start": "132290",
    "end": "140694"
  },
  {
    "text": "OK, so it's yi times theta\ni, sorry, minus b of theta i.",
    "start": "140694",
    "end": "146770"
  },
  {
    "text": "Then there's going\nto be some parameter. And then I have\nplus c of yi phi.",
    "start": "146770",
    "end": "152810"
  },
  {
    "text": "OK. So just the\nexponential went away when I took the log\nof the likelihood. And I have n observations,\nso I'm summing",
    "start": "152810",
    "end": "158600"
  },
  {
    "text": "over all n observations. All right. Then we had a bunch of\nformulas that we came up to be. So if I look at the\nexpectation of yi--",
    "start": "158600",
    "end": "166100"
  },
  {
    "text": "so that's really the\nconditional of yi, given xi. But like here, it\nreally doesn't matter.",
    "start": "166100",
    "end": "172230"
  },
  {
    "text": "It's just going to be\ndifferent for each i. This is denoted by mu i. And we showed that this\nwas beta prime of theta i.",
    "start": "172230",
    "end": "180830"
  },
  {
    "text": "Then the other equation\nthat we found was that. And so what we want to\nmodel is this thing.",
    "start": "180830",
    "end": "186350"
  },
  {
    "text": "We want it to be equal to\nxi transpose beta- sorry g of this thing. ",
    "start": "186350",
    "end": "194918"
  },
  {
    "text": "All right. So that's our model. And then we had that\nthe variance was also",
    "start": "194918",
    "end": "201650"
  },
  {
    "text": "given by the second derivative. I'm not going to go into it. What's actually\ninteresting is to see, if we want to express theta i as\na function of xi, what we get,",
    "start": "201650",
    "end": "212240"
  },
  {
    "text": "going from xi to mu i by g\ninverse, and then to theta i",
    "start": "212240",
    "end": "218900"
  },
  {
    "text": "by b inverse, we\nget that theta i is equal to h of xi transpose\nbeta, h of xi transpose beta,",
    "start": "218900",
    "end": "231659"
  },
  {
    "text": "where h is the inverse-- so which order is --this?",
    "start": "231660",
    "end": "238890"
  },
  {
    "text": "Is the inverse of g, and then\nthe compose would be prime. OK?",
    "start": "238890",
    "end": "245209"
  },
  {
    "text": "So we remember that last time,\nthose are all computations that we've made,\nbut they're going",
    "start": "245210",
    "end": "250610"
  },
  {
    "text": "to be useful in our derivation. And the first thing\nwe did last time is to show that, if I look now\nat the derivative of the log",
    "start": "250610",
    "end": "257690"
  },
  {
    "text": "likelihood with respect to\none coordinate of beta, which is going to give me the\ngradient if I do that for all",
    "start": "257690",
    "end": "263060"
  },
  {
    "text": "the coordinates, what\nwe ended up finding is that we can rewrite\nit in this form, some",
    "start": "263060",
    "end": "268400"
  },
  {
    "text": "of yi tilde minus mu tilde. So let's remind ourselves that-- ",
    "start": "268400",
    "end": "276340"
  },
  {
    "text": "so y tilde is just y divided--",
    "start": "276340",
    "end": "281560"
  },
  {
    "text": "well, OK y tilde i is yi-- is it times or divided-- times g prime of mu i.",
    "start": "281560",
    "end": "290889"
  },
  {
    "text": "Mu tilde i is mu i\ntimes g prime of mu i.",
    "start": "290890",
    "end": "300140"
  },
  {
    "text": "And then that was just\nan artificial thing, so that we could actually\ndivide the weights by g prime.",
    "start": "300140",
    "end": "307070"
  },
  {
    "text": "But the real thing that built\nthe weights are this h prime. And there's this\nnormalization factor.",
    "start": "307070",
    "end": "312310"
  },
  {
    "text": "And so if we read it\nlike that-- so if I also write that wi is h prime of\nxi transpose beta divided",
    "start": "312310",
    "end": "322900"
  },
  {
    "text": "by g prime of mu\ni times phi, then I could actually rewrite\nmy gradient, which",
    "start": "322900",
    "end": "330820"
  },
  {
    "text": "is a vector, in the\nfollowing matrix form, the gradient ln at beta.",
    "start": "330820",
    "end": "340819"
  },
  {
    "text": "So the gradient of my\nlog likelihood of beta took the following form. It was x transpose w, and\nthen y tilde minus mu tilde.",
    "start": "340820",
    "end": "353600"
  },
  {
    "text": "And here, w was just\nthe matrix with w1, w2, all the way to wn\non the diagonal and 0",
    "start": "353600",
    "end": "362020"
  },
  {
    "text": "on of the up diagonals. OK? So that was just\ntaking the derivative",
    "start": "362020",
    "end": "368340"
  },
  {
    "text": "and doing a slight\nmanipulations that said, well, let's just divide\nwhatever is here by g",
    "start": "368340",
    "end": "374669"
  },
  {
    "text": "prime and multiply whatever\nis here by g prime. So today, we'll see why\nwe make this division",
    "start": "374670",
    "end": "379680"
  },
  {
    "text": "and multiplication by g prime,\nwhich seems to make no sense, but it actually comes from\nthe Hessian computations.",
    "start": "379680",
    "end": "386620"
  },
  {
    "text": "So the Hessian\ncomputations are going to be a little more annoying. Actually, let me start directly\nwith the coordinate y's",
    "start": "386620",
    "end": "393600"
  },
  {
    "text": "derivative, right? So to build this gradient,\nwhat we used, in the end, was that the partial derivative\nof ln with respect to the gth",
    "start": "393600",
    "end": "401880"
  },
  {
    "text": "coordinate of beta was\nequal to the sum over i",
    "start": "401880",
    "end": "409220"
  },
  {
    "text": "of yi tilde minus mu\ni tilde times wi times",
    "start": "409220",
    "end": "415520"
  },
  {
    "text": "the gth coordinate of xi. OK?",
    "start": "415520",
    "end": "421310"
  },
  {
    "text": "So now, let's just take\nanother derivative, and that's going to give us\nthe entries of the Hessian.",
    "start": "421310",
    "end": "427810"
  },
  {
    "text": "OK, so we're going to\nthe second derivative. So what I want to compute is\nthe derivative with respect",
    "start": "427810",
    "end": "436949"
  },
  {
    "text": "to beta j and beta k.  OK.",
    "start": "436950",
    "end": "442330"
  },
  {
    "text": "So where does beta j-- so here, I already took\nthe derivative with respect to beta j.",
    "start": "442330",
    "end": "447550"
  },
  {
    "text": "So this is just the\nderivative with respect to beta k of the derivative\nwith respect to beta j.",
    "start": "447550",
    "end": "452850"
  },
  {
    "text": " So what I need to do is to\ntake the derivative of this guy",
    "start": "452850",
    "end": "459289"
  },
  {
    "text": "with respect to beta k. Where does beta k show up here? ",
    "start": "459290",
    "end": "468920"
  },
  {
    "text": "It's set in, in two places. AUDIENCE: In the y's? PHILIPPE RIGOLLET: No,\nit's not in the y's.",
    "start": "468920",
    "end": "474969"
  },
  {
    "text": "The y's are my data, right?  But I mean, it's\nin the y tildes.",
    "start": "474970",
    "end": "482220"
  },
  {
    "text": "Yeah, because it's in mu, right? Mu depends on beta. Mu is g inverse of\nxi transpose beta.",
    "start": "482220",
    "end": "489270"
  },
  {
    "text": "And it's also in the wi's. Actually, everything that you\nsee is directly-- well, OK, w",
    "start": "489270",
    "end": "497070"
  },
  {
    "text": "depends on mu n on\nbeta explicitly. But the rest depends only on mu.",
    "start": "497070",
    "end": "504480"
  },
  {
    "text": "And so we might want\nto be a little-- well, we can actually use the--",
    "start": "504480",
    "end": "510660"
  },
  {
    "text": "did I use the\nchain rule already?  Yeah, it's here.",
    "start": "510660",
    "end": "516950"
  },
  {
    "text": "But OK, well, let's go for it. ",
    "start": "516950",
    "end": "529200"
  },
  {
    "text": "Oh yeah, OK. Sorry, I should not\nwrite it like that, because that was actually--",
    "start": "529200",
    "end": "534390"
  },
  {
    "text": "right, so I make my life\nmiserable by just multiplying and dividing by\nthis g prime of mu.",
    "start": "534390",
    "end": "540800"
  },
  {
    "text": "I should not do this, right? So what I should just write\nis say that this guy here-- I'm actually going to\nremove the g prime of mu,",
    "start": "540800",
    "end": "549180"
  },
  {
    "text": "because I just make something\nthat depends on theta appear when it\nreally should not. So let's just look at the\nlast but one equality.",
    "start": "549180",
    "end": "555880"
  },
  {
    "start": "555880",
    "end": "563000"
  },
  {
    "text": "OK. So that's the one over\nthere, and then I have xi j. OK, so here, it make my\nlife much more simple,",
    "start": "563000",
    "end": "569800"
  },
  {
    "text": "because yi does\nnot depend on beta, but this guy depends on beta,\nand this guy depends on beta. All right.",
    "start": "569800",
    "end": "575074"
  },
  {
    "text": "So when I take the\nderivative, I'm going to have to be a\nlittle more careful now. But I just have a\nderivative of a product,",
    "start": "575074",
    "end": "580300"
  },
  {
    "text": "nothing more complicated. So this is what? Well, the sum is\ngoing to be linear,",
    "start": "580300",
    "end": "585350"
  },
  {
    "text": "so it's going to come out. Then I'm going to have to take\nthe derivative of this term.",
    "start": "585350",
    "end": "591170"
  },
  {
    "text": "So it's just going\nto be 1 over psi. Then the derivative\nof mu i with respect",
    "start": "591170",
    "end": "598460"
  },
  {
    "text": "to beta k, which I will\njust write like this,",
    "start": "598460",
    "end": "604440"
  },
  {
    "text": "times h prime of xi\ntranspose beta xi j.",
    "start": "604440",
    "end": "609920"
  },
  {
    "text": "And then I'm going to have the\nother one, which is yi minus mu",
    "start": "609920",
    "end": "615570"
  },
  {
    "text": "i over 5 times the second\nderivative of h of xi transpose",
    "start": "615570",
    "end": "624640"
  },
  {
    "text": "beta. And then I'm going to\ntake the derivative of this guy with respect to beta\nj with beta k, which is just",
    "start": "624640",
    "end": "630190"
  },
  {
    "text": "xi k. So I have xi j times xi k.",
    "start": "630190",
    "end": "636200"
  },
  {
    "text": "OK. So I still need to\ncompute this guy. So what is the\npartial derivative",
    "start": "636200",
    "end": "642590"
  },
  {
    "text": "with respect to beta k of g? So mu is g of--",
    "start": "642590",
    "end": "649310"
  },
  {
    "text": "worry, it's g inverse\nof xi transpose beta. ",
    "start": "649310",
    "end": "656610"
  },
  {
    "text": "OK? So what do I get? Well, I'm going\nto get definitely the second derivative of g.",
    "start": "656610",
    "end": "662990"
  },
  {
    "start": "662990",
    "end": "671558"
  },
  {
    "text": "Well, OK, that's\nactually not a bad idea. ",
    "start": "671558",
    "end": "677857"
  },
  {
    "text": "Well, no, that's OK. I can make the second-- what makes my life\neasier, actually? ",
    "start": "677857",
    "end": "686690"
  },
  {
    "text": "Give me one second. Well, there's no\none that actually",
    "start": "686690",
    "end": "693230"
  },
  {
    "text": "makes my life so much easier. Let's just write it. Let's go with this guy. So it's going to be g prime\nprime of xi transpose beta",
    "start": "693230",
    "end": "703300"
  },
  {
    "text": "times xi k. OK?",
    "start": "703300",
    "end": "710140"
  },
  {
    "text": "So now, what do I have\nif I collect my terms? I have that this whole thing\nhere, the second derivative is,",
    "start": "710140",
    "end": "725990"
  },
  {
    "text": "well, I have the sum\nfrom 1 equal 1 to n. Then I have terms that\nI can factor out, right?",
    "start": "725990",
    "end": "733199"
  },
  {
    "text": "Both of these guys have xi j,\nand this guy pulls out an xi k. And it's also here, xi\nj times xi k, right?",
    "start": "733200",
    "end": "741449"
  },
  {
    "text": "So everybody here is xi j xi k.",
    "start": "741450",
    "end": "746690"
  },
  {
    "text": "And now, I just have to take\nthe terms that I have here. The 1 over phi, I can\nactually pull out in front.",
    "start": "746690",
    "end": "753490"
  },
  {
    "text": "And I'm left with the\nsecond derivative of g times",
    "start": "753490",
    "end": "760399"
  },
  {
    "text": "the first derivative of h, both\ntaken at xi transpose beta.",
    "start": "760400",
    "end": "766370"
  },
  {
    "text": "And then, I have\nthis yi minus mu i times the second derivative of\nh, taken at xi transpose beta.",
    "start": "766370",
    "end": "772166"
  },
  {
    "start": "772166",
    "end": "780180"
  },
  {
    "text": "OK. But here, I'm looking\nat Fisher scoring. I'm not looking at\nNewton's method, which",
    "start": "780180",
    "end": "787200"
  },
  {
    "text": "means that I can actually\ntake the expectation of the second derivative. So when I start taking\nthe expectation,",
    "start": "787200",
    "end": "793260"
  },
  {
    "text": "what's going to happen-- so if I take the expectation\nof this whole thing here, well, this guy, it's not--",
    "start": "793260",
    "end": "801829"
  },
  {
    "text": "and when I say expectation,\nit's always conditionally on xi. So let's write it--",
    "start": "801830",
    "end": "807470"
  },
  {
    "text": "x1 xn. So I take conditional. This is just deterministic.",
    "start": "807470",
    "end": "812790"
  },
  {
    "text": "But what is the\nconditional expectation of yi minus mu i times this\nguy, conditionally on xi?",
    "start": "812790",
    "end": "819570"
  },
  {
    "text": " 0, right? Because this is just the\nconditional expectation of yi,",
    "start": "819570",
    "end": "825790"
  },
  {
    "text": "and everything else\ndepends on xi only, so I can push it out of the\nconditional expectation.",
    "start": "825790",
    "end": "830810"
  },
  {
    "text": "So I'm left only with this term. ",
    "start": "830810",
    "end": "846460"
  },
  {
    "text": "OK. ",
    "start": "846460",
    "end": "853850"
  },
  {
    "text": "So now I need to-- sorry, and I have\nxi xj, xi j xi j.",
    "start": "853850",
    "end": "863185"
  },
  {
    "text": "OK So now, I want to go to\nsomething that's slightly more",
    "start": "863185",
    "end": "874850"
  },
  {
    "text": "convenient for me. So maybe we can\nskip that part here, because this is not going to\nbe convenient for me anyway.",
    "start": "874850",
    "end": "880790"
  },
  {
    "text": "So I just want to go back to\nsomething that looks eventually like this.",
    "start": "880790",
    "end": "888150"
  },
  {
    "text": "OK, that's what\nI'm going to want. So I need to have my xi show\nup with some weight somehow.",
    "start": "888150",
    "end": "893700"
  },
  {
    "text": "And the weight should involve\nh prime divided by g prime. Again, the reason why I want\nto see g prime coming back",
    "start": "893700",
    "end": "900840"
  },
  {
    "text": "is because I had g prime\ncoming in the original w. This is actually the\nsame definition as the w",
    "start": "900840",
    "end": "906690"
  },
  {
    "text": "that I used when I was\ncomputing the gradient. Those are exactly\nthese w's, those guys.",
    "start": "906690",
    "end": "913600"
  },
  {
    "text": "So I need to have g\nprime that shows up. And that's where\nI'm going to have to make a little bit\nof computation here.",
    "start": "913600",
    "end": "921240"
  },
  {
    "text": "And it's coming from this\nkind of consideration.",
    "start": "921240",
    "end": "926459"
  },
  {
    "text": "OK? So this thing here-- ",
    "start": "926460",
    "end": "933680"
  },
  {
    "text": "well, actually, I'm missing\nthe phi over there, right?",
    "start": "933680",
    "end": "939180"
  },
  {
    "text": "There should be a phi here. OK. So we have exactly this thing,\nbecause this tells me that,",
    "start": "939180",
    "end": "946482"
  },
  {
    "text": "if I look at the Hessian-- ",
    "start": "946482",
    "end": "953840"
  },
  {
    "text": "so this was entry-wise,\nand this is exactly the form of something\nof the form of the k.",
    "start": "953840",
    "end": "958930"
  },
  {
    "text": "This is exactly the jth kth\nentry of xi xi transpose.",
    "start": "958930",
    "end": "966120"
  },
  {
    "text": "Right? We've used that before. So if I want to write\nthis in a vector form, this is just going to be the\nsum of something that depends",
    "start": "966120",
    "end": "973010"
  },
  {
    "text": "on i times xi xi transpose. So this is 1 over phi sum\nfrom i equal 1 to n of g",
    "start": "973010",
    "end": "980660"
  },
  {
    "text": "prime prime xi transpose beta\nh prime xi transpose beta xi xi",
    "start": "980660",
    "end": "988579"
  },
  {
    "text": "transpose. OK? And that's for\nthe entire matrix. Here, that was just the j\nkth entries of this matrix.",
    "start": "988580",
    "end": "994820"
  },
  {
    "text": " And you can just check\nthat, if I take this matrix,",
    "start": "994820",
    "end": "1001640"
  },
  {
    "text": "the j kth entry is just the\nproduct of the jth coordinate and the kth coordinate of xi.",
    "start": "1001640",
    "end": "1008780"
  },
  {
    "text": "All right. So now I need to\ndo my rewriting.",
    "start": "1008780",
    "end": "1014000"
  },
  {
    "text": "Can I write this?  So I'm missing\nsomething here, right?",
    "start": "1014000",
    "end": "1020070"
  },
  {
    "start": "1020070",
    "end": "1031790"
  },
  {
    "text": "Oh, I know where\nit's coming from. ",
    "start": "1031790",
    "end": "1038630"
  },
  {
    "text": "Mu is not g prime of x beta. Mu is g inverse\nof x beta, right?",
    "start": "1038630",
    "end": "1044386"
  },
  {
    "text": " So the derivative of x\nprime is not g prime prime.",
    "start": "1044386",
    "end": "1054670"
  },
  {
    "text": "It's like this guy--",
    "start": "1054670",
    "end": "1059915"
  },
  {
    "text": " no, 1 over this, right?",
    "start": "1059915",
    "end": "1066660"
  },
  {
    "text": " Yeah.",
    "start": "1066660",
    "end": "1072083"
  },
  {
    "start": "1072083",
    "end": "1086880"
  },
  {
    "text": "OK? The derivative of g inverse is\n1 over g prime of gene inverse.",
    "start": "1086880",
    "end": "1092180"
  },
  {
    "text": " I need you guys, OK?",
    "start": "1092180",
    "end": "1098260"
  },
  {
    "text": "All right. So now, I'm going to\nhave to rewrite this. This guy is still\ngoing to go away. It doesn't matter,\nbut now this thing",
    "start": "1098260",
    "end": "1103351"
  },
  {
    "text": "is becoming h prime over g prime\nof g inverse of xi transpose",
    "start": "1103351",
    "end": "1110179"
  },
  {
    "text": "beta, which is the same\nhere, which is the same here.",
    "start": "1110180",
    "end": "1121820"
  },
  {
    "start": "1121820",
    "end": "1132220"
  },
  {
    "text": "OK? Everybody approves? All right. Well, now, it's\nactually much nicer.",
    "start": "1132220",
    "end": "1138460"
  },
  {
    "text": "What is g inverse of\nxi transpose beta? ",
    "start": "1138460",
    "end": "1145154"
  },
  {
    "text": "Well, that was exactly the\nmistake that I just made, right? It's mu i itself.",
    "start": "1145154",
    "end": "1150960"
  },
  {
    "text": "So this guy is really\ng prime of mu i.",
    "start": "1150960",
    "end": "1158330"
  },
  {
    "text": "Sorry, just the bottom, right?  So now, I have something\nwhich looks like a sum from i",
    "start": "1158330",
    "end": "1172870"
  },
  {
    "text": "equal 1 to n of h prime\nof xi transpose beta,",
    "start": "1172870",
    "end": "1178470"
  },
  {
    "text": "divided by g prime of mu i phi\ntimes xi xi transpose, which",
    "start": "1178470",
    "end": "1186780"
  },
  {
    "text": "I can certainly write in\nmatrix form as x transpose wx,",
    "start": "1186780",
    "end": "1194550"
  },
  {
    "text": "where w is exactly\nthe same as before.",
    "start": "1194550",
    "end": "1200410"
  },
  {
    "text": "So it's w1 wn. And wi is h prime\nof xi transpose beta",
    "start": "1200410",
    "end": "1211380"
  },
  {
    "text": "divided by g prime of mu i. There's a prime here\ntimes phi, which",
    "start": "1211380",
    "end": "1220880"
  },
  {
    "text": "is the same that we had here. And it's supposed to be\nthe same that we have here,",
    "start": "1220880",
    "end": "1226610"
  },
  {
    "text": "except the phi is in white. That's why it's not there.",
    "start": "1226610",
    "end": "1231820"
  },
  {
    "text": "OK. ",
    "start": "1231820",
    "end": "1237654"
  },
  {
    "text": "All right? So it's actually simpler than\nwhat's on the slides, I guess.",
    "start": "1237655",
    "end": "1242891"
  },
  {
    "text": "All right. So now, if you pay\nattention, I actually never force this g prime\nof mu i to be here.",
    "start": "1242891",
    "end": "1249059"
  },
  {
    "text": "Actually, I even tried to\nmake a mistake to not have it. And so this g prime of mu i\nshows up completely naturally.",
    "start": "1249060",
    "end": "1257670"
  },
  {
    "text": "If I had started with this,\nyou would have never questioned",
    "start": "1257670",
    "end": "1264200"
  },
  {
    "text": "why I actually didn't\nmultiply by g prime and divided by g prime\ncompletely artificially here.",
    "start": "1264200",
    "end": "1269700"
  },
  {
    "text": "It just shows up\nnaturally in the weights. But it's just more\nnatural for me to compute the first\nderivative first",
    "start": "1269700",
    "end": "1275771"
  },
  {
    "text": "than the second\nderivative second, OK? And so we just did it\nthe other way around. But now, let's assume we\nforgot about everything.",
    "start": "1275771",
    "end": "1283620"
  },
  {
    "text": "We have this. This is a natural way of\nwriting it, x transpose wx. If I want something that\ninvolves some weights,",
    "start": "1283620",
    "end": "1290309"
  },
  {
    "text": "I have to force them in by\ndividing by g prime of mu i and therefore, multiplying\nyi n mu i by this wi.",
    "start": "1290310",
    "end": "1300410"
  },
  {
    "text": "OK? So now, if we recap what we've\nactually found, we got that--",
    "start": "1300410",
    "end": "1306490"
  },
  {
    "text": " let me write it here.",
    "start": "1306490",
    "end": "1311539"
  },
  {
    "start": "1311540",
    "end": "1318740"
  },
  {
    "text": "We also have that\nthe expectation of H ln of beta x transpose xw.",
    "start": "1318740",
    "end": "1332100"
  },
  {
    "text": "So if I go back to my\niterations over there, I should actually\nupdate beta k plus 1",
    "start": "1332100",
    "end": "1340259"
  },
  {
    "text": "to be equal to beta\nk plus the inverse. So that's actually equal\nto negative i of beta k--",
    "start": "1340260",
    "end": "1350250"
  },
  {
    "text": "well, yeah. That's negative i\nof beta, I guess. ",
    "start": "1350250",
    "end": "1358230"
  },
  {
    "text": "Oh, and beta here shows up in\nw, right? w depends on beta. So that's going to be beta k.",
    "start": "1358230",
    "end": "1364460"
  },
  {
    "text": "So let me call it wk.  So that's the diagonal of\nH prime xi transpose beta",
    "start": "1364460",
    "end": "1374460"
  },
  {
    "text": "k, this time, divided by\ng prime of mu i k phi.",
    "start": "1374460",
    "end": "1381799"
  },
  {
    "text": "OK? So this beta k induces\na mu by looking at g inverse of xi\ntranspose beta k.",
    "start": "1381800",
    "end": "1391141"
  },
  {
    "text": "All right. So mu i k is g inverse\nof xi transpose beta k.",
    "start": "1391141",
    "end": "1401804"
  },
  {
    "text": "So that's 2 to the--\nsorry, that's an iteration. And so now, if I actually\nwrite these things together,",
    "start": "1401804",
    "end": "1408080"
  },
  {
    "text": "I get minus x\ntranspose wx inverse.",
    "start": "1408080",
    "end": "1417820"
  },
  {
    "text": "So that's wk.  And then I have my\ngradient here that I",
    "start": "1417820",
    "end": "1425260"
  },
  {
    "text": "have to apply at k,\nwhich is x transpose wk.",
    "start": "1425260",
    "end": "1430810"
  },
  {
    "text": "And then I have y tilde k minus\nmu tilde k, where, again, the",
    "start": "1430810",
    "end": "1438610"
  },
  {
    "text": "indices-- I mean the superscript\nk are pretty natural. y tilde k just means that--",
    "start": "1438610",
    "end": "1445720"
  },
  {
    "text": "so that's just yi. So that's just yi times\ng prime of mu i k.",
    "start": "1445720",
    "end": "1454650"
  },
  {
    "text": "And mu tilde k is, if I\nlook at the i coordinate,",
    "start": "1454650",
    "end": "1461050"
  },
  {
    "text": "it's just going to be mu\ni times g prime of mu i.",
    "start": "1461050",
    "end": "1467960"
  },
  {
    "text": " OK? So I just add superscripts\nk to everything.",
    "start": "1467960",
    "end": "1474470"
  },
  {
    "text": "So I know that those things\nget updated real time, right? Every time I make one iteration,\nI get a new value for beta,",
    "start": "1474470",
    "end": "1481669"
  },
  {
    "text": "I get a new value for\nmu, and therefore, I get a new value for w. Yes? AUDIENCE: [INAUDIBLE] the\nFisher equation [INAUDIBLE]??",
    "start": "1481670",
    "end": "1490210"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah,\nthat's a good point. So that's definitely\na plus, because this is a positive,\nsemi-definite matrix.",
    "start": "1490210",
    "end": "1496030"
  },
  {
    "text": "So this is a plus. And well, that's probably\nwhere I erased it.",
    "start": "1496030",
    "end": "1501330"
  },
  {
    "start": "1501330",
    "end": "1515919"
  },
  {
    "text": "OK. Let's see where I\nmade my mistake. ",
    "start": "1515920",
    "end": "1523510"
  },
  {
    "text": "So there should be a minus here.",
    "start": "1523510",
    "end": "1528602"
  },
  {
    "text": "There should be a minus here. There should be a minus even\nat the beginning, I believe. So that means that what\nis my-- oh, yeah, yeah.",
    "start": "1528602",
    "end": "1537940"
  },
  {
    "text": "So you see, when we\ngo back to the first, so what I erased was basically\nthis thing here, yi minus mu i.",
    "start": "1537940",
    "end": "1547440"
  },
  {
    "text": "And when I took the\nfirst derivative-- so it was the derivative\nwith respect to H prime.",
    "start": "1547440",
    "end": "1553170"
  },
  {
    "text": "So the derivative with\nrespect to the second term-- I mean, the derivative\nof the second term was actually killed,\nbecause we took",
    "start": "1553170",
    "end": "1559754"
  },
  {
    "text": "the expectation of this guy. But when we took the derivative\nof the first term, which is the only one that\nstayed, this guy went away.",
    "start": "1559754",
    "end": "1565747"
  },
  {
    "text": "But there was a negative\nsign from this guy, because that's the thing\nwe took the negative off. So it's really, when I\ntake my second derivative,",
    "start": "1565747",
    "end": "1572920"
  },
  {
    "text": "I should carry out the\nminus signs everywhere. ",
    "start": "1572920",
    "end": "1582084"
  },
  {
    "text": "OK? So it's just I forget\nthis minus throughout. ",
    "start": "1582084",
    "end": "1591700"
  },
  {
    "text": "You see the first term went\naway, on the first line there. The first term\nwent away, because the conditional expectation\nof yi, given xi 0.",
    "start": "1591700",
    "end": "1598929"
  },
  {
    "text": "And then I had this minus\nsign in front of everyone, and I forgot it. ",
    "start": "1598930",
    "end": "1604660"
  },
  {
    "text": "All right. Any other mistake that I made? ",
    "start": "1604660",
    "end": "1611230"
  },
  {
    "text": "We're good? All right. So now, this is what\nwe have, that xk--",
    "start": "1611230",
    "end": "1628360"
  },
  {
    "text": "sorry, that beta k plus\n1 is equal to beta k",
    "start": "1628360",
    "end": "1634220"
  },
  {
    "text": "plus this thing. OK? And if you look at this\nthing, it sort of reminds us of something.",
    "start": "1634220",
    "end": "1640700"
  },
  {
    "text": "Remember the least\nsquares estimator. So here, I'm going to\nactually deviate slightly from the slides.",
    "start": "1640700",
    "end": "1645820"
  },
  {
    "text": "And I will tell you how. The slides take\nbeta k and put it in here, which is one way to go.",
    "start": "1645820",
    "end": "1653220"
  },
  {
    "text": "And just think of this as a\nbig least square solution. Or you can keep the beta k,\nsolve another least squares,",
    "start": "1653220",
    "end": "1661040"
  },
  {
    "text": "and then add it to the\nbeta k that you have. It's the same thing. So I will take the\ndifferent routes. So you have the two\noptions, all right?",
    "start": "1661040",
    "end": "1667445"
  },
  {
    "start": "1667445",
    "end": "1687409"
  },
  {
    "text": "OK. So when we did the\nleast squares-- so parenthesis least squares--",
    "start": "1687410",
    "end": "1695880"
  },
  {
    "text": " we had y equals x\nbeta plus epsilon.",
    "start": "1695880",
    "end": "1703809"
  },
  {
    "text": "And our estimator beta\nhat was x transpose x inverse x transpose y, right?",
    "start": "1703810",
    "end": "1713382"
  },
  {
    "text": "And that was just solving\nthe first order condition, and that's what we found. Now look at this--",
    "start": "1713382",
    "end": "1720679"
  },
  {
    "text": "x transpose bleep x inverse,\nx transpose bleep something.",
    "start": "1720680",
    "end": "1726770"
  },
  {
    "text": "OK? So this looks like, if this\nis the same as the left board,",
    "start": "1726770",
    "end": "1738120"
  },
  {
    "text": "if wk is equal to the\nidentity matrix, meaning we",
    "start": "1738120",
    "end": "1744140"
  },
  {
    "text": "don't see it, and y is equal\nto y tilde k minus mu tilde k--",
    "start": "1744140",
    "end": "1751040"
  },
  {
    "text": " so those similarities, the\nfact that we just squeeze in--",
    "start": "1751040",
    "end": "1756950"
  },
  {
    "text": "so the fact that the response\nvariable is different is really not a problem. We just have to\npretend that this",
    "start": "1756950",
    "end": "1762559"
  },
  {
    "text": "is equal to y tilde\nminus mu tilde. I mean, that's just\nthe least squares. When you call a software that\ndoes least squares for you,",
    "start": "1762560",
    "end": "1769440"
  },
  {
    "text": "you just tell it what y\nis, you tell it with x is, and it makes the computation. So you would just lie to\nit and say all the actual y",
    "start": "1769440",
    "end": "1775470"
  },
  {
    "text": "I want is this thing. And then we need to somehow\nincorporate those weights.",
    "start": "1775470",
    "end": "1782420"
  },
  {
    "text": "And so the question\nis, is that easy to do? And the answer is yes,\nbecause this is a setup where",
    "start": "1782420",
    "end": "1788390"
  },
  {
    "text": "this would actually arise. So one of the things that's\nvery specific to what we did here and with\nleast squares, we assume",
    "start": "1788390",
    "end": "1794750"
  },
  {
    "text": "that epsilon, when we did\nat least the inference, we assumed that\nepsilon was normal 0",
    "start": "1794750",
    "end": "1801080"
  },
  {
    "text": "and the covariance matrix\nwas the identity, right? What if the covariance\nmatrix is not the identity?",
    "start": "1801080",
    "end": "1807179"
  },
  {
    "text": "If the covariance matrix\nis not the identity, then your maximum\nlikelihood is not exactly these least squares.",
    "start": "1807180",
    "end": "1813600"
  },
  {
    "text": "If the covariance\nmatrix is any matrix you have another solution,\nwhich involves the inverse of the covariance\nmatrix that you have,",
    "start": "1813600",
    "end": "1820620"
  },
  {
    "text": "but if your covariance matrix,\nin particular, is diagonal-- which would mean that\neach observation that you",
    "start": "1820620",
    "end": "1826559"
  },
  {
    "text": "get in this system of\nequations is still independent, but the variances can\nchange from one line",
    "start": "1826560",
    "end": "1832530"
  },
  {
    "text": "to another, from one\nobservation to another-- then it's called\nheteroscedastic.",
    "start": "1832530",
    "end": "1837570"
  },
  {
    "text": "\"Hetero\" means \"not the same.\" \"Scedastic\" is \"scale.\" And a heteroscedastic\ncase, you would have",
    "start": "1837570",
    "end": "1845280"
  },
  {
    "text": "something slightly different. And it makes sense\nthat, if you know that some observations have\nmuch less variance than others,",
    "start": "1845280",
    "end": "1852970"
  },
  {
    "text": "you might want to\ngive them more weight. OK? So if you think about\nyour usual drawing,",
    "start": "1852970",
    "end": "1862940"
  },
  {
    "text": "and maybe you have\nsomething like this, but the actual line is really--",
    "start": "1862940",
    "end": "1868600"
  },
  {
    "text": "OK, let's say you have this guy\nas well, so just a few here. If you start drawing this\nthing, if you do least squares,",
    "start": "1868600",
    "end": "1876474"
  },
  {
    "text": "you're going to see\nsomething that looks like this on those points. But now, if I tell you\nthat, on this side,",
    "start": "1876474",
    "end": "1882640"
  },
  {
    "text": "the variance is equal to 100,\nmeaning that those points are actually really far\nfrom the true one,",
    "start": "1882640",
    "end": "1889030"
  },
  {
    "text": "and here on this side, the\nvariance is equal to 1, meaning that those points are\nactually close to the line you're looking for, then the\nline you should be fitting",
    "start": "1889030",
    "end": "1896151"
  },
  {
    "text": "is probably this\nguy, meaning do not trust the guys that\nhave a lot of variance.",
    "start": "1896151",
    "end": "1902210"
  },
  {
    "text": "And so you need somehow\nto incorporate that. If you know that those things\nhave much more variance than these guys, you\nwant to weight this.",
    "start": "1902210",
    "end": "1909370"
  },
  {
    "text": "And the way you do it is by\nusing weighted least squares. OK. So we're going to\nopen in parentheses",
    "start": "1909370",
    "end": "1914661"
  },
  {
    "text": "on weighted least squares. It's not a fundamental\nstatistical question, but it's useful for us,\nbecause this is exactly",
    "start": "1914661",
    "end": "1920470"
  },
  {
    "text": "what's going to spit out-- something that looks like this\nwith this matrix w in there. OK.",
    "start": "1920470",
    "end": "1925660"
  },
  {
    "text": "So let's go back in\ntime for a second. Assume we're still covering\nleast squares regression.",
    "start": "1925660",
    "end": "1932840"
  },
  {
    "text": "So now, I'm going to assume\nthat y is x beta plus epsilon,",
    "start": "1932840",
    "end": "1939220"
  },
  {
    "text": "but this time, epsilon is a\nmultivariate Gaussian in, say, p dimensions with mean 0.",
    "start": "1939220",
    "end": "1945940"
  },
  {
    "text": "And covariance matrix, I\nwill write it as w inverse, because w is going to be the\none that's going to show up.",
    "start": "1945940",
    "end": "1952789"
  },
  {
    "text": "OK? So this is the so-called\nheteroscedastic. That's how it's spelled,\nand yet another name",
    "start": "1952790",
    "end": "1963559"
  },
  {
    "text": "that you can pick for your\nsoccer team or a capella group. All right. So the maximum\nlikelihood, in this case--",
    "start": "1963560",
    "end": "1972289"
  },
  {
    "text": "so actually, let's compute\nthe maximum likelihood for this problem, right? So the log likelihood is what?",
    "start": "1972289",
    "end": "1978770"
  },
  {
    "text": "Well, we're going to have\nthe term that tells us that it's going to be-- so OK.",
    "start": "1978770",
    "end": "1984120"
  },
  {
    "text": "What is the density of\na multivariate Gaussian? ",
    "start": "1984120",
    "end": "1990339"
  },
  {
    "text": "So it's going to be a\nmultivariate Gaussian in p dimension with mean x\nbeta and covariance matrix w",
    "start": "1990339",
    "end": "1997270"
  },
  {
    "text": "inverse, right? So that's the\ndensity that we want. Well, it's of the form 1 over\ndeterminant of w inverse times",
    "start": "1997270",
    "end": "2010490"
  },
  {
    "text": "2 pi to the p/2.",
    "start": "2010490",
    "end": "2015734"
  },
  {
    "text": "OK? And times exponential, and now,\nwhat I have is x minus x beta",
    "start": "2015734",
    "end": "2027570"
  },
  {
    "text": "transpose w-- so that's\nthe inverse of w inverse-- x minus x beta divided by 2.",
    "start": "2027570",
    "end": "2038340"
  },
  {
    "text": "OK? So this is x minus mu\ntranspose sigma inverse x minus mu divided by 2.",
    "start": "2038340",
    "end": "2044919"
  },
  {
    "text": "And if you want a sanity\ncheck, just assume that sigma--",
    "start": "2044920",
    "end": "2050766"
  },
  {
    "text": "yeah? AUDIENCE: Is it x\nminus x beta or y? PHILIPPE RIGOLLET: Well, you\nknow, if you want this to be y,",
    "start": "2050766",
    "end": "2058290"
  },
  {
    "text": "then this is y, right? Sure. Yeah, maybe it's less confusing.",
    "start": "2058290",
    "end": "2064960"
  },
  {
    "text": "So if you should do p is equal\nto 1, then what does it mean? It means that you\nhave this mean here.",
    "start": "2064960",
    "end": "2071469"
  },
  {
    "text": "So let's forget\nabout what it is. But this guy is going to be\njust 1 sigma squared, right? So what you see here is the\ninverse of sigma squared.",
    "start": "2071469",
    "end": "2078699"
  },
  {
    "text": "So that's going to be 2 over 2\nsigma squared, like we usually see it. The determinant of\nw inverse is just",
    "start": "2078699",
    "end": "2084309"
  },
  {
    "text": "the product of\nthe entry of the 1 by 1 matrix, which\nis just sigma square.",
    "start": "2084310",
    "end": "2093341"
  },
  {
    "text": "OK? So that should be actually--",
    "start": "2093341",
    "end": "2098390"
  },
  {
    "text": "yeah, no, that's actually--\nyeah, that's sigma square. And then I have this 2 pi. So square root of this,\nbecause p is equal to 1,",
    "start": "2098390",
    "end": "2104670"
  },
  {
    "text": "I get sigma square\nroot 2 pi, which is the normalization that I get. This is not going\nto matter, because, when I look at\nthe log likelihood",
    "start": "2104670",
    "end": "2112640"
  },
  {
    "text": "as a function of beta-- so I'm assuming\nthat w is known--",
    "start": "2112640",
    "end": "2117720"
  },
  {
    "text": "what I get is something\nwhich is a constant. So it's minus p minus\nn times p/2 times",
    "start": "2117720",
    "end": "2125520"
  },
  {
    "text": "log that w inverse times 2 pi.",
    "start": "2125520",
    "end": "2131290"
  },
  {
    "text": "OK? So this is just going\nto be a constant. It won't matter when I do\nthe maximum likelihood. And then I'm going to have what?",
    "start": "2131290",
    "end": "2136723"
  },
  {
    "text": "I'm going to have plus 1/2\nof y minus x beta transpose w",
    "start": "2136723",
    "end": "2144507"
  },
  {
    "text": "y minus x beta.  So if I want to take the\nmaximum of this guy--",
    "start": "2144508",
    "end": "2153520"
  },
  {
    "text": "sorry, there's a minus here. So if I want to take\nthe maximum of this guy,",
    "start": "2153520",
    "end": "2158590"
  },
  {
    "text": "I'm going to have to take\nthe minimum of this thing. And the minimum of this thing,\nif you take the derivative,",
    "start": "2158590",
    "end": "2164530"
  },
  {
    "text": "you get to see-- so that's what we have, right? We need to compute\nthe minimum of y minus x beta transpose\nw minus y minus x beta.",
    "start": "2164530",
    "end": "2173980"
  },
  {
    "text": "And the solution that you get-- I mean, you can actually\ncheck this for yourself.",
    "start": "2173980",
    "end": "2180320"
  },
  {
    "text": "The way you can see this\nis by doing the following. If you're lazy and you don't\nwant to redo the entire thing--",
    "start": "2180320",
    "end": "2187702"
  },
  {
    "text": "maybe I should keep that guy. ",
    "start": "2187702",
    "end": "2196109"
  },
  {
    "text": "W is diagonal, right? I'm going to assume that\nso w inverse is diagonal,",
    "start": "2196110",
    "end": "2202540"
  },
  {
    "text": "and I'm going to assume that\nno variance is equal to 0 and no variance is\nequal to infinity, so that both w inverse and\nw have only positive entries",
    "start": "2202540",
    "end": "2212050"
  },
  {
    "text": "on the diagonal. All right? So in particular, I can\ntalk about the square root of w, which is just the\nmatrix, the diagonal matrix,",
    "start": "2212050",
    "end": "2218520"
  },
  {
    "text": "with the square roots\non the diagonal. OK? And so I want to minimize in\nbeta y minus x beta transpose w",
    "start": "2218520",
    "end": "2228960"
  },
  {
    "text": "y minus x beta. So I'm going to write\nw as square root of w times square root of\nw, which I can, because w--",
    "start": "2228960",
    "end": "2237584"
  },
  {
    "text": "and it's just the\nsimplest thing, right? If w is w1 wn, so that's my\nw, then the square root of w",
    "start": "2237584",
    "end": "2248030"
  },
  {
    "text": "is just square root of\nw1 square root of wn, and then 0 is elsewhere.",
    "start": "2248030",
    "end": "2253960"
  },
  {
    "text": "OK? So the product of\nthose two matrices gives me definitely\nback what I want, and that's the usual\nmatrix product.",
    "start": "2253960",
    "end": "2261387"
  },
  {
    "text": "Now, what I'm going to do is I'm\ngoing to push one on one side and push the other\none on the other side. So that gives me\nthat this is really",
    "start": "2261387",
    "end": "2267180"
  },
  {
    "text": "the minimum over beta of-- well, here I have\nthis transposed, so I have to put it\non the other side. w is clearly symmetric and\nso is square root of w.",
    "start": "2267180",
    "end": "2275970"
  },
  {
    "text": "So the transpose doesn't matter. And so what I'm left\nwith is square root of wy minus square root of wx\nbeta transpose, and then times",
    "start": "2275970",
    "end": "2286290"
  },
  {
    "text": "itself. So that's square root\nwy minus square root w--",
    "start": "2286290",
    "end": "2295010"
  },
  {
    "text": "oh, I don't have enough space-- x beta.",
    "start": "2295010",
    "end": "2300130"
  },
  {
    "text": "OK, and that stops here. But this is the same thing\nthat we've been doing before.",
    "start": "2300130",
    "end": "2305680"
  },
  {
    "text": "This is a new y. Let's call it y prime. This is a new x. Let's call it x prime.",
    "start": "2305680",
    "end": "2311250"
  },
  {
    "text": "And now, this is just the\nleast squares estimator associated to a response y prime\nand a design matrix x prime.",
    "start": "2311250",
    "end": "2319000"
  },
  {
    "text": "So I know that the solution is x\nprime transpose x prime inverse",
    "start": "2319000",
    "end": "2327460"
  },
  {
    "text": "x prime transpose y prime.",
    "start": "2327460",
    "end": "2333020"
  },
  {
    "text": "And now, I'm just going\nto substitute again what my x prime is in\nterms of x and what",
    "start": "2333020",
    "end": "2338559"
  },
  {
    "text": "my y prime is in terms of y. And that gives me exactly x\nsquare root w square root w",
    "start": "2338560",
    "end": "2346630"
  },
  {
    "text": "x inverse. And then I have x transpose\nsquare root w for this guy.",
    "start": "2346630",
    "end": "2357490"
  },
  {
    "text": "And then I have square\nroot wy for that guy. And that's exactly\nwhat I wanted.",
    "start": "2357490",
    "end": "2363400"
  },
  {
    "text": "I'm left with x transpose\nwx inverse x transpose wy.",
    "start": "2363400",
    "end": "2370880"
  },
  {
    "text": " OK? ",
    "start": "2370880",
    "end": "2378020"
  },
  {
    "text": "So that's a simple way\nto take into account the w that we had before.",
    "start": "2378020",
    "end": "2384150"
  },
  {
    "text": "And you could actually do it\nwith any matrix that's positive semi-definite, because\nyou can actually talk about the square\nroot of those matrices.",
    "start": "2384150",
    "end": "2392203"
  },
  {
    "text": "And it's just the square root\nof a matrix is just a matrix such that, when you\nmultiply it by itself,",
    "start": "2392204",
    "end": "2398260"
  },
  {
    "text": "it gives you the\noriginal matrix. OK?",
    "start": "2398260",
    "end": "2403560"
  },
  {
    "text": "So here, that was\njust a shortcut that consisted in\nsaying, OK, maybe I don't want to recompute the\ngradient of this quantity,",
    "start": "2403560",
    "end": "2412910"
  },
  {
    "text": "set it equal to 0, and see\nwhat beta hat had should be. Instead, I am going to\nassume that I already",
    "start": "2412910",
    "end": "2419810"
  },
  {
    "text": "know that, if I\ndid not have the w, I would know how to solve it. And that's exactly what I did.",
    "start": "2419810",
    "end": "2425810"
  },
  {
    "text": "I said, well, I\nknow that this is the minimum of something\nthat looks like this, when I have the primes.",
    "start": "2425810",
    "end": "2432020"
  },
  {
    "text": "And then I just substitute\nback my w in there. All right. So that' just the\nlazy computation.",
    "start": "2432020",
    "end": "2438390"
  },
  {
    "text": "But again, if you\ndon't like it, you can always take the\ngradient of this guy. Yes? AUDIENCE: Why is\nthe solution written",
    "start": "2438390",
    "end": "2444611"
  },
  {
    "text": "in the slides different? PHILIPPE RIGOLLET:\nBecause there's a mistake. ",
    "start": "2444612",
    "end": "2449647"
  },
  {
    "text": "Yeah, there's a\nmistake on the slides. ",
    "start": "2449647",
    "end": "2458385"
  },
  {
    "text": "How did I make that one? I'm actually trying\nto parse it back. ",
    "start": "2458385",
    "end": "2471570"
  },
  {
    "text": "I mean, it's clearly\nwrong, right? Oh, no, it's not. ",
    "start": "2471570",
    "end": "2484590"
  },
  {
    "text": "No, it is. So it's not clearly wrong. ",
    "start": "2484590",
    "end": "2492680"
  },
  {
    "text": "Actually, it is clearly wrong. Because if I put\nthe identity here,",
    "start": "2492680",
    "end": "2497839"
  },
  {
    "text": "those are still\nassociative, right? So this product is\nactually not compatible. So it's wrong, but there's\njust this extra thing",
    "start": "2497840",
    "end": "2504140"
  },
  {
    "text": "that I probably copy-pasted\nfrom some place. Since this is one\nof my latest slide, I'll just color it in white.",
    "start": "2504140",
    "end": "2511280"
  },
  {
    "text": "But yeah, sorry, there's a mis--\nthis parenthesis is not here. Thank you. AUDIENCE: [INAUDIBLE].",
    "start": "2511280",
    "end": "2516388"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah.  OK?",
    "start": "2516388",
    "end": "2523172"
  },
  {
    "text": "AUDIENCE: So why not\nsquare root [INAUDIBLE]?? PHILIPPE RIGOLLET: Because\nI have two of them. I have one that comes from the\nx prime that's here, this guy.",
    "start": "2523172",
    "end": "2531180"
  },
  {
    "text": "And then I have one that\ncomes from this guy here. OK, so the solution--",
    "start": "2531180",
    "end": "2537530"
  },
  {
    "text": "let's write it in some place\nthat's actually legible-- ",
    "start": "2537530",
    "end": "2545530"
  },
  {
    "text": "which is the correction\nfor this thing is x transpose wx\ninverse x transpose wy.",
    "start": "2545530",
    "end": "2554930"
  },
  {
    "text": "OK? So you just squeeze\nin this w in there. And that's exactly\nwhat we had before,",
    "start": "2554930",
    "end": "2561860"
  },
  {
    "text": "x transpose wx inverse\nx transpose w some y.",
    "start": "2561860",
    "end": "2567740"
  },
  {
    "text": "OK? And what I claim is that this\nis routinely implemented.",
    "start": "2567740",
    "end": "2573050"
  },
  {
    "text": "As you can imagine,\nheteroscedastic linear regression is something\nthat's very common. So every time you a\nleast squares formula,",
    "start": "2573050",
    "end": "2580100"
  },
  {
    "text": "you also have a way to\nput in some weights. You don't have to\nput diagonal weights, but here, that's all we need.",
    "start": "2580100",
    "end": "2585718"
  },
  {
    "text": " So here on the slides,\nagain, I took the beta k,",
    "start": "2585718",
    "end": "2592310"
  },
  {
    "text": "and I put it in there, so that\nI have only one least square solution to formulate.",
    "start": "2592310",
    "end": "2597370"
  },
  {
    "text": "But let's do it\nslightly differently. What I'm going to\ndo here now is I'm going to say, OK, let's feed\nit to some least squares.",
    "start": "2597370",
    "end": "2604430"
  },
  {
    "text": "So let's do weighted least\nsquares on a response,",
    "start": "2604430",
    "end": "2612599"
  },
  {
    "text": "y being y tilde k minus mu tilde\nk, and design matrix being,",
    "start": "2612600",
    "end": "2624810"
  },
  {
    "text": "well, just the x itself. So that doesn't change.",
    "start": "2624810",
    "end": "2630240"
  },
  {
    "text": "And the weights-- so\nthe weights are what?",
    "start": "2630240",
    "end": "2640090"
  },
  {
    "text": "The weights are the\nwk that I had here. So wki is h prime\nof xi transpose beta",
    "start": "2640090",
    "end": "2655630"
  },
  {
    "text": "k divided by g prime of\nmu i at time k times phi.",
    "start": "2655630",
    "end": "2664380"
  },
  {
    "text": " OK, and so this, if I solve\nit, will spit out something",
    "start": "2664380",
    "end": "2672619"
  },
  {
    "text": "that I will call a solution. I will call it u hat k plus 1.",
    "start": "2672620",
    "end": "2681289"
  },
  {
    "text": "And to get beta\nhat k plus 1, all I need to do is to do beta\nk plus u hat k plus 1--",
    "start": "2681290",
    "end": "2693215"
  },
  {
    "text": "sorry, beta-- yeah. OK?",
    "start": "2693215",
    "end": "2698730"
  },
  {
    "text": "And that's because-- so\nhere, that's not clear. But I started from\nthere, remember?",
    "start": "2698730",
    "end": "2704079"
  },
  {
    "text": "I started from this guy here. So I'm just solving a least\nsquares, a weighted least",
    "start": "2704080",
    "end": "2710775"
  },
  {
    "text": "square that's going\nto give me this thing. That's what I called\nu hat k plus 1. And then I add it to beta k, and\nthat gives me beta k minus 1.",
    "start": "2710775",
    "end": "2718475"
  },
  {
    "text": "So I just have this\nintermediate step, which is removed in the slides.",
    "start": "2718475",
    "end": "2725238"
  },
  {
    "text": "OK? So then you can repeat\nuntil convergence. What does it mean to\nrepeat until convergence?",
    "start": "2725238",
    "end": "2732270"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]?",
    "start": "2732270",
    "end": "2737434"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYeah, exactly. So you just set some\nthreshold and you say, I promise you that this\nwill converge, right?",
    "start": "2737435",
    "end": "2743550"
  },
  {
    "text": "So you know that at some point,\nyou're going to be there. You're going to go\nthere, but you're never going to be exactly there.",
    "start": "2743550",
    "end": "2749403"
  },
  {
    "text": "And so you just say, OK, I\nwant this accuracy on my data. Actually, the machine\nis a little strong.",
    "start": "2749403",
    "end": "2755712"
  },
  {
    "text": "Especially if you have 10\nobservations to start with, you know you're going to\nhave something that's going",
    "start": "2755712",
    "end": "2761788"
  },
  {
    "text": "to have some statistical error. So that should actually guide\nyou into what kind of error you want to be making.",
    "start": "2761789",
    "end": "2766930"
  },
  {
    "text": "So for example, a\ngood rule of thumb is that if you have\nn observations,",
    "start": "2766930",
    "end": "2771960"
  },
  {
    "text": "you just take some within-- if you want the L2\ndistance between the beta--",
    "start": "2771960",
    "end": "2777059"
  },
  {
    "text": "the two consecutive beta\nto be less than 1/n, you should be good enough. It doesn't have to be\nthat machine precision.",
    "start": "2777060",
    "end": "2784560"
  },
  {
    "text": "And so it's clear how\nwe do this, right? So here, I just have to maintain\na bunch of things, right?",
    "start": "2784560",
    "end": "2790680"
  },
  {
    "text": "So remember, when I want to\nrecompute-- at every step, I have to recompute\na bunch of things. So I have to\nrecompute the weights.",
    "start": "2790680",
    "end": "2796890"
  },
  {
    "text": "But if I want to recompute\nthe weights, not only do I need to previous\niterate, but I need to know how the previous\niterate impacts my means.",
    "start": "2796890",
    "end": "2806040"
  },
  {
    "text": "So at each step, I have\nto recalculate mu i k by doing g prime, rate?",
    "start": "2806040",
    "end": "2813090"
  },
  {
    "text": "Remember mu i k was just g\ninverse of xi transpose beta k,",
    "start": "2813090",
    "end": "2822670"
  },
  {
    "text": "right? So I have to recompute that. And then I use this\nto compute my weights.",
    "start": "2822670",
    "end": "2829339"
  },
  {
    "text": "I also use this to\ncompute my y, right?",
    "start": "2829340",
    "end": "2835790"
  },
  {
    "text": "so my y depends also\non g prime of mu i k. I feed that to my weighted\nleast squares engine.",
    "start": "2835790",
    "end": "2844950"
  },
  {
    "text": "It spits out the u hat k, that\nI add to my previous beta k. And that gives me my\nnew beta k plus 1.",
    "start": "2844950",
    "end": "2850605"
  },
  {
    "text": " OK. So here's the\npseudocode, if you want",
    "start": "2850605",
    "end": "2855980"
  },
  {
    "text": "to take some time to parse it. All right.",
    "start": "2855980",
    "end": "2861280"
  },
  {
    "text": "So here again, the\ntrick is not much. It's just saying, if you don't\nfeel like implementing Fisher",
    "start": "2861280",
    "end": "2869400"
  },
  {
    "text": "scoring or inverting your\nHessian at every step, then a weighted least\nsquares is actually going",
    "start": "2869400",
    "end": "2874620"
  },
  {
    "text": "to do it for you automatically. All right. Then that's just\na numerical trick. There's nothing really\nstatistical about this,",
    "start": "2874620",
    "end": "2880950"
  },
  {
    "text": "except the fact that this\ncalls for a solution for each of the step reminded us\nof sum of the squares,",
    "start": "2880950",
    "end": "2889682"
  },
  {
    "text": "except that there was\nsome extra weights.  OK. So to conclude, we'll\nneed to know, of course,",
    "start": "2889682",
    "end": "2898670"
  },
  {
    "text": "xy, the link function.  Why do we need the\nvariance function?",
    "start": "2898670",
    "end": "2904170"
  },
  {
    "start": "2904170",
    "end": "2909530"
  },
  {
    "text": "I'm not sure we actually\nneed the variance function. No, I don't know why I say that.",
    "start": "2909530",
    "end": "2916220"
  },
  {
    "text": "You need phi, not the\nvariance function. So where do you start\nactually, right?",
    "start": "2916220",
    "end": "2921370"
  },
  {
    "text": "So clearly, if you start\nvery close to your solution, you're actually going\nto do much better.",
    "start": "2921370",
    "end": "2926809"
  },
  {
    "text": "And one good way to start-- so for the beta itself, it's\nnot clear what it's going to be. But you can actually\nget a good idea",
    "start": "2926810",
    "end": "2933490"
  },
  {
    "text": "of what beta is by just having\na good idea of what mu is. Because mu is g inverse\nof xi transpose beta.",
    "start": "2933490",
    "end": "2941830"
  },
  {
    "text": "And so what you\ncould do is to try to set mu to be the actual\nobservations that you have,",
    "start": "2941830",
    "end": "2947560"
  },
  {
    "text": "because that's the\nbest guess that you have for their expected value. And then you just say,\nOK, once I have my mu,",
    "start": "2947560",
    "end": "2954740"
  },
  {
    "text": "I know that my mu is a\nfunction of this thing. So I can write g of mu and solve\nit, using your least squares",
    "start": "2954740",
    "end": "2961380"
  },
  {
    "text": "estimator, right? So g of mu is of\nthe form x beta.",
    "start": "2961380",
    "end": "2968970"
  },
  {
    "text": "So you just solve for--\nonce you have your mu, you pass it through g, and\nthen you solve for the beta",
    "start": "2968970",
    "end": "2976349"
  },
  {
    "text": "that you want. And then that's the beta\nthat you initialize with. ",
    "start": "2976350",
    "end": "2982954"
  },
  {
    "text": "OK? And actually, this was your\nquestion from last time. As soon as I use\nthe canonical link,",
    "start": "2982954",
    "end": "2990320"
  },
  {
    "text": "Fisher scoring\nand Newton-Raphson are the same thing, because\nthe Hessian is actually",
    "start": "2990320",
    "end": "2997720"
  },
  {
    "text": "deterministic in that\ncase, just because when",
    "start": "2997720",
    "end": "3005869"
  },
  {
    "text": "you use the canonical link,\nH is the identity, which means that its second\nderivative is equal to 0.",
    "start": "3005870",
    "end": "3012050"
  },
  {
    "text": "So this term goes away even\nwithout taking the expectation. So remember, the\nterm that went away",
    "start": "3012050",
    "end": "3017839"
  },
  {
    "text": "was of the form yi\nminus mu i divided",
    "start": "3017840",
    "end": "3023420"
  },
  {
    "text": "by phi times h prime prime\nof xi transpose beta, right?",
    "start": "3023420",
    "end": "3029609"
  },
  {
    "text": "That's the term that we said,\noh, the conditional expectation of this guy is 0. But if h prime prime\nis already equal to 0,",
    "start": "3029609",
    "end": "3036384"
  },
  {
    "text": "then there's nothing\nthat changes. There's nothing that goes away. It was already equal to 0. And that always happens when\nyou have the canonical link,",
    "start": "3036384",
    "end": "3043710"
  },
  {
    "text": "because h is g b prime inverse.",
    "start": "3043710",
    "end": "3054450"
  },
  {
    "text": "And the canonical link\nis b prime inverse, so this thing is the identity.",
    "start": "3054450",
    "end": "3060176"
  },
  {
    "text": "So the second derivative of\nf of x is equal to x is 0.",
    "start": "3060176",
    "end": "3066780"
  },
  {
    "text": "OK. My screen says end of show. So we can start\nwith some questions.",
    "start": "3066780",
    "end": "3073862"
  },
  {
    "text": "AUDIENCE: I just\nwanted to clarify. So iterative-- what is\nit say for iterative--",
    "start": "3073862",
    "end": "3079127"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nReweighted least squares. AUDIENCE: Reweighted\nleast squares is an implementation of the\nFisher scoring [INAUDIBLE]?? PHILIPPE RIGOLLET:\nThat's an implementation",
    "start": "3079127",
    "end": "3085631"
  },
  {
    "text": "that's just making calls to\nweighted least squares oracles. It's called an oracle sometimes.",
    "start": "3085631",
    "end": "3090730"
  },
  {
    "text": "An oracle is what you assume the\nmachine can do easily for you. So if you assume\nthat your machine is very good at multiplying\nby the inverse of a matrix,",
    "start": "3090730",
    "end": "3098150"
  },
  {
    "text": "you might as well just do\nFisher scoring yourself, right? It's just a way so that you\ndon't have to actually do it. And usually, those\nthings are implemented--",
    "start": "3098150",
    "end": "3106460"
  },
  {
    "text": "and I just said routinely--\nin statistical software. But they're implemented\nvery efficiently in statistical software.",
    "start": "3106460",
    "end": "3112440"
  },
  {
    "text": "So this is going to be one\nof the fastest ways you're going to have to\nsolve, to do this step,",
    "start": "3112440",
    "end": "3119165"
  },
  {
    "text": "especially for\nlarge-scale problems. AUDIENCE: So the thing\nthat computers can do well is the multiplier [INAUDIBLE].",
    "start": "3119165",
    "end": "3125105"
  },
  {
    "text": "What's the thing that\nthe computers can do fast and what's the thing\nthat [INAUDIBLE]?? PHILIPPE RIGOLLET:\nSo if you were",
    "start": "3125105",
    "end": "3130900"
  },
  {
    "text": "to do this in the\nsimplest possible way, your iterations for,\nsay, Fisher scoring",
    "start": "3130900",
    "end": "3138070"
  },
  {
    "text": "is just multiply by the inverse\nof the Fisher information, right? AUDIENCE: So finding\nthat inverse is slow?",
    "start": "3138070",
    "end": "3144160"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah,\nso it takes a bit of time. Whereas, since you know you're\ngoing to multiply directly",
    "start": "3144160",
    "end": "3150330"
  },
  {
    "text": "by something, if you just say-- those things are not\nas optimized as solving least squares.",
    "start": "3150330",
    "end": "3155580"
  },
  {
    "text": "Actually, the way\nit's typically done is by doing some least squares. So you might as well just do\nleast squares that you like.",
    "start": "3155580",
    "end": "3161190"
  },
  {
    "text": "And there's also less--  well, no, there's no--",
    "start": "3161190",
    "end": "3167770"
  },
  {
    "text": "well, there is less\nrecalculation, right? Here, your Fisher,\nyou would have to recompute the entire\nmatrix of Fisher information.",
    "start": "3167770",
    "end": "3174720"
  },
  {
    "text": "Whereas here, you don't have to. Right? You really just have to compute\nsome vectors and the vector",
    "start": "3174720",
    "end": "3179849"
  },
  {
    "text": "of weights, right? So the Fisher information\nmatrix has, say, n choose two entries that\nyou need to compute, right?",
    "start": "3179850",
    "end": "3185910"
  },
  {
    "text": "It's symmetric, so it's\norder n squared entries. But here, the only things you\nupdate, if you think about it,",
    "start": "3185910",
    "end": "3191460"
  },
  {
    "text": "are this weight matrix. So there is only the\ndiagonal elements that you need to update, and\nthese vectors in there also.",
    "start": "3191460",
    "end": "3199330"
  },
  {
    "text": "There's two inverses n squared. So that's much less thing\nto actually put in there. It does it for you somehow.",
    "start": "3199330",
    "end": "3205099"
  },
  {
    "text": " Any other question?",
    "start": "3205100",
    "end": "3210809"
  },
  {
    "text": " Yeah? AUDIENCE: So if I have\na data set [INAUDIBLE],,",
    "start": "3210810",
    "end": "3217950"
  },
  {
    "text": "then I can always try to model\nit with least squares, right? PHILIPPE RIGOLLET:\nYeah, you can. AUDIENCE: And so this is like\nsetting my weight equal to 1--",
    "start": "3217950",
    "end": "3224670"
  },
  {
    "text": "the identity,\nessentially, right? PHILIPPE RIGOLLET:\nWell, not exactly, because the g also shows\nup in this correction",
    "start": "3224670",
    "end": "3230640"
  },
  {
    "text": "that you have here, right? AUDIENCE: Yeah. PHILIPPE RIGOLLET: I mean, I\ndon't know what you mean by-- AUDIENCE: I'm just\ntrying to say,",
    "start": "3230640",
    "end": "3236725"
  },
  {
    "text": "are there ever situations where\nI'm trying to model a data set and I would want to pick my\nweights in a particular way?",
    "start": "3236725",
    "end": "3243910"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah. AUDIENCE: OK. PHILIPPE RIGOLLET: I mean-- AUDIENCE: [INAUDIBLE]\nexample [INAUDIBLE].. PHILIPPE RIGOLLET:\nWell, OK, there's",
    "start": "3243910",
    "end": "3249420"
  },
  {
    "text": "the heteroscedastic\ncase for sure. So if you're going to actually\ncompute those things-- and more generally, I don't\nthink you should think",
    "start": "3249420",
    "end": "3255340"
  },
  {
    "text": "of those as being weights. You should really think\nof those as being matrices that you invert. And don't think of\nit as being diagonal,",
    "start": "3255340",
    "end": "3261370"
  },
  {
    "text": "but really think of them\nas being full matrices. So if you have-- when we wrote weighted least\nsquares here, this was really--",
    "start": "3261370",
    "end": "3270279"
  },
  {
    "text": "the w, I said, is diagonal. But all the computations really\nnever really use the fact that it's diagonal. So what shows up here\nis just the inverse",
    "start": "3270280",
    "end": "3278500"
  },
  {
    "text": "of your covariance matrix. And so if you have\ndata that's correlated, this is where it's\ngoing to show up.",
    "start": "3278500",
    "end": "3285330"
  }
]