[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5620"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5620",
    "end": "12280"
  },
  {
    "text": "To make a donation or\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "12280",
    "end": "18869"
  },
  {
    "text": "at ocw.mit.edu.  HAIM SOMPOLINSKY:\nMy topic today is",
    "start": "18870",
    "end": "24210"
  },
  {
    "text": "discussing sensory\nrepresentations in deep cortex-like\narchitectures.",
    "start": "24210",
    "end": "29950"
  },
  {
    "text": "I should say the\ntopic is perhaps toward a theory of\nsensory representations",
    "start": "29950",
    "end": "38300"
  },
  {
    "text": "in deep networks. As you will see, our\nattempt is to develop",
    "start": "38300",
    "end": "45540"
  },
  {
    "text": "a systematic theoretical\nunderstanding of the capacity and limitations of\narchitectures of that type.",
    "start": "45540",
    "end": "54420"
  },
  {
    "text": "The general context\nis well known. In many sensory systems,\nwe see information",
    "start": "54420",
    "end": "62160"
  },
  {
    "text": "is propagating from the\nperiphery, like the retina,",
    "start": "62160",
    "end": "67770"
  },
  {
    "text": "to primary visual cortex, and\nthen, of course, many stages up to a very high level, or maybe\nthe hippocampal structure.",
    "start": "67770",
    "end": "77250"
  },
  {
    "text": "It's not purely feedforward. There are backward, massive\nbackward or top-down",
    "start": "77250",
    "end": "83130"
  },
  {
    "text": "connections, the\nrecurrent connections, and some of those extra\nfeatures I'll talk about.",
    "start": "83130",
    "end": "88409"
  },
  {
    "text": "But the most intuitive\nfeature of that is simply a transformation\nor filtering",
    "start": "88410",
    "end": "96000"
  },
  {
    "text": "of data across multiple stages. Similarly in auditory pathway.",
    "start": "96000",
    "end": "102540"
  },
  {
    "text": "In other systems, we see a\nsimilar structure or aspect of similar structure as well.",
    "start": "102540",
    "end": "109042"
  },
  {
    "text": "A well-known and a\nclassical formative system for computational\nscience is cerebellum,",
    "start": "109042",
    "end": "116549"
  },
  {
    "text": "where you have information\ncoming from the mossy fiber layer, then expand enormously\ninto a granule layer,",
    "start": "116550",
    "end": "124920"
  },
  {
    "text": "and then converge\nto a Purkinje cell. So if you look at\na single Purkinje",
    "start": "124920",
    "end": "130888"
  },
  {
    "text": "cell, the output of the\ncerebellum, as a unit, then you see there is,\nfirst, an expansion",
    "start": "130889",
    "end": "138030"
  },
  {
    "text": "from 1,000 to two\norders of magnitude higher in the granule layer.",
    "start": "138030",
    "end": "143760"
  },
  {
    "text": "And then convergence of\n200,000 or so parallel fibers",
    "start": "143760",
    "end": "149330"
  },
  {
    "text": "onto a single Purkinje cell. And there are, those type\nof modules, many, many",
    "start": "149330",
    "end": "154350"
  },
  {
    "text": "across the cerebellum. So, again, a transformation\nwhich involves, in this case,",
    "start": "154350",
    "end": "160170"
  },
  {
    "text": "expansion and then convergence. In the basal ganglia, which\nI wouldn't categorize it",
    "start": "160170",
    "end": "167490"
  },
  {
    "text": "as a sensory system. More related to motor. Nevertheless, you see\ncortex converging first",
    "start": "167490",
    "end": "174540"
  },
  {
    "text": "to various stages of\nthe basal ganglia, and then expanding\nagain to cortex.",
    "start": "174540",
    "end": "181680"
  },
  {
    "text": "Hippocampus has also multiple\npathways, but some of them include a convergence.",
    "start": "181680",
    "end": "187380"
  },
  {
    "text": "For instance, convergence\nto a CA3, and then expansion",
    "start": "187380",
    "end": "193810"
  },
  {
    "text": "again to cortex. But there are other\nmultiple pathways as well,",
    "start": "193810",
    "end": "199680"
  },
  {
    "text": "different stages of sensory\ninformation propagating, of course, across them.",
    "start": "199680",
    "end": "207599"
  },
  {
    "text": "And, finally, the\nartificial network story of deep neural networks,\nall of you may have heard.",
    "start": "207600",
    "end": "214816"
  },
  {
    "text": "Input layer, then\nsequence of stages. Purely feedforward.",
    "start": "214816",
    "end": "220440"
  },
  {
    "text": "And at least the\ncanonical leading network",
    "start": "220440",
    "end": "226800"
  },
  {
    "text": "is one that the output layer\nhas object recognition, object classification task.",
    "start": "226800",
    "end": "232990"
  },
  {
    "text": "And the whole network\nis studied by backprop, supervised learning for that.",
    "start": "232990",
    "end": "241100"
  },
  {
    "text": "What I'll talk about is more\nin the spirit of the idea",
    "start": "241100",
    "end": "247140"
  },
  {
    "text": "that the first stages\nare more general purpose than the specific classic\ntask in the output layer.",
    "start": "247140",
    "end": "256060"
  },
  {
    "text": "So there are many issues. Number of stages that are\nrequired, the size of them, why compression or expansion.",
    "start": "256060",
    "end": "263520"
  },
  {
    "text": "In many systems, you'll see that\nthe fraction of active neurons is small in the expanded layer.",
    "start": "263520",
    "end": "271330"
  },
  {
    "text": "That's what we call sparseness. So high sparseness\nmeans small number of neurons active at\nany given stimulus.",
    "start": "271330",
    "end": "278340"
  },
  {
    "text": "It's just the terminology\nis somewhat confusing. So high sparseness is small\nnumber of active neurons.",
    "start": "278340",
    "end": "287110"
  },
  {
    "text": "One important and crucial\nquestion is how to transform. What are the\nfilters, the weights",
    "start": "287110",
    "end": "293910"
  },
  {
    "text": "that are good for transforming\nsensory information from one layer to another?",
    "start": "293910",
    "end": "299560"
  },
  {
    "text": "And, in particular,\nwhether random weights is good enough, or maybe\neven optimal in some sense.",
    "start": "299560",
    "end": "310380"
  },
  {
    "text": "Or one needs more\nstructure, the more learned type of synaptic way.",
    "start": "310380",
    "end": "316080"
  },
  {
    "text": "This is a crucial question,\nperhaps not for machine learning but for computational\nneuroscience, because there",
    "start": "316080",
    "end": "322919"
  },
  {
    "text": "is some experimental\nevidence, for at least of some of the systems\nthat are studied,",
    "start": "322920",
    "end": "329560"
  },
  {
    "text": "that the mapping from\nthe compressed, original",
    "start": "329560",
    "end": "336180"
  },
  {
    "text": "representation to the\nsparse representation is actually done by\nrandomly connected weights.",
    "start": "336180",
    "end": "343440"
  },
  {
    "text": "So one example is\nolfactory cortex. The mapping of\nolfactory representation",
    "start": "343440",
    "end": "349710"
  },
  {
    "text": "from the olfactory bulb,\nso from glomerulus layer, to the piriform cortex\nseems to be random,",
    "start": "349710",
    "end": "357486"
  },
  {
    "text": "as far as one can say. Similarly, in the\ncerebellum, the example that I mentioned\nbefore, when one",
    "start": "357486",
    "end": "364199"
  },
  {
    "text": "looks at the mapping\nfrom the mossy fiber to the granule cell,\nagain enormous expansion",
    "start": "364200",
    "end": "370500"
  },
  {
    "text": "by a few orders of magnitude. Nevertheless, they\nseem to be random. Now, of course, one cannot\nsay exclusively that they are",
    "start": "370500",
    "end": "378370"
  },
  {
    "text": "random and there are no subtle\ncorrelations or structures. But, nevertheless, there\nis a strong motivation",
    "start": "378370",
    "end": "384780"
  },
  {
    "text": "to ask whether random\nprojections are good enough. And if not, what does\nit mean structured?",
    "start": "384780",
    "end": "393330"
  },
  {
    "text": "What kind of structure is\nappropriate for this task?",
    "start": "393330",
    "end": "398819"
  },
  {
    "text": "Question of top-down\nand feedback loops, recurrent connections,\nand so on. So that's all I hope to,\nat least briefly, mention",
    "start": "398820",
    "end": "408720"
  },
  {
    "text": "later on in my talk. So before I continue, most of,\nor a large part of the talk",
    "start": "408720",
    "end": "416639"
  },
  {
    "text": "will be based on published\nand unpublished work with Baktash Babadi,\nwho was, until recently,",
    "start": "416640",
    "end": "423050"
  },
  {
    "text": "a postdoctoral, a Swartz\nFellow at Harvard University. Went to practice medicine.",
    "start": "423050",
    "end": "430370"
  },
  {
    "text": "Elia Frankin, a master student\nat the Hebrew University. SueYeon, who all you\nknow here, at Harvard.",
    "start": "430370",
    "end": "438440"
  },
  {
    "text": "Uri Cohen, a PhD student\nat the Hebrew University. And Dan Lee from\nPenn University.",
    "start": "438440",
    "end": "445740"
  },
  {
    "text": "So here is our formalization\nof the problem. We have an input\nlayer, denoted 0.",
    "start": "445740",
    "end": "454350"
  },
  {
    "text": "Typically, it's a small,\nit's a compressed layer",
    "start": "454350",
    "end": "459750"
  },
  {
    "text": "with dense representation. So here, every input\nwill generate maybe half",
    "start": "459750",
    "end": "465240"
  },
  {
    "text": "of the population, on average, Then there is a\nfeedforward layer",
    "start": "465240",
    "end": "471599"
  },
  {
    "text": "of synaptic weights,\nwhich expand to a higher-dimension layer,\nwhich we call cortical layer.",
    "start": "471600",
    "end": "480530"
  },
  {
    "text": "It's expanded in terms\nof the number of neurons. So this will be S1.",
    "start": "480530",
    "end": "485759"
  },
  {
    "text": "It is sparse because the\nf, the fraction of neurons that are active for each given\ninput vector, will be small.",
    "start": "485760",
    "end": "495790"
  },
  {
    "text": "So it is expanded and sparse. That will be the first part\nof my talk, discussing this.",
    "start": "495790",
    "end": "502710"
  },
  {
    "text": "Then, later on, I'll talk\nabout staging, cascading",
    "start": "502710",
    "end": "508830"
  },
  {
    "text": "this transformation\nto several stages. And ultimately\nthere is a readout,",
    "start": "508830",
    "end": "515190"
  },
  {
    "text": "will be some\nclassification task. 1 will be one classification.",
    "start": "515190",
    "end": "520210"
  },
  {
    "text": "2 will be another\nclassification rule, et cetera, each one of them\nwith synaptic weights",
    "start": "520210",
    "end": "525460"
  },
  {
    "text": "which are learned to\nperform that task. So we call that a\nsupervised layer, and that's",
    "start": "525460",
    "end": "531960"
  },
  {
    "text": "the unsupervised layer. So that's a formalization\nof the problem. And, as you will see, we'll\nmake enormously simplifying",
    "start": "531960",
    "end": "541230"
  },
  {
    "text": "abstraction of the\nreal biological system in order to try to\ngain some insight",
    "start": "541230",
    "end": "548010"
  },
  {
    "text": "about the computational\ncapacity of such systems. So the first\nimportant question is",
    "start": "548010",
    "end": "554640"
  },
  {
    "text": "what is the statistics,\nthe statistical structure of the input? So the input is kind of\nn-dimensional vector,",
    "start": "554640",
    "end": "561570"
  },
  {
    "text": "where n, or n0, when n is\nthe number of units here. So each sensory event evokes\na pattern of activity here.",
    "start": "561570",
    "end": "570430"
  },
  {
    "text": "But what is the structure,\nthe statistical structure, that we are working with?",
    "start": "570430",
    "end": "576750"
  },
  {
    "text": "And the simplest one that\nwe are going to discuss is the following.",
    "start": "576750",
    "end": "582160"
  },
  {
    "text": "So we assume, basically,\nthat the inputs are coming from a\nmixture of, so to speak,",
    "start": "582160",
    "end": "587700"
  },
  {
    "text": "a mixture of\nGaussian statistics. It's not going to be Gaussian\nbecause, for simplicity, we'll",
    "start": "587700",
    "end": "594120"
  },
  {
    "text": "assume they are binary. But this doesn't\nmatter actually. So imagine that this is kind\nof a graphical represent--",
    "start": "594120",
    "end": "604850"
  },
  {
    "text": "a caricature of\nhigh-dimensional space. And imagine the inputs,\nthe sensory inputs,",
    "start": "604850",
    "end": "611690"
  },
  {
    "text": "imagine that they are clustered\naround templates or cluster centers.",
    "start": "611690",
    "end": "616875"
  },
  {
    "text": " So these will be the\ncenters of these balls.",
    "start": "616875",
    "end": "624589"
  },
  {
    "text": "And the input itself is\ncoming from the neighborhoods",
    "start": "624590",
    "end": "630080"
  },
  {
    "text": "of those templates. So each input will be\none point in this space,",
    "start": "630080",
    "end": "635870"
  },
  {
    "text": "and it will be originating from\none of those ensembles of one",
    "start": "635870",
    "end": "641089"
  },
  {
    "text": "of those states. So that's a simple architecture.",
    "start": "641090",
    "end": "646279"
  },
  {
    "text": "And we are going-- in real space, it\nwill be mapping",
    "start": "646280",
    "end": "652850"
  },
  {
    "text": "from one of those states\ninto another state in the next layer.",
    "start": "652850",
    "end": "659030"
  },
  {
    "text": "And then, finally, the\ntask will be to take-- imagine that some of those\nballs are classified as plus.",
    "start": "659030",
    "end": "667370"
  },
  {
    "text": "Let's say the olfactory factory\nstimuli, and some of them are classified as appetitive,\nsome of them as aversive.",
    "start": "667370",
    "end": "676630"
  },
  {
    "text": "So the output layer,\nthe readout unit, has to classify some of\nthose spheres as a plus,",
    "start": "676630",
    "end": "686360"
  },
  {
    "text": "and some of them are minus. And, of course,\ndepending on how many of them are in a dimensionality,\nand their location,",
    "start": "686360",
    "end": "693850"
  },
  {
    "text": "this may or may not\nbe an easy problem. So, for instance,\nhere it's fine.",
    "start": "693850",
    "end": "701125"
  },
  {
    "text": "There is-- a linear classifier\non the input space can do it. Here, I think there are, there\nshould be some mistakes here.",
    "start": "701125",
    "end": "710850"
  },
  {
    "text": "Yeah, here. So here is a case where the\nlinear classifier at the input",
    "start": "710850",
    "end": "719150"
  },
  {
    "text": "layer cannot do it. And that's our--\nthat's a theme which",
    "start": "719150",
    "end": "726350"
  },
  {
    "text": "is very popular, both in\ncomputation neural science",
    "start": "726350",
    "end": "732889"
  },
  {
    "text": "and system neural science\nstudies in machine learning.",
    "start": "732890",
    "end": "739070"
  },
  {
    "text": "And the following\nquestion comes up. Suppose we see that there\nis a transformation of data",
    "start": "739070",
    "end": "745310"
  },
  {
    "text": "from, let's say, a photoreceptor\nlayer in vision to the ganglion",
    "start": "745310",
    "end": "754790"
  },
  {
    "text": "cells at the output\nof the retina, then to cortex in\nseveral stages.",
    "start": "754790",
    "end": "760040"
  },
  {
    "text": "How do we gauge,\nhow do we assess",
    "start": "760040",
    "end": "767110"
  },
  {
    "text": "what is the advantage for the\nbrain to transform information",
    "start": "767111",
    "end": "772610"
  },
  {
    "text": "from one, let's say, from retina\nto V1, and so on and so forth. After all, in this\nfeedforward's architecture,",
    "start": "772610",
    "end": "781639"
  },
  {
    "text": "no net information is\ngenerated at the next layer.",
    "start": "781640",
    "end": "786660"
  },
  {
    "text": "So if no net information\nis generated, the question is, what did we\ngain by these transformations?",
    "start": "786660",
    "end": "795090"
  },
  {
    "text": "And one possible answer is that\nit is reformatted, reformatting",
    "start": "795090",
    "end": "802520"
  },
  {
    "text": "the sensory representation into\ndifferent representation which will make subsequent\ncomputations simpler.",
    "start": "802520",
    "end": "810019"
  },
  {
    "text": "So what does it mean, subsequent\ncomputation is simpler? One notion of simplicity is\nwhether subsequent computation",
    "start": "810020",
    "end": "820130"
  },
  {
    "text": "can be realized by a\nsimple linear readout.",
    "start": "820130",
    "end": "825320"
  },
  {
    "text": "So that's the strategy that\nwe are going to adopt here.",
    "start": "825320",
    "end": "833180"
  },
  {
    "text": "And this is to ask,\nas the information, as the representation\nis changing",
    "start": "833180",
    "end": "838760"
  },
  {
    "text": "as you go from one\nlayer to another, how well a linear readout will\nbe able to perform the task.",
    "start": "838760",
    "end": "846890"
  },
  {
    "text": "So that's the input. That's the story. And then, as I said,\nthere is an input,",
    "start": "846890",
    "end": "853760"
  },
  {
    "text": "unsupervised representations,\nand supervised at the end. ",
    "start": "853760",
    "end": "859670"
  },
  {
    "text": "I need to introduce notations. Bear with me. This is a computational talk.",
    "start": "859670",
    "end": "864680"
  },
  {
    "text": "I cannot just talk about ideas,\nbecause the whole thing is to be able to actually come up\nwith a quantitative theory that",
    "start": "864680",
    "end": "873710"
  },
  {
    "text": "tests ideas. So let me introduce notations. So at the centers,\nat each layer,",
    "start": "873710",
    "end": "881780"
  },
  {
    "text": "you can ask what is the\nrepresentation of the centers of these stimuli?",
    "start": "881780",
    "end": "887000"
  },
  {
    "text": "And I'll denote the\ncenter by a bar. And mu is index of the patterns.",
    "start": "887000",
    "end": "893050"
  },
  {
    "text": "So mu goes from 1 to P. P Is\nthe number of those balls, number of those spheres, or\nnumber of those clusters,",
    "start": "893050",
    "end": "899870"
  },
  {
    "text": "if you think about\nclustering some sensory data. So P would be the\nnumber of clusters.",
    "start": "899870",
    "end": "905920"
  },
  {
    "text": "i, from 1 to N, is simply the\nneuron or the unit activation",
    "start": "905920",
    "end": "911622"
  },
  {
    "text": "at each mu. And L is the layer. So 0 is the input layer. It's up to L layer.",
    "start": "911622",
    "end": "919680"
  },
  {
    "text": "So this would be 0, 1. The mean activation\nat each layer from 1",
    "start": "919680",
    "end": "929670"
  },
  {
    "text": "on will just have\nto be a constant to be f. f goes from 0 to 1. The smaller f is, the sparser\nthe representation is.",
    "start": "929670",
    "end": "938040"
  },
  {
    "text": "We will assume that the input\nrepresentation is dense. So this is 0.5.",
    "start": "938040",
    "end": "943899"
  },
  {
    "text": "N, again, we'll assume\nto be, for simplicity, a constant across layers, except\nfor the first layer, where",
    "start": "943900",
    "end": "949820"
  },
  {
    "text": "there is expansion. You can vary those parameters,\nand actually the theory",
    "start": "949820",
    "end": "955320"
  },
  {
    "text": "accommodates\nvariations of those. But that's the\nsimplest architecture. You expend a dense\nrepresentation",
    "start": "955320",
    "end": "962730"
  },
  {
    "text": "into a sparse higher\ndimension, and you keep doing it as you go along.",
    "start": "962730",
    "end": "969810"
  },
  {
    "text": "So that's notation. Now, how do we assess what\nis the next stages doing",
    "start": "969810",
    "end": "981950"
  },
  {
    "text": "to those clusters. So, as I said, one measure\nis take a linear classifier",
    "start": "981950",
    "end": "988080"
  },
  {
    "text": "and see how linear\nclassifier performs. But, actually, you can\nalso look at the statistics",
    "start": "988080",
    "end": "996720"
  },
  {
    "text": "of the injected sensory\nstimuli at each layer",
    "start": "996720",
    "end": "1002329"
  },
  {
    "text": "and learn something from it. And, basically, I'm\ngoing to suggest looking at two major\nstatistical aspects",
    "start": "1002330",
    "end": "1011450"
  },
  {
    "text": "of the data in each layer\nof the transformation. One of them is noise, and\none of them is correlation.",
    "start": "1011450",
    "end": "1019310"
  },
  {
    "text": "So what is noise? So, again, noise will be\nsimply the radius, or measure",
    "start": "1019310",
    "end": "1026959"
  },
  {
    "text": "of the radius of the sphere. So if you had only the\ntemplates as inputs,",
    "start": "1026960",
    "end": "1035540"
  },
  {
    "text": "the problem would be simple. Problem would be easy as long\nas we have enough dimension. You expand it.",
    "start": "1035540",
    "end": "1041390"
  },
  {
    "text": "You can easily do\nlinear classifier and solve the problem. So the problem, in\nour case, is the fact",
    "start": "1041390",
    "end": "1048800"
  },
  {
    "text": "that the input is actually\nthe infinite number of inputs, or exponentially large\nnumber of possible inputs,",
    "start": "1048800",
    "end": "1054110"
  },
  {
    "text": "because they all come from a\nGaussian or a binarized version",
    "start": "1054110",
    "end": "1062870"
  },
  {
    "text": "of a Gaussian noise\naround the templates. And I'll denote\nthe noise by delta. 0 means no noise.",
    "start": "1062870",
    "end": "1071240"
  },
  {
    "text": "The normalization is such that\n1 means that they are random. So delta equals to 1 means that,\nbasically, you cannot tell,",
    "start": "1071240",
    "end": "1080360"
  },
  {
    "text": "the input, whether it's coming\nfrom here or from any other points in the input space.",
    "start": "1080360",
    "end": "1085920"
  },
  {
    "text": "The other thing,\ncorrelations, is more subtle.",
    "start": "1085920",
    "end": "1090980"
  },
  {
    "text": "So I'm going to assume\nthat those balls are coming from kind of\nuniform distribution.",
    "start": "1090980",
    "end": "1097730"
  },
  {
    "text": "Imagine you take\na template here. You draw a ball around it. You take a template. Here, you draw a ball.",
    "start": "1097730",
    "end": "1103080"
  },
  {
    "text": "Everything is kind of\nuniformly distributed. The only structure\nis the fact that data",
    "start": "1103080",
    "end": "1108590"
  },
  {
    "text": "comes from this\nmixture of Gaussians or noisy patterns\naround those centers.",
    "start": "1108590",
    "end": "1118940"
  },
  {
    "text": "So that's fine. But as you project those\nclusters into the next stage,",
    "start": "1118940",
    "end": "1128150"
  },
  {
    "text": "I claim that those\ncenters, those templates, get new representation, which\ncan actually have structure",
    "start": "1128150",
    "end": "1136205"
  },
  {
    "text": "in them, simply by the fact\nthat you put all of them",
    "start": "1136205",
    "end": "1141260"
  },
  {
    "text": "into this common synaptic\nweights into the next layer.",
    "start": "1141260",
    "end": "1146400"
  },
  {
    "text": "And I'm going to measure this\nby Q. And, basically, low Q or 0",
    "start": "1146400",
    "end": "1151850"
  },
  {
    "text": "Q is basically a kind of\nrandomly uniformly distributed centers. And I'll always start from\nthat at the input layer.",
    "start": "1151850",
    "end": "1160100"
  },
  {
    "text": "But then there is\na danger, or it might happen that,\nas you propagate",
    "start": "1160100",
    "end": "1166370"
  },
  {
    "text": "this information or\nthis representation through the next layer, the\ncenters will look like that,",
    "start": "1166370",
    "end": "1172725"
  },
  {
    "text": "or the data, structure of\nthe data, looks like that. So, on average, the distance\nbetween two centers,",
    "start": "1172725",
    "end": "1179450"
  },
  {
    "text": "on average, is the same as here. But they are clumped together. It's kind of random\nclustering of the clusters.",
    "start": "1179450",
    "end": "1187010"
  },
  {
    "text": "And that can be\ninduced by the fact that the data is feedforwarded\nfrom this representation.",
    "start": "1187010",
    "end": "1198080"
  },
  {
    "text": "That can pose a problem. If there is no noise, then\nthere is, again, no problem.",
    "start": "1198080",
    "end": "1203550"
  },
  {
    "text": "You can differentiate\nbetween them, and so on. But if there is noise, this\ncan aggravate the situation,",
    "start": "1203550",
    "end": "1209250"
  },
  {
    "text": "because some of the clusters\nbecome dangerously close",
    "start": "1209250",
    "end": "1214410"
  },
  {
    "text": "to each other. And we will come to it. But, anyway, so we have\nthis delta, the noise,",
    "start": "1214410",
    "end": "1219750"
  },
  {
    "text": "the size of the clusters, and\nwe have Q, the correlations, how they are clumped\nin each representation.",
    "start": "1219750",
    "end": "1228280"
  },
  {
    "text": "And now we can ask\nhow delta evolve when you go from\none presentation to another, how Q evolve from\none presentation to another,",
    "start": "1228280",
    "end": "1236340"
  },
  {
    "text": "and how linear classifier\nperformance will change from one\nrepresentation to another.",
    "start": "1236340",
    "end": "1241510"
  },
  {
    "text": "So the simplicity\nof this assumption",
    "start": "1241510",
    "end": "1248820"
  },
  {
    "text": "allows for a kind of systematic,\nanalytical exploration or study",
    "start": "1248820",
    "end": "1256539"
  },
  {
    "text": "of this.  These are definitions.",
    "start": "1256540",
    "end": "1262290"
  },
  {
    "text": "Let's go on. So what will be the\nideal situation? So the ideal situation\nwill be that I",
    "start": "1262290",
    "end": "1268559"
  },
  {
    "text": "start from some level\nof noise, which is",
    "start": "1268560",
    "end": "1274971"
  },
  {
    "text": "my spheres at the input layer. I may or may not start\nwith some correlation.",
    "start": "1274971",
    "end": "1280890"
  },
  {
    "text": "The simplest case would be\nthat I start from randomly distributed centers.",
    "start": "1280890",
    "end": "1286480"
  },
  {
    "text": "So this would be 0. And the best situation\nwill be that, as I propagate the sensory\nstimuli, delta, the noise,",
    "start": "1286480",
    "end": "1295920"
  },
  {
    "text": "will go to 0. As I said, if the\nnoise goes to 0, you are left with\nbasically points.",
    "start": "1295920",
    "end": "1302580"
  },
  {
    "text": "And those points, if there\nis enough dimensionality, those points would be\neasily classifiable.",
    "start": "1302580",
    "end": "1308470"
  },
  {
    "text": "It would also be\ngood, if the noise doesn't go to 0, to have\nalso kind of uniformly spread",
    "start": "1308470",
    "end": "1317110"
  },
  {
    "text": "clusters. So it will be good to\nkeep Q to be small. ",
    "start": "1317110",
    "end": "1324630"
  },
  {
    "text": "So let's look at one layer. So let's look at this. We have the input layer,\nthe output layer here,",
    "start": "1324630",
    "end": "1330730"
  },
  {
    "text": "and the readout. The first question is what to\nchoose for this input layer.",
    "start": "1330730",
    "end": "1339340"
  },
  {
    "text": "So the simplest answer\nwould be choose random. So what we do, we\njust take Gaussian.",
    "start": "1339340",
    "end": "1347410"
  },
  {
    "text": "The Gaussian weights in\nthis layer are very simple. 0 mean, with some normalization.",
    "start": "1347410",
    "end": "1353140"
  },
  {
    "text": "It doesn't matter. Then we project them into\neach one of these guys here.",
    "start": "1353140",
    "end": "1359130"
  },
  {
    "text": "And then we add threshold\nto enforce the sparsity",
    "start": "1359130",
    "end": "1364971"
  },
  {
    "text": "that we want. So whatever the activation here\nis, whatever the input here is,",
    "start": "1364971",
    "end": "1374790"
  },
  {
    "text": "the threshold makes\nsure that only the f with the largest\ninput will be active,",
    "start": "1374790",
    "end": "1381900"
  },
  {
    "text": "and the rest will be 0. So there is a nonlinearity,\nwhich is of course extremely important. If you map one layer to another\nwith a linear transformation,",
    "start": "1381900",
    "end": "1391350"
  },
  {
    "text": "you don't gain anything in\nterms of classification. So there is a nonlinearity,\nsimply a threshold nonlinearity",
    "start": "1391350",
    "end": "1400559"
  },
  {
    "text": "after an input projection. All right. So how we are going to do this?",
    "start": "1400560",
    "end": "1407460"
  },
  {
    "text": "So it's straightforward to\nactually compute analytically what will happen to a noise.",
    "start": "1407460",
    "end": "1412890"
  },
  {
    "text": "So imagine you take two input\nvectors with some Hamming distance apart from each other.",
    "start": "1412890",
    "end": "1418860"
  },
  {
    "text": "You map them by convolving\nthem, so to speak,",
    "start": "1418860",
    "end": "1426120"
  },
  {
    "text": "with Gaussian weights,\nand then thresholding them to get some sparsity. So f is the sparsity.",
    "start": "1426120",
    "end": "1432420"
  },
  {
    "text": "The smaller the f is,\nthe sparser it is. So this is the noise level,\nthe normalized Gaussian--",
    "start": "1432420",
    "end": "1439835"
  },
  {
    "text": "I'm sorry-- sphere radius\nor Hamming distance in the output layer, and\nalso the input layer.",
    "start": "1439835",
    "end": "1447950"
  },
  {
    "text": "Well if 0 at start, then of\ncourse you start at the origin. If you are random at the input,\nyou will be random there.",
    "start": "1447950",
    "end": "1453300"
  },
  {
    "text": "So these points are fine. But, as you see,\nimmediately there is an amplification of the\nnoise as you go from the input",
    "start": "1453300",
    "end": "1462210"
  },
  {
    "text": "to the output. So you start from 0.2, but you\nget actually, after one layer,",
    "start": "1462210",
    "end": "1469080"
  },
  {
    "text": "to 0.6. And, actually, the\nsparser it is--",
    "start": "1469080",
    "end": "1476610"
  },
  {
    "text": "so this is a relatively\nhigh sparsity, or at least you go from here\nto here by increasing sparsity,",
    "start": "1476610",
    "end": "1483690"
  },
  {
    "text": "namely f becomes smaller. And as f becomes\nsmaller, this curve is actually steeper and steeper.",
    "start": "1483690",
    "end": "1490150"
  },
  {
    "text": "So not only you\namplify noise, but you also-- the amplification\nbecomes worse the sparser",
    "start": "1490150",
    "end": "1498245"
  },
  {
    "text": "the representation is. So that is the kind\nof negative result.",
    "start": "1498245",
    "end": "1506710"
  },
  {
    "text": "The idea that you can gain\nby expanding data to a higher",
    "start": "1506710",
    "end": "1513299"
  },
  {
    "text": "dimension and make\nthem more separable later on dates back to David\nMarr's classical theory",
    "start": "1513300",
    "end": "1523290"
  },
  {
    "text": "of the cerebellum. But what we show here is\nthat, if you think not about clean data, a set of points\nthat you want to separate,",
    "start": "1523290",
    "end": "1531510"
  },
  {
    "text": "but you think about the\nmore realistic case of you have noisy data, or\ndata with high variance,",
    "start": "1531510",
    "end": "1540362"
  },
  {
    "text": "then the situation\nis very different. So a random expansion\nactually amplifies those.",
    "start": "1540362",
    "end": "1548130"
  },
  {
    "text": "And that's a theme that will-- actually, we will live\nwith it as we go along.",
    "start": "1548130",
    "end": "1553169"
  },
  {
    "text": "Random expansion is doing the\nseparation of the templates.",
    "start": "1553170",
    "end": "1558910"
  },
  {
    "text": "But the problem is it also\nseparates two nearby points within a cluster. It also separates them.",
    "start": "1558910",
    "end": "1564480"
  },
  {
    "text": "So everything becomes\nseparated from each other. And this is why\nnoise is amplified.",
    "start": "1564480",
    "end": "1572580"
  },
  {
    "text": "Now, what about the most subtle\nthing, which is the kind of overlap between the centers?",
    "start": "1572580",
    "end": "1579065"
  },
  {
    "text": "So, on average, the centers are\nas far apart as random things. But if you look, not\non average, but you",
    "start": "1579065",
    "end": "1585480"
  },
  {
    "text": "look at the\nindividual pairs, you see that there is an\nexcess correlations",
    "start": "1585480",
    "end": "1590680"
  },
  {
    "text": "or overlap between them. So this is overlap\nbetween the centers. Again, on average, it is 0,\nbut the variance is not 0.",
    "start": "1590680",
    "end": "1597900"
  },
  {
    "text": "On average it's like\nrandom, but the variance is different, is\nlarger than random.",
    "start": "1597900",
    "end": "1603330"
  },
  {
    "text": "And there is an amplification. There is a generation\nof this excess overlap,",
    "start": "1603330",
    "end": "1611250"
  },
  {
    "text": "although it's nicely\ncontrolled by sparsity. So as sparsity goes down,\nthese correlations go down.",
    "start": "1611250",
    "end": "1619080"
  },
  {
    "text": "So that's not a\ntremendous problem. The major problem, as\nI said, is the noise.",
    "start": "1619080",
    "end": "1625990"
  },
  {
    "text": "By the way, you can nicely do\nan exercise where you generate,",
    "start": "1625990",
    "end": "1632740"
  },
  {
    "text": "you look at this cortical\nlayer representation,",
    "start": "1632740",
    "end": "1637929"
  },
  {
    "text": "and you do SVM, or\nPCA, and you look at the eigenvalue spectrum.",
    "start": "1637930",
    "end": "1642950"
  },
  {
    "text": "So if you just look at random\nsparse points, and you look at the SVD this is the\neigenvalues number ranked--",
    "start": "1642950",
    "end": "1652360"
  },
  {
    "text": "then that's what you find. It's the famous\nMarchenko-Pastur distribution.",
    "start": "1652360",
    "end": "1659019"
  },
  {
    "text": "But, in our case, you see\nthere is an extra power. In this case, the\ninput layer is 100,",
    "start": "1659020",
    "end": "1666220"
  },
  {
    "text": "so the extra power\nin the input layer, in the first input eigenvalue.",
    "start": "1666220",
    "end": "1671890"
  },
  {
    "text": "Now, why is it so? What Q is telling\nus, what nonzero Q",
    "start": "1671890",
    "end": "1677490"
  },
  {
    "text": "is telling us is the following. You take a set of random\npoints, and you project them",
    "start": "1677490",
    "end": "1684730"
  },
  {
    "text": "into higher dimensions. You start with 100 dimensions,\nand you project them in 1,000 dimensions.",
    "start": "1684730",
    "end": "1690820"
  },
  {
    "text": "On average, they are random. But actually-- so\nyou would imagine",
    "start": "1690820",
    "end": "1696395"
  },
  {
    "text": "that it's a perfect thing. You project them\nwith random weights. Then you would\nimagine that you just",
    "start": "1696395",
    "end": "1702490"
  },
  {
    "text": "created a set of random points\nin the expanded dimension",
    "start": "1702490",
    "end": "1708240"
  },
  {
    "text": "representation. If this was so, then\nif you do SVM or PCA",
    "start": "1708240",
    "end": "1714160"
  },
  {
    "text": "on this representation,\nyou will find what you expect from a PCA\nof a set of random points.",
    "start": "1714160",
    "end": "1720190"
  },
  {
    "text": "And this is this one. In fact, there is a trace of\nlow dimensionality in the data.",
    "start": "1720190",
    "end": "1730250"
  },
  {
    "text": "So I think that's an\nimportant point, which I would like to explain. You start from a set of points.",
    "start": "1730250",
    "end": "1736450"
  },
  {
    "text": "If you don't threshold\nthem and you just map them into 1,000-dimensional space,\nthose 100-dimensional input",
    "start": "1736450",
    "end": "1746710"
  },
  {
    "text": "will remain 100-dimensional. Just be rotated, and\nso on, but everything will live in\n100-dimensional space.",
    "start": "1746710",
    "end": "1754970"
  },
  {
    "text": "Now you add thresholding,\nhigh thresholding by sparsity.",
    "start": "1754970",
    "end": "1760299"
  },
  {
    "text": "So those 100-dimensional\nsubspace becomes now 1,000-dimensional sparsity\nbecause of the nonlinearity.",
    "start": "1760300",
    "end": "1768900"
  },
  {
    "text": "But this nonlinearity, although\nit takes 100-dimensional input and makes them\n1,000-dimensional,",
    "start": "1768900",
    "end": "1776559"
  },
  {
    "text": "it's still not like random. This 1,000-dimensional\ncloud is still elongated.",
    "start": "1776560",
    "end": "1785230"
  },
  {
    "text": "It's not simply\nuniformly distributed. And this is the signature\nthat you see here.",
    "start": "1785230",
    "end": "1792400"
  },
  {
    "text": "In the first largest\n100 eigenvalues, there is extra power\nrelative to the random.",
    "start": "1792400",
    "end": "1802870"
  },
  {
    "text": "The rest is not 0. So if you look here,\nthis goes up to 1,000. The rest is not 0.",
    "start": "1802870",
    "end": "1809860"
  },
  {
    "text": "So the system is,\nstrictly speaking, 1,000-dimensional space,\nbut it's not random.",
    "start": "1809860",
    "end": "1817370"
  },
  {
    "text": "It has increased\npower in 100 channels. If you do a readout,\na linear classifier",
    "start": "1817370",
    "end": "1826460"
  },
  {
    "text": "readout, what you find in this-- again, when you expand\nwith random weights,",
    "start": "1826460",
    "end": "1839860"
  },
  {
    "text": "you find that there is\nan optimal sparsity.",
    "start": "1839860",
    "end": "1845120"
  },
  {
    "text": "So this is the readout error. For a classifier, the\noutput is a function",
    "start": "1845120",
    "end": "1851326"
  },
  {
    "text": "of the sparsity for\ndifferent levels of noise. And you see that, in the\ncase of random weights,",
    "start": "1851326",
    "end": "1859960"
  },
  {
    "text": "there is a very high\nsparsity, is bad.",
    "start": "1859960",
    "end": "1865470"
  },
  {
    "text": "There Is an optimal\nsparsity or sparseness, and then there is\na shallow increase",
    "start": "1865470",
    "end": "1870820"
  },
  {
    "text": "in the error when you go\nto a denser representation. One important point\nwhich I want to emphasize",
    "start": "1870820",
    "end": "1878530"
  },
  {
    "text": "coming from the analysis--\nlet me skip equations-- and this is what you see here. The question is, can I do better\nby further increase the layer?",
    "start": "1878530",
    "end": "1888070"
  },
  {
    "text": "So here I plot the readout\nerror as a function of the size of cortical layer.",
    "start": "1888070",
    "end": "1894400"
  },
  {
    "text": "Can I do better? If I make the kernel\ndimensionality infinite,",
    "start": "1894400",
    "end": "1899830"
  },
  {
    "text": "can I do better? Well, it can do better if\nyou start with 0 noise. But if you have noisy\ninputs, then basically,",
    "start": "1899830",
    "end": "1912265"
  },
  {
    "text": "the performance saturates. And that's kind of surprising. ",
    "start": "1912265",
    "end": "1918670"
  },
  {
    "text": "We were expecting\nthat, if you go to a larger and larger\nrepresentation, eventually",
    "start": "1918670",
    "end": "1924382"
  },
  {
    "text": "the error will go to 0. But it doesn't go to 0. And that actually\nhappens even for what we",
    "start": "1924382",
    "end": "1929980"
  },
  {
    "text": "call structured representation. And that's the same for\ndifferent types of readout--",
    "start": "1929980",
    "end": "1935559"
  },
  {
    "text": "perceptual, and\npseudo-inverse, SVM. All of them show this\nsaturation as you increase",
    "start": "1935560",
    "end": "1942039"
  },
  {
    "text": "the size of cortical layer. And that's one of the very\nimportant outcome of our study.",
    "start": "1942040",
    "end": "1949110"
  },
  {
    "text": "That when you talk\nabout noisy inputs, you can think about it as kind\nof more generalization task.",
    "start": "1949110",
    "end": "1955590"
  },
  {
    "text": "Then there is a limit\nabout what you gain by expanding representation.",
    "start": "1955590",
    "end": "1963105"
  },
  {
    "text": "Even if you expand in a\nnonlinear fashion and you increase the dimensionality,\nyou cannot combat the noise,",
    "start": "1963105",
    "end": "1970440"
  },
  {
    "text": "at least up to some level. Beyond some level, there is\nno point of further expansion, because basically\nthe error saturates",
    "start": "1970440",
    "end": "1982190"
  },
  {
    "text": "Let me, since time\ngoes fast, let me talk about the alternatives. So if random weights\nare not doing so well,",
    "start": "1982190",
    "end": "1989167"
  },
  {
    "text": "what are the alternatives? The alternative is to do some\nkind of unsupervised learning.",
    "start": "1989167",
    "end": "1994760"
  },
  {
    "text": "Here we are doing it\nin a kind of a shortcut of unsupervised learning.",
    "start": "1994760",
    "end": "2000230"
  },
  {
    "text": "What is the shortcut? We say the following. Imagine that these\nlayers, the learner",
    "start": "2000230",
    "end": "2006880"
  },
  {
    "text": "knows about the representation\nof the clusters. It doesn't know the labels.",
    "start": "2006880",
    "end": "2013350"
  },
  {
    "text": "In other words, whether\nthose are pluses and those are minuses, which one\nare pluses and minuses. But he does know about\nthe statistical structure",
    "start": "2013350",
    "end": "2020860"
  },
  {
    "text": "of the input, and\nthis is this S bar. These are the centers.",
    "start": "2020860",
    "end": "2025900"
  },
  {
    "text": "So we want to encode the\nstatistical structure of these input in this\nexpansion of the weights.",
    "start": "2025900",
    "end": "2034660"
  },
  {
    "text": "And the way we do the simplest\nway is the kind of Hebb rule. We do the following. We say let's first choose, or\nrecruit, or allocate a state,",
    "start": "2034660",
    "end": "2045150"
  },
  {
    "text": "a sparse state here, randomly\nchosen, to associate,",
    "start": "2045150",
    "end": "2050530"
  },
  {
    "text": "to represent each\none of the clusters. So these are the R. R are the\nrandomly chosen patterns here.",
    "start": "2050530",
    "end": "2058149"
  },
  {
    "text": "And then we associate\nbetween those randomly chosen representations and\nthe actual centers",
    "start": "2058150",
    "end": "2067090"
  },
  {
    "text": "of the clusters of the inputs. So this is S bar and R. And\nthen we do the association by the simple, what's\ncalled Hebb rule.",
    "start": "2067090",
    "end": "2074440"
  },
  {
    "text": "So this Hebbian rule\nassociates cluster center with a randomly\nassigned state",
    "start": "2074440",
    "end": "2083919"
  },
  {
    "text": "in the cortical layer in\na kind of simple summation",
    "start": "2083920",
    "end": "2089859"
  },
  {
    "text": "or outer product\nfor the Hebb rule. There are more\nsophisticated ways to do it, but that's the simplest\none of doing it.",
    "start": "2089860",
    "end": "2096280"
  },
  {
    "text": "So it turns out that this simple\nrule has enormous potential",
    "start": "2096280",
    "end": "2103090"
  },
  {
    "text": "for suppressing noise. So, again, this is the input\nnoise and the output noise. The Hamming distance of\nthe input and the output",
    "start": "2103090",
    "end": "2110830"
  },
  {
    "text": "properly normalized. And you see that, as you go to\nhigher and higher sparseness,",
    "start": "2110830",
    "end": "2116740"
  },
  {
    "text": "to lower and lower\nf, this is basically the input noise is completely\nquenched when f is large.",
    "start": "2116740",
    "end": "2124930"
  },
  {
    "text": "When f is 0.01, for instance,\nthis is this already. Sub-linear when f 0.05 is\nhere, and so and so forth.",
    "start": "2124930",
    "end": "2133119"
  },
  {
    "text": "So sparse representation,\nin particular, are very effective\nin suppressing noise,",
    "start": "2133120",
    "end": "2139580"
  },
  {
    "text": "but provided the inputs have\nkind of unsupervised learning",
    "start": "2139580",
    "end": "2145210"
  },
  {
    "text": "encoded into them\nwhich embed into them the cluster structure\nof the inputs.",
    "start": "2145210",
    "end": "2151361"
  },
  {
    "text": " The same or similar\nthing is true for Q",
    "start": "2151361",
    "end": "2158170"
  },
  {
    "text": "for these correlations. If you look at the-- this\nwas a random correlation.",
    "start": "2158170",
    "end": "2163630"
  },
  {
    "text": "This is a function of f, and\nthis is Q, the correlation. It's extremely suppressed\nfor sparse representation.",
    "start": "2163630",
    "end": "2172840"
  },
  {
    "text": "Basically, it's\nexponentially small with 1/f, so it's basically 0 for\nsparse representation.",
    "start": "2172840",
    "end": "2178660"
  },
  {
    "text": "Which means that those\ncenters look like randomly distributed, essentially,\nand with very small noise.",
    "start": "2178660",
    "end": "2185200"
  },
  {
    "text": "So you took these\nspheres and you basically map them into random points\nwith a very small radius.",
    "start": "2185200",
    "end": "2193570"
  },
  {
    "text": "So it's not surprising\nthat, in this case, the error for small f-- the\nerror, even for large noise",
    "start": "2193570",
    "end": "2202670"
  },
  {
    "text": "values, the error is\nbasically small, 0. Nevertheless, it\nis still saturating",
    "start": "2202670",
    "end": "2208750"
  },
  {
    "text": "as a function of the network\nsize, of the cortical size. So the saturation of performance\nas a function of cortical size",
    "start": "2208750",
    "end": "2217520"
  },
  {
    "text": "is a general property\nof such systems. Nevertheless, the performance\nitself for any given size",
    "start": "2217520",
    "end": "2225400"
  },
  {
    "text": "is extremely impressive, I would\nsay, when the system is sparse",
    "start": "2225400",
    "end": "2233829"
  },
  {
    "text": "and the noise level\nis kind of moderate. ",
    "start": "2233830",
    "end": "2240370"
  },
  {
    "text": "OK, let me skip this\nbecause I don't have time. Let me briefly talk about\nextension of this story",
    "start": "2240370",
    "end": "2247570"
  },
  {
    "text": "to multi-layer. So we are now briefly\ndiscussing what happens if you take this story\nand you just propagate it",
    "start": "2247570",
    "end": "2254800"
  },
  {
    "text": "as you go along\nthe architecture. So let's start with\nrandom weights.",
    "start": "2254800",
    "end": "2260120"
  },
  {
    "text": "So the idea is maybe\nsomething is good happening. Although initially\nperformance was poor,",
    "start": "2260120",
    "end": "2265609"
  },
  {
    "text": "maybe we can improve\nthe performance by cascading such layers. And the answer is no,\nparticularly the noise level.",
    "start": "2265610",
    "end": "2273010"
  },
  {
    "text": "This is now the\nnumber of layers. What we discussed\nbefore is here. And you see the problem\nbecomes worse and worse.",
    "start": "2273010",
    "end": "2280120"
  },
  {
    "text": "As you continue to\npropagate those signals, the noise is amplified\nand essentially goes to 1.",
    "start": "2280120",
    "end": "2288050"
  },
  {
    "text": "So basically you will get\njust random performance if you keep doing it\nwith random weights.",
    "start": "2288050",
    "end": "2296420"
  },
  {
    "text": "The reason-- where is it? I missed a slide.",
    "start": "2296420",
    "end": "2301600"
  },
  {
    "text": "The reason is,\nbasically, that if you think about the mapping\nfrom one layer of noise",
    "start": "2301600",
    "end": "2308380"
  },
  {
    "text": "to another layer of noise, there\nare two fixed points, 0 and 1. The 0 fixed point is unstable.",
    "start": "2308380",
    "end": "2313570"
  },
  {
    "text": "Everything goes eventually to 1. So it is a nice--",
    "start": "2313570",
    "end": "2319090"
  },
  {
    "text": "this system gives you\na nice perspective about this deep network\nby thinking about it",
    "start": "2319090",
    "end": "2328390"
  },
  {
    "text": "as a kind of dynamical system. For instance, what\nis the level of noise at one layer, how it's\nrelated to the level of noise",
    "start": "2328390",
    "end": "2336520"
  },
  {
    "text": "at previous layer. So it's kind of iterative map. Delta n versus delta n minus 1.",
    "start": "2336520",
    "end": "2341950"
  },
  {
    "text": "And what's good about it\nis, once you kind of draw this curve, one layer is\nmapped to another layer,",
    "start": "2341950",
    "end": "2348100"
  },
  {
    "text": "you can know what happens\nto a deep network. We could just iterate this. You have to find what\nare the fixed points,",
    "start": "2348100",
    "end": "2353560"
  },
  {
    "text": "and which one is stable\nand which one is not. In this case, the 1 is\nstable, the 0 is unstable.",
    "start": "2353560",
    "end": "2358970"
  },
  {
    "text": "So, unfortunately,\nfrom any level of noise that you will start,\nyou eventually go to 1.",
    "start": "2358970",
    "end": "2364390"
  },
  {
    "text": " Correlations, is a\nsimilar story, but--",
    "start": "2364390",
    "end": "2369750"
  },
  {
    "text": "and the error will go to 0.5. So that's very well.",
    "start": "2369750",
    "end": "2376610"
  },
  {
    "text": "There are cases, by the way,\nthat you can find parameters where initially you\nimprove, like here.",
    "start": "2376610",
    "end": "2383000"
  },
  {
    "text": "But then eventually\nit will go to 0.5. Now, if we do similar--",
    "start": "2383000",
    "end": "2389720"
  },
  {
    "text": "if we compare this\nto what happened to the structured\nweights if you keep doing the same kind of\nunsupervised Hebbian",
    "start": "2389720",
    "end": "2397490"
  },
  {
    "text": "learning from one\nlayer to another-- and I'll skip the details--\nyou see the opposite.",
    "start": "2397490",
    "end": "2403970"
  },
  {
    "text": "So here are parameter\nvalue in which one stage of the expansion\nstage is actually",
    "start": "2403970",
    "end": "2414470"
  },
  {
    "text": "increasing the noise. And this is because f is not too\nsmall, and the load is large,",
    "start": "2414470",
    "end": "2421580"
  },
  {
    "text": "and the noise is starting. So you can have such situation. But even in such situation,\neventually the system",
    "start": "2421580",
    "end": "2428390"
  },
  {
    "text": "goes into stages where the\nnoise basically goes to 0.",
    "start": "2428390",
    "end": "2434855"
  },
  {
    "text": "And if you compare\nthe story why it is so to kind of\niterative map picture,",
    "start": "2434855",
    "end": "2442230"
  },
  {
    "text": "you see that the picture\nis very different. You have one fixed point at 0. You have one fixed point at 1.",
    "start": "2442230",
    "end": "2447740"
  },
  {
    "text": "You have intermediate\nfixed point at high value. But this is an unstable\nfixed point, and both of them",
    "start": "2447740",
    "end": "2453550"
  },
  {
    "text": "are stable fixed points. So if you start from even\nfrom large values of noise, eventually you\nwill iterate to 0.",
    "start": "2453550",
    "end": "2460700"
  },
  {
    "text": "So it does buy\nyou to actually go into several stages of this\ndeep network to make sure",
    "start": "2460700",
    "end": "2470120"
  },
  {
    "text": "that the noise is\nsuppressed to 0. Similarly for the correlations. Even if the parameters are such\nthat initially correlations",
    "start": "2470120",
    "end": "2477440"
  },
  {
    "text": "are increased, and you can\nfind parameters like that, eventually correlations\nwill go to almost 0.",
    "start": "2477440",
    "end": "2482720"
  },
  {
    "text": " And this is comparison\nof the readout error",
    "start": "2482720",
    "end": "2488840"
  },
  {
    "text": "as a function of the layers\nwith structured weights, and I compare it with\nthe readout error",
    "start": "2488840",
    "end": "2494840"
  },
  {
    "text": "of infinitely wide\nlayer, kind of a kernel with infinitely wide kernel.",
    "start": "2494840",
    "end": "2501710"
  },
  {
    "text": "And you can see\nthat, at least for-- here I compare the same type of\nunsupervised learning but two",
    "start": "2501710",
    "end": "2509240"
  },
  {
    "text": "different architectures. One is deep network\narchitecture, and the another one is shallow\narchitecture, infinitely wide.",
    "start": "2509240",
    "end": "2517180"
  },
  {
    "text": "I'm not claiming that we can\nshow that there is no kernel or shallow architecture\nwhich will do better,",
    "start": "2517180",
    "end": "2523940"
  },
  {
    "text": "but I'm saying if we compare\nthe same learning rule but with the two\ndifferent architectures,",
    "start": "2523940",
    "end": "2531049"
  },
  {
    "text": "you'll find that\nyou do gain by going into multiple stages\nof nonlinearity",
    "start": "2531050",
    "end": "2537980"
  },
  {
    "text": "than by using an\ninfinitely wide layer. ",
    "start": "2537980",
    "end": "2544210"
  },
  {
    "text": "I'll skip this. I want to go briefly\nto two more issues.",
    "start": "2544210",
    "end": "2551070"
  },
  {
    "text": "One issue is the\nrecurrent networks. Why recurrent networks?",
    "start": "2551070",
    "end": "2556200"
  },
  {
    "text": "The primary reason\nis that, in each one of those stages that I refer\nto, if you look at the biology,",
    "start": "2556200",
    "end": "2563860"
  },
  {
    "text": "on most of them-- not all of them but most\nof them, and definitely in neocortex--",
    "start": "2563860",
    "end": "2570150"
  },
  {
    "text": "you find massive recurrent\nor lateral interactions between each one of the layers.",
    "start": "2570150",
    "end": "2576400"
  },
  {
    "text": "So, again, we would\nlike to ask, what",
    "start": "2576400",
    "end": "2581700"
  },
  {
    "text": "is the computational advantage\nof having this recurrent layer. Now, in our case, we had an\nextra motivation, and this is--",
    "start": "2581700",
    "end": "2591090"
  },
  {
    "text": "remember that I started in\nsaying that, in some cases,",
    "start": "2591090",
    "end": "2596140"
  },
  {
    "text": "there is experimental evidence\nthat the initial projection is random.",
    "start": "2596140",
    "end": "2601710"
  },
  {
    "text": "So that we ask ourselves,\nwhat happens if we do this. If we start from\nrandom projection, feedforward projection, and\nthen add recurrent connections.",
    "start": "2601710",
    "end": "2611160"
  },
  {
    "text": "Think about it as from the\nolfactory bulb, for instance, to piriform cortex, perhaps\nrandom feedforward projections.",
    "start": "2611160",
    "end": "2618470"
  },
  {
    "text": "But then the association,\nrecurrent connections in piriform cortex\nare structured.",
    "start": "2618470",
    "end": "2625200"
  },
  {
    "text": "How do we do that? We start, we imagine starting\nfrom random projection,",
    "start": "2625200",
    "end": "2631020"
  },
  {
    "text": "generating initial\nrepresentation by the random projection,\nand then stabilizing",
    "start": "2631020",
    "end": "2637110"
  },
  {
    "text": "those representation\ninto attractors by the recurrent connections. And that actually\nworks pretty well.",
    "start": "2637110",
    "end": "2644780"
  },
  {
    "text": "It's not the optimal\narchitecture, but it's pretty well. For instance, noise,\nwhich is initially",
    "start": "2644780",
    "end": "2650849"
  },
  {
    "text": "increased by the\nrandom projections, were quenched by\nconvergence to attractors.",
    "start": "2650850",
    "end": "2656510"
  },
  {
    "text": "And, similarly, Q\nwill not go to 0, but will not continue\ngrowing, but will go to an intermediate layer.",
    "start": "2656510",
    "end": "2663150"
  },
  {
    "text": "And the error is pretty well. So if you look at in this\ncase, the error really",
    "start": "2663150",
    "end": "2670260"
  },
  {
    "text": "goes down to very low values. But now it's not layers. Now it is the\nnumber of iterations",
    "start": "2670260",
    "end": "2676440"
  },
  {
    "text": "of the recurrent connections. So you start from just input\nlayer, or random projection,",
    "start": "2676440",
    "end": "2684000"
  },
  {
    "text": "and then you iterate the\ndynamics and it goes to 0. So it's not the layers. It's just the dynamics of\nthe convergence to attractor.",
    "start": "2684000",
    "end": "2695160"
  },
  {
    "text": "My final point. I have 3 or 4 minutes? OK. My final point before wrapping\nup is the question of top-down.",
    "start": "2695160",
    "end": "2704670"
  },
  {
    "text": "So recurrent, we\nbriefly talked about it.",
    "start": "2704670",
    "end": "2709980"
  },
  {
    "text": "But incorporating contextual\nknowledge is a major question. How can you improve\non deep networks",
    "start": "2709980",
    "end": "2717900"
  },
  {
    "text": "by incorporating, not simply\nthe feedforward sensory input, but other sources of knowledge\nabout this particular stimulus?",
    "start": "2717900",
    "end": "2728400"
  },
  {
    "text": "And it's important that we are\nnot talking about knowledge about the statistics\nof the input which",
    "start": "2728400",
    "end": "2735690"
  },
  {
    "text": "can be incorporated\ninto the learning of the feedforward one. But we're talking\nabout inputs which are,",
    "start": "2735690",
    "end": "2742450"
  },
  {
    "text": "or knowledge, which we have now\non the network which already has learned whatever\nit has learned.",
    "start": "2742450",
    "end": "2749430"
  },
  {
    "text": "So we have a mature network,\nwhatever the architecture is. We have a sensory input. It goes feedforward.",
    "start": "2749430",
    "end": "2755400"
  },
  {
    "text": "And now we have additional\ninformation, about context for instance,\nthat we want to incorporate with\nthe sensory input",
    "start": "2755400",
    "end": "2763140"
  },
  {
    "text": "to improve the performance. So how do we do that?",
    "start": "2763140",
    "end": "2769260"
  },
  {
    "text": "It turns out to be non-trivial\ncomputational problem. It is very straightforward to\ndo it in Bayesian framework,",
    "start": "2769260",
    "end": "2778980"
  },
  {
    "text": "where you simply\nupdate the prior of what the sensory input is\nby this contextual information.",
    "start": "2778980",
    "end": "2789070"
  },
  {
    "text": "But if you want to\nimplement it in the network, you find that it's\nnot easy to find",
    "start": "2789070",
    "end": "2796910"
  },
  {
    "text": "the appropriate architecture. So I'll just briefly\ntalk about how we do it.",
    "start": "2796910",
    "end": "2803530"
  },
  {
    "text": "So imagine you have, again,\nthese sensory inputs, but now there is some\ncontext, different contexts.",
    "start": "2803530",
    "end": "2810460"
  },
  {
    "text": "And imagine you\nhave an information that the input is coming from\nthat particular part of state",
    "start": "2810460",
    "end": "2820710"
  },
  {
    "text": "space. So basically the question is\nhow to amplify selectively a specific set of states in\na distributed representation.",
    "start": "2820710",
    "end": "2828630"
  },
  {
    "text": "So usually when we talk\nabout attention, or gating, or questions like that,\nwe think about, OK, we",
    "start": "2828630",
    "end": "2835020"
  },
  {
    "text": "have these neurons. We suppress those, or\nmaybe amplify other ones.",
    "start": "2835020",
    "end": "2840650"
  },
  {
    "text": "Or we have a set of\naxons, or pathways. We suppress those,\nand amplify those.",
    "start": "2840650",
    "end": "2846890"
  },
  {
    "text": "But what about a\nrepresentation which is more distributed where you have\nto really suppress states",
    "start": "2846890",
    "end": "2853039"
  },
  {
    "text": "rather than neural populations. So I just won't go-- again,\nit's a complicated architecture.",
    "start": "2853040",
    "end": "2862850"
  },
  {
    "text": "But, basically, we're using some\nsort of a mixed representation,",
    "start": "2862850",
    "end": "2868010"
  },
  {
    "text": "where we take the sensory\ninput and the category or contextual input,\nmix the nonlinearity,",
    "start": "2868010",
    "end": "2875510"
  },
  {
    "text": "use them to clean it,\nand propagate this. So it's a more\ncomplicated architecture, but it works beautifully.",
    "start": "2875510",
    "end": "2881960"
  },
  {
    "text": "Let me show you here an\nexample, and you'll have a flavor of what we are doing. So now the input, we have\nthose 900 spheres or templates,",
    "start": "2881960",
    "end": "2893420"
  },
  {
    "text": "but they are organized\ninto 30 categories,",
    "start": "2893420",
    "end": "2900109"
  },
  {
    "text": "and 30 tokens per category. Now, the tokens, which are\nthe actual sensory inputs,",
    "start": "2900110",
    "end": "2907400"
  },
  {
    "text": "are represented by,\nlet's say, 200 neurons. And you have a small\nnumber of neurons",
    "start": "2907400",
    "end": "2912694"
  },
  {
    "text": "representing a category. Maybe 20 is enough. So that's important,\nand you don't have to really\nexpand dramatically",
    "start": "2912695",
    "end": "2919320"
  },
  {
    "text": "the representation. So this is the input.",
    "start": "2919320",
    "end": "2925140"
  },
  {
    "text": "And now we have\nvery noisy inputs. If you look at the readout,\nthis is layers here,",
    "start": "2925140",
    "end": "2931440"
  },
  {
    "text": "and there is readout error. If you do it on the input\nlayer, or any subsequent layer",
    "start": "2931440",
    "end": "2937140"
  },
  {
    "text": "here, but without\ntop-down information. With structured interactions\nand all that I told you,",
    "start": "2937140",
    "end": "2943610"
  },
  {
    "text": "this is such a noisy input where\nthe performance is basically 0.5.",
    "start": "2943610",
    "end": "2948799"
  },
  {
    "text": "There is nothing that you can\ndo without top-down information in this network.",
    "start": "2948800",
    "end": "2954539"
  },
  {
    "text": "You can ask what will\nbe the performance. If you have an ideal observer\nthat looks at the noisy input",
    "start": "2954540",
    "end": "2961910"
  },
  {
    "text": "and makes maximum\nlikelihood categorization.",
    "start": "2961910",
    "end": "2967309"
  },
  {
    "text": "Well, then it will\ndo much better. Also not 0, so this\nis at this level. ",
    "start": "2967310",
    "end": "2973970"
  },
  {
    "text": "This higher error is\nin virtue of the fact that this network\nis still not doing",
    "start": "2973970",
    "end": "2982190"
  },
  {
    "text": "what an optimal maximum\nlikelihood observer will do. So this is the network.",
    "start": "2982190",
    "end": "2988320"
  },
  {
    "text": "This is a maximum likelihood\nreadout, both of them without extra\ntop-down information.",
    "start": "2988320",
    "end": "2996320"
  },
  {
    "text": "And in the network that\nI kind of hinted about, if you add this top-down\ninformation by generating",
    "start": "2996320",
    "end": "3004570"
  },
  {
    "text": "mixed representation, you get\na performance which is really dramatically improved.",
    "start": "3004570",
    "end": "3011210"
  },
  {
    "text": "And as you keep doing it\none layer from another,",
    "start": "3011210",
    "end": "3016690"
  },
  {
    "text": "you really get a very\nnice performance. So let me just summarize.",
    "start": "3016690",
    "end": "3024010"
  },
  {
    "text": " There is one more\nbefore summarizing.",
    "start": "3024010",
    "end": "3031735"
  },
  {
    "text": "Yeah, OK. Before that. OK. So two points to bear in mind.",
    "start": "3031735",
    "end": "3040930"
  },
  {
    "text": "One of them is that what\nI discussed to you today relies on assuming either\nrandom, comparing random",
    "start": "3040930",
    "end": "3051460"
  },
  {
    "text": "projection, to unsupervised\nlearning of a very simple type, of a kind of Hebbian type.",
    "start": "3051460",
    "end": "3059010"
  },
  {
    "text": "The output can be Hebbian, or\nperceptron, or SVM, and so on.",
    "start": "3059010",
    "end": "3067480"
  },
  {
    "text": "You could ask,\nwhat happens if you use learning rules, more\nsophisticated learning rules",
    "start": "3067480",
    "end": "3072850"
  },
  {
    "text": "for the unsupervised weights? Some of them we've studied. But, anyway, that's something\nwhich is important to explore.",
    "start": "3072850",
    "end": "3080470"
  },
  {
    "text": "And another very important\nissue for thinking about object\nrecognition in vision",
    "start": "3080470",
    "end": "3088109"
  },
  {
    "text": "and in other real-life\nproblem is input statistics.",
    "start": "3088110",
    "end": "3093340"
  },
  {
    "text": "Because what we assumed\nis a very simple mixture of Gaussian model. So you can think about\nthe task of the network",
    "start": "3093340",
    "end": "3100930"
  },
  {
    "text": "is to take the invariance,\nwhich is the variation away",
    "start": "3100930",
    "end": "3106270"
  },
  {
    "text": "from the center of the\nspherical variation, and to generate representation\nwhich is invariant to that.",
    "start": "3106270",
    "end": "3112870"
  },
  {
    "text": "But this is a very simple\ninvariance problem, because the\ninvariance was simply",
    "start": "3112870",
    "end": "3118450"
  },
  {
    "text": "restricted to these simple\ngeometric structures.",
    "start": "3118450",
    "end": "3123790"
  },
  {
    "text": "More problems which are closer\nto what real-life problems are",
    "start": "3123790",
    "end": "3132070"
  },
  {
    "text": "will have inputs which\nare, essentially, have",
    "start": "3132070",
    "end": "3137200"
  },
  {
    "text": "some structure,\nbut the structure can be of a variety of shapes.",
    "start": "3137200",
    "end": "3142860"
  },
  {
    "text": "Each one of them correspond\nto an object, or a cluster, or a manifold representing an\nentity, a perceptual entity.",
    "start": "3142860",
    "end": "3151850"
  },
  {
    "text": "But how you go from this\nnice, simple problem",
    "start": "3151850",
    "end": "3157220"
  },
  {
    "text": "of this spherical invariance\nproblem to those problems,",
    "start": "3157220",
    "end": "3163099"
  },
  {
    "text": "it's of course a\nchallenging problem. And that's the work which we\nare now, ongoing work, also",
    "start": "3163100",
    "end": "3170300"
  },
  {
    "text": "with SueYeon Chung and Dan Lee. But it's a story which is still\nat the stage of unfolding.",
    "start": "3170300",
    "end": "3180130"
  },
  {
    "start": "3180130",
    "end": "3193532"
  }
]