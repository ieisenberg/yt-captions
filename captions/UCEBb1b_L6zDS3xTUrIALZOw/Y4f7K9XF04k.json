[
  {
    "start": "0",
    "end": "152000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation, or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  GILBERT STRANG: So this\nis a pretty key lecture.",
    "start": "18140",
    "end": "25110"
  },
  {
    "text": "This lecture is about principal\ncomponent analysis, PCA-- which is a major tool in\nunderstanding a matrix of data.",
    "start": "25110",
    "end": "35390"
  },
  {
    "text": "So what is PCA about? Well first of all,\nlet me remember what",
    "start": "35390",
    "end": "41610"
  },
  {
    "text": "was the whole point of last-- yesterday's lecture-- the\nsingular value decomposition,",
    "start": "41610",
    "end": "47489"
  },
  {
    "text": "that any matrix A could be\nbroken into r rank 1 pieces--",
    "start": "47490",
    "end": "54850"
  },
  {
    "text": "r being the rank of the matrix. And each piece has a\nU times a V transpose.",
    "start": "54850",
    "end": "63250"
  },
  {
    "text": "And the good-- special thing\nis, the U's are orthonormal,",
    "start": "63250",
    "end": "68590"
  },
  {
    "text": "and also, the V's\nare orthonormal. OK. So that's the whole matrix.",
    "start": "68590",
    "end": "73900"
  },
  {
    "text": "But we have a big\nmatrix, and we want to get the important\ninformation out of it-- not all the information.",
    "start": "73900",
    "end": "80000"
  },
  {
    "text": "And people say, in\nmachine learning,",
    "start": "80000",
    "end": "85690"
  },
  {
    "text": "if you've learned all the\ntraining data, you haven't learned anything, really. You've just copied it all in.",
    "start": "85690",
    "end": "93520"
  },
  {
    "text": "The whole point of neural\nnets and the process",
    "start": "93520",
    "end": "100450"
  },
  {
    "text": "of machine learning is to learn\nimportant facts about the data. And now, here we're at the\nmost basic stage of that.",
    "start": "100450",
    "end": "110020"
  },
  {
    "text": "And I claim that the important\nfacts about the matrix are in its largest\nk singular values--",
    "start": "110020",
    "end": "120340"
  },
  {
    "text": "the largest k pieces. We can take-- k\nequal 1 would tell us the largest single piece.",
    "start": "120340",
    "end": "126940"
  },
  {
    "text": "But maybe we have space\nand computing power to handle a hundred pieces.",
    "start": "126940",
    "end": "132010"
  },
  {
    "text": "So I would take k equal 100. The matrix might have\nranked thousands. So I claim that Ak is the best.",
    "start": "132010",
    "end": "140620"
  },
  {
    "text": "Now here's the one theorem\nfor today, that Ak--",
    "start": "140620",
    "end": "147900"
  },
  {
    "text": "using the first k\npieces of the SVD--",
    "start": "147900",
    "end": "153129"
  },
  {
    "start": "152000",
    "end": "279000"
  },
  {
    "text": "is the best approximation\nto A of rank k. So I'll write that down.",
    "start": "153130",
    "end": "158770"
  },
  {
    "text": "So that really says\nwhy the SVD is perfect. OK.",
    "start": "158770",
    "end": "163780"
  },
  {
    "text": "So that statement\nsays, that if B--",
    "start": "163780",
    "end": "171640"
  },
  {
    "text": "another matrix-- has rank k,\nthen the distance from A to B--",
    "start": "171640",
    "end": "183730"
  },
  {
    "text": "the error you're making\nin just using B-- that error is greater\nthan or equal to the error",
    "start": "183730",
    "end": "191020"
  },
  {
    "text": "you make for the best guy. ",
    "start": "191020",
    "end": "196320"
  },
  {
    "text": "Now that's a pretty\nstraightforward, beautiful fact.",
    "start": "196320",
    "end": "201420"
  },
  {
    "text": "And it goes back to\npeople who discovered the SVD in the first place.",
    "start": "201420",
    "end": "208240"
  },
  {
    "text": "But then a couple\nof psychologists gave a proof in a later paper--",
    "start": "208240",
    "end": "216660"
  },
  {
    "text": "and it's often called\nthe Eckart-Young Theorem. There is the theorem. Isn't that straightforward?",
    "start": "216660",
    "end": "223860"
  },
  {
    "text": "And the hypothesis\nis straightforward. That's pretty nice.",
    "start": "223860",
    "end": "231280"
  },
  {
    "text": "But of course, we have\nto think, why is it true? Why is it true?",
    "start": "231280",
    "end": "237500"
  },
  {
    "text": "And to give meaning\nto the theorem, we have to say what\nthese double bars are.",
    "start": "237500",
    "end": "244350"
  },
  {
    "text": "Do you know the\nright name for this? So that double bar around\na matrix is called the--",
    "start": "244350",
    "end": "253620"
  },
  {
    "text": "the norm of the\nmatrix, the norm. So I have to say something\nabout matrix norms.",
    "start": "253620",
    "end": "259669"
  },
  {
    "text": "How big is-- that's a\nmeasure of how big it is. And what I have to say is, there\nare many different measures",
    "start": "259670",
    "end": "268970"
  },
  {
    "text": "of a matrix-- how large that matrix is. Let me tell you, for today,\nthree possible measures",
    "start": "268970",
    "end": "276380"
  },
  {
    "text": "of a matrix.  So different ways to measure--",
    "start": "276380",
    "end": "281960"
  },
  {
    "start": "279000",
    "end": "420000"
  },
  {
    "text": "I'll call the matrix\njust A, maybe. But then I'm going to apply\nthe measure to A minus B,",
    "start": "281960",
    "end": "290139"
  },
  {
    "text": "and to A minus AK, and\nshow that that is smaller.",
    "start": "290140",
    "end": "295250"
  },
  {
    "text": "OK. So I want to tell you\nabout the norm of A-- about some possible norms of\nA. And actually, the norms I'm",
    "start": "295250",
    "end": "304710"
  },
  {
    "text": "going to take today will be-- will have the special feature\nthat they can be found--",
    "start": "304710",
    "end": "310940"
  },
  {
    "text": "computed by their\nsingular values. So let me mention the L2 norm.",
    "start": "310940",
    "end": "318850"
  },
  {
    "text": "That is the largest\nsingular value. So that's an important\nmeasure of the--",
    "start": "318850",
    "end": "324560"
  },
  {
    "text": "sort of the size of a matrix. I'm talking here about\na general m by n matrix",
    "start": "324560",
    "end": "332030"
  },
  {
    "text": "A. Sigma 1 is an\nimportant norm--",
    "start": "332030",
    "end": "337340"
  },
  {
    "text": "often called the L2 norm. And that's where\nthat index 2 goes. Oh. I should really\nstart with vectors--",
    "start": "337340",
    "end": "344090"
  },
  {
    "text": "norms of vectors-- and then\nbuild to the norms of matrices. Let me do norms of\nvectors over on this side.",
    "start": "344090",
    "end": "351069"
  },
  {
    "text": "The L2 norm of a vector--  do we know what that is?",
    "start": "351070",
    "end": "358340"
  },
  {
    "text": "That's the regular length of\nthe vector that we all expect--",
    "start": "358340",
    "end": "364220"
  },
  {
    "text": "the square root of v1\nsquared up to vn squared.",
    "start": "364220",
    "end": "372850"
  },
  {
    "text": "The hypotenuse-- the\nlength of the hypotenuse",
    "start": "372850",
    "end": "378110"
  },
  {
    "text": "in n dimensional space. That's the L2 norm,\nbecause of that 2.",
    "start": "378110",
    "end": "383270"
  },
  {
    "text": "The L1 norm of a vector is\njust add up those pieces",
    "start": "383270",
    "end": "391039"
  },
  {
    "text": "without squaring and\nsquare rooting them. Just add them.",
    "start": "391040",
    "end": "396900"
  },
  {
    "text": "That's the L1 norm. And you might say, why\ndo we want two norms?",
    "start": "396900",
    "end": "403320"
  },
  {
    "text": "Or there are more norms. Let me just tell you one more. The infinity norm-- and there\nis a reason for the 1 and the 2",
    "start": "403320",
    "end": "412790"
  },
  {
    "text": "and the infinity-- is the largest of the v's. ",
    "start": "412790",
    "end": "418740"
  },
  {
    "text": "OK. Have you met norms before? I don't know.",
    "start": "418740",
    "end": "424020"
  },
  {
    "start": "420000",
    "end": "592000"
  },
  {
    "text": "These are vector norms,\nbut maybe you have met. Then we're going to have matrix\nnorms, that maybe will be new.",
    "start": "424020",
    "end": "430560"
  },
  {
    "text": " So this is the norm that\nwe usually think of.",
    "start": "430560",
    "end": "437530"
  },
  {
    "text": "But this one has become\nreally, really important, and let me tell you just why.",
    "start": "437530",
    "end": "443080"
  },
  {
    "text": "And then we'll-- later section\nof the notes and a later lecture in this course\nwill develop that--",
    "start": "443080",
    "end": "450840"
  },
  {
    "text": "develop this. This is the L1 norm. So this is L2, L1,\nand L infinity--",
    "start": "450840",
    "end": "458990"
  },
  {
    "text": "[INAUDIBLE] So what's special\nabout this one?",
    "start": "458990",
    "end": "464020"
  },
  {
    "text": "Well, it just turned out--\nand it was only discovered in our lifetimes--",
    "start": "464020",
    "end": "469660"
  },
  {
    "text": "that when you minimize some\nfunction using the L1 norm,",
    "start": "469660",
    "end": "479350"
  },
  {
    "text": "you minimize some, let's\nsay, signal the noise,",
    "start": "479350",
    "end": "486090"
  },
  {
    "text": "or whatever you minimize-- some function. If you use L1, the\nwinning vector--",
    "start": "486090",
    "end": "496360"
  },
  {
    "text": "the minimizing vector--\nturns out to be sparse. And what does sparse mean?",
    "start": "496360",
    "end": "503870"
  },
  {
    "text": "Sparse means mostly\nzero components. Somehow, when I minimize in L2--",
    "start": "503870",
    "end": "509930"
  },
  {
    "text": "which historically\ngoes back to Gauss, the greatest\nmathematician of all time.",
    "start": "509930",
    "end": "517520"
  },
  {
    "text": "When you minimize something in\nL2, you do the least squares. And you find that the guy\nthat gives you the minimum",
    "start": "517520",
    "end": "525950"
  },
  {
    "text": "has a lot of little numbers-- lot of little components. Because when you're\nsquare those little ones,",
    "start": "525950",
    "end": "532010"
  },
  {
    "text": "they don't hurt much. But Gauss-- so Gauss\ndidn't do least L1 norm.",
    "start": "532010",
    "end": "540610"
  },
  {
    "text": "That has different\nnames-- basis pursuit. And it comes into signal\nprocessing and sensing.",
    "start": "540610",
    "end": "553380"
  },
  {
    "text": "Right. And then it was discovered\nthat if you minimize--",
    "start": "553380",
    "end": "558760"
  },
  {
    "text": " as we'll see in that norm-- you amazingly get-- the\nwinning vector has--",
    "start": "558760",
    "end": "569649"
  },
  {
    "text": "is mostly zeros. And the advantage of that\nis that you can understand what its components are.",
    "start": "569650",
    "end": "576490"
  },
  {
    "text": "The one with many\nsmall components, you have no interpretation\nfor that answer.",
    "start": "576490",
    "end": "583150"
  },
  {
    "text": "But for an answer that just\nhas a few non-zero components, you really see what's happening.",
    "start": "583150",
    "end": "588610"
  },
  {
    "text": "And then this is a\nimportant one, too. OK. Now I'm going to\nturn just to-- so",
    "start": "588610",
    "end": "597070"
  },
  {
    "start": "592000",
    "end": "704000"
  },
  {
    "text": "what's the property of a norm? Well, you can see that the\nnorm of C times a vector is--",
    "start": "597070",
    "end": "605410"
  },
  {
    "text": "just multiplying by\n6, or 11, or minus pi, or whatever-- is\nthe size of C. Norms",
    "start": "605410",
    "end": "615100"
  },
  {
    "text": "have that nice property. They're homogeneous,\nor whatever word.",
    "start": "615100",
    "end": "620890"
  },
  {
    "text": "If you double the vector,\nyou should double the norm-- double the length. That makes sense.",
    "start": "620890",
    "end": "626329"
  },
  {
    "text": "And then the important\nproperty is that-- is the famous\ntriangle in equality--",
    "start": "626330",
    "end": "634100"
  },
  {
    "text": "that if v and w are two\nsides of a triangle,",
    "start": "634100",
    "end": "640959"
  },
  {
    "text": "and you take the norm of v\nand add to the norm of w-- the two sides-- you get more than the straight\nnorm along the hypotenuse.",
    "start": "640960",
    "end": "650210"
  },
  {
    "text": "Yeah. So those are properties\nthat we require, and the fact that the norm\nis positive, which is--",
    "start": "650210",
    "end": "659350"
  },
  {
    "text": "I won't write down. But it's important too. OK. So those are norms, and\nthose will apply also",
    "start": "659350",
    "end": "665269"
  },
  {
    "text": "to matrix norms. So if I double the matrix,\nI want to double its norm.",
    "start": "665270",
    "end": "672320"
  },
  {
    "text": "And of course, that\nworks for that 2 norm. And actually, probably-- so the\ntriangle in equality for this",
    "start": "672320",
    "end": "682130"
  },
  {
    "text": "norm is saying that the largest\nsingular value of A plus B--",
    "start": "682130",
    "end": "687170"
  },
  {
    "text": "two matrices-- is less\nor equal to the larger the singular value of A plus\nthe largest singular value of B.",
    "start": "687170",
    "end": "695210"
  },
  {
    "text": "And that's-- we won't take\nclass time to check minor,",
    "start": "695210",
    "end": "702770"
  },
  {
    "text": "straightforward\nthings like that. So now I'm going to continue\nwith the three norms",
    "start": "702770",
    "end": "708209"
  },
  {
    "start": "704000",
    "end": "818000"
  },
  {
    "text": "that I want to tell you about.  That's a very important one.",
    "start": "708210",
    "end": "714930"
  },
  {
    "text": "Then there is another\nnorm that's named-- has an F. And it's\nnamed after Frobenius.",
    "start": "714930",
    "end": "724180"
  },
  {
    "text": "Sorry about that.  And what is that norm?",
    "start": "724180",
    "end": "731530"
  },
  {
    "text": "That norm looks at all the\nentries in the matrix-- just like it was a long vector--",
    "start": "731530",
    "end": "737950"
  },
  {
    "text": "and squares them all,\nand adds them up. So in a way, it's like\nthe 2 norm for a vector.",
    "start": "737950",
    "end": "743830"
  },
  {
    "text": "It's-- so the squared-- or shall I put square root? Maybe I should.",
    "start": "743830",
    "end": "748959"
  },
  {
    "text": "It's the square root of all the\nlittle people in the matrix.",
    "start": "748960",
    "end": "755660"
  },
  {
    "text": "So a1, n squared, plus the\nnext a2, 1 squared, and so on.",
    "start": "755660",
    "end": "763600"
  },
  {
    "text": "You finally get\nto a-m-n squared. You just treat the matrix\nlike a long vector.",
    "start": "763600",
    "end": "772150"
  },
  {
    "text": "And take this square\nroot just like so.",
    "start": "772150",
    "end": "777510"
  },
  {
    "text": "That's the Frobenius norm. And then finally,\nnot so well known,",
    "start": "777510",
    "end": "785050"
  },
  {
    "text": "is something that's\nmore like L1. It's called the nuclear norm.",
    "start": "785050",
    "end": "792830"
  },
  {
    "text": " And not all the faculty would\nknow about this nuclear norm.",
    "start": "792830",
    "end": "799600"
  },
  {
    "text": "So it is the sum of the\nsigma of the singular values. ",
    "start": "799600",
    "end": "807129"
  },
  {
    "text": "I guess there are r of them. So that's where we would stop.",
    "start": "807130",
    "end": "812440"
  },
  {
    "text": "Oh, OK. So those are three norms. Now why do I pick on\nthose three norms?",
    "start": "812440",
    "end": "821480"
  },
  {
    "start": "818000",
    "end": "921000"
  },
  {
    "text": "And here's the point-- that for those three norms,\nthis statement is true.",
    "start": "821480",
    "end": "829230"
  },
  {
    "text": "I could cook up\nother matrix norms for which this wouldn't work. But for these three\nhighly important norms,",
    "start": "829230",
    "end": "836460"
  },
  {
    "text": "this Eckart-Young statement,\nthat the closest rank k",
    "start": "836460",
    "end": "841710"
  },
  {
    "text": "approximation is found\nfrom the first k pieces.",
    "start": "841710",
    "end": "846890"
  },
  {
    "text": "You see, that's a good\nthing, because this is what we compute from the SVD.",
    "start": "846890",
    "end": "853530"
  },
  {
    "text": "So now we've solved an\napproximation problem. We found the best B is Ak.",
    "start": "853530",
    "end": "861720"
  },
  {
    "text": "And the point is, it could use\nall the-- any of those norms. So there would be a--",
    "start": "861720",
    "end": "868410"
  },
  {
    "text": "well, somebody finally\ncame up with a proof that does all three norms at once.",
    "start": "868410",
    "end": "875709"
  },
  {
    "text": "In the notes, I do that one\nseparately from Frobenius.",
    "start": "875710",
    "end": "884400"
  },
  {
    "text": "And actually, I found-- in an MIT thesis-- I was just reading a\ncourse 6 PhD thesis--",
    "start": "884400",
    "end": "893070"
  },
  {
    "text": "and the author-- who is speaking\ntomorrow, or Friday in IDSS--",
    "start": "893070",
    "end": "901090"
  },
  {
    "text": " Dr. [? Cerebro ?] found a\nnice new proof of Frobenius.",
    "start": "901090",
    "end": "909120"
  },
  {
    "text": "And it's in the notes, as\nwell as an older proof.",
    "start": "909120",
    "end": "914370"
  },
  {
    "text": "OK. You know, as I talk\nhere, I'm not too sure",
    "start": "914370",
    "end": "920340"
  },
  {
    "text": "whether it is essential for\nme to go through the proof,",
    "start": "920340",
    "end": "927740"
  },
  {
    "start": "921000",
    "end": "1159000"
  },
  {
    "text": "either in the L2 norm-- which takes half a\npage in then notes--",
    "start": "927740",
    "end": "933740"
  },
  {
    "text": "or in the Frobenius\nnorm, which takes more. I'd rather you see the point.",
    "start": "933740",
    "end": "940920"
  },
  {
    "text": "The point is that, in\nthese norms-- and now, what is special about\nthese norms of a matrix?",
    "start": "940920",
    "end": "948879"
  },
  {
    "text": "These depend only\non the sigmas-- only on the-- oh. Oh. I'll finish that sentence,\nbecause it was true.",
    "start": "948880",
    "end": "956019"
  },
  {
    "text": "These norms depend only\non the singular values.",
    "start": "956020",
    "end": "961640"
  },
  {
    "text": "Right? That one, at least, depends\nonly on the singular value. It's the largest one. This one is the sum of them all.",
    "start": "961640",
    "end": "968870"
  },
  {
    "text": "This one comes into the Netflix\ncompetition, by the way. This was the right norm to win\na zillion dollars in the Netflix",
    "start": "968870",
    "end": "977120"
  },
  {
    "text": "competition. So what did Netflix put--\nit did a math competition.",
    "start": "977120",
    "end": "982640"
  },
  {
    "text": "It had movie preferences from\nmany, many Netflix subscribers.",
    "start": "982640",
    "end": "992940"
  },
  {
    "text": "They gave their ranking\nto a bunch of movies.",
    "start": "992940",
    "end": "998280"
  },
  {
    "text": "But of course,\nthey hadn't seen-- none of them had\nseen all the movies. So the matrix of rankings--",
    "start": "998280",
    "end": "1004370"
  },
  {
    "text": "where you had the\nranker and the matrix-- ",
    "start": "1004370",
    "end": "1009790"
  },
  {
    "text": "is a very big matrix. But it's got missing entries. If the ranker didn't see\nthe movie, he isn't--",
    "start": "1009790",
    "end": "1018020"
  },
  {
    "text": "he or she isn't ranking it. So what's the idea\nabout Netflix? So they offered like a\nmillion dollar prize.",
    "start": "1018020",
    "end": "1025490"
  },
  {
    "text": "And a lot of math and\ncomputer science people fought for that prize. And over the years, they got\nlike higher 92, 93, 94% right.",
    "start": "1025490",
    "end": "1038390"
  },
  {
    "text": "But it turned out\nthat this was-- well, you had to-- in the end, you had to\nuse a little psychology",
    "start": "1038390",
    "end": "1045649"
  },
  {
    "text": "of how people voted. So it was partly about\nhuman psychology. But it was also a very\nlarge matrix problem",
    "start": "1045650",
    "end": "1053419"
  },
  {
    "text": "with an incomplete matrix-- an incomplete matrix. And so it had to be completed.",
    "start": "1053420",
    "end": "1058550"
  },
  {
    "text": "You had to figure out\nwhat would the ranker have said about the post\nif he hadn't seen it,",
    "start": "1058550",
    "end": "1066679"
  },
  {
    "text": "but had ranked several\nother movies, like All",
    "start": "1066680",
    "end": "1073250"
  },
  {
    "text": "the President's\nMen, or whatever-- given a ranking to those? You have to-- and that's a\nrecommender system, of course.",
    "start": "1073250",
    "end": "1080900"
  },
  {
    "text": "That's how you get\nrecommendations from Amazon. They've got a big\nmatrix calculation here.",
    "start": "1080900",
    "end": "1089429"
  },
  {
    "text": "And if you've bought a\ncouple of math books, they're going to tell you\nabout more math books--",
    "start": "1089430",
    "end": "1095750"
  },
  {
    "text": "more than you want to know. Right. OK. So anyway, it just\nturned out that this norm",
    "start": "1095750",
    "end": "1104770"
  },
  {
    "text": "was the right one to minimize. I can't give you all the details\nof the Netflix competition,",
    "start": "1104770",
    "end": "1112240"
  },
  {
    "text": "but this turned out\nto be the right norm to do a minimum problem,\na best not least squares.",
    "start": "1112240",
    "end": "1118870"
  },
  {
    "text": "These squares would\nlook at some other norm, but a best nuclear norm\ncompletion of the matrix.",
    "start": "1118870",
    "end": "1127030"
  },
  {
    "text": "And that-- and now it's--",
    "start": "1127030",
    "end": "1132290"
  },
  {
    "text": "so now it's being put to much\nmore serious uses for MRI--",
    "start": "1132290",
    "end": "1140320"
  },
  {
    "text": "magnetic resonance stuff,\nwhen you go in and get--",
    "start": "1140320",
    "end": "1145480"
  },
  {
    "text": "it's a noisy system,\nbut you get-- it gives a excellent\npicture of what's going on.",
    "start": "1145480",
    "end": "1159190"
  },
  {
    "start": "1159000",
    "end": "1226000"
  },
  {
    "text": "So I'll just write Netflix here. So it gets in the--",
    "start": "1159190",
    "end": "1165120"
  },
  {
    "text": "and then MRIs. ",
    "start": "1165120",
    "end": "1170780"
  },
  {
    "text": "So what's the point about MRIs?  So if you don't--",
    "start": "1170780",
    "end": "1176300"
  },
  {
    "text": "if you stay in long enough,\nyou get all the numbers. There isn't missing data.",
    "start": "1176300",
    "end": "1182779"
  },
  {
    "text": "But if you-- as with a child-- you might want to\njust have the child in for a few\nminutes, then that's",
    "start": "1182780",
    "end": "1190250"
  },
  {
    "text": "not enough to get\na complete picture. And you have,\nagain, missing data",
    "start": "1190250",
    "end": "1195350"
  },
  {
    "text": "in your matrix in the\nimage from the MRI.",
    "start": "1195350",
    "end": "1205760"
  },
  {
    "text": "So then, of course, you've\ngot to complete that matrix. You have to fill in,\nwhat would the MRI have",
    "start": "1205760",
    "end": "1213320"
  },
  {
    "text": "seen in those positions where\nit didn't look long enough? And again, a nuclear norm\nis a good one for that.",
    "start": "1213320",
    "end": "1225419"
  },
  {
    "text": "OK. So there will be a whole section\non norms, maybe just about--",
    "start": "1225420",
    "end": "1234390"
  },
  {
    "start": "1226000",
    "end": "1769000"
  },
  {
    "text": "in stellar by now. OK.",
    "start": "1234390",
    "end": "1239880"
  },
  {
    "text": "So I'm not going to-- let me just say,\nwhat does this say?",
    "start": "1239880",
    "end": "1247020"
  },
  {
    "text": "What does this tell us? I'll just give an example.",
    "start": "1247020",
    "end": "1252380"
  },
  {
    "text": "Maybe I'll take-- start with\nthe example that's in the notes. Suppose k is 2.",
    "start": "1252380",
    "end": "1259440"
  },
  {
    "text": "So I'm looking among\nall rank 2 matrices. And suppose my matrix is 4,\n3, 2, 1, and all the rest 0's.",
    "start": "1259440",
    "end": "1269470"
  },
  {
    "text": " Diagonal. ",
    "start": "1269470",
    "end": "1276410"
  },
  {
    "text": "And it's rank 4 matrix. I can see its singular values. They're sitting there.",
    "start": "1276410",
    "end": "1282970"
  },
  {
    "text": "Those would be the singular\nvalues, and the eigenvalues, and everything, of course.",
    "start": "1282970",
    "end": "1288140"
  },
  {
    "text": "Now, what would be A2? What would be the\nbest approximation",
    "start": "1288140",
    "end": "1293929"
  },
  {
    "text": "of rank 2 to that matrix, in\nthis sense to be completed?",
    "start": "1293930",
    "end": "1304240"
  },
  {
    "text": "What would A2 do? Yeah. It would be 4 and 3. It would pick the two largest.",
    "start": "1304240",
    "end": "1311520"
  },
  {
    "text": "So I'm looking at Ak. This is k to the 2, so\nit has to have rank 2.",
    "start": "1311520",
    "end": "1317130"
  },
  {
    "text": "This has got rank 4. The biggest pieces are those. ",
    "start": "1317130",
    "end": "1324280"
  },
  {
    "text": "OK. So this thing says that if\nI had any other matrix B,",
    "start": "1324280",
    "end": "1331490"
  },
  {
    "text": "it would be further away\nfrom A than this guy. It says that this\nis the closest.",
    "start": "1331490",
    "end": "1337390"
  },
  {
    "text": "And I just-- could you think\nof a matrix that could possibly",
    "start": "1337390",
    "end": "1343180"
  },
  {
    "text": "be closer, and be rank 2? Rank two 2 the tricky thing.",
    "start": "1343180",
    "end": "1348700"
  },
  {
    "text": "The matrices of rank 2\nform a kind of crazy set. If I add a rank 2 matrix\nto a rank 2 matrix,",
    "start": "1348700",
    "end": "1356230"
  },
  {
    "text": "probably the rank is up to 4. So the rank 2 matrices are\nall kind of floating around",
    "start": "1356230",
    "end": "1364140"
  },
  {
    "text": "in their own little corners. This looks like the best one. But in the notes I suggest,\nwell, you could get a rank 2--",
    "start": "1364140",
    "end": "1375410"
  },
  {
    "text": "well, what about B? What about this B? For this guy, I\ncould get closer--",
    "start": "1375410",
    "end": "1385210"
  },
  {
    "text": "maybe not exact-- but closer,\nmaybe by taking 3.5, 3.5.",
    "start": "1385210",
    "end": "1395210"
  },
  {
    "text": "But I only want to use rank-- I've only got two\nrank 2 to play with. So I better make\nthis into a rank--",
    "start": "1395210",
    "end": "1401690"
  },
  {
    "text": "I have to make this into\na rank 1 piece, and then the 2 and the 1.",
    "start": "1401690",
    "end": "1407870"
  },
  {
    "text": "So you see what I-- what I thought of? I thought, man,\nmaybe that's better--",
    "start": "1407870",
    "end": "1413060"
  },
  {
    "text": "like on the diagonal,\nI'm coming closer. Well, I'm not getting\nit exactly here.",
    "start": "1413060",
    "end": "1419750"
  },
  {
    "text": "But then I've got one\nleft to play with. And I'll put, maybe,\n1.5 down here.",
    "start": "1419750",
    "end": "1425480"
  },
  {
    "text": " OK.",
    "start": "1425480",
    "end": "1430770"
  },
  {
    "text": "So that's a rank 2 matrix-- two little rank 1s. And on the diagonal,\nit's better.",
    "start": "1430770",
    "end": "1438299"
  },
  {
    "text": "3.5s-- I'm only\nmissing by a half. 1.5s-- I'm missing by half.",
    "start": "1438300",
    "end": "1443820"
  },
  {
    "text": "So I'm only missing by\na half on the diagonal where this guy was missing by 2.",
    "start": "1443820",
    "end": "1450010"
  },
  {
    "text": "So maybe I've found\nsomething better. But I had to pay a price of\nthese things off the diagonal",
    "start": "1450010",
    "end": "1458000"
  },
  {
    "text": "to keep the rank low. And they kill me. So that B will be\nfurther away from A.",
    "start": "1458000",
    "end": "1466960"
  },
  {
    "text": "The error, if I computed A\nminus B, and computed its norm,",
    "start": "1466960",
    "end": "1472190"
  },
  {
    "text": "I would see bigger\nthan A minus A2.",
    "start": "1472190",
    "end": "1477450"
  },
  {
    "text": "Yeah. So, you see the\npoint of the theorem? That's really what I'm trying\nto say, that it's not obvious.",
    "start": "1477450",
    "end": "1484700"
  },
  {
    "text": "You may feel, well,\nit's totally obvious. Pick 4 and 3. What else could do it?",
    "start": "1484700",
    "end": "1490800"
  },
  {
    "text": "But it depends on\nthe norm and so on. So it's not--",
    "start": "1490800",
    "end": "1495940"
  },
  {
    "text": "Eckart-Young had to think of a\nproof, and other people, too. OK.",
    "start": "1495940",
    "end": "1501100"
  },
  {
    "text": "So that's-- now, but you\ncould say-- also say-- object that I started with\na diagonal matrix here.",
    "start": "1501100",
    "end": "1509200"
  },
  {
    "text": "That's so special. But what I want to say\nis the diagonal matrix is not that special,\nbecause I could take A--",
    "start": "1509200",
    "end": "1520370"
  },
  {
    "text": "so let me now just call\nthis diagonal matrix D-- or let me call it\nsigma to give it",
    "start": "1520370",
    "end": "1525880"
  },
  {
    "text": "another sort of\nappropriate name. ",
    "start": "1525880",
    "end": "1531960"
  },
  {
    "text": "So if I thought of\nmatrices, what I want to say",
    "start": "1531960",
    "end": "1537120"
  },
  {
    "text": "is, this could be\nthe sigma matrix.",
    "start": "1537120",
    "end": "1542210"
  },
  {
    "text": "And there could be a\nU on the left of it, and a sigma on the right of it.",
    "start": "1542210",
    "end": "1547680"
  },
  {
    "text": "So A is U sigma V transpose. So this is my sigma.",
    "start": "1547680",
    "end": "1555460"
  },
  {
    "text": "And this is like any\northogonal matrix U. And this is like\nany V transpose.",
    "start": "1555460",
    "end": "1563146"
  },
  {
    "start": "1563146",
    "end": "1568900"
  },
  {
    "text": "Right? I'm just saying, here's a\nwhole lot more matrices. There is just one matrix.",
    "start": "1568900",
    "end": "1574450"
  },
  {
    "text": "But now, I have\nall these matrices with Us multiplying on the\nleft, and V transpose ones",
    "start": "1574450",
    "end": "1580830"
  },
  {
    "text": "on the right. And I ask you this\nquestion, what are the singular values\nof that matrix, A?",
    "start": "1580830",
    "end": "1588639"
  },
  {
    "text": "Here the singular\nvalues were clear-- 4, 3, 2, and 1. What are the singular\nvalues of this matrix A,",
    "start": "1588640",
    "end": "1595150"
  },
  {
    "text": "when I've multiplied by a\northogonal guy on both sides?",
    "start": "1595150",
    "end": "1600720"
  },
  {
    "text": "That's a key question. What are the singular\nvalues of that one?",
    "start": "1600720",
    "end": "1606264"
  },
  {
    "text": "AUDIENCE: 4, 3, 2, 1. GILBERT STRANG: 4, 3, 2, 1. Didn't change. Why is that?",
    "start": "1606265",
    "end": "1611700"
  },
  {
    "text": "Because the singular\nvalues are the-- because this has a SVD form--",
    "start": "1611700",
    "end": "1617980"
  },
  {
    "text": "orthogonal times diagonal\ntimes orthogonal. And that diagonal contains\nthe singular values.",
    "start": "1617980",
    "end": "1624290"
  },
  {
    "text": "What I'm saying is, that my-- and our-- trivial little\nexample here, actually",
    "start": "1624290",
    "end": "1631390"
  },
  {
    "text": "was all 4 by 4's that have\nthese singular values. I could-- my whole problem\nis orthogonally invariant,",
    "start": "1631390",
    "end": "1641850"
  },
  {
    "text": "a math guy would say. When I multiply by U or\na V transpose, or both--",
    "start": "1641850",
    "end": "1647540"
  },
  {
    "text": "the problem doesn't change. Norms don't change. Yeah, that's a point. Yeah. I realize it now.",
    "start": "1647540",
    "end": "1653480"
  },
  {
    "text": "This is the point. If I multiply the matrix A\nby an orthogonal matrix U,",
    "start": "1653480",
    "end": "1662740"
  },
  {
    "text": "it has all the same norms-- doesn't change the norm. Actually, that was true way back\nfor vectors with this length--",
    "start": "1662740",
    "end": "1673790"
  },
  {
    "text": "with this length. What's the deal about vectors? Suppose I have a vector\nV, and I've computed",
    "start": "1673790",
    "end": "1680830"
  },
  {
    "text": "its hypotenuse and the norm. And now I look at Q times\nV in that same 2 norm.",
    "start": "1680830",
    "end": "1689230"
  },
  {
    "text": " What's special about that?",
    "start": "1689230",
    "end": "1695880"
  },
  {
    "text": "So I took any vector V and\nI know what its length is-- hypotenuse.",
    "start": "1695880",
    "end": "1701400"
  },
  {
    "text": "Now I multiply by Q. What happens to the length?",
    "start": "1701400",
    "end": "1706540"
  },
  {
    "text": "Doesn't change. Doesn't change. Orthogonal matrix--\nyou could think",
    "start": "1706540",
    "end": "1712990"
  },
  {
    "text": "of it as just like rotating\nthe triangle in space. The hypotenuse doesn't change.",
    "start": "1712990",
    "end": "1718820"
  },
  {
    "text": "And we've checked that,\nbecause we could-- the check is to square it.",
    "start": "1718820",
    "end": "1724720"
  },
  {
    "text": "And then you're doing\nQV, transpose QV.",
    "start": "1724720",
    "end": "1730669"
  },
  {
    "text": "And you simplify\nit the usual way. And then you have Q transpose\nQ equal the identity.",
    "start": "1730670",
    "end": "1737240"
  },
  {
    "text": "And you're golden. Yeah. So the result is you get\nthe same answer as V.",
    "start": "1737240",
    "end": "1750200"
  },
  {
    "text": "So let me put it in a\nsentence now, pause.",
    "start": "1750200",
    "end": "1756409"
  },
  {
    "text": "Multiplying that norm is not\nchanged by orthogonal matrix.",
    "start": "1756410",
    "end": "1764930"
  },
  {
    "text": "And these norms are not\nchanged by orthogonal matrices, because if I multiply the A\nhere by an orthogonal matrix,",
    "start": "1764930",
    "end": "1773019"
  },
  {
    "start": "1769000",
    "end": "1899000"
  },
  {
    "text": "I have-- this is my A. If\ni multiply by a Q,",
    "start": "1773020",
    "end": "1779102"
  },
  {
    "text": "then I have QU\nsigma V transpose. And what is really\nthe underlying point?",
    "start": "1779102",
    "end": "1786550"
  },
  {
    "text": "That QU is an orthogonal matrix\njust as good as U. So if I--",
    "start": "1786550",
    "end": "1793495"
  },
  {
    "text": "let me put this down. QA would be QU\nsigma V transpose.",
    "start": "1793495",
    "end": "1801490"
  },
  {
    "text": "And now I'm asking you,\nwhat's the singular value decomposition for QA?",
    "start": "1801490",
    "end": "1808448"
  },
  {
    "text": "And I hope I may\nactually-- seeing it. What's the singular value\ndecomposition of QA?",
    "start": "1808448",
    "end": "1815750"
  },
  {
    "text": "What are the singular values? What's the diagonal matrix? Just look there for it.",
    "start": "1815750",
    "end": "1821920"
  },
  {
    "text": "The diagram matrix is sigma. What goes on the right of it? The V transpose.",
    "start": "1821920",
    "end": "1827270"
  },
  {
    "text": "And what goes on the\nleft of it is QU. Plus, that's orthogonal\ntimes orthogonal.",
    "start": "1827270",
    "end": "1833460"
  },
  {
    "text": "Everybody in this\nroom has to know that if I multiply two\northogonal matrices,",
    "start": "1833460",
    "end": "1838740"
  },
  {
    "text": "the result is,\nagain, orthogonal. So I can multiply by Q, and it\nonly affects the U part, not",
    "start": "1838740",
    "end": "1846345"
  },
  {
    "text": "the sigma part. And so it doesn't change\nany of those norms.",
    "start": "1846345",
    "end": "1851600"
  },
  {
    "text": "OK. So that's fine. That's what I wanted to\nsay about the Eckart-Young",
    "start": "1851600",
    "end": "1859140"
  },
  {
    "text": "Theorem-- not proving it, but\nhopefully giving you",
    "start": "1859140",
    "end": "1864330"
  },
  {
    "text": "an example there\nof what it means-- that this is the best rank\nto approximate that one.",
    "start": "1864330",
    "end": "1874559"
  },
  {
    "text": "OK. So that's the key\nmath behind PCA.",
    "start": "1874560",
    "end": "1884220"
  },
  {
    "text": "So now I have to-- want to, not just have to--\nbut want to tell you about PCA.",
    "start": "1884220",
    "end": "1891210"
  },
  {
    "text": "So what's that about? So we have a bunch of\ndata, and we want to see--",
    "start": "1891210",
    "end": "1900640"
  },
  {
    "start": "1899000",
    "end": "2134000"
  },
  {
    "text": "so let me take a bunch of data-- bunch of data points--",
    "start": "1900640",
    "end": "1906341"
  },
  {
    "text": "say, points in the plane. So I have a bunch of\ndata points in the plane.",
    "start": "1906342",
    "end": "1913200"
  },
  {
    "text": "So here's my data vector. First, vector x1-- well, x. Is at a good--",
    "start": "1913200",
    "end": "1918970"
  },
  {
    "text": "maybe v1.  These are just two\ncomponent guys.",
    "start": "1918970",
    "end": "1924450"
  },
  {
    "text": "v2. They're just columns\nwith two components. So I'm just measuring\nheight and age,",
    "start": "1924450",
    "end": "1934440"
  },
  {
    "text": "and I want to find the\nrelationship between height and age. So the first row is meant--\nis the height of my data.",
    "start": "1934440",
    "end": "1943830"
  },
  {
    "text": "And the second row is the ages. So these are-- so I've\ngot say a lot of people,",
    "start": "1943830",
    "end": "1951214"
  },
  {
    "text": "and these are the heights\nand these are the ages.",
    "start": "1951214",
    "end": "1958160"
  },
  {
    "text": "And I've got n points in 2D. ",
    "start": "1958160",
    "end": "1969050"
  },
  {
    "text": "And I want to make\nsense out of that. I want to look for the\nrelationship between height",
    "start": "1969050",
    "end": "1974690"
  },
  {
    "text": "and age. I'm actually going to look\nfor a linear row relation between height and age.",
    "start": "1974690",
    "end": "1983160"
  },
  {
    "text": "So first of all, these\nare all over the place.",
    "start": "1983160",
    "end": "1989250"
  },
  {
    "text": "So the first step that\na statistician does, is to get mean 0.",
    "start": "1989250",
    "end": "1997150"
  },
  {
    "text": "Get the average to be 0. So what is-- so all these\npoints are all over the place.",
    "start": "1997150",
    "end": "2004330"
  },
  {
    "text": "From row 1, the height, I\nsubtract the average height. So this is A-- the matrix I'm\nreally going to work on is",
    "start": "2004330",
    "end": "2012850"
  },
  {
    "text": "my matrix A-- minus\nthe average height-- well, in all components.",
    "start": "2012850",
    "end": "2020830"
  },
  {
    "text": "So this is a, a, a, a--",
    "start": "2020830",
    "end": "2026460"
  },
  {
    "text": "I'm subtracting the mean, so\naverage height and average age.",
    "start": "2026460",
    "end": "2035510"
  },
  {
    "text": "Oh, that was a\nbrilliant notation, a sub a can't be a sub a.",
    "start": "2035510",
    "end": "2044600"
  },
  {
    "text": "You see what the\nmatrix has done-- this matrix 2 means?",
    "start": "2044600",
    "end": "2050270"
  },
  {
    "text": "It's just made each row\nof A. Now adds to row.",
    "start": "2050270",
    "end": "2059149"
  },
  {
    "text": "Now add to what? ",
    "start": "2059150",
    "end": "2064969"
  },
  {
    "text": "If I have a bunch\nof things, and I've subtracted off their mean-- so the mean, or the\naverage is now 0--",
    "start": "2064969",
    "end": "2072679"
  },
  {
    "text": "then those things add up to-- AUDIENCE: Zero. GILBERT STRANG: Zero. Right.",
    "start": "2072679",
    "end": "2077750"
  },
  {
    "text": "I've just brought these points\ninto something like here.",
    "start": "2077750",
    "end": "2083360"
  },
  {
    "text": "This is age, and this is height.",
    "start": "2083360",
    "end": "2090460"
  },
  {
    "text": "And let's see. And by subtracting,\nit no longer is",
    "start": "2090460",
    "end": "2097359"
  },
  {
    "text": "unreasonable to have negative\nage and negative height, because--",
    "start": "2097360",
    "end": "2103810"
  },
  {
    "text": "so, right. The little kids, when I\nsubtract it off the average age,",
    "start": "2103810",
    "end": "2112170"
  },
  {
    "text": "they ended up with\na negative age. The older ones ended\nup still positive.",
    "start": "2112170",
    "end": "2117960"
  },
  {
    "text": "And somehow, I've got\na whole lot of points, but hopefully, their\nmean is now zero.",
    "start": "2117960",
    "end": "2130630"
  },
  {
    "text": "Do you see that I've\ncentered the data at 0, 0? And I'm looking for-- what\nam I looking for here?",
    "start": "2130630",
    "end": "2139030"
  },
  {
    "start": "2134000",
    "end": "2223000"
  },
  {
    "text": "I'm looking for the best line.  That's what I want to find.",
    "start": "2139030",
    "end": "2148589"
  },
  {
    "text": "And that would be\na problem in PCA. What's the best linear relation?",
    "start": "2148590",
    "end": "2153839"
  },
  {
    "text": "Because PCA is limited. PCA isn't all of deep\nlearning by any means.",
    "start": "2153840",
    "end": "2159210"
  },
  {
    "text": "The whole success\nof deep learning was the final realization,\nafter a bunch of years,",
    "start": "2159210",
    "end": "2165030"
  },
  {
    "text": "that they had to have a\nnonlinear function in there to get to model serious data.",
    "start": "2165030",
    "end": "2174340"
  },
  {
    "text": "But here's PCA as\na linear business. And I'm looking\nfor the best line.",
    "start": "2174340",
    "end": "2181740"
  },
  {
    "text": " And you will say, wait a minute.",
    "start": "2181740",
    "end": "2188329"
  },
  {
    "text": "I know how to find the best\nline, just use least squares.",
    "start": "2188330",
    "end": "2194610"
  },
  {
    "text": "Gauss did it. Can't be all bad. But PCA-- and I\nwas giving a talk",
    "start": "2194610",
    "end": "2202369"
  },
  {
    "text": "in New York when I was\njust learning about it. And somebody said,\nwhat you're doing",
    "start": "2202370",
    "end": "2208790"
  },
  {
    "text": "with PCA has to be the\nsame as least squares-- it's finding the best line. And I knew it wasn't,\nbut I didn't know how",
    "start": "2208790",
    "end": "2216970"
  },
  {
    "text": "to answer that question best. And now, at least,\nI know better.",
    "start": "2216970",
    "end": "2223580"
  },
  {
    "start": "2223000",
    "end": "2432000"
  },
  {
    "text": "So the best line\nin least squares-- can I remind you\nabout least squares? Because this is\nnot least squares.",
    "start": "2223580",
    "end": "2230600"
  },
  {
    "text": "The best line of least squares-- so I have some data points. And I have a best line\nthat goes through them.",
    "start": "2230600",
    "end": "2238610"
  },
  {
    "text": "And least squares, I don't\nalways center the data to mean zero, but I could.",
    "start": "2238610",
    "end": "2245390"
  },
  {
    "text": "But what do you minimize\nin least squares-- least squares?",
    "start": "2245390",
    "end": "2250750"
  },
  {
    "text": " If you remember the\npicture in linear algebra",
    "start": "2250750",
    "end": "2257050"
  },
  {
    "text": "books of least squares,\nyou measure the errors-- the three errors.",
    "start": "2257050",
    "end": "2263559"
  },
  {
    "text": "And it's how much you're\nwrong at those three points.",
    "start": "2263560",
    "end": "2269020"
  },
  {
    "text": " Those are the three errors.",
    "start": "2269020",
    "end": "2274890"
  },
  {
    "text": "A-- difference\nbetween Ax and B-- the B minus Ax that you square.",
    "start": "2274890",
    "end": "2285150"
  },
  {
    "text": "And you add up\nthose three errors. And what's different over here?",
    "start": "2285150",
    "end": "2292000"
  },
  {
    "text": "I mean, there's more points,\nbut that's not the point. That's not the difference.",
    "start": "2292000",
    "end": "2297500"
  },
  {
    "text": "The difference is, in PCA,\nyou're measuring perpendicular",
    "start": "2297500",
    "end": "2303550"
  },
  {
    "text": "to the line. You're adding up all these\nlittle guys, squaring them.",
    "start": "2303550",
    "end": "2310570"
  },
  {
    "text": "So you're adding up their\nsquares and minimizing. So the points-- you see\nit's a different problem?",
    "start": "2310570",
    "end": "2318039"
  },
  {
    "text": "And therefore it has\na different answer. And this answer turns out to\ninvolve the SVD, the sigmas.",
    "start": "2318040",
    "end": "2333770"
  },
  {
    "text": "Where this answer, you remember\nfrom ordinary linear algebra, just when you\nminimize that, you got",
    "start": "2333770",
    "end": "2341420"
  },
  {
    "text": "to an equation that leads to\nwhat equation for the best x?",
    "start": "2341420",
    "end": "2347520"
  },
  {
    "text": "So do you remember? AUDIENCE: [INAUDIBLE] GILBERT STRANG: Yeah. What is it now?",
    "start": "2347520",
    "end": "2352589"
  },
  {
    "text": "Everybody should know. And we will actually\nsee it in this course,",
    "start": "2352590",
    "end": "2359280"
  },
  {
    "text": "because we're doing the\nheart of linear algebra here. We haven't done it yet, though.",
    "start": "2359280",
    "end": "2364859"
  },
  {
    "text": "And tell me again, what equation\ndo I solve for that problem?",
    "start": "2364860",
    "end": "2370460"
  },
  {
    "text": "AUDIENCE: A transpose A. GILBERT STRANG: A transpose\nA x hat equal A transpose b.",
    "start": "2370460",
    "end": "2377099"
  },
  {
    "text": " Called the normal equations.",
    "start": "2377100",
    "end": "2382150"
  },
  {
    "start": "2382150",
    "end": "2387460"
  },
  {
    "text": "It's sort part of-- it's this regression\nin statistics language.",
    "start": "2387460",
    "end": "2396740"
  },
  {
    "text": "That's a regression problem. This is a different problem. OK. Just so now you see the answer.",
    "start": "2396740",
    "end": "2404690"
  },
  {
    "text": "So that involves--\nwell, they both involve A transpose A.\nThat's sort of interesting,",
    "start": "2404690",
    "end": "2410290"
  },
  {
    "text": "because you have a\nrectangular matrix A, and then sooner or later,\nA transpose A is coming.",
    "start": "2410290",
    "end": "2416290"
  },
  {
    "text": "But this involves solving a\nlinear system of equations. So it's fast.",
    "start": "2416290",
    "end": "2423000"
  },
  {
    "text": "And we will do it. And it's very important. It's probably the most\nimportant application in 18.06.",
    "start": "2423000",
    "end": "2433650"
  },
  {
    "text": "But it's not the\nsame as this one. So this is now in 18.06,\nmaybe the last day is PCA.",
    "start": "2433650",
    "end": "2442920"
  },
  {
    "text": "So I didn't put those letters-- Principal Component\nAnalysis-- PCA.",
    "start": "2442920",
    "end": "2453905"
  },
  {
    "text": " Which statisticians have\nbeen doing for a long time.",
    "start": "2453905",
    "end": "2459519"
  },
  {
    "text": "We're not doing\nsomething brand new here. But the result is that we--",
    "start": "2459520",
    "end": "2468589"
  },
  {
    "text": "so how does a statistician\nthink about this problem, or that data matrix?",
    "start": "2468590",
    "end": "2476370"
  },
  {
    "text": "What-- if you have\na matrix of data-- 2 by 2 rows and many columns--\nso many, many samples--",
    "start": "2476370",
    "end": "2488190"
  },
  {
    "text": "what-- and we've\nmade the mean zero. So that's a first\nstep a statistician",
    "start": "2488190",
    "end": "2494040"
  },
  {
    "text": "takes to check on the mean. What's the next step?",
    "start": "2494040",
    "end": "2499130"
  },
  {
    "text": "What else does a\nstatistician do with data to measure how-- its size?",
    "start": "2499130",
    "end": "2505400"
  },
  {
    "text": "There's another number. There's a number that\ngoes with the mean, and it's the variance--",
    "start": "2505400",
    "end": "2511900"
  },
  {
    "text": "the mean and the variance. So somehow we're\ngoing to do variances. And it will really be\ninvolved, because we",
    "start": "2511900",
    "end": "2519460"
  },
  {
    "text": "have two sets of data--\nheights and ages. We're really going to\nhave a covariance--",
    "start": "2519460",
    "end": "2526090"
  },
  {
    "text": "covariance matrix--\nand it will be 2 by 2.",
    "start": "2526090",
    "end": "2535340"
  },
  {
    "text": " Because it will tell us not only\nthe variance in the heights--",
    "start": "2535340",
    "end": "2542339"
  },
  {
    "text": "that's the first\nthing a statistician would think about-- some small people,\nsome big people--",
    "start": "2542340",
    "end": "2548910"
  },
  {
    "text": "and variation in ages-- but also the link between them. How are the height, age pairs--",
    "start": "2548910",
    "end": "2557190"
  },
  {
    "text": "does more height-- does more\nage go with more height? And of course, it does.",
    "start": "2557190",
    "end": "2562410"
  },
  {
    "text": "That's the whole point here. So it's this covariance matrix. And that covariance matrix--",
    "start": "2562410",
    "end": "2568530"
  },
  {
    "text": "or the sample covariance matrix,\nto give it its full name-- ",
    "start": "2568530",
    "end": "2575869"
  },
  {
    "text": "what's the-- so just touching\non statistics for a moment here. What's the-- when we see that\nword sample in the name, what",
    "start": "2575870",
    "end": "2586049"
  },
  {
    "text": "is that telling us? It's telling us that this matrix\nis computed from the samples,",
    "start": "2586050",
    "end": "2593970"
  },
  {
    "text": "not from a theoretical\nprobability distribution. We might have a\nproposed distribution",
    "start": "2593970",
    "end": "2601830"
  },
  {
    "text": "that the height\nfollows the age-- height follows the\nage by some formula.",
    "start": "2601830",
    "end": "2608250"
  },
  {
    "text": "And that would give us\ntheoretical variances.",
    "start": "2608250",
    "end": "2614340"
  },
  {
    "text": "We're doing sample\nvariances, also called empirical covariance made.",
    "start": "2614340",
    "end": "2620430"
  },
  {
    "text": "Empirical says--\nempirical-- that word means, from the\ninformation, from the data.",
    "start": "2620430",
    "end": "2625930"
  },
  {
    "text": "So that's what we do. And it is exactly-- it's AA transpose.",
    "start": "2625930",
    "end": "2632020"
  },
  {
    "start": "2632020",
    "end": "2637610"
  },
  {
    "text": "You have to normalize it by\nthe number of data points, N.",
    "start": "2637610",
    "end": "2646680"
  },
  {
    "text": "And then, for some reason-- best known to statisticians--",
    "start": "2646680",
    "end": "2652520"
  },
  {
    "text": "it's N minus 1. ",
    "start": "2652520",
    "end": "2657950"
  },
  {
    "text": "And of course, they've\ngot to be right. They've been around a long time\nand it should be N minus 1,",
    "start": "2657950",
    "end": "2663440"
  },
  {
    "text": "because somehow 1\ndegree of freedom was accounted for\nwhen we took away--",
    "start": "2663440",
    "end": "2670930"
  },
  {
    "text": "when we made the mean 0. So we-- anyway, no problem.",
    "start": "2670930",
    "end": "2676800"
  },
  {
    "text": "But the N minus 1 is not going\nto affect our computation here.",
    "start": "2676800",
    "end": "2683180"
  },
  {
    "text": "This is the matrix\nthat tells us that's what we've got to work with.",
    "start": "2683180",
    "end": "2690820"
  },
  {
    "text": "That's what we've got to work\nwith-- the matrix AA transpose. And then the-- so we\nhave this problem.",
    "start": "2690820",
    "end": "2700220"
  },
  {
    "text": "So we have a-- yeah. I guess we really have\na minimum problem. We want to find--",
    "start": "2700220",
    "end": "2707160"
  },
  {
    "text": "yeah. What problem are we solving? And it's-- yeah.",
    "start": "2707160",
    "end": "2712820"
  },
  {
    "text": "So our problem was\nnot least squares-- not the same as least squares.",
    "start": "2712820",
    "end": "2721619"
  },
  {
    "text": "Similar, but not the same. We want to minimize. So we're looking\nfor that best line",
    "start": "2721620",
    "end": "2729520"
  },
  {
    "text": "where age equals some number,\nc, times the height, times the--",
    "start": "2729520",
    "end": "2738420"
  },
  {
    "text": "yeah-- or height. Maybe it would have been better\nto age here and height here. No. No, because there\nare two unknowns.",
    "start": "2738420",
    "end": "2746510"
  },
  {
    "text": "So I'm looking for c. I'm looking for the number c-- looking for the number c.",
    "start": "2746510",
    "end": "2752369"
  },
  {
    "text": "And with just two minutes in\nclass left, what is that number",
    "start": "2752370",
    "end": "2758870"
  },
  {
    "text": "c going to be, when I finally\nget the problem stated properly, and then solve it?",
    "start": "2758870",
    "end": "2766720"
  },
  {
    "text": "I'm going to learn that the\nbest ratio of age to height is sigma 1.",
    "start": "2766720",
    "end": "2773980"
  },
  {
    "text": "Sigma 1. That's the one that tells us\nhow those two are connected,",
    "start": "2773980",
    "end": "2781280"
  },
  {
    "text": "and the orthogonal-- and\nwhat will be the best--",
    "start": "2781280",
    "end": "2786820"
  },
  {
    "text": "yeah. No. Maybe I didn't answer\nthat the right-- maybe I didn't get that right.",
    "start": "2786820",
    "end": "2793730"
  },
  {
    "text": "Because I'm looking for-- I'm looking for the vector\nthat points in the right way.",
    "start": "2793730",
    "end": "2800520"
  },
  {
    "text": "Yeah. I'm sorry.  I think the answer is, it's\ngot to be there in the SVD.",
    "start": "2800520",
    "end": "2808490"
  },
  {
    "text": "I think it's the\nvector you want. It's the principal\ncomponent you want.",
    "start": "2808490",
    "end": "2814280"
  },
  {
    "text": "Let's do that\nproperly on Friday. I hope you see-- because this was\na first step away",
    "start": "2814280",
    "end": "2823250"
  },
  {
    "text": "from the highlights\nof linear algebra to problem solve\nby linear algebra,",
    "start": "2823250",
    "end": "2828740"
  },
  {
    "text": "and practical\nproblems, and my point is that the SVD solves these.",
    "start": "2828740",
    "end": "2835305"
  },
  {
    "start": "2835305",
    "end": "2835805"
  }
]