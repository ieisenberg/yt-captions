[
  {
    "start": "0",
    "end": "271000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  PROFESSOR STRANG:\n[INAUDIBLE] Professor Suvrit",
    "start": "18140",
    "end": "24020"
  },
  {
    "text": "Sra from EECS who taught 6.036\nand the graduate version.",
    "start": "24020",
    "end": "31480"
  },
  {
    "text": "And maybe some of you had him in\none or other of those classes.",
    "start": "31480",
    "end": "37750"
  },
  {
    "text": "So he graciously\nagreed to come today and to talk about Stochastic\nGradient Descent, SGD.",
    "start": "37750",
    "end": "45940"
  },
  {
    "text": " And it's terrific.",
    "start": "45940",
    "end": "51340"
  },
  {
    "text": "Yeah, yeah. So we're not quite\nat 1:05, but close.",
    "start": "51340",
    "end": "56960"
  },
  {
    "text": " If everything is\nready, then we're off.",
    "start": "56960",
    "end": "63750"
  },
  {
    "text": "OK. Good. PROFESSOR SRA: And your\ncutoff is like 1:55?",
    "start": "63750",
    "end": "69832"
  },
  {
    "text": "PROFESSOR STRANG: Yeah. PROFESSOR SRA: OK. PROFESSOR STRANG: But this\nis not a sharp cutoff.",
    "start": "69832",
    "end": "75486"
  },
  {
    "text": "PROFESSOR SRA: Why is there\n[INAUDIBLE] fluctuation? PROFESSOR STRANG: There you go.",
    "start": "75486",
    "end": "81903"
  },
  {
    "text": "PROFESSOR SRA: Somebody changed\ntheir resolution it seems, but that's fine.",
    "start": "81903",
    "end": "87100"
  },
  {
    "text": "It doesn't bother us. So I'm going to tell you about,\nlet's say, one of the most",
    "start": "87100",
    "end": "94670"
  },
  {
    "text": "ancient optimization\nmethods, much simpler than, in fact,\nthe more advanced methods",
    "start": "94670",
    "end": "100610"
  },
  {
    "text": "you have already seen in class. And interestingly, this\nmore ancient method",
    "start": "100610",
    "end": "107510"
  },
  {
    "text": "remains \"the\" method for\ntraining large scale machine learning systems.",
    "start": "107510",
    "end": "115152"
  },
  {
    "text": "So there's a little bit\nof history around that. I'm not going to go too\nmuch into the history.",
    "start": "115152",
    "end": "120470"
  },
  {
    "text": "But the bottom line,\nwhich probably Gil has also mentioned\nto you in class,",
    "start": "120470",
    "end": "128360"
  },
  {
    "text": "that at least four large data\nscience problems, in the end, stuff reduces to solving\nan optimization problem.",
    "start": "128360",
    "end": "137180"
  },
  {
    "text": "And in current times these\noptimization problems are pretty large. So people actually\nstarted liking stuff",
    "start": "137180",
    "end": "145980"
  },
  {
    "text": "like gradient descent, which\nwas invented by Cauchy back in the day. And this is how I'm writing\nthe abstract problem.",
    "start": "145980",
    "end": "155550"
  },
  {
    "text": "And what I want to see is-- OK, is it fitting on the page? This is my implementation in\nMATLAB of gradient descent,",
    "start": "155550",
    "end": "162950"
  },
  {
    "text": "just to set the stage that\nthis stuff really looks simple. You've already seen\ngradient descent.",
    "start": "162950",
    "end": "168320"
  },
  {
    "text": "And today, essentially,\nin a nutshell,",
    "start": "168320",
    "end": "173780"
  },
  {
    "text": "what really changes\nin this implementation of gradient descent\nis this part.",
    "start": "173780",
    "end": "179858"
  },
  {
    "text": "That's it. So you've seen gradient descent. I'm only going to\nchange this one line.",
    "start": "179858",
    "end": "186760"
  },
  {
    "text": "And the change of that\none line, surprisingly, is driving all the deep\nlearning tool boxes",
    "start": "186760",
    "end": "193870"
  },
  {
    "text": "and all of large scale\nmachine learning, et cetera. This is an oversimplification,\nbut morally, that's it.",
    "start": "193870",
    "end": "200550"
  },
  {
    "text": "So let's look at\nwhat's happening. So I will become very\nconcrete pretty soon.",
    "start": "200550",
    "end": "208860"
  },
  {
    "text": "But abstractly, what\nI want you to look at is the kinds of\noptimization problems",
    "start": "208860",
    "end": "215329"
  },
  {
    "text": "we are solving in\nmachine learning. And I'll give you very concrete\nexamples of these optimization",
    "start": "215330",
    "end": "222690"
  },
  {
    "text": "problems so that you can\nrelate to them better. But I'm just writing\nthis as the key topic,",
    "start": "222690",
    "end": "229540"
  },
  {
    "text": "that all the\noptimization problems that I'm going to talk about\ntoday, they look like that.",
    "start": "229540",
    "end": "235920"
  },
  {
    "text": "You're trying to\nfind an x over a cost function, where the\ncost function can",
    "start": "235920",
    "end": "241980"
  },
  {
    "text": "be written as a sum. In modern day machine\nlearning parlance these are also called\nfinite sum problems,",
    "start": "241980",
    "end": "248620"
  },
  {
    "text": "in case you run into that term. And they just call it finite\nbecause n is finite here.",
    "start": "248620",
    "end": "255620"
  },
  {
    "text": "In pure optimization\ntheory parlance, n can actually go to infinity. And then they're called\nstochastic optimization",
    "start": "255620",
    "end": "262670"
  },
  {
    "text": "problems-- just for terminology, if\nwhile searching the internet you run into some\nsuch terminology",
    "start": "262670",
    "end": "269210"
  },
  {
    "text": "so you kind of\nknow what it means. So here is our setup\nin machine learning.",
    "start": "269210",
    "end": "277340"
  },
  {
    "start": "271000",
    "end": "431000"
  },
  {
    "text": "We have a bunch\nof training data. ",
    "start": "277340",
    "end": "282939"
  },
  {
    "text": "On this slide, I'm\ncalling x1 through xn. These are the training\ndata, the raw features.",
    "start": "282940",
    "end": "290430"
  },
  {
    "text": "Later, actually, I'll\nstop writing x for them and write them\nwith the letter a. But hopefully, that's OK.",
    "start": "290430",
    "end": "297159"
  },
  {
    "text": "So x1 through xn, these could be\njust raw images, for instance,",
    "start": "297160",
    "end": "302770"
  },
  {
    "text": "in ImageNet or some\nother image data set. They could be text documents. They could be anything.",
    "start": "302770",
    "end": "308379"
  },
  {
    "text": "y1 through yn, in\nclassical machine learning, think of them as\nplus minus 1 labels--",
    "start": "308380",
    "end": "314620"
  },
  {
    "text": "cat, not cat-- or\nin a regression setup as some real number.",
    "start": "314620",
    "end": "320483"
  },
  {
    "text": "So that's our training data. We have d dimensional\nraw vectors, n of those.",
    "start": "320483",
    "end": "326580"
  },
  {
    "text": "And we have corresponding\nlabels which can be either plus or minus\n1 in a classification setting",
    "start": "326580",
    "end": "332360"
  },
  {
    "text": "or a real number in\na regression setting. It's kind of immaterial\nfor my lecture right now.",
    "start": "332360",
    "end": "337970"
  },
  {
    "text": "So that's the input. And whenever anybody says\nlarge scale machine learning,",
    "start": "337970",
    "end": "344960"
  },
  {
    "text": "what do we really mean? What we mean is that both\nn and d can be large.",
    "start": "344960",
    "end": "352410"
  },
  {
    "text": "So what does that mean in words? That n is the number of\ntraining data points.",
    "start": "352410",
    "end": "358260"
  },
  {
    "text": "So n could be, these days, what? A million, 10\nmillion, 100 million, depends on how big computers\nand data sets you've got.",
    "start": "358260",
    "end": "364755"
  },
  {
    "text": "So n can be huge. d, the dimensionality,\nthe vectors that we are working with--",
    "start": "364755",
    "end": "370110"
  },
  {
    "text": "the raw vectors-- that\ncan also be pretty large. Think of x is an image.",
    "start": "370110",
    "end": "376350"
  },
  {
    "text": "If it's a megapixel image, wow,\nd's like a million already. If you are somebody like\nCriteo or Facebook or Google,",
    "start": "376350",
    "end": "385160"
  },
  {
    "text": "and your serving web\nadvertisements, d-- these are the features--",
    "start": "385160",
    "end": "391750"
  },
  {
    "text": "could be like in several\nhundred million, even a billion, where they encode\nall sorts of nasty stuff",
    "start": "391750",
    "end": "398890"
  },
  {
    "text": "and information they\ncollect about you as users. So many nasty things\nthey can collect, right?",
    "start": "398890",
    "end": "404030"
  },
  {
    "text": "So d and n are huge. And it's because both\nd and n are huge,",
    "start": "404030",
    "end": "409520"
  },
  {
    "text": "we are interested in\nthinking of optimization methods for large scale\nmachine learning that can",
    "start": "409520",
    "end": "415480"
  },
  {
    "text": "handle such big d and n. And this is driving\na lot of research",
    "start": "415480",
    "end": "420742"
  },
  {
    "text": "on some theoretical\ncomputer science, including the search for sublinear\ntime algorithms and all sorts of data structures\nand hashing tricks just",
    "start": "420742",
    "end": "428310"
  },
  {
    "text": "to deal with these\ntwo quantities. So here is an example--",
    "start": "428310",
    "end": "433640"
  },
  {
    "start": "431000",
    "end": "768000"
  },
  {
    "text": "super toy example. And I hope really\nthat I can squeeze in a little bit of proof\nlater on towards the end.",
    "start": "433640",
    "end": "442010"
  },
  {
    "text": "I'll take a vote here in class\nto see if you are interested. Let's look at the\nmost classic question,",
    "start": "442010",
    "end": "447889"
  },
  {
    "text": "least squares regression. A is a matrix of observations--\nor sorry, measurements. b",
    "start": "447890",
    "end": "454680"
  },
  {
    "text": "are the observations. You're trying to solve\nAx minus b whole square. Of course, a linear\nsystem of equations,",
    "start": "454680",
    "end": "461010"
  },
  {
    "text": "the most classical\nproblem in linear algebra, can also be written\nlike that, let's say.",
    "start": "461010",
    "end": "467020"
  },
  {
    "text": "This can be expanded. Hopefully, you are\ncomfortable with this norm.",
    "start": "467020",
    "end": "475360"
  },
  {
    "text": "So x2 squared, this\nis just defined as that's the definition\nof that notation.",
    "start": "475360",
    "end": "482380"
  },
  {
    "text": "But I'll just write\nit only once now. I hope you are fully\nfamiliar with that.",
    "start": "482380",
    "end": "489260"
  },
  {
    "text": "So by expanding that, I managed\nto write least squares problem in terms of what I call\nthe finite sum right.",
    "start": "489260",
    "end": "498220"
  },
  {
    "text": "So it's just going over\nall the roles in a. The end roles, let's say. So that's the classical\nleast squares problem.",
    "start": "498220",
    "end": "505700"
  },
  {
    "text": "It assumes this finite sum\nform that we care about.",
    "start": "505700",
    "end": "511900"
  },
  {
    "text": "Another random example is\nsomething called Lasso. Maybe if anybody of you has\nplayed with machine learning",
    "start": "511900",
    "end": "517690"
  },
  {
    "text": "or statistics\ntoolkits, you may have seen something called Lasso. Lasso is essentially\nleast squares, but there's another\nsimple term at the end.",
    "start": "517690",
    "end": "527170"
  },
  {
    "text": "That again, looks like f of i. Support vector machines,\nonce a workhorse of--",
    "start": "527170",
    "end": "535000"
  },
  {
    "text": "there's still a\nworkhorse horse of people who work with small\nto medium sized data.",
    "start": "535000",
    "end": "541840"
  },
  {
    "text": "Deep learning stuff requires\nhuge amount of data. If you have small to\nmedium amount of data, logistic regression support,\nvector machines, trees, et",
    "start": "541840",
    "end": "548779"
  },
  {
    "text": "cetera, this will be\nyour first go to methods. They are still very widely used.",
    "start": "548780",
    "end": "553910"
  },
  {
    "text": "These problems are,\nagain, written in terms of a loss over training data.",
    "start": "553910",
    "end": "559320"
  },
  {
    "text": "So this again, has this\nawesome format, which I'll just now record here.",
    "start": "559320",
    "end": "565140"
  },
  {
    "text": "I may not even\nneed to repeat it. Sometimes I write it\nwith a normalization--",
    "start": "565140",
    "end": "570980"
  },
  {
    "text": "you may wonder at\nsome point, why-- as that finite sum problem.",
    "start": "570980",
    "end": "576800"
  },
  {
    "text": "And maybe the example\nthat you wanted to see is something like that.",
    "start": "576800",
    "end": "585260"
  },
  {
    "text": "So deep neural networks that\nare very popular these days, they are just yet\nanother example",
    "start": "585260",
    "end": "591260"
  },
  {
    "text": "of this finite sum problem. How are they an example of that? So you have n\ntraining data points,",
    "start": "591260",
    "end": "599560"
  },
  {
    "text": "there's a neural network\nloss, like cross entropy, or what have you, squared\nloss, cross-- any kind of loss.",
    "start": "599560",
    "end": "606800"
  },
  {
    "text": "yi's are the labels-- cat not cat, or\nmaybe a multiclass.",
    "start": "606800",
    "end": "612230"
  },
  {
    "text": "And then you have\na transfer function called a deep neural network\nwhich takes raw images as input",
    "start": "612230",
    "end": "619690"
  },
  {
    "text": "and generates a prediction\nwhether this is a dog or not. That whole thing I'm\njust calling DNN.",
    "start": "619690",
    "end": "626420"
  },
  {
    "text": "So it's a function of ai's\nwhich are the training data. X are the weight matrices\nof the neural network,",
    "start": "626420",
    "end": "632420"
  },
  {
    "text": "so I've just compressed\nthe whole neural network into this notation. Once again, it's nothing but\nan instance of that finite sum.",
    "start": "632420",
    "end": "641350"
  },
  {
    "text": "So that fi in there captures\nthe entire neural network architecture.",
    "start": "641350",
    "end": "648170"
  },
  {
    "text": "But mathematically, it's still\njust one particular instance of this finite sum problem.",
    "start": "648170",
    "end": "655270"
  },
  {
    "text": "And then people who do a lot of\nstatistics, maximum likelihood estimation.",
    "start": "655270",
    "end": "661360"
  },
  {
    "text": "This is log likelihood\nover n observations. You want to maximize\nlog likelihood.",
    "start": "661360",
    "end": "668290"
  },
  {
    "text": "Once again, just a finite sum. So pretty much most\nof the problems",
    "start": "668290",
    "end": "673900"
  },
  {
    "text": "that we're interested in\nmachine learning and statistics, when I write them down as\nan optimization problem,",
    "start": "673900",
    "end": "680080"
  },
  {
    "text": "they look like these\nfinite sum problems. And that's the reason to\ndevelop specialized optimization",
    "start": "680080",
    "end": "688380"
  },
  {
    "text": "procedures to solve such\nfinite some problems. And that's where SGD comes in.",
    "start": "688380",
    "end": "695640"
  },
  {
    "text": "OK. So that's kind of\njust the backdrop. Let's look at now how to go\nabout solving these problems.",
    "start": "695640",
    "end": "701780"
  },
  {
    "text": " So hopefully this iteration\nis familiar to you--",
    "start": "701780",
    "end": "711459"
  },
  {
    "text": "gradient descent, right? OK. So just for notation,\nf of x refers",
    "start": "711460",
    "end": "722290"
  },
  {
    "text": "to that entire summation. F sub i of x refers\nto a single component.",
    "start": "722290",
    "end": "728069"
  },
  {
    "text": "So if you were to try to solve-- that is, to minimize this cost\nfunction, neural network, SVM,",
    "start": "728070",
    "end": "734790"
  },
  {
    "text": "what have you using\ngradient descent, that's what one iteration\nwould look like.",
    "start": "734790",
    "end": "742750"
  },
  {
    "text": "Because it's a finite sum,\ngradients are linear operators. Gradient of the sum is\nthe sum of the gradient--",
    "start": "742750",
    "end": "749350"
  },
  {
    "text": "that's gradient descent for you.  And now, I'll just ask\na rhetoric question",
    "start": "749350",
    "end": "756520"
  },
  {
    "text": "that, if you put yourself\nin the shoes of you're [INAUDIBLE] algorithm\ndesigners, some things that you",
    "start": "756520",
    "end": "762070"
  },
  {
    "text": "may want to think\nabout-- what may you not like about this iteration,\ngiven that big n, big d story",
    "start": "762070",
    "end": "768910"
  },
  {
    "start": "768000",
    "end": "933000"
  },
  {
    "text": "that I told you? So anybody have any\nreservations or about drawbacks of this iteration?",
    "start": "768910",
    "end": "775680"
  },
  {
    "text": "Any comments? AUDIENCE: It's a pretty big sum. PROFESSOR SRA: It's\na pretty big sum.",
    "start": "775680",
    "end": "782339"
  },
  {
    "text": "Especially if n\nis say, a billion on some bigger,\n[INAUDIBLE] number.",
    "start": "782340",
    "end": "787920"
  },
  {
    "text": "That is definitely\na big drawback.  Because that is the prime\ndrawback for large scale,",
    "start": "787920",
    "end": "795300"
  },
  {
    "text": "that n be huge. There can be variety\nof other drawbacks. Some of those you may\nhave seen previously",
    "start": "795300",
    "end": "801390"
  },
  {
    "text": "when people compare whether to\nthe gradient or to do Newton, et. Cetera but for the purpose\nof today, for finite sums,",
    "start": "801390",
    "end": "809130"
  },
  {
    "text": "the big drawback is computing\ngradient at a single point--",
    "start": "809130",
    "end": "814560"
  },
  {
    "text": "there's a subscript\nxk missing there-- involves computing the\ngradient of that entire sum.",
    "start": "814560",
    "end": "820630"
  },
  {
    "text": "That sum is some is huge. So getting a single\ngradient to do a single step of gradient\ndescent for a large data set",
    "start": "820630",
    "end": "829030"
  },
  {
    "text": "could take you hours or days. ",
    "start": "829030",
    "end": "834350"
  },
  {
    "text": "So that's a major drawback. But then if you\nidentify that drawback, anybody have any\nideas how to counter",
    "start": "834350",
    "end": "843160"
  },
  {
    "text": "that drawback, at least, say,\npurely from an engineering perspective? ",
    "start": "843160",
    "end": "850090"
  },
  {
    "text": "I heard something. Can you speak up? AUDIENCE: [INAUDIBLE]",
    "start": "850090",
    "end": "855610"
  },
  {
    "text": "PROFESSOR SRA: Using\nsome kind of badge? AUDIENCE: Yeah. PROFESSOR SRA: You are\nwell ahead of my slides.",
    "start": "855610",
    "end": "861210"
  },
  {
    "text": "We are coming to that. And maybe somebody else has,\nessentially, the same idea.",
    "start": "861210",
    "end": "866360"
  },
  {
    "text": "Anybody want to suggest how\nto circumvent that big n stuff in there?",
    "start": "866360",
    "end": "872975"
  },
  {
    "text": "Anything-- suppose you\nare implementing this. What would you do? ",
    "start": "872975",
    "end": "878233"
  },
  {
    "text": "AUDIENCE: One example at a time. PROFESSOR SRA: One\nexample at a time. AUDIENCE: [INAUDIBLE] a\nrandom sample full set of n.",
    "start": "878234",
    "end": "887318"
  },
  {
    "text": "PROFESSOR SRA: Random\nsample of the full n. So these are all\nexcellent ideas. And hence, you\nfolks in the class",
    "start": "887318",
    "end": "895410"
  },
  {
    "text": "have discovered the\nmost important method for optimizing machine\nlearning problems, sitting here in a few moments.",
    "start": "895410",
    "end": "901889"
  },
  {
    "text": "Isn't that great? So the part that is missing\nis of course to make sense of, does this idea work?",
    "start": "901890",
    "end": "907540"
  },
  {
    "text": "Why does it work? So this idea is really at the\nheart of stochastic gradient",
    "start": "907540",
    "end": "914240"
  },
  {
    "text": "descent. So let's see.",
    "start": "914240",
    "end": "919370"
  },
  {
    "text": "Maybe I can show you an\nexample, actually, that I-- ",
    "start": "919370",
    "end": "925810"
  },
  {
    "text": "I'll show you a simulation I\nfound on somebody's nice web page about that.",
    "start": "925810",
    "end": "933300"
  },
  {
    "text": "So exactly your idea, just\nput in slight mathematical notation, that what\nif at each iteration,",
    "start": "933300",
    "end": "941940"
  },
  {
    "text": "we randomly pick some integer,\ni k out of the n training data points, and we instead\njust perform this update.",
    "start": "941940",
    "end": "951485"
  },
  {
    "text": " So instead of using\nthe full gradient,",
    "start": "951485",
    "end": "962199"
  },
  {
    "text": "you just compute the gradient\nof a single randomly chosen data point.",
    "start": "962200",
    "end": "967430"
  },
  {
    "text": "So what have you done with that? One iteration is\nnow n times faster. If n were a million or a\nbillion wow, that's super fast.",
    "start": "967430",
    "end": "977388"
  },
  {
    "text": "But why should this work right?  I could have done\nmany other things.",
    "start": "977388",
    "end": "983250"
  },
  {
    "text": "I could have not done any update\nand just output the 0 vector. That would take even less time.",
    "start": "983250",
    "end": "990330"
  },
  {
    "text": "That's also an idea. It's a bad idea, but it's\nan idea in a similar league. I could have done a\nvariety of other things.",
    "start": "990330",
    "end": "996270"
  },
  {
    "text": "Why would you think that just\nreplacing that sum with just one random example may work?",
    "start": "996270",
    "end": "1001880"
  },
  {
    "text": " Let's see a little\nbit more about that.",
    "start": "1001880",
    "end": "1008180"
  },
  {
    "text": "So of course, it's\nn times faster, and the key question\nfor us here, right now--",
    "start": "1008180",
    "end": "1015860"
  },
  {
    "text": "the scientific question--\nis does this make sense? It makes great\nengineering sense.",
    "start": "1015860",
    "end": "1022040"
  },
  {
    "text": "Does it make algorithmic\nor mathematical sense? ",
    "start": "1022040",
    "end": "1027449"
  },
  {
    "text": "So this idea of doing stuff\nin the stochastic manner was actually originally\nproposed by Robbins and Monro,",
    "start": "1027450",
    "end": "1034270"
  },
  {
    "text": "somewhere, I think, around 1951. And that's the most\nadvanced method that we are essentially\nusing currently.",
    "start": "1034270",
    "end": "1042260"
  },
  {
    "text": "So I'll show you that\nthis idea makes sense. But maybe let's first just\nlook at a comparison of SGD",
    "start": "1042260",
    "end": "1050270"
  },
  {
    "text": "with gradient descent in\nthis guy's simulation. ",
    "start": "1050270",
    "end": "1057960"
  },
  {
    "text": "So this is that MATLAB\ncode of gradient descent, and this is just a simulation\nof gradient descent.",
    "start": "1057960",
    "end": "1067830"
  },
  {
    "text": "As you pick a different step\nsize, that gamma in there, you move towards the optimum.",
    "start": "1067830",
    "end": "1073200"
  },
  {
    "text": "If the step size is small,\nyou make many small steps,",
    "start": "1073200",
    "end": "1078330"
  },
  {
    "text": "and you keep making slow\nprogress, and you reach there. That's for a\nwell-conditioned problem",
    "start": "1078330",
    "end": "1084210"
  },
  {
    "text": "and an ill-conditioned problem. It takes you even larger. In a neural network type\nproblem which is nonconvex,",
    "start": "1084210",
    "end": "1091590"
  },
  {
    "text": "you have to typically work\nwith smaller step sizes. And if you take bigger ones,\nyou can get crazy oscillations.",
    "start": "1091590",
    "end": "1096810"
  },
  {
    "text": "But that's gradient descent. In comparison, let's hope\nthat this loads correctly.",
    "start": "1096810",
    "end": "1106190"
  },
  {
    "text": "Well, there's even a\npicture of Robbins, who was a co-discoverer of the\nstochastic gradient method.",
    "start": "1106190",
    "end": "1111960"
  },
  {
    "text": "There's a nice\nsimulation, that instead of making that kind of\ndeterministic descent--",
    "start": "1111960",
    "end": "1118809"
  },
  {
    "text": "after all, gradient descent\nis called \"gradient descent.\" At every step it descends-- it\ndecreases the cost function.",
    "start": "1118810",
    "end": "1128820"
  },
  {
    "text": "Stochastic gradient descent\nis actually a misnomer. At every step it\ndoesn't do any descent.",
    "start": "1128820",
    "end": "1134850"
  },
  {
    "text": "It does not decrease\nthe cost function. So you see, at every step, those\nare the contours of the cost",
    "start": "1134850",
    "end": "1139900"
  },
  {
    "text": "function. Sometimes it goes up,\nsometimes it goes down. It fluctuates around, but it\nkind of stochastically still",
    "start": "1139900",
    "end": "1146460"
  },
  {
    "text": "seems to be making progress\ntowards the optimum. And stochastic gradient\ndescent, because it's not",
    "start": "1146460",
    "end": "1154420"
  },
  {
    "text": "using exact gradients,\njust working with these random\nexamples, it actually",
    "start": "1154420",
    "end": "1160810"
  },
  {
    "text": "is much more sensitive\nto step sizes. And you can see, as I increase\nthe step size, its behavior.",
    "start": "1160810",
    "end": "1169070"
  },
  {
    "text": "This is actually full simulation\nfor [INAUDIBLE] problem. So initially, what I\nwant you to notice is--",
    "start": "1169070",
    "end": "1176039"
  },
  {
    "text": "let me go through\nthis a few times-- keep looking at what patterns\nyou may notice in how",
    "start": "1176040",
    "end": "1184889"
  },
  {
    "text": "that line is fluctuating. Hopefully this is big\nenough for everybody to see.",
    "start": "1184890",
    "end": "1190320"
  },
  {
    "text": "So this slider that I'm\nshifting is just the step size. So let me just remind you, in\ncase you forgot, the iteration.",
    "start": "1190320",
    "end": "1198350"
  },
  {
    "text": "We are running x k plus 1\nis x k minus some eta k--",
    "start": "1198350",
    "end": "1204690"
  },
  {
    "text": "It's called alpha there-- times some randomly\nchosen data point.",
    "start": "1204690",
    "end": "1211840"
  },
  {
    "text": "You compute its gradient. This is SGD. That's what we are running.",
    "start": "1211840",
    "end": "1218110"
  },
  {
    "text": "And we threw away\ntons of information. We didn't use the full gradient. We're just using\nthis crude gradient.",
    "start": "1218110",
    "end": "1227000"
  },
  {
    "text": "So this process\nis very sensitive to the other parameter in the\nsystem, which is the step size.",
    "start": "1227000",
    "end": "1232150"
  },
  {
    "text": "Much more sensitive than\ngradient descent, in fact. And let's see. As I vary the step\nsize, see if you",
    "start": "1232150",
    "end": "1238330"
  },
  {
    "text": "can notice some\npatterns on how it tries to go towards an optimum.",
    "start": "1238330",
    "end": "1244090"
  },
  {
    "start": "1244090",
    "end": "1259890"
  },
  {
    "text": "There's a zoomed in version,\nalso, of this later, here. ",
    "start": "1259890",
    "end": "1265330"
  },
  {
    "text": "I'll come to that shortly. I'll repeat again, and then I'll\nask you for your observations--",
    "start": "1265330",
    "end": "1271663"
  },
  {
    "text": "if you notice some patterns.  I don't know if they're\nnecessarily apparent.",
    "start": "1271663",
    "end": "1276980"
  },
  {
    "text": "That's the thing with patterns. Because I know the answer,\nso I see the pattern. If you don't know the\nanswer, you may or may not",
    "start": "1276980",
    "end": "1282320"
  },
  {
    "text": "see the pattern. But I want to see if you\nactually see the pattern as I change the step size.",
    "start": "1282320",
    "end": "1288480"
  },
  {
    "text": "So maybe that was\nenough simulation. Anybody have any comments\non what kind of pattern you may have observed?",
    "start": "1288480",
    "end": "1294780"
  },
  {
    "text": "Yep. AUDIENCE: It seems like the\nclustering in the middle is getting larger\nand more widespread. PROFESSOR SRA: Yeah, definitely.",
    "start": "1294780",
    "end": "1300960"
  },
  {
    "text": "That's a great observation. Any other comments? ",
    "start": "1300960",
    "end": "1309790"
  },
  {
    "text": "There's one more interesting\nthing happening here, which is a very, very\ntypical thing for SGD,",
    "start": "1309790",
    "end": "1316670"
  },
  {
    "text": "and one of the reasons\nwhy people love SGD. Let me do that\nonce again briefly.",
    "start": "1316670",
    "end": "1323540"
  },
  {
    "text": "OK, this is tiny step size-- almost zero. Close to zero-- it's\nnot exactly zero.",
    "start": "1323540",
    "end": "1330970"
  },
  {
    "text": "So you see what happens\nfor a very tiny step size? It doesn't look that\nstochastic, right?",
    "start": "1330970",
    "end": "1336929"
  },
  {
    "text": "But that's kind of obvious from\nthere if eta k is very tiny, you'll hardly make any move.",
    "start": "1336930",
    "end": "1344460"
  },
  {
    "text": "So things will look very stable. And in fact, the speed at which\nstochastic gradient converges,",
    "start": "1344460",
    "end": "1351240"
  },
  {
    "text": "that's extremely sensitive to\nhow you pick the step size. It's still an open\nresearch problem to come up with the best\nway to pick step sizes.",
    "start": "1351240",
    "end": "1357740"
  },
  {
    "text": "So it's even that simple, it\ndoesn't mean it's trivial. And as I vary the step\nsize, it make some progress,",
    "start": "1357740",
    "end": "1367220"
  },
  {
    "text": "and it goes towards\nthe solution. Are you now beginning\nto see that it seems to be making a more stable\nprogress in the beginning?",
    "start": "1367220",
    "end": "1375140"
  },
  {
    "text": "And when it comes\nclose to the solution, it's fluctuating more.",
    "start": "1375140",
    "end": "1381430"
  },
  {
    "text": "And the bigger the\nstep size, the amount of fluctuation near\nthe solution is wilder",
    "start": "1381430",
    "end": "1388870"
  },
  {
    "text": "as he noticed back there. But one very interesting thing\nis more or less constant.",
    "start": "1388870",
    "end": "1397090"
  },
  {
    "text": "There is more fluctuation\nalso on the outside, but you see that the\ninitial part still seems to be making\npretty good progress.",
    "start": "1397090",
    "end": "1404230"
  },
  {
    "text": "And as you come close to the\nsolution, it fluctuates more. And that is a very\nprincipally typical behavior",
    "start": "1404230",
    "end": "1410530"
  },
  {
    "text": "of stochastic gradient\ndescent, that in the beginning,",
    "start": "1410530",
    "end": "1415620"
  },
  {
    "text": "it makes rapid strides. So you may see\nyour training loss decrease super fast and\nthen kind of peter out.",
    "start": "1415620",
    "end": "1426620"
  },
  {
    "text": "And it's this\nparticular behavior which guard people\nsuper excited, that, hey, in machine\nlearning, we are working",
    "start": "1426620",
    "end": "1432710"
  },
  {
    "text": "with all sorts of big data. I just want a quick and dirty\nprogress on my training.",
    "start": "1432710",
    "end": "1439040"
  },
  {
    "text": "I don't care about getting\nto the best optimum. Because in machine\nlearning, you don't just",
    "start": "1439040",
    "end": "1444929"
  },
  {
    "text": "care about solving the\noptimization problem, you actually care\nabout finding solutions",
    "start": "1444930",
    "end": "1450180"
  },
  {
    "text": "that work well on unseen data. So that means you don't\nwant to over fit and solve the optimization\nproblem supremely well.",
    "start": "1450180",
    "end": "1457740"
  },
  {
    "text": "So it's great to make\nrapid initial progress. And if after that progress\npeters out, it's OK.",
    "start": "1457740",
    "end": "1464909"
  },
  {
    "text": "This intuitionistic\nstatement that I'm making, in some nice cases like\nconvex optimization problems,",
    "start": "1464910",
    "end": "1471600"
  },
  {
    "text": "one can mathematically\nfully quantify these. One can prove theorems\nto quantify each thing",
    "start": "1471600",
    "end": "1477059"
  },
  {
    "text": "that I said in terms of how\nclose, how fast, and so on. We'll see a little bit of that.",
    "start": "1477060",
    "end": "1483150"
  },
  {
    "text": "And this is what\nreally happens to SGD. It makes great initial\nprogress, and regardless",
    "start": "1483150",
    "end": "1490500"
  },
  {
    "text": "of how you use step sizes,\nclose to the optimum it can either get\nstuck, or enter",
    "start": "1490500",
    "end": "1496620"
  },
  {
    "text": "some kind of chaos dynamics,\nor just behave like crazy. So that's typical of SGD.",
    "start": "1496620",
    "end": "1503650"
  },
  {
    "text": "And let's look at now\nslight mathematical insight into roughly why this\nbehavior may happen.",
    "start": "1503650",
    "end": "1513330"
  },
  {
    "text": "This is a trivial,\none-dimensional optimization problem, but it\nconveys the crux of why",
    "start": "1513330",
    "end": "1520740"
  },
  {
    "text": "this behavior is displayed by\nstochastic gradient methods. That it works really\nwell in the beginning,",
    "start": "1520740",
    "end": "1526470"
  },
  {
    "text": "and then, God knows\nwhat happens when it comes close to the\noptimum, anything can happen. So let's look at that.",
    "start": "1526470",
    "end": "1532080"
  },
  {
    "start": "1532080",
    "end": "1538399"
  },
  {
    "text": "OK. So let's look at a simple,\none-dimensional optimization",
    "start": "1538400",
    "end": "1543610"
  },
  {
    "text": "problem. I'll kind of draw it out\nmaybe on the other side so that people on this\nside are not disadvantaged.",
    "start": "1543610",
    "end": "1552490"
  },
  {
    "text": "So I'll just draw out at\nleast squares problem--",
    "start": "1552490",
    "end": "1557830"
  },
  {
    "text": "x is one dimensional. Previously, I had ai transpose\nx Now, ai is also a scalar.",
    "start": "1557830",
    "end": "1563610"
  },
  {
    "text": "So it's just 1D stuff-- everything is 1D",
    "start": "1563610",
    "end": "1568780"
  },
  {
    "text": "So this is our setup. Think of ai into x minus b i.",
    "start": "1568780",
    "end": "1578530"
  },
  {
    "text": "These are quadratic functions,\nso they look like this.",
    "start": "1578530",
    "end": "1585060"
  },
  {
    "text": "Corresponding to\ndifferent eyes, there's like some different\nfunctions sitting and so on.",
    "start": "1585060",
    "end": "1591570"
  },
  {
    "text": " So these are my n\ndifferent loss functions,",
    "start": "1591570",
    "end": "1603080"
  },
  {
    "text": "and I want to minimize those. ",
    "start": "1603080",
    "end": "1612410"
  },
  {
    "text": "We know-- we can actually\nexplicitly compute the solution of that problem.",
    "start": "1612410",
    "end": "1618470"
  },
  {
    "text": "So you set the derivative\nof f of x to 0 so. You set the gradient\nof f of x to 0.",
    "start": "1618470",
    "end": "1625410"
  },
  {
    "text": "Hopefully, that's\neasy for you to do. So if you do that\ndifferentiation,",
    "start": "1625410",
    "end": "1631530"
  },
  {
    "text": "will get gradient of f\nof x will just given by.",
    "start": "1631530",
    "end": "1636860"
  },
  {
    "text": "Well, you can do\nthat in your head, I'll just write it\nout explicitly. aix minus bi times ai Is equal to\nzero, and you solve that for x.",
    "start": "1636860",
    "end": "1650660"
  },
  {
    "text": "You get x star, the optimum\nof this least squares problem.",
    "start": "1650660",
    "end": "1655912"
  },
  {
    "text": "So we actually know how\nto solve it pretty easily. ",
    "start": "1655912",
    "end": "1663697"
  },
  {
    "text": "That's a really cool\nexample actually. I got that from textbook by\nProfessor Dimitry [INAUDIBLE]..",
    "start": "1663697",
    "end": "1670850"
  },
  {
    "text": "Now, a very interesting thing. We are not going to\nuse the full gradient, we are only going\nto use the gradients",
    "start": "1670850",
    "end": "1677780"
  },
  {
    "text": "of individual components. So what does the minimum of an\nindividual component look like?",
    "start": "1677780",
    "end": "1685610"
  },
  {
    "text": "Well, the minimum of\nan individual component is attained when we can\nset this thing to 0.",
    "start": "1685610",
    "end": "1691070"
  },
  {
    "text": "And that thing becomes 0 if\nwe just pick x equal to bi divided by ai, right?",
    "start": "1691070",
    "end": "1698050"
  },
  {
    "text": "So a single component can\nbe minimized by that choice. ",
    "start": "1698050",
    "end": "1704640"
  },
  {
    "text": "So you can do a little\nbit of arithmetic mean, geometric mean\ntype inequalities",
    "start": "1704640",
    "end": "1710419"
  },
  {
    "text": "to draw this picture. ",
    "start": "1710420",
    "end": "1717519"
  },
  {
    "text": "So over all i from\n1 through n, this is the minimum value of\nthis ratio, ai by bi.",
    "start": "1717520",
    "end": "1726290"
  },
  {
    "text": "And let's say this is the\nmax value of ai by bi.",
    "start": "1726290",
    "end": "1735200"
  },
  {
    "text": "And we know that closed\nform solution, that is the true solution.",
    "start": "1735200",
    "end": "1741049"
  },
  {
    "text": "So you can verify\nwith some algebra that that solution will\nlie in this interval.",
    "start": "1741050",
    "end": "1749010"
  },
  {
    "text": "So you may want to-- this is a tiny exercise for you.",
    "start": "1749010",
    "end": "1757220"
  },
  {
    "text": "Hopefully some of you\nlove inequalities like me. So this is hopefully\nnot such a bad exercise.",
    "start": "1757220",
    "end": "1762590"
  },
  {
    "text": "But you can verify\nthat within this range of the individual\nmins and max is where",
    "start": "1762590",
    "end": "1768559"
  },
  {
    "text": "the combined solution lies. So of course, intuitively,\nwith a physics styles thinking, you would have guessed\nthat right away.",
    "start": "1768560",
    "end": "1773845"
  },
  {
    "text": " That means when\nyou're outside where",
    "start": "1773845",
    "end": "1779570"
  },
  {
    "text": "the individual solutions, let's\ncall this the far out zone. ",
    "start": "1779570",
    "end": "1786170"
  },
  {
    "text": "And also, this side\nis the far out zone. And this region, within which\nthe true minimum can lie,",
    "start": "1786170",
    "end": "1793190"
  },
  {
    "text": "you can say, OK, that's\nthe region of confusion. ",
    "start": "1793190",
    "end": "1798710"
  },
  {
    "text": "Why I'm calling it the\nregion of confusion? Because there, by\nminimizing an individual fi,",
    "start": "1798710",
    "end": "1805420"
  },
  {
    "text": "you're not going\nto be able to tell what is the combined x star. That's all.",
    "start": "1805420",
    "end": "1812210"
  },
  {
    "text": "And a very interesting\nthing happens now, just to gain some mathematical\ninsight into that simulation",
    "start": "1812210",
    "end": "1817730"
  },
  {
    "text": "that I showed you, that if\nyou have a scalar x that",
    "start": "1817730",
    "end": "1823790"
  },
  {
    "text": "is outside this region of\nconfusion, which states",
    "start": "1823790",
    "end": "1828850"
  },
  {
    "text": "that if you're far\nfrom the region within which an optimum can lie.",
    "start": "1828850",
    "end": "1834299"
  },
  {
    "text": "So you're far away. So you've just started\nout your progress, you made a random\ninitialization, most likely",
    "start": "1834300",
    "end": "1839370"
  },
  {
    "text": "far away from where\nthe solution is. So suppose that's where you are. What happens when you're\nin that far out region?",
    "start": "1839370",
    "end": "1846039"
  },
  {
    "text": "So if you're in\nthe far out region, you use a stochastic gradient\nof some i-th component.",
    "start": "1846040",
    "end": "1855470"
  },
  {
    "text": "So the full gradient\nwill look like that. A stochastic gradient looks\nlike just one component.",
    "start": "1855470",
    "end": "1863059"
  },
  {
    "text": "And when you're far out,\noutside that min and max regime,",
    "start": "1863060",
    "end": "1868820"
  },
  {
    "text": "then you can check by\njust looking at it,",
    "start": "1868820",
    "end": "1876393"
  },
  {
    "text": "that a stochastic gradient,\nin the far away regime,",
    "start": "1876394",
    "end": "1881520"
  },
  {
    "text": "has exactly the same sign\nas the full gradient. What does gradient descent do?",
    "start": "1881520",
    "end": "1887150"
  },
  {
    "text": "It says, well, walk\nin the direction of the negative gradient. And far away from the\noptimum, outside the region",
    "start": "1887150",
    "end": "1895860"
  },
  {
    "text": "of confusion, you're stochastic\ngradient has the same sign as the true gradient.",
    "start": "1895860",
    "end": "1901290"
  },
  {
    "text": "Maybe in more linear\nalgebra terms, it makes an acute angle\nwith your gradient.",
    "start": "1901290",
    "end": "1908820"
  },
  {
    "text": "So that means if even though\na stochastic gradient is not exactly the full\ngradient, it has",
    "start": "1908820",
    "end": "1915240"
  },
  {
    "text": "some component in the\ndirection of the true gradient. This is one 1D. Here it is, exactly\nthe same sign.",
    "start": "1915240",
    "end": "1921059"
  },
  {
    "text": "In multiple dimensions,\nthis is the idea that it'll have some\ncomponent in the direction",
    "start": "1921060",
    "end": "1926490"
  },
  {
    "text": "of true gradient\nwhen you're far away.  Which means, if you then\nuse that direction to make",
    "start": "1926490",
    "end": "1935300"
  },
  {
    "text": "an update in that\nstyle, you will end up making solid progress.",
    "start": "1935300",
    "end": "1941920"
  },
  {
    "text": "And the beauty\nis, in the time it would have taken you to do\none single iteration of batch",
    "start": "1941920",
    "end": "1949210"
  },
  {
    "text": "gradient descent, far\naway you can do millions stochastic steps, and, each\nstep will make some progress.",
    "start": "1949210",
    "end": "1955420"
  },
  {
    "text": "And that's where we see\nthis dramatic, initial-- again, in the 1D case this\nis explicit mathematically.",
    "start": "1955420",
    "end": "1963490"
  },
  {
    "text": "In the high-D case,\nthis is more intuitive. Without further assumptions\nabout angles, et,",
    "start": "1963490",
    "end": "1968780"
  },
  {
    "text": "we can't make such\na broad claim. But intuitively, this\nis what's happening, and why you see this\nawesome initial speed.",
    "start": "1968780",
    "end": "1976470"
  },
  {
    "text": " And once you're inside\nthe region of confusion,",
    "start": "1976470",
    "end": "1984610"
  },
  {
    "text": "then this behavior breaks down. Some stochastic\ngradient may have the same sign as the full\ngradient, some may not.",
    "start": "1984610",
    "end": "1991390"
  },
  {
    "text": "And that's why you can\nget crazy fluctuations. So this simple 1D\nexample kind of exactly",
    "start": "1991390",
    "end": "1998690"
  },
  {
    "text": "shows you what we\nsaw in that picture. ",
    "start": "1998690",
    "end": "2004090"
  },
  {
    "text": "And people really love\nthis initial progress. Because, often we also\ndo early stopping. You train for some time, and\nthen you say, OK, I'm done.",
    "start": "2004090",
    "end": "2011560"
  },
  {
    "text": " So importantly, if you are\npurely an optimization person,",
    "start": "2011560",
    "end": "2020990"
  },
  {
    "text": "not thinking so much in\nterms of machine learning, then please keep in mind that\nstochastic gradient descent",
    "start": "2020990",
    "end": "2027559"
  },
  {
    "text": "or stochastic gradient method\nis not such a great optimization method. Because once in the\nregion of confusion,",
    "start": "2027560",
    "end": "2034000"
  },
  {
    "text": "it can just fluctuate\nall over forever. And in machine learning,\nyou say, oh, the region",
    "start": "2034000",
    "end": "2039910"
  },
  {
    "text": "of confusion, that's fine. It'll make my method robust. It'll make my neural network\ntraining more robust.",
    "start": "2039910",
    "end": "2045120"
  },
  {
    "text": "It's generalize\nbetter, et cetera, er cetera-- we like that. So it depends on which\nframe of mind you're in.",
    "start": "2045120",
    "end": "2053770"
  },
  {
    "text": "So that's the awesome thing\nabout the stochastic gradient method.",
    "start": "2053770",
    "end": "2059625"
  },
  {
    "text": "So I'll give you now\nkey mathematical ideas",
    "start": "2059625",
    "end": "2066060"
  },
  {
    "text": "behind the success of SGD. This was like\nlittle illustration. Very abstractly, this is\nan idea that [INAUDIBLE]",
    "start": "2066060",
    "end": "2075239"
  },
  {
    "text": "throughout machine learning and\nthroughout theoretical computer science and statistics, anytime\nyou're faced with the need",
    "start": "2075239",
    "end": "2083790"
  },
  {
    "text": "to compute an\nexpensive quantity, resort to randomization to\nspeed up the computation.",
    "start": "2083790",
    "end": "2091899"
  },
  {
    "text": "SGD is one example. The true gradient was\nexpensive to compute,",
    "start": "2091900",
    "end": "2097640"
  },
  {
    "text": "so we create a randomized\nestimate of the true gradient.",
    "start": "2097640",
    "end": "2103250"
  },
  {
    "text": "And the randomized estimate\nis much faster to compute.",
    "start": "2103250",
    "end": "2108670"
  },
  {
    "text": "And mathematically, what\nwill start happening is, depending on how good your\nrandomized estimate is,",
    "start": "2108670",
    "end": "2115900"
  },
  {
    "text": "your method may or may not\nconvert to the right answer.",
    "start": "2115900",
    "end": "2121260"
  },
  {
    "text": "So of course, one has\nto be careful about what particular randomized\nestimate one makes.",
    "start": "2121260",
    "end": "2128069"
  },
  {
    "text": "But really abstractly,\neven if I hadn't shown you, the main\nidea, this idea you can apply in\nmany other settings.",
    "start": "2128070",
    "end": "2135484"
  },
  {
    "text": "If you have a\ndifficult quantity, come up with a\nrandomized estimate and save on computation.",
    "start": "2135485",
    "end": "2141360"
  },
  {
    "text": "This is a very important theme\nthroughout machine learning and data science.",
    "start": "2141360",
    "end": "2148140"
  },
  {
    "start": "2147000",
    "end": "2402000"
  },
  {
    "text": "And this is the key property. So stochastic gradient descent,\nit uses stochastic gradients.",
    "start": "2148140",
    "end": "2156569"
  },
  {
    "text": "Stochastic is, here,\nused very loosely. And it just means that\nsome randomization. That's all it means.",
    "start": "2156570",
    "end": "2163550"
  },
  {
    "text": "And the property-- the\nkey property that we have is in expectation.",
    "start": "2163550",
    "end": "2170770"
  },
  {
    "text": "The expectation is over\nwhatever randomness you used. So if you picked some random\ntraining data point out",
    "start": "2170770",
    "end": "2179980"
  },
  {
    "text": "of the million,\nthen the expectation is over the probability\ndistribution over what kind of\nrandomness you used.",
    "start": "2179980",
    "end": "2187539"
  },
  {
    "text": "If you picked uniformly at\nrandom from a million points, then this expectation is over\nthat uniform probability.",
    "start": "2187540",
    "end": "2194319"
  },
  {
    "text": "But the key property for\nSGD, or at least the version",
    "start": "2194320",
    "end": "2199640"
  },
  {
    "text": "of SGD I'm talking about, is\nthat that over that randomness.",
    "start": "2199640",
    "end": "2204740"
  },
  {
    "text": "The thing that you're\npretending to use, instead of the true gradient\nn expectation actually it is the true gradient.",
    "start": "2204740",
    "end": "2212200"
  },
  {
    "text": "So in statistics\nlanguage, this is called the stochastic\ngradient that we use is an unbiased estimate\nof the true gradient.",
    "start": "2212200",
    "end": "2223130"
  },
  {
    "text": "And this is a very\nimportant property in the mathematical analysis\nof stochastic gradient descent,",
    "start": "2223130",
    "end": "2228620"
  },
  {
    "text": "that it is an\nunbiased estimate, And Intuitively speaking\nanytime you did any proof",
    "start": "2228620",
    "end": "2236050"
  },
  {
    "text": "in class, or in the book,\nor lecture, or to wherever, where you were using\ntrue gradients,",
    "start": "2236050",
    "end": "2243099"
  },
  {
    "text": "more or less, you can\ndo those same proofs-- more or less, not always. Using stochastic gradients\nby encapsulating everything",
    "start": "2243100",
    "end": "2253640"
  },
  {
    "text": "within expectations\nover the randomness. I'll show you an example\nof what I mean by that.",
    "start": "2253640",
    "end": "2258980"
  },
  {
    "text": "I'm just trying to\nsimplify that for you. And In particular, the\nunbiasedness is great.",
    "start": "2258980",
    "end": "2268710"
  },
  {
    "text": "So it means I can\nkind of plug-in these stochastic gradients in\nplace of the true gradient,",
    "start": "2268710",
    "end": "2275430"
  },
  {
    "text": "and I'm still doing\nsomething meaningful. So this is answering\nthat earlier question, why this random stuff?",
    "start": "2275430",
    "end": "2283130"
  },
  {
    "text": "Why should we think it may work? But there's another\nvery important aspect to why it works, beyond\nthis unbiasedness,",
    "start": "2283130",
    "end": "2292260"
  },
  {
    "text": "that the amount of noise, or\nthe amount of stochasticity",
    "start": "2292260",
    "end": "2299250"
  },
  {
    "text": "is controlled. So just because it is\nan unbiased estimate,",
    "start": "2299250",
    "end": "2304760"
  },
  {
    "text": "doesn't mean that it's\ngoing to work that well. Why? Because it could still\nfluctuate hugely, right?",
    "start": "2304760",
    "end": "2312140"
  },
  {
    "text": "Essentially, plus infinity\nhere, minus infinity here. You take an average, you get 0. So that is essentially\nunbiased, but the fluctuation",
    "start": "2312140",
    "end": "2320119"
  },
  {
    "text": "is gigantic. So whenever talking\nabout estimates, what's the other key quantity\nwe need to care about",
    "start": "2320120",
    "end": "2327080"
  },
  {
    "text": "beyond expectation? AUDIENCE: Variance. AUDIENCE: Variance. PROFESSOR SRA: Variance. And really, the key\nthing that governs",
    "start": "2327080",
    "end": "2335180"
  },
  {
    "text": "the speed at which stochastic\ngradient descent does the job that we want it to\ndo is, how much variance do",
    "start": "2335180",
    "end": "2342740"
  },
  {
    "text": "the stochastic gradients have? Just this simple\nstatistical point, in fact,",
    "start": "2342740",
    "end": "2349150"
  },
  {
    "text": "is at the heart of a\nsequence of research progress in the past five years in the\nfield of stochastic gradient,",
    "start": "2349150",
    "end": "2357280"
  },
  {
    "text": "where people have worked\nreally hard to come up with newer and newer,\nfancier and fancier",
    "start": "2357280",
    "end": "2362950"
  },
  {
    "text": "versions of stochastic\ngradient which have the unbiasedness\nproperty, but have",
    "start": "2362950",
    "end": "2368470"
  },
  {
    "text": "smaller and smaller variance. And the smaller the\nvariance you have,",
    "start": "2368470",
    "end": "2373730"
  },
  {
    "text": "the better your\nstochastic gradient is as a replacement\nof the true gradient.",
    "start": "2373730",
    "end": "2381099"
  },
  {
    "text": "And of course, the\nbetter [INAUDIBLE] of the true gradient, then\nyou truly get that n times up.",
    "start": "2381100",
    "end": "2387369"
  },
  {
    "text": " So the speed of\nconvergence depends",
    "start": "2387370",
    "end": "2393020"
  },
  {
    "text": "on how noisy the\nstochastic gradients are. It seems like I'm\ngoing too slow. I won't be able to do\na proof, which sucks.",
    "start": "2393020",
    "end": "2400289"
  },
  {
    "text": "But let me actually tell\nyou then about, rather than",
    "start": "2400290",
    "end": "2405350"
  },
  {
    "start": "2402000",
    "end": "2455000"
  },
  {
    "text": "the proof, I think I'll\nshare the proof with Gil. Because the proof that I\nwanted to actually show you,",
    "start": "2405350",
    "end": "2414740"
  },
  {
    "text": "gives a proof of\nstochastic gradient is well-behaved on both\nconvex and nonconvex problems.",
    "start": "2414740",
    "end": "2421400"
  },
  {
    "text": "And the proof I\nwanted to show was for the nonconvex case, because\nit applies to neural networks. So you may be curious\nabout that proof.",
    "start": "2421400",
    "end": "2427690"
  },
  {
    "text": "And remarkably, that\nproof is much simpler than the case of\nconvex problems. So let me just mention\nsome very important points",
    "start": "2427690",
    "end": "2435050"
  },
  {
    "text": "about stochastic gradient. So even though this method\nhas been around since 1951, every deep learning\ntool kit has it,",
    "start": "2435050",
    "end": "2441050"
  },
  {
    "text": "and we are studying it\nin class, there are still gaps between what we\ncan say theoretically",
    "start": "2441050",
    "end": "2447770"
  },
  {
    "text": "and what happens in practice. And I'll show you\nthose gaps already, and encourage you to think\nabout those if you wish.",
    "start": "2447770",
    "end": "2455040"
  },
  {
    "start": "2455000",
    "end": "2666000"
  },
  {
    "text": "So let's look back\nat our problem and deliver two variants. So here are the two variants.",
    "start": "2455040",
    "end": "2460490"
  },
  {
    "text": "I'm going to ask\nif any of you is familiar with these variants\nin some way or the other.",
    "start": "2460490",
    "end": "2465780"
  },
  {
    "text": "So I just call it feasible. Here, there are no constraints. So start with any random\nvector of your choice.",
    "start": "2465780",
    "end": "2475070"
  },
  {
    "text": "In deep network training\nyou have to work harder. And then, this is the\niteration you run--",
    "start": "2475070",
    "end": "2480740"
  },
  {
    "text": "option 1 and option 2. So option 1 says, that was\nthe idea we had in class,",
    "start": "2480740",
    "end": "2486800"
  },
  {
    "text": "randomly pick some\ntraining data point, use its stochastic gradient. ",
    "start": "2486800",
    "end": "2493839"
  },
  {
    "text": "What do we mean\nby randomly pick? The moment you use\nthe word random,",
    "start": "2493840",
    "end": "2498970"
  },
  {
    "text": "you have to define\nwhat's the randomness. So one randomness is\nuniform probability",
    "start": "2498970",
    "end": "2505580"
  },
  {
    "text": "over n training data points. That is one randomness. The other version is you\npick a training data point",
    "start": "2505580",
    "end": "2514020"
  },
  {
    "text": "without replacement. So with replacement means\nuniformly at random.",
    "start": "2514020",
    "end": "2520549"
  },
  {
    "text": "Each time you draw a\nnumber from 1 through n, use their stochastic\ngradient, move on. Which means the same point can\neasily be picked twice, also.",
    "start": "2520550",
    "end": "2529210"
  },
  {
    "text": "And without replacement means,\nif you've picked a point number three, you're not\ngoing to pick it again",
    "start": "2529210",
    "end": "2535450"
  },
  {
    "text": "until you've gone through\nthe entire training data set. Those are two types\nof randomness.",
    "start": "2535450",
    "end": "2543093"
  },
  {
    "text": "Which version would you use?  There is no right or\nwrong answer to this.",
    "start": "2543093",
    "end": "2549400"
  },
  {
    "text": "I'm just taking a poll. What would you use? Think that you're writing\na program for this,",
    "start": "2549400",
    "end": "2557380"
  },
  {
    "text": "and maybe think really\npragmatically, practically. So that's enough of a hint.",
    "start": "2557380",
    "end": "2563049"
  },
  {
    "text": "Which one would you use-- I'm just curious.  Who would use 1?",
    "start": "2563050",
    "end": "2568340"
  },
  {
    "text": "Please, raise hands. OK. And the exclusion--\nthe compliment thereof.",
    "start": "2568340",
    "end": "2575448"
  },
  {
    "text": "I don't know. Maybe some people are undecided. Who would use 2? Very few people.",
    "start": "2575448",
    "end": "2580540"
  },
  {
    "text": "Ooh, OK.  How many of you use\nneural network training",
    "start": "2580540",
    "end": "2586599"
  },
  {
    "text": "toolkits like TensorFlow,\nPyTorch, whatnot? Which version are they using?",
    "start": "2586600",
    "end": "2592040"
  },
  {
    "text": " Actually, every person in the\nreal world is using version 2.",
    "start": "2592040",
    "end": "2601039"
  },
  {
    "text": "Are you really going to\nrandomly go through your RAM each time to pick random points?",
    "start": "2601040",
    "end": "2607170"
  },
  {
    "text": "That'll kill your GPU\nperformance like anything. What people do is\ntake a data set,",
    "start": "2607170",
    "end": "2614230"
  },
  {
    "text": "use a pre-shuffle operation,\nand then just stream through the data. What does streaming\nthrough the data mean?",
    "start": "2614230",
    "end": "2621210"
  },
  {
    "text": "Without replacement. So all the toolkits\nactually are using the without replacement version,\neven though, intuitively,",
    "start": "2621210",
    "end": "2629510"
  },
  {
    "text": "uniform random feels much nicer. And that feeling\nis not ill-founded, because that's the\nonly version we know",
    "start": "2629510",
    "end": "2635750"
  },
  {
    "text": "how to analyze mathematically. So even for this method,\neverybody studies it. There are a million\npapers on it.",
    "start": "2635750",
    "end": "2642590"
  },
  {
    "text": "The version that\nis used in practice is not the version we\nknow how to analyze. It's a major open problem in\nthe field of stochastic gradient",
    "start": "2642590",
    "end": "2651110"
  },
  {
    "text": "to actually analyze the version\nthat we use in practice. It's kind of embarrassing,\nbut without replacement means",
    "start": "2651110",
    "end": "2659550"
  },
  {
    "text": "non-IAD probability theory,\nand non-IAD probability theory is not so easy.",
    "start": "2659550",
    "end": "2664980"
  },
  {
    "text": "That's the answer. OK. So the other version is\nthis mini-batch idea-- which you mentioned\nreally early on--",
    "start": "2664980",
    "end": "2673650"
  },
  {
    "start": "2666000",
    "end": "2970000"
  },
  {
    "text": "is that rather than\npick one random point,",
    "start": "2673650",
    "end": "2679559"
  },
  {
    "text": "I'll pick a mini batch. So I had a million points-- each time, instead of picking\none, maybe I'll pick 10, or.",
    "start": "2679560",
    "end": "2686440"
  },
  {
    "text": "100, or 1,000, or what have you. So this averages things.",
    "start": "2686440",
    "end": "2691940"
  },
  {
    "text": "Averaging things\nreduces the variance. So this is actually\na good thing,",
    "start": "2691940",
    "end": "2697223"
  },
  {
    "text": "because the more\nquantities you average, the less noise you have. That's kind of what\nhappened in probability.",
    "start": "2697223",
    "end": "2704380"
  },
  {
    "text": "So we pick a mini-batch, and\nthe stochastic estimate now",
    "start": "2704380",
    "end": "2711299"
  },
  {
    "text": "is this not just\na single gradient, but averaged over a mini-batch.",
    "start": "2711300",
    "end": "2717490"
  },
  {
    "text": "So a mini-batch of size 1\nis the pure vanilla SGD. Mini-batch of size n is nothing\nother than pure gradient",
    "start": "2717490",
    "end": "2723910"
  },
  {
    "text": "descent. Something in between is\nwhat people actually use. And again, the\ntheoretical analysis",
    "start": "2723910",
    "end": "2730380"
  },
  {
    "text": "only exists if the mini-batch\nis picked with replacement not without replacement.",
    "start": "2730380",
    "end": "2736440"
  },
  {
    "text": "So one of the reasons actually--\na very important thing-- in theory, you don't\ngain too much in terms",
    "start": "2736440",
    "end": "2743150"
  },
  {
    "text": "of computational gains\non convergent speed by using mini-batches. But mini-batches are\nreally crucial, especially",
    "start": "2743150",
    "end": "2750740"
  },
  {
    "text": "in the deep learning,\nGPU-style training, because they allow you\nto do things in parallel.",
    "start": "2750740",
    "end": "2758329"
  },
  {
    "text": "Each thread or each core\nor subcore or small chip or what have, you\ndepending on your hardware,",
    "start": "2758330",
    "end": "2764880"
  },
  {
    "text": "can be working with one\nstochastic gradient. So mini-batches, the larger\nthe mini batch the more things",
    "start": "2764880",
    "end": "2770162"
  },
  {
    "text": "you can do in parallel.  So mini-batches are\ngreatly exploited by people",
    "start": "2770162",
    "end": "2776740"
  },
  {
    "text": "to give you a cheap\nversion of parallelism.",
    "start": "2776740",
    "end": "2782117"
  },
  {
    "text": "And where does the\nparallelism happen? You can think that each core\ncomputes a stochastic gradient.",
    "start": "2782117",
    "end": "2788670"
  },
  {
    "text": "So the hard part is not\nadding these things up and making the update to x,\nthe hard part is computing",
    "start": "2788670",
    "end": "2795859"
  },
  {
    "text": "a stochastic gradient. So if you can compute\n10,000 of those in parallel because you have 10,000\ncores, great for you.",
    "start": "2795860",
    "end": "2804240"
  },
  {
    "text": "And that's the reason people\nlove using mini-batches. But a nice side\nremark here, this also",
    "start": "2804240",
    "end": "2811800"
  },
  {
    "text": "brings us closer to the\nresearch edge of things again. That, well, you'd love to\nuse very large mini-batches",
    "start": "2811800",
    "end": "2818280"
  },
  {
    "text": "so that you can fully max\nout on the parallelism available to you. Maybe you have a\nmulti-GPU system,",
    "start": "2818280",
    "end": "2824070"
  },
  {
    "text": "if you're friends\nwith nVidia or Google. I only have two GPUs.",
    "start": "2824070",
    "end": "2829300"
  },
  {
    "text": "But it depends on how\nmany GPU shows you have. You'd like to really\nmax out on parallelism so that you can really\ncrunch through big data sets",
    "start": "2829300",
    "end": "2837930"
  },
  {
    "text": "as fast as possible. But you know what happens\nwith very large mini-batches? ",
    "start": "2837930",
    "end": "2844510"
  },
  {
    "text": "So if you have very\nlarge mini-batches, stochastic gradient\nstarts looking more like?",
    "start": "2844510",
    "end": "2850480"
  },
  {
    "text": " Full gradient\ndescent, which is also called batch gradient descent.",
    "start": "2850480",
    "end": "2858349"
  },
  {
    "text": "That's not a bad thing. That's awesome for optimization. But it is a weird conundrum\nthat happens in training",
    "start": "2858350",
    "end": "2864890"
  },
  {
    "text": "deep neural networks. This type of problem we wouldn't\nhave for convex optimization. But in deep neural\nnetworks, this",
    "start": "2864890",
    "end": "2870780"
  },
  {
    "text": "is really disturbing\nthing happens, that if you use this\nvery large mini-batches,",
    "start": "2870780",
    "end": "2876770"
  },
  {
    "text": "your method starts\nresembling gradient descent. That means it decreases\nnoise so much so",
    "start": "2876770",
    "end": "2882590"
  },
  {
    "text": "that this region of\nconfusion shrinks so much--",
    "start": "2882590",
    "end": "2887990"
  },
  {
    "text": "which all sounds\ngood, but it ends up being really bad for\nmachine learning. That's what I said,\nthat in machine learning",
    "start": "2887990",
    "end": "2893360"
  },
  {
    "text": "you want some region\nof uncertainty. And what it means actually\nis, a lot of people",
    "start": "2893360",
    "end": "2899590"
  },
  {
    "text": "have been working on this,\nincluding at big companies, that if you reduce that region\nof uncertainty too much,",
    "start": "2899590",
    "end": "2907730"
  },
  {
    "text": "you end up over-fitting\nyour neural network. And then it starts sucking\nin its test data, unseen data",
    "start": "2907730",
    "end": "2916550"
  },
  {
    "text": "performance. So even though for parallelism,\nprogramming, optimization",
    "start": "2916550",
    "end": "2922490"
  },
  {
    "text": "theory, big\nmini-batch is awesome, unfortunately there's price\nto be paid, that it hurts",
    "start": "2922490",
    "end": "2931200"
  },
  {
    "text": "your test error performance. And there are all\nsorts of methods people are trying to cook\nup, including shrinking data",
    "start": "2931200",
    "end": "2940800"
  },
  {
    "text": "accordingly, or chaining\nneural network architecture, and all sorts of ideas. You can cook up your ideas for\nyour favorite architecture,",
    "start": "2940800",
    "end": "2947609"
  },
  {
    "text": "how to make a large\nmini-batch without hurting the final performance. But it's still somewhat\nof an open question",
    "start": "2947610",
    "end": "2954450"
  },
  {
    "text": "on how to optimally select how\nlarge your mini-batch should be.",
    "start": "2954450",
    "end": "2960720"
  },
  {
    "text": "So even though these\nideas are simple, you see that every\nsimple idea leads to an entire sub area of SGD.",
    "start": "2960720",
    "end": "2970880"
  },
  {
    "start": "2970000",
    "end": "3181000"
  },
  {
    "text": "So here are\npractical challenges. People have various heuristics\nfor solving these challenges.",
    "start": "2970880",
    "end": "2977630"
  },
  {
    "text": "You can cook up your\nown, but it's not that one idea always works. So if you look at SGD,\nwhat are the moving parts?",
    "start": "2977630",
    "end": "2988310"
  },
  {
    "text": "The moving parts in SGD-- the gradients, stochastic\ngradient, the step size, the mini batch.",
    "start": "2988310",
    "end": "2994220"
  },
  {
    "text": "So how should I\npick step sizes-- very non-trivial problem.",
    "start": "2994220",
    "end": "2999800"
  },
  {
    "text": "Different deep\nlearning toolkits may have different ways of\nautomating that tuning, but it's one of\nthe painful things.",
    "start": "2999800",
    "end": "3007540"
  },
  {
    "text": "Which mini batch to use? With replacement,\nwithout replacement I already showed you. But which mini batch should I\nuse, how large that should be?",
    "start": "3007540",
    "end": "3015470"
  },
  {
    "text": "Again, not an easy\nquestion to answer. How to compute\nstochastic gradients.",
    "start": "3015470",
    "end": "3020698"
  },
  {
    "text": "Does anybody know how stochastic\ngradients are computed for deep network training?",
    "start": "3020698",
    "end": "3025710"
  },
  {
    "text": "Anybody know?  There is a very famous algorithm\ncalled back propagation.",
    "start": "3025710",
    "end": "3034430"
  },
  {
    "text": "That back propagation\nalgorithm is used to compute a single\nstochastic gradient.",
    "start": "3034430",
    "end": "3039799"
  },
  {
    "text": "Some people use the word\nback prop to mean SGD. But what back prop really\nmeans is some kind of algorithm",
    "start": "3039800",
    "end": "3047060"
  },
  {
    "text": "which computes for you a\nsingle stochastic gradient. And hence this TensorFlow,\net cetera-- these",
    "start": "3047060",
    "end": "3054813"
  },
  {
    "text": "toolkits-- they come up\nwith all sorts of ways to automate the\ncomputation of a gradient. Because, really,\nthat's the main thing.",
    "start": "3054813",
    "end": "3061466"
  },
  {
    "text": "And then other ideas\nlike gradient clipping, and momentum, et cetera. There's a bunch of other ideas. And the theoretical challenges,\nI mentioned to you already--",
    "start": "3061467",
    "end": "3069810"
  },
  {
    "text": "proving that it works,\nthat it actually solves what it set out to do. Unfortunately, I was too slow.",
    "start": "3069810",
    "end": "3075870"
  },
  {
    "text": "I couldn't show you the\nawesome five-line proof that I have that SGD\nworks for neural networks.",
    "start": "3075870",
    "end": "3083420"
  },
  {
    "text": "And theoretical analysis, as\nI said, it's really laggy.",
    "start": "3083420",
    "end": "3088609"
  },
  {
    "text": "My proof also uses\nthe with replacement. And the without\nreplacement version,",
    "start": "3088610",
    "end": "3094390"
  },
  {
    "text": "which is the one that\nis actually implemented, there's very little\nprogress on that.",
    "start": "3094390",
    "end": "3100390"
  },
  {
    "text": "There is some progress. There's a bunch of\npapers, including from our colleagues at MIT,\nbut it's quite unsolved.",
    "start": "3100390",
    "end": "3107050"
  },
  {
    "text": "And the biggest\nquestion, which most of the people in\nmachine learning are currently excited about\nthese days is stuff like,",
    "start": "3107050",
    "end": "3116980"
  },
  {
    "text": "why does SGD work so\nwell for neural networks? We use this crappy\noptimization method,",
    "start": "3116980",
    "end": "3122900"
  },
  {
    "text": "it very rapidly\ndoes some fitting-- the data is large,\nneural network is large,",
    "start": "3122900",
    "end": "3127990"
  },
  {
    "text": "and then this neural\nnetwork ends up having great\nclassification performance. Why is that happening?",
    "start": "3127990",
    "end": "3134060"
  },
  {
    "text": "It's called trying to explain-- build a theory of\ngeneralization. Why does an SGD-trained\nneural network",
    "start": "3134060",
    "end": "3141430"
  },
  {
    "text": "work better than neural\nnetworks train with more fancy optimization methods? It's a mystery, and\nmost of the people",
    "start": "3141430",
    "end": "3147849"
  },
  {
    "text": "who take interest in\ntheoretical machine learning and statistics, that\nis one of the mysteries they're trying to understand.",
    "start": "3147850",
    "end": "3153730"
  },
  {
    "text": "So I think that's\nmy story of SGD. And this is the part we\nskipped, but it's OK.",
    "start": "3153730",
    "end": "3161480"
  },
  {
    "text": "The intuition behind SGD is\nmuch more important in this. So I think we can close.",
    "start": "3161480",
    "end": "3168723"
  },
  {
    "text": "PROFESSOR STRANG: Thank you. [APPLAUSE]",
    "start": "3168723",
    "end": "3175529"
  },
  {
    "text": "And maybe I can learn the\nproof for Monday's lecture. PROFESSOR SRA: Exactly. Yeah, I think so.",
    "start": "3175530",
    "end": "3180695"
  },
  {
    "text": "That'll be great.",
    "start": "3180695",
    "end": "3182800"
  }
]