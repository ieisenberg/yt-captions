[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "start": "0",
    "end": "15520"
  },
  {
    "text": "PROFESSOR: Hi, everyone. We're getting started now. So this week's lecture\nis really picking up",
    "start": "15520",
    "end": "21150"
  },
  {
    "text": "where last week's left off. You may remember we spent\nthe last week talking about cause inference.",
    "start": "21150",
    "end": "26430"
  },
  {
    "text": "And I told you\nhow, for last week, we're going to focus\non a one-time setting.",
    "start": "26430",
    "end": "31590"
  },
  {
    "text": "Well, as we know,\nlots of medicine has to do with multiple\nsequential decisions across time.",
    "start": "31590",
    "end": "37590"
  },
  {
    "text": "And that'll be the focus\nof this whole week's worth of discussions. And as I thought about\nreally what should I",
    "start": "37590",
    "end": "44280"
  },
  {
    "text": "teach in this\nlecture, I realized that the person who knew\nthe most about the topic was in fact a postdoctoral\nresearcher in my lab.",
    "start": "44280",
    "end": "52200"
  },
  {
    "text": "Most about this topic\nin the general area of the medical field. FREDRIK D. JOHANSSON: Thanks. I'll take it.",
    "start": "52200",
    "end": "58732"
  },
  {
    "text": "AUDIENCE: Global [INAUDIBLE]. FREDRIK D. JOHANSSON:\nIt's very fair. PROFESSOR: And so I invited\nhim to come to us today",
    "start": "58732",
    "end": "66760"
  },
  {
    "text": "and to give this as\nan invited lecture. And this is Fredrik Johansson. He'll be a professor in\nChalmers, in Sweden, starting",
    "start": "66760",
    "end": "74250"
  },
  {
    "text": "in September. FREDRIK D. JOHANSSON:\nThank you so much, David. That's very generous. Yeah, so as David\nmentioned, last time we",
    "start": "74250",
    "end": "80760"
  },
  {
    "text": "looked a lot at causal effects. And that's where we will\nstart on this discussion, too.",
    "start": "80760",
    "end": "87067"
  },
  {
    "text": "So I'll just start\nwith this reminder, here-- we essentially introduced\nfour quantities last time, or the last two lectures,\nas far as I know.",
    "start": "87067",
    "end": "95340"
  },
  {
    "text": "We had two potential outcomes,\nwhich represented the outcomes that we would see of\nsome treatment choice",
    "start": "95340",
    "end": "101670"
  },
  {
    "text": "under the various choices. So, the two different choices-- 1 and 0.",
    "start": "101670",
    "end": "107580"
  },
  {
    "text": "We had a set of covariates,\nx and a treatment, t. And we were interested\nin, essentially,",
    "start": "107580",
    "end": "113040"
  },
  {
    "text": "what is the effect\nof this treatment, t, on the outcome, y,\ngiven the covariates, x.",
    "start": "113040",
    "end": "118320"
  },
  {
    "text": "And the effect that we\nfocused on that time was the conditional\naverage treatment effect,",
    "start": "118320",
    "end": "123595"
  },
  {
    "text": "which is exactly the\ndifference between these potential outcomes-- a condition on the features.",
    "start": "123595",
    "end": "129030"
  },
  {
    "text": "So the whole last\nweek was about trying to identify this quantity\nusing various methods.",
    "start": "129030",
    "end": "135340"
  },
  {
    "text": "And the question that\ndidn't come up so much-- or one question that\ndidn't come up too much-- is how do we use this quantity?",
    "start": "135340",
    "end": "142140"
  },
  {
    "text": "We might be\ninterested in it, just in terms of its\nabsolute magnitude.",
    "start": "142140",
    "end": "147930"
  },
  {
    "text": "How large is the effect? But we might also be\ninterested in designing a policy for how to\ntreat our patients based",
    "start": "147930",
    "end": "154290"
  },
  {
    "text": "on this quantity. So today, we will\nfocus on policies.",
    "start": "154290",
    "end": "159847"
  },
  {
    "start": "156000",
    "end": "250000"
  },
  {
    "text": "And what I mean by\nthat, specifically, is something that\ntakes into account what we know about a patient and\nproduces a choice or an action",
    "start": "159847",
    "end": "169370"
  },
  {
    "text": "as an output. Typically, we'll\nthink of policies as depending on medical\nhistory, perhaps",
    "start": "169370",
    "end": "175191"
  },
  {
    "text": "which treatments they\nhave received previously, what state is the\npatient currently in.",
    "start": "175192",
    "end": "181560"
  },
  {
    "text": "But we can also base it\npurely on this number that we produce last time-- the\nconditional average treatment effect.",
    "start": "181560",
    "end": "186720"
  },
  {
    "text": "And one very natural\npolicy is to say, pi of x is equal to the\nindicator function representing if this\nCATE is positive.",
    "start": "186720",
    "end": "194500"
  },
  {
    "text": "So if the effect is positive,\nwe treat the patient. If the effect is\nnegative, we don't. And of course, positive will\nbe relative to the usefulness",
    "start": "194500",
    "end": "202380"
  },
  {
    "text": "of the outcome being high. But yeah, this is a very\nnatural policy to consider.",
    "start": "202380",
    "end": "207990"
  },
  {
    "text": "However, we can also think about\nmuch more complicated policies",
    "start": "207990",
    "end": "213530"
  },
  {
    "text": "that are not just\nbased on this number-- the quality of the outcome.",
    "start": "213530",
    "end": "219550"
  },
  {
    "text": "We can think about\npolicies that take into account legislation or cost\nof medication or side effects.",
    "start": "219550",
    "end": "225004"
  },
  {
    "text": "We're not going\nto do that today, but that's something\nthat you can keep in mind as we discuss these things. ",
    "start": "225005",
    "end": "231600"
  },
  {
    "text": "So David mentioned,\nwe should now move from the one-step\nsetting, where we have a single treatment\nacting at a single time",
    "start": "231600",
    "end": "238810"
  },
  {
    "text": "and we only have to take\ninto account the state of a patient once, basically. And we will move from that\nto the sequential setting.",
    "start": "238810",
    "end": "246040"
  },
  {
    "text": "And my first example of such a\nsetting is sepsis management.",
    "start": "246040",
    "end": "251280"
  },
  {
    "start": "250000",
    "end": "368000"
  },
  {
    "text": "So, sepsis is a complication\nof an infection, which can have very disastrous consequences.",
    "start": "251280",
    "end": "257816"
  },
  {
    "text": "It can lead to organ failure\nand ultimately death. And it's actually\none of the leading causes of deaths in the ICU.",
    "start": "257816",
    "end": "263879"
  },
  {
    "text": "So it's of course important\nthat we can manage and treat this condition.",
    "start": "263880",
    "end": "268963"
  },
  {
    "text": "When you start treating\nsepsis, the primary target-- the first things you\nshould think about fixing--",
    "start": "268963",
    "end": "273980"
  },
  {
    "text": "is the infection itself. If we don't treat\nthe infection, things are going to keep being bad.",
    "start": "273980",
    "end": "279413"
  },
  {
    "text": "But even if we figure\nout the right antibiotic to treat the infection that is\nthe source of the septic shock",
    "start": "279413",
    "end": "284780"
  },
  {
    "text": "or the septic\ninflammation, there are a lot of\ndifferent conditions that we need to manage.",
    "start": "284780",
    "end": "290660"
  },
  {
    "text": "Because the infection\nitself can lead to fever, breathing difficulties, low\nblood pressure, high heart",
    "start": "290660",
    "end": "297930"
  },
  {
    "text": "rate-- all these kinds of things\nthat are symptoms, but not the cause in themselves.",
    "start": "297930",
    "end": "303033"
  },
  {
    "text": "But we still have to\nmanage them somehow so that the patient\nsurvives and is comfortable.",
    "start": "303033",
    "end": "309370"
  },
  {
    "text": "So when I say sepsis\nmanagement, I'm talking about managing\nsuch quantities over time--",
    "start": "309370",
    "end": "315580"
  },
  {
    "text": "over a patient's\nstay in the hospital. So, last time-- again, just\nto really hammer this in-- we",
    "start": "315580",
    "end": "323319"
  },
  {
    "text": "talked about potential\noutcomes and the choice of a single treatment. So we can think about this in\nthe septic setting as a patient",
    "start": "323320",
    "end": "330669"
  },
  {
    "text": "coming in-- or a patient\nalready being in the hospital, presumably-- and is presenting with\nbreathing difficulties.",
    "start": "330670",
    "end": "336669"
  },
  {
    "text": "So that means that their blood\noxygen will be low because they can't breathe on their own. And we might want to put them\non mechanical ventilation",
    "start": "336670",
    "end": "343150"
  },
  {
    "text": "so that we can ensure that\nthey get sufficient oxygen. We can view this\nas a single choice.",
    "start": "343150",
    "end": "348950"
  },
  {
    "text": "Should we put the patient on\nmechanical ventilation or not? But what we need to\ntake into account here",
    "start": "348950",
    "end": "356630"
  },
  {
    "text": "is what will happen after\nwe make that choice. What will be the side effects\nof this choice going further?",
    "start": "356630",
    "end": "362210"
  },
  {
    "text": "Because we want to make sure\nthat the patient is comfortable and in good health\nthroughout their stay.",
    "start": "362210",
    "end": "368370"
  },
  {
    "start": "368000",
    "end": "474000"
  },
  {
    "text": "So today, we will move towards\nsequential decision making.",
    "start": "368370",
    "end": "373500"
  },
  {
    "text": "And in particular, what\nI alluded to just now is that decisions\nmade in sequence",
    "start": "373500",
    "end": "378539"
  },
  {
    "text": "may have the property that\nchoices early on rule out certain choices later. And we'll see an example\nof that very soon.",
    "start": "378540",
    "end": "386675"
  },
  {
    "text": "And in particular, we'll\nbe interested in coming up with a policy for making\ndecisions repeatedly",
    "start": "386675",
    "end": "393090"
  },
  {
    "text": "that optimizes a given outcome-- something that we care about. It could be minimize\nthe risk of death.",
    "start": "393090",
    "end": "400620"
  },
  {
    "text": "It could be a reward that says\nthat the vitals of a patients are in the right range.",
    "start": "400620",
    "end": "405810"
  },
  {
    "text": "We might want to optimize that. But essentially,\nthink about it now as having this choice\nof administering",
    "start": "405810",
    "end": "413280"
  },
  {
    "text": "a medication or an\nintervention at any time, t-- and having the best\npolicy for doing so.",
    "start": "413280",
    "end": "420440"
  },
  {
    "text": "OK, I'm going to skip that one. OK, so I mentioned already\none potential choice",
    "start": "420440",
    "end": "426376"
  },
  {
    "text": "that we might want to\nmake in the management of a septic patient,\nwhich is to put them on mechanical ventilation\nbecause they can't breathe",
    "start": "426377",
    "end": "432110"
  },
  {
    "text": "on their own. A side effect of\ndoing so is that they might suffer discomfort\nfrom being intubated.",
    "start": "432110",
    "end": "441650"
  },
  {
    "text": "The procedure is not painless,\nit's not without discomfort. So something that you\nmight have to do--",
    "start": "441650",
    "end": "446703"
  },
  {
    "text": "putting them on\nmechanical ventilation-- is to sedate the patient. So this is an action\nthat is informed",
    "start": "446703",
    "end": "454427"
  },
  {
    "text": "by the previous action,\nbecause if we didn't put the patient on\nmechanical ventilation, maybe we wouldn't consider\nthem for sedation.",
    "start": "454427",
    "end": "459725"
  },
  {
    "text": " When we sedate a\npatient, we run the risk",
    "start": "459725",
    "end": "465810"
  },
  {
    "text": "of lowering their\nblood pressure. So we might need to\nmanage that, too.",
    "start": "465810",
    "end": "471780"
  },
  {
    "text": "So if their blood\npressure gets too low, maybe we need to\nadminister vasopressors, which artificially raise\nthe blood pressure,",
    "start": "471780",
    "end": "478200"
  },
  {
    "text": "or fluids or anything else\nthat takes care of this issue. So just think of\nthis as an example",
    "start": "478200",
    "end": "485050"
  },
  {
    "text": "of choices cascading, in\nterms of their consequences, as we roll forward in time.",
    "start": "485050",
    "end": "490590"
  },
  {
    "text": " Ultimately, we will face the\nend of the patient's stay.",
    "start": "490590",
    "end": "497690"
  },
  {
    "start": "493000",
    "end": "587000"
  },
  {
    "text": "And hopefully, we managed the\npatient in a successful way so that their response or\ntheir outcome is a good one.",
    "start": "497690",
    "end": "506330"
  },
  {
    "text": "What I'm illustrating\nhere is that, for any one patient in our hospitals or\nin the health care system,",
    "start": "506330",
    "end": "512990"
  },
  {
    "text": "we will only observe\none trajectory through these options. So I will show this type\nof illustration many times,",
    "start": "512990",
    "end": "521250"
  },
  {
    "text": "but I hope that you can realize\nthe scope of the decision space",
    "start": "521250",
    "end": "527300"
  },
  {
    "text": "here. Essentially, at any point, we\ncan choose a different action. And usually, the\nnumber of decisions",
    "start": "527300",
    "end": "532520"
  },
  {
    "text": "that we make in an ICU\nsetting, for example,",
    "start": "532520",
    "end": "537810"
  },
  {
    "text": "is much larger\nthan we could ever test in a randomized trial. Think of all of these\ndifferent trajectories",
    "start": "537810",
    "end": "545449"
  },
  {
    "text": "as being different arms in a\nrandomized controlled trial that you want to compare the\neffects or the outcomes of.",
    "start": "545450",
    "end": "553009"
  },
  {
    "text": "It's infeasible to run\nsuch a trial, typically. So one of the big\nreasons that we are talking about reinforcement\nlearning today and talking",
    "start": "553010",
    "end": "559640"
  },
  {
    "text": "about learning\npolicies, rather than causal effects in the setup\nthat we did last week, is because the space of\npossible action trajectories",
    "start": "559640",
    "end": "566930"
  },
  {
    "text": "is so large. ",
    "start": "566930",
    "end": "575010"
  },
  {
    "text": "Having said that, we now\nturn to trying to find,",
    "start": "575010",
    "end": "580620"
  },
  {
    "text": "essentially, the policy that\npicks this orange path here-- that leads to a good outcome. And to reason\nabout such a thing,",
    "start": "580620",
    "end": "588290"
  },
  {
    "start": "587000",
    "end": "699000"
  },
  {
    "text": "we need to also reason about\nwhat is a good outcome? What is good reward for\nour agent, as it proceeds",
    "start": "588290",
    "end": "596220"
  },
  {
    "text": "through time and makes choices? Some policies that we\nproduce as machine learners",
    "start": "596220",
    "end": "602970"
  },
  {
    "text": "might not be appropriate\nfor a health care setting. We have to somehow restrict\nourself to something that's",
    "start": "602970",
    "end": "608700"
  },
  {
    "text": "realistic. I won't focus very\nmuch on this today. It's something that will come\nup in the discussion tomorrow,",
    "start": "608700",
    "end": "614580"
  },
  {
    "text": "hopefully. And also the notion of\nevaluating something for use in the\nhealth care system",
    "start": "614580",
    "end": "620400"
  },
  {
    "text": "will also be talked\nabout tomorrow. AUDIENCE: Thursday. FREDRIK D. JOHANSSON:\nSorry, Thursday.",
    "start": "620400",
    "end": "626370"
  },
  {
    "text": "Next time. OK, so I'll start by just\nbriefly mentioning some success",
    "start": "626370",
    "end": "633510"
  },
  {
    "text": "stories. And these are not\nfrom the health care setting, as you can\nguess from the pictures. How many have seen\nsome of these pictures?",
    "start": "633510",
    "end": "641760"
  },
  {
    "text": "OK, great-- almost everyone. ",
    "start": "641760",
    "end": "647360"
  },
  {
    "text": "Yeah, so these are from various\nvideo games-- almost all of them. Well, games anyhow.",
    "start": "647360",
    "end": "654630"
  },
  {
    "text": "And these are good examples\nof when reinforcement learning",
    "start": "654630",
    "end": "659680"
  },
  {
    "text": "works, essentially. That's why I use these\nin this slide here--",
    "start": "659680",
    "end": "665170"
  },
  {
    "text": "because, essentially,\nit's very hard to argue that the\ncomputer or the program that eventually beat Lee Sedol.",
    "start": "665170",
    "end": "672160"
  },
  {
    "text": "I think it's in this picture\nbut also, later, Go champions, essentially. In the AlphaGo picture\nin the top left,",
    "start": "672160",
    "end": "678451"
  },
  {
    "text": "it's hard to argue that\nthey're not doing a good job, because they clearly\nbeat humans here.",
    "start": "678452",
    "end": "684220"
  },
  {
    "text": "But one of the things\nI want you to keep in mind throughout\nthis talk is what is different between\nthese kinds of scenarios?",
    "start": "684220",
    "end": "690850"
  },
  {
    "text": "And we'll come\nback to that later. And what is different\nto the health care setting, essentially?",
    "start": "690850",
    "end": "696400"
  },
  {
    "text": "So I simply added\nanother example here, that's why I recognize it. So there was recently one\nthat's a little bit closer",
    "start": "696400",
    "end": "702097"
  },
  {
    "text": "to my heart, which is AlphaStar. I play StarCraft. I like StarCraft, so it\nshould be on the slide.",
    "start": "702097",
    "end": "709380"
  },
  {
    "text": "Anyway, let's move on. Broadly speaking,\nthese can be summarized",
    "start": "709380",
    "end": "715548"
  },
  {
    "start": "712000",
    "end": "887000"
  },
  {
    "text": "in the following picture. What goes into those systems? There's a lot more nuance when\nit comes to something like Go.",
    "start": "715548",
    "end": "722960"
  },
  {
    "text": "But for the purpose\nof this class, we will summarize\nthem with a slide. So essentially, one of\nthe three quantities",
    "start": "722960",
    "end": "730292"
  },
  {
    "text": "that matters for a\nreinforcement learning is the state of the environment,\nthe state of the game, the state of the patient--",
    "start": "730292",
    "end": "736260"
  },
  {
    "text": "the state of the thing that we\nwant to optimize, essentially. So in this case, I've\nchosen Tic-tac-toe here.",
    "start": "736260",
    "end": "742770"
  },
  {
    "text": "We have a state which\nrepresents the current positions of the circles and crosses.",
    "start": "742770",
    "end": "748620"
  },
  {
    "text": "And given that state of the\ngame, my job as a player is to choose one of\nthe possible actions--",
    "start": "748620",
    "end": "757829"
  },
  {
    "text": "one of the free squares\nto put my cross in. So I'm the blue\nplayer here and I can consider these five choices\nfor where to put my next cross.",
    "start": "757830",
    "end": "767680"
  },
  {
    "text": "And each of those will lead\nme to a new state of the game. If I put my cross\nover here, that",
    "start": "767680",
    "end": "774389"
  },
  {
    "text": "means that I'm now in this box. And I have a new set of\nactions available to me for the next round, depending\non what the red player does.",
    "start": "774390",
    "end": "782720"
  },
  {
    "text": "So we have the state,\nwe have the actions, and we have the next\nstate, essentially-- we have a trajectory or\na transition of states.",
    "start": "782720",
    "end": "788329"
  },
  {
    "text": "And the last quantity that we\nneed is the notion of a reward. That's very important\nfor reinforcement learning, because that's what's\ndriving the learning itself.",
    "start": "788330",
    "end": "795420"
  },
  {
    "text": "We strive to optimize the reward\nor the outcome of something. So if we look at the action\nto the farthest right here,",
    "start": "795420",
    "end": "802410"
  },
  {
    "text": "essentially I left myself open\nto an attack by the red player here, because I didn't\nput my cross there.",
    "start": "802410",
    "end": "808050"
  },
  {
    "text": "Which means that, probably,\nif the red player is decent, he will put his\ncircle here and I will incur a loss, essentially.",
    "start": "808050",
    "end": "815070"
  },
  {
    "text": "So my reward will be negative,\nif we take positive to be good. And this is something that I\ncan learn from going forward.",
    "start": "815070",
    "end": "821375"
  },
  {
    "text": "Essentially, what\nI want to avoid is ending up in\nthis state that's shown in the bottom right here.",
    "start": "821375",
    "end": "826470"
  },
  {
    "text": "This is the basic\nidea of reinforcement learning for video games\nand for anything else.",
    "start": "826470",
    "end": "834160"
  },
  {
    "text": "So if we take this board\nanalogy or this example and move to the\nhealth care setting,",
    "start": "834160",
    "end": "839290"
  },
  {
    "text": "we can think of the state of\na patient as the game board or the state of the game.",
    "start": "839290",
    "end": "846120"
  },
  {
    "text": "We will always call\nthis St in this talk. The treatments that we prescribe\nor interventions will be At.",
    "start": "846120",
    "end": "854170"
  },
  {
    "text": "And these are like the actions\nin the game, obviously. The outcomes of a patient--\ncould be mortality, could be managing vitals--",
    "start": "854170",
    "end": "860329"
  },
  {
    "text": "will be as the rewards in\nthe game, having lost or won. And then up at the end here,\nwhat could possibly go wrong.",
    "start": "860330",
    "end": "867339"
  },
  {
    "text": "Well, as I alluded\nto before, health is not a game in the same sense\nthat a video game is a game.",
    "start": "867340",
    "end": "873840"
  },
  {
    "text": "But they share a lot of\nmathematical structure. So that's why I make\nthe analogy here. These quantities\nhere-- S, A, and R--",
    "start": "873840",
    "end": "883190"
  },
  {
    "text": "will form something\ncalled a decision process. And that's what we'll\ntalk about next.",
    "start": "883190",
    "end": "888470"
  },
  {
    "start": "887000",
    "end": "2190000"
  },
  {
    "text": "This is the outline\nfor today and Thursday. I won't get to this\ntoday, but this",
    "start": "888470",
    "end": "894570"
  },
  {
    "text": "is the talks we're considering. So a decision process\nis essentially the world that describes\nthe data that we access",
    "start": "894570",
    "end": "902690"
  },
  {
    "text": "or the world that we're\nmanaging our agent in. ",
    "start": "902690",
    "end": "909920"
  },
  {
    "text": "Very often, if you've ever seen\nreinforcement learning taught, you have seen this picture\nin some form, usually.",
    "start": "909920",
    "end": "916100"
  },
  {
    "text": "Sometimes there's a\nmouse and some cheese and there's other\nthings going on, but you know what\nI'm talking about.",
    "start": "916100",
    "end": "923503"
  },
  {
    "text": "But there are the\nsame basic components. So there's the\nconcept of an agent-- let's think doctor for now--",
    "start": "923503",
    "end": "930410"
  },
  {
    "text": "that takes actions\nrepeatedly over time. So this t here indicates\nan index of time",
    "start": "930410",
    "end": "935990"
  },
  {
    "text": "and we see that\nessentially increasing as we spin around\nthis wheel here. We move forward in time.",
    "start": "935990",
    "end": "941690"
  },
  {
    "text": "So an agent takes an action\nand, at any time point, receives a reward\nfor that action.",
    "start": "941690",
    "end": "947210"
  },
  {
    "text": "And that would be\nRt, as I said before. The environment is responsible\nfor giving that reward.",
    "start": "947210",
    "end": "952560"
  },
  {
    "text": "So for example, if I'm\nthe doctor, I'm the agent, I make an action or an\nintervention to my patient,",
    "start": "952560",
    "end": "958790"
  },
  {
    "text": "the patient will\nbe the environment. And essentially, responses do\nnot respond to my intervention.",
    "start": "958790",
    "end": "965200"
  },
  {
    "text": "The state here is the\nstate of the patient, as I mentioned\nbefore, for example. But it might also be a state\nmore broadly than the patient,",
    "start": "965200",
    "end": "973779"
  },
  {
    "text": "like the settings of the\nmachine that they're attached to or the availability of\ncertain drugs in the hospital",
    "start": "973780",
    "end": "981610"
  },
  {
    "text": "or something like that. So we can think\na little bit more broadly around the patient, too. I said partially observed here,\nin that I might not actually",
    "start": "981610",
    "end": "989207"
  },
  {
    "text": "know everything about the\npatient that's relevant to me. And we will come back a\nlittle bit later to that.",
    "start": "989207",
    "end": "994420"
  },
  {
    "text": "So there are two different\nformalizations that are very close to each other, which\nis when you'd know everything",
    "start": "994420",
    "end": "1000510"
  },
  {
    "text": "about s and when you don't. We will, for the longest\npart of this talk, focus on the way I\nknow everything that is",
    "start": "1000510",
    "end": "1006660"
  },
  {
    "text": "relevant about the environment. OK, to make this all\na bit more concrete, I'll return to the picture\nthat I showed you before,",
    "start": "1006660",
    "end": "1014080"
  },
  {
    "text": "but now put it in context\nof the paper that you read. Was that the compulsory one? The mechanical ventilation?",
    "start": "1014080",
    "end": "1021020"
  },
  {
    "text": "OK, great. So in this case, they had an\ninteresting reward structure,",
    "start": "1021020",
    "end": "1029069"
  },
  {
    "text": "essentially. The thing that they\nwere trying to optimize was the reward related to\nthe vitals of the patient. But also whether they were\nkept on mechanical ventilation",
    "start": "1029069",
    "end": "1037760"
  },
  {
    "text": "or not. And the idea of this\npaper is that you don't want to keep a\npatient unnecessarily",
    "start": "1037760",
    "end": "1043879"
  },
  {
    "text": "on mechanical ventilation,\nbecause it has the side effects that we talked about before.",
    "start": "1043880",
    "end": "1049260"
  },
  {
    "text": "So at any point in\ntime, essentially, we can think about taking\na patient on or off--",
    "start": "1049260",
    "end": "1054330"
  },
  {
    "text": "and also dealing with\nthe sedatives that are prescribed to them.",
    "start": "1054330",
    "end": "1060490"
  },
  {
    "text": "So in this example, the\nstate that they considered in this application included\nthe demographic information",
    "start": "1060490",
    "end": "1069570"
  },
  {
    "text": "of the patient, which doesn't\nreally change over time. Their physiological\nmeasurements, ventilator settings,\nconsciousness level,",
    "start": "1069570",
    "end": "1077670"
  },
  {
    "text": "the dosages of the\nsedatives they use, which could be an\naction, I suppose--",
    "start": "1077670",
    "end": "1082740"
  },
  {
    "text": "and a number of other things. And these are the values that\nwe have to keep track of,",
    "start": "1082740",
    "end": "1088529"
  },
  {
    "text": "moving forward in time. The actions concretely included\nwhether to intubate or extubate",
    "start": "1088530",
    "end": "1095400"
  },
  {
    "text": "the patient, as well\nas the administer and dosing the sedatives.",
    "start": "1095400",
    "end": "1102370"
  },
  {
    "text": "So this is, again, an example\nof a so-called decision process. And essentially, the\nprocess is the distribution",
    "start": "1102370",
    "end": "1110710"
  },
  {
    "text": "of these quantities that I've\nbeen talking about over time. So we have the states, the\nactions, and the rewards.",
    "start": "1110710",
    "end": "1115960"
  },
  {
    "text": "They all traverse or they\nall evolve over time. And the loss of how that\nhappens is the decision process.",
    "start": "1115960",
    "end": "1124202"
  },
  {
    "text": "I mentioned before\nthat we will be talking about policies today. And typically,\nthere's a distinction",
    "start": "1124203",
    "end": "1131649"
  },
  {
    "text": "between what is called a\nbehavior policy and a target policy-- or there are\ndifferent words for this.",
    "start": "1131650",
    "end": "1137140"
  },
  {
    "text": "Essentially, the\nthing that we observe is usually called\na behavior policy. By that, I mean if we go to\na hospital and watch what's",
    "start": "1137140",
    "end": "1143620"
  },
  {
    "text": "happening there at\nthe moment, that will be the behavior policy. And I will denote that mu. So that is what we have to\nlearn from, essentially.",
    "start": "1143620",
    "end": "1152630"
  },
  {
    "text": "So decision processes so\nfar are incredibly general.",
    "start": "1152630",
    "end": "1157850"
  },
  {
    "text": "I haven't said anything about\nwhat this distribution is like, but the absolutely\ndominant restriction",
    "start": "1157850",
    "end": "1164600"
  },
  {
    "text": "that people make when they\nstudy system processes is to look at Markov\ndecision processes.",
    "start": "1164600",
    "end": "1169920"
  },
  {
    "text": "And these have a specific\nconditional independent structure that I will illustrate\nin the next slide-- well, I'll just define it\nmathematically here.",
    "start": "1169920",
    "end": "1176380"
  },
  {
    "text": "It says, essentially,\nthat all of the quantities that we care about-- the states. I guess that should say state.",
    "start": "1176380",
    "end": "1182720"
  },
  {
    "text": "Rewards and the actions only\ndepend on the most recent state in action.",
    "start": "1182720",
    "end": "1187890"
  },
  {
    "text": " If we observe an action taken\nby a doctor in the hospital,",
    "start": "1187890",
    "end": "1196160"
  },
  {
    "text": "for example-- to make a mark of\nassumption, we'd say that this\ndoctor did not look",
    "start": "1196160",
    "end": "1201289"
  },
  {
    "text": "at anything that\nhappened earlier in time or any other\ninformation than what is in the state variable\nthat we observe at that time.",
    "start": "1201290",
    "end": "1208730"
  },
  {
    "text": "That is the assumption\nthat we make. Yeah? AUDIENCE: Is that an assumption\nyou can make for a health care?",
    "start": "1208730",
    "end": "1215630"
  },
  {
    "text": "Because in the end, you don't\nhave access to the real state, but only about what's measured\nabout the state in health care.",
    "start": "1215630",
    "end": "1223700"
  },
  {
    "text": "FREDRIK D. JOHANSSON:\nIt's a very good question. So the nice thing in terms of\ninferring causal quantities",
    "start": "1223700",
    "end": "1230330"
  },
  {
    "text": "is that we only\nneed the things that were used to make the\ndecision in the first place. So the doctor can only act\non such information, too.",
    "start": "1230330",
    "end": "1237529"
  },
  {
    "text": "Unless we don't\nrecord everything that the doctor knows--\nwhich is also the case. So that is something that we\nhave to worry about for sure.",
    "start": "1237530",
    "end": "1245300"
  },
  {
    "text": "Another way to lose\ninformation, as I mentioned, that is relevant for\nthis is if we look to--",
    "start": "1245300",
    "end": "1252315"
  },
  {
    "text": " What's the opposite of far?",
    "start": "1252315",
    "end": "1258375"
  },
  {
    "text": "AUDIENCE: Near. FREDRIK D. JOHANSSON: Too near\nback in time, essentially. So we don't look at the\nentire history of the patient.",
    "start": "1258375",
    "end": "1264270"
  },
  {
    "text": "And when I say St\nhere, it doesn't have to be the instantaneous\nsnapshot of a patient.",
    "start": "1264270",
    "end": "1273112"
  },
  {
    "text": "We can also Include\nhistory there. Again, we'll come back\nto that a little later. ",
    "start": "1273113",
    "end": "1280350"
  },
  {
    "text": "OK, so the Markov assumption\nessentially looks like this. Or this is how I will\nillustrate, anyway.",
    "start": "1280350",
    "end": "1287100"
  },
  {
    "text": "We have a sequence of states\nhere that evolve over time. I'm allowing myself\nto put some dots here,",
    "start": "1287100",
    "end": "1292470"
  },
  {
    "text": "because I don't want\nto draw forever. But essentially, you could think\nof this pattern repeating--",
    "start": "1292470",
    "end": "1297630"
  },
  {
    "text": "where the previous state\ngoes into the next state, the action goes\ninto the next state, and the action and state\ngoes in through the reward.",
    "start": "1297630",
    "end": "1305320"
  },
  {
    "text": "This is the world that we\nwill live in for this lecture. Something that's not allowed\nunder the mark of assumption",
    "start": "1305320",
    "end": "1310950"
  },
  {
    "text": "is an edge like this, which says\nthat an action at an early time influences an action\nat a later time.",
    "start": "1310950",
    "end": "1318150"
  },
  {
    "text": "And specifically, it can't\ndo so without passing through a state, for example.",
    "start": "1318150",
    "end": "1324360"
  },
  {
    "text": "It very well can\nhave an influence on At by this trajectory\nhere, but not directly.",
    "start": "1324360",
    "end": "1331132"
  },
  {
    "text": "That that's the Markov\nassumption in this case. ",
    "start": "1331132",
    "end": "1338299"
  },
  {
    "text": "So you can see that if I\nwere to draw the graph of all",
    "start": "1338300",
    "end": "1343330"
  },
  {
    "text": "the different\nmeasurements that we see during a state,\nessentially there",
    "start": "1343330",
    "end": "1351100"
  },
  {
    "text": "are a lot of errors that I could\nhave had in this picture that I don't have. So it may seem that the\nMarkov assumption is",
    "start": "1351100",
    "end": "1356710"
  },
  {
    "text": "a very strong one, but one\nway to ensure that the Markov assumption is more likely\nis to include more things",
    "start": "1356710",
    "end": "1363370"
  },
  {
    "text": "in your state, including\nsummaries of the history, et cetera, that I\nmentioned before.",
    "start": "1363370",
    "end": "1368810"
  },
  {
    "text": "An even stronger restriction\nof decision processes is to assume that\nthe states over time",
    "start": "1368810",
    "end": "1375590"
  },
  {
    "text": "are themselves independent. So this goes by\ndifferent names--",
    "start": "1375590",
    "end": "1381530"
  },
  {
    "text": "sometimes under the\nname contextual bandits. But the bandits part of that\nitself is not so relevant here.",
    "start": "1381530",
    "end": "1387590"
  },
  {
    "text": "So let's not go into\nthat name too much. But essentially, what\nwe can say is here, the state at a later time point\nis not influenced directly",
    "start": "1387590",
    "end": "1395300"
  },
  {
    "text": "by the state at a\nprevious time point, nor the action of the\nprevious time point. So if you remember\nwhat you did last week,",
    "start": "1395300",
    "end": "1402950"
  },
  {
    "text": "this looks like\nbasically T repetitions of the very simple\ngraph that we had for estimating\npotential outcomes.",
    "start": "1402950",
    "end": "1409250"
  },
  {
    "text": "And that is indeed\nmathematically equivalent. If we assume that\nthis S here represents",
    "start": "1409250",
    "end": "1417390"
  },
  {
    "text": "the state of a patient\nand all patients are drawn from some sum\nprocess, essentially.",
    "start": "1417390",
    "end": "1423360"
  },
  {
    "text": "So that S0, 1, et cetera,\nup to St are all i.i.d.",
    "start": "1423360",
    "end": "1428549"
  },
  {
    "text": "draws of the same distribution. Then we have,\nessentially, a model for t different patients\nwith a single time step",
    "start": "1428550",
    "end": "1437970"
  },
  {
    "text": "or single action,\ninstead of them being dependent in some way.",
    "start": "1437970",
    "end": "1443490"
  },
  {
    "text": "So we can see that by going\nbackwards through my slides,",
    "start": "1443490",
    "end": "1448530"
  },
  {
    "text": "this is essentially\nwhat we had last week. And we just have\nto add more arrows to get to whatever we have\nthis week, which indicates",
    "start": "1448530",
    "end": "1455610"
  },
  {
    "text": "that last week was a\nspecial case of this-- just as David said before.",
    "start": "1455610",
    "end": "1460850"
  },
  {
    "text": "It also hints at the\nreinforcement learning problem being more complicated than\nthe potential outcomes problem.",
    "start": "1460850",
    "end": "1466940"
  },
  {
    "text": "And we'll see more\nexamples of that later. But, like with causal\neffect estimation",
    "start": "1466940",
    "end": "1474740"
  },
  {
    "text": "that we did last week, we're\ninterested in the influences of just a few\nvariables, essentially.",
    "start": "1474740",
    "end": "1480890"
  },
  {
    "text": "So last time we studied the\neffect of a single treatment choice. And in this case, we\nwill study the influence",
    "start": "1480890",
    "end": "1486559"
  },
  {
    "text": "of these various actions\nthat we take along the way. That will be the goal. And it could be either\nthrough an immediate effect",
    "start": "1486560",
    "end": "1494539"
  },
  {
    "text": "on the immediate reward or\nit can be through the impact that an action has on the\nstate trajectory itself.",
    "start": "1494540",
    "end": "1500340"
  },
  {
    "start": "1500340",
    "end": "1507760"
  },
  {
    "text": "I told you about the\nworld now that we live in. We have these Ss and As and Rs. And I haven't told you so\nmuch about the goal that we're",
    "start": "1507760",
    "end": "1515140"
  },
  {
    "text": "trying to solve or the problem\nthat we're trying to solve. Most RL-- or reinforcement\nand learning--",
    "start": "1515140",
    "end": "1522130"
  },
  {
    "text": "is aimed at optimizing\nthe value of a policy",
    "start": "1522130",
    "end": "1527590"
  },
  {
    "text": "or finding a policy\nthat has a good return, a good sum of rewards. There are many names for\nthis, but essentially a policy",
    "start": "1527590",
    "end": "1534640"
  },
  {
    "text": "that does well. The notion of well that we\nwill be using in this lecture",
    "start": "1534640",
    "end": "1540970"
  },
  {
    "text": "is that of a return. So the return at a time step\nt, following the policy, pi,",
    "start": "1540970",
    "end": "1550330"
  },
  {
    "text": "that I had before, is the\nsum of the future rewards that we see if we were to\nact according to that policy.",
    "start": "1550330",
    "end": "1557900"
  },
  {
    "text": "So essentially, I stop now. I ask, OK, if I keep on\ndoing the same as I've done through my whole life--",
    "start": "1557900",
    "end": "1564272"
  },
  {
    "text": "maybe that was a good policy. I don't know. And keep going until the end\nof time, how well will I do?",
    "start": "1564272",
    "end": "1570400"
  },
  {
    "text": "What is the sum of those\nrewards that I get, essentially? That's the return.",
    "start": "1570400",
    "end": "1575680"
  },
  {
    "text": "The value is the\nexpectation of such things. So if I'm not the\nonly person, but there is the whole population\nof us, the expectation",
    "start": "1575680",
    "end": "1582460"
  },
  {
    "text": "over that population is\nthe value of the policy. So if we take patients as a\nbetter analogy than my life,",
    "start": "1582460",
    "end": "1588409"
  },
  {
    "text": "maybe, the expectation\nis over patients. If we fact on every patient in\nour population the same way--",
    "start": "1588410",
    "end": "1594700"
  },
  {
    "text": "according to the same\npolicy, that is-- what is the expected\nreturn over those patients?",
    "start": "1594700",
    "end": "1601610"
  },
  {
    "text": "So as an example, I drew\na few trajectories again, because I like drawing. And we can think about three\ndifferent patients here.",
    "start": "1601610",
    "end": "1609120"
  },
  {
    "text": "They start in different states. And they will have different\naction trajectories as a result.",
    "start": "1609120",
    "end": "1614570"
  },
  {
    "text": "So we're treating them\nwith the same policy. Let's call it pi. But because they're\nin different states,",
    "start": "1614570",
    "end": "1619870"
  },
  {
    "text": "they will have different\nactions at the same times. So here we take a 0\naction, we go down.",
    "start": "1619870",
    "end": "1626038"
  },
  {
    "text": "Here, we take a 0\naction, we go down. That's what that means here. The specifics of this\nis not so important.",
    "start": "1626038",
    "end": "1631385"
  },
  {
    "text": "But what I want you\nto pay attention to is that, after each\naction, we get a reward.",
    "start": "1631385",
    "end": "1636860"
  },
  {
    "text": "And at the end, we can sum\nthose up and that's our return. So each patient has one value\nfor their own trajectory.",
    "start": "1636860",
    "end": "1647820"
  },
  {
    "text": "And the value of the policy\nis then the average value of such trajectories. ",
    "start": "1647820",
    "end": "1655570"
  },
  {
    "text": "So that is what we're\ntrying to optimize. We have now a notion of good\nand we want to find a pi such",
    "start": "1655570",
    "end": "1661950"
  },
  {
    "text": "that V pi up there is good. That's the goal. ",
    "start": "1661950",
    "end": "1668690"
  },
  {
    "text": "So I think it's time for\na bit of an example here. I want you to play\nalong in a second.",
    "start": "1668690",
    "end": "1676430"
  },
  {
    "text": "You're going to\nsolve this problem. It's not a hard one. So I think you'll manage.",
    "start": "1676430",
    "end": "1681740"
  },
  {
    "text": "I think you'll be fine. But this is now yet another\nexample of a world to be in. This is the robot in a room.",
    "start": "1681740",
    "end": "1688250"
  },
  {
    "text": "And I've stolen this\nslide from David, who stole it from Peter Bodik. ",
    "start": "1688250",
    "end": "1695010"
  },
  {
    "text": "Yeah, so credits to him. The rules of this world\nsays the following--",
    "start": "1695010",
    "end": "1701210"
  },
  {
    "text": "if you tell the robot, who is\ntraversing this set of tiles here--",
    "start": "1701210",
    "end": "1706620"
  },
  {
    "text": "if you tell the robot\nto go up, there's a chance he doesn't go up,\nbut goes somewhere else.",
    "start": "1706620",
    "end": "1711929"
  },
  {
    "text": "So we have the stochastic\ntransitions, essentially. If I say up, he\ngoes up with point",
    "start": "1711930",
    "end": "1716990"
  },
  {
    "text": "a probability and somewhere else\nwith uniform probability, say. So 0.8 up and then 0.2--",
    "start": "1716990",
    "end": "1724909"
  },
  {
    "text": "this is the only\npossible direction to go in if you start here. So 0.2 in that way.",
    "start": "1724910",
    "end": "1730767"
  },
  {
    "text": "There's a chance you move in\nthe wrong direction is what I'm trying to illustrate here.",
    "start": "1730767",
    "end": "1735980"
  },
  {
    "text": "There's no chance\nthat they're going in the opposite direction. So if I say right here,\nit can't go that way.",
    "start": "1735980",
    "end": "1741259"
  },
  {
    "text": " The rewards in\nthis game is plus 1",
    "start": "1741260",
    "end": "1747700"
  },
  {
    "text": "in the green box up there,\nminus 1 in the box here. And these are also\nterminal states.",
    "start": "1747700",
    "end": "1753970"
  },
  {
    "text": "So I haven't told\nyou what that is, but it's essentially a state\nin which the game ends. So once you get to either plus\n1 or minus 1, the game is over.",
    "start": "1753970",
    "end": "1763679"
  },
  {
    "text": "For each step that\nthe robot takes, it incurs 0.04 negative reward.",
    "start": "1763680",
    "end": "1769260"
  },
  {
    "text": "So that says,\nessentially, that if you keep going for a long time,\nyour reward would be bad.",
    "start": "1769260",
    "end": "1774720"
  },
  {
    "text": "The value of the\npolicy will be bad. So you want to be efficient.",
    "start": "1774720",
    "end": "1779840"
  },
  {
    "text": "So basically, you\ncan figure out-- you want to get to the green\nthing, that's one part of it.",
    "start": "1779840",
    "end": "1785910"
  },
  {
    "text": "But you also want\nto do it quickly. So what I want you to do now\nis to essentially figure out",
    "start": "1785910",
    "end": "1791210"
  },
  {
    "text": "what is the best\npolicy, in terms of in which way should\nthe arrows point in each",
    "start": "1791210",
    "end": "1798020"
  },
  {
    "text": "of these different boxes? Fill in the question\nmark with an arrow pointing in some direction.",
    "start": "1798020",
    "end": "1804070"
  },
  {
    "text": "We know the different\ntransitions will be stochastic, so you might need to\ntake that into account.",
    "start": "1804070",
    "end": "1809210"
  },
  {
    "text": "But essentially,\nfigure out how do I have a policy that gives me\nthe biggest expected reward?",
    "start": "1809210",
    "end": "1814300"
  },
  {
    "text": "And I'll ask you in a\nfew minutes if one of you is brave enough to put it on the\nboard or something like that. ",
    "start": "1814300",
    "end": "1821520"
  },
  {
    "text": "AUDIENCE: We start the\ndiscount over time? FREDRIK D. JOHANSSON:\nThere's no discount. AUDIENCE: Can we\ntalk to our neighbor?",
    "start": "1821520",
    "end": "1827650"
  },
  {
    "text": "FREDRIK D. JOHANSSON: Yes. It's encouraged. ",
    "start": "1827650",
    "end": "1833260"
  },
  {
    "text": "[INTERPOSING VOICES] FREDRIK D. JOHANSSON:\nSo I had a question.",
    "start": "1833260",
    "end": "1838440"
  },
  {
    "text": "What is the action space? And essentially, the action\nspace is always up, down,",
    "start": "1838440",
    "end": "1844660"
  },
  {
    "text": "left, or right, depending\non if there's a wall or not. So you can't go right\nhere, for example.",
    "start": "1844660",
    "end": "1851292"
  },
  {
    "text": "AUDIENCE: You can't\ngo left either. FREDRIK D. JOHANSSON: You\ncan't go left, exactly. Good point.",
    "start": "1851292",
    "end": "1856470"
  },
  {
    "text": "So each box at the\nend, when you're done, should contain an arrow\npointing in some direction. All right, I think\nwe'll see if anybody",
    "start": "1856470",
    "end": "1864000"
  },
  {
    "text": "has solved this problem now. Who thinks they have solved it?",
    "start": "1864000",
    "end": "1869200"
  },
  {
    "text": "Great. Would you like to\nshare your solution? AUDIENCE: Yeah, so I think\nit's going to go up first.",
    "start": "1869200",
    "end": "1878503"
  },
  {
    "text": "FREDRIK D. JOHANSSON: I'm going\nto try and replicate this. Ooh, sorry about that.",
    "start": "1878503",
    "end": "1884130"
  },
  {
    "text": "OK, you're saying up here? AUDIENCE: Yeah. The basic idea is you want to\nreduce the chance that you're",
    "start": "1884130",
    "end": "1890620"
  },
  {
    "text": "ever adjacent to the red box. So just do everything you\ncan to stay far from it.",
    "start": "1890620",
    "end": "1896380"
  },
  {
    "text": "Yeah, so attempt\nto go up and then once you eventually get there,\nyou just have to go right. FREDRIK D. JOHANSSON: OK.",
    "start": "1896380",
    "end": "1902330"
  },
  {
    "text": "And then? AUDIENCE: [INAUDIBLE]. FREDRIK D. JOHANSSON: OK. So what about these ones? ",
    "start": "1902330",
    "end": "1909807"
  },
  {
    "text": "This is also part of\nthe policy, by the way.  AUDIENCE: I hadn't\nthought about this.",
    "start": "1909807",
    "end": "1916260"
  },
  {
    "text": "FREDRIK D. JOHANSSON: OK. AUDIENCE: But those,\nyou [INAUDIBLE],, right? FREDRIK D. JOHANSSON: No. AUDIENCE: Minus 0.04.",
    "start": "1916260",
    "end": "1922945"
  },
  {
    "text": "FREDRIK D. JOHANSSON:\nSo discount usually means something else. We'll get to that later. But that is a reward for\njust taking any step.",
    "start": "1922945",
    "end": "1931880"
  },
  {
    "text": "If you move into a space\nthat is not terminal, you incur that negative reward. AUDIENCE: So if you\nkeep bouncing around",
    "start": "1931880",
    "end": "1937690"
  },
  {
    "text": "for a really long time, you\nincur a long negative reward. FREDRIK D. JOHANSSON:\nIf we had this, there's some chance I'd\nnever get out of all this.",
    "start": "1937690",
    "end": "1943160"
  },
  {
    "text": "And very little chance\nof that working out. But it's a very bad\npolicy, because you keep moving back and forth. ",
    "start": "1943160",
    "end": "1949220"
  },
  {
    "text": "All right, we had\nan arm somewhere. What should I do here?",
    "start": "1949220",
    "end": "1954856"
  },
  {
    "text": "AUDIENCE: You could take a vote. FREDRIK D. JOHANSSON: OK. Who thinks right? Really?",
    "start": "1954857",
    "end": "1959860"
  },
  {
    "text": "Who thinks left? OK, interesting. I don't actually remember.",
    "start": "1959860",
    "end": "1966020"
  },
  {
    "text": "Let's see. Go ahead. AUDIENCE: I was just\nsaying, that's an easy one. FREDRIK D. JOHANSSON:\nYeah, so this is the part",
    "start": "1966020",
    "end": "1972938"
  },
  {
    "text": "that we already determined. If we had deterministic\ntransitions, this would be great,\nbecause we don't have",
    "start": "1972938",
    "end": "1977990"
  },
  {
    "text": "to think about the other ones. This is what Peter\nput on the slide.",
    "start": "1977990",
    "end": "1983040"
  },
  {
    "text": "So I'm going to have to disagree\nwith the vote there, actually. It depends, actually,\nheavily on the minus 0.04.",
    "start": "1983040",
    "end": "1993680"
  },
  {
    "text": "So if you increase\nthat by a little bit, you might want to\ngo that way instead. Or if you decrease-- I don't remember.",
    "start": "1993680",
    "end": "1999298"
  },
  {
    "text": "Decrease, exactly. And if you increase it, you\nmight get something else. It might actually be\ngood to terminate.",
    "start": "1999298",
    "end": "2004390"
  },
  {
    "text": "So those details\nmatter a little bit. But I think you've\ngot the general idea. And especially I like\nthat you commented",
    "start": "2004390",
    "end": "2009807"
  },
  {
    "text": "that you want to stay\naway from the red one, because if you look at\nthese different paths. You go up there and there--",
    "start": "2009807",
    "end": "2015669"
  },
  {
    "text": "they have the same\nnumber of states, but there's less chance\nyou end up in the red box if you take the upper route.",
    "start": "2015670",
    "end": "2022030"
  },
  {
    "text": "Great. So we have an\nexample of a policy and we have an example\nof a decision process. And things are\nworking out so far.",
    "start": "2022030",
    "end": "2029560"
  },
  {
    "text": "But how do we do this? As far as the class goes, this\nwas a blackbox experiment.",
    "start": "2029560",
    "end": "2036110"
  },
  {
    "text": "I don't know anything about\nhow you figured that out. So reinforcement\nlearning is about that-- reinforcement learning\nis try and come",
    "start": "2036110",
    "end": "2042340"
  },
  {
    "text": "up with a policy in a rigorous\nway, hopefully-- ideally. So that would be\nthe next topic here.",
    "start": "2042340",
    "end": "2049728"
  },
  {
    "text": "Up until this point, are there\nany questions that you've been dying to ask, but haven't?",
    "start": "2049728",
    "end": "2055169"
  },
  {
    "text": "AUDIENCE: I'm curious how\nmuch behavioral biases could play into the first\nMarkov assumption?",
    "start": "2055170",
    "end": "2060699"
  },
  {
    "text": "So for example, if\nyou're a clinician who's been working for\n30 years and you're just really used to giving\na certain treatment.",
    "start": "2060699",
    "end": "2066297"
  },
  {
    "text": "An action that you\ngave in the past-- that habit might influence\nan action in the future. And if that is a\nworry, how one might",
    "start": "2066298",
    "end": "2073419"
  },
  {
    "text": "think about addressing it. FREDRIK D. JOHANSSON:\nInteresting.",
    "start": "2073420",
    "end": "2078460"
  },
  {
    "text": "I guess it depends a little\nbit on how it manifests, in that if it also influenced\nyour most recent action, maybe",
    "start": "2078460",
    "end": "2085239"
  },
  {
    "text": "you have an observation of\nthat already in some sense. ",
    "start": "2085239",
    "end": "2091929"
  },
  {
    "text": "It's a very broad question. What effect will that have? Did you have something\nspecific in mind? AUDIENCE: I guess I\nwas just wondering",
    "start": "2091929",
    "end": "2098222"
  },
  {
    "text": "if it violated that assumption,\nthat an action of the past influenced an action-- FREDRIK D. JOHANSSON:\nInteresting.",
    "start": "2098222",
    "end": "2104167"
  },
  {
    "text": "So I guess my response there is\nthat the action didn't really",
    "start": "2104167",
    "end": "2109450"
  },
  {
    "text": "depend on the choice\nof action before, because the policy\nremained the same. You could have a bias towards\nan action without that",
    "start": "2109450",
    "end": "2116230"
  },
  {
    "text": "being dependent on what\nyou gave as action before, if you know what I mean. Say my probability of\ngiving action one is 1,",
    "start": "2116230",
    "end": "2122786"
  },
  {
    "text": "then it doesn't matter\nthat I give it in the past. My policy is still the same. So, not necessarily.",
    "start": "2122787",
    "end": "2129290"
  },
  {
    "text": "It could have\nother consequences. We might have reason to come\nback to that question later.",
    "start": "2129290",
    "end": "2134740"
  },
  {
    "text": "Yup. AUDIENCE: Just\npractically, I would think that a doctor would\nwant to be consistent.",
    "start": "2134740",
    "end": "2142820"
  },
  {
    "text": "And so you wouldn't,\nfor example, want to put somebody\non a ventilator and then immediately take them\noff and then immediately put",
    "start": "2142820",
    "end": "2149423"
  },
  {
    "text": "them back on again. So that would be\nan example where the past action influences\nwhat you're going to do.",
    "start": "2149423",
    "end": "2155115"
  },
  {
    "text": "FREDRIK D. JOHANSSON:\nCompletely, yeah. I think that's a great example. And what you would hope is that\nthe state variable in that case",
    "start": "2155115",
    "end": "2163340"
  },
  {
    "text": "includes some notion\nof treatment history. That's what your job is then.",
    "start": "2163340",
    "end": "2168740"
  },
  {
    "text": "So that's why state can\nbe somewhat misleading as a term-- at least\nfor me, I'm not American or English-speaking.",
    "start": "2168740",
    "end": "2177190"
  },
  {
    "text": "But yeah, I think of it as\ntoo instantaneous sometimes. So we'll move into\nreinforcement learning now.",
    "start": "2177190",
    "end": "2182660"
  },
  {
    "text": "And what I had you do\non the last slide--",
    "start": "2182660",
    "end": "2188539"
  },
  {
    "text": "well, I don't know which\nmethod you use, but most likely the middle one. There are three very\ncommon paradigms",
    "start": "2188540",
    "end": "2196730"
  },
  {
    "start": "2190000",
    "end": "2324000"
  },
  {
    "text": "for reinforcement learning. And they are essentially divided\nby what they focus on modeling.",
    "start": "2196730",
    "end": "2203630"
  },
  {
    "text": "Unsurprisingly,\nmodel-based RL focused on-- well, it has some sort\nof model in it, at least.",
    "start": "2203630",
    "end": "2209359"
  },
  {
    "text": " What you mean by\nmodel in this case is a model of the transitions.",
    "start": "2209360",
    "end": "2215720"
  },
  {
    "text": "So what state will I end up in,\ngiven the action in the state I'm in at the moment?",
    "start": "2215720",
    "end": "2221160"
  },
  {
    "text": "So model-based RL tries to\nessentially create a model for the environment\nor of the environment. ",
    "start": "2221160",
    "end": "2228740"
  },
  {
    "text": "There are several examples\nof model-based RL. One of them is\nG-computation, which comes out of the statistic\nliterature, if you like.",
    "start": "2228740",
    "end": "2235910"
  },
  {
    "text": "And MDPs are essentially-- that's a Markov\ndecision process, which is essentially trying to\nestimate the whole distribution",
    "start": "2235910",
    "end": "2242780"
  },
  {
    "text": "that we talked about before. ",
    "start": "2242780",
    "end": "2248620"
  },
  {
    "text": "There are various ups\nand downsides of this. We won't have time to go into\nall of these paradigms today. We will actually focus only\non value-based RL today.",
    "start": "2248620",
    "end": "2257835"
  },
  {
    "text": "Yeah, you can ask me\noffline if you are interested in model-based RL. The rightmost one here\nis policy-based RL,",
    "start": "2257835",
    "end": "2264320"
  },
  {
    "text": "where you essentially\nfocus only on modeling the policy that was used in\nthe data that you observed.",
    "start": "2264320",
    "end": "2272570"
  },
  {
    "text": "And the policy that you want\nto essentially arrive at. So you're optimizing\na policy and you",
    "start": "2272570",
    "end": "2278390"
  },
  {
    "text": "are estimating a policy\nthat was used in the past. And the middle one focuses\non neither of those",
    "start": "2278390",
    "end": "2284990"
  },
  {
    "text": "and focuses on only\nestimating the return-- that was the G. Or the\nreward function as a function",
    "start": "2284990",
    "end": "2291080"
  },
  {
    "text": "of your actions and states. And it's interesting\nto me that you can pick any of the variables--",
    "start": "2291080",
    "end": "2297260"
  },
  {
    "text": "A, S, and R-- and model those. And you can arrive at something\nreasonable in reinforcement learning.",
    "start": "2297260",
    "end": "2302710"
  },
  {
    "text": "This one is particularly\ninteresting, because it doesn't\ntry to understand",
    "start": "2302710",
    "end": "2309010"
  },
  {
    "text": "how do you arrive at\na certain return based on the actions in the states? It's just optimize\nthe policy directly.",
    "start": "2309010",
    "end": "2315880"
  },
  {
    "text": "And it has some obvious-- well, not obvious, but it has\nsome downsides, not doing that.",
    "start": "2315880",
    "end": "2322010"
  },
  {
    "text": "OK, anyway, we're going to\nfocus on value-based RL. And the very dominant\ninstantiation of value-based RL",
    "start": "2322010",
    "end": "2329090"
  },
  {
    "start": "2324000",
    "end": "2346000"
  },
  {
    "text": "is Q-learning. I'm sure you've heard of it. It is what drove the success\nstories that I showed before,",
    "start": "2329090",
    "end": "2334990"
  },
  {
    "text": "the goal in the\nStarCraft and everything. G-estimation is another example\nof this, which, again, has come",
    "start": "2334990",
    "end": "2341170"
  },
  {
    "text": "from the statistic literature. But we'll focus on\nQ-learning today.",
    "start": "2341170",
    "end": "2346599"
  },
  {
    "start": "2346000",
    "end": "2587000"
  },
  {
    "text": "So Q-learning is an example\nof dynamic programming, in some sense. That's how it's\nusually explained.",
    "start": "2346600",
    "end": "2352260"
  },
  {
    "text": "And I just wanted to check--\nhow many have heard the phrase dynamic programming before? OK, great.",
    "start": "2352260",
    "end": "2358030"
  },
  {
    "text": "So I won't go into details of\ndynamic programming in general. But the general idea\nis one of recursion.",
    "start": "2358030",
    "end": "2365950"
  },
  {
    "text": "In this case, you know\nsomething about what is a good terminal state.",
    "start": "2365950",
    "end": "2371500"
  },
  {
    "text": "And then you want to\nfigure out how to get there and how to get to the state\nbefore that and the state before that and so on.",
    "start": "2371500",
    "end": "2377470"
  },
  {
    "text": "That is the recursion\nthat we're talking about. The end state that is the\nbest here is fairly obvious-- that is the plus 1 here.",
    "start": "2377470",
    "end": "2383349"
  },
  {
    "text": " The only way to get there\nis by stopping here first,",
    "start": "2383350",
    "end": "2390940"
  },
  {
    "text": "because you can't move from here\nsince it's a terminal state. Your only bet is that one.",
    "start": "2390940",
    "end": "2397010"
  },
  {
    "text": "And then we can ask what is\nthe best way to get to 3, 1? How do we get to the state\nbefore the best state?",
    "start": "2397010",
    "end": "2402100"
  },
  {
    "text": "Well, we can say that\none way is go from here. And one way from here.",
    "start": "2402100",
    "end": "2407470"
  },
  {
    "text": "And as we got from\nthe audience before, this is a slightly worse\nway to get there then",
    "start": "2407470",
    "end": "2412600"
  },
  {
    "text": "from there, because here we\nhave a possibility of ending up in minus 1.",
    "start": "2412600",
    "end": "2417950"
  },
  {
    "text": "So then we recurse\nfurther and essentially, we end up with something\nlike this that says-- ",
    "start": "2417950",
    "end": "2425860"
  },
  {
    "text": "or what I tried to illustrate\nhere is that the green boxes-- I'm sorry for any colorblind\nmembers of the audience,",
    "start": "2425860",
    "end": "2433339"
  },
  {
    "text": "because this was a\npoor choice of mine. Anyway, this bottom\nside here is mostly red and this is mostly green.",
    "start": "2433340",
    "end": "2439412"
  },
  {
    "text": "And you can follow the green\ncolor here, essentially, to get to the best end state.",
    "start": "2439412",
    "end": "2445460"
  },
  {
    "text": "And what I used here\nto color this in is this idea of knowing\nhow good a state is,",
    "start": "2445460",
    "end": "2450859"
  },
  {
    "text": "depending on how good the\nstate after that state is. So I knew that plus 1 is a\ngood end state over there.",
    "start": "2450860",
    "end": "2457960"
  },
  {
    "text": "And that led me to recurse\nbackwards, essentially.",
    "start": "2457960",
    "end": "2463869"
  },
  {
    "text": "So the question,\nthen, is how do we know that that state\nover there is a good one?",
    "start": "2463870",
    "end": "2470210"
  },
  {
    "text": "When we have it\nvisualized in front of us, it's very easy to see. And it's very easy because we\nknow that plus 1 is a terminal",
    "start": "2470210",
    "end": "2476230"
  },
  {
    "text": "state here. It ends there, so those\nare the only states we need to consider in this case.",
    "start": "2476230",
    "end": "2481680"
  },
  {
    "text": "But more in general,\nhow do we learn what is the value of a state?",
    "start": "2481680",
    "end": "2488520"
  },
  {
    "text": "That will be the\npurpose of Q-learning. If we have an idea of\nwhat is a good state,",
    "start": "2488520",
    "end": "2495040"
  },
  {
    "text": "we can always do that recursion\nthat I explained very briefly.",
    "start": "2495040",
    "end": "2500260"
  },
  {
    "text": "You find a state that\nhas the high value and you figure out\nhow to get there. ",
    "start": "2500260",
    "end": "2507930"
  },
  {
    "text": "So we're going to have to\ndefine now what I mean by value.",
    "start": "2507930",
    "end": "2513069"
  },
  {
    "text": "I've used that word a\nfew different times. I say recall here, but I\ndon't know if I actually",
    "start": "2513070",
    "end": "2518740"
  },
  {
    "text": "had it on a slide before. So let's just say this is\nthe definition of value that we will be working with. ",
    "start": "2518740",
    "end": "2527175"
  },
  {
    "text": "I think I had it on a\nslide before, actually. This is the expected return. Remember, this G\nhere was the sum",
    "start": "2527175",
    "end": "2532670"
  },
  {
    "text": "of rewards going into the\nfuture, starting at time, t. And the value,\nthen, of this state",
    "start": "2532670",
    "end": "2540680"
  },
  {
    "text": "is the expectation\nof such returns. ",
    "start": "2540680",
    "end": "2545690"
  },
  {
    "text": "Before, I said that\nthe value of an policy was the expectation\nof returns, period.",
    "start": "2545690",
    "end": "2552140"
  },
  {
    "text": "And the value of a\nstate and the policy is the value of that return\nstarting in a certain state.",
    "start": "2552140",
    "end": "2558329"
  },
  {
    "text": " We can stratify this\nfurther if we like and say",
    "start": "2558330",
    "end": "2564010"
  },
  {
    "text": "that the value of\na state action pair is the expected return,\nstarting in a certain state",
    "start": "2564010",
    "end": "2570270"
  },
  {
    "text": "and taking an action, a. And after that,\nfollowing the policy, pi.",
    "start": "2570270",
    "end": "2575990"
  },
  {
    "text": "This would be the\nso-called Q value of a state-action pair-- s, a.",
    "start": "2575990",
    "end": "2581000"
  },
  {
    "text": " And this is where\nQ-learning gets its name.",
    "start": "2581000",
    "end": "2587539"
  },
  {
    "start": "2587000",
    "end": "3599000"
  },
  {
    "text": "So Q-learning attempts to\nestimate the Q function-- the expected return starting in\na state, s, and taking action,",
    "start": "2587540",
    "end": "2594797"
  },
  {
    "text": "a--  from data. ",
    "start": "2594797",
    "end": "2602090"
  },
  {
    "text": "The Q-learning is\nalso associated with a deterministic policy. So the policy and the\nQ function go together",
    "start": "2602090",
    "end": "2608840"
  },
  {
    "text": "in this specific way. If we have a Q\nfunction, Q, that tries to estimate the value of a\npolicy, pi, the pi itself is",
    "start": "2608840",
    "end": "2617330"
  },
  {
    "text": "the arg max according to that\nQ. It sounds a little recursive, but hopefully it will be OK.",
    "start": "2617330",
    "end": "2624010"
  },
  {
    "text": "Maybe it's more obvious\nif we look here. So Q, I said before, was\nthe value of starting an s, taking action, a, and\nthen following policy, pi.",
    "start": "2624010",
    "end": "2633560"
  },
  {
    "text": "This is defined by the\ndecision process itself. The best pi, the best policy, is\nthe one that has the highest Q.",
    "start": "2633560",
    "end": "2642363"
  },
  {
    "text": "And this is what\nwe call a Q-star.  Well, that is not what\nwe call Q-star, that",
    "start": "2642363",
    "end": "2648119"
  },
  {
    "text": "is what we call little q-star. Q-star, the best estimate of\nthis, is obviously the thing itself.",
    "start": "2648120",
    "end": "2653320"
  },
  {
    "text": "So if you can find\na good function that assigns a value to\na state-action pair, the best such\nfunction you can get",
    "start": "2653320",
    "end": "2659310"
  },
  {
    "text": "is the one that is\nequal to little q-star.  I hope that wasn't\ntoo confusing.",
    "start": "2659310",
    "end": "2666670"
  },
  {
    "text": "I'll show on the next slide\nwhy that might be reasonable. So Q-learning is based\non a general idea",
    "start": "2666670",
    "end": "2674190"
  },
  {
    "text": "from dynamic programming,\nwhich is the so-called Bellman question.",
    "start": "2674190",
    "end": "2679450"
  },
  {
    "text": "There we go. ",
    "start": "2679450",
    "end": "2686680"
  },
  {
    "text": "This is an instantiation of\nBellman optimality, which says that the best\nstate-action value",
    "start": "2686680",
    "end": "2695950"
  },
  {
    "text": "function has the\nproperty that it is equal to the immediate reward\nof taking action, a, and state,",
    "start": "2695950",
    "end": "2703210"
  },
  {
    "text": "s, plus this, which\nis the maximum Q value for the next state.",
    "start": "2703210",
    "end": "2708420"
  },
  {
    "text": "So we're going to stare\nat this for a bit, because there's a\nbit here to digest.",
    "start": "2708420",
    "end": "2714500"
  },
  {
    "text": "Remember, q-star assigns a\nvalue to any state action pair. So we have q-star here,\nwe have q-star here.",
    "start": "2714500",
    "end": "2722160"
  },
  {
    "text": "This thing here is supposed\nto represent the value going forward in time after I've\nmade this choice, action, a,",
    "start": "2722160",
    "end": "2729260"
  },
  {
    "text": "and state, s.  If I have a good idea of how\ngood it is to take action,",
    "start": "2729260",
    "end": "2736790"
  },
  {
    "text": "a, instead of s, it should\nboth incorporate the immediate reward that I get-- that's RT-- and how good that choice\nwas going forward.",
    "start": "2736790",
    "end": "2744220"
  },
  {
    "text": "So think about mechanical\nventilation, as I said before. If we put a patient on\nmechanical ventilation, we have to do a bunch of\nother things after that.",
    "start": "2744220",
    "end": "2750720"
  },
  {
    "text": "If none of those other things\nlead to a good outcome, this part will be low.",
    "start": "2750720",
    "end": "2756630"
  },
  {
    "text": "Even if the immediate\nreturn is good. So for the optimal q-star,\nthis quantity holds.",
    "start": "2756630",
    "end": "2763710"
  },
  {
    "text": "We know that-- we\ncan prove that. So the question is how\ndo we find this thing? How do we find q-star?",
    "start": "2763710",
    "end": "2769290"
  },
  {
    "text": "Because q-star is not only\nthe thing that gives you the optimal policy-- it also satisfied this equality.",
    "start": "2769290",
    "end": "2775987"
  },
  {
    "text": "This is not true for\nevery Q function, but it's true for\nthe optimal one. ",
    "start": "2775987",
    "end": "2781460"
  },
  {
    "text": "Questions? ",
    "start": "2781460",
    "end": "2786990"
  },
  {
    "text": "If you haven't seen this before,\nit might be a little tough to digest.",
    "start": "2786990",
    "end": "2792130"
  },
  {
    "text": "Is the notation clear? Essentially, here\nyou have the state that you are arriving\nat the next time. A prime is the parameter of this\nhere, or the argument to this.",
    "start": "2792130",
    "end": "2800990"
  },
  {
    "text": "You're taking the best\npossible q-star value and then state that you arrive at after. Yeah, go ahead.",
    "start": "2800990",
    "end": "2806425"
  },
  {
    "text": "AUDIENCE: Can you instantiate an\nexample you have on the board? FREDRIK D. JOHANSSON: Yes. Actually, I might do a\nfull example of Q-learning",
    "start": "2806425",
    "end": "2812840"
  },
  {
    "text": "in a second. Yes, I will. I'll get to that example then. ",
    "start": "2812840",
    "end": "2820453"
  },
  {
    "text": "Yeah, I was debating\nwhether to do that. It might take some time,\nbut it could be useful. So where are we? ",
    "start": "2820453",
    "end": "2829510"
  },
  {
    "text": "So what I showed you before--\nthe Bellman inequality. We know that this holds\nfor the optimal thing.",
    "start": "2829510",
    "end": "2834880"
  },
  {
    "text": "And if there is a quality\nthat is true at an optimum, one general idea in optimization\nis this so-called fixed point",
    "start": "2834880",
    "end": "2841900"
  },
  {
    "text": "iteration that you can\ndo to arrive there. And that's essentially what\nwe will do to get to a good Q.",
    "start": "2841900",
    "end": "2849609"
  },
  {
    "text": "So a nice thing\nabout Q-learning is that if your states and action\nspaces are small and discrete,",
    "start": "2849610",
    "end": "2856750"
  },
  {
    "text": "you can represent the\nQ function as a table. So all you have to\nkeep track of is, how good is the certain\naction in a certain state?",
    "start": "2856750",
    "end": "2864970"
  },
  {
    "text": "Or all actions in\nall states, rather? So that's what we did here. This is a table.",
    "start": "2864970",
    "end": "2871329"
  },
  {
    "text": "I've described to you the policy\nhere, but what we'll do next is to describe the\nvalue of each action.",
    "start": "2871330",
    "end": "2878560"
  },
  {
    "text": "So you can think of a value of\ntaking the right one, bottom, top, and left, essentially.",
    "start": "2878560",
    "end": "2884080"
  },
  {
    "text": "Those will be the values\nthat we need to consider. And so what Q-learning can\ndo with discrete states is",
    "start": "2884080",
    "end": "2890930"
  },
  {
    "text": "to essentially start\nfrom somewhere, start from some idea of\nwhat Q is-- could be random,",
    "start": "2890930",
    "end": "2896450"
  },
  {
    "text": "could be 0. And then repeat the following\nfixed-point iteration, where you update your\nformer idea of what",
    "start": "2896450",
    "end": "2905000"
  },
  {
    "text": "Q should be, with\nits current value plus essentially a mixture of\nthe immediate reward for taking",
    "start": "2905000",
    "end": "2912830"
  },
  {
    "text": "action, At, in that state,\nand the future reward, as judged by your current\nestimate of the Q function.",
    "start": "2912830",
    "end": "2918810"
  },
  {
    "text": "So we'll do that\nnow in practice. Yeah. AUDIENCE: Throughout\nthis, where are we getting the transition\nprobabilities",
    "start": "2918810",
    "end": "2924349"
  },
  {
    "text": "or the behavior of the game? FREDRIK D. JOHANSSON: So\nthey're not used here, actually. A value-based RL-- I\ndidn't say that explicitly,",
    "start": "2924350",
    "end": "2930030"
  },
  {
    "text": "but they don't rely on knowing\nthe transition probabilities. What you might ask is where\ndo we get the S and the As",
    "start": "2930030",
    "end": "2936290"
  },
  {
    "text": "and the Rs from? And we'll get to that. How do we estimate these? We'll get to that later.",
    "start": "2936290",
    "end": "2943130"
  },
  {
    "text": "Good question, though. I'm going to throw a\nvery messy slide at you. Here you go.",
    "start": "2943130",
    "end": "2949780"
  },
  {
    "text": "A lot of numbers. So what I've done now here\nis a more exhaustive version of what I put on the board.",
    "start": "2949780",
    "end": "2955720"
  },
  {
    "text": "For each little triangle\nhere represents the Q value for the state-action pair.",
    "start": "2955720",
    "end": "2961300"
  },
  {
    "text": "So this triangle is,\nagain, for the action right if you're in this state. ",
    "start": "2961300",
    "end": "2967869"
  },
  {
    "text": "So what I've put on\nthe first slide here is the immediate\nreward of each action.",
    "start": "2967870",
    "end": "2978770"
  },
  {
    "text": "So we know that any step\nwill cost us minus 0.04. So that's why there's\na lot of those here.",
    "start": "2978770",
    "end": "2984710"
  },
  {
    "text": "These white boxes here\nare not possible actions. Up here, you have a\n0.96, because it's",
    "start": "2984710",
    "end": "2991350"
  },
  {
    "text": "1, which is the immediate\nreward of going right here, minus 0.04. These two are minus 1.04\nfor the same reason--",
    "start": "2991350",
    "end": "3001220"
  },
  {
    "text": "because you arrive in minus 1. OK, so that's the first step\nand the second step done.",
    "start": "3001220",
    "end": "3007369"
  },
  {
    "text": "We initialize Qs to be 0. And then we picked these two\nparameters of the problem,",
    "start": "3007370",
    "end": "3012770"
  },
  {
    "text": "alpha and gamma, to be 1. And then we did the first\niteration of Q-learning,",
    "start": "3012770",
    "end": "3018170"
  },
  {
    "text": "where we set the Q to\nbe the old version of Q, which was 0, plus alpha\ntimes this thing here.",
    "start": "3018170",
    "end": "3025850"
  },
  {
    "text": "So Q was 0, that means\nthat this is also 0. So the only thing we need to\nlook at is this thing here.",
    "start": "3025850",
    "end": "3031820"
  },
  {
    "text": "This also is 0, because\nthe Qs for all states were 0, so the only thing\nwe end up with is R.",
    "start": "3031820",
    "end": "3037780"
  },
  {
    "text": "And that's what populated\nthis table here. ",
    "start": "3037780",
    "end": "3044000"
  },
  {
    "text": "Next timestep-- I'm\ndoing Q-learning now in a way where I update all\nthe state-action pairs at once.",
    "start": "3044000",
    "end": "3051380"
  },
  {
    "text": "How can I do that? Well, it depends on the question\nI got there, essentially. What data do I observe? Or how do I get to know the\nrewards of the S&A pairs?",
    "start": "3051380",
    "end": "3059800"
  },
  {
    "text": "We'll come back to that. So in the next step, I have\nto update everything again.",
    "start": "3059800",
    "end": "3069360"
  },
  {
    "text": "So it's the previous Q\nvalue, which was minus 0.04 for a lot of things, then plus\nthe immediate reward, which",
    "start": "3069360",
    "end": "3076589"
  },
  {
    "text": "was this RT. And I have to keep going. So the dominant thing\nfor the table this time",
    "start": "3076590",
    "end": "3083100"
  },
  {
    "text": "was that the best Q value\nfor almost all of these boxes was minus 0.04.",
    "start": "3083100",
    "end": "3089580"
  },
  {
    "text": "So essentially I will\nadd the immediate reward plus that almost everywhere. What is interesting, though,\nis that here, the best Q value",
    "start": "3089580",
    "end": "3097240"
  },
  {
    "text": "was 0.96. And it will remain so. That means that the best Q\nvalue for the adjacent states--",
    "start": "3097240",
    "end": "3104020"
  },
  {
    "text": "we look at this max\nhere and get 0.96 out.",
    "start": "3104020",
    "end": "3109840"
  },
  {
    "text": "And then add the\nimmediate reward. Getting to here gives\nme 0.96 minus 0.04",
    "start": "3109840",
    "end": "3116230"
  },
  {
    "text": "for the immediate reward. And now we can figure out\nwhat will happen next.",
    "start": "3116230",
    "end": "3122890"
  },
  {
    "text": "These values will spread\nout as you go further away",
    "start": "3122890",
    "end": "3129730"
  },
  {
    "text": "from the plus 1. I don't think we should\ngo through all of this, but you get a\nsense, essentially, how information is moved\nfrom the plus 1 and away.",
    "start": "3129730",
    "end": "3139000"
  },
  {
    "text": "And I'm sure that's\nhow you solved it yourself, in your head.",
    "start": "3139000",
    "end": "3144220"
  },
  {
    "text": "But this makes it clear\nwhy you can do that, even if you don't know where\nthe terminal states are or where the value of the\nstate-action pairs are.",
    "start": "3144220",
    "end": "3152710"
  },
  {
    "text": " AUDIENCE: Doesn't\nthis calculation assume that if you want to\nmove in a certain direction,",
    "start": "3152710",
    "end": "3160250"
  },
  {
    "text": "you will move in that direction? FREDRIK D. JOHANSSON: Yes. Sorry. Thanks for reminding me. That should have been\nin the slide, yes.",
    "start": "3160250",
    "end": "3166520"
  },
  {
    "text": "Thank you.  I'm going to skip\nthe rest of this. I hope you forgive me.",
    "start": "3166520",
    "end": "3172030"
  },
  {
    "text": "We can talk more about it later. ",
    "start": "3172030",
    "end": "3178138"
  },
  {
    "text": "Thanks for reminding\nme, Pete, there, that one of the things\nI exploited here was that I assume just\ndeterministic transitions.",
    "start": "3178138",
    "end": "3185000"
  },
  {
    "text": "Another thing that I\nrelied very heavily on here is that I can represent\nthis Q function as a table.",
    "start": "3185000",
    "end": "3190164"
  },
  {
    "text": "I drew all these boxes and\nI filled the numbers in. That's easy enough. But what if I have thousands\nof states and thousands",
    "start": "3190165",
    "end": "3195829"
  },
  {
    "text": "of actions? That's a large table. And not only is it a large\ntable for me to keep in memory--",
    "start": "3195830",
    "end": "3201450"
  },
  {
    "text": "it's also very bad\nfor me statistically. If I want to observe anything\nabout a state-action pair,",
    "start": "3201450",
    "end": "3206900"
  },
  {
    "text": "I have to do that\naction in that state. And if you think about treating\npatients in a hospital, you're not going to try\neverything in every state,",
    "start": "3206900",
    "end": "3213440"
  },
  {
    "text": "usually. You're also not going to have\ninfinite numbers of patients. So how do you figure out\nwhat is the immediate reward",
    "start": "3213440",
    "end": "3220220"
  },
  {
    "text": "of taking a certain\naction in a certain state? And this is where a function\napproximation comes in.",
    "start": "3220220",
    "end": "3227720"
  },
  {
    "text": "Essentially, if you can't\nrepresent your data set table, either for statistical\nreasons or for memory reasons,",
    "start": "3227720",
    "end": "3237609"
  },
  {
    "text": "let's say, you might want to\napproximate the Q function with a parametric or with\na non-parametric function.",
    "start": "3237610",
    "end": "3246150"
  },
  {
    "text": "And this is exactly\nwhat we can do. So we can draw now an analogy\nto what we did last week.",
    "start": "3246150",
    "end": "3251260"
  },
  {
    "text": "I'm going to come back\nto this, but essentially instead of doing this\nfixed-point iteration that we",
    "start": "3251260",
    "end": "3259500"
  },
  {
    "text": "did before, we will try and\nlook for a function Q theta that is equal to R plus gamma max Q.",
    "start": "3259500",
    "end": "3269122"
  },
  {
    "text": "Remember before, we had\nthe Bellman inequality? We said that q-star S,\nA is equal to R S, A,",
    "start": "3269122",
    "end": "3278880"
  },
  {
    "text": "let's say, plus gamma max A\nprime q star S prime A prime,",
    "start": "3278880",
    "end": "3298880"
  },
  {
    "text": "where S prime is the state\nwe get to after taking action A in state S. So the\nonly thing I've done here",
    "start": "3298880",
    "end": "3304230"
  },
  {
    "text": "is to take this equality and\nmake it instead a loss function",
    "start": "3304230",
    "end": "3309300"
  },
  {
    "text": "on the violation\nof this equality. So by minimizing\nthis quantity, I",
    "start": "3309300",
    "end": "3315079"
  },
  {
    "text": "will find something\nthat has approximately the Bellman equality that\nwe talked about before.",
    "start": "3315080",
    "end": "3320880"
  },
  {
    "text": "This is the idea of\nfitted Q-learning, where you substitute the\ntabular representation",
    "start": "3320880",
    "end": "3328270"
  },
  {
    "text": "with the function\napproximations, essentially. So just to make this\na bit more concrete, we can think about\nthe case where",
    "start": "3328270",
    "end": "3333625"
  },
  {
    "text": "we have only a single step. There's only a single\naction to make,",
    "start": "3333625",
    "end": "3338760"
  },
  {
    "text": "which means that there is no\nfuture part of this equation here. This part goes away,\nbecause there's only one",
    "start": "3338760",
    "end": "3344970"
  },
  {
    "text": "stage in our trajectory. So we have only the\nimmediate reward. We have only the Q function.",
    "start": "3344970",
    "end": "3351080"
  },
  {
    "text": "Now, this is exactly a\nregression equation in the way",
    "start": "3351080",
    "end": "3356720"
  },
  {
    "text": "that you've seen it when\nestimating potential outcomes. RT here represents\nthe outcome of doing",
    "start": "3356720",
    "end": "3367060"
  },
  {
    "text": "action A and state\nS. And Q here will be our estimate of this RT. ",
    "start": "3367060",
    "end": "3375620"
  },
  {
    "text": "Again, I've said this before--\nif we have a single time point in our process,\nthe problem reduces",
    "start": "3375620",
    "end": "3382820"
  },
  {
    "text": "to estimating\npotential outcomes, just the way we\nsaw it last time. We have curves that\ncorrespond outcomes",
    "start": "3382820",
    "end": "3390410"
  },
  {
    "text": "under different actions. And we can do\nregression adjustment, trying to find an F such\nthat this quantity is small",
    "start": "3390410",
    "end": "3397010"
  },
  {
    "text": "so that we can model each\ndifferent potential outcomes. And that's exactly what happens\nwith the fitted Q iteration",
    "start": "3397010",
    "end": "3402632"
  },
  {
    "text": "if you have a single\ntimestep, too. ",
    "start": "3402632",
    "end": "3407670"
  },
  {
    "text": "So to make it even\nmore concrete, we can say that there's some\ntarget value, G hat, which",
    "start": "3407670",
    "end": "3415860"
  },
  {
    "text": "represents the immediate reward\nand the future rewards that is the target of our regression.",
    "start": "3415860",
    "end": "3421980"
  },
  {
    "text": "And we're fitting some\nfunction to that value. ",
    "start": "3421980",
    "end": "3430010"
  },
  {
    "text": "So the question\nwe got before was how do I know the\ntransition matrix?",
    "start": "3430010",
    "end": "3436970"
  },
  {
    "text": "How do I get any information\nabout this thing? I say here on the\nslide that, OK,",
    "start": "3436970",
    "end": "3442480"
  },
  {
    "text": "we have some target that's\nR plus future Q values. We have some prediction\nand we have an expectation",
    "start": "3442480",
    "end": "3447820"
  },
  {
    "text": "of our transitions here. But how do I\nevaluate this thing?",
    "start": "3447820",
    "end": "3453510"
  },
  {
    "text": "The transitions I have to\nget from somewhere, right? And another way to say\nthat is what are the inputs",
    "start": "3453510",
    "end": "3459970"
  },
  {
    "text": "and the outputs\nof our regression? Because when we estimate\npotential outcomes, we have a very\nclear idea of this.",
    "start": "3459970",
    "end": "3468070"
  },
  {
    "text": "We know that y, the outcome\nitself, is a target.",
    "start": "3468070",
    "end": "3473080"
  },
  {
    "text": "And the input is\nthe covariates, x. But here, we have a moving\ntarget, because this Q hat,",
    "start": "3473080",
    "end": "3481510"
  },
  {
    "text": "it has to come from\nsomewhere, too. This is something that\nwe estimate as well. So usually what happens is that\nwe alternate between updating",
    "start": "3481510",
    "end": "3490630"
  },
  {
    "text": "this target, Q, and Q theta. So essentially, we copy Q\ntheta to become our new Q hat",
    "start": "3490630",
    "end": "3495880"
  },
  {
    "text": "and we iterate this somehow. But I still haven't told you how\nto evaluate this expectation.",
    "start": "3495880",
    "end": "3502100"
  },
  {
    "text": "So usually in RL, there are a\nfew different ways to do this. And either depending on where\nyou coming from, essentially,",
    "start": "3502100",
    "end": "3513190"
  },
  {
    "text": "these are varyingly viable. So if we look back\nat this thing here,",
    "start": "3513190",
    "end": "3521819"
  },
  {
    "text": "it relies on having\ntuples of transitions-- the state, the action,\nthe next state, and the reward that I got.",
    "start": "3521820",
    "end": "3527849"
  },
  {
    "text": "So I have to somehow\nobserve those. And I can obtain\nthem in various ways.",
    "start": "3527850",
    "end": "3534420"
  },
  {
    "text": "A very common one when\nit comes to learning to play video\ngames, for example, is that you do something\ncalled on-policy exploration.",
    "start": "3534420",
    "end": "3540109"
  },
  {
    "text": "That means that you observe\ndata from the policy that you're\ncurrently optimizing. You just play the game\naccording to the policies",
    "start": "3540110",
    "end": "3546359"
  },
  {
    "text": "that you have at the moment. And the analogy in\nhealth care would be that you have some idea\nof how to treat patients",
    "start": "3546360",
    "end": "3552480"
  },
  {
    "text": "and you just do that\nand see what happens. That could be\nproblematic, especially if you've got that policy--",
    "start": "3552480",
    "end": "3558810"
  },
  {
    "text": "if you randomly initialized\nit or if you got it for some somewhere very suboptimal.",
    "start": "3558810",
    "end": "3564700"
  },
  {
    "text": "A different thing that\nwe're more, perhaps, comfortable with in health\ncare, in a restricted setting,",
    "start": "3564700",
    "end": "3570670"
  },
  {
    "text": "is the idea of a\nrandomized trial, where, instead of trying out some\npolicy that you're currently learning, you decide\non a population",
    "start": "3570670",
    "end": "3578200"
  },
  {
    "text": "where it's OK to flip\na coin, essentially, between different\nactions that you have. ",
    "start": "3578200",
    "end": "3585655"
  },
  {
    "text": "The difference between\nthe sequential setting and the one-step\nsetting is that now we have to randomize a\nsequence of actions, which",
    "start": "3585655",
    "end": "3592000"
  },
  {
    "text": "is a little bit unlike\nthe clinical trials that you have seen\nbefore, I think. The last one, which is\nthe most studied one",
    "start": "3592000",
    "end": "3598840"
  },
  {
    "text": "when it comes to\npractice, I would say, is the one that we\ntalk about this week--",
    "start": "3598840",
    "end": "3605470"
  },
  {
    "text": "is off-policy evaluation\nor learning, in which case",
    "start": "3605470",
    "end": "3610750"
  },
  {
    "text": "you observe health care\nrecords, for example. You observe registries. You observe some data from\nthe health care system where patients have\nalready been treated",
    "start": "3610750",
    "end": "3617680"
  },
  {
    "text": "and you try to\nextract a good policy based on that information. So that means that you see\nthese transitions between state",
    "start": "3617680",
    "end": "3624160"
  },
  {
    "text": "and action and the next\nstate and the reward. You see that based on\nwhat happened in the past and you have to figure\nout a pattern there",
    "start": "3624160",
    "end": "3630460"
  },
  {
    "text": "that helps you come up with a\ngood action or a good policy. So we'll focus on\nthat one for now.",
    "start": "3630460",
    "end": "3638330"
  },
  {
    "text": "The last part of this talk\nwill be about, essentially,",
    "start": "3638330",
    "end": "3644730"
  },
  {
    "text": "what we have to be\ncareful with when we learn with off-policy data.",
    "start": "3644730",
    "end": "3650153"
  },
  {
    "text": "Any questions up\nuntil this point?  Yeah.",
    "start": "3650153",
    "end": "3655910"
  },
  {
    "text": "AUDIENCE: So if\n[INAUDIBLE] getting there for the [INAUDIBLE],, are\nthere any requirements that",
    "start": "3655910",
    "end": "3663059"
  },
  {
    "text": "has to be met by\n[INAUDIBLE],, like how we had [INAUDIBLE]\nand cause inference?",
    "start": "3663060",
    "end": "3669467"
  },
  {
    "text": "FREDRIK D. JOHANSSON:\nYeah, I'll get to that on the next set of slides. Thank you.",
    "start": "3669467",
    "end": "3674980"
  },
  {
    "text": "Any other questions about\nthe Q-learning part? A colleague of mine,\nRahul, he said-- or maybe he just paraphrased\nit from someone else.",
    "start": "3674980",
    "end": "3682130"
  },
  {
    "text": "But essentially,\nyou have to see RL 10 times before you get it,\nor something to that effect.",
    "start": "3682130",
    "end": "3687349"
  },
  {
    "text": "I had the same experience. So hopefully you have\nquestions for me after. AUDIENCE: Human\nreinforcement learning.",
    "start": "3687350",
    "end": "3692900"
  },
  {
    "text": " FREDRIK D. JOHANSSON: Exactly. But I think what you should\ntake from the last two sections,",
    "start": "3692900",
    "end": "3701180"
  },
  {
    "text": "if not how to do\nQ-learning in detail, because I glossed\nover a lot of things. You should take with you the\nidea of dynamic programming",
    "start": "3701180",
    "end": "3708230"
  },
  {
    "text": "and figuring out,\nhow can I learn about what's good early on in my\nprocess from what's good late?",
    "start": "3708230",
    "end": "3713329"
  },
  {
    "text": "And the idea of moving\ntowards a good state and not just arriving\nthere immediately.",
    "start": "3713330",
    "end": "3718640"
  },
  {
    "text": "And there are many ways\nto think about that. OK, we'll move on to\noff-policy learning.",
    "start": "3718640",
    "end": "3725780"
  },
  {
    "text": "And again, the set-up here is\nthat we receive trajectories of patient states, actions,\nand rewards from some source.",
    "start": "3725780",
    "end": "3732990"
  },
  {
    "text": "We don't know what these sources\nnecessarily-- well, we probably know what the source is. But we don't know how these\nactions were performed,",
    "start": "3732990",
    "end": "3739400"
  },
  {
    "text": "i.e., we don't know what the\npolicy was that generated these trajectories. And this is the\nsame set-up as when you estimated causal effects\nlast week, to a large extent.",
    "start": "3739400",
    "end": "3748580"
  },
  {
    "text": "We say that the actions\nare drawn, again, according to some behavior\npolicy unknown to us.",
    "start": "3748580",
    "end": "3753925"
  },
  {
    "text": "But we want to\nfigure out what is the value of a new policy, pi. So when I showed\nyou very early on--",
    "start": "3753925",
    "end": "3760069"
  },
  {
    "text": "I wish I had that slide again. But essentially, a bunch of\npatient trajectories and some",
    "start": "3760070",
    "end": "3765740"
  },
  {
    "text": "return. Patient trajectories,\nsome return. The average of those,\nthat's called a value.",
    "start": "3765740",
    "end": "3772190"
  },
  {
    "text": "If we have trajectories\naccording to a certain policy, that is the value\nof that policy-- the average of these things.",
    "start": "3772190",
    "end": "3779029"
  },
  {
    "text": "But when we have trajectories\naccording to one policy and want to figure out\nthe value of another one, that's the same problem as the\ncovariate adjustment problem",
    "start": "3779030",
    "end": "3786900"
  },
  {
    "text": "that you had last\nweek, essentially. Or the confounding\nproblem, essentially.",
    "start": "3786900",
    "end": "3793319"
  },
  {
    "text": "The trajectories\nthat we draw are biased according to the\npolicy of the clinician that",
    "start": "3793320",
    "end": "3798860"
  },
  {
    "text": "created them. And we want to figure out the\nvalue of a different policy. So it's the same\nas the confounding",
    "start": "3798860",
    "end": "3804320"
  },
  {
    "text": "problem from the last time. And because it is the same as\nthe confounding from last time,",
    "start": "3804320",
    "end": "3810260"
  },
  {
    "text": "we know that this is at\nleast as hard as doing that. We have confounding-- I already\nalluded to variance issues.",
    "start": "3810260",
    "end": "3816309"
  },
  {
    "text": "And you mentioned overlap\nor positivity as well. And in fact, we need to make\nthe same assumptions but even",
    "start": "3816310",
    "end": "3822155"
  },
  {
    "text": "stronger assumptions\nfor this to be possible.  These are sufficient conditions.",
    "start": "3822155",
    "end": "3828190"
  },
  {
    "text": "So, under very\ncertain circumstances, you don't need them. I should say, these are\nfairly general assumptions",
    "start": "3828190",
    "end": "3835240"
  },
  {
    "text": "that are still strict-- that's how I should put it. So last time, we\nlooked at something called strong ignorability.",
    "start": "3835240",
    "end": "3840970"
  },
  {
    "text": "I realized the text is\npretty small in here. Can you see in the back? Is that OK? OK, great. So strong ignorability said\nthat the potential outcomes--",
    "start": "3840970",
    "end": "3847839"
  },
  {
    "text": "Y0 and Y1-- are conditionally\nindependent of the treatment, t, given the set of variables,\nx, or the variable, x.",
    "start": "3847840",
    "end": "3855940"
  },
  {
    "text": "And that's saying that\nit doesn't matter if we know what treatment was given. We can figure out\njust based on x",
    "start": "3855940",
    "end": "3862930"
  },
  {
    "text": "what would happen under\neither treatment arm, where we should treat this patient,\nwith t equals 0, t equals 1.",
    "start": "3862930",
    "end": "3869610"
  },
  {
    "text": "We had an idea of-- or an assumption\nof-- overlap, which says that any treatment\ncould be observed",
    "start": "3869610",
    "end": "3875970"
  },
  {
    "text": "in any state or any context, x. ",
    "start": "3875970",
    "end": "3884250"
  },
  {
    "text": "That's what that means. And that is only\nto ensure that we can estimate at least a\nconditional average treatment",
    "start": "3884250",
    "end": "3890820"
  },
  {
    "text": "effect at x. And if we want to estimate\nthe average treatment",
    "start": "3890820",
    "end": "3896958"
  },
  {
    "text": "effect in a population,\nwe would need to have that for every\nx in that population.",
    "start": "3896958",
    "end": "3902617"
  },
  {
    "text": "So what happens in\nthe sequential case is that we need even\nstronger assumptions.",
    "start": "3902617",
    "end": "3908070"
  },
  {
    "text": "There's some notation I\nhaven't introduced here and I apologize for that. But there's a bar here\nover these Ss and As--",
    "start": "3908070",
    "end": "3915508"
  },
  {
    "text": "I don't know if you can see it. That usually indicates\nin this literature that you're looking at the\nsequence, up to the index here.",
    "start": "3915508",
    "end": "3923099"
  },
  {
    "text": "So all the states up\nuntil t have observed and all the actions\nup until t minus 1.",
    "start": "3923100",
    "end": "3929266"
  },
  {
    "start": "3929267",
    "end": "3934790"
  },
  {
    "text": "So in order for the best\npolicy to be identifiable-- or the value of a positive\nto be identifiable--",
    "start": "3934790",
    "end": "3941480"
  },
  {
    "text": "we need this strong condition. So the return of a\npolicy is independent of the current action,\ngiven everything that",
    "start": "3941480",
    "end": "3948230"
  },
  {
    "text": "happened in the past. ",
    "start": "3948230",
    "end": "3954310"
  },
  {
    "text": "This is weaker than\nthe Markov assumption, to be clear, because there, we\nsaid that anything that happens in the future is\nconditionally independent,",
    "start": "3954310",
    "end": "3960500"
  },
  {
    "text": "given the current state. So this is weaker,\nbecause we now just need to observe\nsomething in the history.",
    "start": "3960500",
    "end": "3968957"
  },
  {
    "text": "We need to observe all\nconfounders in the history, in this instance. We don't need to\nsummarize them in S.",
    "start": "3968957",
    "end": "3974089"
  },
  {
    "text": "And we'll get back to\nthis in the next slide. Positivity is the real\ndifficult one, though, because what we're saying\nis that at any point",
    "start": "3974090",
    "end": "3980780"
  },
  {
    "text": "in the trajectory, any action\nshould be possible in order",
    "start": "3980780",
    "end": "3986120"
  },
  {
    "text": "for us to estimate the value\nof any possible policy. And we know that that's not\ngoing to be true in practice.",
    "start": "3986120",
    "end": "3991770"
  },
  {
    "text": "We're not going to consider\nevery possible action at every possible point in\nthe health care setting.",
    "start": "3991770",
    "end": "3997819"
  },
  {
    "text": "There's just no way. So what that tells\nus is that we can't estimate the value of\nevery possible policy.",
    "start": "3997820",
    "end": "4005440"
  },
  {
    "text": "We can only estimate\nthe value of policies that are consistent with\nthe support that we do have.",
    "start": "4005440",
    "end": "4014660"
  },
  {
    "text": "If we never see\naction 4 at time 3, there's no way we can learn\nabout a policy that does that--",
    "start": "4014660",
    "end": "4021130"
  },
  {
    "text": "that takes action 4 at time 3. That's what I'm trying to say. So in some sense,\nthis is stronger,",
    "start": "4021130",
    "end": "4029950"
  },
  {
    "text": "just because of how\nsequential settings work. It's more about the application\ndomain than anything,",
    "start": "4029950",
    "end": "4035950"
  },
  {
    "text": "I would say. In the next set of\nslides, we'll focus on sequential randomization\nor sequential ignorability,",
    "start": "4035950",
    "end": "4041810"
  },
  {
    "text": "as it's sometimes called. And tomorrow, we'll\ntalk a little bit about the statistics\ninvolved in or resulting",
    "start": "4041810",
    "end": "4048819"
  },
  {
    "text": "from the positivity\nassumption and things like importance\nweighting, et cetera. Did I say tomorrow?",
    "start": "4048820",
    "end": "4054310"
  },
  {
    "text": "I meant Thursday.  So last recap on the\npotential outcome story.",
    "start": "4054310",
    "end": "4062490"
  },
  {
    "text": "This is a slide-- I'm not sure if he\nshowed this one, but it's one that we\nused in a lot of talks. And it, again, just serves\nto illustrate the idea",
    "start": "4062490",
    "end": "4070260"
  },
  {
    "text": "of a one-timestep decision. So we have here, Anna. A patient comes in. She has high blood sugar\nand some other properties.",
    "start": "4070260",
    "end": "4079050"
  },
  {
    "text": "And we're debating whether to\ngive her medication A or B. And to do that, we\nwant to figure out what would be her blood sugar\nunder these different choices",
    "start": "4079050",
    "end": "4086970"
  },
  {
    "text": "a few months down the line? So I'm just using this\nhere to introduce you",
    "start": "4086970",
    "end": "4092010"
  },
  {
    "text": "to the patient, Anna. And we're going to talk\nabout Anna a little bit more. So treating Anna once, we can\nrepresent as this causal graph",
    "start": "4092010",
    "end": "4099899"
  },
  {
    "text": "that you've seen a\nlot of times now. We had some treatment,\nA, we had some state, S, and some outcome, R. We want to\nfigure out the effect of this A",
    "start": "4099899",
    "end": "4107969"
  },
  {
    "text": "on the outcome, R. Ignorability in\nthis case just says that the potential outcomes\nunder each action, A,",
    "start": "4107970",
    "end": "4116189"
  },
  {
    "text": "is conditionally\nindependent of A, given S. And so we know that\nignorability and overlap is",
    "start": "4116189",
    "end": "4126089"
  },
  {
    "text": "sufficient conditions for\nidentification of this effect. But what happens now if\nwe add another time point?",
    "start": "4126090",
    "end": "4133370"
  },
  {
    "text": "OK, so in this case, if I\nhave no extra arrows here-- I just have completely\nindependent time points--",
    "start": "4133370",
    "end": "4138469"
  },
  {
    "text": "ignorability\nclearly still holds. There's no links going from\nA to R, there's no from S",
    "start": "4138470",
    "end": "4144130"
  },
  {
    "text": "to R, et cetera. So ignorability is still fine. ",
    "start": "4144130",
    "end": "4155259"
  },
  {
    "text": "If Anna's health status in the\nfuture depends on the actions that I take now, here,\nthen the situation",
    "start": "4155260",
    "end": "4166850"
  },
  {
    "text": "is a little bit different. So this is now not in the\ncompletely independent actions",
    "start": "4166850",
    "end": "4172640"
  },
  {
    "text": "that I make, but\nthe actions here influence the state\nin the future. So we've seen this.",
    "start": "4172640",
    "end": "4178160"
  },
  {
    "text": "This is a Markov decision\nprocess, as you've seen before. This is very likely in practice.",
    "start": "4178160",
    "end": "4185239"
  },
  {
    "text": "Also, if Anna, for\nexample, is diabetic, as we saw in the example\nthat I mentioned, it's likely that\nshe will remain so.",
    "start": "4185240",
    "end": "4192890"
  },
  {
    "text": "This previous state will\ninfluence the future state. These things seem very\nreasonable, right?",
    "start": "4192890",
    "end": "4198630"
  },
  {
    "text": "But now I'm trying to\nargue about the sequential ignorability assumption. How can we break that?",
    "start": "4198630",
    "end": "4204210"
  },
  {
    "text": "How can we break\nignorability when it comes to the sequential, say? ",
    "start": "4204210",
    "end": "4215110"
  },
  {
    "text": "If you have an action here-- so the outcome at a later point\ndepends on an earlier choice.",
    "start": "4215110",
    "end": "4221415"
  },
  {
    "text": "That might certainly\nbe the case, because we could have a\ndelayed effect of something. So if we measure,\nsay, a lab value which",
    "start": "4221415",
    "end": "4228460"
  },
  {
    "text": "could be in the\nright range or not, it could very well\ndepend on medication we gave a long time ago.",
    "start": "4228460",
    "end": "4236140"
  },
  {
    "text": "And it's also likely\nthat the reward could depend on a state which\nis much earlier, depending",
    "start": "4236140",
    "end": "4241840"
  },
  {
    "text": "on what we include in\nthat state variable. We already have an example,\nI think, from the audience on that.",
    "start": "4241840",
    "end": "4247147"
  },
  {
    "text": "So actually, ignorability should\nhave a big red cross over it, because it doesn't hold there. And it's luckily\non the next slide.",
    "start": "4247147",
    "end": "4253373"
  },
  {
    "text": "Because there are\neven more errors that we can have, conceivably,\nin the medical setting. The example that\nwe got from Pete",
    "start": "4253373",
    "end": "4259720"
  },
  {
    "text": "before was, essentially,\nthat if we've tried an action previously, we\nmight not want to try it again.",
    "start": "4259720",
    "end": "4264870"
  },
  {
    "text": "Or if we knew that\nsomething worked previously, we might want to do it again. So if we had a good\nreward here, we might want to do the\nsame thing twice.",
    "start": "4264870",
    "end": "4272810"
  },
  {
    "text": "And this arrow here\nsays that if we know that a patient had\na symptom earlier on,",
    "start": "4272810",
    "end": "4278940"
  },
  {
    "text": "we might want to base\nour actions on it later. We've known that the patient\nhad an allergic reaction at some point, for example.",
    "start": "4278940",
    "end": "4285010"
  },
  {
    "text": "We might not want to use that\nmedication at a later time. AUDIENCE: But you can always\nput everything in a state.",
    "start": "4285010",
    "end": "4290250"
  },
  {
    "text": "FREDRIK D. JOHANSSON: Exactly. So this depends on what\nyou put in the state. ",
    "start": "4290250",
    "end": "4296970"
  },
  {
    "text": "This is an example\nwhere I should introduce these arrows to show\nthat, if I haven't got that information here, then\nI introduce this dependence.",
    "start": "4296970",
    "end": "4306030"
  },
  {
    "text": "So if I don't have\nthe information about allergic reaction or\nsome symptom before in here,",
    "start": "4306030",
    "end": "4314660"
  },
  {
    "text": "then I have to do\nsomething else. So exactly that is the point.",
    "start": "4314660",
    "end": "4320400"
  },
  {
    "text": "If I can summarize\nhistory in some good way-- if I can compress all\nof these four variables",
    "start": "4320400",
    "end": "4328570"
  },
  {
    "text": "into some variable age\nstandard for the history, then I have ignorability,\nwith respect to that history,",
    "start": "4328570",
    "end": "4334540"
  },
  {
    "text": "H. This is your solution and\nit introduces a new problem,",
    "start": "4334540",
    "end": "4343030"
  },
  {
    "text": "because history is usually\na really large thing.",
    "start": "4343030",
    "end": "4348719"
  },
  {
    "text": "We know that history grows\nwith time, obviously. But usually we don't\nobserve patients for the same number\nof time points.",
    "start": "4348720",
    "end": "4354260"
  },
  {
    "text": "So how do we represent\nthat for a program? How do we represent that\nto a learning algorithm? That's something we\nhave to deal with.",
    "start": "4354260",
    "end": "4361270"
  },
  {
    "text": "You can pad history\nwith 0s, et cetera, but if you keep every\ntimestep and repeat every variable in\nevery timestep,",
    "start": "4361270",
    "end": "4366800"
  },
  {
    "text": "you get a very large object. That might introduce\nstatistical problems, because now you have\nmuch more variance if you",
    "start": "4366800",
    "end": "4372967"
  },
  {
    "text": "have new variables, et cetera. So one thing that\npeople do is that they look some amount of\ntime backwards-- so",
    "start": "4372967",
    "end": "4379595"
  },
  {
    "text": "instead of just looking\nat one timestep back, you now look at a\nlength k window. And your state essentially\ngrows by a factor, k.",
    "start": "4379595",
    "end": "4388120"
  },
  {
    "text": "And another alternative\nis to try and learn a summary function. Learn some function that\nis relevant for predicting",
    "start": "4388120",
    "end": "4393290"
  },
  {
    "text": "the outcome that takes all\nof the history into account, but has a smaller representation\nthan just t times the variables",
    "start": "4393290",
    "end": "4400489"
  },
  {
    "text": "that you have. But this is something that\nneeds to happen, usually. ",
    "start": "4400490",
    "end": "4409070"
  },
  {
    "text": "Most health care\ndata, in practice-- you have to make\nchoices about this. I just want to stress that\nthat's something you really can't avoid.",
    "start": "4409070",
    "end": "4414250"
  },
  {
    "text": " The last point I want to make\nis that unobserved confounding",
    "start": "4414250",
    "end": "4420460"
  },
  {
    "text": "is also a problem that\nis not avoidable just due to summarizing history.",
    "start": "4420460",
    "end": "4426850"
  },
  {
    "text": "We can introduce\nnew confounding. That is a problem, if we\ndon't summarize history well. But we can also have\nunobserved confounders,",
    "start": "4426850",
    "end": "4433212"
  },
  {
    "text": "just like we can in\nthe one-step setting. ",
    "start": "4433212",
    "end": "4438219"
  },
  {
    "text": "One example is if we have\nan unobserved confounded in the same way\nas we did before.",
    "start": "4438220",
    "end": "4443390"
  },
  {
    "text": "It impacts both the\naction at time 1 and the reward at time 1.",
    "start": "4443390",
    "end": "4449590"
  },
  {
    "text": "But of course, now we're\nin the sequential setting. The confounding structure\ncould be much more complicated. We could have a confounder\nthat influences an early action",
    "start": "4449590",
    "end": "4457000"
  },
  {
    "text": "and a late reward. So it might be a\nlittle harder for us to characterize what is the\nset of potential confounders?",
    "start": "4457000",
    "end": "4464083"
  },
  {
    "text": "So I just wanted\nto point that out and to reinforce that\nthis is only harder than the one-step setting.",
    "start": "4464083",
    "end": "4469520"
  },
  {
    "text": "So we're wrapping up now. I just want to end on\na point about the games",
    "start": "4469520",
    "end": "4477050"
  },
  {
    "text": "that we looked at before. One of the big reasons\nthat these algorithms were",
    "start": "4477050",
    "end": "4483170"
  },
  {
    "text": "so successful in\nplaying games was that we have full observability\nin these settings. We know everything from\nthe game board itself--",
    "start": "4483170",
    "end": "4493520"
  },
  {
    "text": "when it comes to Go, at least. We can debate that when it\ncomes to the video games. But in Go, we have complete\nobservability of the board.",
    "start": "4493520",
    "end": "4501650"
  },
  {
    "text": "Everything we need to know\nfor an optimal decision is there at any time point. ",
    "start": "4501650",
    "end": "4509712"
  },
  {
    "text": "Not only can we observe\nit through the history, but in the case of Go, you don't\neven need to look at history.",
    "start": "4509712",
    "end": "4514980"
  },
  {
    "text": "We certainly have Markov\ndynamics with respect to the board itself. You don't ever have to remember\nwhat was a move earlier on,",
    "start": "4514980",
    "end": "4522140"
  },
  {
    "text": "unless you want to read into\nyour opponent, I suppose. But that's a game\ntheoretic notion",
    "start": "4522140",
    "end": "4527420"
  },
  {
    "text": "we're not going\nto get into here. But more importantly, we\ncan explore the dynamics",
    "start": "4527420",
    "end": "4533450"
  },
  {
    "text": "of these systems\nalmost limitlessly, just by simulation\nand self-play. And that's true regardless if\nyou have full observability",
    "start": "4533450",
    "end": "4540415"
  },
  {
    "text": "or not-- like in\nStarCraft, you might not have full observability. But you can try your\nthings out endlessly.",
    "start": "4540415",
    "end": "4546780"
  },
  {
    "text": "And contrast that with\nhaving, I don't know, 700 patients with rheumatoid\narthritis or something",
    "start": "4546780",
    "end": "4552750"
  },
  {
    "text": "like that. Those are the samples you have. You're not going\nto get new ones.",
    "start": "4552750",
    "end": "4559270"
  },
  {
    "text": "So that is an amazing\nobstacle for us to overcome if we want\nto do this in a good way.",
    "start": "4559270",
    "end": "4564780"
  },
  {
    "text": "The current\nalgorithms are really inefficient with the\ndata that they use.",
    "start": "4564780",
    "end": "4569980"
  },
  {
    "text": "And that's why this limitless\nexploration or simulation has been so important\nfor these games.",
    "start": "4569980",
    "end": "4577719"
  },
  {
    "text": "And that's also\nwhy the games are the success stories of this. A last point is that\ntypically for these settings",
    "start": "4577720",
    "end": "4584690"
  },
  {
    "text": "that I put here, we have\nno noise, essentially. We get perfect observations\nof actions and states",
    "start": "4584690",
    "end": "4591650"
  },
  {
    "text": "and outcomes and\neverything like that. And that's really true in\nany real-world application. All right. I'm going to wrap up.",
    "start": "4591650",
    "end": "4597590"
  },
  {
    "text": "Tomorrow-- nope,\nThursday, David is",
    "start": "4597590",
    "end": "4602960"
  },
  {
    "text": "going to talk about\nmore explicitly if we want to do this properly\nin health care, what's going to happen?",
    "start": "4602960",
    "end": "4608230"
  },
  {
    "text": "We're going to have a great\ndiscussion, I'm sure, as well. So don't mind the slide. It's Thursday.",
    "start": "4608230",
    "end": "4613930"
  },
  {
    "text": "All right. Thanks a lot. [APPLAUSE]",
    "start": "4613930",
    "end": "4619900"
  },
  {
    "start": "4619900",
    "end": "4627000"
  }
]