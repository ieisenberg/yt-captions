[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5340"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5340",
    "end": "11640"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11640",
    "end": "18110"
  },
  {
    "text": "at ocw.MIT.edu.  PROFESSOR 1: So as he\ngives an introduction,",
    "start": "18110",
    "end": "25150"
  },
  {
    "text": "myself along with Catherine,\nDavid, [INAUDIBLE],, and Charlotte\n[? are going to present ?] you guys Probablistic and\nInfinite Horizon Planning.",
    "start": "25150",
    "end": "31735"
  },
  {
    "text": "To give you a brief overview of\nwhat we're going to talk about, we're going to start with the\nQuadrotor motivating example.",
    "start": "31735",
    "end": "36740"
  },
  {
    "start": "33000",
    "end": "732000"
  },
  {
    "text": "We're going to move into\nplanning with Markov decision processses, give you a little\nbit about value iteration",
    "start": "36740",
    "end": "42070"
  },
  {
    "text": "before discussing\nheuristic guided solvers. And we're going to go into the\nmore stochastic case, partially",
    "start": "42070",
    "end": "47130"
  },
  {
    "text": "observable Markov\ndecision processes and operating in belief space.",
    "start": "47130",
    "end": "52890"
  },
  {
    "text": "So we very often now see\nquadrotor motion planning as a problem, given, for\nexample, with the Amazon",
    "start": "52890",
    "end": "60029"
  },
  {
    "text": "fulfillment center. We start with a goal\nconfiguration, start configuration, set of\nactions we can take,",
    "start": "60029",
    "end": "65562"
  },
  {
    "text": "and some type of reward\nfunction or cost function. So for instance, if we\nhave a quadrotor starting",
    "start": "65562",
    "end": "71664"
  },
  {
    "text": "at the Amazon\nfulfillment center, and we want to get\nto 77 Mass Ave, and let's say we want to take\nthe shortest path to get there.",
    "start": "71665",
    "end": "77390"
  },
  {
    "text": "We would follow the\nred dashed line. But, as we can see, it comes\nvery close to these obstacles. So we're looking\nat very higher risk",
    "start": "77390",
    "end": "83792"
  },
  {
    "text": "for mission failure, crashes. If there's any\nuncertainty in its path, we're going to have a problem.",
    "start": "83792",
    "end": "89440"
  },
  {
    "text": "So one of the ways that we\ncan compensate for that is we can also done a\ngreen path, which is adjusted to give us\na little bit of space.",
    "start": "89440",
    "end": "97632"
  },
  {
    "text": "Are there any more questions? So as you can see, the\nlevel of uncertainty",
    "start": "97632",
    "end": "105300"
  },
  {
    "text": "allows us to determine how\neasy or difficult the problem's going to be to solve. On the easier side we have\ndeterministic dynamics",
    "start": "105300",
    "end": "110940"
  },
  {
    "text": "and deterministic sensors. In this case our\nactions are going to be executed as\ncommanded, and our sensors",
    "start": "110940",
    "end": "116420"
  },
  {
    "text": "are going to tell us\nexactly where we are. This will be something like\ndead reckoning validated through sensing, very redundant.",
    "start": "116420",
    "end": "122970"
  },
  {
    "text": "If we moved to a little bit\nmore at a difficult case, we have deterministic dynamics. Our commands are\nstill being executed,",
    "start": "122970",
    "end": "128740"
  },
  {
    "text": "but maybe we have\na noisy camera. So we have some\nuncertainty in the sensors. These are the cases where\nwe would see dead reckoning,",
    "start": "128740",
    "end": "135445"
  },
  {
    "text": "but we would compensate\nwith Kalman filtering to get rid of that noise level. Now, down at the-- yes?",
    "start": "135445",
    "end": "140660"
  },
  {
    "text": "AUDIENCE: Sorry, what\nis dead reckoning? PROFESSOR 1: It's essentially-- you are saying I want\nto execute this option,",
    "start": "140660",
    "end": "146749"
  },
  {
    "text": "and it's going to\nexecute it exactly. AUDIENCE: OK.  PROFESSOR 1: Then\ntowards the bottom,",
    "start": "146749",
    "end": "152740"
  },
  {
    "text": "we have stochastic dynamics\nand deterministic sensors. So in this case maybe\nthere's some uncertainty in our actions.",
    "start": "152740",
    "end": "157790"
  },
  {
    "text": "But we can validate\nthrough sensing. This is where we're going\nto spend the next section on Markov decision processes.",
    "start": "157790",
    "end": "165069"
  },
  {
    "text": "And then briefly,\nlater in the lecture, we're going to talk about this\nmost difficult case, which is the stochastic dynamics\nand stochastic sensors.",
    "start": "165070",
    "end": "171415"
  },
  {
    "text": "Our execution maybe, maybe\nnot, our sensors maybe a little bit of noise. And this is where we\nsee partially observable",
    "start": "171415",
    "end": "178080"
  },
  {
    "text": "Markov decision processes. PROFESSOR 2: Can we make\njust a brief clarification",
    "start": "178080",
    "end": "183300"
  },
  {
    "text": "of the dead reckoning? So dead reckoning is\nwhere you estimate you position using a\nprobabilistic file, but you don't use any\nobservation stuffs.",
    "start": "183300",
    "end": "189290"
  },
  {
    "text": "That's the thing.  PROFESSOR 1: OK.",
    "start": "189290",
    "end": "194350"
  },
  {
    "text": "So we talked a little bit\nabout action uncertainty, where we're going to focus. And this is a case\nwhere, for instance,",
    "start": "194350",
    "end": "199870"
  },
  {
    "text": "even if you tell a quadrotor\nto stay in its place and a gust of wind\ncomes by, it's not",
    "start": "199870",
    "end": "206320"
  },
  {
    "text": "going to stay in that\nsame place, right? It's going to have a\nlittle bit of movement. And we can see, this\ncauses things in disparity",
    "start": "206320",
    "end": "211696"
  },
  {
    "text": "where you have a\ncommand of trajectory and your actual trajectory and\nthen you need to associate.",
    "start": "211697",
    "end": "217220"
  },
  {
    "text": "So in order to\ncompensate for that, we want to model things\nwith that uncertainty or else we have these\nhigher situations where",
    "start": "217220",
    "end": "224090"
  },
  {
    "text": "we command to follow some line. We don't incorporate\nthe uncertainty, and we see a crash. ",
    "start": "224090",
    "end": "231159"
  },
  {
    "text": "So this allows us to introduce\nPlanning with Markov Decision Processes. ",
    "start": "231159",
    "end": "238560"
  },
  {
    "text": "So MDPs have a set of states-- some actions you can\ntake-- a transition model.",
    "start": "238560",
    "end": "244370"
  },
  {
    "text": "So essentially,\nwhat's the probability of reaching some state\nif you take an action?",
    "start": "244370",
    "end": "249442"
  },
  {
    "text": "An immediate reward function\nand a discount factor. This discount\nfactor is important because it allows\nus to prioritize",
    "start": "249442",
    "end": "255007"
  },
  {
    "text": "gaining an immediate\nreward as opposed to an uncertain future award. So the concept of, bird in the\nhand is worth two in the bush.",
    "start": "255007",
    "end": "261250"
  },
  {
    "text": "Now we want to find\nan optimum policy that will essentially\nback an action-- the best action--\nfor each state.",
    "start": "261250",
    "end": "267120"
  },
  {
    "text": "And what we hope\nto get from this is maximized expected\nlifetime reward. So we want to maximize the\naccumulative reward we get over",
    "start": "267120",
    "end": "273820"
  },
  {
    "text": "the times. So let's walk\nthrough an example. If we have a quadrotor with\na perfect sensor, and let's",
    "start": "273820",
    "end": "281925"
  },
  {
    "text": "put it in this environment\n[INAUDIBLE] 7x7 bridge. Our set of states are obviously\n[INAUDIBLE] space in them.",
    "start": "281925",
    "end": "288552"
  },
  {
    "text": "Can anybody tell me what\nsome of the actions might be? AUDIENCE: [INAUDIBLE]",
    "start": "288553",
    "end": "295944"
  },
  {
    "text": "PROFESSOR 1: (LAUGHING) Yep. Up, down, left, right. In this case, we call them\nNorth, South, East, West, or Null. ",
    "start": "295944",
    "end": "303202"
  },
  {
    "text": "The next thing we need\nfor this example-- arbitrarily gave ourselves\na transition probability. So we said that you have\n2% chance of following",
    "start": "303202",
    "end": "309800"
  },
  {
    "text": "your commanded action with\na 25% chance of moving to the left or the right. Next we have the\nreward function.",
    "start": "309800",
    "end": "316675"
  },
  {
    "text": "And again, we\narbitrarily decided that we want it to be a reward. If you get to the state 6-5,\nthe increment [INAUDIBLE]",
    "start": "316675",
    "end": "324130"
  },
  {
    "text": "AUDIENCE: Alicia, you had\n[INAUDIBLE] left or the right, is that clockwise or\ncounter clockwise?",
    "start": "324131",
    "end": "332654"
  },
  {
    "text": "Let's say you had planned\nto go to the right, would that mean you have a 75%\n[INAUDIBLE] or 25% chance of",
    "start": "332654",
    "end": "339430"
  },
  {
    "text": "[INAUDIBLE]? What if you just\nwaited [INAUDIBLE]?? PROFESSOR 1: We said\nclockwise, counterclockwise from the intended\ndirection of action.",
    "start": "339430",
    "end": "346132"
  },
  {
    "text": "AUDIENCE: Thanks. PROFESSOR 1: Uh-huh. And finally, we give ourselves\na discount factor of 0.9.",
    "start": "346132",
    "end": "352169"
  },
  {
    "text": "So let's assume for\na second that we have our optimal policy. And let's say that our optimal\npolicy says, from this state,",
    "start": "352170",
    "end": "358720"
  },
  {
    "text": "we want to take\nthe action North. Right? As we discussed, we\nhave a 50% chance of going North and 25% chance\nof going to the left or right.",
    "start": "358720",
    "end": "366260"
  },
  {
    "text": "So after that time step,\nthese are possible states that we could end up in.",
    "start": "366260",
    "end": "371820"
  },
  {
    "text": "Right? So now let's assume\nfor a second that we can take our next action.",
    "start": "371820",
    "end": "377080"
  },
  {
    "text": "And our next action\nsays, go North. Again, we have the same\nprobability distribution. And these are the\nstates we could end up",
    "start": "377080",
    "end": "383220"
  },
  {
    "text": "in after two time steps. We can see that this starts\ngetting very complicated, right? And there are increasing\namounts of uncertainty.",
    "start": "383220",
    "end": "390340"
  },
  {
    "text": "So does anybody have any\nideas on how we could collapse this distribution? Keeping in mind that our\nsenses, at this point,",
    "start": "390340",
    "end": "397150"
  },
  {
    "text": "are deterministic. Yep. AUDIENCE: Fly to a corner? PROFESSOR 1: I'm sorry.",
    "start": "397150",
    "end": "402844"
  },
  {
    "text": "AUDIENCE: Fly to a corner? PROFESSOR 1: We could do that. AUDIENCE: You just sense\nhow far away the red box is. PROFESSOR 1: We could do that.",
    "start": "402844",
    "end": "410440"
  },
  {
    "text": "AUDIENCE: Quick comments. So the blue state\nis not actually one. PROFESSOR 1: I'm sorry. AUDIENCE: So the problem\nis you're not actually one.",
    "start": "410440",
    "end": "417255"
  },
  {
    "text": "AUDIENCE: Yeah. The issue is if\nyou went to (1,3) and then you transitioned\nto the left, that's where",
    "start": "417255",
    "end": "422958"
  },
  {
    "text": "the 0.0625 is coming from. AUDIENCE: [INAUDIBLE] then,\nshouldn't those numbers always",
    "start": "422958",
    "end": "428389"
  },
  {
    "text": "are actually 1-- your probability distribution\nalways are actually 1? PROFESSOR 1: Yep. AUDIENCE: So it's just a point. It's just off the screen.",
    "start": "428390",
    "end": "434609"
  },
  {
    "text": "PROFESSOR 1: We would add\nanother section to the screen and just move the grid over. We just cut it off for graphics.",
    "start": "434609",
    "end": "442299"
  },
  {
    "text": "AUDIENCE: OK.  PROFESSOR 1: Yeah. So those are all great points.",
    "start": "442299",
    "end": "449740"
  },
  {
    "text": "The easiest way to do it is\njust take an observation. So at this point we say,\nafter our first time set, we weren't sure which of\nthese three states we were in.",
    "start": "449740",
    "end": "456979"
  },
  {
    "text": "So we took an\nobservation and said, wait, we're actually here\nwith complete certainty. So to make this a\nlittle bit clear,",
    "start": "456980",
    "end": "463350"
  },
  {
    "text": "we're going to look at\nit from a tree view. Right? We said that we\nstarted at a state. We took an action. And this is our possible states\nwe could have ended up in.",
    "start": "463350",
    "end": "469895"
  },
  {
    "text": "We're going to collapse this\nby taking this observation. And now have a\ncomplete study here. And we take our\nnext action and see",
    "start": "469895",
    "end": "478250"
  },
  {
    "text": "that we have moved out here. So this allows us to basically\nignore the history of states.",
    "start": "478250",
    "end": "484500"
  },
  {
    "text": "We have the same percentage\nprobability from each time set. This will be really useful\nin completely collapsing",
    "start": "484500",
    "end": "492240"
  },
  {
    "text": "the distribution every single\ntime you take an observation. Anybody have any\nquestions on that?",
    "start": "492240",
    "end": "499170"
  },
  {
    "text": "OK. So let's go back now and\nfigure out how he came up with his optimal policy.",
    "start": "499170",
    "end": "505340"
  },
  {
    "text": "They way we do that is\nthrough dynamic programming. There's two different ways. You can do it either through\nvalue iteration or policy",
    "start": "505340",
    "end": "510825"
  },
  {
    "text": "iteration. For this lecture, we're going\nto focus on value iteration. ",
    "start": "510825",
    "end": "517779"
  },
  {
    "text": "So let's take this\nsame example we had. We still want to maximize\nthe expected reward. And so to start, we're\ngoing to initialize",
    "start": "517780",
    "end": "524250"
  },
  {
    "text": "the values of each state to 0. ",
    "start": "524250",
    "end": "529284"
  },
  {
    "text": "Let's for example,\nstart at (2,0). We're going to\nfocus on suite 6-5. And we're going\nto say that we're",
    "start": "529284",
    "end": "534640"
  },
  {
    "text": "going to take the Null\naction to start with. From there you can\nsee with a probability",
    "start": "534640",
    "end": "540710"
  },
  {
    "text": "distribution 4, 6, 5, we\nhave a 50% of staying. We have 25% chance\nof going to 5-5.",
    "start": "540710",
    "end": "547829"
  },
  {
    "text": "And a 25% chance\nof going to 7-5. The next item of\ninformation is the values",
    "start": "547829",
    "end": "554089"
  },
  {
    "text": "at each of those states\nthat we could end up in. And currently,\nthey're all set to 0.",
    "start": "554090",
    "end": "559502"
  },
  {
    "text": "And finally, the\nmost important part here will be with the\nreward [INAUDIBLE] 6-5, taking the null action,\nregardless of what state",
    "start": "559502",
    "end": "566100"
  },
  {
    "text": "you end up, is going to be 1,\nas we defined in our initial set up. ",
    "start": "566100",
    "end": "571774"
  },
  {
    "text": "So let's see how we would\ncalculate the next time step value of the state.",
    "start": "571774",
    "end": "577269"
  },
  {
    "text": "You'd start by taking\nprobabilities, right?  And from there, we would\nadd the reward here.",
    "start": "577269",
    "end": "583900"
  },
  {
    "text": "And because we said\nthat the reward does not dependent on the state\nwe end up in, [INAUDIBLE] should be across all\nthree probabilities.",
    "start": "583900",
    "end": "590845"
  },
  {
    "text": "From there, we're going to\nadd in the discounted value that we had from before.",
    "start": "590845",
    "end": "598475"
  },
  {
    "text": "So a way to look at this\nin a more generalized form is to say that across all the\nstates that you can end up in, you're going to look at the\nprobability of ending up",
    "start": "598475",
    "end": "605640"
  },
  {
    "text": "in that state. You're going to\nmultiply that by the sum of the reward and the discounted\nlifetime value that it has.",
    "start": "605640",
    "end": "610788"
  },
  {
    "text": " So we want to make sure\nthat we're getting the best",
    "start": "610788",
    "end": "618009"
  },
  {
    "text": "possible values. So we need to incorporate\nall the other actions that we can take from that state.",
    "start": "618010",
    "end": "624064"
  },
  {
    "text": "So what's going\nto happen is we're going to take that\ngeneral formula, we're going to repeat it over\nall of the possible actions.",
    "start": "624064",
    "end": "629076"
  },
  {
    "text": "And then we're going to\ntake the maximum of that. So, for this example,\nthe state is very easy.",
    "start": "629076",
    "end": "634829"
  },
  {
    "text": "All of them are the\nsame for this case. But we go fast, and we\nsay we get a value of 1. And we update it by\nshowing the graph.",
    "start": "634830",
    "end": "641186"
  },
  {
    "text": " This gives us what's called\nthe value [INAUDIBLE] Backup--",
    "start": "641186",
    "end": "648209"
  },
  {
    "text": "or Update equation. This will be really\nimportant because it reaches across the\nentire states space and allows us to\nprovide a history.",
    "start": "648210",
    "end": "654073"
  },
  {
    "text": " So what this would\nend up looking like is we're going to\niteratively calculate",
    "start": "654073",
    "end": "660003"
  },
  {
    "text": "the values across the\nentire state space So at t0, we determine that\nall the values are 0.",
    "start": "660003",
    "end": "665320"
  },
  {
    "text": "At t1, (6,5) gets a\nvalue of 1, and at t2, we see that value start\nto propagate out.",
    "start": "665320",
    "end": "673106"
  },
  {
    "text": "Make sense so far?  So the way this works is you\nwould repeat those iterations",
    "start": "673106",
    "end": "679460"
  },
  {
    "text": "until your changes in value\nbecome what we would consider small enough, which would\nindicate your approximation is close enough\nto the real value.",
    "start": "679460",
    "end": "685620"
  },
  {
    "text": "From there, you would\nextract the optimal policy from the lifetime values. So you see the [INAUDIBLE]\nin the Bellman equation.",
    "start": "685620",
    "end": "691155"
  },
  {
    "text": "And you're now just\ntaking the action from it. And you would map those\nactions to your states. An example of what this might\nlook like propagating out--",
    "start": "691155",
    "end": "698792"
  },
  {
    "text": "if you say blue was the\nreward and red is an obstacle, et cetera-- you can see, as that\nvalue propagates out,",
    "start": "698792",
    "end": "704970"
  },
  {
    "text": "you start seeing your\npolicy by the arrows. Anybody have any questions?",
    "start": "704970",
    "end": "710381"
  },
  {
    "text": " So one last thing to\nmention about this, though,",
    "start": "710381",
    "end": "716370"
  },
  {
    "text": "is the complexity\nfor each iteration is dependent on the size\nof the state space squared and the number of\nactions you can take.",
    "start": "716370",
    "end": "722112"
  },
  {
    "text": "So you can imagine, as your\nstate space expands or you gather more actions\nit's very complex,",
    "start": "722112",
    "end": "727689"
  },
  {
    "text": "in which case time and\nvalue depiction becomes very time intensive and costly. So this allows us to move into\nthe Heuristic-Guided solvers.",
    "start": "727690",
    "end": "736897"
  },
  {
    "start": "732000",
    "end": "808000"
  },
  {
    "text": "AUDIENCE: Thank you. [INAUDIBLE] transition,\none quick question. Can I just get a show of\nhands-- how many of you",
    "start": "736897",
    "end": "743625"
  },
  {
    "text": "have learned value iteration\nbefore versus how many of you have seen it for the first time? So how many have seen it before?",
    "start": "743625",
    "end": "750770"
  },
  {
    "text": "OK. And then, how many is\nthis their first time? [INAUDIBLE]",
    "start": "750770",
    "end": "757312"
  },
  {
    "text": "PROFESSOR 3: Any questions\non value iteration before we jump in? It's going to be an essential\npart of how we're going",
    "start": "757312",
    "end": "762641"
  },
  {
    "text": "to do [INAUDIBLE] solvers. Anyone who hasn't [INAUDIBLE]. ",
    "start": "762642",
    "end": "769639"
  },
  {
    "text": "So the most important thing\nwe said about value iteration is that it's super slow. It's going to have to go\nover every possible state",
    "start": "769640",
    "end": "776880"
  },
  {
    "text": "and every possible\naction that it can take. Our state space is\nmulti-dimensional, and we can take a lot\nof different actions.",
    "start": "776880",
    "end": "783130"
  },
  {
    "text": "That going to be really costly\nand really hard to [INAUDIBLE]..",
    "start": "783130",
    "end": "788150"
  },
  {
    "text": "So the approach we're\nprobably going to want to take is using some sort of\nbest first search applet?",
    "start": "788150",
    "end": "796210"
  },
  {
    "text": "Who can tell me some\nexample algorithms that already do that?",
    "start": "796210",
    "end": "802503"
  },
  {
    "text": "AUDIENCE: A star. PROFESSOR 3: A star. So that's very good. And that's exactly what we're\ngoing to base our stuff off of.",
    "start": "802504",
    "end": "808610"
  },
  {
    "start": "808000",
    "end": "1189000"
  },
  {
    "text": "The A star is for\ndeterministic graph search. If we have a graph, we\ncan use it heuristic,",
    "start": "808610",
    "end": "815510"
  },
  {
    "text": "and we walk down it\nand search the space that we're most interested in\nuntil [INAUDIBLE] variable.",
    "start": "815510",
    "end": "821520"
  },
  {
    "text": "So going to introduce to new\nitems focusing on the last one. AO star is an\nalgorithm we can use",
    "start": "821520",
    "end": "828579"
  },
  {
    "text": "to search for graphs that\nhave this \"and\" problem. AO stands for And Or graphs as\nopposed to the simple graphs",
    "start": "828580",
    "end": "837520"
  },
  {
    "text": "that we have [INAUDIBLE]. The And is a way to express\nprobabilistic coupling",
    "start": "837520",
    "end": "843360"
  },
  {
    "text": "between edges. So if we explore one\nthing, we might have to explore other functions. We'll discuss that a little\nbit more in the next slide.",
    "start": "843360",
    "end": "849890"
  },
  {
    "text": "LAO is a bit more\nof a generalization. What it does is allows\nus to search loopy graphs",
    "start": "849890",
    "end": "856510"
  },
  {
    "text": "and deal with the\nprobabilistic coupling. And allows us to search\nand find the best path",
    "start": "856510",
    "end": "863090"
  },
  {
    "text": "with a heuristic guided\nalgorithm over infinite time horizon, possibly revisiting\nstates using the tools",
    "start": "863090",
    "end": "869550"
  },
  {
    "text": "we just had-- valued iterations-- to\nunderstand what the next best thing to do is.",
    "start": "869550",
    "end": "874840"
  },
  {
    "text": "So we'll talk about exactly how\nto get our MDPs that we just saw, these little arrow\nexamples to these And/Or graphs.",
    "start": "874840",
    "end": "882660"
  },
  {
    "text": "We'll talk about two cases. One simple one where we could\napply the AO star algorithm is where you have a qualicopter.",
    "start": "882660",
    "end": "889450"
  },
  {
    "text": "And if you command\nan action North, you have a high\nprobability of going North, but you might go East.",
    "start": "889450",
    "end": "896390"
  },
  {
    "text": "And vise versa. If you go East,\nyou might go North. This is expressed here\nwith the growing tree.",
    "start": "896390",
    "end": "904066"
  },
  {
    "text": "And these And edgers\nthat connect it. But despite the action of\nreading my command from 0 to 0.",
    "start": "904066",
    "end": "909500"
  },
  {
    "text": "We might end up\nin (1,0) or (0,1). Right? As we propagate\noutward, you can see",
    "start": "909500",
    "end": "914700"
  },
  {
    "text": "how we're never\ngoing to loop back to a statement we've been to. We're going to be moving in the\nNortheast direction constantly.",
    "start": "914700",
    "end": "920227"
  },
  {
    "text": "Our [INAUDIBLE] and our\nprobability distribution across this tree explands. And that's the\ncoupling of the edges.",
    "start": "920228",
    "end": "928089"
  },
  {
    "text": "Because as we explore\ndown the tree, we have to explore all\nthe edges together. Anyone have any questions\non just this kind",
    "start": "928090",
    "end": "934773"
  },
  {
    "text": "of conversion formulation? AUDIENCE: I have a\nbroader question. We mentioned that MDPs\ndeal with finite states.",
    "start": "934773",
    "end": "943010"
  },
  {
    "text": "Do we always just\ndiscretize a continuous row into a set of planning states? PROFESSOR 3: Yes.",
    "start": "943010",
    "end": "948240"
  },
  {
    "text": "That's our prerequisite for\nsearching over the state space. We can do that as\nfinely as possible,",
    "start": "948240",
    "end": "953574"
  },
  {
    "text": "but yes, discretization\nis there. So now let's look at this case.",
    "start": "953574",
    "end": "960339"
  },
  {
    "text": "Instead of the Northeast, let's\nsay it's not deterministic whether we command an\naction north or South",
    "start": "960340",
    "end": "965910"
  },
  {
    "text": "that we go north or south. Here we see this loopy\nstructure begin to emerge. We see that we might--",
    "start": "965910",
    "end": "972360"
  },
  {
    "text": "on our first action commanding\nNorth, we might go to plus 1. And then commanding North\nagain, but we have our",
    "start": "972360",
    "end": "978371"
  },
  {
    "text": "And edge and our\nprobability distribution is [? back ?] to 0. What this does is\nthis loopy structure.",
    "start": "978372",
    "end": "984775"
  },
  {
    "text": "And this is exactly what\nwe're going to be exploring. This is a very\nreal world scenario where it's a very likely we\nmight return to somewhere",
    "start": "984775",
    "end": "992029"
  },
  {
    "text": "we just came from, just because\nof the uncertain dynamics we have. This is the type of\nproblem [INAUDIBLE]..",
    "start": "992030",
    "end": "999840"
  },
  {
    "text": "So we're going to use the\nLAO to start out with. We're going to talk about\nthe three main things",
    "start": "999840",
    "end": "1005540"
  },
  {
    "text": "that [INAUDIBLE] has a\nheuristic guided envelope. And what that means is that\nwe have our large state space",
    "start": "1005540",
    "end": "1011410"
  },
  {
    "text": "here. But we're only going to\nlook at a portion of it. This greyed-out portion.",
    "start": "1011410",
    "end": "1017149"
  },
  {
    "text": "We're only going to\nlook at the portion that is interesting to us-- the\nportion that provides us",
    "start": "1017150",
    "end": "1022320"
  },
  {
    "text": "with the biggest rewards--\nthe portion that's reachable if we follow an optimal policy. We figure this out with\nsome admissible heuristic.",
    "start": "1022320",
    "end": "1030609"
  },
  {
    "text": "We'll estimate our\nrewards just like A star. The idea here was that\nwe'll keep the problem small",
    "start": "1030609",
    "end": "1037089"
  },
  {
    "text": "so we don't have to search the\nvaluation for a giant state space.",
    "start": "1037089",
    "end": "1042329"
  },
  {
    "text": "What we'll do next is at\nthe state space expands, we get a bigger\npicture of the states",
    "start": "1042329",
    "end": "1048146"
  },
  {
    "text": "that we're interested in. We're going to do\n[? an audio ?] [INAUDIBLE].. And then we're going to figure\nout what the best action is.",
    "start": "1048146",
    "end": "1054510"
  },
  {
    "text": "And in the ideal\ncase, the states that we would never reach\nusing an optimal policy",
    "start": "1054510",
    "end": "1060148"
  },
  {
    "text": "are never going to be explored. Because our policy is going to\nsay, no don't go over there. That's a dead end.",
    "start": "1060149",
    "end": "1065481"
  },
  {
    "text": "Or that's getting you farther\naway from [INAUDIBLE].. So we're going to be searching\nin a very specific part of the state space that\nis useful to explore--",
    "start": "1065481",
    "end": "1073390"
  },
  {
    "text": "that will get us closer and\ngive us a higher reward. ",
    "start": "1073390",
    "end": "1085419"
  },
  {
    "text": "What's important here,\nthe L stands for Loops. It's an extension of\nthe AO star algorithm,",
    "start": "1085420",
    "end": "1090770"
  },
  {
    "text": "which is by itself is an\nextension of the A star algorithm. We can handle infinite\nhorizon problems, like transition\nin different ways.",
    "start": "1090770",
    "end": "1097975"
  },
  {
    "text": "And really models the real world\nscenarios we're interesed in. Any questions so far\non the broad scope",
    "start": "1097975",
    "end": "1103850"
  },
  {
    "text": "of what AO star is going to do? Yeah. AUDIENCE: Can you put the\n[INAUDIBLE] what you're doing",
    "start": "1103850",
    "end": "1109634"
  },
  {
    "text": "over here, but-- PROFESSOR 3: Sure. The AO stands for And Or graphs. So that's graphs that we have\nhere edges that are coupled.",
    "start": "1109634",
    "end": "1117575"
  },
  {
    "text": "If you took time to\ndo this transition, you might end up here or\nyou might end up there. So that's the notion\nthat of this probability",
    "start": "1117576",
    "end": "1124455"
  },
  {
    "text": "coupling, that we'll see\nin action as we [INAUDIBLE]",
    "start": "1124455",
    "end": "1133399"
  },
  {
    "text": "we're going to do, we're\ngoing to input this MDP or AO graph with transition\nprobabilities or reward",
    "start": "1133400",
    "end": "1141000"
  },
  {
    "text": "function heuristics. These are all things\nthat we defined prior to figuring out a plan.",
    "start": "1141000",
    "end": "1146330"
  },
  {
    "text": "What we're going\nto come out with is an optimal policy for\nevery regional state. It's a little different than\nwhat value iteration is,",
    "start": "1146330",
    "end": "1152500"
  },
  {
    "text": "which is an optimal policy\nfrom every possible state. But we argue that\nthat's all we need.",
    "start": "1152500",
    "end": "1158840"
  },
  {
    "text": "If we know where we're\nstarting and we follow our optimal policy,\nwe're only going to explore a certain\nportion of the state space.",
    "start": "1158840",
    "end": "1165760"
  },
  {
    "text": "And we're going to\nexplore that together. [INAUDIBLE] plan\na little bit more efficiently than iterating for\na high [INAUDIBLE] heuristic.",
    "start": "1165760",
    "end": "1175362"
  },
  {
    "text": "Any questions on that? So we'll define some\nterms that we're",
    "start": "1175362",
    "end": "1181445"
  },
  {
    "text": "going to use throughout this. We've already talked\nabout our state space. This is just a\nsmall portion of it",
    "start": "1181445",
    "end": "1186649"
  },
  {
    "text": "that we're going to\nwork with as we're going to walk through this example. Next we're going to define\nsomething called our envelope.",
    "start": "1186650",
    "end": "1192225"
  },
  {
    "start": "1189000",
    "end": "1206000"
  },
  {
    "text": "And that's the\nsub-portion of our states that we're going\nto be looking at. We're going to initialize that\nto just this zero [INAUDIBLE]..",
    "start": "1192225",
    "end": "1199429"
  },
  {
    "text": "But as we progress through the\nalgorithm, it's going to grow. It's going to grow only in the\nareas that we're interested in.",
    "start": "1199429",
    "end": "1204770"
  },
  {
    "text": " A subset of this envelope\nis the terminal states.",
    "start": "1204770",
    "end": "1210430"
  },
  {
    "start": "1206000",
    "end": "1309000"
  },
  {
    "text": "Now these aren't\ngoal states, these are just [INAUDIBLE]\nof our space that we've added\nto our envelopes.",
    "start": "1210430",
    "end": "1216176"
  },
  {
    "text": "And this is what it would be\nin the expanded [INAUDIBLE].. And it's just the nodes that\nhaven't been expanded yet.",
    "start": "1216176",
    "end": "1222329"
  },
  {
    "text": "Here they haven't drawn\nanything incredible but you can imagine the\nstate space goes out further because [INAUDIBLE].",
    "start": "1222329",
    "end": "1229375"
  },
  {
    "text": "You can imagine that\nthis goes out further. And they're keeping\ntrack of the nodes that we haven't expanded yet.",
    "start": "1229375",
    "end": "1237380"
  },
  {
    "text": "Or likewise, we've showed that\nwe initialize [INAUDIBLE].. ",
    "start": "1237380",
    "end": "1244289"
  },
  {
    "text": "The other few things that\nwe're going to define-- we've already defined the states\nthat are in our envelope.",
    "start": "1244290",
    "end": "1249864"
  },
  {
    "text": "That's the blue or the red. We're going to define cost\nheuristic or reward function, R",
    "start": "1249864",
    "end": "1255790"
  },
  {
    "text": "E, and a transition probability\nmatrix, or set of matrices,",
    "start": "1255790",
    "end": "1260890"
  },
  {
    "text": "that we're going to use our\noptimal policy search on. What's important here is\nthat our reward function",
    "start": "1260890",
    "end": "1267910"
  },
  {
    "text": "and transition\nprobabilities are slightly altered to account for the\nfact that we haven't explored the entire state space.",
    "start": "1267910",
    "end": "1273680"
  },
  {
    "text": "We see here that if a node\nis in ST, in other words, it's one of those\nterminal nodes.",
    "start": "1273680",
    "end": "1281350"
  },
  {
    "text": "We're going to say we\ncan't transition out of it, because we don't know\nwhat's beyond it so far. And we're going to set it\nfor reward to be a heuristic.",
    "start": "1281350",
    "end": "1288674"
  },
  {
    "text": "Whenever we think\nthat the reward is going to be when we begin\nto explore and go further.",
    "start": "1288675",
    "end": "1295309"
  },
  {
    "text": "Like I said, we're just going to\nfeed this into a policy search just like we discussed\nwith value iterations",
    "start": "1295310",
    "end": "1301650"
  },
  {
    "text": "on the sub problem. And we're going to search for\nan optimal policy [INAUDIBLE].. So far so good?",
    "start": "1301650",
    "end": "1307446"
  },
  {
    "text": " This is the general steps. This is very text-y, but\nwe're going to definitely walk",
    "start": "1307446",
    "end": "1313900"
  },
  {
    "start": "1309000",
    "end": "1465000"
  },
  {
    "text": "through every single step. We're going to do two full\niterations of the algorithm. So like I said, we're\ngoing to create RE and TE.",
    "start": "1313900",
    "end": "1320481"
  },
  {
    "text": "That's are reward function\ntransition probabilities using the definitions we showed. We're going to use\nvalue iteration",
    "start": "1320482",
    "end": "1326640"
  },
  {
    "text": "to find the optimal\npolicy of the sub space that we're interested in now. And then this is probably\nthe most important step.",
    "start": "1326640",
    "end": "1334029"
  },
  {
    "text": "Knowing this optimal policy, we\ntake a look at what new states that aren't in our\nterminal states--",
    "start": "1334030",
    "end": "1339580"
  },
  {
    "text": "nodes that we haven't\nexplored yet-- what new states we\nmight visit now. So let's say we have our policy,\nand it says we'll go North.",
    "start": "1339580",
    "end": "1347526"
  },
  {
    "text": "And we know that we haven't\nexplored the North state yet. We know this is the\nstate that we're going to reach following\nwhat we consider now to be",
    "start": "1347526",
    "end": "1354287"
  },
  {
    "text": "already an optimal [INAUDIBLE]. So that's the states we're\ngoing to expand next. We're going to do\nsome bookkeeping,",
    "start": "1354287",
    "end": "1360300"
  },
  {
    "text": "adding them to the\nterminal states [INAUDIBLE] once we expand it, then\nadding them to our envelope. What's important here\nis that we're only",
    "start": "1360300",
    "end": "1366430"
  },
  {
    "text": "going to add states that we\nvisited yet to our envelope. And this is basically\nthe little hack",
    "start": "1366430",
    "end": "1371480"
  },
  {
    "text": "that allows us to deal\nwith loopy graphs. We're not going to\ncontinually explore nodes",
    "start": "1371480",
    "end": "1377880"
  },
  {
    "text": "that we might reach a second\ntime, probabilistically. We're going to let\nvalue iteration handle that [INAUDIBLE]",
    "start": "1377880",
    "end": "1385092"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nstates are expanded.",
    "start": "1385092",
    "end": "1391068"
  },
  {
    "text": "I'm the one who got\nconfused on that. Are you saying that you're\ngoing to just repeat until there aren't any\nmore terminals to look at.",
    "start": "1391068",
    "end": "1398538"
  },
  {
    "text": "And if that's the case, how\nis that possible if you have an infinite horizon [INAUDIBLE]",
    "start": "1398538",
    "end": "1404701"
  },
  {
    "text": "PROFESSOR 3: Sure. So if you can imagine-- and\nwe'll talk about termination at the end. But you can imagine that as\nwe have these terminal states,",
    "start": "1404701",
    "end": "1413540"
  },
  {
    "text": "but you have a policy\nthat guides you to a part of the state space\nthat we've already expanded. Imagine you've\nreached your goal.",
    "start": "1413540",
    "end": "1419733"
  },
  {
    "text": "Your optimal policy is\ngoing to say, stay put. Right? And it's not going to\nsay, move North again. AUDIENCE: It's just\nthe goal [INAUDIBLE]",
    "start": "1419733",
    "end": "1426094"
  },
  {
    "text": "PROFESSOR 3: Essentially,\nthe goal state is definitely an example\nin the more extreme case that there's nothing else\nyou can do that's going",
    "start": "1426094",
    "end": "1431964"
  },
  {
    "text": "to get you closer to our goal. The idea is that your\npolicy on your sub space",
    "start": "1431964",
    "end": "1439020"
  },
  {
    "text": "never tells you to\ngo to a terminal. Nobody can [INAUDIBLE]\ninherently worse than",
    "start": "1439020",
    "end": "1444096"
  },
  {
    "text": "[INAUDIBLE] AUDIENCE: Planning\noptimal policy means running value\niteration entirely.",
    "start": "1444096",
    "end": "1449862"
  },
  {
    "text": "PROFESSOR 3: Yes. We were going to treat it\nessentially as a black box. But the trick here is\nthat we're doing it",
    "start": "1449862",
    "end": "1455014"
  },
  {
    "text": "on a smaller portion of\nspace of a different world. ",
    "start": "1455014",
    "end": "1460780"
  },
  {
    "text": "All right. I'm going to put\nthe steps up there. Hopefully you can see this. But for now, we'll\njust walk through",
    "start": "1460780",
    "end": "1468143"
  },
  {
    "start": "1465000",
    "end": "1504000"
  },
  {
    "text": "from the beginning of\nwhat we're going to do. So we said that our envelope\nand our terminal nodes",
    "start": "1468143",
    "end": "1474890"
  },
  {
    "text": "are just at 0 to start. So very simply, we\nuse these definitions",
    "start": "1474890",
    "end": "1480230"
  },
  {
    "text": "and say, OK, the\ntransition probability as 0 to any node right now is 0.",
    "start": "1480230",
    "end": "1485531"
  },
  {
    "text": "Because it's in that terminal. [INAUDIBLE] That's just so that\nwe don't transition out of it.",
    "start": "1485531",
    "end": "1490669"
  },
  {
    "text": "We can develop the policy\nbased on only this portion of the space [INAUDIBLE]. And our reward\nfunction, we're just",
    "start": "1490670",
    "end": "1496760"
  },
  {
    "text": "going to apply it to be\nthe heuristic [INAUDIBLE].. Let's say that's 20. And for a move-on from there.",
    "start": "1496760",
    "end": "1505234"
  },
  {
    "start": "1504000",
    "end": "1533000"
  },
  {
    "text": "[INAUDIBLE] started\nto value iteration. And we'll run this\nvalue iteration. So this is a very basic case.",
    "start": "1505234",
    "end": "1511539"
  },
  {
    "text": "We're just going to-- we're\nusing this to kind of build up the machinery, understand\nwhat we're doing. It's a very basic case where you\nonly node we have is that zero.",
    "start": "1511540",
    "end": "1519700"
  },
  {
    "text": "We can't transition out of it. All we have is some heuristic. So the only thing we\ncan do is nothing.",
    "start": "1519700",
    "end": "1524960"
  },
  {
    "text": "So the action we're going\nto take from S0 is nothing. Very simple case just to get\nus warmed up and understand",
    "start": "1524960",
    "end": "1530780"
  },
  {
    "text": "what's happening. So using this policy and knowing\nthat those 0s in our terminal",
    "start": "1530780",
    "end": "1537258"
  },
  {
    "text": "node are node reset. We're going to take\nthe intersection between this terminal mode set.",
    "start": "1537258",
    "end": "1543370"
  },
  {
    "text": "And the nodes that we might\nreach following our policy. And we know that we're at S0. And we know that action that\nour optimal policy says to take",
    "start": "1543370",
    "end": "1553235"
  },
  {
    "text": "is not. So we know we're there. And we know it's in\nour terminal node set. So that's the only thing\nthat we could reach so far",
    "start": "1553235",
    "end": "1561655"
  },
  {
    "text": "using our optimal policy. So far so good? Clear? ",
    "start": "1561655",
    "end": "1569340"
  },
  {
    "text": "So that's exactly what\nwe're going to expand. Just expand S0, A, B,\nand C defined those",
    "start": "1569340",
    "end": "1574632"
  },
  {
    "text": "to our terminal nodes. So that's up there the\nsymbols [INAUDIBLE]",
    "start": "1574633",
    "end": "1579702"
  },
  {
    "text": "from our terminal nodes\nadded as children. Then we added the\nchildren to the end. ",
    "start": "1579702",
    "end": "1589390"
  },
  {
    "text": "Here we go. We're going to do\na little bit more. We're going to do\nthe same thing again. But now we obviously have more\nnodes and a bigger sub space",
    "start": "1589390",
    "end": "1596274"
  },
  {
    "text": "to explore. So using these definitions\nwe see our reward function",
    "start": "1596274",
    "end": "1603200"
  },
  {
    "text": "is a tuple of the node\nthat we start from. And so this S0.",
    "start": "1603200",
    "end": "1608670"
  },
  {
    "text": "And one of these three actions. And we'll take A1, A2, and A3. When we have our instantaneous\nreward functions, 6,4, and 8",
    "start": "1608670",
    "end": "1616350"
  },
  {
    "text": "as being our rewards\nfrom doing those actions. From A, B, and C, no matter\nwhat action you take,",
    "start": "1616350",
    "end": "1623327"
  },
  {
    "text": "it's just heuristic. That's part of it. And likewise, we're going to\ntake a look at this transition",
    "start": "1623327",
    "end": "1630430"
  },
  {
    "text": "probability [INAUDIBLE]. This is for [INAUDIBLE]\ntransitioning from a state S0 for saying that if we take\nactions A1, A2, and A3,",
    "start": "1630430",
    "end": "1637509"
  },
  {
    "text": "what's the probability of being\ndone in nodes A, B, and C? If you look so far\nhere, we're going",
    "start": "1637510",
    "end": "1643632"
  },
  {
    "text": "to look at something\ndeterministic. If we take an\naction, we'll end up where we say we're\ngoing to end up. And we'll see how this algorithm\ncollapses down to A star",
    "start": "1643632",
    "end": "1651059"
  },
  {
    "text": "if everything's deterministic. We're also obviously going to\nlook at the probabilistic case",
    "start": "1651060",
    "end": "1658100"
  },
  {
    "text": "where we say [INAUDIBLE]\nsmall probability that it might end up with [INAUDIBLE]. That's going to\nnecessitate that we're",
    "start": "1658100",
    "end": "1663590"
  },
  {
    "text": "going to have to look at\nand expand B together with A if we were to decide we\nwant to try [INAUDIBLE]..",
    "start": "1663590",
    "end": "1670015"
  },
  {
    "text": "And likewise, if we\ntry to take action A3, we have to expand\nthem all three nodes. So the tighter the probabilistic\ncoupling, the more of the space",
    "start": "1670015",
    "end": "1677409"
  },
  {
    "text": "we're going to have to explore.  So just off this, assuming\nthat if you take action A1, A2,",
    "start": "1677410",
    "end": "1685346"
  },
  {
    "text": "and A3, can someone tell me what\nthe policy is for the rewards here?",
    "start": "1685346",
    "end": "1692200"
  },
  {
    "text": "And we're interested in a policy\nfrom S0 what we're going to do. Take a look at the\nrewards and judge",
    "start": "1692200",
    "end": "1699500"
  },
  {
    "text": "what the best action to take\nis from a purely deterministic sense. ",
    "start": "1699500",
    "end": "1708562"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\ndo you add [INAUDIBLE] or do you just [INAUDIBLE]?",
    "start": "1708562",
    "end": "1714370"
  },
  {
    "text": "You have the reward that\nyou have gotten so far. ",
    "start": "1714370",
    "end": "1725130"
  },
  {
    "text": "PROFESSOR 3: Does that help\nyou answer the question? AUDIENCE: Oh, yeah. [INAUDIBLE] student. PROFESSOR 3: So the\npolicy preference",
    "start": "1725130",
    "end": "1731600"
  },
  {
    "text": "says, from S0, take action A1. And that's going to stay there.",
    "start": "1731600",
    "end": "1737666"
  },
  {
    "text": "[INAUDIBLE] from A to C, there's\nno action to take [INAUDIBLE].. What that means is that the\nnodes that we might reach",
    "start": "1737666",
    "end": "1744800"
  },
  {
    "text": "that are in our terminal\nstate set, using our policy, is node A. All right.",
    "start": "1744800",
    "end": "1749900"
  },
  {
    "text": "So that's the node\ngoing to expand. And this is where you really\nsee that all we've done is collapse down to A star.",
    "start": "1749900",
    "end": "1756620"
  },
  {
    "text": "A star would say, OK. What's the best node\nusing some heuristic.",
    "start": "1756620",
    "end": "1763350"
  },
  {
    "text": "Action a1 one takes\nus to that best node, and we're going to\nexpand just that.",
    "start": "1763350",
    "end": "1769680"
  },
  {
    "text": "So when everything\nis deterministic, basically this algorithm\ncollapses down to A star. ",
    "start": "1769680",
    "end": "1777056"
  },
  {
    "text": "Nothing super interesting. The interesting\ncase does come up when we start doing\nmore probablistic ones.",
    "start": "1777056",
    "end": "1782900"
  },
  {
    "text": "That's where nodes\nare probabilistically helpful to the\nscanned graph sense. ",
    "start": "1782900",
    "end": "1788550"
  },
  {
    "text": "So now we have our policy. The policies are politely\ngoing to remain the same, because they have very little\nprobability on the edge actions",
    "start": "1788550",
    "end": "1798250"
  },
  {
    "text": "that we might\naccidentally hit up. What would I want to look\nat is what [INAUDIBLE]",
    "start": "1798250",
    "end": "1806250"
  },
  {
    "text": "and how reachable following\nour optimal policy. ",
    "start": "1806250",
    "end": "1815159"
  },
  {
    "text": "So we talked about that if\nwe were to actually read. We know some probability of\nending up in [INAUDIBLE] nodes.",
    "start": "1815160",
    "end": "1822012"
  },
  {
    "text": "So taking action A3 makes\nC, 2, and A all reachable. What's reachable in\ntaking action A1?",
    "start": "1822012",
    "end": "1828650"
  },
  {
    "start": "1828650",
    "end": "1836111"
  },
  {
    "text": "AUDIENCE: A and B. PROFESSOR 3:\n[INAUDIBLE] And that's where we get the notion on\nthis probabilistic algorithm",
    "start": "1836111",
    "end": "1842700"
  },
  {
    "text": "that we're going\nexpand things together. Explore the part\nof the state space that we reach both\nhigher optimal polity",
    "start": "1842700",
    "end": "1850039"
  },
  {
    "text": "and that we might accidentally\nend up in if we [INAUDIBLE] the policy. And this is what guarantees\nthat we'll have an action",
    "start": "1850040",
    "end": "1857870"
  },
  {
    "text": "to take from any state\nthat we intended to go to or that we might\nend up [INAUDIBLE]..",
    "start": "1857870",
    "end": "1863440"
  },
  {
    "text": "That's how our state\nexpands, our envelope expands to encompass only\nthe reachable and interesting",
    "start": "1863440",
    "end": "1869029"
  },
  {
    "text": "states that we want to look at. It's not as simple\nas just examining",
    "start": "1869030",
    "end": "1874360"
  },
  {
    "text": "not only the heuristic,\nbut with A start because we have this\nprobabilistic [INAUDIBLE]..",
    "start": "1874360",
    "end": "1879850"
  },
  {
    "text": "You can see that if you have\na tighter coupling, then you don't get to exploit\nthis optimization as much.",
    "start": "1879850",
    "end": "1888970"
  },
  {
    "text": "For example, A3 we\nwould have had to expand more as opposed to A1. We can just stick to A and B\nand ignore expanding C for now.",
    "start": "1888970",
    "end": "1898930"
  },
  {
    "text": "[INAUDIBLE]  AUDIENCE: Our [INAUDIBLE]\nWe know that when people",
    "start": "1898930",
    "end": "1904950"
  },
  {
    "text": "do the statistic [INAUDIBLE]. So in this case, if you take\naction a because of a and b,",
    "start": "1904950",
    "end": "1911340"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "1911340",
    "end": "1921880"
  },
  {
    "text": "PROFESSOR 3: So in\nthe complete sense of running this algorithm,\nwe shouldn't print it. Because what if we do?",
    "start": "1921880",
    "end": "1927190"
  },
  {
    "text": "If we have that 2%\nchance, [INAUDIBLE].. We need to have a\npolicy for correcting.",
    "start": "1927190",
    "end": "1932430"
  },
  {
    "text": "So we end up at\nB. You can imagine that these are going to try\nto push back to whatever path",
    "start": "1932430",
    "end": "1937502"
  },
  {
    "text": "that A was looking for. I suppose that the\nprobability is low enough, you can have some\ncutoff percentage where",
    "start": "1937502",
    "end": "1944850"
  },
  {
    "text": "you've decoupled [INAUDIBLE]-- PROFESSOR 2: So\njust a quick point. So I think that's an excellent\npoint and an excellent answer.",
    "start": "1944850",
    "end": "1950979"
  },
  {
    "text": "We're going to talk\nabout next week-- exactly what's going\nto happen and if you can prove lower\nprobability on the paper",
    "start": "1950979",
    "end": "1957120"
  },
  {
    "text": "right here will be\nlecturing on that. Good question. PROFESSOR 3: That also\ngets us into the sense",
    "start": "1957120",
    "end": "1963380"
  },
  {
    "text": "that if every state in the\nworld were probabilistically coupled-- let's say we had some\ntransporter to go with the Star",
    "start": "1963380",
    "end": "1970370"
  },
  {
    "text": "Trek examples. If we had this transported that\nnon-deterministically put us",
    "start": "1970370",
    "end": "1976610"
  },
  {
    "text": "in any state in\nthe world, we have to explore the whole world,\nbecause we could end up [INAUDIBLE]. So luckily that's not the case\nyet and we can take advantage",
    "start": "1976610",
    "end": "1984410"
  },
  {
    "text": "of the fact that we ended\nup most likely where we commanded with some\nprobability of [INAUDIBLE]",
    "start": "1984410",
    "end": "1994510"
  },
  {
    "text": "This is exactly what\nwe just talked about. We coupled these nodes\nwith this And edge. And we expand those, too.",
    "start": "1994510",
    "end": "2001230"
  },
  {
    "text": "Is everyone understanding\nthe holding intuition, and the logic for why both\nof them have the [INAUDIBLE]??",
    "start": "2001230",
    "end": "2006549"
  },
  {
    "text": "Even if there's only a small\nprobability that we end up [INAUDIBLE]? ",
    "start": "2006550",
    "end": "2015092"
  },
  {
    "text": "So and this is what\nwe're going to repeat until the states are expanded. You can imagine that\nthe next time we",
    "start": "2015092",
    "end": "2021070"
  },
  {
    "text": "run our value\niteration, we're now running it on all of\nthese colored nodes-- both the blue and the red.",
    "start": "2021070",
    "end": "2027644"
  },
  {
    "text": "We can imagine\nthat next time, now that we have a little\nbit more information what lies beyond A and B, that\nour policy might say, oh,",
    "start": "2027644",
    "end": "2033476"
  },
  {
    "text": "you know what? Actually from S0 C, action\nA3 was the best to take.",
    "start": "2033477",
    "end": "2040350"
  },
  {
    "text": "What that does is say,\nOK, with a regional map, it's A, B, and C. And\nwe expanded those nodes.",
    "start": "2040350",
    "end": "2046230"
  },
  {
    "text": "We'll we've already\nexpanded A and B, so we move into the next\npart of the sub space, that as we gain\nmore information,",
    "start": "2046230",
    "end": "2052149"
  },
  {
    "text": "we run value iteration\nand we can expand on why.",
    "start": "2052150",
    "end": "2057788"
  },
  {
    "text": "Steve asked a good question. When we did our dry run,\nabout whether there is a way to save on the\ncomputation you did",
    "start": "2057789",
    "end": "2064059"
  },
  {
    "text": "prior to the value iteration and\nadd these [INAUDIBLE] states. And we've looked\nat it a little bit.",
    "start": "2064059",
    "end": "2069908"
  },
  {
    "text": "I've seen some stuff. But I haven't found a paper\nthat specifically deals with it.",
    "start": "2069909",
    "end": "2076608"
  },
  {
    "text": "You can imagine\nhow you've already run value iteration on your\nprevious iteration [INAUDIBLE]",
    "start": "2076609",
    "end": "2082450"
  },
  {
    "text": "and you add the new terminal\nedges which you'd expand on. And you run them again\nuntil it stabilizes.",
    "start": "2082450",
    "end": "2088100"
  },
  {
    "text": "And that way you've\nsaved the computation of having to run valuation\nmultiple times [INAUDIBLE]",
    "start": "2088100",
    "end": "2094446"
  },
  {
    "text": "state space. AUDIENCE: So I'm\ntrying to think of how this is different from\nsomething like [INAUDIBLE]",
    "start": "2094446",
    "end": "2101854"
  },
  {
    "text": "for [INAUDIBLE]. ",
    "start": "2101854",
    "end": "2107661"
  },
  {
    "text": "PROFESSOR 3: I think I don't\nknow enough about that. But my basic understanding\nsays that what's useful",
    "start": "2107662",
    "end": "2114770"
  },
  {
    "text": "here is it's this\nexplicit [INAUDIBLE].. I don't know how\nmuch [INAUDIBLE]..",
    "start": "2114770",
    "end": "2120713"
  },
  {
    "text": " AUDIENCE: And also, as long as\nyour heuristic is admissible,",
    "start": "2120714",
    "end": "2126080"
  },
  {
    "text": "it's guaranteed [INAUDIBLE]\nnot all [INAUDIBLE] algorithms.",
    "start": "2126080",
    "end": "2133910"
  },
  {
    "text": "Just like A star is optimal,\nas long as you've got a consistent [INAUDIBLE]. ",
    "start": "2133910",
    "end": "2143092"
  },
  {
    "text": "PROFESSOR 3: All right. So that's definitely\nthe idea here. We coupled these in\nthe only explored",
    "start": "2143092",
    "end": "2148680"
  },
  {
    "text": "of the portion of\nthe state space that we'll reach an\noptimal policy [INAUDIBLE]..",
    "start": "2148680",
    "end": "2155384"
  },
  {
    "start": "2155000",
    "end": "2305000"
  },
  {
    "text": "So we'll talk about quickly\nanother determination. We've touched on most of this.",
    "start": "2155384",
    "end": "2160770"
  },
  {
    "text": "So it's most likely\nwhen there are no more states to expand this\nwhen we've reached our goal.",
    "start": "2160770",
    "end": "2166340"
  },
  {
    "text": "It's when our policy that we\nrun on our entire envelope from value iteration\ndoesn't say that we should",
    "start": "2166340",
    "end": "2172110"
  },
  {
    "text": "go to anymore\nterminal [INAUDIBLE] that we haven't looked at\nyet, that we haven't seen yet.",
    "start": "2172110",
    "end": "2177484"
  },
  {
    "text": "We've said that those\nare only the things that are reachable and needed. Both, reachable because we're\nfollowing optimal policy,",
    "start": "2177484",
    "end": "2184550"
  },
  {
    "text": "and needed, only if\nwe might accidentally end up in the probabilistic.",
    "start": "2184550",
    "end": "2190030"
  },
  {
    "text": "This is gives us the sense of\nrigorous that we get policy on the entire state space that\nwe can end up in following",
    "start": "2190030",
    "end": "2197390"
  },
  {
    "text": "the optimal policy.  The third bullet\nhere touches on if we",
    "start": "2197390",
    "end": "2204924"
  },
  {
    "text": "don't expand the states that\nare probabilistically coupled and we do accidentally\nend up there, we risk getting lost\nand not having a policy.",
    "start": "2204924",
    "end": "2212840"
  },
  {
    "text": "We can compute this all off line\nand have a plan before we even start planning to\nknow exactly where we",
    "start": "2212840",
    "end": "2218456"
  },
  {
    "text": "want to go even if our\ndynamics aren't [INAUDIBLE]..",
    "start": "2218456",
    "end": "2226690"
  },
  {
    "text": "We're come back to this. This was our motivating example.",
    "start": "2226690",
    "end": "2231880"
  },
  {
    "text": "And so we show that\nthese real platforms can be modeled\nstochastically and then we can pretty easily\ndeal with that.",
    "start": "2231880",
    "end": "2238360"
  },
  {
    "text": "Search our state space and\ndeal with those probabilities and expand the nodes\nthat we might end up.",
    "start": "2238360",
    "end": "2244646"
  },
  {
    "text": "Right? And the heuristic\nallows us to not have to explore these\nareas of state space.",
    "start": "2244646",
    "end": "2249994"
  },
  {
    "text": "I never actually end up there. We'll always be a\ncommanding toward 77.",
    "start": "2249995",
    "end": "2255494"
  },
  {
    "text": "We're never going to\ntry to command backward. Sure, if there's a gust of wind\nand we have some probability there. But you can imagine that\nwe're going to only explore",
    "start": "2255495",
    "end": "2262349"
  },
  {
    "text": "a small portion of this. Because we'll always be\ntrying to correct to get back to the top four blocks.",
    "start": "2262350",
    "end": "2268590"
  },
  {
    "text": "And using our\nreward function, we get to determine if we\nwant to fly a quick path",
    "start": "2268590",
    "end": "2274515"
  },
  {
    "text": "or if we want to\nfly a safer path. For example, our time\ntimes our probability [INAUDIBLE] we want to\nperhaps reduce that.",
    "start": "2274515",
    "end": "2282172"
  },
  {
    "text": " All right.",
    "start": "2282172",
    "end": "2288004"
  },
  {
    "text": "Are there any questions\nabout planning with MDPs, anything like that.",
    "start": "2288004",
    "end": "2294570"
  },
  {
    "text": "I love this stuff. So the more questions,\nthe more I get to talk. Fine.",
    "start": "2294570",
    "end": "2300597"
  },
  {
    "text": "What I'm going to be\ntalking about for the rest of this lecture is\nextending beyond MDPs",
    "start": "2300597",
    "end": "2308079"
  },
  {
    "start": "2305000",
    "end": "2351000"
  },
  {
    "text": "to a broader class\nof problems called POMDPs, Partially Observable\nMarkov Decision Processes.",
    "start": "2308080",
    "end": "2315010"
  },
  {
    "text": "I love this stuff. I think it's really cool. They're really fun problems. We're going to talk about why\nthey're so much harder to plan",
    "start": "2315010",
    "end": "2321660"
  },
  {
    "text": "with, to execute--\nbut why they're important to at least know\nabout so that you can model",
    "start": "2321660",
    "end": "2327269"
  },
  {
    "text": "real world problems with them. And then we're going to\ndelve into a case study of a specific POMDP solver.",
    "start": "2327270",
    "end": "2335180"
  },
  {
    "text": "We're not going to go into as\nmuch detail as we did for MDPs, but we're going to look\nat what powerful results",
    "start": "2335180",
    "end": "2340630"
  },
  {
    "text": "we can get by\nplanning with POMDPs. ",
    "start": "2340630",
    "end": "2345986"
  },
  {
    "text": "PROFESSOR 4: So first,\nI want to rephrase this in the overall talk. Right?",
    "start": "2345986",
    "end": "2351270"
  },
  {
    "start": "2351000",
    "end": "2377000"
  },
  {
    "text": "We have this spectrum\nof uncertainty. And coupled with\nuncertainty is difficulty",
    "start": "2351270",
    "end": "2356420"
  },
  {
    "text": "of planning, of solving,\nof executing a problem. And we've killed\nthese first two cases.",
    "start": "2356420",
    "end": "2362170"
  },
  {
    "text": "That was really easy. And then we just discussed\nthe bottom [INAUDIBLE],, MDPs. What I'm going to\ntalk about is the case",
    "start": "2362170",
    "end": "2369535"
  },
  {
    "text": "where both your dynamics and\nyour sensors are stochastic. Why is that important?",
    "start": "2369535",
    "end": "2375440"
  },
  {
    "text": "It's because when\nwe first saw this slide-- our motivating\nexample slide, we only saw the left hand side.",
    "start": "2375440",
    "end": "2381990"
  },
  {
    "start": "2377000",
    "end": "2582000"
  },
  {
    "text": "We said, our actions\nare uncertain. But good news, we have\na perfect sensor-- a perfect camera.",
    "start": "2381990",
    "end": "2388079"
  },
  {
    "text": "But that's unrealistic. I think, we have\nall, to some extent, experienced the fact that no\nsensor is totally perfect.",
    "start": "2388080",
    "end": "2396280"
  },
  {
    "text": "Your camera might have\nfluctuated pixel values. Your laser range finder is\ngoing to never read out exactly",
    "start": "2396280",
    "end": "2403530"
  },
  {
    "text": "the right number all the time. You can have a camera in\ndifferent lighting conditions that will behave differently.",
    "start": "2403530",
    "end": "2409599"
  },
  {
    "text": "You might not be able to\nobserve your full state. That's, in a way, an\nimperfect sensor, right?",
    "start": "2409600",
    "end": "2414790"
  },
  {
    "text": "If I'm in this room,\nI have imperfect eyes. I can't map out\nall of MIT's campus because I'm blocked by walls.",
    "start": "2414790",
    "end": "2421473"
  },
  {
    "text": "How can you deal with the\nfact that you can't see all your obstacles all the time? We've already talked\nabout some cases--",
    "start": "2421473",
    "end": "2428740"
  },
  {
    "text": "that there are some\nalgorithms that can help us with that, like D Star Lite. But can you reason about these\nthings probabilistically?",
    "start": "2428740",
    "end": "2435846"
  },
  {
    "text": "And then finally, you might\nbe in a non-unique environment where you cannot resolve your\nstate with certainty no matter",
    "start": "2435846",
    "end": "2443430"
  },
  {
    "text": "how good your sensors are. Imagine you're in a building\nwith two identical hallways. You're dropped off\nin one of them.",
    "start": "2443430",
    "end": "2450359"
  },
  {
    "text": "How can you figure\nout where you are? You can't unless\nyou start exploring.",
    "start": "2450360",
    "end": "2455916"
  },
  {
    "text": "And so we've got to deal\nwith this uncertainty, right? it's Part of\nevery single problem.",
    "start": "2455916",
    "end": "2463760"
  },
  {
    "text": "When observational\nuncertainty is slowing, you can maybe ignore it. But it's there.",
    "start": "2463760",
    "end": "2469770"
  },
  {
    "text": "And so we're going to\nformulate this as a POMDP, a partially observable\nMarkov Decision Process.",
    "start": "2469770",
    "end": "2476150"
  },
  {
    "text": "And this next slide is\njust like the MDP slide. Hairy, but important. Right?",
    "start": "2476150",
    "end": "2482380"
  },
  {
    "text": "We can formulate a POMDP,\nwhich is seven elements. And MDP too five. Most of them for all those\nare carried over here.",
    "start": "2482380",
    "end": "2490655"
  },
  {
    "text": "We've got our set of\nstates where we can be. We've got a set of\nactions what we can do.",
    "start": "2490655",
    "end": "2495880"
  },
  {
    "text": "We've got our transition\nmodel which says, given that I\nstarted in one state and then I took\nan action, what's",
    "start": "2495880",
    "end": "2501810"
  },
  {
    "text": "the probability I end\nup somewhere else? And like David\nwas talking about, hopefully that distribution\nis pretty local--",
    "start": "2501810",
    "end": "2509220"
  },
  {
    "text": "we're not teleporting\nall over the world. We've got our reward function. This is exactly the\nsame as for MDPs.",
    "start": "2509220",
    "end": "2516240"
  },
  {
    "text": "And we've got our\ndiscount factor down here. The key difference of POMDP\nis these two elements.",
    "start": "2516240",
    "end": "2523070"
  },
  {
    "text": "We've got a set of\npossible observations and a probabilistic\nmodel for the probability",
    "start": "2523070",
    "end": "2528930"
  },
  {
    "text": "of making an observation given\nyour state and the action you just took.",
    "start": "2528930",
    "end": "2534800"
  },
  {
    "text": "Now it's important, I\nthink, it matches up really well with real\nworld sensors having",
    "start": "2534800",
    "end": "2540250"
  },
  {
    "text": "this probabilistic model. If you have a laser range\nfinder, for example, and you're standing one\nfoot away from the wall--",
    "start": "2540250",
    "end": "2546950"
  },
  {
    "text": "now a perfect sensor\nwould always say, you're one foot\naway from the wall. You're one foot\naway from the wall.",
    "start": "2546950",
    "end": "2553060"
  },
  {
    "text": "Every single\nreading is constant. But realistically, there might\nbe Gaussian noise, for example.",
    "start": "2553060",
    "end": "2558180"
  },
  {
    "text": "Or at a more extreme\ncase, it says, your one foot away\nfrom the wall. You're two feet. You're right there. There's this distribution.",
    "start": "2558180",
    "end": "2564765"
  },
  {
    "text": "And so you would ideally\ncharacterize this distribution. And you plug that\ninto this model",
    "start": "2564765",
    "end": "2570450"
  },
  {
    "text": "and that formulates your POMDP. This sounds really hairy, but if\nyou work through just a sample",
    "start": "2570450",
    "end": "2577799"
  },
  {
    "text": "iteration of living in a POMDP\nworld, that's not too bad. You start at some state,\nS. You take an action,",
    "start": "2577800",
    "end": "2584862"
  },
  {
    "start": "2582000",
    "end": "2892000"
  },
  {
    "text": "A. With some\nprobability, you're going to end up in a bunch of\ndifferent states based on your transition model.",
    "start": "2584863",
    "end": "2590710"
  },
  {
    "text": "At that point, we can use the\nlessons we learned from MDP land where we said, when\nwe make observations,",
    "start": "2590710",
    "end": "2596595"
  },
  {
    "text": "we reduce our uncertainty. we\ncollapse into a single state. So we say, let's\nmake an observation.",
    "start": "2596595",
    "end": "2603120"
  },
  {
    "text": "But this time, observations\naren't guaranteed to resolve all our uncertainty. So we make an observation.",
    "start": "2603120",
    "end": "2608400"
  },
  {
    "text": "And that observation\nis probabilistic based on our current state\nand the action we just took.",
    "start": "2608400",
    "end": "2615550"
  },
  {
    "text": "And again, obviously, it\ndepends on your current state. Because if you're one\nfoot away from a wall,",
    "start": "2615550",
    "end": "2620710"
  },
  {
    "text": "hopefully you'll get a\ndifferent characterization of observations than if you're\n20 feet away from the wall.",
    "start": "2620710",
    "end": "2626790"
  },
  {
    "text": "Otherwise, your sensor\nit's totally useless. Are there any questions\nabout this formulation?",
    "start": "2626790",
    "end": "2632777"
  },
  {
    "text": "AUDIENCE: Yep. Quick question. So we will take observation,\nany other observation and then you try to infer\nwhich state you're in,",
    "start": "2632777",
    "end": "2638940"
  },
  {
    "text": "is it just a clustering problem? For instance, the multi-cluster\nGaussian [INAUDIBLE] models.",
    "start": "2638940",
    "end": "2646140"
  },
  {
    "text": "So class A, class B,\nclass C, which are state, then taking an\nobservation there's a high probability\n[INAUDIBLE] This",
    "start": "2646140",
    "end": "2654362"
  },
  {
    "text": "is what we're tyring to do for\neach observation over here. So we're trying to find\nclustering [INAUDIBLE]..",
    "start": "2654362",
    "end": "2661300"
  },
  {
    "text": "PROFESSOR 4: So you\ncould, I imagine, implement an algorithm\nwhere, yeah, every time you make an observation, you\nthen try to say, all right.",
    "start": "2661300",
    "end": "2668460"
  },
  {
    "text": "What's my most likely estimate,\nor maybe my [INAUDIBLE] least cost estimate?",
    "start": "2668460",
    "end": "2673740"
  },
  {
    "text": "But inherent with\nthat is the risk that you're discarding a\nlot of information, right? Because you're going to\ngenerate a probability",
    "start": "2673740",
    "end": "2680724"
  },
  {
    "text": "distribution over your state. And so, yes, you\ncan say, I'm going",
    "start": "2680725",
    "end": "2686470"
  },
  {
    "text": "to stick with the maximum\nlikelihood estimate. But if you can,\nyou should probably try to maintain\nthat distribution",
    "start": "2686470",
    "end": "2691924"
  },
  {
    "text": "as long as possible. OK. And we'll see that\nthis is really",
    "start": "2691925",
    "end": "2697570"
  },
  {
    "text": "computationally\nexpensive unless you start making some assumptions. And in the case study\nwe're going to look into,",
    "start": "2697570",
    "end": "2704280"
  },
  {
    "text": "that's exactly what [INAUDIBLE]. But I've seen a lot\nin the literature that as much as\nyou can, you want",
    "start": "2704280",
    "end": "2711540"
  },
  {
    "text": "to maintain these distributions\nfor improved accuracy. Any other questions?",
    "start": "2711540",
    "end": "2716748"
  },
  {
    "text": " All right. Well, we're going to compare\nnow the execution of a POMDP",
    "start": "2716748",
    "end": "2724070"
  },
  {
    "text": "to the execution of an MDP. We started out-- we're living\nin the same real world.",
    "start": "2724070",
    "end": "2731480"
  },
  {
    "text": "We've got our same\ntransition model. Everything is peachy. We take action one, and\nwe want to go North.",
    "start": "2731480",
    "end": "2737089"
  },
  {
    "text": "We have this distribution\nthat we generate the states. And at this point,\nwhat did we say?",
    "start": "2737090",
    "end": "2742380"
  },
  {
    "text": "We said, we hate\nthe fact that we have to deal with three cases. Three is two too many. So let's make an observation\nto collapse this distribution.",
    "start": "2742380",
    "end": "2750520"
  },
  {
    "text": "Now I've described a\nlot about noisy sensors, right, where basically\nit's a true measurement",
    "start": "2750520",
    "end": "2755990"
  },
  {
    "text": "plus some noise, maybe\nGaussian distribution. There's another partially\nobservable sensor",
    "start": "2755990",
    "end": "2762360"
  },
  {
    "text": "you can have in the\nPOMDP which really feeds into the name,\nPartially Observable.",
    "start": "2762360",
    "end": "2767849"
  },
  {
    "text": "What if you can only\nobserve part of your state? For example, if you're\nliving in an x-y grid,",
    "start": "2767850",
    "end": "2774960"
  },
  {
    "text": "maybe you can only\nobserve your y dimension. This match up, in a\nreal world example,",
    "start": "2774960",
    "end": "2780550"
  },
  {
    "text": "to quadrotor flying\ndown a hallway. Catherine was working\non a DARPA project",
    "start": "2780550",
    "end": "2786032"
  },
  {
    "text": "with quadrotors\nflying down hallways. If the hallway is too long,\nyour laser range finder isn't going to be\nable to determine",
    "start": "2786032",
    "end": "2792339"
  },
  {
    "text": "where you are along one axis. But it can tell where\nyou are along another. And so that's what I've said.",
    "start": "2792340",
    "end": "2798420"
  },
  {
    "text": "I've said, pretend this\nquadrotor has that sensor. We can only observe\nits y component.",
    "start": "2798420",
    "end": "2803900"
  },
  {
    "text": "And it says, my\ny component is 3. I have no idea what\nmy x component is.",
    "start": "2803900",
    "end": "2809170"
  },
  {
    "text": "Well, this sucks. Right? Because we got\nrid of this state, but then we couldn't decide,\nare we at state (1,3) or (3,3)?",
    "start": "2809170",
    "end": "2818536"
  },
  {
    "text": "There's no way of\nresolving this. So we can re-normalize. We can add in the effect from\nthe observation probabilities",
    "start": "2818536",
    "end": "2825260"
  },
  {
    "text": "saying, maybe, in\nfact, I'm far more likely to observe a y\ncomponent of 3 if I'm at (1,3).",
    "start": "2825260",
    "end": "2832550"
  },
  {
    "text": "But in this case, we\nsay it's equally likely to make that observation\nfor those two states.",
    "start": "2832550",
    "end": "2838284"
  },
  {
    "text": "And so now we've got to\ndeal with these two cases. And so we can take\nour next action.",
    "start": "2838284",
    "end": "2843550"
  },
  {
    "text": "Instead of resetting\nto a single state, we've got to keep\ngrowing this tree.",
    "start": "2843550",
    "end": "2848751"
  },
  {
    "text": "And what's the key\ndifference between this and when we were\nexecuting our MDP? Except we didn't\nmanage to collapse back",
    "start": "2848751",
    "end": "2855670"
  },
  {
    "text": "to a single state. We didn't manage to\nreset the problem. And this is annoying.",
    "start": "2855670",
    "end": "2862630"
  },
  {
    "text": "Because you can't\nexecute a policy and say, I'm certain that this\nis my configuration. So your policy can't\nmap from exact states",
    "start": "2862630",
    "end": "2870385"
  },
  {
    "text": "to actions, because you\nnever know your exact state.",
    "start": "2870385",
    "end": "2875416"
  },
  {
    "text": "Does this make sense? Has everyone lost hope\nin planning, right? Yeah.",
    "start": "2875416",
    "end": "2881609"
  },
  {
    "text": "AUDIENCE: So in here because-- so from the left to\nthe right, you're",
    "start": "2881610",
    "end": "2886920"
  },
  {
    "text": "basically mapping from one\nbelief state to another belief state. So it's like a one arrow thing. But and then from\nthat second layer,",
    "start": "2886920",
    "end": "2895040"
  },
  {
    "text": "you should have two arrows. One with probably\n0.5 that observes 3,",
    "start": "2895040",
    "end": "2900800"
  },
  {
    "text": "and it gives rise to that\nbelief state it just showed. In one, with probability of 0.5\nwhere the sensor is being 4.",
    "start": "2900800",
    "end": "2907734"
  },
  {
    "text": "Because if you have\na 0 probability of looking at (2,4),\nyou might as well just observe 4 as well as your y.",
    "start": "2907734",
    "end": "2913160"
  },
  {
    "text": "So in this case, I'm not\nsure if you were just trying to show one branch\nof your POMDP planning,",
    "start": "2913160",
    "end": "2921204"
  },
  {
    "text": "but basically what\nyou have to do is you would go like\nthis to the second layer. You would have a branch with\na 0.5 probability on either.",
    "start": "2921205",
    "end": "2928990"
  },
  {
    "text": "One giving you 3,\none giving you 4. For the one that is showed,\nyou've got that relief state.",
    "start": "2928990",
    "end": "2934580"
  },
  {
    "text": "For the other one, you get the\nstate (2,4) with probably 1. PROFESSOR 4: Yeah.",
    "start": "2934580",
    "end": "2940220"
  },
  {
    "text": "So you were perfectly\ndescribing planning, right? I should have made\nthis more clear.",
    "start": "2940220",
    "end": "2946125"
  },
  {
    "text": "This isn't planning. This is executing. We have this policy that\nwe're going to execute. And so if we were\nplanning, we would",
    "start": "2946125",
    "end": "2952220"
  },
  {
    "text": "have to consider all these\nbranches and say, well, yeah.",
    "start": "2952220",
    "end": "2957320"
  },
  {
    "text": "There's a 50% chance here I'll\nend up here, in which case, my y value is going\nto read 4 and you have to grow this whole thing.",
    "start": "2957320",
    "end": "2962413"
  },
  {
    "text": "I'm saying, no. This is real time execution. yeah.",
    "start": "2962413",
    "end": "2967940"
  },
  {
    "text": "Great question. Any others? Well, this is a great time\nto transition to, well,",
    "start": "2967940",
    "end": "2975290"
  },
  {
    "text": "we can't just magically\nbe handed these policies. How do we actually\ngenerate them?",
    "start": "2975290",
    "end": "2980436"
  },
  {
    "text": "How do we start planning\nin the belief space? The belief space is\nthe space distributions",
    "start": "2980436",
    "end": "2986290"
  },
  {
    "text": "of possible configurations. So I'm going to talk about a\ngeneral class of algorithms.",
    "start": "2986290",
    "end": "2992870"
  },
  {
    "text": "A lot of planners in POMDP\nland and the belief space plan with probabilistic\nroadmaps- PRMs.",
    "start": "2992870",
    "end": "3001025"
  },
  {
    "text": "The goal is to generate a\npolicy that maps from a belief state to an action.",
    "start": "3001025",
    "end": "3007180"
  },
  {
    "text": "And I'm going to go\ninto a little more what a belief state is. But the general\nalgorithm in this graphic",
    "start": "3007180",
    "end": "3014799"
  },
  {
    "text": "illustrates these four steps. We're going to sample points\nfrom the configuration space,",
    "start": "3014800",
    "end": "3020570"
  },
  {
    "text": "as if everything\nwas deterministic. We're going to connect those\npoints to nearby points. And define nearby\nhowever you want.",
    "start": "3020570",
    "end": "3027260"
  },
  {
    "text": "It could be your closest\nneighbors, all neighbors within a radius, whatever. As long as those edges don't\ncollide with obstacles.",
    "start": "3027260",
    "end": "3035390"
  },
  {
    "text": "Once you've done that, somehow-- and there's some magic in this-- you're going to transform\nyour configured action space",
    "start": "3035390",
    "end": "3043190"
  },
  {
    "text": "probabilistic roadmap\nto a probabilistic road map in the belief state. Great.",
    "start": "3043190",
    "end": "3048210"
  },
  {
    "text": "Once you've done\nthat, you can just do shortest path depending on\nwhatever cost function you use. And what's really cool is you\nget different paths for when",
    "start": "3048210",
    "end": "3057560"
  },
  {
    "text": "you stay in the\nconfiguration space and when you go to\nthe belief space. So the green path, that\nbottom right figure,",
    "start": "3057560",
    "end": "3065900"
  },
  {
    "text": "seems a lot longer\nthan the red path. And the reason is the green path\nwas planned in the belief space",
    "start": "3065900",
    "end": "3072120"
  },
  {
    "text": "and followed a lot of landmarks\nthat the quadrotor could take measurements off of.",
    "start": "3072120",
    "end": "3077330"
  },
  {
    "text": "So it was really confident\nabout its position whereas the red path is the\nshorter path that had a higher",
    "start": "3077330",
    "end": "3082590"
  },
  {
    "text": "likelihood of a collision. And we're going to look\ninto that figure more later.",
    "start": "3082590",
    "end": "3089620"
  },
  {
    "text": "I'm going to segment this\nalgorithm into two parts. The first part-- these\nfirst two steps--",
    "start": "3089620",
    "end": "3094780"
  },
  {
    "text": "it's just probabilistic\nroad maps. Who here has heard about\nprobabilistic road maps?",
    "start": "3094780",
    "end": "3100360"
  },
  {
    "text": "Raise your hand. OK. 50/50. I'm so excited for the\n50% who haven't heard.",
    "start": "3100360",
    "end": "3106750"
  },
  {
    "text": "One of the top\nalgorithms, in my opinion. It's really simple, and\nit's really powerful. ",
    "start": "3106750",
    "end": "3113980"
  },
  {
    "text": "Here's basically almost\na complete implementation of probabilistic road maps.",
    "start": "3113980",
    "end": "3119180"
  },
  {
    "text": "It's pseudocode so don't copy\npaste, but it's almost there. You're going to\nconstruct a graph.",
    "start": "3119180",
    "end": "3125670"
  },
  {
    "text": "You're going to add\nyour start goal. Your start configuration\nbeing the green dot and then goal configuration, the red dot.",
    "start": "3125670",
    "end": "3131519"
  },
  {
    "text": "And then you're just going\nto keep sampling notes those from the free space. You say, how about (2,3)?",
    "start": "3131519",
    "end": "3137970"
  },
  {
    "text": "You're going to add\nthat to your graph. You're going to connect that\nnode to a bunch of other nodes nearby.",
    "start": "3137970",
    "end": "3143213"
  },
  {
    "text": "And then you're\njust going to keep sampling until maybe\nyou have enough nodes or maybe until you have\nyour complete path.",
    "start": "3143214",
    "end": "3149860"
  },
  {
    "text": "Should be happening there. And then once you've\ngot this whole graph,",
    "start": "3149860",
    "end": "3156262"
  },
  {
    "text": "you can just find the\nshortest path along that. And there are some\nreally cool results that if you sample in a good\nway, and so on, asymptotically.",
    "start": "3156262",
    "end": "3163960"
  },
  {
    "text": "As you start\nsampling more, you're going to asymptotically approach\nthe best path in a completely",
    "start": "3163960",
    "end": "3171560"
  },
  {
    "text": "continuous space. The power of\nprobabilistic road maps and a bunch of\nrandomized algorithms",
    "start": "3171560",
    "end": "3177800"
  },
  {
    "text": "though is that they scale\npretty well to high dimensions. So you don't need to actually\nconsider the continuous space.",
    "start": "3177800",
    "end": "3183890"
  },
  {
    "text": "You can just sample [INAUDIBLE]. Are there any questions about\nprobabilistic road maps?",
    "start": "3183890",
    "end": "3189801"
  },
  {
    "text": " Really cool. If you are interested, and\nyou just heard about PRMs.",
    "start": "3189802",
    "end": "3195670"
  },
  {
    "text": "You probably haven't\nheard about RRTs. Those are also really cool.",
    "start": "3195670",
    "end": "3200957"
  },
  {
    "text": "AUDIENCE: Just a quick\n[INAUDIBLE] question. So for any node, [INAUDIBLE]\nat this uniform example",
    "start": "3200957",
    "end": "3206050"
  },
  {
    "text": "[INAUDIBLE].  PROFESSOR 4: Sorry.",
    "start": "3206050",
    "end": "3211220"
  },
  {
    "text": "When you're choosing\nwhere to place the dot? Or what to connect to? AUDIENCE: [INAUDIBLE]\nthere's a dot there on the side [INAUDIBLE] right?",
    "start": "3211220",
    "end": "3217400"
  },
  {
    "text": "So when I do the sampling, I\njust [INAUDIBLE] this node. I uniformly choose one\nof them [INAUDIBLE]..",
    "start": "3217400",
    "end": "3222420"
  },
  {
    "text": " PROFESSOR 4: So in general,\nprobabilistic roadmaps,",
    "start": "3222420",
    "end": "3229290"
  },
  {
    "text": "you can throw in whatever\nsampler you want. The way this particular one-- the way I implement this is\nyou sample points uniformly",
    "start": "3229290",
    "end": "3237400"
  },
  {
    "text": "from the entire space. If it's inside an obstacle\nand you remove it, once you place it--",
    "start": "3237400",
    "end": "3244089"
  },
  {
    "text": "this was connect\nto the K closest-- I think K is 7 in this case.",
    "start": "3244090",
    "end": "3250750"
  },
  {
    "text": "And if there's an edge\nthat if that edge collides with an obstacle, remove it.",
    "start": "3250750",
    "end": "3256520"
  },
  {
    "text": "I'd be happy to go into more\ndetail for that PRMs later. All right.",
    "start": "3256520",
    "end": "3261740"
  },
  {
    "text": "But that's not enough. Because what we described\nas PRMs in the configuration",
    "start": "3261740",
    "end": "3267680"
  },
  {
    "start": "3266000",
    "end": "3339000"
  },
  {
    "text": "space, but what we\nneed to do is somehow elevate a PRM from the\nconfiguration space to a belief space.",
    "start": "3267680",
    "end": "3274675"
  },
  {
    "text": "And this is really hard. We don't have access to\nthese raw configurations.",
    "start": "3274675",
    "end": "3280570"
  },
  {
    "text": "Let's imagine we were in\nthis really simple world where the quadrotor could\nbe in three possible states.",
    "start": "3280570",
    "end": "3286120"
  },
  {
    "text": "One, two, three. Really easy, right? Sample a bunch of points. They're going to end up\nin one, two, or three.",
    "start": "3286120",
    "end": "3292150"
  },
  {
    "text": "You could pretty quickly\ncover the entire space. But this simple\nconfiguration space",
    "start": "3292150",
    "end": "3297790"
  },
  {
    "text": "will transform to the belief\nspace becomes infinite. You have infinite possible\ndistributions to consider.",
    "start": "3297790",
    "end": "3305495"
  },
  {
    "text": "There is the\ndistribution where you have 100% chance probability-- 100% probability that you're\nin state 1, 100% probability",
    "start": "3305495",
    "end": "3313902"
  },
  {
    "text": "that you're in state 2. 100% probability that\nyou're in state 3. And then everything in between.",
    "start": "3313902",
    "end": "3320100"
  },
  {
    "text": "We went from three to infinite. This is not boding well. And even if you start\nsaying, well, I'm",
    "start": "3320100",
    "end": "3326132"
  },
  {
    "text": "not going to consider\nthe whole distribution. I just care about the\nmean and the variance, it's still not pretty, right?",
    "start": "3326132",
    "end": "3334850"
  },
  {
    "text": "Well, this is where we have\nto start making approximations",
    "start": "3334850",
    "end": "3342455"
  },
  {
    "start": "3339000",
    "end": "3437000"
  },
  {
    "text": "And this is where you\nstart getting differences in POMDP planners-- where\nthey make assumptions,",
    "start": "3342455",
    "end": "3348474"
  },
  {
    "text": "where they make approximations. So these images are\nfrom the belief road map",
    "start": "3348475",
    "end": "3355170"
  },
  {
    "text": "paper from the Robust Robotics\ngroup a couple of years ago. But I'm going to talk about\na different planner soon.",
    "start": "3355170",
    "end": "3361790"
  },
  {
    "text": "But the idea behind a\nlot of these planners is maybe we can start saying\nwhat these distributions are",
    "start": "3361790",
    "end": "3369680"
  },
  {
    "text": "going to look like\nbased on our models. So to our planning problem,\nif we know we started at (2,3)",
    "start": "3369680",
    "end": "3376650"
  },
  {
    "text": "and we know our\ntransition distribution, we can start saying, well, this\nis my probability distribution.",
    "start": "3376650",
    "end": "3382729"
  },
  {
    "text": "And then when I make\nand observation, I can build\ndistributions off that. And so, if you\ncould exhaustively",
    "start": "3382729",
    "end": "3391000"
  },
  {
    "text": "propagate these distributions\nforward, that would be great. But it's unrealistic.",
    "start": "3391000",
    "end": "3396180"
  },
  {
    "text": "And I just want to point\nin terms of the visual way",
    "start": "3396180",
    "end": "3401440"
  },
  {
    "text": "to represent these\ndistributions. A really nice way of saying,\nin the deterministic world, you have these dots and edges.",
    "start": "3401440",
    "end": "3407950"
  },
  {
    "text": "In the probabilistic\nworld, these circles, these ellipsoids,\nrepresent uncertainty.",
    "start": "3407950",
    "end": "3414380"
  },
  {
    "text": "Typically, it's the one\nstandard deviation or three standard deviations away. And so you can start\nbuilding into the map,",
    "start": "3414380",
    "end": "3421976"
  },
  {
    "text": "these are the distributions\nand the variances I can see in these nodes. Are there any questions\nabout this stuff.",
    "start": "3421976",
    "end": "3429742"
  },
  {
    "text": " All right. Well, we're going to delve now\ninto a specific case study.",
    "start": "3429742",
    "end": "3437680"
  },
  {
    "text": "Feedback Based Information\nState Roadmaps-- FIRM. From now on, that's the only\nway I'm going to refer to it.",
    "start": "3437680",
    "end": "3443740"
  },
  {
    "text": " The idea behind\nthis is you're going to sample mean configurations\nfrom your configuration space.",
    "start": "3443740",
    "end": "3453260"
  },
  {
    "text": "Then you want to\nbuild an LQR-- that's",
    "start": "3453260",
    "end": "3458355"
  },
  {
    "text": "Linear Quadratic\nRegulator controller-- around these mean points.",
    "start": "3458355",
    "end": "3464910"
  },
  {
    "text": "And that will generate what\nvariance you can tolerate. So LQR controllers, if you\ndon't know, they're really nice.",
    "start": "3464910",
    "end": "3473470"
  },
  {
    "text": "Around a small region\naround a point, they can drive a quadrotor,\nfor example, to that figure.",
    "start": "3473470",
    "end": "3480243"
  },
  {
    "text": "And so if you build these LQR\ncontrollers around points, you can say, all\nright, anytime I end up",
    "start": "3480243",
    "end": "3486119"
  },
  {
    "text": "in this cloud in\nthe belief space-- so any sort of distribution and\ncan bring it back to that mean.",
    "start": "3486120",
    "end": "3494810"
  },
  {
    "text": "And so now, what we've done is\nwe've generated every point, we just need to connect them. And the idea is if you\nhave a feedback based,",
    "start": "3494810",
    "end": "3504050"
  },
  {
    "text": "they can get from one cloud to\nanother anywhere in the clouds. Then you can get\nfrom point to point.",
    "start": "3504050",
    "end": "3511760"
  },
  {
    "text": "All right. This is how you\ngenerate the graph. And what's cool is the\nway they formulated the problem is they\nsaid, well, set up",
    "start": "3511760",
    "end": "3519359"
  },
  {
    "text": "the cost of executing an edge. We switched from\nrewards to costs because we're pessimists now.",
    "start": "3519360",
    "end": "3526700"
  },
  {
    "text": "Well, the cost is going\nto be a linear combination of the expected time to execute\nthat edge and the uncertainty",
    "start": "3526700",
    "end": "3533610"
  },
  {
    "text": "along that edge. Now, this is actually\nreally cool to play with. What do you think would happen\nif I set beta to 0 in this?",
    "start": "3533610",
    "end": "3544475"
  },
  {
    "text": "Like, what sort of\n[INAUDIBLE] would you get? I will cold call because\nI know a few names here.",
    "start": "3544475",
    "end": "3549870"
  },
  {
    "text": " Anybody? I don't want to cold call\nsomeone who may [INAUDIBLE]..",
    "start": "3549870",
    "end": "3557181"
  },
  {
    "text": "AUDIENCE: You're going\nto get the shortest path. PROFESSOR 4: Shortest\npath, that's right, right? This turn goes away.",
    "start": "3557181",
    "end": "3562599"
  },
  {
    "text": "The cost is a\nfunction of the time-- shortest path. Now, what's cool is one\nday I was messing around",
    "start": "3562600",
    "end": "3569220"
  },
  {
    "text": "with this code. I'm like, I wonder what happens\nif I set alpha to 0, right? So your cost is purely a\nfunction of uncertainty.",
    "start": "3569220",
    "end": "3577180"
  },
  {
    "text": "In turns out what the\nquadrotor does it just hangs out where it starts. It says, I'm in no hurry.",
    "start": "3577180",
    "end": "3584400"
  },
  {
    "text": "I know where I am. I'm just going to stay here.  But I find this amazing, right?",
    "start": "3584400",
    "end": "3591212"
  },
  {
    "text": "Because this almost\nmodels behavior even. You could start\nsaying, do I want to be a risky quadrotor\nor a safe one?",
    "start": "3591212",
    "end": "3596990"
  },
  {
    "text": "Like, how important it is for\nme to get somewhere on time or be safe? And it's just those\ntwo parameters.",
    "start": "3596990",
    "end": "3602940"
  },
  {
    "text": "Or you could even make it 1,\nlike alpha and 1 minus alpha. I think this stuff\nis really cool.",
    "start": "3602940",
    "end": "3610170"
  },
  {
    "text": "The one detail that I'm\nreally going to into for FIRM is the cost equation, which\nis based on the Bellman backup",
    "start": "3610170",
    "end": "3615900"
  },
  {
    "text": "equation that we had. The cost to go, right, the\nexpected cost from a belief",
    "start": "3615900",
    "end": "3621560"
  },
  {
    "text": "state [INAUDIBLE] is-- well, you're going to\ntake the best action.",
    "start": "3621560",
    "end": "3627146"
  },
  {
    "text": "So you're going to take the\nmide of this whole thing. You're going to say, it's\nthe cost of executing",
    "start": "3627146",
    "end": "3633400"
  },
  {
    "text": "a specific action plus the cost\nof colliding with something,",
    "start": "3633400",
    "end": "3638569"
  },
  {
    "text": "an obstacle, times the\nprobability of colliding, right? That's this term. And then you've got to\nsay, well, OK, and then",
    "start": "3638570",
    "end": "3647240"
  },
  {
    "text": "once I reach a state,\nwhat's the cost from there? And then I could weight\nthat by the probability of ending up in that state.",
    "start": "3647240",
    "end": "3653970"
  },
  {
    "text": "Does this equation\nmake sense to people? There are a lot of symbols,\nand honestly, I hate notation,",
    "start": "3653970",
    "end": "3661390"
  },
  {
    "text": "but it works. You just plug in--\nthe cost is I'm going to take the best action. It's going to be\nthe cost of using",
    "start": "3661390",
    "end": "3667326"
  },
  {
    "text": "that action, the cost\nof colliding times the probability of colliding\ngiven that I used that action",
    "start": "3667326",
    "end": "3672539"
  },
  {
    "text": "and I started where\nI started, and then the cost from where I end up.",
    "start": "3672540",
    "end": "3677980"
  },
  {
    "text": "And since you end up in a\nprobability distribution, we need to consider\nall these cases.",
    "start": "3677980",
    "end": "3683300"
  },
  {
    "text": "Yeah? AUDIENCE: When you define\nlike an action from one place to another, do you\nalways think of it",
    "start": "3683300",
    "end": "3688445"
  },
  {
    "text": "as starting from\nmean that you sampled or from anywhere [INAUDIBLE]? PROFESSOR 4: So it's from the--",
    "start": "3688445",
    "end": "3695520"
  },
  {
    "text": "so formally, it was\nfrom the belief state, which is the mean, yeah, plus\nthe variance once it stabilized",
    "start": "3695520",
    "end": "3702700"
  },
  {
    "text": "to that point. And the way that variance is\ngenerated, I should have said, is, you're going to have\nmodels of these quadrotors.",
    "start": "3702700",
    "end": "3710000"
  },
  {
    "text": "And so I spent a good time\nin the ACL with John Howell,",
    "start": "3710000",
    "end": "3715695"
  },
  {
    "text": "in Course 16. And you just like let\nthe quadrotor hover. You measure its position\nfor a long time.",
    "start": "3715695",
    "end": "3721569"
  },
  {
    "text": "And you get a distribution\nover where it goes.",
    "start": "3721570",
    "end": "3726848"
  },
  {
    "text": "AUDIENCE: So what does\nthe letter M stand for? PROFESSOR 4: What\nis the letter M? AUDIENCE: Yeah. PROFESSOR 4: Right. So you're summing over-- these\nare the belief states that you",
    "start": "3726848",
    "end": "3734260"
  },
  {
    "text": "could end up in, right? So if everything\nwere deterministic, this would just\nbe ignore the sum.",
    "start": "3734260",
    "end": "3740280"
  },
  {
    "text": "It's where you end up. Realistically you could\nend up in some other state",
    "start": "3740280",
    "end": "3745386"
  },
  {
    "text": "we haven't considered. AUDIENCE: So just\na quick question. So those, if we're operating\non a Gaussian space,",
    "start": "3745386",
    "end": "3752090"
  },
  {
    "text": "then you have\nGaussian observations. So those are sums over the\nobservation samples that are generated when [INAUDIBLE].",
    "start": "3752090",
    "end": "3759030"
  },
  {
    "text": "So that is a finite sum over the\npossible infinite observation states we might have.",
    "start": "3759030",
    "end": "3764270"
  },
  {
    "text": "PROFESSOR 4: Yeah. So there's definitely\n[INAUDIBLE] in terms of where\nyou could end up.",
    "start": "3764270",
    "end": "3770720"
  },
  {
    "text": "And even the action\nspace, there's a set feedback controller\nthat you're allowed.",
    "start": "3770720",
    "end": "3776440"
  },
  {
    "text": "I think the observation, if\nyou modeled it as a Gaussian, if you made some\nnice assumptions,",
    "start": "3776440",
    "end": "3781640"
  },
  {
    "text": "it can be tractable\nas continuous. AUDIENCE: Oh, yeah. I was [INAUDIBLE]. So for the start, so if you\nstart with Gaussian noise,",
    "start": "3781640",
    "end": "3788410"
  },
  {
    "text": "then you have a linear model. Then your prediction's\ngoing to be Gaussian. PROFESSOR 4: Yeah.",
    "start": "3788410",
    "end": "3793746"
  },
  {
    "text": "AUDIENCE: But then you\nhave Gaussian observations, which is great because then\nyour update is Gaussian but the observation\nspace is infinite.",
    "start": "3793747",
    "end": "3801140"
  },
  {
    "text": "So basically not only you\nhave two sample positions, but you also have two sample\npotential observations",
    "start": "3801140",
    "end": "3806655"
  },
  {
    "text": "that you might get\nas you go along.  PROFESSOR 4: Yeah. AUDIENCE: So that\nyou can basically-- and it's great.",
    "start": "3806655",
    "end": "3812410"
  },
  {
    "text": "But you end up basically\nreducing a possibly infinite branching, which is your\nGaussian to kind of a",
    "start": "3812410",
    "end": "3819330"
  },
  {
    "text": "like a Monte Carlo Tree search. You generate a whole bunch\nof meaningful samples, and those are the ones\nthat you consider.",
    "start": "3819330",
    "end": "3825480"
  },
  {
    "text": "So is that where the\nsum is coming from? PROFESSOR 4: Yeah, basically. And a fun fact, the theorem\nthat I read and trusted",
    "start": "3825480",
    "end": "3832686"
  },
  {
    "text": "was that if you\njust sample randomly right from your\nconfiguration space, you have zero probability\nof constructing",
    "start": "3832686",
    "end": "3840130"
  },
  {
    "text": "a graph that without any\nassumptions will be connected. Whereas for PRMs,\nyou sample enough",
    "start": "3840130",
    "end": "3845350"
  },
  {
    "text": "and things will turn out nicely,\nnot the case with the belief state. That's why we need to\nmake these assumptions.",
    "start": "3845350",
    "end": "3851728"
  },
  {
    "text": "We're killing it. We know POMDPs. Now we get to look at some\nreally fun graphics generated",
    "start": "3851728",
    "end": "3857030"
  },
  {
    "text": "from real flights using FIRM. ",
    "start": "3857030",
    "end": "3864090"
  },
  {
    "text": "The big takeaway is that\nFIRM prefers safer paths. We've got two images\nthat look really similar.",
    "start": "3864090",
    "end": "3870120"
  },
  {
    "text": "We're going to talk\nabout one and then show why they're slightly different. The test flight that we put this\nquadrotor under was we said,",
    "start": "3870120",
    "end": "3878289"
  },
  {
    "text": "we're going to start\nat this configuration. We're going to go\nthis configuration. And there's this\nbig, blue obstacle.",
    "start": "3878289",
    "end": "3885869"
  },
  {
    "text": "And there are landmarks that you\ncan take measurements off of. So those are these red dots.",
    "start": "3885870",
    "end": "3891060"
  },
  {
    "text": "We want to compare two planners\nright, a PRM and a FIRM planner.",
    "start": "3891060",
    "end": "3896270"
  },
  {
    "text": "The PRM planner said, right,\nI just want to minimize time. And so it found the\nfirst path is stay",
    "start": "3896270",
    "end": "3901710"
  },
  {
    "text": "to the left of the obstacle. But that's actually\na really narrow path between the obstacle\nand the wall.",
    "start": "3901710",
    "end": "3908250"
  },
  {
    "text": "On the other hand,\nthe FIRM planner said, right, I want to minimize\nthe linear combination of time",
    "start": "3908250",
    "end": "3913890"
  },
  {
    "text": "and uncertainty. And so if you ramp\nup the term the wait for uncertainty, at some\npoint the path sort of",
    "start": "3913890",
    "end": "3920460"
  },
  {
    "text": "pops over the obstacle. It's really cool. It snaps. And then you get\nthis safer path.",
    "start": "3920460",
    "end": "3925500"
  },
  {
    "text": "But that's longer. And how do we know it's safer? Because these ellipsoids\nthat we've drawn",
    "start": "3925500",
    "end": "3931890"
  },
  {
    "text": "are the 3D versions of what\nwe saw for PRM elevated to the belief space version.",
    "start": "3931890",
    "end": "3937280"
  },
  {
    "text": "It's the uncertainty\nthat the quadrotor has over its true state.",
    "start": "3937280",
    "end": "3942350"
  },
  {
    "text": "Why is the PRM plan-- why does it have\nsuch big ellipsoids? It's because when it's\nbehind the obstacle,",
    "start": "3942350",
    "end": "3948089"
  },
  {
    "text": "it can't make any of\nthese measurements because it can't\nsee the landmarks. So it's basically\nusing dead reckoning.",
    "start": "3948090",
    "end": "3955490"
  },
  {
    "text": "And the transition model,\nright, is not deterministic. So it's uncertainty grows.",
    "start": "3955490",
    "end": "3961045"
  },
  {
    "text": "And so we can see these\nellipsoids are bigger for PRM than FIRM. And then the reason\nI have two images",
    "start": "3961045",
    "end": "3966450"
  },
  {
    "text": "is they are slightly different. It's that these landmarks\nwere fictitious landmarks",
    "start": "3966450",
    "end": "3972220"
  },
  {
    "text": "that we just said, you can-- and we would generate fake\nmeasurements off of them. And so we could tune the\nnoise of the landmarks.",
    "start": "3972220",
    "end": "3980276"
  },
  {
    "text": "Maybe a little bit\nof cheating, but it allowed us to say, would it\nincrease the noise from some--",
    "start": "3980277",
    "end": "3985350"
  },
  {
    "text": "the dimension was\nnumber 0.05 to 0.15. You can see the uncertainty\nellipsoids from PRM",
    "start": "3985350",
    "end": "3992554"
  },
  {
    "text": "grow when we increased the\nnoise, whereas for FIRM, they stay about the same.",
    "start": "3992554",
    "end": "3998110"
  },
  {
    "text": "And importantly,\nthese ellipsoids grow enough that they start\noverlapping with the obstacles.",
    "start": "3998110",
    "end": "4003140"
  },
  {
    "text": "That represents a very high\nprobability of collision. Whereas the FIRM,\nthe way it managed",
    "start": "4003140",
    "end": "4008495"
  },
  {
    "text": "to keep these ellipsoids\nso small is we kept the cost function the\nsame, right-- the same weight",
    "start": "4008495",
    "end": "4015800"
  },
  {
    "text": "on uncertainty, same\nweight on the time, right? But the uncertainty\nwas so much higher. And so we decided, well, I can\nsacrifice a little bit of time.",
    "start": "4015800",
    "end": "4024194"
  },
  {
    "text": "I can take the slower path. I can just hang out\nby landmark, really make sure I know where\nI am before continuing.",
    "start": "4024195",
    "end": "4030920"
  },
  {
    "text": "And so the path-- the\nduration of the flight took a lot longer\nfor FIRM as time would increase but\nthe uncertainty would",
    "start": "4030920",
    "end": "4037020"
  },
  {
    "text": "stay about constant. Do people understand\nthese graphics?",
    "start": "4037020",
    "end": "4043168"
  },
  {
    "text": "Great. All right. We've got a graph\nthat just represents",
    "start": "4043168",
    "end": "4048549"
  },
  {
    "text": "a little more formally\nthat growing uncertainly. As noise increases, the variance\nalong a single dimension, z,",
    "start": "4048550",
    "end": "4055640"
  },
  {
    "text": "y, to x, for PRM, which\nis that, and FIRM.",
    "start": "4055640",
    "end": "4061809"
  },
  {
    "text": "The variance for PRM is\nalways higher, and it grows. For FIRM, it's lower,\nand stays about constant.",
    "start": "4061810",
    "end": "4068630"
  },
  {
    "text": "That's the big takeaway. FIRM minimizes this uncertainty.",
    "start": "4068630",
    "end": "4074440"
  },
  {
    "text": "And then the final image\nfrom these results that I want to show is in simulation.",
    "start": "4074440",
    "end": "4080510"
  },
  {
    "text": "They said, well, let's actually\nmeasure how often it crashes. We didn't want to do\nthis in the real world because we don't want to crash\nthe quadrotor that many times.",
    "start": "4080510",
    "end": "4088790"
  },
  {
    "text": "The gist of it is comparing\na reactive planner and a deterministic\none [INAUDIBLE]..",
    "start": "4088790",
    "end": "4094670"
  },
  {
    "text": "As noise increases-- noise was\nsimulated with wind strength-- the number of crashes\nincreases for PRM, basically.",
    "start": "4094670",
    "end": "4101739"
  },
  {
    "text": "And for FIRM, it stays\nconstant and low. The reason there\nare two lines is",
    "start": "4101740",
    "end": "4107540"
  },
  {
    "text": "there were two planners with\ndifferent time horizons. The important thing is\nFIRM is low and constant.",
    "start": "4107540",
    "end": "4113716"
  },
  {
    "text": "PRM grows. ",
    "start": "4113717",
    "end": "4120680"
  },
  {
    "text": "We've talked now throughout all\nprobabilistic planning you ever need to know, right?",
    "start": "4120680",
    "end": "4126023"
  },
  {
    "text": "No, not quite. But we have covered\na lot of stuff. What are the big takeaways?",
    "start": "4126023",
    "end": "4131170"
  },
  {
    "text": "We've learned that real-world\nproblems are stochastic, right? Quadrotors are not\nthese perfect machines",
    "start": "4131170",
    "end": "4137396"
  },
  {
    "text": "that we wish they were. But it's important to\nmodel them as stochastic. The problem is once you start\nmodeling them as stochastic,",
    "start": "4137396",
    "end": "4144430"
  },
  {
    "text": "it becomes a lot\nharder to solve. But if you make\nsome assumptions, or even if you don't,\nif you're just get smart",
    "start": "4144430",
    "end": "4151068"
  },
  {
    "text": "and you do the heuristics, you\ncan resolve this complexity. And so I hope you remember\nthese three points,",
    "start": "4151069",
    "end": "4157324"
  },
  {
    "text": "you remember this graphic\nor that graphic, the idea that if you take\nuncertainty into account,",
    "start": "4157324",
    "end": "4162399"
  },
  {
    "text": "you get fundamentally\ndifferent paths [INAUDIBLE].. And that can be a good thing.",
    "start": "4162399",
    "end": "4168770"
  },
  {
    "text": "What questions do\nyou have, anything? Yeah? AUDIENCE: [INAUDIBLE]\nso far people have [INAUDIBLE] problems.",
    "start": "4168770",
    "end": "4176278"
  },
  {
    "text": "So how do the same,\nyou know, [INAUDIBLE]?? So for instance, we suggest\nthat this is the maximum risk",
    "start": "4176279",
    "end": "4183028"
  },
  {
    "text": "we want to take. Is it possible to integrate\neach of our constraints into your optimization\nproblem and solve it?",
    "start": "4183029",
    "end": "4189028"
  },
  {
    "text": "Or [INAUDIBLE]? PROFESSOR 4: So I imagine one\nthing that I would like to test",
    "start": "4189029",
    "end": "4196800"
  },
  {
    "text": "is if we go back to this really\ncrude version of our cost",
    "start": "4196800",
    "end": "4205165"
  },
  {
    "text": "equation, we can imagine\nsaying that if we want to come up with a\nbound for uncertainty",
    "start": "4205165",
    "end": "4210590"
  },
  {
    "text": "that we can tolerate,\nyou could maybe like setting an intercept\nfor this to be that bound and then just ramping this up.",
    "start": "4210590",
    "end": "4217650"
  },
  {
    "text": "AUDIENCE: Oh, [INAUDIBLE]\nmultiplier [INAUDIBLE].. PROFESSOR 4:\nSomething like that.",
    "start": "4217650",
    "end": "4222680"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. GUEST SPEAKER: Yeah, exactly. So it only comes into effect. Possibly what I said\nis a terrible idea.",
    "start": "4222680",
    "end": "4229170"
  },
  {
    "text": "But it's-- especially\nin simulation, you can just try it out\nand see if it works. AUDIENCE: And just\nanother question,",
    "start": "4229170",
    "end": "4235830"
  },
  {
    "text": "you said propagated probability\nsolution on the networks is hard.",
    "start": "4235830",
    "end": "4241040"
  },
  {
    "text": "[INAUDIBLE]  So therefore we can assume\nan illustrated area, and then",
    "start": "4241040",
    "end": "4249435"
  },
  {
    "text": "[INAUDIBLE] normal\ndistribution, they are contrary to each other. Therefore you can\n[INAUDIBLE] observation",
    "start": "4249435",
    "end": "4256500"
  },
  {
    "text": "and then updating [INAUDIBLE]\nand form a distribution, right? PROFESSOR 4: Sure.",
    "start": "4256500",
    "end": "4261610"
  },
  {
    "text": "So I think that might be making\nsome assumptions that we might",
    "start": "4261610",
    "end": "4268080"
  },
  {
    "text": "not be willing to make about\nthe nature of the distributions that you're trying to propagate. I need think about\nthis some more.",
    "start": "4268080",
    "end": "4275690"
  },
  {
    "text": "But I think intuitively, we can\nunderstand that any time you're propagating a distribution\nversus a single discrete value,",
    "start": "4275690",
    "end": "4282580"
  },
  {
    "text": "it's definitely not\ngoing to be easier. And so as your distributions\nbecome more complex.",
    "start": "4282580",
    "end": "4291120"
  },
  {
    "text": "Perhaps if you're modeling a\nreal-world stochastic sensor, you might not be able to perform\nthese efficient updates using",
    "start": "4291120",
    "end": "4298119"
  },
  {
    "text": "conjugate priority.  Any other questions?",
    "start": "4298120",
    "end": "4304140"
  },
  {
    "text": "Otherwise we-- yeah? AUDIENCE: So all these FIRM\nexamples you're showing, this is all planning\ndone offline.",
    "start": "4304140",
    "end": "4309690"
  },
  {
    "text": "[INAUDIBLE] offline, right? Have you expanded this at\nall to like an online case?",
    "start": "4309690",
    "end": "4315359"
  },
  {
    "text": "Or does that all\nrequire re-planning? And how long does it take? PROFESSOR 4: Right. So it's not good, I\ncan tell you that much.",
    "start": "4315359",
    "end": "4322530"
  },
  {
    "text": "What's nice is FIRM\ngenerates a policy. So if you construct\nyour PRM to say",
    "start": "4322530",
    "end": "4328600"
  },
  {
    "text": "don't just find\nthe shortest path, keep sampling\npoints until you're confident that\nyou're always going",
    "start": "4328600",
    "end": "4334445"
  },
  {
    "text": "to end up near some point. You can construct\na policy and then just online look up\nwhat's my belief that",
    "start": "4334445",
    "end": "4341380"
  },
  {
    "text": "matches most closely.  It took for-- I think we\nsampled 600 nodes in like--",
    "start": "4341380",
    "end": "4351015"
  },
  {
    "text": "what are the dimensions on this? So 2 to 3 meter by\n8 by 3 meter thing--",
    "start": "4351016",
    "end": "4358880"
  },
  {
    "text": "it took about 15 minutes. It was really slow. Now, granted it was\na virtual machine. But it's not something we\nwant to re-plan on the fly.",
    "start": "4358880",
    "end": "4367600"
  },
  {
    "text": "With PRM, typically\na lot faster. AUDIENCE: Who was a lot\nfaster in that case?",
    "start": "4367600",
    "end": "4372730"
  },
  {
    "text": "PROFESSOR 4: So when\nwe just did PRM, I think it was one minute\nor something, at least",
    "start": "4372730",
    "end": "4378735"
  },
  {
    "text": "an order of magnitude. And you could use other\nplanners as well, like RRTs.",
    "start": "4378735",
    "end": "4384390"
  },
  {
    "text": "And there are RRT versions of-- there are POMDT\nversions of RRTs.",
    "start": "4384390",
    "end": "4389596"
  },
  {
    "text": "But [INAUDIBLE]. All right, I'm going to\nturn it over to Steve so we can learn about this project.",
    "start": "4389596",
    "end": "4396387"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "4396387",
    "end": "4433860"
  },
  {
    "text": "STEVE: So I'm just going\nto say a few good words about the Grand Challenge. So again, the final\nassignment will",
    "start": "4433860",
    "end": "4439275"
  },
  {
    "text": "be released tonight for this. And as Professor\nWarrens mentioned, it's going to be descoped a\nbit from what we originally",
    "start": "4439275",
    "end": "4445740"
  },
  {
    "text": "had in the syllabus due\nto time constraints. But here's a preview of\nwhat you'll be doing.",
    "start": "4445740",
    "end": "4450880"
  },
  {
    "text": "So for an overview, it will\nbe a class-wide collaboration. So what we're going\nto do, you guys",
    "start": "4450880",
    "end": "4456969"
  },
  {
    "text": "are going to stay in your\nadvanced lecture teams and each basically\napply what you've done, the great work\nyou've done on that,",
    "start": "4456970",
    "end": "4463190"
  },
  {
    "text": "onto our hardwork robot\nthat we have. so it's not a competition where\neach team will be",
    "start": "4463190",
    "end": "4468414"
  },
  {
    "text": "competing against each other. And so again,\nwe're descoping it.",
    "start": "4468414",
    "end": "4473970"
  },
  {
    "text": "The syllabus originally said\nit was 20% of your grade. It's probably going to be more\nlike 10% or 15% in the end.",
    "start": "4473970",
    "end": "4479950"
  },
  {
    "text": "So this is the robot\nyou guys will be using. It's called an\n[INAUDIBLE] robot.",
    "start": "4479950",
    "end": "4485029"
  },
  {
    "text": "Usually it has sort of\na robotic arm on it. But we're actually not\ngoing to be using it for this [INAUDIBLE].",
    "start": "4485029",
    "end": "4490847"
  },
  {
    "text": "This base made from a\ncompany from Spain, Orbonix. It's a pretty cool robot. One cool thing about it is that\nit's actually omnidirectional.",
    "start": "4490848",
    "end": "4498870"
  },
  {
    "text": "So there are wheels\non your wheels here. And what that means is\nthat it can drive sideways, left to right.",
    "start": "4498870",
    "end": "4504690"
  },
  {
    "text": "It would make parallel\nparking your car very easy. [LAUGHTER] We're not going to be\ndriving it probably this fast",
    "start": "4504690",
    "end": "4511210"
  },
  {
    "text": "because it's\nactually super heavy. And we don't want\nanyone to-- yeah, we forgot to screw\nin that [INAUDIBLE]..",
    "start": "4511210",
    "end": "4517550"
  },
  {
    "text": "[LAUGHTER] It will be screwed in\nduring the competition. But it's a pretty fun robot.",
    "start": "4517550",
    "end": "4522660"
  },
  {
    "text": "So you guys will\nbe working on this. And so the actual\nchallenge itself,",
    "start": "4522660",
    "end": "4530420"
  },
  {
    "text": "as we've mentioned many\ntimes throughout this class, will be a modified\norienteering your challenge.",
    "start": "4530420",
    "end": "4535797"
  },
  {
    "text": "So there's going to be a few\ndifferent challenge stations that you have to drive to.",
    "start": "4535797",
    "end": "4541150"
  },
  {
    "text": "And at the stations, there'll be\nsmall computational challenges. And those computational\nchallenges will be a subset of the advanced\nlecture teams, not all of them.",
    "start": "4541150",
    "end": "4551530"
  },
  {
    "text": "And the goal is to complete\nas many of those as you can and try to do that as\nquickly as possible.",
    "start": "4551530",
    "end": "4558030"
  },
  {
    "text": "And it's also going to be\nheld indoors in our lab space, where we just showed you. So that way if it\nrains, we can still have the Grand Challenge\nat the end of the semester.",
    "start": "4558030",
    "end": "4565680"
  },
  {
    "text": "So this is sort of how it's set\nup as of last night actually.",
    "start": "4565680",
    "end": "4572310"
  },
  {
    "text": "So basically the\nrobot will be abe to drive around in a\nsmall little LEGO maze that we set up.",
    "start": "4572310",
    "end": "4577456"
  },
  {
    "text": "And there's going to be\nsort of different things that you have to do with\nin different places.",
    "start": "4577456",
    "end": "4582710"
  },
  {
    "text": "So what are you actually\ngoing to be doing? What assignment are you\nguys going to be doing? Well, it's actually\na little flexible.",
    "start": "4582710",
    "end": "4588580"
  },
  {
    "text": "Since each of you\ndid different things for your advanced\nlectures, each team is going to have a bit of a\ndifferent assignment applied",
    "start": "4588580",
    "end": "4595842"
  },
  {
    "text": "to this. I have some proposed\nideas that I'm going to talk about in\nthe next slide here. But the big thing is that\nthese are just ideas.",
    "start": "4595842",
    "end": "4603250"
  },
  {
    "text": "You guys have a lot of\nflexibility in this. You'll be probably working\nwith the [INAUDIBLE] a lot to have\naccess to the robot.",
    "start": "4603250",
    "end": "4611489"
  },
  {
    "text": "We can arrange extra\noffice hours for you guys to come use the\nhardware to test things.",
    "start": "4611490",
    "end": "4617034"
  },
  {
    "text": "So we'll be arranging\nall of that things as sort of on an\nas-needed basis. If you want to-- basically it'll sort\nof depend on your team.",
    "start": "4617034",
    "end": "4626370"
  },
  {
    "text": "AUDIENCE: And the people who'll\nbe helping out are you, us-- STEVE: Yes, me and\nTiago, who gave a lecture",
    "start": "4626370",
    "end": "4631490"
  },
  {
    "text": "earlier in the semester\nand possibly also a few other people from our lab. But we'll be the main\ncontact points for it.",
    "start": "4631490",
    "end": "4638680"
  },
  {
    "text": "So of course it should\ngo without saying, but all the team members\nshould contribute equally within your team.",
    "start": "4638680",
    "end": "4644360"
  },
  {
    "text": "So it'll be maybe less structure\nthan in advanced lecture. But just make sure that\neveryone's contributing equally",
    "start": "4644360",
    "end": "4649544"
  },
  {
    "text": "in the assignment. And it's going to involve using\nthis thing called the Robot",
    "start": "4649544",
    "end": "4655480"
  },
  {
    "text": "Operating System, or ROS,\nwhich is basically a software framework for communicating and\nit's used a lot in robotics.",
    "start": "4655480",
    "end": "4662719"
  },
  {
    "text": "Just a quick show of\nhands, how many of you have used ROS before? Oh, wow, so a lot of used ROS.",
    "start": "4662719",
    "end": "4668350"
  },
  {
    "text": "How many have heard of\nROS, if not used it? OK, so a lot of people. So that's a good starting point.",
    "start": "4668350",
    "end": "4674540"
  },
  {
    "text": "So here are the the proposed\ntasks for each group. And of course, all of\nthese are up for change.",
    "start": "4674540",
    "end": "4680630"
  },
  {
    "text": "If you guys want to\nchange it, let me know. Basically, so some\nof the groups, it's very clear\nhow it immediately",
    "start": "4680630",
    "end": "4687020"
  },
  {
    "text": "applied to the Grand Challenge. So incremental path planning--\nwell, we have a mobile robot,",
    "start": "4687020",
    "end": "4692650"
  },
  {
    "text": "so maybe we can actually\nprint that on the robot and get it to change or plan\nif something gets in the way.",
    "start": "4692650",
    "end": "4698630"
  },
  {
    "text": "The semantic\nglobalization group-- obviously very applicable\nto the Grand Challenge.",
    "start": "4698630",
    "end": "4703920"
  },
  {
    "text": "You need to know where you are. So the robot that we\nhave now can actually do normal metric localization.",
    "start": "4703920",
    "end": "4710590"
  },
  {
    "text": "So it can sort of\nknow where it is. But what will be\ninteresting to see is compare the semantic\nlocalization to that one.",
    "start": "4710590",
    "end": "4716150"
  },
  {
    "text": "But how would you do\nsemantic localization? Well, we can use a camera. And we can choose\nthe visual learning",
    "start": "4716150",
    "end": "4723110"
  },
  {
    "text": "through deep classification--\nthe visual classification through deep learning group. So I think these\ntwo groups would",
    "start": "4723110",
    "end": "4728525"
  },
  {
    "text": "have a really nice synergy\nand a really cool way to work together. So the MCTS group\nare very cool, but I",
    "start": "4728525",
    "end": "4735581"
  },
  {
    "text": "had a little trouble thinking\nabout exactly how that would apply. So maybe you look like\nyou have an idea maybe. AUDIENCE: So to\nclarify, all these",
    "start": "4735581",
    "end": "4742180"
  },
  {
    "text": "are separate runs of the robot? STEVE: So we're\nactually probably going to run all of them-- so the grid is-- this would be\nprobably one run of the robot.",
    "start": "4742180",
    "end": "4749820"
  },
  {
    "text": "We're going to decouple these\nso that if one or a subset these don't work super well, the\nother groups will still",
    "start": "4749820",
    "end": "4756490"
  },
  {
    "text": "be able to run. So we're carefully\nplanning that out too. So the MCTS group,\nI was thinking maybe",
    "start": "4756490",
    "end": "4764409"
  },
  {
    "text": "could solve-- that could be\na really nice way to-- one of those computational\nchallenges. Maybe you have to\nplay against a human.",
    "start": "4764410",
    "end": "4770890"
  },
  {
    "text": "And if it wins, great. You can go faster or\nyou get more points. So you could implement\non a different game",
    "start": "4770890",
    "end": "4776855"
  },
  {
    "text": "other than connect four or\npossibly and sort of [wrap it around [? rod ?] that\nto call with that. So each ability group\nI think would also",
    "start": "4776855",
    "end": "4783480"
  },
  {
    "text": "be a great place to do one\nof these challenge stations. So we could give\nyou guys puzzle, say maybe it's some sort\nof maze-like state space.",
    "start": "4783480",
    "end": "4791334"
  },
  {
    "text": "And you have to see could we\neven the reach the goal here? And if you get the answer\nright, well, you an move on to the next stage or\nget more points or something",
    "start": "4791334",
    "end": "4798420"
  },
  {
    "text": "like that. AUDIENCE: But again, these\nare just suggestions. So for example, they\ncan do the reachability for doing motion planning.",
    "start": "4798420",
    "end": "4804195"
  },
  {
    "text": "STEVE: Yeah, so if you guys\nhave other suggestions on how to implement your team's stuff\ninto the Grand Challenge, definitely send me an\nemail, preferably today",
    "start": "4804195",
    "end": "4810760"
  },
  {
    "text": "or as soon as you\nthink of the things. And we can change these. These are just\nsuggestions for right now.",
    "start": "4810760",
    "end": "4818470"
  },
  {
    "text": "The last two are sort of\nmore planning related. So planning with temporal logic,\nwhich was Monday's lecture,",
    "start": "4818470",
    "end": "4826090"
  },
  {
    "text": "I thought would be cool\nway to sort of control the robot's high-level action. So maybe that could\ninvolve modeling",
    "start": "4826090",
    "end": "4832800"
  },
  {
    "text": "are Grand Challenge with PDDL. And maybe with linear\ntemporal logic goals, models that you get [INAUDIBLE].",
    "start": "4832800",
    "end": "4839430"
  },
  {
    "text": "Then you could compile that\nand call up to turn around it and execute that plan\non the actual robot,",
    "start": "4839430",
    "end": "4844460"
  },
  {
    "text": "do [INAUDIBLE] high-level\nactions of the robot. That's a possibility. And for today's group,\nthe infinite horizon",
    "start": "4844460",
    "end": "4851390"
  },
  {
    "text": "probablistics planning, maybe\nyou could do something actually similar. But instead of modeling\nthe domain as PDDL",
    "start": "4851390",
    "end": "4859110"
  },
  {
    "text": "model it as an NVP and\nsolve it with LAO star, and get sort of a policy on\nhow to control the robot.",
    "start": "4859110",
    "end": "4865180"
  },
  {
    "text": "Maybe we can change\nit a little bit so that certain squares can be\nmore risky than others or so on.",
    "start": "4865180",
    "end": "4870925"
  },
  {
    "text": "So again, there's flexibility\nin all of these here. So these are just some\nof the suggestions.",
    "start": "4870925",
    "end": "4876208"
  },
  {
    "text": "Does anyone have any\nquestions before we-- this all I have. So anyone have any questions? Yeah? ",
    "start": "4876208",
    "end": "4881990"
  },
  {
    "text": "AUDIENCE: How are we\nworking on [INAUDIBLE]?? Are you [INAUDIBLE]? STEVE: So it's\nbasically up to you",
    "start": "4881991",
    "end": "4888530"
  },
  {
    "text": "guys really divide up the\nwork amongst yourselves. It's really different\nfor every team.",
    "start": "4888530",
    "end": "4893780"
  },
  {
    "text": "So we're-- AUDIENCE: Sorry. So each team is\ndeveloping [INAUDIBLE].. STEVE: Right, yeah. Each team is really in their\nown separate package FIRM.",
    "start": "4893780",
    "end": "4900532"
  },
  {
    "text": "For example, these\ntwo teams, there's probably going to be a\ncommon interface where the output of this one goes\nto the input of that one.",
    "start": "4900532",
    "end": "4907419"
  },
  {
    "text": "So for that one, we're\ngoing to give you sort of the\ninterface [INAUDIBLE] message type package for that. But other than than, like\nfor dividing up the work,",
    "start": "4907419",
    "end": "4914690"
  },
  {
    "text": "you guys [INAUDIBLE] that. PROFESSOR 2: So your\nplans will be integration",
    "start": "4914690",
    "end": "4920280"
  },
  {
    "text": "if you're trying [INAUDIBLE]\npieces the group is doing. That we think is, thing\nthat uses soft engineering",
    "start": "4920280",
    "end": "4928230"
  },
  {
    "text": "skills [INAUDIBLE]. It can be really unpleasant\nto do, take a lot of time.",
    "start": "4928230",
    "end": "4934010"
  },
  {
    "text": "So we don't want you to\nhave that experience. So that's why you can...",
    "start": "4934010",
    "end": "4939300"
  },
  {
    "text": "If not only what\nwe wanted you to do is to be able to get your\nown capability [INAUDIBLE].. ",
    "start": "4939300",
    "end": "4945860"
  },
  {
    "text": "If you guys choose to\nintegrate with other teams because you're really\nexcited about that",
    "start": "4945860",
    "end": "4951290"
  },
  {
    "text": "and because it looks\nlike the people that you're working\nfor [INAUDIBLE].. Then that's purely your choice.",
    "start": "4951290",
    "end": "4957910"
  },
  {
    "text": "Is that fair enough? STEVE: Sure. It's fine. Any more questions? ",
    "start": "4957910",
    "end": "4964910"
  },
  {
    "text": "OK. Sounds great. [APPLAUSE] ",
    "start": "4964910",
    "end": "4970810"
  }
]