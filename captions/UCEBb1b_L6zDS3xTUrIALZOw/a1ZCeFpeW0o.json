[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "19219"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nWe keep on talking about principal component\nanalysis, which we essentially",
    "start": "19219",
    "end": "24870"
  },
  {
    "text": "introduced as a way to\nwork with a bunch of data. So the data that's given to\nus when we want to do PCA",
    "start": "24870",
    "end": "31560"
  },
  {
    "text": "is a bunch of vectors X1 to Xn. So they are random vectors.",
    "start": "31560",
    "end": "40090"
  },
  {
    "start": "40090",
    "end": "45290"
  },
  {
    "text": "in Rd. And what we mentioned\nis that we're going to be using linear\nalgebra-- in particular,",
    "start": "45290",
    "end": "51742"
  },
  {
    "text": "the spectral theorem-- that\nguarantees to us that if I look at the convenience\nmatrix of this guy, or its empirical\ncovariance matrix,",
    "start": "51742",
    "end": "57890"
  },
  {
    "text": "since they're\nsymmetric real matrices and they are positive\nsemidefinite, there exists a diagonalization\ninto non-negative eigenvalues.",
    "start": "57890",
    "end": "66830"
  },
  {
    "text": "And so here, those\nthings live in Rd, so it's a really large space. And what we want to\ndo is to map it down",
    "start": "66830",
    "end": "74600"
  },
  {
    "text": "into a space that\nwe can visualize, hopefully a space\nof size 2 or 3.",
    "start": "74600",
    "end": "79610"
  },
  {
    "text": "Or if not, then we're just going\nto take more and start looking at subspaces altogether.",
    "start": "79610",
    "end": "84920"
  },
  {
    "text": "So think of the case where d\nis large but not larger than n.",
    "start": "84920",
    "end": "93119"
  },
  {
    "text": "So let's say, you have a\nlarge number of points. The question is, is it possible\nto project those things onto",
    "start": "93120",
    "end": "100590"
  },
  {
    "text": "a lower dimensional\nspace, d prime, which is much less than d-- so\nthink of d prime equals, say,",
    "start": "100590",
    "end": "109480"
  },
  {
    "text": "2 or 3-- and so that you keep\nas much information",
    "start": "109480",
    "end": "114490"
  },
  {
    "text": "about the cloud of points\nthat you had originally. So again, the example\nthat we could have is that X1 to Xn for, say,\nXi for patient i's recording",
    "start": "114490",
    "end": "124060"
  },
  {
    "text": "a bunch of body measurements\nand maybe blood pressure, some symptoms, et cetera.",
    "start": "124060",
    "end": "130639"
  },
  {
    "text": "And then we have a\ncloud of n patients. And we're trying to\nvisualize maybe to see if-- If I could see, for\nexample, that there's",
    "start": "130639",
    "end": "136930"
  },
  {
    "text": "two groups of\npatients, maybe I would know that I have two\ngroups of different disease or maybe two groups\nof different patients",
    "start": "136930",
    "end": "142959"
  },
  {
    "text": "that respond differently\nto a particular disease or drug et cetera. So visualizing is\ngoing to give us",
    "start": "142960",
    "end": "148900"
  },
  {
    "text": "quite a bit of insight about\nwhat the spatial arrangement of those vectors are.",
    "start": "148900",
    "end": "155980"
  },
  {
    "text": "And so PCA says, well, here,\nof course, in this question, one thing that's not defined\nis what is information.",
    "start": "155980",
    "end": "162880"
  },
  {
    "text": "And we said that\none thing we might want to do when we project\nis that points do not collide with each other.",
    "start": "162880",
    "end": "168267"
  },
  {
    "text": "And so that means we're\ntrying to find directions, where after I project, the\npoints are still pretty spread out.",
    "start": "168267",
    "end": "173860"
  },
  {
    "text": "And so I can see\nwhat's going on. And PCA says-- OK,\nso there's many ways to answer this question.",
    "start": "173860",
    "end": "179500"
  },
  {
    "text": "And PCA says, let's just\nfind a subspace of dimension d prime that keeps as much\ncovariance structure as",
    "start": "179500",
    "end": "188110"
  },
  {
    "text": "possible. And the reason is\nthat those directions",
    "start": "188110",
    "end": "193390"
  },
  {
    "text": "are the ones that maximize\nthe variance, which is a proxy for the spread. There's many, many\nways to do this.",
    "start": "193390",
    "end": "199540"
  },
  {
    "text": "There's actually a\nGoogle video that was released maybe last week\nabout the data visualization",
    "start": "199540",
    "end": "206440"
  },
  {
    "text": "team of Google that shows\nyou something called t-SNE, which is\nessentially something",
    "start": "206440",
    "end": "211554"
  },
  {
    "text": "that tries to do that. It takes points in\nvery high dimensions and tries to map them\nin lower dimensions, so that you can\nactually visualize them.",
    "start": "211554",
    "end": "218280"
  },
  {
    "text": "And t-SNE is some\nalternative to PCA that gives an other definition\nfor the word information.",
    "start": "218280",
    "end": "226850"
  },
  {
    "text": "I'll talk about this towards\nthe end, how you can actually somewhat automatically\nextend everything",
    "start": "226850",
    "end": "232730"
  },
  {
    "text": "we've said for PCA to an\ninfinite family of procedures.",
    "start": "232730",
    "end": "238830"
  },
  {
    "text": "So how do we do this? Well, the way we do\nthis is as follows. So remember, given\nthose guys, we",
    "start": "238830",
    "end": "245010"
  },
  {
    "text": "can form something which is\ncalled S, which is the sample, or the empirical\ncovariance matrix.",
    "start": "245010",
    "end": "256885"
  },
  {
    "text": " And since from\ncouple of slides ago,",
    "start": "256885",
    "end": "262210"
  },
  {
    "text": "we know that S has a\neigenvalue decomposition, S is equal to PDP transpose,\nwhere P is orthogonal.",
    "start": "262210",
    "end": "272930"
  },
  {
    "text": " So that's where we use our\nlinear algebra results. So that means that P transpose P\nis equal to PP transpose, which",
    "start": "272930",
    "end": "283640"
  },
  {
    "text": "is the identity. So remember, S is\na d by d matrix.",
    "start": "283640",
    "end": "290370"
  },
  {
    "text": "And so P is also d by d. And d is diagonal.",
    "start": "290370",
    "end": "295860"
  },
  {
    "text": " And I'm actually going to take,\nwithout loss of generality,",
    "start": "295860",
    "end": "302860"
  },
  {
    "text": "I'm going to assume that d-- so it's going to be\ndiagonal-- and I'm going to have something\nthat looks like lambda 1",
    "start": "302860",
    "end": "310240"
  },
  {
    "text": "to lambda d. Those are called the\neigenvalues of S. What we know is that lambda\nj's are non-negative.",
    "start": "310240",
    "end": "319035"
  },
  {
    "text": "And actually, what I'm\ngoing to assume without loss of generalities is lambda 1\nis larger than lambda 2, which",
    "start": "319036",
    "end": "324820"
  },
  {
    "text": "is larger than lambda d.",
    "start": "324820",
    "end": "330259"
  },
  {
    "text": "Because in particular,\nthis decomposition-- the spectrum decomposition--\nis not entirely unique.",
    "start": "330259",
    "end": "335470"
  },
  {
    "text": "I could permute\nthe columns of P, and I would still have\nan orthogonal matrix.",
    "start": "335470",
    "end": "342600"
  },
  {
    "text": "And to balance that,\nI would also have to permute the entries of d. So there's as many\ndecompositions",
    "start": "342600",
    "end": "349680"
  },
  {
    "text": "as there are permutations. So there's actually quite a bit. But the bag of\neigenvalues is unique.",
    "start": "349680",
    "end": "356759"
  },
  {
    "text": "The set of\neigenvalues is unique. The ordering is\ncertainly not unique. So here, I'm just\ngoing to pick--",
    "start": "356760",
    "end": "362729"
  },
  {
    "text": "I'm going to nail down one\nparticular permutation-- actually, maybe two in\ncase I have equalities.",
    "start": "362730",
    "end": "368070"
  },
  {
    "text": "But let's say, I pick\none that satisfies this. And the reason why I do this\nis really not very important.",
    "start": "368070",
    "end": "375450"
  },
  {
    "text": "It's just to say,\nI'm going to want to talk about the largest\nof those eigenvalues.",
    "start": "375450",
    "end": "380500"
  },
  {
    "text": "So this is just\ngoing to be easier for me to say that\nthis one is lambda 1, rather than say it's lambda 7.",
    "start": "380500",
    "end": "386730"
  },
  {
    "text": "So this is just to say that\nthe largest eigenvalue of S",
    "start": "386730",
    "end": "399980"
  },
  {
    "text": "is lambda 1. If I didn't do that, I would\njust call it maybe lambda max,",
    "start": "399980",
    "end": "405550"
  },
  {
    "text": "and you would just know\nwhich one I'm talking about. ",
    "start": "405550",
    "end": "412910"
  },
  {
    "text": "So what's happening now\nis that if I look at d,",
    "start": "412910",
    "end": "421520"
  },
  {
    "text": "then it turns out\nthat if I start-- so if I do P transpose Xi, I am\nactually projecting my Xi's--",
    "start": "421520",
    "end": "429890"
  },
  {
    "text": "I'm basically changing\nthe basis for my Xi's. And now, D is the\nempirical covariance matrix",
    "start": "429890",
    "end": "435140"
  },
  {
    "text": "of those guys. So let's check that. So what it means is\nthat if I look at--",
    "start": "435140",
    "end": "442010"
  },
  {
    "text": " so what I claim is\nthat P transpose Xi--",
    "start": "442010",
    "end": "449120"
  },
  {
    "text": "that's a new vector, let's\ncall it Yi, it's also in Rd--",
    "start": "449120",
    "end": "455180"
  },
  {
    "text": "and what I claim is that the\ncovariance matrix of this guy is actually now this\ndiagonal matrix, which",
    "start": "455180",
    "end": "461840"
  },
  {
    "text": "means in particular that\nif they were Gaussian, then they would be independent. But I also know now that\nthere's no correlation",
    "start": "461840",
    "end": "468889"
  },
  {
    "text": "across coordinates of Yi. So to prove this, let me assume\nthat X bar is equal to 0.",
    "start": "468890",
    "end": "480939"
  },
  {
    "text": "And the reason why I do\nthis is because it's just annoying to carry out all\nthis censuring constantly and I talk about S. So\nwhen X bar is equal to 0,",
    "start": "480939",
    "end": "489400"
  },
  {
    "text": "that implies that S\nhas a very simple form. It's of the form\nsum from i equal 1 to n of Xi Xi transpose.",
    "start": "489400",
    "end": "498790"
  },
  {
    "text": "So that's my S. But what I want is the S of Y--",
    "start": "498790",
    "end": "504370"
  },
  {
    "text": "So OK, that implies\nalso that P times X bar, which is equal to P times\nX bar is also equal to 0.",
    "start": "504370",
    "end": "514690"
  },
  {
    "text": "So that means that Y bar-- Y has mean 0, if this is 0.",
    "start": "514690",
    "end": "520240"
  },
  {
    "text": "So if I look at the sample\ncovariance matrix of Y, it's just going to\nbe something that",
    "start": "520240",
    "end": "525879"
  },
  {
    "text": "looks like the sum of the\nouter products or the Yi Yi transpose. ",
    "start": "525880",
    "end": "533290"
  },
  {
    "text": "And again, the reason why\nI make this assumption is so that I don't have to write\nminus X bar X bar transpose.",
    "start": "533290",
    "end": "541400"
  },
  {
    "text": "But you can do it. And it's going to\nwork exactly the same. ",
    "start": "541400",
    "end": "546790"
  },
  {
    "text": "So now, I look at this S prime. And so what is this S prime? Well, I'm just going\nto replace Yi with PXi.",
    "start": "546790",
    "end": "554340"
  },
  {
    "text": "So it's the sum from i equal\n1 to n of PXi PXi transpose,",
    "start": "554340",
    "end": "562850"
  },
  {
    "text": "which is equal to the sum from-- sorry there's a 1/n. ",
    "start": "562850",
    "end": "572360"
  },
  {
    "text": "So it's equal to 1/n\nsum from i equal 1 to n of PXi Xi\ntranspose P transpose.",
    "start": "572360",
    "end": "583490"
  },
  {
    "text": "Agree? I just said that the transpose\nof AB is the transpose of B",
    "start": "583490",
    "end": "588580"
  },
  {
    "text": "times the transpose of A.",
    "start": "588580",
    "end": "593830"
  },
  {
    "text": "And so now, I can\npush the sum in. P does not depend on i. So this thing here is\nequal to PS P transpose,",
    "start": "593830",
    "end": "605800"
  },
  {
    "text": "because the sum of the Xi Xi\ntranspose divided by n is S. But what is PS P transpose?",
    "start": "605800",
    "end": "612200"
  },
  {
    "text": "Well, we know that\nS is equal to-- sorry that's P transpose.",
    "start": "612200",
    "end": "619340"
  },
  {
    "text": "So this was with a P transpose. I'm sorry, I made an\nimportant mistake here. So Yi is P transpose Xi.",
    "start": "619340",
    "end": "625420"
  },
  {
    "text": "So this is P transpose\nand P transpose here, which means that\nthis is P transpose and this is double transpose,\nwhich is just nothing",
    "start": "625420",
    "end": "632449"
  },
  {
    "text": "and that transpose and nothing.  So now, I write S\nas PD P transpose.",
    "start": "632450",
    "end": "641600"
  },
  {
    "text": "That's the spectral\ndecomposition that I had before. That's my eigenvalue\ndecomposition, which means that now,\nif I look at S prime,",
    "start": "641600",
    "end": "649050"
  },
  {
    "text": "it's P transpose times\nPD P transpose P.",
    "start": "649050",
    "end": "656000"
  },
  {
    "text": "But now, P transpose\nP is the identity, P transpose P is the identity. So this is actually\njust equal to D.",
    "start": "656000",
    "end": "666646"
  },
  {
    "text": "And again, you can\ncheck that this also works if you have to center\nall those guys as you go.",
    "start": "666646",
    "end": "672840"
  },
  {
    "text": "But if you think about\nit, this is the same thing as saying that I just\nreplaced Xi by Xi minus X bar.",
    "start": "672840",
    "end": "679530"
  },
  {
    "text": "And then it's true that Y bar\nis also P times Xi minus X bar.",
    "start": "679530",
    "end": "686590"
  },
  {
    "text": "So now, we have that D is\nthe empirical covariance matrix of those guys-- the Yi's, which are\nP transpose Xi's.",
    "start": "686590",
    "end": "693112"
  },
  {
    "text": "And so in particular,\nwhat it means is that if I look at the\ncovariance of Yj Yk--",
    "start": "693112",
    "end": "702810"
  },
  {
    "text": " So that's the covariance\nof the j-th coordinate of Y",
    "start": "702810",
    "end": "708920"
  },
  {
    "text": "and the k-th coordinate of Y.\nI'm just not putting an index. But maybe, let's say the\nfirst one or something like this-- any of\nthem, their IID.",
    "start": "708920",
    "end": "716141"
  },
  {
    "text": "Then what is this covariance? It's actually 0 if j\nis different from k.",
    "start": "716142",
    "end": "721759"
  },
  {
    "text": "And the covariance\nbetween Yj and Yj, which is just the variance\nof Yj, is equal to lambda j--",
    "start": "721760",
    "end": "733070"
  },
  {
    "text": "the j-th largest eigenvalue. So the eigenvalues capture the\nvariance of my observations",
    "start": "733070",
    "end": "742580"
  },
  {
    "text": "in this new coordinate system. And they're\ncompletely orthogonal. So what does that mean?",
    "start": "742580",
    "end": "747590"
  },
  {
    "text": "Well, again, remember,\nif I chop off the head of my Gaussian\nin multi dimensions,",
    "start": "747590",
    "end": "754160"
  },
  {
    "text": "we said that what\nwe started from was something that\nlooked like this.",
    "start": "754160",
    "end": "759560"
  },
  {
    "text": "And we said, well, there's one\ndirection that's important, that's this guy, and one\nimportant that's this guy.",
    "start": "759560",
    "end": "765230"
  },
  {
    "text": "When I applied a transformation\nP transpose, what I'm doing is that I'm realigning this\nthing with the new axes.",
    "start": "765230",
    "end": "771110"
  },
  {
    "text": "Or in a way, rather\nto be fair, I'm not actually realigning\nthe ellipses with the axes.",
    "start": "771110",
    "end": "779600"
  },
  {
    "text": "I'm really realigning the\naxes with the ellipses. So really, what I'm doing is\nI'm saying, after I apply P,",
    "start": "779600",
    "end": "785360"
  },
  {
    "text": "I'm just rotating this\ncoordinate system. So now, it becomes this guy.",
    "start": "785360",
    "end": "792670"
  },
  {
    "start": "792670",
    "end": "799360"
  },
  {
    "text": "And now, my ellipses\nactually completely align. And what happens here is\nthat this coordinate is",
    "start": "799360",
    "end": "805730"
  },
  {
    "text": "independent of that coordinate. And that's what we write\nhere, if they are Gaussian.",
    "start": "805730",
    "end": "811714"
  },
  {
    "text": "I didn't really tell this-- I'm only making statements\nabout covariances. If they are Gaussians,\nthose implied statements",
    "start": "811715",
    "end": "816768"
  },
  {
    "text": "about independence.  So as I said, the\nvariance now, lambda 1,",
    "start": "816768",
    "end": "824590"
  },
  {
    "text": "is actually the variance\nof P transpose Xi.",
    "start": "824590",
    "end": "834700"
  },
  {
    "text": " But if I look now at\nthe-- so this is a vector,",
    "start": "834700",
    "end": "840140"
  },
  {
    "text": "so I need to look at the\nfirst coordinate of this guy. ",
    "start": "840140",
    "end": "848490"
  },
  {
    "text": "So it turns out that\ndoing this is actually the same thing as looking\nat the variance of what?",
    "start": "848490",
    "end": "855440"
  },
  {
    "text": "Well, the first\ncolumn of P times Xi.",
    "start": "855440",
    "end": "861480"
  },
  {
    "text": "So that's the variance of-- I'm going to call it v1\ntranspose Xi, where P--",
    "start": "861480",
    "end": "870344"
  },
  {
    "start": "870344",
    "end": "884390"
  },
  {
    "text": "So the v1 vd in Rd\nare eigenvectors.",
    "start": "884390",
    "end": "893920"
  },
  {
    "text": "And each vi is\nassociated to lambda i. So that's what we saw when\nwe talked about this eigen",
    "start": "893920",
    "end": "899740"
  },
  {
    "text": "decomposition a\ncouple of slides back. That's the one here.",
    "start": "899740",
    "end": "906040"
  },
  {
    "text": "So if I call the\ncolumns of P v1 to vd, this is what's happening.",
    "start": "906040",
    "end": "913600"
  },
  {
    "text": "So when I look at lambda\n1, it's just the variance of Xi inner product with v1.",
    "start": "913600",
    "end": "919700"
  },
  {
    "text": "And we made this picture\nwhen we said, well, let's say v1 is here\nand then x1 is here.",
    "start": "919700",
    "end": "925870"
  },
  {
    "text": "And if vi has a unique\nnorm, then the inner product",
    "start": "925870",
    "end": "931180"
  },
  {
    "text": "between Xi and v1 is just\nthe length of this guy here.",
    "start": "931180",
    "end": "938050"
  },
  {
    "text": "So that's the variance of the\nXi says the length of Xi-- so this is 0-- that's the\nlength of Xi when I project it",
    "start": "938050",
    "end": "943720"
  },
  {
    "text": "onto the direction\nthat span by v1. If v1 has length 2, this is\nreally just twice this length.",
    "start": "943720",
    "end": "952210"
  },
  {
    "text": "If vi has length 3,\nit's three times this. But it turns out that since\nP satisfies P transpose",
    "start": "952210",
    "end": "961570"
  },
  {
    "text": "P is equal to the identity-- that's an orthogonal\nmatrix, that's right here--",
    "start": "961570",
    "end": "967900"
  },
  {
    "text": "then this is actually\nsaying the same thing as vj transpose vj, which is\nreally the norm squared of vj,",
    "start": "967900",
    "end": "978759"
  },
  {
    "text": "is equal to 1. And vj transpose vk is equal\nto 0, if j is different from k.",
    "start": "978760",
    "end": "986519"
  },
  {
    "text": " The eigenvectors are\northogonal to each other.",
    "start": "986520",
    "end": "991560"
  },
  {
    "text": "And they're actually\nall of norm 1. ",
    "start": "991560",
    "end": "997390"
  },
  {
    "text": "So now, I know that this\nis indeed a direction. And so when I look\nat v1 transpose Xi,",
    "start": "997390",
    "end": "1004290"
  },
  {
    "text": "I'm really measuring\nexactly this length. And what is this length? It's the length of\nthe projection of Xi",
    "start": "1004290",
    "end": "1009660"
  },
  {
    "text": "onto this line. That's the line\nthat's spanned by v1. So if I had a very high\ndimensional problem",
    "start": "1009660",
    "end": "1017680"
  },
  {
    "text": "and I started to look\nat the direction v1-- let's say v1 now is\nnot a eigenvector,",
    "start": "1017680",
    "end": "1023884"
  },
  {
    "text": "it's any direction-- then\nif I want to do this lower dimensional projection, then\nI have to understand how those",
    "start": "1023884",
    "end": "1031819"
  },
  {
    "text": "Xi's project onto the\nline that's spanned by v1, because this is all that I'm\ngoing to be keeping at the end of the day about Xi's.",
    "start": "1031819",
    "end": "1037646"
  },
  {
    "text": " So what we want is\nto find the direction",
    "start": "1037646",
    "end": "1043199"
  },
  {
    "text": "where those Xi's,\nthose projections, have a lot of variance. And we know that the variance\nof Xi on this direction",
    "start": "1043200",
    "end": "1048569"
  },
  {
    "text": "is actually exactly\ngiven by lambda 1. ",
    "start": "1048569",
    "end": "1056890"
  },
  {
    "text": "Sorry, that's the\nempirical var-- yeah, I should\ncall variance hat.",
    "start": "1056890",
    "end": "1062480"
  },
  {
    "text": "That's the empirical variance. Everything is in empirical here. We're talking about the\nempirical covariance matrix.",
    "start": "1062480",
    "end": "1068679"
  },
  {
    "text": "And so I also have that lambda\n2 is the empirical variance",
    "start": "1068680",
    "end": "1074150"
  },
  {
    "text": "of when I project Xi onto\nv2, which is the second one,",
    "start": "1074150",
    "end": "1079160"
  },
  {
    "text": "just for exactly this reason. ",
    "start": "1079160",
    "end": "1087474"
  },
  {
    "text": "Any question? ",
    "start": "1087474",
    "end": "1094170"
  },
  {
    "text": "So lambda j's are going\nto be important for us. Lambda j measure the\nspread of the points",
    "start": "1094170",
    "end": "1099320"
  },
  {
    "text": "when I project them onto a\nline which is a one dimensional space. And so I'm going to have-- let's\nsay I want to pick only one,",
    "start": "1099320",
    "end": "1105800"
  },
  {
    "text": "I'm going to have to find the\none dimensional space that carries the most variance. And I claim that\nv1 is the one that",
    "start": "1105800",
    "end": "1112070"
  },
  {
    "text": "actually maximizes the spread. So the claim-- so for\nany direction, u in Rd--",
    "start": "1112070",
    "end": "1135900"
  },
  {
    "text": "and by direction, I really\njust mean that the norm of u is equal to 1.",
    "start": "1135900",
    "end": "1140920"
  },
  {
    "text": "I need to play fair-- I'm going to compare myself to\nother things of lengths one, so I need to play fair and\nlook at directions of length 1.",
    "start": "1140920",
    "end": "1147600"
  },
  {
    "text": "Now, if I'm interested\nin the empirical variance",
    "start": "1147600",
    "end": "1156320"
  },
  {
    "text": "of X1 transpose-- sorry, u transpose X1 u\ntranspose Xn, then this thing",
    "start": "1156321",
    "end": "1169150"
  },
  {
    "text": "is maximized for\nu equals v1, where",
    "start": "1169150",
    "end": "1177950"
  },
  {
    "text": "v1 is the eigenvector\nassociated to lambda 1 and lambda 1 is not\nany eigenvalues, it's the largest of all those.",
    "start": "1177950",
    "end": "1185090"
  },
  {
    "text": "So it's the largest eigenvalue. ",
    "start": "1185090",
    "end": "1190606"
  },
  {
    "text": "So why is that true?  Well, there's also a claim\nthat for any direction u--",
    "start": "1190607",
    "end": "1200840"
  },
  {
    "text": "so that's 1 and 2-- the variance of u\ntranspose X-- now,",
    "start": "1200840",
    "end": "1208990"
  },
  {
    "text": "this is just a random variable,\nand I'm looking about the true variance-- this is maximized for u\nequals, let's call it w1,",
    "start": "1208990",
    "end": "1227440"
  },
  {
    "text": "where w1 is the\neigenvector of sigma--",
    "start": "1227440",
    "end": "1238320"
  },
  {
    "text": "Now, I'm talking about\nthe true variance. Whereas, here, I was talking\nabout the empirical variance. So the true variance\nis the eigenvectors",
    "start": "1238320",
    "end": "1244950"
  },
  {
    "text": "of the true sigma\nassociated to the largest",
    "start": "1244950",
    "end": "1255630"
  },
  {
    "text": "eigenvalue of sigma. ",
    "start": "1255630",
    "end": "1262870"
  },
  {
    "text": "So I did not give it a name. Here, that was lambda 1\nfor the empirical one. For the true one,\nyou can give it another name, mu 1 if you want.",
    "start": "1262870",
    "end": "1270330"
  },
  {
    "text": "But that's just the same thing. All it's saying is like,\nwherever I see empirical,",
    "start": "1270330",
    "end": "1275490"
  },
  {
    "text": "I can remove it. ",
    "start": "1275490",
    "end": "1287690"
  },
  {
    "text": "So why is this claim true? Well, let's look at the\nsecond one, for example. ",
    "start": "1287690",
    "end": "1298179"
  },
  {
    "text": "So what is the variance\nof u transpose X?",
    "start": "1298180",
    "end": "1304480"
  },
  {
    "text": "So that's what I want to know. So that's the expectation--\nso let's assume that X is 0,",
    "start": "1304480",
    "end": "1314850"
  },
  {
    "text": "again, for same\nreasons as before. So what is the variance? It's just the expectation\nof the square? ",
    "start": "1314850",
    "end": "1326460"
  },
  {
    "text": "I don't need to remove\nthe expectation. And the expedition\nof the square is just the expectation\nof u transpose X.",
    "start": "1326460",
    "end": "1332700"
  },
  {
    "text": "And then I'm going to write\nthe other one X transpose u. ",
    "start": "1332700",
    "end": "1339510"
  },
  {
    "text": "And we know that this\nis deterministic. So I'm just going to take\nthat this is just u transpose",
    "start": "1339510",
    "end": "1345570"
  },
  {
    "text": "expectation of X X transpose u.",
    "start": "1345570",
    "end": "1351995"
  },
  {
    "text": "And what is this guy? ",
    "start": "1351995",
    "end": "1359305"
  },
  {
    "text": "That's covariance sigma. That's just what sigma is. ",
    "start": "1359305",
    "end": "1364730"
  },
  {
    "text": "So the variance I can write\nas u transpose sigma u. We've made this\ncomputation before.",
    "start": "1364730",
    "end": "1371272"
  },
  {
    "text": "And now what I want to claim\nis that this thing is actually less than the largest\neigenvalue, which I actually",
    "start": "1371272",
    "end": "1377275"
  },
  {
    "text": "called lambda 1 here. I should probably not. And the P is-- well, OK. ",
    "start": "1377275",
    "end": "1386429"
  },
  {
    "text": "Let's just pretend\neverything is not empirical. So now, I'm going to write\nsigma as P lambda 1 lambda n P",
    "start": "1386430",
    "end": "1402580"
  },
  {
    "text": "transpose. That's just the\neigendecomposition, where I admittedly reuse the\nsame notation as I did for S.",
    "start": "1402580",
    "end": "1412090"
  },
  {
    "text": "So I should really put\nsome primes everywhere, so you know those are\nthings that are actually different in practice.",
    "start": "1412090",
    "end": "1418630"
  },
  {
    "text": "So this is just that the\ndecomposition of sigma. You seem confused, Helen.",
    "start": "1418630",
    "end": "1424510"
  },
  {
    "text": "You have a question? Yeah? AUDIENCE: What is-- when you\ntalked about the empirical data",
    "start": "1424510",
    "end": "1433830"
  },
  {
    "text": "and-- PHILIPPE RIGOLLET: So OK-- ",
    "start": "1433830",
    "end": "1440670"
  },
  {
    "text": "so I can make\neverything I'm saying, I can talk about\neither the variance or the empirical variance. And you can just add the\nword empirical in front of it",
    "start": "1440670",
    "end": "1447720"
  },
  {
    "text": "whenever you want. The same thing works. But just for the sake of\nremoving the confusion,",
    "start": "1447720",
    "end": "1453120"
  },
  {
    "text": "let's just do it again\nwith S. So I'm just",
    "start": "1453120",
    "end": "1460408"
  },
  {
    "text": "going to do everything\nwith S. So I'm going to assume that\nX bar is equal to 0. And here, I'm going to talk\nabout the empirical variance,",
    "start": "1460409",
    "end": "1467780"
  },
  {
    "text": "which is just 1/n\nsum from i equal 1 to n of u transpose Xi squared.",
    "start": "1467780",
    "end": "1475272"
  },
  {
    "text": "So it's the same thing. Everywhere you see\nan expectation, you just put in average. ",
    "start": "1475272",
    "end": "1485929"
  },
  {
    "text": "And then I get 1/n\nsum from i equal 1 to n of Xi Xi transpose.",
    "start": "1485930",
    "end": "1493032"
  },
  {
    "text": "And now, I'm going\nto call this guy S, because that's what it is.",
    "start": "1493032",
    "end": "1498200"
  },
  {
    "text": "So this is u transpose Su. But just defined that I could\njust replace the expectation by averages everywhere,\nyou can tell",
    "start": "1498200",
    "end": "1503910"
  },
  {
    "text": "that the thing is going to work\nfor either one or the other. So now, this thing\nwas actually-- so now, I don't have any problem\nwith my notation.",
    "start": "1503910",
    "end": "1510240"
  },
  {
    "text": "This is actually the\ndecomposition of S. That's just the\nspectral decomposition",
    "start": "1510240",
    "end": "1516030"
  },
  {
    "text": "and it's to its eigenvalues. And so now, what I have is that\nwhen I look at u transpose Su,",
    "start": "1516030",
    "end": "1527080"
  },
  {
    "text": "this is actually equal\nto P u transpose S Pu.",
    "start": "1527080",
    "end": "1534919"
  },
  {
    "text": " OK.",
    "start": "1534920",
    "end": "1540500"
  },
  {
    "text": "There's a transpose somewhere. That's this guy.  And that's this guy.",
    "start": "1540500",
    "end": "1546161"
  },
  {
    "start": "1546161",
    "end": "1557057"
  },
  {
    "text": "Now-- sorry, that's\nnot P, that's D. That's D, that's\nthis diagonal matrix.",
    "start": "1557057",
    "end": "1565000"
  },
  {
    "start": "1565000",
    "end": "1570269"
  },
  {
    "text": "Let's look at this thing. And let's call P transpose\nu, let's call it b.",
    "start": "1570269",
    "end": "1575810"
  },
  {
    "text": "So that's also a vector in Rd. What is it? It's just, I take a\nunit vector, and then",
    "start": "1575810",
    "end": "1581370"
  },
  {
    "text": "I apply P transpose to it. So that's basically what\nhappens to a unit vector when I apply the same\nchange of basis that I did.",
    "start": "1581370",
    "end": "1589820"
  },
  {
    "text": "So I'm just changing my\northogonal system the same way I did for the other ones.",
    "start": "1589820",
    "end": "1596360"
  },
  {
    "text": "So what's happening\nwhen I write this? Well, now I have that u\ntranspose Su is b transpose Db.",
    "start": "1596360",
    "end": "1606590"
  },
  {
    "text": "But now, doing b transpose\nDb when D is diagonal and b is a vector is\na very simple thing.",
    "start": "1606590",
    "end": "1612690"
  },
  {
    "text": "I can expand it. This is what? This is just the\nsum from j equal 1 to d of lambda j bj squared.",
    "start": "1612690",
    "end": "1621650"
  },
  {
    "text": " So that's just like matrix\nvector multiplication.",
    "start": "1621650",
    "end": "1628947"
  },
  {
    "text": "And in particular, I know\nthat the largest of those guys is lambda 1 and those\nguys are all non-negative.",
    "start": "1628947",
    "end": "1634010"
  },
  {
    "text": "So this thing is actually\nless than lambda 1 times the sum from j equal 1 to\nd of lambda j squared--",
    "start": "1634010",
    "end": "1640429"
  },
  {
    "text": " sorry, bj squared. ",
    "start": "1640430",
    "end": "1647559"
  },
  {
    "text": "And this is just the\nnorm of b squared.",
    "start": "1647560",
    "end": "1654010"
  },
  {
    "text": "So if I want to prove what's on\nthe slide, all I need to check is that b has norm, which is--",
    "start": "1654010",
    "end": "1660965"
  },
  {
    "text": "AUDIENCE: 1. PHILIPPE RIGOLLET: At most, 1. It's going to be at most 1. Why? Well, because b is really\njust a change of basis for u.",
    "start": "1660965",
    "end": "1671690"
  },
  {
    "text": "And so if I take a vector,\nI'm just changing its basis. I'm certainly not\nchanging its length--",
    "start": "1671690",
    "end": "1677540"
  },
  {
    "text": "think of a rotation,\nand I can also flip it, but think of a rotation-- ",
    "start": "1677540",
    "end": "1682839"
  },
  {
    "text": "well, actually, for vector, it's\njust going to be a rotation. And so now, what\nI have I just have to check that the norm of\nb squared is equal to what?",
    "start": "1682839",
    "end": "1691970"
  },
  {
    "text": "Well, it's equal to the norm\nof P transpose u squared, which is equal to u\ntranspose P P transpose u.",
    "start": "1691970",
    "end": "1701620"
  },
  {
    "text": "But P is orthogonal. So this thing is actually\njust the identity. So that's just u\ntranspose u, which",
    "start": "1701620",
    "end": "1708307"
  },
  {
    "text": "is equal to the norm u\nsquared, which is equal to 1, because I took u to have\nnorm 1 in the first place.",
    "start": "1708307",
    "end": "1717070"
  },
  {
    "text": "And so this-- you're right--\nwas actually of norm equal to 1. I just needed to have\nit less, but it's equal. And so what I'm left with is\nthat this thing is actually",
    "start": "1717070",
    "end": "1724350"
  },
  {
    "text": "equal to lambda 1. So I know that for\nevery u that I pick--",
    "start": "1724350",
    "end": "1730029"
  },
  {
    "text": "that has norm-- So I'm just reminding\nyou that u here has norm squared equal to 1.",
    "start": "1730030",
    "end": "1737730"
  },
  {
    "text": "For every u that I\npick, this u transpose Su is at mostly lambda 1.",
    "start": "1737730",
    "end": "1742890"
  },
  {
    "text": " So that's the u transpose\nSu is at most lambda 1.",
    "start": "1742890",
    "end": "1751250"
  },
  {
    "text": "And we know that that's\nthe variance, that's the empirical variance,\nwhen I project my points onto direction spanned by u.",
    "start": "1751250",
    "end": "1757500"
  },
  {
    "text": " So now, I have an\nempirical variance,",
    "start": "1757500",
    "end": "1763040"
  },
  {
    "text": "which is at most lambda 1. But I also know that if I take u\nto be something very specific--",
    "start": "1763040",
    "end": "1768457"
  },
  {
    "text": "I mean, it was on\nthe previous board-- if I take u to be\nequal to v1, then this thing is actually\nnot an inequality,",
    "start": "1768457",
    "end": "1775270"
  },
  {
    "text": "this is an equality. And the reason is, when I\nactually take u to be v1,",
    "start": "1775270",
    "end": "1781990"
  },
  {
    "text": "all of these bj's are going to\nbe 0, except for the one that's b1, which is itself equal to 1.",
    "start": "1781990",
    "end": "1790360"
  },
  {
    "text": "So I mean, we can\nbriefly check this. But if I take v-- ",
    "start": "1790360",
    "end": "1799106"
  },
  {
    "text": "if u is equal to v1, what\nI have is that u transpose",
    "start": "1799106",
    "end": "1807100"
  },
  {
    "text": "Su is equal to P transpose\nv1 D P transpose v1.",
    "start": "1807100",
    "end": "1824799"
  },
  {
    "text": "But what is P transpose v1? Well, remember P\ntranspose is just",
    "start": "1824800",
    "end": "1831960"
  },
  {
    "text": "the matrix that has\nvectors v1 transpose here, v2 transpose here, all the\nway to vd transpose here.",
    "start": "1831960",
    "end": "1840110"
  },
  {
    "text": "And we know that when I take\nvj transpose vk, I get 0,",
    "start": "1840110",
    "end": "1845570"
  },
  {
    "text": "if j is different from k. And if j is equal to k, I get 1. So P transpose v1\nis equal to what?",
    "start": "1845570",
    "end": "1853690"
  },
  {
    "start": "1853690",
    "end": "1865039"
  },
  {
    "text": "Take v1 here and multiply it. So the first coordinate\nis going to be v1 transpose v1, which is 1.",
    "start": "1865040",
    "end": "1872870"
  },
  {
    "text": "The second coordinate\nis going to be v2 transpose v1, which is 0.",
    "start": "1872870",
    "end": "1879030"
  },
  {
    "text": "And so I get 0's\nall the way, right? So that means that this\nthing here is really",
    "start": "1879030",
    "end": "1885470"
  },
  {
    "text": "just the vector 1, 0, 0. And here, this is just\nthe vector 1, 0, 0.",
    "start": "1885470",
    "end": "1892220"
  },
  {
    "text": "So when I multiply\nit with this guy, I am only picking up\nthe top left element",
    "start": "1892220",
    "end": "1897980"
  },
  {
    "text": "of D, which is lambda 1. So for every one,\nit's less lambda 1.",
    "start": "1897980",
    "end": "1904940"
  },
  {
    "text": "And for v1, it's\nequal to lambda 1, which means that it's\nmaximized for a equals v1.",
    "start": "1904940",
    "end": "1912590"
  },
  {
    "text": "And that's where\nI said that this is the fanciest non-convex\nproblem we know how to solve. This was a problem that\nwas definitely non-convex.",
    "start": "1912590",
    "end": "1919610"
  },
  {
    "text": "We were maximizing a convex\nfunction over a sphere. But we know that v1,\nwhich is something--",
    "start": "1919610",
    "end": "1926156"
  },
  {
    "text": "I mean, of course,\nyou still have to believe me that\nyou can compute the spectral decomposition\nefficiently--",
    "start": "1926156",
    "end": "1931670"
  },
  {
    "text": "but essentially, if you've\ntaken linear algebra, you know that you can\ndiagonalize a matrix.",
    "start": "1931670",
    "end": "1937020"
  },
  {
    "text": "And so you get that v1\nis just the maximum. So you can find your\nmaximum just by looking at the spectral decomposition.",
    "start": "1937020",
    "end": "1944109"
  },
  {
    "text": "You don't have to\ndo any optimization or anything like this. So let's recap.",
    "start": "1944109",
    "end": "1949870"
  },
  {
    "text": "Where are we? We've established\nthat if I start with my empirical covariance\nmatrix, I can diagonalize it",
    "start": "1949870",
    "end": "1957820"
  },
  {
    "text": "and PD P transpose. And then if I take the\neigenvector associated",
    "start": "1957820",
    "end": "1964250"
  },
  {
    "text": "to the largest eigenvalues-- so\nif I permute the columns of P and of D's in such\na way that they",
    "start": "1964250",
    "end": "1970810"
  },
  {
    "text": "are ordered from the\nlargest to the smallest when I look at the diagonal\nelements of D,",
    "start": "1970810",
    "end": "1976490"
  },
  {
    "text": "then if I pick the first\ncolumn of P, it's v1. And v1 is the direction on\nwhich, if I project my points,",
    "start": "1976490",
    "end": "1984750"
  },
  {
    "text": "they are going to carry the\nmost empirical variance. Well, that's a good way. If I told you,\npick one direction",
    "start": "1984750",
    "end": "1993064"
  },
  {
    "text": "along which if you were\nto project your points they would be as spread out\nas possible, that's probably the one you would pick.",
    "start": "1993064",
    "end": "1999270"
  },
  {
    "text": "And so that's exactly\nwhat PCA is doing for us. It says, OK, if you ask me\nto take d prime equal to 1,",
    "start": "1999270",
    "end": "2008780"
  },
  {
    "text": "I will take v1. I will just take the direction\nthat's spanned by v1.",
    "start": "2008780",
    "end": "2013892"
  },
  {
    "text": "And that's just when I come\nback to this picture that was here before, this is v1.",
    "start": "2013892",
    "end": "2023750"
  },
  {
    "text": "Of course, here, I\nonly have two of them. So v2 has to be this\nguy, or this guy, or I mean or this thing.",
    "start": "2023750",
    "end": "2029940"
  },
  {
    "text": "I mean, I don't know\nthem up to sine. But then if I have three--",
    "start": "2029940",
    "end": "2035600"
  },
  {
    "text": "think of like an olive\nin three dimensions-- then maybe I have one\ndirection that's slightly more elongated than the other one.",
    "start": "2035600",
    "end": "2042180"
  },
  {
    "text": "And so I'm going to\npick the second one. And so the procedure is\nto say, well, first, I'm",
    "start": "2042180",
    "end": "2047330"
  },
  {
    "text": "going to pick v1 the same way\nI pick v1 in the first place. So the first\ndirection I am taking",
    "start": "2047330",
    "end": "2052610"
  },
  {
    "text": "is the leading eigenvector. And then I'm looking\nfor a direction.",
    "start": "2052610",
    "end": "2058199"
  },
  {
    "text": "Well, if I found\none-- the one I'm going to want to find-- if you\nsay you can take d equal 2,",
    "start": "2058199",
    "end": "2063239"
  },
  {
    "text": "you're going to need\nthe basis for this guy. So the second one has to be\northogonal to the first one you've already picked.",
    "start": "2063239",
    "end": "2068705"
  },
  {
    "text": "And so the second\none you pick is the one that's just,\namong all those that are orthogonal to v1, maximized\nthe empirical variance",
    "start": "2068705",
    "end": "2076529"
  },
  {
    "text": "when you project onto it.  And it turns out that this\nis actually exactly v2.",
    "start": "2076529",
    "end": "2084000"
  },
  {
    "text": "You don't have to\nredo anything again. You're eigendecomposition,\nthis is just the second column\nof P. Clearly, v2",
    "start": "2084000",
    "end": "2094690"
  },
  {
    "text": "is orthogonal to v1. We just used it here. This 0 here just says this\nv2 is orthogonal to v1.",
    "start": "2094690",
    "end": "2103730"
  },
  {
    "text": "So they're like this. And now, what I said-- what this slide\ntells you extra-- is that v2 among all\nthose directions that are",
    "start": "2103730",
    "end": "2110670"
  },
  {
    "text": "orthogonal-- I mean, there's still\nd minus 1 of them-- this is the one that\nmaximizes the, say,",
    "start": "2110670",
    "end": "2116030"
  },
  {
    "text": "residual empirical\nvariance-- the one that was not explained by the first\ndirection that you picked.",
    "start": "2116030",
    "end": "2121950"
  },
  {
    "text": "And you can check that. I mean, it's becoming a bit\nmore cumbersome to write down,",
    "start": "2121950",
    "end": "2127200"
  },
  {
    "text": "but you can check that. If you're not convinced,\nplease raise your concern. I mean, basically, one\nway you view this to--",
    "start": "2127200",
    "end": "2138640"
  },
  {
    "text": "I mean, you're not really\ndropping a coordinate, because v1 is not a coordinate. But let's assume actually for\nsimplicity that v1 was actually",
    "start": "2138641",
    "end": "2146040"
  },
  {
    "text": "equal to e1, that the direction\nthat carries the most variance is the one that\njust says, just look",
    "start": "2146040",
    "end": "2151440"
  },
  {
    "text": "at the first coordinate of X.\nSo if that was the case, then",
    "start": "2151440",
    "end": "2156520"
  },
  {
    "text": "clearly the orthogonal\ndirections are the ones that comprise only\nof the coordinates 2 to d.",
    "start": "2156520",
    "end": "2163420"
  },
  {
    "text": "So you could actually just\ndrop the first coordinate and do the same thing on\na slightly shorter vector",
    "start": "2163420",
    "end": "2168460"
  },
  {
    "text": "of length d minus 1. And then you would just look\nat the largest eigenvector of these guys, et\ncetera, et cetera.",
    "start": "2168460",
    "end": "2174530"
  },
  {
    "text": "So in a way, that's\nwhat's happening, except that you rotate it\nbefore you actually do this. And that's exactly\nwhat's happening.",
    "start": "2174530",
    "end": "2182260"
  },
  {
    "text": "So what we put together here\nis essentially three things.",
    "start": "2182260",
    "end": "2190890"
  },
  {
    "text": "One was statistics. Statistics says, if\nyou won't spread, if you want information, you\nshould be looking at variance.",
    "start": "2190890",
    "end": "2199230"
  },
  {
    "text": "The second one was optimization. Optimization said, well, if you\nwant to maximize spread, well,",
    "start": "2199230",
    "end": "2204870"
  },
  {
    "text": "you have to maximize variance\nin a certain direction. And that means maximizing\nover the sphere of vectors",
    "start": "2204870",
    "end": "2211920"
  },
  {
    "text": "that have unique norm. And that's an optimization\nproblem, which actually turned out to be difficult.",
    "start": "2211920",
    "end": "2218310"
  },
  {
    "text": "But then the third thing that\nwe use to solve this problem was linear algebra. Linear algebra\nsaid, well, it looks",
    "start": "2218310",
    "end": "2223410"
  },
  {
    "text": "like it's a difficult\noptimization problem. But it turns out that the\nanswer comes in almost-- I mean, it's not a closed form,\nbut those things are so used,",
    "start": "2223410",
    "end": "2231210"
  },
  {
    "text": "that it's almost a closed form-- says, just pick the\neigenvectors in order",
    "start": "2231210",
    "end": "2237240"
  },
  {
    "text": "of their associated eigenvalues\nfrom largest to smallest. ",
    "start": "2237240",
    "end": "2243020"
  },
  {
    "text": "And that's why principal\ncomponent analysis has been so popular and has\ngained huge amount of traction",
    "start": "2243020",
    "end": "2249079"
  },
  {
    "text": "since we had computers that were\nallowed to compute eigenvalues and eigenvectors for\nmatrices of gigantic sizes.",
    "start": "2249080",
    "end": "2257429"
  },
  {
    "text": "You can actually do that. If I give you-- I don't know, this Google\nvideo, for example, is talking about words.",
    "start": "2257429",
    "end": "2263750"
  },
  {
    "text": "They want to do just the,\nsay, principal component analysis of words. So I give you all the\nwords in the dictionary.",
    "start": "2263750",
    "end": "2270230"
  },
  {
    "text": "And-- sorry, well,\nyou would have to have a representation\nfor words, so it's a little more\ndifficult. But how do I do this?",
    "start": "2270230",
    "end": "2279500"
  },
  {
    "text": " Let's say, for example,\npages of a book.",
    "start": "2279500",
    "end": "2286382"
  },
  {
    "text": "I want to understand\nthe pages of a book. And I need to turn\nit into a number. And a page of a book is\nbasically the word count.",
    "start": "2286382",
    "end": "2293150"
  },
  {
    "text": "So I just count the number\nof times \"the\" shows up, the number of times \"and\"\nshows up, number of times \"dog\" shows up.",
    "start": "2293150",
    "end": "2299099"
  },
  {
    "text": "And so that gives me a vector. It's in pretty high dimensions. It's as many dimensions as there\nare words in the dictionary.",
    "start": "2299100",
    "end": "2305350"
  },
  {
    "text": "And now, I want to visualize\nhow those pages get together-- are two pages very\nsimilar or not.",
    "start": "2305350",
    "end": "2310450"
  },
  {
    "text": "And so what you would\ndo is essentially just compute the largest\neigenvector of this matrix--",
    "start": "2310450",
    "end": "2315470"
  },
  {
    "text": "maybe the two largest-- and\nthen project this into a plane. Yeah. AUDIENCE: Can we assume\nthe number of points",
    "start": "2315470",
    "end": "2321325"
  },
  {
    "text": "was far larger\nthan the dimension? PHILIPPE RIGOLLET:\nYeah, but there's many pages in the world.",
    "start": "2321325",
    "end": "2326834"
  },
  {
    "text": "There's probably more\npages in the world than there's words\nin the dictionary. ",
    "start": "2326834",
    "end": "2334960"
  },
  {
    "text": "Yeah, so of course, if\nyou are in high dimensions and you don't have\nenough points, it's going to be\nclearly an issue.",
    "start": "2334960",
    "end": "2340240"
  },
  {
    "text": "If you have two points,\nthen the leading eigenvector is going to be\njust the line that goes through those\ntwo points, regardless",
    "start": "2340240",
    "end": "2346879"
  },
  {
    "text": "of what the dimension is. And clearly, you're\nnot learning anything. ",
    "start": "2346879",
    "end": "2353849"
  },
  {
    "text": "So you have to pick,\nsay, the k largest one. If you go all the way, you're\njust reordering your thing, and you're not actually\ngaining anything.",
    "start": "2353850",
    "end": "2360550"
  },
  {
    "text": "You start from d\nand you go too d. So at some point, this\nprocedure has to stop.",
    "start": "2360550",
    "end": "2366299"
  },
  {
    "text": "And let's say it stops at k. Now, of course, you\nshould ask me a question,",
    "start": "2366300",
    "end": "2371359"
  },
  {
    "text": "which is, how do you choose k? So that's, of course,\na natural question.",
    "start": "2371360",
    "end": "2377400"
  },
  {
    "text": "Probably the basic answer\nis just pick k equals 3, because you can\nactually visualize it.",
    "start": "2377400",
    "end": "2383220"
  },
  {
    "text": "But what happens if I\ntake k is equal to 4? If I take is equal\nto 4, I'm not going",
    "start": "2383220",
    "end": "2391860"
  },
  {
    "text": "to be able to plot points\nin four dimensions. Well, I could, I\ncould add color, or I could try to be a\nlittle smart about it.",
    "start": "2391860",
    "end": "2397440"
  },
  {
    "text": "But it's actually\nquite difficult. And so what people tend to do,\nif you have four dimensions,",
    "start": "2397440",
    "end": "2404420"
  },
  {
    "text": "they actually do a bunch\nof two dimensional plots. And that's what a computer--\na computer is not very good-- I mean, by default,\nthey don't spit out",
    "start": "2404420",
    "end": "2410750"
  },
  {
    "text": "three dimensional plots. So let's say they want to plot\nonly two dimensional things. So they're going to take the\nfirst directions of, say, v1,",
    "start": "2410750",
    "end": "2417440"
  },
  {
    "text": "v2. Let's say you have\nthree, but you want to have only two\ndimensional plots. And then it's going to do\nv1, v3; and then v2, v3.",
    "start": "2417440",
    "end": "2429660"
  },
  {
    "text": "So really, you take\nall three of them, but it's really just\nshowing you all choices",
    "start": "2429660",
    "end": "2435240"
  },
  {
    "text": "of pairs of those guys. So if you were to\nkeep k is equal to 5,",
    "start": "2435240",
    "end": "2441960"
  },
  {
    "text": "you would have five,\nchoose two different plots. ",
    "start": "2441960",
    "end": "2448540"
  },
  {
    "text": "So this is the actual\nprincipal component algorithm, how it's implemented.",
    "start": "2448540",
    "end": "2453640"
  },
  {
    "text": "And it's actually fairly simple. I mean, it looks like\nthere's lots of steps. But really, there's only\none that's important. So the first one is the input.",
    "start": "2453640",
    "end": "2459850"
  },
  {
    "text": "I give you a bunch of points,\nx1 to xn in d dimensions.",
    "start": "2459850",
    "end": "2464860"
  },
  {
    "text": "And step two is, well, compute\ntheir empirical covariance matrix S. The points themselves,\nwe don't really care.",
    "start": "2464860",
    "end": "2470569"
  },
  {
    "text": "We care about their\nempirical covariance matrix. So it's a d by d matrix. Now, I'm going to feed that.",
    "start": "2470570",
    "end": "2475750"
  },
  {
    "text": "And that's where the actual\ncomputation starts happening. I'm going to feed that\nto something that knows how to diagonalize this matrix.",
    "start": "2475750",
    "end": "2481089"
  },
  {
    "text": "And you have to\ntrust me, if I want to compute the k\nlargest eigenvalues and my matrix is\nd by d, it's going",
    "start": "2481090",
    "end": "2487960"
  },
  {
    "text": "to take me about k times\nd squared operations. So if I want only three,\nit's 3 times d squared,",
    "start": "2487960",
    "end": "2494980"
  },
  {
    "text": "which is about-- d squared is the time for me\nit takes to just even read the matrix sigma.",
    "start": "2494980",
    "end": "2501040"
  },
  {
    "text": "So that's not too bad. So what it's going to\nspit out, of course, is the diagonal matrix\nD. And those are nice,",
    "start": "2501040",
    "end": "2508230"
  },
  {
    "text": "because they allow\nme to tell me what",
    "start": "2508230",
    "end": "2513720"
  },
  {
    "text": "is the order in which I should\nbe taking the columns of P. But what's really important\nto me is v1 to vd,",
    "start": "2513720",
    "end": "2518930"
  },
  {
    "text": "because those are going to be\nthe ones I'm going to be using to draw those plots.",
    "start": "2518930",
    "end": "2524250"
  },
  {
    "text": "And now, I'm going\nto say, OK, I need to actually choose some set k. And I'm going to basically\ntruncate and look",
    "start": "2524250",
    "end": "2531630"
  },
  {
    "text": "only at the first\nk columns of P. Once I have those\ncolumns, what I",
    "start": "2531630",
    "end": "2538299"
  },
  {
    "text": "want to do is to project\nonto the linear span of those columns. And there's actually\na simple way",
    "start": "2538300",
    "end": "2543340"
  },
  {
    "text": "to do this, which is just take\nthis matrix P, which is really the matrix of projection onto\nthe linear span of those k",
    "start": "2543340",
    "end": "2549460"
  },
  {
    "text": "columns. And you just take Pk transpose. And then you apply this to\nevery single one of your points.",
    "start": "2549460",
    "end": "2558069"
  },
  {
    "text": "Now Pk transpose, what is\nthe size of the matrix Pk? ",
    "start": "2558070",
    "end": "2566410"
  },
  {
    "text": "Yeah, [INAUDIBLE]? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: So\nPk is just this matrix.",
    "start": "2566410",
    "end": "2572100"
  },
  {
    "text": "I take the v1 and I stop at vk-- well-- AUDIENCE: [INAUDIBLE]",
    "start": "2572100",
    "end": "2577656"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nd by k, right? Each of the column\nis an eigenvector. It's of dimension d.",
    "start": "2577656",
    "end": "2582840"
  },
  {
    "text": "I mean, that's a vector\nin the original space. So I have this d by k matrix. So all it is is if I had my--",
    "start": "2582840",
    "end": "2591359"
  },
  {
    "text": "well, I'm going to talk in\na second about Pk transpose. Pk transpose is\njust this guy, where",
    "start": "2591360",
    "end": "2597060"
  },
  {
    "text": "I stop at the k-th vector. So Pk transpose is k by d.",
    "start": "2597060",
    "end": "2602369"
  },
  {
    "text": "So now, when I take Yi,\nwhich is Pk transpose Xi, I end up with a point\nwhich is in k dimensions.",
    "start": "2602370",
    "end": "2609329"
  },
  {
    "text": "I have only k coordinates. So I took every single one\nof my original points Xi, which had d coordinates, and\nI turned it into a point that",
    "start": "2609330",
    "end": "2615780"
  },
  {
    "text": "has only k coordinates. Particularly, I could\nhave k is equal to 2. This matrix is exactly\nthe one that projects.",
    "start": "2615780",
    "end": "2622820"
  },
  {
    "text": "If you think about\nit for one second, this is just the\nmatrix that says-- well, we actually did\nthat several times.",
    "start": "2622820",
    "end": "2628609"
  },
  {
    "text": "The matrix, so that\nwas this P transpose u that showed up somewhere. And so that's just\nthe matrix that",
    "start": "2628610",
    "end": "2637460"
  },
  {
    "text": "take your point X in,\nsay, three dimensions, and then just project it\ndown to two dimensions.",
    "start": "2637460",
    "end": "2644750"
  },
  {
    "text": "And that's just-- it goes to the\nclosest point in the subspace. Now, here, the floor is flat.",
    "start": "2644750",
    "end": "2652650"
  },
  {
    "text": "But we can pick any\nsubspace we want, depending on what\nthe lambdas are.",
    "start": "2652650",
    "end": "2658310"
  },
  {
    "text": "So the lambdas were\nimportant for us to be able to identify\nwhich columns to pick.",
    "start": "2658310",
    "end": "2663609"
  },
  {
    "text": "The fact that we assumed\nthat they were ordered tells us that we can\npick the first ones. If they were not\nordered, it would be just a subset of the\ncolumns, depending on what",
    "start": "2663610",
    "end": "2670583"
  },
  {
    "text": "the size of the eigenvalue is. So each column is labeled.",
    "start": "2670583",
    "end": "2676509"
  },
  {
    "text": "And so then, of course, we\nstill have this question of, how do I pick k? So there's definitely the\nmatter of convenience.",
    "start": "2676509",
    "end": "2682760"
  },
  {
    "text": "Maybe 2 is convenient. If it works for 2, you don't\nhave to go any farther. But you might want\nto say, well--",
    "start": "2682760",
    "end": "2690680"
  },
  {
    "text": "originally, I did\nthat to actually keep as much information as possible. I know that the\nultimate thing is",
    "start": "2690680",
    "end": "2696230"
  },
  {
    "text": "to keep as much information,\nwhich would be to k is equal d-- that's as much\ninformation as you want. But it's essentially the\nsame question about, well,",
    "start": "2696230",
    "end": "2703310"
  },
  {
    "text": "if I want to compress\na JPEG image, how much information should\nI keep so it's still visible?",
    "start": "2703310",
    "end": "2710099"
  },
  {
    "text": "And so there's some\nrules for that. But none of them is\nactually really a science. So it's really a\nmatter of what you",
    "start": "2710100",
    "end": "2716599"
  },
  {
    "text": "think is actually tolerable. And we're just going to start\nreplacing this choice by maybe",
    "start": "2716600",
    "end": "2721970"
  },
  {
    "text": "another parameter. So here, we're going to\nbasically replace k by alpha, and so we just do stuff.",
    "start": "2721970",
    "end": "2729359"
  },
  {
    "text": "So the first one that\npeople do that is probably the most popular one-- OK, the most popular\none is definitely",
    "start": "2729360",
    "end": "2735860"
  },
  {
    "text": "take k is equal to 2\nor 3, because it's just convenient to visualize.",
    "start": "2735860",
    "end": "2741319"
  },
  {
    "text": "The second most popular\none is the scree plot.",
    "start": "2741320",
    "end": "2748050"
  },
  {
    "text": "So the scree plot-- remember, I have my\nvalues, lambda j's.",
    "start": "2748050",
    "end": "2754180"
  },
  {
    "text": "And I've chosen the\nlambda j's to decrease. So the indices are\nchosen in such a way",
    "start": "2754180",
    "end": "2759380"
  },
  {
    "text": "that lambda is a\ndecreasing function. So I have lambda 1, and\nlet's say it's this guy here. And then I have lambda 2, and\nlet's say it's this guy here.",
    "start": "2759380",
    "end": "2766790"
  },
  {
    "text": "And then I have lambda 3, and\nlet's say it's this guy here, lambda 4, lambda 5, lambda 6.",
    "start": "2766790",
    "end": "2772760"
  },
  {
    "text": "And all I care about is\nthat this thing decreases. The scree plot says\nsomething like this--",
    "start": "2772760",
    "end": "2779580"
  },
  {
    "text": "if there's an inflection point,\nmeaning that you can sort of do something like this and\nthen something like this,",
    "start": "2779580",
    "end": "2785230"
  },
  {
    "text": "you should stop at 3. That's what the\nscree plot tells you. What it's saying in a way\nis that the percentage",
    "start": "2785230",
    "end": "2794590"
  },
  {
    "text": "of the marginal\nincrement of explained variance that you get\nstarts to decrease after you",
    "start": "2794590",
    "end": "2801990"
  },
  {
    "text": "pass this inflection point. So let's see why I way this. Well, here, what I\nhave-- so this ratio",
    "start": "2801990",
    "end": "2812390"
  },
  {
    "text": "that you see there is\nactually the percentage of explained variance. So what it means is that, if I\nlook at lambda 1 plus lambda k,",
    "start": "2812390",
    "end": "2821590"
  },
  {
    "text": "and then I divide by lambda\n1 plus lambda d, well,",
    "start": "2821590",
    "end": "2828260"
  },
  {
    "text": "what is this? Well, this lambda\n1 plus lambda d is the total amount of variance\nthat I get in my points.",
    "start": "2828260",
    "end": "2834530"
  },
  {
    "text": "That's the trace of sigma. So that's the variance\nin the first direction",
    "start": "2834530",
    "end": "2840640"
  },
  {
    "text": "plus the variance in\nthe second direction plus the variance in\nthe third direction. That's basically all the\nvariance that I have possible.",
    "start": "2840640",
    "end": "2846571"
  },
  {
    "text": " Now, this is the variance that\nI kept in the first direction.",
    "start": "2846571",
    "end": "2852174"
  },
  {
    "text": "This is the variance that I\nkept in the second direction, all the way to the variance that\nI kept in the k-th direction.",
    "start": "2852175",
    "end": "2857190"
  },
  {
    "text": "So I know that this number is\nalways less than or equal to 1. And it's larger than 1.",
    "start": "2857190",
    "end": "2863540"
  },
  {
    "text": "And this is just\nthe proportion, say, of variance explained\nby v1 to vk,",
    "start": "2863540",
    "end": "2879520"
  },
  {
    "text": "or simply, the proportion of\nexplained variance by my PCA, say.",
    "start": "2879520",
    "end": "2885720"
  },
  {
    "text": "So now, what this\nthing is telling me, its says, well, if\nI look at this thing and I start seeing this\ninflection point, it's saying,",
    "start": "2885720",
    "end": "2893050"
  },
  {
    "text": "oh, here, you're gaining\na lot and lot of variance. And then at some point,\nyou stop gaining a lot",
    "start": "2893050",
    "end": "2899089"
  },
  {
    "text": "in your proportion of\nexplained variance. So this will\ntranslate in something where when I look at this ratio,\nlambda 1 plus lambda k divided",
    "start": "2899090",
    "end": "2908490"
  },
  {
    "text": "by lambda 1 plus\nlambda d, this would translate into a function\nthat would look like this.",
    "start": "2908490",
    "end": "2914195"
  },
  {
    "text": "And what it's telling you,\nit says, well, maybe you should stop here, because\nhere every time you add one, you don't get as much\nas you did before.",
    "start": "2914195",
    "end": "2920520"
  },
  {
    "text": "You actually get like\nsmaller marginal returns. ",
    "start": "2920520",
    "end": "2930910"
  },
  {
    "text": "So explained variance is\nthe numerator of this ratio.",
    "start": "2930910",
    "end": "2936630"
  },
  {
    "text": "And the total variance\nis the denominator. Those are pretty\nstraightforward terms that you would want\nto use for this.",
    "start": "2936630",
    "end": "2943320"
  },
  {
    "text": "So if your goal is to\ndo data visualization-- so why would you\ntake k larger than 2?",
    "start": "2943320",
    "end": "2950100"
  },
  {
    "text": "Let's say, if you\ntake k larger than 6, you can start to\nimagine that you're going to have six, choose two,\nwhich starts to be annoying.",
    "start": "2950100",
    "end": "2955364"
  },
  {
    "text": "And if you have k\nis equal to 10-- because you could start\nin dimension 50,000-- and then k equal to\n10 would be the place",
    "start": "2955364",
    "end": "2961080"
  },
  {
    "text": "where you have this thing\nthat's a lot of plots that you would have to show. So it's not always for\ndata visualization.",
    "start": "2961080",
    "end": "2966900"
  },
  {
    "text": "Once I've actually\ndone this, I've actually effectively reduced\nthe dimension of my problem.",
    "start": "2966900",
    "end": "2972460"
  },
  {
    "text": "And what I could do\nwith what I have is do a regression on those guys. The v1-- so I\nforgot to tell you--",
    "start": "2972460",
    "end": "2979010"
  },
  {
    "text": "why is that called principal\ncomponent analysis? Well, the vj's that\nI keep, v1 to vk",
    "start": "2979010",
    "end": "2986910"
  },
  {
    "text": "are called principal components.",
    "start": "2986910",
    "end": "2991932"
  },
  {
    "start": "2991932",
    "end": "2999019"
  },
  {
    "text": "And they effectively act\nas the summary of my Xi's.",
    "start": "2999020",
    "end": "3004690"
  },
  {
    "text": "When I mentioned\nimage compression, I started with a point\nXi that was d numbers--",
    "start": "3004690",
    "end": "3010840"
  },
  {
    "text": "let's say 50,000 numbers. And now, I'm saying,\nactually, you can throw out those\n50,000 numbers.",
    "start": "3010840",
    "end": "3016270"
  },
  {
    "text": "If you actually know only\nthe k numbers that you need-- the 6 numbers that you need-- you're going to\nhave something that",
    "start": "3016270",
    "end": "3022318"
  },
  {
    "text": "was pretty close to getting\nwhat information you had. So in a way, there is\nsome form of compression that's going on here.",
    "start": "3022318",
    "end": "3027810"
  },
  {
    "text": "And what you can do is that\nthose principal components, you can actually use\nnow for regression.",
    "start": "3027810",
    "end": "3034119"
  },
  {
    "text": "If I want to regress\nY onto X that's",
    "start": "3034120",
    "end": "3039130"
  },
  {
    "text": "very high dimensional,\nbefore I do this, if I don't have enough points,\nmaybe what I can actually do",
    "start": "3039130",
    "end": "3044320"
  },
  {
    "text": "is to do principal\ncomponent analysis throughout my\nexercise, replace them",
    "start": "3044320",
    "end": "3049510"
  },
  {
    "text": "by those compressed versions,\nand do linear aggression on those guys. And that's called principal\ncomponent regression,",
    "start": "3049510",
    "end": "3055330"
  },
  {
    "text": "not surprisingly. And that's something\nthat's pretty popular. And you can do with k is\nequal to 10, for example. ",
    "start": "3055330",
    "end": "3063020"
  },
  {
    "text": "So for data visualization, I did\nnot find a Thanksgiving themed picture.",
    "start": "3063020",
    "end": "3068269"
  },
  {
    "text": "But I found one that\nhas turkey in it. Get it? ",
    "start": "3068270",
    "end": "3075310"
  },
  {
    "text": "So this is actually a\ngene data set that was--",
    "start": "3075310",
    "end": "3081820"
  },
  {
    "text": "so when you see\nsomething like this, you can imagine that someone\nhas been preprocessing",
    "start": "3081820",
    "end": "3087056"
  },
  {
    "text": "the hell out of this thing. This is not like, oh, I\ncollect data on 23andMe and I'm just going\nto run PCA on this.",
    "start": "3087056",
    "end": "3092670"
  },
  {
    "text": "It just doesn't\nhappen like that. And so what happened is that--\nso let's assume that this was",
    "start": "3092670",
    "end": "3098740"
  },
  {
    "text": "a bunch of preprocessed data,\nwhich are gene expression levels-- so 500,000 genes\namong 1,400 Europeans.",
    "start": "3098740",
    "end": "3107650"
  },
  {
    "text": "So here, I actually\nhave less observations than I have samples. And that's when you use\nprincipal component regression",
    "start": "3107650",
    "end": "3114880"
  },
  {
    "text": "most of the time, so\nit doesn't stop you. And then what you do is you say,\nOK, have those 500,000 genes",
    "start": "3114880",
    "end": "3121480"
  },
  {
    "text": "among-- so here, that means that\nthere's 1,400 points here.",
    "start": "3121480",
    "end": "3126760"
  },
  {
    "text": "And I actually take\nthose 500,000 directions. So each person has a vector\nof, say, 500,000 genes",
    "start": "3126760",
    "end": "3133347"
  },
  {
    "text": "that are attached to them. And I project them onto\ntwo dimensions, which should be extremely lossy.",
    "start": "3133347",
    "end": "3139380"
  },
  {
    "text": "I lose a lot of information. And indeed, I do, because\nI'm one of these guys.",
    "start": "3139380",
    "end": "3144790"
  },
  {
    "text": "And I'm pretty sure I'm very\ndifferent from this guy, even though probably from\nan American perspective,",
    "start": "3144790",
    "end": "3150070"
  },
  {
    "text": "we're all the same. But I think we have like\nslightly different genomes.",
    "start": "3150070",
    "end": "3155690"
  },
  {
    "text": "And so the thing is\nnow we have this-- so you see there's lots of\nSwiss that participate in this.",
    "start": "3155690",
    "end": "3161980"
  },
  {
    "text": "But actually, those two\nprincipal components recover sort of\nthe map of Europe. I mean, OK, again, this is\nactually maybe fine-grained",
    "start": "3161980",
    "end": "3170169"
  },
  {
    "text": "for you guys. But right here, there's\nPortugal and Spain, which are those colors. So here is color-coded.",
    "start": "3170169",
    "end": "3175450"
  },
  {
    "text": "And here is Turkey, of\ncourse, which we know has very different genomes.",
    "start": "3175450",
    "end": "3182230"
  },
  {
    "text": "So Turks are very\nat the boundary. So you can see all the greens. They stay very far apart\nfrom everything else.",
    "start": "3182230",
    "end": "3188560"
  },
  {
    "text": "And then the rest\nhere is pretty mixed. But it sort of recovers--\nif you look at the colors, it sort of recovers that.",
    "start": "3188560",
    "end": "3194500"
  },
  {
    "text": "So in a way, those two\nprincipal components are just the geographic feature. So if you insist to compress\nall the genomic information",
    "start": "3194500",
    "end": "3205570"
  },
  {
    "text": "of these people into two\nnumbers, what you're actually going to get is\nlongitude and latitude,",
    "start": "3205570",
    "end": "3211320"
  },
  {
    "text": "which is somewhat\nsurprising, but not so much if you think that's\nit's been preprocessed.",
    "start": "3211320",
    "end": "3217740"
  },
  {
    "start": "3217740",
    "end": "3223119"
  },
  {
    "text": "So what do you do\nbeyond practice? Well, you could try to\nactually study those things.",
    "start": "3223120",
    "end": "3230780"
  },
  {
    "text": "If you think about\nit for a second, we did not do any statistics. I talked to you about\nIID observations,",
    "start": "3230780",
    "end": "3237460"
  },
  {
    "text": "but we never used the fact\nthat they were independent. The way we typically\nuse independence is to have central\nlimit theorem, maybe.",
    "start": "3237460",
    "end": "3244270"
  },
  {
    "text": "I mentioned the fact that\nthe covariances of the word Gaussian would actually give me\nsomething which is independent.",
    "start": "3244270",
    "end": "3249519"
  },
  {
    "text": "We didn't care. This was a data analysis, data\nmining process that we did.",
    "start": "3249520",
    "end": "3256280"
  },
  {
    "text": "I give you points, and you just\nput them through the crank. There was an algorithm\nin six steps.",
    "start": "3256280",
    "end": "3261350"
  },
  {
    "text": "And you just put it through\nand that's what you got. Now, of course, there's some\nwork which studies says, OK,",
    "start": "3261350",
    "end": "3266940"
  },
  {
    "text": "if my data is actually generated\nfrom some process-- maybe, my points are multivariate\nGaussian with some structure",
    "start": "3266940",
    "end": "3273050"
  },
  {
    "text": "on the covariance-- how well am I recovering\nthe covariance structure? And that's where\nstatistics kicks in.",
    "start": "3273050",
    "end": "3278990"
  },
  {
    "text": "And that's where we stop. So this is actually a bit\nmore difficult to study.",
    "start": "3278990",
    "end": "3284730"
  },
  {
    "text": "But in a way, it's not\nentirely satisfactory, because we could work\nfor a couple of boards",
    "start": "3284730",
    "end": "3290320"
  },
  {
    "text": "and I would just basically\nsort of reverse engineer this and find some models under which\nit's a good idea to do that.",
    "start": "3290320",
    "end": "3297457"
  },
  {
    "text": "And what are those models? Well, those are the models\nthat sort of give you sort of prominent directions\nthat you want to find.",
    "start": "3297457",
    "end": "3303911"
  },
  {
    "text": "And it will say, yes, if you\nhave enough observations, you will find those\ndirections along which your data is elongated.",
    "start": "3303911",
    "end": "3310150"
  },
  {
    "text": "So that's essentially\nwhat you want to do. So that's exactly what\nthis thing is telling you.",
    "start": "3310150",
    "end": "3320660"
  },
  {
    "text": "So where does the\nstatistics lie from? Well, everything, remember--\nso actually that's",
    "start": "3320660",
    "end": "3326020"
  },
  {
    "text": "where Alana was confused--\nthe idea was to say, well, if I have a true\ncovariance matrix sigma",
    "start": "3326020",
    "end": "3332590"
  },
  {
    "text": "and I never really\nhave access to it, I'm just running PCA on the\nempirical covariance matrix,",
    "start": "3332590",
    "end": "3338870"
  },
  {
    "text": "how do those results relate? And this is something\nthat you can study.",
    "start": "3338870",
    "end": "3344269"
  },
  {
    "text": "So for example, if\nn goes to infinity and the number of points,\nyour dimension, is fixed,",
    "start": "3344270",
    "end": "3355840"
  },
  {
    "text": "then S goes to sigma\nin any sense you want. Maybe each entry is going\nto each entry of sigma,",
    "start": "3355840",
    "end": "3362860"
  },
  {
    "text": "for example. So S is a good estimator. We know that the\nempirical covariance is a consistent as the mater. And if d is fixed, this\nis actually not an issue.",
    "start": "3362860",
    "end": "3370230"
  },
  {
    "text": "So in particular, if you run\nPCA on the sample covariance matrix, you look\nat, say, v1, then",
    "start": "3370230",
    "end": "3376150"
  },
  {
    "text": "v1 is going to converge to the\nlargest eigenvector of sigma as n goes to infinity,\nbut for d fixed.",
    "start": "3376150",
    "end": "3383990"
  },
  {
    "text": "And that's a story that\nwe know since the '60s. More recently, people have\nstarted challenging this.",
    "start": "3383990",
    "end": "3390906"
  },
  {
    "text": "Because what's happening\nwhen you fix the dimension and let the sample\nsize go to infinity, you're certainly not\nallowing for this.",
    "start": "3390906",
    "end": "3398961"
  },
  {
    "text": "It's certainly not explaining\nto you anything about the fact when d is equal to 500,000\nand n is equal to 1,400.",
    "start": "3398961",
    "end": "3404512"
  },
  {
    "text": "Because when d is fixed\nand n goes to infinity, in particular, n is\nmuch larger than d, which is not the case here.",
    "start": "3404512",
    "end": "3410280"
  },
  {
    "text": "And so when n is much larger\nthan d, things go well. But if d is less than n,\nit's not clear what happens.",
    "start": "3410280",
    "end": "3417430"
  },
  {
    "text": "And particularly, if d is of the\norder of n, what's happening? So there's an entire theory\nin mathematics that's called",
    "start": "3417430",
    "end": "3424320"
  },
  {
    "text": "random matrix theory that\nstudies the behavior of exactly this question-- what is the\nbehavior of the spectrum--",
    "start": "3424320",
    "end": "3430769"
  },
  {
    "text": "the eigenvalues\nand eigenvectors-- of a matrix in which I put\nrandom numbers and I let--",
    "start": "3430770",
    "end": "3436470"
  },
  {
    "text": "so the matrix I'm interested\nin here is the matrix of X's. When I stack all my\nX's next to each other,",
    "start": "3436470",
    "end": "3441830"
  },
  {
    "text": "so that's a matrix of size,\nsay, d by n, so each column",
    "start": "3441830",
    "end": "3446940"
  },
  {
    "text": "is of size d, it's one person. And so I put them. And when I let the\nmatrix go to infinity, I let both d and n to infinity.",
    "start": "3446940",
    "end": "3453920"
  },
  {
    "text": "But I want the aspect ratio,\nd/n, to go to some constant. That's what they do.",
    "start": "3453920",
    "end": "3458940"
  },
  {
    "text": "And what's nice is that in the\nend, you have this constant-- let's call it gamma-- that shows up in\nall the asymptotics.",
    "start": "3458940",
    "end": "3464550"
  },
  {
    "text": "And then you can\nreplace it by d/n. And you know that you still have\na handle of both the dimension",
    "start": "3464550",
    "end": "3470520"
  },
  {
    "text": "and the sample size. Whereas, usually the dimension\ngoes away, as you let n go to infinity without having\ndimension going to infinity.",
    "start": "3470520",
    "end": "3477370"
  },
  {
    "text": "And so now, when\nthis happens, as soon as d/n goes to a\nconstant, you can show that essentially there's\nan angle between the largest",
    "start": "3477370",
    "end": "3487380"
  },
  {
    "text": "eigenvector of sigma and the\nlargest eigenvector of S, as n",
    "start": "3487380",
    "end": "3494460"
  },
  {
    "text": "and d go to infinity. There is always an\nangle-- you can actually write it explicitly. And it's an angle that\ndepends on this ratio, gamma--",
    "start": "3494460",
    "end": "3502240"
  },
  {
    "text": "the asymptotic ratio of d/n. And so there's been a lot of\nunderstanding how to correct,",
    "start": "3502240",
    "end": "3509392"
  },
  {
    "text": "how to pay attention to this. This creates some biases that\nwere sort of overlooked before. In particular, when\nI do this, this",
    "start": "3509392",
    "end": "3517470"
  },
  {
    "text": "is not the proportion\nof explained variance, when n and d are similar.",
    "start": "3517470",
    "end": "3522940"
  },
  {
    "text": "This is an estimated\nnumber computed from S. This is computed from S. All\nthese guys are computed from S.",
    "start": "3522940",
    "end": "3528030"
  },
  {
    "text": "So those are\nactually not exactly where you want them to be. And there's some nice work that\nallows you to recalibrate what",
    "start": "3528030",
    "end": "3534510"
  },
  {
    "text": "this ratio should be, how\nthis ratio should be computed, so it's a better\nrepresentative of what the proportion of explained\nvariance actually is.",
    "start": "3534510",
    "end": "3544680"
  },
  {
    "text": "So then, of course,\nthere's the question of-- so that's when d/n\ngoes to some constant.",
    "start": "3544680",
    "end": "3549869"
  },
  {
    "text": "So the best case--\nso that was '60s-- d is fixed and it's\nmuch larger than d.",
    "start": "3549870",
    "end": "3555040"
  },
  {
    "text": "And then random matrix theory\ntells you, well, d and n are sort of the same\norder of magnitude.",
    "start": "3555040",
    "end": "3560680"
  },
  {
    "text": "When they go to infinity, the\nratio goes to some constant. Think of it as being order 1. To be fair, if d is 100 times\nlarger than n, it still works.",
    "start": "3560680",
    "end": "3570440"
  },
  {
    "text": "And it depends on\nwhat you think what the infinity is at this point. But I think the random matrix\ntheory results are very useful.",
    "start": "3570440",
    "end": "3577880"
  },
  {
    "text": "But then even in\nthis case, I told you that the leading\neigenvector of S is actually an angle of the\nleading eigenvector of--",
    "start": "3577880",
    "end": "3588812"
  },
  {
    "text": "So what's happening is that-- ",
    "start": "3588812",
    "end": "3596970"
  },
  {
    "text": "so let's say that d/n\ngoes to some gamma. And what I claim is\nthat, if you look at--",
    "start": "3596970",
    "end": "3604470"
  },
  {
    "text": "so that's v1, that's the v1 of\nS. And then there's the v1 of-- so this should be of size 1.",
    "start": "3604470",
    "end": "3611759"
  },
  {
    "text": "So that's the v1 of sigma. Then those things are going\nto have an angle, which is some function of gamma. It's complicated, but\nthere's a function of gamma",
    "start": "3611760",
    "end": "3618670"
  },
  {
    "text": "that you can see there. And there's some models. When gamma goes\nto infinity, which",
    "start": "3618670",
    "end": "3624620"
  },
  {
    "text": "means that d is now\nmuch larger than n, this angle is 90\ndegrees, which means",
    "start": "3624620",
    "end": "3630860"
  },
  {
    "text": "that you're getting nothing. Yeah. AUDIENCE: If d is not\non your lower plane,",
    "start": "3630860",
    "end": "3637288"
  },
  {
    "text": "so like gamma is 0,\nis there still angle? PHILIPPE RIGOLLET: No,\nbut that's consistent--",
    "start": "3637289",
    "end": "3643780"
  },
  {
    "text": "the fact that it's\nconsistent when-- so the angle is a function-- AUDIENCE: d is not a\nconstant [INAUDIBLE]??",
    "start": "3643780",
    "end": "3649605"
  },
  {
    "text": " PHILIPPE RIGOLLET:\nd is not a constant? So if d is little of n?",
    "start": "3649605",
    "end": "3657090"
  },
  {
    "text": "Then gamma goes to 0 and\nf of gamma goes to 0. So f of gamma is\na function that--",
    "start": "3657090",
    "end": "3662490"
  },
  {
    "text": "so for example, if f of gamma-- this is the sine of the\nangle, for example--",
    "start": "3662490",
    "end": "3668960"
  },
  {
    "text": "then it's a function that starts\nat 0, and that goes like this. ",
    "start": "3668960",
    "end": "3675340"
  },
  {
    "text": "But as soon as gamma is\npositive, it goes away from 0. ",
    "start": "3675340",
    "end": "3680650"
  },
  {
    "text": "So now when gamma\ngoes to infinity, then this thing goes\nto a right angle, which",
    "start": "3680650",
    "end": "3686350"
  },
  {
    "text": "means I'm getting just junk. So this is not my\nleading eigenvector. So how do you do this? Well, just like\neverywhere in statistics,",
    "start": "3686350",
    "end": "3693849"
  },
  {
    "text": "you have to just make\nmore assumptions. You have to assume\nthat you're not looking for the leading\neigenvector or the direction",
    "start": "3693850",
    "end": "3699220"
  },
  {
    "text": "that carries the most variance. But you're looking, maybe,\nfor a special direction. And that's what\nsparse PCA is doing.",
    "start": "3699220",
    "end": "3704910"
  },
  {
    "text": "Sparse PCA is saying, I'm not\nlooking for any direction new that carries the most variance.",
    "start": "3704910",
    "end": "3710290"
  },
  {
    "text": "I'm only looking for a\ndirection new that is sparse. Think of it, for example, as\nhaving 10 non-zero coordinates.",
    "start": "3710290",
    "end": "3718460"
  },
  {
    "text": "So that's a lot of\ndirections still to look for. But once you do this,\nthen you actually",
    "start": "3718460",
    "end": "3725560"
  },
  {
    "text": "have not only--\nthere's a few things that actually you\nget from doing this. The first one is you\nactually essentially replace",
    "start": "3725560",
    "end": "3732160"
  },
  {
    "text": "d by k, which means\nthat n now just-- I'm sorry, let's say S\nnon-zero coefficients.",
    "start": "3732160",
    "end": "3738480"
  },
  {
    "text": "You replace d by S,\nwhich means that n only has to be much larger than S\nfor this thing to actually work.",
    "start": "3738480",
    "end": "3744740"
  },
  {
    "text": "Now, of course, you've\nset your goal weaker. Your goal is not to\nfind any direction, only a sparse direction.",
    "start": "3744740",
    "end": "3750360"
  },
  {
    "text": "But there's something\nvery valuable about sparse directions,\nis that they actually are interpretable. When I found the v--",
    "start": "3750360",
    "end": "3757810"
  },
  {
    "text": "let's say that the v\nthat I found before was 0.2, and then 0.9, and\nthen 1.1 minus 3, et cetera.",
    "start": "3757810",
    "end": "3768390"
  },
  {
    "text": "So that was the coordinates\nof my leading eigenvector in the original\ncoordinate system.",
    "start": "3768390",
    "end": "3774410"
  },
  {
    "text": "What does it mean? Well, it means that if\nI see a large number, that means that this\nv is very close--",
    "start": "3774410",
    "end": "3781609"
  },
  {
    "text": "so that's my original\ncoordinate system. Let's call it e1 and e2. So that's just 1,\n0; and then 0, 1.",
    "start": "3781610",
    "end": "3789230"
  },
  {
    "text": "Then clearly, from\nthe coordinates of v, I can tell if my v is like\nthis, or it's like this, or it's like this.",
    "start": "3789230",
    "end": "3795610"
  },
  {
    "text": "Well, I mean, they should\nall be of the same size. So I can tell if\nit's here or here or here, depending\non-- like here,",
    "start": "3795610",
    "end": "3804739"
  },
  {
    "text": "that means I'm going\nto see something where the Y-coordinate it much\nlarger than the X-coordinate. Here, I'm going to see something\nwhere the X-coordinate is much",
    "start": "3804739",
    "end": "3810960"
  },
  {
    "text": "larger than the Y-coordinate. And here, I'm going\nto see something where the X-coordinate\nis about the same size of the Y-coordinate.",
    "start": "3810960",
    "end": "3818390"
  },
  {
    "text": "So when things\nstarts to be bigger, you're going to have\nto make choices. What does it mean to be bigger--",
    "start": "3818390",
    "end": "3823900"
  },
  {
    "text": "when d is 100,000,\nI mean, the sum of the squares of those\nguys have to be equal to 1.",
    "start": "3823900",
    "end": "3831160"
  },
  {
    "text": "So they're all\nvery small numbers. And so it's hard for you to\ntell which one is a big number and which ones is\na small number. Why would you want to know this?",
    "start": "3831160",
    "end": "3837378"
  },
  {
    "text": "Because it's\nactually telling you that if v is very close to\ne1, then that means that e1--",
    "start": "3837378",
    "end": "3843219"
  },
  {
    "text": "in the case of the\ngene example, that would mean that e1 is the\ngene that's very important.",
    "start": "3843219",
    "end": "3848510"
  },
  {
    "text": "Maybe there's actually\njust two genes that explain those two things. And those are the genes\nthat have been picked up.",
    "start": "3848510",
    "end": "3854150"
  },
  {
    "text": "There's two genes that I\nencode geographic location, and that's it. And so it's very\nimportant for you",
    "start": "3854150",
    "end": "3859640"
  },
  {
    "text": "to be able to\ninterpret what v means. Where it has large\nvalues, it means that maybe it has large\nvalues for e1, e2, and e3.",
    "start": "3859640",
    "end": "3866689"
  },
  {
    "text": "And it means that it's a\ncombination of e1, e2, and e3. And now, you can\ninterpret, because you have only three variables to find.",
    "start": "3866689",
    "end": "3873150"
  },
  {
    "text": "And so sparse PCA\nbuilds that in. Sparse PCA says,\nlisten, I'm going",
    "start": "3873150",
    "end": "3879920"
  },
  {
    "text": "to want to have at most\n10 non-zero coefficients. And the rest, I want to be 0. I want to be able to be a\ncombination of at most 10",
    "start": "3879920",
    "end": "3887040"
  },
  {
    "text": "of my original variables. And now, I can do\ninterpretation.",
    "start": "3887040",
    "end": "3892740"
  },
  {
    "text": "So the problem\nwith sparse PCA is that it becomes very\ndifficult numerically to solve this problem.",
    "start": "3892740",
    "end": "3898320"
  },
  {
    "text": "I can write it. So the problem is simply\nmaximize the variance u",
    "start": "3898320",
    "end": "3905700"
  },
  {
    "text": "transpose, say, Su\nsubject to-- well, I wanted to have u2 equal to 1.",
    "start": "3905700",
    "end": "3912180"
  },
  {
    "text": "So that's the original PCA. But now, I also\nwant that the sum of the indicators of the\nuj that are not equal to 0",
    "start": "3912180",
    "end": "3919320"
  },
  {
    "text": "is at most, say, 10. This constraint is\nvery non-convex.",
    "start": "3919320",
    "end": "3926550"
  },
  {
    "text": "So I can relax it\nto a convex one like we did for\nlinear aggression.",
    "start": "3926550",
    "end": "3931720"
  },
  {
    "text": "But now, I've totally\nmessed up with the fact that I could use linear\nalgebra to solve this problem.",
    "start": "3931720",
    "end": "3937930"
  },
  {
    "text": "And so now, you have to go\nthrough much more complicated optimization techniques,\nwhich are called semidefinite\nprograms, which do not",
    "start": "3937930",
    "end": "3944350"
  },
  {
    "text": "scale well in high dimensions. And so you have to do\na bunch of tricks-- numerical tricks.",
    "start": "3944350",
    "end": "3949660"
  },
  {
    "text": "But there are some packages\nthat implements some heuristics or some other things--",
    "start": "3949660",
    "end": "3955140"
  },
  {
    "text": "iterative\nthresholding, all sorts of various numerical\ntricks that you can do. But the problem they are trying\nto solve is exactly this.",
    "start": "3955140",
    "end": "3961270"
  },
  {
    "text": "Among all directions that\nI have norm 1, of course, because it's the direction\nthat have at most, say, 10 non-zero coordinates, I want\nto find the one that maximizes",
    "start": "3961270",
    "end": "3969382"
  },
  {
    "text": "the empirical variance. ",
    "start": "3969382",
    "end": "3983030"
  },
  {
    "text": "Actually, let me let\nme just so you this. ",
    "start": "3983030",
    "end": "4001910"
  },
  {
    "text": "I wanted to show\nyou an output of PCA",
    "start": "4001910",
    "end": "4007829"
  },
  {
    "text": "where people are actually\ntrying to do directly-- ",
    "start": "4007830",
    "end": "4016043"
  },
  {
    "text": "maybe-- there you go.",
    "start": "4016043",
    "end": "4025903"
  },
  {
    "start": "4025903",
    "end": "4040700"
  },
  {
    "text": "So right here, you\nsee this is SPSS.",
    "start": "4040700",
    "end": "4046690"
  },
  {
    "text": "That's a statistical software. And this is an output\nthat was preprocessed",
    "start": "4046690",
    "end": "4053100"
  },
  {
    "text": "by a professional-- not preprocessed,\npost-processed. So that's something\nwhere they read PCA.",
    "start": "4053100",
    "end": "4058520"
  },
  {
    "text": "So what is the data? This is raw data\nabout you ask doctors",
    "start": "4058520",
    "end": "4063890"
  },
  {
    "text": "what they think of the\nbehavior of a particular sales representative for\npharmaceutical companies.",
    "start": "4063890",
    "end": "4069740"
  },
  {
    "text": "So pharmaceutical\ncompanies are trying to improve their sales force. And they're asking\ndoctors how would they",
    "start": "4069740",
    "end": "4076430"
  },
  {
    "text": "rate-- what do they value\nabout their interaction with a sales representative. So basically, there's\na bunch of questions.",
    "start": "4076430",
    "end": "4084140"
  },
  {
    "text": "One offers credible point\nof view on something trends,",
    "start": "4084140",
    "end": "4090410"
  },
  {
    "text": "provides valuable\nnetworking opportunities. This is one question. Rate this on a\nscale from 1 to 5.",
    "start": "4090410",
    "end": "4095750"
  },
  {
    "text": "That was the question. And they had a bunch\nof questions like this. And then they asked 1,000\ndoctors to make those ratings.",
    "start": "4095750",
    "end": "4102409"
  },
  {
    "text": "And what they want--\nso each doctor now is a vector of ratings. And they want to know if there's\ndifferent groups of doctors,",
    "start": "4102410",
    "end": "4108960"
  },
  {
    "text": "what do doctors respond to. If there's different\ngroups, then maybe they know that they\ncan actually address them separately, et cetera.",
    "start": "4108960",
    "end": "4115500"
  },
  {
    "text": "And so to do that, of course,\nthere's lots of questions. And so what you want is\nto just first project into lower dimensions,\nso you can actually",
    "start": "4115500",
    "end": "4121588"
  },
  {
    "text": "visualize what's going on. And this is what\nwas done for this. So these are the three\nfirst principal component",
    "start": "4121589",
    "end": "4127489"
  },
  {
    "text": "that came out. And even though we ordered\nthe values of the lambdas, there's no reason why the\nentries of v should be ordered.",
    "start": "4127490",
    "end": "4136130"
  },
  {
    "text": "And if you look at\nthe values of v here, they look like they're\npretty much ordered. It starts at 0.784, and then\nyou're at 0.3 around here.",
    "start": "4136130",
    "end": "4144142"
  },
  {
    "text": "There's something that goes up\nagain, and then you go down. Actually, it's marked in red\nevery time it goes up again.",
    "start": "4144142",
    "end": "4151200"
  },
  {
    "text": "And so now, what they\ndid is they said, OK, I need to\ninterpret those guys.",
    "start": "4151200",
    "end": "4156270"
  },
  {
    "text": "I need to tell you what this is. If you tell me, we found\nthe principal component that really discriminates\nthe doctors in two groups,",
    "start": "4156270",
    "end": "4164866"
  },
  {
    "text": "the drug company is\ngoing to come back to you and say, OK, what is\nthis characteristic? And you say, oh, it's\nactually a linear combination",
    "start": "4164866",
    "end": "4171509"
  },
  {
    "text": "of 40 characteristics. And they say, well, we\ndon't need you to do that. I mean, it cannot be a linear\ncombination of anything you",
    "start": "4171510",
    "end": "4178109"
  },
  {
    "text": "didn't ask. And so for that,\nfirst of all, there's a post-processing of PCA, which\nsays, OK, once I actually,",
    "start": "4178109",
    "end": "4184859"
  },
  {
    "text": "say, found three\nprincipal components, that means that I found the\ndimension three space on which",
    "start": "4184859",
    "end": "4191370"
  },
  {
    "text": "I want to project my points. In this base, I can pick\nany direction I want. So the first thing\nis that you do",
    "start": "4191370",
    "end": "4197100"
  },
  {
    "text": "some sort of local arrangements,\nso that those things look like they are increasing\nand then decreasing. So you just change, you\nrotate your coordinate system",
    "start": "4197100",
    "end": "4206130"
  },
  {
    "text": "in this three dimensional space\nthat you've actually isolated. And so once you do\nthis, the reason",
    "start": "4206130",
    "end": "4211830"
  },
  {
    "text": "to do that is that\nit sort of makes them big, sharp differences\nbetween large and small values of the coordinates\nof the thing you had.",
    "start": "4211830",
    "end": "4218220"
  },
  {
    "text": "And why do you want this? Because now, you\ncan say, well, I'm going to start looking at the\nones that have large values.",
    "start": "4218220",
    "end": "4223590"
  },
  {
    "text": "And what do they say? They say in-depth knowledge,\nin-depth knowledge, in-depth knowledge,\nknowledge about. This thing is clearly\nsomething that",
    "start": "4223590",
    "end": "4230280"
  },
  {
    "text": "actually characterizes\nthe knowledge of my sales representative. And so that's something that\ndoctors are sensitive to.",
    "start": "4230280",
    "end": "4238310"
  },
  {
    "text": "That's something that\nreally discriminates the doctors in a way. There's lots of variance\nalong those things, or at least a lot of variance--",
    "start": "4238311",
    "end": "4245576"
  },
  {
    "text": "I mean, doctors are separate\nin terms of their experience with respect to this. And so what they\ndid is said, OK,",
    "start": "4245576",
    "end": "4251102"
  },
  {
    "text": "all these guys, some of\nthose they have large values, but I don't know how\nto interpret them. And so I'm just going\nto put the first block,",
    "start": "4251102",
    "end": "4256890"
  },
  {
    "text": "and I'm going to call\nit medical knowledge, because all those things are\nknowledge about medical stuff. Then here, I didn't know\nhow to interpret those guys.",
    "start": "4256890",
    "end": "4263538"
  },
  {
    "text": "But those guys, there's a big\nclump of large coordinates, and they're about respectful\nof my time, listens, friendly",
    "start": "4263538",
    "end": "4270720"
  },
  {
    "text": "but courteous. This is all about the\nquality of interaction. So this block was actually\ncalled quality of interaction.",
    "start": "4270720",
    "end": "4277446"
  },
  {
    "text": "And then there\nwas a third block, which you can tell starts to\nbe spreading a little thin. There's just much less of them.",
    "start": "4277446",
    "end": "4282864"
  },
  {
    "text": "But this thing was\nactually called fair and critical opinion. And so now, you have three\ndiscriminating directions.",
    "start": "4282864",
    "end": "4290010"
  },
  {
    "text": "And you can actually\ngive them a name. Wouldn't it be beautiful if\nall the numbers in the gray box came non-zero and\nall the other numbers",
    "start": "4290010",
    "end": "4296699"
  },
  {
    "text": "came zero-- there\nwas no ad hoc choice. I mean, this is probably\nan afternoon of work to like scratch out\nall these numbers",
    "start": "4296700",
    "end": "4302850"
  },
  {
    "text": "and put all these\ncolor codes, et cetera. Whereas, you could just have\nsomething that tells you, OK, here are the non-zeros.",
    "start": "4302850",
    "end": "4309090"
  },
  {
    "text": "If you can actually make a story\naround why this group of thing actually makes sense, such\nas it is medical knowledge,",
    "start": "4309090",
    "end": "4314820"
  },
  {
    "text": "then good for you. Otherwise, you could\njust say, I can't. And that's what sparse\nPCA does for you. Sparse PCA outputs something\nwhere all those numbers would",
    "start": "4314820",
    "end": "4322890"
  },
  {
    "text": "be zero. And there would be exactly,\nsay, 10 non-zero coordinates. And you can turn\nthis knob off 10.",
    "start": "4322890",
    "end": "4328380"
  },
  {
    "text": "You can make it 9.  Depending on what\nyour major is, maybe you can actually go\non with 20 of them",
    "start": "4328380",
    "end": "4335310"
  },
  {
    "text": "and have the ability to\ntell the story about 20 different variables and how\nthey fit in the same group.",
    "start": "4335310",
    "end": "4340650"
  },
  {
    "text": "And depending on\nhow you feel, it's easy to rerun the PCA\ndepending on the value that you want here.",
    "start": "4340650",
    "end": "4346535"
  },
  {
    "text": "And so you could actually\njust come up with the one you prefer. And so that's the\nsparse PCA thing",
    "start": "4346535",
    "end": "4352354"
  },
  {
    "text": "which I'm trying to promote. I mean, this is not\nsuper well-spread. It's a fairly new idea,\nmaybe at most 10 years old.",
    "start": "4352354",
    "end": "4359300"
  },
  {
    "text": "And it's not\ncompletely well-spread in statistical packages. But that's clearly\nwhat people are trying to emulate currently.",
    "start": "4359300",
    "end": "4366601"
  },
  {
    "text": "Yes? AUDIENCE: So what\nexactly does it mean that the doctors\nhave a lot of variance in medical knowledge,\nquality of interaction,",
    "start": "4366601",
    "end": "4373100"
  },
  {
    "text": "and fair and critical opinion? Like, it was saying that\nthese are like the main things",
    "start": "4373100",
    "end": "4380200"
  },
  {
    "text": "that doctors vary on,\nsome doctors care. Like we could sort of\ncharacterize a doctor by, oh,",
    "start": "4380200",
    "end": "4385590"
  },
  {
    "text": "he cares this much about\nmedical knowledge, this much about the quality\nof interaction, and this much about\ncritical opinion.",
    "start": "4385590",
    "end": "4391446"
  },
  {
    "text": "And that says most of the story\nabout what this doctor wants from a drug representative?",
    "start": "4391446",
    "end": "4397790"
  },
  {
    "text": "PHILIPPE RIGOLLET: Not really. I mean, OK, let's say\nyou pick only one. So that means that you\nwould take all your doctors,",
    "start": "4397790",
    "end": "4411534"
  },
  {
    "text": "and you would have\none direction, which is quality of interaction. And there would be just\nspread out points here.",
    "start": "4411535",
    "end": "4418710"
  },
  {
    "text": " So there are two\nthings that can happen.",
    "start": "4418710",
    "end": "4424270"
  },
  {
    "text": "The first one is that\nthere's a clump here, and then there's a clump here. That still represents\na lot of variance.",
    "start": "4424270",
    "end": "4430680"
  },
  {
    "text": "And if this happens,\nyou probably want to go back in\nyour data and see were these people visited\nby a different group",
    "start": "4430680",
    "end": "4438540"
  },
  {
    "text": "than these people,\nor maybe these people have a different specialty. ",
    "start": "4438540",
    "end": "4445250"
  },
  {
    "text": "I mean, you have to\nlook back at your data and try to understand\nwhy you would have different groups of people. And if it's like completely\nevenly spread out,",
    "start": "4445250",
    "end": "4453510"
  },
  {
    "text": "then all it's saying\nis that, if you want to have a uniform\nquality of interaction, you need to take\nmeasures on this.",
    "start": "4453510",
    "end": "4460410"
  },
  {
    "text": "You need to have this to\nnot be discrimination. But I think really when it's\nbecoming interesting it's not",
    "start": "4460410",
    "end": "4466530"
  },
  {
    "text": "when it's complete spread out. It's when there's\na big group here. And then there's\nalmost no one here, and then there's\na big group here.",
    "start": "4466530",
    "end": "4472020"
  },
  {
    "text": "And then maybe there's\nsomething you can do. And so those two things actually\ngive you a lot of variance.",
    "start": "4472020",
    "end": "4480690"
  },
  {
    "text": "So actually, maybe\nI'll talk about this.",
    "start": "4480690",
    "end": "4487489"
  },
  {
    "text": "Here, this is sort of a mixture. You have a mixture of\ntwo different populations of doctors.",
    "start": "4487490",
    "end": "4493040"
  },
  {
    "text": "And it turns out that\nprincipal component analysis-- so a mixture is when you\nhave different populations--",
    "start": "4493040",
    "end": "4499750"
  },
  {
    "text": "think of like two\nGaussians that are just centered at two\ndifferent points, and maybe they're\nin high dimensions.",
    "start": "4499750",
    "end": "4505460"
  },
  {
    "text": "And those are\nclusters of people, and you want to be able to\ndifferentiate those guys. If you're in very\nhigh dimensions,",
    "start": "4505460",
    "end": "4510770"
  },
  {
    "text": "it's going to be very\ndifficult. But one of the first processing tools\nthat people do is to do PCA. Because if you have one big\ngroup here and one big group",
    "start": "4510770",
    "end": "4518046"
  },
  {
    "text": "here, it means that\nthere's a lot of variance along the direction that\ngoes through the centers of those groups. And that's essentially\nwhat happened here.",
    "start": "4518046",
    "end": "4524630"
  },
  {
    "text": "You could think of this as being\ntwo blobs in high dimensions. But you're really\njust projecting them into one dimension.",
    "start": "4524630",
    "end": "4530809"
  },
  {
    "text": "And this dimension, hopefully,\ngoes through the center. And so as preprocessing--\nso I'm going to stop here.",
    "start": "4530810",
    "end": "4537460"
  },
  {
    "text": "But PCA is not just made\nfor dimension reduction.",
    "start": "4537460",
    "end": "4542719"
  },
  {
    "text": "It's used for\nmixtures, for example. It's also used when you\nhave graphical data. What is the idea of PCA?",
    "start": "4542720",
    "end": "4548750"
  },
  {
    "text": "It just says, if you have a\nmatrix that seems to have low rank-- meaning that there's a\nlot of those lambda i's that",
    "start": "4548750",
    "end": "4556370"
  },
  {
    "text": "are very small-- and then I see that\nplus noise, then it's a good idea to\ndo PCA on this thing.",
    "start": "4556370",
    "end": "4562790"
  },
  {
    "text": "And in particular, people\nuse that in networks a lot. So you take the adjacency\nmatrix of a graph--",
    "start": "4562790",
    "end": "4568300"
  },
  {
    "text": "well, you sort of preprocess it\na little bit, so it looks nice. And then if you have, for\nexample, two communities",
    "start": "4568300",
    "end": "4573590"
  },
  {
    "text": "in there, it should\nlook like something that is low rank plus some noise. And low rank means that there's\njust very few non-zero--",
    "start": "4573590",
    "end": "4582670"
  },
  {
    "text": "well, low rank means this. Low rank means that if\nyou do the scree plot, you will see\nsomething like this, which means that if you throw\nout all the smaller ones,",
    "start": "4582670",
    "end": "4589720"
  },
  {
    "text": "it should not really matter\nin the overall structure. And so you can use all--",
    "start": "4589720",
    "end": "4595429"
  },
  {
    "text": "these techniques are used\neverywhere these days, not just in PCA. So we call it PCA\nas statisticians.",
    "start": "4595430",
    "end": "4601670"
  },
  {
    "text": "But people call it the\nspectral methods or SVD.",
    "start": "4601670",
    "end": "4606699"
  },
  {
    "text": "So everyone-- ",
    "start": "4606700",
    "end": "4612248"
  }
]