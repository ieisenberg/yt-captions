[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6930"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6930",
    "end": "13410"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13410",
    "end": "18760"
  },
  {
    "text": " PROFESSOR: So, what I'll talk\nabout here is how to actually",
    "start": "18760",
    "end": "25100"
  },
  {
    "text": "understand the performance of\nyour application, and what are some of the things you can do\nto actually improve your",
    "start": "25100",
    "end": "30860"
  },
  {
    "text": "performance. You're going to hear more about\nautomated optimizations, compile the optimizations\non Monday.",
    "start": "30860",
    "end": "38809"
  },
  {
    "text": "There will be two\ntalks on that. You'll get some cell-specific\noptimizations that you can do, so some cell-specific tricks on\nTuesday in the recitation.",
    "start": "38810",
    "end": "49010"
  },
  {
    "text": "So here, this is meant to be a\nmore general purpose talk on how can you debug performance\nanomalies and performance",
    "start": "49010",
    "end": "54809"
  },
  {
    "text": "problems. Then what are some\nway that you can actually improve the performance. Where do you look after you've\ndone your parallelization.",
    "start": "54810",
    "end": "62410"
  },
  {
    "text": "So just to review the key\nconcepts to parallelism.",
    "start": "62410",
    "end": "67680"
  },
  {
    "text": "Coverage -- how much parallelism\ndo you have in your application. All of you know, because\nyou all had perfect",
    "start": "67680",
    "end": "73900"
  },
  {
    "text": "scores on the last quiz. So that means if you take a look\nat your program, you find the parallel parts and that\ntells you how much parallelism",
    "start": "73900",
    "end": "81200"
  },
  {
    "text": "that you have. If you don't\nhave more than a certain fraction, there's really nothing\nelse you can do with parallelism.",
    "start": "81200",
    "end": "86240"
  },
  {
    "text": "So the rest of the talk will\nhelp you address the question of well, where do you go\nfor last frontier.",
    "start": "86240",
    "end": "91979"
  },
  {
    "text": "The granularity. We talked about how the\ngranularity of your work and how much work are you doing on\neach processor affects your",
    "start": "91980",
    "end": "99360"
  },
  {
    "text": "load balancing, and how it\nactually affects your communication costs. If you have a lot of things\ncolocate on a single",
    "start": "99360",
    "end": "105510"
  },
  {
    "text": "processor, then you don't have\nto do a whole lot of communication across processors,\nbut if you",
    "start": "105510",
    "end": "110960"
  },
  {
    "text": "distribute things at a finer\nlevel, then you're doing a whole lot of communication. So we'll look at the\ncommunication costs again and",
    "start": "110960",
    "end": "116990"
  },
  {
    "text": "some tricks that you can\napply to optimize that. Then the last thing that we had\ntalked about in one of the",
    "start": "116990",
    "end": "124360"
  },
  {
    "text": "previous lectures is locality,\nin locality of communication versus computation, and both\nof those are critical.",
    "start": "124360",
    "end": "131570"
  },
  {
    "text": "So we'll have some\nexamples of that. So just to review the\ncommunication cost model, so I",
    "start": "131570",
    "end": "138292"
  },
  {
    "text": "had flashed up on the screen a\nwhile ago this equation that captures all the factors that\ngo into figuring out how",
    "start": "138292",
    "end": "147650"
  },
  {
    "text": "expensive is it to actually send\ndata from one processor to the other. Or this could even apply on\na single machine where a",
    "start": "147650",
    "end": "155640"
  },
  {
    "text": "processor is talking\nto the memory -- you know, loads and stores. The same cost model really\napplies there.",
    "start": "155640",
    "end": "161690"
  },
  {
    "text": "If you look at how\nprocessors -- a uniprocessor tries to improve\ncommunication, and some of the things we've\nmentioned really early on in",
    "start": "161690",
    "end": "168275"
  },
  {
    "text": "the course for improving\ncommunication costs, what we focused on is this overlap. There are things you can do,\nfor example, sending fewer",
    "start": "168275",
    "end": "175010"
  },
  {
    "text": "messages, optimizing how you're\npacking data into your messages, reducing the cost of\nthe network in terms of the",
    "start": "175010",
    "end": "182450"
  },
  {
    "text": "latency, using architecture\nsupport, increasing the bandwidth and so on.",
    "start": "182450",
    "end": "188620"
  },
  {
    "text": "But really the biggest impact\nthat you can get is really just from overlap because you\nhave direct control over that, especially in parallel\nprogramming.",
    "start": "188620",
    "end": "196490"
  },
  {
    "text": "So let's look at a small review\n-- you know, what did it mean to overlap.",
    "start": "196490",
    "end": "202510"
  },
  {
    "text": "So we had some synchronization\npoint or some point in the execution and then\nwe get data.",
    "start": "202510",
    "end": "208300"
  },
  {
    "text": "Then once the data's arrived,\nwe compute on that data. So this could be\na uniprocessor.",
    "start": "208300",
    "end": "213710"
  },
  {
    "text": "A CPU issues a load, it goes\nout to memory, memory sends back the data, and then the\nCU can continue operating.",
    "start": "213710",
    "end": "222100"
  },
  {
    "text": "But the uniprocessors\ncan pipeline. They allow you to have\nmultiple loads going out to memory.",
    "start": "222100",
    "end": "227180"
  },
  {
    "text": "So you can get the effect of\nhiding over overlapping a lot of that communication latency.",
    "start": "227180",
    "end": "233730"
  },
  {
    "text": "But there are limits to the\ncommunication, to the pipelining effects. If the work that you're doing is\nreally equal to the amount",
    "start": "233730",
    "end": "243435"
  },
  {
    "text": "of data that you're\nfetching, then you have really good overlap. So we went over this in the\nrecitation and we showed you",
    "start": "243435",
    "end": "248609"
  },
  {
    "text": "an example of where pipelining\ndoesn't have any performance effects and so you might not\nwant to do it because it doesn't give you the performance\nbang for the",
    "start": "248610",
    "end": "255730"
  },
  {
    "text": "complexity you invest in it. So, if things are really nicely\nmatched you get good",
    "start": "255730",
    "end": "260829"
  },
  {
    "text": "overlap, here you only get god\noverlap -- sorry, these, for some reason, should be\nshifted over one.",
    "start": "260830",
    "end": "266370"
  },
  {
    "text": "So where else do you look\nfor performance? So there are two kinds\nof communication.",
    "start": "266370",
    "end": "272350"
  },
  {
    "text": " There's inherent communication\nand your algorithm, and this is a result of how you actually\npartition your data",
    "start": "272350",
    "end": "281650"
  },
  {
    "text": "and how you partitioned\nyour computation. Then there's artifacts that come\nup because of the way you",
    "start": "281650",
    "end": "288170"
  },
  {
    "text": "actually do the implementation\nand how you map it to the architecture. So, if you have poor\ndistribution of data across",
    "start": "288170",
    "end": "295120"
  },
  {
    "text": "memory, then you might\nunnecessarily end up fetching data that you don't need.",
    "start": "295120",
    "end": "305370"
  },
  {
    "text": "So, you might also have\nredundant data fetchers. So let's talk about that\nin more detail. ",
    "start": "305370",
    "end": "311820"
  },
  {
    "text": "The way I'm going to do this\nis to draw from wisdom in uniprocessors. So in uniprocessors, CPUs\ncommunicate with memory, and",
    "start": "311820",
    "end": "319970"
  },
  {
    "text": "really conceptually, I think\nthat's no different than multiple processors talking\nto multiple processors. It's really all about where the\ndata is flowing and how",
    "start": "319970",
    "end": "328190"
  },
  {
    "text": "the memories are structured. So, loads and stores are the\nuniprocessor as what and what",
    "start": "328190",
    "end": "334190"
  },
  {
    "text": "are to distributed memory. So if you think of Cell, what\nwould go in those two blanks? ",
    "start": "334190",
    "end": "342819"
  },
  {
    "text": "Can you get this? ",
    "start": "342820",
    "end": "348360"
  },
  {
    "text": "I heard the answer there. You get input. So, DMA get and DMA put.",
    "start": "348360",
    "end": "353400"
  },
  {
    "text": "That's really just the\nload and a store. It's just doing, instead of\nloading one particular data element, you're loading a\nwhole chunk of memory.",
    "start": "353400",
    "end": "361460"
  },
  {
    "text": "So, on a uniprocessor, how do\nyou overlap communication? Well, architecture, the memory\nsystem is designed in a way to",
    "start": "361460",
    "end": "368210"
  },
  {
    "text": "exploit two properties that\nhave been observed in computation. Spacial locality and temporal\nlocality, and I'll look at",
    "start": "368210",
    "end": "374030"
  },
  {
    "text": "each one separately. So in spacial locality,\nCPU asks for a",
    "start": "374030",
    "end": "379640"
  },
  {
    "text": "data address of 1,000. What the memory does, it'll send\ndata address of 1,000,",
    "start": "379640",
    "end": "384650"
  },
  {
    "text": "plus a whole bunch of other data\nthat's neighboring to it, so 1,000 to 1,064.",
    "start": "384650",
    "end": "390290"
  },
  {
    "text": "Really, how much data you\nactually send, you know, what is the granularity of\ncommunication depends on",
    "start": "390290",
    "end": "402290"
  },
  {
    "text": "architectural parameters. So in common architecture it's\nreally the block side. So if you have a cache where the\norganization says you have",
    "start": "402290",
    "end": "409259"
  },
  {
    "text": "a block side to 32 words, 32\nbytes, then this is how much you transfer from main\nmemory to the caches.",
    "start": "409260",
    "end": "417210"
  },
  {
    "text": "So this works well when the CPU\nactually uses that data. If I send you 64 data bytes and\nI only use one of them,",
    "start": "417210",
    "end": "425530"
  },
  {
    "text": "then what have I done? I've wasted bandwidth. Plus, I need to store all that\nextra data in the cache so I've wasted my cache capacity.",
    "start": "425530",
    "end": "432240"
  },
  {
    "text": "So that's bad and you\nwant to avoid it. Temporal locality is\na clustering of",
    "start": "432240",
    "end": "439290"
  },
  {
    "text": "references in time. So if you access some particular\ndata element, then",
    "start": "439290",
    "end": "444470"
  },
  {
    "text": "what the memory assumes is\nyou're going reuse that data over and over and over again, so\nit stores it in the cache.",
    "start": "444470",
    "end": "449840"
  },
  {
    "text": "So your memory hierarchy has\nthe main memory at the top level, and that's your\nslowest memory but the biggest capacity.",
    "start": "449840",
    "end": "456410"
  },
  {
    "text": "Then as you get closer and\ncloser to the processor, you end up with smaller caches --\nthese are local, smaller",
    "start": "456410",
    "end": "462460"
  },
  {
    "text": "storage, but they're faster. So, if you reuse a data element\nthen it gets cached at",
    "start": "462460",
    "end": "468470"
  },
  {
    "text": "the lowest data level, and so\nthe assumption there is that you're gonna reuse it over\nand over and over again.",
    "start": "468470",
    "end": "473580"
  },
  {
    "text": "If you do that, then what\nyou've done is you've amortized the cost of bringing\nin that data over many, many references.",
    "start": "473580",
    "end": "478990"
  },
  {
    "text": "So that works out really well. But if you don't reuse that\nparticular data elements over and over again, then you've\nwasted cache capacity.",
    "start": "478990",
    "end": "486770"
  },
  {
    "text": "You still need to fetch the data\nbecause the CPU asks for it, but you might not have had\nthe cache, so that would have",
    "start": "486770",
    "end": "492080"
  },
  {
    "text": "created more space in your cache\nto have something else in there that might have\nbeen more useful.",
    "start": "492080",
    "end": "497350"
  },
  {
    "text": "So in the multiprocessor case,\nhow do you reduce these artifactual costs in\ncommunication.",
    "start": "497350",
    "end": "504180"
  },
  {
    "text": "So, DCMA gets inputs on the\ncell, or just in just message",
    "start": "504180",
    "end": "512750"
  },
  {
    "text": "passing, you're exchanging\nmessages. Typically, you're communicating\nover a course or",
    "start": "512750",
    "end": "519460"
  },
  {
    "text": "large blocks of data. What you're usually getting is\na continuous chunk of memory,",
    "start": "519460",
    "end": "524490"
  },
  {
    "text": "although you could do some\nthings in software or in hardware to gather data from\ndifferent memory locations and",
    "start": "524490",
    "end": "531000"
  },
  {
    "text": "pack them into contiguous\nlocations. The reason you pack them into\ncontiguous locations again to export spatial locality when\nyou store the data locally.",
    "start": "531000",
    "end": "539610"
  },
  {
    "text": "So to exploit the spatial\nlocality characteristics, what you want to make sure is that\nyou actually are going to have",
    "start": "539610",
    "end": "545890"
  },
  {
    "text": "good spatial locality in your\nactual computation. So you want things that are\niterating over loops with",
    "start": "545890",
    "end": "551130"
  },
  {
    "text": "well-defined indices with\nindices that go over very short ranges, or they're very\nsequential or have fixed",
    "start": "551130",
    "end": "558010"
  },
  {
    "text": "striped patterns where you're\nnot wasting a lot of the data that you have brought in. Otherwise, you have to\nessentially just increase your",
    "start": "558010",
    "end": "563730"
  },
  {
    "text": "communication because every\nfetch is getting you only a small fraction of what\nyou actually need. So, intuitively this\nshould make sense.",
    "start": "563730",
    "end": "571520"
  },
  {
    "text": "Temporal locality just says I\nbrought in some data and so I want to maximize this utility. So if I have any computation in\na parallel system, I might",
    "start": "571520",
    "end": "579519"
  },
  {
    "text": "be able to reorder my tasks in\na way that I have explicit control over the scheduling --\nwhich stripe executes when.",
    "start": "579520",
    "end": "585930"
  },
  {
    "text": "Then you want to make sure that\nall the computation that needs that particular data\nhappens adjacent in time or in",
    "start": "585930",
    "end": "592470"
  },
  {
    "text": "some short time window so that\nyou can amortize the cost. Are those two concepts clear?",
    "start": "592470",
    "end": "598860"
  },
  {
    "text": "Any questions on this? ",
    "start": "598860",
    "end": "603920"
  },
  {
    "text": "So, you've done all of that. You've parallelized your code,\nyou've taken care of your communication costs, you've\ntried to reduce",
    "start": "603920",
    "end": "610269"
  },
  {
    "text": "it as much as possible. Where else can you look for\nperformance -- things just don't look like they're\nperforming as",
    "start": "610270",
    "end": "616140"
  },
  {
    "text": "well as they could? So, the last frontier is\nperhaps single thread performance, so I'm going\nto talk about that.",
    "start": "616140",
    "end": "623380"
  },
  {
    "text": "So what is really\na single thread. So if you think of what you're\ndoing with parallel programming, you're taking a\nbunch of tasks -- this is the",
    "start": "623380",
    "end": "629340"
  },
  {
    "text": "work that you have to do -- and\nyou group them together into threads or the equivalent\nof threads, and some threads",
    "start": "629340",
    "end": "635800"
  },
  {
    "text": "will run on individual cores. So essentially you have one\nthread running on a core, and if that performance goes fast,\nthen your overall execution",
    "start": "635800",
    "end": "642570"
  },
  {
    "text": "can also benefit from that. So, that's single thread\nperformance. So if you look at a timeline,\nhere you have sequential code",
    "start": "642570",
    "end": "649920"
  },
  {
    "text": "going on, then we hit some\nparallel part of the computation. We have multiple executions\ngoing on.",
    "start": "649920",
    "end": "656259"
  },
  {
    "text": "Each one of these is a\nthread of execution. Really, my finish line depends\non who's the longest thread,",
    "start": "656260",
    "end": "663380"
  },
  {
    "text": "who's the slowest one to\ncomplete, and that's going to essentially control\nmy speed up.",
    "start": "663380",
    "end": "670300"
  },
  {
    "text": "So I can improve this by doing\nbetter load balancing. If I distribute the\nwork [? so that ?]",
    "start": "670300",
    "end": "677250"
  },
  {
    "text": "everybody's doing equivalent\namount of work, then I can shift that finish line\nearlier in time.",
    "start": "677250",
    "end": "682630"
  },
  {
    "text": "That can work reasonably well. So we talked about load\nbalancing before. We can also make execution\non each processor faster.",
    "start": "682630",
    "end": "690620"
  },
  {
    "text": "If each one of these threads\nfinishes faster or I've done the load balancing and now I\ncan even squeeze out more",
    "start": "690620",
    "end": "695800"
  },
  {
    "text": "performance by shrinking each\none of those lines, then I can get performance improvement\nthere as well.",
    "start": "695800",
    "end": "701459"
  },
  {
    "text": "So that's improving single\nthread performance. But how do we actually\nunderstand what's going on? How do I know where\nto optimize?",
    "start": "701460",
    "end": "708360"
  },
  {
    "text": "How do I know how long each\nthread is taking? How do I know how long\nmy program is taking? Where are the problems?",
    "start": "708360",
    "end": "714090"
  },
  {
    "text": "So, there are performance\nmonitoring tools that hard designed to help you do that. So what's the most\ncoarse-grained way of figuring",
    "start": "714090",
    "end": "722290"
  },
  {
    "text": "out how long your\nprogram took? You have some sample piece of\ncode shown over here, you",
    "start": "722290",
    "end": "728090"
  },
  {
    "text": "might compile it, and then you\nmight just use time -- standard units command say run\nthis program and tell me how",
    "start": "728090",
    "end": "735790"
  },
  {
    "text": "much time it took to run. So you get some alpha back from\ntime that said you took about two seconds of user time,\nthis is actual code, you",
    "start": "735790",
    "end": "743209"
  },
  {
    "text": "took some small amount of time\nin system code, and this is your overall execution, this is\nhow much of the processor",
    "start": "743210",
    "end": "749910"
  },
  {
    "text": "you actually use. So, a 95% utilization. Then you might apply\nsome optimization. So here we'll use the compiler,\nwe'll change the",
    "start": "749910",
    "end": "756430"
  },
  {
    "text": "optimization level, compile\nthe same code, run it, and we'll see wow, performance\nimproved. So we increased 99% utilization,\nmy running time",
    "start": "756430",
    "end": "764420"
  },
  {
    "text": "went down by a small chunk. But did we really learn\nanything about what's going on here?",
    "start": "764420",
    "end": "771100"
  },
  {
    "text": "There's some code going on,\nthere's a loop here, there's a loop here, there's some\nfunctions with more loops.",
    "start": "771100",
    "end": "776649"
  },
  {
    "text": "So where is the actual\ncomputation time going? So how would I actually go\nabout understanding this? So what are some tricks you\nmight have used in trying to",
    "start": "776650",
    "end": "784060"
  },
  {
    "text": "figure out how long something\ntook in your computation? AUDIENCE: [INAUDIBLE PHRASE]. ",
    "start": "784060",
    "end": "797010"
  },
  {
    "text": "PROFESSOR: Right. So you might have a timer, you\nrecord the time here, you",
    "start": "797010",
    "end": "802700"
  },
  {
    "text": "compute and then you stop the\ntimer and then you might printout or record how\nlong that particular block of code took.",
    "start": "802700",
    "end": "808959"
  },
  {
    "text": "Then you might have a histogram\nof them and then you might analyze the histogram to\nfind out the distribution. You might repeat this over and\nover again for many different",
    "start": "808960",
    "end": "815810"
  },
  {
    "text": "loops or many different\nparts that are code. If you have a preconceived\nnotion of where the problem is",
    "start": "815810",
    "end": "822320"
  },
  {
    "text": "then you instrument that\nand see if your hypothesis is correct. That can help you identify the\nproblems. But increasingly you",
    "start": "822320",
    "end": "829880"
  },
  {
    "text": "can actually get more accurate\nmeasurements. So, in the previous routine,\nyou're using the time, you were looking at how much time\nhas elapsed in seconds or in",
    "start": "829880",
    "end": "841490"
  },
  {
    "text": "small increments. But you can actually use\nhardware counters today to actually measure clock\ncycles, clock ticks.",
    "start": "841490",
    "end": "847620"
  },
  {
    "text": "That might be more useful. Actually, it's more useful\nbecause you can measure a lot more events than just\nclock ticks.",
    "start": "847620",
    "end": "853100"
  },
  {
    "text": " The counters in modern\narchitectures are really",
    "start": "853100",
    "end": "858839"
  },
  {
    "text": "specialized registers that count\nup events and then you can go in there and probe and\nask what is the value in this",
    "start": "858840",
    "end": "864890"
  },
  {
    "text": "register, and you can\nuse that as part of your performance tuning. You use them much in the same\nway as you would have done to",
    "start": "864890",
    "end": "872069"
  },
  {
    "text": "start a regular timer or\nstop a regular timer.",
    "start": "872070",
    "end": "877260"
  },
  {
    "text": "There are specialized\nlibraries that run. Unfortunately, these are very architecture-specific at this point. There's not really a common\nstandard that says grab a",
    "start": "877260",
    "end": "884940"
  },
  {
    "text": "timer at each different\narchitecture in a uniform way, although that's getting\nbetter with some standards coming out from--.",
    "start": "884940",
    "end": "892570"
  },
  {
    "text": "I'll talk about that in\njust a few slides. You can use this to, for\nexample, measure your communication to computation\ncost. So you can wrap your DMA",
    "start": "892570",
    "end": "900360"
  },
  {
    "text": "get and DMA put by timer calls,\nand you can measure your actual work by timer calls,\nand figuring out how",
    "start": "900360",
    "end": "907880"
  },
  {
    "text": "much overlap can you get from\noverlap in communication and communication computation and\nis that really worthwhile to",
    "start": "907880",
    "end": "914960"
  },
  {
    "text": "do pipelining. But this really requires\nmanual changes to code.",
    "start": "914960",
    "end": "920120"
  },
  {
    "text": "You have to go in there\nand start the timers. You have to have maybe an idea\nof where the problem is, and",
    "start": "920120",
    "end": "926320"
  },
  {
    "text": "you have the Heisenberg\neffect. If you have a loop and you want\nto measure code within the loop because you have a\nnested loop inside of that,",
    "start": "926320",
    "end": "932910"
  },
  {
    "text": "then now you're effecting the\nperformance of the outer loop. That can be problematic. So it has a better effect\nbecause you can't really make",
    "start": "932910",
    "end": "939110"
  },
  {
    "text": "an accurate measurement on the\nthing you're inspecting. So there's a slightly better\napproach, dynamic profiling.",
    "start": "939110",
    "end": "947100"
  },
  {
    "text": "Dynamic profiling is really\nthere's an event-based profiling and time-based\nprofiling. Conceptually they do\nthe same thing.",
    "start": "947100",
    "end": "953530"
  },
  {
    "text": "What's going on here is your\nprogram is running and you're going to say I'm interested in\nevents such as cache misses.",
    "start": "953530",
    "end": "961050"
  },
  {
    "text": "Whenever n number of cache\nmisses happen, let's say 1,000, let me know. So you get an interrupt\nwhenever 1,000",
    "start": "961050",
    "end": "967800"
  },
  {
    "text": "cache misses happen. Then you can update a counter\nor use that to trigger some",
    "start": "967800",
    "end": "973290"
  },
  {
    "text": "optimizations or analysis. This works really nicely because\nyou don't have to touch your code.",
    "start": "973290",
    "end": "979820"
  },
  {
    "text": "You essentially run your program\nas you normally do with just one modification\nthat includes running the",
    "start": "979820",
    "end": "986380"
  },
  {
    "text": "dynamic profiler, as well as\nyour actual computation. As far as multiple languages\nbecause all it does is just",
    "start": "986380",
    "end": "993610"
  },
  {
    "text": "takes your binary so you can\nprogram in any language, any programming model.",
    "start": "993610",
    "end": "998940"
  },
  {
    "text": "It's quite efficient to actually\nuse these dynamic profiling tools. The sampling frequencies are\nreasonably small, you can make",
    "start": "998940",
    "end": "1006770"
  },
  {
    "text": "them reasonably small and still\nhave it be efficient. So some counter examples.",
    "start": "1006770",
    "end": "1012010"
  },
  {
    "text": "Clock cycles, so you can\nmeasure clock ticks. Pipeline stalls. This might be interesting if\nyou want to optimize your",
    "start": "1012010",
    "end": "1018660"
  },
  {
    "text": "instruction schedule -- you'll\nactually see this in the recitation next week. Cache hits, cache misses -- you\ncan get an idea of how bad",
    "start": "1018660",
    "end": "1025560"
  },
  {
    "text": "your cache performance is and\nhow much time you're spending in the memory system. Number of instructions, loads,\nstores, floating",
    "start": "1025560",
    "end": "1031419"
  },
  {
    "text": "point ops and so on. Then you can derive some useful\nmeasures from that.",
    "start": "1031420",
    "end": "1036669"
  },
  {
    "text": "So I can get an idea of\nprocessor utilization -- divide cycles by time and that\ngives me utilization.",
    "start": "1036670",
    "end": "1042850"
  },
  {
    "text": "I can derive some other things\nand maybe some of the more interesting things like\nmemory of traffic.",
    "start": "1042850",
    "end": "1048350"
  },
  {
    "text": "So how much data am I actually\nsending between a CPU and a processor, or how much data am\nI communicating from one",
    "start": "1048350",
    "end": "1053480"
  },
  {
    "text": "processor to the other. So I can just grab the counters\nfor number of loads and number of stores, figure out\nwhat the cache line size",
    "start": "1053480",
    "end": "1060500"
  },
  {
    "text": "is -- usually those are\ndocumented or there are calibration tools you can run\nto get that value, and you figure out memory of traffic.",
    "start": "1060500",
    "end": "1066860"
  },
  {
    "text": "Another one would be\nbandwidth consumed. So bandwidth is memory of\ntraffic per second. So how would you measure that?",
    "start": "1066860",
    "end": "1073110"
  },
  {
    "text": " It's just the traffic divided\nby the wall clock time.",
    "start": "1073110",
    "end": "1079770"
  },
  {
    "text": "There's some others that\nyou can calculate. So these can be really useful\nin helping you figure out where are the things you\nshould go focus in on.",
    "start": "1079770",
    "end": "1086889"
  },
  {
    "text": "I'm going to show you\nsome examples. The way these tools work is\nyou have your application source code, you compile\nit down to a binary.",
    "start": "1086890",
    "end": "1094320"
  },
  {
    "text": "You take your binary and you\nrun it, and then that can generate some profile that\nstores locally on your disk.",
    "start": "1094320",
    "end": "1102200"
  },
  {
    "text": "Then you can take that profile\nand analyze it by some sort of interpreter, and some cases you\ncan actually analyze the",
    "start": "1102200",
    "end": "1108920"
  },
  {
    "text": "binary as well, and reannotate\nyour source code. That can actually be very useful\nbecause it'll tell you",
    "start": "1108920",
    "end": "1114390"
  },
  {
    "text": "this particular line of your\ncode is the one where you're spending most of your\ntime computing.",
    "start": "1114390",
    "end": "1119780"
  },
  {
    "text": "So some tools -- have any\nof you used these tools? Anybody use Gprof,\nfor example?",
    "start": "1119780",
    "end": "1125320"
  },
  {
    "text": "Good. So, you might have an idea of\nhow these could be used. There are others.",
    "start": "1125320",
    "end": "1130870"
  },
  {
    "text": "HPCToolkit, which I commonly\nuse from Rice. Pappy is very common because it\nhas a very nice interface",
    "start": "1130870",
    "end": "1137820"
  },
  {
    "text": "for grabbing all kinds\nof counters. VTune from Intel, and there's\nothers that work in different",
    "start": "1137820",
    "end": "1143010"
  },
  {
    "text": "ways and so there are binary\ninstrumenters that do the same things and do it slightly more\nefficiently and actually give",
    "start": "1143010",
    "end": "1148899"
  },
  {
    "text": "you the ability to compile\nyour code at run time and optimize it, taking advantage\nof the profiling information",
    "start": "1148900",
    "end": "1154230"
  },
  {
    "text": "you've collected. So here's a sample\nof running Gprof.",
    "start": "1154230",
    "end": "1160669"
  },
  {
    "text": "Gprof should be available on\nany Linux system, it's even available on Cygwin,\nif you see Cygwin.",
    "start": "1160670",
    "end": "1167080"
  },
  {
    "text": "I've compiled some code --\nthis is MPEG 2D code, a reference implementation. I specify some parameters\nto run it.",
    "start": "1167080",
    "end": "1173950"
  },
  {
    "text": "Here I add this dash Rflag which\nsays use a particular kind of inverse DCDT that's a\nfloating point precise that",
    "start": "1173950",
    "end": "1182010"
  },
  {
    "text": "uses double precision for\nthe floating point computations in DCT -- inverse DCT rather.",
    "start": "1182010",
    "end": "1188210"
  },
  {
    "text": "So you can see actually where\nmost of the time is being spent in the computation. So here's a time per function,\nso each row",
    "start": "1188210",
    "end": "1195270"
  },
  {
    "text": "represents a function. So this is the percent of the\ntime, this is the actual time",
    "start": "1195270",
    "end": "1205860"
  },
  {
    "text": "in seconds, how many times I\nactually called this function, and some other useful things. So the second function that's\nused here that happens is MPEG",
    "start": "1205860",
    "end": "1215920"
  },
  {
    "text": "intrablock decoding and here\nyou're doing some spatial decomposition, restoring\nspatial pictures by 5%.",
    "start": "1215920",
    "end": "1222730"
  },
  {
    "text": "So if you were optimize this\nparticular code, where would you go look? ",
    "start": "1222730",
    "end": "1229520"
  },
  {
    "text": "You would look in the\nreference DCT. So, MPEG has two versions\nof DCT --",
    "start": "1229520",
    "end": "1234899"
  },
  {
    "text": "one that uses floating point,\nanother that just uses some numerical tricks to operate over\nintegers for a loss of",
    "start": "1234900",
    "end": "1242240"
  },
  {
    "text": "precision, but they find that\nacceptable as part of this application.",
    "start": "1242240",
    "end": "1247670"
  },
  {
    "text": "So you omit the Rflag, it\nactually uses a different function for doing the DCT.",
    "start": "1247670",
    "end": "1253760"
  },
  {
    "text": "Now you see the distribution of\nwhere the time is spent in your computation changes. Now there's a new function\nthat's become the bottleneck",
    "start": "1253760",
    "end": "1260100"
  },
  {
    "text": "and it's called Form Component\nPrediction. Then IDCT column, which\nactually is the main",
    "start": "1260100",
    "end": "1266550"
  },
  {
    "text": "replacement of the previous\ncode, this one, is now about 1/3 of the actual computation.",
    "start": "1266550",
    "end": "1272250"
  },
  {
    "text": "So, this could be useful because\nyou can Gprof your application, figure out where\nthe bottlenecks are in terms",
    "start": "1272250",
    "end": "1277700"
  },
  {
    "text": "of performance, and you might\ngo in there and tweak the algorithm completely. You might go in there and sort\nof look at some problems that",
    "start": "1277700",
    "end": "1284260"
  },
  {
    "text": "might be implementation bugs\nor performance bugs and be able to fix those.",
    "start": "1284260",
    "end": "1290580"
  },
  {
    "text": "Any questions on that? You can do sort of more\naccurate things.",
    "start": "1290580",
    "end": "1296340"
  },
  {
    "text": "So, Gprof largely uses one\nmechanism, HPCToolkit uses the",
    "start": "1296340",
    "end": "1302419"
  },
  {
    "text": "performance counters to actually\ngive you more finer grade measurements\nif you want them.",
    "start": "1302420",
    "end": "1308809"
  },
  {
    "text": "So, in the HPCToolkit, you run\nyour program in the same way. You have MPEG 2D code, dash dash\njust says this is where",
    "start": "1308810",
    "end": "1317130"
  },
  {
    "text": "the parameters are to impact\n2D code following that dash dash, and you can add some\nparameters in there that say",
    "start": "1317130",
    "end": "1323290"
  },
  {
    "text": "these are counters I'm\ninterested in measuring. So the first one is\ntotal cycles. The second one is the L1, so\nprimary cache load misses.",
    "start": "1323290",
    "end": "1333180"
  },
  {
    "text": "Then you might want to count\nthe floating point instructions and the\ntotal instructions. As you run your program you\nactually get a profiling",
    "start": "1333180",
    "end": "1339490"
  },
  {
    "text": "output, and then you can process\nthat file and it'll spit out some summaries\nfor you.",
    "start": "1339490",
    "end": "1345220"
  },
  {
    "text": "So it'll tell you this is the\ntotal number of cycles, 698 samples with this frequency.",
    "start": "1345220",
    "end": "1350580"
  },
  {
    "text": "So if you multiply the two\ntogether, you an idea of how many cycles your computation\ntook.",
    "start": "1350580",
    "end": "1356590"
  },
  {
    "text": "How many load misses? So it's 27 samples at\nthis frequency. So remember what's going on\nhere is the counter is",
    "start": "1356590",
    "end": "1363040"
  },
  {
    "text": "measuring events, and when the\nevent reaches a particular threshold it let's you know. So here the sampling threshold\nis 32,000.",
    "start": "1363040",
    "end": "1372169"
  },
  {
    "text": "So whenever 32,000 floating\npoint instructions occur you get a sample. So you're just counting how many\ninterrupts you're getting",
    "start": "1372170",
    "end": "1377745"
  },
  {
    "text": "or how many samples. So you multiply the two\ntogether you can get the final counts.",
    "start": "1377745",
    "end": "1382770"
  },
  {
    "text": "It can do things like Gprof,\nit'll tell you where your time is and where you spent\nmost of your time.",
    "start": "1382770",
    "end": "1389580"
  },
  {
    "text": "Actually breaks it down\ninto your module. So, MPEG calls some standard\nlibraries, libsi.",
    "start": "1389580",
    "end": "1395620"
  },
  {
    "text": "I could break it down by\nfunctions, break it down by line number. You can even annotate\nyour source code.",
    "start": "1395620",
    "end": "1401800"
  },
  {
    "text": "So here's just a simple example\nthat I used earlier, and each one of these columns\nrepresent one of the metrics",
    "start": "1401800",
    "end": "1408960"
  },
  {
    "text": "that we measured, and you can\nsee most of my time is spent here, 36% at this particular\nstatement.",
    "start": "1408960",
    "end": "1415460"
  },
  {
    "text": "So that can be very useful. You can go in there and\nsay, I want to do some [? dization ?], I can maybe\nreduce this overhead in some",
    "start": "1415460",
    "end": "1423710"
  },
  {
    "text": "way to get better performance. Any questions on that? Yup. AUDIENCE: [INAUDIBLE PHRASE]?",
    "start": "1423710",
    "end": "1431210"
  },
  {
    "text": "PROFESSOR: I don't know.  Unfortunately, I don't know\nthe answer to that. ",
    "start": "1431210",
    "end": "1439600"
  },
  {
    "text": "There's some nice gooies for\nsome of these tools. So VTune has a nice interface. I use HPCViewer.",
    "start": "1439600",
    "end": "1446530"
  },
  {
    "text": "I use HPCToolkit, which provides\nHPCViewer, so I just grab the screenshot from one\nof the tutorials on this.",
    "start": "1446530",
    "end": "1454200"
  },
  {
    "text": "You have your source code. It shows you some of the same\ninformation I had on a previous slide, but in a\nnicer graphical format.",
    "start": "1454200",
    "end": "1461950"
  },
  {
    "text": "So, now I have all this\ninformation, how do I actually",
    "start": "1461950",
    "end": "1467710"
  },
  {
    "text": "improve the performance? Well, if you look at what is\nthe performance time on a uniprocessor, it's time spent\ncomputing plus the time spent",
    "start": "1467710",
    "end": "1476120"
  },
  {
    "text": "waiting for data or\nwaiting for some other things to complete. You have instructional level\nparallels, which is really",
    "start": "1476120",
    "end": "1482360"
  },
  {
    "text": "critical for uniprocessors,\nand architect that sort of spent massive amounts of effort\nin providing multiple",
    "start": "1482360",
    "end": "1489059"
  },
  {
    "text": "functional units,\ndeeply pipeline the instruction pipeline. Doing things like speculation,\nprediction to keep that",
    "start": "1489060",
    "end": "1496440"
  },
  {
    "text": "instructional level of\nparallelism number high so you can get really good\nperformance. You can do things like looking\nat the assembly code and",
    "start": "1496440",
    "end": "1503630"
  },
  {
    "text": "re-ordering instructions to\navoid instruction hazards in the pipeline. You might look at a register\nallocation.",
    "start": "1503630",
    "end": "1511500"
  },
  {
    "text": "But that's really very\nlow-hanging fruit. You have to reach really high\nto grab that kind of fruit.",
    "start": "1511500",
    "end": "1517910"
  },
  {
    "text": "You'll actually, unfortunately,\nget the experience that is part of\nthe next recitation. So apologies in advance.",
    "start": "1517910",
    "end": "1524020"
  },
  {
    "text": "But you'll see that --\nwell I'm not going to talk about that. Instead I'm going to focus about\nsome things that are perhaps lower-hanging fruit.",
    "start": "1524020",
    "end": "1530220"
  },
  {
    "text": "So data level parallelism. So we've used SIMD in\nsome of recitations.",
    "start": "1530220",
    "end": "1535925"
  },
  {
    "text": "I'm giving you a short\nexample of that. Here, I'm going to talk about\nhow you actually get data",
    "start": "1535925",
    "end": "1541190"
  },
  {
    "text": "level parallelism or how do you\nactually find the SIMD in your computation so you can\nget that added advantage.",
    "start": "1541190",
    "end": "1548520"
  },
  {
    "text": "Some nice things about data\nlevel parallelism in the form of short vector instructions\nis that the harder really",
    "start": "1548520",
    "end": "1554290"
  },
  {
    "text": "becomes simpler. You issue one instruction and\nthat same instruction operates over multiple data elements\nand you get",
    "start": "1554290",
    "end": "1561520"
  },
  {
    "text": "better instruction bandwidth. I just have to fetch one\ninstruction and if my vector lens is 10, than that\neffectively does 10",
    "start": "1561520",
    "end": "1567000"
  },
  {
    "text": "instructions for me. The architecture can get\nsimpler, reduces the complexity. So it has some nice\nadvantages.",
    "start": "1567000",
    "end": "1573880"
  },
  {
    "text": "The thing to go after is\nthe memory hierarchy. This is because of that speed\ngap that we showed earlier on",
    "start": "1573880",
    "end": "1580270"
  },
  {
    "text": "in the course between memory\nspeed and processor speed, and if you optimize a performance\nusually it's like 1%",
    "start": "1580270",
    "end": "1587180"
  },
  {
    "text": "performance and your cache\nregistry gives you some significant performance\nimprovement in",
    "start": "1587180",
    "end": "1593590"
  },
  {
    "text": "your overall execution. So you want to go after that\nbecause that's the biggest",
    "start": "1593590",
    "end": "1599230"
  },
  {
    "text": "beast in the room.  A brief overview of SIMD and\nthen some detailed examples as",
    "start": "1599230",
    "end": "1607230"
  },
  {
    "text": "to how you actually go about\nextracting short vector instructions. So, here we have an example\nof scaleacode.",
    "start": "1607230",
    "end": "1614700"
  },
  {
    "text": "We're iterating in a loop from\nzero to n, and we're just adding some array elements a to\nb and storing results in c.",
    "start": "1614700",
    "end": "1623330"
  },
  {
    "text": "So in the scaler mode, we just\nhave one add, each value of a and b is one register.",
    "start": "1623330",
    "end": "1629610"
  },
  {
    "text": "We add those together,\nwe write the value to a separate register. In the vector mode, we can pack\nmultiple data elements,",
    "start": "1629610",
    "end": "1637419"
  },
  {
    "text": "so here let's assume our vector\nlens is four, I can pack four of these data values\ninto one vector register.",
    "start": "1637420",
    "end": "1643419"
  },
  {
    "text": "You can pack four of these data\nelements into another vector register, and now my\nsingle vector instruction has",
    "start": "1643420",
    "end": "1649010"
  },
  {
    "text": "the effect of doing four ads\nat th same time, and it can store results into four\nelements of c.",
    "start": "1649010",
    "end": "1656940"
  },
  {
    "text": "Any questions on that? AUDIENCE: [UNINTELLIGIBLE] ",
    "start": "1656940",
    "end": "1664820"
  },
  {
    "text": "PROFESSOR: No. We'll get to that. ",
    "start": "1664820",
    "end": "1669850"
  },
  {
    "text": "So, let's look at those to sort\nof give you a more lower level feel for this.",
    "start": "1669850",
    "end": "1676030"
  },
  {
    "text": "Same code, I've just shown\ndata dependence graph. I've omitted things like the\nincrement of the loop and the",
    "start": "1676030",
    "end": "1684610"
  },
  {
    "text": "branch, just focusing on\nthe main computation. So I have two loads, one brings\na sub i, the other",
    "start": "1684610",
    "end": "1689890"
  },
  {
    "text": "brings b sub i. I do the add and I get c sub i\nand then I can store that. So that might be sort of a\ngeneric op code sequence that",
    "start": "1689890",
    "end": "1697490"
  },
  {
    "text": "you have. If you're scheduling\nthat, then in the first slot I can do those two loads in\nparallel, second cycle I can",
    "start": "1697490",
    "end": "1704840"
  },
  {
    "text": "do the add, third cycle\nI can do the store. I can further improve\nthis performance. If you took 6.035 you might see\nsoftware pipelining, you",
    "start": "1704840",
    "end": "1712409"
  },
  {
    "text": "can actually overlap some\nof these operations. Not really that important\nhere.",
    "start": "1712410",
    "end": "1717750"
  },
  {
    "text": "So, what would the cycle or the\nschedule look like on a cycle-by-cycle basis if this\nwas defector output?",
    "start": "1717750",
    "end": "1725669"
  },
  {
    "text": " In the scaler case, you have\nn iterations, right?",
    "start": "1725670",
    "end": "1731600"
  },
  {
    "text": "Each iteration's taking three\ncycles so that's your overall execution on time --\nn times 3 cycles.",
    "start": "1731600",
    "end": "1738769"
  },
  {
    "text": "In the vector case, each load\nis bringing you four data elements, so a sub i\nto a sub i plus 3.",
    "start": "1738770",
    "end": "1747550"
  },
  {
    "text": "Similarly for b. Then you add those together. So the schedule would look\nessentially the same.",
    "start": "1747550",
    "end": "1754059"
  },
  {
    "text": "The op codes are different,\nand here what your overall",
    "start": "1754060",
    "end": "1759210"
  },
  {
    "text": "execution time be? Well, what I've done is each\niteration is now doing four",
    "start": "1759210",
    "end": "1764530"
  },
  {
    "text": "additions for me. So if you notice, the loop\nbounds have changed. Instead of going from i to n\nby increments of 1, now I'm",
    "start": "1764530",
    "end": "1771400"
  },
  {
    "text": "going by increments of 4. So, overall, instead of having\nn iterations, I can get by",
    "start": "1771400",
    "end": "1777940"
  },
  {
    "text": "with n over 4 iterations. That make sense? So, what would my speed\nup be in this case?",
    "start": "1777940",
    "end": "1784110"
  },
  {
    "start": "1784110",
    "end": "1792830"
  },
  {
    "text": "4. So you can get more and more\nspeed up if your vector lens",
    "start": "1792830",
    "end": "1797990"
  },
  {
    "text": "is longer, because then I can\ncut down on the number of iterations that I need.",
    "start": "1797990",
    "end": "1804309"
  },
  {
    "text": "Depending on the length of my\nvector register and the data types that I have, that\neffectively gives me different",
    "start": "1804310",
    "end": "1809920"
  },
  {
    "text": "kinds of vector lens for\ndifferent data types. So you saw on Cell you have 128\nbit registers and you can",
    "start": "1809920",
    "end": "1815860"
  },
  {
    "text": "pack those with bytes,\ncharacters, bytes, shorts, integers, floats or doubles.",
    "start": "1815860",
    "end": "1822700"
  },
  {
    "text": "So each one of those gives you\ndifferent kinds of a different vector lens. ",
    "start": "1822700",
    "end": "1829750"
  },
  {
    "text": "SIMD is really now, SIMD\nextensions are increasingly popular. They're available on\na lot of ISAs.",
    "start": "1829750",
    "end": "1834990"
  },
  {
    "text": " Alt of x, MMX, SSE\nare available",
    "start": "1834990",
    "end": "1840080"
  },
  {
    "text": "on a lot x86 machines. And, of course, in Cell, in\nfact, on the SPU, all your instructions are SIMD\ninstruction, and when you're",
    "start": "1840080",
    "end": "1847000"
  },
  {
    "text": "doing a scaler instruction,\nyou're actually using just one chunk of your vector register\nand your vector pipeline.",
    "start": "1847000",
    "end": "1856310"
  },
  {
    "text": "So how do you actually use\nthese SIMD instructions? Unfortunately, it's library\ncalls or using inline assembly",
    "start": "1856310",
    "end": "1865190"
  },
  {
    "text": "or using intrinsics. You'll get hands-on experience\nwith this with Cell, so you",
    "start": "1865190",
    "end": "1870240"
  },
  {
    "text": "might complain about that\nwhen you actually do it. Compile technology is actually\ngetting better, and you'll see",
    "start": "1870240",
    "end": "1877860"
  },
  {
    "text": "that one of the reasons we're\nusing an XLC compiler is because it has these vector data\ntypes, which also latest",
    "start": "1877860",
    "end": "1885320"
  },
  {
    "text": "versions of GCC have that allow\nyou to express data types as vector data types,\nand the compiler can more",
    "start": "1885320",
    "end": "1891740"
  },
  {
    "text": "easily or more naturally get the\nparallelism for you, SIMD parallelism with you having to\ngo in there and do it by hand.",
    "start": "1891740",
    "end": "1898640"
  },
  {
    "text": "But if you were to do it by\nhand, or, in fact, what the compilers are trying to\nautomate are different techniques for looking for where\nthe SIMD parallelism is.",
    "start": "1898640",
    "end": "1908730"
  },
  {
    "text": "There was some work done here\nabout six years ago by Sam Larson, who is now graduated,\non super word level",
    "start": "1908730",
    "end": "1917300"
  },
  {
    "text": "parallelism. so I'm going to focus the rest\nof this talk on this concept of SIMDization because I think\nit's probably the one that's",
    "start": "1917300",
    "end": "1923169"
  },
  {
    "text": "most useful for extracting\nparallelism in some of the codes that you're doing. So this is really ideal for\nSIMD where you have really",
    "start": "1923170",
    "end": "1931150"
  },
  {
    "text": "short vector lens, 2 to 8. What you're looking for is\nSIMDization that exists within",
    "start": "1931150",
    "end": "1936750"
  },
  {
    "text": "a basic block. So within a code block, within\na body of a loop or within",
    "start": "1936750",
    "end": "1941780"
  },
  {
    "text": "some control flow even. You can uncover this with simple\nanalysis, and this",
    "start": "1941780",
    "end": "1946910"
  },
  {
    "text": "really has pushed the boundary\non what automatic compilers can do.",
    "start": "1946910",
    "end": "1953280"
  },
  {
    "text": "Some of work that's gone on\nat IMB, what they call the octipiler, has eventually been\ntransferred to the XLC",
    "start": "1953280",
    "end": "1958980"
  },
  {
    "text": "compiler do a lot of defecniques\nthat build on SLP and expand in various ways to\nbroaden the scope of what you",
    "start": "1958980",
    "end": "1966090"
  },
  {
    "text": "can automatically parallize. So here's an example of how\nyou might actually derive",
    "start": "1966090",
    "end": "1972200"
  },
  {
    "text": "SIMDization or opportunities\nfor SIMDization. So you have some code, let's\nsay you're doing RGB",
    "start": "1972200",
    "end": "1977280"
  },
  {
    "text": "computations where you're just\nadding the r elements, that's",
    "start": "1977280",
    "end": "1983190"
  },
  {
    "text": "a red, green and blue. So this might be in a loop, and\nwhat you might notice it",
    "start": "1983190",
    "end": "1989630"
  },
  {
    "text": "well I can pack the RGB elements\ninto one register. I can pack these into another\nregister, and I can pack these",
    "start": "1989630",
    "end": "1996190"
  },
  {
    "text": "literals into a third\nregister. So that gives me a way to pack\ndata together into SIMD",
    "start": "1996190",
    "end": "2003220"
  },
  {
    "text": "registers, and now I can replace\nthis scaler code with",
    "start": "2003220",
    "end": "2009150"
  },
  {
    "text": "instructions that pack\nthe vector register. I can do the computations\nin parallel and I can unpack them.",
    "start": "2009150",
    "end": "2014420"
  },
  {
    "text": "We'll talk about that with a\nlittle bit more illustration in a second. Any questions on this? ",
    "start": "2014420",
    "end": "2022720"
  },
  {
    "text": "Perhaps the biggest improvement\nthat you can get from SIMDization is by looking\nat adjacent memory references.",
    "start": "2022720",
    "end": "2028520"
  },
  {
    "text": "Rather than doing one load you\ncan do a vector load, which really gives you a bigger\nbandwidth to memory.",
    "start": "2028520",
    "end": "2035230"
  },
  {
    "text": "So in this case, I have a load\nfrom I1, I2, and since these",
    "start": "2035230",
    "end": "2040580"
  },
  {
    "text": "memory locations are continuous,\nI can replace them by one vector load that brings\nin all these data",
    "start": "2040580",
    "end": "2047530"
  },
  {
    "text": "elements in one shot. That essentially eliminates\nthree load instructions, which",
    "start": "2047530",
    "end": "2052540"
  },
  {
    "text": "are potentially most heavy\nweight for one ligher weight instruction because it amortizes\nbandwidth and",
    "start": "2052540",
    "end": "2060450"
  },
  {
    "text": "exploits things like\nspatial locality.  Another one, vectorizable\nloops.",
    "start": "2060450",
    "end": "2067530"
  },
  {
    "text": "So this is probably one of\nthe most advanced ways of exploiting SIMDization,\nespecially in really long",
    "start": "2067530",
    "end": "2074200"
  },
  {
    "text": "vector codes, so traditional\nsupercomputers like the Cray, and you'll probably hear Simmon\ntalk about this in the",
    "start": "2074200",
    "end": "2079370"
  },
  {
    "text": "next lecture. So I have some loop\nand I hvae this particular statement here.",
    "start": "2079370",
    "end": "2084540"
  },
  {
    "text": "So how can I get SIMD\ncode out of this? Anybody have any ideas? ",
    "start": "2084540",
    "end": "2101750"
  },
  {
    "text": "Anybody know about\nloop unrolling?  So if I unroll this loop, I\nessentially -- that same trick",
    "start": "2101750",
    "end": "2110170"
  },
  {
    "text": "that I had shown earlier,\nalthough I didn't quite do it this way. I change a loop bound\nfrom n to --",
    "start": "2110170",
    "end": "2116440"
  },
  {
    "text": "the increment from -- rather\nthan stepping through one at a time, stepping through\nfour at a time. Now the loop body, rather than\ndoing one addition at a time,",
    "start": "2116440",
    "end": "2125190"
  },
  {
    "text": "I'm doing four additions\nat a time. So now this is very natural for\na vectorization, right? Vector load, vector load, vector\nstore plus the vector",
    "start": "2125190",
    "end": "2132410"
  },
  {
    "text": "add in the middle. Is that intuitive? ",
    "start": "2132410",
    "end": "2138400"
  },
  {
    "text": "So this gives you another way\nof extracting parallelism. Looking at traditional loops,\nseeing whether you can actually unroll it in different\nways, be able to get",
    "start": "2138400",
    "end": "2146510"
  },
  {
    "text": "that SIMD parallelization. The last one I'll talk about\nis about partial vectorization.",
    "start": "2146510",
    "end": "2153400"
  },
  {
    "text": "Either it might be some things\nwhere you have a mix of statements. So here I have a loop where I\nhave some load and then I'm",
    "start": "2153400",
    "end": "2161890"
  },
  {
    "text": "doing some computation here. So what could I do here? ",
    "start": "2161890",
    "end": "2169780"
  },
  {
    "text": "It's not as symmetric\nas the other loop. AUDIENCE: There's no vector\nand [INAUDIBLE PHRASE].",
    "start": "2169780",
    "end": "2178200"
  },
  {
    "text": "PROFESSOR: Right. So you might omit that. But could you do anything\nabout the subtraction? AUDIENCE: [INAUDIBLE PHRASE].",
    "start": "2178200",
    "end": "2184870"
  },
  {
    "text": "PROFESSOR: If I can unroll\nthis again, right? Now there's no dependencies\nbetween this instruction and",
    "start": "2184870",
    "end": "2190680"
  },
  {
    "text": "this instruction, so I can\nreally move these together, and once I've moved these\ntogether then these loads",
    "start": "2190680",
    "end": "2196580"
  },
  {
    "text": "become contiguous. These loads are contiguous so I\ncan replace these by vector codes, vector equivalents.",
    "start": "2196580",
    "end": "2204640"
  },
  {
    "text": "So now the vector load bring\nin L0, L1, I have the addition, that brings in those\ntwo elements in a vector, and",
    "start": "2204640",
    "end": "2212800"
  },
  {
    "text": "then I can do my scaler\nadditions. But what do I do about the\nvalue getting out of this",
    "start": "2212800",
    "end": "2219300"
  },
  {
    "text": "vector register into this scaler\nregister that I need for the absolute values. So this is where the benefits\nversus cost of",
    "start": "2219300",
    "end": "2226740"
  },
  {
    "text": "SIMDization come in. So the benefits are great\nbecause you can replace multiple instructions by one\ninstruction, or you can just",
    "start": "2226740",
    "end": "2235460"
  },
  {
    "text": "cut down the number of\ninstructions by specific factor, your vector lens. Low stores can be replaced by\none wide memory operation, and",
    "start": "2235460",
    "end": "2243400"
  },
  {
    "text": "this is probably the biggest\nopportunity for performance improvements. But the cost is that you have\nto pack data into the data",
    "start": "2243400",
    "end": "2250300"
  },
  {
    "text": "registers an you have to unpack\nit out so that you can have those kinds of\ncommunications between this",
    "start": "2250300",
    "end": "2257460"
  },
  {
    "text": "vector register here and the\nvalue here, this value here and this value here. Often you can't simply access\nvector values without doing",
    "start": "2257460",
    "end": "2265210"
  },
  {
    "text": "this packing and unpacking. ",
    "start": "2265210",
    "end": "2272210"
  },
  {
    "text": "So how do you actually do\nthe packing, unpacking? This is predominantly where a\nlot of the complexity goes.",
    "start": "2272210",
    "end": "2277880"
  },
  {
    "start": "2277880",
    "end": "2283910"
  },
  {
    "text": "So the value of a here is\ninitialized by some function and the value of b here is\ninitialized by some function,",
    "start": "2283910",
    "end": "2289140"
  },
  {
    "text": "and these might not be\nthings that I can SIMdize very easily. So what I need to do is move\nthat value into the first",
    "start": "2289140",
    "end": "2296480"
  },
  {
    "text": "element of vector register, and\nmove the second value into the second element of\nthe vector register. So if I have a four-way vector\nregister, then I have to do",
    "start": "2296480",
    "end": "2303470"
  },
  {
    "text": "four of these moves, and that\nessentially is the packing. Then I could do my vector\ncomputation, which is really",
    "start": "2303470",
    "end": "2310590"
  },
  {
    "text": "these two statements here. Then eventually I have to do my\nunpacking because I have to",
    "start": "2310590",
    "end": "2315750"
  },
  {
    "text": "get the values out to do this\noperation and this operation. So there's an extraction\nthat has to happen",
    "start": "2315750",
    "end": "2322180"
  },
  {
    "text": "out of my SIMD register.  But you can amortize the cost\nof the packing and unpacking",
    "start": "2322180",
    "end": "2328980"
  },
  {
    "text": "by just reusing your\nvector registers. So these are like register\nallocation techniques.",
    "start": "2328980",
    "end": "2334490"
  },
  {
    "text": "So if I pack things into a\nvector register, I find all cases where I can actually reuse\nthat vector register and",
    "start": "2334490",
    "end": "2339890"
  },
  {
    "text": "I try to find opportunities\nfor extra SIMDization. So in the other case then, I\npack one then I can reuse that",
    "start": "2339890",
    "end": "2348119"
  },
  {
    "text": "same vector register. So what are some ways I can look\nfor to amortize the cost?",
    "start": "2348120",
    "end": "2353690"
  },
  {
    "text": " The interesting thing about\nmemory operations is while",
    "start": "2353690",
    "end": "2358700"
  },
  {
    "text": "there are many different ways\nyou can pack scaler values into a vector register, there's\nreally only one way",
    "start": "2358700",
    "end": "2364700"
  },
  {
    "text": "you can pack loads coming in\nfrom memory into a vector register is because you want\nthe loads to be sequential,",
    "start": "2364700",
    "end": "2371289"
  },
  {
    "text": "you want to exploit the\nspatial locality. So one vector load really gives\nyou specific ordering. So, that really constrains\nyou in various ways.",
    "start": "2371290",
    "end": "2380140"
  },
  {
    "text": "So you might bend over backwards\nin some cases to actually get your code to\nbe able to you reuse the",
    "start": "2380140",
    "end": "2386349"
  },
  {
    "text": "wide-word load without having\nto do too much packing or unpacking because that'll start eating into your benefits.",
    "start": "2386350",
    "end": "2393520"
  },
  {
    "text": "So simple example of how\nyou might find the SLP",
    "start": "2393520",
    "end": "2400900"
  },
  {
    "text": "parallelism. So the first thing you want to\ndo is start with are the instructions that give you the\nmost benefit, so it's memory",
    "start": "2400900",
    "end": "2408740"
  },
  {
    "text": "references. So here there are two\nmemory references. They happen to be adjacent, so\nI'm accessing a contiguous",
    "start": "2408740",
    "end": "2415680"
  },
  {
    "text": "memory chunks, so I can\nparallelized that. That would be my first step. I can do a vector load and\nthat assignment can",
    "start": "2415680",
    "end": "2423580"
  },
  {
    "text": "become a and b. I can look for opportunities\nwhere I can propagate this",
    "start": "2423580",
    "end": "2429730"
  },
  {
    "text": "vector values within the\nvector register that's holding a and b.",
    "start": "2429730",
    "end": "2435840"
  },
  {
    "text": "So the way I do that is I look\nfor uses of a and b. In this case, there are\nthese two statements.",
    "start": "2435840",
    "end": "2440960"
  },
  {
    "text": " So I can look for opportunities",
    "start": "2440960",
    "end": "2446440"
  },
  {
    "text": "to vectorize that. ",
    "start": "2446440",
    "end": "2452160"
  },
  {
    "text": "So in this case, both of these\ninstructions are also vectorizable. Now I have a vector subtraction,\nand I have a",
    "start": "2452160",
    "end": "2459990"
  },
  {
    "text": "vector register holding\nnew values h and j. So I follow that chain again\nof where data's flowing.",
    "start": "2459990",
    "end": "2468200"
  },
  {
    "text": "I find these operations and I\ncan vectorize that as well. ",
    "start": "2468200",
    "end": "2478160"
  },
  {
    "text": "So, sign up with a vectorizable\nloop where all my instructions, all my scale\ninstructions are now in SIMD",
    "start": "2478160",
    "end": "2484520"
  },
  {
    "text": "instructions. I can cut down on loop\niterations of total number of",
    "start": "2484520",
    "end": "2490000"
  },
  {
    "text": "instructions that I issue. But I've made some implicit\nassumption here. Anybody know what it is?",
    "start": "2490000",
    "end": "2495020"
  },
  {
    "text": "AUDIENCE: Do you actually\nneed that many",
    "start": "2495020",
    "end": "2502538"
  },
  {
    "text": "iterations of the loop? PROFESSOR: Well, so you can\nfactor down the cost. so here",
    "start": "2502538",
    "end": "2507680"
  },
  {
    "text": "I've vectorized by 2, so I would\ncut down the number of iterations by 2. AUDIENCE: You could have an\nodd number of iterations?",
    "start": "2507680",
    "end": "2512830"
  },
  {
    "text": "PROFESSOR: Right, so you could\nhave an odd number of iterations. What do you do about the\nremaining iterations. You might have to do scaler\ncode for that.",
    "start": "2512830",
    "end": "2519760"
  },
  {
    "text": "What are some other\nassumptions? Maybe it will be clear here. ",
    "start": "2519760",
    "end": "2527380"
  },
  {
    "text": "So in vectorizing this, what\nhave I assumed about relationships between\nthese statements?",
    "start": "2527380",
    "end": "2533039"
  },
  {
    "text": "I've essentially reorganized\nall the statements so that assumes I have this liberty to\nmove instructions around.",
    "start": "2533040",
    "end": "2539520"
  },
  {
    "text": "Yup? AUDIENCE: [UNINTELLIGIBLE]\na and b don't change [INAUDIBLE PHRASE]. PROFESSOR: Right. So there's nothing in here\nthat's changing the values.",
    "start": "2539520",
    "end": "2547830"
  },
  {
    "text": "There's no dependencies between\nthese statements -- no flow dependencies and no other\nkind of constraints that limit",
    "start": "2547830",
    "end": "2553910"
  },
  {
    "text": "this kind of movement. So in real code it's not\nactually the case. You end up with patterns of\ncomputation where you can get",
    "start": "2553910",
    "end": "2560150"
  },
  {
    "text": "really a nice case of classic\ncases you can vectorize those really nicely. In a lot of other codes you\nhave a mix of vectorizable",
    "start": "2560150",
    "end": "2568020"
  },
  {
    "text": "code and scaler code and there's\na lot of communication between the two. So the cost is really something\nsignificant that you",
    "start": "2568020",
    "end": "2574089"
  },
  {
    "text": "have to consider.  This was, as I mentioned, done\nin somebody's Masters thesis",
    "start": "2574090",
    "end": "2580780"
  },
  {
    "text": "and eventually led to some\nadditional work that was his PhD thesis.",
    "start": "2580780",
    "end": "2586770"
  },
  {
    "text": "So in some of the early work,\nwhat he did was he looked at a bunch of benchmarks and looked\nat how much available",
    "start": "2586770",
    "end": "2592470"
  },
  {
    "text": "parallelism you have in terms\nof this kind of short vector parallelism, or rather SLP\nwhere you're looking for",
    "start": "2592470",
    "end": "2600260"
  },
  {
    "text": "vectorizable code within basic\nblocks, which really differed from a classic way of people\nlooking for vectorization.",
    "start": "2600260",
    "end": "2606100"
  },
  {
    "text": "[? And you ?] have\nwell-structured loops and doing kinds of transformations\nyou'll hear about next week.",
    "start": "2606100",
    "end": "2612119"
  },
  {
    "text": "So for different kinds of vector\nregisters, so these are your vector lens. So going from 128 bits to 1,024\nbits, you can actually",
    "start": "2612120",
    "end": "2621470"
  },
  {
    "text": "reduce a whole lot\nof instructions. So what I'm showing here\nis the percent dynamic instruction reduction.",
    "start": "2621470",
    "end": "2627420"
  },
  {
    "text": "So if I take my baseline\napplication and just compile it in a normal way and I run it\nagain an instruction count.",
    "start": "2627420",
    "end": "2633670"
  },
  {
    "text": "I apply this SLP technique that\nfind the SIMDization and then run my application again,\nuse the performance counters",
    "start": "2633670",
    "end": "2639500"
  },
  {
    "text": "to count the number\nof instructions and compare the two. I can get 60%, 50%, 40%.",
    "start": "2639500",
    "end": "2647020"
  },
  {
    "text": "In some cases I can completely\neliminate almost 90% or more of the instructions.",
    "start": "2647020",
    "end": "2652660"
  },
  {
    "text": "So it's a lot of opportunity for\nperformance improvements that might be apparent.",
    "start": "2652660",
    "end": "2659410"
  },
  {
    "text": "One because I'm reducing the\ninstruction bandwidth, I'm reducing the amount of space I\nneed in my instruction cache,",
    "start": "2659410",
    "end": "2665369"
  },
  {
    "text": "I have fewer instructions so I\ncan fit more instructions into my instruction cache, you reduce\nthe number of branches.",
    "start": "2665370",
    "end": "2671190"
  },
  {
    "text": "You get better bandwidth to the\nmemory, better use of the memory bandwidth.",
    "start": "2671190",
    "end": "2677080"
  },
  {
    "text": "Overall, you're running fewer\niterations, so you're getting lots of potential\nfor performance.",
    "start": "2677080",
    "end": "2684050"
  },
  {
    "text": "So, I actually ran this\non the AltiVec. This was one of the earliest\ngenerations of AltiVec, which",
    "start": "2684050",
    "end": "2690710"
  },
  {
    "text": "SIMD instructions didn't have\nI believe double precision",
    "start": "2690710",
    "end": "2697099"
  },
  {
    "text": "floating point, so not all the\nbenchmarks you see on the previsou slide are here, only\nthe ones that could run",
    "start": "2697100",
    "end": "2702470"
  },
  {
    "text": "reasonably accurately\nwith a single precision floating point. What they measure is the\nactual speed up.",
    "start": "2702470",
    "end": "2708240"
  },
  {
    "text": "Doing this SIMDization versus\nnot doing a SIMDization, how much performance you can get. The thing to take away is in\nsome cases where you have",
    "start": "2708240",
    "end": "2717550"
  },
  {
    "text": "nicely structured loops and some\nnice patterns, you can get up to 7x speed up\non some benchmarks.",
    "start": "2717550",
    "end": "2724250"
  },
  {
    "text": "What might be the maximum speed\nup that you can get depends on the vector lens,\nso 8, for example on some",
    "start": "2724250",
    "end": "2731210"
  },
  {
    "text": "architectures depending\non the data type. Is there any questions\non that?",
    "start": "2731210",
    "end": "2738500"
  },
  {
    "text": "So as part of the next\nrecitation, you'll actually get an exercise of going through\nand SIMDizing for Cell, and whether that actually\nmeans SIMDize",
    "start": "2738500",
    "end": "2745300"
  },
  {
    "text": "instructions for Cell might take\nstatements and sort of replace them by intrinsic\nfunctions, which eventually",
    "start": "2745300",
    "end": "2750320"
  },
  {
    "text": "map down to actually assembly\nop codes that you'll need. So you don't actually have to\nprogram at the assembly level,",
    "start": "2750320",
    "end": "2755940"
  },
  {
    "text": "although in effect, you're\nprobably doing the same thing. Last thing we'll talk about\ntoday is optimizing for the",
    "start": "2755940",
    "end": "2763040"
  },
  {
    "text": "memeory hierarchy. In addition to data level\nparallelism, looking for performance enhancements in the\nmemory system gives you",
    "start": "2763040",
    "end": "2771250"
  },
  {
    "text": "the best opportunities because\nof this big gap in performance",
    "start": "2771250",
    "end": "2779250"
  },
  {
    "text": "between memory access latencies\nand what the CPU efficiency is.",
    "start": "2779250",
    "end": "2784840"
  },
  {
    "text": "So exploiting locality in\na memroy system is key. So these concepts of temporal\nand spatial locality.",
    "start": "2784840",
    "end": "2790940"
  },
  {
    "text": "So let's look at an example. Let's say I have a loop and in\nthis loop I have some code",
    "start": "2790940",
    "end": "2797870"
  },
  {
    "text": "that's embodied in some function\na, some code embodied in some function b, and some\ncode in some function c.",
    "start": "2797870",
    "end": "2805660"
  },
  {
    "text": "The values produced by a are\nconsumed by the function b, and similarly the values\nconsumed by b",
    "start": "2805660",
    "end": "2811280"
  },
  {
    "text": "are consumed by c. So this is a general data flow\ngraph that you might have for",
    "start": "2811280",
    "end": "2817039"
  },
  {
    "text": "this function. Let's say that all the data\ncould go into a small array",
    "start": "2817040",
    "end": "2823240"
  },
  {
    "text": "that then I can communicate\nbetween functions.",
    "start": "2823240",
    "end": "2828869"
  },
  {
    "text": "So if I look at my actual cache\nsize and how the working set of each of these functions\nis, so let's say this is my",
    "start": "2828870",
    "end": "2835400"
  },
  {
    "text": "cache size -- this is how\nmany instructions I can pack into the cache. Looking at the collective number\nof instructions in each",
    "start": "2835400",
    "end": "2842490"
  },
  {
    "text": "one of these functions,\nI overflow that. I have more instructions\nI can fit into my",
    "start": "2842490",
    "end": "2847810"
  },
  {
    "text": "cache any one time. So what does that mean for my\nactual cash performance?",
    "start": "2847810",
    "end": "2852890"
  },
  {
    "text": "So when I run a, what do I\nexpect the cache hit and miss",
    "start": "2852890",
    "end": "2858490"
  },
  {
    "text": "rate behavior to be like? So in the first iteration, I\nneed the instructions for a.",
    "start": "2858490",
    "end": "2865829"
  },
  {
    "text": "I've never seen a before so I\nhave to fetch that data from memory and put in the cache. So, the attachments.",
    "start": "2865830",
    "end": "2873320"
  },
  {
    "text": "So what about b? ",
    "start": "2873320",
    "end": "2878330"
  },
  {
    "text": "Then c? Same thing. So now I'm back at the\ntop of my loop.",
    "start": "2878330",
    "end": "2883920"
  },
  {
    "text": "So if everything fit in\nthe cache then I would expect a to be a what?",
    "start": "2883920",
    "end": "2890230"
  },
  {
    "text": "You'll be a hit. But since I've constrained this\nproblem such that the working set doesn't really fit\nin the cache, what that means",
    "start": "2890230",
    "end": "2897220"
  },
  {
    "text": "is that I have to fetch some\nnew instructions for a. So let's say I have\nto fetch all the instructions for a again.",
    "start": "2897220",
    "end": "2902460"
  },
  {
    "text": "That leads me to another miss. Now, bringing a again into my\ncache kicks out some extra",
    "start": "2902460",
    "end": "2909610"
  },
  {
    "text": "instructions because I need to\nmake room in a finite memory so I kick out b.",
    "start": "2909610",
    "end": "2915090"
  },
  {
    "text": "Bring in b and I end\nup kicking out c. So you end up with a pattern\nwhere everything is a miss.",
    "start": "2915090",
    "end": "2921530"
  },
  {
    "text": "This is a problem because the\nway the loop is structured, collectively I just can't pack\nall those instructions into",
    "start": "2921530",
    "end": "2928025"
  },
  {
    "text": "the cache, so I end up taking\na lot of cache misses and that's bad for performance.",
    "start": "2928025",
    "end": "2935030"
  },
  {
    "text": "But I can look at an\nalternative way of doing this loop. I can split up this loop into\nthree where in one loop I do",
    "start": "2935030",
    "end": "2942960"
  },
  {
    "text": "all the a instructions, in the\nsecond loop I do all the b's, and the third loop\nI do all the c's.",
    "start": "2942960",
    "end": "2949260"
  },
  {
    "text": "Now my working set\nis really small. So the instructions for a fit in\nthe cache, instructions for",
    "start": "2949260",
    "end": "2956020"
  },
  {
    "text": "b fit in the cache, and\ninstructions for c fit in the cache. So what do I expect for the\nfirst time I see a?",
    "start": "2956020",
    "end": "2964329"
  },
  {
    "text": "Miss. Then second time? ",
    "start": "2964330",
    "end": "2971270"
  },
  {
    "text": "It'll be hit, because I've\nbrought in a, I haven't run b",
    "start": "2971270",
    "end": "2976730"
  },
  {
    "text": "or c yet, the number of\ninstructions I need for a is smaller than what I\ncan fit into the cache, so that's great.",
    "start": "2976730",
    "end": "2982770"
  },
  {
    "text": "Nothing gets kicked out. So every one of those\niterations for a becomes a hit.",
    "start": "2982770",
    "end": "2988450"
  },
  {
    "text": "So that's good. I've improved performance. For b I have the same pattern.",
    "start": "2988450",
    "end": "2994550"
  },
  {
    "text": "First time I see b it's a miss,\nevery time after that it's a hit. Similarly for c. So my cache miss rate goes from\nbeing one, everything's a",
    "start": "2994550",
    "end": "3002490"
  },
  {
    "text": "miss, to decreasing to 1 over\nn where n is essentially how much I can run the loop.",
    "start": "3002490",
    "end": "3009500"
  },
  {
    "text": "So we call that full scaling\nbecause we've taken the loop where we've distributed, and\nwe've scaled every one of those smaller loops to the\nmaximum that we could get.",
    "start": "3009500",
    "end": "3016550"
  },
  {
    "text": " Now what about the data? So we have the same example.",
    "start": "3016550",
    "end": "3022340"
  },
  {
    "text": "Here we saw that the instruction\nworking set is big, but what about the data?",
    "start": "3022340",
    "end": "3029420"
  },
  {
    "text": "So let's say in this case\nI'm sending just a small amount of data. Then the behavior\nis really good.",
    "start": "3029420",
    "end": "3035640"
  },
  {
    "text": "It's a small amount of data\nthat I need to communicate from a to b. A small amount of\ndata you need to communicate from b to c.",
    "start": "3035640",
    "end": "3042040"
  },
  {
    "text": "So it's great. No problems with\nthe data cache. What happens in full\nscaling case? AUDIENCE: It's not correct to\ncommunicate from a to b.",
    "start": "3042040",
    "end": "3053330"
  },
  {
    "text": "PROFESSOR: What do you mean\nit's not correct? AUDIENCE: Oh, it's not communicating at the same time. ",
    "start": "3053330",
    "end": "3058740"
  },
  {
    "text": "PROFESSOR: Yeah, it's not\nat the same time. In fact, just assume\nthis is sequential.",
    "start": "3058740",
    "end": "3063890"
  },
  {
    "text": "So I run a, I store some data,\nand then when I run b I grab that data.",
    "start": "3063890",
    "end": "3070680"
  },
  {
    "text": "This is in sequential. AUDIENCE: How do you know that\nthe transmission's valid then?",
    "start": "3070680",
    "end": "3076609"
  },
  {
    "text": "We could use some\nglobal variable. PROFESSOR: Simple case. There's no global variables.",
    "start": "3076610",
    "end": "3082750"
  },
  {
    "text": "All the data that b needs\ncomes from a. So if I run a I produce\nall the data and",
    "start": "3082750",
    "end": "3088290"
  },
  {
    "text": "that's all that b needs.  So in the full scaling case,\nwhat do I expect to",
    "start": "3088290",
    "end": "3094110"
  },
  {
    "text": "happen for the data? Remember, in the full scaling\ncase, all the working sets for",
    "start": "3094110",
    "end": "3099730"
  },
  {
    "text": "the instructions are small so\nthey all fit in the cache. But now I'm running a for a lot\nlonger so I have to store",
    "start": "3099730",
    "end": "3106260"
  },
  {
    "text": "a lot more data for b. Similarly, I'm running b for a\nlot longer so I have to store a lot more data for c.",
    "start": "3106260",
    "end": "3112690"
  },
  {
    "text": "So what do I expect to happen\nwith the working set here? ",
    "start": "3112690",
    "end": "3118180"
  },
  {
    "text": "Instructions are still good,\nbut the data might be bad because I've run a for a lot\nmore iterations at one shot.",
    "start": "3118180",
    "end": "3126410"
  },
  {
    "text": "So now I have to buffer all\nthis data for a to b. Similarly, I've run b for a long\ntime so I have to buffer",
    "start": "3126410",
    "end": "3131960"
  },
  {
    "text": "a whole lot data for b to c. Is that clear? AUDIENCE: No. PROFESSOR: So let's say every\ntime a runs it produces one",
    "start": "3131960",
    "end": "3139040"
  },
  {
    "text": "data element. So now in this case,\nevery iteration produces one data element.",
    "start": "3139040",
    "end": "3144970"
  },
  {
    "text": "That's fine. That's clear? Here I run a n times, so I\nproduce n data elements.",
    "start": "3144970",
    "end": "3151720"
  },
  {
    "text": "And b let's say produces\none data element. So if my cache can only hold\nlet's say n by 2 data",
    "start": "3151720",
    "end": "3159560"
  },
  {
    "text": "elements, then there's\nan overflow. So what that means is not\neverything's in the cache, and",
    "start": "3159560",
    "end": "3164769"
  },
  {
    "text": "that's bad because of the same\nreasons we saw for the instructions. When I need those data I have\nto go out to memory and get",
    "start": "3164770",
    "end": "3169780"
  },
  {
    "text": "them again, so it's extra\ncommunication, extra redundancy. AUDIENCE: In this case where you\ndon't need to store the a variables\n[UNINTELLIGIBLE PHRASE].",
    "start": "3169780",
    "end": "3175780"
  },
  {
    "text": " PROFESSOR: But notice these were\nsequential simple case.",
    "start": "3175780",
    "end": "3183000"
  },
  {
    "text": "I need all the data from a to\nrun all the iterations for b.",
    "start": "3183000",
    "end": "3188210"
  },
  {
    "text": "Then, yeah, this goes away. So let's say this goes away,\nbut still b produces n",
    "start": "3188210",
    "end": "3193569"
  },
  {
    "text": "elements and that overflows\nthe cache. ",
    "start": "3193570",
    "end": "3199770"
  },
  {
    "text": "So there's a third example where\nI don't fully distribute everything, I partially\ndistribute some of the loops.",
    "start": "3199770",
    "end": "3206520"
  },
  {
    "text": "I can fully scale a and b\nbecause I can fit those instructions in the cache.",
    "start": "3206520",
    "end": "3211600"
  },
  {
    "text": "That gets me around this\nproblem, because now a and b are just communicating\none day data element.",
    "start": "3211600",
    "end": "3217850"
  },
  {
    "text": "But c is still a problem because\nI still have to run b n times in the end before I can\nrun c so there are n data",
    "start": "3217850",
    "end": "3223430"
  },
  {
    "text": "elements in flight. So the data for b still becomes\na problem in terms of",
    "start": "3223430",
    "end": "3230480"
  },
  {
    "text": "its locality. Is that clear? So, any ideas on how\nI can improve this?",
    "start": "3230480",
    "end": "3237640"
  },
  {
    "text": "AUDIENCE: assuming you have the\nwrong cache line and you have to do one or two memory\nacceses to get the cache back. ",
    "start": "3237640",
    "end": "3250920"
  },
  {
    "text": "PROFESSOR: So, programs\ntypically have really good",
    "start": "3250920",
    "end": "3256690"
  },
  {
    "text": "instruction locality just\nbecause the nature of the way we run them. We have small loops and they\niterate over and over again.",
    "start": "3256690",
    "end": "3263350"
  },
  {
    "text": "Data is actually where you spend\nmost of your time in the memory system. It's fetching data. So I didn't actually understand\nwhy you think data",
    "start": "3263350",
    "end": "3271650"
  },
  {
    "text": "is less expensive than\ninstructions. AUDIENCE: What I'm saying say\nyou want to read an array, read the first, say, 8\nelements to 8 words",
    "start": "3271650",
    "end": "3279434"
  },
  {
    "text": "in the cache box. Well then you'd get 7 hits, so\nevery 8 iterations you have to",
    "start": "3279435",
    "end": "3284514"
  },
  {
    "text": "do a rewrite. PROFESSOR: Right. So that assumes that you have\nreally good spatial locality,",
    "start": "3284514",
    "end": "3289800"
  },
  {
    "text": "because you've assumed that I've\nbrought in 8 elements and I'm going to use every\none of them. So if that's the case you have\nreally good spatial locality",
    "start": "3289800",
    "end": "3295910"
  },
  {
    "text": "and that's, in fact,\nwhat you want. It's the same kind of thing\nthat I showed for the instruction cache. The first thing is a miss,\nthe rest are hits.",
    "start": "3295910",
    "end": "3304450"
  },
  {
    "text": "The reason data is more\nexpensive, you simply have a lot more data reads than\nyou have instructions.",
    "start": "3304450",
    "end": "3310490"
  },
  {
    "text": "Typically you have small\nloops, hundreds of instructions and they might\naccess really big arrays that are millions of data\nreferences.",
    "start": "3310490",
    "end": "3318160"
  },
  {
    "text": "So that becomes a problem. So ideas on how to\nimprove this? AUDIENCE: That's a loop?",
    "start": "3318160",
    "end": "3323340"
  },
  {
    "text": "PROFESSOR: That's a loop. So what would you do in\nthe smaller loop? AUDIENCE: [INAUDIBLE PHRASE].",
    "start": "3323340",
    "end": "3330740"
  },
  {
    "text": "PROFESSOR: Something\nlike that? AUDIENCE: Yeah. PROFESSOR: OK. So in a nested loop, you have\na smaller loop that has a",
    "start": "3330740",
    "end": "3339190"
  },
  {
    "text": "small number of iterations,\nso 64. So, 64 might be just as much as\nI can buffer for the data",
    "start": "3339190",
    "end": "3347109"
  },
  {
    "text": "in the cache. Then I wrap that loop with one\nouter loop that completes the whole number of iterations.",
    "start": "3347110",
    "end": "3352860"
  },
  {
    "text": "So if I had to do n, then\nI divide n by 64. So that can work out\nreally well.",
    "start": "3352860",
    "end": "3358060"
  },
  {
    "text": "So there's different kinds of\nblocking techniques that you can use on getting your data to\nfit into your local store",
    "start": "3358060",
    "end": "3363800"
  },
  {
    "text": "or into your cache to exploit\nthese spatial and temporal properties. Question? AUDIENCE: Would it not be\nbetter to use a small",
    "start": "3363800",
    "end": "3372135"
  },
  {
    "text": "[UNINTELLIGIBLE] size so\nyou could run a, b, c sequentially? ",
    "start": "3372135",
    "end": "3377210"
  },
  {
    "text": "PROFESSOR: You could\ndo that as well. But the problem with running a,\nb, c sequentially is that if they're in the same\nloop, you end up with",
    "start": "3377210",
    "end": "3383930"
  },
  {
    "text": "instructions being bad. That would really, this case --\nso even if you change this number you don't get around\nthe instructions.",
    "start": "3383930",
    "end": "3390790"
  },
  {
    "text": " So you're going to see more\noptimizations that do more of",
    "start": "3390790",
    "end": "3398039"
  },
  {
    "text": "these loop tricks. I talk about unrolling without\nreally defining what unrolling",
    "start": "3398040",
    "end": "3403230"
  },
  {
    "text": "is or going into a\nlot of details. Loop distribution, loop fision,\nsome of the things, like loop tiling,\nloop blocking.",
    "start": "3403230",
    "end": "3409960"
  },
  {
    "text": "I think Simmon's going to cover\nsome of these next week. So this was implemented, this\nwas done by another Master",
    "start": "3409960",
    "end": "3416890"
  },
  {
    "text": "student at MIT who graduated\nabout two years ago, to show",
    "start": "3416890",
    "end": "3422470"
  },
  {
    "text": "that if you factor in cache\nconstraints versus ignoring cache constraints, how much\nperformance you can get.",
    "start": "3422470",
    "end": "3427650"
  },
  {
    "text": "This was done in the context\nof StreamIt. So, in fact, some of you might\nhave recognized a to b to c as",
    "start": "3427650",
    "end": "3432710"
  },
  {
    "text": "being interconnected as\npipeline filters. We ran it on different\nprocessors, so the StrongARM",
    "start": "3432710",
    "end": "3439690"
  },
  {
    "text": "processor's really small. InOrder processor has no L1\ncache in this particular model",
    "start": "3439690",
    "end": "3445780"
  },
  {
    "text": "that we used. But it had a really\nlong latency -- sorry, it had no L2 cache.",
    "start": "3445780",
    "end": "3451570"
  },
  {
    "text": "It had really long latency\nto memory. Pentium, an x86 processor.",
    "start": "3451570",
    "end": "3457710"
  },
  {
    "text": "Reasonably fast. It had a\ncomplicated memory system and a lot, a lot of memory overlap\nin terms of references.",
    "start": "3457710",
    "end": "3463490"
  },
  {
    "text": "Then the Itanium processor,\nwhich had a huge L2 cache at",
    "start": "3463490",
    "end": "3469490"
  },
  {
    "text": "its disposal. So what you can see is that\nlower bars indicate bigger speed ups.",
    "start": "3469490",
    "end": "3476060"
  },
  {
    "text": "This is normalized run time. So on the processor where you\ndon't actually have caches to",
    "start": "3476060",
    "end": "3481089"
  },
  {
    "text": "save you, and the memory\ncommunication is really expensive, you can get a lot\nof benefit from doing the",
    "start": "3481090",
    "end": "3486490"
  },
  {
    "text": "cache aware scaling, that loop\nnesting to take advantage of packing instructions instead of\ninstruction cache, packing",
    "start": "3486490",
    "end": "3492430"
  },
  {
    "text": "data into data cache and not\nhaving to go out to memory if you don't to. So you can reduce run time to\nabout 1/3 of what it was with",
    "start": "3492430",
    "end": "3503470"
  },
  {
    "text": "this kind of cache\noptimization. On the Pentium3 where you have\na cache to help you out, the",
    "start": "3503470",
    "end": "3510710"
  },
  {
    "text": "benefits are there, but you\ndon't get as big a benefit from ignoring the cache\nconstraints versus being aware",
    "start": "3510710",
    "end": "3518550"
  },
  {
    "text": "of the cache constraints. So here you're actually doing\nsome of that middle column",
    "start": "3518550",
    "end": "3524480"
  },
  {
    "text": "whereas here we're doing\nthird columns, the cache aware fusion. ",
    "start": "3524480",
    "end": "3531890"
  },
  {
    "text": "In a Itanium you really get no\nbenefits between the two. Yep? AUDIENCE: Can you explain what\nthe left columns are?",
    "start": "3531890",
    "end": "3542140"
  },
  {
    "text": "PROFESSOR: These? AUDIENCE: Yeah. PROFESSOR: So this is tricky.",
    "start": "3542140",
    "end": "3547270"
  },
  {
    "text": " So the left columns\nare doing this.",
    "start": "3547270",
    "end": "3553000"
  },
  {
    "text": "AUDIENCE: OK, sort of assuming\nthat icache is there. PROFESSOR: Right, and the third\ncolumn is doing this.",
    "start": "3553000",
    "end": "3560500"
  },
  {
    "text": "So you want to do this because\nthe icache locality is the best. So you always want to go\nto full or maximum scaling.",
    "start": "3560500",
    "end": "3569890"
  },
  {
    "text": "I'm actually fudging a little\njust for sake of clarity. Here you're actually doing this\nnesting to improve both",
    "start": "3569890",
    "end": "3577120"
  },
  {
    "text": "the instruction and\nthe data locality.  So you can get really good\nperformance improvement.",
    "start": "3577120",
    "end": "3583160"
  },
  {
    "text": "So what does that mean for\nyour Cell projects or for Cell, we'll talk about that next\nweek at the recitation.",
    "start": "3583160",
    "end": "3589020"
  },
  {
    "text": " Yeah? AUDIENCE: Is there\nsome big reasons [UNINTELLIGIBLE PHRASE].",
    "start": "3589020",
    "end": "3596400"
  },
  {
    "text": "PROFESSOR: Well it just means\nthat if you have caches to save you, and they're really big\ncaches and they're really efficient, the law of\ndiminishing returns.",
    "start": "3596400",
    "end": "3606990"
  },
  {
    "text": "That's where profiling\ncomes in. So you look at the profiling\nresults, you look at your cache misses, how many cache\nmisses are you taking.",
    "start": "3606990",
    "end": "3612360"
  },
  {
    "text": "If it's really significant,\nthen you look at ways to improve it. If your cache misses are really\nlow, you missed rate is",
    "start": "3612360",
    "end": "3618110"
  },
  {
    "text": "really low, then it doesn't make\nsense to spend time and energy focusing on that. Good question.",
    "start": "3618110",
    "end": "3624830"
  },
  {
    "text": "So, any other questions? So summarizing the gamut of\nprogramming for performance.",
    "start": "3624830",
    "end": "3633410"
  },
  {
    "text": "So you tune to parallelism\nfirst, because if you can't find the concurrency, your\nAmdahl's law, you're not going to a get a whole lot\nof speed up.",
    "start": "3633410",
    "end": "3640440"
  },
  {
    "text": "But then once you figured out\nwhat the parallelism is, then what you want to do is really\nget the performance on each",
    "start": "3640440",
    "end": "3645740"
  },
  {
    "text": "processor, the single track\nperformance to be really good. You shouldn't ignore that. The modern processors\nare complex.",
    "start": "3645740",
    "end": "3651630"
  },
  {
    "text": "You need instructional level\nparallelism, you need data level parallelism, you\nneed memory hierarchy optimizations, and so you\nshould consider those",
    "start": "3651630",
    "end": "3659210"
  },
  {
    "text": "optimizations. Here, profiling tools could\nreally help you figure out where the biggest benefits to\nperformance will come from.",
    "start": "3659210",
    "end": "3666029"
  },
  {
    "text": " You may have to, in fact,\nchange everything.",
    "start": "3666030",
    "end": "3671490"
  },
  {
    "text": "You may have to change your\nalgorithm, your data structures, your program\nstructure. So in the MPEG decoder case, for\nexample, I showed you that",
    "start": "3671490",
    "end": "3677460"
  },
  {
    "text": "if you change the flag that\nsays don't use double precision inverse DCT, use a\nnumerical hack, then you can",
    "start": "3677460",
    "end": "3685530"
  },
  {
    "text": "get performance improvements\nbut you're changing your algorithm really. You really want to focus on just\nthe biggest nuggets --",
    "start": "3685530",
    "end": "3692119"
  },
  {
    "text": "where is most of the performance\ncoming in, or where's the biggest performance\nbottleneck, and that's the thing you\nwant optimize.",
    "start": "3692120",
    "end": "3698060"
  },
  {
    "text": "So remember the law of\ndiminishing returns. Don't spend your time on doing\nthings that aren't going to get you anything significant\nin return.",
    "start": "3698060",
    "end": "3706010"
  },
  {
    "text": "That's it. Any questions? ",
    "start": "3706010",
    "end": "3711070"
  },
  {
    "text": "OK. How are you guys doing\nwith the projects? ",
    "start": "3711070",
    "end": "3716830"
  },
  {
    "text": "So, one of the added benefits of\nthe central CBS repository is I get notifications too\nwhen you submit things.",
    "start": "3716830",
    "end": "3724620"
  },
  {
    "text": "So I know of only two projects\nthat have been submitting things regularly. So, I hope that'll\npick up soon.",
    "start": "3724620",
    "end": "3731950"
  },
  {
    "text": "I guess a few minutes to finish\nthe quiz and then we'll see you next week. Have a good weekend. ",
    "start": "3731950",
    "end": "3737869"
  }
]