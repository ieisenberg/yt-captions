[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6540"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or to view\nadditional materials from",
    "start": "6540",
    "end": "12780"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at OCW.MIT.edu.",
    "start": "12780",
    "end": "17800"
  },
  {
    "start": "17800",
    "end": "23949"
  },
  {
    "text": "PROFESSOR: OK. We are talking about jointly\nGaussian random variables.",
    "start": "23950",
    "end": "32300"
  },
  {
    "text": "One comment through all of this\nand through all of the notes is that you can add a\nmean to Gaussian random",
    "start": "32300",
    "end": "41860"
  },
  {
    "text": "variables, or you\ncan talk about zero-mean random variables.",
    "start": "41860",
    "end": "47470"
  },
  {
    "text": "Here we're using random\nvariables mostly to talk about noise. When we're talking about noise,\nyou really should be",
    "start": "47470",
    "end": "56440"
  },
  {
    "text": "talking about zero-mean random\nvariables, because you can always take the mean out.",
    "start": "56440",
    "end": "62750"
  },
  {
    "text": "And because of that, I don't\nlike to state everything twice once for variables of processes\nwithout a mean, and",
    "start": "62750",
    "end": "70830"
  },
  {
    "text": "once for variables of processes\nwith a mean. And after looking at the notes,\nI think I've been a",
    "start": "70830",
    "end": "77620"
  },
  {
    "text": "little inconsistent\nabout that. I think the point is you can\nkeep yourself straight by just",
    "start": "77620",
    "end": "85829"
  },
  {
    "text": "saying the only thing\nimportant is the case without a mean. Putting the mean in is something\nunfortunately done",
    "start": "85830",
    "end": "94270"
  },
  {
    "text": "by people who like complexity. And they have unfortunately\ngot in the",
    "start": "94270",
    "end": "101440"
  },
  {
    "text": "notation on their side. So anytime you want\nto talk about a zero-mean random variable.",
    "start": "101440",
    "end": "107680"
  },
  {
    "text": "You have to say zero-mean\nrandom variable. And if you say random variable,\nit means it could",
    "start": "107680",
    "end": "113060"
  },
  {
    "text": "have a mean or not\nhave a mean. And of course the way the\nnotation should be stated is",
    "start": "113060",
    "end": "118940"
  },
  {
    "text": "you talk about random variables\nas things which don't have means. And then you can talk about\nrandom variables plus means as",
    "start": "118940",
    "end": "125960"
  },
  {
    "text": "things which do have means,\nwhich would make life easier but unfortunately it's\nnot that way.",
    "start": "125960",
    "end": "132040"
  },
  {
    "text": "So anytime you see something\nand are wondering about whether I've been careful about\nthe mean or not, the",
    "start": "132040",
    "end": "140170"
  },
  {
    "text": "answer is I well might\nnot have been. And two, it's not\nvery important.",
    "start": "140170",
    "end": "145720"
  },
  {
    "text": "So anyway. Here I'll be talking all\nabout zero-mean things.",
    "start": "145720",
    "end": "152110"
  },
  {
    "text": "A k-tuple of zero-mean random\nvariables is said to be",
    "start": "152110",
    "end": "157210"
  },
  {
    "text": "jointly Gaussian if you can\nexpress them in this way here. Namely as a linear combination\nof IID normal Gaussian again",
    "start": "157210",
    "end": "168490"
  },
  {
    "text": "random variables. OK. In your homework, those of you\nwho've done it already, have",
    "start": "168490",
    "end": "174239"
  },
  {
    "text": "realized that just having two\nGaussian random variables is not enough to make\nthose two random",
    "start": "174240",
    "end": "183070"
  },
  {
    "text": "variables be jointly Gaussian. They can be individually\nGaussian but not jointly Gaussian.",
    "start": "183070",
    "end": "189709"
  },
  {
    "text": "This is sort of important\nbecause when you start manipulating things as we will\ndo when we're generating",
    "start": "189710",
    "end": "197120"
  },
  {
    "text": "signals to send and things like\nthis, you can very easily wind up with things which look\nGaussian and are appropriately",
    "start": "197120",
    "end": "204280"
  },
  {
    "text": "modeled as Gaussian, but which\nare not jointly Gaussian. Things which are jointly\nGaussian are",
    "start": "204280",
    "end": "210870"
  },
  {
    "text": "defined in this way. We will come up with\na couple of other definitions of them later.",
    "start": "210870",
    "end": "216440"
  },
  {
    "text": "But you've undoubtedly been\ntaught that any old Gaussian random variables are\njointly Gaussian.",
    "start": "216440",
    "end": "222840"
  },
  {
    "text": "You've probably seen joint\ndensities for them and things like this. Those joint densities only apply\nwhen you have jointly",
    "start": "222840",
    "end": "230730"
  },
  {
    "text": "Gaussian random variables. And the fundamental definition\nis this.",
    "start": "230730",
    "end": "235750"
  },
  {
    "text": "This fundamental definition\nmakes sense, because the way",
    "start": "235750",
    "end": "242010"
  },
  {
    "text": "that you generate these noise\nrandom variables is usually from some very large underlying\nset of very, very",
    "start": "242010",
    "end": "250829"
  },
  {
    "text": "small random variables. And the Law of Large Numbers\nsays that when you add up a very, very large number of\nsmall underlying random",
    "start": "250830",
    "end": "258799"
  },
  {
    "text": "variables, you normalize the sum\nso it has some reasonable",
    "start": "258800",
    "end": "264460"
  },
  {
    "text": "variance then that random\nvariable is going to be appropriately modeled\nas being Gaussian.",
    "start": "264460",
    "end": "270440"
  },
  {
    "text": "If you at the same time look\nat linear combinations of",
    "start": "270440",
    "end": "275950"
  },
  {
    "text": "those things for the same\nreason, it's appropriate to model each of the random\nvariables you're looking at as",
    "start": "275950",
    "end": "283780"
  },
  {
    "text": "a linear combination of some\nunderlying set of noise variables which is\nvery, very large.",
    "start": "283780",
    "end": "289800"
  },
  {
    "text": "Probably not Gaussian. But here when we're trying to\ndo this, because of the",
    "start": "289800",
    "end": "295289"
  },
  {
    "text": "central limit theorem, we just\nmodel these as normal Gaussian random variables\nto start with.",
    "start": "295290",
    "end": "301810"
  },
  {
    "text": "So that's where that definition\ncomes from. It's why when you're looking at\nnoise processes in the real",
    "start": "301810",
    "end": "309680"
  },
  {
    "text": "world, and trying to model them\nin a sensible way this jointly Gaussian is the\nthing you almost",
    "start": "309680",
    "end": "316510"
  },
  {
    "text": "always come up with. OK. So one important point here, and\nthe notes point this out.",
    "start": "316510",
    "end": "324280"
  },
  {
    "text": "If each of these random\nvariables Z sub i is",
    "start": "324280",
    "end": "332280"
  },
  {
    "text": "independent of each of the\nothers, and they have arbitrary variances, then\nbecause of this formula, the",
    "start": "332280",
    "end": "341355"
  },
  {
    "text": "set of Z i's are going to\nbe jointly Gaussian. Why is that? Well you simply make a matrix\nhere A, which is diagonal.",
    "start": "341355",
    "end": "351960"
  },
  {
    "text": "And the elements of this\ndiagonal matrix are sigma 1 squared, sigma 2 squared, sigma\n3 squared, and so forth.",
    "start": "351960",
    "end": "359840"
  },
  {
    "start": "359840",
    "end": "366310"
  },
  {
    "text": "If you have a vector Z, which\nis then sigma 1 squared down",
    "start": "366310",
    "end": "375580"
  },
  {
    "text": "to sigma k squared times a noise\nvector N1 to N sub k.",
    "start": "375580",
    "end": "385479"
  },
  {
    "text": "What you wind up with is Z sub\ni is going to be equal to --",
    "start": "385480",
    "end": "392340"
  },
  {
    "text": "I guess I don't want those\nsquares in there. Sigma 1 up to sigma k -- Z sub i is going to be equal to\nsigma i, N sub i; N sub i",
    "start": "392340",
    "end": "402670"
  },
  {
    "text": "is a Gaussian random variable\nwith variance one and therefore Z sub i as Gaussian\nrandom variable with variance",
    "start": "402670",
    "end": "410039"
  },
  {
    "text": "sigma sub i squared. OK. So anyway, one special case of\nthis formula is anytime that",
    "start": "410040",
    "end": "418270"
  },
  {
    "text": "you want to deal with a set of\nindependent Gaussian random variables with arbitrary\nvariances, they are always",
    "start": "418270",
    "end": "428720"
  },
  {
    "text": "going to be jointly Gaussian. Saying that they're\nuncorrelated is not enough for that.",
    "start": "428720",
    "end": "434650"
  },
  {
    "text": "You really need the statement\nthat they're independent of each other.",
    "start": "434650",
    "end": "440009"
  },
  {
    "text": "OK. That's sort of where\nwe were last time. ",
    "start": "440010",
    "end": "447950"
  },
  {
    "text": "When you look at this formula,\nyou can look at it in terms of sample values.",
    "start": "447950",
    "end": "453880"
  },
  {
    "text": "And if we look at a sample value\nwhat's happening is that",
    "start": "453880",
    "end": "458980"
  },
  {
    "text": "the sample value of the random\nvector Z, namely little z, is",
    "start": "458980",
    "end": "464940"
  },
  {
    "text": "going to be defined as some\nmatrix A times this sample value for the normal\nvector N. OK.",
    "start": "464940",
    "end": "473650"
  },
  {
    "text": "And what we want to look\nat is geometrically what happens there.",
    "start": "473650",
    "end": "479090"
  },
  {
    "text": "Well this matrix a is going to\nmap a unit vector, E sub i",
    "start": "479090",
    "end": "487210"
  },
  {
    "text": "into the i'th column\nof A. Why is that? Well you look at A, which is\nwhatever it is, A sub 1, 1,",
    "start": "487210",
    "end": "497140"
  },
  {
    "text": "blah, blah, blah up\nto A sub k, k.",
    "start": "497140",
    "end": "502280"
  },
  {
    "text": "And you look at multiplying it\nby a vector which is zero only",
    "start": "502280",
    "end": "508040"
  },
  {
    "text": "in the i'th position. And what's this matrix\nmultiplication going to do?",
    "start": "508040",
    "end": "517060"
  },
  {
    "text": "It's going to simply pick\nout the i'th column of this matrix here.",
    "start": "517060",
    "end": "522490"
  },
  {
    "text": "OK. So a is going to map e sub i\ninto the i'th column of A. OK.",
    "start": "522490",
    "end": "528670"
  },
  {
    "text": "So then the question is what is\nthis matrix A going to do",
    "start": "528670",
    "end": "535709"
  },
  {
    "text": "to some small cube\nin the n plane?",
    "start": "535710",
    "end": "541030"
  },
  {
    "text": "OK. If you take a small cube in the\nn plane from 0 to delta",
    "start": "541030",
    "end": "547110"
  },
  {
    "text": "along the n1 line is going to\nmap into 0 to this point here,",
    "start": "547110",
    "end": "555170"
  },
  {
    "text": "which is A e sub 1.",
    "start": "555170",
    "end": "560389"
  },
  {
    "text": "This point here is going to\nmap into A times e sub 2.",
    "start": "560390",
    "end": "566630"
  },
  {
    "text": "This is just drawing for two\ndimensions of course. So in fact all the points in\nthis cube are going to get map",
    "start": "566630",
    "end": "573279"
  },
  {
    "text": "into this little\nrectangle here. OK. Namely, that's what a matrix\ntimes a vector is going to do.",
    "start": "573280",
    "end": "581740"
  },
  {
    "text": "Anybody awake out there? You're all looking at me as\nif I'm totally insane. ",
    "start": "581740",
    "end": "590180"
  },
  {
    "text": "OK. Every everyone following this? OK.",
    "start": "590180",
    "end": "595779"
  },
  {
    "text": "So perhaps this is\njust too trivial. I hope not. OK.",
    "start": "595780",
    "end": "601080"
  },
  {
    "text": "So unit cubes get mapped\ninto rectangles here. If I take a unit cube up here,\nit's going to get mapped into",
    "start": "601080",
    "end": "610030"
  },
  {
    "text": "the same kind of unit\nrectangle here. If I visualize tiling this plane\nhere with little tiny",
    "start": "610030",
    "end": "616520"
  },
  {
    "text": "cubes, delta on a side, what's\ngoing to happen? Each of these little cubes\nis going to map into a",
    "start": "616520",
    "end": "624690"
  },
  {
    "text": "parallelogram over here, and\nthese parallelograms going to tile this space here.",
    "start": "624690",
    "end": "631120"
  },
  {
    "text": "OK. Which means that each little\ncube here maps into one of these rectangles.",
    "start": "631120",
    "end": "636940"
  },
  {
    "text": "Each rectangle here maps back\ninto a little cube here. Which means that I'm\nlooking at a very",
    "start": "636940",
    "end": "642490"
  },
  {
    "text": "special case of a here. I'm looking at a case where\na is non-singular.",
    "start": "642490",
    "end": "648330"
  },
  {
    "text": "In other words, I can get the\nany point in this plane by starting with some point\nin this plane.",
    "start": "648330",
    "end": "654420"
  },
  {
    "text": "Which means for any point here,\nI can go back here also. In other words, I can also write\nn is equal to A to the",
    "start": "654420",
    "end": "665250"
  },
  {
    "text": "minus 1 times Z. And this\nmatrix has to exist.",
    "start": "665250",
    "end": "670310"
  },
  {
    "text": "OK. That's what you mean\ngeometrically by a non-singular matrix. It means that all points in\nthis plane get mapped into",
    "start": "670310",
    "end": "678149"
  },
  {
    "text": "points in this plane. And get mapped into only one\npoint in this plane, and get",
    "start": "678150",
    "end": "683649"
  },
  {
    "text": "mapped into only one point\nin this plane. Where every point in this\nplane is the map of some point here.",
    "start": "683650",
    "end": "689830"
  },
  {
    "text": "In other words you can go\nfrom here to there. You can also go back again.",
    "start": "689830",
    "end": "695060"
  },
  {
    "text": "OK. The volume of a parallelepiped\nhere, and this is in an arbitrary number of dimensions\nis going to be the determinant",
    "start": "695060",
    "end": "702610"
  },
  {
    "text": "of A. And you all know how\nto find determinants. Namely you program a computer\nto do it, and the",
    "start": "702610",
    "end": "708680"
  },
  {
    "text": "computer does it. I mean it used to be we had to\ndo this by an enormous amount of calculation.",
    "start": "708680",
    "end": "714340"
  },
  {
    "text": "And of course nobody\ndoes that anymore. OK. So we can find the volume of\nthis parallelepiped, it's this",
    "start": "714340",
    "end": "721060"
  },
  {
    "text": "determinant. And what does all\nof that mean? Well first let me ask\nyou the question.",
    "start": "721060",
    "end": "728410"
  },
  {
    "text": "What's going to happen if\nthe determinant of a is equaled to zero?",
    "start": "728410",
    "end": "733760"
  },
  {
    "text": "What does that mean\ngeometrically?  What does that mean here\nin terms of this",
    "start": "733760",
    "end": "740080"
  },
  {
    "text": "two dimensional diagram? AUDIENCE: [INAUDIBLE] PROFESSOR: What? AUDIENCE: Projection\nonto a line.",
    "start": "740080",
    "end": "745860"
  },
  {
    "text": "PROFESSOR: Yeah. This little cube here is simply\ngoing to get projected onto some line here.",
    "start": "745860",
    "end": "751620"
  },
  {
    "text": "Like for example that. In other words it means that\nthis matrix is not invertible",
    "start": "751620",
    "end": "759140"
  },
  {
    "text": "for one thing, but it also means\neverything here gets mapped onto some lower\ndimensional",
    "start": "759140",
    "end": "765220"
  },
  {
    "text": "sub-space here in general. OK. Now remember that for a minute,\nand we'll come back to",
    "start": "765220",
    "end": "770890"
  },
  {
    "text": "that when we start talking about\nprobability densities. ",
    "start": "770890",
    "end": "778890"
  },
  {
    "text": "OK because of the picture we\nwere just looking at, the density of the random variables\nZ at A times N,",
    "start": "778890",
    "end": "791710"
  },
  {
    "text": "namely the density of Z at this\nparticular value here is",
    "start": "791710",
    "end": "797640"
  },
  {
    "text": "just the density that we get\ncorresponding to the density",
    "start": "797640",
    "end": "803170"
  },
  {
    "text": "of some point here mapped\nover into here. And what's going to happen when\nwe take a point here and",
    "start": "803170",
    "end": "809620"
  },
  {
    "text": "map it over into here? If you have a certain amount\nof density here, which is probability per unit volume.",
    "start": "809620",
    "end": "817220"
  },
  {
    "text": "Now when you map it into here,\nand the determinant is bigger than zero, what you're doing\nis mapping a little volume",
    "start": "817220",
    "end": "824330"
  },
  {
    "text": "into a big volume. And if you're doing that over\nsmall enough region where the",
    "start": "824330",
    "end": "829660"
  },
  {
    "text": "probability density is of such\nessentially fixed, what's going to happen to the\nprobability density over here?",
    "start": "829660",
    "end": "837240"
  },
  {
    "text": "It's going to get scaled down,\nand it's going to get scaled down precisely by that\ndeterminant.",
    "start": "837240",
    "end": "844850"
  },
  {
    "text": "OK. So what this is saying is the\nprobability density of the random variable z, which is\nlinear combination of these",
    "start": "844850",
    "end": "853000"
  },
  {
    "text": "normal random variables, is in\nfact the probability density of this normal vector N, and we\nknow what that probability",
    "start": "853000",
    "end": "861529"
  },
  {
    "text": "density is, divided by this\ndeterminant of a. In fact this is a general\nformula for any old",
    "start": "861530",
    "end": "868480"
  },
  {
    "text": "probability density at all. You can start out with anything\nwhich you call a random vector N and you can\nderive the probability density",
    "start": "868480",
    "end": "877209"
  },
  {
    "text": "of any linear combination\nof those elements in precisely this way.",
    "start": "877210",
    "end": "882680"
  },
  {
    "text": "So long as this volume element\nis non-zero, which means",
    "start": "882680",
    "end": "889620"
  },
  {
    "text": "you're not mapping an entire\nspace into a sub-space. When you're mapping an entire\nspace into a sub-space and you",
    "start": "889620",
    "end": "897720"
  },
  {
    "text": "define density as being density\nper unit volume, of course the density in this map\nspace doesn't exist anymore.",
    "start": "897720",
    "end": "906140"
  },
  {
    "text": "Which is exactly what\nthis formula says. If this determinant is zero, it\nmeans this density here is",
    "start": "906140",
    "end": "912710"
  },
  {
    "text": "going to be infinite in the\nregions where this z exists at",
    "start": "912710",
    "end": "917980"
  },
  {
    "text": "all, which is just in this\nlinear sub-space, and what do you do about that?",
    "start": "917980",
    "end": "923920"
  },
  {
    "text": "I mean do you get all\nfrustrated about it? Or do you say what's going\non and treat it in",
    "start": "923920",
    "end": "931540"
  },
  {
    "text": "some sensible way? ",
    "start": "931540",
    "end": "937199"
  },
  {
    "text": "I mean the thing is\nhappening here. And this talks about\nit a little bit.",
    "start": "937200",
    "end": "942830"
  },
  {
    "text": "If a is singular, then\nA is going to map Rk into a proper sub-space.",
    "start": "942830",
    "end": "948240"
  },
  {
    "text": "Determinant A is going\nto be equal to 0. The density doesn't exist.",
    "start": "948240",
    "end": "953400"
  },
  {
    "text": "So what do you do about it? I mean what does this mean\nif you're mapping",
    "start": "953400",
    "end": "961820"
  },
  {
    "text": "into a smaller sub-space. ",
    "start": "961820",
    "end": "966840"
  },
  {
    "text": "What does it mean in terms\nof this diagram here? ",
    "start": "966840",
    "end": "973680"
  },
  {
    "text": "Well in the diagram here\nit's pretty clear. Because these little cubes here\nare getting mapped into straight lines here.",
    "start": "973680",
    "end": "980880"
  },
  {
    "text": "Yeah? What? AUDIENCE: [INAUDIBLE]",
    "start": "980880",
    "end": "987110"
  },
  {
    "text": "PROFESSOR: Some linear\ncombinations of this are being mapped into 0.",
    "start": "987110",
    "end": "992650"
  },
  {
    "text": "Namely if this straight line\nis this way any Z which is",
    "start": "992650",
    "end": "999050"
  },
  {
    "text": "going in this direction is\nbeing mapped into 0. Any vector Z which is going in\nthis direction has to be",
    "start": "999050",
    "end": "1010520"
  },
  {
    "text": "identically equaled to 0. In other words some linear\ncombination of n1 and n2 is a",
    "start": "1010520",
    "end": "1019010"
  },
  {
    "text": "random variable which takes on\nthe value 0 identically. Why do you try to represent that\nin a probabilistic sense?",
    "start": "1019010",
    "end": "1029420"
  },
  {
    "text": "Why don't you just take it out\nof consideration altogether? Here what it means is that z1\nand z2 are simply linear",
    "start": "1029420",
    "end": "1037890"
  },
  {
    "text": "combinations of each other. OK. In other words once you know\nwhat the sample value of z1",
    "start": "1037890",
    "end": "1044990"
  },
  {
    "text": "is, you can find the\nsample values z2. In other words z2 is a linear\ncombination of z1.",
    "start": "1044990",
    "end": "1055070"
  },
  {
    "text": "It's linearly dependent on z1,\nwhich means that you can identify it exactly once\nyou know what z1 is.",
    "start": "1055070",
    "end": "1063030"
  },
  {
    "text": "Which means you might as\nwell not call it a random variable at all. Which means you might as well\nview this situation where the",
    "start": "1063030",
    "end": "1070880"
  },
  {
    "text": "determinant is 0 as where the\nvector Z is really just one",
    "start": "1070880",
    "end": "1076650"
  },
  {
    "text": "random variable, and everything\nelse is determined. So you throw out these extra\nrandom variables,",
    "start": "1076650",
    "end": "1085570"
  },
  {
    "text": "pseudo-random variables, which\nare really just linear combinations of the others. So you deal with a smaller\ndimensional set.",
    "start": "1085570",
    "end": "1092059"
  },
  {
    "text": "You find the probability\ndensity of the smaller dimensional set, and you don't\nworry about all of these",
    "start": "1092060",
    "end": "1098269"
  },
  {
    "text": "mathematical peculiarities that\nwould arise otherwise. OK. So once we do that, A is going\nto be non-singular.",
    "start": "1098270",
    "end": "1108270"
  },
  {
    "text": "Because we're going to be left\nwith a set of random variables, which are\nnot linearly dependent on each other.",
    "start": "1108270",
    "end": "1114580"
  },
  {
    "text": "They can be statistically\ndependent on each other, but not linearly dependent, OK.",
    "start": "1114580",
    "end": "1120490"
  },
  {
    "text": "So for all z then, since\ndeterminant A is not 0, the",
    "start": "1120490",
    "end": "1126170"
  },
  {
    "text": "probability density at some\narbitrary vector Z is going to be the normal joint density\nevaluated at a to the minus 1z",
    "start": "1126170",
    "end": "1139400"
  },
  {
    "text": "divided by the determinant\nof A. OK. In other words we're just\nworking the map backwards.",
    "start": "1139400",
    "end": "1145850"
  },
  {
    "text": "This formula is the same as this\nformula, except instead of writing An here, we're\nwriting z here.",
    "start": "1145850",
    "end": "1152230"
  },
  {
    "text": "And when An is equal to z then\nlittle n has to be equal to A",
    "start": "1152230",
    "end": "1157450"
  },
  {
    "text": "to the minus 1z. And what does that say? It says that the joint\nprobability density has to be",
    "start": "1157450",
    "end": "1163870"
  },
  {
    "text": "equal to this quantity here. Which in matrix terms looks\nrather simple, it",
    "start": "1163870",
    "end": "1171075"
  },
  {
    "text": "looks rather nice. You can rewrite this. This is a norm here,\nso it's this vector",
    "start": "1171075",
    "end": "1178340"
  },
  {
    "text": "there times this vector. Where in fact when you want to\nmultiply vectors in this way,",
    "start": "1178340",
    "end": "1184960"
  },
  {
    "text": "you're taking inner product of\nthe vector with this cell. These are real vectors\nwe're looking at.",
    "start": "1184960",
    "end": "1190050"
  },
  {
    "text": "Because we're trying to model\nreal noise, because we're modeling the noise on\ncommunication channels.",
    "start": "1190050",
    "end": "1195840"
  },
  {
    "text": "So this is going to be the\ninner product of A to the",
    "start": "1195840",
    "end": "1201270"
  },
  {
    "text": "minus 1z with itself, which\nmeans you want to look at the transform of a to\nthe minus 1z.",
    "start": "1201270",
    "end": "1208490"
  },
  {
    "text": "Now what's the transform\nof a to the minus 1z? It's z transform times a to\nthe minus 1 transform.",
    "start": "1208490",
    "end": "1215590"
  },
  {
    "text": "So we wind up with a to the\nminus 1 transform times a to the minus 1 times z.",
    "start": "1215590",
    "end": "1221029"
  },
  {
    "text": "So we have some kind of peculiar\nbilinear form here. So for any sample value of the\nrandom vector Z we can find",
    "start": "1221030",
    "end": "1232269"
  },
  {
    "text": "the probability density in terms\nof this quantity here.",
    "start": "1232270",
    "end": "1237510"
  },
  {
    "text": "Which looks a little bit\npeculiar, but it doesn't look too bad. ",
    "start": "1237510",
    "end": "1245720"
  },
  {
    "text": "OK. Now we want to simplify\nthat a little bit. ",
    "start": "1245720",
    "end": "1251670"
  },
  {
    "text": "Anytime you're dealing with\nzero-mean random variables -- now remember I'm going to forget\nto say zero-mean half",
    "start": "1251670",
    "end": "1257430"
  },
  {
    "text": "the time because everything is\nconcerned with zero-mean random variables. The covariance of Z1 and Z2 is\nexpected value of Z1 times Z2.",
    "start": "1257430",
    "end": "1268240"
  },
  {
    "text": "So if you have a k-tuple Z,\nthe covariance is a matrix whose i,j element is expected\nvalue of Zi times Zj.",
    "start": "1268240",
    "end": "1277370"
  },
  {
    "text": "And what that means is that the\ncovariance matrix, this is a matrix now, it's going to be\nthe expected value of z times",
    "start": "1277370",
    "end": "1286330"
  },
  {
    "text": "z transpose. Z is a random vector, which is\na column random vector, Z",
    "start": "1286330",
    "end": "1293400"
  },
  {
    "text": "transpose is a row-random\nvector, which is this simply",
    "start": "1293400",
    "end": "1298890"
  },
  {
    "text": "turned upside down, turned\nby 90 degrees. Now when you multiply the\ncomponents of this vector",
    "start": "1298890",
    "end": "1304820"
  },
  {
    "text": "together, you can see that what\nyou get is the elements of this covariance matrix.",
    "start": "1304820",
    "end": "1310130"
  },
  {
    "text": "In other words this is just\nstandard matrix manipulation, which I hope most of you or at\nleast partly familiar with.",
    "start": "1310130",
    "end": "1318560"
  },
  {
    "text": "OK. When we then talk about the\nexpected values Z times Z transpose we can write this as\nthe expected value of A times",
    "start": "1318560",
    "end": "1327210"
  },
  {
    "text": "N, which is what z is. N transpose times A transpose,\nwhich is what Z transpose is.",
    "start": "1327210",
    "end": "1335640"
  },
  {
    "text": "And miraculously here the N and\nthe N transpose are in the middle here, where it's easy\nto deal with them, because",
    "start": "1335640",
    "end": "1344330"
  },
  {
    "text": "these are normal Gaussian\nrandom variables. And when you look at this column\ntimes this row, since",
    "start": "1344330",
    "end": "1353380"
  },
  {
    "text": "all diagonal elements are\nindependent of each other, and all of them have variance one,\nthe expected value of N times",
    "start": "1353380",
    "end": "1360670"
  },
  {
    "text": "N transpose is simply\nthe identity matrix. All the randomness goes out of\nthis which it obviously has to",
    "start": "1360670",
    "end": "1368809"
  },
  {
    "text": "because we're looking at a\ncovariance which is just a matrix and not something\nrandom.",
    "start": "1368810",
    "end": "1374000"
  },
  {
    "text": "So you wind up with this\ncovariance matrix is equal to a rather arbitrary matrix A,\nbut not singular, times the",
    "start": "1374000",
    "end": "1382830"
  },
  {
    "text": "transpose of that matrix. OK. We've assumed that A is\nnon-singular and therefore",
    "start": "1382830",
    "end": "1390340"
  },
  {
    "text": "it's not too hard to see the k\nsub Z is non-singular also.",
    "start": "1390340",
    "end": "1397620"
  },
  {
    "text": "And explicitly case of Z mainly\nits co-variance matrix, the inverse of it, is A to the\nminus 1 transpose times A to",
    "start": "1397620",
    "end": "1408980"
  },
  {
    "text": "the minus 1. Namely you take the inverse and\nyou start flipping things and you do all of these\nneat matrix things",
    "start": "1408980",
    "end": "1415210"
  },
  {
    "text": "that you always do. And you should review them\nif you've forgotten that.",
    "start": "1415210",
    "end": "1421290"
  },
  {
    "text": "So that when we write our\nprobability density, which was this in terms of this\ntransformation here, what we",
    "start": "1421290",
    "end": "1429180"
  },
  {
    "text": "get is the density of Z is\nequal to, in place of",
    "start": "1429180",
    "end": "1434270"
  },
  {
    "text": "determinant of A, we get the\nsquare root of the determinant in the k sub Z. You probably\ndon't remember that, but is",
    "start": "1434270",
    "end": "1443170"
  },
  {
    "text": "what you get. And it's sort of a blah. Here this is more interesting.",
    "start": "1443170",
    "end": "1449110"
  },
  {
    "text": "This is minus 1/2 z transpose\ntimes Kz to the minus 1 times z.",
    "start": "1449110",
    "end": "1455559"
  },
  {
    "text": "What does this tell you? Look at this formula.",
    "start": "1455560",
    "end": "1461490"
  },
  {
    "text": "Is it just a big formula or\nwhat does it say to you?",
    "start": "1461490",
    "end": "1466510"
  },
  {
    "text": "You got to look at these things\nand see what they say. I mean we've gone through\na lot of work to derive",
    "start": "1466510",
    "end": "1473890"
  },
  {
    "text": "something here. ",
    "start": "1473890",
    "end": "1483140"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Well it\nis Gaussian. Yes.",
    "start": "1483140",
    "end": "1488889"
  },
  {
    "text": "I mean that's the way we define\njointly Gaussian. But what's the funny\nthing about",
    "start": "1488890",
    "end": "1495760"
  },
  {
    "text": "this probability density? What does it depend on? ",
    "start": "1495760",
    "end": "1509090"
  },
  {
    "text": "What do I have to tell you in\norder for you to calculate this probability density\nfor every z you",
    "start": "1509090",
    "end": "1515140"
  },
  {
    "text": "want to plug in here? I have to tell you what this\ncovariance matrix is.",
    "start": "1515140",
    "end": "1521750"
  },
  {
    "text": "And once I tell you what the\ncovariance matrix is, there is nothing more to be specified.",
    "start": "1521750",
    "end": "1526960"
  },
  {
    "text": "In other words, a jointly\nGaussian random vector is completely specified by\nits covariance matrix.",
    "start": "1526960",
    "end": "1534019"
  },
  {
    "text": "And it's specified exactly\nthis way by its covariance matrix. OK.",
    "start": "1534020",
    "end": "1539100"
  },
  {
    "text": "There's nothing more there.  So this says anytime you're\ndealing with jointly Gaussian,",
    "start": "1539100",
    "end": "1546950"
  },
  {
    "text": "the only thing you have to\nbe interested in is this covariance here.",
    "start": "1546950",
    "end": "1552520"
  },
  {
    "text": "Namely all you need to have\njointly Gaussian is somebody has to tell you what the\ncovariance is, and somebody",
    "start": "1552520",
    "end": "1560290"
  },
  {
    "text": "has to tell you also that\nit's jointly Gaussian.",
    "start": "1560290",
    "end": "1567230"
  },
  {
    "text": "Jointly Gaussian plus a given\ncovariance specifies the probability density.",
    "start": "1567230",
    "end": "1573870"
  },
  {
    "text": "OK. What does that tell you? Well let's look at an example\nwhere we just have two random",
    "start": "1573870",
    "end": "1582100"
  },
  {
    "text": "variables, Z1 and Z2. then expected value of Z1\nsquared is the upper.",
    "start": "1582100",
    "end": "1588870"
  },
  {
    "text": "Left hand element of that\ncovariance matrix, which we'll",
    "start": "1588870",
    "end": "1595520"
  },
  {
    "text": "call sigma 1 squared. The lower, right hand side of\nthe matrix is K22, which we'll",
    "start": "1595520",
    "end": "1604050"
  },
  {
    "text": "call sigma 2 squared.  And we're going to let rho be\nthe normalize covariance.",
    "start": "1604050",
    "end": "1611429"
  },
  {
    "text": "We're just defining a bunch of\nthings here, because this is the way people usually\ndefine this.",
    "start": "1611430",
    "end": "1617470"
  },
  {
    "text": "So rho will be the normalized\ncross covariance. Then the determinant of the\nKz is this mess here.",
    "start": "1617470",
    "end": "1626100"
  },
  {
    "text": "For A to be non-singular, we\nhave to have rho less than 1. If rho is equal to 1 then this\ndeterminant is going to be",
    "start": "1626100",
    "end": "1633059"
  },
  {
    "text": "equal to 0, and we're back in\nthis awful case that we don't want to think about.",
    "start": "1633060",
    "end": "1638260"
  },
  {
    "text": "So then if we go through the\ntrouble of finding out what the inverse of K sub z is, we\nfind this 1 over 1 minus row",
    "start": "1638260",
    "end": "1647580"
  },
  {
    "text": "square times this matrix here. The probability density plugging\nthis in is this big",
    "start": "1647580",
    "end": "1654710"
  },
  {
    "text": "thing here. OK what does that tell you? ",
    "start": "1654710",
    "end": "1665580"
  },
  {
    "text": "Well the thing that it tells\nme is that I never want to deal with this, and I\nparticularly don't want to",
    "start": "1665580",
    "end": "1671880"
  },
  {
    "text": "deal with it if I'm dealing with\nthree or more variables. OK. In other words the interesting\nthing here is the simple",
    "start": "1671880",
    "end": "1679250"
  },
  {
    "text": "formula we had before, which\nis this formula.",
    "start": "1679250",
    "end": "1687470"
  },
  {
    "text": "OK. And we have computers these\ndays which say given nice",
    "start": "1687470",
    "end": "1693150"
  },
  {
    "text": "formulas like this, their\nstandard computer routines to calculate things like this.",
    "start": "1693150",
    "end": "1699930"
  },
  {
    "text": "And you never want to look at\nsome awful mess like this. OK.",
    "start": "1699930",
    "end": "1706700"
  },
  {
    "text": "And if you put a mean into here,\nwhich you will see in every textbook on random\nvariables and probability you",
    "start": "1706700",
    "end": "1714020"
  },
  {
    "text": "ever look at, this thing becomes\nso ugly that you were probably convinced before you\ntook this class that jointly",
    "start": "1714020",
    "end": "1721530"
  },
  {
    "text": "Gaussian random variables were\nthings you wanted to avoid like the plague. And if you really have to deal\nwith explicit formulas like",
    "start": "1721530",
    "end": "1728460"
  },
  {
    "text": "this, you're absolutely right. You do want to avoid them like\nthe plague, because you can't get any insight from what that\nsays, or at least I can't.",
    "start": "1728460",
    "end": "1737140"
  },
  {
    "text": "So I say OK, we have\nto deal with this. But yet we like to get a little\nmore insight about what",
    "start": "1737140",
    "end": "1743070"
  },
  {
    "text": "this means. And to do this, we like to find\na little bit more about what these bilinear forms\nare all about.",
    "start": "1743070",
    "end": "1751250"
  },
  {
    "text": "Those of you who have taken any\ncourse on linear algebra have dealt with these\nbilinear forms.",
    "start": "1751250",
    "end": "1758190"
  },
  {
    "text": "And played with them forever. And those of you who haven't are\nprobably puzzled about how",
    "start": "1758190",
    "end": "1764720"
  },
  {
    "text": "to deal with them. The notes have an appendix,\nwhich is about two pages long",
    "start": "1764720",
    "end": "1770850"
  },
  {
    "text": "which tells you what you have to\nknow about these matrices.",
    "start": "1770850",
    "end": "1776880"
  },
  {
    "text": "And I will just sort of quote\nthose results as we go.",
    "start": "1776880",
    "end": "1782610"
  },
  {
    "text": "Incidentally those results\nare not hard to derive.",
    "start": "1782610",
    "end": "1787880"
  },
  {
    "text": "And not hard to find\nout about. You can simply derive them on\nyour own, or you can look at",
    "start": "1787880",
    "end": "1793660"
  },
  {
    "text": "Strang's book on linear algebra,\nwhich is about the simplest way to get them. And that's all you need to do.",
    "start": "1793660",
    "end": "1804680"
  },
  {
    "text": "OK. We've said the probability\ndensity depends on this bilinear form z transpose\ntimes Kz to the",
    "start": "1804680",
    "end": "1812040"
  },
  {
    "text": "minus 1 time z. What is this? Is this a matrix or\na vector or what?",
    "start": "1812040",
    "end": "1817830"
  },
  {
    "text": " How many people think\nit's a matrix?",
    "start": "1817830",
    "end": "1824190"
  },
  {
    "text": " How many people think\nit's a vector?",
    "start": "1824190",
    "end": "1830110"
  },
  {
    "text": "You think it's a vector? OK. Well in a very peculiar\nsense it is. How many people think\nit's a number?",
    "start": "1830110",
    "end": "1837330"
  },
  {
    "text": "Good. OK. It is a number, and it's\na number because this is a row vector.",
    "start": "1837330",
    "end": "1844450"
  },
  {
    "text": "This is a matrix. This is a column vector. And if you think of multiplying\na matrix times a",
    "start": "1844450",
    "end": "1850650"
  },
  {
    "text": "column vector, you get\na column vector. And if you take a row vector\ntimes a column",
    "start": "1850650",
    "end": "1855940"
  },
  {
    "text": "vector, you got a number. OK. So this is just a number which\ndepends on little z.",
    "start": "1855940",
    "end": "1864420"
  },
  {
    "text": "OK. Kz is called a positive\ndefinite matrix. And it's called a positive\ndefinite matrix, because this",
    "start": "1864420",
    "end": "1871600"
  },
  {
    "text": "thing is always non-negative. And it always has to be\nnon-negative because this",
    "start": "1871600",
    "end": "1877799"
  },
  {
    "text": "refers to the -- ",
    "start": "1877800",
    "end": "1888020"
  },
  {
    "text": "if I put capital Z in here,\nnamely if I put the random vector in here Z, then what this\nis, is the variance of a",
    "start": "1888020",
    "end": "1896299"
  },
  {
    "text": "particular random variable. So it has to be greater\nthan or equal to zero.",
    "start": "1896300",
    "end": "1902100"
  },
  {
    "text": "So anyway K sub z is always\nnon-negative definite.",
    "start": "1902100",
    "end": "1907789"
  },
  {
    "text": "Here it's going to be positive\ndefinite, because we've already assumed that the matrix\nA was non-singular, and",
    "start": "1907790",
    "end": "1914030"
  },
  {
    "text": "therefore the matrix Kz has\nto be non-singular. So this has to be positive\ndefinite.",
    "start": "1914030",
    "end": "1919370"
  },
  {
    "text": "So it has an inverse, K sub\nz minus 1 is also positive definite which means this\nquantity is always greater",
    "start": "1919370",
    "end": "1927130"
  },
  {
    "text": "than zero, if z is non-zero. You can take these positive\ndefinite matrices and you can",
    "start": "1927130",
    "end": "1934340"
  },
  {
    "text": "find eigenvectors and\neigenvalues for them. Do you ever actually calculate\nthese eigenvectors and",
    "start": "1934340",
    "end": "1940340"
  },
  {
    "text": "eigenvalues? I hope not. It's a mess to do it.",
    "start": "1940340",
    "end": "1946309"
  },
  {
    "text": "I mean it's just as bad as\nwriting that awful formula we had before. So you don't want to actually do\nthis, but it's nice to know",
    "start": "1946310",
    "end": "1953419"
  },
  {
    "text": "that these things exist. And because these vectors exist,\nand in fact if you have",
    "start": "1953420",
    "end": "1964120"
  },
  {
    "text": "a matrix which is k by k, little\nk by little k, then there are k such eigenvectors\nand they can be chosen",
    "start": "1964120",
    "end": "1973130"
  },
  {
    "text": "orthonormal. OK. In other words each of these Q\nsub i are orthogonal to each",
    "start": "1973130",
    "end": "1980770"
  },
  {
    "text": "of the others. You can clearly scale them,\nbecause you scale this and scale this together.",
    "start": "1980770",
    "end": "1986420"
  },
  {
    "text": "And it's going to maintain\nequality. So you just scale them down so\nyou can make them normalize.",
    "start": "1986420",
    "end": "1998380"
  },
  {
    "text": "If you have a bunch of\neigenvectors with the same eigenvalue, then the whole\nlinear sub-space formed by",
    "start": "1998380",
    "end": "2006660"
  },
  {
    "text": "that set of eigenvectors all\nhave the same eigenvalue",
    "start": "2006660",
    "end": "2013540"
  },
  {
    "text": "lambda sub i. Namely you take any linear\ncombination of these things",
    "start": "2013540",
    "end": "2018560"
  },
  {
    "text": "which satisfy this equation\nfor a particular lambda. And any linear combination\nwill satisfy the",
    "start": "2018560",
    "end": "2024860"
  },
  {
    "text": "same the same equation. So you can simply choose an\northonormal set among them to",
    "start": "2024860",
    "end": "2031910"
  },
  {
    "text": "satisfy this. And if you look at Q sub i and\nQ sub j, which have different",
    "start": "2031910",
    "end": "2038290"
  },
  {
    "text": "eigenvalues, then it's pretty\neasy to show that in fact they have to be orthogonal\nto each other.",
    "start": "2038290",
    "end": "2045769"
  },
  {
    "text": "So anyway when you do this you\nwind up with this form becomes",
    "start": "2045770",
    "end": "2053060"
  },
  {
    "text": "just the sum over i of lambda\nsub i to the minus 1. Namely these eigenvalues\nto the minus 1.",
    "start": "2053060",
    "end": "2059770"
  },
  {
    "text": "These eigenvalues are\nall positive. Times the inner product\nof z with Q sub i.",
    "start": "2059770",
    "end": "2066730"
  },
  {
    "text": "In other words, you take\nwhatever vector Z you're interested in here, you\nproject it on these k",
    "start": "2066730",
    "end": "2075069"
  },
  {
    "text": "orthonormal vectors Q sub i. You get those k values.",
    "start": "2075070",
    "end": "2081599"
  },
  {
    "text": "And then this form here is\njust that sum there.",
    "start": "2081600",
    "end": "2087679"
  },
  {
    "text": "So when you write\nthe probability density in that way -- we still have this here\nwe'll get rid of that",
    "start": "2087680",
    "end": "2094510"
  },
  {
    "text": "in the minute -- you have e to the minus sum over\ni, these inner product",
    "start": "2094510",
    "end": "2100839"
  },
  {
    "text": "terms squared divided by\n2 times lambda sub i. That's just because this\nis equal to this.",
    "start": "2100840",
    "end": "2107010"
  },
  {
    "text": "It's just substituting this for\nthis in the formula for the probability density.",
    "start": "2107010",
    "end": "2114180"
  },
  {
    "text": "OK. What does that say\npictorially? Let me show you a picture\nof it first.",
    "start": "2114180",
    "end": "2122210"
  },
  {
    "text": "It says that for any positive\ndefinite matrix and therefore",
    "start": "2122210",
    "end": "2127839"
  },
  {
    "text": "for any covariance matrix,\nyou can always find these",
    "start": "2127840",
    "end": "2137440"
  },
  {
    "text": "orthonormal vectors. I've drawn them here\nfor two dimensions. They're just some arbitrary\nvector q1; q2 has to be",
    "start": "2137440",
    "end": "2145060"
  },
  {
    "text": "orthogonal to it. And if you look at the square\nroot of lambda 1 times q1, you",
    "start": "2145060",
    "end": "2152369"
  },
  {
    "text": "got a point here. You look at the square root of\nlambda 2 times q2, you got a point here.",
    "start": "2152370",
    "end": "2158180"
  },
  {
    "text": "If you then look at this\nprobability density here, you see that all the points on this\nellipse here have to have",
    "start": "2158180",
    "end": "2167760"
  },
  {
    "text": "the same sum of z\ntimes Q sub i.",
    "start": "2167760",
    "end": "2175730"
  },
  {
    "text": "OK. It looked a little\nbetter over here. Yes.",
    "start": "2175730",
    "end": "2181120"
  },
  {
    "text": "OK. Namely the points little z for\nwhich this is constant are the",
    "start": "2181120",
    "end": "2187380"
  },
  {
    "text": "points which form\nthis ellipse. And it's the ellipse which has\nthe axes square root of lambda",
    "start": "2187380",
    "end": "2194940"
  },
  {
    "text": "i times Q sub i. And then you just imagine it\nif it's lined up this way.",
    "start": "2194940",
    "end": "2204170"
  },
  {
    "text": "And think of what you would get\nif you were looking at the",
    "start": "2204170",
    "end": "2211720"
  },
  {
    "text": "lines of equal probability\ndensity for independent Gaussian random variables with\ndifferent variances.",
    "start": "2211720",
    "end": "2218880"
  },
  {
    "text": "I mean we already pointed out\nif the variances are all the same, these equal probability\ncontours are spheres.",
    "start": "2218880",
    "end": "2226990"
  },
  {
    "text": "If you now expand on some of\nthe axes, you get ellipses.",
    "start": "2226990",
    "end": "2233040"
  },
  {
    "text": "And now we've taken these\narbitrary vectors, so these are not in the directions we\nstarted out with, but in some",
    "start": "2233040",
    "end": "2239859"
  },
  {
    "text": "arbitrary directions. We have q1 and q2 are\northornormal to each other,",
    "start": "2239860",
    "end": "2247140"
  },
  {
    "text": "because that's the way\nwe've chosen them. And then the probability\ndensity has this form",
    "start": "2247140",
    "end": "2253640"
  },
  {
    "text": "which is this form. And the other thing we can do is\nto take this form here and",
    "start": "2253640",
    "end": "2265480"
  },
  {
    "text": "say gee this is just a\nprobability density for a bunch of independent random\nvariables, where the",
    "start": "2265480",
    "end": "2272870"
  },
  {
    "text": "independent random variables are\nthe inner product of the",
    "start": "2272870",
    "end": "2279540"
  },
  {
    "text": "random vector Z with Q sub 1 up\nto the inner product of z",
    "start": "2279540",
    "end": "2287030"
  },
  {
    "text": "with Q sub k. So these are independent\nGaussian random variables. They have variances\nlambda sub i.",
    "start": "2287030",
    "end": "2294480"
  },
  {
    "text": "And this is the nicest formula\nfor the probability density of",
    "start": "2294480",
    "end": "2299850"
  },
  {
    "text": "an arbitrary set of jointly\ndensity of jointly Gaussian random variables.",
    "start": "2299850",
    "end": "2305900"
  },
  {
    "text": "OK. In other words what this says\nis in general if you have a set of jointly Gaussian random\nvariables and I have this",
    "start": "2305900",
    "end": "2314190"
  },
  {
    "text": "messy form here, all you're\ndoing is looking at them in a wrong coordinate system.",
    "start": "2314190",
    "end": "2320240"
  },
  {
    "text": "If you switch them around, you\nlook at them this way instead of this way, you're going to\nhave independent Gaussian",
    "start": "2320240",
    "end": "2327990"
  },
  {
    "text": "random variables. And the way to look at them\nis found by solving this",
    "start": "2327990",
    "end": "2335240"
  },
  {
    "text": "eigenvector eigenvalue equation,\nwhich will tell you what these orthogonal\ndirections are.",
    "start": "2335240",
    "end": "2341720"
  },
  {
    "text": "And then it'll tell you how to\nget this nice picture that looks this way.",
    "start": "2341720",
    "end": "2346780"
  },
  {
    "text": "OK. OK so that tells us what we\nneed to know, maybe even a",
    "start": "2346780",
    "end": "2353040"
  },
  {
    "text": "little more than we have to\nknow, about jointly Gaussian random variables.",
    "start": "2353040",
    "end": "2359320"
  },
  {
    "text": "But there's one more\nbonus we get. ",
    "start": "2359320",
    "end": "2364530"
  },
  {
    "text": "And the bonus is\nthe following. If you create a matrix here B\nwhere the i'th row of B is",
    "start": "2364530",
    "end": "2374780"
  },
  {
    "text": "this vector Q sub i divided by\nthe square root of lambda sub i, then what this is going to\ndo is the corresponding",
    "start": "2374780",
    "end": "2383819"
  },
  {
    "text": "element here is going to be a\nnormalized Gaussian random variable with variance one and\nall of these are going to be",
    "start": "2383820",
    "end": "2392339"
  },
  {
    "text": "independent of each other. OK. That's essentially what\nwe were saying before.",
    "start": "2392340",
    "end": "2398330"
  },
  {
    "text": "This is just another way of\nsaying the same thing. That when you squish this\nprobability density around and",
    "start": "2398330",
    "end": "2408720"
  },
  {
    "text": "look at it in a different frame\nof reference, and then you scale the random variables\ndown or up, what you wind up",
    "start": "2408720",
    "end": "2415460"
  },
  {
    "text": "with is IID normal Gaussian\nrandom variables.",
    "start": "2415460",
    "end": "2420640"
  },
  {
    "text": "OK. But that says that Z is\nequal to B to the minus 1 times N prime.",
    "start": "2420640",
    "end": "2426470"
  },
  {
    "text": "Well so what? Here's the reason for so what. We started out with a definition\nof jointly",
    "start": "2426470",
    "end": "2432800"
  },
  {
    "text": "Gaussian, which is probably not\nthe definition of jointly Gaussian you've ever seen if\nyou've seen this before.",
    "start": "2432800",
    "end": "2441190"
  },
  {
    "text": "Then what we did was to say,\nOK if you have jointly Gaussian random variables\nand they're not linearly",
    "start": "2441190",
    "end": "2447770"
  },
  {
    "text": "independent and none of them are\nlinearly dependent on the",
    "start": "2447770",
    "end": "2454700"
  },
  {
    "text": "others, then this matrix\nA is invertible. From that we derive the\nprobability density.",
    "start": "2454700",
    "end": "2463250"
  },
  {
    "text": "From the probability density\nwe derive this. OK. The only thing you need\nto get this is",
    "start": "2463250",
    "end": "2470099"
  },
  {
    "text": "the probability density. And this says that anytime you\nhave a random vector Z with",
    "start": "2470100",
    "end": "2476520"
  },
  {
    "text": "this probability density that\nwe just wrote down. ",
    "start": "2476520",
    "end": "2483170"
  },
  {
    "text": "Then you have the property that\nN prime is equal to BZ, and Z is equal to B to\nthe minus 1 N prime.",
    "start": "2483170",
    "end": "2490600"
  },
  {
    "text": "Which says that if you have this\nprobability density, then you have jointly Gaussian\nrandom variables.",
    "start": "2490600",
    "end": "2497440"
  },
  {
    "text": "So we have an alternate definition of jointly Gaussian. Random variables are jointly\nGaussian if they have this",
    "start": "2497440",
    "end": "2504319"
  },
  {
    "text": "probability density.  You can somehow represent them\nas linear combinations of",
    "start": "2504320",
    "end": "2512400"
  },
  {
    "text": "normal random variables. OK. Then there's something\neven simpler. It says that if all linear\ncombinations of a random",
    "start": "2512400",
    "end": "2520080"
  },
  {
    "text": "vector Z are Gaussian, then\nZ is jointly Gaussian. Why is that?",
    "start": "2520080",
    "end": "2525910"
  },
  {
    "text": "well If you look at this formula\nhere, it says take any old random vector at all that\nhas a covariance matrix.",
    "start": "2525910",
    "end": "2534750"
  },
  {
    "text": "From that covariance matrix, we\ncan solve for all of these eigenvectors.",
    "start": "2534750",
    "end": "2540230"
  },
  {
    "start": "2540230",
    "end": "2557510"
  },
  {
    "text": "If I find the appropriate random\nvariables here from this transformation, those\nrandom variables are then",
    "start": "2557510",
    "end": "2566250"
  },
  {
    "text": "uncorrelated from each other,\nthey are all statistically independent of each other, And\nit follows from that, that if",
    "start": "2566250",
    "end": "2573970"
  },
  {
    "text": "all these linear combinations\nare Gaussian then Z has to be jointly Gaussian.",
    "start": "2573970",
    "end": "2580150"
  },
  {
    "text": "OK. So we now have three\ndefinitions. This one is the simplest\none to state.",
    "start": "2580150",
    "end": "2585740"
  },
  {
    "text": "It's the hardest one\nto work with. That's life you know.",
    "start": "2585740",
    "end": "2591210"
  },
  {
    "text": "This one is probably the most\nstraightforward, because you don't have to know anything to\nunderstand this definition.",
    "start": "2591210",
    "end": "2599730"
  },
  {
    "text": "This original definition is\nphysically the most appealing because it shows why noise\nvectors actually",
    "start": "2599730",
    "end": "2607780"
  },
  {
    "text": "do have this property.  OK. So here's a summary\nof all of this.",
    "start": "2607780",
    "end": "2615329"
  },
  {
    "text": "It says if Kz is singular, you\nwant to remove the linearly independent random variables.",
    "start": "2615330",
    "end": "2621820"
  },
  {
    "text": "You just take them away because\nthey're uniquely specified in terms of\nthe other variables.",
    "start": "2621820",
    "end": "2628600"
  },
  {
    "text": "And then you take the resulting\nnon-singular matrix,",
    "start": "2628600",
    "end": "2635540"
  },
  {
    "text": "and Z is going to be jointly\nGaussian if and only if Z is equal to AN for some normal\nrandom variable N. If Z has",
    "start": "2635540",
    "end": "2644470"
  },
  {
    "text": "jointly Gaussian density or if\nall linear combinations of Z are Gaussian all of this for 0\nmean would mean it applies to",
    "start": "2644470",
    "end": "2652930"
  },
  {
    "text": "fluctuation. OK. So why do we have to know all of\nthat about jointly Gaussian",
    "start": "2652930",
    "end": "2660240"
  },
  {
    "text": "random variables? Well because everything about\nGaussian processes depends on",
    "start": "2660240",
    "end": "2666680"
  },
  {
    "text": "jointly Gaussian random\nvariables. You can't do anything with\nGaussian processes without",
    "start": "2666680",
    "end": "2672910"
  },
  {
    "text": "being able to look\nat these jointly Gaussian random variables. And the reason is that we said\nthat Z of t is a Gaussian",
    "start": "2672910",
    "end": "2680770"
  },
  {
    "text": "process if for every k and every\nset of time instance,",
    "start": "2680770",
    "end": "2687200"
  },
  {
    "text": "every set of e, that's\nt1 to t sub k. If Z of t1 up to Z of\ntk is a jointly",
    "start": "2687200",
    "end": "2694049"
  },
  {
    "text": "Gaussian random vector. OK. So that directly links the\ndefinition of a Gaussian",
    "start": "2694050",
    "end": "2700250"
  },
  {
    "text": "process to Gaussian\nrandom vectors. OK. Supposed the sample functions\nof Z of t or L2 with",
    "start": "2700250",
    "end": "2707450"
  },
  {
    "text": "probability one. I want to say a little bit about\nthis because otherwise you can't sort out any of these\nthings about how L2",
    "start": "2707450",
    "end": "2716800"
  },
  {
    "text": "theory connects with\nrandom processes.",
    "start": "2716800",
    "end": "2721910"
  },
  {
    "text": "OK. So I'm going to start out by\njust assuming that all these sample functions are going\nto be L2 functions with",
    "start": "2721910",
    "end": "2730280"
  },
  {
    "text": "probability 1. One way to ensure this is to\nlook only at processes of the",
    "start": "2730280",
    "end": "2736290"
  },
  {
    "text": "form Z of t equals some sum\nof Z sub i times ti of t.",
    "start": "2736290",
    "end": "2742210"
  },
  {
    "text": "OK remember at the beginning\nwe started looking at this process the sum of a set of\nnormalized Gaussian random",
    "start": "2742210",
    "end": "2750470"
  },
  {
    "text": "variables times sinc\nfunctions times displaced sinc functions.",
    "start": "2750470",
    "end": "2755800"
  },
  {
    "text": "You can also do the same\nthing with Fourier coefficients or anything. You got a fairly general set\nof processes that way.",
    "start": "2755800",
    "end": "2763020"
  },
  {
    "text": " unfortunately they don't quite\nwork, because if you look at",
    "start": "2763020",
    "end": "2769779"
  },
  {
    "text": "the sinc functions and you\nlook at noise, which is independent and identically\ndistributed in time, then the",
    "start": "2769780",
    "end": "2776510"
  },
  {
    "text": "sample functions are going\nto have infinite energy. I mean that kind of process\njust runs on forever.",
    "start": "2776510",
    "end": "2784370"
  },
  {
    "text": "It runs on with finite\npower forever. And therefore it has\ninfinite energy.",
    "start": "2784370",
    "end": "2790510"
  },
  {
    "text": "And therefore the simplest\nprocess to look at, the sample",
    "start": "2790510",
    "end": "2795550"
  },
  {
    "text": "functions are non-L2 with\nprobability one, which is sort",
    "start": "2795550",
    "end": "2801680"
  },
  {
    "text": "of unfortunate. So we say OK we don't care about\nthat, because if you",
    "start": "2801680",
    "end": "2807490"
  },
  {
    "text": "want to look at that process\na sum of Z sub i times sinc functions, what do\nyou care about?",
    "start": "2807490",
    "end": "2816410"
  },
  {
    "text": "I mean you only care about the\nterms in that expansion, which run from the big bang until\nthe next big bang.",
    "start": "2816410",
    "end": "2827600"
  },
  {
    "text": "OK. We certainly don't care about it\nbefore that or after that.",
    "start": "2827600",
    "end": "2832620"
  },
  {
    "text": "And if we look within those\nfinite time limits, then all these functions are\ngoing to be L2.",
    "start": "2832620",
    "end": "2839569"
  },
  {
    "text": "Because they just last for\na finite amount of time. So all we need to do is to\ntruncate these things somehow.",
    "start": "2839570",
    "end": "2846890"
  },
  {
    "text": "And we're going to diddle around\na little bit with a question of how to truncate\nthese series.",
    "start": "2846890",
    "end": "2853650"
  },
  {
    "text": "But for the time being we\njust say we can do that. And we will do it. So we can look at any processes\nthe form sum of Zi",
    "start": "2853650",
    "end": "2861450"
  },
  {
    "text": "times phi i of t, where the Zi\nare independent and the phi i of t are orthonormal.",
    "start": "2861450",
    "end": "2868510"
  },
  {
    "text": "And to make things L2, we're\ngoing to assume that the sum",
    "start": "2868510",
    "end": "2874180"
  },
  {
    "text": "over i of Zi squared bar\nis less than infinity.",
    "start": "2874180",
    "end": "2884770"
  },
  {
    "text": "OK. In other words we only take a\nfinite number of these things, or if we want to take infinite\na number of them, the",
    "start": "2884770",
    "end": "2891230"
  },
  {
    "text": "variances are going\nto go off to zero. And I don't know whether you're\nproving it in the",
    "start": "2891230",
    "end": "2897600"
  },
  {
    "text": "homework this time or you will\nprove it in the homework next time, I forget, but you're going\nto look at the question",
    "start": "2897600",
    "end": "2904460"
  },
  {
    "text": "of why this finite variance\ncondition makes these sample functions be L2 with\nprobability one.",
    "start": "2904460",
    "end": "2917120"
  },
  {
    "text": "OK. So if you had this condition,\nthen all your sample functions",
    "start": "2917120",
    "end": "2923050"
  },
  {
    "text": "are going to be L2. I'm going to get off\nall of this L2 business relatively shortly.",
    "start": "2923050",
    "end": "2928869"
  },
  {
    "text": "I want to do a little bit\nof it to start with. Because if any of you have start\ndoing any research in",
    "start": "2928870",
    "end": "2935600"
  },
  {
    "text": "this area, at some point you're\ngoing to be merrily working away calculating\nall sorts of things.",
    "start": "2935600",
    "end": "2942880"
  },
  {
    "text": "And suddenly you're going to\nfind that none of it exists, because of these problems\nof infinite energy.",
    "start": "2942880",
    "end": "2949380"
  },
  {
    "text": "And you're going to\nget very puzzled. So one of the things I tried to\ndo in the notes is to write them in way that you can\nunderstand them at a first",
    "start": "2949380",
    "end": "2957240"
  },
  {
    "text": "reading without worrying\nabout any of this. And then when you go back for a\nsecond reading, you can pick",
    "start": "2957240",
    "end": "2963860"
  },
  {
    "text": "up all the mathematics\nthat you need. So that in fact you won't have\nthe problem of suddenly",
    "start": "2963860",
    "end": "2969810"
  },
  {
    "text": "finding out that three-quarters\nof your thesis has to be thrown away, because\nyou've been dealing with",
    "start": "2969810",
    "end": "2975150"
  },
  {
    "text": "things that don't\nmake any sense. OK. So, we're going to define linear\nfunctionals in the",
    "start": "2975150",
    "end": "2981180"
  },
  {
    "text": "following way. We're going to first look at the\nsample functions of this",
    "start": "2981180",
    "end": "2986340"
  },
  {
    "text": "random process Z. OK. Now we talked about\nthis last time.",
    "start": "2986340",
    "end": "2991600"
  },
  {
    "text": "If you have a random process Z\nthan really what you have is a set of functions defined\non some samples space.",
    "start": "2991600",
    "end": "2999530"
  },
  {
    "text": "So the quantities you're\ninterested in is what is the value of the random process at\ntime t for sample point omega.",
    "start": "2999530",
    "end": "3011760"
  },
  {
    "text": "OK. If we look at that and make it\nfor a given omega, this thing",
    "start": "3011760",
    "end": "3019490"
  },
  {
    "text": "becomes a function of t. In fact for a given omega, this\nis just what we've been calling a sample element\nof the random process.",
    "start": "3019490",
    "end": "3028000"
  },
  {
    "text": "So if we take this sample\nelement, look at the inner product of that with some\nfunction g of t.",
    "start": "3028000",
    "end": "3034600"
  },
  {
    "text": "In other words we look at the\nintegral of Z of t omega times g of t, dt.",
    "start": "3034600",
    "end": "3041290"
  },
  {
    "text": "And if all these sample\nfunctions are L2 and if g of t is L2, what happens when you\ntake the integral of an L2",
    "start": "3041290",
    "end": "3049170"
  },
  {
    "text": "function times an L2 function,\nwhich is the inner product of",
    "start": "3049170",
    "end": "3056020"
  },
  {
    "text": "something L2 with something L2\nwhich says something with finite energy inner\nproduct with",
    "start": "3056020",
    "end": "3062930"
  },
  {
    "text": "something with finite energy. Well the Schwarz inequality\ntells you that if this has",
    "start": "3062930",
    "end": "3071560"
  },
  {
    "text": "finite energy and this has\nfinite energy, the inner product exists.",
    "start": "3071560",
    "end": "3077740"
  },
  {
    "text": "That's the reason why we went\nthrough the Schwarz inequality. It's the main reason\nfor doing that.",
    "start": "3077740",
    "end": "3082900"
  },
  {
    "text": "So these things have\nfinite value. So V of omega the results of\ndoing this namely V as a",
    "start": "3082900",
    "end": "3090220"
  },
  {
    "text": "function of the sample space\nis a real number. And it's a real number for the\nsample points of omega with",
    "start": "3090220",
    "end": "3098380"
  },
  {
    "text": "probability one, which means\nwe can talk about V as a random variable.",
    "start": "3098380",
    "end": "3103800"
  },
  {
    "text": "OK. And now V is a random variable\nwhich is defined in this way.",
    "start": "3103800",
    "end": "3109290"
  },
  {
    "text": "And from now on we will call\nthese things linear functionals which are in fact\nthe integral of a random",
    "start": "3109290",
    "end": "3115529"
  },
  {
    "text": "process times a function. And we can take that\nkind of integral.",
    "start": "3115530",
    "end": "3122520"
  },
  {
    "text": "It sort of looks like the linear\ncombinations of things we were doing before when we\nwere talking about matrices",
    "start": "3122520",
    "end": "3131930"
  },
  {
    "text": "and random vectors. ",
    "start": "3131930",
    "end": "3141280"
  },
  {
    "text": "OK. If we restrict the random\nprocess to have the following form, where these are\nindependent and these are",
    "start": "3141280",
    "end": "3147110"
  },
  {
    "text": "orthonormal, then one of these\nlinear functionals is given by",
    "start": "3147110",
    "end": "3153160"
  },
  {
    "text": "the random variable V is going\nto be the integral of Z of t times g of t, but\nZ of t is this.",
    "start": "3153160",
    "end": "3160789"
  },
  {
    "text": "And at this point we're not\ngoing to fuss about interchanging integrals\nwith summations.",
    "start": "3160790",
    "end": "3167910"
  },
  {
    "text": "You have the machinery to do it,\nbecause we're now dealing with an L2 space. We're not going to\nfuss about it.",
    "start": "3167910",
    "end": "3174730"
  },
  {
    "text": "And I advise you not\nto fuss about it. So we have a sum of these random\nvariables here times",
    "start": "3174730",
    "end": "3182080"
  },
  {
    "text": "these integrals here. These integrals here are just\nprojections of g of t on this",
    "start": "3182080",
    "end": "3187319"
  },
  {
    "text": "space of orthonormal\nfunctions. So whatever space of orthonormal\nfunctions gives",
    "start": "3187320",
    "end": "3192589"
  },
  {
    "text": "you your jollies, use it talk\nabout the inner products on that space.",
    "start": "3192590",
    "end": "3198130"
  },
  {
    "text": "This gives you a nice inner\nproducts space of sequences of numbers.",
    "start": "3198130",
    "end": "3204760"
  },
  {
    "text": "And then if the z i are jointly\nGaussian, then V is going to be Gaussian.",
    "start": "3204760",
    "end": "3210109"
  },
  {
    "text": "And then to generalize this one\nlittle bit further, if you take a whole bunch of L2\nfunctions, g1 of t g2 of t and",
    "start": "3210110",
    "end": "3220010"
  },
  {
    "text": "so forth, you can talk about\na whole bunch of random variables V1 up to V sub j, 0.",
    "start": "3220010",
    "end": "3227670"
  },
  {
    "text": "And V sub j is going to be\nthe integral of Z of t times gj of tdt.",
    "start": "3227670",
    "end": "3233450"
  },
  {
    "text": "Remember this thing\nlooks very simple. It looks like the --",
    "start": "3233450",
    "end": "3240180"
  },
  {
    "text": "like the convolutions you've\nbeen doing all your life. It's not. It's really a rather\npeculiar quantity.",
    "start": "3240180",
    "end": "3248510"
  },
  {
    "text": "This in fact is what we call a\nlinear functional, and is the integral of a random\nprocess times this.",
    "start": "3248510",
    "end": "3256720"
  },
  {
    "text": "Which we have defined in terms\nof the sample functions of the random process.",
    "start": "3256720",
    "end": "3262270"
  },
  {
    "text": "And now we said OK now that we\nunderstand what it is, we will just write this all the time.",
    "start": "3262270",
    "end": "3268770"
  },
  {
    "text": "But I just caution\nyou not to let familiarity breed contempt.",
    "start": "3268770",
    "end": "3273980"
  },
  {
    "text": "Because this is a rather\npeculiar notion.",
    "start": "3273980",
    "end": "3279440"
  },
  {
    "text": "And a rather powerful notion. OK. So these things are\njointly Gaussian.",
    "start": "3279440",
    "end": "3285800"
  },
  {
    "text": "We want to take the expected\nvalue of V sub i times V sub j, and now we're going to do\nthis without worrying about",
    "start": "3285800",
    "end": "3295020"
  },
  {
    "text": "being careful at all. We have the expected value of\nthe integral of Z of t, gi of",
    "start": "3295020",
    "end": "3300900"
  },
  {
    "text": "t, td times the integral of\nZ tau gj of tau, d tau.",
    "start": "3300900",
    "end": "3307520"
  },
  {
    "text": "And now we're going to slide\nthis expected value inside of both of these integrals.",
    "start": "3307520",
    "end": "3314010"
  },
  {
    "text": "And not worry about it. And therefore what we're going\nto have is a double integral",
    "start": "3314010",
    "end": "3321400"
  },
  {
    "text": "of gi of t expected value of z\nof t time z of tau times gj of",
    "start": "3321400",
    "end": "3327500"
  },
  {
    "text": "tau dt, d tau, which\nis this thing here.",
    "start": "3327500",
    "end": "3334470"
  },
  {
    "text": "Which you should compare with\nwhat we've been dealing with most of the lecture today.",
    "start": "3334470",
    "end": "3341300"
  },
  {
    "text": "This is the same kind of form\nfor a covariance function as",
    "start": "3341300",
    "end": "3347200"
  },
  {
    "text": "we've been dealing with for\ncovariance matrices. It has very similar effects.",
    "start": "3347200",
    "end": "3355660"
  },
  {
    "text": "I mean before you were just\ntalking about finite dimensional matrices, which is\nall simple mathematically in",
    "start": "3355660",
    "end": "3362059"
  },
  {
    "text": "eigenfunctions and\neigenvalues. You have eigenfunctions\nand eigenvalues of",
    "start": "3362060",
    "end": "3367120"
  },
  {
    "text": "these things also. And so long as these are defined\nnicely by these L2",
    "start": "3367120",
    "end": "3374690"
  },
  {
    "text": "properties we've been\ntalking about. In fact you can deal with these\nin virtually the same",
    "start": "3374690",
    "end": "3380130"
  },
  {
    "text": "way that you can deal with\nthe matrices we were",
    "start": "3380130",
    "end": "3385369"
  },
  {
    "text": "dealing with before. If you just remember what the\nresults are from matrices, you can guess what they are for\nthese covariance functions.",
    "start": "3385370",
    "end": "3394770"
  },
  {
    "text": "OK. But anyway you can find the\nexpected value of Vi times Vj by this formula.",
    "start": "3394770",
    "end": "3400630"
  },
  {
    "text": "Again we're dealing with\nzero-mean and therefore we don't have to worry about the\nmean, put that in later.",
    "start": "3400630",
    "end": "3406820"
  },
  {
    "text": " And that all exists.",
    "start": "3406820",
    "end": "3414390"
  },
  {
    "text": "OK. So the next thing we want\nto deal with, hitting you with a lot today.",
    "start": "3414390",
    "end": "3419590"
  },
  {
    "text": "But I mean the trouble is a lot\nof this is half familiar",
    "start": "3419590",
    "end": "3425500"
  },
  {
    "text": "to most of you. People who have taken various\ncommunication courses at",
    "start": "3425500",
    "end": "3430680"
  },
  {
    "text": "various places have all been\nexposed to random processes in some highly trivialized sense.",
    "start": "3430680",
    "end": "3438980"
  },
  {
    "text": "But the major results are the\nsame as the results we're going through here. And all we're doing here is\nadding a little bit of",
    "start": "3438980",
    "end": "3445010"
  },
  {
    "text": "carefulness about what works\nand what doesn't work. Incidentally in the notes which\nis towards the end of",
    "start": "3445010",
    "end": "3452849"
  },
  {
    "text": "lectures 14 and 15, we give\nthree examples which let you",
    "start": "3452850",
    "end": "3459250"
  },
  {
    "text": "know why in fact we want to\nlook primarily at random processes which are defined in\nterms of a sum of independent",
    "start": "3459250",
    "end": "3468790"
  },
  {
    "text": "Gaussian random variables time\northonormal functions. And if you look at those\nthree examples, some",
    "start": "3468790",
    "end": "3476430"
  },
  {
    "text": "of them have problems. Because of the fact that\neverything you're dealing with",
    "start": "3476430",
    "end": "3482559"
  },
  {
    "text": "has infinite energy. And therefore it doesn't\nreally make any sense.",
    "start": "3482560",
    "end": "3487810"
  },
  {
    "text": "And one of them I should\ntalk about this just a little bit in class. And I think I still have a\ncouple of minutes, is a very",
    "start": "3487810",
    "end": "3494730"
  },
  {
    "text": "strange process where\nZ of t is IID.",
    "start": "3494730",
    "end": "3504140"
  },
  {
    "text": "In fact just let it be normal. ",
    "start": "3504140",
    "end": "3510310"
  },
  {
    "text": "And independent for all t. ",
    "start": "3510310",
    "end": "3519170"
  },
  {
    "text": "OK. In other words you generate a\nrandom process by looking at",
    "start": "3519170",
    "end": "3525810"
  },
  {
    "text": "an uncountably infinite\ncollection of normal random variables.",
    "start": "3525810",
    "end": "3532260"
  },
  {
    "text": "How do you deal with\nsuch a process? I don't know how to\ndeal with it.",
    "start": "3532260",
    "end": "3538920"
  },
  {
    "text": "I mean it sounds like\nit's simple. If I put this on a quiz,\nthree-quarters of you would",
    "start": "3538920",
    "end": "3546609"
  },
  {
    "text": "say oh that's very simple. What we're dealing with is a\nfamily of impulse functions.",
    "start": "3546610",
    "end": "3553280"
  },
  {
    "text": "Spaced arbitrarily\nclosely together. This is not impulse function.",
    "start": "3553280",
    "end": "3559410"
  },
  {
    "text": "Impulse functions are even worse\nthan this, but this is bad enough.",
    "start": "3559410",
    "end": "3565000"
  },
  {
    "text": "When we start talking about\nspectral density, we can explain this a little bit better\nby thinking this kind",
    "start": "3565000",
    "end": "3573020"
  },
  {
    "text": "of process it doesn't\nmake any sense. But this kind of process, if\nyou look at its spectral",
    "start": "3573020",
    "end": "3579680"
  },
  {
    "text": "density, it's going to have a\nspectral density which is zero",
    "start": "3579680",
    "end": "3586440"
  },
  {
    "text": "everywhere, but whose integral\nover all frequencies is one. ",
    "start": "3586440",
    "end": "3593059"
  },
  {
    "text": "OK. In other words it's not\nsomething you want to wish on your on your worse friend.",
    "start": "3593060",
    "end": "3599670"
  },
  {
    "text": "It makes a certain amount of\nsense as a limit of things. You can look at a very broadband\nprocess where in",
    "start": "3599670",
    "end": "3606450"
  },
  {
    "text": "fact you spread the process\nout enormously. You can make pseudo noise which\nlooks sort of like this.",
    "start": "3606450",
    "end": "3613270"
  },
  {
    "text": "And you make the process broader\nand broader and lower and lower intensity\neverywhere.",
    "start": "3613270",
    "end": "3618370"
  },
  {
    "text": "But it still has this\nenergy of one. It still has a power\nof one everywhere.",
    "start": "3618370",
    "end": "3624770"
  },
  {
    "text": "And it just is ugly. OK. Now if you never worried about\nthese questions of L2, you",
    "start": "3624770",
    "end": "3632480"
  },
  {
    "text": "would look at a process like\nthat and say, gee there must be some easy way to handle that\nbecause it's probably the",
    "start": "3632480",
    "end": "3639140"
  },
  {
    "text": "easiest process you\ncan define. I mean everything is normal.",
    "start": "3639140",
    "end": "3645050"
  },
  {
    "text": "If you look at any set at\ndifferent times, you get a set of IID normal Gaussian\nvariables.",
    "start": "3645050",
    "end": "3652559"
  },
  {
    "text": "You try to put it together, and\nit doesn't mean anything. If you pass it through a filter,\nthe filter is going to",
    "start": "3652560",
    "end": "3659700"
  },
  {
    "text": "cancel it all out. So anyway, that's one reason\nwhy we want to look at this",
    "start": "3659700",
    "end": "3669190"
  },
  {
    "text": "restricted class of random\nprocesses we're looking at.",
    "start": "3669190",
    "end": "3674470"
  },
  {
    "text": "OK. What we're interested in now is\nwe want to take a Gaussian random process really, but you\ncan take any random process.",
    "start": "3674470",
    "end": "3684140"
  },
  {
    "text": "We want to pass it through a\nfilter, and we want to look at the random process\nthat comes out.",
    "start": "3684140",
    "end": "3690180"
  },
  {
    "text": "OK. And that certainly is a very\nphysical kind of operation. I mean any kind of communication\nsystem that you",
    "start": "3690180",
    "end": "3697320"
  },
  {
    "text": "build is going to have\nnoise on the channel.",
    "start": "3697320",
    "end": "3702870"
  },
  {
    "text": "And one of the first things\nyou're going to do is you're going to filter what\nyou've received.",
    "start": "3702870",
    "end": "3708860"
  },
  {
    "text": "So you have to have some way\nof dealing with this. OK. And the way we sort of been\ndealing with it all along in",
    "start": "3708860",
    "end": "3718190"
  },
  {
    "text": "terms of the transmitted way\nforms we've been dealing with is to say OK.",
    "start": "3718190",
    "end": "3723860"
  },
  {
    "text": "What we're going to do is to\nfirst look what happens when we take sample functions of\nthis, pass them through the",
    "start": "3723860",
    "end": "3730360"
  },
  {
    "text": "filter, and then what comes\nout is going to be some function again. And then we're back into\nwhat you studied as an",
    "start": "3730360",
    "end": "3738599"
  },
  {
    "text": "undergraduate talking about\nfunctions through filters. We're going to jazz it up a\nlittle bit by saying these",
    "start": "3738600",
    "end": "3745329"
  },
  {
    "text": "functions are going to\nbe L2 the filters are going to be L2. So that in fact you know\nyou'd get something",
    "start": "3745330",
    "end": "3751020"
  },
  {
    "text": "out that make sense. So what we're doing is looking\nat these sample functions V",
    "start": "3751020",
    "end": "3757480"
  },
  {
    "text": "the output at time tau for\nsample point omega is going to",
    "start": "3757480",
    "end": "3763020"
  },
  {
    "text": "be the convolution of the input\nat time t and sample",
    "start": "3763020",
    "end": "3771760"
  },
  {
    "text": "point omega. Remember this one sample point\nexists for all time.",
    "start": "3771760",
    "end": "3777260"
  },
  {
    "text": "That's why these sample points\nand sample spaces are so damn complicated. Because they have everything\nin them.",
    "start": "3777260",
    "end": "3784650"
  },
  {
    "text": "OK. So there's one sample point\nwhich exists for all of them. This is a sample function.",
    "start": "3784650",
    "end": "3791650"
  },
  {
    "text": "You're passing the sample\nfunction through a filter, which is just normal\nconvolution.",
    "start": "3791650",
    "end": "3797920"
  },
  {
    "text": "What comes out then. If in fact we express this\nrandom process in terms of",
    "start": "3797920",
    "end": "3806240"
  },
  {
    "text": "this orthonormal sum the way\nwe've been doing before. Is you get the sum over j of\nthis, which is a sample value",
    "start": "3806240",
    "end": "3816060"
  },
  {
    "text": "of the j'th random variable\ncoming out times the integral of pj of t, h of tau\nminus t, d tau.",
    "start": "3816060",
    "end": "3823800"
  },
  {
    "text": "OK. For each tau that you look at,\nthis is just a sample value of a linear functional.",
    "start": "3823800",
    "end": "3830030"
  },
  {
    "text": "OK. If I want to look at this at one\nvalue of tau, I have this integral here which is\na random process.",
    "start": "3830030",
    "end": "3839990"
  },
  {
    "text": "A sample value of a\nrandom process at omega time a function.",
    "start": "3839990",
    "end": "3845230"
  },
  {
    "text": "This is just a function\nof t for given tau. OK. So this is a linear\nfunctional.",
    "start": "3845230",
    "end": "3851770"
  },
  {
    "text": "As a type we've been\ntalking about. And that linear functional\nis then given by",
    "start": "3851770",
    "end": "3857740"
  },
  {
    "text": "this for each tau. This is a sample value\nof a linear functional we can talk about.",
    "start": "3857740",
    "end": "3866020"
  },
  {
    "text": "OK. These things are then, if you\nlook over the whole sample space omega, V of tau becomes\na random variable.",
    "start": "3866020",
    "end": "3875230"
  },
  {
    "text": "OK. V of tau is the random variable\nwhose sample values",
    "start": "3875230",
    "end": "3881940"
  },
  {
    "text": "are V of t omega, and they're\ngiven by this.",
    "start": "3881940",
    "end": "3889200"
  },
  {
    "text": "So if z of t is Gaussian\nprocess, you get jointly",
    "start": "3889200",
    "end": "3895050"
  },
  {
    "text": "Gaussian linear functionals at\neach of any set of times tau",
    "start": "3895050",
    "end": "3902400"
  },
  {
    "text": "1, tau 2 up to tau sub k. So this just gives you a whole\nset of linear functionals.",
    "start": "3902400",
    "end": "3909650"
  },
  {
    "text": "And if z of t is a Gaussian\nprocess, then all these linear",
    "start": "3909650",
    "end": "3914930"
  },
  {
    "text": "functionals are going to be\njointly Gaussian also. And bingo.",
    "start": "3914930",
    "end": "3920460"
  },
  {
    "text": "What we have then is an\nalternate way to generate Gaussian processes. OK.",
    "start": "3920460",
    "end": "3925730"
  },
  {
    "text": "In other words you can generate\na Gaussian process by specifying at each set of k\ntimes, you have jointly",
    "start": "3925730",
    "end": "3934320"
  },
  {
    "text": "Gaussian random variables. But once you do that, once you\nunderstand one Gaussian",
    "start": "3934320",
    "end": "3939890"
  },
  {
    "text": "process, you're off\nand running. Because then you can pass\nit through any L2 filter",
    "start": "3939890",
    "end": "3946060"
  },
  {
    "text": "that you want to. And you generate another\nGaussian process. So for example, if you start\nout with this sinc type",
    "start": "3946060",
    "end": "3957000"
  },
  {
    "text": "process, I mean we'll see that\nthat has a spectral density,",
    "start": "3957000",
    "end": "3964820"
  },
  {
    "text": "which is flat over\nall frequencies. And we'll talk about spectral\ndensity tomorrow.",
    "start": "3964820",
    "end": "3970670"
  },
  {
    "text": "But then you pass it through a\nlinear filter, and you can give it any spectral density\nthat you want.",
    "start": "3970670",
    "end": "3976260"
  },
  {
    "text": "So at this point, we really\nhave enough to talk about arbitrary covariance functions\njust by starting out with",
    "start": "3976260",
    "end": "3986810"
  },
  {
    "text": "random processes, which are\ndefined in terms of some sequence of orthonormal\nrandom variables.",
    "start": "3986810",
    "end": "3993180"
  },
  {
    "start": "3993180",
    "end": "4000280"
  },
  {
    "text": "OK. Now we can get to covariance\nfunction of a filter process in the same way as we got the\nmatrix for linear functionals.",
    "start": "4000280",
    "end": "4012200"
  },
  {
    "text": "OK. And this is just computation. OK. So what is it? The covariance function of\nthis output process V",
    "start": "4012200",
    "end": "4021720"
  },
  {
    "text": "evaluated at one time\nr another time s. One of the nasty things about\nnotation when you start",
    "start": "4021720",
    "end": "4030550"
  },
  {
    "text": "dealing with a covariance\nfunction of the input and the output to a linear filter is\nyou suddenly need to worry",
    "start": "4030550",
    "end": "4038860"
  },
  {
    "text": "about two times at the\ninput and two other times at the output. OK. Because this thing is then\nexpected value of V sub r",
    "start": "4038860",
    "end": "4049869"
  },
  {
    "text": "times the expected\nvalue of V subs. OK. This is a random variable, which\nis the process V of r",
    "start": "4049870",
    "end": "4056510"
  },
  {
    "text": "evaluated at time r. This is the random variable,\nwhich is the process V of t",
    "start": "4056510",
    "end": "4063410"
  },
  {
    "text": "evaluated at a particular\ntime s. This is going to be the expected\nvalue of what this",
    "start": "4063410",
    "end": "4072060"
  },
  {
    "text": "random variable now is the\nintegral of the random process z of t times this function\nhere, which is now",
    "start": "4072060",
    "end": "4083365"
  },
  {
    "text": "a function of t. Because we're looking at\na fixed value of r. So this is a linear functional,",
    "start": "4083365",
    "end": "4090290"
  },
  {
    "text": "which is a random variable. This is another linear\nfunctional. This is evaluated at some time\ns, which is the output of the",
    "start": "4090290",
    "end": "4098440"
  },
  {
    "text": "filter at time s. Then we will throw caution to\nthe wind interchange integrals",
    "start": "4098440",
    "end": "4104940"
  },
  {
    "text": "with expectation\nand everything. And then in the middle we'll\nhave expected value of z of t",
    "start": "4104940",
    "end": "4111210"
  },
  {
    "text": "times z of tau, which is the\ncovariance function of z.",
    "start": "4111210",
    "end": "4116859"
  },
  {
    "text": "OK. So the covariance function of\nz then specifies what the",
    "start": "4116860",
    "end": "4122150"
  },
  {
    "text": "covariance function\nof the output is. OK.",
    "start": "4122150",
    "end": "4127310"
  },
  {
    "text": "So whenever you pass a random\nprocess through a filter if",
    "start": "4127310",
    "end": "4133810"
  },
  {
    "text": "you know what the covariance\nfunction of input to the filter is, you can find the\ncovariance function of the",
    "start": "4133810",
    "end": "4140609"
  },
  {
    "text": "output of the filter. That's a kind of a nasty\nformula, it's not very nice.",
    "start": "4140610",
    "end": "4147200"
  },
  {
    "text": "But anyway the thing that it\ntells you is that whether this",
    "start": "4147200",
    "end": "4152799"
  },
  {
    "text": "random process is\nGaussian or not. The only thing that determines\nthe covariance function of the",
    "start": "4152800",
    "end": "4159139"
  },
  {
    "text": "output of the filter is the\ncovariance function of the input to the filter plus of\ncourse the filter response,",
    "start": "4159140",
    "end": "4165420"
  },
  {
    "text": "which is needed OK. ",
    "start": "4165420",
    "end": "4171299"
  },
  {
    "text": "And this is just the same kind\nof bilinear form that we were dealing with before.",
    "start": "4171300",
    "end": "4177699"
  },
  {
    "text": "Next time we will talk a little\nbit about the fact that when you're dealing with a\nbilinear form like this, you",
    "start": "4177700",
    "end": "4185210"
  },
  {
    "text": "can take these covariance\nfunctions and they have the same kind of eigenvalues and\neigenvectors that we had",
    "start": "4185210",
    "end": "4192870"
  },
  {
    "text": "before for a matrix. Namely this is again going to\nbe positive definite as a",
    "start": "4192870",
    "end": "4199630"
  },
  {
    "text": "function, and we will be able\nto find its eigenvectors and its eigenvalues.",
    "start": "4199630",
    "end": "4205179"
  },
  {
    "text": "We can't calculate them. Computers can calculate them. People who've spend their\nlives doing this",
    "start": "4205180",
    "end": "4211310"
  },
  {
    "text": "can calculate them. I wouldn't suggest that you\nspend your life doing this.",
    "start": "4211310",
    "end": "4216770"
  },
  {
    "text": "Because again you would be\nsetting yourself up as a second class computer, and\nyou don't make any",
    "start": "4216770",
    "end": "4225060"
  },
  {
    "text": "profit out of that. But anyway, we can find this\nin principle from this.",
    "start": "4225060",
    "end": "4230270"
  },
  {
    "text": "OK. One of the things that we\nhaven't talked about at all yet, and which we will start\ntalking about next time in",
    "start": "4230270",
    "end": "4237349"
  },
  {
    "text": "which the next set of lecture\nnotes, lecture 16, we'll deal",
    "start": "4237350",
    "end": "4242720"
  },
  {
    "text": "with is the question\nof stationarity. Let me say just a little bit\nabout that to get into it.",
    "start": "4242720",
    "end": "4249940"
  },
  {
    "text": "And then we'll talk a lot\nmore about it next time. The notes will probably be on\nthe web some time tomorrow.",
    "start": "4249940",
    "end": "4257620"
  },
  {
    "text": "I hope before noon if you\nwant to look at them. Physically if you look at a\nstochastic process, and you",
    "start": "4257620",
    "end": "4268670"
  },
  {
    "text": "want to model it. I mean suppose you want to\nmodel a noise process.",
    "start": "4268670",
    "end": "4274010"
  },
  {
    "text": "How do you model a\nnoise process? Well you look at it over\na long period of time.",
    "start": "4274010",
    "end": "4280850"
  },
  {
    "text": "You start taking statistics\nabout it over a long period of time.",
    "start": "4280850",
    "end": "4286480"
  },
  {
    "text": "And somehow you want to model\nit in such a way, I mean the only thing you can look at\nis statistics over a",
    "start": "4286480",
    "end": "4292719"
  },
  {
    "text": "long period of time. So if you're only looking at one\nprocess, you can look at it for a year and then you can\nmodel it, and then you can use",
    "start": "4292720",
    "end": "4301060"
  },
  {
    "text": "that model for the\nnext 10 years. And what that's assuming is that\nthe noise process looks",
    "start": "4301060",
    "end": "4307960"
  },
  {
    "text": "the same way this year\nas it does next year. You can go further than that\nand say, OK I'm going to",
    "start": "4307960",
    "end": "4315360"
  },
  {
    "text": "manufacture cell phones or some\nother kind of widget. And what I'm interested in then\nis what these noise wave",
    "start": "4315360",
    "end": "4322820"
  },
  {
    "text": "forms are to the whole\ncollection of my widgets. Namely different people\nwill buy my widgets.",
    "start": "4322820",
    "end": "4328270"
  },
  {
    "text": "They will use them in\ndifferent places. So I'm interested in modeling\nthe noise over this whole set",
    "start": "4328270",
    "end": "4333700"
  },
  {
    "text": "of widgets. But still if you're doing that\nyou're still almost forced to",
    "start": "4333700",
    "end": "4339810"
  },
  {
    "text": "deal with models which have the\nsame statistics over at",
    "start": "4339810",
    "end": "4345090"
  },
  {
    "text": "least a broad range of times. Sometimes when we're dealing\nwith wireless communication we",
    "start": "4345090",
    "end": "4358550"
  },
  {
    "text": "say, no the channel keeps\nchanging in time. and the channel keeps changing\nslowly in time.",
    "start": "4358550",
    "end": "4364420"
  },
  {
    "text": "And therefore you don't have\nthe same statistics now as you have then.",
    "start": "4364420",
    "end": "4370659"
  },
  {
    "text": "If you want to understands that,\nbelieve me, the only way you're going to understand it is\nto first understand how to",
    "start": "4370660",
    "end": "4378180"
  },
  {
    "text": "deal with statistics for\nthe channel which stay the same forever. And once you understand those\nstatistics, you will then be",
    "start": "4378180",
    "end": "4387010"
  },
  {
    "text": "in a position to start to\nunderstand what happens when these statistics\nchange slowly.",
    "start": "4387010",
    "end": "4393970"
  },
  {
    "text": "OK. In other words, what our\nmodeling assumption is in this course, and I believe it's the\nright modeling assumption for",
    "start": "4393970",
    "end": "4402750"
  },
  {
    "text": "all engineers, is that you never\nstart with some physical",
    "start": "4402750",
    "end": "4407840"
  },
  {
    "text": "phenomena and say, I want to\ntest the hell out of this until I find an appropriate\nstatistical model for it.",
    "start": "4407840",
    "end": "4416500"
  },
  {
    "text": "You do that only after\nyou know enough about random processes. That you know how to deal with\nan enormous variety of",
    "start": "4416500",
    "end": "4424680"
  },
  {
    "text": "different home cooked\nrandom processes. OK. So what we do in a course like\nthis is we deal with lots of",
    "start": "4424680",
    "end": "4431920"
  },
  {
    "text": "different home cooked random\nprocesses, which is in fact why we've done rather peculiar\nthings like saying, let's look",
    "start": "4431920",
    "end": "4438880"
  },
  {
    "text": "at a random process which\ncomes from a sum of independent random variables\nmultiplied",
    "start": "4438880",
    "end": "4446830"
  },
  {
    "text": "by orthonormal functions. And you see what we've\naccomplished by that already.",
    "start": "4446830",
    "end": "4452110"
  },
  {
    "text": "Namely by starting out that way\nwe've been able to define Gaussian processes.",
    "start": "4452110",
    "end": "4458270"
  },
  {
    "text": "We've been able to define what\nhappens when one of those Gaussian processes goes\nthrough a filter.",
    "start": "4458270",
    "end": "4464110"
  },
  {
    "text": "And in fact, that gives a way\nof generating a lot more random processes.",
    "start": "4464110",
    "end": "4469250"
  },
  {
    "text": "And next time what we're going\nto do is not to say how is it",
    "start": "4469250",
    "end": "4474380"
  },
  {
    "text": "that we know that processes\nare stationary. How do you test whether\nprocesses are stationary or not.",
    "start": "4474380",
    "end": "4480119"
  },
  {
    "text": "But we're just going to assume\nthat they're stationary. In other words if they had the\nsame statistics now as they're",
    "start": "4480120",
    "end": "4485910"
  },
  {
    "text": "going to have next year. And those statistics stay\nthe same forever.",
    "start": "4485910",
    "end": "4491080"
  },
  {
    "text": "And then see what we\ncan say about it. To give you a clue as to how\nto start looking at this,",
    "start": "4491080",
    "end": "4496940"
  },
  {
    "text": "remember what we said quite\na long time ago about Markov chains.",
    "start": "4496940",
    "end": "4503640"
  },
  {
    "text": "OK. And now when you look at Markov\nchains, remember that what happens at one time is\nstatistically a function of",
    "start": "4503640",
    "end": "4511270"
  },
  {
    "text": "what happened at the unit\nof time before it. OK.",
    "start": "4511270",
    "end": "4517690"
  },
  {
    "text": "But we can still model those\nMarkov change as being stationary.",
    "start": "4517690",
    "end": "4523410"
  },
  {
    "text": "Because the dependents at this\ntime on the previous sample of time is the same now as it will\nbe five years from now.",
    "start": "4523410",
    "end": "4533020"
  },
  {
    "text": "OK. In other words you can't just\nlook at the process at one instant of time and say\nthis is independent",
    "start": "4533020",
    "end": "4538980"
  },
  {
    "text": "of all other times. That's not what stationary\nmeans. What stationary means is the way\nthe process depends on the",
    "start": "4538980",
    "end": "4547230"
  },
  {
    "text": "past at time t is the same is\nthe way it depends on the past at some later time tau.",
    "start": "4547230",
    "end": "4554430"
  },
  {
    "text": "And that in fact is the way\nwe're going to define stationarity. It's at these joint sample times\nthat we're going to be",
    "start": "4554430",
    "end": "4563739"
  },
  {
    "text": "looking at. Which I had a better\nword for that.",
    "start": "4563740",
    "end": "4569390"
  },
  {
    "text": "Join sets of epochs that\nwe'll be looking at. The joint statistics over a set\nof epics is going to be",
    "start": "4569390",
    "end": "4575570"
  },
  {
    "text": "the same now as it will be at\nsometime in the future. And that's the way we're going\nto define stationarity.",
    "start": "4575570",
    "end": "4582040"
  },
  {
    "text": "A prelude of what we're going\nto find when we do that is that this covariance function,\nif the covariance function at",
    "start": "4582040",
    "end": "4590690"
  },
  {
    "text": "time t and time tau is the same\nif you translate it up to",
    "start": "4590690",
    "end": "4596160"
  },
  {
    "text": "t plus t1 and tau plus t1, then\nin fact this function is",
    "start": "4596160",
    "end": "4601560"
  },
  {
    "text": "going to be a function only if\nthe difference t minus tau. It's going to be a function\nof one variable",
    "start": "4601560",
    "end": "4607850"
  },
  {
    "text": "instead of two variables. OK. So as soon as we get this\nfunction being a function of",
    "start": "4607850",
    "end": "4613710"
  },
  {
    "text": "one variable instead of two\nvariables, first thing we're going to do is to take the\nFourier transform of this.",
    "start": "4613710",
    "end": "4622010"
  },
  {
    "text": "Because then we'll be taking\nthe Fourier transform of a function of a single variable.",
    "start": "4622010",
    "end": "4627710"
  },
  {
    "text": "We're going to call\nthat the spectral density of the process. And we're going to find that for\nstationary processes the",
    "start": "4627710",
    "end": "4637100"
  },
  {
    "text": "spectrum densities tell you everything if they're Gaussian. Why is that?",
    "start": "4637100",
    "end": "4642930"
  },
  {
    "text": "Well the inverse Fourier\ntransform is this covariance function.",
    "start": "4642930",
    "end": "4648469"
  },
  {
    "text": "And we've now seen that the\ncovariance function tells you everything about a\nGaussian process.",
    "start": "4648470",
    "end": "4654200"
  },
  {
    "text": "So if you know the spectral\ndensity for a stationary process, it will tell\nyou everything. We will also have to fiddle\naround a little bit about how",
    "start": "4654200",
    "end": "4662720"
  },
  {
    "text": "we define stationarity. But at the same time\ndon't have this infinite energy problem.",
    "start": "4662720",
    "end": "4669580"
  },
  {
    "text": "And the way we're going\nto do it is the way we've done it all along. We're going to take something\nthat looked stationary, we're going to truncate it over some\nlong period of time, and we're",
    "start": "4669580",
    "end": "4678210"
  },
  {
    "text": "going to have our cake and\neat it too that way. OK. So we'll do that next time.",
    "start": "4678210",
    "end": "4683660"
  },
  {
    "start": "4683660",
    "end": "4683886"
  }
]