[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13330"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13330",
    "end": "21650"
  },
  {
    "text": "PROFESSOR: We introduced\nthe data last time. These were some\nmacroeconomic variables",
    "start": "21650",
    "end": "27700"
  },
  {
    "text": "that can be used for forecasting\nthe economy in terms of growth",
    "start": "27700",
    "end": "33990"
  },
  {
    "text": "and factors such as\ninflation or unemployment.",
    "start": "33990",
    "end": "39330"
  },
  {
    "text": "The case note goes through\nanalyzing just three of these economic time\nseries-- the unemployment rate,",
    "start": "39330",
    "end": "47690"
  },
  {
    "text": "the federal funds rate,\nand a measure of the CPI, or Consumer Price Index. ",
    "start": "47690",
    "end": "56449"
  },
  {
    "text": "When one fits a vector\nautoregression model to this data, it turns\nout that the roots",
    "start": "56450",
    "end": "68940"
  },
  {
    "text": "of the characteristic polynomial\nare 1.002, then 0.9863.",
    "start": "68940",
    "end": "76800"
  },
  {
    "text": "And you recall when our\ndiscussion of vector autoregressive models, there's\na characteristic equation",
    "start": "76800",
    "end": "83140"
  },
  {
    "text": "sort of in matrix\nform, the determinant is just like the univariate\nautoregressive case.",
    "start": "83140",
    "end": "89720"
  },
  {
    "text": "And in order for the process\nto be invertible, basically,",
    "start": "89720",
    "end": "104120"
  },
  {
    "text": "the roots of the\ncharacteristic polynomial need to be less\nthan 1 in magnitude.",
    "start": "104120",
    "end": "110370"
  },
  {
    "text": "In this implementation of the\nvector autoregression model, the characteristic\nroots are the inverses",
    "start": "110370",
    "end": "117220"
  },
  {
    "text": "of the characteristic roots\nthat we've been discussing. So anyway, this particular fit\nof the vector autoregression",
    "start": "117220",
    "end": "123770"
  },
  {
    "text": "model suggests that the\nprocess is non-stationary.",
    "start": "123770",
    "end": "131370"
  },
  {
    "text": "And so one should be\nconsidering different series",
    "start": "131370",
    "end": "137580"
  },
  {
    "text": "to model this as a\nstationary time series. But in terms of interpreting\nthe regression model,",
    "start": "137580",
    "end": "146519"
  },
  {
    "text": "one can see-- to accommodate\nthe non-stationarity,",
    "start": "146520",
    "end": "156320"
  },
  {
    "text": "we can take differences\nof all the series and fit the vector\nautoregression",
    "start": "156320",
    "end": "163360"
  },
  {
    "text": "to the difference series. So one way of eliminating any\nnon-stationarity in time series",
    "start": "163360",
    "end": "169210"
  },
  {
    "text": "models, basically\neliminate the random walk aspect of the processes, is to\nbe modeling first differences.",
    "start": "169210",
    "end": "177290"
  },
  {
    "text": "And so doing that with\nthis series-- let's see.",
    "start": "177290",
    "end": "186180"
  },
  {
    "text": "Here is just a graph of\nthe time series properties of the difference series.",
    "start": "186180",
    "end": "191800"
  },
  {
    "text": " So with our original series, we\ntake differences and eliminate",
    "start": "191800",
    "end": "199180"
  },
  {
    "text": "missing values in this R code. And this\nautocorrelation function",
    "start": "199180",
    "end": "205300"
  },
  {
    "text": "shows us basically\nthe correlations",
    "start": "205300",
    "end": "211100"
  },
  {
    "text": "and autocorrelations\nof individual series and the cross-correlations\nacross the different series.",
    "start": "211100",
    "end": "216950"
  },
  {
    "text": "So along the diagonals are\nthe autocorrelation function. And one can see\nthat every series",
    "start": "216950",
    "end": "223799"
  },
  {
    "text": "is correlation one with itself. But then at the first\nlag, positive for the Fed",
    "start": "223800",
    "end": "232380"
  },
  {
    "text": "funds and the CPI measure. And there's also some\ncross-correlations",
    "start": "232380",
    "end": "238980"
  },
  {
    "text": "that are strong. And whether or not a\ncorrelation is strong or not",
    "start": "238980",
    "end": "244180"
  },
  {
    "text": "depends upon how much\nuncertainty there is in our estimate\nof the correlation. And these dashed\nlines here correspond",
    "start": "244180",
    "end": "251750"
  },
  {
    "text": "to plus or minus two standard\ndeviations of the correlation",
    "start": "251750",
    "end": "256980"
  },
  {
    "text": "coefficient when the correlation\ncoefficient is equal to 0.",
    "start": "256980",
    "end": "263440"
  },
  {
    "text": "So any correlations that sort\nof go beyond those bounds",
    "start": "263440",
    "end": "268470"
  },
  {
    "text": "is statistically significant.  The partial autocorrelation\nfunction is graphed here.",
    "start": "268470",
    "end": "279210"
  },
  {
    "text": "And let's say our\ntime series problem set goes through some discussion\nof the partial autocorrelation",
    "start": "279210",
    "end": "286040"
  },
  {
    "text": "coefficients and the\ninterpretation of those. The partial autocorrelation\ncoefficients",
    "start": "286040",
    "end": "291910"
  },
  {
    "text": "are the correlation\nbetween one variable",
    "start": "291910",
    "end": "297450"
  },
  {
    "text": "and the lag of another\nafter explaining for all lower degree lags. So it's like the incremental\ncorrelation of a variable",
    "start": "297450",
    "end": "306479"
  },
  {
    "text": "with a lag term that exists. And so if we are actually\nfitting regression models where",
    "start": "306480",
    "end": "313830"
  },
  {
    "text": "we include extra lags\nof a given variable, that partial\nautocorrelation coefficient",
    "start": "313830",
    "end": "320569"
  },
  {
    "text": "is essentially the correlation\nassociated with the addition of the final lagged variable.",
    "start": "320570",
    "end": "327620"
  },
  {
    "text": "So here, we can see that\neach of these series is quite strongly\ncorrelated with itself.",
    "start": "327620",
    "end": "333950"
  },
  {
    "text": "But there are also\nsome cross-correlations with, like, the unemployment\nrate and the Fed funds rate.",
    "start": "333950",
    "end": "342750"
  },
  {
    "text": "Basically, the Fed\nfunds rate tends to go down when the\nunemployment rate goes up.",
    "start": "342750",
    "end": "350400"
  },
  {
    "text": "And so this data is\nindicating the association between these\nmacroeconomic variables",
    "start": "350400",
    "end": "356639"
  },
  {
    "text": "and the evidence\nof that behavior. In terms of modeling the\nactual structural relations",
    "start": "356640",
    "end": "362100"
  },
  {
    "text": "between these, we need\nseveral, up to about 10 or 12 variables more\nthan these three.",
    "start": "362100",
    "end": "368380"
  },
  {
    "text": "And then one can have\na better understanding of the drivers of various\nmacroeconomic features.",
    "start": "368380",
    "end": "375750"
  },
  {
    "text": "But this sort of\nillustrates the use of these methods with this\nreduced variable case. ",
    "start": "375750",
    "end": "382830"
  },
  {
    "text": "Let me also go\ndown here and just comment on the unemployment\nrate or the Fed funds rate.",
    "start": "382830",
    "end": "393710"
  },
  {
    "start": "393710",
    "end": "406050"
  },
  {
    "text": "When fitting these vector\nautoregressive models with the packages\nthat exist in R,",
    "start": "406050",
    "end": "412069"
  },
  {
    "text": "they give us output which\nprovides the specification of each of the\nautoregressive models",
    "start": "412070",
    "end": "421440"
  },
  {
    "text": "for the different dependent\nvariables, the different series of the process.",
    "start": "421440",
    "end": "427620"
  },
  {
    "text": "And so here is the case of the\nregression model for Fed funds",
    "start": "427620",
    "end": "433610"
  },
  {
    "text": "as a function of\nunemployment rate lagged, Fed funds rate lagged,\nand CPI lagged.",
    "start": "433610",
    "end": "441040"
  },
  {
    "text": "These are all on\nthe different scale. When you're looking at\nthese results, what's",
    "start": "441040",
    "end": "447729"
  },
  {
    "text": "important is\nbasically how strong the signal-to-noise\nratio is for estimating",
    "start": "447730",
    "end": "453850"
  },
  {
    "text": "these autoregressive\nparameters, vector autoregressive parameters.",
    "start": "453850",
    "end": "459130"
  },
  {
    "text": "And so with the Fed funds,\nyou can look at the t values. And t values that\nare larger than 2",
    "start": "459130",
    "end": "465920"
  },
  {
    "text": "are certainly quite significant. And you can see that basically\nwhen the unemployment rate",
    "start": "465920",
    "end": "473539"
  },
  {
    "text": "coefficient is a negative\n0.71, so if the unemployment",
    "start": "473540",
    "end": "479250"
  },
  {
    "text": "rate goes up, we expect to\nsee the Fed rate going down",
    "start": "479250",
    "end": "485270"
  },
  {
    "text": "the next month. And the Fed funds rate for the\nlag 1 has a t value of 7.97.",
    "start": "485270",
    "end": "495650"
  },
  {
    "text": "So these are now models\non the differences. So if the Fed funds\nrate was increased",
    "start": "495650",
    "end": "501480"
  },
  {
    "text": "last month or last quarter, it's\nlikely to be increased again. And that's partly a factor\nof how slow the economy is",
    "start": "501480",
    "end": "511560"
  },
  {
    "text": "in reacting to changes\nand how the Fed doesn't want to shock the economy with\nlarge changes in their policy",
    "start": "511560",
    "end": "520200"
  },
  {
    "text": "rates. Another thing to notice here\nis that there's actually",
    "start": "520200",
    "end": "526600"
  },
  {
    "text": "a negative coefficient\non the lag 2 Fed funds term, a negative 0.17.",
    "start": "526600",
    "end": "534490"
  },
  {
    "text": "And in interpreting\nthese kinds of models, I think it's helpful\njust to think of,",
    "start": "534490",
    "end": "542510"
  },
  {
    "text": "if you have Fed\nfunds sub t, that's equal to minus 0.71 times the\nunemployment rate at t minus 1.",
    "start": "542510",
    "end": "553970"
  },
  {
    "text": "And then we have plus 0.37 times\nthe Fed funds, so t minus 1.",
    "start": "553970",
    "end": "564050"
  },
  {
    "text": "And this is delta. And then minus 1.8\ntimes the Fed funds.",
    "start": "564050",
    "end": "571330"
  },
  {
    "text": "So t minus 2. In interpreting\nthese coefficients,",
    "start": "571330",
    "end": "579290"
  },
  {
    "text": "notice that these\ntwo terms correspond to 0.19 times the Fed funds\nchange 1 lag ago plus 0.18",
    "start": "579290",
    "end": "597110"
  },
  {
    "text": "times the change in that rate. ",
    "start": "597110",
    "end": "603550"
  },
  {
    "text": "So when you see\nmultiple lags coming into play in these models,\nthe interpretation of them",
    "start": "603550",
    "end": "611720"
  },
  {
    "text": "can be made by considering\ndifferent transformations",
    "start": "611720",
    "end": "617560"
  },
  {
    "text": "essentially of the\nunderlying variables. In this form, you can see\nthat OK, the Fed funds",
    "start": "617560",
    "end": "623130"
  },
  {
    "text": "tends to change the way it\nchanged the previous month.",
    "start": "623130",
    "end": "630180"
  },
  {
    "text": "But it also may change\ndepending on the double change",
    "start": "630180",
    "end": "638644"
  },
  {
    "text": "in the previous month. So there's a degree of\nacceleration in the Fed funds that is being captured here.",
    "start": "638644",
    "end": "644450"
  },
  {
    "text": "So the interpretation\nof these models sometimes requires some care.",
    "start": "644450",
    "end": "651930"
  },
  {
    "text": "This kind of analysis,\nI find it quite useful. ",
    "start": "651930",
    "end": "662600"
  },
  {
    "text": "So let's push on\nto the next topic.",
    "start": "662600",
    "end": "669709"
  },
  {
    "text": "So today's topics are going\nto begin with a discussion of cointegration.",
    "start": "669710",
    "end": "675639"
  },
  {
    "text": "Cointegration is a major topic\nin time series analysis, which is dealing with the analysis\nof non-stationary time series.",
    "start": "675640",
    "end": "683980"
  },
  {
    "text": "And in the previous\ndiscussion, we addressed\nnon-stationarity of series",
    "start": "683980",
    "end": "689910"
  },
  {
    "text": "by taking first\ndifferences to eliminate that non-stationarity. ",
    "start": "689910",
    "end": "696440"
  },
  {
    "text": "But we may be losing\nsome information with that differencing.",
    "start": "696440",
    "end": "701450"
  },
  {
    "text": "And cointegration\nprovides a framework within which we\ncharacterize all available",
    "start": "701450",
    "end": "707440"
  },
  {
    "text": "information for\nstatistical modeling, in a very systematic way.",
    "start": "707440",
    "end": "712920"
  },
  {
    "text": "So let's introduce the\ncontext within which",
    "start": "712920",
    "end": "718579"
  },
  {
    "text": "cointegration is relevant. It's relevant when we\nhave a stochastic process,",
    "start": "718580",
    "end": "725810"
  },
  {
    "text": "a multivariate\nstochastic process, which is integrated of some order d.",
    "start": "725810",
    "end": "732060"
  },
  {
    "text": "And to be integrated\nof order d means that if we take the\nd-th difference,",
    "start": "732060",
    "end": "738920"
  },
  {
    "text": "then that d-th\ndifference is stationary. ",
    "start": "738920",
    "end": "743980"
  },
  {
    "text": "So and if you look\nat a time series",
    "start": "743980",
    "end": "753720"
  },
  {
    "text": "and you plot that over time,\nwell, OK, a stationary time series we know should be\nsomething that basically",
    "start": "753720",
    "end": "763010"
  },
  {
    "text": "has a constant mean over time. There's some steady\nmean which that has.",
    "start": "763010",
    "end": "768580"
  },
  {
    "text": "And the variability\nis also constant. With some other time series,\nit might increase linearly",
    "start": "768580",
    "end": "779000"
  },
  {
    "text": "over time. And a series that increases\nlinearly over time, well, if you take first\ndifferences, that",
    "start": "779000",
    "end": "785069"
  },
  {
    "text": "tends to take out\nthat linear trend. If there are higher order\ndifferencing is required, then",
    "start": "785070",
    "end": "790230"
  },
  {
    "text": "that means that there's some\ncurvature, quadratic say, that may exist in the data\nthat is being taken out.",
    "start": "790230",
    "end": "798759"
  },
  {
    "text": "So this differencing is required\nto result in stationarity.",
    "start": "798760",
    "end": "805460"
  },
  {
    "text": "If the process does have vector\nautoregressive representation",
    "start": "805460",
    "end": "812430"
  },
  {
    "text": "in spite of its\nnon-stationarity, then it can be represented by\na polynomial lag of the x's is",
    "start": "812430",
    "end": "823920"
  },
  {
    "text": "equal to white noise epsilon. And the polynomial\nphi of L going",
    "start": "823920",
    "end": "833590"
  },
  {
    "text": "to have a factor term\nin there of 1 minus L,",
    "start": "833590",
    "end": "839180"
  },
  {
    "text": "basically the first\ndifference to the d power. So if taking these the\nd-th order difference",
    "start": "839180",
    "end": "846300"
  },
  {
    "text": "reduces it to\nstationarity, then we",
    "start": "846300",
    "end": "852430"
  },
  {
    "text": "can express this vector\nautoregression in this way. So the phi star of L\nbasically represents",
    "start": "852430",
    "end": "866620"
  },
  {
    "text": "the stationary vector\nautoregressive process on the d-th difference series.",
    "start": "866620",
    "end": "873255"
  },
  {
    "start": "873255",
    "end": "887730"
  },
  {
    "text": "Now, as it says here, each\nof the component series",
    "start": "887730",
    "end": "892779"
  },
  {
    "text": "may be non-stationary and\nintegrated, say of order one. But the process itself may\nnot be jointly integrated.",
    "start": "892780",
    "end": "902770"
  },
  {
    "text": "In that it may be that there\nare linear combinations",
    "start": "902770",
    "end": "908900"
  },
  {
    "text": "of our multivariate series\nwhich are stationary. And so these linear\ncombinations basically",
    "start": "908900",
    "end": "920570"
  },
  {
    "text": "represent the stationary\nfeatures of the process. And those features can be\napparent without looking",
    "start": "920570",
    "end": "931160"
  },
  {
    "text": "at differences. So in a sense, if\nyou just focused on differences of these\nnon-stationary multivariate",
    "start": "931160",
    "end": "938880"
  },
  {
    "text": "series, you would be\nlosing out on information of the stationary structure\nof contemporaneous components",
    "start": "938880",
    "end": "949900"
  },
  {
    "text": "of the multivariate series. And so cointegration\ndeals with this situation",
    "start": "949900",
    "end": "956130"
  },
  {
    "text": "where some linear combinations\nof the multivariate series",
    "start": "956130",
    "end": "961480"
  },
  {
    "text": "in fact are stationary. ",
    "start": "961480",
    "end": "968810"
  },
  {
    "text": "So how do we represent\nthat mathematically?",
    "start": "968810",
    "end": "975090"
  },
  {
    "text": "Well, we say that this\nmultivariate time series process is cointegrated if\nthere exists an m-vector beta",
    "start": "975090",
    "end": "984360"
  },
  {
    "text": "such that, defining linear\nweights on the x's, and",
    "start": "984360",
    "end": "989470"
  },
  {
    "text": "beta prime X_t is a\nstationary process. ",
    "start": "989470",
    "end": "997920"
  },
  {
    "text": "The cointegration vector of\nbeta can be scaled arbitrarily. So it's common\npractice, if one has",
    "start": "997920",
    "end": "1009110"
  },
  {
    "text": "an interest, some primary\ninterest, perhaps, in the first component\nseries of process, to set that equal to 1.",
    "start": "1009110",
    "end": "1016680"
  },
  {
    "text": "And the expression\nbasically says that our time t value\nof the first series",
    "start": "1016680",
    "end": "1026470"
  },
  {
    "text": "is related in a stationary\nway to a linear combination",
    "start": "1026470",
    "end": "1031929"
  },
  {
    "text": "of the other m minus 1 series. And this is a long-run\nequilibrium type relationship.",
    "start": "1031930",
    "end": "1041859"
  },
  {
    "text": "How does this arise? Well, it arises in many, many\nways in economics and finance.",
    "start": "1041859",
    "end": "1050570"
  },
  {
    "text": " The term structure of interest\nrates, purchase power parity.",
    "start": "1050570",
    "end": "1056000"
  },
  {
    "text": " In the terms structure\nof interest rates,",
    "start": "1056000",
    "end": "1062659"
  },
  {
    "text": "basically the differences\nbetween yields on interest rates over\ndifferent maturities,",
    "start": "1062660",
    "end": "1070260"
  },
  {
    "text": "those differences\nmight be stationary. The overall level of interest\nmight not be stationary,",
    "start": "1070260",
    "end": "1076780"
  },
  {
    "text": "but the spreads ought\nto be stationary. The purchase power parity\nin foreign exchange,",
    "start": "1076780",
    "end": "1084679"
  },
  {
    "text": "if you look at the\nvalue of currencies",
    "start": "1084680",
    "end": "1090940"
  },
  {
    "text": "for different countries,\nbasically different countries ought to be able to purchase\nthe same goods for roughly",
    "start": "1090940",
    "end": "1099710"
  },
  {
    "text": "the same price. And so if there are\ndisparities in currency values, purchase power parity suggests\nthat things will revert back",
    "start": "1099710",
    "end": "1107740"
  },
  {
    "text": "to some norm where everybody\nis paying on average over time",
    "start": "1107740",
    "end": "1112900"
  },
  {
    "text": "the same amount for\ndifferent goods. Otherwise, there\nwould be arbitrage. ",
    "start": "1112900",
    "end": "1120029"
  },
  {
    "text": "Money demand, covered\ninterest rate parity, law of one price,\nspot and futures. Let me show you\nanother example that",
    "start": "1120030",
    "end": "1128470"
  },
  {
    "text": "will be in the case\nstudy for this chapter.",
    "start": "1128470",
    "end": "1134820"
  },
  {
    "start": "1134820",
    "end": "1140289"
  },
  {
    "text": "View, full screen.",
    "start": "1140290",
    "end": "1146410"
  },
  {
    "text": "Let's think about\nenergy futures. In fact, next Tuesday's\ntalk from Morgan Stanley",
    "start": "1146410",
    "end": "1153450"
  },
  {
    "text": "is going to be an expert in\ncommodity futures and options.",
    "start": "1153450",
    "end": "1158490"
  },
  {
    "text": "And that should be\nvery interesting. Anyway, here, I'm\nlooking at energy futures",
    "start": "1158490",
    "end": "1168919"
  },
  {
    "text": "from the Energy\nInformation Administration. Actually, for this\ncourse, trying to get data that's freely\navailable to students",
    "start": "1168920",
    "end": "1176970"
  },
  {
    "text": "is one of the things we do. So this data is actually\navailable from the Energy",
    "start": "1176970",
    "end": "1182646"
  },
  {
    "text": "Information Administration\nof the government, which is now open, so I guess\nthat'll be updated over time.",
    "start": "1182646",
    "end": "1188960"
  },
  {
    "text": "But basically these\nenergy futures are traded on the Chicago\nMercantile Exchange.",
    "start": "1188960",
    "end": "1195570"
  },
  {
    "text": "And basically CL is crude,\nWest Texas intermediate crude,",
    "start": "1195570",
    "end": "1203289"
  },
  {
    "text": "light crude, which we have\nhere, a time series from 2006",
    "start": "1203290",
    "end": "1208760"
  },
  {
    "text": "to basically yesterday. And you can see how at the\nstart of the period around $60",
    "start": "1208760",
    "end": "1216340"
  },
  {
    "text": "and then went up\nto close to $140, and then it dropped\ndown to around $40.",
    "start": "1216340",
    "end": "1222440"
  },
  {
    "text": "And it's been hovering\naround $100 lately. The second series here is\ngasoline, RBOB gasoline.",
    "start": "1222440",
    "end": "1233040"
  },
  {
    "text": "Always have to look this up. This is that reformulated blend\nstock for oxygenated blending",
    "start": "1233040",
    "end": "1242690"
  },
  {
    "text": "gasoline. Anyway, futures on this product\nare traded at the CME as well.",
    "start": "1242690",
    "end": "1248030"
  },
  {
    "text": "And then heating oil. And what's happening\nwith these data",
    "start": "1248030",
    "end": "1256780"
  },
  {
    "text": "is that we have basically\na refinery which processes",
    "start": "1256780",
    "end": "1268880"
  },
  {
    "text": "crude oil as an input.",
    "start": "1268880",
    "end": "1275990"
  },
  {
    "text": "And it basically\nrefines it, distills it, and generates outputs, which\ninclude heating oil, gasoline,",
    "start": "1275990",
    "end": "1296600"
  },
  {
    "text": "and various other things\nlike jet fuel and others.",
    "start": "1296600",
    "end": "1301679"
  },
  {
    "text": "So if we're looking\nat the prices, the futures prices of, say,\ngasoline and heating oil,",
    "start": "1301680",
    "end": "1309510"
  },
  {
    "text": "relating those to crude\noil, well, certainly,",
    "start": "1309510",
    "end": "1315710"
  },
  {
    "text": "the cost of producing these\nproducts should depend on the cost of the input .",
    "start": "1315710",
    "end": "1321820"
  },
  {
    "text": "So I've got in the next plot,\na translation of these futures",
    "start": "1321820",
    "end": "1330480"
  },
  {
    "text": "contracts into their\nprice per barrel.",
    "start": "1330480",
    "end": "1335510"
  },
  {
    "text": "Turns out crude is quoted\nin dollars per barrel. And the gasoline heating\noil are in cents per gallon.",
    "start": "1335510",
    "end": "1344389"
  },
  {
    "text": "So one multiplies. There are 42\ngallons in a barrel. So you multiply those\nprevious years by 42.",
    "start": "1344390",
    "end": "1350960"
  },
  {
    "text": "And this shows the plot of\nthe prices of the futures where we're looking at\nessentially the same units of output relative to input.",
    "start": "1350960",
    "end": "1360600"
  },
  {
    "text": "And what's evident here is that\nwhile the futures for gasoline,",
    "start": "1360600",
    "end": "1365700"
  },
  {
    "text": "the blue, is consistently above\nthe green, the input, and same for heating oil.",
    "start": "1365700",
    "end": "1372519"
  },
  {
    "text": "And those vary depending\non which is greater. So if we look at the\ndifference between, say,",
    "start": "1372520",
    "end": "1382600"
  },
  {
    "text": "the price of the heating\noil future and the crude oil future, what does\nthat represent?",
    "start": "1382600",
    "end": "1391625"
  },
  {
    "text": " That's the spread in value of\nthe output minus the input.",
    "start": "1391625",
    "end": "1400780"
  },
  {
    "text": "Ray? AUDIENCE: [INAUDIBLE] cost\nof running the refinery? ",
    "start": "1400780",
    "end": "1407146"
  },
  {
    "text": "PROFESSOR: So cost of refining. So let's look at, say,\nheating oil minus CL and, say,",
    "start": "1407146",
    "end": "1419700"
  },
  {
    "text": "this RBOB minus CL. So it's cost of refining.",
    "start": "1419700",
    "end": "1426670"
  },
  {
    "text": "What else could\nbe a factor here? AUDIENCE: Supply and demand\ncharacteristics [INAUDIBLE].",
    "start": "1426670",
    "end": "1431820"
  },
  {
    "text": "PROFESSOR: Definitely. Supply and demand. If one product is demanded\na lot more than another. ",
    "start": "1431820",
    "end": "1438280"
  },
  {
    "text": "Supply and demand. ",
    "start": "1438280",
    "end": "1445820"
  },
  {
    "text": "Anything else? AUDIENCE: Maybe for\nthe outputs, if you were to find the difference\nbetween the outputs,",
    "start": "1445820",
    "end": "1451340"
  },
  {
    "text": "it would be something cyclical. For example, in the\nwinter, heating oil is going to get far more\nvaluable as gasoline,",
    "start": "1451340",
    "end": "1457840"
  },
  {
    "text": "because people drive less\nand people demand more for heating homes. PROFESSOR: Absolutely. That's a very significant\nfactor with these.",
    "start": "1457840",
    "end": "1465669"
  },
  {
    "text": "There are seasonal effects\nthat drive supply and demand. And so we can put\nseasonal effects in there",
    "start": "1465670",
    "end": "1475460"
  },
  {
    "text": "as affecting supply and demand. But certainly, you might expect\nto see seasonal structure here. Anything else?",
    "start": "1475460",
    "end": "1483720"
  },
  {
    "text": "Put on your traders hat. Profit, yes.",
    "start": "1483720",
    "end": "1489309"
  },
  {
    "text": "The refinery needs\nto make some profit. So there has to be some\nlevel of profit that's",
    "start": "1489310",
    "end": "1498520"
  },
  {
    "text": "acceptable and appropriate. So we have all these\nthings driving basically",
    "start": "1498520",
    "end": "1505250"
  },
  {
    "text": "these differences. Let's just take a look\nat those differences. These are actually\ncalled the crack spreads.",
    "start": "1505250",
    "end": "1514880"
  },
  {
    "text": "Cracking in the\nbusiness of refining is basically the\nbreaking down of oil",
    "start": "1514880",
    "end": "1522220"
  },
  {
    "text": "into components, products. And on the top is the\ngasoline crack spread.",
    "start": "1522220",
    "end": "1531800"
  },
  {
    "text": "And the bottom is the\nheating oil crack spread. And one can see\nthat as time series,",
    "start": "1531800",
    "end": "1537720"
  },
  {
    "text": "these actually look stationary. There certainly doesn't appear\nto be a linear trend up.",
    "start": "1537720",
    "end": "1545920"
  },
  {
    "text": "But there are, of course, many\nfactors that could affect this.",
    "start": "1545920",
    "end": "1551390"
  },
  {
    "text": "So with that as motivation, how\nwould we model such a series?",
    "start": "1551390",
    "end": "1559110"
  },
  {
    "text": "So let's go back to\nour lecture here. ",
    "start": "1559110",
    "end": "1566419"
  },
  {
    "text": "All right, View, full size. ",
    "start": "1566420",
    "end": "1575760"
  },
  {
    "text": "This is going to be a\nvery technical discussion, but it's, at the end of the day,\nI think fairly straightforward.",
    "start": "1575760",
    "end": "1585460"
  },
  {
    "text": "And the objective\nactually of this lecture is to provide an introduction\nto the notation here, which",
    "start": "1585460",
    "end": "1591240"
  },
  {
    "text": "should make it seem like it's a\nvery straightforward derivation process of these models.",
    "start": "1591240",
    "end": "1597799"
  },
  {
    "text": "So let's begin with just a recap\nof the vector autoregressive",
    "start": "1597800",
    "end": "1602890"
  },
  {
    "text": "model of order p. This is the extension of\nthe univariate case where we have a vector C of\nconstants, m constants,",
    "start": "1602890",
    "end": "1612870"
  },
  {
    "text": "and matrices phi_1 to\nphi_p corresponding to basically how the\nautoregression of one series",
    "start": "1612870",
    "end": "1621649"
  },
  {
    "text": "depends on all the other series. And then there's multivariate\nwhite noise eta_t,",
    "start": "1621650",
    "end": "1628270"
  },
  {
    "text": "which has mean 0 and some\ncovariance structure in it.",
    "start": "1628270",
    "end": "1633630"
  },
  {
    "text": "And the stationarity-- if\nthis series were stationary,",
    "start": "1633630",
    "end": "1639830"
  },
  {
    "text": "then the determinant of\nthis matrix polynomial",
    "start": "1639830",
    "end": "1648049"
  },
  {
    "text": "would have roots outside the\nunit circle for complex z.",
    "start": "1648050",
    "end": "1653360"
  },
  {
    "text": "And if it's not stationary,\nthen some of those roots",
    "start": "1653360",
    "end": "1659290"
  },
  {
    "text": "will be on the unit\ncircle or beyond. So let's actually go to\nthat non-stationary case",
    "start": "1659290",
    "end": "1665125"
  },
  {
    "text": "and suppose that the process\nis integrated of order one.",
    "start": "1665125",
    "end": "1670540"
  },
  {
    "text": "So if we were to take\nfirst differences, we would have stationarity. ",
    "start": "1670540",
    "end": "1682690"
  },
  {
    "text": "Well, the derivation\nof the model proceeds by converting the\noriginal vector autoregressive",
    "start": "1682690",
    "end": "1692150"
  },
  {
    "text": "equation into an\nequation that's mostly relating to differences but\nwith also some extra terms.",
    "start": "1692150",
    "end": "1699560"
  },
  {
    "text": "So let's begin the process\nby just subtracting the lagged value of\nthe multivariate vector",
    "start": "1699560",
    "end": "1706620"
  },
  {
    "text": "from the original series. So we subtract X_(t-1)\nfrom both sides, and we get delta X_t is equal to\nC plus phi_1 minus I_m X_(t-1)",
    "start": "1706620",
    "end": "1717330"
  },
  {
    "text": "plus the rest. So that's a very simple step. We're just subtracting the\nlagged multivariate series",
    "start": "1717330",
    "end": "1726220"
  },
  {
    "text": "from both sides. Now, what we want\nto do is convert",
    "start": "1726220",
    "end": "1733290"
  },
  {
    "text": "the second term in the middle\nline into a difference term.",
    "start": "1733290",
    "end": "1739930"
  },
  {
    "text": "So what do we do? Well, we can subtract and add\nphi_1 minus I_m times X_(t-2).",
    "start": "1739930",
    "end": "1747900"
  },
  {
    "text": "If we do that,\nsubtract and add that, we then get the delta X_t is\nC plus a multiple of delta",
    "start": "1747900",
    "end": "1753809"
  },
  {
    "text": "X_(t-1) plus this\nmultiple of X_(t-2).",
    "start": "1753810",
    "end": "1759530"
  },
  {
    "text": "So we basically\nreduced the equations to differences in\nthe first two terms",
    "start": "1759530",
    "end": "1765290"
  },
  {
    "text": "or in the current\nseries and the lagged. But then we have the original\nseries for lags t minus 2.",
    "start": "1765290",
    "end": "1773550"
  },
  {
    "text": "We can continue this\nprocess with the third.",
    "start": "1773550",
    "end": "1778660"
  },
  {
    "text": "And then at the\nend of the day, we end up getting this equation\nfor the difference of the series",
    "start": "1778660",
    "end": "1786149"
  },
  {
    "text": "is equal to a constant\nplus a matrix multiple of the first difference\nmultivariate series,",
    "start": "1786150",
    "end": "1793880"
  },
  {
    "text": "plus another matrix times\nthe second difference, all the way down to\nthe p-th difference,",
    "start": "1793880",
    "end": "1801720"
  },
  {
    "text": "or the p minus first difference. But at the end,\nwe're left with terms",
    "start": "1801720",
    "end": "1807400"
  },
  {
    "text": "at p lags that have no\ndifferences in them. So we've been able to\nrepresent this series",
    "start": "1807400",
    "end": "1814440"
  },
  {
    "text": "as an autoregressive\nfunction of differences. But there's also a term on\nthe undifferenced series",
    "start": "1814440",
    "end": "1824010"
  },
  {
    "text": "at the end that's left over. And or this argument\ncan actually",
    "start": "1824010",
    "end": "1834900"
  },
  {
    "text": "proceed by eliminating\ndifferences in the reverse way, starting with the\np-th lag and going up.",
    "start": "1834900",
    "end": "1842649"
  },
  {
    "text": "And one then can represent\nthis as delta X_t is C plus some\nmatrix times just the",
    "start": "1842650",
    "end": "1850169"
  },
  {
    "text": "lagged series plus various\nmatrices times the differences",
    "start": "1850170",
    "end": "1856000"
  },
  {
    "text": "going back p minus 1 lags. ",
    "start": "1856000",
    "end": "1865460"
  },
  {
    "text": "And so at the end of\nthe day, this model basically for delta\nX_t is a constant",
    "start": "1865460",
    "end": "1874269"
  },
  {
    "text": "plus a matrix times the\nprevious lagged series",
    "start": "1874270",
    "end": "1880760"
  },
  {
    "text": "or the first lag of the\nmultivariate time series, plus various autoregressive\nlags of the differenced series.",
    "start": "1880760",
    "end": "1890320"
  },
  {
    "text": " So these notes give you\nthe formulas for those,",
    "start": "1890320",
    "end": "1896130"
  },
  {
    "text": "and they're very easy to\nverify if you go through them one by one.",
    "start": "1896130",
    "end": "1901594"
  },
  {
    "text": " And when we look at this\nexpression for the model,",
    "start": "1901594",
    "end": "1911760"
  },
  {
    "text": "this expresses the\nstochastic process model",
    "start": "1911760",
    "end": "1917270"
  },
  {
    "text": "for the difference series. This difference\nseries is stationary.",
    "start": "1917270",
    "end": "1923780"
  },
  {
    "text": "We've eliminated\nthe non-stationarity in the process. So that means the\nright-hand side",
    "start": "1923780",
    "end": "1929160"
  },
  {
    "text": "has to be stationary as well. And so while the terms which\nare matrix multiples of lags",
    "start": "1929160",
    "end": "1939890"
  },
  {
    "text": "of the differenced\nseries, those are going to be stationary\nbecause we're just taking lags of the\nstationary multivariate time",
    "start": "1939890",
    "end": "1947680"
  },
  {
    "text": "series, the difference series. But this pi X_t term has\nto be stationary as well.",
    "start": "1947680",
    "end": "1956880"
  },
  {
    "text": "So this pi X_t contains\nthe cointegrating terms. And fitting a sort of\ncointegrated vector",
    "start": "1956880",
    "end": "1966600"
  },
  {
    "text": "autoregression model involves\nidentifying this term, pi X_t.",
    "start": "1966600",
    "end": "1973490"
  },
  {
    "text": "And given that the original\nseries had unit roots,",
    "start": "1973490",
    "end": "1980870"
  },
  {
    "text": "it has to be the case that\npi, the matrix, is singular.",
    "start": "1980870",
    "end": "1986195"
  },
  {
    "text": " So it's basically\na transformation",
    "start": "1986195",
    "end": "1992080"
  },
  {
    "text": "of the data that\neliminates that unit root in the overall series.",
    "start": "1992080",
    "end": "1999880"
  },
  {
    "text": "So the matrix pi\nis of reduced rank, and it's either rank\nzero, in which case",
    "start": "1999880",
    "end": "2007675"
  },
  {
    "text": "there's no cointegrating\nrelationships, or its rank is less than m.",
    "start": "2007676",
    "end": "2014500"
  },
  {
    "text": "And the matrix pi does\ndefine the cointegrating relationships.",
    "start": "2014500",
    "end": "2020550"
  },
  {
    "text": "Now, these cointegrating\nrelationships are the relationships in the\nprocess that are stationary.",
    "start": "2020550",
    "end": "2028990"
  },
  {
    "text": "And so basically there's\na lot of information in that multivariate series\nwith contemporaneous values",
    "start": "2028990",
    "end": "2037880"
  },
  {
    "text": "of the series. There is stationary structure\nat every single time point, which can be the\ntarget of the modeling.",
    "start": "2037880",
    "end": "2048199"
  },
  {
    "text": "So this matrix pi is\nof rank r less than m.",
    "start": "2048199",
    "end": "2056250"
  },
  {
    "text": "And so it can be expressed\nas basically alpha beta",
    "start": "2056250",
    "end": "2062100"
  },
  {
    "text": "prime, where these matrices\nare of rank r, alpha and beta.",
    "start": "2062100",
    "end": "2070540"
  },
  {
    "text": "And the columns of beta define\nlinearly independent vectors which cointegrate x. And the decomposition\nof pi isn't unique.",
    "start": "2070540",
    "end": "2077908"
  },
  {
    "text": "You can basically, for any\ninvertible r by r matrix g,",
    "start": "2077909",
    "end": "2083388"
  },
  {
    "text": "define another set of\ncointegrating relationships. So in the linear algebra\nstructure of these problems,",
    "start": "2083389",
    "end": "2090340"
  },
  {
    "text": "there's basically an\nr-dimensional space where the process is\nstationary, and how",
    "start": "2090340",
    "end": "2096360"
  },
  {
    "text": "you define the coordinate system\nin that space is up to you",
    "start": "2096360",
    "end": "2102020"
  },
  {
    "text": "or subject to some choice.",
    "start": "2102020",
    "end": "2108130"
  },
  {
    "text": "So how do we estimate\nthese models? Well, rather nice result\nof Sims, Stock, and Watson.",
    "start": "2108130",
    "end": "2115520"
  },
  {
    "text": "Actually, Sims,\nChristopher Sims, he got the Nobel Prize a\nfew years ago for his work",
    "start": "2115520",
    "end": "2121790"
  },
  {
    "text": "in econometrics. And so this is a rather\nsignificant work that he did.",
    "start": "2121790",
    "end": "2133850"
  },
  {
    "text": "Anyway, he, together\nwith Stock and Watson, prove that if you're estimating\na vector autoregression model,",
    "start": "2133850",
    "end": "2141119"
  },
  {
    "text": "then the least squares\nestimator of the original model is basically sufficient\nto do an analysis",
    "start": "2141120",
    "end": "2149150"
  },
  {
    "text": "of this cointegrated vector\nautoregression process.",
    "start": "2149150",
    "end": "2156599"
  },
  {
    "text": "The parameter estimates\nfrom just fitting the vector autoregression are\nconsistent for the underlying",
    "start": "2156600",
    "end": "2163609"
  },
  {
    "text": "parameters. And they have\nasymptotic distributions that are identical to those of\nmaximum likelihood estimators.",
    "start": "2163610",
    "end": "2169980"
  },
  {
    "text": "And so what ends up happening\nis the least squares estimates",
    "start": "2169980",
    "end": "2178359"
  },
  {
    "text": "of the vector autoregression\nparameters lead to an estimation\nof the pi matrix.",
    "start": "2178360",
    "end": "2187270"
  },
  {
    "text": "And the constraints on the pi\nmatrix which are basically pi",
    "start": "2187270",
    "end": "2200290"
  },
  {
    "text": "is of reduced rank, those\nwill hold asymptotically. So let's just go back\nto the equation before,",
    "start": "2200290",
    "end": "2209240"
  },
  {
    "text": "to see if that\nlooks familiar here.",
    "start": "2209240",
    "end": "2214490"
  },
  {
    "text": " So what that work says\nis that if we basically",
    "start": "2214490",
    "end": "2223069"
  },
  {
    "text": "fit the linear regression\nmodel regressing the difference series on the lag of the series\nplus lags of differences,",
    "start": "2223070",
    "end": "2233930"
  },
  {
    "text": "the least squares estimates\nof these underlying parameters will give us asymptotically\nefficient estimates",
    "start": "2233930",
    "end": "2241690"
  },
  {
    "text": "of this overall process. So we don't need to use any new\ntools to specify these models.",
    "start": "2241690",
    "end": "2251635"
  },
  {
    "start": "2251635",
    "end": "2263800"
  },
  {
    "text": "There's an advanced literature\non estimation methods for these models.",
    "start": "2263800",
    "end": "2269950"
  },
  {
    "text": "Johansen does describe\nmaximum likelihood estimation",
    "start": "2269950",
    "end": "2275050"
  },
  {
    "text": "when the innovation terms\nare normally distributed.",
    "start": "2275050",
    "end": "2281260"
  },
  {
    "text": "And that methodology applies\nreduced rank regression",
    "start": "2281260",
    "end": "2287270"
  },
  {
    "text": "methodology and\nyields tests for what",
    "start": "2287270",
    "end": "2293150"
  },
  {
    "text": "the rank is of the\ncointegrating relationship. And these methods are\nimplemented in our packages.",
    "start": "2293150",
    "end": "2300269"
  },
  {
    "start": "2300270",
    "end": "2305710"
  },
  {
    "text": "Let's see. Let me just go back now\nto the-- so let's see.",
    "start": "2305710",
    "end": "2320890"
  },
  {
    "text": "The case study on\nthe crack spread data",
    "start": "2320890",
    "end": "2327690"
  },
  {
    "text": "actually goes through sort of\ntesting for non-stationarity in these underlying series.",
    "start": "2327690",
    "end": "2334040"
  },
  {
    "text": "And actually, why don't\nI just show you that? Let's go back here.",
    "start": "2334040",
    "end": "2339450"
  },
  {
    "start": "2339450",
    "end": "2357522"
  },
  {
    "text": "If you can see this, for\nthe crack spread data,",
    "start": "2357522",
    "end": "2363460"
  },
  {
    "text": "looking at the\ncrude oil futures, basically the crude oil\nfuture can be evaluated to see if it's non-stationary.",
    "start": "2363460",
    "end": "2370790"
  },
  {
    "text": "And there's this augmented\nDickey-Fuller test for non-stationarity.",
    "start": "2370790",
    "end": "2376350"
  },
  {
    "text": "And it basically has a null\nhypothesis that the model",
    "start": "2376350",
    "end": "2383160"
  },
  {
    "text": "or the series is non-stationary,\nor it has a unit root, versus the alternative\nthat it doesn't.",
    "start": "2383160",
    "end": "2389040"
  },
  {
    "text": "And so testing that\nnull hypothesis that it's non-stationary\nyields a p-value of 0.164",
    "start": "2389040",
    "end": "2396120"
  },
  {
    "text": "for CLC1, the first\nnearest contract,",
    "start": "2396121",
    "end": "2401690"
  },
  {
    "text": "near month contract of\nthe futures for crude.",
    "start": "2401690",
    "end": "2407400"
  },
  {
    "text": "And so the data\nsuggests that crude has a distribution that's\nnon-stationary, integrated",
    "start": "2407400",
    "end": "2414060"
  },
  {
    "text": "order 1. And the HOC1 also basically\nhas a test for-- p-value",
    "start": "2414060",
    "end": "2423950"
  },
  {
    "text": "for non-stationarity of 0.3265. So we can't reject\nnon-stationarity or unit root",
    "start": "2423950",
    "end": "2431000"
  },
  {
    "text": "in those series with\nthese test statistics. In analyzing the data, this\nsuggests that we basically",
    "start": "2431000",
    "end": "2439260"
  },
  {
    "text": "need to accommodate that\nnon-stationarity when we specify the models. ",
    "start": "2439260",
    "end": "2446924"
  },
  {
    "text": "Let me just see if\nthere's some results here. ",
    "start": "2446925",
    "end": "2515180"
  },
  {
    "text": "For this series,\nactually the case notes will go through actually\nconducting this Johansen",
    "start": "2515180",
    "end": "2521270"
  },
  {
    "text": "procedure for\ntesting for the rank of the cointegrated process. And that test basically has\ndifferent test statistic",
    "start": "2521270",
    "end": "2531630"
  },
  {
    "text": "for testing whether the rank is\n0, 1, less than or equal to 1, or less than or equal to 2.",
    "start": "2531630",
    "end": "2536870"
  },
  {
    "text": "And one can see that\nthere's marginal-- the test statistic is almost\nsignificant at the 10% level",
    "start": "2536870",
    "end": "2545930"
  },
  {
    "text": "for the overall series. It's not significant\nfor the rank",
    "start": "2545930",
    "end": "2552670"
  },
  {
    "text": "being less than or equal to 1. And so these results, it\ndoesn't suggest there's",
    "start": "2552670",
    "end": "2558390"
  },
  {
    "text": "strong non-stationarity. But certainly with\nthat non-stationarity",
    "start": "2558390",
    "end": "2565359"
  },
  {
    "text": "is no more than rank\none for the series. And the eigenvector\ncorresponding",
    "start": "2565360",
    "end": "2572030"
  },
  {
    "text": "to the stationary\nrelationship is given by these coefficients\nof 1 on the crude oil future,",
    "start": "2572030",
    "end": "2580940"
  },
  {
    "text": "1.3 on the RBOB and minus\n1.7 on the heating oil. ",
    "start": "2580940",
    "end": "2588640"
  },
  {
    "text": "So what this suggests\nis that there's considerable variability in\nthese energy futures contracts.",
    "start": "2588640",
    "end": "2600880"
  },
  {
    "text": "What appears to be stationary\nis some linear combination of crude plus gasoline\nminus heating oil.",
    "start": "2600880",
    "end": "2608670"
  },
  {
    "text": "And in terms of why does\nit combine that way, well, there are all\nkinds of factors",
    "start": "2608670",
    "end": "2615280"
  },
  {
    "text": "that we went through-- cost of\nrefining, supply and demand, seasonality, which\naffect things.",
    "start": "2615280",
    "end": "2621370"
  },
  {
    "text": "And so when analyzed, sort\nof ignoring seasonality, these would be the linear\ncombinations that appear",
    "start": "2621370",
    "end": "2630000"
  },
  {
    "text": "to be stationary over time. Yeah?  AUDIENCE: Why did you\nchoose to use the futures",
    "start": "2630000",
    "end": "2635680"
  },
  {
    "text": "prices as opposed to the spot? And how did you combine the\ndata with actual [INAUDIBLE]? PROFESSOR: I chose this\nbecause if refiners are wanting",
    "start": "2635680",
    "end": "2647819"
  },
  {
    "text": "to hedge their risks, then they\nwill go to the futures market to hedge those.",
    "start": "2647820",
    "end": "2654059"
  },
  {
    "text": "And so working with\nthese data, one can then consider problems of\nhedging refinery production",
    "start": "2654060",
    "end": "2664370"
  },
  {
    "text": "risks. And so that's why. AUDIENCE: [INAUDIBLE]",
    "start": "2664370",
    "end": "2670960"
  },
  {
    "text": "PROFESSOR: OK, well, the Energy\nInformation Administration provides historical data\nwhich gives the first month,",
    "start": "2670960",
    "end": "2679270"
  },
  {
    "text": "the second month, the third\nmonth available for each of these contracts. And so I chose the\nfirst month contract",
    "start": "2679270",
    "end": "2687720"
  },
  {
    "text": "for each of these features. Those 10 are the most liquid. Depending on what\none is hedging,",
    "start": "2687720",
    "end": "2694440"
  },
  {
    "text": "one would use perhaps\nlonger periods for those. There's some very\nnice finance problems",
    "start": "2694440",
    "end": "2702450"
  },
  {
    "text": "dealing with hedging,\nhedging these kinds of risks, and as well as trading\nthese kinds of risk. Traders can try to exploit\nshort term movements in these.",
    "start": "2702450",
    "end": "2711029"
  },
  {
    "start": "2711030",
    "end": "2729870"
  },
  {
    "text": "Anyway, I'll let you\nlook through these, the case note later. And it does provide some detail\non the coefficient estimates.",
    "start": "2729870",
    "end": "2736810"
  },
  {
    "text": "And one can basically\nget a handle on how these things\nare being specified. ",
    "start": "2736810",
    "end": "2743980"
  },
  {
    "text": "So let's go back. ",
    "start": "2743980",
    "end": "2758260"
  },
  {
    "text": "The next topic I want to cover\nis linear state-space models.",
    "start": "2758260",
    "end": "2766490"
  },
  {
    "text": "It turns out that many\nof these time series",
    "start": "2766490",
    "end": "2772725"
  },
  {
    "text": "models appropriate in\neconomics and finance can be expressed as a\nlinear state-space model.",
    "start": "2772725",
    "end": "2780290"
  },
  {
    "start": "2780290",
    "end": "2788590"
  },
  {
    "text": "I'm going to introduce the\ngeneral notation first and then provide illustrations\nof this general notation",
    "start": "2788590",
    "end": "2795100"
  },
  {
    "text": "with a number of\ndifferent examples. So the formulation is we have\nbasically an observation vector",
    "start": "2795100",
    "end": "2806205"
  },
  {
    "text": "at time t, y_t. This is our multivariate time\nseries that we're modeling. Now, I've chosen it\nto be k-dimensional",
    "start": "2806205",
    "end": "2813930"
  },
  {
    "text": "for the observations. There's an underlying\nstate vector",
    "start": "2813930",
    "end": "2820720"
  },
  {
    "text": "that's of m dimensions,\nwhich basically characterizes the state of the\nprocess at time t.",
    "start": "2820720",
    "end": "2831740"
  },
  {
    "text": "There's an observation error\nvector at time t, epsilon_t. So it's k by 1 as well,\ncorresponding to y.",
    "start": "2831740",
    "end": "2838830"
  },
  {
    "text": "And there's a state transition\ninnovation error vector, which is n by 1,\nwhich actually can",
    "start": "2838830",
    "end": "2851240"
  },
  {
    "text": "be different from m, the\ndimension of the state vector. So we have-- in the state\nspace specification,",
    "start": "2851240",
    "end": "2861300"
  },
  {
    "text": "we're going to specify\ntwo equations, one for how the states evolve\nover time and another for how",
    "start": "2861300",
    "end": "2867640"
  },
  {
    "text": "the observations or\nmeasurements evolve, depending on the\nunderlying states. So let's first focus\non a state equation",
    "start": "2867640",
    "end": "2875400"
  },
  {
    "text": "which describes how\nthe state progresses from the state at time t to\nthe state at time t plus 1.",
    "start": "2875400",
    "end": "2885680"
  },
  {
    "text": "Because this is a linear\nstate-space model, basically the state\nat t plus 1 is",
    "start": "2885680",
    "end": "2890710"
  },
  {
    "text": "going to be some linear\nfunction of the states at time t plus some noise.",
    "start": "2890710",
    "end": "2896640"
  },
  {
    "text": "And that noise is\ngiven by eta_t,",
    "start": "2896640",
    "end": "2902569"
  },
  {
    "text": "being independent identically\ndistributed white noise, or normally distributed\nwith some covariance matrix",
    "start": "2902570",
    "end": "2911599"
  },
  {
    "text": "Q_t, positive definite. And R_t is some\nlinear transformation",
    "start": "2911600",
    "end": "2917740"
  },
  {
    "text": "of those, which\ncharacterize the uncertainty in the particular states.",
    "start": "2917740",
    "end": "2922880"
  },
  {
    "text": "So there's a great\ndeal of flexibility here in how things\ndepend on each other. And right now, it will appear\njust like a lot of notation.",
    "start": "2922880",
    "end": "2933090"
  },
  {
    "text": "But as we see it\nin different cases, you'll see how these\nterms come into play. And they're very\nstraightforward.",
    "start": "2933090",
    "end": "2939260"
  },
  {
    "text": " So we're considering simple\nlinear transformations",
    "start": "2939260",
    "end": "2944800"
  },
  {
    "text": "of the states plus noise. And then the observation\nequation or measurement equation is a linear\ntransformation",
    "start": "2944800",
    "end": "2953079"
  },
  {
    "text": "of the underlying\nstates plus noise.  So the matrix Z_t is the\nobservation coefficients",
    "start": "2953080",
    "end": "2960230"
  },
  {
    "text": "matrix. And the noise or innovations\nepsilon_t are, we'll assume,",
    "start": "2960230",
    "end": "2965792"
  },
  {
    "text": "independent\nidentically distributed normal, multivariate\nnormal random variables with some covariance matrix H_t.",
    "start": "2965792",
    "end": "2973550"
  },
  {
    "text": "To be fully general,\nthe subscript t means the covariance\ncan depend on time t.",
    "start": "2973550",
    "end": "2980800"
  },
  {
    "text": "It doesn't have to, but it can. These two equations\ncan be written together",
    "start": "2980800",
    "end": "2988600"
  },
  {
    "text": "in a joint equation where\nwe see that the underlying state at time t, s, gets\ntransformed with T sub t",
    "start": "2988600",
    "end": "2999370"
  },
  {
    "text": "to the state at t plus 1 plus\nresidual innovation term.",
    "start": "2999370",
    "end": "3004550"
  },
  {
    "text": "And the observation equation\ny_t is Z_t s_t plus that. So we're representing how\nthe states evolve over time",
    "start": "3004550",
    "end": "3012430"
  },
  {
    "text": "and how the observations\ndepend on the underlying states in this joint equation. ",
    "start": "3012430",
    "end": "3019770"
  },
  {
    "text": "And the structure of\nbasically this sort of linear function of states\nplus error, the error term u_t",
    "start": "3019770",
    "end": "3028400"
  },
  {
    "text": "here is normally distributed\nwith covariance matrix omega,",
    "start": "3028400",
    "end": "3033740"
  },
  {
    "text": "which has this structure. It's a block diagonal.",
    "start": "3033740",
    "end": "3038849"
  },
  {
    "text": "We have the covariance\nof the epsilons as the H. And the covariance of R_t\neta_t is R_t Q_t R_t transpose.",
    "start": "3038850",
    "end": "3048859"
  },
  {
    "text": "So you may recall when we\ntake a covariance matrix",
    "start": "3048860",
    "end": "3054660"
  },
  {
    "text": "of linear function of random\nvariables given by a matrix,",
    "start": "3054660",
    "end": "3061210"
  },
  {
    "text": "then it's that linear function\nR times the covariance matrix times the transpose.",
    "start": "3061210",
    "end": "3067970"
  },
  {
    "text": "So that term comes into play. So let's see how a\ncapital asset pricing",
    "start": "3067970",
    "end": "3076859"
  },
  {
    "text": "model with time-varying\nbetas can be represented as a linear state-space model. ",
    "start": "3076860",
    "end": "3084220"
  },
  {
    "text": "You'll recall, we discussed\nthis model a few lectures ago, where we have the excess\nreturn of a given stock, r_t,",
    "start": "3084220",
    "end": "3093869"
  },
  {
    "text": "is a linear function of the\nexcess return of the market",
    "start": "3093870",
    "end": "3099150"
  },
  {
    "text": "portfolio, r_(m,t), plus error. What we're going to do now\nis extend that previous model",
    "start": "3099150",
    "end": "3108310"
  },
  {
    "text": "by adding time dependence, t,\nto the regression parameters.",
    "start": "3108310",
    "end": "3114170"
  },
  {
    "text": "The alpha is not a constant. It is going to vary by time. And the beta is also\ngoing to very by time.",
    "start": "3114170",
    "end": "3122700"
  },
  {
    "text": "And how will they vary by time? Well, we're going to\nassume that the alpha_t is",
    "start": "3122700",
    "end": "3130030"
  },
  {
    "text": "a Gaussian random walk. And the beta is also a\nGaussian random walk.",
    "start": "3130030",
    "end": "3137982"
  },
  {
    "start": "3137982",
    "end": "3148809"
  },
  {
    "text": "And with that set up, we\nhave the following expression for the state equation.",
    "start": "3148810",
    "end": "3155450"
  },
  {
    "text": "OK, the state equation, which\nis just the unknown parameters-- it's the alpha and the\nbeta at given time t.",
    "start": "3155450",
    "end": "3160990"
  },
  {
    "text": " The state at time\nt gets adjusted to the state at time t plus 1\nby just adding these random walk",
    "start": "3160990",
    "end": "3169340"
  },
  {
    "text": "terms to it. So it's a very simple process. We have the identity\ntimes the previous state",
    "start": "3169340",
    "end": "3175270"
  },
  {
    "text": "plus the identity times this\nvector of these innovations. So s_(t+1) is equal to\nT_t s_t plus R_t eta_t,",
    "start": "3175270",
    "end": "3184119"
  },
  {
    "text": "where this matrix, T sub\nt and R sub t are trivial; they're just the identity.",
    "start": "3184120",
    "end": "3190290"
  },
  {
    "text": "And eta_t has a\ncovariance matrix",
    "start": "3190290",
    "end": "3195710"
  },
  {
    "text": "which is just given by\nQ_t, sigma squared nu, sigma squared epsilon.",
    "start": "3195710",
    "end": "3202560"
  },
  {
    "text": "This is a complex way, perhaps,\nof representing this model.",
    "start": "3202560",
    "end": "3208680"
  },
  {
    "text": "But it puts this simple model\ninto that linear state-space framework. ",
    "start": "3208680",
    "end": "3216670"
  },
  {
    "text": "Now, the observation equation\nis given by this expression",
    "start": "3216670",
    "end": "3225660"
  },
  {
    "text": "defining the Z_t matrix as the\nunit element and r_(m,t) So",
    "start": "3225660",
    "end": "3232250"
  },
  {
    "text": "it's basically a row vector, or\na row matrix, one-row matrix.",
    "start": "3232250",
    "end": "3238150"
  },
  {
    "text": "And epsilon_t is the\nwhite noise process. Now, putting these\nequations together,",
    "start": "3238150",
    "end": "3245569"
  },
  {
    "text": "we basically have the equation\nfor the state transition and the observation\nequation together.",
    "start": "3245570",
    "end": "3253230"
  },
  {
    "text": "We have this form for that. ",
    "start": "3253230",
    "end": "3265780"
  },
  {
    "text": "So now, let's\nconsider a second case of linear regression\nmodels where",
    "start": "3265780",
    "end": "3271359"
  },
  {
    "text": "we have a time varying beta. In a way, this case\nwe just looked at",
    "start": "3271360",
    "end": "3277140"
  },
  {
    "text": "is a simple case of that. But let's look at\na more general case where we have p independent\nvariables, which",
    "start": "3277140",
    "end": "3285270"
  },
  {
    "text": "could be time-varying. So we have a\nregression model almost",
    "start": "3285270",
    "end": "3291670"
  },
  {
    "text": "as we've considered\nit previously. y_t is equal to x_t transpose\nbeta_t plus epsilon_t.",
    "start": "3291670",
    "end": "3298400"
  },
  {
    "text": "The difference now is our\nregression coefficients beta are allowed to\nchange over time.",
    "start": "3298400",
    "end": "3303580"
  },
  {
    "start": "3303580",
    "end": "3309880"
  },
  {
    "text": "How do they change over time? Well, we're going to\nassume that those also follow independent random\nwalks with variances",
    "start": "3309880",
    "end": "3319120"
  },
  {
    "text": "of the random walks that\nmay depend on the component. So the joint\nstate-space equation",
    "start": "3319120",
    "end": "3324770"
  },
  {
    "text": "here is given by the identity\ntimes s_t plus eta_t.",
    "start": "3324770",
    "end": "3332530"
  },
  {
    "text": "That's basically the random\nwalk process for the underlying regression parameters.",
    "start": "3332530",
    "end": "3337600"
  },
  {
    "text": "And y_t is equal\nto x_t transpose times the same regression\nparameters plus the observation",
    "start": "3337600",
    "end": "3346081"
  },
  {
    "text": "error. ",
    "start": "3346081",
    "end": "3356480"
  },
  {
    "text": "I guess needless to say, if we\nconsider the special case where the random walk\nprocess is degenerate",
    "start": "3356480",
    "end": "3364610"
  },
  {
    "text": "and they're basically\nsteps of size zero, then we get the normal linear\nregression model coming out",
    "start": "3364610",
    "end": "3370410"
  },
  {
    "text": "of this. If we were to be specifying\nthe linear state-space",
    "start": "3370410",
    "end": "3377950"
  },
  {
    "text": "implementation of this model and\nconsider successive estimates of the model\nparameters over time,",
    "start": "3377950",
    "end": "3385270"
  },
  {
    "text": "then these equations would\ngive us recursive estimates for updating\nregressions as we add",
    "start": "3385270",
    "end": "3394080"
  },
  {
    "text": "additional values to the\ndata, additional observations to the data. ",
    "start": "3394080",
    "end": "3403880"
  },
  {
    "text": "Let's look at autoregressive\nmodels of order p.",
    "start": "3403880",
    "end": "3409960"
  },
  {
    "text": "The autoregressive model of\norder p for a univariate time",
    "start": "3409960",
    "end": "3415780"
  },
  {
    "text": "series has the setup given here.",
    "start": "3415780",
    "end": "3421670"
  },
  {
    "text": "It's a polynomial\nlag of the response",
    "start": "3421670",
    "end": "3427470"
  },
  {
    "text": "variable y_t is equal to\nthe innovation epsilon_t. And we can define\nthe state vector",
    "start": "3427470",
    "end": "3436130"
  },
  {
    "text": "to be equal to the vector of\np values, p successive values",
    "start": "3436130",
    "end": "3444980"
  },
  {
    "text": "of the process. And so we basically\nget a combination",
    "start": "3444980",
    "end": "3453710"
  },
  {
    "text": "here of the observation equation\nand state equation joining where basically\none of the states",
    "start": "3453710",
    "end": "3466720"
  },
  {
    "text": "is actually equal\nto the observation. And basically, with\nthis definition",
    "start": "3466720",
    "end": "3472600"
  },
  {
    "text": "for a state of the vector\nat the next time point t,",
    "start": "3472600",
    "end": "3479160"
  },
  {
    "text": "that is equal to this\nlinear transformation of the lagged state vector\nplus that innovation term.",
    "start": "3479160",
    "end": "3489114"
  },
  {
    "text": "I dropped the mic. ",
    "start": "3489114",
    "end": "3496599"
  },
  {
    "text": "So the notation here\nshows the structure for how this linear\nstate-space model is evolving.",
    "start": "3496600",
    "end": "3506240"
  },
  {
    "text": "Basically, the\nobservation equation is the linear\ncombination of the five",
    "start": "3506240",
    "end": "3512410"
  },
  {
    "text": "multiples of lags of the\nvalues plus the residual. And the previous\nlags of the states",
    "start": "3512410",
    "end": "3520240"
  },
  {
    "text": "are just simply the identities\ntimes those values, shifted.",
    "start": "3520240",
    "end": "3526200"
  },
  {
    "text": "So it's a very simple structure\nfor the autoregressive process",
    "start": "3526200",
    "end": "3531690"
  },
  {
    "text": "as a linear state-space model.  We have, as I was just saying,\nfor the transition matrix T sub",
    "start": "3531690",
    "end": "3542470"
  },
  {
    "text": "t, this matrix and the\nobservation equation",
    "start": "3542470",
    "end": "3549750"
  },
  {
    "text": "is essentially picking out\nthe first element of the state vector, which has no\nmeasurement error.",
    "start": "3549750",
    "end": "3556540"
  },
  {
    "text": "So that simplifies that. ",
    "start": "3556540",
    "end": "3561940"
  },
  {
    "text": "The moving average\nmodel of order q",
    "start": "3561940",
    "end": "3567210"
  },
  {
    "text": "could also be expressed as\na linear state-space model. ",
    "start": "3567210",
    "end": "3577240"
  },
  {
    "text": "Remember, the\nmoving average model is one where our response\nvariable, y, is simply",
    "start": "3577240",
    "end": "3583030"
  },
  {
    "text": "some linear combination\nof innovations,",
    "start": "3583030",
    "end": "3588290"
  },
  {
    "text": "q past innovations. And this state\nvector, if we consider",
    "start": "3588290",
    "end": "3595350"
  },
  {
    "text": "the state vector just\nbeing basically q lags of the innovations,\nthen the transition",
    "start": "3595350",
    "end": "3604400"
  },
  {
    "text": "of those underlying states is\ngiven by this expression here. ",
    "start": "3604400",
    "end": "3614690"
  },
  {
    "text": "And we have a state equation,\nan observation equation, which has these forms for these\nvarious transition matrices",
    "start": "3614690",
    "end": "3623500"
  },
  {
    "text": "and for how the innovation\nterms are related.",
    "start": "3623500",
    "end": "3630615"
  },
  {
    "start": "3630615",
    "end": "3640840"
  },
  {
    "text": "Let me just finish\nup with example showing with the autoregressive\nmoving average model.",
    "start": "3640840",
    "end": "3647780"
  },
  {
    "text": "And many years ago,\nit was actually very difficult to\nspecify the estimation",
    "start": "3647780",
    "end": "3655490"
  },
  {
    "text": "methods for autoregressive\nmoving average models. But the implementation\nof these models",
    "start": "3655490",
    "end": "3660800"
  },
  {
    "text": "as linear state-space models\nfacilitated that greatly. And with the ARMA model,\nthe setup basically",
    "start": "3660800",
    "end": "3673030"
  },
  {
    "text": "is a combination of\nthe autoregressive moving average processes. We have an\nautoregression of the y's",
    "start": "3673030",
    "end": "3680279"
  },
  {
    "text": "is equal to a moving\naverage of the residuals or the innovations.",
    "start": "3680280",
    "end": "3685510"
  },
  {
    "text": " And it's convenient in the setup\nfor linear state-space models",
    "start": "3685510",
    "end": "3692549"
  },
  {
    "text": "to define the dimension m,\nwhich is the maximum of p and q",
    "start": "3692550",
    "end": "3697720"
  },
  {
    "text": "plus 1, and think of having\nbasically a possibly m order",
    "start": "3697720",
    "end": "3705859"
  },
  {
    "text": "polynomial lag for each\nof those two series. And we can basically\nconstrain those values",
    "start": "3705860",
    "end": "3715060"
  },
  {
    "text": "to be 0 if m is greater than\np or m is greater than q. ",
    "start": "3715060",
    "end": "3726880"
  },
  {
    "text": "And Harvey, in a very\nimportant work in '93, actually defined a particular\nstate-space representation",
    "start": "3726880",
    "end": "3737080"
  },
  {
    "text": "for this process. And I guess it's\nimportant to know that with these linear\nstate-space models,",
    "start": "3737080",
    "end": "3744310"
  },
  {
    "text": "we're dealing with\ncharacterizing structure in m-dimensional space.",
    "start": "3744310",
    "end": "3751750"
  },
  {
    "text": "There's often some choice in how\nyou represent your underlying states.",
    "start": "3751750",
    "end": "3757670"
  },
  {
    "text": "You can basically\nre-parametrize the models by considering invertible\nlinear transformations",
    "start": "3757670",
    "end": "3767079"
  },
  {
    "text": "of the underlying states. So let me go back here.",
    "start": "3767080",
    "end": "3772820"
  },
  {
    "text": " In expressing the state\nequation generally",
    "start": "3772820",
    "end": "3779990"
  },
  {
    "text": "is T sub t s_t plus R_t eta_t. This matrix T sub t\nand st-- basically s_t",
    "start": "3779990",
    "end": "3788540"
  },
  {
    "text": "can be replaced by a linear\ntransformation of s_t, so long as we multiply\nthe T sub t by the inverse",
    "start": "3788540",
    "end": "3796730"
  },
  {
    "text": "of that transformation. So there's flexibility\nin the choice of our linear state-space\nspecification.",
    "start": "3796730",
    "end": "3802340"
  },
  {
    "text": "And so there really are many\ndifferent equivalent linear",
    "start": "3802340",
    "end": "3808820"
  },
  {
    "text": "state-space models for a\ngiven process depending on exactly how you\ndefine the states",
    "start": "3808820",
    "end": "3815599"
  },
  {
    "text": "and the underlying\ntransformation matrix T. And the beauty of Harvey's\nwork was coming up",
    "start": "3815600",
    "end": "3824900"
  },
  {
    "text": "with a nice representation\nfor the states, where we had very simple forms\nfor the various matrices.",
    "start": "3824900",
    "end": "3833100"
  },
  {
    "text": "And the lecture notes here\ngo through the derivation of that for the ARMA process.",
    "start": "3833100",
    "end": "3839430"
  },
  {
    "text": "And this derivation\nis-- I just want",
    "start": "3839430",
    "end": "3844490"
  },
  {
    "text": "to go through the\nfirst case just to highlight how\nthe argument goes.",
    "start": "3844490",
    "end": "3851020"
  },
  {
    "text": "We basically have this equation,\nwhich is the original equation for an ARMA(p,q) process.",
    "start": "3851020",
    "end": "3857345"
  },
  {
    "text": " And Harvey says, well,\ndefine the first--",
    "start": "3857345",
    "end": "3865809"
  },
  {
    "text": "or the state at time t to\nbe equal to the observation at time t.",
    "start": "3865810",
    "end": "3871820"
  },
  {
    "text": "If we do that, then how\ndoes this equation relate",
    "start": "3871820",
    "end": "3878250"
  },
  {
    "text": "to the basically-- this is the\nstate at the next time point, t",
    "start": "3878250",
    "end": "3886000"
  },
  {
    "text": "plus 1, is equal to phi_1\ntimes the state at time t, plus a second state at time\nt and a residual innovation",
    "start": "3886000",
    "end": "3900340"
  },
  {
    "text": "eta_t. So by choosing the first state\nto be the observation value",
    "start": "3900340",
    "end": "3909109"
  },
  {
    "text": "at that time, we can then\nsolve for the second state,",
    "start": "3909110",
    "end": "3916680"
  },
  {
    "text": "which is given by\nthis expression, just by rewriting our model\nequation in terms of s_(1,t),",
    "start": "3916680",
    "end": "3925730"
  },
  {
    "text": "s_(2,t) and eta_t. So this s_(2,t) is this function\nof the observations and eta_t.",
    "start": "3925730",
    "end": "3936950"
  },
  {
    "text": "So it's a very\nsimple specification of the second state. Just what is that\nsecond state element",
    "start": "3936950",
    "end": "3948020"
  },
  {
    "text": "given this definition\nof the first one? And one can do this\nprocess iteratively",
    "start": "3948020",
    "end": "3954650"
  },
  {
    "text": "getting rid of the\nobservations and replacing them by underlying states.",
    "start": "3954650",
    "end": "3961290"
  },
  {
    "text": "And at the end of\nthe day, you end up with this very simple form\nfor the transition matrix T.",
    "start": "3961290",
    "end": "3969490"
  },
  {
    "text": "Basically, the T has the\nautoregressive components as the first column\nof the T matrix.",
    "start": "3969490",
    "end": "3976410"
  },
  {
    "text": "And this R matrix has\nthis vector of the moving average components.",
    "start": "3976410",
    "end": "3982550"
  },
  {
    "text": "So it's a very nice way\nto represent the model.",
    "start": "3982550",
    "end": "3988330"
  },
  {
    "text": "Coming up with it was something\nvery clever that he did. But what one can see is\nthat this basic model where",
    "start": "3988330",
    "end": "3996580"
  },
  {
    "text": "you have the states\ntransitioning according",
    "start": "3996580",
    "end": "4001620"
  },
  {
    "text": "to a linear transformation of\nthe previous state plus error, and the observation being some\nfunction of the current states,",
    "start": "4001620",
    "end": "4009910"
  },
  {
    "text": "plus error or not, depending\non the formulation, is the representation.",
    "start": "4009910",
    "end": "4015035"
  },
  {
    "text": " Now, with all of\nthese models, a reason",
    "start": "4015035",
    "end": "4023770"
  },
  {
    "text": "why linear state-space\nmodeling is in fact effective",
    "start": "4023770",
    "end": "4028860"
  },
  {
    "text": "is that their specification is\nfully specified with the Kalman",
    "start": "4028860",
    "end": "4039711"
  },
  {
    "text": "filter.  So with this formulation of\nlinear state-space models,",
    "start": "4039711",
    "end": "4052100"
  },
  {
    "text": "the Kalman filter\nas a methodology is the recursive computation\nof the probability density",
    "start": "4052100",
    "end": "4061380"
  },
  {
    "text": "functions for the underlying\nstates at basically",
    "start": "4061380",
    "end": "4068535"
  },
  {
    "text": "t plus 1 given\ninformation up to time t, as well as the joint\ndensity of the future state",
    "start": "4068535",
    "end": "4076710"
  },
  {
    "text": "and the future observation at\nt plus 1, given information up to time t.",
    "start": "4076710",
    "end": "4082370"
  },
  {
    "text": "And also just the\nmarginal distribution of the next observation given\nthe information up to time t.",
    "start": "4082370",
    "end": "4090380"
  },
  {
    "start": "4090380",
    "end": "4100489"
  },
  {
    "text": "So what I want to do is\njust go through with you",
    "start": "4100490",
    "end": "4106509"
  },
  {
    "text": "how the Kalman filter is\nimplemented and defined.",
    "start": "4106510",
    "end": "4111549"
  },
  {
    "text": "And the implementation\nof the Kalman filter requires us to have some\nnotation that's a bit involved,",
    "start": "4111550",
    "end": "4120939"
  },
  {
    "text": "but we'll hopefully explain it\nso it's very straightforward.",
    "start": "4120939",
    "end": "4126710"
  },
  {
    "text": "There are basically conditional\nmeans of the states. ",
    "start": "4126710",
    "end": "4132089"
  },
  {
    "text": "s sub t given t\nis the mean value of the state at time t given\nthe information up to time t.",
    "start": "4132090",
    "end": "4139509"
  },
  {
    "text": "If we condition\non t minus 1, then it's the expectation\nof the state at time t given the\ninformation up to t minus 1.",
    "start": "4139510",
    "end": "4146299"
  },
  {
    "text": " And then y t t minus\n1 is the expectation",
    "start": "4146300",
    "end": "4152100"
  },
  {
    "text": "of the observation given\ninformation up to t minus 1. There's also\nconditional covariances",
    "start": "4152100",
    "end": "4158779"
  },
  {
    "text": "and mean squared errors. All these covariances\nare determined by omegas.",
    "start": "4158780",
    "end": "4166619"
  },
  {
    "text": "The subscript corresponds to\nstates s, or observation y.",
    "start": "4166620",
    "end": "4173240"
  },
  {
    "text": "And basically, the\nconditioning set is either information up to\ntime t, t minus 1 or t minus 1",
    "start": "4173240",
    "end": "4179149"
  },
  {
    "text": "in the second case. And we want to compute\nbasically the covariance matrix",
    "start": "4179149",
    "end": "4185370"
  },
  {
    "text": "of the states given whatever\nthe information is, information up to time t, t minus 1.",
    "start": "4185370",
    "end": "4192439"
  },
  {
    "text": "So these covariance\nmatrices are the expectation",
    "start": "4192439",
    "end": "4197810"
  },
  {
    "text": "of the state minus\ntheir expectation under the conditioning times\nthe state minus the expectation",
    "start": "4197810",
    "end": "4206850"
  },
  {
    "text": "transpose. That's the definition of\nthat covariance matrix. So the different\ndefinitions here",
    "start": "4206850",
    "end": "4212230"
  },
  {
    "text": "correspond to just\nwhether we're conditioning on different information. ",
    "start": "4212230",
    "end": "4217900"
  },
  {
    "text": "And then the observation\ninnovations or residuals",
    "start": "4217900",
    "end": "4223170"
  },
  {
    "text": "are the difference\nbetween an observation y_t",
    "start": "4223170",
    "end": "4229510"
  },
  {
    "text": "and its estimate given\ninformation up to t minus 1. ",
    "start": "4229510",
    "end": "4237190"
  },
  {
    "text": "So the residuals in this process\nare the innovation residuals, one period ahead.",
    "start": "4237190",
    "end": "4244200"
  },
  {
    "text": "And the Kalman filter\nconsists of four steps.",
    "start": "4244200",
    "end": "4250780"
  },
  {
    "text": "We basically want to, first,\npredict the state vector",
    "start": "4250780",
    "end": "4260800"
  },
  {
    "text": "one step ahead. So given our estimate of the\nstate vector at time t minus 1,",
    "start": "4260800",
    "end": "4270139"
  },
  {
    "text": "we want to predict this\nstate vector at time t. And we also want to\npredict the observation",
    "start": "4270140",
    "end": "4278219"
  },
  {
    "text": "at time t given our estimate\nat state vector time t minus 1.",
    "start": "4278220",
    "end": "4283820"
  },
  {
    "text": "And so at time t minus 1, we\ncan estimate these quantities.",
    "start": "4283820",
    "end": "4291674"
  },
  {
    "text": "[INAUDIBLE]  At t minus 1, we can\nbasically predict",
    "start": "4291674",
    "end": "4300969"
  },
  {
    "text": "what the state is going\nto and predict what the observation is going to be. And we can estimate\nhow much error there's",
    "start": "4300969",
    "end": "4307166"
  },
  {
    "text": "going to be in those estimates,\nby these covariance matrices. ",
    "start": "4307166",
    "end": "4319420"
  },
  {
    "text": "The second step is\nupdating these predictions",
    "start": "4319420",
    "end": "4325140"
  },
  {
    "text": "to get our estimate of the state\ngiven the observation at time t",
    "start": "4325140",
    "end": "4331900"
  },
  {
    "text": "and to update our uncertainty\nabout that state given this new observation. So basically, our estimate\nof the state at time t",
    "start": "4331900",
    "end": "4341350"
  },
  {
    "text": "is an adjustment to our\nestimate given information up to t minus 1, plus a function of\nthe difference between what we",
    "start": "4341350",
    "end": "4351164"
  },
  {
    "text": "observed and what we predicted.  And this T_t function matrix is\ncalled the filter gain matrix.",
    "start": "4351164",
    "end": "4362870"
  },
  {
    "text": "And basically, it\ncharacterizes how do we adjust our prediction\nof the underlying state",
    "start": "4362870",
    "end": "4370070"
  },
  {
    "text": "depending on what happened. So that's the\nfilter gain matrix. ",
    "start": "4370070",
    "end": "4377150"
  },
  {
    "text": "So we actually do\ngain information with each observation about what\nthe new value of the process",
    "start": "4377150",
    "end": "4383160"
  },
  {
    "text": "is. And that information\nis characterized by filter gain matrix.",
    "start": "4383160",
    "end": "4389190"
  },
  {
    "text": "You'll notice that\nthe uncertainty in the state at time t, this\nomega_s of t given t, that's",
    "start": "4389190",
    "end": "4395720"
  },
  {
    "text": "equal to the covariance\nmatrix given t minus 1. So it's our beginning level\nof uncertainty adjusted",
    "start": "4395720",
    "end": "4403330"
  },
  {
    "text": "by a term that tells us\nhow much information did we get from that new information.",
    "start": "4403330",
    "end": "4409580"
  },
  {
    "text": "So notice that there's\na minus sign there. We're basically\nreducing our uncertainty",
    "start": "4409580",
    "end": "4415599"
  },
  {
    "text": "about the state given the\ninformation in the innovation",
    "start": "4415600",
    "end": "4424602"
  },
  {
    "text": "that we now have observed.  Then, there's a\nforecasting step which",
    "start": "4424602",
    "end": "4431869"
  },
  {
    "text": "is used to forecast the\nstate one period forward,",
    "start": "4431870",
    "end": "4439310"
  },
  {
    "text": "is simply given by this\nlinear transformation of the previous state. And we can also update\nour covariance matrix",
    "start": "4439310",
    "end": "4445890"
  },
  {
    "text": "for future states given\nthe previous state by applying this formula\nwhich is a recursive formula",
    "start": "4445890",
    "end": "4453530"
  },
  {
    "text": "for estimating covariances. So we have\nforecasting algorithms",
    "start": "4453530",
    "end": "4464760"
  },
  {
    "text": "that are simple linear\nfunctions of these estimates. And then finally,\nthere's a smoothing step",
    "start": "4464760",
    "end": "4475650"
  },
  {
    "text": "which is characterizing\nthe conditional expectation",
    "start": "4475650",
    "end": "4483960"
  },
  {
    "text": "of underlying states, given\ninformation in the whole time",
    "start": "4483960",
    "end": "4489950"
  },
  {
    "text": "series. And so ordinarily with Kalman\nfilters, Kalman filters",
    "start": "4489950",
    "end": "4495440"
  },
  {
    "text": "are applied\nsequentially over time where one basically\nis predicting ahead",
    "start": "4495440",
    "end": "4501090"
  },
  {
    "text": "one step, updating\nthat prediction, predicting ahead another\nstep, updating the information",
    "start": "4501090",
    "end": "4508320"
  },
  {
    "text": "on the states. And that overall\nprocess is the process",
    "start": "4508320",
    "end": "4519410"
  },
  {
    "text": "of actually computing\nthe likelihood function for these linear\nstate-space models.",
    "start": "4519410",
    "end": "4525210"
  },
  {
    "text": "And so the Kalman filter is\nbasically ultimately applied",
    "start": "4525210",
    "end": "4532140"
  },
  {
    "text": "for successive\nforecasting of the process but also for helping us identify\nwhat the underlying model",
    "start": "4532140",
    "end": "4539599"
  },
  {
    "text": "parameters are using\nmaximum likelihood methods. And so the likelihood function\nfor the linear state-space",
    "start": "4539600",
    "end": "4548290"
  },
  {
    "text": "model is basically the--\nor the log-likelihood is the log-likelihood of\nthe entire data series,",
    "start": "4548290",
    "end": "4554920"
  },
  {
    "text": "give the unknown parameters. But that can be\nexpressed as the product",
    "start": "4554920",
    "end": "4560020"
  },
  {
    "text": "of the conditional distributions\nof each successive observation, given the history.",
    "start": "4560020",
    "end": "4567150"
  },
  {
    "text": "And so basically, the\nlikelihood of theta is the likelihood of\nthe first observation",
    "start": "4567150",
    "end": "4572390"
  },
  {
    "text": "times the density of the\nsecond observation given the first times and so\nforth for the whole series.",
    "start": "4572390",
    "end": "4578990"
  },
  {
    "text": "And so the likelihood\nfunction is basically a function of all these\nterms that we were computing",
    "start": "4578990",
    "end": "4585489"
  },
  {
    "text": "with the Kalman filter.  And with the Kalman\nfilter, it basically",
    "start": "4585490",
    "end": "4593469"
  },
  {
    "text": "provides all the terms\nnecessary for this estimation. If the error terms are\nnormally distributed,",
    "start": "4593470",
    "end": "4602269"
  },
  {
    "text": "then the means and\nvariances of these estimates are in fact characterizing\nthe exact distributions",
    "start": "4602270",
    "end": "4612750"
  },
  {
    "text": "of the process. Basically, we're taking--\nif the innovation series are all normal random\nvariables, then",
    "start": "4612750",
    "end": "4619290"
  },
  {
    "text": "the linear\nstate-space model, all it's doing is taking linear\ncombinations of normals for the underlying states and\nfor the actual observations.",
    "start": "4619290",
    "end": "4627410"
  },
  {
    "text": "And normal\ndistributions are fully characterized by\ntheir mean vectors and covariance matrices. And the Kalman\nfilter provides a way",
    "start": "4627410",
    "end": "4634050"
  },
  {
    "text": "to update these distributions\nfor all these features",
    "start": "4634050",
    "end": "4641570"
  },
  {
    "text": "of a model, the\nunderlying states as well as the distributions\nof the observations. So that's a brief introduction\nthe Kalman filter.",
    "start": "4641570",
    "end": "4655250"
  },
  {
    "text": "Let's finish there. Thank you.",
    "start": "4655250",
    "end": "4658489"
  }
]