[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13960",
    "end": "19140"
  },
  {
    "text": " PROFESSOR: I'm going to spend\nmost of time talking about",
    "start": "19140",
    "end": "26560"
  },
  {
    "text": "chapters one, two, and three. A little bit talking about\nchapter four, because we've",
    "start": "26560",
    "end": "32220"
  },
  {
    "text": "been doing so much with chapter\nfour in the last couple of weeks that you\nprobably remember that more.",
    "start": "32220",
    "end": "39980"
  },
  {
    "text": "OK. The basics, which we started out\nwith, and which you should never forget, is that any time\nyou develop a probability",
    "start": "39980",
    "end": "48800"
  },
  {
    "start": "40000",
    "end": "1340000"
  },
  {
    "text": "model, you've got to specify\nwhat the sample space is and",
    "start": "48800",
    "end": "53840"
  },
  {
    "text": "what the probability measure\non that sample space is. And in practice, and in almost\neverything we've talked about",
    "start": "53840",
    "end": "61850"
  },
  {
    "text": "so far, there's really a basic\ncountable set of random variables which determine\neverything else.",
    "start": "61850",
    "end": "68490"
  },
  {
    "text": "In other words, when you find\nthe joint probability distribution on that set of\nrandom variables, that tells",
    "start": "68490",
    "end": "76729"
  },
  {
    "text": "you everything else\nof interest. And a sample point or a sample\npath on that set of random",
    "start": "76730",
    "end": "85200"
  },
  {
    "text": "variables is in a collection of\nsample values, one sample value for each random\nvariable.",
    "start": "85200",
    "end": "93979"
  },
  {
    "text": "It's very convenient, especially\nwhen you're in an exam and a little bit rushed,\nto confuse random variables",
    "start": "93980",
    "end": "103630"
  },
  {
    "text": "with the sample values for\nthe random variables. And that's fine.",
    "start": "103630",
    "end": "108920"
  },
  {
    "text": "I just want to caution you\nagain, and I've done this many times, that about half the\nmistakes that people make--",
    "start": "108920",
    "end": "118410"
  },
  {
    "text": "half of the conceptual mistakes\nthat people make doing problems and doing quizzes\nare connected with",
    "start": "118410",
    "end": "126200"
  },
  {
    "text": "getting confused at some point\nabout what's a random variable and what's a sample value\nof that random variable.",
    "start": "126200",
    "end": "132209"
  },
  {
    "text": "And you start thinking about\nsample values as just numbers. And I do that too.",
    "start": "132210",
    "end": "139090"
  },
  {
    "text": "It's convenient for thinking\nabout things. But you have to know that that's\nnot the whole story.",
    "start": "139090",
    "end": "146790"
  },
  {
    "text": "Often, we have uncountable\nsets of random variables. Like in renewal processes, we\nhave the counting renewal",
    "start": "146790",
    "end": "154720"
  },
  {
    "text": "process, which typically has an\nuncountable set of random variables, a number of arrivals\nup to each time, t,",
    "start": "154720",
    "end": "163860"
  },
  {
    "text": "where t is a continuous valued\nrandom variable. But in almost all of those\ncases, you can define things",
    "start": "163860",
    "end": "172810"
  },
  {
    "text": "in terms of simpler sets of\nrandom variables, like the interarrival times,\nwhich are IID.",
    "start": "172810",
    "end": "179480"
  },
  {
    "text": " Most of the processes we've\ntalked about really have a",
    "start": "179480",
    "end": "185959"
  },
  {
    "text": "pretty simple description if\nyou look for the simplest description of them. ",
    "start": "185960",
    "end": "193730"
  },
  {
    "text": "If you have a sequence of\nIID random variables-- which is what we have for\nPoisson and renewal processes,",
    "start": "193730",
    "end": "205269"
  },
  {
    "text": "and what we have for Markov\nchains is not that much more complicated--",
    "start": "205270",
    "end": "210310"
  },
  {
    "text": "the laws of large numbers are\nuseful to specify what the",
    "start": "210310",
    "end": "215500"
  },
  {
    "text": "long term behavior is. The sample time average is, as\nwe all know by now, is the sum",
    "start": "215500",
    "end": "227280"
  },
  {
    "text": "of the random variables\ndivided by n. So it's a sample average\nof these quantities.",
    "start": "227280",
    "end": "233090"
  },
  {
    "text": "The random variable, which has\na main x bar, the expected value of x, that's\nalmost obvious.",
    "start": "233090",
    "end": "240140"
  },
  {
    "text": "You just take the expected value\nof s sub n, and it's n times the expected value of x\ndivided by n, and you're done.",
    "start": "240140",
    "end": "248360"
  },
  {
    "text": "And the variance, since these\nrandom variables are independent, you find that\nalmost as easily.",
    "start": "248360",
    "end": "255540"
  },
  {
    "text": "That has this very\nsimple-minded distribution function.",
    "start": "255540",
    "end": "260850"
  },
  {
    "text": "Remember, we usually work\nwith distribution functions in this class.",
    "start": "260850",
    "end": "266960"
  },
  {
    "text": "And often, the exercises are\nmuch easier when you do them",
    "start": "266960",
    "end": "272580"
  },
  {
    "text": "in terms of the distribution\nfunction than if you use formulas you remember from\nelementary courses, which are",
    "start": "272580",
    "end": "280760"
  },
  {
    "text": "specialized to-- which are specialized to\nprobability density and",
    "start": "280760",
    "end": "287140"
  },
  {
    "text": "probability mass functions, and\noften have more special conditions on them than that.",
    "start": "287140",
    "end": "293110"
  },
  {
    "text": "But anyway, the distribution\nfunction starts to look like this.",
    "start": "293110",
    "end": "298570"
  },
  {
    "text": "As n gets bigger, you notice\nthat what's happening is that you get a distribution which\nis scrunching in this way,",
    "start": "298570",
    "end": "308860"
  },
  {
    "text": "which is starting to\nlook smoother. The jumps in it gets smaller. And you start out with this\nthing which is kind of crazy.",
    "start": "308860",
    "end": "318630"
  },
  {
    "text": "And by time, n is even 50. You get something which\nalmost looks like a--",
    "start": "318630",
    "end": "325770"
  },
  {
    "text": "I don't know how we tell\nthe difference between those two things. I thought we could,\nbut we can't. I certainly can't up there.",
    "start": "325770",
    "end": "331669"
  },
  {
    "text": "But anyway, the one that's\ntightest in is the one",
    "start": "331670",
    "end": "337650"
  },
  {
    "text": "for n equals 50. And what these laws of large\nnumbers all say in some sense",
    "start": "337650",
    "end": "344150"
  },
  {
    "text": "is that this distribution\nfunction gets crunched in",
    "start": "344150",
    "end": "351380"
  },
  {
    "text": "towards an impulse\nat the mean. And then they say other more\nspecialized things about how",
    "start": "351380",
    "end": "358259"
  },
  {
    "text": "this happens, about sample\npaths and all of that. But the idea is that this\ndistribution function is",
    "start": "358260",
    "end": "366270"
  },
  {
    "text": "heading towards a\nunit impulse. The weak law of large numbers\nthen says that if the expected",
    "start": "366270",
    "end": "374440"
  },
  {
    "text": "value of the magnitude of x\nis less than infinity-- and usually when we talk about\nrandom variables having a",
    "start": "374440",
    "end": "381660"
  },
  {
    "text": "mean, that's exactly\nwhat we mean. If that condition is not\nsatisfied, then we usually say",
    "start": "381660",
    "end": "391220"
  },
  {
    "text": "that the random variable\ndoesn't have a mean. And you'll see that every time\nyou look at anything in",
    "start": "391220",
    "end": "397300"
  },
  {
    "text": "probability theory. When people say the mean exists,\nthat's what they always mean.",
    "start": "397300",
    "end": "403830"
  },
  {
    "text": "And what the theorem says then\nis exactly what we were talking about before.",
    "start": "403830",
    "end": "409060"
  },
  {
    "text": "The probability that the\ndifference between s n over n,",
    "start": "409060",
    "end": "414940"
  },
  {
    "text": "and the mean of x bar, the\nprobability that it's greater than or equal to epsilon\nequals 0 in the limit.",
    "start": "414940",
    "end": "423090"
  },
  {
    "text": "So it's saying that you put\nepsilon limits on that distribution function and let\nn get bigger and bigger, it",
    "start": "423090",
    "end": "430860"
  },
  {
    "text": "goes to 1 and 0. It says the probability of s n\nover n, less than or equal to",
    "start": "430860",
    "end": "438120"
  },
  {
    "text": "x, approaches a unit step as\nn approaches infinity.",
    "start": "438120",
    "end": "443240"
  },
  {
    "text": "This says this is the condition\nfor convergence in probability.",
    "start": "443240",
    "end": "450440"
  },
  {
    "text": "What we're saying is that that\nalso means convergence and distribution function, and\ndistribution for this case.",
    "start": "450440",
    "end": "458740"
  },
  {
    "text": "And then we also, when we got\nto renewal processes, we talked about the strong\nlaw of large numbers.",
    "start": "458740",
    "end": "465330"
  },
  {
    "text": "And that says that the expected\nvalue of x is finite. Then this limit approaches\nx on a sample path basis.",
    "start": "465330",
    "end": "476630"
  },
  {
    "text": "In other words, for every sample\npath, except this set of probability 0, this\ncondition holds true.",
    "start": "476630",
    "end": "485020"
  },
  {
    "text": "That doesn't seem like it's\nvery different or very important for the time being.",
    "start": "485020",
    "end": "490610"
  },
  {
    "text": "But when we started studying\nrenewal processes, which is where we actually talked about\nthis, we saw that in fact, it",
    "start": "490610",
    "end": "499120"
  },
  {
    "text": "let us talk about this, which\nsays that if you take any",
    "start": "499120",
    "end": "504830"
  },
  {
    "text": "function of s n over n-- in other words, a function\nof a real value--",
    "start": "504830",
    "end": "511590"
  },
  {
    "text": "a function of a-- a real valued function of a-- ",
    "start": "511590",
    "end": "520009"
  },
  {
    "text": "a real valued function\nof a real value, yes. What you get is that\nsame function",
    "start": "520010",
    "end": "526470"
  },
  {
    "text": "applied to the mean here. And that's the thing\nwhich is so useful for renewal processes.",
    "start": "526470",
    "end": "532630"
  },
  {
    "text": "And it's what usually makes\nthe strong law of large numbers so much easier to\nuse than the weak law.",
    "start": "532630",
    "end": "538730"
  },
  {
    "start": "538730",
    "end": "544220"
  },
  {
    "text": "That's a plug for\nthe strong law. There are many extensions of the\nweek love telling how fast the convergence is.",
    "start": "544220",
    "end": "550910"
  },
  {
    "text": "One thing you should always\nremember about the central limit theorem, is it really\ntells you something about the",
    "start": "550910",
    "end": "557509"
  },
  {
    "text": "weak law of large numbers. It tells you how fast that\nconvergence is and what the convergence looks like.",
    "start": "557510",
    "end": "564720"
  },
  {
    "text": "It says that if the variance\nof this underlying random variable is finite, then this\nlimit here is equal to the",
    "start": "564720",
    "end": "574000"
  },
  {
    "text": "normal distribution function,\nthe Gaussian at variance 1 and mean 0.",
    "start": "574000",
    "end": "581350"
  },
  {
    "text": "And that becomes a little easier\nto see what it's saying if you look at it this way.",
    "start": "581350",
    "end": "586870"
  },
  {
    "text": "It says probability that s\nn over n minus x bar-- namely the difference between\nthe sum and the mean which",
    "start": "586870",
    "end": "596890"
  },
  {
    "text": "it's converging to-- the probability that that's less\nthan or equal to y sigma over square root of\nn is this normal",
    "start": "596890",
    "end": "604010"
  },
  {
    "text": "Gaussian random variable. It says that as n gets bigger\nand bigger, this quantity here",
    "start": "604010",
    "end": "611740"
  },
  {
    "text": "gets tighter and tighter. What it says in terms of the\npicture here, in terms of this",
    "start": "611740",
    "end": "618620"
  },
  {
    "text": "picture, it says that as n gets\nbigger and bigger, this picture here scrunches down as\n1 over the square root of n.",
    "start": "618620",
    "end": "628560"
  },
  {
    "text": "And it also becomes Gaussian. | it tells you exactly what\nkind of convergence you",
    "start": "628560",
    "end": "633759"
  },
  {
    "text": "actually have here. Is not only saying that this\ndoes converge to a unit step.",
    "start": "633760",
    "end": "639200"
  },
  {
    "text": "It says how it converges. And that's a nice thing,\nconceptually.",
    "start": "639200",
    "end": "648240"
  },
  {
    "text": "You don't always need\nit in problems. But you need it for\nunderstanding what's going on.",
    "start": "648240",
    "end": "654600"
  },
  {
    "start": "654600",
    "end": "659889"
  },
  {
    "text": "We're moving backwards,\nit seems. ",
    "start": "659890",
    "end": "666180"
  },
  {
    "text": "Now, 1, 2, Poisson processes. We talked about arrival\nprocesses.",
    "start": "666180",
    "end": "672630"
  },
  {
    "text": "You'd almost think that all\nprocesses are arrival processes at this point. But any time you start to think\nabout that, think of a",
    "start": "672630",
    "end": "679770"
  },
  {
    "text": "Markov chain. And a Markov chain is not an\narrival process, ordinarily.",
    "start": "679770",
    "end": "686150"
  },
  {
    "text": "Some of them can be\nviewed that way. But most of them can't. An arrival processes\nis an increasing",
    "start": "686150",
    "end": "691990"
  },
  {
    "text": "sequence of random variables. 0 less than s1, which is the\ntime of the first arrival, s2,",
    "start": "691990",
    "end": "700020"
  },
  {
    "text": "which is a time of the second\narrival, and so forth. Interarrival times are x1 equals\ns1, and x i equals s i",
    "start": "700020",
    "end": "708220"
  },
  {
    "text": "minus s i minus 1. The picture, which you should\nhave indelibly printed on the",
    "start": "708220",
    "end": "715480"
  },
  {
    "text": "back of your brain someplace\nby this time, is this picture here. s1, s2, s3, are the times\nat which arrivals occur.",
    "start": "715480",
    "end": "724930"
  },
  {
    "text": "These are random variables, so\nthese arrivals come in at random times. x1, x2, x3 are the intervals\nbetween arrivals.",
    "start": "724930",
    "end": "734690"
  },
  {
    "text": "And N of t is the number of\narrivals that have occurred up until time t.",
    "start": "734690",
    "end": "739860"
  },
  {
    "text": "So every time the t passes one\nof these arrival times, N of t",
    "start": "739860",
    "end": "746800"
  },
  {
    "text": "pops up by one, pops up by one\nagain, pops up by one again. The sample value\npops up by one.",
    "start": "746800",
    "end": "754200"
  },
  {
    "text": "Arrival process can model\narrivals to a queue, departures from a queue,\nlocations of breaks in an oil",
    "start": "754200",
    "end": "760320"
  },
  {
    "text": "line, an enormous number\nof things. It's not just arrivals\nwe're talking about.",
    "start": "760320",
    "end": "766260"
  },
  {
    "text": "It's all of these other\nthings, also. But it's something laid out on\na one-dimensional axis where",
    "start": "766260",
    "end": "774330"
  },
  {
    "text": "things happen at various\nplaces on that one-dimensional axis.",
    "start": "774330",
    "end": "779700"
  },
  {
    "text": "So that's the way to view it.",
    "start": "779700",
    "end": "785100"
  },
  {
    "text": "OK, same picture again. Process can be specified by the\njoint distribution of the",
    "start": "785100",
    "end": "791509"
  },
  {
    "text": "arrival epochs or the\ninterarrival times, and, in fact, of the counting process.",
    "start": "791510",
    "end": "798090"
  },
  {
    "text": "If you see a sample path of\nthe counting process, then",
    "start": "798090",
    "end": "805200"
  },
  {
    "text": "from that you can determine the\nsample path of the arrival times and the sample path of\nthe interarrival times.",
    "start": "805200",
    "end": "813220"
  },
  {
    "text": "And since any set of these\nrandom variables specifies all",
    "start": "813220",
    "end": "818319"
  },
  {
    "text": "three of these things, the\nthree are all equivalent. OK, we have this important\ncondition here.",
    "start": "818320",
    "end": "827149"
  },
  {
    "text": "And I always sort of forget\nthis, but when these arrivals",
    "start": "827150",
    "end": "835960"
  },
  {
    "text": "are highly delayed, when there's\na long period of time between each arrival, what that\nsays is the accounting",
    "start": "835960",
    "end": "845380"
  },
  {
    "text": "process is getting small. So big interarrival times\ncorresponds to a small",
    "start": "845380",
    "end": "852570"
  },
  {
    "text": "value of N of t. And you can see that in\nthe picture here. If you spread out these\narrivals, you make s1 all the",
    "start": "852570",
    "end": "860019"
  },
  {
    "text": "way out here. Then N of t doesn't become\n1 until way out here.",
    "start": "860020",
    "end": "866190"
  },
  {
    "text": "So N of t as a function of t is\ngetting smaller as s sub n",
    "start": "866190",
    "end": "872930"
  },
  {
    "text": "is getting larger. S sub n is the minimum of the\nset of t, such that N of t is",
    "start": "872930",
    "end": "881560"
  },
  {
    "text": "greater than or equal to N.\nSounds like a unpleasantly complicated expression.",
    "start": "881560",
    "end": "889460"
  },
  {
    "text": "If any of you can find a simpler\nway to say it than that, I would be absolutely\ndelighted to hear it.",
    "start": "889460",
    "end": "895950"
  },
  {
    "text": "But I don't think there is. I think the simpler way to say\nit is this picture here.",
    "start": "895950",
    "end": "901150"
  },
  {
    "text": "And the picture says it. And you can sort of figure out\nall those logical statements",
    "start": "901150",
    "end": "908769"
  },
  {
    "text": "from the picture, which\nis intuitively a lot clearer, I think. ",
    "start": "908770",
    "end": "917270"
  },
  {
    "text": "So now, renewal processes is\nan arrival process with IID",
    "start": "917270",
    "end": "923380"
  },
  {
    "text": "interarrival times. And a Poisson process is a\nrenewal process where the",
    "start": "923380",
    "end": "928800"
  },
  {
    "text": "interarrival random variables\nare exponential. So, Poisson process\nis a special",
    "start": "928800",
    "end": "935290"
  },
  {
    "text": "case of renewal process. Why are these exponential\ninterarrival",
    "start": "935290",
    "end": "940920"
  },
  {
    "text": "arrival times so important? Well, it's because they're\nmemoryless.",
    "start": "940920",
    "end": "946550"
  },
  {
    "text": "And the memoryless property says\nthat the probability that x is greater than t plus x is\nequal to the probability that",
    "start": "946550",
    "end": "954535"
  },
  {
    "text": "it's greater than x times the\nprobability that it's greater than t for all x and t greater\nthan or equal to 0.",
    "start": "954535",
    "end": "961830"
  },
  {
    "text": "This makes better sense if\nyou say it conditionally. The probability that x is\ngreater than t plus x, given",
    "start": "961830",
    "end": "969040"
  },
  {
    "text": "that it's greater than t, is\nthe same as the probability that x is greater that--",
    "start": "969040",
    "end": "974800"
  },
  {
    "text": "capital X is greater\nthan little x. This really gives you\nthe memoryless",
    "start": "974800",
    "end": "980420"
  },
  {
    "text": "property in a nutshell. It says if you're looking at\nthis process as it evolves,",
    "start": "980420",
    "end": "985860"
  },
  {
    "text": "and you see an arrival, and then\nyou start looking for the next arrival, it says that no\nmatter how long you've been",
    "start": "985860",
    "end": "992160"
  },
  {
    "text": "looking, the distribution\nfunction, as the time to wait until the next arrival,\nis the same",
    "start": "992160",
    "end": "998930"
  },
  {
    "text": "exponential random variable. So you never gain anything\nby waiting.",
    "start": "998930",
    "end": "1004220"
  },
  {
    "text": "You might as well\nbe impatient. But it doesn't do any good\nto be impatient. Doesn't to any good to wait.",
    "start": "1004220",
    "end": "1011130"
  },
  {
    "text": "It doesn't do any good\nto not wait. No matter what you do, this\ndamn thing always takes an",
    "start": "1011130",
    "end": "1016279"
  },
  {
    "text": "exponential amount\nof time to occur. OK, that's what it means\nto be memoryless.",
    "start": "1016280",
    "end": "1021410"
  },
  {
    "text": "And the exponential is the only memoryless random variable. ",
    "start": "1021410",
    "end": "1030774"
  },
  {
    "text": "How about a geometric\nrandom variable? The geometric random variable\nis memoryless if you're only",
    "start": "1030775",
    "end": "1039189"
  },
  {
    "text": "looking at integer times. Here we're talking about\ntimes on a continuum.",
    "start": "1039190",
    "end": "1052180"
  },
  {
    "text": "That's what this says. Well, that's what this says.",
    "start": "1052180",
    "end": "1058409"
  },
  {
    "text": "And if you look at discrete\ntimes, then a geometric random",
    "start": "1058410",
    "end": "1066590"
  },
  {
    "text": "variable is memoryless also. ",
    "start": "1066590",
    "end": "1075020"
  },
  {
    "text": "We're given a Poisson\nof rate lambda. The interval from any given t\ngreater than 0 until the first",
    "start": "1075020",
    "end": "1081289"
  },
  {
    "text": "arrival after t is a\nrandom variable. Let's call it z1. We already said that that\nrandom variable was",
    "start": "1081290",
    "end": "1088650"
  },
  {
    "text": "exponential. And it's independent of all\narrivals which occur before",
    "start": "1088650",
    "end": "1097040"
  },
  {
    "text": "that starting time t. So looking at any starting\ntime t, doesn't make any",
    "start": "1097040",
    "end": "1103220"
  },
  {
    "text": "difference what has happened\nback here. That's not only the\nlast arrival, but all the other arrivals.",
    "start": "1103220",
    "end": "1109630"
  },
  {
    "text": "The time until the next arrival\nis exponential. The time until each arrival\nafter that is exponential",
    "start": "1109630",
    "end": "1116520"
  },
  {
    "text": "also, which says that if you\nlook at this process starting",
    "start": "1116520",
    "end": "1121690"
  },
  {
    "text": "at time t, it's a Poisson\nprocess again, where all the",
    "start": "1121690",
    "end": "1127250"
  },
  {
    "text": "times have to be shifted, of\ncourse, but it's a Poisson process starting at time t.",
    "start": "1127250",
    "end": "1132830"
  },
  {
    "text": "The corresponding counting\nprocess, we can call it n",
    "start": "1132830",
    "end": "1140570"
  },
  {
    "text": "tilde of t and tau, where tau is\ngreater than or equal to t, where this is the number of\narrivals in the original",
    "start": "1140570",
    "end": "1149690"
  },
  {
    "text": "process up until time tau minus\nthe number of arrivals up until time t.",
    "start": "1149690",
    "end": "1156340"
  },
  {
    "text": "If you look at that difference,\nso many arrivals up until t, so many more\nup until time tau.",
    "start": "1156340",
    "end": "1166549"
  },
  {
    "text": "You look at the difference\nbetween tau and t. The number of arrivals in that\ninterval is the same Poisson",
    "start": "1166550",
    "end": "1177080"
  },
  {
    "text": "distributing random\nvariable again. So, it has the same\ndistribution as N",
    "start": "1177080",
    "end": "1183080"
  },
  {
    "text": "of tau minus t. And that's called the stationary\nincrement property. It says that no matter where you\nstart a Poisson process,",
    "start": "1183080",
    "end": "1190720"
  },
  {
    "text": "it always looks exactly\nthe same. It says that if you wait for one\nhour and start then, it's",
    "start": "1190720",
    "end": "1198370"
  },
  {
    "text": "exactly the same as what\nit was before. If we had Poisson processes in\nthe world, it wouldn't do any",
    "start": "1198370",
    "end": "1205960"
  },
  {
    "text": "good to travel on certain days\nrather than other days. It wouldn't do any good to leave\nto drive home at one",
    "start": "1205960",
    "end": "1213169"
  },
  {
    "text": "hour rather than another hour. You'd have the same travel\nall the time. It's all equal.",
    "start": "1213170",
    "end": "1218980"
  },
  {
    "text": "It would be an awful world\nif it were stationary.  The independent increment\nproperties for counting",
    "start": "1218980",
    "end": "1226750"
  },
  {
    "text": "process is that for all\nsequences of ordered times--",
    "start": "1226750",
    "end": "1233170"
  },
  {
    "text": "0 less than t1 less than\nt2 up to t k-- the random variables n of t1--",
    "start": "1233170",
    "end": "1240309"
  },
  {
    "text": "and now we're talking about the\nnumber of arrivals between t1 and t2, the number\nof arrivals between",
    "start": "1240310",
    "end": "1247509"
  },
  {
    "text": "n minus 1 and tn. These are all independent\nof each other. That's what this independent\nincrement property says.",
    "start": "1247510",
    "end": "1255390"
  },
  {
    "text": "And we see from what we've said\nabout this memoryless property that the Poisson\nprocess does indeed have this",
    "start": "1255390",
    "end": "1262679"
  },
  {
    "text": "independent increment\nproperty. Poisson processes have both the\nstationary and independent",
    "start": "1262680",
    "end": "1268720"
  },
  {
    "text": "increment properties. And this looks like an immediate\nconsequence of that.",
    "start": "1268720",
    "end": "1275760"
  },
  {
    "text": "It's not. Remember, we had to struggle\nwith this for a bit. But it says plus Poisson\nprocesses can be defined by",
    "start": "1275760",
    "end": "1282500"
  },
  {
    "text": "the stationary and independent\nincrement properties, plus either the Poisson PMF for N\nof t, or this incremental",
    "start": "1282500",
    "end": "1292730"
  },
  {
    "text": "property, the probability that N\ntilde of t and t plus delta,",
    "start": "1292730",
    "end": "1298660"
  },
  {
    "text": "and the number of arrivals\nbetween t and t plus delta, the probability that that's\n1 is equal to",
    "start": "1298660",
    "end": "1306169"
  },
  {
    "text": "lambda times delta. In other words, this view of a\nPoisson process is the view",
    "start": "1306170",
    "end": "1313039"
  },
  {
    "text": "that you get when you sort\nof forget about time. And you think of arrivals from\nouter space coming down and",
    "start": "1313040",
    "end": "1320220"
  },
  {
    "text": "hitting on a line. And they hit on that\nline randomly. And each one of them\nis independent",
    "start": "1320220",
    "end": "1325860"
  },
  {
    "text": "of every other one. And that's what you get if you\nwind up with a density of",
    "start": "1325860",
    "end": "1335350"
  },
  {
    "text": "lambda arrivals per unit time. OK, we talked about all\nof that, of course.",
    "start": "1335350",
    "end": "1342120"
  },
  {
    "start": "1340000",
    "end": "1680000"
  },
  {
    "text": "The probability distributions--  there are many of them for\na Poisson process.",
    "start": "1342120",
    "end": "1349380"
  },
  {
    "text": "The Poisson process is\nremarkable in the sense that anything you want to find,\nthere's generally a simple",
    "start": "1349380",
    "end": "1355320"
  },
  {
    "text": "formula for it. If it's complicated, you're\nprobably not looking at it the right way.",
    "start": "1355320",
    "end": "1362010"
  },
  {
    "text": "So many things come out\nvery, very simply. The probability-- the joint probability\ndistribution of all of the",
    "start": "1362010",
    "end": "1370580"
  },
  {
    "text": "arrival times up until time N is\nan exponential just in the",
    "start": "1370580",
    "end": "1378669"
  },
  {
    "text": "last one, which says that the\nintermediate arrival epochs",
    "start": "1378670",
    "end": "1385080"
  },
  {
    "text": "are equally likely to be\nanywhere, just as long as they satisfy this ordering\nrestriction, s1 less than s2.",
    "start": "1385080",
    "end": "1393440"
  },
  {
    "text": "That's what this formula says. It says that the joint density\nof these arrival times doesn't",
    "start": "1393440",
    "end": "1400490"
  },
  {
    "text": "depend on anything except the\ntime of the last one. ",
    "start": "1400490",
    "end": "1405740"
  },
  {
    "text": "But it does depend on the fact\nthat they're [INAUDIBLE]. From that, you can find\nvirtually everything else if",
    "start": "1405740",
    "end": "1411435"
  },
  {
    "text": "you want to. That really is saying exactly\nthe same thing as we were just",
    "start": "1411435",
    "end": "1416600"
  },
  {
    "text": "saying a while ago. This is the viewpoint of looking\nat this line from",
    "start": "1416600",
    "end": "1421740"
  },
  {
    "text": "outer space with arrivals coming\nin, coming in uniformly",
    "start": "1421740",
    "end": "1427040"
  },
  {
    "text": "distributed over this line\ninterval, and each of them independent of each other one.",
    "start": "1427040",
    "end": "1434080"
  },
  {
    "text": "That's what you wind\nup saying. This density, then, of the\nn-th arrival, if you just",
    "start": "1434080",
    "end": "1441490"
  },
  {
    "text": "integrate all this stuff, you\nget the Erlang formula. Probability of arrival n in\nt to t plus delta is--",
    "start": "1441490",
    "end": "1452940"
  },
  {
    "text": "now this is the derivation that\nwe went through before, going from Erlang to Poisson.",
    "start": "1452940",
    "end": "1460309"
  },
  {
    "text": "You can go from Poisson to\nErlang too, if you want to. But it's a little easier\nto go this way.",
    "start": "1460310",
    "end": "1466320"
  },
  {
    "text": "The probability of arrival in\nt to t plus delta is the probability that n of t is\nequal to n minus 1 times",
    "start": "1466320",
    "end": "1475890"
  },
  {
    "text": "lambda delta plus an o\nof delta, of course. And the probability that n of\nt is equal to n minus 1 from",
    "start": "1475890",
    "end": "1486270"
  },
  {
    "text": "this formula here is going to be\nthe density of when s sub n",
    "start": "1486270",
    "end": "1493050"
  },
  {
    "text": "appears, divided by lambda. That's exactly what this\nformula here says.",
    "start": "1493050",
    "end": "1498910"
  },
  {
    "text": "So that's just the Poisson\ndistribution. We've been through\nthat derivation.",
    "start": "1498910",
    "end": "1504910"
  },
  {
    "text": "It's almost a derivation worth\nremembering, because it just appears so often.",
    "start": "1504910",
    "end": "1511940"
  },
  {
    "text": "As you've seen from the problem\nsets we've done, almost every problem you can\ndream of, dealing with Poisson",
    "start": "1511940",
    "end": "1520970"
  },
  {
    "text": "processes, the easy way to do\nthem comes from this property",
    "start": "1520970",
    "end": "1527150"
  },
  {
    "text": "of combining and splitting\nPoisson processes. It says if n1 of t, n2 of t,\nup to n sub k of t are",
    "start": "1527150",
    "end": "1535169"
  },
  {
    "text": "independent Poisson\nprocesses-- what do you mean by\na process being independent of another process?",
    "start": "1535170",
    "end": "1542200"
  },
  {
    "text": "Well, the process is specified\nby the interarrival times for that process.",
    "start": "1542200",
    "end": "1547660"
  },
  {
    "text": "So what we're saying here is the\ninterarrival times for the first process are independent\nof the interarrival times of",
    "start": "1547660",
    "end": "1554470"
  },
  {
    "text": "the second process,\nindependent of the interarrival times for the third\nprocess, and so forth.",
    "start": "1554470",
    "end": "1560620"
  },
  {
    "text": "Again, this is a view of someone\nfrom outer space, throwing darts onto a line.",
    "start": "1560620",
    "end": "1566179"
  },
  {
    "text": "And if you have multiple people\nthrowing darts on a line, but they're all equally\ndistributed, all uniformly",
    "start": "1566180",
    "end": "1573450"
  },
  {
    "text": "distributed over the line,\nthis is exactly the model you get.",
    "start": "1573450",
    "end": "1580669"
  },
  {
    "text": "So we have two views here. The first one is to look at\nthe arrival epochs that's",
    "start": "1580670",
    "end": "1586480"
  },
  {
    "text": "generated from each process. And then combine all arrivals\ninto one Poisson process.",
    "start": "1586480",
    "end": "1591710"
  },
  {
    "text": "So we look at all these Poisson\nprocesses, and then take the sum of them, and we\nget a Poisson process.",
    "start": "1591710",
    "end": "1598340"
  },
  {
    "text": "The other way to look at it-- and going back and forth between\nthese two views is the way you solve problems--",
    "start": "1598340",
    "end": "1605059"
  },
  {
    "text": "you look at the combined\nsequence of arrival epochs first. And then for each arrival that\ncomes in, you think of an IID",
    "start": "1605060",
    "end": "1612400"
  },
  {
    "text": "random variable independent\nof all the other random variables, which decides for\neach arrival which of the",
    "start": "1612400",
    "end": "1622860"
  },
  {
    "text": "sub-processes it goes to. So there's this hidden\nprocess--",
    "start": "1622860",
    "end": "1628679"
  },
  {
    "text": "well, it's not hidden. You can see what it's doing\nfrom looking at all the sub-processes.",
    "start": "1628680",
    "end": "1634340"
  },
  {
    "text": "And each arrival then is\nassociated with the given",
    "start": "1634340",
    "end": "1640669"
  },
  {
    "text": "sub-process, with the\nprobability mass function lambda sub i over the\nsum of lambda sub j.",
    "start": "1640670",
    "end": "1648159"
  },
  {
    "text": "So this is the workhorse\nof Poisson type queueing problems. You study queuing theory,\nevery page, you",
    "start": "1648160",
    "end": "1655990"
  },
  {
    "text": "see this thing used. If you look at Kleinrock's books\non queueing, they're",
    "start": "1655990",
    "end": "1661480"
  },
  {
    "text": "very nice books because they\ncover so many different queueing situations.",
    "start": "1661480",
    "end": "1667040"
  },
  {
    "text": "You find him using this\non every page. And he never tells you that he's\nusing it, but that's what",
    "start": "1667040",
    "end": "1674059"
  },
  {
    "text": "he's doing. So that's a useful\nthing to know.",
    "start": "1674060",
    "end": "1679360"
  },
  {
    "text": "We then talked about conditional\narrivals and order statistics.",
    "start": "1679360",
    "end": "1685590"
  },
  {
    "start": "1680000",
    "end": "1827000"
  },
  {
    "text": "The conditional distribution\nof the N first arrivals--",
    "start": "1685590",
    "end": "1692279"
  },
  {
    "text": "namely, s sub 1 s sub\n2 up to s sub n--",
    "start": "1692280",
    "end": "1697670"
  },
  {
    "text": "given the number of arrivals in\nN of t is just n factorial",
    "start": "1697670",
    "end": "1704250"
  },
  {
    "text": "over t to the n. Again, it doesn't depend on\nwhere these arrivals are.",
    "start": "1704250",
    "end": "1709380"
  },
  {
    "text": "It's just a function which is\nindependent of each arrival. It's the same kind of\nconditioning we had before.",
    "start": "1709380",
    "end": "1716659"
  },
  {
    "text": "It's n factorial divided\nby t to the n. Because of the fact that if\nyou order these random",
    "start": "1716660",
    "end": "1724360"
  },
  {
    "text": "variables, t1 less than t2 less\nthan t3, and so forth, up",
    "start": "1724360",
    "end": "1729450"
  },
  {
    "text": "until time t, and then you say\nhow many different ways can I arrange a set of numbers, each\nbetween 0 and t so that we",
    "start": "1729450",
    "end": "1741590"
  },
  {
    "text": "have different orderings\nof them. And you can choose any one\nof the N to be the first.",
    "start": "1741590",
    "end": "1746700"
  },
  {
    "text": "You can choose any one\nof the remaining n minus 1 to be the second. And that's where this is n\nfactorial comes from here.",
    "start": "1746700",
    "end": "1754670"
  },
  {
    "text": "And that, again we've\nbeen over. The probability that s1 is\ngreater than tau, given that",
    "start": "1754670",
    "end": "1761660"
  },
  {
    "text": "they're interarrivals in the\noverall interval t, comes from",
    "start": "1761660",
    "end": "1767540"
  },
  {
    "text": "just looking at N uniformly\ndistributed random variables between 0 and t.",
    "start": "1767540",
    "end": "1773190"
  },
  {
    "text": "And then what do you do with\nthose uniformly distributed random variables? Well, you ask the question,\nwhat's the probability that",
    "start": "1773190",
    "end": "1780490"
  },
  {
    "text": "all of them occur\nafter time tau? And that's just t minus tau\ndivided by t raised to the",
    "start": "1780490",
    "end": "1787820"
  },
  {
    "text": "n-th power. And see, all of these formulas\njust come from particular viewpoints about what's\ngoing on.",
    "start": "1787820",
    "end": "1794360"
  },
  {
    "text": "You have a number\nof viewpoints. One of them is throwing\ndarts at a line. One of them is having\nexponential",
    "start": "1794360",
    "end": "1801140"
  },
  {
    "text": "interarrival times. One of them is these uniform\ninterarrivals.",
    "start": "1801140",
    "end": "1806660"
  },
  {
    "text": "It's only a very small\nnumber of tricks. And you just use them in\nvarious combinations.",
    "start": "1806660",
    "end": "1813600"
  },
  {
    "text": "So the joint distribution of s1\nto s n, given N of t equals n, is the same as the joint\ndistribution of N uniform",
    "start": "1813600",
    "end": "1821250"
  },
  {
    "text": "random variables after\nthey've been ordered. ",
    "start": "1821250",
    "end": "1828649"
  },
  {
    "start": "1827000",
    "end": "2270000"
  },
  {
    "text": "So let's go on to finite\nstate Markov chains. ",
    "start": "1828650",
    "end": "1835240"
  },
  {
    "text": "Seems like we're covering an\nenormous amount of material in this course. And I think we are. But as I'm trying to say, as\nwe go along, it's all--",
    "start": "1835240",
    "end": "1844290"
  },
  {
    "text": "I mean, everything follows from\na relatively small set of principles. Of course, it's harder to\nunderstand the small set of",
    "start": "1844290",
    "end": "1851100"
  },
  {
    "text": "principles and how to apply them\nthan it is to understand all the details. But that's--",
    "start": "1851100",
    "end": "1856710"
  },
  {
    "text": " but on the other hand, if you\nunderstand the principles, then all those details,\nincluding the ones we haven't",
    "start": "1856710",
    "end": "1864620"
  },
  {
    "text": "talked about, are easy\nto deal with. An integer-time stochastic\nprocess--",
    "start": "1864620",
    "end": "1871750"
  },
  {
    "text": "x1, x2, x3, blah, blah, blah-- is a Markov chain if for all n,\nnamely the number of them",
    "start": "1871750",
    "end": "1879220"
  },
  {
    "text": "that we're looking at-- well-- ",
    "start": "1879220",
    "end": "1885880"
  },
  {
    "text": "for all n, i, j, k, l, and so\nforth, the probability that the n-th of these random\nvariables is equal to j, given",
    "start": "1885880",
    "end": "1895769"
  },
  {
    "text": "what all of the others are-- and\nthese are not ordered now. I mean, in a Markov chain,\nnothing is ordered.",
    "start": "1895770",
    "end": "1901460"
  },
  {
    "text": "We're not talking about\nan arrival process. We're just talking about a frog\njumping around on lily",
    "start": "1901460",
    "end": "1907220"
  },
  {
    "text": "pads, if you arrange the lily\npads in a linear way, if these",
    "start": "1907220",
    "end": "1912659"
  },
  {
    "text": "are random variables. The probability that the n-th\nlocation is equal to j, given",
    "start": "1912660",
    "end": "1920530"
  },
  {
    "text": "that the previous locations are\ni, k, back to m, is just",
    "start": "1920530",
    "end": "1926410"
  },
  {
    "text": "some probability p sub\ni j, a conditional probability of j given i.",
    "start": "1926410",
    "end": "1934120"
  },
  {
    "text": "In other words, once if you're\nlooking at what happens at time n, once you know what\nhappened at time n minus 1,",
    "start": "1934120",
    "end": "1942340"
  },
  {
    "text": "everything else is\nof no concern. This process evolves by having\na history of only one time",
    "start": "1942340",
    "end": "1949399"
  },
  {
    "text": "unit, a little like the\nPoisson process. The Poisson process evolves\nby being totally",
    "start": "1949400",
    "end": "1956070"
  },
  {
    "text": "independent of the past. Here, you put a little\ndependence in the past. But the dependence is only to\nlook at the last thing that",
    "start": "1956070",
    "end": "1964150"
  },
  {
    "text": "happened, and nothing before the\nlast time that happened. So p sub i j depends\nonly on i and j.",
    "start": "1964150",
    "end": "1973850"
  },
  {
    "text": "And the initial probability mass\nfunction is arbitrary.",
    "start": "1973850",
    "end": "1979169"
  },
  {
    "text": "Markov chain is finite-state if\nthe sample space for each x i, as a finite set S. And the\nsample space S is usually",
    "start": "1979170",
    "end": "1987400"
  },
  {
    "text": "taken to be integers\n1 up to M. In all these formulas we write,\nwe're always summing",
    "start": "1987400",
    "end": "1993490"
  },
  {
    "text": "from one to M. And the reason\nfor that is we've assumed the states are 1, 2, 3, up to M.\nSometimes it's more convenient",
    "start": "1993490",
    "end": "2002120"
  },
  {
    "text": "to think of different\nstate spaces.  But all the formulas\nwe use are based on",
    "start": "2002120",
    "end": "2009040"
  },
  {
    "text": "this state space here. Markov up chain is completely\ndescribed by these transition",
    "start": "2009040",
    "end": "2016500"
  },
  {
    "text": "probabilities plus the initial\nprobabilities. If you want to write down the\nprobability of what x is this",
    "start": "2016500",
    "end": "2024389"
  },
  {
    "text": "some time N given what was at\nsome time 0, all you have to do is trace all the paths from\n0 out to N, add up the",
    "start": "2024390",
    "end": "2032890"
  },
  {
    "text": "probabilities of all of those\npaths, and that tells you the probability you want.",
    "start": "2032890",
    "end": "2038020"
  },
  {
    "text": "All probabilities and be\ncalculated just from knowing what these transition\nprobabilities are.",
    "start": "2038020",
    "end": "2046240"
  },
  {
    "text": "Note that when we're dealing\nwith Poisson processes, we defined everything in\nterms of how many--",
    "start": "2046240",
    "end": "2055520"
  },
  {
    "text": "how many variables are there in\ndefining a Poisson process? How many things do you have to\nspecify before I know exactly",
    "start": "2055520",
    "end": "2065020"
  },
  {
    "text": "what Poisson process\nI'm talking about? ",
    "start": "2065020",
    "end": "2070540"
  },
  {
    "text": "Only the Poisson rate. Only one parameter is necessary",
    "start": "2070540",
    "end": "2075649"
  },
  {
    "text": "for a Poisson process. For a finite-state Markov\nprocess, you need a lot more.",
    "start": "2075650",
    "end": "2083219"
  },
  {
    "text": "What you need is all of these\nvalues, p sub i j.",
    "start": "2083219",
    "end": "2088310"
  },
  {
    "text": "If you sum p sub i j over\nj, you have to get 1. So that removes one of them.",
    "start": "2088310",
    "end": "2094829"
  },
  {
    "text": "But as soon as you specify that\ntransition matrix, you've specified everything.",
    "start": "2094830",
    "end": "2099960"
  },
  {
    "text": "So there's nothing more to know about the Poisson process. There's only all these gruesome\nderivations that we",
    "start": "2099960",
    "end": "2106060"
  },
  {
    "text": "go through. But everything is initially\ndetermined.",
    "start": "2106060",
    "end": "2111600"
  },
  {
    "text": "Set of transition probabilities\nis usually viewed as the Markov chain. And the initial probabilities\nare usually viewed as just a",
    "start": "2111600",
    "end": "2119760"
  },
  {
    "text": "parameter that we deal with. In other words, we-- in other words, what we study\nis the particular Markov",
    "start": "2119760",
    "end": "2128250"
  },
  {
    "text": "chain, whether it's recurrent,\nwhether it's transient, whatever it is. How you break it up into\nclasses, all of that stuff",
    "start": "2128250",
    "end": "2135770"
  },
  {
    "text": "only depends on these transition\nprobabilities and doesn't depend on\nwhere you start.",
    "start": "2135770",
    "end": "2140815"
  },
  {
    "start": "2140815",
    "end": "2146920"
  },
  {
    "text": "Now, a finite-state Markov chain\ncan be described either as a directed graph\nor as a matrix.",
    "start": "2146920",
    "end": "2154230"
  },
  {
    "text": "I hope you've seen by this\ntime that some things are easier to look at if you look at\nthings in terms of a graph.",
    "start": "2154230",
    "end": "2163040"
  },
  {
    "text": "Some things are easier to look\nat if you look at something like this matrix.",
    "start": "2163040",
    "end": "2168660"
  },
  {
    "text": "And some problems can be solved\nby inspection, if you draw a graph of it.",
    "start": "2168660",
    "end": "2174700"
  },
  {
    "text": "Some can be solved almost\nby inspection if you look at the matrix. If you're doing things by\ncomputer, usually computers",
    "start": "2174700",
    "end": "2183460"
  },
  {
    "text": "deal with matrices more easily\nthan with graphs. If you're dealing with a Markov\nchain with 100,000",
    "start": "2183460",
    "end": "2191070"
  },
  {
    "text": "states, you're not going to\nlook at the graph and determine very much from it,\nbecause it's typically going",
    "start": "2191070",
    "end": "2198330"
  },
  {
    "text": "to be fairly complicated-- unless it has some very\nsimple structure. And sometimes that simple\nstructure is determined.",
    "start": "2198330",
    "end": "2206440"
  },
  {
    "text": "If it's something where\nyou can only-- where you have the states\nnumbered from 1 to 100,000,",
    "start": "2206440",
    "end": "2212190"
  },
  {
    "text": "and you can only go from state\ni to state i plus 1, or from state i to i plus 1, or\ni minus 1, then it",
    "start": "2212190",
    "end": "2219910"
  },
  {
    "text": "becomes very simple. And you like to look at\nit as a graph again. But ordinarily, you don't\nlike to do that.",
    "start": "2219910",
    "end": "2227670"
  },
  {
    "text": "But the nice thing about this\ngraph is that it tells you",
    "start": "2227670",
    "end": "2235000"
  },
  {
    "text": "very simply and visually which\ntransition probabilities are zero, and which transition\nprobabilities are non-zero.",
    "start": "2235000",
    "end": "2243810"
  },
  {
    "text": "And that's the thing that\nspecifies which states are recurrent, which states are\ntransient, and all of that.",
    "start": "2243810",
    "end": "2251650"
  },
  {
    "text": "All of that kind of elementary\nanalysis about a Markov chain all comes from looking at this\ngraph and seeing whether you",
    "start": "2251650",
    "end": "2260299"
  },
  {
    "text": "can get from one state to\nanother state by some process.",
    "start": "2260300",
    "end": "2266290"
  },
  {
    "text": "So let's move on from that. Talk about the classification\nof states.",
    "start": "2266290",
    "end": "2273620"
  },
  {
    "start": "2270000",
    "end": "2410000"
  },
  {
    "text": "We started out with the\nidea of a walk and a path and a cycle.",
    "start": "2273620",
    "end": "2279370"
  },
  {
    "text": "I'm not sure these terms are\nuniform throughout the field. But a walk is an ordered\nstring of nodes, like",
    "start": "2279370",
    "end": "2287550"
  },
  {
    "text": "i0, i1, up to i n. You can have repeated elements\nhere, but you need a directed",
    "start": "2287550",
    "end": "2294960"
  },
  {
    "text": "arc from i sub n minus\n1 to i sub m. Like for example, in this stupid\nMarkov chain here--",
    "start": "2294960",
    "end": "2303035"
  },
  {
    "text": " I mean, when you're drawing\nthings is LaTeX, it's kind of",
    "start": "2303035",
    "end": "2308880"
  },
  {
    "text": "hard to draw those nice\nlittle curves there. And because of that, when you\nonce draw a Markov chain, you",
    "start": "2308880",
    "end": "2314610"
  },
  {
    "text": "never want to change it. And that's why these nodes\nhave a very small set of Markov chains in them.",
    "start": "2314610",
    "end": "2320530"
  },
  {
    "text": "It's just to save me some work,\ndrawing and drawing",
    "start": "2320530",
    "end": "2326580"
  },
  {
    "text": "these diagrams.  An example of a walk, as you\nstart in 4, you take the self",
    "start": "2326580",
    "end": "2335700"
  },
  {
    "text": "loop, go back to 4 at time 2. Then you go to state\n1 at time 3.",
    "start": "2335700",
    "end": "2341660"
  },
  {
    "text": "Then you go to state\n2 at time 4. Then you go to stage\n3, time 5.",
    "start": "2341660",
    "end": "2348140"
  },
  {
    "text": "And back to state 2 at time 6. You have repeated nodes there.",
    "start": "2348140",
    "end": "2353300"
  },
  {
    "text": "You have repeated nodes\nseparated here. Another example of a\nwalk is 4, 1, 2, 3.",
    "start": "2353300",
    "end": "2360630"
  },
  {
    "text": "Example of a path, the path\ncan't have any repeated nodes. We'd like to look at paths,\nbecause if you're going to be",
    "start": "2360630",
    "end": "2367060"
  },
  {
    "text": "able to get from one node to\nanother node, and there's some walk that goes all around the\nplace and gets to that final",
    "start": "2367060",
    "end": "2373420"
  },
  {
    "text": "node, there's also path\nthat goes there. If you look at the walk, you\njust leave that all the cycles",
    "start": "2373420",
    "end": "2379900"
  },
  {
    "text": "along the way, and\nyou get to the n. And a cycle, of course, which I\ndidn't define, is something",
    "start": "2379900",
    "end": "2385980"
  },
  {
    "text": "which starts at one node, goes\nthrough a path, and then finally comes back to the same\nnode that it started at.",
    "start": "2385980",
    "end": "2392730"
  },
  {
    "text": "And it doesn't make any\ndifference for the cycle 2, 3, 2 whether you call it\n2, 3, 2 or 3, 2, 3.",
    "start": "2392730",
    "end": "2401610"
  },
  {
    "text": "That's the same cycle, and\nit's not even worth distinguishing between\nthose two ideas.",
    "start": "2401610",
    "end": "2407200"
  },
  {
    "text": "OK That's that.",
    "start": "2407200",
    "end": "2412723"
  },
  {
    "text": " If there's a path from--",
    "start": "2412723",
    "end": "2420010"
  },
  {
    "text": "where did I-- ",
    "start": "2420010",
    "end": "2426110"
  },
  {
    "text": "node j is accessible from i,\nwhich we abbreviate as i",
    "start": "2426110",
    "end": "2431800"
  },
  {
    "text": "has a path to j. If there's a walk from i to\nj, which means that p",
    "start": "2431800",
    "end": "2438010"
  },
  {
    "text": "sup i j to the n-- this is the transition\nprobability, the probability",
    "start": "2438010",
    "end": "2444150"
  },
  {
    "text": "that x sub n is equal to\nj, given that x sub",
    "start": "2444150",
    "end": "2449160"
  },
  {
    "text": "0 is equal to i. And we use this all the time. If this is greater than zero\nfor some n greater than 0.",
    "start": "2449160",
    "end": "2457369"
  },
  {
    "text": "In other words, j is accessible\nfrom i if there's a",
    "start": "2457370",
    "end": "2466950"
  },
  {
    "text": "path from i that goes to j. ",
    "start": "2466950",
    "end": "2472300"
  },
  {
    "text": "And trivially, if i go to j, and\nthere's a path from j to k, then there has to be\na path from i to k.",
    "start": "2472300",
    "end": "2481520"
  },
  {
    "text": "If you've ever tried to make up\na mapping program to find how to get from here to there,\nthis is one of the most useful",
    "start": "2481520",
    "end": "2488910"
  },
  {
    "text": "things you use. If there's a way to get here\nto there, and a way to get from here to there, then there's\na way to get from here",
    "start": "2488910",
    "end": "2495330"
  },
  {
    "text": "all the way to the end. And if you look up what most of\nthese map programs do, you",
    "start": "2495330",
    "end": "2502650"
  },
  {
    "text": "see that they overuse this\nenormously and they wind up taking you from here to there\nby some bizarre path just",
    "start": "2502650",
    "end": "2510910"
  },
  {
    "text": "because it happens to go through\nsome intermediate node on the way. So two nodes communicate--",
    "start": "2510910",
    "end": "2518680"
  },
  {
    "text": "i double arrow j-- if j is accessible from i, and\nif i is accessible from j.",
    "start": "2518680",
    "end": "2528859"
  },
  {
    "text": "That means there's a path from\ni to j, and another path from j back to i, if you shorten\nthem as much as you can.",
    "start": "2528860",
    "end": "2536260"
  },
  {
    "text": "There's a cycle. It starts at i, goes through j,\nand comes back to i again.",
    "start": "2536260",
    "end": "2543530"
  },
  {
    "text": "I didn't say that quite right,\nso delete that from what",
    "start": "2543530",
    "end": "2549810"
  },
  {
    "text": "you've just heard. A class C of states as a\nnon-empty set, such that i and",
    "start": "2549810",
    "end": "2555630"
  },
  {
    "text": "j communicate for each\ni j in this class. But i does not communicate\nwith j for each i and C--",
    "start": "2555630",
    "end": "2565330"
  },
  {
    "text": " for i and C and j, not in C.",
    "start": "2565330",
    "end": "2573210"
  },
  {
    "text": "The convenient way to think\nabout this-- and I should have stated this as a theorem in\nthe notes, because it's--",
    "start": "2573210",
    "end": "2579670"
  },
  {
    "text": " I think it's something that\nwe all use without even",
    "start": "2579670",
    "end": "2586130"
  },
  {
    "text": "thinking about it. It says that the entire set of\nstates, or the entire set of",
    "start": "2586130",
    "end": "2592480"
  },
  {
    "text": "nodes in a graph, is partitioned\ninto classes. The class C, containing, is i\nin union with all of the j's",
    "start": "2592480",
    "end": "2602860"
  },
  {
    "text": "that communicate with i. So if you want to find this\npartition, you start out with an arbitrary node, you find all\nof the other nodes that it",
    "start": "2602860",
    "end": "2611280"
  },
  {
    "text": "communicates with, and you\nfind them by picking them one at a time.",
    "start": "2611280",
    "end": "2616320"
  },
  {
    "text": "You pick all of the nodes\nfor which p sub i j is greater than 0.",
    "start": "2616320",
    "end": "2622540"
  },
  {
    "text": "Then you pick-- and p sub j i is great-- well-- blah.",
    "start": "2622540",
    "end": "2627780"
  },
  {
    "text": " If you want to find the set of\nnodes that are accessible from",
    "start": "2627780",
    "end": "2635400"
  },
  {
    "text": "i, you start out looking at i. You look at all the states\nwhich are accessible",
    "start": "2635400",
    "end": "2640640"
  },
  {
    "text": "from i in one step. Then you look at all the steps,\nall of the states,",
    "start": "2640640",
    "end": "2646870"
  },
  {
    "text": "which you can access from\nany one of those. Those are the states which are\naccessible in two states--",
    "start": "2646870",
    "end": "2652720"
  },
  {
    "text": "in two steps, then in three\nsteps, and so forth. So you find all the nodes that\nare accessible from node i.",
    "start": "2652720",
    "end": "2661380"
  },
  {
    "text": "And then you turn around and\ndo it the other way. And presto, you have all of\nthese classes of states all",
    "start": "2661380",
    "end": "2669599"
  },
  {
    "text": "very simply. For finite-state change, the\nstate i is transient if",
    "start": "2669600",
    "end": "2674990"
  },
  {
    "text": "there's a j in S such that\ni goes into j, but j",
    "start": "2674990",
    "end": "2680200"
  },
  {
    "text": "does not go into i. In other words, if I'm a state\ni, and I can get to you, but",
    "start": "2680200",
    "end": "2686900"
  },
  {
    "text": "you can't get back to me,\nthen I'm transient.",
    "start": "2686900",
    "end": "2695450"
  },
  {
    "text": "Because the way Markov chains\nwork, we keep going from one",
    "start": "2695450",
    "end": "2701599"
  },
  {
    "text": "step to the next step to the\nnext step to the next step. And if I keep returning to\nmyself, then eventually I'm",
    "start": "2701600",
    "end": "2709710"
  },
  {
    "text": "going to go to you. And once I go to you, I'll\nnever get back again. So because of that, these\ntransient states are states",
    "start": "2709710",
    "end": "2718540"
  },
  {
    "text": "where eventually you\nleave them and you never get back again. As soon as we start talking\nabout countable state Markov",
    "start": "2718540",
    "end": "2726190"
  },
  {
    "text": "chains, you'll see that\nthis definition doesn't work anymore. You can--",
    "start": "2726190",
    "end": "2732620"
  },
  {
    "text": "it is very possible to just\nwander away in a countable state Markov chain, and you\nnever get back again that way.",
    "start": "2732620",
    "end": "2740390"
  },
  {
    "text": "After you wander away too far,\nthe probability of getting back gets smaller and smaller.",
    "start": "2740390",
    "end": "2745539"
  },
  {
    "text": "You keep getting further\nand further away. The probability of returning\ngets smaller and smaller, so",
    "start": "2745540",
    "end": "2752810"
  },
  {
    "text": "that you have transience\nthat way also. But here, the situation is\nsimpler for a finite-state",
    "start": "2752810",
    "end": "2759470"
  },
  {
    "text": "Markov chain. And you can define transience if\nthere's a j in S such that",
    "start": "2759470",
    "end": "2765570"
  },
  {
    "text": "i goes into j, but j\ndoesn't go into i. If i's not transient,\nthen it's recurrent.",
    "start": "2765570",
    "end": "2773160"
  },
  {
    "text": "Usually you define recurrence\nfirst and transience later, but it's a little simpler\nthis way.",
    "start": "2773160",
    "end": "2779470"
  },
  {
    "text": "All states in a class are\ntransient, or all are recurrent, and a finite-state\nMarkov chain contains at least",
    "start": "2779470",
    "end": "2786330"
  },
  {
    "text": "one recurrent class. You did that in your homework. And you were surprised at how\ncomplicated it was to do it.",
    "start": "2786330",
    "end": "2793040"
  },
  {
    "text": "I hope that after you wrote\ndown a proof of this, you stopped and thought about what\nyou were actually proving,",
    "start": "2793040",
    "end": "2801800"
  },
  {
    "text": "which intuitively is something\nvery, very simple. It's just looking at all of\nthe transient classes.",
    "start": "2801800",
    "end": "2808960"
  },
  {
    "text": "Starting at one transient\nclass, you find if there's another--",
    "start": "2808960",
    "end": "2814950"
  },
  {
    "text": "if there's another state you can\nget to from OK i which is also transient, and then you\nfind if there's another state",
    "start": "2814950",
    "end": "2822170"
  },
  {
    "text": "you get to from there which\nis also transient. And eventually, you have to come\nto a state from which you",
    "start": "2822170",
    "end": "2828500"
  },
  {
    "text": "can't go to some other state,\nfrom which you can't get back. ",
    "start": "2828500",
    "end": "2837349"
  },
  {
    "text": "That was explaining it almost\nas badly as the problem statement explained it. And I hope that after you did\nthe problem, even if you can't",
    "start": "2837350",
    "end": "2845460"
  },
  {
    "text": "explain it to someone,\nyou have an understanding of why it's true. It shouldn't be surprising\nafter you do that.",
    "start": "2845460",
    "end": "2854920"
  },
  {
    "text": "So the finite-state Markov chain\ncontains at least one recurrent class.",
    "start": "2854920",
    "end": "2860200"
  },
  {
    "text": " OK, the period of a state\ni as the greatest common",
    "start": "2860200",
    "end": "2866720"
  },
  {
    "start": "2861000",
    "end": "3599000"
  },
  {
    "text": "denominator of n, such that\np i n is greater than 0.",
    "start": "2866720",
    "end": "2871730"
  },
  {
    "text": "Again, a very complicated\ndefinition for a simple kind of idea. Namely, you start out\nin a state i.",
    "start": "2871730",
    "end": "2878670"
  },
  {
    "text": "You look at all of the times at\nwhich you can get back to state i again.",
    "start": "2878670",
    "end": "2883940"
  },
  {
    "text": "If you find it that set of\ntimes has a period in it, namely, if every sequences of\nstates is a multiple of some",
    "start": "2883940",
    "end": "2899550"
  },
  {
    "text": "d, then you know that the state\nis periodic if d is",
    "start": "2899550",
    "end": "2905410"
  },
  {
    "text": "greater than 1. And what you have to do is to\nfind the largest such number. And that's the period\nof the state.",
    "start": "2905410",
    "end": "2912040"
  },
  {
    "text": "All states in the same class\nhave the same period. A recurring class with period\nd greater than one can be",
    "start": "2912040",
    "end": "2918690"
  },
  {
    "text": "partitioned into sub-class-- this is the best way\nof looking at periodic classes of states.",
    "start": "2918690",
    "end": "2925819"
  },
  {
    "text": "If you have a periodic class of\nstates, then you can always separate it into\nd sub-classes.",
    "start": "2925820",
    "end": "2933960"
  },
  {
    "text": "And in such a set of\nsub-classes, transitions from",
    "start": "2933960",
    "end": "2939300"
  },
  {
    "text": "S1 and the states in\nS1 only go to S2. Transitions from states\nin S2 only go to S3.",
    "start": "2939300",
    "end": "2947710"
  },
  {
    "text": "Up to, transitions from S\nd only go back to S1. They have to go someplace,\nso they go back to S1.",
    "start": "2947710",
    "end": "2956050"
  },
  {
    "text": "So as you cycle around, it takes\nd steps to cycle from 1",
    "start": "2956050",
    "end": "2962500"
  },
  {
    "text": "back to 1 again. It takes d steps to cycle\nfrom 2 back to 2 again.",
    "start": "2962500",
    "end": "2968410"
  },
  {
    "text": "So you can see the structure of\nthe Markov chain and why, in fact, it does have to be--",
    "start": "2968410",
    "end": "2974810"
  },
  {
    "text": "why that class has\nto be periodic. An ergodic class is a recurrent\naperiodic class.",
    "start": "2974810",
    "end": "2981869"
  },
  {
    "text": "In other words, it's a class\nwhere the period is equal to 1, which means there really\nisn't any period.",
    "start": "2981870",
    "end": "2988450"
  },
  {
    "text": "A Markov chain with only one\nclass is ergodic if the class is ergodic.",
    "start": "2988450",
    "end": "2994640"
  },
  {
    "text": "And the big theorem here-- I mean, this is probably the\nmost important theorem about",
    "start": "2994640",
    "end": "2999670"
  },
  {
    "text": "finite-state Markov chains. You have an ergodic,\nfinite-state Markov chain.",
    "start": "2999670",
    "end": "3005100"
  },
  {
    "text": "Then the limit as n goes to\ninfinity of the probability of",
    "start": "3005100",
    "end": "3012300"
  },
  {
    "text": "arriving in state j after n\nsteps, given that you started in state i, is just some\nfunction of j.",
    "start": "3012300",
    "end": "3020780"
  },
  {
    "text": "In other words, when n gets very\nlarge, it doesn't depend on how large M is.",
    "start": "3020780",
    "end": "3027369"
  },
  {
    "text": "It stays the same. It becomes independent of n. It doesn't depend on\nwhere you started.",
    "start": "3027370",
    "end": "3032450"
  },
  {
    "text": "No matter where you start\nin a finite-state ergodic Markov chain. After a very long time, the\nprobability of being in a",
    "start": "3032450",
    "end": "3040580"
  },
  {
    "text": "state j is independent of where\nyou started, and it's independent of how long\nyou've been running.",
    "start": "3040580",
    "end": "3048170"
  },
  {
    "text": "So that's a very strong\nkind of-- it's a very strong kind\nof limit theorem.",
    "start": "3048170",
    "end": "3054890"
  },
  {
    "text": "It's very much like the law of\nlarge numbers and all of these other things.",
    "start": "3054890",
    "end": "3060030"
  },
  {
    "text": "I'm going to talk a little bit\nat the end about what that relationship really is. ",
    "start": "3060030",
    "end": "3067360"
  },
  {
    "text": "Except what it says is, after a\nlong time, you're in steady state, which is why\nit's called the",
    "start": "3067360",
    "end": "3072670"
  },
  {
    "text": "steady state theorem. Yes? AUDIENCE: Could you define the\nsteady states for periodic changes [INAUDIBLE]?",
    "start": "3072670",
    "end": "3078636"
  },
  {
    "text": " PROFESSOR: I try to avoid doing\nthat because you have",
    "start": "3078636",
    "end": "3086460"
  },
  {
    "text": "steady state probabilities. The steady state probabilities\nthat you have are, you take--",
    "start": "3086460",
    "end": "3091809"
  },
  {
    "text": " is if you have these\nsub-classes.",
    "start": "3091810",
    "end": "3098760"
  },
  {
    "text": "Then you wind up with a steady\nstate within each sub-class. If you assign a probability\nof the probability in the",
    "start": "3098760",
    "end": "3106900"
  },
  {
    "text": "sub-class, divided by d, then\nyou get what is the steady state probability.",
    "start": "3106900",
    "end": "3112930"
  },
  {
    "text": "If you start out in that steady\nstate, then you're in each sub-class with probability\n1 over d.",
    "start": "3112930",
    "end": "3120130"
  },
  {
    "text": "And you shift to the next\nsub-class and you're still in steady state, because you have\na probability, 1 over d, of",
    "start": "3120130",
    "end": "3128340"
  },
  {
    "text": "being in each of those\nsub-classes to start with. You shift and you're still in\none of the sub-classes with",
    "start": "3128340",
    "end": "3136970"
  },
  {
    "text": "probability 1 over d. So there still is a steady\nstate in that sense, but",
    "start": "3136970",
    "end": "3142690"
  },
  {
    "text": "there's not a steady state\nin any nice sense. ",
    "start": "3142690",
    "end": "3151940"
  },
  {
    "text": "So anyway, that's\nthe way it is.",
    "start": "3151940",
    "end": "3159470"
  },
  {
    "text": "But you see, if you understand\nthis theorem for ergodic",
    "start": "3159470",
    "end": "3164859"
  },
  {
    "text": "finite state and Markov\nchains, and then you understand about periodic\nchange and this set of",
    "start": "3164860",
    "end": "3172540"
  },
  {
    "text": "sub-classes, you can\nsee within each sub-class, if you look at--",
    "start": "3172540",
    "end": "3179450"
  },
  {
    "text": "if you look at--  if you look at time 0, time d,\ntime 2d, times 3d and 4d, then",
    "start": "3179450",
    "end": "3191500"
  },
  {
    "text": "whatever state you start in,\nyou're going to be in the same class after d steps, the same\nclass after 2d steps.",
    "start": "3191500",
    "end": "3199380"
  },
  {
    "text": "You're going to have\na transition matrix over d steps. And this theorem still applies\nto these sub-classes over",
    "start": "3199380",
    "end": "3207360"
  },
  {
    "text": "periods of d. So the hard part of it\nis proving this. After you prove this, then you\nsee that the same thing",
    "start": "3207360",
    "end": "3215180"
  },
  {
    "text": "happens over each sub-class\nafter that. ",
    "start": "3215180",
    "end": "3223650"
  },
  {
    "text": "That's a pretty major theorem. It's difficult to prove. A sub-step is to show that for\nan ergodic M state Markov",
    "start": "3223650",
    "end": "3230890"
  },
  {
    "text": "chain, the probability of being\nin state j at time n,",
    "start": "3230890",
    "end": "3236380"
  },
  {
    "text": "given that you're in state i at\ntime 0, is positive for all i j, and all n greater than\nM minus 1 squared plus 1.",
    "start": "3236380",
    "end": "3245869"
  },
  {
    "text": "It's very surprising that you\nhave to go this many states--",
    "start": "3245870",
    "end": "3250900"
  },
  {
    "text": "this many steps before you get\nto the point that all these transition probabilities\nare positive.",
    "start": "3250900",
    "end": "3258440"
  },
  {
    "text": "You look at this particular kind\nof Markov chain in the homework, and I hope what you\nfound out from it was that if",
    "start": "3258440",
    "end": "3266660"
  },
  {
    "text": "you start, say, in state two,\nthen at the next time, you",
    "start": "3266660",
    "end": "3272039"
  },
  {
    "text": "have to be in 3. Next time, you have to be in\n4, you have to be in 5, you have to be in 6.",
    "start": "3272040",
    "end": "3278560"
  },
  {
    "text": "In other words, the size of\nthe set that you can be in after one step is just 1.",
    "start": "3278560",
    "end": "3286550"
  },
  {
    "text": "One possible state here, one\npossible state here, one possible state here.",
    "start": "3286550",
    "end": "3292640"
  },
  {
    "text": "The next step, you're in either\n1 or 2, and as you travel around, the size of the\nset of states you can be in at",
    "start": "3292640",
    "end": "3301600"
  },
  {
    "text": "these different steps, is 2,\nuntil you get all the way around again.",
    "start": "3301600",
    "end": "3307510"
  },
  {
    "text": "And then there's\na way to get-- when you get to state 6 again,\nthe set of states enlarges.",
    "start": "3307510",
    "end": "3315050"
  },
  {
    "text": "So finally you get up to a\nset of states, which is up to M minus 1.",
    "start": "3315050",
    "end": "3320800"
  },
  {
    "text": "And that's why you get the M\nminus 1 squared here, plus 1. And this is the only Markov\nchain there is.",
    "start": "3320800",
    "end": "3328710"
  },
  {
    "text": "You can have as many\nstates going around here as you want to.",
    "start": "3328710",
    "end": "3333770"
  },
  {
    "text": "But you have to have this\nstructure at the end, where there's one special state and\none way of circumventing it,",
    "start": "3333770",
    "end": "3339930"
  },
  {
    "text": "which means there's one cycle\nof size M minus 1, and one cycle of size M. And that's the\nonly way you can get it.",
    "start": "3339930",
    "end": "3348440"
  },
  {
    "text": "And that's the only Markov chain\nthat meets this bound with equality.",
    "start": "3348440",
    "end": "3353640"
  },
  {
    "text": "In all other cases, you get this\nproperty much earlier.",
    "start": "3353640",
    "end": "3361470"
  },
  {
    "text": "And often, you get it after just\na linear amount of time. ",
    "start": "3361470",
    "end": "3369359"
  },
  {
    "text": "The other part of this major\ntheorem that you reach steady state says, let P be\ngreater than 0.",
    "start": "3369360",
    "end": "3377350"
  },
  {
    "text": "In other words, let\nall the transition probabilities be positive.",
    "start": "3377350",
    "end": "3382410"
  },
  {
    "text": "And then define some quantity\nalpha as a minimum of the",
    "start": "3382410",
    "end": "3388039"
  },
  {
    "text": "transition probabilities. And then the theorem says, for\nall states j and all n greater",
    "start": "3388040",
    "end": "3394109"
  },
  {
    "text": "than or equal to 1, the maximum\nover the initial states minus the minimum over\nthe initial states of P sub i",
    "start": "3394110",
    "end": "3403180"
  },
  {
    "text": "j to the n plus-- first step,\nthat difference is less than",
    "start": "3403180",
    "end": "3409040"
  },
  {
    "text": "or equal to the difference\na the n-th step, times 1 minus 2 alpha.",
    "start": "3409040",
    "end": "3414300"
  },
  {
    "text": "Now 1 minus 2 alpha is\nas a positive number. And this says that this maximum\nminus minimum is 1",
    "start": "3414300",
    "end": "3423700"
  },
  {
    "text": "minus 2 alpha to the n, which\nsays that the limit of the maximizing term is equal\nto the limit of",
    "start": "3423700",
    "end": "3431220"
  },
  {
    "text": "the minimizing term. And what does that say? It says that everything in the\nmiddle gets squeezed together.",
    "start": "3431220",
    "end": "3438740"
  },
  {
    "text": "And it says exactly what we want\nit to say, that the limit",
    "start": "3438740",
    "end": "3444200"
  },
  {
    "text": "of P sub l j to the n is\nindependent of l, after n gets",
    "start": "3444200",
    "end": "3450380"
  },
  {
    "text": "very large. Because the maximum and\nthe minimum get very close to each other.",
    "start": "3450380",
    "end": "3457560"
  },
  {
    "text": "We also showed that [? our ?]\napproaches that limit exponentially. That's what this says.",
    "start": "3457560",
    "end": "3463640"
  },
  {
    "text": "The exponent here is just this\nalpha, determined in that way.",
    "start": "3463640",
    "end": "3469859"
  },
  {
    "text": "And the theorem for ergodic\nMarkov chains then follows by just looking at successive h\nsteps in the Markov chain when",
    "start": "3469860",
    "end": "3481380"
  },
  {
    "text": "h is large enough so that all\nthese transition probabilities are positive.",
    "start": "3481380",
    "end": "3487360"
  },
  {
    "text": " So you go out far enough\nthat all the transition probabilities are positive.",
    "start": "3487360",
    "end": "3493859"
  },
  {
    "text": "And then you look at repetitions\nof that, and apply this theorem. And suddenly you have this\ngeneral theorem,",
    "start": "3493860",
    "end": "3501570"
  },
  {
    "text": "which is what we wanted. ",
    "start": "3501570",
    "end": "3507200"
  },
  {
    "text": "An ergodic unichain is a Markov\nup chain with one ergodic recurring class,\nplus perhaps a set",
    "start": "3507200",
    "end": "3513870"
  },
  {
    "text": "of transient states. And most of the things we talk\nabout in this course are for",
    "start": "3513870",
    "end": "3519600"
  },
  {
    "text": "unichains, usually ergodic\nunichains, because if you have",
    "start": "3519600",
    "end": "3525870"
  },
  {
    "text": "multiple recurrent classes,\nit just makes a mess. You wind up in this recurrent\nclass, or",
    "start": "3525870",
    "end": "3531780"
  },
  {
    "text": "this recurrent class. And aside from the question of\nwhich one you get to, you",
    "start": "3531780",
    "end": "3540079"
  },
  {
    "text": "don't much care about it. And the theorem here is for an\nergodic finite-state unichain.",
    "start": "3540080",
    "end": "3545790"
  },
  {
    "text": "The limit of P sub i j to the\nn probability of being in state j at time n, given that\nyou're in state i at time 0,",
    "start": "3545790",
    "end": "3555130"
  },
  {
    "text": "is equal to pi sub j. In other words, this limit\nhere exists for all i j.",
    "start": "3555130",
    "end": "3562330"
  },
  {
    "text": "And the limit is independent\nof i. And it's independent of n\nas n gets big enough.",
    "start": "3562330",
    "end": "3567900"
  },
  {
    "text": " And then also, we can choose\nthis so that this set of",
    "start": "3567900",
    "end": "3582970"
  },
  {
    "text": "probabilities here satisfies\nthis, what's called the steady state condition, the sum\nof pi i times P sub i j",
    "start": "3582970",
    "end": "3591780"
  },
  {
    "text": "is equal to pi j. In other words, if you start out\nin steady state, and you look at the probabilities of\nbeing in the different states",
    "start": "3591780",
    "end": "3600299"
  },
  {
    "text": "at the next time unit, this is\nthe probability of being in",
    "start": "3600300",
    "end": "3606610"
  },
  {
    "text": "state j at time n plus 1, if\nthis is the probability of being in state i at time n.",
    "start": "3606610",
    "end": "3614420"
  },
  {
    "text": "So that condition\ngets satisfied. That condition is satisfied. You just stay in steady\nstate forever.",
    "start": "3614420",
    "end": "3622760"
  },
  {
    "text": "And pi i has to be positive for\na recurrent i, and pi i is",
    "start": "3622760",
    "end": "3629210"
  },
  {
    "text": "equal to 0 otherwise. So this is just a\ngeneralization",
    "start": "3629210",
    "end": "3635230"
  },
  {
    "text": "of the ergodic theorem. And this is not what people\nrefer to as the ergodic",
    "start": "3635230",
    "end": "3643400"
  },
  {
    "text": "theorem, which is a much more\ngeneral theorem than this. This is the ergodic theorem for\nthe case of finite state",
    "start": "3643400",
    "end": "3650900"
  },
  {
    "text": "Markov chains. You can restate this in matrix\nform as the limit of the",
    "start": "3650900",
    "end": "3659190"
  },
  {
    "text": "matrix P to the n-th power. What I didn't mention here and\nwhat I probably didn't mention",
    "start": "3659190",
    "end": "3666680"
  },
  {
    "text": "enough in the notes is\nthat P sub i j--",
    "start": "3666680",
    "end": "3671880"
  },
  {
    "start": "3671880",
    "end": "3692359"
  },
  {
    "text": "but also, if you take the matrix\nP times P time P, n",
    "start": "3692360",
    "end": "3707560"
  },
  {
    "text": "times, namely, you take the\nmatrix, P to the n.",
    "start": "3707560",
    "end": "3713880"
  },
  {
    "text": "This says the P sub i j\nis the i j element.",
    "start": "3713880",
    "end": "3720720"
  },
  {
    "start": "3720720",
    "end": "3729900"
  },
  {
    "text": "I'm sure all of you know that\nby now, because you've been using it all the time.",
    "start": "3729900",
    "end": "3735310"
  },
  {
    "text": "And what this says here-- what we've said before is that\nevery row of this matrix, P to",
    "start": "3735310",
    "end": "3746150"
  },
  {
    "text": "the n, is the same. Every row is equal to pi.",
    "start": "3746150",
    "end": "3751290"
  },
  {
    "text": "P to the n tends to a matrix\nwhich is pi 1, pi 2,",
    "start": "3751290",
    "end": "3767786"
  },
  {
    "text": "up to pi sub n. Pi 1, pi 2, up to pi sub n.",
    "start": "3767786",
    "end": "3777000"
  },
  {
    "text": " Pi 1, pi 2, up to pi sub n.",
    "start": "3777000",
    "end": "3786770"
  },
  {
    "text": "And the easiest way to express\nthis is the vector e times pi,",
    "start": "3786770",
    "end": "3794660"
  },
  {
    "text": "where e is transposed.",
    "start": "3794660",
    "end": "3804960"
  },
  {
    "text": "In other words, if you take a\ncolumn matrix, column 1, 1, 1,",
    "start": "3804960",
    "end": "3812755"
  },
  {
    "text": "1, 1, and you multiply this by\na row vector, pi 1 times pi",
    "start": "3812755",
    "end": "3820670"
  },
  {
    "text": "sub n, what you get is, for this\nfirst row multiplied by",
    "start": "3820670",
    "end": "3828030"
  },
  {
    "text": "this, this gives you-- well, in fact, if you\nmultiply this out,",
    "start": "3828030",
    "end": "3833480"
  },
  {
    "text": "this is what you get. And if you've never gone through\nthe trouble of seeing",
    "start": "3833480",
    "end": "3838650"
  },
  {
    "text": "that this multiplication leads\nto this, please do it, because",
    "start": "3838650",
    "end": "3843880"
  },
  {
    "text": "it's important to notice\nthat correspondence. ",
    "start": "3843880",
    "end": "3854530"
  },
  {
    "text": "We got specific results by\nlooking at the eigenvalues and eigenvectors of stochastic\nmatrices.",
    "start": "3854530",
    "end": "3860880"
  },
  {
    "text": "And a stochastic matrix is the\nmatrix of a Markov chain. ",
    "start": "3860880",
    "end": "3868500"
  },
  {
    "text": "So some of these things\nare sort of obvious. Lambda is an eigenvalue of P, if\nand only if P minus lambda",
    "start": "3868500",
    "end": "3876870"
  },
  {
    "text": "i is singular.  This set of relationships\nis not obvious.",
    "start": "3876870",
    "end": "3885040"
  },
  {
    "text": "This is obvious linear\nalgebra. This is something that when\nyou study eigenvalues and",
    "start": "3885040",
    "end": "3891250"
  },
  {
    "text": "eigenvectors in linear algebra,\nyou recognize that this is a summary of\na lot of things.",
    "start": "3891250",
    "end": "3897270"
  },
  {
    "text": "If and only if this determinant\nis equal to 0, which is true if and only if\nthere's some vector nu for",
    "start": "3897270",
    "end": "3905650"
  },
  {
    "text": "which P times nu equals lambda\ntimes nu for nu unequal to 0.",
    "start": "3905650",
    "end": "3912559"
  },
  {
    "text": "And if and only if pi P equals\nlambda pi for some pi unequal to 0.",
    "start": "3912560",
    "end": "3918210"
  },
  {
    "text": "In other words, if this\ndeterminant is equal to 0, it",
    "start": "3918210",
    "end": "3923250"
  },
  {
    "text": "means that the matrix P minus\nlambda i is singular.",
    "start": "3923250",
    "end": "3932040"
  },
  {
    "text": "If the matrix is singular, there\nhas to be some solution to this equation here.",
    "start": "3932040",
    "end": "3938369"
  },
  {
    "text": "There has to be some\nsolution to this left eigenvector equation.",
    "start": "3938370",
    "end": "3944530"
  },
  {
    "text": "Now, once you see this, you\nnotice that e is always a right eigenvector of P. Every\nstochastic matrix in the world",
    "start": "3944530",
    "end": "3953750"
  },
  {
    "text": "has the property that e is a\nright eigenvector of it.",
    "start": "3953750",
    "end": "3958920"
  },
  {
    "text": "Why is that? Because all of the rows of a\nstochastic matrix sum to 1.",
    "start": "3958920",
    "end": "3965230"
  },
  {
    "text": "If you start off in state i, the\nsum of the possible states you can be at in the next\nstep is equal to 1.",
    "start": "3965230",
    "end": "3974530"
  },
  {
    "text": "You have to go somewhere. So e is always a right\neigenvector of P with",
    "start": "3974530",
    "end": "3981650"
  },
  {
    "text": "eigenvalue 1. Since e is also is a right\neigenvector of P with eigenvalue 1, we go up here.",
    "start": "3981650",
    "end": "3989850"
  },
  {
    "text": "We look at these if and\nonly if statements. We see, then, P must\nbe singular.",
    "start": "3989850",
    "end": "3994890"
  },
  {
    "text": "And then pi times P\nequals lambda pi. So no matter how many recurrent\nclasses we have, no",
    "start": "3994890",
    "end": "4001410"
  },
  {
    "text": "matter what periodicity we have\nin each of them, there's",
    "start": "4001410",
    "end": "4006430"
  },
  {
    "text": "always a solution to pi\ntimes P equals pi.",
    "start": "4006430",
    "end": "4013170"
  },
  {
    "text": "There's always at least one\nsteady state vector. ",
    "start": "4013170",
    "end": "4019320"
  },
  {
    "text": "This determinant has an M-th\ndegree polynomial in lambda. M-th degree polynomials\nhave M roots.",
    "start": "4019320",
    "end": "4028150"
  },
  {
    "text": "They aren't necessarily\ndistinct. The multiplicity of an\neigenvalue is the number roots",
    "start": "4028150",
    "end": "4034039"
  },
  {
    "text": "of that value. And the multiplicity\nof lambda equals 1.",
    "start": "4034040",
    "end": "4039780"
  },
  {
    "text": "How many different roots\nare there which have lambda equals 1? Well it turns out to be just\nthe number of recurrent",
    "start": "4039780",
    "end": "4046940"
  },
  {
    "text": "classes that you have. If you have a bunch of recurrent\nclasses, within each",
    "start": "4046940",
    "end": "4052750"
  },
  {
    "text": "recurring class, there's a\nsolution to pi P equals pi, which is non-zero only one\nthat recurrent class.",
    "start": "4052750",
    "end": "4061540"
  },
  {
    "text": "Namely, you take this huge\nMarkov chain and you say, I don't care about any\nof this except this",
    "start": "4061540",
    "end": "4068650"
  },
  {
    "text": "one recurrent class. If we look at this one recurrent\nclass, and solve for",
    "start": "4068650",
    "end": "4073990"
  },
  {
    "text": "the steady state probability in\nthat one recurrent class, then we get an eigenvector\nwhich is non-zero on that",
    "start": "4073990",
    "end": "4081220"
  },
  {
    "text": "class, 0 everywhere else, that\nhas an eigenvalue 1. And for every other recurrent\nclass, we",
    "start": "4081220",
    "end": "4088050"
  },
  {
    "text": "get the same situation. So the multiplicity of lambda\nequals 1 is equal to the",
    "start": "4088050",
    "end": "4094150"
  },
  {
    "text": "number of recurrent classes. If you didn't get that proof\non the fly, it gets",
    "start": "4094150",
    "end": "4101949"
  },
  {
    "text": "proved in the notes. And if you don't get the proof,\njust remember that",
    "start": "4101950",
    "end": "4107130"
  },
  {
    "text": "that's the way it is.  For the special case where all\nM eigenvalues are distinct,",
    "start": "4107130",
    "end": "4114859"
  },
  {
    "text": "the right eigenvectors are\nlinearly independent. You remember that proof we went\nthrough that all of the",
    "start": "4114859",
    "end": "4122620"
  },
  {
    "text": "left eigenvectors and all the\nright eigenvectors are all orthonormal to each other,\nor you can make them all",
    "start": "4122620",
    "end": "4129870"
  },
  {
    "text": "orthonormal to each other? That says that if the right\neigenvectors are linearly",
    "start": "4129870",
    "end": "4137380"
  },
  {
    "text": "independent, you can represent\nthem as the columns of an invertible matrix U.\nThen P times U is",
    "start": "4137380",
    "end": "4144750"
  },
  {
    "text": "equal to U times lambda. What does this equations say?",
    "start": "4144750",
    "end": "4149799"
  },
  {
    "text": "You split it up into a\nbunch of equations. ",
    "start": "4149800",
    "end": "4156500"
  },
  {
    "text": "P times U and we look at it as\nnu 1, nu 2, nu sub [? n ?].",
    "start": "4156500",
    "end": "4186080"
  },
  {
    "text": "I guess better put the\nsuperscripts on it.",
    "start": "4186080",
    "end": "4192580"
  },
  {
    "text": " If I take the matrix U and just\nview it as M different",
    "start": "4192580",
    "end": "4201270"
  },
  {
    "text": "columns, then what this\nis saying is that this is equal to--",
    "start": "4201270",
    "end": "4206545"
  },
  {
    "start": "4206545",
    "end": "4217290"
  },
  {
    "text": "nu 1, nu 2, nu M, times lambda\n1, lambda 2, up to lambda M.",
    "start": "4217290",
    "end": "4235540"
  },
  {
    "text": "Now you multiply this out,\nand what do you get? You get nu 1 times lambda 1.",
    "start": "4235540",
    "end": "4241860"
  },
  {
    "text": "You get a nu 2 times lambda 2\nfor the second column, nu M times lambda M for the last\ncolumn, and here you get P",
    "start": "4241860",
    "end": "4249820"
  },
  {
    "text": "times nu 1 is equal to a nu 1\ntimes lambda 1, and so forth. So all this vector equation says\nis the same thing that",
    "start": "4249820",
    "end": "4259239"
  },
  {
    "text": "these n M individual eigenvector\nequations say.",
    "start": "4259240",
    "end": "4264760"
  },
  {
    "text": "It's just a more compact way\nof saying the same thing.",
    "start": "4264760",
    "end": "4271159"
  },
  {
    "text": "And if these eigenvectors span\nthis space, then this set of",
    "start": "4271160",
    "end": "4277300"
  },
  {
    "text": "eigenvectors are linearly\nindependent of each other. And when you look at the set of\nthem, this matrix here has",
    "start": "4277300",
    "end": "4284860"
  },
  {
    "text": "to have an inverse. So you can also express this\nas P equals this vector--",
    "start": "4284860",
    "end": "4294890"
  },
  {
    "text": "this matrix of right\neigenvectors times the",
    "start": "4294890",
    "end": "4300820"
  },
  {
    "text": "diagonal matrix lambda, times\nthe inverse of this matrix.",
    "start": "4300820",
    "end": "4306630"
  },
  {
    "text": "Matrix U to the minus 1 turns\nout to have rows equal to the left eigenvectors.",
    "start": "4306630",
    "end": "4311730"
  },
  {
    "text": "That's because these\neigenvectors-- that's because the right\neigenvectors and the left",
    "start": "4311730",
    "end": "4317440"
  },
  {
    "text": "eigenvectors are orthogonal\nto each other. ",
    "start": "4317440",
    "end": "4324670"
  },
  {
    "text": "When we then split up this\nmatrix into a sum of M",
    "start": "4324670",
    "end": "4329690"
  },
  {
    "text": "different matrices, each matrix\nhaving only one-- ",
    "start": "4329690",
    "end": "4361270"
  },
  {
    "text": "and so forth. Then what you get-- here's this--",
    "start": "4361270",
    "end": "4368489"
  },
  {
    "text": "this nice equation here, which\nsays that if all the",
    "start": "4368490",
    "end": "4374730"
  },
  {
    "text": "eigenvalues are distinct, then\nyou can always represent a stochastic matrix as the sum of\nlambda i times nu to the i",
    "start": "4374730",
    "end": "4383420"
  },
  {
    "text": "times pi to the i. More importantly, if you take\nthis equation here and look at",
    "start": "4383420",
    "end": "4390000"
  },
  {
    "text": "P to the n, P to the n is U\ntimes lambda times U to the minus 1, times U times lambda\ntimes U to the minus 1, blah,",
    "start": "4390000",
    "end": "4398820"
  },
  {
    "text": "blah, blah forever. Each U to the minus 1 cancels\nout with the following U. And",
    "start": "4398820",
    "end": "4404030"
  },
  {
    "text": "you wind up with P to the n\nequals U times lambda to the",
    "start": "4404030",
    "end": "4409329"
  },
  {
    "text": "n, U to the minus 1. Which says that P to the\nn is just a sum here.",
    "start": "4409330",
    "end": "4420250"
  },
  {
    "text": "It's the sum of the eigenvalues\nto the n-th power times these pairs of\neigenvectors here.",
    "start": "4420250",
    "end": "4427320"
  },
  {
    "text": "So this is a general\ndecomposition for P to the n. What we're interested in is what\nhappens as n gets large.",
    "start": "4427320",
    "end": "4436010"
  },
  {
    "text": "If we have a unit chain, we\nalready know what happens as n gets large. We know that as n gets large,\nwe wind up with just 1 times",
    "start": "4436010",
    "end": "4447110"
  },
  {
    "text": "this eigenvector e times\nthis eigenvector pi.",
    "start": "4447110",
    "end": "4452480"
  },
  {
    "text": "Which says that all of the other\neigenvalues have to go to 0, which says that the\nmagnitude of these other",
    "start": "4452480",
    "end": "4459670"
  },
  {
    "text": "eigenvalues are less than 1. So they're all going away. ",
    "start": "4459670",
    "end": "4466600"
  },
  {
    "text": "So the facts here are that all\neigenvalues lambda have to",
    "start": "4466600",
    "end": "4472300"
  },
  {
    "text": "satisfy the magnitude\nof lambda is less than or equal to 1. That's what I just argued.",
    "start": "4472300",
    "end": "4479680"
  },
  {
    "text": "For each recurrent class C,\nthere's one lambda equals 1, with a left side and vector\nequals the steady state on",
    "start": "4479680",
    "end": "4487750"
  },
  {
    "text": "that recurrent class\nand 0 elsewhere. The right eigenvector nu\nsatisfies the limit as n goes",
    "start": "4487750",
    "end": "4495230"
  },
  {
    "text": "to infinity. So the probability that x sub n\nis in this recurring class,",
    "start": "4495230",
    "end": "4500930"
  },
  {
    "text": "given that x sub 0 is equal\nto 0, is equal to the i-th component of that right\neigenvector.",
    "start": "4500930",
    "end": "4508700"
  },
  {
    "text": "In other words, if you have a\nMarkov chain which has several recurrent classes, and you\nwant to find out what the",
    "start": "4508700",
    "end": "4516480"
  },
  {
    "text": "probability is, starting in the\ntransient state, of going",
    "start": "4516480",
    "end": "4523630"
  },
  {
    "text": "to one of those classes, this is\nwhat tells you that answer.",
    "start": "4523630",
    "end": "4529170"
  },
  {
    "text": "This says that the probability\nthat you go to a particular recurrent class C, given that\nyou start off in a particular",
    "start": "4529170",
    "end": "4537530"
  },
  {
    "text": "transient state i, is whatever\nthat right eigenvector turns out to be.",
    "start": "4537530",
    "end": "4542690"
  },
  {
    "text": "And you can solve that right\neigenvector problem just as an M by M set of linear\nequations.",
    "start": "4542690",
    "end": "4548920"
  },
  {
    "text": "So you can find the\nprobabilities of going through each transient state just by\nsolving that set of linear",
    "start": "4548920",
    "end": "4556370"
  },
  {
    "text": "equations and finding those\neigenvector equations.",
    "start": "4556370",
    "end": "4561650"
  },
  {
    "text": "For each recurrent periodic\nclass of period d, there are d eigenvalues equally spaced\non the unit circle.",
    "start": "4561650",
    "end": "4569140"
  },
  {
    "text": "There are no other eigenvalues\nwith lambda equals 1-- with a magnitude of lambda equals 1.",
    "start": "4569140",
    "end": "4575080"
  },
  {
    "text": "In other words, for each\nrecurrent class, you get one eigenvalue that's equal to 1.",
    "start": "4575080",
    "end": "4580699"
  },
  {
    "text": "If that recurrent class is\nperiodic, you get a bunch of other eigenvalues put around\nthe unit circle.",
    "start": "4580700",
    "end": "4590639"
  },
  {
    "text": "And those are all the\neigenvalues there are. Oh my God.",
    "start": "4590640",
    "end": "4596296"
  },
  {
    "text": "It's-- I thought I was talking\nquickly. But anyway, if the eigenvectors\ndon't span the",
    "start": "4596296",
    "end": "4604870"
  },
  {
    "text": "space, then P to the n is equal\nto U times this Jordan",
    "start": "4604870",
    "end": "4610360"
  },
  {
    "text": "reform, U to the minus 1, where\nJ is a Jordan form. What you saw in the homework\nwhen you looked at the--",
    "start": "4610360",
    "end": "4618320"
  },
  {
    "text": " when you looked at the\nMarkov chain--",
    "start": "4618320",
    "end": "4624074"
  },
  {
    "start": "4624075",
    "end": "4648120"
  },
  {
    "text": "OK. This is one recurrent class\nwith this one node in it.",
    "start": "4648120",
    "end": "4655020"
  },
  {
    "text": "These two nodes are\nboth transient. If you look at how long it takes\nto get from here over to",
    "start": "4655020",
    "end": "4661719"
  },
  {
    "text": "there, those transition\nprobabilities do not correspond to this\nequation here.",
    "start": "4661720",
    "end": "4671620"
  },
  {
    "text": "Instead, P sub 1 2-- ",
    "start": "4671620",
    "end": "4677400"
  },
  {
    "text": "P sub 2 3, the way I've\ndrawn it here. P sub 1 3 is n times this\neigenvalue, which",
    "start": "4677400",
    "end": "4687140"
  },
  {
    "text": "is 1/2 in this case. And it doesn't correspond to\nthis, which is why you need a",
    "start": "4687140",
    "end": "4692820"
  },
  {
    "text": "Jordan form. I said that Jordan forms\nare excessively ugly.",
    "start": "4692820",
    "end": "4697860"
  },
  {
    "text": "Jordan forms are really very\nclassy and nice ways of dealing with a problem\nwhich is very ugly.",
    "start": "4697860",
    "end": "4704460"
  },
  {
    "text": "So don't blame Jordan. Jordan simplified\nthings for us.",
    "start": "4704460",
    "end": "4709670"
  },
  {
    "text": "So that's roughly as far as we\nwent with Markov chains.",
    "start": "4709670",
    "end": "4716840"
  },
  {
    "text": " Renewal processes, we don't have\nto review them because",
    "start": "4716840",
    "end": "4724910"
  },
  {
    "text": "you're already immediately\nfamiliar with them. ",
    "start": "4724910",
    "end": "4730610"
  },
  {
    "text": "I will do one thing next time\nwith renewal classes and",
    "start": "4730610",
    "end": "4735909"
  },
  {
    "text": "Markov chains, which is to\nexplain to you why the expected amount of time to get\nfrom one state back to itself",
    "start": "4735910",
    "end": "4744660"
  },
  {
    "text": "is equal to 1 over pi-- 1 over pi sub i. You did that in the homework.",
    "start": "4744660",
    "end": "4750790"
  },
  {
    "text": "And it was an awful\nway to do it. And there's a nice\nway to do it. I'll talk about that next time.",
    "start": "4750790",
    "end": "4755860"
  },
  {
    "start": "4755860",
    "end": "4758527"
  }
]