[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13320",
    "end": "18520"
  },
  {
    "text": " RUSS TEDRAKE: Today is sort of\nthe culmination of everything",
    "start": "18520",
    "end": "24449"
  },
  {
    "text": "we've been doing in the\nmodel free optimal control. OK. So we talked a lot about\nthe policy gradient methods.",
    "start": "24450",
    "end": "34440"
  },
  {
    "text": "So under the model\nfree category here. ",
    "start": "34440",
    "end": "47182"
  },
  {
    "text": "And we've talked a lot about\nmodel free policy gradient methods. ",
    "start": "47182",
    "end": "60059"
  },
  {
    "text": "And then the last\nweek or so, we spent talking about model\nfree methods based",
    "start": "60060",
    "end": "65850"
  },
  {
    "text": "on learning value functions. ",
    "start": "65850",
    "end": "75930"
  },
  {
    "text": "OK. Now both of those have some\npros and some cons to them.",
    "start": "75930",
    "end": "82560"
  },
  {
    "text": "OK? ",
    "start": "82560",
    "end": "92100"
  },
  {
    "text": "So the policy gradient\nmethods, what's good about policy\ngradient methods? ",
    "start": "92100",
    "end": "99876"
  },
  {
    "text": "STUDENT: They scale-- RUSS TEDRAKE: They scale with-- [INTERPOSING VOICES]",
    "start": "99876",
    "end": "105390"
  },
  {
    "text": "RUSS TEDRAKE: OK. And they can scale well\nto high dimensions.",
    "start": "105390",
    "end": "114619"
  },
  {
    "text": "We'll qualify that. ",
    "start": "114620",
    "end": "125940"
  },
  {
    "text": "Right? It's actually still only a local\nsearch, that's why they scale. ",
    "start": "125940",
    "end": "131420"
  },
  {
    "text": "And the performance of\nthe model free methods",
    "start": "131420",
    "end": "137690"
  },
  {
    "text": "degrades with the number\nof policy parameters. ",
    "start": "137690",
    "end": "152750"
  },
  {
    "text": "But if you have an\ninfinite dimensional system with one parameter\nyou want to optimize,",
    "start": "152750",
    "end": "158000"
  },
  {
    "text": "then you're in pretty good shape\nwith a policy gradient method. Right? ",
    "start": "158000",
    "end": "165675"
  },
  {
    "text": "OK.  What else? What are other pros and cons\nof policy gradient methods?",
    "start": "165675",
    "end": "172090"
  },
  {
    "text": "What's a con? Well, I said a lot of it\nin the parentheses already.",
    "start": "172090",
    "end": "177980"
  },
  {
    "text": "But it's local. What are some other cons\nabout policy gradient methods?",
    "start": "177980",
    "end": "186508"
  },
  {
    "text": "STUDENT: [INAUDIBLE].  RUSS TEDRAKE: Yeah.",
    "start": "186509",
    "end": "191830"
  },
  {
    "text": "Right. So this performance\ndegradation typically is summarized by\npeople saying they",
    "start": "191830",
    "end": "196939"
  },
  {
    "text": "tend to have high variance. Right? ",
    "start": "196940",
    "end": "207680"
  },
  {
    "text": "Variance in the\nupdate, which can",
    "start": "207680",
    "end": "214849"
  },
  {
    "text": "lead to mean that you need\nmany trials to converge.",
    "start": "214850",
    "end": "220550"
  },
  {
    "start": "220550",
    "end": "229620"
  },
  {
    "text": "I mean, fundamentally, if\nwe're sampling policy space and making some\nstochastic update,",
    "start": "229620",
    "end": "235790"
  },
  {
    "text": "it might be that it\nrequires many, many samples, for instance, to accurately\nestimate the gradient.",
    "start": "235790",
    "end": "241500"
  },
  {
    "text": "And if we're making a\nmove after every sample, then it might take\nmany, many trials for us to find the minimum of that.",
    "start": "241500",
    "end": "246890"
  },
  {
    "text": "It's a noisy descent. Yeah? STUDENT: You also have to\nchoose like a [INAUDIBLE]..",
    "start": "246890",
    "end": "252200"
  },
  {
    "text": "RUSS TEDRAKE: Good. That wasn't even on my list. But I totally agree.",
    "start": "252200",
    "end": "258440"
  },
  {
    "text": "OK. I'll put it right up here. ",
    "start": "258440",
    "end": "280010"
  },
  {
    "text": "There's one other very big\nadvantage to the policy gradient algorithms.",
    "start": "280010",
    "end": "285253"
  },
  {
    "start": "285253",
    "end": "290929"
  },
  {
    "text": "We take advantage of smoothness. They require smoothness to work.",
    "start": "290929",
    "end": "297020"
  },
  {
    "text": "That's both a pro\nand a con, right?  But the big one that we\nhaven't said yet, I think,",
    "start": "297020",
    "end": "304880"
  },
  {
    "text": "is the convergence is sort\nof virtually guaranteed. You're doing a direct search.",
    "start": "304880",
    "end": "311840"
  },
  {
    "text": "And exactly, you're doing a\nstochastic gradient descent in exactly the parameters\nyou care about.",
    "start": "311840",
    "end": "318050"
  },
  {
    "text": "Convergence is sort of\ntrivial and guaranteed. OK? ",
    "start": "318050",
    "end": "338900"
  },
  {
    "text": "That turns out to be\nprobably one of the biggest motivating reasons\nfor the community to have put their efforts\ninto policy gradient.",
    "start": "338900",
    "end": "346100"
  },
  {
    "text": "Because if you look at the\nvalue function methods,",
    "start": "346100",
    "end": "359870"
  },
  {
    "text": "in many cases-- now, I told you about one case\nwith function approximation, still linear function\napproximation,",
    "start": "359870",
    "end": "366380"
  },
  {
    "text": "where there are stronger\nconvergence results. And that was the least\nsquares policy iteration.",
    "start": "366380",
    "end": "372289"
  },
  {
    "text": "But most of the cases we've\nhad, the convergence results were fairly weak.",
    "start": "372290",
    "end": "378259"
  },
  {
    "text": "We told you that temporal\ndifference learning converges if the policy remains fixed.",
    "start": "378260",
    "end": "383280"
  },
  {
    "text": "OK? But if you're not\ncareful, if you do temporal difference learning\nwith the policy changing, with a function\napproximator involved,",
    "start": "383280",
    "end": "390410"
  },
  {
    "text": "convergence is not guaranteed. OK? In fact, they often, I mean a\nlot of these methods struggle",
    "start": "390410",
    "end": "400430"
  },
  {
    "text": "with convergence. Not just the proofs,\nwhich are more involved.",
    "start": "400430",
    "end": "405750"
  },
  {
    "text": "But there's a\nhandful of, I guess, sort of in the big switch\nfrom value methods to policy",
    "start": "405750",
    "end": "413750"
  },
  {
    "text": "gradient methods,\nthere are a number of papers showing sort\nof trivial examples of--",
    "start": "413750",
    "end": "424324"
  },
  {
    "text": " can I call them TD\ncontrol methods?",
    "start": "424325",
    "end": "429750"
  },
  {
    "text": "So temporal difference\nlearning where you're actually also updating your policy\nof TD control methods",
    "start": "429750",
    "end": "439130"
  },
  {
    "text": "with function approximation,\nwhich diverge.",
    "start": "439130",
    "end": "446190"
  },
  {
    "text": "Right? ",
    "start": "446190",
    "end": "454550"
  },
  {
    "text": "There was even one that-- I think it might\nhave been, I forget whose-- it might have been\nLehman Baird's example.",
    "start": "454550",
    "end": "461840"
  },
  {
    "text": "But where they actually showed\nthat the method will actually oscillate between the best\npossible representation",
    "start": "461840",
    "end": "468890"
  },
  {
    "text": "of the value function and the\nworst possible representation of the value function. And it's sort of stably\noscillated between the two.",
    "start": "468890",
    "end": "475490"
  },
  {
    "text": "Right? Which was obviously something\nthat they cooked up. But still, that makes the point.",
    "start": "475490",
    "end": "480660"
  },
  {
    "text": "Right? ",
    "start": "480660",
    "end": "503440"
  },
  {
    "text": "Even the convergence result\nwe did give you for LSPI, least squares policy\niteration, still",
    "start": "503440",
    "end": "510522"
  },
  {
    "text": "had no guarantee that\nit wasn't going to, it could certainly oscillate. They gave a bound\non the oscillation.",
    "start": "510522",
    "end": "515590"
  },
  {
    "text": "But that bound has\nto be interpreted. Even the LSPI could\nstill oscillate.",
    "start": "515590",
    "end": "522510"
  },
  {
    "text": "And that's one of the stronger\nconvergence results we have. ",
    "start": "522510",
    "end": "527769"
  },
  {
    "text": "OK. But they're\nrelatively efficient. Right? So we put up with a lot of that.",
    "start": "527770",
    "end": "533950"
  },
  {
    "text": "And we keep trying to\nuse them because they're efficient to learn in the\nsense that you're just",
    "start": "533950",
    "end": "539230"
  },
  {
    "text": "learning a scalar value over\nall your states and actions. That's a relatively\ncompact thing to learn. I told you, I tried\nto argue last time",
    "start": "539230",
    "end": "544893"
  },
  {
    "text": "that it's easier than\nlearning a model even by just",
    "start": "544893",
    "end": "549940"
  },
  {
    "text": "dimensionality arguments. And they tend to be\nefficient because the TD methods in particular\nreuse your estimates.",
    "start": "549940",
    "end": "557177"
  },
  {
    "text": "And they tend to be\nefficient in data. They reuse old estimates. They use your old\nestimate of the value function to update your new\nestimate of the algorithms.",
    "start": "557177",
    "end": "565839"
  },
  {
    "text": "So when they do work,\nthey tend to learn faster. And they can, with the\nleast squares methods,",
    "start": "565840",
    "end": "572050"
  },
  {
    "text": "they tend to be\nefficient in data. And therefore, in time. Number of trials.",
    "start": "572050",
    "end": "579280"
  },
  {
    "text": "When these things do work,\nthey're the tool of choice. The problem is-- and\nthere are great examples",
    "start": "579280",
    "end": "585610"
  },
  {
    "text": "of them working-- but\nthere's not enough guarantees of them working. ",
    "start": "585610",
    "end": "593040"
  },
  {
    "text": "And if you want to\nsort of summarize why these value methods\nstruggle and why",
    "start": "593040",
    "end": "601350"
  },
  {
    "text": "they can struggle to converge\nand they even diverge, you can sort of think of it\nin a single line, I think.",
    "start": "601350",
    "end": "607890"
  },
  {
    "text": "The basic fundamental problem\nwith the value methods is that a very small change\nin your estimate of the value",
    "start": "607890",
    "end": "613920"
  },
  {
    "text": "function, if you\nmake a little change, can cause a dramatic\nchange in your policy.",
    "start": "613920",
    "end": "619920"
  },
  {
    "text": "Right? So let's say my value\nfunctions tip this way. Right? And I change my\nparameters a little bit. Now it's tipped this way.",
    "start": "619920",
    "end": "625950"
  },
  {
    "text": "My policy just went from\ngoing left to going right, for instance. And now you're trying to\nupdate your value function",
    "start": "625950",
    "end": "635010"
  },
  {
    "text": "as the policy changed. And just things can start\noscillating out of control.",
    "start": "635010",
    "end": "640465"
  },
  {
    "text": "Does that make sense? ",
    "start": "640465",
    "end": "693110"
  },
  {
    "text": "OK.  That's a reasonably accurate,\nI think, lay of the land",
    "start": "693110",
    "end": "701100"
  },
  {
    "text": "in the methods we've\ntold you about so far. If you can find a value method\nthat converges nicely, use it.",
    "start": "701100",
    "end": "707709"
  },
  {
    "text": "It's going to be faster than\na policy gradient method. It's more efficient\nin reusing data. You're learning a fairly\ncompact structure.",
    "start": "707710",
    "end": "713740"
  },
  {
    "text": "Value iteration has always been\nour most efficient algorithm, when it works.",
    "start": "713740",
    "end": "719590"
  },
  {
    "text": "But the policy\ngradient algorithms are guaranteed to work. And they're fairly\nsimple to implement.",
    "start": "719590",
    "end": "725210"
  },
  {
    "text": "And they can just be sort of\nlocal search in the policy space. Directly in the space that you\ncare about, really your policy.",
    "start": "725210",
    "end": "733150"
  },
  {
    "text": "So the big idea, which\nis the culmination of the methods we've talked\nabout in the model free stuff so far, is to try to take\nthe advantages of both",
    "start": "733150",
    "end": "741160"
  },
  {
    "text": "by putting them together. Represent both a value function\nand a policy simultaneously.",
    "start": "741160",
    "end": "747760"
  },
  {
    "text": "There's extra\nrepresentational costs there. But if you're willing to do\nthat and make slower changes",
    "start": "747760",
    "end": "753250"
  },
  {
    "text": "to the policy based on guesses\nthat are coming from the value function, then you can\novercome a lot of the stability",
    "start": "753250",
    "end": "760660"
  },
  {
    "text": "problems of the value methods. You get the strong convergence\nresults of the policy gradient.",
    "start": "760660",
    "end": "765700"
  },
  {
    "text": "And you get some of the\nmore, ideally, efficiency. You can reduce your\nvariance of your update.",
    "start": "765700",
    "end": "771850"
  },
  {
    "text": "You make more effective updates\nby using a value function. OK? ",
    "start": "771850",
    "end": "792100"
  },
  {
    "text": "So the actor is the playful\nname for the policy.",
    "start": "792100",
    "end": "798069"
  },
  {
    "text": "And the critic is your\nvalue estimate telling you how well you're going to do. ",
    "start": "798070",
    "end": "830110"
  },
  {
    "text": "And one of the\nbig ideas there is you'd like it to be a\ntwo time scale algorithm. ",
    "start": "830110",
    "end": "849990"
  },
  {
    "text": "Policy is changing slower\nthan the greedy policy",
    "start": "849990",
    "end": "860360"
  },
  {
    "text": "from the value function. ",
    "start": "860360",
    "end": "882160"
  },
  {
    "text": "OK. So the idea is an actor critic\nare actually very, very simple.",
    "start": "882160",
    "end": "888060"
  },
  {
    "text": "The proofs are ugly. There's only a handful\nof papers you've got to look at if you\nwant to get into the dirt.",
    "start": "888060",
    "end": "897930"
  },
  {
    "text": "But these, I think,\nare the algorithms of choice today for a\nmodel free optimization.",
    "start": "897930",
    "end": "906730"
  },
  {
    "text": "OK. So just to give you a couple\nof the key papers here. So Konda and Tsitsiklis.",
    "start": "906730",
    "end": "914370"
  },
  {
    "text": "John's right upstairs. Had an actor critic\npaper in 2003",
    "start": "914370",
    "end": "921000"
  },
  {
    "text": "that has all the algorithm\nderivation and proofs. ",
    "start": "921000",
    "end": "926220"
  },
  {
    "text": "Sutton has a similar one in '99\nthat's called Policy Gradient.",
    "start": "926220",
    "end": "931810"
  },
  {
    "text": "But it's actually\nthe same sort of math as in Konda and Tsitsiklis.",
    "start": "931810",
    "end": "937380"
  },
  {
    "text": "And then our friend Jan Peters\nhas got a newer take on it.",
    "start": "937380",
    "end": "944370"
  },
  {
    "text": "He calls it Natural\nActor Critic,",
    "start": "944370",
    "end": "955470"
  },
  {
    "text": "which is a popular one today. It should be easy to find. OK. So I want to give\nyou the basic tools.",
    "start": "955470",
    "end": "962370"
  },
  {
    "text": "And then instead of\ngetting into all the math, I'll give you a case\nstudy, which was my thesis.",
    "start": "962370",
    "end": "968590"
  },
  {
    "text": "Works out. ",
    "start": "968590",
    "end": "992600"
  },
  {
    "text": "So probably John already said\nquickly what the big idea was. Right? So John told you about the\nreinforced type algorithms",
    "start": "992600",
    "end": "1004270"
  },
  {
    "text": "and the weight perturbation. ",
    "start": "1004270",
    "end": "1012759"
  },
  {
    "text": "In the reinforced algorithms,\nwe have some parameter vector. Let's call it alpha.",
    "start": "1012760",
    "end": "1018370"
  },
  {
    "text": "And I'm going to change alpha\nwith a very simple update rule. In the simple case, maybe\nI'll run my system twice.",
    "start": "1018370",
    "end": "1026050"
  },
  {
    "text": "I'll run it once with the-- I'll get once, I'll sample\nthe output with alpha.",
    "start": "1026050",
    "end": "1033339"
  },
  {
    "text": "And then once I'll do it\nwith alpha plus some noise. Let's say I'll run it from\nthe same initial condition.",
    "start": "1033339",
    "end": "1040810"
  },
  {
    "text": "Compare those two. ",
    "start": "1040810",
    "end": "1046250"
  },
  {
    "text": "And then multiply the difference\ntimes the noise I added. Right? ",
    "start": "1046250",
    "end": "1068180"
  },
  {
    "text": "And that's actually\na good estimator, a reasonable estimator\nof the gradient.",
    "start": "1068180",
    "end": "1073860"
  },
  {
    "text": "And if I multiply by\nthe learning rate, then I've got a gradient\ndescent type update.",
    "start": "1073860",
    "end": "1080980"
  },
  {
    "text": "OK?  So this is not useful\nin its current form.",
    "start": "1080980",
    "end": "1087039"
  },
  {
    "text": "John told you about the\nbetter forms of it, too. But the problem\nwith this is that I have to run the system\ntwice from exactly",
    "start": "1087040",
    "end": "1094440"
  },
  {
    "text": "the same initial conditions. You don't want to run two trials\nto simulate the thing exactly",
    "start": "1094440",
    "end": "1099960"
  },
  {
    "text": "twice for every one update.",
    "start": "1099960",
    "end": "1106289"
  },
  {
    "text": "And it sort of assumes that\nthis is a deterministic update.",
    "start": "1106290",
    "end": "1111300"
  },
  {
    "text": "The more general form here\nwould be to not keep, not",
    "start": "1111300",
    "end": "1117090"
  },
  {
    "text": "run the system twice. But use, for instance,\nsome estimate of what reward I'd expect to\nget from this initial condition.",
    "start": "1117090",
    "end": "1126280"
  },
  {
    "text": "And compare that to\nthe learning trial. So we just went from policy\ngradient to actor critic",
    "start": "1126280",
    "end": "1132330"
  },
  {
    "text": "just like that. This is the simplest form of it. But let's think about\nwhat just happened.",
    "start": "1132330",
    "end": "1137620"
  },
  {
    "text": "So if I do have an estimate\nof my value function, I have an estimate of my\ncost to go from every state.",
    "start": "1137620",
    "end": "1143490"
  },
  {
    "text": "Right? Then that helps me make\na policy gradient update. Because if I run a single trial,\nthen I can compare the reward",
    "start": "1143490",
    "end": "1151470"
  },
  {
    "text": "I expected to get\nwith the reward I actually got very compactly.",
    "start": "1151470",
    "end": "1157200"
  },
  {
    "text": "OK? So this is the reward\nI actually got. I run a trial, one trial. Even if it's noisy with\nmy perturb parameters,",
    "start": "1157200",
    "end": "1164100"
  },
  {
    "text": "I change my parameters\na little bit. I run a trial. And what I want\nto efficiently do is compare it to the reward I\nshould have expected to get,",
    "start": "1164100",
    "end": "1170687"
  },
  {
    "text": "given I had the parameters\nI had a minute ago. Right? That's nothing but a\nvalue function right here.",
    "start": "1170687",
    "end": "1177090"
  },
  {
    "text": "OK? So the simplest way to think\nabout an actor critic algorithm is go ahead and use a TD\nlearning kind of algorithm.",
    "start": "1177090",
    "end": "1184410"
  },
  {
    "text": " Every time I'm running\nmy robot, go ahead",
    "start": "1184410",
    "end": "1189720"
  },
  {
    "text": "and work on in the\nbackground learning a value function of the system.",
    "start": "1189720",
    "end": "1195900"
  },
  {
    "text": "And simply use that to\ncompare the samples you get from your policy search.",
    "start": "1195900",
    "end": "1203235"
  },
  {
    "text": "Do you guys remember the sort\nof weight perturbation type updates enough for\nthat to make sense?",
    "start": "1203235",
    "end": "1208730"
  },
  {
    "text": "Yeah? STUDENT: So in this case, that\n[INAUDIBLE] into your system",
    "start": "1208730",
    "end": "1214570"
  },
  {
    "text": "but just through\nsome expectation. RUSS TEDRAKE: Excellent. That's where you're getting it.",
    "start": "1214570",
    "end": "1219910"
  },
  {
    "text": "From temporal\ndifference learning. In the case of a stochastic\nsystem, where both of these",
    "start": "1219910",
    "end": "1226450"
  },
  {
    "text": "are going to be noisy\nrandom variables, this actually can be better\nthan running it twice.",
    "start": "1226450",
    "end": "1231700"
  },
  {
    "text": "Because this is the\nexpected value accumulated through experience. Right? And that's what you really want\nto compare your noisy sample",
    "start": "1231700",
    "end": "1239380"
  },
  {
    "text": "to the expected value. So in the stochastic\ncase, you actually do better by comparing it to the\nexpected value of your update.",
    "start": "1239380",
    "end": "1246850"
  },
  {
    "start": "1246850",
    "end": "1252250"
  },
  {
    "text": "What you can show\nby various tools is that comparing to the\nexpected value of your update,",
    "start": "1252250",
    "end": "1260210"
  },
  {
    "text": "which is the value\nfunction here, can dramatically reduce the\nvariance of your estimator.",
    "start": "1260210",
    "end": "1266179"
  },
  {
    "text": "OK? ",
    "start": "1266180",
    "end": "1299570"
  },
  {
    "text": "You should always think\nabout policy gradient as every one of\nthese steps trying to estimate the change\nin the performance",
    "start": "1299570",
    "end": "1306590"
  },
  {
    "text": "based on a change in parameters. But in general,\nwhat you get back",
    "start": "1306590",
    "end": "1312410"
  },
  {
    "text": "is the true gradient\nplus a bunch of noise, because you're just taking\na random sample here in one dimension of change.",
    "start": "1312410",
    "end": "1320570"
  },
  {
    "text": "If this is a good estimate\nof the value function, then it can reduce the\nvariance of that update.",
    "start": "1320570",
    "end": "1327650"
  },
  {
    "text": "Question? STUDENT: [INAUDIBLE]. ",
    "start": "1327650",
    "end": "1334288"
  },
  {
    "text": "RUSS TEDRAKE: The\nguarantees of convergence are still intact because\nyou're doing gradient descent. You can actually do, you\ncan do almost anything here.",
    "start": "1334288",
    "end": "1341330"
  },
  {
    "text": "This can be zero. And gradient descent, the\npolicy gradient actually still converges. It doesn't converge very fast.",
    "start": "1341330",
    "end": "1347930"
  },
  {
    "text": "But you can still\nactually show that it'll, on average, converge. OK? So it's actually quite robust\nto the thing you subtract out.",
    "start": "1347930",
    "end": "1357530"
  },
  {
    "text": "Because, especially if this\nthing doesn't depend on alpha, then it has zero expectation.",
    "start": "1357530",
    "end": "1362580"
  },
  {
    "text": "So it doesn't even affect the\nexpected value of your update. So it actually does not affect\nthe convergence results at all.",
    "start": "1362580",
    "end": "1368399"
  },
  {
    "text": "So the convergence\nresults are still intact. But the performance\nshould get better because you have a better\nestimate of your J. Right?",
    "start": "1368400",
    "end": "1376458"
  },
  {
    "text": "And that should be\nintuitively obvious, actually. Right? If I did something and\nI said, how did I do?",
    "start": "1376458",
    "end": "1384320"
  },
  {
    "text": "And [INAUDIBLE]\njust always said, you should have gotten a\nfour every single time.",
    "start": "1384320",
    "end": "1390303"
  },
  {
    "text": "If I got a lousy estimator of\nhow well I should have done, I'd say, OK. Look, I got a six that time. And he says, you\nshould have had a four.",
    "start": "1390303",
    "end": "1396200"
  },
  {
    "text": "Six, you should have had a four. Then he's giving\nme no information. And that's not helping\nme evaluate my policy. Right?",
    "start": "1396200",
    "end": "1401610"
  },
  {
    "text": "If someone said, OK. We did something a\nlittle different. I expected you to get a\nsix, but you got a 6.1.",
    "start": "1401610",
    "end": "1407660"
  },
  {
    "text": "Well, that's a much cleaner\nlearning signal for me to use. STUDENT: [INAUDIBLE]\nthe worst possible--",
    "start": "1407660",
    "end": "1421310"
  },
  {
    "text": "RUSS TEDRAKE: Yeah, absolutely. So that's the important\npoint is that it's got to be uncorrelated with the\nnoise you add to your system.",
    "start": "1421310",
    "end": "1427790"
  },
  {
    "text": "OK? If it's not correlated\nwith the noise you add in, then it actually goes\naway in expectation.",
    "start": "1427790",
    "end": "1433306"
  },
  {
    "text": "So the variance can be very bad\nif you have the worst possible value estimate. But the convergence\nstill happens.",
    "start": "1433307",
    "end": "1441620"
  },
  {
    "text": " Like I said, zero\nactually works. Right?",
    "start": "1441620",
    "end": "1448313"
  },
  {
    "text": "Which is sort of surprising. Right? If I have a reward function\nthat always returns between zero",
    "start": "1448313",
    "end": "1454550"
  },
  {
    "text": "and 10, and I'm trying\nto optimize my update,",
    "start": "1454550",
    "end": "1460580"
  },
  {
    "text": "then I would always move in the\ndirection of the noise I add.",
    "start": "1460580",
    "end": "1465799"
  },
  {
    "text": "But I move more often in the\nones that gave me high scores. And actually, it still\ndoes a gradient descent",
    "start": "1465800",
    "end": "1471080"
  },
  {
    "text": "on the cost function. It's actually worth\nthinking about that. It's actually pretty cool that\nit's so robust, that estimator.",
    "start": "1471080",
    "end": "1479570"
  },
  {
    "text": "But certainly with a good\nestimator, it works better. ",
    "start": "1479570",
    "end": "1485960"
  },
  {
    "text": "I don't know how\nmuch John told you. But we don't actually like\ntalking about the variance. We like talking about the\nsignal to noise ratio.",
    "start": "1485960",
    "end": "1491050"
  },
  {
    "text": "Did you tell them about the\nsignal noise ratio, John? STUDENT: I don't remember. RUSS TEDRAKE: Quickly? Yeah. So John's got a nice paper.",
    "start": "1491050",
    "end": "1497000"
  },
  {
    "text": "Maybe he was being modest. John has a nice paper\nanalyzing the performance of these with a signal to\nnoise ratio analysis, which",
    "start": "1497000",
    "end": "1503270"
  },
  {
    "text": "is another way to look at the\nperformance of the update.",
    "start": "1503270",
    "end": "1513440"
  },
  {
    "start": "1513440",
    "end": "1519240"
  },
  {
    "text": "So that's enough to do, to take\nthe power of the value methods and start putting them to use\nin the policy gradient methods.",
    "start": "1519240",
    "end": "1526759"
  },
  {
    "text": "OK? The cool thing is, like I said,\nas long as it's uncorrelated with z, it can be a very\nbad approximate of the value",
    "start": "1526760",
    "end": "1532410"
  },
  {
    "text": "function. It won't break convergence. The better the value estimate,\nthe faster your convergence is.",
    "start": "1532410",
    "end": "1539420"
  },
  {
    "text": "OK? This isn't the update\nthat people typically use when they talk about\nactor critic updates. The Konda and Tsitsiklis one\nhas a slightly more beautiful",
    "start": "1539420",
    "end": "1546630"
  },
  {
    "text": "thing. This is maybe what you think\nof as an episodic update. Right?",
    "start": "1546630",
    "end": "1551929"
  },
  {
    "text": "This is, I just said we\nstarted initial condition x. Maybe I should right\nan x zero or something. But we just start with\ninitial condition x.",
    "start": "1551930",
    "end": "1558282"
  },
  {
    "text": "We run our robot for a little\nbit with these parameters. We compare it to\nwhat we expected.",
    "start": "1558282",
    "end": "1563850"
  },
  {
    "text": "And we make an update\nmaybe once per trial. That's a perfectly\ngood algorithm for making an update\nonce per trial.",
    "start": "1563850",
    "end": "1570440"
  },
  {
    "text": "There's a more beautiful\nsort of online update. Right? If you actually want\nto, let's say you",
    "start": "1570440",
    "end": "1582059"
  },
  {
    "text": "have an infinite horizon thing. Infinite horizon problem.",
    "start": "1582060",
    "end": "1587280"
  },
  {
    "start": "1587280",
    "end": "1603160"
  },
  {
    "text": "There's actually a theorem,\nI've debated how much of this to go into. But I'll at least list\nthe theorem for you",
    "start": "1603160",
    "end": "1608889"
  },
  {
    "text": "because it's nice. They call it the policy\ngradient theorem,",
    "start": "1608890",
    "end": "1622760"
  },
  {
    "text": "which says partial\nJ partial alpha,",
    "start": "1622760",
    "end": "1628640"
  },
  {
    "text": "where in the infinite\nhorizon case typically there's different ways to\ndefine infinite horizons.",
    "start": "1628640",
    "end": "1634460"
  },
  {
    "text": "This is typically done in\nan average reward setting. ",
    "start": "1634460",
    "end": "1640309"
  },
  {
    "text": "It can be made to work\nfor other formulations. But I'll just be careful\nto say the one that I",
    "start": "1640310",
    "end": "1646460"
  },
  {
    "text": "know it's a correct proof for. The policy gradient can\nactually be written as--",
    "start": "1646460",
    "end": "1653705"
  },
  {
    "text": " let me write it out. ",
    "start": "1653705",
    "end": "1674480"
  },
  {
    "text": "This guy is the stationary\ndistribution of executing",
    "start": "1674480",
    "end": "1690020"
  },
  {
    "text": "of the state action, of\nexecuting pi of alpha.",
    "start": "1690020",
    "end": "1697640"
  },
  {
    "text": "This guy is the Q function\nexecuting alpha, the true Q function. And this is the state action.",
    "start": "1697640",
    "end": "1703899"
  },
  {
    "start": "1703900",
    "end": "1711800"
  },
  {
    "text": "And this guy is\nactually the gradient of the log\nprobabilities, which is the same thing we saw in the\npolicy gradient algorithms.",
    "start": "1711800",
    "end": "1719840"
  },
  {
    "text": "The log probabilities\nof executing pi.",
    "start": "1719840",
    "end": "1725570"
  },
  {
    "start": "1725570",
    "end": "1737889"
  },
  {
    "text": "Yeah. Gradient of the log probability.",
    "start": "1737890",
    "end": "1743080"
  },
  {
    "text": "I'm not trying to give you\nenough to completely get this. But just I want you\nto know that it exists and know where to find it.",
    "start": "1743080",
    "end": "1750250"
  },
  {
    "text": "And what it reveals is\na very nice relationship between the Q function\nand the gradients",
    "start": "1750250",
    "end": "1758679"
  },
  {
    "text": "that we were already computing\nin our reinforced type algorithms. OK?",
    "start": "1758680",
    "end": "1763809"
  },
  {
    "text": "And it turns out an\nupdate of the form-- ",
    "start": "1763810",
    "end": "1792520"
  },
  {
    "text": "this is gradient of the\nlog probabilities again. I'll just write it. ",
    "start": "1792520",
    "end": "1805646"
  },
  {
    "text": "That would be doing\ngradient descent on this if you're running\nfrom sample paths. This term disappears if\nI'm just pulling x and u",
    "start": "1805646",
    "end": "1815140"
  },
  {
    "text": "from the distribution\nthat happens when I run the system that gives\nme this stationary distribution",
    "start": "1815140",
    "end": "1822340"
  },
  {
    "text": "coefficient for free. OK? And then if I could\nsomehow multiply the true Q",
    "start": "1822340",
    "end": "1828340"
  },
  {
    "text": "function times my\neligibility-- this one, I definitely have access to. This one, I can only guess,\nbecause I have access",
    "start": "1828340",
    "end": "1839170"
  },
  {
    "text": "to my policy. I can compute that. ",
    "start": "1839170",
    "end": "1846730"
  },
  {
    "text": "But this guy, I\nhave to estimate. ",
    "start": "1846730",
    "end": "1853799"
  },
  {
    "text": "OK? So if I put a hat on\nthere, then that's actually a good estimator\nof the policy gradient using",
    "start": "1853800",
    "end": "1863790"
  },
  {
    "text": "an approximate Q function. And in the case where you hold\nup your updates for a long time",
    "start": "1863790",
    "end": "1871020"
  },
  {
    "text": "and then make an estimate\nin an episodic case, it actually results in\nthat actual algorithm.",
    "start": "1871020",
    "end": "1878230"
  },
  {
    "text": "OK? ",
    "start": "1878230",
    "end": "1884320"
  },
  {
    "text": "Getting to that from with\na more detailed explanation is painful. But it's good to know.",
    "start": "1884320",
    "end": "1890429"
  },
  {
    "text": "I think the way you're\ngoing to appreciate actor critic algorithms,\nthough, is by seeing them work. OK? So let me show you how I\nmade them work on a walking",
    "start": "1890430",
    "end": "1901300"
  },
  {
    "text": "robot for my thesis. I've already done this. Is it going to turn on? ",
    "start": "1901300",
    "end": "1936305"
  },
  {
    "text": "Since I think everybody's\nhere, maybe we should do, while it's booting, I'll\ndo a quick context switch. Let's figure out\nprojects real quick.",
    "start": "1936305",
    "end": "1942830"
  },
  {
    "text": "And then we'll go back. I don't want to run\nout of time and forget to say all the last\ndetails about the projects. Yeah?",
    "start": "1942830",
    "end": "1948100"
  },
  {
    "text": " Somehow, I never remember\nto post the syllabus",
    "start": "1948100",
    "end": "1954940"
  },
  {
    "text": "with all the dates on there. We're posting it now. But I can't believe\nI didn't post",
    "start": "1954940",
    "end": "1959980"
  },
  {
    "text": "a long time ago on the website. But I hope you know that the\nend of term is coming fast.",
    "start": "1959980",
    "end": "1965590"
  },
  {
    "text": "Yeah? And you know you're\ndoing a write up. Right? And that write up,\nwe're going to say",
    "start": "1965590",
    "end": "1972519"
  },
  {
    "text": "that the 21st,\nwhich is basically the last day I can possibly\nstill grade them by,",
    "start": "1972520",
    "end": "1983290"
  },
  {
    "text": "the write up as described, which\nis sort of I said six pages-- sort of an [INAUDIBLE]\ntype format--",
    "start": "1983290",
    "end": "1989380"
  },
  {
    "text": "is going to be due\non May 21 online. ",
    "start": "1989380",
    "end": "2000490"
  },
  {
    "text": "OK. But next week, last week\nof term already, we're",
    "start": "2000490",
    "end": "2006052"
  },
  {
    "text": "going to try to do oral\npresentations so you guys can tell me-- eight minutes each is\nwhat works out to be.",
    "start": "2006052",
    "end": "2012820"
  },
  {
    "text": "You get to tell us what\nyou've been working on. OK? ",
    "start": "2012820",
    "end": "2030280"
  },
  {
    "text": "For each project,\nthere are a few of you that are working in pairs.",
    "start": "2030280",
    "end": "2035320"
  },
  {
    "text": "But we'll still just do\neight minutes per project. And we have 19 total projects.",
    "start": "2035320",
    "end": "2041140"
  },
  {
    "text": " So I figure we do eight--",
    "start": "2041140",
    "end": "2047710"
  },
  {
    "text": " sorry, nine-- next Thursday,\nwhich is going to be the 14th.",
    "start": "2047710",
    "end": "2059469"
  },
  {
    "text": "Is that right? 5-14. ",
    "start": "2059469",
    "end": "2066969"
  },
  {
    "text": "And nine on 5-12,\nworking back here, which leaves some unlucky son\nof a gun going on Thursday.",
    "start": "2066969",
    "end": "2075879"
  },
  {
    "start": "2075880",
    "end": "2083446"
  },
  {
    "text": "And the way I've\nalways done this is I have a MATLAB script here that\nhas everybody's name in it.",
    "start": "2083447",
    "end": "2091629"
  },
  {
    "text": "Yeah. Why is it not on here? OK. ",
    "start": "2091630",
    "end": "2098230"
  },
  {
    "text": "I have a MATLAB script\nwith all your names in it. OK? And I'm going to do a\nrand perm on the names.",
    "start": "2098230",
    "end": "2105970"
  },
  {
    "text": "And it'll print up\nwhat day you're going. STUDENT: Maybe in\nfairness to that person,",
    "start": "2105970",
    "end": "2111310"
  },
  {
    "text": "would we all be happy\nto stay an extra eight minutes on whatever it is? Tuesday?",
    "start": "2111310",
    "end": "2116802"
  },
  {
    "text": "RUSS TEDRAKE: Let's\ndo it this way first. And then we'll figure it out. [LAUGHTER]",
    "start": "2116802",
    "end": "2122690"
  },
  {
    "text": "And yes. So I'm going to call\nrand perm in MATLAB.",
    "start": "2122690",
    "end": "2127780"
  },
  {
    "text": "And for dramatic\neffect this year, I've added pause statements\nbetween the print commands.",
    "start": "2127780",
    "end": "2134110"
  },
  {
    "text": "[LAUGHTER] So we should have a good\ntime with this, I think. I will, at least.",
    "start": "2134110",
    "end": "2139850"
  },
  {
    "text": "OK. Good.  Let's make this nice and big. ",
    "start": "2139850",
    "end": "2148210"
  },
  {
    "text": "I actually was going to\njust use a few slides from the middle of this. But I thought I'd at least\nlet you see the motivation",
    "start": "2148210",
    "end": "2153760"
  },
  {
    "text": "behind it, which very well. And I'll go through it quickly. But just to see at least\nmy take on it in 2005,",
    "start": "2153760",
    "end": "2160900"
  },
  {
    "text": "which hasn't\nchanged a whole lot. It's matured, I hope. But I've told you\nabout walking robots.",
    "start": "2160900",
    "end": "2168670"
  },
  {
    "text": "We spent more time talking\nabout passive walkers than we talked about some\nof the other approaches.",
    "start": "2168670",
    "end": "2173860"
  },
  {
    "text": "But there's actually a lot of\ngood walking robots out there. Even in 2005, there\nwere a lot of good ones. This one is M2 from the Leg Lab.",
    "start": "2173860",
    "end": "2179662"
  },
  {
    "text": "The wiring could\nhave been cleaner. But it's actually a\npretty beautiful robot in a lot of ways.",
    "start": "2179662",
    "end": "2185230"
  },
  {
    "text": "The simulations of it are great. It hasn't walked\nvery nicely yet. But it's a detail.",
    "start": "2185230",
    "end": "2190760"
  },
  {
    "text": "[LAUGHTER] Honda's ASIMO had sort of the\nsame sort of humble beginnings.",
    "start": "2190760",
    "end": "2196493"
  },
  {
    "text": "As you can imagine,\nit's not really fair that academics have to compete\nwith people like Honda. Right? I mean, so our robots\nlooked like what",
    "start": "2196493",
    "end": "2202840"
  },
  {
    "text": "you saw on the last page. And ASIMO looks like\nwhat it looks like. But it's kind of fun to\nsee where ASIMO came from.",
    "start": "2202840",
    "end": "2208190"
  },
  {
    "text": "So this is ASIMO 0.000. Right? And this is actually\nthe progression of their ASIMO robots.",
    "start": "2208190",
    "end": "2214405"
  },
  {
    "text": " That's the first one they\ntold the world about in '97.",
    "start": "2214405",
    "end": "2221210"
  },
  {
    "text": "Rocked the world of robotics. I was in the Leg Lab, remember. At the time, we were\nkind of like, oh wow.",
    "start": "2221210",
    "end": "2227776"
  },
  {
    "text": "They did that? Wow. That sort of certain changes\nour view of the world. That's P3.",
    "start": "2227777",
    "end": "2233359"
  },
  {
    "text": "And that's ASIMO. Right? Really, really still one of the\nmost beautiful robots around.",
    "start": "2233360",
    "end": "2239587"
  },
  {
    "text": "You know about\nunder-actuated systems. I don't have to tell you that. You know about acrobots. You know walking\nis under-actuated.",
    "start": "2239587",
    "end": "2247910"
  },
  {
    "text": "Right? Just to say it again--\nand I said it quickly-- but essentially,\nthe way ASIMO works",
    "start": "2247910",
    "end": "2253610"
  },
  {
    "text": "is they are trying to\navoid under-actuation. Right? When you watch videos\nof ASIMO walking,",
    "start": "2253610",
    "end": "2258740"
  },
  {
    "text": "it's always got its\nfoot flat on the ground. There's an exception where it\nruns with an [? arrow ?] phase",
    "start": "2258740",
    "end": "2264292"
  },
  {
    "text": "that you need a high\nspeed camera to see. But-- [LAUGHTER]",
    "start": "2264292",
    "end": "2269530"
  },
  {
    "text": "It's true. And that's just a\nsmall sort of deviation where they sort of turn off\nthe stability of the control",
    "start": "2269530",
    "end": "2276500"
  },
  {
    "text": "system for long enough. And they can recover. Their controller\nis robust enough in the flat on the\nground phase that they",
    "start": "2276500",
    "end": "2282380"
  },
  {
    "text": "can catch small\ndisturbances which are their uncontrolled\naerial phase.",
    "start": "2282380",
    "end": "2287930"
  },
  {
    "text": "So for the most part, they keep\ntheir foot flat on the ground. They assume that their foot\nis bolted to the ground, which",
    "start": "2287930",
    "end": "2294260"
  },
  {
    "text": "would make them fully actuated. Right? And then they do a lot\nof work to make sure that that assumption\nstays valid.",
    "start": "2294260",
    "end": "2299325"
  },
  {
    "text": "So they're constantly estimating\nthe center of pressure of that foot and trying to\nkeep it inside the foot, which",
    "start": "2299325",
    "end": "2304340"
  },
  {
    "text": "means the foot will not tip. And this is if you've heard\nof ZMP control, that's the ZMP control idea.",
    "start": "2304340",
    "end": "2309450"
  },
  {
    "text": "OK? And then they do good\nrobotics in between there. They're designing desired\ntrajectories carefully.",
    "start": "2309450",
    "end": "2315050"
  },
  {
    "text": "They're keeping the knees\nbent to avoid singularities. They're doing some--\ndepends on the story.",
    "start": "2315050",
    "end": "2320120"
  },
  {
    "text": "I've heard good\nclaims that they do very smart adaptive\ntrajectory tracking control.",
    "start": "2320120",
    "end": "2325490"
  },
  {
    "text": "I've heard more recently\nthat they just do PD control. And that's good enough because\nthey've got these enormous gear ratios. And that's good enough.",
    "start": "2325490",
    "end": "2333319"
  },
  {
    "text": "OK. So you've seen ASIMO working. ",
    "start": "2333320",
    "end": "2339770"
  },
  {
    "text": "The problem with it is that\nit's really inefficient. Right? Uses way too much energy.",
    "start": "2339770",
    "end": "2344900"
  },
  {
    "text": "Walks slowly. And has no robustness. Right? I've told you that story. ",
    "start": "2344900",
    "end": "2352170"
  },
  {
    "text": "Here's one view of everything\nwe've been doing in this class. The fundamental thing\nthat ASIMO is not doing in its control system\nis thinking about the future.",
    "start": "2352170",
    "end": "2362390"
  },
  {
    "text": "OK? So if you were taking a\nreinforcement learning class, you would have started off with\ntalking about delayed reward.",
    "start": "2362390",
    "end": "2368550"
  },
  {
    "text": "And that's what makes the\nlearning problem difficult. Right? I didn't use the words\ndelayed reward in this class.",
    "start": "2368550",
    "end": "2373892"
  },
  {
    "text": "But it's actually\nexactly the same thing. The fact that we're\noptimizing a cost function over some\ninterval into the future",
    "start": "2373892",
    "end": "2381530"
  },
  {
    "text": "means that I'm thinking\nabout the future. I'm planning over the future. I'm doing long term planning.",
    "start": "2381530",
    "end": "2387740"
  },
  {
    "text": "And if you think about having to\nwait to the end of that future to figure out if what\nyou did made sense, that's the delayed\nreward problem.",
    "start": "2387740",
    "end": "2394100"
  },
  {
    "text": "It's exactly the thing that\nreinforcement learning folks use to convince other people\nthat reinforcement learning is",
    "start": "2394100",
    "end": "2400820"
  },
  {
    "text": "hard. OK? So the problem in\nwalking is that you could do better if you\nstopped just trying to be",
    "start": "2400820",
    "end": "2406190"
  },
  {
    "text": "fully actuated all the time. We start thinking\nabout the future. Think about long term\nstability instead of trying to be fully actuated.",
    "start": "2406190",
    "end": "2411599"
  },
  {
    "text": "OK?  The hoppers, there are examples\nof really dynamically dexterous",
    "start": "2411600",
    "end": "2418560"
  },
  {
    "text": "locomotion. But there's not general\nsolutions to that. That's what this class\nhas been trying to go for.",
    "start": "2418560",
    "end": "2424720"
  },
  {
    "text": "So we do optimal control. We would love to have\nanalytical approximations",
    "start": "2424720",
    "end": "2430079"
  },
  {
    "text": "for optimal control for\nfull humanoids like ASIMO. Love to have it. Don't have it. We're not even close.",
    "start": "2430080",
    "end": "2436589"
  },
  {
    "text": "You know the tools\nthat we have now. But even if we did have an\nanalytical approximation of optimal control-- maybe\nwe will in a few years,",
    "start": "2436590",
    "end": "2443113"
  },
  {
    "text": "who knows-- we'd still like\nto have learning. Right? All this model free\nstuff is still valuable",
    "start": "2443113",
    "end": "2449692"
  },
  {
    "text": "because, if the world\nchanges, you'd like to adapt. Right? So my thesis was\nbasically about trying",
    "start": "2449692",
    "end": "2456030"
  },
  {
    "text": "to show that I could\ndo online optimization on a real system in real time.",
    "start": "2456030",
    "end": "2462640"
  },
  {
    "text": "And I told you about\nAndrews Helicopters. There's a lot of\nwork on Sony Dogs that do loop trajectory\noptimization from trial",
    "start": "2462640",
    "end": "2469700"
  },
  {
    "text": "and error. So Sony came out. And they had this\nsort of walking gait. Right? And then people start\nusing them for soccer.",
    "start": "2469700",
    "end": "2475020"
  },
  {
    "text": "And they said, how fast\ncan we make this thing go? It turns out the\nfastest thing they do on an IBO is to make it\nwalk on its knees like this.",
    "start": "2475020",
    "end": "2480540"
  },
  {
    "text": "And they found that from\na policy gradient search where they basically made\nthe dog walk back and forth between sort of a pink\ncone and a blue cone,",
    "start": "2480540",
    "end": "2486600"
  },
  {
    "text": "just back and forth all day\nlong doing policy gradient. And they figured out this\nis a nice fast way to go.",
    "start": "2486600",
    "end": "2492870"
  },
  {
    "text": "And then they won the\nsoccer competition. [LAUGHTER] Not actually sure if\nthat last part is true. I don't know who won.",
    "start": "2492870",
    "end": "2498215"
  },
  {
    "text": "But I'd like to think it's true. There are people that do\na lot of walking robots.",
    "start": "2498215",
    "end": "2503890"
  },
  {
    "text": "I think I showed you the\nUNH bipeds that were some of the first learning bipeds.",
    "start": "2503890",
    "end": "2510150"
  },
  {
    "text": "Right? I told you about these all term. Right? So there's large continuously\nin action spaces, complex dynamics.",
    "start": "2510150",
    "end": "2516600"
  },
  {
    "text": "We want to minimize\nthe number of trials. The dynamics are\ntough for walking.",
    "start": "2516600",
    "end": "2522357"
  },
  {
    "text": "Because of the collisions. And there's this delayed reward. So in my thesis,\nthe thing I did was",
    "start": "2522357",
    "end": "2527790"
  },
  {
    "text": "tried to build a robot\nthat learned well. That was my goal. I simultaneously designed\na good learning system",
    "start": "2527790",
    "end": "2533538"
  },
  {
    "text": "but also built a robot where\nlearning would work really well. Instead of working on ASIMO,\nI worked on this little dinky thing I call Toddler.",
    "start": "2533538",
    "end": "2539340"
  },
  {
    "text": "Yeah? And I spent a lot of time\non that little robot.",
    "start": "2539340",
    "end": "2544510"
  },
  {
    "text": "So you know about\npassive walking. ",
    "start": "2544510",
    "end": "2552960"
  },
  {
    "text": "This is the simplest, this\nis the first passive walker I built. Passive walking 101 here.",
    "start": "2552960",
    "end": "2558870"
  },
  {
    "text": "So it's sort of a funny story. I mean, I was in a\nneuroscience lab. I worked with the Leg Lab. But my advisor was\nin neuroscience.",
    "start": "2558870",
    "end": "2566060"
  },
  {
    "text": "They spent lots of money on\nmicroscopes and lots of money. So at some point, I said, can\nI spend a little bit of money",
    "start": "2566060",
    "end": "2571940"
  },
  {
    "text": "on a machine shop? And I promise it'll cost\nless than that lens you just spent on that one microscope?",
    "start": "2571940",
    "end": "2577033"
  },
  {
    "text": "And so, he gave me a little\nbit of money to go down. I was basically in a closet\nat the end of the hall.",
    "start": "2577033",
    "end": "2582057"
  },
  {
    "text": "My tools looked like\nthings like this. Like, I couldn't even afford\nanother piece of rubber when I cut off a corner.",
    "start": "2582057",
    "end": "2588019"
  },
  {
    "text": "And that's actually a CD rack\nthat I got rid of somewhere. And that's my little\nwooden ramp that I",
    "start": "2588020",
    "end": "2593089"
  },
  {
    "text": "was using for passive walking. But I built these\nlittle passive walkers with a little sureline CNC\nmill that walked stably",
    "start": "2593090",
    "end": "2603680"
  },
  {
    "text": "in 3D down a small ramp. Yeah?  I don't know why\nit's playing badly.",
    "start": "2603680",
    "end": "2609080"
  },
  {
    "text": " So that was the first steps.",
    "start": "2609080",
    "end": "2614300"
  },
  {
    "text": "If we're going to do\nwalking, it's not hard. Those feet are\nactually CNC-ed out.",
    "start": "2614300",
    "end": "2620299"
  },
  {
    "text": "I spent a lot of\ntime on those feet. They're a curvature that\nwas designed carefully to get stability.",
    "start": "2620300",
    "end": "2626059"
  },
  {
    "text": "STUDENT: It's just a\nsimple [INAUDIBLE].. RUSS TEDRAKE: Yeah. Just a pin joint. That's a walking robot.",
    "start": "2626060",
    "end": "2634069"
  },
  {
    "text": "At the time, people had been\nworking on passive walkers for a long time. But nobody had sort of\ndone the obvious thing,",
    "start": "2634070",
    "end": "2641108"
  },
  {
    "text": "which is add a few motors\nand make it walk on the flat. Nobody had done it. So that's what I set out\nto do with the learning.",
    "start": "2641108",
    "end": "2646640"
  },
  {
    "text": " Turns out a few people did\nit around the same time.",
    "start": "2646640",
    "end": "2651790"
  },
  {
    "text": "So we wrote a paper together. But the basic story was we went\nfrom this simple thing that",
    "start": "2651790",
    "end": "2657570"
  },
  {
    "text": "was passive to the\nactuated version. The hip joint here on this\nrobot is still passive.",
    "start": "2657570",
    "end": "2662610"
  },
  {
    "text": "OK? Put actuators in at the ankle. So we had a new degrees\nof freedom with actuators so that it could\npush off the ground",
    "start": "2662610",
    "end": "2669210"
  },
  {
    "text": "but still keep its\nmostly passive gait. Actually, it's extruded\nstock here stacked",
    "start": "2669210",
    "end": "2675180"
  },
  {
    "text": "with gyros and rate gyros\nand all the kinds of sensors. It's got a 700 megahertz\nPentium in its belly,",
    "start": "2675180",
    "end": "2683627"
  },
  {
    "text": "which kind of stung. In retrospect, I couldn't make\nvery many efficiency arguments about the robot because it's\ncarrying a computer the size",
    "start": "2683627",
    "end": "2688830"
  },
  {
    "text": "of a desktop at the time. You know? And so, there's five\nbatteries total on the system.",
    "start": "2688830",
    "end": "2694000"
  },
  {
    "text": "Right? Those four are\npowering the computer. There's one little one in there\nthat's powering the motors.",
    "start": "2694000",
    "end": "2699119"
  },
  {
    "text": "And still those big\nfour drained like 50% faster than the other ones.",
    "start": "2699120",
    "end": "2705070"
  },
  {
    "text": "But it's computationally\npowerful. Right? I actually ran a little\nweb server off there just because I\nthought it was funny.",
    "start": "2705070",
    "end": "2710400"
  },
  {
    "text": "[LAUGHTER] And the arms look like I've\nadded degrees of freedom. But actually,\nthey're mechanically",
    "start": "2710400",
    "end": "2716790"
  },
  {
    "text": "attached to the opposite leg. So when I move this,\nthat bar across the front was making that coupling\nhappen, which is",
    "start": "2716790",
    "end": "2722940"
  },
  {
    "text": "important for the 3D walking. Because if you\nwant to walk down, if you have no arms actually\nand you swing a big heavy foot,",
    "start": "2722940",
    "end": "2728630"
  },
  {
    "text": "then you're going to\nget a big yaw moment. And the robots\noften walk like this and went off the\nside of the ramp.",
    "start": "2728630",
    "end": "2733700"
  },
  {
    "text": "So you put the big batteries\non the side and then everything walks straight. And it's good. So in total, there's\nnine degrees of freedom",
    "start": "2733700",
    "end": "2740260"
  },
  {
    "text": "if you count all the things\nthat could possibly move. And there's four motors\nto do the controls. So that's under-actuated.",
    "start": "2740260",
    "end": "2745500"
  },
  {
    "text": "Right?  We've got the robot dynamics. Oops. I've used a Mac now.",
    "start": "2745500",
    "end": "2751520"
  },
  {
    "text": "I used to use a Windows. So apparently my u is now O hat. [LAUGHTER]",
    "start": "2751520",
    "end": "2757540"
  },
  {
    "text": "Sorry. That's actually tau. OK. So tau. Yeah. So I had most almost the\nmanipulator equations.",
    "start": "2757540",
    "end": "2764910"
  },
  {
    "text": "But I had to go through\nthis little hobby servo. So it wasn't quite the\nmanipulator equation.",
    "start": "2764910",
    "end": "2771210"
  },
  {
    "text": "And the goal was to find a\ncontrol policy pi that was-- so it was already stable\ndown a small ramp.",
    "start": "2771210",
    "end": "2777182"
  },
  {
    "text": "And the way I formulated\nthe problem is I wanted to take that\nsame limit cycle that I could find\nexperimentally down a ramp",
    "start": "2777182",
    "end": "2783628"
  },
  {
    "text": "and make it so it worked\non whatever slope. So make that return map\ndynamics invariant to slope.",
    "start": "2783628",
    "end": "2790020"
  },
  {
    "text": "And to do that, you\nneed to add energy. And you need to find\na control policy. So my goal was to find this\npi, stabilize the limit cycle",
    "start": "2790020",
    "end": "2799050"
  },
  {
    "text": "solution that I saw downhill\nto make it work on any slope.",
    "start": "2799050",
    "end": "2804460"
  },
  {
    "text": "So this was just showing that\nToddler, with its computer",
    "start": "2804460",
    "end": "2809609"
  },
  {
    "text": "turned off, its motors\nare turned on-- actually, this one is even\nthe motors are off. And there's just little\nsplints on the ankle.",
    "start": "2809610",
    "end": "2816000"
  },
  {
    "text": "Just showing that it was\nalso a passive walker. And showing that I dramatically\nimproved my hardware experience",
    "start": "2816000",
    "end": "2821310"
  },
  {
    "text": "by getting a little\nproform treadmill that was off of the back lot. And I painted it\nyellow and stuff.",
    "start": "2821310",
    "end": "2828240"
  },
  {
    "text": "So this thing would\nactually walk all day long. It would. So it's a little trick.",
    "start": "2828240",
    "end": "2835589"
  },
  {
    "text": "At the very edge, in the middle,\nthere's nothing going on. But at the very edge\nof the treadmill, I put a little lip there.",
    "start": "2835590",
    "end": "2841150"
  },
  {
    "text": "So if it happened to wander\nitself over to the side, it had that lip and walked\nback towards the middle. OK? And I put a little wedge on\nthe front and on the back",
    "start": "2841150",
    "end": "2848550"
  },
  {
    "text": "so it sort of would try to stay\nin the middle of the treadmill. And that thing would\njust walk all day long. It would drive you crazy hearing\nthose footsteps all day long.",
    "start": "2848550",
    "end": "2857400"
  },
  {
    "text": "[LAUGHTER] But it worked. It worked well. It still works today,\nmost of the time. ",
    "start": "2857400",
    "end": "2864600"
  },
  {
    "text": "So I use the words\npolicy gradient. But this was really an\nactor critic algorithm.",
    "start": "2864600",
    "end": "2870870"
  },
  {
    "text": "So I used linear, it's actually\na very centric grid in phi. But a linear function\napproximator.",
    "start": "2870870",
    "end": "2877260"
  },
  {
    "text": "And the basic story\nwas policy gradient. OK? So it was something in\nbetween this perfectly online",
    "start": "2877260",
    "end": "2884099"
  },
  {
    "text": "at every dt, make an update. And it was not quite the\nepisodic run a trial,",
    "start": "2884100",
    "end": "2890160"
  },
  {
    "text": "stop, run a trial, stop. The cost function was\nreally a long term cost.",
    "start": "2890160",
    "end": "2896310"
  },
  {
    "text": "But I did it once per footstep. OK? So every time the robot\nliterally took a footstep,",
    "start": "2896310",
    "end": "2901980"
  },
  {
    "text": "I would make a small change\nto the policy parameters. See how well it walked. See where it hit the return map.",
    "start": "2901980",
    "end": "2907830"
  },
  {
    "text": "And then change the\nparameters again. Change the parameters again. And every time that\nfoot hit the ground, I would evaluate the change\nin walking performance",
    "start": "2907830",
    "end": "2915900"
  },
  {
    "text": "and make the change in W\nbased on that result. OK. I'll show you the algorithm\nthat I used a second, which",
    "start": "2915900",
    "end": "2921060"
  },
  {
    "text": "you'll now recognize. So the way to think\nabout that sampling in W is that you're estimating\nthe policy gradient.",
    "start": "2921060",
    "end": "2926980"
  },
  {
    "text": "And you're performing online\nstochastic gradient descent. Right? So the time, the way I\ndescribed the big challenge",
    "start": "2926980",
    "end": "2933600"
  },
  {
    "text": "is, what is the cost\nfunction for walking? And how do you achieve\nfast provable convergence,",
    "start": "2933600",
    "end": "2938850"
  },
  {
    "text": "despite noisy\ngradient estimates? You guys know about return maps.",
    "start": "2938850",
    "end": "2943860"
  },
  {
    "text": "This is my picture of return\nmaps from a long time ago. ",
    "start": "2943860",
    "end": "2949810"
  },
  {
    "text": "So this is the Van\nder Pol Oscillator. This is the return map here. The important point\nhere, so this is",
    "start": "2949810",
    "end": "2956609"
  },
  {
    "text": "the samples on the return map. This is the velocity at the n-th\ncrossing versus the velocity at the n-th plus 1 crossing.",
    "start": "2956610",
    "end": "2962460"
  },
  {
    "text": "The blue line is the\nline of slope one. So it's stable, the\nVan der Pol Oscillator,",
    "start": "2962460",
    "end": "2967590"
  },
  {
    "text": "because it's above the line\nhere and below the line there. And you can evaluate\nlocal stability by linearizing and\ntaking the eigenvalues.",
    "start": "2967590",
    "end": "2974010"
  },
  {
    "text": "We've talked about these things. But I don't know if I made\nthe point nicely before.",
    "start": "2974010",
    "end": "2979620"
  },
  {
    "text": "That if you can pick anything,\nif you want your return map to look like\nanything in the world, if you could pick,\nwhat would you pick?",
    "start": "2979620",
    "end": "2985860"
  },
  {
    "text": "You'd pick a flat line. Right? That's the deadbeat controller. I used the word deadbeat.",
    "start": "2985860",
    "end": "2992790"
  },
  {
    "text": "So that's where my cost\nfunction came from. The cost function that tried to\nsay that the robot was walking",
    "start": "2992790",
    "end": "2997860"
  },
  {
    "text": "well-- ",
    "start": "2997860",
    "end": "3003000"
  },
  {
    "text": "wow-- penalized my\ninstantaneous cost",
    "start": "3003000",
    "end": "3008370"
  },
  {
    "text": "function, penalized the square\ndistance between my sample on the return map and\nthe desired return map,",
    "start": "3008370",
    "end": "3015450"
  },
  {
    "text": "which is that green line. OK. So basically I wanted, I\ntried to drive the system to have a deadbeat controller.",
    "start": "3015450",
    "end": "3022289"
  },
  {
    "text": "And I did, and there's limits. There's actuator limits\nthat's going to mean it's never going to get there. But my cost function was\ntrying to force that.",
    "start": "3022290",
    "end": "3027330"
  },
  {
    "text": "Every time I got\na sample, it was trying to push that\nsample more towards the deadbeat controller.",
    "start": "3027330",
    "end": "3032517"
  },
  {
    "start": "3032517",
    "end": "3038720"
  },
  {
    "text": "Then basically, it worked. It worked really well. The robot began walking\nin one minute, which",
    "start": "3038720",
    "end": "3044960"
  },
  {
    "text": "means it started getting\nits foot cleared. So the first thing, if\nI set W equal to zero,",
    "start": "3044960",
    "end": "3050210"
  },
  {
    "text": "it was configured so that when\nthe policy parameters were zero, it was a passive walker. So I put it on flat.",
    "start": "3050210",
    "end": "3056000"
  },
  {
    "text": "I picked it up. I picked it up a lot. And I drop it. It runs out of energy\nand stands still.",
    "start": "3056000",
    "end": "3061310"
  },
  {
    "text": "Because it was just\na passive walker, it's not getting energy from-- it's only losing energy. OK?",
    "start": "3061310",
    "end": "3066560"
  },
  {
    "text": "So now, I pick it up. I drop it. And every time it takes a step,\nit's twiddling the parameters",
    "start": "3066560",
    "end": "3072897"
  },
  {
    "text": "at the ankle a little bit. OK? So it started going\nlike this a little bit. And then after about a minute\nof dropping it-- and quickly,",
    "start": "3072897",
    "end": "3079580"
  },
  {
    "text": "I wrote a script that would\nkick it into place so I stopped dropping it-- OK. So I gave a little script\nso it would go like this.",
    "start": "3079580",
    "end": "3084585"
  },
  {
    "text": "And in about one minute, it\nwas sort of marching in place. OK? And then I started\ndriving it around.",
    "start": "3084585",
    "end": "3089970"
  },
  {
    "text": "I had a little\njoystick which said, I want your desired\nbody to go like this. And it started walking around. And in about five minutes, it\nwas sort of walking around.",
    "start": "3089970",
    "end": "3097860"
  },
  {
    "text": "I'll show you the\nvideo here in a second. And then, I said 20\nminutes for convergence. That was conservative. Most of the time,\nit was 10 minutes.",
    "start": "3097860",
    "end": "3104120"
  },
  {
    "text": "It would converge to the\npolicy that was locally optimal in this policy class.",
    "start": "3104120",
    "end": "3109550"
  },
  {
    "text": "But it worked very well. And I just sort of sent\nit off down the hall. And it would walk. OK?",
    "start": "3109550",
    "end": "3115640"
  },
  {
    "text": "And doing the\nstability analysis, it showed the learn controllers\nis considerably more stable",
    "start": "3115640",
    "end": "3121160"
  },
  {
    "text": "than the controllers I\ndesigned by hand, which I spent a long time on those, too. ",
    "start": "3121160",
    "end": "3128839"
  },
  {
    "text": "And now, here's a\nreally key point. OK? So you might ask, how much is\nthis sort of approximate value",
    "start": "3128840",
    "end": "3138587"
  },
  {
    "text": "function, how important is that? That's sort of the\ntopic for today. Right? How important is this\napproximate value function?",
    "start": "3138587",
    "end": "3144940"
  },
  {
    "text": "Well, it turns out, if I\nwere to reset the policy, if I just set the policy\nparameters to zero again",
    "start": "3144940",
    "end": "3151977"
  },
  {
    "text": "but keep the value function\nfrom the previous time it learned, then the whole\nthing speeds up dramatically.",
    "start": "3151977",
    "end": "3159500"
  },
  {
    "text": "So instead of converging\nin 20 minutes, the thing converges\nin like two minutes. OK? So just by virtue of having\na good value estimate there,",
    "start": "3159500",
    "end": "3168640"
  },
  {
    "text": "learning goes\ndramatically faster. And it's only when\nI have to learn them both simultaneously\nthat it takes more",
    "start": "3168640",
    "end": "3173830"
  },
  {
    "text": "like 10 or 20 minutes. And it worked so fast that\nI never built a robot.",
    "start": "3173830",
    "end": "3179140"
  },
  {
    "text": "I never built a\nmodel for the robot. Actually, I tried later. It's tough. The dynamics of that--",
    "start": "3179140",
    "end": "3185500"
  },
  {
    "text": "I mean, it's a curved foot\nwith rubber on it, right? It was just very hard\nto model accurately.",
    "start": "3185500",
    "end": "3194405"
  },
  {
    "text": "And I didn't need to. It worked. It learned very quickly. Quickly enough that it was\nadapting to the terrain",
    "start": "3194405",
    "end": "3200650"
  },
  {
    "text": "as it walked. All right. So here's the Poincar maps\nfrom that little Toddler robot projected onto a plane.",
    "start": "3200650",
    "end": "3207940"
  },
  {
    "text": "So I picked it up\na bunch of times. I tried to make it just\nwalk in place here.",
    "start": "3207940",
    "end": "3213460"
  },
  {
    "text": "Before learning,\nit was obviously only stable at the\nzero, zero fix point. It was running out of energy on\nevery step and going to zero.",
    "start": "3213460",
    "end": "3221589"
  },
  {
    "text": "After learning, this is what\nthe return map looked like. OK?",
    "start": "3221590",
    "end": "3227170"
  },
  {
    "text": "So it actually could start\nfrom stopped reliably. Right? This is actually far better\nthan I expected it to do.",
    "start": "3227170",
    "end": "3235290"
  },
  {
    "text": "If you do your little\nstaircase analysis of this, so it gets up to the fixed point\nin two steps or three steps",
    "start": "3235290",
    "end": "3244780"
  },
  {
    "text": "for most initial conditions. Right? And from a very large range\nof initial conditions,",
    "start": "3244780",
    "end": "3249819"
  },
  {
    "text": "as large as I care\nto sample from. So you could go up there--\nand people did actually. We had a little--",
    "start": "3249820",
    "end": "3255579"
  },
  {
    "text": "after we got it\nworking, the press came. And then everybody was asking\nme, the reporters were saying,",
    "start": "3255580",
    "end": "3261887"
  },
  {
    "text": "can I have my kid\nplay with the robot? Or can we put on a\ntreadmill at the gym? Rich Sutton put his\nfingers under it",
    "start": "3261887",
    "end": "3269170"
  },
  {
    "text": "and was like playing\nwith it at dips one time. So it got disturbed\nin every possible way.",
    "start": "3269170",
    "end": "3274270"
  },
  {
    "text": "And for the most part,\nit worked really-- I mean, so if you give\nit a big push this way, it actually takes energy out\nand comes back and recovers",
    "start": "3274270",
    "end": "3281560"
  },
  {
    "text": "in two steps. You stop it. It goes back up. And it recovers. And in the worst case, I had\nsome demo to give or something.",
    "start": "3281560",
    "end": "3290710"
  },
  {
    "text": "And I took it out of the case. It had traveled\nthrough the airport. The customs people always asked\nme if it had commercial value.",
    "start": "3290710",
    "end": "3299140"
  },
  {
    "text": "It doesn't have\ncommercial value.  But it broke somewhere\nin the travel.",
    "start": "3299140",
    "end": "3305380"
  },
  {
    "text": "And I didn't realize it. I picked it up and\nheaded to do its demo. And it's going like this. And it's sort of walking.",
    "start": "3305380",
    "end": "3311103"
  },
  {
    "text": "And it looks a little funny. And people are so\nrelatively happy with it. Turns out the ankle\nhad completely snapped.",
    "start": "3311103",
    "end": "3316150"
  },
  {
    "text": "But in just a few\nsteps, it actually found a policy that was\nwalking with a broken ankle. [LAUGHTER]",
    "start": "3316150",
    "end": "3321640"
  },
  {
    "text": "So it works. It really worked. It really did work. I'm not sure-- I mean, yeah.",
    "start": "3321640",
    "end": "3327789"
  },
  {
    "text": "It really worked. OK. So here's the basic video. This was the beginning.",
    "start": "3327790",
    "end": "3333704"
  },
  {
    "text": "I was paranoid. So I had pads on it to make sure\nit didn't fall down and break. This is the little policy\nthat would kick it up",
    "start": "3333705",
    "end": "3339520"
  },
  {
    "text": "into a random initial\ncondition like that. And now it's learning. It falls down.",
    "start": "3339520",
    "end": "3344569"
  },
  {
    "text": "I don't know why it's\nplaying so badly. This is after a few minutes. It's stepping in place.",
    "start": "3344570",
    "end": "3349720"
  },
  {
    "text": "It's walking. ",
    "start": "3349720",
    "end": "3354820"
  },
  {
    "text": "And then I started\ndriving it around. I say, OK. Let's walk around. And it stumbles.",
    "start": "3354820",
    "end": "3360070"
  },
  {
    "text": "But really, really fast,\nit learned a policy that could stabilize it. ",
    "start": "3360070",
    "end": "3367210"
  },
  {
    "text": "Right? And after a few minutes, this\nis the disturbance tests.",
    "start": "3367210",
    "end": "3373600"
  },
  {
    "text": "I actually haven't shown\nthese in a long time. ",
    "start": "3373600",
    "end": "3379750"
  },
  {
    "text": "It's really robust\nto those things. And then you can send\nit off down the hall. And now, this is a little\nrobot with big feet admittedly.",
    "start": "3379750",
    "end": "3387960"
  },
  {
    "text": "But you know, it's like\nthe linoleum in E25-- this is in E25--\nwas really not flat.",
    "start": "3387960",
    "end": "3395160"
  },
  {
    "text": "I mean, it's sort of\nembarrassing to tell people, look at the floor. It's not flat. But for that robot, I mean\nthere's huge disturbances",
    "start": "3395160",
    "end": "3402160"
  },
  {
    "text": "as it walked down the floor. But the policy parameters\nwere changing quite a bit.",
    "start": "3402160",
    "end": "3408539"
  },
  {
    "text": "You could walk off\ntile onto carpet. And in a few steps, it\nwould adjust its parameters and keep on walking.",
    "start": "3408540",
    "end": "3414090"
  },
  {
    "text": "This was it walking from\nE25 towards the Media Lab, if you recognize that. ",
    "start": "3414090",
    "end": "3431349"
  },
  {
    "text": "OK. So one of the things\nI said is that one of the problems with\nthe value estimate is you make a small change\nin the value function,",
    "start": "3431350",
    "end": "3437462"
  },
  {
    "text": "you get a big change\nin the policy. Theoretically, no problem.",
    "start": "3437462",
    "end": "3442740"
  },
  {
    "text": "In practice, you don't\nprobably want that. Right? One of the beautiful things\nabout the policy gradient algorithms is you make a\nsmall change to the policy.",
    "start": "3442740",
    "end": "3450573"
  },
  {
    "text": "It doesn't look like the\nrobot's doing crazy things. So every time,\neverything you saw there, it was always learning. Right?",
    "start": "3450573",
    "end": "3455970"
  },
  {
    "text": "Learning did not look\nlike a big deviation from nominal behavior. I never turned off\nlearning with this. Right?",
    "start": "3455970",
    "end": "3461370"
  },
  {
    "text": "It turned out in the\npolicy gradient setting, I could add such a\nsmall amount of noise to the policy parameters,\nwhich was a very central grid",
    "start": "3461370",
    "end": "3468299"
  },
  {
    "text": "over the state space, such\na small amount of noise that you couldn't even\ntell it was learning.",
    "start": "3468300",
    "end": "3475060"
  },
  {
    "text": "Right? But it was enough to pull\nout a gradient estimate and keep going. So it didn't look like it\nwas trying random things.",
    "start": "3475060",
    "end": "3480735"
  },
  {
    "text": "But then, if it walked off on\nthe carpet and did a bad thing, it would still adapt. That was something\nI didn't expect.",
    "start": "3480735",
    "end": "3486540"
  },
  {
    "text": "It just was a very\nnice sort of match between the amount of\nnoise you had to add and the speed of learning.",
    "start": "3486540",
    "end": "3494550"
  },
  {
    "text": "The value estimate was a low\ndimensional approximation of the value function. Very low.",
    "start": "3494550",
    "end": "3500023"
  },
  {
    "text": "Like ridiculously low. One dimension. Right? But it was sufficient\nto decrease the variance and allow fast convergence.",
    "start": "3500023",
    "end": "3506240"
  },
  {
    "text": "I never got it to work before\nI put a value function in. ",
    "start": "3506240",
    "end": "3511530"
  },
  {
    "text": "And here's this question. So I ended up choosing\ngamma to be pretty low. Gamma was 0.2.",
    "start": "3511530",
    "end": "3518099"
  },
  {
    "text": "I did try with zero times. What did that mean? So that's how far I carried\nback my eligibility, which",
    "start": "3518100",
    "end": "3524280"
  },
  {
    "text": "means how many steps\nam I looking at it. So that you could think of it\nas a receding horizon optimal control. How many steps\nahead do you look?",
    "start": "3524280",
    "end": "3530910"
  },
  {
    "text": "Right. Except it's discounted. OK? So 0.2 is really\nheavy discounted.",
    "start": "3530910",
    "end": "3537445"
  },
  {
    "text": "Really, really heavy. It means I was basically\nlooking one step ahead and not worrying\nabout things well",
    "start": "3537445",
    "end": "3543570"
  },
  {
    "text": "into the future, which made\nmy learning faster but meant I didn't take really aggressive\ncorrections that were",
    "start": "3543570",
    "end": "3549660"
  },
  {
    "text": "multi-step sort of corrections. Only very rarely, if the\ncost really warranted it. OK.",
    "start": "3549660",
    "end": "3555247"
  },
  {
    "text": "So that was always\nsomething I thought would be cool if I\ncould get that higher and show a reason why\nmulti-step corrections made",
    "start": "3555247",
    "end": "3562349"
  },
  {
    "text": "it a lot more stable. STUDENT: Did it\nnot work as well? RUSS TEDRAKE: It\ndidn't learn as fast.",
    "start": "3562350",
    "end": "3568809"
  },
  {
    "text": "At some point, I\ndecided I'm going to try to make the point\nthat these things can really learn fast. And so, I started\nturning all the knobs.",
    "start": "3568810",
    "end": "3574890"
  },
  {
    "text": "Simple policy, simple value\nfunction, low look ahead. And it worked.",
    "start": "3574890",
    "end": "3580380"
  },
  {
    "text": "But it was fast. STUDENT: Is gamma used\n[INAUDIBLE] the same as lambda?",
    "start": "3580380",
    "end": "3589359"
  },
  {
    "text": "RUSS TEDRAKE: It's a gamma in a\ndiscounted reward formulation.",
    "start": "3589360",
    "end": "3595070"
  },
  {
    "text": "STUDENT: So there is\nno eligibility trace? RUSS TEDRAKE: The\neligibility trace for the reinforce in\na discounted problem",
    "start": "3595070",
    "end": "3601142"
  },
  {
    "text": "is the same as the\ndiscount factor. ",
    "start": "3601142",
    "end": "3610990"
  },
  {
    "text": "So in my lab now,\nwe're doing a lot of these model based things. We're doing LQR trees. We're doing a lot of things.",
    "start": "3610990",
    "end": "3616617"
  },
  {
    "text": "In fact, the linear\ncontrols are working so beautifully in simulation\nthat Rick Corey, one",
    "start": "3616617",
    "end": "3623250"
  },
  {
    "text": "of our guys, started joshing me. He's like, why didn't you\njust do LQR on Toddler?",
    "start": "3623250",
    "end": "3628680"
  },
  {
    "text": "And he was giving me a\nhard time for a long time. Now he's asking about\nmodel free methods again because it's really hard\nto get a good model of very",
    "start": "3628680",
    "end": "3639450"
  },
  {
    "text": "underactuated systems. I mean, the plane that I'll\ntell you about more on Thursday,",
    "start": "3639450",
    "end": "3644700"
  },
  {
    "text": "our perching plane we've seen\nquickly, is one actuator. And depending on how\nyou count the elevator,",
    "start": "3644700",
    "end": "3652110"
  },
  {
    "text": "eight degrees of\nfreedom roughly. And sorry, eight\nstate variables.",
    "start": "3652110",
    "end": "3658530"
  },
  {
    "text": "And it's just very, very\nhard to build a good model for that that's accurate for the\nlong trajectory, the trajectory",
    "start": "3658530",
    "end": "3666600"
  },
  {
    "text": "all the way to the perch\nsuch that LQR could just stabilize it. We're trying. But there's something sort of\nbeautiful about these things",
    "start": "3666600",
    "end": "3673470"
  },
  {
    "text": "that just work without\nbuilding a perfect model. OK? ",
    "start": "3673470",
    "end": "3680820"
  },
  {
    "text": "The big picture is\nroughly the class you saw. This is actually, I had\nforgotten about this.",
    "start": "3680820",
    "end": "3686250"
  },
  {
    "text": "This was one of my backup\nslides from before. But this is the basic\nlearning plot, which",
    "start": "3686250",
    "end": "3693330"
  },
  {
    "text": "is just one average run here. If I reset the\nlearning parameters,",
    "start": "3693330",
    "end": "3698940"
  },
  {
    "text": "how quickly would it minimize\nthe average one step error? And it was pretty fast.",
    "start": "3698940",
    "end": "3704020"
  },
  {
    "text": "And then actually,\nthat's a lot of steps. That's more than I remember. But this takes\nsteps once a second.",
    "start": "3704020",
    "end": "3710040"
  },
  {
    "text": "And so, in a handful of minutes,\nit does hundreds of steps. OK. And this is the policy in two\ndimensions that it learned.",
    "start": "3710040",
    "end": "3718320"
  },
  {
    "text": "So if you think about a theta\nrole and theta role dot,",
    "start": "3718320",
    "end": "3724500"
  },
  {
    "text": "I don't know if you have\nintuition about this, but the sort of yin\nand yang of the Toddler",
    "start": "3724500",
    "end": "3732240"
  },
  {
    "text": "was that you wanted to push when\nyou're in this side of the face portrait and push\nwith this foot when",
    "start": "3732240",
    "end": "3738601"
  },
  {
    "text": "you're on this side\nof the face portrait. I did things like I\nmirrored, the left ankle",
    "start": "3738602",
    "end": "3744840"
  },
  {
    "text": "was doing the inverse, the\nmirror of the right ankle. Right? So everything I could do\nto try to minimize the size",
    "start": "3744840",
    "end": "3752549"
  },
  {
    "text": "of the function I was learning. And that's actually sort\nof a beautiful picture of how it needed to push in\norder to stabilize and skate.",
    "start": "3752550",
    "end": "3762510"
  },
  {
    "start": "3762510",
    "end": "3768008"
  },
  {
    "text": "Any questions about that? ",
    "start": "3768008",
    "end": "3779560"
  },
  {
    "text": "All right. So that's one success story\nfrom model free learning",
    "start": "3779560",
    "end": "3788130"
  },
  {
    "text": "on real robots. It learns in a few minutes. There's other success stories. I'll try to talk about\nmore of them on Thursday.",
    "start": "3788130",
    "end": "3794549"
  },
  {
    "text": "But at this point,\nI've basically given you all the tools that\nwe talk about in research",
    "start": "3794550",
    "end": "3801930"
  },
  {
    "text": "to make these robots tick. Their state estimation,\nI didn't talk about. There's Morse's idea that\nwe didn't talk about.",
    "start": "3801930",
    "end": "3808420"
  },
  {
    "text": "But this is, I've given\nyou a pretty big swath of algorithms here. So really I want to now\nhear from you next week.",
    "start": "3808420",
    "end": "3816000"
  },
  {
    "text": "And I want to give\nyou a few more case studies so you feel that\nthese things actually work in practice.",
    "start": "3816000",
    "end": "3821131"
  },
  {
    "text": "And you can go off and you\nuse them in your research. Yeah, John? STUDENT: If there is a lot of\nstuff that's been published",
    "start": "3821132",
    "end": "3827849"
  },
  {
    "text": "and a lot of interest\n[INAUDIBLE] stochasticity, then it would make sense to\nhave a large gamma [INAUDIBLE]..",
    "start": "3827850",
    "end": "3834790"
  },
  {
    "text": "Right? There'd be no\nreason, it would be a faulty way of trying\nto interpret that data. Right?",
    "start": "3834790",
    "end": "3840228"
  },
  {
    "text": "RUSS TEDRAKE: Yeah. I mean, I think that so Katie's\nstuff, the metastability stuff, argued that for most of\nthese walking systems,",
    "start": "3840228",
    "end": "3846308"
  },
  {
    "text": "it doesn't make sense to look\nvery far in the future anyways. Because the dynamics\nof the system mix with the stochasticity,\nwhich I think is the same thing",
    "start": "3846308",
    "end": "3853230"
  },
  {
    "text": "you just said. Yeah. Yeah. STUDENT: The general dimensions\nof the robot [INAUDIBLE]",
    "start": "3853230",
    "end": "3859700"
  },
  {
    "text": "when you're\ndesigning that robot, thinking about this model free\nlearning when you started?",
    "start": "3859700",
    "end": "3865770"
  },
  {
    "text": "[INAUDIBLE] helps it be\na little more stable. RUSS TEDRAKE: Good. So I'm glad you asked that.",
    "start": "3865770",
    "end": "3873420"
  },
  {
    "text": "So it's definitely\nvery stable, which was experimentally convenient. Right? Because I didn't have\nto pick it up as much.",
    "start": "3873420",
    "end": "3879940"
  },
  {
    "text": "But it actually learns fine\nwhen it starts off unstable. So the way I tested that is, if\nthe ramp was very steep, then",
    "start": "3879940",
    "end": "3885300"
  },
  {
    "text": "it starts oscillating\nand falls off sideways. So just to show that it can\nstabilize and unstabilize. It's like, oh, the\nsame cost function.",
    "start": "3885300",
    "end": "3891623"
  },
  {
    "text": "It's absolutely no different. I showed that it\nstabilized that. And it just meant\nI had to pick it up when it fell down\na bunch of times.",
    "start": "3891623",
    "end": "3897670"
  },
  {
    "text": "But the same algorithm\nworks for that. So it's not really the stability\nthat I was counting on. That was just\nexperimentally nice.",
    "start": "3897670",
    "end": "3903300"
  },
  {
    "text": "The big clown feet\nand everything were because that's how I knew\nhow to tune the passive gait.",
    "start": "3903300",
    "end": "3909630"
  },
  {
    "text": "Right? In the passive walkers\nwe work on these days, you always see point feet. Because I care about\nrough terrain now.",
    "start": "3909630",
    "end": "3915660"
  },
  {
    "text": "And those clown feet are\nnot good for rough terrain. So we could try to\nget rid of that. STUDENT: You're saying if you\nwanted to scale that out, you",
    "start": "3915660",
    "end": "3921968"
  },
  {
    "text": "had mentioned the [INAUDIBLE]\nrobots [INAUDIBLE] would you have the same\nsuccess [INAUDIBLE]??",
    "start": "3921968",
    "end": "3929010"
  },
  {
    "text": "RUSS TEDRAKE: Got you. I think it's fine. I think that it would look\nridiculous that big maybe.",
    "start": "3929010",
    "end": "3935155"
  },
  {
    "text": "And I wouldn't scale\nthe feet quite that big. Right? That would be ridiculous.",
    "start": "3935155",
    "end": "3940840"
  },
  {
    "text": "But I don't think there's any\nscaling issues there really. It's the inertia of the\nrelative links that matters.",
    "start": "3940840",
    "end": "3947170"
  },
  {
    "text": "And I think you can\nscale that properly. At some point\nyou're going to just look ridiculous if you don't\nhave knees and you're that big.",
    "start": "3947170",
    "end": "3953140"
  },
  {
    "text": "So yeah. Energetically, the\nmechanical cost of transport,",
    "start": "3953140",
    "end": "3959900"
  },
  {
    "text": "if you just look at the power\ncoming out of the batteries-- sorry, actually the work\ndone by the actuators, the actual work done\nby the actuators.",
    "start": "3959900",
    "end": "3966043"
  },
  {
    "text": "It was comparable to a human,\n20 times better than ASIMO. But if you plot the current\ncoming out of the batteries,",
    "start": "3966043",
    "end": "3974600"
  },
  {
    "text": "it was three times worse than\nASIMO or something like that. Because it's got these little\nitty bitty steps and really",
    "start": "3974600",
    "end": "3980060"
  },
  {
    "text": "big computer. And that was, in retrospect,\nmaybe not the best decision. Although I never had to\nworry about computation.",
    "start": "3980060",
    "end": "3986138"
  },
  {
    "text": "I never had to\noptimize my algorithms to run on a small embedded chip.",
    "start": "3986138",
    "end": "3991250"
  },
  {
    "text": "STUDENT: Can you talk a little\nbit about the [INAUDIBLE]?? RUSS TEDRAKE: You can\nactually see it here.",
    "start": "3991250",
    "end": "3998040"
  },
  {
    "text": "So this is the\nbarycentric policy space",
    "start": "3998040",
    "end": "4003430"
  },
  {
    "text": "that were the parameters. Yeah.",
    "start": "4003430",
    "end": "4008950"
  },
  {
    "text": "So it was tiled over\n0.5, 0.5 roughly.",
    "start": "4008950",
    "end": "4014109"
  },
  {
    "text": "And you could see the\ndensity of the tiling there. Yeah. And that was trained.",
    "start": "4014110",
    "end": "4019810"
  },
  {
    "text": "So there was no generalization. So the fact that those looked\nlike sort of consistent blobs was just from experience\nand eligibility traces",
    "start": "4019810",
    "end": "4025780"
  },
  {
    "text": "carrying through.  But those are not constrained\nby the function approximator",
    "start": "4025780",
    "end": "4031300"
  },
  {
    "text": "to be similar more\nthan one block away. There's literally a\nbarycentric grid there. And then the value estimate\nwas theta equals zero.",
    "start": "4031300",
    "end": "4040690"
  },
  {
    "text": "The different theta dots. It was just the same size tiles. But a line just\nstraight up the middle.",
    "start": "4040690",
    "end": "4047770"
  },
  {
    "text": "STUDENT: So your joystick\nwould just change theta? Or not the theta? But it would just\nchange the position.",
    "start": "4047770",
    "end": "4056290"
  },
  {
    "text": "RUSS TEDRAKE: The joystick\nwas, so the policy was mostly for the side to side angles,\nwhich would give me limit cycle stability.",
    "start": "4056290",
    "end": "4061730"
  },
  {
    "text": "And then I could just\njoystick control the front to back angles. So this thing, we could\njust lean it forward. It starts walking forward.",
    "start": "4061730",
    "end": "4067660"
  },
  {
    "text": "Even uphill. That's fine. You lean back, it\nstarts walking back. It was really basically this. Yeah.",
    "start": "4067660",
    "end": "4072990"
  },
  {
    "text": "If you want it to turn,\nyou've got to go like this. And it would do its thing. Right?",
    "start": "4072990",
    "end": "4078315"
  },
  {
    "text": "So that was it. It wasn't sort of\nhighly maneuverable. Yeah.",
    "start": "4078315",
    "end": "4084700"
  },
  {
    "text": "STUDENT: It seems like there\nare some [INAUDIBLE] to step to step, having\neach step be like--",
    "start": "4084700",
    "end": "4092047"
  },
  {
    "text": "RUSS TEDRAKE: A trial. STUDENT: So a section\non your Poincar map. RUSS TEDRAKE: Yeah. STUDENT: I don't know if\nthat would work for flapping.",
    "start": "4092048",
    "end": "4098560"
  },
  {
    "text": "RUSS TEDRAKE: Absolutely. STUDENT: If\n[INAUDIBLE] up or down is a similar kind of thing. RUSS TEDRAKE: I think it would. We were thinking\nabout it that way.",
    "start": "4098560",
    "end": "4104870"
  },
  {
    "text": "So you're absolutely right. So it was nice to\nbe able to, it was very important to\nbe able to add noise",
    "start": "4104870",
    "end": "4109930"
  },
  {
    "text": "by sort of making a persistent\nchange in my policy. So this whole\nfunction, adding noise",
    "start": "4109930",
    "end": "4115149"
  },
  {
    "text": "meant this whole function\nwould change a little bit. And then I would stay\nconstant for that whole run.",
    "start": "4115149",
    "end": "4120341"
  },
  {
    "text": "And then change a little bit. If you add noise every\nDT, for instance, then you have to worry about it filtering\nout with motors and stuff.",
    "start": "4120342",
    "end": "4126759"
  },
  {
    "text": "This was actually a very\nconvenient discretization in time on the point grade map. Yeah.",
    "start": "4126760",
    "end": "4131761"
  },
  {
    "text": "So I think that was one\nof the keys to success. John? STUDENT: The actuators you\ntook, were they pushing off",
    "start": "4131762",
    "end": "4138250"
  },
  {
    "text": "the sort of stance foot? [INTERPOSING VOICES] RUSS TEDRAKE: Or\npulling it back up.",
    "start": "4138250",
    "end": "4143600"
  },
  {
    "text": "But yes. STUDENT: So you just\nactuated the stance foot. That was the\nactuator [INAUDIBLE]..",
    "start": "4143600",
    "end": "4148689"
  },
  {
    "text": "RUSS TEDRAKE: The units were,\nI guess they were scaled out. I did actually do the\nkinematics of the link. So it was literally\na linear command in--",
    "start": "4148689",
    "end": "4157670"
  },
  {
    "text": "those are probably meters\nor something in the-- ",
    "start": "4157670",
    "end": "4163540"
  },
  {
    "text": "no. It's way too big. [INTERPOSING VOICES] STUDENT: Touching down, but\ntouch down at the same angle?",
    "start": "4163540",
    "end": "4169480"
  },
  {
    "text": "RUSS TEDRAKE: No. The swing foot was\nalso being controlled. So it would get a\nbig penalty actually",
    "start": "4169480",
    "end": "4175180"
  },
  {
    "text": "if it was at a weird angle\nwhen it touched down. It would hit and it would\nlose all its energy.",
    "start": "4175180",
    "end": "4181130"
  },
  {
    "text": "But that was free to\nmake that mistake. STUDENT: So you have\ntwo actions then? The address to [INAUDIBLE]?",
    "start": "4181130",
    "end": "4188812"
  },
  {
    "text": "RUSS TEDRAKE: No. Well, it's one action. But the policy is being run\non two different actuators at the same time.",
    "start": "4188812",
    "end": "4194220"
  },
  {
    "text": "So one of them is over in\nthis side of the state space. And the other one's over\nin the side of the state space at the same time. STUDENT: OK. So it just used different data.",
    "start": "4194220",
    "end": "4200232"
  },
  {
    "text": "But they're-- OK. RUSS TEDRAKE: Yeah. So just it was learning\non both of those sides at the same time.",
    "start": "4200232",
    "end": "4205258"
  },
  {
    "start": "4205258",
    "end": "4221179"
  },
  {
    "text": "I'm a big fan of simplicity. It's easy to make\nthings that work. I mean, I think it's a good\nway to get things working.",
    "start": "4221180",
    "end": "4227480"
  },
  {
    "text": "So that's what the test\nwill be as we go forward in how complex we can\nmake these things. But in sort of the simple\ncase, they work really well.",
    "start": "4227480",
    "end": "4235960"
  },
  {
    "text": " Great.",
    "start": "4235960",
    "end": "4241030"
  },
  {
    "text": "OK. So thanks for putting up with\nthe randomized algorithm. We'll see you on Thursday.",
    "start": "4241030",
    "end": "4248119"
  },
  {
    "start": "4248120",
    "end": "4249000"
  }
]