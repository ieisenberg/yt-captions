[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or give\nyou additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "text": " PHILIPPE RIGOLLET: So\nagain, before we start,",
    "start": "17880",
    "end": "23880"
  },
  {
    "text": "there is a survey online\nif you haven't done so. I would guess at least\none of you has not.",
    "start": "23880",
    "end": "30599"
  },
  {
    "text": "Some of you have entered their\nanswers and their thoughts, and I really appreciate this. It's actually very helpful.",
    "start": "30600",
    "end": "36180"
  },
  {
    "text": "So it seems that the\ncourse is going fairly well from what I've read so far.",
    "start": "36180",
    "end": "42100"
  },
  {
    "text": "So if you don't think\nthis is the case, please enter your\nopinion and tell us how we can make it better.",
    "start": "42100",
    "end": "47440"
  },
  {
    "text": "One of the things\nthat was said is that I speak too fast,\nwhich is absolutely true.",
    "start": "47440",
    "end": "53370"
  },
  {
    "text": "I just can't help it. I get so excited, but I\nwill really do my best.",
    "start": "53370",
    "end": "59370"
  },
  {
    "text": "I will try to. I think I always start OK.",
    "start": "59370",
    "end": "64800"
  },
  {
    "text": "I just end not so well. So last time we talked about\nthis chi squared distribution,",
    "start": "64800",
    "end": "70920"
  },
  {
    "text": "which is just another\ndistribution that's so common that it\ndeserves its own name.",
    "start": "70920",
    "end": "76140"
  },
  {
    "text": "And this is\nsomething that arises when we sum the squares of\nindependent standard Gaussian",
    "start": "76140",
    "end": "82200"
  },
  {
    "text": "random variables. And in particular,\nwhy is that relevant? It's because if I look\nat the sample variance,",
    "start": "82200",
    "end": "87960"
  },
  {
    "text": "then it is a chi\nsquare distribution, and the parameter\nthat shows up is also known as the degrees of\nfreedom, is the number",
    "start": "87960",
    "end": "95430"
  },
  {
    "text": "of observations of minus one. And so as I said,\nthis chi squared distribution has an explicit\nprobability density function,",
    "start": "95430",
    "end": "103110"
  },
  {
    "text": "and I tried to draw it. And one of the comments was\nalso about my handwriting, so I will actually not rely\non it for detailed things.",
    "start": "103110",
    "end": "112080"
  },
  {
    "text": "So this is what the chi squared\nwith one degree of freedom would look like. And really, what this is is just\nthe distribution of the square",
    "start": "112080",
    "end": "117862"
  },
  {
    "text": "of a standard Gaussian. I'm summing only one,\nso that's what it is. Then when I go to 2,\nthis is what it is--",
    "start": "117862",
    "end": "123810"
  },
  {
    "text": "3, 4, 5, 6, and 10. And as I move, you\ncan see this thing is becoming flatter and flatter,\nand it's pushing to the right.",
    "start": "123810",
    "end": "131840"
  },
  {
    "text": "And that's because I'm\nsumming more and more squares, and in expectation we\njust get one every time.",
    "start": "131840",
    "end": "138600"
  },
  {
    "text": "So it really means that the\nmass is moving to infinity. In particular, a chi\nsquared distribution",
    "start": "138600",
    "end": "146140"
  },
  {
    "text": "with n degrees of freedom\nis going to infinity as n goes to infinity.",
    "start": "146140",
    "end": "152790"
  },
  {
    "text": "Another distribution\nthat I asked you to think about--\nanybody looked around",
    "start": "152790",
    "end": "158010"
  },
  {
    "text": "about the student\nt-distribution, what the history of this thing was? So I'll tell you a little bit.",
    "start": "158010",
    "end": "164545"
  },
  {
    "text": "I understand if you\ndidn't have time. So the t-distribution is\nanother common distribution",
    "start": "164545",
    "end": "170470"
  },
  {
    "text": "that is so common\nthat it will be used and will have its table\nof quintiles that are",
    "start": "170470",
    "end": "176980"
  },
  {
    "text": "drawn at the back of the book. Now, remember, when I\nmentioned the Gaussian, I said,",
    "start": "176980",
    "end": "182110"
  },
  {
    "text": "well, there are several\nvalues for alpha that we're interested in. And so I wanted to draw\na table for the Gaussian.",
    "start": "182110",
    "end": "191110"
  },
  {
    "text": "We had something that\nlooked like this, and I said, well, q alpha\nover 2 to get alpha over 2",
    "start": "191110",
    "end": "201030"
  },
  {
    "text": "to the right of this number. And we said that there is\na table for this things, for common values of theta.",
    "start": "201030",
    "end": "208329"
  },
  {
    "text": "Well, if you try to envision\nwhat this table will look like, it's actually a\npretty sad table,",
    "start": "208330",
    "end": "214000"
  },
  {
    "text": "because it's basically\none list of numbers. Why would I call it a table? Because all I need\nto tell you is something that looks like this.",
    "start": "214000",
    "end": "220120"
  },
  {
    "text": "If I tell you this is alpha\nand this is q alpha over 2 and then I say, OK,\nbasically the three alphas",
    "start": "220120",
    "end": "227170"
  },
  {
    "text": "that I told you I care about are\nsomething like 1%, 5%, and 10%,",
    "start": "227170",
    "end": "234870"
  },
  {
    "text": "then my table will just\ngive me q alpha over 2. So that's alpha, and\nthat's q alpha over 2. And that's going\nto tell me that--",
    "start": "234870",
    "end": "241330"
  },
  {
    "text": "I don't remember this\none, but this guy is 1.96. This guy is something like 2.45.",
    "start": "241330",
    "end": "248920"
  },
  {
    "text": "I think this one\nis like 1.65 maybe. And maybe you can\nbe a little finer,",
    "start": "248920",
    "end": "255274"
  },
  {
    "text": "but it's not going\nto be an entire page at the back of the book. And the reason is\nbecause I only need to draw these things\nfor d1 standard Gaussian",
    "start": "255274",
    "end": "262840"
  },
  {
    "text": "when the parameters\nare 0 for the mean and 1 for the variance. Now, if I'm actually doing\nthis for the the chi squared,",
    "start": "262840",
    "end": "270740"
  },
  {
    "text": "I basically have to give\nyou one table per values of the degrees of freedom,\nbecause those things",
    "start": "270740",
    "end": "277727"
  },
  {
    "text": "are different. There is no way I can take-- for Gaussian's, if you\ngive me a different mean,",
    "start": "277727",
    "end": "283070"
  },
  {
    "text": "I can substract it and make it\nback to be a standard Gaussian. For the chi squared,\nthere is no such thing.",
    "start": "283070",
    "end": "289345"
  },
  {
    "text": "There is no thing\nthat just takes the chi squared with d\ndegrees of freedom, nd, and turns it into,\nsay, a chi square",
    "start": "289345",
    "end": "294935"
  },
  {
    "text": "with one degree of freedom. This just does not happen. So the word is standardized.",
    "start": "294935",
    "end": "301010"
  },
  {
    "text": "Make it a standard chi squared. There is no such thing\nas standard chi squared. So what it means\nis that I'm going to need one row like that\nfor each value of the number",
    "start": "301010",
    "end": "309860"
  },
  {
    "text": "of degrees of freedom. So that will certainly fill a\npage at the back of a book-- maybe even more.",
    "start": "309860",
    "end": "316400"
  },
  {
    "text": "I need one per sample size. So if I want to go from\nsimple size 1 to 1,000, I need 1,000 rows.",
    "start": "316400",
    "end": "324470"
  },
  {
    "text": "So now the student\ndistribution is one that arises where it looks\nvery much like the Gaussian",
    "start": "324470",
    "end": "330740"
  },
  {
    "text": "distribution, and there's a\nvery simple reason for that, is that I take a standard Gaussian\nand I divide it by something.",
    "start": "330740",
    "end": "337820"
  },
  {
    "text": "That's how I get the student. What do I divide it with? Well, I take an\nindependent chi square--",
    "start": "337820",
    "end": "342900"
  },
  {
    "text": "I'm going to call it v-- and I want it to be\nindependent from z. And I'm going to divide\nz by root v over d.",
    "start": "342900",
    "end": "352040"
  },
  {
    "text": "So I start with a chi squared v. So this guy is chi squared d.",
    "start": "352040",
    "end": "358330"
  },
  {
    "text": "I start with z, which is n 0, 1. I'm going to assume that\nthose guys are independent.",
    "start": "358330",
    "end": "366639"
  },
  {
    "text": "In my t-distribution,\nI'm going to write a T. Capital T is z divided by\nthe square root of v over d.",
    "start": "366640",
    "end": "377150"
  },
  {
    "text": "Why would I want to do this? Well, because this\nis exactly what happens when a divide not by\nthe true variance, a Gaussian,",
    "start": "377150",
    "end": "385800"
  },
  {
    "text": "but by its empirical variance. So let's see why in a second. So I know that if you give\nme some random variable--",
    "start": "385800",
    "end": "394480"
  },
  {
    "text": "let's call it x, which\nis N mu sigma squared-- then I can do this.",
    "start": "394480",
    "end": "400000"
  },
  {
    "text": "x minus mu divided by sigma.",
    "start": "400000",
    "end": "405012"
  },
  {
    "text": "I'm going to call this thing\nz, because this thing actually has some standard\nGaussian distribution.",
    "start": "405012",
    "end": "411220"
  },
  {
    "text": "I have standardized\nx into something that I can read the quintiles\nat the back of the book.",
    "start": "411220",
    "end": "418090"
  },
  {
    "text": "So that's this process\nthat I want to do. Now, to be able to do this,\nI need to know what mu is,",
    "start": "418090",
    "end": "423430"
  },
  {
    "text": "and I need to know\nwhat sigma is. Otherwise I'm not going to be\nable to make this operation.",
    "start": "423430",
    "end": "429680"
  },
  {
    "text": "mu I can sort of get away\nwith, because remember, when we're doing\nconfidence intervals",
    "start": "429680",
    "end": "435760"
  },
  {
    "text": "we're actually solving for mu. So it was good\nthat mu was there. When we're doing\nhypothesis testing,",
    "start": "435760",
    "end": "442070"
  },
  {
    "text": "we're actually plugging in here\nthe mu that shows up in h0. So that was good.",
    "start": "442070",
    "end": "447720"
  },
  {
    "text": "We had this thing. Think of mu as being\np, for example. But this guy here, we don't\nnecessarily know what it is.",
    "start": "447720",
    "end": "456730"
  },
  {
    "text": "I just had to tell you for\nthe entire first chapter, assume you have Gaussian\nrandom variables",
    "start": "456730",
    "end": "461860"
  },
  {
    "text": "and that you know\nwhat the variance is. And the reason why\nI said assume you know it-- and I said\nsometimes you can read it",
    "start": "461860",
    "end": "467567"
  },
  {
    "text": "on the side of the box of\nmeasuring equipment in the lab. That was just the\nway I justified it,",
    "start": "467567",
    "end": "474910"
  },
  {
    "text": "but the real reason why I did\nthis is because I would not be able to perform this\noperation if I actually did not",
    "start": "474910",
    "end": "480580"
  },
  {
    "text": "know what sigma was. But from data, we know that\nwe can form this estimator",
    "start": "480580",
    "end": "487340"
  },
  {
    "text": "Sn, which is 1 over n,\nsum from i equals 1 to n of Xi, minus X bar squared.",
    "start": "487340",
    "end": "495430"
  },
  {
    "text": "And this thing is approximately\nequal to sigma squared. That's the sample\nvariance, and it's actually",
    "start": "495430",
    "end": "501100"
  },
  {
    "text": "a good estimator just by the\nlaw of large number, actually. This thing, by the law of large\nnumber, as n goes to infinity--",
    "start": "501100",
    "end": "509472"
  },
  {
    "text": " well, let's say\nit in probability goes to sigma squared by\nthe law of large number.",
    "start": "509472",
    "end": "516570"
  },
  {
    "text": "So it's a consistent\nestimator of sigma squared. So now, what I\nwant to do is to be",
    "start": "516570",
    "end": "523080"
  },
  {
    "text": "able to use this estimator\nrather than using sigma. And the way I'm\ngoing to do it is I'm going to say, OK,\nwhat I want to form",
    "start": "523080",
    "end": "530220"
  },
  {
    "text": "is x minus mu divided\nby Sn this time.",
    "start": "530220",
    "end": "538399"
  },
  {
    "text": "I don't know what the\ndistribution of this guy is. Sorry, it's square root of Sn. This is sigma squared.",
    "start": "538400",
    "end": "545430"
  },
  {
    "text": "So this is what I would take. And I could think\nof Slutsky, maybe, something like this that would\ntell me, well, just use that",
    "start": "545430",
    "end": "554150"
  },
  {
    "text": "and pretend it's a Gaussian. And we'll see how\nactually it's sort of valid to do that,\nbecause Slutsky tells us",
    "start": "554150",
    "end": "560899"
  },
  {
    "text": "it is valid to do that. But what we can\nalso do is to say, well, this is actually equal to\nx minus mu, divided by sigma,",
    "start": "560900",
    "end": "568940"
  },
  {
    "text": "which I knew what the\ndistribution of this guy is. And then what I'm going to\ndo is I'm going to just-- well, I'm going to cancel this\neffect, sigma over square root",
    "start": "568940",
    "end": "578670"
  },
  {
    "text": "Sn. So I didn't change anything. I just put the sigma here. So now what I know what I\nknow is that this is some z,",
    "start": "578670",
    "end": "587009"
  },
  {
    "text": "and it has some standard\nGaussian distribution. What is this guy?",
    "start": "587010",
    "end": "594480"
  },
  {
    "text": "Well, I know that Sn-- we wrote this here.",
    "start": "594480",
    "end": "599870"
  },
  {
    "text": "Maybe I shouldn't have\nput those pictures, because now I keep on\nskipping before and after. We know that Sn times n\ndivided by sigma squared",
    "start": "599870",
    "end": "614180"
  },
  {
    "text": "is actually chi\nsquared n minus 1. ",
    "start": "614180",
    "end": "622579"
  },
  {
    "text": "So what do I have here? I have that chi squared-- so here I have something that\nlooks like 1 over square root",
    "start": "622580",
    "end": "629720"
  },
  {
    "text": "of Sn divided by sigma squared. ",
    "start": "629720",
    "end": "635590"
  },
  {
    "text": "This is what this guy is if\nI just do some more writing. And maybe I actually want to\nmake my life a little easier.",
    "start": "635590",
    "end": "641710"
  },
  {
    "text": "I'm actually going\nto plug in my n here, and so I'm going to have to\nmultiply by square root of n",
    "start": "641710",
    "end": "648693"
  },
  {
    "text": "here. ",
    "start": "648693",
    "end": "656438"
  },
  {
    "text": "Everybody's with me? So now what I end up\nwith is something that",
    "start": "656438",
    "end": "661510"
  },
  {
    "text": "looks like this, where I have-- here I started with x.",
    "start": "661510",
    "end": "667775"
  },
  {
    "start": "667775",
    "end": "675630"
  },
  {
    "text": "I should really start\nwith Xn bar minus mu times square root of n.",
    "start": "675630",
    "end": "681650"
  },
  {
    "text": "That's what the central\nlimit theorem would tell me. I need to work with the\naverage rather than just one observation.",
    "start": "681650",
    "end": "687953"
  },
  {
    "text": "So if I start with this, then\nI pick up a square root of n here. ",
    "start": "687953",
    "end": "703180"
  },
  {
    "text": "So if I had the sigma\nhere, I would know that this thing is actually-- Xn bar minus mu divided by\nsigma times the square root of n",
    "start": "703180",
    "end": "714110"
  },
  {
    "text": "would be a standard Gaussian. So if I put Xn\nbar here, I really need to put this thing that\ngoes around the Xn bar.",
    "start": "714110",
    "end": "720620"
  },
  {
    "text": " That's just my\ncentral limit theorem",
    "start": "720620",
    "end": "726120"
  },
  {
    "text": "that says if I average, then my\nvariance has shrunk by a factor 1 over n.",
    "start": "726120",
    "end": "732779"
  },
  {
    "text": "Now, I can still do this. That was still fine. And now I said that this\nthing is basically this guy.",
    "start": "732780",
    "end": "746620"
  },
  {
    "text": "So what I know is\nthat this thing is a chi squared with n\nminus 1 degrees of freedom,",
    "start": "746620",
    "end": "752810"
  },
  {
    "text": "so this guy here is\nchi squared with n minus 1 degrees of freedom.",
    "start": "752810",
    "end": "760710"
  },
  {
    "text": "Let me call this thing v in the\nspirit of what was used there and in the spirit of\nwhat is written here.",
    "start": "760710",
    "end": "769690"
  },
  {
    "text": "So this guy was called v,\nso I'm going to call this v. So what I can write is\nthat square root of n Xn",
    "start": "769690",
    "end": "777000"
  },
  {
    "text": "bar minus mu divided\nby square root of Sn",
    "start": "777000",
    "end": "782570"
  },
  {
    "text": "is equal to z times\nsquare root of n",
    "start": "782570",
    "end": "790870"
  },
  {
    "text": "divided by square root of\nv. Everybody's with me here?",
    "start": "790870",
    "end": "800460"
  },
  {
    "text": " Which I can rewrite as z times\nsquare root of v divided by n",
    "start": "800460",
    "end": "817070"
  },
  {
    "text": "And if you look at what the\ndefinition of this thing is, I'm almost there. What is the only thing\nthat's wrong here?",
    "start": "817070",
    "end": "825480"
  },
  {
    "text": "This is a student\ndistribution, right? So there's two things. The first one was that\nthey should be independent,",
    "start": "825480",
    "end": "831840"
  },
  {
    "text": "and they actually\nare independent. That's what Cochran's\ntheorem tells me, and you just have to\ncount on me for this.",
    "start": "831840",
    "end": "837209"
  },
  {
    "text": "I told you already that Sn\nwas independent of Xn bar. So those two guys\nare independent,",
    "start": "837210",
    "end": "844510"
  },
  {
    "text": "which implies that the\nnumerator and denominator here are independent. That's what Cochran's\ntheorem tells us.",
    "start": "844510",
    "end": "852259"
  },
  {
    "text": "But is this exactly\nwhat I should be seeing if I wanted to\nhave my sample variance, if I",
    "start": "852260",
    "end": "857990"
  },
  {
    "text": "want to have to write this? Is this actually the definition\nof a student distribution?",
    "start": "857990",
    "end": "863480"
  },
  {
    "text": "Yes? No. ",
    "start": "863480",
    "end": "868890"
  },
  {
    "text": "So we see z divided by\nsquare root of v over d. That looks pretty much\nlike it, except there's",
    "start": "868890",
    "end": "875690"
  },
  {
    "text": "a small discrepancy. What is the discrepancy? ",
    "start": "875690",
    "end": "887260"
  },
  {
    "text": "There's just the square\nroot of n minus 1 thing. So here, v has n minus\n1 degrees of freedom.",
    "start": "887260",
    "end": "895520"
  },
  {
    "text": "And in the definition, if the\nv has d degrees of freedom, I divide it by d, not by d minus\n1 or not by d plus 1, actually,",
    "start": "895520",
    "end": "904380"
  },
  {
    "text": "in this case. So I have this extra thing. Well, there's two ways\nI can address this.",
    "start": "904380",
    "end": "909570"
  },
  {
    "text": " The first one is\nby saying, well,",
    "start": "909570",
    "end": "914910"
  },
  {
    "text": "this is actually equal\nto z over square root of v divided by n minus\n1 times square root of n",
    "start": "914910",
    "end": "927390"
  },
  {
    "text": "over n minus 1. ",
    "start": "927390",
    "end": "932810"
  },
  {
    "text": "I can always do that and\nsay for n large enough this thing is actually\ngoing to be pretty small, or I can take account for it.",
    "start": "932810",
    "end": "939260"
  },
  {
    "text": "Or for any n you give me,\nI can compute this number. And so rather than\nhaving a t-distribution,",
    "start": "939260",
    "end": "945615"
  },
  {
    "text": "I'm going to have a\nt-distribution time this deterministic\nnumber, which is just a function of my\nnumber of observations.",
    "start": "945615",
    "end": "952370"
  },
  {
    "text": "But what I actually\nwant to do instead is probably use a slightly\ndifferent normalization,",
    "start": "952370",
    "end": "960520"
  },
  {
    "text": "which is just to say, well,\nwhy do I have to define Sn-- ",
    "start": "960520",
    "end": "970260"
  },
  {
    "text": "where was my Sn? Yeah, why do I have to define\nSn tend to be divided by n? Actually, this is\na biased estimator,",
    "start": "970260",
    "end": "977730"
  },
  {
    "text": "and if I wanted to be\nunbiased, I can actually just put an n minus 1 here. You can check that.",
    "start": "977730",
    "end": "983550"
  },
  {
    "text": "You can expend this thing\nand compute the expectation. You will see that it's\nactually not sigma squared, but n over n minus\n1 sigma squared.",
    "start": "983550",
    "end": "991360"
  },
  {
    "text": "So you can actually\njust make it unbiased. Let's call this\nguy tilde, and then when I put this tilde here what\nI actually get is s tilde here",
    "start": "991360",
    "end": "1003890"
  },
  {
    "text": "and s tilde here. I need actually to\nhave n minus 1 here",
    "start": "1003890",
    "end": "1009920"
  },
  {
    "text": "to have this s tilde be a\nchi squared distribution.",
    "start": "1009920",
    "end": "1015800"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] defined\nthis way so that you--",
    "start": "1015801",
    "end": "1022010"
  },
  {
    "text": "PHILIPPE RIGOLLET: So basically,\nthis is what the story did. So the story was, well,\nrather than using always",
    "start": "1022010",
    "end": "1028358"
  },
  {
    "text": "the central limit theorem\nand just pretending that my Sn is actually\nthe true sigma squared,",
    "start": "1028359",
    "end": "1033800"
  },
  {
    "text": "since this is something\nI'm going to do a lot, I might as well just\ncompute the distribution,",
    "start": "1033800",
    "end": "1039459"
  },
  {
    "text": "like the quintiles for this\nparticular distribution, which clearly does not depend\non any unknown parameter.",
    "start": "1039460",
    "end": "1044470"
  },
  {
    "text": "d is the only parameter\nthat shows up here, and it's completely\ncharacterized by the number of\nobservations that you have,",
    "start": "1044470",
    "end": "1050510"
  },
  {
    "text": "which you definitely know. And so people said, let's just\nbe slightly more accurate. And in a second, I'll show you\nhow the distribution of the T--",
    "start": "1050510",
    "end": "1058899"
  },
  {
    "text": "so we know that if the\nsample size is large enough, this should not have any\ndifference with the Gaussian distribution.",
    "start": "1058900",
    "end": "1064480"
  },
  {
    "text": "I mean, those two\nthings should be the same because we've\nactually not paid attention to this discrepancy by\nusing empirical variance rather",
    "start": "1064480",
    "end": "1071410"
  },
  {
    "text": "than true so far. And so we'll see what\nthe difference is, and this difference actually\nmanifests itself only",
    "start": "1071410",
    "end": "1077830"
  },
  {
    "text": "in small sample sizes. So those are things\nthat matter mostly if you have less than,\nsay, 50 observations.",
    "start": "1077830",
    "end": "1084512"
  },
  {
    "text": "Then you might want to\nbe slightly more precise and use t-distribution\nrather than Gaussian. So this is just a matter of\nbeing slightly more precise.",
    "start": "1084512",
    "end": "1092640"
  },
  {
    "text": "If you have more\nthan 50 observations, just drop everything\nand just pretend that this is the true one. ",
    "start": "1092640",
    "end": "1099610"
  },
  {
    "text": "Any other questions? So now I have this\nthing, and so I'm",
    "start": "1099610",
    "end": "1105450"
  },
  {
    "text": "on my way to changing this guy. So here now, I have not\nroot n but root n minus 1.",
    "start": "1105450",
    "end": "1111540"
  },
  {
    "start": "1111540",
    "end": "1127680"
  },
  {
    "text": "So I have a z. So this guy here is S. Yet\nWhere did I get my root",
    "start": "1127680",
    "end": "1135441"
  },
  {
    "text": "n from in the first place?  Yeah, because I wanted this guy.",
    "start": "1135441",
    "end": "1142270"
  },
  {
    "text": "And so now what I am\nleft with is Xn minus mu divided by Sn tilde, which\nis the new one, which is now",
    "start": "1142270",
    "end": "1148900"
  },
  {
    "text": "indeed of the form z v\nroot n minus 1, which now I",
    "start": "1148900",
    "end": "1154540"
  },
  {
    "text": "can write it as z v minus 1. And so now I have\nexactly what I want,",
    "start": "1154540",
    "end": "1162410"
  },
  {
    "text": "and so this guy is n 0, 1. And this guy is chi squared with\nn minus 1 degrees of freedom.",
    "start": "1162410",
    "end": "1170429"
  },
  {
    "text": "And so now I'm back\nto what I want. So rather than using Sn to be\nthe empirical variance where",
    "start": "1170430",
    "end": "1177360"
  },
  {
    "text": "I just divide my normatizations\nby n, if I use n minus 1, I'm perfect. Of course, I can still use\nn and do this multiplying",
    "start": "1177360",
    "end": "1185730"
  },
  {
    "text": "by root n minus 1\nover n at the end. But that just doesn't\nmake as much sense. ",
    "start": "1185730",
    "end": "1192534"
  },
  {
    "text": "Everybody's fine with what\nthis T n distribution is doing and why this last\nline is correct?",
    "start": "1192535",
    "end": "1198970"
  },
  {
    "text": "So that's just\nbasically because it's been defined so that this\nis actually happening.",
    "start": "1198970",
    "end": "1204250"
  },
  {
    "text": "That was your question, and\nthat's really what happened. So what is this\nstudent t-distribution?",
    "start": "1204250",
    "end": "1211320"
  },
  {
    "text": "Where does the name come from? Well, it does not come from\nMr. T. And if you know who Mr.",
    "start": "1211320",
    "end": "1218429"
  },
  {
    "text": "T was-- you're probably\ntoo young for that-- he was our hero in the 80s.",
    "start": "1218430",
    "end": "1223470"
  },
  {
    "text": "And it comes from this guy. His name is Sean\nWilliam Gosset--",
    "start": "1223470",
    "end": "1229200"
  },
  {
    "text": "1908. So that was back in the day. And this guy actually worked\nat the Guinness Brewery in Dublin, Ireland.",
    "start": "1229200",
    "end": "1235150"
  },
  {
    "text": "And Mr. Guinness back then\nwas a bit of a fascist, and he didn't want him to\nactually publish papers.",
    "start": "1235150",
    "end": "1241320"
  },
  {
    "text": "And so what he had to do is\nto use a fake name to do that. And he was not very creative,\nand he used a name \"student.\"",
    "start": "1241320",
    "end": "1250649"
  },
  {
    "text": "Because I guess he\nwas a student of life. And so here's the guy, actually.",
    "start": "1250650",
    "end": "1255990"
  },
  {
    "text": "So back in 1908,\nit was actually not difficult to put your\nname or your pen name",
    "start": "1255990",
    "end": "1261270"
  },
  {
    "text": "on a distribution. So what does this\nthing look like? How does it compare to the\nstandard normal distribution?",
    "start": "1261270",
    "end": "1269620"
  },
  {
    "text": "You think it's going to have\nheavier or lighter tails compared to the\nstandard distribution, the Gaussian distribution?",
    "start": "1269620",
    "end": "1277529"
  },
  {
    "text": "Yeah, because they have extra\nuncertainty in the denominator, so it's actually going to make\nthings wiggle a little wider.",
    "start": "1277530",
    "end": "1285090"
  },
  {
    "text": "So let's start with\na reference, which is the standard\nnormal distribution. So that's my usual\nbell-shaped curve.",
    "start": "1285090",
    "end": "1291300"
  },
  {
    "text": "And this is actually\nthe t-distribution with 50 degrees of freedom. So right now, that's probably\nwhere you should just",
    "start": "1291300",
    "end": "1297929"
  },
  {
    "text": "stand up and leave,\nbecause you're like, why are we wasting our time? Those are actually pretty much\nthe same thing, and it is true.",
    "start": "1297930",
    "end": "1303750"
  },
  {
    "text": "If you have 50 observations,\nboth the central limit theorem-- so here one of the\nthings that you need to know",
    "start": "1303750",
    "end": "1309210"
  },
  {
    "text": "is that if I want to talk about\nt-distribution for, say, eight",
    "start": "1309210",
    "end": "1314659"
  },
  {
    "text": "observations, I need those\nobservations to be Gaussian for real. There's no central\nlimit theorem happening at eight observations.",
    "start": "1314660",
    "end": "1320732"
  },
  {
    "text": "But really, what\nthis is telling me is not that the central\nlimit theorem kicks in. It's telling me what are the\nasymptotics that kick in?",
    "start": "1320732",
    "end": "1327320"
  },
  {
    "start": "1327320",
    "end": "1333620"
  },
  {
    "text": "The law of large number, right? This is exactly this guy.",
    "start": "1333620",
    "end": "1339260"
  },
  {
    "text": "That's here. When I write this statement,\nwhat this picture is really",
    "start": "1339260",
    "end": "1344530"
  },
  {
    "text": "telling us is that for n is\nequal to 50, I'm at the limit already almost.",
    "start": "1344530",
    "end": "1349720"
  },
  {
    "text": "There's virtually no\ndifference between using the left-hand side or\nusing sigma squared.",
    "start": "1349720",
    "end": "1356860"
  },
  {
    "text": "And now I start reducing. 40, I'm still pretty good. We can start seeing that\nthis thing is actually losing some mass\non top, and that's",
    "start": "1356860",
    "end": "1363009"
  },
  {
    "text": "because it's actually\npushing it to the left and to the right in the tails. And then we keep going,\nkeep going, keep going.",
    "start": "1363010",
    "end": "1369940"
  },
  {
    "text": "So that's at 10. When you're at 10, there's\nnot much of a difference. And so you can start\nseeing difference when you're at\nfive, for example.",
    "start": "1369940",
    "end": "1377320"
  },
  {
    "text": "You can see the\ntails become heavier. And the effect of this is\nthat when I'm going to build, for example, a confidence\ninterval to put the same amount",
    "start": "1377320",
    "end": "1385930"
  },
  {
    "text": "of mass to the right\nof some number-- let's say I'm going to look\nat this q alpha over 2-- I'm going to have to\ngo much farther, which",
    "start": "1385930",
    "end": "1391742"
  },
  {
    "text": "is going to result in much\nwider confidence intervals",
    "start": "1391742",
    "end": "1397120"
  },
  {
    "text": "to 4, 3, 2, 1. So that's the t1.",
    "start": "1397120",
    "end": "1402530"
  },
  {
    "text": "Obviously that's the worst. And if you ever use\nthe t1 distribution,",
    "start": "1402530",
    "end": "1410510"
  },
  {
    "text": "please ask yourself, why in the\nworld are you doing statistics based on one observation? ",
    "start": "1410510",
    "end": "1418570"
  },
  {
    "text": "But that's basically what it is. So now that we have\nthis t-distribution,",
    "start": "1418570",
    "end": "1424980"
  },
  {
    "text": "we can define a more\nsophisticated test than just take your\nfavorite estimator",
    "start": "1424980",
    "end": "1430640"
  },
  {
    "text": "and see if it's far from the\nvalue you're currently testing. That was our rationale\nto build a test before.",
    "start": "1430640",
    "end": "1437360"
  },
  {
    "text": "And the first test\nthat's non-trivial is a test that exploits the\nfact that the maximum likelihood",
    "start": "1437360",
    "end": "1444320"
  },
  {
    "text": "estimator, under some\ntechnical condition, has a limit distribution\nwhich is Gaussian with mean 0",
    "start": "1444320",
    "end": "1452720"
  },
  {
    "text": "when properly centered and\na covariance matrix given",
    "start": "1452720",
    "end": "1458360"
  },
  {
    "text": "by the Fisher\ninformation matrix. Remember this Fisher\ninformation matrix? ",
    "start": "1458360",
    "end": "1466080"
  },
  {
    "text": "And so this is the\nsetup that we have. So we have, again, an i.i.d.",
    "start": "1466080",
    "end": "1471190"
  },
  {
    "text": "sample. Now I'm going to assume that I\nhave a d-dimensional parameter space, theta.",
    "start": "1471190",
    "end": "1476889"
  },
  {
    "text": "And that's why I talk about\nFisher information matrix-- and not just Fisher information. It's a number.",
    "start": "1476890",
    "end": "1482799"
  },
  {
    "text": "And I'm going to\nconsider two hypotheses. So you're going to have h0,\ntheta is equal to theta 0.",
    "start": "1482800",
    "end": "1492730"
  },
  {
    "text": "h1, theta is not\nequal to theta 0. And this is basically\nwhat we thought",
    "start": "1492730",
    "end": "1500210"
  },
  {
    "text": "when we said, are we testing\nif a coin is fair or unfair. So fair was p equals 1/2, and\nfair was p different from 1/2.",
    "start": "1500210",
    "end": "1509390"
  },
  {
    "text": "And here I'm just making\nmy life a bit easier. So now, I have this\nmaximum likelihood estimate",
    "start": "1509390",
    "end": "1516860"
  },
  {
    "text": "that I can construct. Because let's say I\nknow what p theta is, and so I can build a maximum\nlikelihood estimator.",
    "start": "1516860",
    "end": "1523250"
  },
  {
    "text": "And I'm going to assume that\nthese technical conditions that ensure that this maximum\nlikelihood properly",
    "start": "1523250",
    "end": "1529010"
  },
  {
    "text": "standardized converges to some\nGaussian are actually satisfy,",
    "start": "1529010",
    "end": "1535920"
  },
  {
    "text": "and so this thing\nis actually true. So the theorem, the\nway I stated it--",
    "start": "1535920",
    "end": "1541870"
  },
  {
    "text": "if you're a little puzzled,\nthis is not the way I stated it. And the first time, the way we\nstated it was that theta hat",
    "start": "1541870",
    "end": "1547580"
  },
  {
    "text": "mle minus theta\nnot-- so here I'm going to place myself\nunder the null hypothesis,",
    "start": "1547580",
    "end": "1553420"
  },
  {
    "text": "so here I'm going\nto say under h0. And honestly, if you have\nany exercise on tests,",
    "start": "1553420",
    "end": "1561050"
  },
  {
    "text": "that's the way that\nit should start. What is the\ndistribution under h0? Because otherwise you don't\nknow what this guy should be.",
    "start": "1561050",
    "end": "1568610"
  },
  {
    "text": "So you have this,\nand what we showed is that this thing was going\nin distribution as n goes to infinity to some\nnormal with mean 0",
    "start": "1568610",
    "end": "1575900"
  },
  {
    "text": "and covariance matrix,\nwhich was i of theta, which was here for\nthe true parameter.",
    "start": "1575900",
    "end": "1581120"
  },
  {
    "text": "But here I'm under\nh0, so there's only one true parameter,\nwhich is theta 0. ",
    "start": "1581120",
    "end": "1592590"
  },
  {
    "text": "This was our limiting\ncentral limit theorem for-- I mean, it's not really\ncentral limited theorem;",
    "start": "1592590",
    "end": "1598830"
  },
  {
    "text": "limited theorem for the\nmaximum likelihood estimator. Everybody remembers that part?",
    "start": "1598830",
    "end": "1607230"
  },
  {
    "text": "The line before said, under\ntechnical conditions, I guess. So now, it's not really\nstated in the same way.",
    "start": "1607230",
    "end": "1613019"
  },
  {
    "text": "If you look at\nwhat's on the slide, here I don't have the\nFisher information matrix, but I really have\nthe identity of rd.",
    "start": "1613020",
    "end": "1619290"
  },
  {
    "text": " If I have a random\nvariable x, which",
    "start": "1619290",
    "end": "1625590"
  },
  {
    "text": "has some covariance\nmatrix sigma, how do I turn this thing\ninto something that",
    "start": "1625590",
    "end": "1632679"
  },
  {
    "text": "has covariance matrix identity? So if this was a sigma squared,\nwell, the thing I would do",
    "start": "1632680",
    "end": "1640120"
  },
  {
    "text": "would be divide by\nsigma, and then I would have a 1,\nwhich is also known as the identity matrix of r1.",
    "start": "1640120",
    "end": "1648360"
  },
  {
    "text": "Now, what is this? This was root of sigma squared. So what I'm looking\nfor is the equivalent",
    "start": "1648360",
    "end": "1655400"
  },
  {
    "text": "of taking sigma and dividing\nby the square root of sigma, which--",
    "start": "1655400",
    "end": "1660800"
  },
  {
    "text": "obviously those are matrices-- I'm certainly not allowed to do. And so what I'm going\nto do is I'm actually going to do the following.",
    "start": "1660800",
    "end": "1668360"
  },
  {
    "text": "Sigma 1 over root\nof sigma squared can be written as sigma\nto the negative 1/2.",
    "start": "1668360",
    "end": "1675669"
  },
  {
    "text": "And this is actually\nthe same thing here. So I'm going to write it as\nsigma to the negative 1/2,",
    "start": "1675670",
    "end": "1682180"
  },
  {
    "text": "and now this guy is\nactually well-defined. So this is a positive\nsymmetric matrix,",
    "start": "1682180",
    "end": "1688808"
  },
  {
    "text": "and you can actually\ndefine the square root by just taking the square\nroot of its eigenvalues,",
    "start": "1688808",
    "end": "1696340"
  },
  {
    "text": "for example. And so you get sigma 1/2\nequals and follows n0 identity.",
    "start": "1696340",
    "end": "1703487"
  },
  {
    "text": " And in general, I'm\ngoing to see something",
    "start": "1703487",
    "end": "1710790"
  },
  {
    "text": "that looks like sigma\n1/2 negative 1/2 sigma sigma negative 1/2.",
    "start": "1710790",
    "end": "1717060"
  },
  {
    "text": "And I have minus 1/2\nplus 1 minus 1/2. This whole thing collapses to 0,\nand it's actually the identity.",
    "start": "1717060",
    "end": "1725330"
  },
  {
    "text": "So that's the actual rule. So if you're not familiar, this\nis basic multivariate Gaussian",
    "start": "1725330",
    "end": "1732409"
  },
  {
    "text": "distribution computations. Take a look at it. If you feel like you\ndon't need to look at it",
    "start": "1732410",
    "end": "1739220"
  },
  {
    "text": "but you the basic maneuver,\nit's fine as well. We're not going to go\nmuch deeper into that,",
    "start": "1739220",
    "end": "1745320"
  },
  {
    "text": "but those are part\nof the thing that are sort of standard\nmanipulations about standard Gaussian vectors.",
    "start": "1745320",
    "end": "1751250"
  },
  {
    "text": "Because obviously,\nstandard Gaussian vectors arise from this theorem a lot.",
    "start": "1751250",
    "end": "1757820"
  },
  {
    "text": "So now I pre-multiplied my\nsigma to minus minus 1/2. Now of course, I'm doing all\nof this in the asymptotics,",
    "start": "1757820",
    "end": "1764630"
  },
  {
    "text": "and so I have this effect. So if I pre-multiply\neverything by sigma to the 1/2,",
    "start": "1764630",
    "end": "1769639"
  },
  {
    "text": "sigma being the Fisher\ninformation matrix at theta 0,",
    "start": "1769640",
    "end": "1774680"
  },
  {
    "text": "then this is actually equivalent\nto saying that square root of n-- ",
    "start": "1774680",
    "end": "1783630"
  },
  {
    "text": "so now i of theta now\nplays the role of sigma--",
    "start": "1783630",
    "end": "1791510"
  },
  {
    "text": "times theta hat mle minus\ntheta not goes in distribution",
    "start": "1791510",
    "end": "1799620"
  },
  {
    "text": "as n goes to infinity to some\nmultivariate standard Gaussian",
    "start": "1799620",
    "end": "1806600"
  },
  {
    "text": "and 0 identity of rd. And here, to make\nsure that we're talking about a\nmultivariate distribution,",
    "start": "1806600",
    "end": "1813080"
  },
  {
    "text": "I can put a d here-- so just so we know we're\ntalking about the multivariate,",
    "start": "1813080",
    "end": "1818420"
  },
  {
    "text": "though it's pretty\nclear from the context, since the covariance matrix\nis actually a matrix and not a number.",
    "start": "1818420",
    "end": "1823840"
  },
  {
    "text": "Michael? AUDIENCE: [INAUDIBLE]. ",
    "start": "1823840",
    "end": "1829623"
  },
  {
    "text": "PHILIPPE RIGOLLET: Oh, yeah. Right. Thanks.  So Yeah, you're right.",
    "start": "1829623",
    "end": "1835230"
  },
  {
    "text": "So that's a minus\nand that's a plus. Thanks.",
    "start": "1835230",
    "end": "1840420"
  },
  {
    "text": "So yeah, anybody has\na way to remember",
    "start": "1840420",
    "end": "1847050"
  },
  {
    "text": "whether it's inverse Fisher\ninformation or Fisher information as a variance\nother than just learning it?",
    "start": "1847050",
    "end": "1854100"
  },
  {
    "text": "It is called information,\nso it's really telling me how much information I have.",
    "start": "1854100",
    "end": "1860620"
  },
  {
    "text": "So when a variance\nincreases, I'm getting less and\nless information, and so this thing should\nactually be 1 over a variance.",
    "start": "1860620",
    "end": "1868175"
  },
  {
    "text": "The notion of information is\n1 over a notion of variance. ",
    "start": "1868175",
    "end": "1873320"
  },
  {
    "text": "So now I just wrote this guy\nlike this, and the reason",
    "start": "1873320",
    "end": "1879370"
  },
  {
    "text": "why I did this is\nbecause now everything on the right-hand side does not\ndepend on any known parameter.",
    "start": "1879370",
    "end": "1886779"
  },
  {
    "text": "There's 0 and identity. Those two things are\njust absolute numbers",
    "start": "1886780",
    "end": "1893755"
  },
  {
    "text": "or absolute quantities,\nwhich means that this thing-- I call this quantity here--",
    "start": "1893755",
    "end": "1902500"
  },
  {
    "text": "what was the name that I used? Started with a \"p.\" Pivotal.",
    "start": "1902500",
    "end": "1907870"
  },
  {
    "text": "So this is a pivotal\nquantity, meaning that its distribution, at\nleast asymptotic distribution,",
    "start": "1907870",
    "end": "1913900"
  },
  {
    "text": "does not depend on\nany unknown parameter. Moreover, it is\nindeed a statistic,",
    "start": "1913900",
    "end": "1920610"
  },
  {
    "text": "because I can\nactually compute it. I know theta 0 and I\nknow theta hat mle.",
    "start": "1920610",
    "end": "1925950"
  },
  {
    "text": "One thing that I did,\nand you should actually complain about this,\nis on the board",
    "start": "1925950",
    "end": "1931020"
  },
  {
    "text": "I actually used i of theta not. And on the slides, it\nsays i of theta hat.",
    "start": "1931020",
    "end": "1940380"
  },
  {
    "text": "And it's exactly the same\nthing that we did before. Do I want to use the\nvariance as a way for me",
    "start": "1940380",
    "end": "1946110"
  },
  {
    "text": "to check whether I'm under\nthe right assumption or not? Or do I actually want\nto leave that part",
    "start": "1946110",
    "end": "1951269"
  },
  {
    "text": "and just plug in the theta\nhat mle, which should go to the true one eventually?",
    "start": "1951270",
    "end": "1956310"
  },
  {
    "text": "Or do I actually want to\njust plug in the theta 0? So this is exactly\nplaying the same role",
    "start": "1956310",
    "end": "1961740"
  },
  {
    "text": "as whether I wanted to\nsee square root of Xn bar 1 minus Xn bar in the\ndenominator of my test",
    "start": "1961740",
    "end": "1968970"
  },
  {
    "text": "statistic for p, or if I wanted\nto see square root of 0.5,",
    "start": "1968970",
    "end": "1975059"
  },
  {
    "text": "1 minus 0.5 when I was\ntesting if p was equal to 0.5. So this is really a choice\nthat's left up to you,",
    "start": "1975060",
    "end": "1983070"
  },
  {
    "text": "and that's something you\ncan really choose the two. And as we said, maybe this\nguy is slightly more precise,",
    "start": "1983070",
    "end": "1989710"
  },
  {
    "text": "but it's not going\nto extend to the case where theta 0 is not reduced\nto one single number.",
    "start": "1989710",
    "end": "1995382"
  },
  {
    "start": "1995383",
    "end": "2000950"
  },
  {
    "text": "Any questions? So now we have our pivotal\ndistribution, so from there",
    "start": "2000950",
    "end": "2006140"
  },
  {
    "text": "this is going to be\nmy test statistic. I'm going to use this\nas a test statistic and declare that if\nthis thing is too large,",
    "start": "2006140",
    "end": "2015799"
  },
  {
    "text": "n absolute value-- because this is really a way to\nquantify how far theta hat is",
    "start": "2015800",
    "end": "2021020"
  },
  {
    "text": "from theta 0. And since theta hat should be\nclose to the true one, when this thing is large\nin absolute value, it means that the true theta\nshould be far from theta 0.",
    "start": "2021020",
    "end": "2030179"
  },
  {
    "text": "So this is my new\ntest statistic.",
    "start": "2030180",
    "end": "2036250"
  },
  {
    "text": "Now, I said it should be\nfar, but this is a vector. So if I want a vector to be\nfar, two vectors to be far,",
    "start": "2036250",
    "end": "2042539"
  },
  {
    "text": "I measure their norm. And so I'm going to form the\nEuclidean norm of this guy. So if I look at the\nEuclidean norm of n--",
    "start": "2042540",
    "end": "2050600"
  },
  {
    "text": " and Euclidean norm\nis the one you know--",
    "start": "2050600",
    "end": "2056510"
  },
  {
    "start": "2056510",
    "end": "2062840"
  },
  {
    "text": "I'm going to take its square. Let me now put a 2 here. So that's just the\nEuclidean norm,",
    "start": "2062840",
    "end": "2068739"
  },
  {
    "text": "and so the norm of vector\nx is just x transpose x.",
    "start": "2068739",
    "end": "2076679"
  },
  {
    "text": "In the slides, the transpose\nis denoted by prime. Wow, that's hard to say. Put prime in quotes.",
    "start": "2076679",
    "end": "2082419"
  },
  {
    "start": "2082420",
    "end": "2088510"
  },
  {
    "text": "That's a statistic\nstandard that people do. They put prime for transpose. Everybody knows what\nthe transpose is?",
    "start": "2088510",
    "end": "2096138"
  },
  {
    "text": "So I just make it flat\nand I do it like this, and then that means\nthat's actually equal to the sum of the\ncoordinates Xi squared.",
    "start": "2096139",
    "end": "2103620"
  },
  {
    "text": " And that's what you know. But here, I'm just writing\nit in terms of vectors.",
    "start": "2103620",
    "end": "2110880"
  },
  {
    "text": "And so when I run to write\nthis, this is equivalent, this is equal to-- well, the square root of n is\ngoing to pick up the square.",
    "start": "2110880",
    "end": "2117500"
  },
  {
    "text": "So I get square root of\nn times square root of n. So this guy is just 1/2.",
    "start": "2117500",
    "end": "2123210"
  },
  {
    "text": "So 1/2 times 1/2 is\ngoing to give me 1, and so I get theta\nhat mle minus theta.",
    "start": "2123210",
    "end": "2129360"
  },
  {
    "text": "And then I have e of theta not. And then I get theta\nhat mle minus theta not.",
    "start": "2129360",
    "end": "2137630"
  },
  {
    "text": "And so by definition, I'm\ngoing to say that this is my test statistic Tn.",
    "start": "2137630",
    "end": "2145319"
  },
  {
    "text": "And now I'm going to have a test\nthat rejects if Tn is large,",
    "start": "2145320",
    "end": "2150480"
  },
  {
    "text": "because Tn is really measuring\nthe distance between theta hat and theta 0.",
    "start": "2150480",
    "end": "2155670"
  },
  {
    "text": "So my test now is going\nto be psi, which rejects.",
    "start": "2155670",
    "end": "2180530"
  },
  {
    "text": "So it says 1 if Tn is larger\nthan some threshold T.",
    "start": "2180530",
    "end": "2187300"
  },
  {
    "text": "And how do I pick this T? Well, by controlling\nmy type I error-- sorry, the c by controlling\nmy type I error.",
    "start": "2187300",
    "end": "2195730"
  },
  {
    "text": "So to choose c, what\nwe have to check",
    "start": "2195730",
    "end": "2204300"
  },
  {
    "text": "is that p under theta not-- so here it's theta not--",
    "start": "2204300",
    "end": "2209460"
  },
  {
    "text": "that I reject so that\npsi is equal to 1.",
    "start": "2209460",
    "end": "2215550"
  },
  {
    "text": "I want this to be\nequal to alpha, right? That's how I maximize\nmy type I error",
    "start": "2215550",
    "end": "2221010"
  },
  {
    "text": "under the budget that's actually\ngiven to me, which is alpha. So that's actually equivalent\nto checking whether p not of Tn",
    "start": "2221010",
    "end": "2232910"
  },
  {
    "text": "is larger than c. ",
    "start": "2232910",
    "end": "2239270"
  },
  {
    "text": "And so if I want to find\nthe c, all I need to know is what is the\ndistribution of Tn when",
    "start": "2239270",
    "end": "2245670"
  },
  {
    "text": "theta is equal to theta not? Whatever this distribution is--\nmaybe it has some weird density",
    "start": "2245670",
    "end": "2251819"
  },
  {
    "text": "like this-- whatever this\ndistribution is, I'm just going to be able\nto pick this number,",
    "start": "2251820",
    "end": "2257400"
  },
  {
    "text": "and I'm going to take this\nquintile alpha, here alpha, and I'm going to reject\nif I'm larger than alpha--",
    "start": "2257400",
    "end": "2264029"
  },
  {
    "text": "whatever this guy is. So to be able to do\nthat, I need to know what is the distribution of Tn\nwhen theta is equal to theta 0.",
    "start": "2264030",
    "end": "2276890"
  },
  {
    "text": "What is this distribution? What is Tn?",
    "start": "2276890",
    "end": "2282842"
  },
  {
    "text": "It's the norm squared\nof this vector.",
    "start": "2282842",
    "end": "2288720"
  },
  {
    "text": "What is this vector? What is the asymptotic\ndistribution of this vector? ",
    "start": "2288720",
    "end": "2297912"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE]. PHILIPPE RIGOLLET:\nJust look one board up.",
    "start": "2297912",
    "end": "2303400"
  },
  {
    "text": "What is this\nasymptotic distribution of the vector for which we're\ntaking the norm squared? It's right here.",
    "start": "2303400",
    "end": "2310560"
  },
  {
    "text": "It's a standard\nGaussian multivariate. So when I look at\nthe norm squared--",
    "start": "2310560",
    "end": "2316460"
  },
  {
    "text": "so if z is a standard\nGaussian multivariate,",
    "start": "2316460",
    "end": "2325400"
  },
  {
    "text": "then the norm of z squared, by\ndefinition of the norm squared,",
    "start": "2325400",
    "end": "2331880"
  },
  {
    "text": "is the sum of the Zi squared. ",
    "start": "2331880",
    "end": "2341790"
  },
  {
    "text": "That's just the\ndefinition of the norm. But what is this distribution?",
    "start": "2341790",
    "end": "2346973"
  },
  {
    "text": "AUDIENCE: Chi-squared. PHILIPPE RIGOLLET:\nThat's a chi-square, because those guys\nare all of variance 1.",
    "start": "2346973",
    "end": "2352750"
  },
  {
    "text": "That's what the\ndiagonal tells me-- only ones. And they're independent because\nthey have all these zeros",
    "start": "2352750",
    "end": "2358180"
  },
  {
    "text": "outside of the diagonal. So really, this follows some\nchi-squared distribution.",
    "start": "2358180",
    "end": "2363710"
  },
  {
    "text": "How many degrees of freedom? Well, the number of\nthem that I sell, d.",
    "start": "2363710",
    "end": "2370560"
  },
  {
    "text": "So now I have found\nthe distribution of Tn under this guy.",
    "start": "2370560",
    "end": "2375590"
  },
  {
    "text": "And that's true because\nthis is true under h0.",
    "start": "2375590",
    "end": "2381120"
  },
  {
    "text": "If I was not under\nh0, again, I would need to take another guy here. ",
    "start": "2381120",
    "end": "2389430"
  },
  {
    "text": "How did I use the fact that\ntheta is equal to theta 0 when I centered by theta 0?",
    "start": "2389430",
    "end": "2394640"
  },
  {
    "text": "And that was very important. So now what I know is that\nthis is really equal--",
    "start": "2394640",
    "end": "2401090"
  },
  {
    "text": "why did I put 0 here?  So this here is actually equal.",
    "start": "2401090",
    "end": "2410390"
  },
  {
    "text": "So in the end, I need c\nsuch that the probability--",
    "start": "2410390",
    "end": "2423842"
  },
  {
    "text": "and here I'm not going\nto put a theta 0. I'm just talking\nabout the possibility of the random variable that\nI'm going to put in there.",
    "start": "2423843",
    "end": "2429080"
  },
  {
    "text": "It's a chi-square with d\ndegrees of freedom [INAUDIBLE] is equal to alpha. ",
    "start": "2429080",
    "end": "2435200"
  },
  {
    "text": "I just replaced the\nfact that this guy, Tn, under the distribution\nwas just a chi-square.",
    "start": "2435200",
    "end": "2441230"
  },
  {
    "text": "And this distribution\nhere is just really referring to the\ndistribution of a chi-square. There's no parameters here.",
    "start": "2441230",
    "end": "2446819"
  },
  {
    "text": "And now, that means that I look\nat my chi-square distribution. It sort of looks like this.",
    "start": "2446820",
    "end": "2455170"
  },
  {
    "text": "And I'm going to\npick some alpha here, and I need to read\nthis number q alpha.",
    "start": "2455170",
    "end": "2462040"
  },
  {
    "text": " And so here what I need to do\nis to pick this q alpha here,",
    "start": "2462040",
    "end": "2469010"
  },
  {
    "text": "for c. So take c to be q alpha, the\nquintile of order 1 minus",
    "start": "2469010",
    "end": "2488120"
  },
  {
    "text": "alpha of a chi-squared\ndistribution with this d degree of freedom. And why do I say 1 minus alpha?",
    "start": "2488120",
    "end": "2493910"
  },
  {
    "text": "Because again, the\nquintiles are usually referring to the area that's\nto the left of them by--",
    "start": "2493910",
    "end": "2501680"
  },
  {
    "text": "well, actually, it's\nby a convention.",
    "start": "2501680",
    "end": "2507750"
  },
  {
    "text": "However, in statistics, we\nonly care about the right tail usually, so it's not\nvery convenient for us.",
    "start": "2507750",
    "end": "2515000"
  },
  {
    "text": "And that's why\nrather than calling this guy s sub 1 minus alpha all\nthe time, I write it q alpha.",
    "start": "2515000",
    "end": "2521010"
  },
  {
    "text": "So now you have\nthis q alpha, which is the 1 minus alpha quintile,\nor quintile of order 1 minus",
    "start": "2521010",
    "end": "2528599"
  },
  {
    "text": "alpha of chi squared d. And so now I need\nto use a table. For each d, this thing is going\nto take a different value,",
    "start": "2528600",
    "end": "2535680"
  },
  {
    "text": "and this is why I can not\njust spit out a number to you like I spit out 1.96.",
    "start": "2535680",
    "end": "2541650"
  },
  {
    "text": "Because if I were\nable to do that, that would mean that\nI would remember an entire column of this table\nfor each possible value of d,",
    "start": "2541650",
    "end": "2550760"
  },
  {
    "text": "and that I just don't know. So you need just\nto look at tables, and this is what\nit will tell you.",
    "start": "2550760",
    "end": "2556869"
  },
  {
    "text": "Often software\nwill do that, too. You don't have to\nsearch through tables. And so just as a remark is\nthat this test, Wald's test,",
    "start": "2556870",
    "end": "2566400"
  },
  {
    "text": "is also valid when I have\nthis sort of other alternative that I could see quite a lot-- if I actually have what's\ncalled a one-sided alternative.",
    "start": "2566400",
    "end": "2575670"
  },
  {
    "text": "By the way, this is\ncalled Wald's test-- so taking Tn to be this thing.",
    "start": "2575670",
    "end": "2581250"
  },
  {
    "start": "2581250",
    "end": "2589420"
  },
  {
    "text": "So this is Wald's test. Abraham Wald was a\nfamous statistician",
    "start": "2589420",
    "end": "2595170"
  },
  {
    "text": "in the early 20th century,\nwho actually was at Columbia",
    "start": "2595170",
    "end": "2602767"
  },
  {
    "text": "for quite some time. And that was\nactually at the time where statistics were getting\nvery popular in India,",
    "start": "2602768",
    "end": "2613360"
  },
  {
    "text": "so he was actually\ntraveling all over India in some dinky planes. And one of them crashed,\nand that's how he died--",
    "start": "2613360",
    "end": "2621460"
  },
  {
    "text": "pretty young. But actually, there's a\nhuge school of statistics now in India thanks to him.",
    "start": "2621460",
    "end": "2627220"
  },
  {
    "text": "There's the Indian\nStatistical Institute, which is actually\na pretty big thing and trans the best\nstatisticians.",
    "start": "2627220",
    "end": "2633610"
  },
  {
    "text": "So this is called Wald's\ntest, and it's actually a pretty popular test. Let's just look back a second.",
    "start": "2633610",
    "end": "2639360"
  },
  {
    "text": "So you can do the\nother alternatives, as I said, and for\nthe other alternatives you can actually do this\ntrick where you put theta 0 as",
    "start": "2639360",
    "end": "2646260"
  },
  {
    "text": "well, as long as you\ntake the theta 0 that's the closest to the alternative. You just basically take the\none that's the least favorable",
    "start": "2646260",
    "end": "2653190"
  },
  {
    "text": "to you--  the alternative, I mean. So what is this thing doing?",
    "start": "2653190",
    "end": "2661539"
  },
  {
    "text": "If you did not know anything\nabout statistics and I told you here's a vector--",
    "start": "2661540",
    "end": "2666950"
  },
  {
    "text": "that's the mle\nvector, theta hat mle. ",
    "start": "2666950",
    "end": "2672250"
  },
  {
    "text": "So let's say this theta hat\nmle takes the values, say-- ",
    "start": "2672250",
    "end": "2684520"
  },
  {
    "text": "so let's say theta hat mle takes\nvalues, say, 1.2, 0.9, and 2.1.",
    "start": "2684520",
    "end": "2697430"
  },
  {
    "text": "And then testing h0, theta is\nequal to 1, 1, 2, versus theta",
    "start": "2697430",
    "end": "2706880"
  },
  {
    "text": "is not equal to the same number. That's what I'm testing. So you compute this\nthing and you find this.",
    "start": "2706880",
    "end": "2713475"
  },
  {
    "text": "If you don't know\nany statistics, what are you going to do?  You're just going to check\nif this guy goes to that guy,",
    "start": "2713475",
    "end": "2721400"
  },
  {
    "text": "and probably what you're going\nto do is compute something that looks like the norm squared\nbetween those guys-- so",
    "start": "2721400",
    "end": "2727240"
  },
  {
    "text": "the sum. So you're going to do\n1.2 minus 1 squared plus 0.9 minus 1 squared\nplus 2.1 minus 2 squared",
    "start": "2727240",
    "end": "2738740"
  },
  {
    "text": "and check if this\nnumber is large or not. Maybe you are going to apply\nsome stats to try to understand",
    "start": "2738740",
    "end": "2744140"
  },
  {
    "text": "how those things are,\nbut this is basically what you are going\nto want to do.",
    "start": "2744140",
    "end": "2749760"
  },
  {
    "text": "What Wald's test\nis telling you is that this average is actually\nnot what you should be doing.",
    "start": "2749760",
    "end": "2756829"
  },
  {
    "text": "It's telling you that\nyou should have some sort of a weighted average. Actually, it would\nbe a weighted average",
    "start": "2756830",
    "end": "2761837"
  },
  {
    "text": "if I was guaranteed that\nmy Fisher information matrix was diagonal.",
    "start": "2761837",
    "end": "2768090"
  },
  {
    "text": "If my Fisher information\nmatrix is diagonal, looking at this\nnumber minus this guy,",
    "start": "2768090",
    "end": "2773789"
  },
  {
    "text": "transpose i, and then\nthis guy minus this, that would look like I have some\nweight here, some weight here,",
    "start": "2773790",
    "end": "2779030"
  },
  {
    "text": "and some weight here. ",
    "start": "2779030",
    "end": "2785430"
  },
  {
    "text": "Sorry, that's only three. So if it has non-zero numbers\non all of its nine entries,",
    "start": "2785430",
    "end": "2792880"
  },
  {
    "text": "then what I'm going to\nsee is weird cross-terms. If I look at some vector\npre-multiplying this thing",
    "start": "2792880",
    "end": "2801150"
  },
  {
    "text": "and post-multiplying\nthis thing-- so if I look at something\nthat looks like this, x transpose i of theta\nnot, x transpose--",
    "start": "2801150",
    "end": "2811200"
  },
  {
    "text": "think of x as being theta\nhat mle minus theta--",
    "start": "2811200",
    "end": "2816270"
  },
  {
    "text": "so if I look at what\nthis guy looks like, it's basically a sum over i and\nj of Xi, Xj, i, theta not Ij.",
    "start": "2816270",
    "end": "2828330"
  },
  {
    "text": "And so if none of\nthose things are 0, you're not going to see a sum\nof three terms that are squares,",
    "start": "2828330",
    "end": "2834400"
  },
  {
    "text": "but you're going to see a\nsum of nine cross-products. And it's just weird.",
    "start": "2834400",
    "end": "2840030"
  },
  {
    "text": "This is not something standard. So what is Wald's\ntest doing for you?",
    "start": "2840030",
    "end": "2846450"
  },
  {
    "text": "Well, it's saying,\nI'm actually going to look at all the\ndirections all at once.",
    "start": "2846450",
    "end": "2852283"
  },
  {
    "text": "Some of those\ndirections are going to have more or less variance,\ni.e., less or more information.",
    "start": "2852283",
    "end": "2861660"
  },
  {
    "text": "And so for those\nguys, I'm actually going to use a different weight. So what you're really\ndoing is putting a weight",
    "start": "2861660",
    "end": "2867640"
  },
  {
    "text": "on all directions of\nthe space at once. So what this Wald's\ntest is doing--",
    "start": "2867640",
    "end": "2873280"
  },
  {
    "text": "by squeezing in the\nFisher information matrix, it's placing your problem\ninto the right geometry.",
    "start": "2873280",
    "end": "2880839"
  },
  {
    "text": "It's a geometry that's distorted\nand where balls become ellipses that are distorted\nin some directions",
    "start": "2880840",
    "end": "2887860"
  },
  {
    "text": "and shrunk in\nothers, or depending on if you have more variance\nor less variance in those directions.",
    "start": "2887860",
    "end": "2893565"
  },
  {
    "text": "Those directions\ndon't have to be aligned with the axes of\nyour coordinate system. And if they were,\nthen that would",
    "start": "2893565",
    "end": "2899920"
  },
  {
    "text": "mean you would have a\ndiagonal information matrix, but they might not be.",
    "start": "2899920",
    "end": "2905800"
  },
  {
    "text": "And so there's this weird\ngeometry that shows up. There is actually\nan entire field,",
    "start": "2905800",
    "end": "2911410"
  },
  {
    "text": "admittedly a bit\ndormant these days, that's called\ninformation geometry, and it's really doing\ndifferential geometry",
    "start": "2911410",
    "end": "2919060"
  },
  {
    "text": "on spaces that are defined by\nFisher information matrices.",
    "start": "2919060",
    "end": "2924270"
  },
  {
    "text": "And so you can do\nsome pretty hardcore-- something that I\ncertainly cannot do--",
    "start": "2924270",
    "end": "2930220"
  },
  {
    "text": "differential geometry , just by\nplaying around with statistical models and trying to understand\nwith the geometry of those",
    "start": "2930220",
    "end": "2935830"
  },
  {
    "text": "models are. What does it mean\nfor two points to be close in some curved space?",
    "start": "2935830",
    "end": "2941570"
  },
  {
    "text": "So that's basically the idea. So this thing is basically\ncurving your space. So again, I always\nfeel satisfied",
    "start": "2941570",
    "end": "2950250"
  },
  {
    "text": "when my estimator\non my test does not involve just\ncomputing an average and checking if it's big or not.",
    "start": "2950250",
    "end": "2956520"
  },
  {
    "text": "And that's not what\nwe're doing here. We know that this theta hat\nmle can be complicated--",
    "start": "2956520",
    "end": "2963350"
  },
  {
    "text": "CF problem set, too, I believe. And we know that this Fisher\ninformation matrix can also",
    "start": "2963350",
    "end": "2969093"
  },
  {
    "text": "be pretty complicated. So here, your test is not\ngoing to be trivial at all, and that requires understanding\nthe mathematics behind it.",
    "start": "2969093",
    "end": "2977000"
  },
  {
    "text": "I mean, it all built\nupon this theorem that I just erased,\nI believe, which",
    "start": "2977000",
    "end": "2983540"
  },
  {
    "text": "was that this guy\nhere inside this norm was actually converging\nto some standard Gaussian. ",
    "start": "2983540",
    "end": "2992690"
  },
  {
    "text": "So there's another test\nthat you can actually use. So Wald's test is one option,\nand there's another option.",
    "start": "2992690",
    "end": "3000800"
  },
  {
    "text": "And just like maximum\nlikelihood estimation and method of moments would sometimes\nagree and sometimes disagree,",
    "start": "3000800",
    "end": "3009450"
  },
  {
    "text": "those guys are going to\nsometimes agree and sometimes disagree. And this test is called\nthe likelihood ratio test.",
    "start": "3009450",
    "end": "3017510"
  },
  {
    "text": "So let's parse those words-- \"likelihood,\" \"ratio,\" \"test.\"",
    "start": "3017510",
    "end": "3025322"
  },
  {
    "text": "So at some point,\nI'm going to have to take the likelihood\nof something divided by the likelihood of some other\nthing and then work with this.",
    "start": "3025322",
    "end": "3033980"
  },
  {
    "text": "And this test is just\nsaying the following. Here's the simplest\nprinciple you can think of.",
    "start": "3033980",
    "end": "3039653"
  },
  {
    "text": " You're going to\nhave to understand",
    "start": "3039654",
    "end": "3045930"
  },
  {
    "text": "the notion of likelihood in\nthe context of statistics.",
    "start": "3045930",
    "end": "3051440"
  },
  {
    "text": "You just have to understand\nthe meaning of the word \"likelihood.\" This test is just saying\nif I want to test h0,",
    "start": "3051440",
    "end": "3063740"
  },
  {
    "text": "theta is equal to theta 0,\nversus theta is equal to theta 1, all I have to look at is\nwhether theta 0 is more or less",
    "start": "3063740",
    "end": "3073040"
  },
  {
    "text": "likely than theta 1. And I have an exact\nnumber that spits out.",
    "start": "3073040",
    "end": "3078960"
  },
  {
    "text": "Given a theta 0 or a\ntheta 1 and given data,",
    "start": "3078960",
    "end": "3084760"
  },
  {
    "text": "I can put in this function\ncalled the likelihood, and they tell me exactly\nhow likely those things are.",
    "start": "3084760",
    "end": "3091630"
  },
  {
    "text": "And so all I have to\ncheck is whether one is more likely than the\nother, and so what I can do is form the likelihood\nof theta, say,",
    "start": "3091630",
    "end": "3101450"
  },
  {
    "text": "1 divided by the\nlikelihood of theta 0",
    "start": "3101450",
    "end": "3110070"
  },
  {
    "text": "and check if this\nthing is larger than 1. That would mean that this guy\nis more likely than that guy.",
    "start": "3110070",
    "end": "3117090"
  },
  {
    "text": "That's a natural way to proceed. Now, there's one\ncaveat here, which",
    "start": "3117090",
    "end": "3123190"
  },
  {
    "text": "is that when I do\nhypothesis testing and I have this asymmetry\nbetween h0 and h1,",
    "start": "3123190",
    "end": "3130960"
  },
  {
    "text": "I still need to be\nable to control what my probably of type I error is. And here I basically\nhave no knob.",
    "start": "3130960",
    "end": "3139260"
  },
  {
    "text": "This is something if you\ngive me data in theta 0 and theta 1 I can compute to you\nand spit out the yes/no answer.",
    "start": "3139260",
    "end": "3144470"
  },
  {
    "text": "But I have no way of controlling\nthe type II and type I error,",
    "start": "3144470",
    "end": "3149720"
  },
  {
    "text": "so what we do is that we\nreplace this 1 by some number c. And then we calibrate\nc in such a way",
    "start": "3149720",
    "end": "3155300"
  },
  {
    "text": "that the type I error is\nexactly at level alpha. ",
    "start": "3155300",
    "end": "3160630"
  },
  {
    "text": "So for example, if\nI want to make sure that my type I error is\nalways 0, all I have to do",
    "start": "3160630",
    "end": "3170610"
  },
  {
    "text": "is to say that this\nguy is actually never more likely than that\nguy, meaning never reject. And so if I let\nc go to infinity,",
    "start": "3170610",
    "end": "3177912"
  },
  {
    "text": "then this is actually\ngoing to make my type I error go to zero. But if I let c go to\nnegative infinity,",
    "start": "3177912",
    "end": "3185790"
  },
  {
    "text": "then I'm always\ngoing to conclude",
    "start": "3185790",
    "end": "3192270"
  },
  {
    "text": "that h1 is the right one. So I have this\nstraight off, and I can turn this knob by\nchanging the values of c",
    "start": "3192270",
    "end": "3199349"
  },
  {
    "text": "and get different results. And I'm going to be interested\nin the one that maximizes",
    "start": "3199350",
    "end": "3205890"
  },
  {
    "text": "my chances of rejecting the\nnull hypothesis while staying under my alpha budget\nof type I error.",
    "start": "3205890",
    "end": "3213500"
  },
  {
    "text": "So this is nice when I have\ntwo very simple hypotheses, but to be fair, we've\nactually not seen",
    "start": "3213500",
    "end": "3220430"
  },
  {
    "text": "any tests that correspond\nto real-life example. Where theta 0 was of the\nform am I equal to, say, 0.5",
    "start": "3220430",
    "end": "3229070"
  },
  {
    "text": "or am I equal to\n0.41, we actually sort of suspected\nthat if somebody asked you to perform\nthis test, they've",
    "start": "3229070",
    "end": "3234895"
  },
  {
    "text": "sort of seen the data before\nand they're sort of cheating. So it's typically\nsomething am I equal to 0.5",
    "start": "3234895",
    "end": "3240290"
  },
  {
    "text": "or not equal to 0.5\nor am I equal to 0.5 or larger than 0.5. But it's very rare that you\nactually get only two points",
    "start": "3240290",
    "end": "3246830"
  },
  {
    "text": "to test-- am I this guy or that guy? Now, I could go on. There's actually a nice\nmathematical theory,",
    "start": "3246830",
    "end": "3253432"
  },
  {
    "text": "something called the\nNeyman-Pearson lemma that actually tells me that\nthis test, the likelihood ratio",
    "start": "3253432",
    "end": "3258470"
  },
  {
    "text": "test, is the test, given the\nconstraint of type I error, that will have the\nsmallest type II error.",
    "start": "3258470",
    "end": "3265220"
  },
  {
    "text": "So this is the ultimate test. No one should ever use\nanything different. And we could go on and\ndo this, but in a way,",
    "start": "3265220",
    "end": "3272420"
  },
  {
    "text": "it's completely irrelevant to\npractice because you will never encounter such tests. And I actually find students\nthat they took my class",
    "start": "3272420",
    "end": "3281000"
  },
  {
    "text": "as sophomores and then they're\nstill around a couple of years later and they're\ndoing, and they're like,",
    "start": "3281000",
    "end": "3286930"
  },
  {
    "text": "I have this testing problem and\nI want to use likelihood ratio test, the Neyman-Pearson one,\nbut I just can't because it",
    "start": "3286930",
    "end": "3294740"
  },
  {
    "text": "just never occurs. This just does not happen. So here, rather than\ngoing into details,",
    "start": "3294740",
    "end": "3299750"
  },
  {
    "text": "let's just look at what\nbuilding on this principle we can actually make\na test that will work.",
    "start": "3299750",
    "end": "3305570"
  },
  {
    "text": "So now, for\nsimplicity, I'm going to assume that my\nalternatives-- so now, I still",
    "start": "3305570",
    "end": "3311810"
  },
  {
    "text": "have a d dimensional\nvector theta. And what I'm going to assume\nis that the null hypothesis",
    "start": "3311810",
    "end": "3320840"
  },
  {
    "text": "is actually only testing if\nthe last coefficients from r",
    "start": "3320840",
    "end": "3326750"
  },
  {
    "text": "plus 1 to d are fixed numbers. So in this example, where\nI have theta was equal--",
    "start": "3326750",
    "end": "3335460"
  },
  {
    "text": "so if I have d equals\n3, here's an example. ",
    "start": "3335460",
    "end": "3342120"
  },
  {
    "text": "h0 is theta 2 equals 1,\nand theta 3 equals 2.",
    "start": "3342120",
    "end": "3353510"
  },
  {
    "text": "That's my h0, but I\nsay I don't actually care about what theta\n1 is going to be. ",
    "start": "3353510",
    "end": "3362450"
  },
  {
    "text": "So that's my null hypothesis. I'm not going to specify right\nnow what the alternative is.",
    "start": "3362450",
    "end": "3367500"
  },
  {
    "text": "That's what the null is. And in particular, this null\nis actually not of this form.",
    "start": "3367500",
    "end": "3373240"
  },
  {
    "text": "It's not restricting\nit to one point. It's actually restricting it to\nan infinite amount of points. Those are all the vectors\nof the form theta 1 1,",
    "start": "3373240",
    "end": "3382020"
  },
  {
    "text": "2 for all theta 1 in, say, r.",
    "start": "3382020",
    "end": "3389440"
  },
  {
    "text": "That's a lot of vectors,\nand so it's certainly not like it's equal to\none specific vector. ",
    "start": "3389440",
    "end": "3396670"
  },
  {
    "text": "So now, what I'm going\nto do is I'm actually going to look at the maximum\nlikelihood estimator,",
    "start": "3396670",
    "end": "3403300"
  },
  {
    "text": "and I'm going to say, well, the\nmaximum likelihood estimator, regardless of anything, is\ngoing to be close to. reality.",
    "start": "3403300",
    "end": "3410309"
  },
  {
    "text": "Now, if you actually\ntell me ahead of time that the true parameter\nis of this form,",
    "start": "3410310",
    "end": "3416520"
  },
  {
    "text": "I'm not going to maximize over\nall three coordinates of theta. I'm just going to say,\nwell, I might as well just",
    "start": "3416520",
    "end": "3421740"
  },
  {
    "text": "set the second one to\n1, the third one to 2,",
    "start": "3421740",
    "end": "3426900"
  },
  {
    "text": "and just optimize for this guy. So effectively, you can\nsay if you're telling me",
    "start": "3426900",
    "end": "3431990"
  },
  {
    "text": "that this is the\nreality, I can compute a constrained maximum\nlikelihood estimator",
    "start": "3431990",
    "end": "3437000"
  },
  {
    "text": "which is constrained to look\nlike what you think reality is. So this is what the maximum\nlikelihood estimator is.",
    "start": "3437000",
    "end": "3444270"
  },
  {
    "text": "That's the one that's\nmaximizing, say, here the log likelihood over\nthe entire space of candidate",
    "start": "3444270",
    "end": "3450119"
  },
  {
    "text": "vectors, of\ncandidate parameters. But this partial one, this\nis the constraint mle.",
    "start": "3450120",
    "end": "3456357"
  },
  {
    "text": "That's the one that's actually\nnot maximizing our real thetas, but only over the thetas\nthat are plausible under the null hypothesis.",
    "start": "3456357",
    "end": "3464430"
  },
  {
    "text": "So in particular, if I look\nat ln of this constraint thing",
    "start": "3464430",
    "end": "3472880"
  },
  {
    "text": "theta hat n c compared\nto ln, theta hat--",
    "start": "3472880",
    "end": "3479839"
  },
  {
    "text": "let's say n mle, so\nwe know which one-- which one is bigger?",
    "start": "3479840",
    "end": "3485260"
  },
  {
    "start": "3485260",
    "end": "3493400"
  },
  {
    "text": "The first one is bigger. So why? AUDIENCE: [INAUDIBLE].",
    "start": "3493400",
    "end": "3498755"
  },
  {
    "text": " PHILIPPE RIGOLLET:\nSo the second one is maximized over\na larger space.",
    "start": "3498755",
    "end": "3505070"
  },
  {
    "text": "Right. So I have this all\nof theta, which are all the\nparameters I can take,",
    "start": "3505070",
    "end": "3510250"
  },
  {
    "text": "and let's say theta\n0 is this guy. I'm maximizing a function\nover all these things.",
    "start": "3510250",
    "end": "3515990"
  },
  {
    "text": "So if the true\nmaximum is this here, then the two things are\nequal, but if the maximum",
    "start": "3515990",
    "end": "3521210"
  },
  {
    "text": "is on this side, then\nthe one on the right is actually going to be larger. They're maximizing\nover a bigger space,",
    "start": "3521210",
    "end": "3528050"
  },
  {
    "text": "so this guy has to be\nless than this guy. So maybe it's not easy to see.",
    "start": "3528050",
    "end": "3533450"
  },
  {
    "text": "So let's say that this is\ntheta and this is theta 0",
    "start": "3533450",
    "end": "3541609"
  },
  {
    "text": "and now I have a function. The maximum over theta\n0 is this guy here,",
    "start": "3541610",
    "end": "3549720"
  },
  {
    "text": "but the maximum over the\nentire space is here. ",
    "start": "3549720",
    "end": "3555530"
  },
  {
    "text": "So the maximum\nover a larger space has to be larger than the\nmaximum over a smaller space. It can be equal, but the\none in the bigger space",
    "start": "3555530",
    "end": "3566090"
  },
  {
    "text": "can be even bigger. However, if my\ntrue theta actually",
    "start": "3566090",
    "end": "3573730"
  },
  {
    "text": "did belong to theta 0-- if h0 was true--",
    "start": "3573730",
    "end": "3578880"
  },
  {
    "text": "what would happen? Well, if theta 0 is true,\nthen theta isn't theta 0,",
    "start": "3578880",
    "end": "3585930"
  },
  {
    "text": "and since the maximum likelihood\nshould be close to theta, it should be the case that\nthose two things should",
    "start": "3585930",
    "end": "3591570"
  },
  {
    "text": "be pretty similar. I should be in a case not\nin this kind of thing, but more in this\nkind of position,",
    "start": "3591570",
    "end": "3598110"
  },
  {
    "text": "where the true maximum is\nactually attained at theta 0. And in this case,\nthey're actually of the same size,\nthose two things.",
    "start": "3598110",
    "end": "3605640"
  },
  {
    "text": "If it's not true, then I'm\ngoing to see a discrepancy between the two guys. ",
    "start": "3605640",
    "end": "3612030"
  },
  {
    "text": "So my test is going to be\nbuilt on this intuition that if h0 is true, the values\nof the likelihood at theta hat",
    "start": "3612030",
    "end": "3620700"
  },
  {
    "text": "mle and at the constraint mle\nshould be pretty much the same. But if theta hat-- if it's not true, then\nthe likelihood of the mle",
    "start": "3620700",
    "end": "3629490"
  },
  {
    "text": "should be much larger\nthan the likelihood of the constrained mle.",
    "start": "3629490",
    "end": "3634730"
  },
  {
    "text": " And this is exactly\nwhat this test is doing.",
    "start": "3634730",
    "end": "3640580"
  },
  {
    "text": "So that's the\nlikelihood ratio test. So rather than looking at\nthe ratio of the likelihoods,",
    "start": "3640580",
    "end": "3646660"
  },
  {
    "text": "we look at the difference\nof the log likelihood, which is really the same thing. And there is some weird\nnormalization factor, too,",
    "start": "3646660",
    "end": "3654420"
  },
  {
    "text": "that shows up here. ",
    "start": "3654420",
    "end": "3664910"
  },
  {
    "text": "And this is what we get. So if I look at the\nlikelihood ratio test,",
    "start": "3664910",
    "end": "3678900"
  },
  {
    "text": "so it's looking at two\ntimes ln of theta hat mle",
    "start": "3678900",
    "end": "3685279"
  },
  {
    "text": "minus ln of theta\nhat mle constrained.",
    "start": "3685280",
    "end": "3692070"
  },
  {
    "text": "And this is actually\nthe test statistic. So we've actually decided\nthat this statistic is what?",
    "start": "3692070",
    "end": "3699810"
  },
  {
    "text": " It's non-negative, right? We've also decided\nthat it should",
    "start": "3699810",
    "end": "3705940"
  },
  {
    "text": "be close to zero if h0\nis true and of course then maybe far from\nzero if h0 is not true.",
    "start": "3705940",
    "end": "3712990"
  },
  {
    "text": "So what should be the\nnatural test based on Tn?",
    "start": "3712990",
    "end": "3720320"
  },
  {
    "text": "Let me just check that it's-- well, it's already there.",
    "start": "3720320",
    "end": "3725370"
  },
  {
    "text": "So the natural test is something\nthat looks like indicator that Tn is larger than c.",
    "start": "3725370",
    "end": "3732480"
  },
  {
    "text": "And you should say, well, again? I mean, we just did that. I mean, it is basically the\nsame thing that we just did.",
    "start": "3732480",
    "end": "3739490"
  },
  {
    "text": "Agreed? But the Tn now is different. The Tn is the difference\nof log likelihoods, whereas before the Tn was\nthis theta hat minus theta",
    "start": "3739490",
    "end": "3749970"
  },
  {
    "text": "not transpose identity of\nFisher information matrix theta",
    "start": "3749970",
    "end": "3755630"
  },
  {
    "text": "hat minus theta not. And this, there's no\nreason why this guy should be of the same form.",
    "start": "3755630",
    "end": "3761410"
  },
  {
    "text": "Now, if I have a\nGaussian model, you can check that those two things\nare actually exactly the same. ",
    "start": "3761410",
    "end": "3769040"
  },
  {
    "text": "But otherwise, they don't\nhave any reason to be. And now, what's\nhappening is that",
    "start": "3769040",
    "end": "3774220"
  },
  {
    "text": "under some technical\nconditions-- if h0 is true, now\nwhat happens is that if I want to calibrate\nc, what I need to do",
    "start": "3774220",
    "end": "3782690"
  },
  {
    "text": "is to look at what is the\nc such that this guy is",
    "start": "3782690",
    "end": "3788630"
  },
  {
    "text": "equal to alpha? And that's for the distribution\nof T under the knob.",
    "start": "3788630",
    "end": "3795775"
  },
  {
    "text": " But there's not only one.",
    "start": "3795775",
    "end": "3802050"
  },
  {
    "text": "The null hypothesis\nhere was actually just the family of things.",
    "start": "3802050",
    "end": "3808050"
  },
  {
    "text": "It was not just one vector. It was an entire\nfamily of vectors, just like in this example.",
    "start": "3808050",
    "end": "3813520"
  },
  {
    "text": "So if I want my type I\nerror to be constrained over the entire space,\nwhat I need to make sure of",
    "start": "3813520",
    "end": "3819119"
  },
  {
    "text": "is that the maximum\noverall theta n theta not",
    "start": "3819120",
    "end": "3824440"
  },
  {
    "text": "is actually equal to alpha. ",
    "start": "3824440",
    "end": "3833152"
  },
  {
    "text": "Agreed? Yeah? AUDIENCE: [INAUDIBLE]. ",
    "start": "3833152",
    "end": "3839520"
  },
  {
    "text": "PHILIPPE RIGOLLET: So not equal. In this case, it's\ngoing to be not equal.",
    "start": "3839520",
    "end": "3846858"
  },
  {
    "text": "I mean, it can really\nbe anything you want. It's just you're going to have\na different type II error.",
    "start": "3846858",
    "end": "3852670"
  },
  {
    "text": "I guess here we're sort\nof stuck in a corner. We built this T, and it has\nto be small under the null.",
    "start": "3852670",
    "end": "3858740"
  },
  {
    "text": "And whatever not\nthe null is, we just hope that it's\ngoing to be large. ",
    "start": "3858740",
    "end": "3865150"
  },
  {
    "text": "So even if I tell you\nwhat the alternative is, you're not going to change\nanything about the procedure.",
    "start": "3865150",
    "end": "3871660"
  },
  {
    "text": "So here, q alpha-- so\nwhat I need to know is that if h0 is true,\nthen Tn in this case",
    "start": "3871660",
    "end": "3877540"
  },
  {
    "text": "actually converges to some\nchi-square distribution. And now here, the number\nof degrees of freedom",
    "start": "3877540",
    "end": "3884500"
  },
  {
    "text": "is kind of weird. ",
    "start": "3884500",
    "end": "3898720"
  },
  {
    "text": "But actually, what it should\ntell you is, oh, finally, I know when you call this\nparameter degrees of freedom",
    "start": "3898720",
    "end": "3905030"
  },
  {
    "text": "rather than dimension\nor just d parameter. It's because here what we did\nis we actually pinned down",
    "start": "3905030",
    "end": "3913100"
  },
  {
    "text": "everything, but r--",
    "start": "3913100",
    "end": "3919330"
  },
  {
    "text": "sorry, we pinned\ndown everything but r coordinates of this thing. ",
    "start": "3919330",
    "end": "3926710"
  },
  {
    "text": "And so now I'm actually\nwondering why-- ",
    "start": "3926710",
    "end": "3934102"
  },
  {
    "text": "did I make a mistake here? ",
    "start": "3934102",
    "end": "3940460"
  },
  {
    "text": "I think this should\nbe chi square with r degrees of freedom. ",
    "start": "3940460",
    "end": "3946290"
  },
  {
    "text": "Let me check and send\nyou an update about this, because the number of\ndegrees of freedom,",
    "start": "3946290",
    "end": "3953140"
  },
  {
    "text": "if you talk to normal\npeople they will tell you that here the number of\ndegrees of freedom is r.",
    "start": "3953140",
    "end": "3959830"
  },
  {
    "text": "This is what's allowed\nto move, and that's what's called\ndegrees of freedom. The rest is pinned down\nto being something.",
    "start": "3959830",
    "end": "3966520"
  },
  {
    "text": "So here, this chi-square\nshould be a chi-squared r. And that's something you\njust have to believe me.",
    "start": "3966520",
    "end": "3972993"
  },
  {
    "text": "Anybody guess what theorem\nis going to tell me this? ",
    "start": "3972993",
    "end": "3979049"
  },
  {
    "text": "In some cases, it's going\nto be Cochran's theorem-- just something that tells\nme that thing's [INAUDIBLE].. Now, here, I use the\nvery specific form",
    "start": "3979050",
    "end": "3987020"
  },
  {
    "text": "of the null alternative. And so for those\nof you who are sort of familiar with linear\nalgebra, what I did here is h0",
    "start": "3987020",
    "end": "3995740"
  },
  {
    "text": "consists in saying\nthat theta belongs to an r dimensional\nlinear space.",
    "start": "3995740",
    "end": "4003040"
  },
  {
    "text": "It's actually here, the r\ndimensional linear space of vectors, that have the first\nr coordinates that can move",
    "start": "4003040",
    "end": "4009160"
  },
  {
    "text": "and the last coordinates that\nare fixed to some number.",
    "start": "4009160",
    "end": "4014688"
  },
  {
    "text": "Actually, it's undefined space\nbecause it doesn't necessarily go through zero. And so I have this\ndefined space that",
    "start": "4014688",
    "end": "4020410"
  },
  {
    "text": "has dimension r, and if I were\nto constrain it to any other r",
    "start": "4020410",
    "end": "4025555"
  },
  {
    "text": "dimensional space, that would\nbe exactly the same thing. And so to do that, essentially\nwhat you need to do is to say,",
    "start": "4025555",
    "end": "4030910"
  },
  {
    "text": "if I take any matrix that's say,\ninvertible-- let's call it u-- and then so h0 is going to be\nsomething like of the form u",
    "start": "4030910",
    "end": "4041500"
  },
  {
    "text": "times theta and now I look only\nat the coordinates r plus 1 2d,",
    "start": "4041500",
    "end": "4053210"
  },
  {
    "text": "then I want to fix those\nguys to some numbers. So I want to call them theta,\nso let's call them tau.",
    "start": "4053210",
    "end": "4059040"
  },
  {
    "text": "So it's going to be tau r\nplus 1, all the way to tau d.",
    "start": "4059040",
    "end": "4064850"
  },
  {
    "text": "So this is not part\nof the requirements, but just so you know,\nit's really not a matter",
    "start": "4064850",
    "end": "4070075"
  },
  {
    "text": "of keeping only\nsome coordinates. Really, what matters\nis the dimension in the sense of linear\nsubspaces of the problem,",
    "start": "4070075",
    "end": "4076980"
  },
  {
    "text": "and that's what determines what\nyour degrees of freedom are. ",
    "start": "4076980",
    "end": "4083000"
  },
  {
    "text": "So now that we know what the\nasymptotic distribution is under the null, then\nwe know basically",
    "start": "4083000",
    "end": "4090630"
  },
  {
    "text": "that we know how which table we\nneed to pick our q alpha from.",
    "start": "4090630",
    "end": "4097920"
  },
  {
    "text": "And here, again, the table\nis a chi-squared table, but here, the number\nof degrees of freedom is this weird d minus r\ndegrees of freedom thing.",
    "start": "4097920",
    "end": "4106276"
  },
  {
    "text": " I just said it was r. ",
    "start": "4106277",
    "end": "4114060"
  },
  {
    "text": "I'm just checking,\nactually, if I'm-- ",
    "start": "4114060",
    "end": "4121542"
  },
  {
    "text": "it's r. It's definitely r. ",
    "start": "4121542",
    "end": "4131200"
  },
  {
    "text": "So here we've made tests. We're testing if r parameter\ntheta was explicitly",
    "start": "4131200",
    "end": "4137170"
  },
  {
    "text": "in some set or not. By explicitly, I mean we're\nsaying, is theta like this",
    "start": "4137170",
    "end": "4143140"
  },
  {
    "text": "or is theta not like this? Is theta equal to\ntheta not or is theta not equal to theta not? Are the last\ncoordinates of theta",
    "start": "4143140",
    "end": "4150160"
  },
  {
    "text": "equal to those fixed\nnumbers, or are they not? There was something I was\nstating directly about theta.",
    "start": "4150160",
    "end": "4155555"
  },
  {
    "text": "But there's going to be some\ninstances where you actually want to test something\nabout a function of theta,",
    "start": "4155555",
    "end": "4161200"
  },
  {
    "text": "not theta itself. For example, is the difference\nbetween the first coordinate",
    "start": "4161200",
    "end": "4167350"
  },
  {
    "text": "of theta and the second\ncoordinate of theta positive? That's definitely something\nyou might want to test,",
    "start": "4167350",
    "end": "4172839"
  },
  {
    "text": "because maybe theta 1 is-- let me try to think\nof some good example.",
    "start": "4172840",
    "end": "4179185"
  },
  {
    "start": "4179185",
    "end": "4184618"
  },
  {
    "text": "I don't know. Maybe theta 1 is your drawing\naccuracy with the right hand",
    "start": "4184618",
    "end": "4189778"
  },
  {
    "text": "and theta 2 is the drawing\naccuracy with the left hand, and I'm actually collecting\ndata on young children",
    "start": "4189779",
    "end": "4196320"
  },
  {
    "text": "to be able to test\nearly on whether they're going to be left-handed or\nright-handed, for example.",
    "start": "4196320",
    "end": "4201810"
  },
  {
    "text": "And so I want to just compare\nthose two with respect to each other, but\nI don't necessarily need to know what the absolute\nscore for this handwriting",
    "start": "4201810",
    "end": "4210300"
  },
  {
    "text": "skills are. So sometimes it's just\ninteresting to look at the difference of\nthings or maybe the sum,",
    "start": "4210300",
    "end": "4217520"
  },
  {
    "text": "say the combined effect. Maybe this is my two\nmeasurements of blood pressure,",
    "start": "4217520",
    "end": "4222690"
  },
  {
    "text": "and I just want to talk about\nthe average blood pressure. And so I can make a linear\ncombination of those two,",
    "start": "4222690",
    "end": "4228040"
  },
  {
    "text": "and so those things\nimplicitly depend on theta. And so I can generically\nencapsule them",
    "start": "4228040",
    "end": "4236460"
  },
  {
    "text": "in some test of the form\ng of theta is equal to 0 versus g of theta\nis not equal to 0.",
    "start": "4236460",
    "end": "4242400"
  },
  {
    "text": "And sometimes, in the first\ntest that we saw, g of theta was just the identity or\nmaybe the identity minus 0.5.",
    "start": "4242400",
    "end": "4253350"
  },
  {
    "text": "If g of theta is\ntheta minus 0.5, that's exactly what\nwe've been testing. If g of theta is theta\nminus 0.5 and theta",
    "start": "4253350",
    "end": "4261909"
  },
  {
    "text": "is p, the parameter of a coin,\nthis is exactly of this form. So this is a simple\none, but then there's",
    "start": "4261910",
    "end": "4268930"
  },
  {
    "text": "more complicated\nones we can think of. ",
    "start": "4268930",
    "end": "4274830"
  },
  {
    "text": "So how can I do this?",
    "start": "4274830",
    "end": "4280100"
  },
  {
    "text": "Well, let's just\nfollow a recipe.  So we traced back.",
    "start": "4280100",
    "end": "4286210"
  },
  {
    "text": "We were trying to build a test\nstatistic which was pivotal.",
    "start": "4286210",
    "end": "4291995"
  },
  {
    "text": "We wanted to have\nthis thing that had nothing that depended\non the parameter,",
    "start": "4291995",
    "end": "4297220"
  },
  {
    "text": "and the only thing\nwe had for that that we built in\nour chi-square test one is basically some form\nof central limit theorem.",
    "start": "4297220",
    "end": "4304270"
  },
  {
    "text": "Maybe it's for the maximum\nlikelihood estimator. Maybe it's for the\naverage, but it's basically some form of asymptotic\nnormality of the estimator.",
    "start": "4304270",
    "end": "4312610"
  },
  {
    "text": "And that's what we started\nfrom every single time. So let's assume\nthat I have this,",
    "start": "4312610",
    "end": "4318400"
  },
  {
    "text": "and I'm going to\ntalk very abstractly. Let's assume that I\nstart with an estimator. Doesn't have to be the mle.",
    "start": "4318400",
    "end": "4324880"
  },
  {
    "text": "It doesn't have\nto be the average, but it's just something. And I know that I have the\nestimator such that this guy",
    "start": "4324880",
    "end": "4331960"
  },
  {
    "text": "converges in\ndistribution to some n0, and I have some\ncovariance matrix theta.",
    "start": "4331960",
    "end": "4337900"
  },
  {
    "text": "Maybe it's not the\nFisher information. Maybe that's something that's\nnot as good as the mle,",
    "start": "4337900",
    "end": "4343060"
  },
  {
    "text": "meaning that this\nis going to give me less information than the Fisher\ninformation, less accuracy.",
    "start": "4343060",
    "end": "4349160"
  },
  {
    "text": "And now I can actually just say,\nOK, if I know this about theta, I can apply the multivariate\ndelta method, which tells me",
    "start": "4349160",
    "end": "4363920"
  },
  {
    "text": "that square root of n, g of\ntheta hat, minus g of theta",
    "start": "4363920",
    "end": "4370050"
  },
  {
    "text": "goes in distribution to some n0.",
    "start": "4370050",
    "end": "4376170"
  },
  {
    "text": "And then the price to\npay in one dimension was multiplying the square\nroot of the derivative, and we know that in multivariate\ndimensions pre-multiplying",
    "start": "4376170",
    "end": "4383730"
  },
  {
    "text": "by the gradient,\npost-multiplying by the gradient. So I'm going to write delta\ng of theta transpose sigma--",
    "start": "4383730",
    "end": "4394060"
  },
  {
    "text": "sorry, not delta; nabla-- g of theta-- so gradient.",
    "start": "4394060",
    "end": "4399090"
  },
  {
    "text": "And here, I assume that\ng takes values into rk.",
    "start": "4399090",
    "end": "4405420"
  },
  {
    "text": "That's what's written here.\ng takes value from d to k, but think of k as\nbeing 1 for now.",
    "start": "4405420",
    "end": "4410970"
  },
  {
    "text": "So the gradient is really just\na vector and not a matrix. That's your usual gradient\nfor real valid functions.",
    "start": "4410970",
    "end": "4420680"
  },
  {
    "text": "So effectively, if g takes\nvalues in dimension 1,",
    "start": "4420680",
    "end": "4425797"
  },
  {
    "text": "what is the size of this matrix? ",
    "start": "4425797",
    "end": "4438389"
  },
  {
    "text": "I only ask trivial questions. Remember, that's\nrule number one. It's one by one, right?",
    "start": "4438390",
    "end": "4444320"
  },
  {
    "text": "And you can check it,\nbecause on this side those are just the\ndifference between numbers. And it would be kind\nof weird if they had",
    "start": "4444320",
    "end": "4450200"
  },
  {
    "text": "a covariance matrix at the end. I mean, this is a random\nvariable, not a random vector. So I know that\nthis thing happens.",
    "start": "4450200",
    "end": "4457400"
  },
  {
    "text": "And now, if I basically\ndivide by the square root of this thing-- ",
    "start": "4457400",
    "end": "4470210"
  },
  {
    "text": "so for board I'm working with k\nis equal to 1 divided by square",
    "start": "4470210",
    "end": "4475400"
  },
  {
    "text": "root of delta g of theta\ntranspose sigma delta nabla--",
    "start": "4475400",
    "end": "4481735"
  },
  {
    "text": "sorry, g of theta--  then this thing should go to\nsome standard normal random",
    "start": "4481735",
    "end": "4491580"
  },
  {
    "text": "variable, standard\nnormal distribution.",
    "start": "4491580",
    "end": "4496890"
  },
  {
    "text": "I just divided by square\nroot of the variance here, which is the usual thing. Now, if you do not have\na univariate thing,",
    "start": "4496890",
    "end": "4505580"
  },
  {
    "text": "you do the same\nthing we did before, which is 3 multiplied\nby the covariance matrix",
    "start": "4505580",
    "end": "4511190"
  },
  {
    "text": "to the negative 1/2-- so before this role was\nplayed by the inverse Fisher",
    "start": "4511190",
    "end": "4516920"
  },
  {
    "text": "information matrix. That's why we ended up\nhaving i of theta to the 1/2,",
    "start": "4516920",
    "end": "4522980"
  },
  {
    "text": "and now we just have this gamma,\nwhich is just this function that I wrote up there. That could be potentially k by\nk if g takes values into rk.",
    "start": "4522980",
    "end": "4531848"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE]. PHILIPPE RIGOLLET: Yeah,\nthe gradient of a vector",
    "start": "4531848",
    "end": "4537620"
  },
  {
    "text": "is just the vector with all\nthe derivatives with respect to each component, yes. ",
    "start": "4537620",
    "end": "4545460"
  },
  {
    "text": "So you know the word vector\nfor derivatives, but not for vectors? I mean, the word gradient\nyou use for one-dimensional?",
    "start": "4545460",
    "end": "4554678"
  },
  {
    "text": "Yes, derivative\nin one dimension. ",
    "start": "4554678",
    "end": "4561150"
  },
  {
    "text": "Now, of course, here, you\nnotice there's something-- I actually have a\nlittle caveat here.",
    "start": "4561150",
    "end": "4566699"
  },
  {
    "text": "I want this to have rank k. I want this to be invertible. I want this matrix\nto be invertible.",
    "start": "4566700",
    "end": "4571980"
  },
  {
    "text": "Even for the Fisher\ninformation matrix, I sort of need it\nto be invertible. Even for the original\ntheorem, that was part of my\ntechnical condition,",
    "start": "4571980",
    "end": "4578250"
  },
  {
    "text": "just so that I could actually\nwrite Fisher information matrix inverse. And so here, you can make\nyour life easy and just assume",
    "start": "4578250",
    "end": "4586045"
  },
  {
    "text": "that it's true all the time,\nbecause I'm actually writing in a fairly abstract way. But in practice,\nwe're going to have",
    "start": "4586045",
    "end": "4591380"
  },
  {
    "text": "to check whether\nthis is going to be true for specific distributions. And we will see an\nexample towards the end",
    "start": "4591380",
    "end": "4597230"
  },
  {
    "text": "of the chapter, the\nmultinomial, where it's actually not the case\nthat Fisher information",
    "start": "4597230",
    "end": "4602750"
  },
  {
    "text": "matrix exists.  The asymptotic covariance\nmatrix, is not invertible,",
    "start": "4602750",
    "end": "4609230"
  },
  {
    "text": "so it's not the inverse of\na Fisher information matrix. Because to be the\ninverse of someone,",
    "start": "4609230",
    "end": "4614390"
  },
  {
    "text": "you need to be\ninvertible yourself.  And so now what I can\ndo is apply Slutsky.",
    "start": "4614390",
    "end": "4621910"
  },
  {
    "text": "So here, what I needed to\nhave is theta, the true theta. So what I can do is just\nput some theta hat in there,",
    "start": "4621910",
    "end": "4630670"
  },
  {
    "text": "and so that's the gamma of\ntheta hat that I see there.",
    "start": "4630670",
    "end": "4636489"
  },
  {
    "text": "And if theta is true, then\ng of theta is equal to 0. That's what we assume. That was our h0, was that under\nh0 g of theta is equal to 0.",
    "start": "4636490",
    "end": "4645970"
  },
  {
    "text": "So the number I need\nto plug in here, I don't need to\nreplace theta here.",
    "start": "4645970",
    "end": "4651619"
  },
  {
    "text": "What I need to\nreplace here is 0.  Now let's go back to\nwhat you were saying.",
    "start": "4651620",
    "end": "4658000"
  },
  {
    "text": "Here you could say, let\nme try to replace 0 here, but there is no such thing. There is no g here.",
    "start": "4658000",
    "end": "4663910"
  },
  {
    "text": "It's only the gradient of g. So this thing that says\nreplace theta by theta 0",
    "start": "4663910",
    "end": "4670300"
  },
  {
    "text": "wherever you see it\ncould not work here. If g was invertible,\nI could just",
    "start": "4670300",
    "end": "4677050"
  },
  {
    "text": "say that theta is equal to\ng inverse of 0 in the null",
    "start": "4677050",
    "end": "4682780"
  },
  {
    "text": "and then I could\nplug in that value. But in general, it doesn't\nhave to be invertible.",
    "start": "4682780",
    "end": "4688860"
  },
  {
    "text": "And it might be a pain\nto invert g, even. I mean, it's not\nclear how you can invert all functions like that.",
    "start": "4688860",
    "end": "4695080"
  },
  {
    "text": "And so here you just go\nwith Slutsky, and you say, OK, I'm just going to\nput theta hat in there.",
    "start": "4695080",
    "end": "4700690"
  },
  {
    "text": "But this guy, I know I need to\ncheck whether it's 0 or not. Same recipe we did for theta,\nexcept we do it for g of theta",
    "start": "4700690",
    "end": "4707740"
  },
  {
    "text": "now.  And now I have my\nasymptotic thing.",
    "start": "4707740",
    "end": "4714030"
  },
  {
    "text": "I know this is a\npivotal distribution. This might be a vector. So rather than looking\nat the matrix itself,",
    "start": "4714030",
    "end": "4721130"
  },
  {
    "text": "I'm going to actually\nlook at the norm-- rather than looking\nat the vectors, I'm going to look at\ntheir square norm.",
    "start": "4721130",
    "end": "4726620"
  },
  {
    "text": "That gives me a\nchi square, and I reject when my test statistic,\nwhich is the norm square, exceeds the quintile\nof a chi square--",
    "start": "4726620",
    "end": "4733700"
  },
  {
    "text": "same as before, just\ndoing on your own. Before we part ways, I wanted\nto just mention one thing, which",
    "start": "4733700",
    "end": "4740810"
  },
  {
    "text": "is look at this thing. If g was of dimension 1, the\nEuclidean norm in dimension 1",
    "start": "4740810",
    "end": "4748739"
  },
  {
    "text": "is just the absolute value\nof the number, right?  Which means that when I am\nactually computing this,",
    "start": "4748740",
    "end": "4759460"
  },
  {
    "text": "I'm looking at the square, so\nit's the square of something. So it means that this is\nthe square of a Gaussian.",
    "start": "4759460",
    "end": "4765378"
  },
  {
    "text": "And it's true that,\nindeed, the chi squared 1 is just the\nsquare of a Gaussian. ",
    "start": "4765378",
    "end": "4771420"
  },
  {
    "text": "Sure, this is the tautology,\nbut let's look at this test now. This test was built using Wald's\ntheory and some pretty heavy",
    "start": "4771420",
    "end": "4780860"
  },
  {
    "text": "stuff. But now if I start looking\nat Tn and I think of it as being just the absolute value\nof this quantity over there,",
    "start": "4780860",
    "end": "4787600"
  },
  {
    "text": "squared, what I'm\nreally doing is I'm looking at whether the\nsquare of some Gaussian",
    "start": "4787600",
    "end": "4794510"
  },
  {
    "text": "exceeds the quintile of a chi\nsquared of 1 degree of freedom,",
    "start": "4794510",
    "end": "4800250"
  },
  {
    "text": "which means that this thing\nis actually equivalent-- completely equivalent--\nto the test. So if k is equal to\n1, this is completely",
    "start": "4800250",
    "end": "4810740"
  },
  {
    "text": "equivalent to looking at the\nabsolute value of something and check whether it's\nlarger than, say, q over 2--",
    "start": "4810740",
    "end": "4819260"
  },
  {
    "text": "well, than q alpha-- well, that's q alpha over 2-- so that the probability\nof this thing",
    "start": "4819260",
    "end": "4826219"
  },
  {
    "text": "is actually equal to alpha. And that's exactly what\nwe've been doing before. When we introduced tests\nin the first place,",
    "start": "4826220",
    "end": "4831770"
  },
  {
    "text": "we just took absolute\nvalues, said, well, is the absolute value of\na Gaussian in the limit. And so it's the same thing.",
    "start": "4831770",
    "end": "4837420"
  },
  {
    "text": "So this is actually\nequivalent to the probability that the norm squared is\nlarger so that the chi squared",
    "start": "4837420",
    "end": "4844170"
  },
  {
    "text": "of some normal-- and that's the q alpha\nof some chi squared",
    "start": "4844170",
    "end": "4852199"
  },
  {
    "text": "with one degree of freedom. Those are exactly\nthe two same tests.",
    "start": "4852200",
    "end": "4858350"
  },
  {
    "text": "So in one dimension,\nthose things just collapse into being\none little thing,",
    "start": "4858350",
    "end": "4863437"
  },
  {
    "text": "and that's because there's\nno geometry in one dimension. It's just one dimension, whereas\nif I'm in a higher dimension,",
    "start": "4863437",
    "end": "4868820"
  },
  {
    "text": "then things get distorted\nand things can become weird. ",
    "start": "4868820",
    "end": "4881042"
  }
]