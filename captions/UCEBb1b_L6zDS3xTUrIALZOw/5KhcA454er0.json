[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "start": "0",
    "end": "16467"
  },
  {
    "start": "16000",
    "end": "241000"
  },
  {
    "text": "MICHALE FEE: So for\nthe next few lectures, we're going to be looking at\ndeveloping methods of studying",
    "start": "16467",
    "end": "24029"
  },
  {
    "text": "the computational properties\nof networks of neurons. This is the outline for\nthe next few lectures. Today we are going to introduce\na method of studying networks",
    "start": "24030",
    "end": "34530"
  },
  {
    "text": "called a rate model where\nwe basically replace",
    "start": "34530",
    "end": "40500"
  },
  {
    "text": "spike trains with\nfiring rates in order to develop simple\nmathematical descriptions",
    "start": "40500",
    "end": "45780"
  },
  {
    "text": "of neural networks. And we're going to start by\nintroducing that technique",
    "start": "45780",
    "end": "51660"
  },
  {
    "text": "to the problem of studying\nfeed-forward neural networks. And we'll introduce the\nidea of perceptrons trance",
    "start": "51660",
    "end": "58080"
  },
  {
    "text": "as a method of\ndeveloping networks that can classify their inputs.",
    "start": "58080",
    "end": "64709"
  },
  {
    "text": "Then in the next\nlecture, we're going to turn to largely describing\nmathematical tools based",
    "start": "64709",
    "end": "73290"
  },
  {
    "text": "on matrix operations and\nthe idea of basis sets.",
    "start": "73290",
    "end": "78450"
  },
  {
    "text": "Matrix operations are very\nimportant for studying neural networks. But they're also\na fundamental tool",
    "start": "78450",
    "end": "85020"
  },
  {
    "text": "for analyzing data\nand doing things like reducing the dimensionality\nof high dimensional data sets,",
    "start": "85020",
    "end": "91780"
  },
  {
    "text": "including methods such as\nprincipal components analysis. So it's a very\npowerful set of methods",
    "start": "91780",
    "end": "100680"
  },
  {
    "text": "that apply both to\nstudying the brain and to analyzing the data that\nwe get when we study the brain.",
    "start": "100680",
    "end": "109530"
  },
  {
    "text": "And then finally we'll turn\nto a few lectures that focus on recurrent neural networks.",
    "start": "109530",
    "end": "115680"
  },
  {
    "text": "These are networks where\nthe neurons connect to each other densely\nin a recurrent way,",
    "start": "115680",
    "end": "123060"
  },
  {
    "text": "meaning a neuron will\nconnect to another neuron. And that neuron will connect\nback to the first neuron. And networks that\nhave that property",
    "start": "123060",
    "end": "130050"
  },
  {
    "text": "have very interesting\ncomputational abilities. And we're going to study\nthat in the context of line",
    "start": "130050",
    "end": "136260"
  },
  {
    "text": "attractors and short-term\nmemory and hopfield networks.",
    "start": "136260",
    "end": "143280"
  },
  {
    "text": "So for today, the plan is\nto develop the rate model.",
    "start": "143280",
    "end": "149340"
  },
  {
    "text": "We're going to show how we\ncan build receptive fields with feed forward\nnetworks that we've",
    "start": "149340",
    "end": "156840"
  },
  {
    "text": "described with the rate model. We're going to take\na little detour",
    "start": "156840",
    "end": "162090"
  },
  {
    "text": "and describe vector notation\nand vector algebra, which is very important for these\nmodels, and also for building",
    "start": "162090",
    "end": "169320"
  },
  {
    "text": "up to the matrix\nmethods that we'll talk about in the next lecture.",
    "start": "169320",
    "end": "174719"
  },
  {
    "text": "Again, we'll talk about neural\nnetworks for classification and introduce the\nidea of a perceptron.",
    "start": "174720",
    "end": "179740"
  },
  {
    "text": "So that's for today. So I've already talked\nabout most of this.",
    "start": "179740",
    "end": "186840"
  },
  {
    "text": "Why is it that we want\nto develop a simplified mathematical model of neurons\nthat we can study analytically?",
    "start": "186840",
    "end": "192930"
  },
  {
    "text": "Well, the reason is that we can\nreally develop our intuition about how networks work.",
    "start": "192930",
    "end": "199710"
  },
  {
    "text": "And that intuition\napplies not just to the very simplified\nmathematical model that we're",
    "start": "199710",
    "end": "205260"
  },
  {
    "text": "developing, but also\napplies more broadly to real networks\nwith real neurons that actually generate spikes\nand interact with each other",
    "start": "205260",
    "end": "213209"
  },
  {
    "text": "by the more complex\nbiophysical mechanisms that are going on in the brain. So a good example\nof this is how we",
    "start": "213210",
    "end": "219960"
  },
  {
    "text": "simplified the detailed\nspiking neurons of the Hodgkin-Huxley\nmodel and approximate that",
    "start": "219960",
    "end": "226739"
  },
  {
    "text": "as an integrate and\nfire model, which captures a lot of the\nproperties of real neurons.",
    "start": "226740",
    "end": "232500"
  },
  {
    "text": "Simplifies it enough to\ndevelop an intuition, but captures a lot of\nthe important properties",
    "start": "232500",
    "end": "239310"
  },
  {
    "text": "of real neural circuits. All right, so let's\nstart by developing",
    "start": "239310",
    "end": "246860"
  },
  {
    "start": "241000",
    "end": "925000"
  },
  {
    "text": "the basic idea of a rate model. Let's start with two neurons. We have an input neuron\nand an output neuron.",
    "start": "246860",
    "end": "253110"
  },
  {
    "text": "The input neuron has some\nfiring rate given by u.",
    "start": "253110",
    "end": "258329"
  },
  {
    "text": "And the output neuron has\nsome firing rate given by v. So we're going to essentially\nignore the times of the spikes",
    "start": "258329",
    "end": "267330"
  },
  {
    "text": "and describe the inputs\nand outputs of this network just with firing rates.",
    "start": "267330",
    "end": "273150"
  },
  {
    "text": "You can think of the rate\nas just having units have spikes per second, for example.",
    "start": "273150",
    "end": "278430"
  },
  {
    "text": "Those neurons, the input\nneuron and the output neuron, are connected to each\nother by a synapse. And we're going to replace\nall of the complex structure",
    "start": "278430",
    "end": "287370"
  },
  {
    "text": "of synapses, vesicle release,\nneurotransmitter receptors,",
    "start": "287370",
    "end": "295650"
  },
  {
    "text": "long-term depression\nand paired spike",
    "start": "295650",
    "end": "300750"
  },
  {
    "text": "facilitation and\ndepression, all that stuff we're just going to ignore. And we're going to replace that\nsynapse with a synaptic weight",
    "start": "300750",
    "end": "308460"
  },
  {
    "text": "w.  Just to give you the simplest\nintuition of how a rate",
    "start": "308460",
    "end": "316470"
  },
  {
    "text": "model works, there are\nmodels where we can just treat the firing rate\nof the output neuron,",
    "start": "316470",
    "end": "322890"
  },
  {
    "text": "for example, as\nlinear in its input. And we can simplify\nthis even to the point",
    "start": "322890",
    "end": "330180"
  },
  {
    "text": "where we can describe the\nfiring rate of the output neuron as the synaptic weight w times\nthe firing rate of the input",
    "start": "330180",
    "end": "338280"
  },
  {
    "text": "neuron. So that's just to give you a\nflavor of where we're heading.",
    "start": "338280",
    "end": "343690"
  },
  {
    "text": "And I'm going to justify\nhow we can do this and/or why we can do this.",
    "start": "343690",
    "end": "350310"
  },
  {
    "text": "And then we're going\nto build this up from the case of one input\nneuron and one output neuron",
    "start": "350310",
    "end": "356220"
  },
  {
    "text": "to the case where we can\nhave many input neurons and many output neurons. ",
    "start": "356220",
    "end": "363050"
  },
  {
    "text": "So how do we justify going\nfrom spikes to firing rates? So remember that the response\nof a real output neuron,",
    "start": "363050",
    "end": "372860"
  },
  {
    "text": "a real neuron, to a\nsingle spike at its input, is some change in the\npostsynaptic conductance that",
    "start": "372860",
    "end": "379610"
  },
  {
    "text": "follows an input spike. And in our model\nof a synapse, we described that the input spike\nproduces a transient increase",
    "start": "379610",
    "end": "389420"
  },
  {
    "text": "in the synaptic conductance. And that synaptic\nconductance we modeled as a simple step increase\nin the conductance",
    "start": "389420",
    "end": "397940"
  },
  {
    "text": "followed by an exponential\ndecay as the neurotransmitter gradually unbinds from the\nneurotransmitter receptors.",
    "start": "397940",
    "end": "407450"
  },
  {
    "text": "So we have a transient change\nin the synaptic conductance. That's just a\nmaximum conductance",
    "start": "407450",
    "end": "413180"
  },
  {
    "text": "times an exponential decay. Now remember that we wrote\ndown the postsynaptic--",
    "start": "413180",
    "end": "419930"
  },
  {
    "text": "we can write down the\npostsynaptic current that results from this synaptic input\nas the synaptic conductance",
    "start": "419930",
    "end": "427100"
  },
  {
    "text": "times v minus e synapse, the\nsynaptic reversal potential.",
    "start": "427100",
    "end": "432680"
  },
  {
    "text": " In moving forward\nin this model, we're",
    "start": "432680",
    "end": "439850"
  },
  {
    "text": "not going to worry about\nsynaptic saturation. So we're just going to imagine\nthat the synaptic current is",
    "start": "439850",
    "end": "446539"
  },
  {
    "text": "just proportional to the\nsynaptic conductance.",
    "start": "446540",
    "end": "453440"
  },
  {
    "text": "So now we can write\nthe conductance as just some weight times\na kernel that is just",
    "start": "453440",
    "end": "463070"
  },
  {
    "text": "some kernel of unit area. So what we've done here is we've\njust taken the synaptic current",
    "start": "463070",
    "end": "468680"
  },
  {
    "text": "and we've written\nit as a constant, a synaptic weight, times\nan exponentially decaying",
    "start": "468680",
    "end": "474979"
  },
  {
    "text": "kernel of area, area 1. ",
    "start": "474980",
    "end": "481170"
  },
  {
    "text": "So now if we have a train of\nspikes at the input instead of a single spike,\nwe can write down",
    "start": "481170",
    "end": "487620"
  },
  {
    "text": "that train of spikes,\nthe spike train, as a sum of delta\nfunctions where the spike times are t sub i.",
    "start": "487620",
    "end": "494430"
  },
  {
    "text": "And if you want to plot\nthe synaptic current as a function of\ntime, you would just",
    "start": "494430",
    "end": "500100"
  },
  {
    "text": "take that spike train\ninput and do what with that linear kernel? We would convolve it, right?",
    "start": "500100",
    "end": "506610"
  },
  {
    "text": "So we would take\nthat spike train, convolve it with that\nlittle exponential kernel.",
    "start": "506610",
    "end": "512039"
  },
  {
    "text": "And that would give us\nthe synaptic current that results from that spike train. ",
    "start": "512039",
    "end": "519960"
  },
  {
    "text": "So let's think for\na moment about what this quantity is right here. What is k, this k which\nis a little kernel that",
    "start": "519960",
    "end": "529500"
  },
  {
    "text": "has an exponential step, and\nthen an exponential decay? What do you get\nwhen you convolve",
    "start": "529500",
    "end": "535530"
  },
  {
    "text": "that kind of smooth kernel\nwith this spike train here?",
    "start": "535530",
    "end": "542078"
  },
  {
    "text": "What does that look like? We did that at one point\nwhen we were in class when we were talking about how\nyou would estimate something",
    "start": "542078",
    "end": "549560"
  },
  {
    "text": "from a spike train. What is that? What is that\nquantity right there?",
    "start": "549560",
    "end": "555190"
  },
  {
    "text": "It's sort of a smoothed\nversion of a spike train, which is how you would\ncalculate what, Habiba?",
    "start": "555190",
    "end": "561510"
  },
  {
    "text": "AUDIENCE: Is it a window\nfor the spike train? MICHALE FEE: Yeah. It's windowed, but what is\nit that you are calculating",
    "start": "561510",
    "end": "568870"
  },
  {
    "text": "when you take a spike\ntrain and you convolve it with some smooth window? AUDIENCE: Low-pass window?",
    "start": "568870",
    "end": "574771"
  },
  {
    "text": "MICHALE FEE: It's like\na low-pass version of the spike train. And remember in the\nlecture on firing rates,",
    "start": "574771",
    "end": "583899"
  },
  {
    "text": "we talked about how\nthat's a good way to get a time-dependent estimate\nof the firing rate of a neuron.",
    "start": "583900",
    "end": "590870"
  },
  {
    "text": "We take the spike train\nand just convolve it with a smooth window. And if the area of that\nsmooth window is 1,",
    "start": "590870",
    "end": "598630"
  },
  {
    "text": "then what we're doing\nis we're estimating the firing rate of the\nneuron as a function of time.",
    "start": "598630",
    "end": "605330"
  },
  {
    "text": "Does that make sense? Yes? AUDIENCE: So k is just a kernel? MICHALE FEE: k is just is\nsmooth kernel that happens",
    "start": "605330",
    "end": "612540"
  },
  {
    "text": "to have this exponential shape. AUDIENCE: Is it like [INAUDIBLE]",
    "start": "612540",
    "end": "618709"
  },
  {
    "text": "MICHALE FEE: Well, that's\nour model for how a synapse-- basically, what I'm\nsaying is that when",
    "start": "618710",
    "end": "626080"
  },
  {
    "text": "you take a spike train and\nput it through a synapse, what comes out the other end\nis a smoothed version of the spike train.",
    "start": "626080",
    "end": "631630"
  },
  {
    "text": "AUDIENCE: OK. MICHALE FEE: That's\nall this is saying. AUDIENCE: OK. [INAUDIBLE] they have\nthis area or quantity?",
    "start": "631630",
    "end": "639470"
  },
  {
    "text": "MICHALE FEE: Yep. If k has-- you remember that\nif k has an area 1, then when",
    "start": "639470",
    "end": "647529"
  },
  {
    "text": "you convolve evolve that\nkernel with the spike train, you get a number that has\nunits of spikes per second.",
    "start": "647530",
    "end": "658160"
  },
  {
    "text": "And that quantity is an\nestimate of the local firing",
    "start": "658160",
    "end": "664040"
  },
  {
    "text": "rate of the neuron. Does that make sense? ",
    "start": "664040",
    "end": "671779"
  },
  {
    "text": "So basically, we can\ntake this spike train, and by convolve it\nwith a smooth window, we can estimate the\nnumber of spikes",
    "start": "671780",
    "end": "679090"
  },
  {
    "text": "per second in that window. So what do we have here?",
    "start": "679090",
    "end": "684379"
  },
  {
    "text": "We have that the current\nis just a constant times an estimate of the\nfiring rate at that time.",
    "start": "684380",
    "end": "692870"
  },
  {
    "text": "If k is a kernel, a\nsmooth kernel with an area normalized to 1, then\nthis quantity is just",
    "start": "692870",
    "end": "699800"
  },
  {
    "text": "an estimate of the firing rate. So let's take a look at that. So here I have just made\na sample spike train",
    "start": "699800",
    "end": "708020"
  },
  {
    "text": "with a bunch of spikes\nthat look like they're increasing in firing rate and\ndecreasing in firing rate.",
    "start": "708020",
    "end": "714410"
  },
  {
    "text": "If we take that spike train and\nconvolve it with this kernel, you can see that you get\nthis sort of broad bump",
    "start": "714410",
    "end": "721100"
  },
  {
    "text": "that looks like it gets higher\nin the middle where the firing rate is higher. And it's lower at the edges\nwhere the firing rate is lower.",
    "start": "721100",
    "end": "727430"
  },
  {
    "text": " So the point is that you\ncan take a spike train",
    "start": "727430",
    "end": "736180"
  },
  {
    "text": "and put it into a neuron. The response of the neuron\nis a smooth low-pass version",
    "start": "736180",
    "end": "744100"
  },
  {
    "text": "of the rate of this\ninput spike train. And so you can think about\nwriting down the input",
    "start": "744100",
    "end": "753170"
  },
  {
    "text": "to this neuron as a weight times\nthe firing rate of the input.",
    "start": "753170",
    "end": "759540"
  },
  {
    "text": "So that was a way of writing\ndown the input to this output",
    "start": "759540",
    "end": "766560"
  },
  {
    "text": "neuron from the input\nneuron, the current input.",
    "start": "766560",
    "end": "772900"
  },
  {
    "text": "Now what is the firing rate of\nthe output neuron in response to that current injection?",
    "start": "772900",
    "end": "780970"
  },
  {
    "text": "So that's what we're\ngoing to ask next. And you can remember that when\nwe talked about the integrate",
    "start": "780970",
    "end": "786399"
  },
  {
    "text": "and fire model, we\nsaw that neurons",
    "start": "786400",
    "end": "793640"
  },
  {
    "text": "in the approximation\nof large inputs have firing rate as\na function of current",
    "start": "793640",
    "end": "799340"
  },
  {
    "text": "that looks like this. It's zero for inputs below\nthe threshold current.",
    "start": "799340",
    "end": "804529"
  },
  {
    "text": "For input currents that\naren't large enough to drive the neuron\nto threshold, the neuron doesn't spike at all.",
    "start": "804530",
    "end": "811550"
  },
  {
    "text": "And then above some\nthreshold, the neuron fires approximately linearly\nat higher input currents.",
    "start": "811550",
    "end": "819470"
  },
  {
    "text": " So the way that we\nthink about this",
    "start": "819470",
    "end": "824770"
  },
  {
    "text": "is that the input on is\nspiking at some rate. It goes through a synapse.",
    "start": "824770",
    "end": "830740"
  },
  {
    "text": "That synapse smooths the input\nand produces some current in the postsynaptic\nneuron that's",
    "start": "830740",
    "end": "837070"
  },
  {
    "text": "proportional approximately to\nthe firing rate of the input neuron. And the output neuron has\nsome output firing rate",
    "start": "837070",
    "end": "844750"
  },
  {
    "text": "that's some function\nof the input current. ",
    "start": "844750",
    "end": "850050"
  },
  {
    "text": "So we can write down the firing\nrate of our output neuron, v. It's just equal to\nsome function of the input",
    "start": "850050",
    "end": "858270"
  },
  {
    "text": "current, which is just some\nfunction of w times the firing",
    "start": "858270",
    "end": "863380"
  },
  {
    "text": "rate of the input neuron. And that right there\nis the basic equation",
    "start": "863380",
    "end": "870690"
  },
  {
    "text": "of the rate model. The output firing\nrate is some function",
    "start": "870690",
    "end": "877940"
  },
  {
    "text": "of a weight times the firing\nrate of the input neuron. ",
    "start": "877940",
    "end": "887610"
  },
  {
    "text": "And everything else\nabout the rate model is just different rate models\nhave different numbers of input",
    "start": "887610",
    "end": "896740"
  },
  {
    "text": "neurons where we have more than\none contribution to the input current. They can have many\noutput neurons.",
    "start": "896740",
    "end": "903640"
  },
  {
    "text": "They can have different FI\ncurves for the output neurons. Some of them are\nnon-linear like this.",
    "start": "903640",
    "end": "909940"
  },
  {
    "text": "Some of them are linear. And we're going to\ncome back and talk about the function of\ndifferent FI curves",
    "start": "909940",
    "end": "918310"
  },
  {
    "text": "and why different FYI\ncurves are useful. Any questions about this? That's the basic idea. ",
    "start": "918310",
    "end": "925760"
  },
  {
    "start": "925000",
    "end": "1110000"
  },
  {
    "text": "All right, good. So let's take one\nparticularly simple version",
    "start": "925760",
    "end": "935010"
  },
  {
    "text": "of the rate model called\na linear rate model. And the linear rate model\nhas a particular FI curve.",
    "start": "935010",
    "end": "942570"
  },
  {
    "text": "That FI curve says that the\nfiring rate of the neuron",
    "start": "942570",
    "end": "948090"
  },
  {
    "text": "is linear in the input current. Now why is this a really\nweird model of a neuron?",
    "start": "948090",
    "end": "956639"
  },
  {
    "text": "What's fundamentally\nnon-biological about this?",
    "start": "956640",
    "end": "962460"
  },
  {
    "text": "AUDIENCE: Negative firing rate. MICHALE FEE: I'm hearing\na bunch of right answers at the same time. AUDIENCE: Negative firing rate.",
    "start": "962460",
    "end": "968432"
  },
  {
    "text": "MICHALE FEE: This\nneuron is allowed to fire at a\nnegative firing rate",
    "start": "968432",
    "end": "975709"
  },
  {
    "text": "if the input\ncurrent is negative. ",
    "start": "975710",
    "end": "982000"
  },
  {
    "text": "That's a pretty\ncrazy thing to do. Why do you think we\nwould want to do that? ",
    "start": "982000",
    "end": "990786"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]? MICHALE FEE: Well,\nno actually we do. So you can have\ninhibitory inputs",
    "start": "990786",
    "end": "1001400"
  },
  {
    "text": "that produce outward currents\nthat hyperpolarize the neuron. Any thoughts about that?",
    "start": "1001400",
    "end": "1009380"
  },
  {
    "text": "It turns out that as soon as\nyou have your output neurons have this kind of FI\ncurve, a linear FI curve,",
    "start": "1009380",
    "end": "1018200"
  },
  {
    "text": "then the math\nbecomes super simple. You can write down very\ncomplex networks of neurons",
    "start": "1018200",
    "end": "1025699"
  },
  {
    "text": "with a bunch of linear\ndifferential equations. And it becomes very\neasy to write down",
    "start": "1025700",
    "end": "1032720"
  },
  {
    "text": "what the solution is to\nhow a network behaves",
    "start": "1032720",
    "end": "1038059"
  },
  {
    "text": "as a function of its inputs. And we're going to spend\na lot of time working",
    "start": "1038060",
    "end": "1044819"
  },
  {
    "text": "with network models that have\nlinear FI curves because you",
    "start": "1044819",
    "end": "1050780"
  },
  {
    "text": "can develop a lot of intuition\nabout how networks behave by using models like this.",
    "start": "1050780",
    "end": "1055789"
  },
  {
    "text": "As soon as you have\nmodels like this, you can't solve the behavior\nof the network analytically.",
    "start": "1055790",
    "end": "1062750"
  },
  {
    "text": "You have to do everything\non the computer. And it becomes very hard\nto derive general solutions",
    "start": "1062750",
    "end": "1069320"
  },
  {
    "text": "for how things behave. So we're going to\nuse this model a lot.",
    "start": "1069320",
    "end": "1075860"
  },
  {
    "start": "1075860",
    "end": "1081450"
  },
  {
    "text": "And in this case again, for the\ncase of this two-neuron network where we have one output neuron\nthat receives a synaptic input",
    "start": "1081450",
    "end": "1090300"
  },
  {
    "text": "from an input neuron, the\nfiring rate of the output neuron is just w, the synaptic\nweight times the firing rate",
    "start": "1090300",
    "end": "1095850"
  },
  {
    "text": "of the input neuron.  And we're going to come\nback to non-linear neurons",
    "start": "1095850",
    "end": "1103730"
  },
  {
    "text": "because that\nnon-linearity actually does really important things. And we're going to talk\nabout what that does.",
    "start": "1103730",
    "end": "1111770"
  },
  {
    "start": "1110000",
    "end": "1264000"
  },
  {
    "text": "So now let's look at the\ncase where our output neuron has not just one\ninput but actually",
    "start": "1111770",
    "end": "1117559"
  },
  {
    "text": "many inputs from a\nbunch of input neurons. So here we have what\nwe call an input layer,",
    "start": "1117560",
    "end": "1124841"
  },
  {
    "text": "a layer of neurons\nin the input layer. Each one of those neurons\nhas a firing rate--",
    "start": "1124841",
    "end": "1131240"
  },
  {
    "text": "u1, u2, u3, u4, u5. ",
    "start": "1131240",
    "end": "1138050"
  },
  {
    "text": "Each of those neurons sends a\nsynapse onto our output neuron. Each one of those synapses\nhas a synaptic weight.",
    "start": "1138050",
    "end": "1145140"
  },
  {
    "text": "This weight is w1. And that's w2, w3, w4, and w5.",
    "start": "1145140",
    "end": "1150445"
  },
  {
    "text": " Now you can see that the total\ninput, the total current,",
    "start": "1150445",
    "end": "1158179"
  },
  {
    "text": "to this output\nneuron is just going to be a sum of the inputs from\neach of the input neurons.",
    "start": "1158180",
    "end": "1166560"
  },
  {
    "text": "The total input is just\na sum of the inputs from each of the input neuron. So the synaptic current--",
    "start": "1166560",
    "end": "1173900"
  },
  {
    "text": "total synaptic current\ninto this neuron is w1 times u1, plus w2\ntimes u2, plus w3 times u3,",
    "start": "1173900",
    "end": "1184860"
  },
  {
    "text": "plus all the rest. So the response of\nour linear neuron,",
    "start": "1184860",
    "end": "1194510"
  },
  {
    "text": "the firing rate of\nour linear neuron, is just a sum over\nall of those inputs.",
    "start": "1194510",
    "end": "1202540"
  },
  {
    "text": "So again, in this\ncase, we're going to say that the total input\ncurrent to this neuron",
    "start": "1202540",
    "end": "1208060"
  },
  {
    "text": "is the sum over this. But then because this\nis a linear neuron,",
    "start": "1208060",
    "end": "1214000"
  },
  {
    "text": "the firing rate is just\nequal to that current input. Does that make sense?",
    "start": "1214000",
    "end": "1219340"
  },
  {
    "text": " So you can see that this\ndescription of the firing",
    "start": "1219340",
    "end": "1226020"
  },
  {
    "text": "rate of the output\nneuron is a sum over all of those contributions.",
    "start": "1226020",
    "end": "1231120"
  },
  {
    "text": "It turns out that\nthis actually can be written in a much more\ncompact way in vector notation.",
    "start": "1231120",
    "end": "1238988"
  },
  {
    "text": "What does that look like? Does anyone know in vector\nnotation what that looks like? AUDIENCE: Dot product. MICHALE FEE: That's\na dot product.",
    "start": "1238988",
    "end": "1245117"
  },
  {
    "text": "That's right. So in general, it's much\neasier to write these responses",
    "start": "1245117",
    "end": "1251040"
  },
  {
    "text": "in vector notation. And so I'm just\ngoing to walk you through some basics\nof vector notation for those of you who might\nneed a few minutes of reminder.",
    "start": "1251040",
    "end": "1257565"
  },
  {
    "text": " Actually before we get\nto the vector notation, I just want to\ndescribe how we can",
    "start": "1257565",
    "end": "1265550"
  },
  {
    "start": "1264000",
    "end": "1692000"
  },
  {
    "text": "use a simple network like this\nto build a receptive field. So you remember that\nwhen we were talking",
    "start": "1265550",
    "end": "1272240"
  },
  {
    "text": "about receptive\nfields of neurons, we described how\na neuron can have",
    "start": "1272240",
    "end": "1277940"
  },
  {
    "text": "a maximal response to a\nparticular pattern of input. So let's say we\nhave a neuron that's",
    "start": "1277940",
    "end": "1284090"
  },
  {
    "text": "sensitive to visual inputs. And as a function\nof one dimension, let's say along the\nretina, this neuron",
    "start": "1284090",
    "end": "1289880"
  },
  {
    "text": "has a big response if light\ncomes in central field, some inhibitory responsive\nlight comes in outside",
    "start": "1289880",
    "end": "1297260"
  },
  {
    "text": "of that central lobe. Well, it turns out\nthat a very simple way",
    "start": "1297260",
    "end": "1302720"
  },
  {
    "text": "to build neurons that have\nreceptive fields like this,",
    "start": "1302720",
    "end": "1308150"
  },
  {
    "text": "for example, is to have\nan input layer that",
    "start": "1308150",
    "end": "1313200"
  },
  {
    "text": "projects to this neuron that\nhas this receptive field and has a pattern\nof synaptic inputs",
    "start": "1313200",
    "end": "1320910"
  },
  {
    "text": "that corresponds to that\npattern in the field. So you can see that\nif this neuron--",
    "start": "1320910",
    "end": "1328120"
  },
  {
    "text": "so let's say these are\nneurons in the retina, let's say retinal\nganglion cells, and this neuron is\nin the thalamus,",
    "start": "1328120",
    "end": "1335320"
  },
  {
    "text": "we can build a\nthalamic neuron that has a center-surround\nreceptive field like this",
    "start": "1335320",
    "end": "1341890"
  },
  {
    "text": "by having let's\nsay this neuron has a strong positive excitatory\nsynaptic weight onto our output",
    "start": "1341890",
    "end": "1350730"
  },
  {
    "text": "neuron. So you can see that\nif you have light here that corresponds to this neuron\nhaving a high firing rate,",
    "start": "1350730",
    "end": "1358809"
  },
  {
    "text": "that neuron is very effective\nat driving the output neuron. And so the output neuron\nhas a positive component",
    "start": "1358810",
    "end": "1365919"
  },
  {
    "text": "of its receptor field\nright there in the middle. Now if this neuron here, which\nis in this part of the retina,",
    "start": "1365920",
    "end": "1372910"
  },
  {
    "text": "if that neuron has a negative\nweight onto the output neuron, then light coming in here\ndriving this neuron will",
    "start": "1372910",
    "end": "1380980"
  },
  {
    "text": "inhibit the output neuron. So if you have a pattern of\nweights that looks like this,",
    "start": "1380980",
    "end": "1388130"
  },
  {
    "text": "0 minus 1, 2 minus 1,\n0, that this neuron will have a receptive\nfield that looks like that",
    "start": "1388130",
    "end": "1396460"
  },
  {
    "text": "as a function of its inputs.  So that's a\non-dimensional example.",
    "start": "1396460",
    "end": "1404592"
  },
  {
    "text": "And you can see\nthat you write down the output here\nas a weighted sum of each one of those inputs.",
    "start": "1404592",
    "end": "1410850"
  },
  {
    "text": "This also works for two\ndimensional receptive fields. For example, if we have\ninput from the retina that",
    "start": "1410850",
    "end": "1417059"
  },
  {
    "text": "looks like this where we have-- I guess this was excitatory\nhere in the center,",
    "start": "1417060",
    "end": "1423150"
  },
  {
    "text": "inhibitory around, you\ncan make a neuron that has a two-dimensional\nreceptor field like this",
    "start": "1423150",
    "end": "1429750"
  },
  {
    "text": "by having inputs to\nthis neuron from all of those different regions\nof the visual field that",
    "start": "1429750",
    "end": "1438180"
  },
  {
    "text": "have different weights\ncorresponding to positive in the center. So neurons in the\npositive synaptic weights",
    "start": "1438180",
    "end": "1446490"
  },
  {
    "text": "under the output neuron. And neurons around the edges\nhave negative synaptic weights.",
    "start": "1446490",
    "end": "1453269"
  },
  {
    "text": "So we can build any receptive\nfield we want into a neuron",
    "start": "1453270",
    "end": "1458430"
  },
  {
    "text": "by just plugging in-- by\nputting in the right set of synaptic weights.",
    "start": "1458430",
    "end": "1465320"
  },
  {
    "text": "Yes? AUDIENCE: So would you\nrule out [INAUDIBLE]",
    "start": "1465320",
    "end": "1478403"
  },
  {
    "text": "MICHALE FEE: So in real life,\nI assume you mean in the brain? AUDIENCE: Yeah. MICHALE FEE: So in the\nbrain, we don't really",
    "start": "1478404",
    "end": "1485820"
  },
  {
    "text": "know how these\nweights are built. So one idea is that\nthere are rules that",
    "start": "1485820",
    "end": "1494790"
  },
  {
    "text": "control the development\nof these circuits, let's say, connections\nof bipolar cells",
    "start": "1494790",
    "end": "1505010"
  },
  {
    "text": "in the retina to\nretinal ganglion cells that control how these\nweights are determined",
    "start": "1505010",
    "end": "1511010"
  },
  {
    "text": "to be positive or negative. Negative weights are implemented\nby bipolar cells connected",
    "start": "1511010",
    "end": "1517250"
  },
  {
    "text": "to amacrine cells,\nwhich are inhibitory, and then connect to\nthe retinal ganglion.",
    "start": "1517250",
    "end": "1524250"
  },
  {
    "text": "So there's a whole\ncircuit that gets built in the retina that\ncontrols whether these weights",
    "start": "1524250",
    "end": "1529400"
  },
  {
    "text": "are positive or negative. And those can be programmed by\ngenetic developmental programs.",
    "start": "1529400",
    "end": "1536450"
  },
  {
    "text": "They can also be controlled by\nexperience with visual stimuli.",
    "start": "1536450",
    "end": "1544559"
  },
  {
    "text": "So there's a lot\nwe don't understand about how these\nweights are controlled",
    "start": "1544560",
    "end": "1551300"
  },
  {
    "text": "or set up or programmed. But the way we think\nabout how receptive fields",
    "start": "1551300",
    "end": "1558290"
  },
  {
    "text": "of these neurons emerge is\nby controlling the weight of those synaptic input.",
    "start": "1558290",
    "end": "1564500"
  },
  {
    "text": "That's the message here-- that receptive fields\nemerge from the pattern",
    "start": "1564500",
    "end": "1572450"
  },
  {
    "text": "of weights from an input\nlayer onto an output layer. ",
    "start": "1572450",
    "end": "1578929"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nhow many [INAUDIBLE]",
    "start": "1578930",
    "end": "1587840"
  },
  {
    "text": "MICHALE FEE: If you're\ngoing to build a model, let's say, of the retina. So it just depends on how\nrealistic you want it to be.",
    "start": "1587840",
    "end": "1595780"
  },
  {
    "text": " If you wanted to make a model\nof a retinal ganglion cell,",
    "start": "1595780",
    "end": "1601720"
  },
  {
    "text": "you could try to build a\nmodel that has as many bipolar neurons as are actually\nin the receptive field",
    "start": "1601720",
    "end": "1612040"
  },
  {
    "text": "of that retinal ganglion cell. Or you could make a\nsimplified model that",
    "start": "1612040",
    "end": "1618460"
  },
  {
    "text": "only has 10 or 100 neurons. Depends on what\nyou want to study.",
    "start": "1618460",
    "end": "1625300"
  },
  {
    "text": "All right any other questions? ",
    "start": "1625300",
    "end": "1632320"
  },
  {
    "text": "And again, even for these\nmore complex models, you can still write\ndown a simple rate model",
    "start": "1632320",
    "end": "1640150"
  },
  {
    "text": "formulation of the firing\nrate of the output neuron. It's just a weighted sum\nof the input firing rate.",
    "start": "1640150",
    "end": "1646400"
  },
  {
    "text": "So each neuron in the input\nlayer fires at some rate.",
    "start": "1646400",
    "end": "1652010"
  },
  {
    "text": "It has a weight w.  To get the contribution\nof this neuron",
    "start": "1652010",
    "end": "1659002"
  },
  {
    "text": "to the firing rate\nof the output neuron, you just take that input firing\nrate times the synaptic weight,",
    "start": "1659003",
    "end": "1664720"
  },
  {
    "text": "and add that up then for\nall the input layer neurons. ",
    "start": "1664720",
    "end": "1673360"
  },
  {
    "text": "So as I said, we've\nbeen describing the response of our linear\nneuron as this weighted sum.",
    "start": "1673360",
    "end": "1678760"
  },
  {
    "text": "And that's a little bit\ncumbersome to carry around. So we're going to start using\nvector notation and matrix",
    "start": "1678760",
    "end": "1686260"
  },
  {
    "text": "notation to describe networks. It's just much more compact.",
    "start": "1686260",
    "end": "1692080"
  },
  {
    "start": "1692000",
    "end": "1756000"
  },
  {
    "text": "So we're going to take a little\ndetour, talk about vectors. So a vector is just a\ncollection of numbers.",
    "start": "1692080",
    "end": "1698920"
  },
  {
    "text": "The number of numbers is\ncalled the dimensionality of the vector. ",
    "start": "1698920",
    "end": "1705130"
  },
  {
    "text": "If a vector has\nonly two numbers, then we can just plot\nthat vector in a plane.",
    "start": "1705130",
    "end": "1712434"
  },
  {
    "text": " So for a 2D vector, if that\nvector has two components, x1",
    "start": "1712435",
    "end": "1720100"
  },
  {
    "text": "and x2, then we can\nplot that vector in that space of x1 and\nx2, put the origin at zero.",
    "start": "1720100",
    "end": "1727330"
  },
  {
    "text": "In this case, the vector\nhas two vector components or elements, x1 and x2.",
    "start": "1727330",
    "end": "1732820"
  },
  {
    "text": "And in two dimensions we\ndescribe that as spaces, as R2, the space of\ntwo real numbers.",
    "start": "1732820",
    "end": "1740920"
  },
  {
    "text": "We can write down that vector\nas a row in row vector notation.",
    "start": "1740920",
    "end": "1746620"
  },
  {
    "text": "So x is x1, x2. We can write it as a\ncolumn vector, x1, x2,",
    "start": "1746620",
    "end": "1755080"
  },
  {
    "text": "organized on top of\neach other, like this. Vector sums are very simple.",
    "start": "1755080",
    "end": "1761630"
  },
  {
    "start": "1756000",
    "end": "1787000"
  },
  {
    "text": "So if you have two\nvectors, x and y, you can write down the sum\nof x and y is x plus y.",
    "start": "1761630",
    "end": "1769870"
  },
  {
    "text": "That's called the resultant. X plus y it can be written like\nthis in column vector notation.",
    "start": "1769870",
    "end": "1776309"
  },
  {
    "text": "You can see that\nthe sum of x and y is just an element by element\nsum of the vector elements.",
    "start": "1776310",
    "end": "1785220"
  },
  {
    "text": "It's called element\nby element addition. Let's look at vector product. So there are multiple\nways of taking",
    "start": "1785220",
    "end": "1792039"
  },
  {
    "start": "1787000",
    "end": "1820000"
  },
  {
    "text": "the product of two vectors. There's an element by element\nproduct, an inner product,",
    "start": "1792040",
    "end": "1798340"
  },
  {
    "text": "an outer product that we'll\ncover in later lectures. And also, something\ncalled the cross product",
    "start": "1798340",
    "end": "1806080"
  },
  {
    "text": "that's very common in physics. But I have not yet seen the\napplication of a cross product",
    "start": "1806080",
    "end": "1813100"
  },
  {
    "text": "to neuroscience. If anybody can find one of\nthose, I'll give extra credit.",
    "start": "1813100",
    "end": "1820149"
  },
  {
    "start": "1820000",
    "end": "1846000"
  },
  {
    "text": " Element by element product\nis called a Hadamard product.",
    "start": "1820150",
    "end": "1826549"
  },
  {
    "text": "So x times y is just the\nelement-by-element product of the elements in\nthe two vectors.",
    "start": "1826550",
    "end": "1835888"
  },
  {
    "text": "In Matlab, that\nelement-by-element product you compute by x dot star y.",
    "start": "1835888",
    "end": "1842610"
  },
  {
    "start": "1842610",
    "end": "1848360"
  },
  {
    "start": "1846000",
    "end": "1983000"
  },
  {
    "text": "Inner product or dot\nproduct looks like this. So if we have two\ncolumn vectors,",
    "start": "1848360",
    "end": "1854030"
  },
  {
    "text": "the dot product of\nx and y is the sum of the element-by-element\nproducts.",
    "start": "1854030",
    "end": "1861710"
  },
  {
    "text": "So x dot y is just x1\ntimes y1 plus x2 times y2,",
    "start": "1861710",
    "end": "1867950"
  },
  {
    "text": "and so on, plus xn times yn. And that's that sum that we\nsaw earlier in our feed forward",
    "start": "1867950",
    "end": "1879790"
  },
  {
    "text": "network. OK. So notice that the dot\nproduct is a scalar.",
    "start": "1879790",
    "end": "1884870"
  },
  {
    "text": "It's a single number. It's no longer a vector. Products have some\nnice properties.",
    "start": "1884870",
    "end": "1891927"
  },
  {
    "text": "They're commutative. So x.y is equal to y.x. They're distributive\nso that vector w dotted",
    "start": "1891927",
    "end": "1899690"
  },
  {
    "text": "into the sum of two vectors\nis just the sum of the two separate dot products.",
    "start": "1899690",
    "end": "1906320"
  },
  {
    "text": "So w dot x plus y\nis just w.x, w.y. And it's also linear.",
    "start": "1906320",
    "end": "1913010"
  },
  {
    "text": "So if you have a x dot y\nthat is equal to a times",
    "start": "1913010",
    "end": "1920000"
  },
  {
    "text": "the quantity x.y.  So if you have vector x and\ny dotted into each other,",
    "start": "1920000",
    "end": "1927340"
  },
  {
    "text": "if you make one of those\nvectors twice as long, then the dot product\nis just twice as big. ",
    "start": "1927340",
    "end": "1934790"
  },
  {
    "text": "A little bit more\nabout inner products. So we can also write\ndown the inner product in matrix notation.",
    "start": "1934790",
    "end": "1941139"
  },
  {
    "text": "So x.y is a matrix\nproduct of a row vector.",
    "start": "1941140",
    "end": "1948070"
  },
  {
    "text": "Column vector, you remember\nhow to multiply two matrices. You multiply the elements of\neach row times the elements",
    "start": "1948070",
    "end": "1958059"
  },
  {
    "text": "of each column. So you can see that\nthis in matrix notation is just the dot product\nof those two vectors.",
    "start": "1958060",
    "end": "1964830"
  },
  {
    "text": "In matrix notation,\nthis is a 1 by n matrix. This is an n by 1.",
    "start": "1964830",
    "end": "1970200"
  },
  {
    "text": "So 1 row by n columns,\ntimes n rows by 1 column.",
    "start": "1970200",
    "end": "1978600"
  },
  {
    "text": "And that is equal to a 1 by 1\nmatrix, which is just a scalar.",
    "start": "1978600",
    "end": "1984030"
  },
  {
    "start": "1983000",
    "end": "2060000"
  },
  {
    "text": "All right, in\nMatlab, let me just show you how to write\ndown these components.",
    "start": "1984030",
    "end": "1989590"
  },
  {
    "text": "So in this case, x is a column\nvector, a 1 by 3 column vector.",
    "start": "1989590",
    "end": "1995610"
  },
  {
    "text": "y is a 1 by 3 column vector. You can calculate those\nvectors like this.",
    "start": "1995610",
    "end": "2000920"
  },
  {
    "text": "And z is x transpose times y.",
    "start": "2000920",
    "end": "2006680"
  },
  {
    "text": "And so that's how\nyou can write down the dot product of two vectors.",
    "start": "2006680",
    "end": "2013520"
  },
  {
    "start": "2013520",
    "end": "2020140"
  },
  {
    "text": "What is the dot product\nof a vector with itself? It's the square\nmagnitude of the vector.",
    "start": "2020140",
    "end": "2029410"
  },
  {
    "text": "So x is just the norm or\nmagnitude of the vector.",
    "start": "2029410",
    "end": "2034890"
  },
  {
    "text": "And you can see that the\nnorm of the vector is just-- you can think\nabout this as being",
    "start": "2034890",
    "end": "2040770"
  },
  {
    "text": "analogous to the\nPythagorean theorem.",
    "start": "2040770",
    "end": "2046500"
  },
  {
    "text": "The length of one\nside of a triangle is just the sum of the\nsquares of all the sides,",
    "start": "2046500",
    "end": "2057609"
  },
  {
    "text": "the square root of that.  So a unit vector is a\nvector that has length 1.",
    "start": "2057610",
    "end": "2067010"
  },
  {
    "start": "2060000",
    "end": "2218000"
  },
  {
    "text": "So a unit vector\nby definition has a magnitude of 1,\nwhich means its dot",
    "start": "2067010",
    "end": "2072710"
  },
  {
    "text": "product with itself is 1. We can turn any vector\ninto a unit vector by just taking that vector,\ndividing by its norm.",
    "start": "2072710",
    "end": "2081289"
  },
  {
    "text": "I'm going to always use this\nnotation with this little caret symbol to represent\na unit vector.",
    "start": "2081290",
    "end": "2087870"
  },
  {
    "text": "So if you see a vector\nwith that little hat on it, that means it's a unit vector.",
    "start": "2087870",
    "end": "2094239"
  },
  {
    "text": "You can express any vector as a\nproduct of a scalar, a length,",
    "start": "2094239",
    "end": "2100600"
  },
  {
    "text": "times a unit vector\nin that direction. ",
    "start": "2100600",
    "end": "2107430"
  },
  {
    "text": "We can find the projection\nor component of any vector in the direction of this\nunit vector as follows.",
    "start": "2107430",
    "end": "2115390"
  },
  {
    "text": "So if we have a\nunit vector x, we can find the projection\nof a vector y",
    "start": "2115390",
    "end": "2123600"
  },
  {
    "text": "onto that unit vector x. How do we do that? We just find the normal\nprojection of that vector.",
    "start": "2123600",
    "end": "2131550"
  },
  {
    "text": "That distance right there\nis called the scalar projection of y onto x.",
    "start": "2131550",
    "end": "2138015"
  },
  {
    "text": " If you write down the\nlength of the vector y,",
    "start": "2138015",
    "end": "2145380"
  },
  {
    "text": "the norm of the vector y in\nthe angle between y and x, then the dot product y.x is\njust equal to the magnitude",
    "start": "2145380",
    "end": "2154170"
  },
  {
    "text": "of y times the cosine of the\nangle between the two vectors. Just simple trigonometry.",
    "start": "2154170",
    "end": "2163490"
  },
  {
    "text": "We can also define what's called\nthe vector projection of y onto x as follows.",
    "start": "2163490",
    "end": "2169700"
  },
  {
    "text": "So we just draw\nthat same picture. So we can find the\nprojection of y onto x",
    "start": "2169700",
    "end": "2176140"
  },
  {
    "text": "and add that as a vector. And that's just this\nscalar projection of y",
    "start": "2176140",
    "end": "2182830"
  },
  {
    "text": "onto x times a unit\nvector in the x direction.",
    "start": "2182830",
    "end": "2187960"
  },
  {
    "text": "So x actually is a unit\nvector in this example. So this vector projection\nof y to x is just",
    "start": "2187960",
    "end": "2195130"
  },
  {
    "text": "defined as y dot x times x. Any questions about that?",
    "start": "2195130",
    "end": "2202240"
  },
  {
    "text": "I'm guessing most of you have\nseen all of this stuff already. But we're going to be\nusing these things a lot.",
    "start": "2202240",
    "end": "2208869"
  },
  {
    "text": "So I just want to make sure\nthat we're all on the same page. ",
    "start": "2208870",
    "end": "2214930"
  },
  {
    "text": "And that's just a scalar\ntimes a unit vector. ",
    "start": "2214930",
    "end": "2221069"
  },
  {
    "start": "2218000",
    "end": "2264000"
  },
  {
    "text": "Let me just give you a\nlittle bit of intuition about dot products here. So a dot product is\nrelated to the cosine",
    "start": "2221070",
    "end": "2228810"
  },
  {
    "text": "of the angle\nbetween two vectors, as we talked about before. The dot product\nis just magnitude",
    "start": "2228810",
    "end": "2233940"
  },
  {
    "text": "of x times the magnitude\nof y times the cosine of the angle between them. So the cosine of the\nangle between two vectors",
    "start": "2233940",
    "end": "2242150"
  },
  {
    "text": "is just the dot product divided\nby the product of the magnitude",
    "start": "2242150",
    "end": "2248569"
  },
  {
    "text": "of each of the two vectors. So if x and y are unit\nvectors, the cosine",
    "start": "2248570",
    "end": "2253580"
  },
  {
    "text": "of the angle between them\nis just the dot product of the unit vectors.",
    "start": "2253580",
    "end": "2259110"
  },
  {
    "text": "So again, if x and\ny are unit vectors, then that dot product is\njust the cosine of the angle.",
    "start": "2259110",
    "end": "2265670"
  },
  {
    "start": "2264000",
    "end": "2306000"
  },
  {
    "text": "Orthogonality. So two vectors are orthogonal,\nare perpendicular, if",
    "start": "2265670",
    "end": "2270680"
  },
  {
    "text": "and only if their\ndot product is 0. So if we have two\nvectors x and y,",
    "start": "2270680",
    "end": "2275970"
  },
  {
    "text": "they are orthogonal if the angle\nbetween them is 90 degrees. x.y is just proportional\nto the cosine of the angle.",
    "start": "2275970",
    "end": "2283320"
  },
  {
    "text": "Cosine of 90 degrees is zero. So if two vectors\nare orthogonal,",
    "start": "2283320",
    "end": "2288890"
  },
  {
    "text": "then their dot\nproduct will be zero. If their dot product\nis zero, then they're orthogonal with each other.",
    "start": "2288890",
    "end": "2296490"
  },
  {
    "text": "And using the notation\nwe just developed, the vector projection\nof y along x",
    "start": "2296490",
    "end": "2302670"
  },
  {
    "text": "is the zero vector, if those\ntwo vectors are orthogonal. There is an intuition\nthat one can",
    "start": "2302670",
    "end": "2310620"
  },
  {
    "text": "think about in terms of the\nrelation between dot product and correlation.",
    "start": "2310620",
    "end": "2316330"
  },
  {
    "text": "So the dot product is related\nto the statistical correlation between the elements\nof those two vectors.",
    "start": "2316330",
    "end": "2322210"
  },
  {
    "text": "So if you have a\nvector x and y, you can write down the cosine of\nthe angle between those two",
    "start": "2322210",
    "end": "2329470"
  },
  {
    "text": "vectors, again, as x.y over\nthe product of the norms. And if you write\nthat out as sums,",
    "start": "2329470",
    "end": "2336250"
  },
  {
    "text": "you can see that\nthis is just the sum of the element-by-element\nproducts-- that's the dot product--",
    "start": "2336250",
    "end": "2341740"
  },
  {
    "text": "divided by the norm of\nx and the norm of y. And if you have taken\na statistics class,",
    "start": "2341740",
    "end": "2349190"
  },
  {
    "text": "you will recognize that\nas just the Pearson correlation of a set of numbers\nx and a set of numbers y.",
    "start": "2349190",
    "end": "2358330"
  },
  {
    "text": "The dot product\nis closely related to the correlation between\ntwo sets of numbers.",
    "start": "2358330",
    "end": "2365920"
  },
  {
    "text": " One other thing that\nI want to point out",
    "start": "2365920",
    "end": "2373030"
  },
  {
    "start": "2366000",
    "end": "2520000"
  },
  {
    "text": "coming back to the\nidea of using this feed forward network as a\nway of receptive field,",
    "start": "2373030",
    "end": "2379960"
  },
  {
    "text": "you can see that the response\nof a neuron in this model",
    "start": "2379960",
    "end": "2384970"
  },
  {
    "text": "is just the dot product\nof the stimulus vector u.",
    "start": "2384970",
    "end": "2390510"
  },
  {
    "text": "The vector of input firing\nrates represents the stimulus,",
    "start": "2390510",
    "end": "2395620"
  },
  {
    "text": "the dot product of\nthe stimulus vector u with the weight vector w. So the firing rate of the\noutput neuron is just w.u.",
    "start": "2395620",
    "end": "2403890"
  },
  {
    "text": " So you can see that\nwhat this means is",
    "start": "2403890",
    "end": "2410829"
  },
  {
    "text": "that the firing rate\nof the output neuron will be high if there is\na high degree of overlap",
    "start": "2410830",
    "end": "2420750"
  },
  {
    "text": "between the input, the\npattern of the input, and the pattern of synaptic\nweights from the input layer",
    "start": "2420750",
    "end": "2428220"
  },
  {
    "text": "to the output neuron. We can see that w.u is big\nwhen w and u are parallel,",
    "start": "2428220",
    "end": "2438330"
  },
  {
    "text": "are highly correlated, which\nmeans a neuron fires a lot when",
    "start": "2438330",
    "end": "2444060"
  },
  {
    "text": "the stimulus matches the pattern\nof those synaptic weights. ",
    "start": "2444060",
    "end": "2451190"
  },
  {
    "text": "Now, so you can see\nthat for a given amount of power in the stimulus--",
    "start": "2451190",
    "end": "2456890"
  },
  {
    "text": "so the power is just the\nsquare magnitude of u-- the stimulus that has the best\noverlap with the receptive",
    "start": "2456890",
    "end": "2464150"
  },
  {
    "text": "field, where cosine\nof that angle is 1, produces the largest response.",
    "start": "2464150",
    "end": "2469880"
  },
  {
    "text": " And so we now have\nactually a definition",
    "start": "2469880",
    "end": "2475890"
  },
  {
    "text": "of the optimal stimulus\nof a neuron in terms of the pattern of\nsynaptic weights.",
    "start": "2475890",
    "end": "2482370"
  },
  {
    "text": "In other words, the\noptimal stimulus is one that's essentially\nproportional to the weight",
    "start": "2482370",
    "end": "2489030"
  },
  {
    "text": "matrix. Any questions so far? All right, so now let's\nturn to the question of how",
    "start": "2489030",
    "end": "2497420"
  },
  {
    "text": "we use neural networks to do\nsome interesting computation.",
    "start": "2497420",
    "end": "2504440"
  },
  {
    "text": "So classification is a\nvery important computation",
    "start": "2504440",
    "end": "2509960"
  },
  {
    "text": "that neural networks\ndo in the brain and actually in the\napplication of neural networks",
    "start": "2509960",
    "end": "2517700"
  },
  {
    "text": "for technology. ",
    "start": "2517700",
    "end": "2522809"
  },
  {
    "start": "2520000",
    "end": "2675000"
  },
  {
    "text": "So what does\nclassification mean? So how does the brain-- how does a neural circuit\ndecide how a particular input--",
    "start": "2522810",
    "end": "2535259"
  },
  {
    "text": "let's say that it looks\nlike you might eat it. How do we decide--",
    "start": "2535260",
    "end": "2541339"
  },
  {
    "text": "how do the neural\ncircuits in our brain decide whether that\nthing that we're seeing is something edible or\nsomething that will make us sick",
    "start": "2541340",
    "end": "2550970"
  },
  {
    "text": "based on past experience? If we see something that\nlooks like an animal or a dog,",
    "start": "2550970",
    "end": "2556010"
  },
  {
    "text": "how do we know whether that's a\nfriendly puppy or a or a wolf?",
    "start": "2556010",
    "end": "2562070"
  },
  {
    "text": "So these are\nclassification problems. And feed forward\ncircuits actually",
    "start": "2562070",
    "end": "2567950"
  },
  {
    "text": "can be very good\nat classification. In fact, recent advances\nin training neural networks",
    "start": "2567950",
    "end": "2575599"
  },
  {
    "text": "have actually resulted in feed\nforward neural networks that actually approach human\nperformance in terms",
    "start": "2575600",
    "end": "2582860"
  },
  {
    "text": "of their ability to make\ndecisions like this. ",
    "start": "2582860",
    "end": "2590930"
  },
  {
    "text": "All right. So basically, a\nfeed forward circuit",
    "start": "2590930",
    "end": "2597559"
  },
  {
    "text": "that does\nclassification like this typically has an input layer. It has a bunch of inputs that\nrepresent sensory stimulus.",
    "start": "2597560",
    "end": "2606319"
  },
  {
    "text": "And a bunch of output\nneurons that represent different categorizations\nof that input stimulus.",
    "start": "2606320",
    "end": "2614420"
  },
  {
    "text": "So you can have a\nretinal input here. Going to other\nlayers of a network.",
    "start": "2614420",
    "end": "2620210"
  },
  {
    "text": "And then at the end\nof that, you can have a network that starts\nfiring when that input was",
    "start": "2620210",
    "end": "2625310"
  },
  {
    "text": "a dog, or starts firing\nanother neuron that starts firing when that input\nwas a cat, or something else.",
    "start": "2625310",
    "end": "2632779"
  },
  {
    "text": "Now in general,\nclassification networks that have one input layer\nand one output layer",
    "start": "2632780",
    "end": "2639810"
  },
  {
    "text": "can't do this problem. You can't take a visual\ninput and have connections",
    "start": "2639810",
    "end": "2647040"
  },
  {
    "text": "to another layer of\nneurons that just light up when the picture that the\nnetwork is seeing is a dog.",
    "start": "2647040",
    "end": "2653760"
  },
  {
    "text": "Another neuron lights\nup when it's a cat. Generally, there are many\nlayers of neurons in between.",
    "start": "2653760",
    "end": "2662970"
  },
  {
    "text": "But today, we're going to\ntalk about a very simplified version of the\nclassification problem",
    "start": "2662970",
    "end": "2670250"
  },
  {
    "text": "and build up to the sorts of\nnetworks that can actually do those more complex problems.",
    "start": "2670250",
    "end": "2676980"
  },
  {
    "start": "2675000",
    "end": "3009000"
  },
  {
    "text": "So I just want to point out\nthat the obviously our brains",
    "start": "2676980",
    "end": "2684090"
  },
  {
    "text": "are very good at\nrecognizing things. We do this all the time. There are hundreds of objects\nin every visual scene.",
    "start": "2684090",
    "end": "2691650"
  },
  {
    "text": "And we're able to recognize\nevery one of those objects. But it turns out that there\nare individual neurons--",
    "start": "2691650",
    "end": "2698310"
  },
  {
    "text": "so in this case, I\nalluded to the idea that there are individual\nnones in this network that",
    "start": "2698310",
    "end": "2704490"
  },
  {
    "text": "light up when the\nsensory input is a dog or light up when the\ninput is an elephant.",
    "start": "2704490",
    "end": "2710920"
  },
  {
    "text": "And it turns out that that's\nactually true in the brain. So there have\nrecently been studies",
    "start": "2710920",
    "end": "2718740"
  },
  {
    "text": "where it's been\npossible to record in parts of the human brain in\npatients that are undergoing",
    "start": "2718740",
    "end": "2725680"
  },
  {
    "text": "brain surgery for the\ntreatment of epilepsy or tumors or things like\nthat where you have to go in",
    "start": "2725680",
    "end": "2735240"
  },
  {
    "text": "and find parts of the\nbrain that are defective, find parts of the\nbrain that are healthy.",
    "start": "2735240",
    "end": "2741559"
  },
  {
    "text": "So when you do a\nsurgery, you can be very careful to just\ndo surgery on the damaged",
    "start": "2741560",
    "end": "2747760"
  },
  {
    "text": "parts of the brain\nand not impact parts of the brain that are healthy. So there are cases now,\nmore and more commonly,",
    "start": "2747760",
    "end": "2755920"
  },
  {
    "text": "where neuroscientists can\nwork with neurosurgeons to actually record from\nneurons in the brain",
    "start": "2755920",
    "end": "2762579"
  },
  {
    "text": "in these patients who are\nin preparation for surgery.",
    "start": "2762580",
    "end": "2769000"
  },
  {
    "text": "And so it's been\npossible to record from neurons in the brain. This was a study from\nItzhak Frieds lab at UCLA.",
    "start": "2769000",
    "end": "2779339"
  },
  {
    "text": "And this shows recording in\nthe right anterior hippocampus. And what this lab did\nwas to find neurons.",
    "start": "2779340",
    "end": "2788720"
  },
  {
    "text": "So these were electrodes\nimplanted in the brain. And then they basically\ntake these patients and they show them thousands\nof pictures and look",
    "start": "2788720",
    "end": "2796450"
  },
  {
    "text": "at how their brains respond\nto different visual inputs. So let me just show you\nwhat you're looking at.",
    "start": "2796450",
    "end": "2802090"
  },
  {
    "text": "These are just different\npictures of celebrities. There's Luke Skywalker, Mother\nTeresa, and some others.",
    "start": "2802090",
    "end": "2814795"
  },
  {
    "text": " This paper is getting old\nenough that you may not recognize most of these people.",
    "start": "2814795",
    "end": "2821860"
  },
  {
    "text": "But if you record from\nneurons in the brain, you can see that--",
    "start": "2821860",
    "end": "2826950"
  },
  {
    "text": "so what do you see here? I think that's Oprah. The image is flashed up on\nthe screen for about a second.",
    "start": "2826950",
    "end": "2834820"
  },
  {
    "text": "You record this neuron spiking. Here you see a couple spikes.",
    "start": "2834820",
    "end": "2839900"
  },
  {
    "text": "Here's when the image\nwas actually presented. And here's where the\nimage was turned off. You can see different trials.",
    "start": "2839900",
    "end": "2846053"
  },
  {
    "text": "So this neuron actually\nhad a little bit of a response\nright there shortly after the stimulus\nwas turned on.",
    "start": "2846053",
    "end": "2853130"
  },
  {
    "text": "But you can see there's not that\nmuch response in these neurons.",
    "start": "2853130",
    "end": "2858259"
  },
  {
    "text": "But when they flashed\na different stimulus-- anybody know who that is?",
    "start": "2858260",
    "end": "2865060"
  },
  {
    "text": "That's Halle Berry.  Look at this neuron.",
    "start": "2865060",
    "end": "2871480"
  },
  {
    "text": "Every time you\nshow this picture, that neuron fires off a\ncouple spikes very precisely.",
    "start": "2871480",
    "end": "2877430"
  },
  {
    "text": "If you look at the\nhistogram, these are histograms underneath\nshowing as a function of time",
    "start": "2877430",
    "end": "2884200"
  },
  {
    "text": "relative to the onset\nof the stimulus, you could see that this\nneuron very reliably spikes. There's a different\npicture of Halle Berry.",
    "start": "2884200",
    "end": "2891010"
  },
  {
    "text": "Neuron spikes. Different picture,\nneuron spikes. Another picture, neuron spikes.",
    "start": "2891010",
    "end": "2896450"
  },
  {
    "text": " A line drawing of Halle\nBerry, the neuron spikes.",
    "start": "2896450",
    "end": "2905039"
  },
  {
    "text": "Catwoman, the neuron spikes.",
    "start": "2905040",
    "end": "2910380"
  },
  {
    "text": "The text, Halle Berry,\nthe neuron spikes. ",
    "start": "2910380",
    "end": "2918903"
  },
  {
    "text": "It's amazing.  So this group got a\nlot of press for this",
    "start": "2918903",
    "end": "2928710"
  },
  {
    "text": "because they also found\nJennifer Aniston neurons.",
    "start": "2928710",
    "end": "2934050"
  },
  {
    "text": "They found other celebrities. This is like some celebrity\npart of the brain.",
    "start": "2934050",
    "end": "2940570"
  },
  {
    "text": "No, it's actually\na part of the brain where you have neurons that\nhave very sparse responses",
    "start": "2940570",
    "end": "2945600"
  },
  {
    "text": "to a wide range of things. But they're extremely\nspecific to particular people",
    "start": "2945600",
    "end": "2954240"
  },
  {
    "text": "or categories or objects. And it actually is consistent\nwith this old notion of what's",
    "start": "2954240",
    "end": "2964860"
  },
  {
    "text": "called the grandmother cell. So back before people\nwere able to record",
    "start": "2964860",
    "end": "2970680"
  },
  {
    "text": "in the human brain like\nthis, there was speculation that there might be\nneurons in the brain that",
    "start": "2970680",
    "end": "2976260"
  },
  {
    "text": "are so specific for\nparticular things, that there might be one\nneuron in your brain that responds when you\nsee your grandmother.",
    "start": "2976260",
    "end": "2984240"
  },
  {
    "text": "And so it turns out\nit's actually true. There are neurons\nin your brain that respond very specifically to\nparticular concepts or people",
    "start": "2984240",
    "end": "2995920"
  },
  {
    "text": "or things. So the question of how\nthese kinds of neurons",
    "start": "2995920",
    "end": "3004170"
  },
  {
    "text": "acquire their responses is\nreally cool and interesting. ",
    "start": "3004170",
    "end": "3012010"
  },
  {
    "start": "3009000",
    "end": "3288000"
  },
  {
    "text": "So that leads us to the\nidea of perceptrons.",
    "start": "3012010",
    "end": "3018490"
  },
  {
    "text": "Perceptron is the simplest\nnotion of how you can have a neuron that responds to a\nparticular thing that detects",
    "start": "3018490",
    "end": "3027700"
  },
  {
    "text": "a particular thing and\nresponds when it sees it and doesn't respond\nwhen it doesn't.",
    "start": "3027700",
    "end": "3033190"
  },
  {
    "text": " So let's start with the\nsimplest notion of a perceptron.",
    "start": "3033190",
    "end": "3041150"
  },
  {
    "text": "So how do we make a neuron that\nfires when it sees something-- let's say a dog--",
    "start": "3041150",
    "end": "3047020"
  },
  {
    "text": "and doesn't fire\nwhen there is no dog? ",
    "start": "3047020",
    "end": "3053330"
  },
  {
    "text": "So in order to think about\nthis a little bit more, so we can begin thinking\nabout this in the case",
    "start": "3053330",
    "end": "3060320"
  },
  {
    "text": "where we have a single neuron\ninput and a single output neuron.",
    "start": "3060320",
    "end": "3065460"
  },
  {
    "text": "So if we have a single input\nneuron, then what comes in has to be-- it can't be an image right?",
    "start": "3065460",
    "end": "3070970"
  },
  {
    "text": "An image is a high\ndimensional thing that has many thousands of pixels.",
    "start": "3070970",
    "end": "3077570"
  },
  {
    "text": "So you can't write that\ndown as a simple model with a single input neuron\nand a single output neuron.",
    "start": "3077570",
    "end": "3085560"
  },
  {
    "text": "So you need to do this\nclassification problem in one-dimension. So we can imagine\nthat we have an input neuron that comes from, let's\nsay, some set of numbers--",
    "start": "3085560",
    "end": "3097165"
  },
  {
    "text": "I'll make up a story\nhere-- some set of neurons that measure\nthe dogginess of an input.",
    "start": "3097165",
    "end": "3103670"
  },
  {
    "text": "So let's say that we\nhave a single input that fires like crazy when it sees\nthis cute little guy here.",
    "start": "3103670",
    "end": "3111320"
  },
  {
    "text": "And fires at a\nnegative rate when it sees that thing, which\ndoesn't look much like a dog.",
    "start": "3111320",
    "end": "3120630"
  },
  {
    "text": "So we have a single input\nthat's a measure of dogginess.",
    "start": "3120630",
    "end": "3125960"
  },
  {
    "text": "And now let's say that we\ntake this dogginess detector and we point it\naround the world. And we walk around outside\nwith our dogginess detector",
    "start": "3125960",
    "end": "3133590"
  },
  {
    "text": "and we make a bunch\nof measurements. So we're going to see\nsomething that looks like this.",
    "start": "3133590",
    "end": "3139042"
  },
  {
    "text": "We're going to see a\nlot of measurements, a lot of observations\ndown here that are close to zero dogginess.",
    "start": "3139042",
    "end": "3144859"
  },
  {
    "text": "And we're going to\nsee a bump of things up here that correspond to dogs. Whenever we point our\ndogginess detector at a dog,",
    "start": "3144860",
    "end": "3151877"
  },
  {
    "text": "it's going to give us\na measurement up here. And we're going to\nget a bunch of those. And those things\ncorrespond to dogs.",
    "start": "3151877",
    "end": "3158450"
  },
  {
    "text": "So we need to build\na network that fires when the input is\nup here and doesn't fire",
    "start": "3158450",
    "end": "3164900"
  },
  {
    "text": "when the input is down there.  So how do we do that?",
    "start": "3164900",
    "end": "3171890"
  },
  {
    "text": "So the central feature\nof classification is this notion of binariness,\nof decision-making.",
    "start": "3171890",
    "end": "3180470"
  },
  {
    "text": "That it fires when you\nsee a dog and doesn't fire when you don't see a dog.",
    "start": "3180470",
    "end": "3186380"
  },
  {
    "text": "So there exists a\nclassification boundary in this stimulus space. You can imagine that there's\nsome points along this",
    "start": "3186380",
    "end": "3193550"
  },
  {
    "text": "dimension above which you'll\nsay that that input is a dog, below which you\nsay that it isn't.",
    "start": "3193550",
    "end": "3200600"
  },
  {
    "text": "And we can imagine that\nthat classification boundary is right here. It's a particular number.",
    "start": "3200600",
    "end": "3207060"
  },
  {
    "text": "It's a particular value\nof our dogginess detector, above which we're\ngoing to call it a dog,",
    "start": "3207060",
    "end": "3212460"
  },
  {
    "text": "and below which we're going\nto call it something else. How do we make this\nneuron respond by firing",
    "start": "3212460",
    "end": "3223440"
  },
  {
    "text": "when there's a dog and not\nfiring when there's no dog? Can we use a linear neuron? ",
    "start": "3223440",
    "end": "3231700"
  },
  {
    "text": "Can we use one of\nour linear neurons that we just talked\nabout before?",
    "start": "3231700",
    "end": "3237430"
  },
  {
    "text": "We can't do that because a\nlinear neuron will always fire more the bigger the input is.",
    "start": "3237430",
    "end": "3244450"
  },
  {
    "text": "And it will fire less\nif the dogginess is 0. And it will even\nfire more negatively if the dogginess\ninput is negative.",
    "start": "3244450",
    "end": "3251690"
  },
  {
    "text": "So a linear neuron is\nterrible for actually making any decisions. Linear neurons always go,\nah, well, maybe that's a dog.",
    "start": "3251690",
    "end": "3261670"
  },
  {
    "text": "Not really. There's no decisions. So in order to\nhave a decision, we",
    "start": "3261670",
    "end": "3267220"
  },
  {
    "text": "need to have a particular\nkind of neuron. And that kind of neuron\nuses something very natural.",
    "start": "3267220",
    "end": "3276309"
  },
  {
    "text": "In biophysics, it's the\nspike threshold of neurons. Neurons only fire when the\ninput is above some threshold,",
    "start": "3276310",
    "end": "3285580"
  },
  {
    "text": "generally. There are neurons that\nare tonically active. But let's not worry about those. So many neurons only\nfire when the input",
    "start": "3285580",
    "end": "3291970"
  },
  {
    "start": "3288000",
    "end": "3468000"
  },
  {
    "text": "is above some threshold. So for decision-making\nand classification, a commonly used kind of neuron\ntakes this idea to an extreme.",
    "start": "3291970",
    "end": "3303520"
  },
  {
    "text": "So for perceptrons, we're\ngoing to use a simplified model of a neuron\nthat's particularly",
    "start": "3303520",
    "end": "3309010"
  },
  {
    "text": "good at making decisions. There's no if, ands,\nor buts about it. It's either off or on.",
    "start": "3309010",
    "end": "3316540"
  },
  {
    "text": "It's called a binary unit. And a binary unit uses\nwhat's called a step",
    "start": "3316540",
    "end": "3323010"
  },
  {
    "text": "function for its FI curve. That step function is 0--",
    "start": "3323010",
    "end": "3329170"
  },
  {
    "text": "the output is 0 if the\ninput is zero or below. And the output is 1 if\nthe input is above 0.",
    "start": "3329170",
    "end": "3339090"
  },
  {
    "text": " We can use that step\nfunction to create",
    "start": "3339090",
    "end": "3345500"
  },
  {
    "text": "a neuron that responds\nwhen the input is above any threshold we want.",
    "start": "3345500",
    "end": "3351619"
  },
  {
    "text": "So we can write down the output\nfiring rate is this function,",
    "start": "3351620",
    "end": "3358430"
  },
  {
    "text": "a step function-- that\nfunction of a quantity that's given by w times u, the synaptic\nweight times the input firing",
    "start": "3358430",
    "end": "3367700"
  },
  {
    "text": "rate, minus that threshold. So you can see if\nw times u, which",
    "start": "3367700",
    "end": "3373040"
  },
  {
    "text": "is the input synaptic current,\nif that synaptic current is above theta, then this\nargument to this function",
    "start": "3373040",
    "end": "3385080"
  },
  {
    "text": "is greater than 0,\nthen the neuron spikes. If this argument is negative,\nthen the neuron doesn't spike.",
    "start": "3385080",
    "end": "3391970"
  },
  {
    "text": "So by changing theta, we can put\nthat decision boundary anywhere",
    "start": "3391970",
    "end": "3397950"
  },
  {
    "text": "we want. Does that make sense? ",
    "start": "3397950",
    "end": "3409440"
  },
  {
    "text": "Usually the way we do\nthis is we pick a theta.",
    "start": "3409440",
    "end": "3414480"
  },
  {
    "text": "We say our neuron\nhas a theta of 1. And then we do everything else--",
    "start": "3414480",
    "end": "3420690"
  },
  {
    "text": "we do everything else\nwe're going to do with this network with a theta. So what I'm going to talk\nabout today are just two cases.",
    "start": "3420690",
    "end": "3427830"
  },
  {
    "text": "Where theta is a fixed\nnumber that's non-zero, or theta that's a fixed\nnumber that is equal to 0.",
    "start": "3427830",
    "end": "3434085"
  },
  {
    "text": "So we're going to talk\nabout those two cases.  So the neuron fires\nwhen the input w",
    "start": "3434085",
    "end": "3441120"
  },
  {
    "text": "u is greater than theta. And it doesn't fire\nwhen it's less. So now the output neuron fires\nwhenever the input neuron",
    "start": "3441120",
    "end": "3449410"
  },
  {
    "text": "has a firing rate greater\nthan this decision boundary. So the decision boundary,\nthe u threshold,",
    "start": "3449410",
    "end": "3457720"
  },
  {
    "text": "is equal to theta divided by w. Does that make sense? U threshold is the\nneuron fires when",
    "start": "3457720",
    "end": "3464680"
  },
  {
    "text": "u is greater than\ntheta divided by w. ",
    "start": "3464680",
    "end": "3471030"
  },
  {
    "text": "So the way we learn, the\nway this network learns to fire when that u is above\nthis classification boundary",
    "start": "3471030",
    "end": "3481270"
  },
  {
    "text": "is simply by\nchanging the weight. Does that make sense? So we're going to\nlearn the weight such",
    "start": "3481270",
    "end": "3488380"
  },
  {
    "text": "that this network fires whenever\nthe input says there's a dog. And it doesn't fire whenever\nthe input says there's no dog.",
    "start": "3488380",
    "end": "3494789"
  },
  {
    "text": " So let's see what happens\nwhen w is really small.",
    "start": "3494790",
    "end": "3502650"
  },
  {
    "text": "If w is really small,\nthen what happens is all of these-- remember,\nthis is the input.",
    "start": "3502650",
    "end": "3508359"
  },
  {
    "text": "That's that the\ndogginess detector. If w is really small,\nthen all these inputs",
    "start": "3508360",
    "end": "3514809"
  },
  {
    "text": "get collapsed to a small input\ncurrent into our output neuron.",
    "start": "3514810",
    "end": "3520780"
  },
  {
    "text": "Does that make sense? So all those different\ninputs, dogs and non-dogs,",
    "start": "3520780",
    "end": "3527520"
  },
  {
    "text": "gets multiplied\nby a small number. And all those inputs\nare close to 0.",
    "start": "3527520",
    "end": "3533790"
  },
  {
    "text": "And if all those\ninputs are close to 0, they're all below the threshold\nfor making this neuron spike.",
    "start": "3533790",
    "end": "3540320"
  },
  {
    "text": "So this network is not\ngood for detecting dogs because it says it never\nfires, whether the input",
    "start": "3540320",
    "end": "3548390"
  },
  {
    "text": "is a dog or a non-dog. Now what happens\nif w is too big?",
    "start": "3548390",
    "end": "3554569"
  },
  {
    "text": "If w is really big, then this\nrange of dogginess values",
    "start": "3554570",
    "end": "3561710"
  },
  {
    "text": "gets multiplied by a big number. And you can see that a bunch of\nnon-dogs make the neuron fire.",
    "start": "3561710",
    "end": "3571355"
  },
  {
    "text": "Does that make sense? So now this one fires\nfor dogs plus doggie-ish",
    "start": "3571355",
    "end": "3576450"
  },
  {
    "text": "looking things,\nwhich, I don't know, maybe it'll fire\nwhen it sees a cat. That's terrible. ",
    "start": "3576450",
    "end": "3583860"
  },
  {
    "text": "So you have to choose w to make\nthis classification network",
    "start": "3583860",
    "end": "3589590"
  },
  {
    "text": "function properly. Does that make sense? And if you choose\nw just right, then",
    "start": "3589590",
    "end": "3596630"
  },
  {
    "text": "that classification\nboundary lands right on the threshold\nof the neuron.",
    "start": "3596630",
    "end": "3603220"
  },
  {
    "text": "And now the neuron spikes\nwhenever there is a dog. And it doesn't spike\nwhenever there's not a dog.",
    "start": "3603220",
    "end": "3610510"
  },
  {
    "text": "So what's the message here? The message is we can\nhave a neuron that",
    "start": "3610510",
    "end": "3616450"
  },
  {
    "text": "has this binary threshold. And what we can do is simply\nby changing the weight,",
    "start": "3616450",
    "end": "3625510"
  },
  {
    "text": "we can make that\nthreshold land anywhere on this space of inputs.",
    "start": "3625510",
    "end": "3630700"
  },
  {
    "text": " And we can actually use the\nerror to set the weight.",
    "start": "3630700",
    "end": "3638370"
  },
  {
    "text": "So let's say that\nwe made errors here. We classify dogs as non-dogs\nbecause the neuron didn't fire.",
    "start": "3638370",
    "end": "3646280"
  },
  {
    "text": "You can see that this was the\ncase when w was too small. So if you classify\ndogs as non-dogs,",
    "start": "3646280",
    "end": "3654290"
  },
  {
    "text": "then you need to make w bigger. And if you classify\nnon-dogs as dogs,",
    "start": "3654290",
    "end": "3661100"
  },
  {
    "text": "you need to make w smaller. And by measuring what\nkind of errors you make,",
    "start": "3661100",
    "end": "3668309"
  },
  {
    "text": "you can actually fix the weights\nto get to the right answer.",
    "start": "3668310",
    "end": "3674460"
  },
  {
    "text": "So this is a method\ncalled supervised learning where you\nset w randomly.",
    "start": "3674460",
    "end": "3682380"
  },
  {
    "text": "You take a guess. And then you look at\nthe mistakes you make. And you use those\nmistakes to fix the w.",
    "start": "3682380",
    "end": "3691450"
  },
  {
    "text": "In other words, you\njust look at the world",
    "start": "3691450",
    "end": "3697310"
  },
  {
    "text": "and you say, oh, that's a dog. And then your mom\nsays, no, that's not",
    "start": "3697310",
    "end": "3702349"
  },
  {
    "text": "a dog, that's something else. And you adjust your weights. ",
    "start": "3702350",
    "end": "3708789"
  },
  {
    "text": "I think that was the\nexample I just gave. You're going to\nmake that w smaller. In another case, you'll make\nthe other kind of mistake",
    "start": "3708790",
    "end": "3714720"
  },
  {
    "text": "and you'll fix the weights.  So this is called a perceptron.",
    "start": "3714720",
    "end": "3721650"
  },
  {
    "text": "And the way you learn the\nweights in a perceptron is you just classify things and you\nfigure out what kind of mistake",
    "start": "3721650",
    "end": "3728680"
  },
  {
    "text": "you made and you use that\nto adjust the weights. So that's the basic idea of\na perceptron and perceptron",
    "start": "3728680",
    "end": "3735790"
  },
  {
    "text": "learning. And there's a lot of\nmathematical formalism that goes into how\nthat learning happens.",
    "start": "3735790",
    "end": "3741760"
  },
  {
    "text": "And we're going to\nget to that in more detail in the next lecture.",
    "start": "3741760",
    "end": "3749420"
  },
  {
    "text": "But before we do that,\nI want to go from having a one-dimensional case.",
    "start": "3749420",
    "end": "3754630"
  },
  {
    "text": "So here we had a one-dimensional\nnetwork that was just operating on dogginess.",
    "start": "3754630",
    "end": "3760630"
  },
  {
    "text": "And then we have a\nsingle neuron that says, was that a dog or not.",
    "start": "3760630",
    "end": "3766170"
  },
  {
    "text": "But in general, you're\nnot classifying things based on one input.",
    "start": "3766170",
    "end": "3771420"
  },
  {
    "text": "Like for example when you\nhave to identify a dog, you have a whole\nimage of something.",
    "start": "3771420",
    "end": "3779350"
  },
  {
    "text": "And you have to classify\nthat based on an image. So let's go from the\none-dimensional case to a two-dimensional case.",
    "start": "3779350",
    "end": "3785080"
  },
  {
    "text": "So the classification isn't\ndone on one-dimension, but it's based on many\ndifferent features.",
    "start": "3785080",
    "end": "3791650"
  },
  {
    "text": "So let's say that we have\ntwo features, furriness and bad breath.",
    "start": "3791650",
    "end": "3797500"
  },
  {
    "text": "That dog doesn't really\nlook like it has bad breath. but mine does.",
    "start": "3797500",
    "end": "3804789"
  },
  {
    "text": "So you can have two\ndifferent features, furriness and bad breath. And dogs are generally,\nlet's say, up here.",
    "start": "3804790",
    "end": "3812190"
  },
  {
    "text": "Now you can have other animals. This guy is\ndefinitely not furry.",
    "start": "3812190",
    "end": "3817960"
  },
  {
    "text": "So he's down here somewhere. And you can have\nthis guy up here. He's definitely furry.",
    "start": "3817960",
    "end": "3824700"
  },
  {
    "text": "So you have these two\ndimensions and a bunch of observations in\nthose two dimensions,",
    "start": "3824700",
    "end": "3829800"
  },
  {
    "text": "in those higher dimensions. And you can see\nthat, in this case,",
    "start": "3829800",
    "end": "3835470"
  },
  {
    "text": "you can't actually apply that\none-dimensional decision-making",
    "start": "3835470",
    "end": "3841590"
  },
  {
    "text": "circuit to discriminate dogs\nfrom these other animals.",
    "start": "3841590",
    "end": "3846670"
  },
  {
    "text": "Why is that? Because if I apply my\none-dimensional perceptron",
    "start": "3846670",
    "end": "3851950"
  },
  {
    "text": "to this problem,\nyou can see that I could put a boundary\nhere and it will",
    "start": "3851950",
    "end": "3858600"
  },
  {
    "text": "misclassify some of these\nnon-furry animals as dogs. Or I could put my\nclassifier here",
    "start": "3858600",
    "end": "3866130"
  },
  {
    "text": "and it will misclassify\nsome of these cats as dogs. So how would I separate dogs\nfrom these other animals",
    "start": "3866130",
    "end": "3875109"
  },
  {
    "text": "if I had this\ntwo-dimensional space? What would I do? How would I put a\nclassification bound?",
    "start": "3875110",
    "end": "3882160"
  },
  {
    "text": "If this doesn't work and this\ndoesn't work, what would I do?",
    "start": "3882160",
    "end": "3887270"
  },
  {
    "text": "You could put a\nboundary right there. So in this little\ntoy problem, that would perfectly separate\ndogs from all these non-dogs.",
    "start": "3887270",
    "end": "3895099"
  },
  {
    "text": " So how do we do that?",
    "start": "3895100",
    "end": "3901000"
  },
  {
    "text": "Well, what we want is some\nway of projecting these inputs",
    "start": "3901000",
    "end": "3908290"
  },
  {
    "text": "onto some other\ndirection so that we can put a classification\nboundary right there.",
    "start": "3908290",
    "end": "3916470"
  },
  {
    "text": "And it turns out there's a very\nsimple network that does that. It looks like this. We take each one of those\ndetectors, a furriness detector",
    "start": "3916470",
    "end": "3925620"
  },
  {
    "text": "and a bad breath detector,\nand we have those two inputs.",
    "start": "3925620",
    "end": "3931470"
  },
  {
    "text": "We have those inputs synapse\nonto our output neuron with some weight w1\nand some weight w2,",
    "start": "3931470",
    "end": "3937800"
  },
  {
    "text": "and we calculate the\nfiring rate of this neuron. Now we have this problem of\nhow do we place this decision",
    "start": "3937800",
    "end": "3946079"
  },
  {
    "text": "boundary correctly. What's the answer? Well, in the\none-dimensional example,",
    "start": "3946080",
    "end": "3951645"
  },
  {
    "text": "what is it that we learned? ",
    "start": "3951645",
    "end": "3957400"
  },
  {
    "text": "What was it that we\nwere actually changing? We were taking guesses.",
    "start": "3957400",
    "end": "3962829"
  },
  {
    "text": "And if we were right\nor wrong, we did what? We changed the weight.",
    "start": "3962830",
    "end": "3968170"
  },
  {
    "text": "And that's exactly\nwhat we do here. We're going to learn to\nchange these weights to put",
    "start": "3968170",
    "end": "3974319"
  },
  {
    "text": "that boundary in\nthe right place.  If we just take a random\nguess for these weights,",
    "start": "3974320",
    "end": "3981260"
  },
  {
    "text": "that line is just going to\nbe some random position. But we can learn to\nplace that line exactly",
    "start": "3981260",
    "end": "3988160"
  },
  {
    "text": "in the right place to\nseparate dogs from non-dogs. So let's just think\na little bit more",
    "start": "3988160",
    "end": "3994040"
  },
  {
    "text": "about how that\ndecision boundary looks as a function of the weight.",
    "start": "3994040",
    "end": "4001030"
  },
  {
    "text": "So let's look at this case\nwhere we have two inputs. So now you can see that the\ninput to this neuron is w.u.",
    "start": "4001030",
    "end": "4011700"
  },
  {
    "text": "So now if we use our binary\nneuron with a threshold,",
    "start": "4011700",
    "end": "4016950"
  },
  {
    "text": "we can see that the firing\nrate of this output neuron is this step function\noperating on or acting",
    "start": "4016950",
    "end": "4025610"
  },
  {
    "text": "on this input, w.u minus theta. ",
    "start": "4025610",
    "end": "4032750"
  },
  {
    "text": "So now what does that look like? The decision\nboundary happens when this quantity is pulled to 0.",
    "start": "4032750",
    "end": "4040099"
  },
  {
    "text": "When this input is greater\nthan 0, the neuron fires. When this input is less\nthan 0, it doesn't fire.",
    "start": "4040100",
    "end": "4045470"
  },
  {
    "text": "So what does that look like? So you can see the\ndecision boundary is when w.u minus theta equals 0.",
    "start": "4045470",
    "end": "4052180"
  },
  {
    "text": "Does anyone know what that is? ",
    "start": "4052180",
    "end": "4057688"
  },
  {
    "text": "Remember, u is our input space. That's what we're asking,\nwhere is this decision",
    "start": "4057688",
    "end": "4063599"
  },
  {
    "text": "boundary in the input space. w is some weights that\nare fixed right now,",
    "start": "4063600",
    "end": "4068610"
  },
  {
    "text": "but we're gradually going\nto change them later. So what is that an equation for?",
    "start": "4068610",
    "end": "4075290"
  },
  {
    "text": "It's a line. That's an equation for a line. If u is our input, you\ncan see w.u equals theta.",
    "start": "4075290",
    "end": "4087880"
  },
  {
    "text": "That's an equation\nfor a line, base of u. The slope and\nposition of that line",
    "start": "4087880",
    "end": "4094750"
  },
  {
    "text": "are controlled by the weights\nw and the threshold theta.",
    "start": "4094750",
    "end": "4100109"
  },
  {
    "text": "So you can see this is w1,\nu1, plus w2, u2 equals theta.",
    "start": "4100109",
    "end": "4106139"
  },
  {
    "text": "In the space of u1 and\nu2, that's just a line. So let's look at the case\nwhere theta equals 0.",
    "start": "4106140",
    "end": "4114600"
  },
  {
    "text": "You can see that if you have\nthis input space, u1 and u2, if you take a particular\ninput u and dot it into w--",
    "start": "4114600",
    "end": "4124890"
  },
  {
    "text": "so let's just pick a w in\nsome random direction-- the neuron fires when the\nprojection of u along w",
    "start": "4124890",
    "end": "4132270"
  },
  {
    "text": "is positive. So you can see here, the\nprojection of u along w is positive.",
    "start": "4132270",
    "end": "4140020"
  },
  {
    "text": "So in this case for this\nu the neuron will fire. ",
    "start": "4140020",
    "end": "4145799"
  },
  {
    "text": "So any u that has a\npositive projection along w",
    "start": "4145800",
    "end": "4151349"
  },
  {
    "text": "will make the neuron spike. So you can see that\nall of these inputs",
    "start": "4151350",
    "end": "4157500"
  },
  {
    "text": "will make the neuron spike. All of these inputs will\nmake the neuron not spike.",
    "start": "4157500",
    "end": "4164422"
  },
  {
    "text": "Does that make sense? So you can see that the\ndecision boundary, this boundary",
    "start": "4164423",
    "end": "4170429"
  },
  {
    "text": "between the inputs\nthat make the neuron spike and the inputs that\ndon't make the neuron spike,",
    "start": "4170430",
    "end": "4177390"
  },
  {
    "text": "is a line that's\northogonal to w. Does that make sense?",
    "start": "4177390",
    "end": "4183120"
  },
  {
    "text": " Because you can see\nthat any u, any input,",
    "start": "4183120",
    "end": "4190930"
  },
  {
    "text": "along this line will\nhave zero projection, will be orthogonal to w. Will have zero projection.",
    "start": "4190930",
    "end": "4197560"
  },
  {
    "text": "And that's going to correspond\nto that decision boundary.",
    "start": "4197560",
    "end": "4202840"
  },
  {
    "text": " So let's just look\nat a couple of cases.",
    "start": "4202840",
    "end": "4212230"
  },
  {
    "text": "So here a set of points that\ncorrespond to our non-dogs.",
    "start": "4212230",
    "end": "4217920"
  },
  {
    "text": "Here are a set of points\nthat correspond to our dog. You can see that if you have\na w in this direction, that",
    "start": "4217920",
    "end": "4223739"
  },
  {
    "text": "produces a decision boundary\nthat nicely separates the dogs from the non-dogs.",
    "start": "4223740",
    "end": "4228980"
  },
  {
    "text": "So what is that w?\nthat w is 1, comma, 0. And we're going to consider\nthe case where theta is 0.",
    "start": "4228980",
    "end": "4236790"
  },
  {
    "text": "Let's look at this case here. So you can see that\nhere are all the dogs. Here are all the non-dogs. You can see that if you drew\na line in this direction,",
    "start": "4236790",
    "end": "4244080"
  },
  {
    "text": "that would be a good\ndecision boundary for that classification problem.",
    "start": "4244080",
    "end": "4249330"
  },
  {
    "text": "You can see that\na w corresponding to solving that problem\nis 1, comma, minus 1,",
    "start": "4249330",
    "end": "4256199"
  },
  {
    "text": "and theta equals 0. ",
    "start": "4256200",
    "end": "4262989"
  },
  {
    "text": "Let's look at the case\nwhere theta is not 0. So here we have w.u minus theta.",
    "start": "4262990",
    "end": "4269730"
  },
  {
    "text": "When theta is not 0, then\nthe decision boundary is w.u equals some non-zero theta.",
    "start": "4269730",
    "end": "4275760"
  },
  {
    "text": "That's also a line. It's a equation for a line. When theta is 0, that\ndecision boundary",
    "start": "4275760",
    "end": "4282239"
  },
  {
    "text": "goes through the origin. When theta is not 0,\nthe decision boundary is offset from the origin.",
    "start": "4282240",
    "end": "4289050"
  },
  {
    "text": "So we could see that\nwhen we had theta is 0, the decision boundary--\nthat network only works if the decision boundary\nis going through the origin.",
    "start": "4289050",
    "end": "4297900"
  },
  {
    "text": "In general, though, we can put\nthe decision boundary anywhere we want by having\nthis non-zero theta.",
    "start": "4297900",
    "end": "4304739"
  },
  {
    "text": "So here's an example. Here are a set of points\nthat are the dogs. Here are a set of points\nthat are the non-dogs.",
    "start": "4304740",
    "end": "4312390"
  },
  {
    "text": "If we wanted to\ndesign a network that separates the dogs\nfrom the non-dogs,",
    "start": "4312390",
    "end": "4317639"
  },
  {
    "text": "we could just draw a line\nthat cleanly separates the green from the red dots.",
    "start": "4317640",
    "end": "4323880"
  },
  {
    "text": "And now we can calculate\nw that gives us that decision boundary. How do we do that?",
    "start": "4323880",
    "end": "4330460"
  },
  {
    "text": "So the decision boundary\nis w minus u.theta. Let's say that we\nwant to calculate this weight vector w1 and w2.",
    "start": "4330460",
    "end": "4337730"
  },
  {
    "text": "And let's just say that our\nneuron has a threshold of 1. So we can see that we have\ntwo points on the decision",
    "start": "4337730",
    "end": "4345080"
  },
  {
    "text": "boundary. We have one point here,\na, comma, 0, right there.",
    "start": "4345080",
    "end": "4350660"
  },
  {
    "text": "We have another point\nhere, 0, comma, b. And we can calculate\nthe decision boundary",
    "start": "4350660",
    "end": "4356960"
  },
  {
    "text": "using ua.w equals theta,\nub.w equals theta.",
    "start": "4356960",
    "end": "4363200"
  },
  {
    "text": "That's two equations and\ntwo unknowns, w1 and w2.",
    "start": "4363200",
    "end": "4370040"
  },
  {
    "text": "So if I gave you a set of points\nand I said calculate a weight",
    "start": "4370040",
    "end": "4377090"
  },
  {
    "text": "for this perceptron that will\nseparate one set of points from another set of\npoints, and I give you",
    "start": "4377090",
    "end": "4386370"
  },
  {
    "text": "a theta for the output\nneuron, all you have to do is draw a line that\nseparates them,",
    "start": "4386370",
    "end": "4392960"
  },
  {
    "text": "and then solve those\ntwo equations to get w1 and w2 for that network.",
    "start": "4392960",
    "end": "4400632"
  },
  {
    "text": "It's very easy to do\nthis in two dimensions. You can just draw a\nline and calculate",
    "start": "4400632",
    "end": "4406920"
  },
  {
    "text": "the w that corresponds to\nthat decision boundary.",
    "start": "4406920",
    "end": "4412260"
  },
  {
    "text": "Any questions about that? Just that, if you\nhave questions, you should ask because\nthat's going to be",
    "start": "4412260",
    "end": "4420119"
  },
  {
    "text": "a question you ought to solve. ",
    "start": "4420120",
    "end": "4427000"
  },
  {
    "text": "So you can see in two dimensions\nyou can just look at the data, decide where's the decision\nboundary, draw a line,",
    "start": "4427000",
    "end": "4432760"
  },
  {
    "text": "and calculate the weights w. But in higher dimensions,\nit's a really hard problem.",
    "start": "4432760",
    "end": "4439150"
  },
  {
    "text": "In high dimensions,\nfirst of all, remember in high dimensions\nyou've got images.",
    "start": "4439150",
    "end": "4447190"
  },
  {
    "text": "Each pixel in that image\nis a different dimension in the classification problem.",
    "start": "4447190",
    "end": "4453400"
  },
  {
    "text": "So how do you write\ndown a set of weights? So imagine that's an\nimage, that's an image.",
    "start": "4453400",
    "end": "4462430"
  },
  {
    "text": "And you want to find\na set of weights so that this neuron fires\nwhen you have the dog,",
    "start": "4462430",
    "end": "4467530"
  },
  {
    "text": "but doesn't fire when\nyou have the cat. That's a really hard problem. You can't look at\nthose things and decide",
    "start": "4467530",
    "end": "4474130"
  },
  {
    "text": "what that w should be.  So there's a way of taking\ninputs and taking the answer,",
    "start": "4474130",
    "end": "4488740"
  },
  {
    "text": "like a 1 for a dog\nand a 0 for non-dogs, and actually finding a set\nof weights that will properly",
    "start": "4488740",
    "end": "4495440"
  },
  {
    "text": "classify those inputs. And that's called the\nperceptron learning rule. And we're going to talk about\nthat in the next lecture.",
    "start": "4495440",
    "end": "4505579"
  },
  {
    "text": "So that's what we did today. And we're going to\ncontinue working on developing methods\nfor understanding",
    "start": "4505580",
    "end": "4514880"
  },
  {
    "text": "neural networks next time. ",
    "start": "4514880",
    "end": "4524000"
  }
]