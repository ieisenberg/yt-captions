[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "7712"
  },
  {
    "text": " STEVEN JOHNSON: So I\nwanted to start the day just by finishing up a couple--",
    "start": "7712",
    "end": "15150"
  },
  {
    "text": "comment on a couple\nof additional things from the last lecture. So last lecture, we talked\nabout finite difference",
    "start": "15150",
    "end": "20460"
  },
  {
    "text": "approximations. In particular, I talked about\nthe most obvious approximation,",
    "start": "20460",
    "end": "25803"
  },
  {
    "text": "which is just from the\ndefinition of the derivative. We approximate df\nby f prime of xdx,",
    "start": "25803",
    "end": "35050"
  },
  {
    "text": "our linear thing, the exact\nderivative for a infinitesimal, dx, by just f of x plus dx minus\nf of x for some finite delta x,",
    "start": "35050",
    "end": "46330"
  },
  {
    "text": "noninfinitesimal. And if you divide\nboth sides by delta x,",
    "start": "46330",
    "end": "52720"
  },
  {
    "text": "the more familiar\nform is was this one. This is called the forward\ndifference approximation .",
    "start": "52720",
    "end": "58600"
  },
  {
    "text": "And we showed that this\nhas an error in it that decreases linearly with dx\nuntil dx becomes really small",
    "start": "58600",
    "end": "70750"
  },
  {
    "text": "and then becomes limited\nby roundoff errors. But I wanted to mention\nthat, of course, there are other ways of\napproximating derivatives",
    "start": "70750",
    "end": "78010"
  },
  {
    "text": "by differences, some of\nwhich are even more accurate. They're called higher\norder finite difference.",
    "start": "78010",
    "end": "84000"
  },
  {
    "text": "So the forward differences\nare first order because the error\ngoes linearly with dx.",
    "start": "84000",
    "end": "90110"
  },
  {
    "text": "The most famous next\nimprovement would be approximate f prime dx\nby instead of f of x plus dx",
    "start": "90110",
    "end": "100510"
  },
  {
    "text": "minus f of x, we take f of x\nplus dx minus f of x minus dx.",
    "start": "100510",
    "end": "106600"
  },
  {
    "text": "And since now you've basically\ndoubled the interval, because you're going\nfrom minus dx to plus dx,",
    "start": "106600",
    "end": "112450"
  },
  {
    "text": "you have to divide by 2. Or if x is a scalar, so then\nyou can just divide by delta x.",
    "start": "112450",
    "end": "120280"
  },
  {
    "text": "The second form is\nthe more familiar form of these center difference\napproximations, which is you",
    "start": "120280",
    "end": "126790"
  },
  {
    "text": "divide both sides by\ndelta x, which only works if delta x is a scalar. And you get f prime\nis approximately f",
    "start": "126790",
    "end": "133540"
  },
  {
    "text": "of x plus dx minus f of x\nminus dx divided by 2 dx,",
    "start": "133540",
    "end": "139420"
  },
  {
    "text": "so again, 2 because you're\ngoing from minus dx to plus dx. So the overall interval is 2dx.",
    "start": "139420",
    "end": "146680"
  },
  {
    "text": "So it turns out that\nthe error in this case, instead of going linearly\nwith delta x, is quadratic.",
    "start": "146680",
    "end": "154845"
  },
  {
    "text": "I guess this should be\norder delta x squared. I left out the x. I'll fix that later.",
    "start": "154845",
    "end": "162520"
  },
  {
    "text": "And the reason it\ngoes quadratically-- so this is called\nsecond order accurate--",
    "start": "162520",
    "end": "170480"
  },
  {
    "text": "you can derive it in\na variety of ways. But one way to derive\nit is to just plug",
    "start": "170480",
    "end": "176299"
  },
  {
    "text": "in the Taylor expansion for f of\nx plus dx and f of x minus dx.",
    "start": "176300",
    "end": "181730"
  },
  {
    "text": "And what happens is\nbecause this is symmetric-- it's f of x plus dx\nminus f of x minus dx,",
    "start": "181730",
    "end": "188269"
  },
  {
    "text": "all the terms with even powers\nof delta x in the Taylor series",
    "start": "188270",
    "end": "193850"
  },
  {
    "text": "all cancel because if it's\nan even power, like remember, the there's f plus f prime dx\nplus f double prime dx over 2,",
    "start": "193850",
    "end": "204860"
  },
  {
    "text": "and so forth-- that's\nyour Taylor series-- f double prime dx\nsquared over 2.",
    "start": "204860",
    "end": "210320"
  },
  {
    "text": "Sorry. Anything with an\neven power of delta x, It doesn't matter if\nit's plus dx or minus dx,",
    "start": "210320",
    "end": "217739"
  },
  {
    "text": "it's the same thing when\nyou square it or take the fourth power and so forth. So when you subtract these\ntwo terms, they cancel.",
    "start": "217740",
    "end": "223549"
  },
  {
    "text": "And so the error becomes\nthe next order term, assuming it's at least\nthree times differentiable.",
    "start": "223550",
    "end": "230600"
  },
  {
    "text": "So it turns out this\ncenter difference formula, the error and the derivative\ngoes like delta x squared,",
    "start": "230600",
    "end": "237739"
  },
  {
    "text": "or if I multiply both\nsides by delta x, the error in the difference\ngoes like delta x cubed. ",
    "start": "237740",
    "end": "244670"
  },
  {
    "text": "And we can check this. We can plot it for sine x. So I plot, for sine x, we're\ngoing to approximate it",
    "start": "244670",
    "end": "252260"
  },
  {
    "text": "at x equals 1 by either the\nfirst order difference, sine",
    "start": "252260",
    "end": "257479"
  },
  {
    "text": "of 1 plus something\nminus sine of 1 over s, or the center\ndifference, sine of 1",
    "start": "257480",
    "end": "264740"
  },
  {
    "text": "plus s minus sine of\n1 minus s over 2s. I don't know why I\ncalled it s there.",
    "start": "264740",
    "end": "272960"
  },
  {
    "text": "And I'll plot it as a\nfunction of delta x. And the forward\ndifference is the blue.",
    "start": "272960",
    "end": "281290"
  },
  {
    "text": "That goes linearly with delta x. The blue line is just a plot of\ndelta x for reference, linear.",
    "start": "281290",
    "end": "292669"
  },
  {
    "text": "And the central difference\nis the red dots.",
    "start": "292670",
    "end": "297890"
  },
  {
    "text": "They go quadratically\nwith delta x.  And the straight red line is\njust a power law quadratic",
    "start": "297890",
    "end": "307048"
  },
  {
    "text": "with delta x. Of course, again, this\nis a log log scale. So power laws turn\ninto straight lines until in both cases, once they\nhit the machine precision,",
    "start": "307048",
    "end": "318090"
  },
  {
    "text": "the error starts getting worse,\nif you make delta x too small. And in both cases, you want to\nhave a delta x that's small,",
    "start": "318090",
    "end": "328810"
  },
  {
    "text": "but not too small, like 10\nto the minus 5, in this case, for the quadratic case,\nor 10 to the minus-- yeah,",
    "start": "328810",
    "end": "336890"
  },
  {
    "text": "5 was about where this-- ",
    "start": "336890",
    "end": "342670"
  },
  {
    "text": "the error is minimum\nin this case. And in fact, there are even\nhigher order differences.",
    "start": "342670",
    "end": "350310"
  },
  {
    "text": "If you take even more points--",
    "start": "350310",
    "end": "355830"
  },
  {
    "text": "in some sense, what\nyou're doing is basically taking multiple points and\nfitting it to a polynomial",
    "start": "355830",
    "end": "361020"
  },
  {
    "text": "and taking the slope from that. So as you get to add even\nmore and more points,",
    "start": "361020",
    "end": "366420"
  },
  {
    "text": "you can fit to higher and\nhigher order degree polynomials and get higher order\ndifference formulas",
    "start": "366420",
    "end": "371760"
  },
  {
    "text": "that give even smaller error\nfor the same size of delta x.",
    "start": "371760",
    "end": "377610"
  },
  {
    "text": "Another even fancier\nthing to do is you can start with\njust some delta x",
    "start": "377610",
    "end": "385670"
  },
  {
    "text": "and then compute a sequence\nof delta x's that you keep dividing it by 2 or something.",
    "start": "385670",
    "end": "391070"
  },
  {
    "text": "And each time you\ndivide it by 2, you fit it to a higher\ndegree polynomial. And you're extrapolating the\nslope to delta x equals 0.",
    "start": "391070",
    "end": "399860"
  },
  {
    "text": "And it turns out there's a\nway to do this adaptively. So you keep decreasing delta\nx by, say, a factor of 2,",
    "start": "399860",
    "end": "405560"
  },
  {
    "text": "and you go to higher and\nhigher order polynomials. But there's a way to\nwatch this and figure out",
    "start": "405560",
    "end": "412100"
  },
  {
    "text": "that at some point when\ndelta x goes too small, you can notice that the\nerror is getting worse. And you can stop.",
    "start": "412100",
    "end": "418050"
  },
  {
    "text": "So you can adaptively\nfind the optimal delta x and the optimal order to\nextrapolate as accurately",
    "start": "418050",
    "end": "425810"
  },
  {
    "text": "as possible. And the fancy version\nof this is called Richardson extrapolation. So it's a really nice technique\nfor extrapolating functions.",
    "start": "425810",
    "end": "432938"
  },
  {
    "text": "I'm not going to go\ninto it in more detail. But it's a fun thing to look up. I have a package\nfor it in Julia.",
    "start": "432938",
    "end": "438050"
  },
  {
    "text": "And there's a Julia package\ncalled FiniteDifferences.jl that has zillions of different\nfinite difference formulas,",
    "start": "438050",
    "end": "443729"
  },
  {
    "text": "including Richardson\nextrapolation. They can get pretty\nhigh order accuracy. But anyway, so I wanted\nto mention those.",
    "start": "443730",
    "end": "450770"
  },
  {
    "text": "And the other thing\nI wanted to mention was what happens when you\ngo to higher dimensions, so",
    "start": "450770",
    "end": "457590"
  },
  {
    "text": "higher dimensional inputs. So suppose instead of\njust x is a number, you have x is a vector\nor something like that.",
    "start": "457590",
    "end": "467350"
  },
  {
    "text": "So of course, this\nformula still works.",
    "start": "467350",
    "end": "472620"
  },
  {
    "text": "We can still\napproximate f prime dx by f of x plus dx minus f of x.",
    "start": "472620",
    "end": "478640"
  },
  {
    "text": "And there are still higher\norder versions as well. We can no longer divide by dx. But that's not a big deal.",
    "start": "478640",
    "end": "486940"
  },
  {
    "text": "But each finite difference,\nif x is a vector, each finite difference\nbasically only",
    "start": "486940",
    "end": "493480"
  },
  {
    "text": "gives you the derivative in\none direction in that space.",
    "start": "493480",
    "end": "499340"
  },
  {
    "text": "So think of, for example, if\nf is a scalar, a scalar valued",
    "start": "499340",
    "end": "504949"
  },
  {
    "text": "function, and x is a\nvector, then what you really want is the gradient. And every component\nof the gradient",
    "start": "504950",
    "end": "510439"
  },
  {
    "text": "is the derivative in one\ndirection in that space. ",
    "start": "510440",
    "end": "518070"
  },
  {
    "text": "But every finite difference\nonly effectively gives you one component of\nthe-- can give you one component of the gradient.",
    "start": "518070",
    "end": "525270"
  },
  {
    "text": "It can only give\nyou the derivative in that direction, dx. So if you want the derivative\nin all possible directions,",
    "start": "525270",
    "end": "531600"
  },
  {
    "text": "and x is n dimensional, you\nneed n finite differences. So for example, if x is\nan RN and f is a scalar,",
    "start": "531600",
    "end": "542370"
  },
  {
    "text": "and you want every\ncomponent of the gradient, and it has n\ncomponents, you want each one of those partial\nderivatives, each one of those",
    "start": "542370",
    "end": "548730"
  },
  {
    "text": "is a separate finite difference. So you need n\nfinite differences.",
    "start": "548730",
    "end": "554170"
  },
  {
    "text": "And so what happens is this\ngets this gets very expensive. So if you have three\nparameters, fine. You need three\nfinite differences.",
    "start": "554170",
    "end": "560470"
  },
  {
    "text": "But if you have a\nmillion parameters, then you need to take a finite\ndifferences a million times.",
    "start": "560470",
    "end": "565680"
  },
  {
    "text": "You need a million\ndifferent delta x's. And if f is a very\nexpensive function,",
    "start": "565680",
    "end": "570960"
  },
  {
    "text": "like evaluating a neural\nnetwork, this is horrible. Evaluating a neural\nnetwork a million times",
    "start": "570960",
    "end": "576430"
  },
  {
    "text": "is very expensive. So the net result is\nthat in high dimensions,",
    "start": "576430",
    "end": "581889"
  },
  {
    "text": "this becomes incredibly\nimpractical to compute finite differences\nif you really want all the derivatives, the\nderivatives in all directions,",
    "start": "581890",
    "end": "589910"
  },
  {
    "text": "which you want for\nthings like optimization. So you really, really need to\ndo derivatives analytically",
    "start": "589910",
    "end": "598180"
  },
  {
    "text": "in high dimensions\nin order for this to be practical for\nthings like machine learning, optimizing neural\nnetworks, to be practical.",
    "start": "598180",
    "end": "605780"
  },
  {
    "text": "That's the only way to get\nthe a million derivatives efficiently. On the other hand, they're\nstill very useful as a check.",
    "start": "605780",
    "end": "613630"
  },
  {
    "text": "So as a check, if you're\ndeveloping your own derivative,",
    "start": "613630",
    "end": "618633"
  },
  {
    "text": "for example, if you're doing\nyour homework problems, and some of those\nfunctions, it's not so easy.",
    "start": "618633",
    "end": "624279"
  },
  {
    "text": "Derivatives-- you might think\nin 1801, derivatives are easy. But when the functions\nget more complicated,",
    "start": "624280",
    "end": "629470"
  },
  {
    "text": "you have vectors in,\nvectors out, or vectors in, matrices out, probably\nyou're seeing it's not always",
    "start": "629470",
    "end": "635740"
  },
  {
    "text": "so easy to get the\nderivative and be confident you did all the\nchain rules and product",
    "start": "635740",
    "end": "640810"
  },
  {
    "text": "rules correctly. So it's really, really\nuseful to check it. And I definitely encourage\nyou on homework to check it.",
    "start": "640810",
    "end": "647710"
  },
  {
    "text": "And you can very easily\ncheck it just by, basically,",
    "start": "647710",
    "end": "652770"
  },
  {
    "text": "just computing a few\nrandom directions, so to choose delta x as a\nrandom vector that's small,",
    "start": "652770",
    "end": "658620"
  },
  {
    "text": "do it a few times. If that matches\nto several decimal places your f prime dx,\nthat's a pretty good sign",
    "start": "658620",
    "end": "667160"
  },
  {
    "text": "that you didn't make a mistake. It's very unlikely that if you\nhave a bug, that it will just",
    "start": "667160",
    "end": "673430"
  },
  {
    "text": "happen to be correct\nto six decimal places in some random\ndirection in space. ",
    "start": "673430",
    "end": "680959"
  },
  {
    "text": "So I just wanted\nto stop with that. So any questions about\nfinite differences before I'm going to move on to\na completely different topic?",
    "start": "680960",
    "end": "687270"
  },
  {
    "start": "687270",
    "end": "693950"
  },
  {
    "text": "So what I want to do now is\ntalk more about gradients.",
    "start": "693950",
    "end": "701390"
  },
  {
    "text": "And I want to\ngeneralize gradients to higher dimensional problems.",
    "start": "701390",
    "end": "706670"
  },
  {
    "text": "So the familiar\ngradient from 18.02 is when you have a function that\ntakes a vector in and gives you",
    "start": "706670",
    "end": "716630"
  },
  {
    "text": "a scalar out, just where\nthe vector is just like an n component column vector.",
    "start": "716630",
    "end": "722120"
  },
  {
    "text": "And then the gradient is an\nn component column vector.",
    "start": "722120",
    "end": "727370"
  },
  {
    "text": "It's a component\nvector of partial f, partial x1, partial f,\npartial x2, and so forth.",
    "start": "727370",
    "end": "734332"
  },
  {
    "text": "And in this class, we\ntry and think of it more holistically that we\nhave the differential, df,",
    "start": "734332",
    "end": "742370"
  },
  {
    "text": "is f of x plus dx minus f of x. So this is just review. ",
    "start": "742370",
    "end": "749620"
  },
  {
    "text": "Our derivative is the linear\noperator that takes in the dx",
    "start": "749620",
    "end": "754779"
  },
  {
    "text": "and gives you df to first order. So this linear operator\ntakes in a vector dx",
    "start": "754780",
    "end": "760490"
  },
  {
    "text": "and gives you out a scalar. So we sometimes call that a\nlinear form, something that",
    "start": "760490",
    "end": "766790"
  },
  {
    "text": "takes a vector in\nand a scalar out. And if this is a column vector,\nan n component column vector,",
    "start": "766790",
    "end": "773970"
  },
  {
    "text": "it's pretty easy to see that\nthe only linear operator that takes a column vector in\nand gives a scalar out",
    "start": "773970",
    "end": "780360"
  },
  {
    "text": "is multiplying by a row\nvector, or equivalently, is taking a dot product\nof dx with some vector.",
    "start": "780360",
    "end": "787590"
  },
  {
    "text": "And that thing we\ncall the gradient. So this f prime of x\nis then a row vector.",
    "start": "787590",
    "end": "792630"
  },
  {
    "text": "It's the transpose of the\ngradient and the gradient, or equivalently, the\ngradient is the thing you take the dot product\nof dx with to get df.",
    "start": "792630",
    "end": "800790"
  },
  {
    "text": "And this is equivalent to the\n18.02 definition of gradient. But what I want to do today is\ntalk about generalizing this",
    "start": "800790",
    "end": "809460"
  },
  {
    "text": "to other kinds of\nvector spaces, to where x is not necessarily a column\nvector, but some other kind",
    "start": "809460",
    "end": "815339"
  },
  {
    "text": "of vector. But we still have a\nscalar valued function. So we're still going\nto generalize to--",
    "start": "815340",
    "end": "822180"
  },
  {
    "text": "we're still talking about\nscalar value functions, for example, a function that\ntakes a matrix in and gives you",
    "start": "822180",
    "end": "827399"
  },
  {
    "text": "a number out. So for example, a\ndeterminant is a function that takes a matrix in,\nbut gives you a scalar out.",
    "start": "827400",
    "end": "834550"
  },
  {
    "text": "And can we talk about the\ngradient of such a function? What is the gradient\nof a determinant?",
    "start": "834550",
    "end": "842829"
  },
  {
    "text": "And so we'd like to have\nit this same kind of thing, where it's some\nkind of dot product, and where the gradient is\nof the same shape as x.",
    "start": "842830",
    "end": "852520"
  },
  {
    "text": "So if x is a column vector\nhere of n components, the gradient is a column vector. And this is going to be\nreally important soon,",
    "start": "852520",
    "end": "858820"
  },
  {
    "text": "when you start talking about\noptimization because then you can think of the gradient\nas the uphill direction",
    "start": "858820",
    "end": "864370"
  },
  {
    "text": "for the function. So if you want to make\nthe function bigger, you go in the direction\nof the gradient. That's uphill.",
    "start": "864370",
    "end": "869410"
  },
  {
    "text": "If you want to make\na function smaller, you go in the opposite\ndirection of the gradient. That's downhill.",
    "start": "869410",
    "end": "875839"
  },
  {
    "text": "So how do we generalize\nthis to other kinds of things that are not\njust n component column",
    "start": "875840",
    "end": "881660"
  },
  {
    "text": "vectors and not the\nfamiliar 18.02 gradient? So I think it's pretty easy to--",
    "start": "881660",
    "end": "890068"
  },
  {
    "text": "what we basically want is\nthe same kind of thing. If x lives in some\narbitrary vector space,",
    "start": "890068",
    "end": "895899"
  },
  {
    "text": "and we have a\nscalar function that takes x in that vector space\nand gives you a number out,",
    "start": "895900",
    "end": "901410"
  },
  {
    "text": "a real number-- everything's\nreal in our class right now. So all our scalars\nare real numbers.",
    "start": "901410",
    "end": "909120"
  },
  {
    "text": "So it's a linear operator\nthat takes a dx, a vector, in and gives you a scalar out.",
    "start": "909120",
    "end": "915210"
  },
  {
    "text": "And we want to\ndefine a gradient. What we want is we\nwant to define this as some kind of dot product.",
    "start": "915210",
    "end": "921220"
  },
  {
    "text": "And so to do that, we need\nto define a dot product for our vector space.",
    "start": "921220",
    "end": "927100"
  },
  {
    "text": "So in general,\ngradients, we're going to be able to define gradients\nany time we have a vector",
    "start": "927100",
    "end": "932490"
  },
  {
    "text": "space, we have a\nscalar value function, and we have a dot product\non the vector space.",
    "start": "932490",
    "end": "937810"
  },
  {
    "text": "So we need to generalize\nour notion of a dot product to other kinds of vector spaces.",
    "start": "937810",
    "end": "943959"
  },
  {
    "text": "So first of all, let me just\ngive you the general definition of a dot product.",
    "start": "943960",
    "end": "949210"
  },
  {
    "text": "So usually, once\nyou generalize it to other kind of vector spaces,\nyou don't call it a dot product anymore.",
    "start": "949210",
    "end": "954459"
  },
  {
    "text": "People call it an inner product. But dot product is a perfectly\ngood colloquial name for that. But the pure mathematicians\nwill look at you",
    "start": "954460",
    "end": "963795"
  },
  {
    "text": "sideways a little bit if\nyou call it a dot product. You call it an inner product. So an inner product,\nor a dot product,",
    "start": "963795",
    "end": "970570"
  },
  {
    "text": "is some rule that takes\ntwo vectors in our space. Call it x and y.",
    "start": "970570",
    "end": "976240"
  },
  {
    "text": "And again, this is some\ngeneral vector space. So it could be column vectors. But it could also be\nmatrices or something",
    "start": "976240",
    "end": "982360"
  },
  {
    "text": "even more complicated. And it's a rule that gives\nyou a scalar out of them.",
    "start": "982360",
    "end": "987490"
  },
  {
    "text": "And we can denote that in 18.02\nstyle, or freshman physics",
    "start": "987490",
    "end": "995170"
  },
  {
    "text": "style, as x dot y. That's there. More commonly, in pure math,\nthey often use angle brackets.",
    "start": "995170",
    "end": "1004170"
  },
  {
    "text": "So they would denote this\nthing by angle x, comma, y. I just wrote it here.",
    "start": "1004170",
    "end": "1010980"
  },
  {
    "text": "In quantum mechanics\nand physics, they like this bracket notation,\nwhere they use angle brackets,",
    "start": "1010980",
    "end": "1016290"
  },
  {
    "text": "but they put a vertical\nbar in between. And there's some\ncomplications there. They really call the\nvectors, they call them x,",
    "start": "1016290",
    "end": "1024000"
  },
  {
    "text": "with brackets around them. But I'm not going to\nworry about that too much. But you may have seen,\nthis is an inner product.",
    "start": "1024000",
    "end": "1030790"
  },
  {
    "text": "This is an inner product. This is an inner product. I'm just going to use\ndots in this class, just to keep it kind\nof elementary looking.",
    "start": "1030790",
    "end": "1039589"
  },
  {
    "text": "So it's a rule that\ntakes two vectors and gives you a scalar out. And right now, we're dealing\nwith just real vector spaces.",
    "start": "1039589",
    "end": "1046639"
  },
  {
    "text": "So our scalar is going\nto be real numbers. And it has to have certain\nproperties to be a dot product.",
    "start": "1046640",
    "end": "1052700"
  },
  {
    "text": "Basically, you need\nto obey certain rules, so that you can do the\nfamiliar rules of algebra",
    "start": "1052700",
    "end": "1057980"
  },
  {
    "text": "with it that you do with\nyour familiar dot products. You want it to work, those\nalgebraic rules to work,",
    "start": "1057980",
    "end": "1063230"
  },
  {
    "text": "with your dot products on\nwhatever crazy vector space you have. And it turns out, you really\njust need three rules.",
    "start": "1063230",
    "end": "1069870"
  },
  {
    "text": "So first of all, we\nwant it to be symmetric. So the rule should\nbe that it should satisfy that x dot y equals\ny dot x, for any x and y.",
    "start": "1069870",
    "end": "1078730"
  },
  {
    "text": "If these are complex\nnumbers, then you need a complex\nconjugate over this.",
    "start": "1078730",
    "end": "1084054"
  },
  {
    "text": " But since this is\nreal numbers, I'm",
    "start": "1084055",
    "end": "1091060"
  },
  {
    "text": "only dealing with real numbers,\nI can just say it's symmetric, but just keep in mind, if you\never work with complex numbers,",
    "start": "1091060",
    "end": "1096340"
  },
  {
    "text": "there is a conjugation here. Second, and of course, our\nfamiliar dot product certainly",
    "start": "1096340",
    "end": "1103360"
  },
  {
    "text": "is symmetric, where you multiply\nthe components and add them up. That's symmetric.",
    "start": "1103360",
    "end": "1111280"
  },
  {
    "text": "And then it has to be linear. And so that means\nif, for example, if you take the dot product of\nx with alpha y plus beta z--",
    "start": "1111280",
    "end": "1121560"
  },
  {
    "text": "and as usual, Greek letters\nmean scalars here-- then that has to be the same\nthing as alpha times x dot y plus beta times x dot z.",
    "start": "1121560",
    "end": "1130080"
  },
  {
    "text": "And again, the familiar dot\nproduct is certainly linear. And since it's symmetric,\nthis also means it's linear.",
    "start": "1130080",
    "end": "1138740"
  },
  {
    "text": "I wrote it down here, it is\nlinear in the second argument. But from rule number one, I\nwould swap the second argument",
    "start": "1138740",
    "end": "1147562"
  },
  {
    "text": "for the first argument. It means it's also linear\nin the first argument. ",
    "start": "1147562",
    "end": "1154170"
  },
  {
    "text": "And the third rule\nthat it has to satisfy if we want to call\nit an inner product is that it has to\nbe non-negative.",
    "start": "1154170",
    "end": "1160559"
  },
  {
    "text": "So if you take the dot product\nof a vector with itself, we're going to call that the\nnorm of the vector squared.",
    "start": "1160560",
    "end": "1167860"
  },
  {
    "text": "So the norm of the vector\nis the square root of this. So x dot itself is norm squared.",
    "start": "1167860",
    "end": "1173460"
  },
  {
    "text": "Just like the\nfamiliar dot product, the dot product with\nitself is the length of the vector squared. That length had better\nnot be a negative number.",
    "start": "1173460",
    "end": "1183390"
  },
  {
    "text": "We're going to require\nthat anything, if it wants to call itself an\ninner product, x dot itself has to\nbe non-negative.",
    "start": "1183390",
    "end": "1190230"
  },
  {
    "text": "And furthermore, it can\nonly equal if and only",
    "start": "1190230",
    "end": "1195330"
  },
  {
    "text": "if x equals 0. So the 0 vector-- every vector space\nmust have a 0 vector.",
    "start": "1195330",
    "end": "1201230"
  },
  {
    "text": "And the 0 vector, its\nlength had better be 0. And it better be the only\nvector that has a length of 0.",
    "start": "1201230",
    "end": "1207880"
  },
  {
    "text": "And if you have\nthese three rules, then you can call\nit an inner product. And any questions\non these rules yet?",
    "start": "1207880",
    "end": "1214820"
  },
  {
    "text": "And I'm going to give\nsome examples in a second. ",
    "start": "1214820",
    "end": "1220190"
  },
  {
    "text": "So this is an inner product. And so now, just\nfor terminology, I'm not going to use\nthis terminology much,",
    "start": "1220190",
    "end": "1225780"
  },
  {
    "text": "but if you have a\ncontinuous vector space-- you know what a vector space is.",
    "start": "1225780",
    "end": "1231530"
  },
  {
    "text": "So if you add an inner product\nand it's a continuous vector space, so it's the\nreal numbers and you can vary the vector\nsmoothly, people",
    "start": "1231530",
    "end": "1238429"
  },
  {
    "text": "call this a Hilbert space. So if you ever hear that\nterm, don't get scared. It just means you\nhave a vector space",
    "start": "1238430",
    "end": "1243602"
  },
  {
    "text": "and you have a dot product. And so now we can\ndefine a gradient. So if you have a\nscalar function that",
    "start": "1243602",
    "end": "1251420"
  },
  {
    "text": "takes a vector in and\ngives you a scalar out, but vector space\nhas a dot product,",
    "start": "1251420",
    "end": "1258290"
  },
  {
    "text": "so it's a Hilbert space,\nthen the derivative",
    "start": "1258290",
    "end": "1264270"
  },
  {
    "text": "must be a linear function that\ntakes a vector in and gives you a scalar out.",
    "start": "1264270",
    "end": "1270410"
  },
  {
    "text": "And it turns out that if\nyou're in a vector space and you have a dot product, and\nyou have a linear function that",
    "start": "1270410",
    "end": "1279250"
  },
  {
    "text": "takes-- it's called a linear\nform that takes a vector in and a vector out, every linear\nform must be of the form",
    "start": "1279250",
    "end": "1287020"
  },
  {
    "text": "the dot product of\nsome vector with the x. This is always true. So this is certainly\ntrue for column vectors.",
    "start": "1287020",
    "end": "1293410"
  },
  {
    "text": "For a column vector, if\nit takes a column vector and gives you a\nnumber out, it must be",
    "start": "1293410",
    "end": "1299312"
  },
  {
    "text": "a dot product with something. It turns out this is true in\ngeneral for any Hilbert space, any vector space with\nan inner product.",
    "start": "1299312",
    "end": "1304810"
  },
  {
    "text": "If you have a linear function,\nvector in, number out, it must be a dot\nproduct with something.",
    "start": "1304810",
    "end": "1310059"
  },
  {
    "text": "This is called the Riesz\nrepresentation theorem, by the way.",
    "start": "1310060",
    "end": "1315490"
  },
  {
    "text": "I feel like I\nspelled Riesz wrong. I think it's C-Z, like this.",
    "start": "1315490",
    "end": "1321700"
  },
  {
    "text": "Yes, I think it's C-Z. It's\na Riesz representation--",
    "start": "1321700",
    "end": "1328240"
  },
  {
    "text": "I'll double check. Or is it Z-C? ",
    "start": "1328240",
    "end": "1333900"
  },
  {
    "text": "No. I think it's C-Z,\none of those names. I can never remember it.",
    "start": "1333900",
    "end": "1339220"
  },
  {
    "text": "Yeah, so this is the theroem\ncalled the Riesz representation theorem. But it's kind of\nan intuitive thing.",
    "start": "1339220",
    "end": "1345010"
  },
  {
    "text": "If you think of column\nvectors, it's very intuitive. If you have a linear rule,\na column vector to a number, it must be a dot product.",
    "start": "1345010",
    "end": "1350672"
  },
  {
    "text": "It turns out this\nis always true. This must be a dot\nproduct with something. And we call that\nthing the gradient.",
    "start": "1350672",
    "end": "1359170"
  },
  {
    "text": "So the gradient for\nany vector space, any scalar function\nof any vector space",
    "start": "1359170",
    "end": "1364450"
  },
  {
    "text": "with an inner\nproduct, is the thing you take the dot product\nwith to get your df.",
    "start": "1364450",
    "end": "1370720"
  },
  {
    "text": "So this is our df.",
    "start": "1370720",
    "end": "1377095"
  },
  {
    "text": " And so it's always\nsomething that has kind of the same shape as x.",
    "start": "1377095",
    "end": "1385450"
  },
  {
    "text": "So let me just do some examples. So of course, the example,\nthe familiar example,",
    "start": "1385450",
    "end": "1391630"
  },
  {
    "text": "is just our vector\nspace is just our n.",
    "start": "1391630",
    "end": "1397240"
  },
  {
    "text": "So it's n component\ncolumn vectors. ",
    "start": "1397240",
    "end": "1405030"
  },
  {
    "text": "And then our dot product is\njust x dot y is x transpose y.",
    "start": "1405030",
    "end": "1414180"
  },
  {
    "text": "That's our familiar dot product.  Well, I shouldn't say therefore.",
    "start": "1414180",
    "end": "1423020"
  },
  {
    "text": " this is the familiar\nEuclidean dot product.",
    "start": "1423020",
    "end": "1429208"
  },
  {
    "text": "It's not the only\npossible dot product--  Euclidean.",
    "start": "1429208",
    "end": "1434440"
  },
  {
    "start": "1434440",
    "end": "1442860"
  },
  {
    "text": "There are actually\nother dot products. You could also have this. So this is-- what is this?",
    "start": "1442860",
    "end": "1450610"
  },
  {
    "text": "This is basically x1y1 plus\nx2y2 plus dot, dot, dot.",
    "start": "1450610",
    "end": "1462520"
  },
  {
    "text": "You just multiply the\ncomponents and add them up. But you could also have,\nfor example, a weighted dot",
    "start": "1462520",
    "end": "1470285"
  },
  {
    "text": "product. ",
    "start": "1470285",
    "end": "1475680"
  },
  {
    "text": "So we have-- let's\ncall it x dot--",
    "start": "1475680",
    "end": "1481115"
  },
  {
    "text": "I need to use a\ndifferent symbol. Let's call it x dot sub w y is--",
    "start": "1481115",
    "end": "1490820"
  },
  {
    "text": "I could do x1y1-- ",
    "start": "1490820",
    "end": "1496730"
  },
  {
    "text": "sorry-- w1x1y1 plus\nw2x2y2 plus dot, dot,",
    "start": "1496730",
    "end": "1505860"
  },
  {
    "text": "dot for weights w1 to wn\nthat all have to be positive.",
    "start": "1505860",
    "end": "1520887"
  },
  {
    "text": "Why do they have to be positive? Because of the non-negativity\nproperty, for us to call this an inner product.",
    "start": "1520887",
    "end": "1527377"
  },
  {
    "text": "Let me put those in black.  If it had property\nproperty 3, you",
    "start": "1527377",
    "end": "1536370"
  },
  {
    "text": "have to have dot\nproducts be non-negative. It means the weights will\nhave to be non-negative.",
    "start": "1536370",
    "end": "1541800"
  },
  {
    "text": "This is a perfectly\ngood inner product. It's not the Euclidean one. It weights each\ndimension differently.",
    "start": "1541800",
    "end": "1546930"
  },
  {
    "text": "But this might be useful\nin statistics or something like that. For example, if you want to\nhave a bunch of measurements,",
    "start": "1546930",
    "end": "1555630"
  },
  {
    "text": "but there's a\ndifferent uncertainty in each measurement, you might\nwant to weight the numbers--",
    "start": "1555630",
    "end": "1561840"
  },
  {
    "text": "the components that\nhave more uncertainty, you might want to\nweight them less. You give them less weight.",
    "start": "1561840",
    "end": "1567960"
  },
  {
    "text": "And the ones that are more\ncertain, give them more weight. Or another example, if\nyou had a vector where",
    "start": "1567960",
    "end": "1575913"
  },
  {
    "text": "the different components\nhave different units, for example, a vector where the\nfirst component is in meters,",
    "start": "1575913",
    "end": "1582149"
  },
  {
    "text": "and the second\ncomponent is in seconds, and the third component\nis in kilograms,",
    "start": "1582150",
    "end": "1588090"
  },
  {
    "text": "then the ordinary\ndot product formula doesn't even make sense. It'll be adding up things\nwith different units.",
    "start": "1588090",
    "end": "1593470"
  },
  {
    "text": "So then you have\nto weight each one. You divide-- the\nfirst weight would",
    "start": "1593470",
    "end": "1598900"
  },
  {
    "text": "be 1 over some characteristic\nlength scale in meters. The second one would be some 1\nover some characteristic length",
    "start": "1598900",
    "end": "1605050"
  },
  {
    "text": "scale in time. The third one would be 1 over\nsome characteristic length scale in kilograms. And that way, each\nterm is dimensionless.",
    "start": "1605050",
    "end": "1611740"
  },
  {
    "text": "You can add them up or\nsomething like that. Another way it's also\nuseful to write this down",
    "start": "1611740",
    "end": "1618330"
  },
  {
    "text": "is linear algebra. So we could also write\nthis down as x transpose y.",
    "start": "1618330",
    "end": "1628950"
  },
  {
    "text": "And what do we put in\nthe middle to get this?",
    "start": "1628950",
    "end": "1634409"
  },
  {
    "text": "Any ideas?  What should it look\nlike in the middle?",
    "start": "1634410",
    "end": "1640549"
  },
  {
    "text": "What can I put here? Let me give you a hint. It's a matrix.",
    "start": "1640550",
    "end": "1645679"
  },
  {
    "text": "What's the entries\nof that matrix? Alex, if someone\nsays an answer, I",
    "start": "1645680",
    "end": "1652460"
  },
  {
    "text": "can't hear anyone in the room. ",
    "start": "1652460",
    "end": "1659130"
  },
  {
    "text": "Alex, is anyone\nraising their hands? AUDIENCE: Yeah. W on the diagonal.",
    "start": "1659130",
    "end": "1664559"
  },
  {
    "text": "Can you hear that? STEVEN JOHNSON: Exactly. I cannot hear\nanything in the room. So you have to speak\ninto the microphone. So that's your job, is to\nrepeat any comments or questions",
    "start": "1664560",
    "end": "1673507"
  },
  {
    "text": "from the microphone. Yes. So it's just a diagonal matrix.",
    "start": "1673507",
    "end": "1682539"
  },
  {
    "text": "And an another example is\nanother weighted dot product.",
    "start": "1682540",
    "end": "1690535"
  },
  {
    "start": "1690535",
    "end": "1699560"
  },
  {
    "text": "Let's call this x-- so this was with--",
    "start": "1699560",
    "end": "1705580"
  },
  {
    "text": "let's call this x dot y sub\ncapital W, for column vectors,",
    "start": "1705580",
    "end": "1715990"
  },
  {
    "text": "would be x transpose y.",
    "start": "1715990",
    "end": "1726370"
  },
  {
    "text": "And you put any matrix in here. But you need it to be\nsymmetric for property one.",
    "start": "1726370",
    "end": "1733840"
  },
  {
    "text": "So we need W to be\nsymmetric for property one.",
    "start": "1733840",
    "end": "1739284"
  },
  {
    "text": " And we need for property\nthree, for positivity, we",
    "start": "1739285",
    "end": "1747090"
  },
  {
    "text": "need W to be positive definite. ",
    "start": "1747090",
    "end": "1757350"
  },
  {
    "text": "And so the one we had before\nis just a special case of that.",
    "start": "1757350",
    "end": "1763390"
  },
  {
    "text": "A diagonal matrix is\ncertainly symmetric. If it has positive\ndiagonals, it's",
    "start": "1763390",
    "end": "1768450"
  },
  {
    "text": "certainly positive definite. But you can have some arbitrary\nthing that mixes them up. And these all are\nuseful in surprisingly--",
    "start": "1768450",
    "end": "1778716"
  },
  {
    "text": "so there's a\nsurprisingly large number of cases in which you\ndon't want to have the traditional\nEuclidean dot product.",
    "start": "1778717",
    "end": "1783840"
  },
  {
    "text": "You want to have something\nmore complicated. And with all of\nthese things, you could then define\ngradients differently.",
    "start": "1783840",
    "end": "1792487"
  },
  {
    "text": "So if you have a\nweighted dot product, it's going to change your\ndefinition of the gradient",
    "start": "1792487",
    "end": "1798059"
  },
  {
    "text": "because it's going to be\nthe thing with that W in it. But most of the time when\nwe're talking about gradients,",
    "start": "1798060",
    "end": "1803520"
  },
  {
    "text": "we're talking\nabout this is the-- you're talking about\nthe familiar one. ",
    "start": "1803520",
    "end": "1811480"
  },
  {
    "text": "The Euclidean product gives\nyou the usual gradient. ",
    "start": "1811480",
    "end": "1820170"
  },
  {
    "text": "But you might need a weighted\ndot product, for example, for grading to make sense if you\nhave a derivative with respect",
    "start": "1820170",
    "end": "1829130"
  },
  {
    "text": "to a bunch of\ndifferent components that have different units. So let's do another example\nthat's even less familiar,",
    "start": "1829130",
    "end": "1836000"
  },
  {
    "text": "though.  Let's say that v is R m by n.",
    "start": "1836000",
    "end": "1848210"
  },
  {
    "text": "let's do m by n. So this is going to\nbe m by n matrices. ",
    "start": "1848210",
    "end": "1858080"
  },
  {
    "text": "Those form a perfectly\ngood vector space. I can take two matrices,\nand I can add them. They have the same shape.",
    "start": "1858080",
    "end": "1863480"
  },
  {
    "text": "I can multiply them by\n2 or some other scalar. I can subtract. So this is fine\nas a vector space.",
    "start": "1863480",
    "end": "1871130"
  },
  {
    "text": "And in fact, using that vec\noperator that Alan talked about in the last lecture,\nyou could convert it",
    "start": "1871130",
    "end": "1877130"
  },
  {
    "text": "into column vectors-- you can stack them up into\ncolumn vectors with m times n components. But I want to leave\nthem as matrices.",
    "start": "1877130",
    "end": "1883670"
  },
  {
    "text": " But yeah, let me mention\nthat because this has come up in a second.",
    "start": "1883670",
    "end": "1890059"
  },
  {
    "text": "So there's an\nisomorphism with this vec",
    "start": "1890060",
    "end": "1898440"
  },
  {
    "text": "function to column vectors. ",
    "start": "1898440",
    "end": "1907799"
  },
  {
    "text": "Vec, vecA that live in R m\ntimes n because this is really",
    "start": "1907800",
    "end": "1921830"
  },
  {
    "text": "an mn dimensional space. You could just take all\nthe entries of the matrix and stack them up.",
    "start": "1921830",
    "end": "1928310"
  },
  {
    "text": "That's what vec does, if you\nremember from last lecture. This just stacks the columns. ",
    "start": "1928310",
    "end": "1940500"
  },
  {
    "text": "And I don't know if people\nknow the term isomorphism. How many people know\nwhat this means?",
    "start": "1940500",
    "end": "1949580"
  },
  {
    "text": "I don't know if people are\nraising their hands or not. Alex, are people\nraising their hands. AUDIENCE: Five people.",
    "start": "1949580",
    "end": "1955460"
  },
  {
    "text": "STEVEN JOHNSON: Yeah. So it basically means that\nif you add two matrices, it's equivalent to adding\nthese two vectors because--",
    "start": "1955460",
    "end": "1961550"
  },
  {
    "text": "or multiply by 2. It's multiplying\nthese vectors by 2 because these vectors are\njust the same component",
    "start": "1961550",
    "end": "1966680"
  },
  {
    "text": "entries of A rearranged. So adding matrices is\nadding the entries. Adding these vectors\nis adding the entries.",
    "start": "1966680",
    "end": "1972410"
  },
  {
    "text": "Multiplying by 2 is\nmultiplying the entries by 2. Multiplying this vector by 2 is\nmultiplying the entries by 2.",
    "start": "1972410",
    "end": "1977670"
  },
  {
    "text": "So it's the same thing. But anyways, and\nsometimes it's useful.",
    "start": "1977670",
    "end": "1984305"
  },
  {
    "text": " It's nice to stick with--\nif your problem is expressed",
    "start": "1984305",
    "end": "1990800"
  },
  {
    "text": "in terms of\nmatrices, usually you want to keep in terms\nof matrices if possible. That's the natural\nway of looking",
    "start": "1990800",
    "end": "1996950"
  },
  {
    "text": "at your degree of freedom\nis an n by n matrix, whereas stacking it up into a\ncolumn vector of mn components",
    "start": "1996950",
    "end": "2003019"
  },
  {
    "text": "is kind of an unnatural way. Internally, the\ncomputer does that. But sometimes it\nhelps you disentangle",
    "start": "2003020",
    "end": "2010820"
  },
  {
    "text": "what's going on if\nyou stack things up into a more familiar vector. ",
    "start": "2010820",
    "end": "2017090"
  },
  {
    "text": "What we'd like to do is define\na dot product of matrices. And again, just\nlike for vectors, there'll be multiple choices.",
    "start": "2017090",
    "end": "2023240"
  },
  {
    "text": "There's never one dot\nproduct for a vector space. There's multiple\npossible choices. And you choose one.",
    "start": "2023240",
    "end": "2029059"
  },
  {
    "text": "And then everything\nelse follows from that. The geometry follows. So I just want to show\nyou the most obvious one,",
    "start": "2029060",
    "end": "2047770"
  },
  {
    "text": "the analog of the familiar\nEuclidean inner product.",
    "start": "2047770",
    "end": "2056503"
  },
  {
    "text": " So what would that be?",
    "start": "2056504",
    "end": "2063339"
  },
  {
    "text": "So if I have two matrices, A\nand B, where this is m by n,",
    "start": "2063340",
    "end": "2070500"
  },
  {
    "text": "and this is m by n, And\nthis is our dot product. This is not this is\nnot a matrix product.",
    "start": "2070500",
    "end": "2075570"
  },
  {
    "text": "This is our inner product. ",
    "start": "2075570",
    "end": "2082069"
  },
  {
    "text": "So we want to give\nthis to be a scalar. ",
    "start": "2082070",
    "end": "2093580"
  },
  {
    "text": "what should I do? Any suggestions?  Alex, you'll need to repeat.",
    "start": "2093580",
    "end": "2100750"
  },
  {
    "text": "I can't-- AUDIENCE: Yeah. This one, multiply by all the\ncomponents, like elementwise.",
    "start": "2100750",
    "end": "2110280"
  },
  {
    "text": "STEVEN JOHNSON:\nExactly, just multiply the elements, the entries of\nthese matrices elementwise,",
    "start": "2110280",
    "end": "2118620"
  },
  {
    "text": "and add them up. So this is the sum of\nthe elementwise products.",
    "start": "2118620",
    "end": "2124230"
  },
  {
    "start": "2124230",
    "end": "2130609"
  },
  {
    "text": "So that's it's a\nlittle bit annoying to write that out in words. So let's see if\nwe can figure out a better way to algebraically\nto write that down.",
    "start": "2130610",
    "end": "2137150"
  },
  {
    "text": "So in Julia, you\ncould write this down. ",
    "start": "2137150",
    "end": "2142760"
  },
  {
    "text": "The one way to do this is to say\nsum of A dot star B. Dot star",
    "start": "2142760",
    "end": "2160850"
  },
  {
    "text": "is an elementwise product\nof two vectors and matrices. And sum just sums then\nsums all the entries.",
    "start": "2160850",
    "end": "2167000"
  },
  {
    "text": "That's what this is. ",
    "start": "2167000",
    "end": "2172520"
  },
  {
    "text": "I could write it as sum\nover i and j of Aij Bij.",
    "start": "2172520",
    "end": "2182075"
  },
  {
    "start": "2182075",
    "end": "2187359"
  },
  {
    "text": "I could write it with vex. It's the same thing as if\nI take the elements of A",
    "start": "2187360",
    "end": "2195039"
  },
  {
    "text": "and I stack them\nup into a vector, and I take the elements of B and\nI stack them up into a vector.",
    "start": "2195040",
    "end": "2201924"
  },
  {
    "start": "2201925",
    "end": "2207820"
  },
  {
    "text": "This is just the ordinary\ndot product of those. VecA is just the same numbers.",
    "start": "2207820",
    "end": "2214280"
  },
  {
    "text": "It's just stacked up. So if I take the ordinary\ndot product of vecA and vecB, that is this\nelementwise product.",
    "start": "2214280",
    "end": "2219910"
  },
  {
    "text": "But again, that that's\na little bit frustrating because if someone hands\nme this vector space,",
    "start": "2219910",
    "end": "2225670"
  },
  {
    "text": "it's probably because\nthey really want to leave things as matrices. That's the natural\nlanguage of the problem.",
    "start": "2225670",
    "end": "2232340"
  },
  {
    "text": "So I'd rather use this--\nthis vec notation is kind of a last resort. If you can't figure out how\nto deal with it as a matrix,",
    "start": "2232340",
    "end": "2240190"
  },
  {
    "text": "you cram it into a vector and\nthen go back to what you know.",
    "start": "2240190",
    "end": "2245920"
  },
  {
    "text": "So it turns out that\nthere's a really nice way to express this\nin linear algebra,",
    "start": "2245920",
    "end": "2253780"
  },
  {
    "text": "using linear algebra operations. And that turns out to be\nthe trace of A transpose B.",
    "start": "2253780",
    "end": "2265570"
  },
  {
    "text": "I don't know if you want to if\nyou want to see that derived. It's actually pretty\nstraightforward to derive this.",
    "start": "2265570",
    "end": "2273690"
  },
  {
    "text": "So this is also this is this\nis also called the Frobenius",
    "start": "2273690",
    "end": "2279119"
  },
  {
    "text": "inner product of two matrices.",
    "start": "2279120",
    "end": "2292680"
  },
  {
    "text": "So if you just write out the\nformula for trace and matrix multiplication, you\ncan actually see that.",
    "start": "2292680",
    "end": "2298490"
  },
  {
    "text": "Another way of seeing\nthat is basically, what is A transpose B? So A times B is take rows\nof a times dot product",
    "start": "2298490",
    "end": "2306560"
  },
  {
    "text": "with columns of B. But if you do\nA transpose of B, A transpose,",
    "start": "2306560",
    "end": "2313010"
  },
  {
    "text": "its rows are the columns of\nA. So what this really is,",
    "start": "2313010",
    "end": "2318110"
  },
  {
    "text": "rows times columns\nhere, is you're taking the dot product\nof each column of A",
    "start": "2318110",
    "end": "2323569"
  },
  {
    "text": "with each column of B. And the trace is the\nsum of the diagonals.",
    "start": "2323570",
    "end": "2329119"
  },
  {
    "text": "So what's the first\ndiagonal entry of this? The first diagonal entry\nis column 1 of A dot column",
    "start": "2329120",
    "end": "2337970"
  },
  {
    "text": "1 of B. The second diagonal\nentry is column 2 of A dot",
    "start": "2337970",
    "end": "2343170"
  },
  {
    "text": "column 2 of B and so forth. And if you add those up, you're\nadding up all the dot product",
    "start": "2343170",
    "end": "2350170"
  },
  {
    "text": "of each column of A with\nthe corresponding column of B. That's exactly this. It's the sum of the\nproduct of all the entries.",
    "start": "2350170",
    "end": "2358600"
  },
  {
    "text": "So this is a really useful\nthing to know about, that you can take a dot\nproduct of two matrices.",
    "start": "2358600",
    "end": "2364960"
  },
  {
    "text": "The obvious inner\nproduct is really the trace of A transpose\nB. And this then",
    "start": "2364960",
    "end": "2370510"
  },
  {
    "text": "gives you the Frobenius\nnorm, which I mentioned",
    "start": "2370510",
    "end": "2379290"
  },
  {
    "text": "the other day, of matrices. So the norm of a\nmatrix of anything,",
    "start": "2379290",
    "end": "2389400"
  },
  {
    "text": "we want it to be the square root\nof the dot product with itself. ",
    "start": "2389400",
    "end": "2397430"
  },
  {
    "text": "And what this is is\nexactly the square root",
    "start": "2397430",
    "end": "2403420"
  },
  {
    "text": "of the trace of A transpose A.",
    "start": "2403420",
    "end": "2413079"
  },
  {
    "text": "So it's really useful. It's kind of the obvious\nEuclidean norm of this.",
    "start": "2413080",
    "end": "2419320"
  },
  {
    "text": "So this is exactly\nthe same thing as the Euclidean norm\nof all the components,",
    "start": "2419320",
    "end": "2434319"
  },
  {
    "text": "if we take and vecA and\ntake it, to just stack up",
    "start": "2434320",
    "end": "2439340"
  },
  {
    "text": "the components, it's that,\nor equivalently, it's the square root of the sum of--",
    "start": "2439340",
    "end": "2446030"
  },
  {
    "text": "if you just take all the\nentries of A and just",
    "start": "2446030",
    "end": "2451867"
  },
  {
    "text": "take the square root of\nthe sum of the squares. ",
    "start": "2451867",
    "end": "2461530"
  },
  {
    "text": "This is the most obvious way to\nmeasure the size of a matrix.",
    "start": "2461530",
    "end": "2467800"
  },
  {
    "text": "And so now, using\nthis, we can now",
    "start": "2467800",
    "end": "2475570"
  },
  {
    "text": "take the gradient of scalar\nfunctions of matrix inputs.",
    "start": "2475570",
    "end": "2494765"
  },
  {
    "start": "2494765",
    "end": "2504200"
  },
  {
    "text": "So for example, let's\nstart with the-- let's do some examples.",
    "start": "2504200",
    "end": "2509690"
  },
  {
    "text": "Let's take some derivatives\nof matrix of scalar functions of matrices.",
    "start": "2509690",
    "end": "2515680"
  },
  {
    "text": "So let's do f of\nA, where this is",
    "start": "2515680",
    "end": "2523220"
  },
  {
    "text": "going to be an m by n matrix. What's a scalar function?",
    "start": "2523220",
    "end": "2528740"
  },
  {
    "text": "The simplest one\nis maybe the one we just learned a second ago. ",
    "start": "2528740",
    "end": "2537710"
  },
  {
    "text": "Let's do that. Let's take that Frobenius norm.",
    "start": "2537710",
    "end": "2543000"
  },
  {
    "text": "This is the sometimes denoted-- sometimes people put an F here.",
    "start": "2543000",
    "end": "2549060"
  },
  {
    "text": "I'm just going to leave\nout the F, but also-- let me just note\nthat in because there",
    "start": "2549060",
    "end": "2555010"
  },
  {
    "text": "are multiple ways of\ndenoting the norm of matrix, just like there are\nmultiple inner products.",
    "start": "2555010",
    "end": "2562320"
  },
  {
    "text": "So when people want\nto be specific, they usually put an F there. But I'm just going\nto-- well, I guess",
    "start": "2562320",
    "end": "2568020"
  },
  {
    "text": "I can leave this here right,\njust so we know what it is. ",
    "start": "2568020",
    "end": "2575170"
  },
  {
    "text": "So this is the square root of\nthe trace of A transpose A.",
    "start": "2575170",
    "end": "2590836"
  },
  {
    "text": "And now, I want to compute\na derivative of this. So first, before I worry\nabout the gradient,",
    "start": "2590836",
    "end": "2596520"
  },
  {
    "text": "let's just make sure we\ncan compute a derivative. We better be able\nto take what is df and use linear algebra,\nlinear algebra.",
    "start": "2596520",
    "end": "2605400"
  },
  {
    "text": "So how do we get df?",
    "start": "2605400",
    "end": "2610859"
  },
  {
    "text": "So I need to use the chain rule. ",
    "start": "2610860",
    "end": "2620330"
  },
  {
    "text": "I want to use the chain\nrule, just to simplify life, so we don't have to rederive\neverything from scratch.",
    "start": "2620330",
    "end": "2625650"
  },
  {
    "text": "And so first of all, we're\napplying the chain rule to the square root function. But it's the square\nroot of a number.",
    "start": "2625650",
    "end": "2632570"
  },
  {
    "text": "Once you take the trace,\nthis is just a number. And it's the square\nroot of a number. So we can use the ordinary--\nonce you have scalar functions,",
    "start": "2632570",
    "end": "2640400"
  },
  {
    "text": "you can use all your ordinary\n1801 first year calculus rules.",
    "start": "2640400",
    "end": "2646307"
  },
  {
    "text": "What we're doing is not\ndifferent from that. It's just a\ngeneralization of that.",
    "start": "2646307",
    "end": "2651900"
  },
  {
    "text": "So you can really just use the\n1801 chain rule to start with.",
    "start": "2651900",
    "end": "2657950"
  },
  {
    "text": "So the derivative of a\nsquare root is 1 over 2,",
    "start": "2657950",
    "end": "2666440"
  },
  {
    "text": "the square root of\nthat thing, of trace of A transpose A times d of what\nyou took the square root of,",
    "start": "2666440",
    "end": "2680010"
  },
  {
    "text": "of trace of A transpose A. This\nis just the 1801 chain rule.",
    "start": "2680010",
    "end": "2689415"
  },
  {
    "text": "So we don't want to throw\naway everything we learned. So once we hit scalars, we\ncan use our familiar rules.",
    "start": "2689415",
    "end": "2696910"
  },
  {
    "text": "What we're doing is exactly\nequivalent to that right. So this is just the ordinary\nchain rule for square roots.",
    "start": "2696910",
    "end": "2705325"
  },
  {
    "text": "It's the derivative, or the\nd in this-- the differential, in this case, of whatever\nis inside the square root",
    "start": "2705325",
    "end": "2711070"
  },
  {
    "text": "times 1/2 that thing\nto the minus 1/2 power. ",
    "start": "2711070",
    "end": "2717960"
  },
  {
    "text": "So let's see if\nwe can go further. So now, so I want to do a--",
    "start": "2717960",
    "end": "2729910"
  },
  {
    "text": "so what's the d of the\ntrace, of the trace",
    "start": "2729910",
    "end": "2738849"
  },
  {
    "text": "of a function of,\nsay, B. So this is trace of B plus\ndB minus trace of B.",
    "start": "2738850",
    "end": "2757460"
  },
  {
    "text": "But the trace is linear. This is the same. The trace of B plus--",
    "start": "2757460",
    "end": "2763710"
  },
  {
    "text": "the trace of the\nsum of two matrices is the same thing as\nthe sum of the traces",
    "start": "2763710",
    "end": "2769020"
  },
  {
    "text": "because if you think\nabout what a trace means, it's just the sum of\nthe diagonal entries.",
    "start": "2769020",
    "end": "2774730"
  },
  {
    "text": "So if I add up two matrices,\ntheir diagonal entries just adds. That means their\ntrace is just add.",
    "start": "2774730",
    "end": "2780030"
  },
  {
    "text": " So this is just trace dB.",
    "start": "2780030",
    "end": "2786230"
  },
  {
    "text": "So this is very closely related\nto problem 1 on the homework.",
    "start": "2786230",
    "end": "2791670"
  },
  {
    "text": "So this thing is-- what do we have? We have 1 over 2. That square root is really just\nour norm of A in the front,",
    "start": "2791670",
    "end": "2800250"
  },
  {
    "text": "so I'll just\nsimplify that there. And then we have the trace.",
    "start": "2800250",
    "end": "2807140"
  },
  {
    "text": "The d of the trace\nis the same thing as the trace of d\nof A transpose A.",
    "start": "2807140",
    "end": "2821550"
  },
  {
    "text": "And now, we can use\nthe product rule. ",
    "start": "2821550",
    "end": "2830630"
  },
  {
    "text": "So this is 1 over 2 norm\nof A with a trace of--",
    "start": "2830630",
    "end": "2842619"
  },
  {
    "text": "and what do we have? We have dA-- that\nneeds to be written--",
    "start": "2842620",
    "end": "2849819"
  },
  {
    "text": "dA transpose A plus\nA transpose dA.",
    "start": "2849820",
    "end": "2859185"
  },
  {
    "start": "2859185",
    "end": "2865790"
  },
  {
    "text": "And now, I want to\nsimplify this if I can, to try and move all\nthe dA's to the right.",
    "start": "2865790",
    "end": "2871619"
  },
  {
    "text": "So any questions so far? Again, Alex, if you\nhave any questions,",
    "start": "2871620",
    "end": "2876880"
  },
  {
    "text": "you have to repeat because I\ncan't hear anyone in the room. AUDIENCE: No questions.",
    "start": "2876880",
    "end": "2881940"
  },
  {
    "text": "STEVEN JOHNSON: Yeah? AUDIENCE: No questions. STEVEN JOHNSON:\nNo questions, yes. ",
    "start": "2881940",
    "end": "2889010"
  },
  {
    "text": "So again, I want to move--",
    "start": "2889010",
    "end": "2897170"
  },
  {
    "text": "again, we're going\nto use linearity. So this whole thing\nis equal to the trace",
    "start": "2897170",
    "end": "2905750"
  },
  {
    "text": "of dA transpose A plus the\ntrace of A transpose dA.",
    "start": "2905750",
    "end": "2916070"
  },
  {
    "text": " I'm going to try and make\nit look like a dot product",
    "start": "2916070",
    "end": "2921160"
  },
  {
    "text": "with A, so I want to\ngroup my terms with dA so all the terms, the dA terms,\nare going to be on the right.",
    "start": "2921160",
    "end": "2927349"
  },
  {
    "text": "So one of the things\nI can use is--",
    "start": "2927350",
    "end": "2932630"
  },
  {
    "text": "so dA transpose A is not the\nsame thing as A transpose dA.",
    "start": "2932630",
    "end": "2943099"
  },
  {
    "text": "But its trace is. So note, over on\nthe right here, I'm",
    "start": "2943100",
    "end": "2949690"
  },
  {
    "text": "just keeping a running\nnote of my properties. That was my note one.",
    "start": "2949690",
    "end": "2958680"
  },
  {
    "text": "Note two, if I have\nthe trace of B, that's the same thing as\nthe trace of B transpose.",
    "start": "2958680",
    "end": "2966750"
  },
  {
    "text": "If I transpose a\nmatrix, it doesn't change the diagonal entries. So this thing here\nis the same thing",
    "start": "2966750",
    "end": "2974070"
  },
  {
    "text": "as the trace of A transpose dA. ",
    "start": "2974070",
    "end": "2980380"
  },
  {
    "text": "The matrix inside is different. But its trace is the same. B is not equal to B\ntranspose, but trace B",
    "start": "2980380",
    "end": "2986110"
  },
  {
    "text": "equals trace of B transpose. So that, of course,\nis the same as this.",
    "start": "2986110",
    "end": "2992330"
  },
  {
    "text": "So I just have two of these. It cancels the two out there.",
    "start": "2992330",
    "end": "2997380"
  },
  {
    "text": "And what you're left with is\n1 over the norm of A times",
    "start": "2997380",
    "end": "3006069"
  },
  {
    "text": "the trace of A transpose dA.",
    "start": "3006070",
    "end": "3012160"
  },
  {
    "start": "3012160",
    "end": "3017380"
  },
  {
    "text": "So this is quite\na simple formula. The 2's cancel.",
    "start": "3017380",
    "end": "3022900"
  },
  {
    "text": "And so what's that? That, I claim, is\nthe dot product--",
    "start": "3022900",
    "end": "3029030"
  },
  {
    "text": "if we just go back\nto our formula, trace of A transpose B is\na dot product of A and B.",
    "start": "3029030",
    "end": "3037855"
  },
  {
    "text": "And so this trace\nhere is exactly the dot product of A with dA.",
    "start": "3037855",
    "end": "3051790"
  },
  {
    "text": " But then have a scale factor.",
    "start": "3051790",
    "end": "3057480"
  },
  {
    "text": "But dot products are linear,\nso I can bring that inside. Let me move this\nup a little bit. ",
    "start": "3057480",
    "end": "3069372"
  },
  {
    "text": "Does everyone see this? So this is, according to my\ndefinition of the dot product, this trace here is exactly the\ndot product of A over norm A,",
    "start": "3069372",
    "end": "3076640"
  },
  {
    "text": "like the unit vector in the\ndirection of, in some sense, with dA. ",
    "start": "3076640",
    "end": "3083862"
  },
  {
    "text": "And so this term here\nis exactly my gradient.",
    "start": "3083862",
    "end": "3095315"
  },
  {
    "text": " This is my nabla\ngradient of norm of A",
    "start": "3095315",
    "end": "3110460"
  },
  {
    "text": "is exactly A over norm of A.",
    "start": "3110460",
    "end": "3122945"
  },
  {
    "text": "So I've just taken the\ngradient of a normal matrix. And if you think of a vector-- ",
    "start": "3122945",
    "end": "3129200"
  },
  {
    "text": "actually, Professor\nEdelman derived this for if you took the\ngradient of norm of x,",
    "start": "3129200",
    "end": "3135470"
  },
  {
    "text": "that was x over norm of x. You derived that last time. This is actually literally\nthe same formula,",
    "start": "3135470",
    "end": "3143819"
  },
  {
    "text": "which makes sense, because\nthis Frobenius norm is really, in spirit, it's the same\nthing as your familiar norm",
    "start": "3143820",
    "end": "3150767"
  },
  {
    "text": "of a vector, just the\nsquare root of the sum of the squared components. So when you take\nthe gradient, if you",
    "start": "3150767",
    "end": "3158240"
  },
  {
    "text": "think of the gradient\nas the derivative of this vector,\neach component of A,",
    "start": "3158240",
    "end": "3163505"
  },
  {
    "text": "you get exactly the same thing. And in fact, I\nshould mention that.",
    "start": "3163505",
    "end": "3168530"
  },
  {
    "text": "So in fact, in\n18.02 terms, if you",
    "start": "3168530",
    "end": "3178180"
  },
  {
    "text": "have a function, the gradient-- if you have a scalar function\nof a matrix for a scalar",
    "start": "3178180",
    "end": "3201610"
  },
  {
    "text": "f of a matrix, this\nis exactly going",
    "start": "3201610",
    "end": "3208640"
  },
  {
    "text": "to be the matrix of\npartial f partial A.",
    "start": "3208640",
    "end": "3218670"
  },
  {
    "text": "GUEST SPEAKER: Steven,\njust giving a quick hello to you and the class. STEVEN JOHNSON: You're here. Great. GUEST SPEAKER: Yeah, I\njust finished my talk.",
    "start": "3218670",
    "end": "3224915"
  },
  {
    "text": "I was just giving a talk on\nJulia and high performance computing. It was a Dell talk.",
    "start": "3224915",
    "end": "3230450"
  },
  {
    "text": "So anyway, yeah, I'm here. But let me not let\nme not interrupt you any more than I just have.",
    "start": "3230450",
    "end": "3236370"
  },
  {
    "text": "Is the technology all working? Maybe I'll just ask that. STEVEN JOHNSON: Yep. GUEST SPEAKER: Good. ",
    "start": "3236370",
    "end": "3242480"
  },
  {
    "text": "STEVEN JOHNSON: So it's\nexactly just the matrix of partial derivatives. So if we wanted it to do\nit elementwise, we could.",
    "start": "3242480",
    "end": "3248358"
  },
  {
    "text": "We could take the partial\nderivative with respect to A11, partial\nderivatives with respect to A12, partial\nderivative with respect",
    "start": "3248358",
    "end": "3256160"
  },
  {
    "text": "to A21, partial derivative\nof A22 and so forth.",
    "start": "3256160",
    "end": "3262490"
  },
  {
    "text": "Of course, this is really\nbecomes really awkward. ",
    "start": "3262490",
    "end": "3269099"
  },
  {
    "text": "But this is for\nmatrix value function. But this is exactly\nequivalent to saying that df is this inner\nproduct, grad f dot dA,",
    "start": "3269100",
    "end": "3284410"
  },
  {
    "text": "which is equal to the trace\nof grad f transposed dA",
    "start": "3284410",
    "end": "3296789"
  },
  {
    "text": "because our inner product really\nis the elementwise product of the components.",
    "start": "3296790",
    "end": "3301890"
  },
  {
    "text": "And so this gradient is what you\nexpect a matrix gradient to be.",
    "start": "3301890",
    "end": "3307539"
  },
  {
    "text": "But as I said, it's\njust really annoying to work with in this form. So maybe I'll do one more.",
    "start": "3307540",
    "end": "3315869"
  },
  {
    "text": "So maybe I'll mention next time,\nor soon, maybe not next time.",
    "start": "3315870",
    "end": "3322140"
  },
  {
    "text": "I don't know. Soon, we'll also do the\ngradient of the determinant of A",
    "start": "3322140",
    "end": "3331990"
  },
  {
    "text": "for an m by m matrix. And it turns out this is\nnot so easy to derive.",
    "start": "3331990",
    "end": "3341175"
  },
  {
    "text": "And you definitely\nwould not want to do this component\nby component. That's a really\nawkward way to do this.",
    "start": "3341175",
    "end": "3346760"
  },
  {
    "text": "It turns out this is exactly\nequal to determinant of A times",
    "start": "3346760",
    "end": "3356960"
  },
  {
    "text": "A inverse transposed. ",
    "start": "3356960",
    "end": "3364500"
  },
  {
    "text": "And let me do a simpler example. ",
    "start": "3364500",
    "end": "3371970"
  },
  {
    "text": "It also illustrates a\nnice property of a trace. Let's do f-- and then\nwe'll take a break.",
    "start": "3371970",
    "end": "3379360"
  },
  {
    "text": "So let's do f of A\nis x transpose A y.",
    "start": "3379360",
    "end": "3389400"
  },
  {
    "text": "So this A is going to be m by n. And this is for some constant\nx in R something and y",
    "start": "3389400",
    "end": "3405100"
  },
  {
    "text": "in R something. So how many components\ndoes x have? ",
    "start": "3405100",
    "end": "3411950"
  },
  {
    "text": "Alex, are people\nshouting out anything. ",
    "start": "3411950",
    "end": "3419400"
  },
  {
    "text": "I can't hear anything. AUDIENCE: No. No one said anything. ",
    "start": "3419400",
    "end": "3425460"
  },
  {
    "text": "STEVEN JOHNSON: This, I\nwanted to give a number. Just think in the shapes. This is n by n.",
    "start": "3425460",
    "end": "3431840"
  },
  {
    "text": "Just for this product\nto even makes sense, how many components\ndoes x have to have?",
    "start": "3431840",
    "end": "3437452"
  },
  {
    "text": "AUDIENCE: M? STEVEN JOHNSON: M. And how\nmany components does y have?",
    "start": "3437452",
    "end": "3444230"
  },
  {
    "text": "AUDIENCE: N. STEVEN JOHNSON: N, yes.  So if we just take a\nmatrix and sandwich it",
    "start": "3444230",
    "end": "3450850"
  },
  {
    "text": "in between two\nvectors of the right-- but they have to match\nthe number of rows, the number of columns,\nthen you get a scalar out.",
    "start": "3450850",
    "end": "3457610"
  },
  {
    "text": "And so then what's df? This one is even easier. So df is just x transpose.",
    "start": "3457610",
    "end": "3465750"
  },
  {
    "text": "Everything is a constant. So I'm just going to use the-- you can go to the product rule.",
    "start": "3465750",
    "end": "3471150"
  },
  {
    "text": "But there's only one. So technically, this\nis the product rule.",
    "start": "3471150",
    "end": "3476220"
  },
  {
    "text": "But dx is 0. dy is 0. Those are constant. So this is just this.",
    "start": "3476220",
    "end": "3483859"
  },
  {
    "text": "But this is a\nfunction that takes in a vector, in this case,\na vector space, a matrix,",
    "start": "3483860",
    "end": "3490760"
  },
  {
    "text": "and gives you a scalar. So I should be able to\nexpress this as a dot product. ",
    "start": "3490760",
    "end": "3497920"
  },
  {
    "text": "So I want to express\nthis as a trace. ",
    "start": "3497920",
    "end": "3505490"
  },
  {
    "text": "So I'm going to expresses\nthis somehow as grad f dot dA.",
    "start": "3505490",
    "end": "3512290"
  },
  {
    "text": " So how can I do that?",
    "start": "3512290",
    "end": "3520260"
  },
  {
    "text": "So there's a couple tricks. The first trick is\nthis is a number. df, this is a scalar.",
    "start": "3520260",
    "end": "3525440"
  },
  {
    "text": " A scalar, I can always write\nas the trace of itself.",
    "start": "3525440",
    "end": "3533260"
  },
  {
    "text": "The trace of a\nscalar is just itself if you think of a scalar\nas a 1 by 1 matrix.",
    "start": "3533260",
    "end": "3540319"
  },
  {
    "text": "So I can always\nput a trace there. But this is not a\ntrace in the form that I want because I want\nthe dA over on the right.",
    "start": "3540320",
    "end": "3546040"
  },
  {
    "text": " So let me just put my\nlittle notes on the side.",
    "start": "3546040",
    "end": "3554400"
  },
  {
    "text": "Note, if you take\nthe trace of AB--",
    "start": "3554400",
    "end": "3561297"
  },
  {
    "text": "do people remember this formula? This is this is the same\nthing as the trace of BA. This is called the cyclic\nproperty of the trace.",
    "start": "3561297",
    "end": "3572670"
  },
  {
    "text": " And if you don't remember\nit, it's pretty easy",
    "start": "3572670",
    "end": "3579370"
  },
  {
    "text": "to convince yourself of it. So that does not\nmean I can rearrange",
    "start": "3579370",
    "end": "3586010"
  },
  {
    "text": "this in any order I want. It means if I have two\nthings, I can swap them. So it's called\ncyclic because if I",
    "start": "3586010",
    "end": "3591770"
  },
  {
    "text": "think of this thing as the first\nthing and this as the second, I can swap it. I can move y over to there,\nor I could move the x over",
    "start": "3591770",
    "end": "3599352"
  },
  {
    "text": "to the other side,\nI can do what's called a cyclic permutation\nof these things right. So I have to think of\nthem-- to apply this,",
    "start": "3599353",
    "end": "3604722"
  },
  {
    "text": "I have to group this\ninto two pieces. So I could group this into, for\nexample, x transpose and this.",
    "start": "3604722",
    "end": "3613250"
  },
  {
    "text": "And then I should be\nable to swap them. ",
    "start": "3613250",
    "end": "3618860"
  },
  {
    "text": "And y-- sorry. I don't want to do it\nin that order, though. I want to swap it\nin the other way.",
    "start": "3618860",
    "end": "3624710"
  },
  {
    "start": "3624710",
    "end": "3632369"
  },
  {
    "text": "So this is my cyclic\npermutation because I want to put the dA in the right.",
    "start": "3632370",
    "end": "3637600"
  },
  {
    "text": "So this is the trace of\ny x transpose times dA.",
    "start": "3637600",
    "end": "3644610"
  },
  {
    "text": " Now, this looks\nlike a dot product. ",
    "start": "3644610",
    "end": "3652500"
  },
  {
    "text": "So this is-- remember,\nthe dot product is always trace of\nsomething transpose dA.",
    "start": "3652500",
    "end": "3659250"
  },
  {
    "text": "So this is really, to\nmake it a dot product, I have to transpose the thing. This is xy transpose dot dA.",
    "start": "3659250",
    "end": "3668660"
  },
  {
    "text": " Does everyone see this?",
    "start": "3668660",
    "end": "3674580"
  },
  {
    "text": "Because remember, the trace of\nthe dot product, again, is-- where is it?",
    "start": "3674580",
    "end": "3682060"
  },
  {
    "text": "The dot product is\ntrace A transpose B. So I have to view this term\nhere as the transpose of that.",
    "start": "3682060",
    "end": "3692980"
  },
  {
    "text": "And so that's our gradient. So this is our gradient of f.",
    "start": "3692980",
    "end": "3701194"
  },
  {
    "start": "3701195",
    "end": "3707480"
  },
  {
    "text": "And it's whatever we take the\ndot product with to get dA. So as you see in the\nhomework, basically,",
    "start": "3707480",
    "end": "3712490"
  },
  {
    "text": "sometimes if you have a\nvector in and a scalar out, you know it has to be a\ndot product in some form.",
    "start": "3712490",
    "end": "3719250"
  },
  {
    "text": "If you have a dot product\nin your vector space, you have to be able to\nwrite it in that form. But sometimes when you\napply the derivative rule,",
    "start": "3719250",
    "end": "3728269"
  },
  {
    "text": "it doesn't look\nlike a dot product. You need to do a little\nbit of rearrangement to get it in the form where\nit looks like a dot product",
    "start": "3728270",
    "end": "3734390"
  },
  {
    "text": "to get the gradient out.  You need to learn a few rules.",
    "start": "3734390",
    "end": "3739519"
  },
  {
    "text": "And they are always\nof this kind of form, where you need to be able to\ntranspose things or learn which",
    "start": "3739520",
    "end": "3745010"
  },
  {
    "text": "things can be\npermuted and so forth. But it's really useful\nonce you get used to this.",
    "start": "3745010",
    "end": "3751760"
  },
  {
    "text": "I could have done the same\nthing by writing this out in terms of all\nthe components of A and then taking the partial\nderivative with respect",
    "start": "3751760",
    "end": "3758480"
  },
  {
    "text": "to each component. If you work hard\nenough, you'll realize that all these partial\nderivatives, in fact,",
    "start": "3758480",
    "end": "3766190"
  },
  {
    "text": "give you the matrix\nxy transpose. But it's a lot more work.",
    "start": "3766190",
    "end": "3771275"
  },
  {
    "text": " This is suddenly now we can take\ngradients of matrix functions.",
    "start": "3771275",
    "end": "3778170"
  },
  {
    "text": "And in fact, we\ncan take gradients of any scalar function\nin any vector spaces with a dot product.",
    "start": "3778170",
    "end": "3783880"
  },
  {
    "text": "So maybe later on, we'll\ntalk about even other kinds of vector spaces, where\nyou can define gradients",
    "start": "3783880",
    "end": "3789240"
  },
  {
    "text": "that are even weirder\nlooking than gradients of matrix functions. And in a subsequent\nlecture, Alan",
    "start": "3789240",
    "end": "3795720"
  },
  {
    "text": "has a nice trick to derive\nthis property of the gradient of a determinant.",
    "start": "3795720",
    "end": "3801993"
  },
  {
    "text": "GUEST SPEAKER: I\nforgot that I had that. Thanks for the reminder. STEVEN JOHNSON: Yes.",
    "start": "3801993",
    "end": "3807380"
  },
  {
    "text": "But any questions? We'll take maybe a\nthree-minute break.",
    "start": "3807380",
    "end": "3812660"
  },
  {
    "text": " AUDIENCE: No questions. STEVEN JOHNSON: No questions?",
    "start": "3812660",
    "end": "3818448"
  },
  {
    "text": "So let's take a\nthree-minute break. It's 12:11. We'll get back to 12:14.",
    "start": "3818448",
    "end": "3824640"
  },
  {
    "start": "3824640",
    "end": "3828000"
  }
]