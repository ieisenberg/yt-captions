[
  {
    "start": "0",
    "end": "1480"
  },
  {
    "text": "The trajectory estimation\nproblem that we considered",
    "start": "1480",
    "end": "4670"
  },
  {
    "text": "gives you a first glimpse\ninto a large field",
    "start": "4670",
    "end": "8220"
  },
  {
    "text": "that deals with\nlinear normal models.",
    "start": "8220",
    "end": "11380"
  },
  {
    "text": "In this segment, I\nwill just give you",
    "start": "11380",
    "end": "13220"
  },
  {
    "text": "a preview of what\nhappens in that field,",
    "start": "13220",
    "end": "16049"
  },
  {
    "text": "although, we will not attempt\nto prove or justify anything.",
    "start": "16050",
    "end": "20250"
  },
  {
    "text": "What happens in this\nfield is that we're",
    "start": "20250",
    "end": "22630"
  },
  {
    "text": "dealing with models where\nthere are some underlying",
    "start": "22630",
    "end": "25970"
  },
  {
    "text": "independent normal\nrandom variables.",
    "start": "25970",
    "end": "28509"
  },
  {
    "text": "And then, the random\nvariables of interest,",
    "start": "28510",
    "end": "31110"
  },
  {
    "text": "the unknown parameters\nand the observations,",
    "start": "31110",
    "end": "33420"
  },
  {
    "text": "can all be expressed\nas linear functions",
    "start": "33420",
    "end": "35960"
  },
  {
    "text": "of these independent normals.",
    "start": "35960",
    "end": "38140"
  },
  {
    "text": "Since linear functions\nof independent normals",
    "start": "38140",
    "end": "40730"
  },
  {
    "text": "are normal, in\nparticular, this means",
    "start": "40730",
    "end": "43320"
  },
  {
    "text": "that the Theta j and the Xi are\nall normal random variables.",
    "start": "43320",
    "end": "48620"
  },
  {
    "text": "Carrying out inference\nwithin this class of models",
    "start": "48620",
    "end": "52719"
  },
  {
    "text": "goes under the name\nof linear regression.",
    "start": "52720",
    "end": "55580"
  },
  {
    "start": "55580",
    "end": "59530"
  },
  {
    "text": "One can proceed using\npretty much the same steps",
    "start": "59530",
    "end": "62440"
  },
  {
    "text": "as we had in the trajectory\nestimation problem",
    "start": "62440",
    "end": "65930"
  },
  {
    "text": "and write down formulas\nfor the posterior.",
    "start": "65930",
    "end": "68700"
  },
  {
    "text": "And it turns out\nthat in every case,",
    "start": "68700",
    "end": "71000"
  },
  {
    "text": "the posterior of\nthe parameter vector",
    "start": "71000",
    "end": "74280"
  },
  {
    "text": "takes a form which is the\nexponential of and the negative",
    "start": "74280",
    "end": "79619"
  },
  {
    "text": "of a quadratic function\nof the parameters.",
    "start": "79620",
    "end": "82830"
  },
  {
    "text": "And this means, in\nparticular, that in order",
    "start": "82830",
    "end": "85370"
  },
  {
    "text": "to find the MAP estimate\nof the vector Theta, what",
    "start": "85370",
    "end": "90470"
  },
  {
    "text": "we need to do is\nto just minimize",
    "start": "90470",
    "end": "92880"
  },
  {
    "text": "this quadratic function\nwith respect to theta.",
    "start": "92880",
    "end": "95939"
  },
  {
    "text": "And minimizing a\nquadratic function",
    "start": "95940",
    "end": "98310"
  },
  {
    "text": "is done by taking\nderivatives and setting them",
    "start": "98310",
    "end": "100820"
  },
  {
    "text": "to zero which leads us to a\nsystem of linear equations,",
    "start": "100820",
    "end": "105800"
  },
  {
    "text": "exactly as in the trajectory\ninference problem.",
    "start": "105800",
    "end": "110000"
  },
  {
    "text": "And this means that numerically\nit is very simple to come up",
    "start": "110000",
    "end": "114037"
  },
  {
    "text": "with a MAP estimate.",
    "start": "114037",
    "end": "114870"
  },
  {
    "start": "114870",
    "end": "117479"
  },
  {
    "text": "There's an interesting\nfact that comes out",
    "start": "117479",
    "end": "120359"
  },
  {
    "text": "of the algebra involved,\nnamely, that the MAP",
    "start": "120360",
    "end": "123660"
  },
  {
    "text": "estimate of each one\nof the parameters",
    "start": "123660",
    "end": "126290"
  },
  {
    "text": "turns out to also be a linear\nfunction of the observations.",
    "start": "126290",
    "end": "130590"
  },
  {
    "text": "We saw this property\nin the estimators",
    "start": "130590",
    "end": "132920"
  },
  {
    "text": "that we derived for simpler\ncases in this lecture sequence.",
    "start": "132920",
    "end": "137640"
  },
  {
    "text": "It turns out that this\nproperty is still true.",
    "start": "137640",
    "end": "141060"
  },
  {
    "text": "And this is an appealing\nand desirable property",
    "start": "141060",
    "end": "143709"
  },
  {
    "text": "because it means that these\nestimators can be applied very",
    "start": "143710",
    "end": "147750"
  },
  {
    "text": "efficiently in\npractice without having",
    "start": "147750",
    "end": "150210"
  },
  {
    "text": "to do any complicated\ncalculations.",
    "start": "150210",
    "end": "153430"
  },
  {
    "text": "Finally, there's a number of\nimportant facts, some of which",
    "start": "153430",
    "end": "157700"
  },
  {
    "text": "we have seen in\nour examples which",
    "start": "157700",
    "end": "160180"
  },
  {
    "text": "are true in very big generality.",
    "start": "160180",
    "end": "162930"
  },
  {
    "start": "162930",
    "end": "165579"
  },
  {
    "text": "One fact is that the maximum a\nposteriori probability estimate",
    "start": "165579",
    "end": "170470"
  },
  {
    "text": "of some parameter\nturns out to be",
    "start": "170470",
    "end": "173000"
  },
  {
    "text": "the same as the conditional\nexpectation of that parameter.",
    "start": "173000",
    "end": "178450"
  },
  {
    "text": "Furthermore, if you look at\nthis joint density of all",
    "start": "178450",
    "end": "183875"
  },
  {
    "text": "the Theta parameters\nand from it you",
    "start": "183875",
    "end": "186750"
  },
  {
    "text": "find the marginal density\nof the Theta parameters",
    "start": "186750",
    "end": "190660"
  },
  {
    "text": "always within this\nconditional universe.",
    "start": "190660",
    "end": "193390"
  },
  {
    "text": "It turns out that this marginal\nposterior PDF is itself normal.",
    "start": "193390",
    "end": "201550"
  },
  {
    "text": "Since it is normal, its mean--\nwhich is this quantity--",
    "start": "201550",
    "end": "206480"
  },
  {
    "text": "is going to be\nequal to its peak.",
    "start": "206480",
    "end": "209980"
  },
  {
    "text": "And therefore, it is equal\nalso to the MAP estimate that",
    "start": "209980",
    "end": "215300"
  },
  {
    "text": "would be derived from\nthis marginal PDF.",
    "start": "215300",
    "end": "219450"
  },
  {
    "text": "So what do we have here?",
    "start": "219450",
    "end": "221640"
  },
  {
    "text": "There are two ways of coming\nup with MAP estimates.",
    "start": "221640",
    "end": "224640"
  },
  {
    "text": "One way is to find the\npeak of the joint PDF,",
    "start": "224640",
    "end": "231079"
  },
  {
    "text": "and then read out the\ndifferent components of Theta.",
    "start": "231079",
    "end": "235170"
  },
  {
    "text": "Another way of coming\nup with MAP estimates",
    "start": "235170",
    "end": "238030"
  },
  {
    "text": "is to find for each\nparameter the marginal PDF",
    "start": "238030",
    "end": "241940"
  },
  {
    "text": "and look at the peak\nof this marginal PDF.",
    "start": "241940",
    "end": "245050"
  },
  {
    "text": "It turns out that for this\nmodel these two approaches",
    "start": "245050",
    "end": "249360"
  },
  {
    "text": "are going to give\nyou the same answer.",
    "start": "249360",
    "end": "251670"
  },
  {
    "text": "Whether you work with the\nmarginal or with the joint,",
    "start": "251670",
    "end": "254849"
  },
  {
    "text": "you get the same MAP estimates.",
    "start": "254850",
    "end": "257160"
  },
  {
    "text": "And this is a reassuring\nproperty to have.",
    "start": "257160",
    "end": "260109"
  },
  {
    "text": "Finally, as in the examples that\nwe have worked in more detail,",
    "start": "260110",
    "end": "264650"
  },
  {
    "text": "it turns out that the mean\nsquared error conditioned",
    "start": "264650",
    "end": "268490"
  },
  {
    "text": "on a particular observation\nis the same no matter",
    "start": "268490",
    "end": "271530"
  },
  {
    "text": "what the value of\nthat observation was.",
    "start": "271530",
    "end": "274780"
  },
  {
    "text": "And furthermore, there are\nfairly simple and easily",
    "start": "274780",
    "end": "278810"
  },
  {
    "text": "computable formulas that\none can apply in order",
    "start": "278810",
    "end": "282940"
  },
  {
    "text": "to find what this\nmean squared error is.",
    "start": "282940",
    "end": "286740"
  },
  {
    "text": "So to summarize,\nthis class of models",
    "start": "286740",
    "end": "291340"
  },
  {
    "text": "that involve linear relations\nand normal random variables",
    "start": "291340",
    "end": "296030"
  },
  {
    "text": "have a rich set of important\nand elegant properties.",
    "start": "296030",
    "end": "300850"
  },
  {
    "text": "This is one of the reasons\nwhy these models are",
    "start": "300850",
    "end": "303860"
  },
  {
    "text": "used very much in practice.",
    "start": "303860",
    "end": "306280"
  },
  {
    "text": "And they're probably the\nmost widely used class",
    "start": "306280",
    "end": "309440"
  },
  {
    "text": "of statistical models.",
    "start": "309440",
    "end": "311850"
  }
]