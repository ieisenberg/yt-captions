[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13960",
    "end": "19790"
  },
  {
    "text": "ocw.mit.edu. ",
    "start": "19790",
    "end": "25210"
  },
  {
    "text": "PROFESSOR: OK, we're ready for\nthe second lecture today. We will start to get into a\nlittle technical material,",
    "start": "25210",
    "end": "33330"
  },
  {
    "text": "which doesn't mean necessarily\nthat it's more important. It just means that it's easier\nbecause it deals with",
    "start": "33330",
    "end": "40630"
  },
  {
    "text": "mathematics. I'm going to spend a little\nbit more time reviewing",
    "start": "40630",
    "end": "46920"
  },
  {
    "text": "probability, as you\nlearned it before. I want to review it at a\nslightly more fundamental",
    "start": "46920",
    "end": "53800"
  },
  {
    "text": "level than what you're\nused to seeing it as. You will understand why as we go\non because when we get into",
    "start": "53800",
    "end": "60899"
  },
  {
    "text": "stochastic processes, we will\nfind that there are lots of very peculiar things\nthat happen.",
    "start": "60900",
    "end": "68700"
  },
  {
    "text": "And when peculiar things happen,\nthe only thing you can do is go back to basics ideas.",
    "start": "68700",
    "end": "75160"
  },
  {
    "text": "And if you don't understand what\nthose basic ideas are, then you're in real trouble.",
    "start": "75160",
    "end": "80640"
  },
  {
    "text": "So we'll start out by talking\nabout expectations just a little bit.",
    "start": "80640",
    "end": "86030"
  },
  {
    "text": " Distribution function of a\nrandom variable often says",
    "start": "86030",
    "end": "96570"
  },
  {
    "text": "more than you're really\ninterested in. In other words, a distribution\nfunction is a function from the whole sample space into\nthe real numbers.",
    "start": "96570",
    "end": "105590"
  },
  {
    "text": "And that's a very complicated\nthing in general. And the expectation is just\none simple number.",
    "start": "105590",
    "end": "111990"
  },
  {
    "text": "And with that one simple number,\nyou get an idea of what that random variable is all\nabout, whether it's big or",
    "start": "111990",
    "end": "118090"
  },
  {
    "text": "it's little or what have you. They're a bunch of formulas that\nyou're familiar with for",
    "start": "118090",
    "end": "124740"
  },
  {
    "text": "finding the expectation. If you have a discrete random\nvariable, the usual formula is",
    "start": "124740",
    "end": "131140"
  },
  {
    "text": "you take all of the possible\nsample values, multiply each of them by the probability\nof that sample value,",
    "start": "131140",
    "end": "138099"
  },
  {
    "text": "and you sum it up. This is what you learned right\nat the beginning of taking",
    "start": "138100",
    "end": "143300"
  },
  {
    "text": "probability. If you've never taken\nprobability, you learned it in statistic classes just as\nsomething you don't know where",
    "start": "143300",
    "end": "150120"
  },
  {
    "text": "it comes from. But it's there.  If you have a continuous random\nvariable, a continuous",
    "start": "150120",
    "end": "157709"
  },
  {
    "text": "random variable is one\nthat has a density. You can find the expectation\nthere.",
    "start": "157710",
    "end": "163200"
  },
  {
    "text": "If you have an arbitrary random\nvariable and it's not negative, then there's this\npeculiar formula here, which I",
    "start": "163200",
    "end": "172520"
  },
  {
    "text": "will point to. ",
    "start": "172520",
    "end": "179510"
  },
  {
    "text": "I think I'll point to it. ",
    "start": "179510",
    "end": "185710"
  },
  {
    "text": "Ah yes, I will point to it. This formula down here, you\nmight or might not have seen.",
    "start": "185710",
    "end": "194810"
  },
  {
    "text": "And I hope by the end of this\ncourse, you'll realize that it's one, more fundamental and\ntwo, probably more useful than",
    "start": "194810",
    "end": "203620"
  },
  {
    "text": "either of these two. And then there's a final one,\nwhich this final formula, I'll",
    "start": "203620",
    "end": "210510"
  },
  {
    "text": "tell you a little bit about\nthat when we get to it. OK, the formula for the expected\nvalue in terms of the",
    "start": "210510",
    "end": "221260"
  },
  {
    "text": "integral of the complimentary\ndistribution function. There's a picture here which\nshows you how it corresponds",
    "start": "221260",
    "end": "229520"
  },
  {
    "text": "to the usual thing you're\nused to for a discrete random variable. Namely what you're doing is\nyou're integrating this",
    "start": "229520",
    "end": "237100"
  },
  {
    "text": "complimentary distribution\nfunction, which is the probability that the random\nvariable x is greater than any",
    "start": "237100",
    "end": "245060"
  },
  {
    "text": "particular x along\nthe axis here. So you integrate this\nfunction along here.",
    "start": "245060",
    "end": "250879"
  },
  {
    "text": "And according to what I'm trying\nto convince you of, just integrating that function\ngives you the expected value.",
    "start": "250880",
    "end": "257979"
  },
  {
    "text": "And the reason is that this top\nlittle square here is a1 times the probability that\nx is equal to a1.",
    "start": "257980",
    "end": "265250"
  },
  {
    "text": "Next one is a2 times the\nprobability it's equal to a2 and so forth down.",
    "start": "265250",
    "end": "271160"
  },
  {
    "text": "And you can obviously generalize\nthis to any discrete random variable,\nwhich is non-negative.",
    "start": "271160",
    "end": "277030"
  },
  {
    "text": "And I'm just talking about\nnon-negative random variables for the moment.",
    "start": "277030",
    "end": "282500"
  },
  {
    "text": "If x has a density, the same\nargument applies to any Riemann sum for that integral.",
    "start": "282500",
    "end": "288580"
  },
  {
    "text": "You can take integrals. You can break them up\ninto little slices. If you break them up into\nlittle slices, you can",
    "start": "288580",
    "end": "294470"
  },
  {
    "text": "represent it in this way. And presto, again, you get that\nthis integral is equal to",
    "start": "294470",
    "end": "301250"
  },
  {
    "text": "the expectation. And if you have any other thing\nat all, you can always",
    "start": "301250",
    "end": "306910"
  },
  {
    "text": "represent it in terms\nof this Riemann sum. Now why is it even more\npowerful then that?",
    "start": "306910",
    "end": "313360"
  },
  {
    "text": "Well, it's more powerful then\nthat because if you took measure theory-- which most of you presumably\nhave not taken yet, and many",
    "start": "313360",
    "end": "322780"
  },
  {
    "text": "of you might never take it-- you will find out that this\nis really the fundamental definition after all.",
    "start": "322780",
    "end": "329220"
  },
  {
    "text": "And integration, when you look\nand measure theoretic terms, instead of taking little slices\nthat go this way, you",
    "start": "329220",
    "end": "336600"
  },
  {
    "text": "wind up taking little slices\nthat go that way. So that any way this is the\nfundamental definition of",
    "start": "336600",
    "end": "344830"
  },
  {
    "text": "expectation. If you're worried about whether\nexpectations exist or not, why is this much nicer?",
    "start": "344830",
    "end": "352180"
  },
  {
    "text": "Because what you're integrating\nhere is simply a function, which is monotonic\ndecreasing with x.",
    "start": "352180",
    "end": "362010"
  },
  {
    "text": "In other words, if you try to\nintegrate it by integrating",
    "start": "362010",
    "end": "368130"
  },
  {
    "text": "this function out to some\nlargest value and then",
    "start": "368130",
    "end": "373900"
  },
  {
    "text": "chopping if off there, what\nyou get is some number. If you extend this chopping off\npoint out, what you get is",
    "start": "373900",
    "end": "383490"
  },
  {
    "text": "a number which keeps\nincreasing. What can happen? As you take a number which is\nincreasing, you either get to",
    "start": "383490",
    "end": "390470"
  },
  {
    "text": "infinity or you get to\nsome finite limit. Nothing else can happen. So there aren't any limiting\nproblems here.",
    "start": "390470",
    "end": "397160"
  },
  {
    "text": "And when you take expectations\nin other ways, there are always questions that\nyou have to ask.",
    "start": "397160",
    "end": "403669"
  },
  {
    "text": "And they're often serious. So this is just a much nicer\nway of doing it.",
    "start": "403670",
    "end": "408750"
  },
  {
    "text": "Anyway, that's the way\nwe're going to do it. And so now we go on.",
    "start": "408750",
    "end": "415020"
  },
  {
    "text": "Oh, I should mention where the\nother formula comes from. This formula back here.",
    "start": "415020",
    "end": "422420"
  },
  {
    "text": " You get that by representing x\nas both the positive part of x",
    "start": "422420",
    "end": "431360"
  },
  {
    "text": "plus the negative part of x. And if you want to see how to\ndo that exactly, it's in the",
    "start": "431360",
    "end": "436610"
  },
  {
    "text": "notes where it talks about\nfirst this and then this. So you just put the\ntwo together.",
    "start": "436610",
    "end": "442770"
  },
  {
    "text": "And then you get an\nexpected value. A word about notation here, and\nthere's nothing I can do",
    "start": "442770",
    "end": "448950"
  },
  {
    "text": "about this. It's an unfortunate thing. When somebody says that the\nexpected value of a random",
    "start": "448950",
    "end": "456840"
  },
  {
    "text": "variable exists, what\ndo they mean? ",
    "start": "456840",
    "end": "463830"
  },
  {
    "text": "Any engineer would try to\nintegrate it and would either",
    "start": "463830",
    "end": "468909"
  },
  {
    "text": "get something which was\nundefined, because it was infinite going this way.",
    "start": "468910",
    "end": "474039"
  },
  {
    "text": "It's minus infinity\ngoing that way. And there's no way to put\nthe two together.",
    "start": "474040",
    "end": "479510"
  },
  {
    "text": "If you get infinity going this\nway, something finite going that way, like with a\nnon-negative random variable,",
    "start": "479510",
    "end": "487700"
  },
  {
    "text": "it's kind of silly to say the\nexpectation doesn't exist. Because really what's happening\nis the expectation",
    "start": "487700",
    "end": "494440"
  },
  {
    "text": "is infinite. Now mathematicians and everybody\nwho writes books, everybody who writes\npapers, everybody--",
    "start": "494440",
    "end": "501840"
  },
  {
    "text": "I think-- defines expected value as\nexisting only if it's finite.",
    "start": "501840",
    "end": "507810"
  },
  {
    "text": "In other words, what you're\ndoing is taking this integral over the set of real values.",
    "start": "507810",
    "end": "513409"
  },
  {
    "text": "And you don't allow plus\ninfinity or minus infinity. So you say that the expectation\ndoes not exist if",
    "start": "513409",
    "end": "532060"
  },
  {
    "text": "in fact it's infinite or it's\nminus infinity or it is undefined completely.",
    "start": "532060",
    "end": "538810"
  },
  {
    "text": "And you say it's undefined\nin all of those cases. And that's just a convention\nthat everybody lives by.",
    "start": "538810",
    "end": "544550"
  },
  {
    "text": "So the other way of saying this\nis if the expected value of the magnitude of the random\nvariable is infinite, then the",
    "start": "544550",
    "end": "553840"
  },
  {
    "text": "expectation doesn't exist. So we will try to say it\nthat way sometimes",
    "start": "553840",
    "end": "559430"
  },
  {
    "text": "when it's really important. OK, let's go on to indicator\nrandom variables.",
    "start": "559430",
    "end": "566010"
  },
  {
    "text": "You're probably familiar\nwith these. For every event you can think\nof, an event is something",
    "start": "566010",
    "end": "573899"
  },
  {
    "text": "which is true, which occurs when\nsome set of the sample points occur and is not\ntrue otherwise.",
    "start": "573900",
    "end": "582460"
  },
  {
    "text": "So the definition of an\nindicator random variable is that the indicator\nfor an event a--",
    "start": "582460",
    "end": "589520"
  },
  {
    "text": "as a function of the\nsample space-- is equal to 1, if omega is in\nthe event a, and 0 otherwise.",
    "start": "589520",
    "end": "598920"
  },
  {
    "text": "So if you draw the distribution\nfunction of it, the distribution function of the\nindicator function is 0 up",
    "start": "598920",
    "end": "611170"
  },
  {
    "text": "until the point 0. Then it jumps up to 1 minus\nthe probability of a. At 1, it jumps all\nthe way up to 1.",
    "start": "611170",
    "end": "619000"
  },
  {
    "text": "So it's simply a binary\nrandom variable. So every event has an indicator\nrandom variable.",
    "start": "619000",
    "end": "625600"
  },
  {
    "text": "Every indicator random\nvariable has a binary random variable. So indicator random variables\nare very simple.",
    "start": "625600",
    "end": "633250"
  },
  {
    "text": "Events are very simple because\nyou can map any event into an indicator random variable\n[INAUDIBLE].",
    "start": "633250",
    "end": "639400"
  },
  {
    "text": "And this also says that since\nwe want to talk about events very often, binary random\nvariables are particularly",
    "start": "639400",
    "end": "646410"
  },
  {
    "text": "important in this field. OK, but what this really says\nnow is that any theorem about",
    "start": "646410",
    "end": "652759"
  },
  {
    "text": "random variables can be\napplied to events. This is one of the few examples\nI know where it's",
    "start": "652760",
    "end": "659360"
  },
  {
    "text": "much harder to find the\nexpectation by taking the complimentary distribution\nfunction and integrating it.",
    "start": "659360",
    "end": "665350"
  },
  {
    "text": "It's not hard. But it's far easier to take\nthe probability that the indicator random variable\nis 0, which is 1 minus",
    "start": "665350",
    "end": "673650"
  },
  {
    "text": "probability of a. The probability is equal to 1,\nwhich is probability of a, and take the expectation, which is\nthe probability of a, and the",
    "start": "673650",
    "end": "682480"
  },
  {
    "text": "standard deviation, which\nis the square root the probability of a times 1 minus\nthe probability of a.",
    "start": "682480",
    "end": "689579"
  },
  {
    "text": "So random variables are sort\nof trivial things in a way. OK, let's go on to multiple\nrandom variables.",
    "start": "689580",
    "end": "696940"
  },
  {
    "text": "Now here's something that's\na trick question in a way. But it's a very important\ntrick question.",
    "start": "696940",
    "end": "703110"
  },
  {
    "text": "Is a random variable specified\nby its distribution function? We've already seen that it's\nnot really specified by its",
    "start": "703110",
    "end": "709620"
  },
  {
    "text": "density or by its probability\nmass function. But we've said a distribution\nfunction is a more general",
    "start": "709620",
    "end": "717190"
  },
  {
    "text": "thing so that every random\nvariable has a distribution function. Does the distribution function\nspecify the random variable?",
    "start": "717190",
    "end": "725900"
  },
  {
    "text": "No, that's the whole\nreason for what Kolmogorov did back in 1933.",
    "start": "725900",
    "end": "734610"
  },
  {
    "text": "Or at least it was one\nof the main reasons for what he was doing. He wanted to straighten out\nthis ambiguity which runs",
    "start": "734610",
    "end": "741550"
  },
  {
    "text": "through the field about\nconfusing random variables with their distribution\nfunction.",
    "start": "741550",
    "end": "747760"
  },
  {
    "text": "Random variables are functions\nfrom the sample space to the real numbers.",
    "start": "747760",
    "end": "754110"
  },
  {
    "text": "And they're not anything else. So if you want to really define\na random variable, you",
    "start": "754110",
    "end": "760550"
  },
  {
    "text": "not only have to know what that\nrandom variable is but you also have to know what\nits relationships are.",
    "start": "760550",
    "end": "766980"
  },
  {
    "text": "It's like if you're trying\nto understand the person. You can't understand the person\nwithout understanding",
    "start": "766980",
    "end": "772110"
  },
  {
    "text": "something about who they know,\nhow they know them, all those other things. All those relationships\nare important.",
    "start": "772110",
    "end": "779360"
  },
  {
    "text": "And it's the same with\nrandom variables. You got to know about all\nthe relationships. Many problems you can solve\njust in terms of",
    "start": "779360",
    "end": "785890"
  },
  {
    "text": "distribution function. But ultimately you have to-- or ultimately in many cases,\nyou have to deal with these",
    "start": "785890",
    "end": "793170"
  },
  {
    "text": "joint distribution functions. And random variables\nare independent. If the joint distribution\nfunction is equal to the",
    "start": "793170",
    "end": "801440"
  },
  {
    "text": "product of the distribution\nfunctions for all x1 to xn,",
    "start": "801440",
    "end": "807890"
  },
  {
    "text": "and that same form carries over\nfor density functions and",
    "start": "807890",
    "end": "814790"
  },
  {
    "text": "for probability mass\nfunctions. ",
    "start": "814790",
    "end": "821756"
  },
  {
    "text": "OK, if you have discrete random\nvariables, the idea of independence is a whole lot more\nintuitive if you express",
    "start": "821757",
    "end": "829990"
  },
  {
    "text": "it in terms of conditional\nprobabilities. The conditional probability\nthat the random variable x",
    "start": "829990",
    "end": "838620"
  },
  {
    "text": "takes on some sample value x\ngiven that the random variable",
    "start": "838620",
    "end": "843690"
  },
  {
    "text": "y takes on a sample value y. ",
    "start": "843690",
    "end": "848750"
  },
  {
    "text": "Just as one side comment here,\nwhen you're doing problems,",
    "start": "848750",
    "end": "854300"
  },
  {
    "text": "you will very often want to\nleave out the subscripts here saying what random variables\nyou're dealing with.",
    "start": "854300",
    "end": "861980"
  },
  {
    "text": "And you will use either capital\nor small letters here mixing up the argument and the\nfunction itself, which",
    "start": "861980",
    "end": "873140"
  },
  {
    "text": "everybody does. And it's perfectly all right. I suggest that you try not to do\nit for a while because you",
    "start": "873140",
    "end": "880480"
  },
  {
    "text": "get so confused doing this,\nnot being able to sort out what's a random variable and\nwhat's a real number.",
    "start": "880480",
    "end": "886960"
  },
  {
    "text": "A lot of wags say random\nvariables are neither random, because they're functions\nof the sample space,",
    "start": "886960",
    "end": "894079"
  },
  {
    "text": "nor are they variables. And both of those are true.",
    "start": "894080",
    "end": "899589"
  },
  {
    "text": "That's immaterial here. It's just that when you start\ngetting confused about a problem, it's important to\nsort out which things are",
    "start": "899590",
    "end": "907840"
  },
  {
    "text": "random variables and which\nthings are arguments. Now this conditional probability\nis something",
    "start": "907840",
    "end": "913700"
  },
  {
    "text": "you're all familiar with. But x and y are independent then\nif the probability of x",
    "start": "913700",
    "end": "920620"
  },
  {
    "text": "conditional on y is the same\nas the probability of x not conditional on y.",
    "start": "920620",
    "end": "926000"
  },
  {
    "text": "In other words, if observing\nwhat y is doesn't tell you anything about what x is, that's\nreally your intuitive",
    "start": "926000",
    "end": "932970"
  },
  {
    "text": "definition of independence. It's what you use if\nyou're dealing with",
    "start": "932970",
    "end": "939050"
  },
  {
    "text": "some real world situation. And you're asking what does\nthis have to do with that?",
    "start": "939050",
    "end": "944280"
  },
  {
    "text": "And if this has nothing to\ndo with that, the random variables over here have nothing\nto do with the random",
    "start": "944280",
    "end": "949950"
  },
  {
    "text": "variables over there, you would\nsay in the real world that these things are\nindependent of each other.",
    "start": "949950",
    "end": "956880"
  },
  {
    "text": "When you have a probability\nmodel, you say they're statistically independent\nof each other. ",
    "start": "956880",
    "end": "971580"
  },
  {
    "text": "OK, so that's the relationship\nbetween the real world and the",
    "start": "971580",
    "end": "981280"
  },
  {
    "text": "models that we're dealing\nwith all the time. We call it independence\nin both cases. But it means somewhat different\nthings in the two",
    "start": "981280",
    "end": "988980"
  },
  {
    "text": "situations. OK, next about IID\nrandom variables.",
    "start": "988980",
    "end": "994270"
  },
  {
    "text": "What are they? Well, the joint distribution\nfunction has to be equal to",
    "start": "994270",
    "end": "1001480"
  },
  {
    "text": "the product of the individual\ndistribution functions.",
    "start": "1001480",
    "end": "1006760"
  },
  {
    "text": "You notice I've done something\nfunny here, which is a convention I always use.",
    "start": "1006760",
    "end": "1012430"
  },
  {
    "text": "A lot of people use it. If you have a bunch of\nindependent random variables,",
    "start": "1012430",
    "end": "1017680"
  },
  {
    "text": "they all have the same\ndistribution function. If they all have the same\ndistribution function, it gets",
    "start": "1017680",
    "end": "1023600"
  },
  {
    "text": "confusing to refer to their\ndistribution functions as a distribution function of\nthe random variable x1,",
    "start": "1023600",
    "end": "1029750"
  },
  {
    "text": "distribution function of\nthe random variable x2. It's nicer to just take a\ngeneric random variable x,",
    "start": "1029750",
    "end": "1037150"
  },
  {
    "text": "which has the same distribution\nas all of these, and express this numerically\nin this way.",
    "start": "1037150",
    "end": "1043929"
  },
  {
    "text": "You have the same product form\nfor probability mass functions and for density functions.",
    "start": "1043930",
    "end": "1049890"
  },
  {
    "text": "So this works throughout. OK next, think about a\nprobability model in which r,",
    "start": "1049890",
    "end": "1059430"
  },
  {
    "text": "the set of real numbers,\nis a sample space. And x is some random variable\non that sample space.",
    "start": "1059430",
    "end": "1066419"
  },
  {
    "text": "Namely x then is a function from\nthe real numbers on to the real numbers.",
    "start": "1066420",
    "end": "1073445"
  },
  {
    "text": "The interesting thing\nhere, and what I'm saying here is obvious.",
    "start": "1073445",
    "end": "1079380"
  },
  {
    "text": "It's something that\nyou all know. It's something that you've all\nbeen using even before you started to learn about\nprobability theory.",
    "start": "1079380",
    "end": "1087010"
  },
  {
    "text": "But at the same time, you\nprobably never thought about it in a serious enough way that\nyou would really make",
    "start": "1087010",
    "end": "1093190"
  },
  {
    "text": "sense out of it. You can always create an\nextended probability model in",
    "start": "1093190",
    "end": "1098889"
  },
  {
    "text": "which the Cartesian space r to\nthe n-- in other words, the space of n real numbers--",
    "start": "1098890",
    "end": "1106919"
  },
  {
    "text": "is the sample space. And x1 to xn are independent\nidentity",
    "start": "1106920",
    "end": "1112860"
  },
  {
    "text": "distributed random variables. This is not obvious. And that's something\nyou have to prove.",
    "start": "1112860",
    "end": "1118730"
  },
  {
    "text": "But it's not hard to prove. And all you have to do is start\nout with a probability",
    "start": "1118730",
    "end": "1126179"
  },
  {
    "text": "model for one random variable. And then just define all\nproducts to be what they're",
    "start": "1126180",
    "end": "1133260"
  },
  {
    "text": "supposed to be and go from the\nproducts to all unions and all intersections.",
    "start": "1133260",
    "end": "1138940"
  },
  {
    "text": "We're just going to assume that\nthat's true because we have to assume it's true if we\ndon't want to use any measure",
    "start": "1138940",
    "end": "1146000"
  },
  {
    "text": "theory here. This is one of the easier\nthings to show using measure theory. But it's something you\nare always used to.",
    "start": "1146000",
    "end": "1153110"
  },
  {
    "text": "When you think of a random\nexperiment, when you think of playing dice with somebody\nor playing cards with",
    "start": "1153110",
    "end": "1159030"
  },
  {
    "text": "someone, you are-- from the very beginning when you\nstarted to talk about odds",
    "start": "1159030",
    "end": "1165650"
  },
  {
    "text": "or anything of that sort, you\nhave always had the idea that this is a game which you\ncan play repeatedly.",
    "start": "1165650",
    "end": "1173790"
  },
  {
    "text": "And each time you play it,\nit's the same game. But the outcome is different. But all the probabilities\nare exactly the same.",
    "start": "1173790",
    "end": "1182510"
  },
  {
    "text": "What this says is you can always\nmake a probability model that way. You can always make a\nprobability model which",
    "start": "1182510",
    "end": "1189590"
  },
  {
    "text": "corresponds to what you've\nalways believed deep in your hearts all your lives.",
    "start": "1189590",
    "end": "1194700"
  },
  {
    "text": "And fortunately, that's true. Otherwise, you wouldn't\nuse probability. OK, so let's move\non from that.",
    "start": "1194700",
    "end": "1203830"
  },
  {
    "text": "The page of philosophy, I will\nstop doing that pretty soon.",
    "start": "1203830",
    "end": "1208919"
  },
  {
    "text": "But I have to get across what\nthe relationship is between the real world and\nthese models that",
    "start": "1208920",
    "end": "1215260"
  },
  {
    "text": "we're dealing with. Because otherwise, you as\nengineers or business people",
    "start": "1215260",
    "end": "1220700"
  },
  {
    "text": "or financial analysts or\nwhatever the heck you're going to become will start believing\nin your probability models.",
    "start": "1220700",
    "end": "1227930"
  },
  {
    "text": "And you will cause untold damage\nby losing track of the",
    "start": "1227930",
    "end": "1233000"
  },
  {
    "text": "fact that these are supposedly\nmodels of something. And you better think of\nwhat they're supposed",
    "start": "1233000",
    "end": "1238160"
  },
  {
    "text": "to be models of. OK, in order to do that, we're\ngoing to study the sample",
    "start": "1238160",
    "end": "1243210"
  },
  {
    "text": "average, namely the sum of n\nrandom variables divided by n.",
    "start": "1243210",
    "end": "1249120"
  },
  {
    "text": "That's the way you take\nsample averages. You add them all up. You divide by n.",
    "start": "1249120",
    "end": "1254380"
  },
  {
    "text": "The law of large numbers, which\nwe're going to talk about very soon, says that s\nsub n over n essentially",
    "start": "1254380",
    "end": "1260880"
  },
  {
    "text": "becomes deterministic as\nn becomes very large.",
    "start": "1260880",
    "end": "1266210"
  },
  {
    "text": "What we mean by that-- and most\nof you have seen that in various ways. We will review it later today.",
    "start": "1266210",
    "end": "1274180"
  },
  {
    "text": "Well, we'll do it today\nand on Wednesday. And there's a big question\nof about what becoming",
    "start": "1274180",
    "end": "1282400"
  },
  {
    "text": "deterministic means. But there is an essential\nidea there. The extended model, namely\nwhen you have one random",
    "start": "1282400",
    "end": "1290130"
  },
  {
    "text": "variable, you create a very\nlarge number of them. If it corresponds to repeated\nexperiments in the real world,",
    "start": "1290130",
    "end": "1297380"
  },
  {
    "text": "then s sub n over n corresponds\nto the arithmetic average in the real world.",
    "start": "1297380",
    "end": "1302600"
  },
  {
    "text": "In the real world, you do take\narithmetic averages. Whenever you open up a\nnewspaper, somebody is taking",
    "start": "1302600",
    "end": "1309650"
  },
  {
    "text": "an arithmetic average of\nsomething and says, gee, this is significant. This shows what's going\non someplace.",
    "start": "1309650",
    "end": "1318550"
  },
  {
    "text": "Models can have two types\nof difficulties. This paragraph is a little\ndifferent than what I wrote in",
    "start": "1318550",
    "end": "1324250"
  },
  {
    "text": "the handout because I realized\nwhat I wrote in the handout didn't make a whole\nlot of sense.",
    "start": "1324250",
    "end": "1330250"
  },
  {
    "text": "OK, the two types of\ndifficulties you have with models, especially when you're\ntrying to model things by IID",
    "start": "1330250",
    "end": "1337289"
  },
  {
    "text": "random variables. In one, a sequence of real\nworld experiments is not",
    "start": "1337290",
    "end": "1343390"
  },
  {
    "text": "sufficiently similar and\nisolated to each other to correspond to the IID\nextended model.",
    "start": "1343390",
    "end": "1350820"
  },
  {
    "text": "In other words, you want to\nmodel things so that each time, each trial of this\nexperiment, you do the same",
    "start": "1350820",
    "end": "1357120"
  },
  {
    "text": "thing but get a potentially\ndifferent answer. Sometimes you rig things without\ntrying to do so in",
    "start": "1357120",
    "end": "1366535"
  },
  {
    "text": "such a way that these\nexperiments are not independent of each other\nand in fact are very,",
    "start": "1366535",
    "end": "1372420"
  },
  {
    "text": "very heavily biased. You find people taking risk\nmodels in the financial world where they take all sorts\nof these things.",
    "start": "1372420",
    "end": "1379840"
  },
  {
    "text": "And they say, oh, all right,\nthese things are all independent of each other. They look independent.",
    "start": "1379840",
    "end": "1385120"
  },
  {
    "text": "And then suddenly a\nscare comes along. And everybody sells\nsimultaneously.",
    "start": "1385120",
    "end": "1390580"
  },
  {
    "text": "And you find out that all these\nrandom variables were not independent at all. They were very closely related\nto each other but in a way you",
    "start": "1390580",
    "end": "1397760"
  },
  {
    "text": "never saw before. OK, the other way that these\nmodels are not true or not",
    "start": "1397760",
    "end": "1406289"
  },
  {
    "text": "valid is that the IID\nextension is OK. But the basic model\nis not right.",
    "start": "1406290",
    "end": "1413340"
  },
  {
    "text": "OK, in other words,\nyou model a coin. It's coming out heads with\nprobability 1/2.",
    "start": "1413340",
    "end": "1418410"
  },
  {
    "text": "And somebody has put\na loaded coin in. And the probability that it\ncomes up heads is 0.45.",
    "start": "1418410",
    "end": "1427760"
  },
  {
    "text": "And the probability that it\ncomes up tails is 0.55. And you might guess that this\nperson always bets on tails",
    "start": "1427760",
    "end": "1434860"
  },
  {
    "text": "and tries to get you\nto bet on heads. So in that case, the basic\nmodel that you're",
    "start": "1434860",
    "end": "1441410"
  },
  {
    "text": "using is not OK. So you have both of these\nkinds of problems. You should try to keep\nthem straight.",
    "start": "1441410",
    "end": "1447190"
  },
  {
    "text": "But we'll learn about these\nproblems through study of the models. Namely, we're not going to go\nthrough an enormous amount of",
    "start": "1447190",
    "end": "1453950"
  },
  {
    "text": "study on how you can bias a coin\nor things of this sort.",
    "start": "1453950",
    "end": "1459834"
  },
  {
    "text": "OK, science, symmetry,\nanalogies, earlier models, all of these are used to model\nreal world situations.",
    "start": "1459834",
    "end": "1467830"
  },
  {
    "text": "Let me again talk about an\nexample I talked about a little bit last time because the\nmodel was so trivial that",
    "start": "1467830",
    "end": "1477730"
  },
  {
    "text": "you probably understood\neverything about the model in the situation. But you didn't understand what\nit was illustrating.",
    "start": "1477730",
    "end": "1486059"
  },
  {
    "text": "You have two dice. One of them is red. And one of them is white.",
    "start": "1486060",
    "end": "1492930"
  },
  {
    "text": "You roll them. By symmetry, each one comes up\nto be 1, 2, 3, up to 6, each",
    "start": "1492930",
    "end": "1500429"
  },
  {
    "text": "with equal probability. If you roll them with two hands\nor something, they're going to be independent\nof each other.",
    "start": "1500430",
    "end": "1507850"
  },
  {
    "text": "And therefore, the probability\nof each pair of outcomes-- red is equal to i.",
    "start": "1507850",
    "end": "1513480"
  },
  {
    "text": "White is equal to j. Probability of each one of\nthose is going to be 136 because that's the size\nof the sample space.",
    "start": "1513480",
    "end": "1520770"
  },
  {
    "text": "Now you take two white dice. And you roll them.",
    "start": "1520770",
    "end": "1525809"
  },
  {
    "text": "What's the sample space? Well, as far as the real world\nis concerned, you can't distinguish a red 1 from\na white 1 and a white",
    "start": "1525810",
    "end": "1535800"
  },
  {
    "text": "2 from a red 2. In other words, those two\npossibilities can't be distinguished.",
    "start": "1535800",
    "end": "1541730"
  },
  {
    "text": "So you might say I want to\nuse a sample space which corresponds to the--",
    "start": "1541730",
    "end": "1546736"
  },
  {
    "text": " what's the word I used here?--\nfinest grain possible outcome",
    "start": "1546736",
    "end": "1553880"
  },
  {
    "text": "that you can observe. And who would do that?",
    "start": "1553880",
    "end": "1559029"
  },
  {
    "text": "You'd be crazy to do that. I mean, you have a nice model\nof rolling dice where each",
    "start": "1559030",
    "end": "1564270"
  },
  {
    "text": "outcome has probability 136. And you would replace that\nwith something where the",
    "start": "1564270",
    "end": "1570120"
  },
  {
    "text": "probability of a 1, 1 is 136. But the probability of a 1, 2 is\n1/18 because you can get a",
    "start": "1570120",
    "end": "1577559"
  },
  {
    "text": "2 in two different ways. You get a 2 in two different\nways, which says you're really",
    "start": "1577560",
    "end": "1583470"
  },
  {
    "text": "thinking about a red die\nand a white die. Otherwise, you wouldn't\nbe able to say that.",
    "start": "1583470",
    "end": "1589440"
  },
  {
    "text": "So the appropriate model here is\ncertainly to think in terms of a red die and a white die.",
    "start": "1589440",
    "end": "1595050"
  },
  {
    "text": "It's what everybody does. They just don't talk about it. OK, so the point that I'm trying\nto make here is that",
    "start": "1595050",
    "end": "1603200"
  },
  {
    "text": "what you call a finest grain\nmodel is not at all clear.",
    "start": "1603200",
    "end": "1608880"
  },
  {
    "text": "And if it's not at all clear in\nthe case of dice, it sure as hell is not clear in most of\nthe kinds of problems you",
    "start": "1608880",
    "end": "1615740"
  },
  {
    "text": "want to deal with. So you need something\nconsiderably more than that.",
    "start": "1615740",
    "end": "1622180"
  },
  {
    "text": "OK, so neither the axioms\nnor experimentation motivate this model. In other words, you really\nhave to use common sense.",
    "start": "1622180",
    "end": "1631940"
  },
  {
    "text": "You have to use judgment. And all of you have that. It's just that by learning\nall this mathematics, you",
    "start": "1631940",
    "end": "1640010"
  },
  {
    "text": "eventually start to think that\nmaybe you shouldn't use your common sense. So I have to keep saying that\nno, you keep on using your",
    "start": "1640010",
    "end": "1647830"
  },
  {
    "text": "common sense. You want to learn what these\nmodels are about. You want to use your\ncommon sense also.",
    "start": "1647830",
    "end": "1654475"
  },
  {
    "text": "And you've got to go back and\nforth between the two of them. OK, that's almost the end\nof our philosophy.",
    "start": "1654475",
    "end": "1662410"
  },
  {
    "text": "I guess one more slide. I'm getting tired\nof this stuff. Comparing models for similar\nsituations and analyzing",
    "start": "1662410",
    "end": "1669680"
  },
  {
    "text": "limited and effective\nmodels helps a lot in clarifying fuzziness.",
    "start": "1669680",
    "end": "1674810"
  },
  {
    "text": "But ultimately, as in\nall of science, some experimentation is needed. This is like any other\nbranch of science.",
    "start": "1674810",
    "end": "1681240"
  },
  {
    "text": "You need experimentation\nsometimes. You don't want to do too much of\nit because you'd always be",
    "start": "1681240",
    "end": "1688190"
  },
  {
    "text": "doing experiments. But the important thing is\nthat the outcome of an",
    "start": "1688190",
    "end": "1693600"
  },
  {
    "text": "experiment is a sample point. It's not a probability.",
    "start": "1693600",
    "end": "1698920"
  },
  {
    "text": "You do an experiment. You get an outcome. And all you find is one sample\npoint, if you do the",
    "start": "1698920",
    "end": "1704050"
  },
  {
    "text": "experiment once. And there's nothing that\nlets you draw a probability from that.",
    "start": "1704050",
    "end": "1709790"
  },
  {
    "text": "The only way you can get things\nthat you would call probabilities is to use an\nextended model, hope the",
    "start": "1709790",
    "end": "1716370"
  },
  {
    "text": "extended model corresponds to\nthe physical situation, and deal with these law of large\nnumbers kind of things.",
    "start": "1716370",
    "end": "1723970"
  },
  {
    "text": "You don't necessarily need\nIID random variables. But you need something that you\nknow about between a large",
    "start": "1723970",
    "end": "1730200"
  },
  {
    "text": "number of random variables\nto get from an outcome to something you could reasonably\ncall a probability.",
    "start": "1730200",
    "end": "1737700"
  },
  {
    "text": "OK, so that's enough. Let's go on to the law\nof large numbers.",
    "start": "1737700",
    "end": "1743200"
  },
  {
    "text": "Let's do it in pictures first. So you can lie back and relax\nfor a minute or stop being",
    "start": "1743200",
    "end": "1750820"
  },
  {
    "text": "bored by all this stuff. What I've done here is to take\nthe simplest random variable I",
    "start": "1750820",
    "end": "1757669"
  },
  {
    "text": "can think of, which as\nyou might guess is a binary random variable. It's either 0 or 1.",
    "start": "1757670",
    "end": "1764360"
  },
  {
    "text": "Here it's 1 with probability\n1/4 and 0 with probability 3/4.",
    "start": "1764360",
    "end": "1770260"
  },
  {
    "text": "I have actually calculated\nthese things. The distribution function of\nx1 plus x2 plus x3 plus x4,",
    "start": "1770260",
    "end": "1778490"
  },
  {
    "text": "this point down here,\nis the probability",
    "start": "1778490",
    "end": "1785390"
  },
  {
    "text": "of all 0's, I guess. And then you get the probability\nof all 0's plus 1,",
    "start": "1785390",
    "end": "1791060"
  },
  {
    "text": "1 and so forth. Here's where you take the sum\nof 20 random variables.",
    "start": "1791060",
    "end": "1797710"
  },
  {
    "text": "And you're looking at the\ndistribution function of the number of 1's that you get.",
    "start": "1797710",
    "end": "1803400"
  },
  {
    "text": "And it comes out like this. Here you're looking at s50. You're adding up 50\nrandom variables.",
    "start": "1803400",
    "end": "1809650"
  },
  {
    "text": "And what's happening as far\nas the gross picture is concerned here?",
    "start": "1809650",
    "end": "1815990"
  },
  {
    "text": "Well, the mean value of s sub\nn is the mean of a sum of",
    "start": "1815990",
    "end": "1821470"
  },
  {
    "text": "random variables. And that's equal to n times\na mean of a single random",
    "start": "1821470",
    "end": "1826820"
  },
  {
    "text": "variable when you have\nidentically distributed random variables or random variables\nthat have the same mean.",
    "start": "1826820",
    "end": "1833600"
  },
  {
    "text": "The variance is equal to\nn times sigma squared. Namely, when you take the\nexpected value of this",
    "start": "1833600",
    "end": "1846269"
  },
  {
    "text": "quantity squared, all these\ncross terms are going to balance out with the\nmean when you do.",
    "start": "1846270",
    "end": "1854360"
  },
  {
    "text": "I mean, all of you know how to\nfind the variance of s sub n. I hope you know how\nto find that.",
    "start": "1854360",
    "end": "1859559"
  },
  {
    "text": "And when you do that,\nit increases with n. And the mean increases with n.",
    "start": "1859560",
    "end": "1866059"
  },
  {
    "text": "The standard deviation, which\ngives you a picture of how wide the distribution is,\nonly goes up as the",
    "start": "1866060",
    "end": "1872809"
  },
  {
    "text": "square root of n. This is really the essence of\nthe weak law of large numbers.",
    "start": "1872810",
    "end": "1881009"
  },
  {
    "text": "I mean, everything else is\nmathematical detail. And then if you go on beyond\nthis and you talk about the",
    "start": "1881010",
    "end": "1892740"
  },
  {
    "text": "sample average, namely the sum\nof these n random variables-- assume them IID again.",
    "start": "1892740",
    "end": "1898970"
  },
  {
    "text": "In fact, assume for this picture\nthat they're the same binary random variables.",
    "start": "1898970",
    "end": "1904230"
  },
  {
    "text": "You look at the sample\naverage. You find the mean of\nthe sample average. And it's the mean of a single\nrandom variable.",
    "start": "1904230",
    "end": "1912400"
  },
  {
    "text": "You find the variance of it. Because of this n here and the\nsquaring that you're doing,",
    "start": "1912400",
    "end": "1917960"
  },
  {
    "text": "the variance of the sum divided\nby n, the sigma squared divided by n, what\nhappens as n gets large?",
    "start": "1917960",
    "end": "1927570"
  },
  {
    "text": "This variance goes to 0. What happens when you have a\nrandom variable, a sequence of",
    "start": "1927570",
    "end": "1933830"
  },
  {
    "text": "random variables, all of which\nhave the same mean and whose standard deviation\nis going to 0?",
    "start": "1933830",
    "end": "1941020"
  },
  {
    "text": "Well, you might play around with\na lot of funny kinds of things that you might think\nof as happening.",
    "start": "1941020",
    "end": "1947310"
  },
  {
    "text": "But essentially what's going\non here is the nice feature",
    "start": "1947310",
    "end": "1952810"
  },
  {
    "text": "that when you add all these\nthings up, the distribution function gets scrunched\ndown into a unit step.",
    "start": "1952810",
    "end": "1962250"
  },
  {
    "text": "In other words, since the\nstandard deviation is going to 0, the sequence of random\nvariables--",
    "start": "1962250",
    "end": "1969810"
  },
  {
    "text": "since they all have\nthe same mean-- they all have smaller and\nsmaller standard deviations.",
    "start": "1969810",
    "end": "1975690"
  },
  {
    "text": "The only way you can do that is\nto scrunch them down into a limiting random variable,\nwhich is deterministic.",
    "start": "1975690",
    "end": "1984210"
  },
  {
    "text": "And you can see that\nhappening here. Namely the largest value is\nthe black thing, which is",
    "start": "1984210",
    "end": "1992669"
  },
  {
    "text": "getting smaller and smaller. And the left side is\ngoing that way. On the right side, it's\ngoing that way.",
    "start": "1992670",
    "end": "1999830"
  },
  {
    "text": "So it looks like it's\napproaching a unit step. That has to be proven.",
    "start": "1999830",
    "end": "2005630"
  },
  {
    "text": "And there's a simple\nproof of it. And we'll see that. And you've all seen\nthat before. And you've all probably\nsaid, ho-hum.",
    "start": "2005630",
    "end": "2012413"
  },
  {
    "text": "But that's the way it is. Now the next thing to look at\nfor this same set of random",
    "start": "2012413",
    "end": "2019000"
  },
  {
    "text": "variables, the same sum, is\nyou look at the normalized sum, namely sn minus\nn times the mean.",
    "start": "2019000",
    "end": "2029470"
  },
  {
    "text": "And you divide that by the\nsquare root of n times sigma. And what do you get?",
    "start": "2029470",
    "end": "2035520"
  },
  {
    "text": "Well, every one of these\nrandom variables-- for every n has mean 0, has mean\n0 because the mean of sn",
    "start": "2035520",
    "end": "2043190"
  },
  {
    "text": "is n times x bar. So you're subtracting off\nof the mean essentially.",
    "start": "2043190",
    "end": "2048429"
  },
  {
    "text": "And every one of them\nhas variance 1. So you've got a whole sequence\nof random variables, which are",
    "start": "2048429",
    "end": "2056589"
  },
  {
    "text": "just sticking there at\nthe same mean, 0, and at the same variance.",
    "start": "2056590",
    "end": "2063040"
  },
  {
    "text": "What's extraordinary when you\ndo that, and you can sort of see this happening a little\nbit, this curve looks like",
    "start": "2063040",
    "end": "2070949"
  },
  {
    "text": "it's going into a fixed\ncurve, which starts out sticking to 0.",
    "start": "2070949",
    "end": "2079250"
  },
  {
    "text": "And then it gradually\ncomes up. And it looks fairly smooth. It goes off this way.",
    "start": "2079250",
    "end": "2085946"
  },
  {
    "text": "And if you read a lot about this\nor if you think that all",
    "start": "2085946",
    "end": "2094219"
  },
  {
    "text": "respectable random variables are\nGaussian random variables, and I hope at the end of this\ncourse you will realize that",
    "start": "2094219",
    "end": "2102019"
  },
  {
    "text": "only most respectable\nrandom variables are Gaussian random variables. There are many very interesting\nrandom variables",
    "start": "2102020",
    "end": "2108575"
  },
  {
    "text": "that aren't. But what the central limit\ntheorem says is that as you add up more and more random\nvariables and you look at this",
    "start": "2108575",
    "end": "2117040"
  },
  {
    "text": "normalized sum here, what you\nget is in fact the normal",
    "start": "2117040",
    "end": "2122850"
  },
  {
    "text": "distribution, which is this\nstrange integral here, that e to the minus x squared\nover 2 times the x.",
    "start": "2122850",
    "end": "2130470"
  },
  {
    "text": "Now what I want to do with the\nrest of our time is to show",
    "start": "2130470",
    "end": "2135900"
  },
  {
    "text": "you why in fact that happens. I've never seen this proof\nof the central",
    "start": "2135900",
    "end": "2142420"
  },
  {
    "text": "limit theorem before. I'm sure that some people\nhave done it. I'm only going to do it for\nthe case of a binomial",
    "start": "2142420",
    "end": "2152230"
  },
  {
    "text": "distribution, which is the only\nplace where this works. But I think in doing this you\nwill see why in fact that",
    "start": "2152230",
    "end": "2161280"
  },
  {
    "text": "strange e to the minus x squared\nover 2 comes up. It sure is not obvious by\nlooking at this problem.",
    "start": "2161280",
    "end": "2168500"
  },
  {
    "text": "OK, so that's what we're\ngoing to do. And I'm hoping that after you\nsee this, you will in fact",
    "start": "2168500",
    "end": "2175840"
  },
  {
    "text": "understand why the central limit\ntheorem is true as well as knowing that it's true.",
    "start": "2175840",
    "end": "2183440"
  },
  {
    "text": "OK, so let's look at the\nBernoulli process. You have a sequence of binary\nrandom variables, each of them",
    "start": "2183440",
    "end": "2192200"
  },
  {
    "text": "is IID, each of them is\n1 with probability p.",
    "start": "2192200",
    "end": "2197220"
  },
  {
    "text": "And a 0 with probability\nq equals 1 minus p. You add them all up.",
    "start": "2197220",
    "end": "2202770"
  },
  {
    "text": "They're IID. And the question is, what\ndoes the distribution of",
    "start": "2202770",
    "end": "2209890"
  },
  {
    "text": "the sum look like? Well, it has a nice\nformula to it.",
    "start": "2209890",
    "end": "2215620"
  },
  {
    "text": "It's that formula down there. You've probably seen that\nformula before. Let's get some idea\nof where it comes",
    "start": "2215620",
    "end": "2221950"
  },
  {
    "text": "from and what it means. Each n tuple that starts with\nk1's and then ends with n",
    "start": "2221950",
    "end": "2230240"
  },
  {
    "text": "minus k0's, each one of those\nhas the same probability.",
    "start": "2230240",
    "end": "2235869"
  },
  {
    "text": "And it's p to the k times\nq to the n minus k. In other words, the probability\nyou get a 1 on the",
    "start": "2235870",
    "end": "2242030"
  },
  {
    "text": "first toss is p. The probability you get a 1 on\nthe second toss also, since",
    "start": "2242030",
    "end": "2248740"
  },
  {
    "text": "those are independent,\nprobability you get two 1's in a row is p squared. Probably you get three 1's\nin a row is p cubed and",
    "start": "2248740",
    "end": "2256460"
  },
  {
    "text": "so forth up to k. Because we're looking at the\nprobability that the first k",
    "start": "2256460",
    "end": "2262280"
  },
  {
    "text": "outputs are 1, so the\nprobability of that is p to the k.",
    "start": "2262280",
    "end": "2267630"
  },
  {
    "text": "That's this term. And the probability that the\nrest of them are all 0's is q",
    "start": "2267630",
    "end": "2273690"
  },
  {
    "text": "to the n minus k.  And this is sometimes confusing\nto you because you",
    "start": "2273690",
    "end": "2281000"
  },
  {
    "text": "often think that this is going\nto be maximized when k is equal to p over n.",
    "start": "2281000",
    "end": "2286610"
  },
  {
    "text": "You have some strange view of\nthe law of large numbers. Well no, this quantity--",
    "start": "2286610",
    "end": "2292019"
  },
  {
    "text": "if p is less than 1/2, it's\ngoing to be largest at k equals zero.",
    "start": "2292020",
    "end": "2297280"
  },
  {
    "text": "The most probable single outcome\nfrom n tosses of a coin, and it's a biased\ncoin, it comes out 1's",
    "start": "2297280",
    "end": "2306830"
  },
  {
    "text": "more often than 0's.  0's are more probable\nthan 1's.",
    "start": "2306830",
    "end": "2314250"
  },
  {
    "text": "The most probable output\nis all 0's. Very improbable, but that's the\nmost probable of all these",
    "start": "2314250",
    "end": "2322130"
  },
  {
    "text": "improbable things. But as you probably know\nalready, there are n choose k",
    "start": "2322130",
    "end": "2330830"
  },
  {
    "text": "different n tuples, all of which\nhave k1's in them and n",
    "start": "2330830",
    "end": "2336470"
  },
  {
    "text": "minus k0's. If you don't know that,\nI didn't even",
    "start": "2336470",
    "end": "2341650"
  },
  {
    "text": "put that in the text. I put most things there. This is one of those basic\ncombinatorial facts.",
    "start": "2341650",
    "end": "2348500"
  },
  {
    "text": "Look it up in Wikipedia. You'll probably get a cleaner\nexplanation of it there than anywhere else.",
    "start": "2348500",
    "end": "2354000"
  },
  {
    "text": "But look it up in any elementary\nprobability book or in any elementary combinatorics\nbook.",
    "start": "2354000",
    "end": "2360780"
  },
  {
    "text": "I'm sure that all of you\nhave seen this stuff. So when you put this together,\nthe probability that the sum",
    "start": "2360780",
    "end": "2369020"
  },
  {
    "text": "of a n random variables, all\nof which are binary, the probability of getting k1's is\nn choose k times p to the k",
    "start": "2369020",
    "end": "2378730"
  },
  {
    "text": "times q to the n minus k. Now you look at that.",
    "start": "2378730",
    "end": "2384390"
  },
  {
    "text": "And if k is 1,000 and if n is\n1,000, I mean, your eyes",
    "start": "2384390",
    "end": "2389680"
  },
  {
    "text": "boggle because you can't\nimagine what that number looks like. So we want to find out\nwhat it looks like.",
    "start": "2389680",
    "end": "2396860"
  },
  {
    "text": "And here's a tricky\nway of doing it.  What we want to do is to see\nhow this varies with k.",
    "start": "2396860",
    "end": "2405550"
  },
  {
    "text": "And in particular, we want to\nsee how it varies with k when n is very large and when k is\nrelatively close to p times n.",
    "start": "2405550",
    "end": "2415130"
  },
  {
    "text": "So what we're going to do\nis take the ratio of the probability of k plus 1 1's\nto the ratio of k1's.",
    "start": "2415130",
    "end": "2423470"
  },
  {
    "text": "And what is that? I've written it out. n choose k, n choose\nk plus 1--",
    "start": "2423470",
    "end": "2430730"
  },
  {
    "text": "which is this term here-- is n factorial divided by k plus\n1 factorial times n minus",
    "start": "2430730",
    "end": "2437710"
  },
  {
    "text": "k minus 1 factorial. This term here-- you put the n factorial down\non the bottom, k factorial",
    "start": "2437710",
    "end": "2446050"
  },
  {
    "text": "times n minus k quantity\nfactorial.",
    "start": "2446050",
    "end": "2451550"
  },
  {
    "text": "And then you take the\np's and the q's. For this term here you have p\nto the k plus 1 q to the n",
    "start": "2451550",
    "end": "2459540"
  },
  {
    "text": "minus k minus 1. And for this one here you\nhave p to the k times p",
    "start": "2459540",
    "end": "2464619"
  },
  {
    "text": "to the n minus k. All that stuff cancels out,\nwhich is really cute.",
    "start": "2464620",
    "end": "2470119"
  },
  {
    "text": "When you look at this term you\nhave p to the k plus 1 over p to the k. That's just p.",
    "start": "2470120",
    "end": "2475400"
  },
  {
    "text": "And here you have q to the\nn minus k minus 1 over q to the n minus k.",
    "start": "2475400",
    "end": "2481090"
  },
  {
    "text": "That's just q in the\ndenominator. So this goes into p over q.",
    "start": "2481090",
    "end": "2486960"
  },
  {
    "text": "This quantity here is almost\na simple n factorial over n factorial is 1.",
    "start": "2486960",
    "end": "2492680"
  },
  {
    "text": "k plus 1 factorial divided by\nk factorial is k plus 1.",
    "start": "2492680",
    "end": "2497700"
  },
  {
    "text": "That's this term here. And the n minus k over n minus\nk minus 1 is n minus k.",
    "start": "2497700",
    "end": "2506270"
  },
  {
    "text": "So this ratio here\nis just that very simple expression there.",
    "start": "2506270",
    "end": "2512288"
  },
  {
    "text": "Now this ratio is strictly\ndecreasing in k.",
    "start": "2512289",
    "end": "2523270"
  },
  {
    "text": "How do I see that? Well, as k gets bigger and\nbigger, what happens? As k gets bigger, the numerator\ngets larger.",
    "start": "2523270",
    "end": "2531680"
  },
  {
    "text": "The denominator-- excuse me, as k gets larger,\nthe numerator gets smaller.",
    "start": "2531680",
    "end": "2539559"
  },
  {
    "text": "The denominator gets larger. So the ratio of the\ntwo gets smaller.",
    "start": "2539560",
    "end": "2548030"
  },
  {
    "text": "So this whole quantity here,\nas k gets larger and larger for fixed n, is just decreasing\nand decreasing and",
    "start": "2548030",
    "end": "2556810"
  },
  {
    "text": "decreasing. Now let's look a little bit at\nwhere this crosses 1, if it",
    "start": "2556810",
    "end": "2564670"
  },
  {
    "text": "does cross 1. And what I claim here is that\nwhen k plus 1 is less than or",
    "start": "2564670",
    "end": "2573280"
  },
  {
    "text": "equal to pn, what\nhappens here?",
    "start": "2573280",
    "end": "2580230"
  },
  {
    "text": "Well if I can do this--",
    "start": "2580230",
    "end": "2586140"
  },
  {
    "text": "I usually get confused\ndoing these things. But if k plus 1 is less than\nor equal to pn, this is the",
    "start": "2586140",
    "end": "2595600"
  },
  {
    "text": "last of these choices here, then\nk is also less than or",
    "start": "2595600",
    "end": "2600730"
  },
  {
    "text": "equal to pn. And therefore, n minus\nk is greater than--",
    "start": "2600730",
    "end": "2607320"
  },
  {
    "text": "in fact, k is strictly\nless than pn. And n minus k is strictly\ngreater than n minus pn.",
    "start": "2607320",
    "end": "2620970"
  },
  {
    "text": "And since q is 1 minus\np, this is n times q. OK, so you take this\ndivided by this.",
    "start": "2620970",
    "end": "2629490"
  },
  {
    "text": "And you take this\ndivided by this. And sure enough, this ratio\nhere is greater than 1 any",
    "start": "2629490",
    "end": "2636920"
  },
  {
    "text": "time you have a k which is\nsmaller than what you think k",
    "start": "2636920",
    "end": "2643710"
  },
  {
    "text": "ought to be, which\nis p over n. OK, so you have these three\nquantities here.",
    "start": "2643710",
    "end": "2650060"
  },
  {
    "text": "Let me go on to the\nnext slide. ",
    "start": "2650060",
    "end": "2655490"
  },
  {
    "text": "Since these ratios are less\nthan 1 when k is large,",
    "start": "2655490",
    "end": "2661619"
  },
  {
    "text": "approximately equal to 1 when k\nis close to pn, and greater",
    "start": "2661620",
    "end": "2667790"
  },
  {
    "text": "than 1 when it's smaller than\n1, if I plot these things, what I find is that as k is\nincreasing, getting closer and",
    "start": "2667790",
    "end": "2679390"
  },
  {
    "text": "closer to pn, it's getting\nlarger and larger. As k is increasing further,\ngetting larger than pn, this",
    "start": "2679390",
    "end": "2689560"
  },
  {
    "text": "ratio says that these\nthings have to be getting smaller and smaller.",
    "start": "2689560",
    "end": "2696670"
  },
  {
    "text": "So just from looking at this, we\nknow that these terms have to be increasing for terms less\nthan pn and have to be",
    "start": "2696670",
    "end": "2704819"
  },
  {
    "text": "decreasing for terms\ngreater than pn. So this is a bell-shaped\ncurve.",
    "start": "2704820",
    "end": "2710730"
  },
  {
    "text": "We've already seen that. It might not be quite clear that\nit's bell-shaped in the",
    "start": "2710730",
    "end": "2716790"
  },
  {
    "text": "sense that it kind of tapers\noff as you get smaller. Because these ratios are getting\nbigger and bigger, as",
    "start": "2716790",
    "end": "2724860"
  },
  {
    "text": "k gets bigger and bigger, the\nratio of this term to this term gets bigger and bigger.",
    "start": "2724860",
    "end": "2730550"
  },
  {
    "text": "So what's happening there? As this ratio gets bigger and\nbigger, these terms get",
    "start": "2730550",
    "end": "2736060"
  },
  {
    "text": "smaller and smaller. But as these terms get smaller\nand smaller, they're getting closer and closer to 0.",
    "start": "2736060",
    "end": "2742210"
  },
  {
    "text": "So even though they're going to\n0 like a bat out of hell,",
    "start": "2742210",
    "end": "2748180"
  },
  {
    "text": "they still can't get\nany smaller than 0. So they just taper down and\nstart to get close to 0.",
    "start": "2748180",
    "end": "2755330"
  },
  {
    "text": "So that is roughly how\nthis sum of binary",
    "start": "2755330",
    "end": "2763910"
  },
  {
    "text": "random variables behave. OK, so let's go on and show that\nthe central limit theorem",
    "start": "2763910",
    "end": "2773950"
  },
  {
    "text": "holds for the Bernoulli\nprocess. And that's just as\neasy really.",
    "start": "2773950",
    "end": "2779549"
  },
  {
    "text": "There's nothing more difficult\nabout it that we have to deal with.",
    "start": "2779550",
    "end": "2785430"
  },
  {
    "text": "This ratio, as we've said, is\nequal to n minus k over k plus 1 times p over k.",
    "start": "2785430",
    "end": "2792849"
  },
  {
    "text": "What we're interested\nin here-- I mean, we've already seen from\nthe last slide that the",
    "start": "2792850",
    "end": "2800730"
  },
  {
    "text": "interesting thing here\nis the big terms. And the big terms are the terms\nwhich are close to pn.",
    "start": "2800730",
    "end": "2807880"
  },
  {
    "text": "So what we'd like to do is look\nat values of k which are close to pn.",
    "start": "2807880",
    "end": "2813120"
  },
  {
    "text": "What I've done here is to plot\nthis as k minus the integer",
    "start": "2813120",
    "end": "2818580"
  },
  {
    "text": "value of pn. So we get integers. What I'm going to assume now,\nbecause this gets a little",
    "start": "2818580",
    "end": "2825030"
  },
  {
    "text": "hairy if I don't do that, I'm\ngoing to assume that pn is equal to an integer.",
    "start": "2825030",
    "end": "2830539"
  },
  {
    "text": "It doesn't make a whole lot of\ndifference to the argument. It just leaves out a lot\nof terms that you don't",
    "start": "2830540",
    "end": "2837040"
  },
  {
    "text": "have to play with. So we'll assume that\npn is an integer.",
    "start": "2837040",
    "end": "2842050"
  },
  {
    "text": "With this example, we're\nlooking at where p is equal to 1/4. Pn is going to be an integer\nwhenever n is a multiple of 4.",
    "start": "2842050",
    "end": "2850890"
  },
  {
    "text": "So things are fine then. If I try to make p equal to 1\nover pi, then that doesn't",
    "start": "2850890",
    "end": "2857360"
  },
  {
    "text": "work so well. But after all, no reason to\nchose p in such a strange way.",
    "start": "2857360",
    "end": "2865799"
  },
  {
    "text": "OK, so I'm going to look at this\nfor a fixed value of n. I'm going to look at it as k\nincreases for k less than pn.",
    "start": "2865800",
    "end": "2876270"
  },
  {
    "text": "I'm going to look at it\nas it decreases for k greater than pn. And I'm going to define k to\nbe equal to the i plus pn.",
    "start": "2876270",
    "end": "2888415"
  },
  {
    "text": "So I'm going to put the whole\nthing in terms of i instead of k.",
    "start": "2888415",
    "end": "2895470"
  },
  {
    "text": "OK, so when I substitute i\nequals k minus pn for k here,",
    "start": "2895470",
    "end": "2905030"
  },
  {
    "text": "what I'm going to get\nis this term. It's going to be the probability\nof pn plus i plus",
    "start": "2905030",
    "end": "2910700"
  },
  {
    "text": "1 over p of-- ",
    "start": "2910700",
    "end": "2917560"
  },
  {
    "text": "fortunately when you're using\ntextures, you can distinguish different kinds of p's. I have too many p's\nin this equation.",
    "start": "2917560",
    "end": "2924490"
  },
  {
    "text": "This is the probability\nmass function. This is just my probability\nof a 1.",
    "start": "2924490",
    "end": "2932180"
  },
  {
    "text": "And p's are things that you\nlike to use a lot in probability. So it's nice to have that\nseparation there.",
    "start": "2932180",
    "end": "2939440"
  },
  {
    "text": "OK, when I take this and I\nsubstitute it into that with k",
    "start": "2939440",
    "end": "2944900"
  },
  {
    "text": "equal to i, what I get is\nn minus pn minus i.",
    "start": "2944900",
    "end": "2950770"
  },
  {
    "text": "That's n minus k over pn plus\ni plus 1 times p over q.",
    "start": "2950770",
    "end": "2957570"
  },
  {
    "text": "Fair enough, OK, all I'm doing\nis replacing k with pn plus i",
    "start": "2957570",
    "end": "2964750"
  },
  {
    "text": "because I want i to be very\nclose to 0 in this argument. Because I've already seen that\nthese terms are only",
    "start": "2964750",
    "end": "2972440"
  },
  {
    "text": "significant when i is relatively\nclose to 0. Because when I get away from 0,\nthese terms are going down",
    "start": "2972440",
    "end": "2978600"
  },
  {
    "text": "very, very fast. So when I do that,\nwhat do I get?",
    "start": "2978600",
    "end": "2985260"
  },
  {
    "text": "I get n minus pn\nis equal to qn.",
    "start": "2985260",
    "end": "2991040"
  },
  {
    "text": "That's nice. So I have an nq here. I have a q here. I have a pn here.",
    "start": "2991040",
    "end": "2996870"
  },
  {
    "text": "I have a p here. I'm going to multiply\nthis p by n. I'm going to multiply\nthis q by n.",
    "start": "2996870",
    "end": "3002660"
  },
  {
    "text": "And I'm going to take a ratio\nof this pair of things. So when I take this ratio,\nI'm going to get nq over",
    "start": "3002660",
    "end": "3011170"
  },
  {
    "text": "nq, which is 1. ",
    "start": "3011170",
    "end": "3019589"
  },
  {
    "text": "And the other terms there\nbecome minus i over nq.",
    "start": "3019590",
    "end": "3026030"
  },
  {
    "text": "In the denominator, I'm going to\ndivide pn plus i plus 1 by",
    "start": "3026030",
    "end": "3033460"
  },
  {
    "text": "p, by pn, which gives me 1 plus\ni plus 1 divided by np.",
    "start": "3033460",
    "end": "3040320"
  },
  {
    "text": "So I get two terms, ratio of\ntwo terms, which are both close to 1 at this point and\nwhich are getting closer and",
    "start": "3040320",
    "end": "3049170"
  },
  {
    "text": "closer to 1 as n gets\nlarger and larger.",
    "start": "3049170",
    "end": "3054910"
  },
  {
    "text": "Now let's take the logarithm\nof this. Let me justify taking the\nlogarithm of it in two",
    "start": "3054910",
    "end": "3062670"
  },
  {
    "text": "different ways. One of them is that what\nwe're trying to prove--",
    "start": "3062670",
    "end": "3069359"
  },
  {
    "text": "and I'm playing the game\nthat all of you",
    "start": "3069360",
    "end": "3077130"
  },
  {
    "text": "always play in quizzes. When you're trying to prove\nsomething, what do you do? You start at the beginning. You work this way.",
    "start": "3077130",
    "end": "3082869"
  },
  {
    "text": "You start at the end. You work back this way. And you hope, at some point, the\ntwo things come together.",
    "start": "3082870",
    "end": "3089090"
  },
  {
    "text": "If they don't come together,\nyou get to this point. And you say, obviously. And then you go to that point\nwhich leads to-- yeah.",
    "start": "3089090",
    "end": "3095440"
  },
  {
    "text": "[LAUGHTER] PROFESSOR: OK, so I'm doing\nthe same thing here.",
    "start": "3095440",
    "end": "3102510"
  },
  {
    "text": "This probability that we're\ntrying to calculate-- well, I've listed it\nhere in terms of--",
    "start": "3102510",
    "end": "3110860"
  },
  {
    "text": "I have put it here in terms of\na distribution function.",
    "start": "3110860",
    "end": "3116600"
  },
  {
    "text": "I will do just as well if I can\ndo it in terms of an PMF. And what I'd like to show is\nthat the PMF of sn minus nX",
    "start": "3116600",
    "end": "3125270"
  },
  {
    "text": "bar over square root of n\ntimes sigma is somehow proportional to e to the\nminus x squared over 2.",
    "start": "3125270",
    "end": "3133740"
  },
  {
    "text": "Now if I want to do that, it\nwill be all right if I can take the logarithm of this\nterm and show that",
    "start": "3133740",
    "end": "3141700"
  },
  {
    "text": "it's a square nX.",
    "start": "3141700",
    "end": "3147660"
  },
  {
    "text": "And if I want to show that this\nlogarithm is a square nX, and I'm looking at the\ndifferentials at each time,",
    "start": "3147660",
    "end": "3155910"
  },
  {
    "text": "what are the differentials going\nto be if the sum of the differentials is quadratic?",
    "start": "3155910",
    "end": "3164510"
  },
  {
    "text": "If the sum of these\ndifferentials is quadratic, then the individual terms\nhave to be linear.",
    "start": "3164510",
    "end": "3170340"
  },
  {
    "text": " If I take a bunch of linear\nterms, if I add up 1 plus 2",
    "start": "3170340",
    "end": "3177490"
  },
  {
    "text": "plus 3 plus 4 plus 5, you've\nall done this I'm sure.",
    "start": "3177490",
    "end": "3183900"
  },
  {
    "start": "3183900",
    "end": "3190079"
  },
  {
    "text": "And down here you write n plus\nn minus 1 plus, plus 1.",
    "start": "3190080",
    "end": "3197960"
  },
  {
    "text": "And what do you get? You get n times n\nplus 1 over 2.",
    "start": "3197960",
    "end": "3205246"
  },
  {
    "text": "You can also approximate\nthat by integrating. Whenever you add up a sum\nof linear terms, you",
    "start": "3205246",
    "end": "3211559"
  },
  {
    "text": "get a square term. And I'm just curious. How many of you have\nseen that?",
    "start": "3211560",
    "end": "3219569"
  },
  {
    "text": "Good, OK. Well, it's only about 1/2. So it's something you've\nprobably seen in high school.",
    "start": "3219570",
    "end": "3227329"
  },
  {
    "text": "Or your haven't seen\nit at all. ",
    "start": "3227330",
    "end": "3233060"
  },
  {
    "text": "So let's go on with\nthis argument. ",
    "start": "3233060",
    "end": "3242250"
  },
  {
    "text": "OK, so I'm going to take\nthe logarithm of this expression here.",
    "start": "3242250",
    "end": "3248119"
  },
  {
    "text": "I'm going to take\nthe logarithm. I'm going to have the logarithm\nof 1 minus i over nq",
    "start": "3248120",
    "end": "3254070"
  },
  {
    "text": "minus the logarithm of 1\nplus i plus 1 over np.",
    "start": "3254070",
    "end": "3260690"
  },
  {
    "text": "And I'm going to use what I\nthink of as one of the most",
    "start": "3260690",
    "end": "3266410"
  },
  {
    "text": "useful inequalities that you\nwill ever see, which is the",
    "start": "3266410",
    "end": "3271750"
  },
  {
    "text": "natural log of 1 plus x. If we use a power expansion, we\nget x minus x squared over",
    "start": "3271750",
    "end": "3284450"
  },
  {
    "text": "2 plus x cubed over 3 minus--",
    "start": "3284450",
    "end": "3290160"
  },
  {
    "text": "it's an alternating series. If x is negative, this\nterm is negative.",
    "start": "3290160",
    "end": "3296630"
  },
  {
    "text": "This term is negative. This term is negative. And all this makes sense\nbecause if I draw this",
    "start": "3296630",
    "end": "3304059"
  },
  {
    "text": "function here, logarithm of\n1 plus x at x equals 0.",
    "start": "3304060",
    "end": "3310290"
  },
  {
    "text": "This is equal to 0. It comes up with a slope of 1.",
    "start": "3310290",
    "end": "3316420"
  },
  {
    "text": "And it levels off. And here it's going\ndown very fast.",
    "start": "3316420",
    "end": "3323870"
  },
  {
    "text": "So these terms, you get\nthese negative terms. And on the positive side, you\nget these alternating terms.",
    "start": "3323870",
    "end": "3332890"
  },
  {
    "text": "So this goes up slowly,\ndown fast. The slope here is x, which\nis this term here.",
    "start": "3332890",
    "end": "3339290"
  },
  {
    "text": "The curvature here gives you\nthe minus x squared over 2.",
    "start": "3339290",
    "end": "3345210"
  },
  {
    "text": "And the approximation, which is\nvery useful here, is that the logarithm of 1 plus x, when\nx is small, is equal to x",
    "start": "3345210",
    "end": "3356309"
  },
  {
    "text": "plus what we call\nlittle l of x. Namely something which goes\nto 0 faster than x",
    "start": "3356310",
    "end": "3363890"
  },
  {
    "text": "as x goes to 0. OK, all of you know\nthat, right?",
    "start": "3363890",
    "end": "3369630"
  },
  {
    "text": "Well, if you don't know\nit, now you know it. It's useful. You will use it again\nand again.",
    "start": "3369630",
    "end": "3376040"
  },
  {
    "text": "OK, so what we're\ngoing to do is-- ",
    "start": "3376040",
    "end": "3381589"
  },
  {
    "text": "well, that's pretty good. Where did I get to that point?",
    "start": "3381590",
    "end": "3387410"
  },
  {
    "text": " I skipped something.",
    "start": "3387410",
    "end": "3394630"
  },
  {
    "text": "What I have shown is that this\nincrement in the probability,",
    "start": "3394630",
    "end": "3402240"
  },
  {
    "text": "in the PMF for s sub n, namely\nthe increment as you increase i by 1, is linear in i.",
    "start": "3402240",
    "end": "3410810"
  },
  {
    "text": "And in fact, the logarithm of\nthis increment is linear in i. So therefore, by what I was\nsaying before, the logarithm",
    "start": "3410810",
    "end": "3418740"
  },
  {
    "text": "of the actual terms should be\nrather than linear in i, they should be quadratic in i.",
    "start": "3418740",
    "end": "3424680"
  },
  {
    "text": "So that's what I'm trying\nto do here. I just missed this\nwhole term here. What I'm interested in now is\ngetting a handle on pn plus",
    "start": "3424680",
    "end": "3434490"
  },
  {
    "text": "some larger value,\nj, divided by the probability of sn for pn.",
    "start": "3434490",
    "end": "3441000"
  },
  {
    "text": "What am I trying to do here? I should've said what\nI was trying to do.",
    "start": "3441000",
    "end": "3447390"
  },
  {
    "text": "This term is just one term. It's fixed. Be nice if we knew what it was,\nwe don't at the moment.",
    "start": "3447390",
    "end": "3455120"
  },
  {
    "text": "But I'm trying to express\neverything else in terms of that one unknown term.",
    "start": "3455120",
    "end": "3460890"
  },
  {
    "text": "And what I'm trying to do is to\nshow that the logarithm of this everything else is going\nto be quadratic in j.",
    "start": "3460890",
    "end": "3470970"
  },
  {
    "text": "And if I can do that, then I\nonly have one undetermined factor in this whole sum.",
    "start": "3470970",
    "end": "3476180"
  },
  {
    "text": "And I can use the fact that PMF\nsummed to 1 to solve the whole problem. So I'm going to express the\nprobability that we get pn",
    "start": "3476180",
    "end": "3487650"
  },
  {
    "text": "plus j1's divided by the\nprobability that we get pn1's.",
    "start": "3487650",
    "end": "3493069"
  },
  {
    "text": "It's the sum of the probability\npn plus i plus 1",
    "start": "3493070",
    "end": "3504440"
  },
  {
    "text": "over pn plus i. And we increase i. We start out at i equals 0.",
    "start": "3504440",
    "end": "3510060"
  },
  {
    "text": "And then the denominator is\nprobability of pn plus 0, which is this term.",
    "start": "3510060",
    "end": "3516900"
  },
  {
    "text": "And each time we increase i by\n1, this term cancels out with",
    "start": "3516900",
    "end": "3522109"
  },
  {
    "text": "the previous or the next\nvalue of this term. And when I get all done, all I\nhave is this expression here.",
    "start": "3522110",
    "end": "3531300"
  },
  {
    "text": "Everybody see that? OK, I see a lot of-- if you don't see it,\njust look at it.",
    "start": "3531300",
    "end": "3539470"
  },
  {
    "text": "And you'll see that this-- I think you'll see\nthat this works. OK, so now I take this\nexpression here.",
    "start": "3539470",
    "end": "3547700"
  },
  {
    "text": "This logarithm is this\nlinear term here. What do I want to do? I want to sum i from\n0 up to j minus 1.",
    "start": "3547700",
    "end": "3558210"
  },
  {
    "text": "What do I get when I sum\ni from 0 to j minus 1? I get this expression here.",
    "start": "3558210",
    "end": "3564330"
  },
  {
    "text": "I get j times j minus\n1 divided by 2n.",
    "start": "3564330",
    "end": "3569870"
  },
  {
    "text": "Oh, I was--  I skipped something.",
    "start": "3569870",
    "end": "3575180"
  },
  {
    "text": "Let's go back a little bit. Because it'll look like\nit was a typo. ",
    "start": "3575180",
    "end": "3583800"
  },
  {
    "text": "When I took this logarithm and\nI applied this approximation to it, I got minus i over nq\nminus i over np minus 1 over",
    "start": "3583800",
    "end": "3595160"
  },
  {
    "text": "np plus square terms in n. When I take i over nq minus i\nover np, I can combine those",
    "start": "3595160",
    "end": "3605020"
  },
  {
    "text": "two things together. I can take ip over npq\nminus iq times npq.",
    "start": "3605020",
    "end": "3615099"
  },
  {
    "text": "And q plus p is equal to 1. So the numerator\nall goes away.",
    "start": "3615100",
    "end": "3620309"
  },
  {
    "text": "And these two terms combine to\nbe minus i over n times p",
    "start": "3620310",
    "end": "3627230"
  },
  {
    "text": "times p times q. And I just has this one last\nlittle term left here.",
    "start": "3627230",
    "end": "3633490"
  },
  {
    "text": "Don't know what to\ndo with that. But then I add up\nall these terms. This one is the one that\nleads to j times j",
    "start": "3633490",
    "end": "3639690"
  },
  {
    "text": "minus 1 over 2 npq. This one is the one that\nleads to j over np.",
    "start": "3639690",
    "end": "3647300"
  },
  {
    "text": "And I just neglect this term,\nwhich is negligible compared to j squared.",
    "start": "3647300",
    "end": "3652920"
  },
  {
    "text": "I get minus j squared\nover 2 npq. ",
    "start": "3652920",
    "end": "3659440"
  },
  {
    "text": "Let me come back later to say\nwhy I'm so eager to neglect this term except that that's\nwhat I have to do if I want to",
    "start": "3659440",
    "end": "3667240"
  },
  {
    "text": "get the right answer. OK, so we'll see why that has\nto be negligible in just a",
    "start": "3667240",
    "end": "3673130"
  },
  {
    "text": "little bit. But now this logarithm is coming\nout to be exactly the",
    "start": "3673130",
    "end": "3679630"
  },
  {
    "text": "term that I want it to be. So finally, the logarithm of\nthe sum of these random",
    "start": "3679630",
    "end": "3689950"
  },
  {
    "text": "variables pn plus j, namely j\noff the mean, divided by that if pn is equal to minus j\nsquared over 2 npq plus some",
    "start": "3689950",
    "end": "3700420"
  },
  {
    "text": "negligible terms. And this says when I\nexponentiate things, that the probability that sn is j off\nthe mean is approximately",
    "start": "3700420",
    "end": "3711300"
  },
  {
    "text": "equal to this term, the\nprobability that it's right at the mean, times e to the minus\nj squared over 2 npq.",
    "start": "3711300",
    "end": "3718965"
  },
  {
    "text": " What that is saying is that this\nsum of terms that I was",
    "start": "3718965",
    "end": "3726020"
  },
  {
    "text": "looking at before--  this term here, this term, this\nterm, this term, this",
    "start": "3726020",
    "end": "3740070"
  },
  {
    "text": "term, and so forth down-- ",
    "start": "3740070",
    "end": "3750280"
  },
  {
    "text": "these terms here are actually\ngoing as minus j squared over",
    "start": "3750280",
    "end": "3757840"
  },
  {
    "text": "2 npq, which is what they should\nbe going as if you have a Gaussian curve here.",
    "start": "3757840",
    "end": "3764809"
  },
  {
    "text": "OK, now there's one other thing\nwe have to do, which is",
    "start": "3764810",
    "end": "3770140"
  },
  {
    "text": "figure out what this term is. And if you look at this as an\nundetermined coefficient on",
    "start": "3770140",
    "end": "3782470"
  },
  {
    "text": "these Gaussian-type terms and\nyou think of what happens if I sum this over all i, well, if\nI sum it over all i what I'm",
    "start": "3782470",
    "end": "3791710"
  },
  {
    "text": "going to get is the sum of all\nof these terms here, which are",
    "start": "3791710",
    "end": "3797490"
  },
  {
    "text": "negligible except where j\nsquared is proportional to n.",
    "start": "3797490",
    "end": "3804360"
  },
  {
    "text": "So I don't have to sum them\nbeyond the point where this approximation makes sense.",
    "start": "3804360",
    "end": "3809910"
  },
  {
    "text": "So I want to sum all\nthese terms. In summing these terms, when n\ngets very, very large, these",
    "start": "3809910",
    "end": "3816540"
  },
  {
    "text": "things are dropping off\nvery, very slowly. The curve is getting\nvery, very wide. If i scrunch the curve back in\nagain, what I get is a Riemann",
    "start": "3816540",
    "end": "3826940"
  },
  {
    "text": "approximation to a normal\ndensity curve. Therefore, I can integrate it.",
    "start": "3826940",
    "end": "3833890"
  },
  {
    "text": "And believe me. If you don't believe me,\nI'll go through it. And you won't like that.",
    "start": "3833890",
    "end": "3839720"
  },
  {
    "text": "When you go through this, what\nyou get is in fact this",
    "start": "3839720",
    "end": "3846750"
  },
  {
    "text": "expression right here, which\nsays that when n gets very,",
    "start": "3846750",
    "end": "3851930"
  },
  {
    "text": "very large and j is the offset\nfrom the mean and is",
    "start": "3851930",
    "end": "3860190"
  },
  {
    "text": "proportional to-- well, it's proportional to\nthe square root of n.",
    "start": "3860190",
    "end": "3865890"
  },
  {
    "text": "Then what I get is this PMF\nhere, which is in fact what the central limit\ntheorem says.",
    "start": "3865890",
    "end": "3872350"
  },
  {
    "text": "And now if you go back and try\nto think of exactly what we've done, what we've done is to\nshow that the logarithm of",
    "start": "3872350",
    "end": "3880109"
  },
  {
    "text": "these differences here is\nin fact linear in i. Therefore, when you sum them,\nyou get something which is",
    "start": "3880110",
    "end": "3887470"
  },
  {
    "text": "quadratic in j. And because of that, all you\nhave to do is normalize with a",
    "start": "3887470",
    "end": "3894000"
  },
  {
    "text": "center term. And you get this. The central limit theorem,\nespecially for the binary",
    "start": "3894000",
    "end": "3901690"
  },
  {
    "text": "case, is almost always done\nby using a Stirling approximation.",
    "start": "3901690",
    "end": "3907040"
  },
  {
    "text": "And a Stirling approximation is\none of these things which is black magic.",
    "start": "3907040",
    "end": "3912740"
  },
  {
    "text": "I don't know any place except in\nWilliam Feller's book where anyone talks about where this\nformula comes from.",
    "start": "3912740",
    "end": "3921240"
  },
  {
    "text": "If you now go back and look\nvery carefully at this derivation, this tells\nyou what the Stirling",
    "start": "3921240",
    "end": "3927390"
  },
  {
    "text": "approximation is. Because if you do this for p\nequals q, what you're doing is",
    "start": "3927390",
    "end": "3934210"
  },
  {
    "text": "actually evaluating n choose\nk where k is very",
    "start": "3934210",
    "end": "3939650"
  },
  {
    "text": "close to and over 2. And that will tell you exactly\nwhat Stirling's",
    "start": "3939650",
    "end": "3945170"
  },
  {
    "text": "approximation has to be. In other words, that's a way\nof deriving Stirling's approximation.",
    "start": "3945170",
    "end": "3950960"
  },
  {
    "text": "The very backward way of\ndoing things it seems. But often backward ways\nare the best ways",
    "start": "3950960",
    "end": "3957330"
  },
  {
    "text": "of doing these things. OK, so I told you I\nwould stop at some",
    "start": "3957330",
    "end": "3965210"
  },
  {
    "text": "point and ask for questions. Yes? AUDIENCE: Can you please go\nback one slide before this slide where can you neglect\na term, which [INAUDIBLE],",
    "start": "3965210",
    "end": "3975695"
  },
  {
    "text": "minus j over np. PROFESSOR: Why did I neglect\nthe j over np?",
    "start": "3975695",
    "end": "3982400"
  },
  {
    "text": "OK, that's a good question. If you look at this curve here,\nand I put the j in.",
    "start": "3982400",
    "end": "3994760"
  },
  {
    "text": "I can put the j in by just\nmaking this expression here look at one smaller value of\nj or one larger value of j.",
    "start": "3994760",
    "end": "4004359"
  },
  {
    "text": "And you get something different\nwhether you're looking at the minus side\nor the plus side. In fact, if p is equal to q,\nthis term cancels out.",
    "start": "4004360",
    "end": "4013849"
  },
  {
    "text": "If p is not equal to q, what\nhappens is that the central limit theorem is approximately\nsymmetric.",
    "start": "4013850",
    "end": "4021940"
  },
  {
    "text": "But in this first ordered term, it's not quite symmetric. It can't be symmetric because\nthis is p times n.",
    "start": "4021940",
    "end": "4030090"
  },
  {
    "text": "And you have all these\nterms out to 1. And you have many, many\nfewer terms back to 0.",
    "start": "4030090",
    "end": "4035690"
  },
  {
    "text": "So it has to be slightly\nasymmetric. But it's only asymmetric over at\nmost a unit of value here,",
    "start": "4035690",
    "end": "4045230"
  },
  {
    "text": "which is not significant. Because as n gets bigger,\nthese terms-- well, as I've done\nit, the terms do",
    "start": "4045230",
    "end": "4052060"
  },
  {
    "text": "not get close together. But if I want to think of it\nas a normalized Gaussian",
    "start": "4052060",
    "end": "4057820"
  },
  {
    "text": "curve, I have to make the\nterms close together. So that extra term is\nnot significant.",
    "start": "4057820",
    "end": "4063580"
  },
  {
    "text": "I wish I had a nicer way of\ntaking care of all the approximations here.",
    "start": "4063580",
    "end": "4068740"
  },
  {
    "text": "I haven't put this in the notes\nbecause I still haven't figured out how to do that.",
    "start": "4068740",
    "end": "4074640"
  },
  {
    "text": "But I still think you get more\ninsight from doing it this way",
    "start": "4074640",
    "end": "4080309"
  },
  {
    "text": "than you do by going through\nStirling's approximation, all those pages and pages\nof algebra.",
    "start": "4080310",
    "end": "4087609"
  },
  {
    "text": "Anything else? ",
    "start": "4087610",
    "end": "4094020"
  },
  {
    "text": "OK, well, see you\nWednesday then. ",
    "start": "4094020",
    "end": "4099050"
  }
]