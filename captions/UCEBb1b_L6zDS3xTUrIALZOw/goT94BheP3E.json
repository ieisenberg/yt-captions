[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13960",
    "end": "19790"
  },
  {
    "text": "ocw.mit.edu.  PROFESSOR: OK, I guess we might\nas well start a minute",
    "start": "19790",
    "end": "26220"
  },
  {
    "text": "early since those of you\nwho are here are here. ",
    "start": "26220",
    "end": "32580"
  },
  {
    "text": "We're coming to the\nend of course. We're deep in chapter 7 now\ntalking about random walks and",
    "start": "32580",
    "end": "41630"
  },
  {
    "text": "detection theory. We'll get into martingales\nsometime next week.",
    "start": "41630",
    "end": "48450"
  },
  {
    "text": "There are four more lectures\nafter this one. The schedule was passed out at\nthe beginning of the term.",
    "start": "48450",
    "end": "55290"
  },
  {
    "text": "I don't know how I did it, but\nI somehow left off the last Wednesday of class.",
    "start": "55290",
    "end": "62860"
  },
  {
    "text": "The final is going to\nbe on Wednesday morning at the ice rink. I don't know what the\nice rink is like.",
    "start": "62860",
    "end": "68259"
  },
  {
    "text": "It doesn't sound like an ideal\nplace to take a final, but I",
    "start": "68260",
    "end": "74140"
  },
  {
    "text": "assume they must have desks\nthere and all that stuff. ",
    "start": "74140",
    "end": "79859"
  },
  {
    "text": "We will send out a notice\nabout that. This is the last homework set\nthat you will have to turn in.",
    "start": "79860",
    "end": "87710"
  },
  {
    "text": "We will probably have another\nset of practice problems and",
    "start": "87710",
    "end": "94550"
  },
  {
    "text": "problems on-- but not things you\nshould turn in.",
    "start": "94550",
    "end": "100870"
  },
  {
    "text": "We will try to get solutions\nout on them fairly quickly, also. So you can do them, but also\nlook at the answers right",
    "start": "100870",
    "end": "109320"
  },
  {
    "text": "after you do them. OK, so let's get back\nto random walks.",
    "start": "109320",
    "end": "115550"
  },
  {
    "text": "And remember what we were\ndoing last time. A random walk, by definition,\nyou have a sequence of IID",
    "start": "115550",
    "end": "124720"
  },
  {
    "start": "120000",
    "end": "320000"
  },
  {
    "text": "random variables. You have partial sums of\nthose random variables.",
    "start": "124720",
    "end": "130360"
  },
  {
    "text": "S sub n is a sum of\nthe first n of those IID random variables.",
    "start": "130360",
    "end": "135690"
  },
  {
    "text": "And the sequence of partial sums\nS1, S2, S3, and so forth,",
    "start": "135690",
    "end": "141790"
  },
  {
    "text": "that sequence is called\na random walk. And if you graph the random\nwalk, it's something which",
    "start": "141790",
    "end": "147860"
  },
  {
    "text": "wanders up and down usually. And sometimes, if the mean of X\nis positive, it wanders off",
    "start": "147860",
    "end": "154840"
  },
  {
    "text": "to infinity. If the mean of X is negative,\nit wanders off to minus infinity.",
    "start": "154840",
    "end": "160225"
  },
  {
    "text": "If the mean of X is 0, it simply\ndiffuses somewhat as time goes on.",
    "start": "160225",
    "end": "166120"
  },
  {
    "text": "And what we're trying to find\nthat is exactly how do these things work.",
    "start": "166120",
    "end": "171820"
  },
  {
    "text": "So our focus here is\ngoing to be on threshold-crossing problems. Namely, what's the probability\nthat this random walk is going",
    "start": "171820",
    "end": "181340"
  },
  {
    "text": "to cross some threshold by or at\nsome particular value of n?",
    "start": "181340",
    "end": "188110"
  },
  {
    "text": "If you have two thresholds,\none above and one below, what's the probability it's\ngoing to cross the one above?",
    "start": "188110",
    "end": "193959"
  },
  {
    "text": "What's the probability it's\ngoing to cross the one below? And if it crosses one of these,\nwhen does it cross it?",
    "start": "193960",
    "end": "201200"
  },
  {
    "text": "If it crosses it, how much\nof an overshoot is there?",
    "start": "201200",
    "end": "206739"
  },
  {
    "text": "All of those problems just come\nin naturally by looking at a sum of IID random\nvariables.",
    "start": "206740",
    "end": "212160"
  },
  {
    "text": "But here we're going to be\ntrying to study them in some consistent manner looking at the\nthresholds particularly.",
    "start": "212160",
    "end": "220500"
  },
  {
    "text": "We've talked a little bit\nabout two particularly",
    "start": "220500",
    "end": "225650"
  },
  {
    "text": "important applications. One is [? GG1Qs ?]. And even far more important than\nthat is this question of",
    "start": "225650",
    "end": "234760"
  },
  {
    "text": "detection, or making decisions,\nor hypothesis testing, all of which\nare the same thing.",
    "start": "234760",
    "end": "241880"
  },
  {
    "text": "You remember we did show that\nthere was at least one threshold-crossing problem\nthat was very, very easy.",
    "start": "241880",
    "end": "249980"
  },
  {
    "text": "It's the threshold problem where\nthe underlying random variable is binary.",
    "start": "249980",
    "end": "256640"
  },
  {
    "text": "You either go up by 1 or you\ngo down by 1 on each step. And the question is, what's the\nprobability that you will",
    "start": "256640",
    "end": "264040"
  },
  {
    "text": "cross some threshold at\nsome k greater than 0?",
    "start": "264040",
    "end": "269570"
  },
  {
    "text": "And it turns out that since\nyou can only go up 1 each time, the probability of getting\nup to some point k is",
    "start": "269570",
    "end": "276620"
  },
  {
    "text": "the probability you\never got up to 1. Given that you got up to 1, it's\nthe probability that you",
    "start": "276620",
    "end": "281910"
  },
  {
    "text": "ever got up to 2. Given you got up to 2, it's\nthe probability you ever got up to 3. That doesn't mean that you\ngo directly from 2 to 3.",
    "start": "281910",
    "end": "289400"
  },
  {
    "text": "After you go to 2, you wander\nall around, and eventually you make it up to 3.",
    "start": "289400",
    "end": "294510"
  },
  {
    "text": "If you do, then the question is,\ndo you ever get from 3 to 4, and so forth.",
    "start": "294510",
    "end": "299580"
  },
  {
    "text": "And we found that the solution\nto that problem was p over 1 minus p to the k-th power of p\nis less than or equal to 1/2.",
    "start": "299580",
    "end": "308110"
  },
  {
    "text": "And we solved that problem, if\nyou remember, back when we were talking about stop when\nyou're ahead if you're playing",
    "start": "308110",
    "end": "315419"
  },
  {
    "text": "coin tossing with somebody. And so let's go further and\nlook particularly at this",
    "start": "315420",
    "end": "326180"
  },
  {
    "start": "320000",
    "end": "420000"
  },
  {
    "text": "problem of detection, and\ndecisions, and hypothesis testing, which is really not a\nparticularly hard problem.",
    "start": "326180",
    "end": "333720"
  },
  {
    "text": "But it's made particularly hard\nby statisticians who have so many special rules, peculiar\ncases, and almost",
    "start": "333720",
    "end": "344870"
  },
  {
    "text": "mythology about making\ndecisions. And you can imagine why because\nas long as you talk",
    "start": "344870",
    "end": "353200"
  },
  {
    "text": "about probability, everybody\nknows you're talking about an abstraction.",
    "start": "353200",
    "end": "358440"
  },
  {
    "text": "As soon as you start talking\nabout making a decision, it suddenly becomes real.",
    "start": "358440",
    "end": "364910"
  },
  {
    "text": "I mean, you look at a\nbunch of data and you have to do something. You look at a bunch of\ncandidates for a job, you have",
    "start": "364910",
    "end": "372210"
  },
  {
    "text": "to choose one. That's always very difficult\nbecause you might not choose the right one.",
    "start": "372210",
    "end": "377270"
  },
  {
    "text": "You might choose a\nvery poor one. But you have to do your best. If you're investing in stocks,\nyou look at all the statistics",
    "start": "377270",
    "end": "385259"
  },
  {
    "text": "of everything. And finally you say,\nthat's where I'm going to put my money. Or if you're looking for a job\nyou say, that's where I'm",
    "start": "385260",
    "end": "392340"
  },
  {
    "text": "going to work, and you\nhope that that's going to work out well. There are all these situations\nwhere you can evaluate",
    "start": "392340",
    "end": "398790"
  },
  {
    "text": "probabilities until you're\nsick in the head. They don't mean anything.",
    "start": "398790",
    "end": "404810"
  },
  {
    "text": "It's only when you make a\ndecision and actually do something with it that it\nreally means something.",
    "start": "404810",
    "end": "410670"
  },
  {
    "text": "So it becomes important\nat this point. The model we use for this,\nsince we're studying",
    "start": "410670",
    "end": "418240"
  },
  {
    "text": "probability theory-- well, actually, we're studying\nrandom processes.",
    "start": "418240",
    "end": "423360"
  },
  {
    "start": "420000",
    "end": "645000"
  },
  {
    "text": "But we're really studying\nprobability theory. You probably noticed\nthat by now.",
    "start": "423360",
    "end": "429360"
  },
  {
    "text": "Since we're studying\nprobability, we study all these problems in terms of\na probabilistic model.",
    "start": "429360",
    "end": "436530"
  },
  {
    "text": "And in the probabilistic model,\nthere's a discrete and, in most cases, binary random\nvariable, H, which is called",
    "start": "436530",
    "end": "445880"
  },
  {
    "text": "the hypothesis random\nvariable. The sample values of H,\nyou might as well",
    "start": "445880",
    "end": "451060"
  },
  {
    "text": "call them 0 and 1. That's the easiest things\nto call binary things.",
    "start": "451060",
    "end": "456460"
  },
  {
    "text": "They're called the alternative\nhypotheses. They have marginal probabilities\nbecause it's a",
    "start": "456460",
    "end": "462350"
  },
  {
    "text": "probability model. You have a random variable. It can only take on the value\n0 and 1, so it has to have",
    "start": "462350",
    "end": "469020"
  },
  {
    "text": "probabilities of\nbeing 0 and 1. Along with that, there\nare all sorts of other random variables.",
    "start": "469020",
    "end": "474880"
  },
  {
    "text": "The situation might be as\ncomplicated as you want. But since we're making\ndecisions, we're making",
    "start": "474880",
    "end": "480770"
  },
  {
    "text": "decisions on the basis of some\nset of alternatives. And here, since we're trying\nto talk about random walks,",
    "start": "480770",
    "end": "487700"
  },
  {
    "text": "and martingales, and things like\nthat, also we restrict our attention to particular\nkinds of observations.",
    "start": "487700",
    "end": "494960"
  },
  {
    "text": "And the particular kind of\nobservation that we restrict attention to here is a sequence\nof random variables,",
    "start": "494960",
    "end": "503160"
  },
  {
    "text": "which we call the observation. You observe Y1. You observe Y2. You observe Y3, and so forth.",
    "start": "503160",
    "end": "509460"
  },
  {
    "text": "In other words, you observe a\nsample value of each of those random variables.",
    "start": "509460",
    "end": "514770"
  },
  {
    "text": "There are a whole sequence\nof them. And we assume, to make life\nsimple for ourselves, that",
    "start": "514770",
    "end": "520409"
  },
  {
    "text": "each of these are independent,\nconditional on the hypothesis. And they're identically\ndistributed conditional on the",
    "start": "520409",
    "end": "527250"
  },
  {
    "text": "hypothesis. That's what this says\nright here.",
    "start": "527250",
    "end": "534150"
  },
  {
    "text": "This makes one more assumption\nthat assumes that these observations are continuous\nrandom variables.",
    "start": "534150",
    "end": "540040"
  },
  {
    "text": "That doesn't make much\ndifference, there are just a few peculiarities that\ncome in if these are",
    "start": "540040",
    "end": "545790"
  },
  {
    "text": "discrete random variables. There also a few peculiarities\nthat come in when they're continuous.",
    "start": "545790",
    "end": "551529"
  },
  {
    "text": "And there are a lot of\npeculiarities that come in when they're absolutely\narbitrary. But for the time being, just\nimagine each of these are",
    "start": "551530",
    "end": "558779"
  },
  {
    "text": "continuous random variables. So for each value of n, we\nlook at n observations.",
    "start": "558780",
    "end": "566060"
  },
  {
    "text": "We can calculate the probability\ndensity that those observations would occur\nconditional on hypothesis 0.",
    "start": "566060",
    "end": "575730"
  },
  {
    "text": "We can find the conditional\nprobability they could occur conditional on hypothesis 1.",
    "start": "575730",
    "end": "581700"
  },
  {
    "text": "And since they're IID, that's\nequal to this product here.",
    "start": "581700",
    "end": "587460"
  },
  {
    "text": "Excuse me, they are not IID,\nthey are conditionally ID. Conditional on the hypothesis.",
    "start": "587460",
    "end": "594500"
  },
  {
    "text": "Namely, the idea is the world\nis one way or the world is another way. If the world is this way,\nthen all of these",
    "start": "594500",
    "end": "601880"
  },
  {
    "text": "hypotheses are IID. You're doing the same experiment\nagain and again and again, but it's based on the\nsame underlying hypothesis.",
    "start": "601880",
    "end": "610590"
  },
  {
    "text": "Or, the underlying hypothesis\nis this over here. You make the number of\nobservations all based on this",
    "start": "610590",
    "end": "618470"
  },
  {
    "text": "same hypothesis, and you make\nas many of these IID observations conditional\non that",
    "start": "618470",
    "end": "625330"
  },
  {
    "text": "observation as you choose. And when you're all done,\nwhat do you do? You have to make\nyour decision.",
    "start": "625330",
    "end": "631140"
  },
  {
    "text": "OK, so this is a very\nsimple-minded model of this very complicated and very\nimportant problem.",
    "start": "631140",
    "end": "638240"
  },
  {
    "text": "But it's close enough to the\ntruth that we can get a lot of observations from it.",
    "start": "638240",
    "end": "644190"
  },
  {
    "text": "Now, I spent a lot last time\ntalking about this. Spend a lot of time this time\ntalking about it because when",
    "start": "644190",
    "end": "651810"
  },
  {
    "start": "645000",
    "end": "925000"
  },
  {
    "text": "we use a probability model for\nthis, when we say that we're studying probability theory.",
    "start": "651810",
    "end": "657740"
  },
  {
    "text": "And therefore, we're going to\nuse probability, we have suddenly allied ourselves\ncompletely with people called",
    "start": "657740",
    "end": "664899"
  },
  {
    "text": "Bayesian statisticians or\nBayesian probabilists.",
    "start": "664900",
    "end": "669980"
  },
  {
    "text": "And we have gone against, turned\nour back on people called Non-Bayesians, or\nsometimes classical.",
    "start": "669980",
    "end": "677150"
  },
  {
    "text": "I hate using the word\n\"classical\" because I like the word \"classics.\" I like the\nclassics for such an unusual",
    "start": "677150",
    "end": "685120"
  },
  {
    "text": "point of view. And the unusual point of view\nis that we refuse to take a probability model.",
    "start": "685120",
    "end": "691840"
  },
  {
    "text": "We accept the fact that on all\nthe observations, all the observations are\nprobabilistic.",
    "start": "691840",
    "end": "697370"
  },
  {
    "text": "We assume we have a nice model\nfor them, which makes sense. We can do whatever we want\nwith that model.",
    "start": "697370",
    "end": "702820"
  },
  {
    "text": "We can change the model. We can do whatever we\nwant with a model. But if you once assume that\nthese two hypotheses that",
    "start": "702820",
    "end": "709870"
  },
  {
    "text": "you're trying to choose between,\nthat they have a priori probabilities, then\npeople get very upset about it",
    "start": "709870",
    "end": "717490"
  },
  {
    "text": "because they say, well,\nif what the a priori probabilities are, why do you\nhave to do a hypothesis test?",
    "start": "717490",
    "end": "723639"
  },
  {
    "text": "You already understand\neverything there is to know about the problem. And they feel this\nis very strange.",
    "start": "723640",
    "end": "728835"
  },
  {
    "text": " It's not strange because you\nuse probability models.",
    "start": "728835",
    "end": "735820"
  },
  {
    "text": "You use models to try to\nunderstand certain things about reality. And you assume as many\nthings as you want to",
    "start": "735820",
    "end": "741860"
  },
  {
    "text": "assume about it. And when you get all done, you\neither use all the assumptions or you don't use them.",
    "start": "741860",
    "end": "747320"
  },
  {
    "text": "What we're going to find today\nis that when you use this assumption of a probability\nmodel, you can answer the",
    "start": "747320",
    "end": "756380"
  },
  {
    "text": "questions that these classical\nstatisticians go to great pains to answer.",
    "start": "756380",
    "end": "761510"
  },
  {
    "text": "And you can ask them\nvery, very simply. So that after we assume the a\npriori probabilities, we can",
    "start": "761510",
    "end": "768160"
  },
  {
    "text": "calculate certain things which\ndon't depend on those a priori probabilities.",
    "start": "768160",
    "end": "773860"
  },
  {
    "text": "And therefore, we\nknow two things. One, we know that if we\ndid know the a priori probabilities, it wouldn't\nmake any difference.",
    "start": "773860",
    "end": "782290"
  },
  {
    "text": "And two, we know that if we\ncan estimate the a priori probabilities, it makes a great\ndeal of difference.",
    "start": "782290",
    "end": "788709"
  },
  {
    "text": "And three-- and this is the most\nimportant point-- you make 100 observations\nof something.",
    "start": "788710",
    "end": "797110"
  },
  {
    "text": "Somebody else says, I don't\nbelieve you, and comes in and makes another 100\nobservations.",
    "start": "797110",
    "end": "802530"
  },
  {
    "text": "Somebody else makes another\n100 observations. Now, even if the second person\ndoesn't believe what the first",
    "start": "802530",
    "end": "809580"
  },
  {
    "text": "person has done, it doesn't make\nsense as a scientist to completely eliminate all of\nthat from consideration.",
    "start": "809580",
    "end": "818800"
  },
  {
    "text": "Namely, what you would like to\ndo is say well, since this person has found such and\nsuch, the a priori",
    "start": "818800",
    "end": "826190"
  },
  {
    "text": "probabilities have changed. And then I can go on and make\nmy 100 observations.",
    "start": "826190",
    "end": "833670"
  },
  {
    "text": "I can either make a hypothesis\ntest based on my 100 observations or I can make a\nhypothesis test assuming that",
    "start": "833670",
    "end": "842090"
  },
  {
    "text": "the other person did\ntheir work well. I can make it based on all\nof these observations.",
    "start": "842090",
    "end": "847640"
  },
  {
    "text": "If you try to do that those\ntwo things in a classical formulation, you run into\na lot of trouble.",
    "start": "847640",
    "end": "853800"
  },
  {
    "text": "If you try to do them in this\nprobabilistic formulation, it's all perfectly\nstraightforward.",
    "start": "853800",
    "end": "858810"
  },
  {
    "text": "Because you can either start\nout with a model in which you're taking 200 observations\nor you can start out with a",
    "start": "858810",
    "end": "865490"
  },
  {
    "text": "model in which you take\n100 observations. And then suddenly, the\nworld changes. This hypothesis takes on,\nperhaps a different value.",
    "start": "865490",
    "end": "873930"
  },
  {
    "text": "You take another hundred\nobservations. So you do whatever you want\nto within a probabilistic",
    "start": "873930",
    "end": "879149"
  },
  {
    "text": "formulation. But the other thing is, all of\nyou that patiently have lived",
    "start": "879150",
    "end": "887250"
  },
  {
    "text": "with this idea of studying\nprobabilistic models all term long.",
    "start": "887250",
    "end": "893160"
  },
  {
    "text": "You might as well keep\non living with it. The fact that we're now\ninterested in making decisions",
    "start": "893160",
    "end": "900709"
  },
  {
    "text": "should not make you think that\neverything you've learned up until this point is baloney.",
    "start": "900710",
    "end": "906580"
  },
  {
    "text": "And to move from here to\na classical statistical formulation of the world would\nreally be saying, I don't",
    "start": "906580",
    "end": "913519"
  },
  {
    "text": "believe in probability theory. It's that bad. So here we go.",
    "start": "913520",
    "end": "918760"
  },
  {
    "start": "918760",
    "end": "923850"
  },
  {
    "text": "I'm sorry, we did that. We were there. Assume that on the basis of\nobserving a sample value of",
    "start": "923850",
    "end": "933500"
  },
  {
    "start": "925000",
    "end": "1630000"
  },
  {
    "text": "this sequence of observations,\nwe have to make a decision about H. We have to choose\nH equals 0 or H equals 1.",
    "start": "933500",
    "end": "942060"
  },
  {
    "text": "We have to detect whether\nor not H is 1. When you do this detection, you\nwould think in the real",
    "start": "942060",
    "end": "948750"
  },
  {
    "text": "world that you've detected\nsomething. If you've made a decision about\nsomething, that you've",
    "start": "948750",
    "end": "954330"
  },
  {
    "text": "tested a hypothesis and you\nfound that which is correct. Not at all.",
    "start": "954330",
    "end": "960460"
  },
  {
    "text": "When you make decisions,\nyou can make errors. And the question of what kinds\nof errors you're making is a",
    "start": "960460",
    "end": "966699"
  },
  {
    "text": "major part of trying\nto make decisions. I mean, those people who make\ndecisions and then can't",
    "start": "966700",
    "end": "974620"
  },
  {
    "text": "believe that they might have\nmade the wrong decision are the worst kind of fools.",
    "start": "974620",
    "end": "980210"
  },
  {
    "text": "And you see them in politics. You see them in business. You see them in academia. You see them all\nover the place.",
    "start": "980210",
    "end": "987280"
  },
  {
    "text": "When you make a decision and\nyou've made a mistake, you get some more evidence. You see that it's a mistake\nand you change.",
    "start": "987280",
    "end": "993890"
  },
  {
    "text": "The whole 19th century\nwas taken up with-- I mean, the scientific community\nwas driven by",
    "start": "993890",
    "end": "1001759"
  },
  {
    "text": "physicists in those days. And the idea of Newton's\nlaws was the most",
    "start": "1001760",
    "end": "1007490"
  },
  {
    "text": "sacred thing they had. ",
    "start": "1007490",
    "end": "1013410"
  },
  {
    "text": "Everybody believed in Newtonian mechanics in those days. When quantum mechanics came\nalong, this wasn't just a",
    "start": "1013410",
    "end": "1021500"
  },
  {
    "text": "minor perturbation in physics. This was a most crucial thing.",
    "start": "1021500",
    "end": "1027069"
  },
  {
    "text": "This said, everything we've\nknown goes out the window. We can't rely on anything\nanymore.",
    "start": "1027069",
    "end": "1033069"
  },
  {
    "text": "But the physicists said, OK,\nI guess we made a mistake. We'll make new observations.",
    "start": "1033069",
    "end": "1039389"
  },
  {
    "text": "We have new observations\nthat can be made. We now see that Newtonian\nmechanics works over a certain",
    "start": "1039390",
    "end": "1045420"
  },
  {
    "text": "range of things. It doesn't work in another\nranges of things. And they go on and\nfind new things.",
    "start": "1045420",
    "end": "1051180"
  },
  {
    "text": "That's the same thing\nwe do here. We take these models. We evaluate our error\nprobabilities.",
    "start": "1051180",
    "end": "1056820"
  },
  {
    "text": "And evaluating them, we then\nsay, well, we've got to go on and take some more\nmeasurements. Or we say we're going\nto live with it.",
    "start": "1056820",
    "end": "1063200"
  },
  {
    "text": "But we face the fact that there\nare errors involved. And in doing that, you have to\ntake a probabilistic model.",
    "start": "1063200",
    "end": "1070990"
  },
  {
    "text": "If you don't take a\nprobabilistic model, it's very hard for you to talk honestly\nabout what error",
    "start": "1070990",
    "end": "1076799"
  },
  {
    "text": "probabilities are. So both ways-- well, I'm preaching\nand I'm sorry.",
    "start": "1076800",
    "end": "1082910"
  },
  {
    "text": "But I've lived for a long time\nwith many statisticians, many",
    "start": "1082910",
    "end": "1090250"
  },
  {
    "text": "of whom get into my own\nfield and who cause a great deal of trouble.",
    "start": "1090250",
    "end": "1096530"
  },
  {
    "text": "So the only thing I can do it\nurge you all to be cautious about this. And to think the matter\nthrough on your own.",
    "start": "1096530",
    "end": "1102690"
  },
  {
    "text": "I'm not telling you to take\nmy point of view on it. I'm telling you, don't take\nother people's point of view",
    "start": "1102690",
    "end": "1108870"
  },
  {
    "text": "without thinking it through.  The probability experiment\nhere really--",
    "start": "1108870",
    "end": "1117269"
  },
  {
    "text": "I mean, every probability model\nwe view in terms of the",
    "start": "1117270",
    "end": "1122530"
  },
  {
    "text": "real world, as you have this set\nof probabilities, a set of",
    "start": "1122530",
    "end": "1128350"
  },
  {
    "text": "possible events. You do the experiment. There's one sample point\nthat comes out.",
    "start": "1128350",
    "end": "1133580"
  },
  {
    "text": "And after the one sample point\ncomes out, then you know what the result of the\nexperiment is.",
    "start": "1133580",
    "end": "1139040"
  },
  {
    "text": "Here, the experiment consists\nboth of what you normally view",
    "start": "1139040",
    "end": "1144990"
  },
  {
    "text": "as the experiment. Namely, taking the\nobservations. And it also involves a\nchoice of hypotheses.",
    "start": "1144990",
    "end": "1153100"
  },
  {
    "text": "Namely, there's not a correct\nhypothesis to start with. The experiment involves\nGod throws his dice.",
    "start": "1153100",
    "end": "1161870"
  },
  {
    "text": "Einstein didn't believe that\nGod threw dice, but I do. And after throwing the dice,\none or the other of these",
    "start": "1161870",
    "end": "1170260"
  },
  {
    "text": "hypotheses turns\nout to be true. All of these observations point\nto that or they point to",
    "start": "1170260",
    "end": "1176650"
  },
  {
    "text": "the other and you\nmake a decision. OK, so the experiment consists\nboth on choosing the",
    "start": "1176650",
    "end": "1182660"
  },
  {
    "text": "hypothesis and on taking\na whole sequence of observations. Now, the other thing to\nnot forget in this--",
    "start": "1182660",
    "end": "1190650"
  },
  {
    "text": "because you really have to get\nthis model in your mind or you're going to get very\nconfused with all the things we do.",
    "start": "1190650",
    "end": "1196390"
  },
  {
    "text": "The experiment consists\non a whole sequence of observations, but only one\nchoice of hypothesis.",
    "start": "1196390",
    "end": "1203540"
  },
  {
    "text": "Namely, you do the experiment. There's a hypothesis that\noccurs, and there's a whole",
    "start": "1203540",
    "end": "1208650"
  },
  {
    "text": "sequence of observations which\nare all IID conditional on that particular hypothesis.",
    "start": "1208650",
    "end": "1213970"
  },
  {
    "text": " So that's the model we're\ngoing to be using.",
    "start": "1213970",
    "end": "1221960"
  },
  {
    "text": "And now life is quite\nsimple once we've explained the model.",
    "start": "1221960",
    "end": "1227040"
  },
  {
    "text": "We can talk about the\nprobability that H is equal to either 0 or 1, conditional\non the",
    "start": "1227040",
    "end": "1234850"
  },
  {
    "text": "sample point we've observed. It's equal to the a priori\nprobability of that hypothesis",
    "start": "1234850",
    "end": "1243570"
  },
  {
    "text": "times the density of the\nobservation conditional on the hypothesis divided by just\na normalization factor.",
    "start": "1243570",
    "end": "1253870"
  },
  {
    "text": "Namely, the overall probability\nof that observation period, which is the\nsum of probability that 0",
    "start": "1253870",
    "end": "1263512"
  },
  {
    "text": "is a correct hypothesis times\nthis plus probability that 1 is a correct hypothesis times\nthe density given 1.",
    "start": "1263512",
    "end": "1272490"
  },
  {
    "text": "This denominator here\nis a pain in the neck, as you can see.",
    "start": "1272490",
    "end": "1277790"
  },
  {
    "text": "But you can avoid ever dealing\nwith a denominator if you take this for H equals 0, divide by\nthis for H equals 1, and then",
    "start": "1277790",
    "end": "1289570"
  },
  {
    "text": "you have this term divided by\nthis term all divided by this term for l equals 1 divided\nby the same thing.",
    "start": "1289570",
    "end": "1298460"
  },
  {
    "text": "So the ratio, the probability\nthat H equals 0 given y over the probability that\nH is 1 equals y is",
    "start": "1298460",
    "end": "1306390"
  },
  {
    "text": "just this ratio here. Now, what's the probability of\nerror if we make a decision at",
    "start": "1306390",
    "end": "1312990"
  },
  {
    "text": "this point? If I've got in this particular\nsequence Y, this quantity here",
    "start": "1312990",
    "end": "1321320"
  },
  {
    "text": "is, in fact, the probability\nthat hypothesis 0 is correct in the model that\nwe have chosen.",
    "start": "1321320",
    "end": "1328020"
  },
  {
    "text": "So this is the probability that\nH is equal to 0 given Y.",
    "start": "1328020",
    "end": "1334870"
  },
  {
    "text": "If we select 1 under these\nconditions, if we select hypothesis 1, if we make a\ndecision and say, I'm going to",
    "start": "1334870",
    "end": "1342700"
  },
  {
    "text": "guess that 1 is the\nright decision. That means that this\nis the probability",
    "start": "1342700",
    "end": "1349070"
  },
  {
    "text": "you've made a mistake. Because this is the probability\nthat H is actually 0 rather than 1.",
    "start": "1349070",
    "end": "1354330"
  },
  {
    "text": "This quantity here is the\nprobability that you've made a mistake given that 1 is the\ncorrect hypothesis.",
    "start": "1354330",
    "end": "1361705"
  },
  {
    "text": " So here we are sitting\nhere with these",
    "start": "1361705",
    "end": "1368240"
  },
  {
    "text": "probabilities of error. We don't have to do any\ncalculations for them. Well, you might have to do a\ngreat deal of calculation to",
    "start": "1368240",
    "end": "1375100"
  },
  {
    "text": "calculate this and to\ncalculate this. But otherwise, the whole thing\nis just sitting there for you.",
    "start": "1375100",
    "end": "1381630"
  },
  {
    "text": "So what do you do if you\nwant to minimize the probability of error? ",
    "start": "1381630",
    "end": "1391780"
  },
  {
    "text": "This was the probability that\nyou're going to make an error if you choose 1. This is the probability of\nerror if you choose 0.",
    "start": "1391780",
    "end": "1399275"
  },
  {
    "text": " We want to minimize the\nprobability of error and we",
    "start": "1399275",
    "end": "1404670"
  },
  {
    "text": "see the observation Y, we want\nto pick the one of these which is largest. And that's all there is to it.",
    "start": "1404670",
    "end": "1413580"
  },
  {
    "text": "This is the decision rule\nthat minimizes the probability of an error.",
    "start": "1413580",
    "end": "1421020"
  },
  {
    "text": "It's based on knowing\nwhat P0 and P1 is. But otherwise, probability that\nH equals l is the correct",
    "start": "1421020",
    "end": "1427600"
  },
  {
    "text": "hypothesis given the observation\nis probability that H equals L given Y. We\nmaximize the a posteriori",
    "start": "1427600",
    "end": "1435000"
  },
  {
    "text": "probability of choosing\ncorrectly by choosing the maximum over l of probability\nthat H equals l given Y.",
    "start": "1435000",
    "end": "1442909"
  },
  {
    "text": "This choosing directly,\nmaximizing the a posteriori",
    "start": "1442910",
    "end": "1448350"
  },
  {
    "text": "probability is called the MAP\nrule, Maximum A posteriori Probability.",
    "start": "1448350",
    "end": "1454590"
  },
  {
    "text": "You can only solve the MAP\nproblem if you assume that you",
    "start": "1454590",
    "end": "1461620"
  },
  {
    "text": "know P0 P1. We do know P0 and P1 if we've\nselected a probability model.",
    "start": "1461620",
    "end": "1467650"
  },
  {
    "text": "So when we select this\nprobability model, we've already assumed what these a\npriori probabilities are, so",
    "start": "1467650",
    "end": "1475000"
  },
  {
    "text": "we now make our observation. And after making our observation, we make a decision.",
    "start": "1475000",
    "end": "1480210"
  },
  {
    "text": "And at that point, we have an a\nposteriori probability that each of the hypotheses\nis correct.",
    "start": "1480210",
    "end": "1487260"
  },
  {
    "text": " Anybody has any issues\nwith this?",
    "start": "1487260",
    "end": "1493350"
  },
  {
    "text": "I mean, it looks painfully\nsimple when you look at this way. And if it doesn't look painfully\nsimple, please ask",
    "start": "1493350",
    "end": "1501650"
  },
  {
    "text": "now or forever hold your\npeace as they say. ",
    "start": "1501650",
    "end": "1506830"
  },
  {
    "text": "Yeah? AUDIENCE: So can you explain\nhow you get the equation? Can you explain how you\nget the equation on the first line?",
    "start": "1506830",
    "end": "1513330"
  },
  {
    "text": "PROFESSOR: On the first\nline right up here? Yes, I use Bayes' law. AUDIENCE: So what is that?",
    "start": "1513330",
    "end": "1519620"
  },
  {
    "text": "So that's P of A given B is\nequal to P of B given A? PROFESSOR: Yes. AUDIENCE: I don't quite\nsee how to--",
    "start": "1519620",
    "end": "1527360"
  },
  {
    "text": "P of A given B is equal to P\nof B given A times P of A",
    "start": "1527360",
    "end": "1537140"
  },
  {
    "text": "divided by P of A and B.",
    "start": "1537140",
    "end": "1543110"
  },
  {
    "text": "If you take this over\nthere then it's-- am I stating Bayes' law\nin a funny way?",
    "start": "1543110",
    "end": "1551350"
  },
  {
    "text": "AUDIENCE: So the thing on\nthe bottom is P of B? OK. PROFESSOR: What? AUDIENCE: OK, I get it.",
    "start": "1551350",
    "end": "1557266"
  },
  {
    "start": "1557266",
    "end": "1562330"
  },
  {
    "text": "PROFESSOR: I mean, I might\nnot be explained it well. AUDIENCE: [INAUDIBLE]. ",
    "start": "1562330",
    "end": "1568120"
  },
  {
    "text": "PROFESSOR: Except if you start\nout with P of A given B is equal to P of B given A times P\nof B divided by P of A. This",
    "start": "1568120",
    "end": "1577090"
  },
  {
    "text": "quantity here is P of Y. So we\nhave probability that H equals",
    "start": "1577090",
    "end": "1588210"
  },
  {
    "text": "l times probability of Y given\nl divided by the probability",
    "start": "1588210",
    "end": "1594590"
  },
  {
    "text": "of l to start with. OK, so you maximize the a\nposteriori probability by",
    "start": "1594590",
    "end": "1603590"
  },
  {
    "text": "choosing the maximum of these. It's called the MAP rule. And it doesn't require you to\ncalculate this quantity, which",
    "start": "1603590",
    "end": "1613550"
  },
  {
    "text": "is sometimes a mess. All it requires you to do\nis to compare these two quantities, which means you\nhave to compare these two",
    "start": "1613550",
    "end": "1621410"
  },
  {
    "text": "quantities. AUDIENCE: It's 10 o'clock. PROFESSOR: Well, excuse me. Yes. Yes, I know.",
    "start": "1621410",
    "end": "1626500"
  },
  {
    "start": "1626500",
    "end": "1633480"
  },
  {
    "start": "1630000",
    "end": "1678000"
  },
  {
    "text": "These things become clearer if\nyou state them in terms of what you call the likelihood\nratio.",
    "start": "1633480",
    "end": "1640320"
  },
  {
    "text": "Likelihood ratio only works when\nyou have two hypotheses. When you have two hypotheses,\nyou call the ratio of one of",
    "start": "1640320",
    "end": "1649490"
  },
  {
    "text": "them to the other one the\nlikelihood ratio. Why do I put 0 up here\nand 1 down here?",
    "start": "1649490",
    "end": "1657289"
  },
  {
    "text": "Absolutely no reason at all,\nit's just convention. And unfortunately, it's\na convention that not everybody follows.",
    "start": "1657290",
    "end": "1663920"
  },
  {
    "text": "So some people have one\nconvention and some people have another convention. If you want to use the other\nconvention, just imagine",
    "start": "1663920",
    "end": "1671650"
  },
  {
    "text": "switching 1 and 1\nin your mind. They're both just\nbinary numbers.",
    "start": "1671650",
    "end": "1678279"
  },
  {
    "start": "1678000",
    "end": "1793000"
  },
  {
    "text": "Then, when you want to look at\nthis MAP rule, the MAP rule is choosing the larger of\nthese two things,",
    "start": "1678280",
    "end": "1686790"
  },
  {
    "text": "which we had back here. That's choosing whether this is\nlarger than this, or vice",
    "start": "1686790",
    "end": "1695620"
  },
  {
    "text": "versa, which is choosing whether\nthis ratio here is greater than the ratio\nof P1 to P0.",
    "start": "1695620",
    "end": "1704630"
  },
  {
    "text": "So that's the same, that's\nthe same thing. So the MAP rule is to calculate\nthe likelihood ratio",
    "start": "1704630",
    "end": "1714960"
  },
  {
    "text": "for this given observation y. And if this is greater\nthan P1 over P0, you",
    "start": "1714960",
    "end": "1721930"
  },
  {
    "text": "select H equals 0. If it's less than or equal to\nP1 over P0, you select H1.",
    "start": "1721930",
    "end": "1731909"
  },
  {
    "text": "Why do I put the strict equality\nhere and the strict inequality here?",
    "start": "1731910",
    "end": "1737880"
  },
  {
    "text": "Again, no reason whatsoever. When you have equality, it\ndoesn't make any difference",
    "start": "1737880",
    "end": "1743490"
  },
  {
    "text": "which you choose. So you could flip a coin. It's a little easier if you just\nsay, we're going to do",
    "start": "1743490",
    "end": "1751150"
  },
  {
    "text": "this under this condition. So we state condition\nthis way.",
    "start": "1751150",
    "end": "1756770"
  },
  {
    "text": "We calculate the likelihood\nratio. We compare it with\na threshold. The threshold here\nis P1 over P0.",
    "start": "1756770",
    "end": "1764840"
  },
  {
    "text": "And then we select something. Why did I put a little\nhat over this?",
    "start": "1764840",
    "end": "1770750"
  },
  {
    "text": "AUDIENCE: Estimation. PROFESSOR: What? AUDIENCE: Because it's\nan estimation. PROFESSOR: What? AUDIENCE: It's an estimation?",
    "start": "1770750",
    "end": "1777150"
  },
  {
    "text": "PROFESSOR: Well, it's not\nreally an estimation. It's a detection. I mean, estimation you usually\nview as being analog.",
    "start": "1777150",
    "end": "1784690"
  },
  {
    "text": "Detection you usually view\nas being digital. And thanks for bringing\nthat up because it's an important point.",
    "start": "1784690",
    "end": "1790130"
  },
  {
    "text": " But in this model, H is either\n0 or 1 in the result of this",
    "start": "1790130",
    "end": "1800139"
  },
  {
    "text": "experiment. We don't know which it is. This is what we've chosen.",
    "start": "1800140",
    "end": "1806220"
  },
  {
    "text": "So H hat is 0 does not mean\nthat H itself is 0.",
    "start": "1806220",
    "end": "1812320"
  },
  {
    "text": "So this is our choice. It might be wrong or\nit might be right.",
    "start": "1812320",
    "end": "1817360"
  },
  {
    "text": "Many decision rules, including\nthe most common and the most sensible, are rules that compare\nlambda of y to a fixed",
    "start": "1817360",
    "end": "1824980"
  },
  {
    "text": "threshold, say, eta, is P1 over\nP0, which is independent",
    "start": "1824980",
    "end": "1831169"
  },
  {
    "text": "of y, which is just\na fixed threshold. The decision rules then vary\nonly in the way that you",
    "start": "1831170",
    "end": "1837350"
  },
  {
    "text": "choose the threshold. Now, what happens as soon\nas I call this eta",
    "start": "1837350",
    "end": "1844030"
  },
  {
    "text": "instead of P1 over P0? My test becomes independent of\nthese a priori probabilities",
    "start": "1844030",
    "end": "1851660"
  },
  {
    "text": "that statisticians have thought\nabout for so long. Namely, after a couple of lines\nof fiddling around with",
    "start": "1851660",
    "end": "1858800"
  },
  {
    "text": "these things, suddenly all\nof that has disappeared.",
    "start": "1858800",
    "end": "1863840"
  },
  {
    "text": "We have a threshold test. The threshold test says,\ntake this ratio--",
    "start": "1863840",
    "end": "1869929"
  },
  {
    "text": "everybody agrees that there's\nsuch a ratio that exists-- and compare it with something.",
    "start": "1869930",
    "end": "1876400"
  },
  {
    "text": "And if it's bigger than that\nsomething, you choose 0.",
    "start": "1876400",
    "end": "1883900"
  },
  {
    "text": "If it's less than that\nthing, you choose 1. And that's the end of it.",
    "start": "1883900",
    "end": "1889809"
  },
  {
    "text": "OK, so we have two questions. One, do we always want to use a\nthreshold test or are there",
    "start": "1889810",
    "end": "1898580"
  },
  {
    "text": "cases where we should\nuse things other than a threshold test? And the second question is, if\nwe're going to use a threshold",
    "start": "1898580",
    "end": "1908010"
  },
  {
    "text": "test, where should we\nset the threshold? I mean, there's nothing that\nsays that you really want to",
    "start": "1908010",
    "end": "1914430"
  },
  {
    "text": "minimize the probability\nof error. I mean, suppose your test\nis to see whether--",
    "start": "1914430",
    "end": "1922740"
  },
  {
    "text": "I mean, something in\nthe news today. I mean, you'd like to take an\nexperiment to see whether your",
    "start": "1922740",
    "end": "1929350"
  },
  {
    "text": "nuclear plant is going\nto explode or not.",
    "start": "1929350",
    "end": "1934410"
  },
  {
    "text": "So you come up with\none decision, it's not going to explode. Or another decision, you\ndecide it will explode.",
    "start": "1934410",
    "end": "1941580"
  },
  {
    "text": "Presumably on the basis of\nthat decision, you do all sorts of things. Do you really want to make\nit a maximum a posteriori",
    "start": "1941580",
    "end": "1950190"
  },
  {
    "text": "probability decision? No. You recognize that if it's\ngoing to explode, and you",
    "start": "1950190",
    "end": "1958900"
  },
  {
    "text": "choose that it's not going to\nexplode and you don't do anything, there is a humongous\ncost associated with that.",
    "start": "1958900",
    "end": "1967380"
  },
  {
    "text": "If you decide the other way,\nthere's a pretty large cost associated with that also. But there's not really much\ncomparison between the two.",
    "start": "1967380",
    "end": "1974940"
  },
  {
    "text": "But anyway, you want to do\nsomething which takes those costs into account. One of the problems in the\nhomework does that.",
    "start": "1974940",
    "end": "1982169"
  },
  {
    "text": "It's really almost trivial to\nreadjust this problem, so that",
    "start": "1982170",
    "end": "1989510"
  },
  {
    "text": "you set the threshold to\ninvolve the costs also. So if you have arbitrary costs\nin making errors, then you",
    "start": "1989510",
    "end": "1999730"
  },
  {
    "text": "change the threshold\na little bit. But you still use a\nthreshold test.",
    "start": "1999730",
    "end": "2005800"
  },
  {
    "start": "2005000",
    "end": "2126000"
  },
  {
    "text": "There's something called maximum\nlikelihood that people like for making decisions.",
    "start": "2005800",
    "end": "2012420"
  },
  {
    "text": "And maximum likelihood\nsays you calculate the likelihood ratio. And if the likelihood\nratio is bigger than",
    "start": "2012420",
    "end": "2019620"
  },
  {
    "text": "1, you go this way. If it's less than 1,\nyou go this way.",
    "start": "2019620",
    "end": "2026530"
  },
  {
    "text": "It's the MAP test if\nthe two a priori probabilities are equal.",
    "start": "2026530",
    "end": "2032140"
  },
  {
    "text": "But in many cases, you want to\nuse it whether or not the a priori probabilities\nare equal.",
    "start": "2032140",
    "end": "2037930"
  },
  {
    "text": "It's a standard test,\nand there are many reasons for using it. Aside from the fact that the a\npriori probabilities might be",
    "start": "2037930",
    "end": "2046510"
  },
  {
    "text": "chosen that way. So anyway, that's one\nother choice. When we go a little further\nday, we'll talk about a",
    "start": "2046510",
    "end": "2053070"
  },
  {
    "text": "Neyman-Pearson test. The Neyman-Pearson test says,\nfor some reason or other, I",
    "start": "2053070",
    "end": "2060559"
  },
  {
    "text": "want to make sure that the\nprobability that my nuclear plant doesn't blow up is\nless than, say, 10",
    "start": "2060560",
    "end": "2069250"
  },
  {
    "text": "to the minus fifth. Why 10 to the minus fifth? Pull it out of the air. Maybe 10 to the minus sixth,\nthat point our probabilities",
    "start": "2069250",
    "end": "2077388"
  },
  {
    "text": "don't make much sense anymore. But however we choose it, we\nchoose our test to say, we",
    "start": "2077389",
    "end": "2084169"
  },
  {
    "text": "can't make the probability of\nerror under one hypothesis bigger than some certain amount\nalpha than what test",
    "start": "2084170",
    "end": "2092300"
  },
  {
    "text": "will minimize the probability\nof error under the other hypothesis.",
    "start": "2092300",
    "end": "2097359"
  },
  {
    "text": "Namely, if I have to get one\nthing right, or I have to get it right almost all the time,\nwhat's the best I can do on",
    "start": "2097360",
    "end": "2103610"
  },
  {
    "text": "the other alternative? And that's the Neyman-Pearson\ntest. That is a favorite test among\nthe non-Bayesians because it",
    "start": "2103610",
    "end": "2114640"
  },
  {
    "text": "doesn't involve the a priori\nprobabilities anymore. So it's a nice one\nin that way.",
    "start": "2114640",
    "end": "2120200"
  },
  {
    "text": "But we'll see, we get\nit anyway using a probability model.",
    "start": "2120200",
    "end": "2126750"
  },
  {
    "start": "2126000",
    "end": "2380000"
  },
  {
    "text": "OK, let's go back to random\nwalks just a little bit to see why we're doing what\nwe're doing.",
    "start": "2126750",
    "end": "2133230"
  },
  {
    "text": "The logarithm of the threshold\nratio is logarithm of this",
    "start": "2133230",
    "end": "2142560"
  },
  {
    "text": "lambda of y. I'm taking m observations. I'm putting that in explicitly,\nis the sum from N",
    "start": "2142560",
    "end": "2149380"
  },
  {
    "text": "equals 1 to m of the log of\nthe individual ratio. In other words, when you--",
    "start": "2149380",
    "end": "2156640"
  },
  {
    "text": "under hypothesis 0, if I\ncalculate the probability of vector y given H equals 0, I'm\nfinding the probability of n",
    "start": "2156640",
    "end": "2169210"
  },
  {
    "text": "things which are IID. So what I'm going to find this\nprobability density is taking",
    "start": "2169210",
    "end": "2176320"
  },
  {
    "text": "the product of the probabilities\nof each of the observations.",
    "start": "2176320",
    "end": "2182390"
  },
  {
    "text": "Most of you know now that\nany time you look at a probability, which is a product\nof observations, what",
    "start": "2182390",
    "end": "2189210"
  },
  {
    "text": "you'd really like to do is to\ntake the logarithm of it. So you're talking about a sum\nof things rather than a",
    "start": "2189210",
    "end": "2195120"
  },
  {
    "text": "product of things because\nwe all know how to add independent random variables.",
    "start": "2195120",
    "end": "2200700"
  },
  {
    "text": "So the log of this likelihood\nratio, which is called the log",
    "start": "2200700",
    "end": "2206109"
  },
  {
    "text": "likelihood ratio as you might\nguess, is just a sum of these likelihood ratios.",
    "start": "2206110",
    "end": "2212599"
  },
  {
    "text": "If we look at this for each m\ngreater than or equal to 1, then given H equals 0,\nit's a random walk.",
    "start": "2212600",
    "end": "2219779"
  },
  {
    "text": "And given H equals 1, it's\nanother random walk. It's the same sequence of sample\nvalues in both cases.",
    "start": "2219780",
    "end": "2226869"
  },
  {
    "text": "Namely, as an experimentalist,\nwe're taking these observations. We don't know whether H equals\n0 or H equals 1 is what the",
    "start": "2226870",
    "end": "2237230"
  },
  {
    "text": "result of the experiment\nis going to be. But what we do know is we know\nwhat those values are.",
    "start": "2237230",
    "end": "2243450"
  },
  {
    "text": "We can calculate this sum. And now, if we condition this\non H equals 0, then this",
    "start": "2243450",
    "end": "2258320"
  },
  {
    "text": "quantity, which is fixed,\nhas a particular probability of occurring.",
    "start": "2258320",
    "end": "2263490"
  },
  {
    "text": "So this is a random variable\nthen under the hypothesis H equals 0.",
    "start": "2263490",
    "end": "2269380"
  },
  {
    "text": "It's a random variable under\nthe hypothesis H equals 1. And this sum of random variables\nbehaves in a very",
    "start": "2269380",
    "end": "2277589"
  },
  {
    "text": "different way under these\ntwo hypotheses. What's going to happen is that\nunder one hypothesis, the",
    "start": "2277590",
    "end": "2284680"
  },
  {
    "text": "expected value of this log\nlikelihood ratio is going to",
    "start": "2284680",
    "end": "2289980"
  },
  {
    "text": "linearly increase with n. If we look at it under the other\nhypothesis, it's going",
    "start": "2289980",
    "end": "2296050"
  },
  {
    "text": "to linearly decrease\nas we increase n. And a nifty test at that point\nis to say, as soon as it",
    "start": "2296050",
    "end": "2304550"
  },
  {
    "text": "crosses a threshold up here or\na threshold down here, we're going to make a decision.",
    "start": "2304550",
    "end": "2311359"
  },
  {
    "text": "And that's called a sequential\ntest in that case because you haven't specified ahead of time,\nI'm going to take 100",
    "start": "2311360",
    "end": "2318500"
  },
  {
    "text": "tests and then make\nup my mind. You've specified that I'm going\nto take as many tests as",
    "start": "2318500",
    "end": "2324420"
  },
  {
    "text": "I need to be relatively sure\nthat I'm getting the right decision, which is what\nyou do in real life.",
    "start": "2324420",
    "end": "2332340"
  },
  {
    "text": "I mean, there's nothing fancy\nabout doing sequential tests. Those are the obvious things to\ndo, except they're a little",
    "start": "2332340",
    "end": "2340420"
  },
  {
    "text": "more tricky to talk about using\nprobability theory.",
    "start": "2340420",
    "end": "2345530"
  },
  {
    "text": "But anyway, that's where\nwe're headed. That's why we're talking about\nhypothesis testing.",
    "start": "2345530",
    "end": "2354660"
  },
  {
    "text": "Because when you look at it in\nthis formulation, we get a random walk.",
    "start": "2354660",
    "end": "2361290"
  },
  {
    "text": "And it gives us a nice example\nof when you want to use random",
    "start": "2361290",
    "end": "2368500"
  },
  {
    "text": "walks crossing a threshold as\na way of making decisions. OK, so that's why we're doing\nwhat we're doing.",
    "start": "2368500",
    "end": "2376640"
  },
  {
    "text": " Now, let's go back and look at\nthreshold tests again, and try",
    "start": "2376640",
    "end": "2388150"
  },
  {
    "text": "to see how we're going to make\nthreshold tests, what the",
    "start": "2388150",
    "end": "2395650"
  },
  {
    "text": "error probabilities will be,\nand try to analyze them a little more than just saying,\nwell, a MAP test does this.",
    "start": "2395650",
    "end": "2403150"
  },
  {
    "text": "Because as soon as you see that\na MAP test does this, you say, well, suppose I use\nsome other test.",
    "start": "2403150",
    "end": "2410030"
  },
  {
    "text": "And what am I going to\nsuffer from that? What am I going to gain by it?",
    "start": "2410030",
    "end": "2415120"
  },
  {
    "text": "So it's worthwhile to, instead\nof looking at even just threshold tests, to say,\nwell, let's look at",
    "start": "2415120",
    "end": "2422579"
  },
  {
    "text": "any old test at all. Now, any test means\nthe following.",
    "start": "2422580",
    "end": "2428745"
  },
  {
    "text": " I have this probability model.",
    "start": "2428745",
    "end": "2434150"
  },
  {
    "text": "I've already bludgeoned you into\naccepting the fact that's the probability model we're\ngoing to be looking at.",
    "start": "2434150",
    "end": "2440300"
  },
  {
    "text": " And we have this--",
    "start": "2440300",
    "end": "2446440"
  },
  {
    "text": "well, we have the likelihood\nratio, but we don't care about that for the moment. But we make this observation.",
    "start": "2446440",
    "end": "2453390"
  },
  {
    "text": "We got to make a decision.  And our decision is going\nto be either 1 or 0.",
    "start": "2453390",
    "end": "2461800"
  },
  {
    "text": "How do we characterize\nthat mathematically? ",
    "start": "2461800",
    "end": "2468299"
  },
  {
    "text": "Or how do we calculate it if\nwe want a computer to make that decision for us?",
    "start": "2468300",
    "end": "2474250"
  },
  {
    "text": "The systematic way to do it is\nfor every possible sequence of y to say ahead of time to give a\nformula, which sequences get",
    "start": "2474250",
    "end": "2484650"
  },
  {
    "text": "mapped into 1 and which\nsequences get mapped into 0.",
    "start": "2484650",
    "end": "2490010"
  },
  {
    "text": "So we're going to call a set A\nthe set of sample sequences",
    "start": "2490010",
    "end": "2496220"
  },
  {
    "text": "that get mapped into\nhypothesis 1. That's the most general binary\nhypothesis test you can do.",
    "start": "2496220",
    "end": "2505620"
  },
  {
    "text": "That includes all\npossible ways of choosing either 1 or 0.",
    "start": "2505620",
    "end": "2510660"
  },
  {
    "text": "You're forced to hire somebody\nor not hire somebody. You can't get them to work for\nyou for two weeks, and then",
    "start": "2510660",
    "end": "2518500"
  },
  {
    "text": "make a decision at that point. Well, sometimes you\ncan in this world. But if it's somebody you really\nwant and other people",
    "start": "2518500",
    "end": "2525890"
  },
  {
    "text": "want them, too, then you've got\nto decide, I'm going to go with this person or I'm not\ngoing to go with them.",
    "start": "2525890",
    "end": "2532150"
  },
  {
    "text": "So under all observations that\nyou've made, you need some way",
    "start": "2532150",
    "end": "2538240"
  },
  {
    "text": "to decide which ones make\nyou go to decision 1.",
    "start": "2538240",
    "end": "2543550"
  },
  {
    "text": "Which ones make you\ngo to decision 0. So we will just say arbitrarily,\nthere's a set A",
    "start": "2543550",
    "end": "2550940"
  },
  {
    "text": "of sample sequences that\nmap into hypothesis 1. And the error probability for\neach hypothesis using test A",
    "start": "2550940",
    "end": "2560070"
  },
  {
    "text": "is given by-- and we'll just\ncall Q sub 0 of A-- this is our name for the\nerror probability.",
    "start": "2560070",
    "end": "2565880"
  },
  {
    "start": "2565880",
    "end": "2573119"
  },
  {
    "text": "Have I twisted this up? No. ",
    "start": "2573120",
    "end": "2578300"
  },
  {
    "text": "Q sub 0 of A is the probability that I actually choose-- ",
    "start": "2578300",
    "end": "2585880"
  },
  {
    "text": "it's the probability that I\nchoose A given that the hypothesis is 0.",
    "start": "2585880",
    "end": "2592210"
  },
  {
    "text": "Q sub 1 of A is the probability\nthat I choose 1.",
    "start": "2592210",
    "end": "2600000"
  },
  {
    "text": "Blah, let me start\nthat over again. ",
    "start": "2600000",
    "end": "2605400"
  },
  {
    "text": "Q0 of A is the probability\nthat I'm going to choose",
    "start": "2605400",
    "end": "2613880"
  },
  {
    "text": "hypothesis 1 given that\nhypothesis 0 was the correct hypothesis. It's the probability that Y is\nin A. That means that H hat is",
    "start": "2613880",
    "end": "2623089"
  },
  {
    "text": "equal to 1 given that\nH is actually 0. So that's the probability we\nmake an error given the",
    "start": "2623090",
    "end": "2632510"
  },
  {
    "text": "hypothesis, the correct\nhypothesis is 0. Q1 of A is the probability of\nmaking an error given that the",
    "start": "2632510",
    "end": "2639880"
  },
  {
    "text": "correct hypothesis is 1. If I have a priori\nprobabilities, I'm going back to assuming a priori\nprobabilities again.",
    "start": "2639880",
    "end": "2647770"
  },
  {
    "text": "The probability of error is? ",
    "start": "2647770",
    "end": "2655340"
  },
  {
    "text": "It's P0 times the probability\nI make an error given that H",
    "start": "2655340",
    "end": "2661970"
  },
  {
    "text": "equals zero. P1 a priori probability\nof 1 given that I make an error given 1.",
    "start": "2661970",
    "end": "2668920"
  },
  {
    "text": "I add these two up. I can write it this way. Don't ask for the time being.",
    "start": "2668920",
    "end": "2675570"
  },
  {
    "text": "I'll just take the P0 out, so\nit's Q0 of A plus P1 over P0",
    "start": "2675570",
    "end": "2682300"
  },
  {
    "text": "Q1 of A. So that's what I've\ncalled eta times Q1 of A.",
    "start": "2682300",
    "end": "2687920"
  },
  {
    "start": "2687000",
    "end": "3090000"
  },
  {
    "text": "For the threshold test based\non eta, the probability of error is the same thing.",
    "start": "2687920",
    "end": "2695119"
  },
  {
    "text": "But that A there is an eta. I hope you can imagine that\nquantity there is an eta.",
    "start": "2695120",
    "end": "2704690"
  },
  {
    "text": "This is an eta. So it's P0 times Q0 of eta\nplus eta times Q1 of eta.",
    "start": "2704690",
    "end": "2710710"
  },
  {
    "text": "So the eta probability, under\nthis crazy test that you've designed, is P0 times\nthis quantity.",
    "start": "2710710",
    "end": "2719180"
  },
  {
    "text": "Under the MAP test, probability\nof error is this quantity here.",
    "start": "2719180",
    "end": "2725710"
  },
  {
    "text": "What do we know about\nthe MAP test? It minimizes the error\nprobability under those a",
    "start": "2725710",
    "end": "2733300"
  },
  {
    "text": "priori probabilities. So what we know about it is\nthat this quantity is less",
    "start": "2733300",
    "end": "2739720"
  },
  {
    "text": "than or equal to\nthis quantity. Take out the P0's and it says\nthat this quantity is less",
    "start": "2739720",
    "end": "2748630"
  },
  {
    "text": "than or equal to\nthis quantity.  Pretty simple.",
    "start": "2748630",
    "end": "2755920"
  },
  {
    "text": "Let's draw a picture that\nshows what that means. Here's a result that we have.",
    "start": "2755920",
    "end": "2761724"
  },
  {
    "text": " We know because of maximum a\nposteriori probability for the",
    "start": "2761724",
    "end": "2769010"
  },
  {
    "text": "threshold test that this is less\nthan or equal to this.",
    "start": "2769010",
    "end": "2775990"
  },
  {
    "text": "This is the minimum\nerror probability. This is the error probability\nyou get with",
    "start": "2775990",
    "end": "2781590"
  },
  {
    "text": "whatever test you like. So let's draw a picture on a\ngraph where the probability of",
    "start": "2781590",
    "end": "2790810"
  },
  {
    "text": "error given H equals 1 is\non the horizontal axis.",
    "start": "2790810",
    "end": "2797810"
  },
  {
    "text": "The probability of error\nconditional on H equals 0 is on this axis.",
    "start": "2797810",
    "end": "2806550"
  },
  {
    "text": "So I can list the probability\nof error for the threshold",
    "start": "2806550",
    "end": "2813420"
  },
  {
    "text": "test, which sits here. I can list the probability of\nerror for this arbitrary test,",
    "start": "2813420",
    "end": "2819810"
  },
  {
    "text": "which sits here. And I know that this quantity\nis greater than or equal to",
    "start": "2819810",
    "end": "2825880"
  },
  {
    "text": "this quantity. So the only thing I have to do\nnow is to sort out using plain",
    "start": "2825880",
    "end": "2834400"
  },
  {
    "text": "geometry, why these numbers\nare what they are.",
    "start": "2834400",
    "end": "2839559"
  },
  {
    "text": "This number here is Q0 of eta\nplus eta times Q1 of eta.",
    "start": "2839560",
    "end": "2846760"
  },
  {
    "text": "Here's Q1 of eta. This distance here\nis Q1 of eta.",
    "start": "2846760",
    "end": "2853630"
  },
  {
    "text": "We have a line of slope minus\neta there that we've drawn. So this point here is, in fact,\nQ0 of eta plus eta times",
    "start": "2853630",
    "end": "2862890"
  },
  {
    "text": "Q1 of eta . That's just plain geometry. This point is Q0 of A plus eta\ntimes Q1 of A. Another line of",
    "start": "2862890",
    "end": "2877880"
  },
  {
    "text": "slope minus et. What we've shown is that this is\nless than or equal to this.",
    "start": "2877880",
    "end": "2885349"
  },
  {
    "start": "2885350",
    "end": "2890620"
  },
  {
    "text": "That's because of\nthe MAP rule. This has to be less than\nor equal to that. So what have we shown here?",
    "start": "2890620",
    "end": "2896740"
  },
  {
    "text": "We've shown that for every test\nA you can imagine, when you draw that test on this\ntwo-dimensional plot of error",
    "start": "2896740",
    "end": "2905880"
  },
  {
    "text": "probability given H equals 1\nversus error probability given H equals 0.",
    "start": "2905880",
    "end": "2913089"
  },
  {
    "text": "Every test in the world lies\nNortheast of this line here. ",
    "start": "2913090",
    "end": "2925620"
  },
  {
    "text": "Yeah? AUDIENCE: Can you say\nagain exactly what axis represents what?",
    "start": "2925620",
    "end": "2931280"
  },
  {
    "text": "PROFESSOR: This axis here\nrepresents the error probability given that H\nequals 1 is the correct",
    "start": "2931280",
    "end": "2938200"
  },
  {
    "text": "hypothesis. This axis is the error\nprobability given that 0 is",
    "start": "2938200",
    "end": "2943619"
  },
  {
    "text": "the correct hypothesis.  So we've defined Q1 of eta and\nQ0 of eta as those two error",
    "start": "2943620",
    "end": "2951200"
  },
  {
    "text": "probabilities. Using the threshold test, or\nusing the MAP test where eta",
    "start": "2951200",
    "end": "2957860"
  },
  {
    "text": "is equal to P0 over P1. And this point here is whatever\nit happens to be for",
    "start": "2957860",
    "end": "2965010"
  },
  {
    "text": "any test that you\nhappen to like. ",
    "start": "2965010",
    "end": "2970630"
  },
  {
    "text": "You might have a supervisor who\nwants to hire somebody and you view that person is a threat\nto yourself, so you've",
    "start": "2970630",
    "end": "2979190"
  },
  {
    "text": "taken all your observations and\nyou then make a decision. If the person is any good,\nyou say, don't hire him.",
    "start": "2979190",
    "end": "2985380"
  },
  {
    "text": "If the person is good\nyou say, hire them. So just the opposite of\nwhat you should do.",
    "start": "2985380",
    "end": "2991770"
  },
  {
    "text": "But whatever you do, this says\nthis is less than or equal to",
    "start": "2991770",
    "end": "2997310"
  },
  {
    "text": "this because of the MAP rule. And therefore, this point lies\nup in that direction",
    "start": "2997310",
    "end": "3005680"
  },
  {
    "text": "of this line here.  You can do this for any eta that\nyou want to do it for.",
    "start": "3005680",
    "end": "3012630"
  },
  {
    "text": " So for every eta that we want\nto use, we get some value of",
    "start": "3012630",
    "end": "3019490"
  },
  {
    "text": "Q0 of eta and Q1 of eta. These go along here\nin some way.",
    "start": "3019490",
    "end": "3025180"
  },
  {
    "text": "You can do the same\nargument again. For every threshold test, every\npoint lies Northeast of",
    "start": "3025180",
    "end": "3033769"
  },
  {
    "text": "the line of slope minus eta\nthrough that threshold test. We get a whole family of curves\nwhen eta is very big,",
    "start": "3033770",
    "end": "3043200"
  },
  {
    "text": "the curve of slope minus\neta goes like this. When eta is very small,\nit goes like this.",
    "start": "3043200",
    "end": "3050260"
  },
  {
    "text": " We just think of ourselves\nplotting all these curves,",
    "start": "3050260",
    "end": "3058000"
  },
  {
    "text": "taking the upper envelope of\nthem because every test has to lie Northeast of every\none of those lines.",
    "start": "3058000",
    "end": "3066770"
  },
  {
    "text": "So we take the upper envelope of\nall of these lines, and we get something that\nlooks like this.",
    "start": "3066770",
    "end": "3075050"
  },
  {
    "text": "We call this the error curve. And this is the upper envelope\nof the straight lines of slope",
    "start": "3075050",
    "end": "3083730"
  },
  {
    "text": "minus eta that go through the\nthreshold tests at eta. ",
    "start": "3083730",
    "end": "3091510"
  },
  {
    "start": "3090000",
    "end": "3340000"
  },
  {
    "text": "You get something else\nfrom that, too. This curve is convex.",
    "start": "3091510",
    "end": "3097330"
  },
  {
    "text": "Why is the curve convex? Well, you might like to take the\nsecond derivative of it,",
    "start": "3097330",
    "end": "3102380"
  },
  {
    "text": "but that's a pain in the neck. But the fundamental definition\nof convexity is that a",
    "start": "3102380",
    "end": "3112110"
  },
  {
    "text": "one-dimensional curve is convex\nif all of its tangents lie underneath the curve.",
    "start": "3112110",
    "end": "3118230"
  },
  {
    "text": "That's the way we've\nconstructed this. It's the upper envelope of a\nbunch of straight lines. Yes?",
    "start": "3118230",
    "end": "3123360"
  },
  {
    "text": "AUDIENCE: Can you please\nexplain, what is u of alpha? PROFESSOR: U of alpha\nis just what I've",
    "start": "3123360",
    "end": "3128930"
  },
  {
    "text": "called this upper envelope. This upper envelope\nis now a function.",
    "start": "3128930",
    "end": "3134420"
  },
  {
    "text": "AUDIENCE: What's\nthe definition? PROFESSOR: What? AUDIENCE: What is\nthe definition? PROFESSOR: The definition is\nthe upper envelope of all",
    "start": "3134420",
    "end": "3139980"
  },
  {
    "text": "these straight lines. AUDIENCE: For changing eta? PROFESSOR: What? AUDIENCE: For changing eta?",
    "start": "3139980",
    "end": "3147490"
  },
  {
    "text": "PROFESSOR: Yes. As eta changes, I get a whole\nbunch of these points.",
    "start": "3147490",
    "end": "3155540"
  },
  {
    "text": "I got a whole bunch\nof these points. I take the upper envelope of all\nof these straight lines.",
    "start": "3155540",
    "end": "3161480"
  },
  {
    "text": " I mean, yes, you'd rather\nsee an equation.",
    "start": "3161480",
    "end": "3167010"
  },
  {
    "text": "But if you see an equation\nit's terribly ugly. I mean, you can program\na computer to do this.",
    "start": "3167010",
    "end": "3175589"
  },
  {
    "text": "as easily as you can\nprogram it to follow a bunch of equations.",
    "start": "3175590",
    "end": "3182800"
  },
  {
    "text": "But anyway, I'm not interested\nin actually solving for this curve in particular. ",
    "start": "3182800",
    "end": "3193420"
  },
  {
    "text": "I am particularly interested\nin the fact that this upper envelope is, in fact, a convex\ncurve and that the threshold",
    "start": "3193420",
    "end": "3202990"
  },
  {
    "text": "tests lie on the curve. The other tests lie Northeast\nof the curve.",
    "start": "3202990",
    "end": "3210690"
  },
  {
    "text": "And that's the reason you want\nto use threshold tests. And it has nothing to do with a\npriori probabilities at all.",
    "start": "3210690",
    "end": "3218810"
  },
  {
    "text": "So you see, the thing we've done\nis to start out assuming a priori probabilities.",
    "start": "3218810",
    "end": "3224119"
  },
  {
    "text": "We've derived this neat result\nusing a priori probabilities.",
    "start": "3224120",
    "end": "3229450"
  },
  {
    "text": "But now we have this\nerror curve.",
    "start": "3229450",
    "end": "3235196"
  },
  {
    "text": "Well, to give you a better\ndefinition of what u of alpha is, u of alpha is the error\nprobability under hypothesis 1",
    "start": "3235196",
    "end": "3247619"
  },
  {
    "text": "if the error probability under\nhypothesis 0 was alpha. You pick an error probability\nhere.",
    "start": "3247620",
    "end": "3256160"
  },
  {
    "text": "You go up to that point here. There's a threshold\ntest there.",
    "start": "3256160",
    "end": "3261750"
  },
  {
    "text": "You read over there. And at that point, you find the\nprobability of error given",
    "start": "3261750",
    "end": "3268480"
  },
  {
    "text": "H equals 1. AUDIENCE: How do you know\nthat the threshold tests lie on the curve?",
    "start": "3268480",
    "end": "3275580"
  },
  {
    "text": "PROFESSOR: Well, this threshold\ntest here is",
    "start": "3275580",
    "end": "3282640"
  },
  {
    "text": "Southwest of all tests. ",
    "start": "3282640",
    "end": "3288420"
  },
  {
    "text": "And therefore, it can't lie\nabove this upper envelope. ",
    "start": "3288420",
    "end": "3297300"
  },
  {
    "text": "Now, I've cheated you\nin one small way. If you have a discrete test,\nwhat you're going to wind up",
    "start": "3297300",
    "end": "3307369"
  },
  {
    "text": "with is just a finite set of\nthese possible points here.",
    "start": "3307370",
    "end": "3312580"
  },
  {
    "text": "So you're going to wind up with\nthe upper envelope of a finite set of straight lines.",
    "start": "3312580",
    "end": "3318130"
  },
  {
    "text": "So the straight line is\nactually going to be-- it's still convex, but it's\npiecewise linear.",
    "start": "3318130",
    "end": "3326120"
  },
  {
    "text": "And it's piecewise linear, and\nthe threshold tests are at the points of that curve.",
    "start": "3326120",
    "end": "3333320"
  },
  {
    "text": "And in between those points,\nyou don't quite know what to do. ",
    "start": "3333320",
    "end": "3340890"
  },
  {
    "start": "3340000",
    "end": "3599000"
  },
  {
    "text": "So since you don't quite know\nwhat to do in between those points, as far as the maximum a\nposteriori probability test",
    "start": "3340890",
    "end": "3351400"
  },
  {
    "text": "goes, you can reach any one of\nthose points, sometimes using",
    "start": "3351400",
    "end": "3358029"
  },
  {
    "text": "one test on one corner of-- I guess it's easier\nif I draw it. ",
    "start": "3358030",
    "end": "3367140"
  },
  {
    "text": "And I didn't want to get into\nthis particularly because it's a little messier.",
    "start": "3367140",
    "end": "3372280"
  },
  {
    "start": "3372280",
    "end": "3378320"
  },
  {
    "text": "So you could have this\nkind of curve. And the notes talk about\nthis in detail.",
    "start": "3378320",
    "end": "3384550"
  },
  {
    "text": "So the threshold test correspond\nto this point. This point says always\ndecide one.",
    "start": "3384550",
    "end": "3393550"
  },
  {
    "text": "Don't pay any attention to the\ntests at all, just say I think one is the right hypothesis.",
    "start": "3393550",
    "end": "3400980"
  },
  {
    "text": "I mean, this is the testing\nphilosophy of people who don't believe in experimentalism.",
    "start": "3400980",
    "end": "3406980"
  },
  {
    "text": "They've already made\nup their mind. They look at the results. They say, that's very\ninteresting.",
    "start": "3406980",
    "end": "3412200"
  },
  {
    "text": "And then they say, I'm\ngoing to choose this. These other points are our\nparticular threshold tests.",
    "start": "3412200",
    "end": "3420944"
  },
  {
    "text": " If you want to get error\nprobabilities in the middle",
    "start": "3420944",
    "end": "3427680"
  },
  {
    "text": "here, what do you do? You use a randomized test. Sometimes you use this.",
    "start": "3427680",
    "end": "3432700"
  },
  {
    "text": "Sometimes you use this. You flip a coin and choose\nwhichever one of these you want to choose.",
    "start": "3432700",
    "end": "3438370"
  },
  {
    "text": " So what this says is the\nNeyman-Pearson test, which is",
    "start": "3438370",
    "end": "3447160"
  },
  {
    "text": "the test that says pick some\nalpha, which is the error",
    "start": "3447160",
    "end": "3456190"
  },
  {
    "text": "probability under hypothesis\n1 that you're willing to tolerate.",
    "start": "3456190",
    "end": "3461660"
  },
  {
    "text": "So you pick alpha. And then it says, minimize the\nerror probability of the other",
    "start": "3461660",
    "end": "3468329"
  },
  {
    "text": "kind, so you read over there. And the Neyman-Pearson test,\nwhat it does is it minimizes",
    "start": "3468330",
    "end": "3476630"
  },
  {
    "text": "the error probability under\nthe other hypothesis.",
    "start": "3476630",
    "end": "3482130"
  },
  {
    "text": "Now, when this curve is\npiecewise linear, the Neyman-Pearson test is not a\nthreshold test, but it's a",
    "start": "3482130",
    "end": "3489530"
  },
  {
    "text": "randomized threshold test. Sometimes when you're at a point\nlike this, you have to",
    "start": "3489530",
    "end": "3495300"
  },
  {
    "text": "use this test and this\ntest sometimes. ",
    "start": "3495300",
    "end": "3500340"
  },
  {
    "text": "For most of the tests that you\ndeal with, Neyman-Pearson test",
    "start": "3500340",
    "end": "3505710"
  },
  {
    "text": "is just the threshold\ntest that's at that particular point.",
    "start": "3505710",
    "end": "3511180"
  },
  {
    "text": " Any questions about that?",
    "start": "3511180",
    "end": "3518099"
  },
  {
    "text": "This is probably one of these\nthings you have to think about a little bit. Yes? AUDIENCE: When you say you have\nto use this test or this",
    "start": "3518100",
    "end": "3524330"
  },
  {
    "text": "test, are you talking about\nthreshold or are you talking about-- because this is always--\nit's either H equals",
    "start": "3524330",
    "end": "3531183"
  },
  {
    "text": "0 or H equal 1, right? What do you mean when you say\nyou have to randomize between",
    "start": "3531184",
    "end": "3536508"
  },
  {
    "text": "the two tests? I mean threshold tests-- ",
    "start": "3536508",
    "end": "3555120"
  },
  {
    "text": "if I have a finite set of\nalternatives, and I'm doing a",
    "start": "3555120",
    "end": "3560290"
  },
  {
    "text": "threshold test on that finite\nset of alternatives, I only have a finite number\nof things I can do.",
    "start": "3560290",
    "end": "3569500"
  },
  {
    "text": "As I increase the threshold,\nI suddenly get to the point where this ratio of likelihoods",
    "start": "3569500",
    "end": "3576750"
  },
  {
    "text": "includes one more point. And then it gets to the point\nwhere it includes one other point and so forth.",
    "start": "3576750",
    "end": "3583770"
  },
  {
    "text": "So that what happens is that\nthis upper envelope is just",
    "start": "3583770",
    "end": "3589430"
  },
  {
    "text": "the upper envelope of a finite\nnumber of points. And this upper envelope of a\nfinite number of points, the",
    "start": "3589430",
    "end": "3596980"
  },
  {
    "text": "threshold tests are just\nthe corners there. So I sometimes have to randomize\nbetween them.",
    "start": "3596980",
    "end": "3604330"
  },
  {
    "text": "If you don't like\nthat, ignore it.  Because for most tests you deal\nwith, almost all books on",
    "start": "3604330",
    "end": "3616450"
  },
  {
    "text": "statistics that I've ever\nseen, it just says the Neyman-Pearson test looks at the\nthreshold curve, at this",
    "start": "3616450",
    "end": "3625130"
  },
  {
    "text": "error curve. And it chooses accordingly. Yes?",
    "start": "3625130",
    "end": "3631228"
  },
  {
    "text": "AUDIENCE: You can put the\nprevious slide back?",
    "start": "3631228",
    "end": "3636590"
  },
  {
    "text": "You told us that because\nof maximum a posteriori",
    "start": "3636590",
    "end": "3642690"
  },
  {
    "text": "probability, if eta is equal to\nP0 divided by P1, then the",
    "start": "3642690",
    "end": "3649869"
  },
  {
    "text": "probability of error\nis minimized. And so the errors of the\ntest A are [INAUDIBLE].",
    "start": "3649870",
    "end": "3656900"
  },
  {
    "text": " But if we start changing eta\nfrom 0 to infinity, it doesn't",
    "start": "3656900",
    "end": "3664738"
  },
  {
    "text": "have to be anymore. [INAUDIBLE], which means\nthe error is not necessarily minimized.",
    "start": "3664738",
    "end": "3671015"
  },
  {
    "text": "So the argument doesn't\nhold anymore. PROFESSOR: As I change eta, I'm\nchanging P1 and P0 also.",
    "start": "3671015",
    "end": "3677880"
  },
  {
    "text": "In other words, now what I'm\ndoing is I'm saying, let's look at this threshold test,\nand let's visualize what",
    "start": "3677880",
    "end": "3687240"
  },
  {
    "text": "happens as I change the a\npriori probabilities. So I'm suddenly becoming a\nclassical statistician instead",
    "start": "3687240",
    "end": "3697390"
  },
  {
    "text": "of a Bayesian one. But I know what the answers\nare from looking at the Bayesian case.",
    "start": "3697390",
    "end": "3703500"
  },
  {
    "text": " OK, so let's move on.",
    "start": "3703500",
    "end": "3713290"
  },
  {
    "text": " I mean, we now sort of see\nthat these tests--",
    "start": "3713290",
    "end": "3722244"
  },
  {
    "text": " well, one thing we've seen is\nwhen you have to make a",
    "start": "3722245",
    "end": "3728119"
  },
  {
    "text": "decision under this kind of\nprobabilistic model we've been talking about-- namely, two\nhypotheses, IID random",
    "start": "3728120",
    "end": "3738070"
  },
  {
    "text": "variable is conditional\non each hypothesis. ",
    "start": "3738070",
    "end": "3743420"
  },
  {
    "text": "Those hypothesis testing\nproblems turn into random walk problems.",
    "start": "3743420",
    "end": "3749350"
  },
  {
    "text": "We also saw that\nthe [? GG1Q ?] when I started looking at when\nthe system becomes empty, and",
    "start": "3749350",
    "end": "3757040"
  },
  {
    "text": "how long it takes to start to\nfill up again, that problem is",
    "start": "3757040",
    "end": "3763010"
  },
  {
    "text": "a random walk problem. So now I want to start to ask\nthe question, what's the probability that a random walk\nwill cross a threshold?",
    "start": "3763010",
    "end": "3772470"
  },
  {
    "text": "I'm going to apply the\nChernoff bound to it. You remember the\nChernoff bound? We talked about it a little\nbit back on the",
    "start": "3772470",
    "end": "3780410"
  },
  {
    "text": "second week of the term. We were talking about the Markov\ninequality and the",
    "start": "3780410",
    "end": "3786420"
  },
  {
    "text": "Chebyshev inequality. And we said that the Chernoff\ninequality was the same sort",
    "start": "3786420",
    "end": "3792200"
  },
  {
    "text": "of thing, except it was based\non e to the rZ rather than x",
    "start": "3792200",
    "end": "3797780"
  },
  {
    "text": "or x squared. And we talked a little bit\nabout its properties.",
    "start": "3797780",
    "end": "3804620"
  },
  {
    "text": "The major thing one uses the\nChernoff bound for is to get good estimates very, very\nfar away from the mean.",
    "start": "3804620",
    "end": "3813020"
  },
  {
    "text": "In other words, good estimates\nof probabilities that are very, very small.",
    "start": "3813020",
    "end": "3818040"
  },
  {
    "text": "I've grown up using these all\nmy life because I've been concerned with error\nprobabilities in",
    "start": "3818040",
    "end": "3823440"
  },
  {
    "text": "communication systems. You typically want error\nprobabilities that run between",
    "start": "3823440",
    "end": "3829630"
  },
  {
    "text": "10 to the minus fifth and\n10 to the minus eighth. So you want to look at points\nwhich are quite far away.",
    "start": "3829630",
    "end": "3838940"
  },
  {
    "text": "I mean, you take a\nlarge number of-- you take a sum of a large number\nof variables, which",
    "start": "3838940",
    "end": "3845230"
  },
  {
    "text": "correspond to a code. And you look at error\nprobabilities for this rather",
    "start": "3845230",
    "end": "3852400"
  },
  {
    "text": "complicated thing. But you're looking very, very\nfar away from the mean, and you're looking at very large\nnumbers of observations.",
    "start": "3852400",
    "end": "3859619"
  },
  {
    "text": "So instead of the kinds of\nthings where we deal with",
    "start": "3859620",
    "end": "3865920"
  },
  {
    "text": "things like the central limit\ntheorem where you're trying to figure out what goes on close\nto the mean, here you're",
    "start": "3865920",
    "end": "3871430"
  },
  {
    "text": "trying to figure out what goes\non very far from the mean. OK, so what the Chernoff bound\nsays is that the probability",
    "start": "3871430",
    "end": "3880990"
  },
  {
    "text": "that a random variable Z is\ngreater than or equal to some constant b.",
    "start": "3880990",
    "end": "3887390"
  },
  {
    "text": "We don't even need sums of\nrandom variables here, it's just a Chernoff bound is a\nbound on the tail of a",
    "start": "3887390",
    "end": "3894590"
  },
  {
    "text": "distribution. Is less than or equal to the\nmoment generating function of",
    "start": "3894590",
    "end": "3899800"
  },
  {
    "text": "that random variable. g sub Z of r is the expected\nvalue of e to the rZ.",
    "start": "3899800",
    "end": "3908090"
  },
  {
    "text": "These generating functions,\nyou can calculate them if you want to. Times e to the minus rb.",
    "start": "3908090",
    "end": "3915390"
  },
  {
    "text": "This is the Markov inequality\nfor the random variable e to the rZ.",
    "start": "3915390",
    "end": "3921750"
  },
  {
    "text": "And go back and review\nchapter 1. I think it's section\n1.43 or something.",
    "start": "3921750",
    "end": "3929770"
  },
  {
    "text": "It's the section that deals with\nthe Markov inequality, the Chebyshev inequality,\nand the Chernoff bound.",
    "start": "3929770",
    "end": "3940970"
  },
  {
    "text": "And as I told you once when we\ntalked about these things, Chernoff is still\nalive and well. He's a statistician\nat Harvard.",
    "start": "3940970",
    "end": "3947840"
  },
  {
    "text": "He was somewhat embarrassed by\nthis inequality becoming so famous because he did it as sort\nof a throw-off thing in a",
    "start": "3947840",
    "end": "3955619"
  },
  {
    "text": "paper where he was trying to\ndo something which was much more mathematically\nsophisticated.",
    "start": "3955620",
    "end": "3962290"
  },
  {
    "text": "And now the poor guy is only\nknown for this thing that he views as being trivial. ",
    "start": "3962290",
    "end": "3971359"
  },
  {
    "text": "But what the bound says is the\nprobability of Z is greater than or equal to b is\nthis inequality.",
    "start": "3971360",
    "end": "3977380"
  },
  {
    "text": "Strangely enough, the\nprobability that Z is less than or equal to b is bounded\nby the same inequality.",
    "start": "3977380",
    "end": "3985710"
  },
  {
    "text": "But one of them, r\nis bigger than 0. And the other one,\nr is less than 0.",
    "start": "3985710",
    "end": "3993220"
  },
  {
    "text": "And you have to go back and\nread that section to understand why. Now, this is most useful when\nit's applied to a sum of",
    "start": "3993220",
    "end": "4000560"
  },
  {
    "text": "random variables. I don't know of any applications\nfor it otherwise.",
    "start": "4000560",
    "end": "4006670"
  },
  {
    "text": "So if the moment-generating\nfunction-- oh, incidentally, also.",
    "start": "4006670",
    "end": "4012870"
  },
  {
    "text": "When most people talk about\nmoment-generating functions, and certainly when people talked\nabout moment-generating",
    "start": "4012870",
    "end": "4019650"
  },
  {
    "text": "functions before the 1950s or\nso, what they were always interested in is the fact that\nif you take derivatives of the",
    "start": "4019650",
    "end": "4028829"
  },
  {
    "text": "moment-generating functions,\nyou generate the moments of the random variable.",
    "start": "4028830",
    "end": "4034980"
  },
  {
    "text": "If you take the derivative of\nthis with respect to r, evaluate it at r equals 0, you\nget the expected value of Z.",
    "start": "4034980",
    "end": "4042970"
  },
  {
    "text": "If you take the second\nderivative evaluated at r equals 0, you get the\nexpected value of Z",
    "start": "4042970",
    "end": "4050720"
  },
  {
    "text": "squared, and so forth. You can see that by just taking\nthe derivative of that.",
    "start": "4050720",
    "end": "4056810"
  },
  {
    "text": "Here, we're looking\nat something else. We're not looking at what goes\non around r equals 0.",
    "start": "4056810",
    "end": "4062200"
  },
  {
    "text": "We're trying to figure out what\ngoes on way on the far tails of these distributions.",
    "start": "4062200",
    "end": "4068760"
  },
  {
    "text": "So if gX of r is e to the rX,\nthen e to the e to the r Sn--",
    "start": "4068760",
    "end": "4076860"
  },
  {
    "text": "Sn is the sum of these\nrandom variables-- is the expected value of the\nproduct of e to the rXi.",
    "start": "4076860",
    "end": "4084589"
  },
  {
    "text": "Namely, it's e to the r. Some of Xi. So that turns into a product.",
    "start": "4084590",
    "end": "4091020"
  },
  {
    "text": "The expected value of a product\nof a finite number of terms is the product of\nthe expected value.",
    "start": "4091020",
    "end": "4099318"
  },
  {
    "text": "So it's gX or r to\nthe n-th power. So if I want to write this, now\nI'm applying the Chernoff",
    "start": "4099319",
    "end": "4107200"
  },
  {
    "text": "bound to the random\nvariable S sub n. What's the probability that S\nsub n is greater than or equal",
    "start": "4107200",
    "end": "4112880"
  },
  {
    "text": "to n times a? It's gX to the n of r times\ne to the minus rna.",
    "start": "4112880",
    "end": "4119000"
  },
  {
    "text": "That's what the Chernoff\nbound says. This is the Chernoff bound over\non the other side of the",
    "start": "4119000",
    "end": "4126639"
  },
  {
    "text": "distribution. This only makes sense and has\ninteresting values when a is",
    "start": "4126640",
    "end": "4134020"
  },
  {
    "text": "bigger than the mean or when\na is less than the mean. And when r is greater than 0 for\nthis one and less than 0",
    "start": "4134020",
    "end": "4141210"
  },
  {
    "text": "for this one. ",
    "start": "4141210",
    "end": "4147370"
  },
  {
    "text": "Now, this is easier to\ninterpret and it's easier to work with.",
    "start": "4147370",
    "end": "4153729"
  },
  {
    "text": "If you take that product of\nterms g to the r to the n-th",
    "start": "4153729",
    "end": "4160818"
  },
  {
    "text": "power and you visualize the\nlogarithm of g to the X.",
    "start": "4160819",
    "end": "4167020"
  },
  {
    "text": "Visualize the logarithm of g\nto the X, then you get this quantity up here.",
    "start": "4167020",
    "end": "4173113"
  },
  {
    "start": "4173114",
    "end": "4181339"
  },
  {
    "text": "You get the probability that Sn\nis greater than or equal to na is this e to the n times\ngamma x of r minus ra.",
    "start": "4181340",
    "end": "4191349"
  },
  {
    "text": "Gamma is the logarithm of the\nmoment-generating function.",
    "start": "4191350",
    "end": "4197600"
  },
  {
    "text": "The logarithm of the\nmoment-generating function is always called the\nsemi-invariant",
    "start": "4197600",
    "end": "4202980"
  },
  {
    "text": "moment-generating function. The name is, again, because\npeople were originally interested in the\nmoment-generating properties",
    "start": "4202980",
    "end": "4210570"
  },
  {
    "text": "of these random variables. If you sit down and take\nthe derivatives, I can",
    "start": "4210570",
    "end": "4217060"
  },
  {
    "text": "probably do it here. It's simple enough that\nI won't get confused. ",
    "start": "4217060",
    "end": "4226639"
  },
  {
    "text": "The derivative with respect to\nr of the logarithm of g of r",
    "start": "4226640",
    "end": "4237140"
  },
  {
    "text": "is first derivative of\nr divided by g of r.",
    "start": "4237140",
    "end": "4244810"
  },
  {
    "text": "And the second derivative\nis then the",
    "start": "4244810",
    "end": "4252890"
  },
  {
    "text": "natural log of g of r. Taking the derivative of that is\nequal to g double prime of",
    "start": "4252890",
    "end": "4260120"
  },
  {
    "text": "r over g of r squared.",
    "start": "4260120",
    "end": "4266000"
  },
  {
    "text": "Tell me if I'm making a mistake\nhere because I usually do when I do this.",
    "start": "4266000",
    "end": "4271360"
  },
  {
    "text": "Minus g of r and g prime of r.",
    "start": "4271360",
    "end": "4279690"
  },
  {
    "text": " Probably divided by\nthis squared.",
    "start": "4279690",
    "end": "4295770"
  },
  {
    "text": "Let's see. Is this right? Who can take derivatives here?",
    "start": "4295770",
    "end": "4301810"
  },
  {
    "text": "AUDIENCE: First term doesn't\nhave a square in it. PROFESSOR: What? AUDIENCE: First term doesn't\nhave a square in the denominator.",
    "start": "4301810",
    "end": "4307150"
  },
  {
    "text": "PROFESSOR: First term? Yeah. Oh, the first thing doesn't\nhave a square.",
    "start": "4307150",
    "end": "4313280"
  },
  {
    "text": "No, you're right. AUDIENCE: Second one\ndoesn't have-- PROFESSOR: And the second\none, let's see.",
    "start": "4313280",
    "end": "4319400"
  },
  {
    "text": "We have--  we just have g prime\nof r squared",
    "start": "4319400",
    "end": "4326230"
  },
  {
    "text": "divided by g of r squared. And we evaluate this\nat r equals 0.",
    "start": "4326230",
    "end": "4332929"
  },
  {
    "text": "This term becomes 1. This term becomes 1. This term becomes the second\nmoment x squared bar.",
    "start": "4332930",
    "end": "4342760"
  },
  {
    "text": "And this term becomes\nx bar squared. And this whole thing becomes the\nvariance of the moment of",
    "start": "4342760",
    "end": "4352030"
  },
  {
    "text": "the random variable rather\nthan the second moment.",
    "start": "4352030",
    "end": "4357980"
  },
  {
    "text": "All of these terms might be\nwrong, but this term is right.",
    "start": "4357980",
    "end": "4363100"
  },
  {
    "text": "And I'm sure all of you can\nrewrite that and evaluate it at r equals 0.",
    "start": "4363100",
    "end": "4368190"
  },
  {
    "text": "So that's why it's called\nthe semi-invariant moment-generating function. It doesn't make any difference\nfor what we're interested in.",
    "start": "4368190",
    "end": "4375610"
  },
  {
    "text": "The thing that we're interested\nin is that this exponent here--",
    "start": "4375610",
    "end": "4380809"
  },
  {
    "text": " as you visualize doing this\nexperiment and taking",
    "start": "4380810",
    "end": "4387490"
  },
  {
    "text": "additional observations, what\nhappens is the probability",
    "start": "4387490",
    "end": "4395000"
  },
  {
    "text": "that you exceed na-- that the n-th sum exceeds n\ntimes some fixed quantity a is",
    "start": "4395000",
    "end": "4405310"
  },
  {
    "text": "going down exponentially\nwith [? the a. ?]  Now, is this bound any good?",
    "start": "4405310",
    "end": "4412100"
  },
  {
    "text": " Well, if you optimize it over\nr, It's essentially",
    "start": "4412100",
    "end": "4419969"
  },
  {
    "text": "exponentially tight. So, in fact, it is good.",
    "start": "4419970",
    "end": "4425210"
  },
  {
    "text": "What does it mean to be\nexponentially tight? That's what I don't want\nto define carefully.",
    "start": "4425210",
    "end": "4430679"
  },
  {
    "text": "There's a theorem in the notes\nthat says what exponentially tight means. And it takes you half an hour to\nread it because it's being",
    "start": "4430680",
    "end": "4438250"
  },
  {
    "text": "stated very carefully. What it says essentially is that\nif I take this quantity",
    "start": "4438250",
    "end": "4448429"
  },
  {
    "text": "here and I subtract--",
    "start": "4448430",
    "end": "4454510"
  },
  {
    "text": "I add an epsilon to it. Namely, e to the n times this\nquantity minus epsilon.",
    "start": "4454510",
    "end": "4462510"
  },
  {
    "text": "So I have an e to the\nminus n epsilon, see it sitting in there? When I take this exponent and I\nreduce it just a little bit,",
    "start": "4462510",
    "end": "4471170"
  },
  {
    "text": "I get a bound that isn't true. This is greater than\nor equal to the quantity with an epsilon.",
    "start": "4471170",
    "end": "4477860"
  },
  {
    "text": "In other words, you can't make\nan exponent that's any smaller than this. You can take coefficients and\nplay with them, but you can't",
    "start": "4477860",
    "end": "4485690"
  },
  {
    "text": "make the exponent any smaller. OK, all of these things you\ncan do them by pictures.",
    "start": "4485690",
    "end": "4495490"
  },
  {
    "text": "I know many of you don't like\ndoing things by pictures. I keep doing them by pictures\nbecause I keep trying to",
    "start": "4495490",
    "end": "4502060"
  },
  {
    "text": "convince you that pictures\nare more rigorous than equations are.",
    "start": "4502060",
    "end": "4507750"
  },
  {
    "text": "At least, many times. If you want to show that\nsomething is convex, you try",
    "start": "4507750",
    "end": "4513690"
  },
  {
    "text": "to show that the second\nderivative is positive. That works sometimes and it\ndoesn't work sometimes.",
    "start": "4513690",
    "end": "4520639"
  },
  {
    "text": "I mean, it works as a function\nis continuous and has a continuous first derivative. It doesn't work. otherwise.",
    "start": "4520640",
    "end": "4527850"
  },
  {
    "text": "When you start taking tangents\nof the curve, and you say the",
    "start": "4527850",
    "end": "4533280"
  },
  {
    "text": "upper envelope of the tangents\nto the curve all lie below the",
    "start": "4533280",
    "end": "4540559"
  },
  {
    "text": "function, then it\nworks perfectly. That's what a convex function\nis by definition. ",
    "start": "4540560",
    "end": "4548210"
  },
  {
    "text": "How do we derive\nall this stuff?  What we're trying to\ndo is to find--",
    "start": "4548210",
    "end": "4556320"
  },
  {
    "text": "I mean, this inequality here is\ntrue for all r, for all r",
    "start": "4556320",
    "end": "4564989"
  },
  {
    "text": "greater than 0 so long as a is\ngreater than the mean of X. It's true for all r for which\nthis moment-generating",
    "start": "4564990",
    "end": "4572969"
  },
  {
    "text": "function exists. Moment-generating functions can\nsometimes blow up, so they",
    "start": "4572970",
    "end": "4578400"
  },
  {
    "text": "don't exist everywhere. So it's true wherever the moment-generating function exists.",
    "start": "4578400",
    "end": "4585140"
  },
  {
    "text": "So we like to find the r for\nwhich this bound is tightest. So what I'm going to do is draw\na picture and show you",
    "start": "4585140",
    "end": "4593240"
  },
  {
    "text": "where it's tightest in\nterms of the picture. What I've drawn here is\nthe semi-invariant",
    "start": "4593240",
    "end": "4600380"
  },
  {
    "text": "moment-generating function. Why didn't I put that down?",
    "start": "4600380",
    "end": "4607580"
  },
  {
    "text": "This is gamma of r. Gamma of r at 0, it's the log\nof the moment-generating",
    "start": "4607580",
    "end": "4615630"
  },
  {
    "text": "function at 0, which is 0. ",
    "start": "4615630",
    "end": "4621090"
  },
  {
    "text": "It's convex. You take its second\nderivative.",
    "start": "4621090",
    "end": "4626190"
  },
  {
    "text": "Its second derivative at r\nequals 0 is pretty easy. Its second derivative of other\nvalues or r you have to",
    "start": "4626190",
    "end": "4632250"
  },
  {
    "text": "struggle with it.  But when you struggle a little\nbit, it is convex.",
    "start": "4632250",
    "end": "4640199"
  },
  {
    "text": "If you've got a curve that goes\ndown like this, then it goes back up again.",
    "start": "4640200",
    "end": "4645800"
  },
  {
    "text": "Sometimes goes off\ntowards infinity. Might do whatever\nit wants to do.",
    "start": "4645800",
    "end": "4650950"
  },
  {
    "text": "Sometimes at a certain value\nof r, it stops existing. Suppose I take the\nsimplest random",
    "start": "4650950",
    "end": "4657750"
  },
  {
    "text": "variable you know about. You only know two simple\nrandom variables.",
    "start": "4657750",
    "end": "4663440"
  },
  {
    "text": "One of them is a binary\nrandom variable. The other one's an exponential\nrandom variable.",
    "start": "4663440",
    "end": "4669420"
  },
  {
    "text": "Suppose I take the exponential\nrandom variable with density alpha times e to the minus\nalpha X. Where does this",
    "start": "4669420",
    "end": "4679020"
  },
  {
    "text": "moment-generating\nfunction exist? You take alpha and I multiply\nit by e to the rX when I",
    "start": "4679020",
    "end": "4696220"
  },
  {
    "text": "integrate it.  Where does this exist?",
    "start": "4696220",
    "end": "4702190"
  },
  {
    "text": " I mean, don't bother\nto integrate it. ",
    "start": "4702190",
    "end": "4711909"
  },
  {
    "text": "If r is bigger than alpha, this\nexponent is bigger than this exponent.",
    "start": "4711910",
    "end": "4717889"
  },
  {
    "text": "And this thing takes off\ntowards infinity. If r is less than a, the\nwhole thing goes to 0.",
    "start": "4717890",
    "end": "4723715"
  },
  {
    "start": "4723715",
    "end": "4729219"
  },
  {
    "text": "gX of r exists for r less\nthan alpha in this case.",
    "start": "4729220",
    "end": "4739020"
  },
  {
    "text": " And in general, if you look at\na moment-generating function,",
    "start": "4739020",
    "end": "4746290"
  },
  {
    "text": "if the tail of that distribution\nfunction is going to 0 exponentially, you find the\nrate at which it's going",
    "start": "4746290",
    "end": "4754880"
  },
  {
    "text": "to 0 exponentially. And that's where the\nmoment-generating function cuts off.",
    "start": "4754880",
    "end": "4761810"
  },
  {
    "text": "It has to cut off. You can't show a result like\nthis, which says something is",
    "start": "4761810",
    "end": "4767070"
  },
  {
    "text": "going to 0, faster than it could\npossibly be going to 0. ",
    "start": "4767070",
    "end": "4773600"
  },
  {
    "text": "So we have to have that\nkind of result. But anyway, we draw\nthis curve. This is mu sub X of r.",
    "start": "4773600",
    "end": "4780349"
  },
  {
    "text": "And then we say, how do we\ngraphically minimize gamma of",
    "start": "4780350",
    "end": "4786520"
  },
  {
    "text": "r minus r times a? Well, what I do because I've\ndone this before and I know",
    "start": "4786520",
    "end": "4797140"
  },
  {
    "text": "how to do it-- I mean, it's not the kind of\nthing where if you sat down you would immediately\nsettle on this.",
    "start": "4797140",
    "end": "4805670"
  },
  {
    "text": "I look at some particular\nvalue of r. If I take a line of slope gamma\nprime of r, that's a",
    "start": "4805670",
    "end": "4816370"
  },
  {
    "text": "tangent to this curve because\nthis curve is convex. So if I take a line through here\nof this slope and I look",
    "start": "4816370",
    "end": "4824380"
  },
  {
    "text": "at where this line hits here,\nwhere does it hit? It hits at gamma sub X of\nr, this point here,",
    "start": "4824380",
    "end": "4834320"
  },
  {
    "text": "minus gamma X of r-- ",
    "start": "4834320",
    "end": "4841880"
  },
  {
    "text": "oh. ",
    "start": "4841880",
    "end": "4850219"
  },
  {
    "text": "Well, what I've done is I've\nalready optimized the problem. I'm trying to find the\nprobability that Sn is greater",
    "start": "4850220",
    "end": "4858600"
  },
  {
    "text": "than or equal to na. I'm trying to minimize this\nexponent here, gamma",
    "start": "4858600",
    "end": "4863870"
  },
  {
    "text": "X of r minus ra. Unfortunately, I really start\nout by taking the derivative",
    "start": "4863870",
    "end": "4870420"
  },
  {
    "text": "of that and setting it equal to\n0, which is what you would all do, too. When I set the derivative of\nthis equal to 0, I get gamma",
    "start": "4870420",
    "end": "4879330"
  },
  {
    "text": "prime of r minus a is equal to\n0, which is what this says.",
    "start": "4879330",
    "end": "4886010"
  },
  {
    "text": "So then we take a line of slope\ngamma x of r equals 0.",
    "start": "4886010",
    "end": "4891539"
  },
  {
    "text": "It's tangent at this\npoint here. You look at this point over here\nand you get the minimum",
    "start": "4891540",
    "end": "4897500"
  },
  {
    "text": "value of the gamma X\nof r minus r0 a. ",
    "start": "4897500",
    "end": "4905370"
  },
  {
    "text": "So what this says is when you\nvary a, you can go through this maximization tilting\nthis curve around.",
    "start": "4905370",
    "end": "4917440"
  },
  {
    "text": "I mean, a determines the slope\nof this line here. If I use a smaller value of\na, the slope is smaller.",
    "start": "4917440",
    "end": "4926930"
  },
  {
    "text": "It hits in here. If I take a larger value of a,\nit comes in further down and",
    "start": "4926930",
    "end": "4934220"
  },
  {
    "text": "the exponent gets bigger. That's not surprising. I want to find out the\nprobability that S sub n is",
    "start": "4934220",
    "end": "4939699"
  },
  {
    "text": "greater than or equal to a. As I increase a, I expect this\nexponent to keep going down as",
    "start": "4939700",
    "end": "4946350"
  },
  {
    "text": "I make a bigger and bigger\nbecause it's harder and harder for it to be greater\nthan or equal to a.",
    "start": "4946350",
    "end": "4953700"
  },
  {
    "text": "So anyway, when you optimize\nthis, you get something exponentially tight.",
    "start": "4953700",
    "end": "4959719"
  },
  {
    "text": "And this is what\nit's equal to. And I would recommend that you\ngo back and read the section",
    "start": "4959720",
    "end": "4966560"
  },
  {
    "text": "of chapter 1, which goes through\nall of this in a little more detail. ",
    "start": "4966560",
    "end": "4976820"
  },
  {
    "text": "Let me go passed that. Don't want to talk about that.",
    "start": "4976820",
    "end": "4983800"
  },
  {
    "text": "Well, when I do this\noptimization, if what I'm",
    "start": "4983800",
    "end": "4989639"
  },
  {
    "text": "looking at is the probability\nthat S sub n is greater than or equal to some alpha rather\nthan n times a when I'm do",
    "start": "4989640",
    "end": "4997000"
  },
  {
    "text": "this optimization and I'm\nlooking at what happens at different values of n, it turns\nout that when n is very",
    "start": "4997000",
    "end": "5004170"
  },
  {
    "text": "big, you get something which\nis tangent there.",
    "start": "5004170",
    "end": "5011770"
  },
  {
    "text": "As n gets smaller, you get these\ntangents that come down that comes in to there,\nand then it starts",
    "start": "5011770",
    "end": "5018619"
  },
  {
    "text": "going back out again. This e to the r star is the\ntightest the bound ever gets.",
    "start": "5018620",
    "end": "5027800"
  },
  {
    "text": "That's the n at which errors\nin the hypothesis testing",
    "start": "5027800",
    "end": "5034389"
  },
  {
    "text": "usually occur. It's the point at which-- it's the n for which Sn greater\nthan or equal to alpha",
    "start": "5034390",
    "end": "5042740"
  },
  {
    "text": "is most likely to occur. And if you evaluate that for\nour friendly binary case",
    "start": "5042740",
    "end": "5052500"
  },
  {
    "text": "again, X equals 1 or X equals\nminus 1, what you find when",
    "start": "5052500",
    "end": "5058290"
  },
  {
    "text": "you evaluate that point alpha r\nstar is that r star is equal",
    "start": "5058290",
    "end": "5065060"
  },
  {
    "text": "to log 1 minus P over P. And our\nbound of probability union of Sn is greater than or equal\nto alpha is approximately e to",
    "start": "5065060",
    "end": "5073830"
  },
  {
    "text": "the minus alpha r star\nis 1 minus P over P to the minus alpha.",
    "start": "5073830",
    "end": "5080690"
  },
  {
    "text": "I mean, why do I torture\nyou with this? Because we solved this problem\nat the beginning of the",
    "start": "5080690",
    "end": "5086570"
  },
  {
    "text": "lecture, remember? The probability that the sum\nS sub n for this binary",
    "start": "5086570",
    "end": "5094539"
  },
  {
    "text": "experiment is greater than or\nequal to k is equal to 1 minus",
    "start": "5094540",
    "end": "5099710"
  },
  {
    "text": "P over P to the minus k. That's what it's equal\nto exactly. When I go through all of this\nChernoff bound stuff, I get",
    "start": "5099710",
    "end": "5109960"
  },
  {
    "text": "the same answer. Now, this is a much harder way\nto do it, but this is a general way of doing it.",
    "start": "5109960",
    "end": "5116240"
  },
  {
    "text": "And that's a very specialized\nway of doing it. So we'll talk more about\nthis next time. ",
    "start": "5116240",
    "end": "5122260"
  }
]