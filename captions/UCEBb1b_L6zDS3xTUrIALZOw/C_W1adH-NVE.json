[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "12720",
    "end": "20507"
  },
  {
    "text": "PHILIPPE RIGOLLET: --of\nour limiting distribution, which happen to be Gaussian. But if the central\nlimit theorem told",
    "start": "20507",
    "end": "25800"
  },
  {
    "text": "us that the limiting\ndistribution of some average was something that\nlooked like a Poisson or an [? exponential, ?]\nthen we would just",
    "start": "25800",
    "end": "32340"
  },
  {
    "text": "have in the same way\ntaken the quintiles of the exponential distribution. So let's go back to what we had.",
    "start": "32340",
    "end": "39440"
  },
  {
    "text": "So generically if you have a\nset of observations X1 to Xn.",
    "start": "39440",
    "end": "46989"
  },
  {
    "text": "So remember for the kiss example\nthey were denoted by R1 to Rn,",
    "start": "46990",
    "end": "52180"
  },
  {
    "text": "because they were turning\nthe head to the right, but let's just go back. We say X1 to Xn,\nand in this case",
    "start": "52180",
    "end": "59800"
  },
  {
    "text": "I'm going to assume\nthey're IID, and I'm going to make them Bernoulli\nwith [INAUDIBLE] p,",
    "start": "59800",
    "end": "65700"
  },
  {
    "text": "and p is unknown, right?  So what did we do from here?",
    "start": "65700",
    "end": "71600"
  },
  {
    "text": "Well, we said p is\nthe expectation of Xi, and actually we didn't even\nthink about it too much.",
    "start": "71600",
    "end": "77990"
  },
  {
    "text": "We said, well, if\nI need to estimate the proportion of people who\nturn their head to the right when they kiss, I\njust basically I'm going to compute the average.",
    "start": "77990",
    "end": "84400"
  },
  {
    "text": "So our p hat was just\nXn bar, which was just 1 over n sum from i\nover 1 2n of the Xi.",
    "start": "84400",
    "end": "92170"
  },
  {
    "text": "The average of the observations\nwas their estimate. And then we wanted to build\nsome confidence intervals",
    "start": "92170",
    "end": "97689"
  },
  {
    "text": "around this. So what we wanted to understand\nis, how much that this p hat fluctuates.",
    "start": "97690",
    "end": "102970"
  },
  {
    "text": "This is a random variable. It's an average of\nrandom variables. It's a random\nvariable, so we want to know what the\ndistribution is. And if we know what\nthe distribution is,",
    "start": "102970",
    "end": "109406"
  },
  {
    "text": "then we actually know,\nwell, where it fluctuates. What the expectation is. Around which value it tends\nto fluctuate et cetera.",
    "start": "109406",
    "end": "115648"
  },
  {
    "text": "And so what the\ncentral limit theorem told us was if I take square\nroot of n times Xn bar minus p,",
    "start": "115649",
    "end": "123310"
  },
  {
    "text": "which is its average. And then I divide it by\nthe standard deviation. ",
    "start": "123310",
    "end": "130840"
  },
  {
    "text": "Then this thing here converges\nas n goes to infinity, and we will say\na little bit more",
    "start": "130840",
    "end": "137380"
  },
  {
    "text": "about what it means\nin distribution to some standard\nnormal random variable.",
    "start": "137380",
    "end": "143157"
  },
  {
    "text": "So that was the\ncentral limit theorem.  So what it means is\nthat when I think",
    "start": "143157",
    "end": "148610"
  },
  {
    "text": "of this as a random variable,\nwhen n is large enough",
    "start": "148610",
    "end": "155410"
  },
  {
    "text": "it's going to look like this. And so I understand\nperfectly its fluctuations. I know that this\nthing here has--",
    "start": "155410",
    "end": "163450"
  },
  {
    "text": "I know the probability\nof being in this zone. I know that this\nnumber here is 0. I know a bunch of things.",
    "start": "163450",
    "end": "169600"
  },
  {
    "text": "And then, in\nparticular, what I was interested in was that\nthe probability, that's",
    "start": "169600",
    "end": "175990"
  },
  {
    "text": "the absolute value of a\nGaussian random variable, exceeds q alpha over\n2, q alpha over 2.",
    "start": "175990",
    "end": "185111"
  },
  {
    "text": "We said that this\nwas equal to what? ",
    "start": "185111",
    "end": "193610"
  },
  {
    "text": "Anybody? What was that? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Alpha, right?",
    "start": "193610",
    "end": "199469"
  },
  {
    "text": "So that's the probability. That's my random variable. So this is by definition q\nalpha over 2 is the number.",
    "start": "199470",
    "end": "207050"
  },
  {
    "text": "So that to the right\nof it is alpha over 2. And this is a negative q\nalpha over 2 by symmetry.",
    "start": "207050",
    "end": "214100"
  },
  {
    "text": "And so the probability\nthat i exceeds-- well, it's not very symmetric,\nbut the probability that i exceeds this\nvalue, q alpha over 2,",
    "start": "214100",
    "end": "221019"
  },
  {
    "text": "is just the sum of\nthe two gray areas.",
    "start": "221020",
    "end": "226250"
  },
  {
    "text": "All right? So now I said that this thing\nwas approximately equal, due to the central\nlimit theorem,",
    "start": "226250",
    "end": "231980"
  },
  {
    "text": "to the probability,\nthat square root of n. Xn bar minus p divided by\nsquare root p 1 minus p.",
    "start": "231980",
    "end": "239063"
  },
  {
    "start": "239063",
    "end": "244970"
  },
  {
    "text": "Well, absolute value was\nlarger than q alpha over 2.",
    "start": "244970",
    "end": "250180"
  },
  {
    "text": "Well, then this thing by default\nis actually approximately equal to alpha, just because of virtue\nof the central limit theorem.",
    "start": "250180",
    "end": "256870"
  },
  {
    "text": "And then we just said,\nwell, I'll solve for p.",
    "start": "256870",
    "end": "263770"
  },
  {
    "text": "Has anyone attempted to solve\nthe degree two equation for p in the homework?",
    "start": "263770",
    "end": "269412"
  },
  {
    "text": "Everybody has tried it? ",
    "start": "269412",
    "end": "275400"
  },
  {
    "text": "So essentially, this is\ngoing to be an equation in p. Sometimes we don't\nwant to solve it. Some of the p's we will replace\nby their worst possible value.",
    "start": "275400",
    "end": "281823"
  },
  {
    "text": "For example, we said one\nof the tricks we had was that this value here,\nsquare root of p 1 minus p,",
    "start": "281823",
    "end": "288830"
  },
  {
    "text": "was always less than one half. Until we could actually get\nthe confidence interval that was larger than all\npossible confidence",
    "start": "288830",
    "end": "295173"
  },
  {
    "text": "intervals for all\npossible values of p, but we could solve for p. Do we all agree on the\nprinciple of what we did?",
    "start": "295174",
    "end": "301570"
  },
  {
    "text": "So that's how you build\nconfidence intervals. Now let's step\nback for a second, and see what was important in\nthe building of this confidence",
    "start": "301570",
    "end": "308069"
  },
  {
    "text": "interval. The really key thing is\nthat I didn't tell you why I formed this thing, right?",
    "start": "308070",
    "end": "315350"
  },
  {
    "text": "We started from\nx bar, and then I took some weird function of x\nbar that depended on p and n.",
    "start": "315350",
    "end": "321000"
  },
  {
    "text": "And the reason is, because\nwhen I take this function, the central limit\ntheorem tells me that it converges to\nsomething that I know.",
    "start": "321000",
    "end": "328009"
  },
  {
    "text": "But this very important thing\nabout the something that I know is that it does not depend on\nanything that I don't know.",
    "start": "328009",
    "end": "335030"
  },
  {
    "text": "For example, if I\nforgot to divide by square root of p 1 minus\np, then this thing would have",
    "start": "335030",
    "end": "340220"
  },
  {
    "text": "had a variance, which\nis the p 1 minus p. If I didn't remove this\np here, the mean here",
    "start": "340220",
    "end": "347620"
  },
  {
    "text": "would have been affected by p. And there's no table\nfor normal p 1.",
    "start": "347620",
    "end": "353041"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Oh, so\nthe square root of n terms come from.",
    "start": "353041",
    "end": "358500"
  },
  {
    "text": "So really you should view this. So there's a rule and sort\nof a quiet rule in math",
    "start": "358500",
    "end": "364780"
  },
  {
    "text": "that you don't write a\ndivided by b over c, right? You write c times a divided\nby b, because it looks nicer.",
    "start": "364780",
    "end": "372714"
  },
  {
    "text": "But the way you want\nto think about this is that this is x bar minus p\ndivided by the square root of p",
    "start": "372714",
    "end": "380600"
  },
  {
    "text": "1 minus p divided by n. And the reason is,\nbecause this is actually",
    "start": "380600",
    "end": "385629"
  },
  {
    "text": "the standard deviation of this-- oh sorry, x bar n. This is actually the standard\ndeviation of this guy,",
    "start": "385630",
    "end": "391510"
  },
  {
    "text": "and the square root of n comes\nfrom the [INAUDIBLE] average.",
    "start": "391510",
    "end": "396540"
  },
  {
    "text": "So the key thing\nwas that this thing, this limiting distribution\ndid not depend on anything",
    "start": "396540",
    "end": "402130"
  },
  {
    "text": "I don't know. And this is actually called\na pivotal distribution. It's pivotal.",
    "start": "402130",
    "end": "407690"
  },
  {
    "text": "I don't need anything. I don't need to know anything,\nand I can read it in a table. Sometimes there's going\nto be complicated things,",
    "start": "407690",
    "end": "414320"
  },
  {
    "text": "but now we have computers. The beauty about Gaussian is\nthat people have studied them to death, and you can\nopen any stats textbook,",
    "start": "414320",
    "end": "420049"
  },
  {
    "text": "and you will see a table\nagain that will tell you for each value of alpha\nyou're interested in, it will tell you what\nq alpha over 2 is.",
    "start": "420049",
    "end": "427220"
  },
  {
    "text": "But there might be some\ncrazy distributions, but as long as they\ndon't depend on anything,",
    "start": "427220",
    "end": "432440"
  },
  {
    "text": "we might actually\nbe able to simulate from them, and in particular\ncompute what q alpha over 2 is for any possible\nvalue [INAUDIBLE]..",
    "start": "432440",
    "end": "439157"
  },
  {
    "text": "And so that's what we're\ngoing to be trying to do. Finding pivotal distributions. How do we take this Xn bar,\nwhich is a good estimate,",
    "start": "439157",
    "end": "446060"
  },
  {
    "text": "and turn it into something\nwhich may be exactly or asymptotically\ndoes not depend",
    "start": "446060",
    "end": "451100"
  },
  {
    "text": "on any unknown parameter. So here is one way\nwe can actually-- so that's what we did for\nthe kiss example, right?",
    "start": "451100",
    "end": "458084"
  },
  {
    "text": "And here I mentioned,\nfor example, in the extreme case,\nwhen n was equal to 3 we would get a different\nthing, but here the CLT",
    "start": "458084",
    "end": "464240"
  },
  {
    "text": "would not be valid. And what that means is that\nmy pivotal distribution",
    "start": "464240",
    "end": "469520"
  },
  {
    "text": "is actually not the\nnormal distribution, but it might be something else.",
    "start": "469520",
    "end": "474620"
  },
  {
    "text": "And I said we can make\ntake exact computations. Well, let's see\nwhat it is, right? If I have three observations,\nso I'm going to have X1, X2, X3.",
    "start": "474620",
    "end": "486610"
  },
  {
    "text": "So now I take the\naverage of those guys. ",
    "start": "486610",
    "end": "493260"
  },
  {
    "text": "OK, so that's my estimate. How many values\ncan this guy take? ",
    "start": "493260",
    "end": "503125"
  },
  {
    "text": "It's a little bit of counting.  Four values.",
    "start": "503125",
    "end": "508529"
  },
  {
    "text": "How did you get to that number? ",
    "start": "508529",
    "end": "517590"
  },
  {
    "text": "OK, so each of these guys\ncan take value 0, 1, right? So the number of values\nthat it can take,",
    "start": "517590",
    "end": "523669"
  },
  {
    "text": "I mean, it's a little\nannoying, because then I have to sum them, right? So basically, I have to\ncount the number of 1's.",
    "start": "523669",
    "end": "531620"
  },
  {
    "text": "So how many 1's\ncan I get, right? Sorry I have to-- yeah, so this\nis the number of 1's that I--",
    "start": "531620",
    "end": "537330"
  },
  {
    "text": "OK, so let's look at that. So we get 0, 0, 0. 0, 0, 1. And then I get\nbasically three of them",
    "start": "537330",
    "end": "543350"
  },
  {
    "text": "that have just the\none in there, right?  So there's three of them.",
    "start": "543350",
    "end": "548920"
  },
  {
    "text": "How many of them\nhave exactly two 1's? 2. Sorry, 3, right?",
    "start": "548920",
    "end": "555269"
  },
  {
    "text": "So it's just this guy where\nI replaced the 0's and the 1. OK, so now I get--",
    "start": "555270",
    "end": "561389"
  },
  {
    "text": "so here I get three\nthat take the value 1, and one that gets the value 0. And then I get three\nthat take the value 2,",
    "start": "561390",
    "end": "568110"
  },
  {
    "text": "and then one that\ntakes the value 1. The value [? 0 ?] 1's, right? OK, so everybody knows what I'm\nmissing here is just the ones",
    "start": "568110",
    "end": "575870"
  },
  {
    "text": "here where I replaced\nthe 0's by 1's. So the number of values\nthat this thing can take is 1, 2, 3, 4.",
    "start": "575870",
    "end": "583079"
  },
  {
    "text": "So someone is counting\nmuch faster than me. And so those numbers, you've\nprobably seen them before, right?",
    "start": "583080",
    "end": "588539"
  },
  {
    "text": "1, 3, 3, 1, remember? And so essentially\nthose guys, it takes only three values,\nwhich are either 1/3, 1.",
    "start": "588539",
    "end": "598760"
  },
  {
    "text": "Sorry, 1/3. Oh OK, so it's 0, sorry.",
    "start": "598760",
    "end": "606399"
  },
  {
    "text": "1/3, 2/3, and 1. Those are the four possible\nvalues you can take.",
    "start": "606400",
    "end": "612790"
  },
  {
    "text": "And so now-- which is\nprobably much easier to count like that--\nand so now all I have to tell you\nif I want to describe",
    "start": "612790",
    "end": "618380"
  },
  {
    "text": "the distribution\nof this probability of this random variable,\nis just the probability that it takes each\nof these values.",
    "start": "618380",
    "end": "624990"
  },
  {
    "text": "So X bar 3 takes the\nvalue 0 probability",
    "start": "624990",
    "end": "630170"
  },
  {
    "text": "that X bar 3 takes the\nvalue 1/3, et cetera. If I give you each of\nthese possible values,",
    "start": "630170",
    "end": "636261"
  },
  {
    "text": "then you will be able to know\nexactly what the distribution is, and hopefully maybe\nto turn it into something",
    "start": "636262",
    "end": "641720"
  },
  {
    "text": "you can compute. Now the thing is that\nthose values will actually depend on the unknown p.",
    "start": "641720",
    "end": "647290"
  },
  {
    "text": "What is the unknown p here? What is the\nprobability that X bar 3 is equal to 0 for example? I'm sorry?",
    "start": "647290",
    "end": "653166"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Yeah, OK. So let's write it without\nmaking the computation So 1/8 is",
    "start": "653166",
    "end": "659930"
  },
  {
    "text": "probably not the\nright answer, right? For example, if p is equal to\n0, what is this probability?",
    "start": "659930",
    "end": "669267"
  },
  {
    "text": "1. If p is 1, what is\nthis probability? 0.",
    "start": "669267",
    "end": "674480"
  },
  {
    "text": "So it will depend on p. So the probability that\nthis thing is equal to 0, is just the probability\nthat all three of those guys",
    "start": "674480",
    "end": "680480"
  },
  {
    "text": "are equal to 0. The probability that X1 is equal\nto 0, and X2 is equal to 0, and X3 is equal to 0.",
    "start": "680480",
    "end": "685532"
  },
  {
    "text": "Now my things are\nindependent, so I do what I actually\nwant to do, which say the probability\nof the intersection is the product of the\nprobabilities, right?",
    "start": "685532",
    "end": "692330"
  },
  {
    "text": "So it's just the probability\nthat each of them is equal to 0 to the power of 3. And the probability that each\nof them, or say one of them",
    "start": "692330",
    "end": "698690"
  },
  {
    "text": "is equal to 0, is\njust 1 minus p. ",
    "start": "698690",
    "end": "705960"
  },
  {
    "text": "And then for this guy I just\nget the probability-- well, it's more complicated, because I\nhave to decide which one it is.",
    "start": "705960",
    "end": "711690"
  },
  {
    "text": "But those things are\njust the probability of some binomial random\nvariables, right? This is just a\nbinomial, X bar 3.",
    "start": "711690",
    "end": "720320"
  },
  {
    "text": "So if I look at X bar 3,\nand then I multiply it by 3, it's just this sum of\nindependent Bernoulli's",
    "start": "720320",
    "end": "725860"
  },
  {
    "text": "with parameter p. So this is actually a binomial\nwith parameter 3 and p.",
    "start": "725860",
    "end": "731696"
  },
  {
    "text": "And there's tables\nfor binomials, and they tell you all this. Now the thing is I want\nto invert this guy, right?",
    "start": "731696",
    "end": "738650"
  },
  {
    "text": "Somehow. This thing depends on p. I don't like it, so\nI'm going to have to find ways to get this\nthings depending on p,",
    "start": "738650",
    "end": "745873"
  },
  {
    "text": "and I could make all\nthese nasty computations, and spend hours doing this. But there's tricks\nto go around this.",
    "start": "745874",
    "end": "751330"
  },
  {
    "text": "There's upper bounds. Just like we just\nsaid, well, maybe I don't want to solve the\nsecond degree equation in p,",
    "start": "751330",
    "end": "756840"
  },
  {
    "text": "because it's just going to\ncapture maybe smaller order terms, right? Things that maybe won't make\na huge difference numerically.",
    "start": "756840",
    "end": "763930"
  },
  {
    "text": "You can check that in\nyour problem set one. Does it make a huge\ndifference numerically to solve the second\ndegree equation,",
    "start": "763930",
    "end": "770589"
  },
  {
    "text": "or to just use the\n[INAUDIBLE] p 1 minus p or even to plug\nin p hat instead of p.",
    "start": "770590",
    "end": "776050"
  },
  {
    "text": "Those are going to\nbe the-- problem set one is to make sure that you\nsee what magnitude of changes",
    "start": "776050",
    "end": "781540"
  },
  {
    "text": "you get by changing from\none method to the other. So what I wanted to\ngo to is something",
    "start": "781540",
    "end": "793420"
  },
  {
    "text": "where we can use\nsomething, which is just a little more brute force. So the probability\nthat-- so here",
    "start": "793420",
    "end": "799600"
  },
  {
    "text": "is this Hoeffding's inequality. We saw that. That's what we've\nfinished on last time. So Hoeffding's\ninequality is actually",
    "start": "799600",
    "end": "805120"
  },
  {
    "text": "one of the most\nuseful inequalities. If any one of you is doing\nanything really to algorithms,",
    "start": "805120",
    "end": "810130"
  },
  {
    "text": "you've seen that\ninequality before. It's extremely convenient\nthat it tells you something about bounded\nrandom variables,",
    "start": "810130",
    "end": "815650"
  },
  {
    "text": "and if you do algorithms\ntypically with things bounded. And that's the case of\nBernoulli's random variables, right? They're bounded between 0 and 1.",
    "start": "815650",
    "end": "822765"
  },
  {
    "text": "And so when I do\nthis thing, when I do Hoeffding's inequality,\nwhat this thing is telling me is for any given epsilon\nhere, for any given epsilon,",
    "start": "822765",
    "end": "833120"
  },
  {
    "text": "what is the probability\nthat Xn bar goes away from its expectation?",
    "start": "833120",
    "end": "838370"
  },
  {
    "text": "All right, then we saw that it\ndecreases somewhat similarly to the way a Gaussian\nwould look like.",
    "start": "838370",
    "end": "844560"
  },
  {
    "text": "So essentially what Hoeffding's\ninequality is telling me, is that I have this picture, when\nI have a Gaussian with mean u,",
    "start": "844560",
    "end": "858120"
  },
  {
    "text": "I know it looks\nlike this, right? What Hoeffding's\ninequality is telling me is that if I actually\ntake the average",
    "start": "858120",
    "end": "864780"
  },
  {
    "text": "of some bounded\nrandom variables, then their probability\ndistribution function or maybe",
    "start": "864780",
    "end": "870494"
  },
  {
    "text": "math function-- this thing\nmight not even have [INAUDIBLE] the density, but let's think\nof it as being a density just",
    "start": "870494",
    "end": "875540"
  },
  {
    "text": "for simplicity-- it's\ngoing to be something that's going to look like this.",
    "start": "875540",
    "end": "880895"
  },
  {
    "text": "It's going to be\nsomewhat-- well, sometimes it's going\nto have to escape just for the sake of\nhaving integral 1.",
    "start": "880895",
    "end": "886540"
  },
  {
    "text": "But it's essentially\ntelling me that those guys stay below those guys.",
    "start": "886540",
    "end": "892680"
  },
  {
    "text": "The probability that\nXn bar exceeds mu is bounded by\nsomething that decays",
    "start": "892680",
    "end": "898940"
  },
  {
    "text": "like to tail of Gaussian. So really that's the picture\nyou should have in mind. When I average bounded\nrandom variables,",
    "start": "898940",
    "end": "905740"
  },
  {
    "text": "I actually have something\nthat might be really rugged. It might not be smooth\nlike a Gaussian, but I know that it's always\nbounded by a Gaussian.",
    "start": "905740",
    "end": "912620"
  },
  {
    "text": "And what's nice about it\nis that when I actually start computing probability\nthat exceeds some number,",
    "start": "912620",
    "end": "917800"
  },
  {
    "text": "say alpha over 2, then I\nknow that this I can actually",
    "start": "917800",
    "end": "924339"
  },
  {
    "text": "get a number, which is just--",
    "start": "924340",
    "end": "929460"
  },
  {
    "text": "sorry, the probability\nthat it exceeds, yeah. So this number that I\nget here is actually going to be somewhat\nsmaller, right?",
    "start": "929460",
    "end": "935424"
  },
  {
    "text": "So that's going to be the q\nalpha over 2 for the Gaussian, and that's going to be the-- I don't know, r alpha over\n2 for this [? Bernoulli ?]",
    "start": "935424",
    "end": "941598"
  },
  {
    "text": "random variable. Like q prime or different q. So I can actually do\nthis without actually",
    "start": "941598",
    "end": "950149"
  },
  {
    "text": "taking any limits, right? This is valid for any n. I don't need to\nactually go to infinity. Now this seems a\nbit magical, right?",
    "start": "950149",
    "end": "957370"
  },
  {
    "text": "I mean, I just said\nwe need n to be, we discussed that we\nwanted n to be larger than 30 last time for\nthe central limit theorem",
    "start": "957370",
    "end": "963660"
  },
  {
    "text": "to kick in, and this\none seems to tell me I can do it for any n. Now there will be a price to pay\nis that I pick up this 2 over b",
    "start": "963660",
    "end": "972970"
  },
  {
    "text": "minus alpha squared. So that's the variance of the\nGaussian that I have, right?",
    "start": "972970",
    "end": "980421"
  },
  {
    "text": "Sort of. That's telling me what\nthe variance should be, and this is actually\nnot as nice. I pick factor 4\ncompared to the Gaussian",
    "start": "980421",
    "end": "987530"
  },
  {
    "text": "that I would get for that. So let's try to solve\nit for our case. So I just told you try it.",
    "start": "987530",
    "end": "993800"
  },
  {
    "text": "Did anybody try to do it?  So we started from\nthis last time, right?",
    "start": "993800",
    "end": "999070"
  },
  {
    "text": " And the reason was\nthat we could say that the probability that this\nthing exceeds q alpha over 2",
    "start": "999070",
    "end": "1006490"
  },
  {
    "text": "is alpha. So that was using CLT, so let's\njust keep it here, and see",
    "start": "1006490",
    "end": "1012000"
  },
  {
    "text": "what we would do differently.  What Hoeffding tells me is\nthat the probability that Xn",
    "start": "1012000",
    "end": "1018529"
  },
  {
    "text": "bar minus-- well, what is mu in this case?",
    "start": "1018530",
    "end": "1024265"
  },
  {
    "text": "It's p, right? It's just notation here. Mu was the average,\nbut we call it",
    "start": "1024265",
    "end": "1029280"
  },
  {
    "text": "p in the case of\nBernoulli's, exceeds-- let's just call it\nepsilon for a second.",
    "start": "1029280",
    "end": "1037220"
  },
  {
    "text": "So we said that this\nwas bounded by what? So Hoeffding tells me\nthat this is bounded by 2 times exponential minus 2.",
    "start": "1037220",
    "end": "1046058"
  },
  {
    "text": "Now the nice thing is that\nI pick up a factor n here, epsilon squared. And what is b minus a\nsquared for the Bernoulli's?",
    "start": "1046059",
    "end": "1053130"
  },
  {
    "text": "1. So I don't have a\ndenominator here. And I'm going to do\nexactly what I did here.",
    "start": "1053130",
    "end": "1058350"
  },
  {
    "text": "I'm going to set this\nguy to be equal to alpha.  So that if I get\nalpha here, then that",
    "start": "1058350",
    "end": "1066640"
  },
  {
    "text": "means that just\nsolving for epsilon, I'm going to have some number,\nwhich will play the role of q",
    "start": "1066640",
    "end": "1072600"
  },
  {
    "text": "alpha over 2, and\nthen I'm going to be able to just say that p\nis between X bar and minus",
    "start": "1072600",
    "end": "1078399"
  },
  {
    "text": "epsilon, and X bar\nn plus epsilon. OK, so let's do it. ",
    "start": "1078400",
    "end": "1085139"
  },
  {
    "text": "So we have to\nsolve the equation. ",
    "start": "1085140",
    "end": "1094572"
  },
  {
    "text": "2 exponential minus 2n\nepsilon squared equals alpha,",
    "start": "1094572",
    "end": "1100770"
  },
  {
    "text": "which means that-- so here I'm going to get,\nthere's a 2 right here.",
    "start": "1100770",
    "end": "1106542"
  },
  {
    "text": "So that means that I\nget alpha over 2 here. Then I take the\nlogs on both sides, and now let me just write it.",
    "start": "1106542",
    "end": "1112058"
  },
  {
    "text": " And then I want to\nsolve for epsilon.",
    "start": "1112058",
    "end": "1119430"
  },
  {
    "text": "So that means that epsilon\nis equal to square root log q over alpha divided by 2n.",
    "start": "1119430",
    "end": "1125860"
  },
  {
    "text": " Yes?",
    "start": "1125860",
    "end": "1131118"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nWhy is b minus a 1? Well, let's just look, right?",
    "start": "1131118",
    "end": "1137840"
  },
  {
    "text": "X lives in the\ninterval b minus a. So I can take b to be 25,\nand a to be my negative 42.",
    "start": "1137840",
    "end": "1146110"
  },
  {
    "text": "But I'm going to try to\nbe as sharp as I can. All right, so what\nis the smallest value you can think of such that\na Bernoulli random variable",
    "start": "1146110",
    "end": "1153340"
  },
  {
    "text": "is larger than or\nequal to this value? ",
    "start": "1153340",
    "end": "1159510"
  },
  {
    "text": "What values does a Bernoulli\nrandom variable take? 0 and 1.",
    "start": "1159510",
    "end": "1164530"
  },
  {
    "text": "So it takes values\nbetween 0 and 1. It just maxes the value.",
    "start": "1164530",
    "end": "1171280"
  },
  {
    "text": "Actually, this is the\nworst possible case for the Hoeffding inequality.",
    "start": "1171280",
    "end": "1178130"
  },
  {
    "text": "So now I just get this\none, and so now you tell me that I have this thing. So when I solve\nthis guy over there.",
    "start": "1178130",
    "end": "1183260"
  },
  {
    "text": "So combining this\nthing and this thing implies that the probability\nthat p lives between Xn",
    "start": "1183260",
    "end": "1193299"
  },
  {
    "text": "bar minus square root log 2\nover alpha divided by 2n and X",
    "start": "1193300",
    "end": "1201660"
  },
  {
    "text": "bar plus the square root log\n2 over alpha divided by 2n",
    "start": "1201660",
    "end": "1210970"
  },
  {
    "text": "is equal to?  I mean, is at least.",
    "start": "1210970",
    "end": "1216882"
  },
  {
    "text": "What is it at least equal to? ",
    "start": "1216882",
    "end": "1222929"
  },
  {
    "text": "Here this controls the\nprobability of them outside of this interval, right? It tells me the probability\nthat Xn bar is far from p",
    "start": "1222930",
    "end": "1231730"
  },
  {
    "text": "by more than epsilon. So there's a probability\nthat they're actually outside of the interval\nthat I just wrote. So it's 1 minus the probability\nof being in the interval.",
    "start": "1231730",
    "end": "1239650"
  },
  {
    "text": "So this is at least\n1 minus alpha. So I just use the fact that a\nprobability of the complement",
    "start": "1239650",
    "end": "1246340"
  },
  {
    "text": "is 1 minus the\nprobability of the set. And since I have an upper bound\non the probability of the set,",
    "start": "1246340",
    "end": "1253460"
  },
  {
    "text": "I have a lower bound on the\nprobability of the complement.",
    "start": "1253460",
    "end": "1259100"
  },
  {
    "text": "So now it's a bit different. Before, we actually wrote\nsomething that was--",
    "start": "1259100",
    "end": "1266640"
  },
  {
    "text": "so let me get it back. So if we go back to the example\nwhere we took the [INAUDIBLE]",
    "start": "1266640",
    "end": "1271990"
  },
  {
    "text": "over p, we got this guy. q alpha over square root of--",
    "start": "1271990",
    "end": "1279990"
  },
  {
    "text": "over 2 square root n. So we had Xn bar plus minus\nq alpha over 2 square root n. Actually, that was q alpha\nover 2n, I'm sorry about that.",
    "start": "1279990",
    "end": "1287340"
  },
  {
    "text": " And so now we have something\nthat replaces this q alpha,",
    "start": "1287340",
    "end": "1294540"
  },
  {
    "text": "and it's essentially square\nroot of 2 log 2 over alpha.",
    "start": "1294540",
    "end": "1300880"
  },
  {
    "text": "Because if I replace\nq alpha by square root of 2 log 2 over\nalpha, I actually",
    "start": "1300880",
    "end": "1307240"
  },
  {
    "text": "get exactly this thing here.  And so the question is,\nwhat would you guess?",
    "start": "1307240",
    "end": "1315970"
  },
  {
    "text": "Is this number, this margin,\nsquare root of log 2 over alpha",
    "start": "1315970",
    "end": "1321789"
  },
  {
    "text": "divided by 2n, is it smaller\nor larger than this guy? q alpha all over 2/3n.",
    "start": "1321790",
    "end": "1328915"
  },
  {
    "text": "Yes? Larger. Everybody agrees with this? Just qualitatively?",
    "start": "1328915",
    "end": "1334690"
  },
  {
    "text": "Right, because we just made a\nvery conservative statement. We do not use anything. This is true always.",
    "start": "1334690",
    "end": "1340100"
  },
  {
    "text": "So it can only be better. The reason in statistics where\nyou use those assumptions that n is large enough, that you\nhave this independence that you",
    "start": "1340100",
    "end": "1347590"
  },
  {
    "text": "like so much, and so you can\nactually have the central limit theorem kick in,\nall these things are for you to have\nenough assumptions",
    "start": "1347590",
    "end": "1355500"
  },
  {
    "text": "so that you can actually make\nsharper and sharper decisions. More and more\nconfident statement. And that's why there's all\nthis junk science out there,",
    "start": "1355500",
    "end": "1362540"
  },
  {
    "text": "because people make too much\nassumptions for their own good. They're saying,\nwell, let's assume that everything is the way I\nlove it, so that I can for sure",
    "start": "1362540",
    "end": "1370720"
  },
  {
    "text": "any minor change, I\nwill be able to say that's because I made an\nimportant scientific discovery",
    "start": "1370720",
    "end": "1375830"
  },
  {
    "text": "rather than, well, that\nwas just [INAUDIBLE] OK?",
    "start": "1375830",
    "end": "1382049"
  },
  {
    "text": "So now here's the fun moment. And actually let me tell you\nwhy we look at this thing.",
    "start": "1382050",
    "end": "1389110"
  },
  {
    "text": "So there's actually--\nwho has seen different types of convergence\nin the probability statistic",
    "start": "1389110",
    "end": "1394328"
  },
  {
    "text": "class?  [INAUDIBLE] students.",
    "start": "1394328",
    "end": "1400430"
  },
  {
    "text": "And so there's\ndifferent types of-- in the real numbers\nthere's very simple.",
    "start": "1400430",
    "end": "1405610"
  },
  {
    "text": "There's one\nconvergence, Xn turns to X. To start thinking\nabout functions, well, maybe you have\nuniform convergence,",
    "start": "1405610",
    "end": "1412230"
  },
  {
    "text": "you have pointwise convergence. So if you've done\nsome real analysis, you know there's different\ntypes of convergence you can think of.",
    "start": "1412230",
    "end": "1417790"
  },
  {
    "text": "And in the convergence\nof random variables, there's also different types,\nbut for different reasons. It's just because the\nquestion is, what do you",
    "start": "1417790",
    "end": "1424802"
  },
  {
    "text": "do with the randomness? When you see that something\nconverges to something, it probably means that\nyou're willing to tolerate",
    "start": "1424802",
    "end": "1430620"
  },
  {
    "text": "low probability things happening\nor where it doesn't happen, and on how you\nhandle those, creates",
    "start": "1430620",
    "end": "1436350"
  },
  {
    "text": "different types of convergence. So to be fair, in statistics the\nonly convergence we care about",
    "start": "1436350",
    "end": "1443340"
  },
  {
    "text": "is the convergence\nin distribution. That's this one. The one that comes from\nthe central limit theorem.",
    "start": "1443340",
    "end": "1449940"
  },
  {
    "text": "That's actually the weakest\npossible you could make. Which is good, because\nthat means it's going to happen more often.",
    "start": "1449940",
    "end": "1456149"
  },
  {
    "text": "And so why do we\nneed this thing? Because the only\nthing we really need to do is to say that\nwhen I start computing",
    "start": "1456150",
    "end": "1461580"
  },
  {
    "text": "probabilities on\nthis random variable, they're going to look\nlike probabilities on that random variable.",
    "start": "1461580",
    "end": "1467840"
  },
  {
    "text": "All right, so for example,\nthink of the following two random variables,\nx and minus x.",
    "start": "1467840",
    "end": "1481070"
  },
  {
    "text": "So this is the same\nrandom variable, and this one is negative. When I look at those\ntwo random variables,",
    "start": "1481070",
    "end": "1488049"
  },
  {
    "text": "think of them as a sequence,\na constant sequence. These two constant sequences\ndo not go to the same number,",
    "start": "1488050",
    "end": "1493970"
  },
  {
    "text": "right? One is plus-- one is x,\nthe other one is minus x. So unless x is the random\nvariable always equal to 0,",
    "start": "1493970",
    "end": "1501240"
  },
  {
    "text": "those two things are different. However, when I compute\nprobabilities on this guy, and when I compute probabilities\non that guy, they're the same.",
    "start": "1501240",
    "end": "1509010"
  },
  {
    "text": "Because x and minus x\nhave the same distribution just by symmetry of the\ngaps in random variables.",
    "start": "1509010",
    "end": "1515430"
  },
  {
    "text": "And so you can see\nthis is very weak. I'm not saying anything about\nthe two random variables being close to each other\nevery time I'm",
    "start": "1515430",
    "end": "1520566"
  },
  {
    "text": "going to flip my coin, right? Maybe I'm going to press my\ncomputer and say, what is x?",
    "start": "1520566",
    "end": "1525684"
  },
  {
    "text": "Well, it's 1.2. Negative x is going\nto be negative 1.2. Those things are\nfar apart, and it doesn't matter, because\nin average those things",
    "start": "1525685",
    "end": "1532230"
  },
  {
    "text": "are going to have the same\nprobabilities that's happening. And that's all we care\nabout in statistics. You need to realize that\nthis is what's important,",
    "start": "1532230",
    "end": "1537810"
  },
  {
    "text": "and why you need to know. Because you have it really good. If your problem is you really\ncare more about convergence",
    "start": "1537810",
    "end": "1543120"
  },
  {
    "text": "almost surely, which is probably\nthe strongest you can think of. So what we're going to do is\ntalk about different types",
    "start": "1543120",
    "end": "1548590"
  },
  {
    "text": "of convergence not to\njust reflect on the fact on how our life is good. It's just that the problem\nis that when the convergence",
    "start": "1548590",
    "end": "1556420"
  },
  {
    "text": "in distribution is so weak that\nit cannot do anything I want with it. In particular, I cannot\nsay that if X converges,",
    "start": "1556420",
    "end": "1564400"
  },
  {
    "text": "Xn converges in distribution,\nand Yn converges in distribution, then Xn plus\nYn converge in distribution",
    "start": "1564400",
    "end": "1570790"
  },
  {
    "text": "to the sum of their limits. I cannot do that. It's just too weak. Think of this example\nand it's preventing you",
    "start": "1570790",
    "end": "1576730"
  },
  {
    "text": "to do quite a lot of things.  So this is converge in\ndistribution to sum n 0, 1.",
    "start": "1576730",
    "end": "1586029"
  },
  {
    "text": "This is converge in\ndistribution to sum n 0, 1. But their sum is 0, and\nit's certainly not--",
    "start": "1586030",
    "end": "1591490"
  },
  {
    "text": "it doesn't look\nlike the sum of two independent Gaussian\nrandom variables, right? And so what we need is to\nhave stronger conditions here",
    "start": "1591490",
    "end": "1600220"
  },
  {
    "text": "and there, so that we can\nactually put things together. And we're going to have\nmore complicated formulas. One of the formulas,\nfor example,",
    "start": "1600220",
    "end": "1606550"
  },
  {
    "text": "is if I replace p by p\nhat in this denominator. We mentioned doing\nthis at some point.",
    "start": "1606550",
    "end": "1613470"
  },
  {
    "text": "So I would need that\np hat goes to p, but I need stronger\nthan n distributions",
    "start": "1613470",
    "end": "1619320"
  },
  {
    "text": "so that this happens. I actually need this to\nhappen in a stronger sense. So here are the first two\nstrongest sense in which",
    "start": "1619320",
    "end": "1627690"
  },
  {
    "text": "random variables can converge. The first one is almost surely.",
    "start": "1627690",
    "end": "1633140"
  },
  {
    "text": "And who has already seen\nthis notation little omega when they're talking\nabout random variables?",
    "start": "1633140",
    "end": "1639490"
  },
  {
    "text": "All right, so very few. So this little omega is-- so\nwhat is a random variable? A random variable is\nsomething that you measure",
    "start": "1639490",
    "end": "1645970"
  },
  {
    "text": "on something that's random. So the example I\nlike to think of is, if you take a ball\nof snow, and put it",
    "start": "1645970",
    "end": "1654910"
  },
  {
    "text": "in the sun for some time. You come back. It's going to have a\nrandom shape, right?",
    "start": "1654910",
    "end": "1659920"
  },
  {
    "text": "It's going to be a random\nblurb of something. But there's still a bunch of\nthings you can measure on it.",
    "start": "1659920",
    "end": "1665020"
  },
  {
    "text": "You can measure its volume. You can measure its\ninner temperature. You can measure\nits surface area.",
    "start": "1665020",
    "end": "1670210"
  },
  {
    "text": "All these things are\nrandom variables, but the ball itself is omega. That's the thing on which\nyou make your measurement.",
    "start": "1670210",
    "end": "1676900"
  },
  {
    "text": "And so a random variable is\njust a function of those omegas. Now why do we make all\nthese things fancy?",
    "start": "1676900",
    "end": "1683210"
  },
  {
    "text": "Because you cannot\ntake any function. This function has to be\nwhat's called measurable, and there's entire\ncourses on measure theory,",
    "start": "1683210",
    "end": "1689070"
  },
  {
    "text": "and not everything\nis measurable. And so that's why you have\nto be a little careful why not everything\nis measurable,",
    "start": "1689070",
    "end": "1694550"
  },
  {
    "text": "because you need some\nsort of nice property. So that the measure\nof something,",
    "start": "1694550",
    "end": "1699797"
  },
  {
    "text": "the union of two things, is less\nthan the sum of the measures, things like that. And so almost surely is telling\nyou that for most of the balls,",
    "start": "1699797",
    "end": "1710940"
  },
  {
    "text": "for most of the omegas,\nthat's the right-hand side. The probability of omega is\nsuch that those things converge",
    "start": "1710940",
    "end": "1717150"
  },
  {
    "text": "to each other is\nactually equal to 1. So it tells me that for almost\nall omegas, all the omegas,",
    "start": "1717150",
    "end": "1725620"
  },
  {
    "text": "if I put them together,\nI get something that has probability of 1. It might be that there are other\nones that have probability 0,",
    "start": "1725620",
    "end": "1730970"
  },
  {
    "text": "but what it's telling\nis that this thing happens for all possible\nrealization of the underlying thing.",
    "start": "1730970",
    "end": "1736340"
  },
  {
    "text": "That's very strong. It essentially says\nrandomness does not matter, because it's happening always.",
    "start": "1736340",
    "end": "1741390"
  },
  {
    "text": " Now convergence in\nprobability allows you to squeeze a little bit\nof probability under the rock.",
    "start": "1741390",
    "end": "1749179"
  },
  {
    "text": "It tells you I want the\nconvergence to hold, but I'm willing to let go\nof some little epsilon.",
    "start": "1749180",
    "end": "1757120"
  },
  {
    "text": "So I'm willing to allow Tn\nto be less than epsilon.",
    "start": "1757120",
    "end": "1763500"
  },
  {
    "text": "Tn minus T to be-- sorry,\nto be larger than epsilon. But the problem is they\nwant this to go to 0",
    "start": "1763500",
    "end": "1769360"
  },
  {
    "text": "as well as n goes to\ninfinity, but for each n this thing does not\nhave to be 0, which is different from here, right?",
    "start": "1769360",
    "end": "1776250"
  },
  {
    "text": "So this probability\nhere is fine. So it's a little weaker, but\nit's a slightly different one.",
    "start": "1776250",
    "end": "1784460"
  },
  {
    "text": "I'm not going to ask you\nto learn and show that one is weaker than the other one. But just know that these\nare two different types.",
    "start": "1784460",
    "end": "1791010"
  },
  {
    "text": "This one is actually much\neasier to check than this one. ",
    "start": "1791010",
    "end": "1802550"
  },
  {
    "text": "Then there's something\ncalled convergence in Lp. So this one is the fact that\nit embodies the following fact.",
    "start": "1802550",
    "end": "1809200"
  },
  {
    "text": "If I give you a random\nvariable with mean 0, and I tell you that its\nvariance is going to 0, right? You have a sequence of random\nvariables, their mean is 0,",
    "start": "1809200",
    "end": "1816795"
  },
  {
    "text": "their expectation is 0, but\ntheir variance is going to 0. So think of Gaussian random\nvariables with mean 0,",
    "start": "1816795",
    "end": "1823500"
  },
  {
    "text": "and a variance\nthat shrinks to 0. And this random variable\nconverges to a spike at 0,",
    "start": "1823500",
    "end": "1829260"
  },
  {
    "text": "so it converges to 0, right? And so what I mean by that is\nthat to have this convergence,",
    "start": "1829260",
    "end": "1835659"
  },
  {
    "text": "all I had to tell you was that\nthe variance was going to 0. And so in L2 this is really\nwhat it's telling you.",
    "start": "1835660",
    "end": "1841700"
  },
  {
    "text": "It's telling you, well, if\nthe variance is going to 0-- well, it's for any\nrandom variable T,",
    "start": "1841700",
    "end": "1846870"
  },
  {
    "text": "so here what I describe\nwas for a deterministic. So Tn goes to a random variable\nT. If you look at the square--",
    "start": "1846870",
    "end": "1855340"
  },
  {
    "text": "the expectation of the square\ndistance, and it goes to 0. But you don't have to limit\nyourself to the square.",
    "start": "1855340",
    "end": "1860540"
  },
  {
    "text": "You can take power of three. You can take power\n67.6, power of 9 pi.",
    "start": "1860540",
    "end": "1866780"
  },
  {
    "text": "You take whatever power you\nwant, it can be fractional. It has to be lower than 1, and\nthat's the convergence in Lp.",
    "start": "1866780",
    "end": "1873920"
  },
  {
    "text": "But we mostly care\nabout integer p. And then here's our star, the\nconvergence in distribution,",
    "start": "1873920",
    "end": "1880107"
  },
  {
    "text": "and that's just the\none that tells you that when I start computing\nprobabilities on the Tn,",
    "start": "1880107",
    "end": "1887289"
  },
  {
    "text": "they're going to look very close\nto the probabilities on the T. So that was our Tn with\nthis guy, for example,",
    "start": "1887290",
    "end": "1894410"
  },
  {
    "text": "and T was this standard\nGaussian distribution. Now here, this is\nnot any probability. This is just the probability\nthen less than or equal to x.",
    "start": "1894410",
    "end": "1902440"
  },
  {
    "text": "But if you remember\nyour probability class, if you can compute\nthose probabilities, you can compute\nany probabilities just by subtracting and just\nbuilding things together.",
    "start": "1902440",
    "end": "1910033"
  },
  {
    "start": "1910034",
    "end": "1915230"
  },
  {
    "text": "Well, I need this for all x's,\nso I want this for each x, So you fix x, and then you\nmake the limit go to infinity.",
    "start": "1915230",
    "end": "1921520"
  },
  {
    "text": "You make n go to\ninfinity, and I want this for the point x's at which\nthe cumulative distribution function of T is continuous.",
    "start": "1921520",
    "end": "1928230"
  },
  {
    "text": "There might be jumps, and that\nI don't actually care for those.",
    "start": "1928230",
    "end": "1935350"
  },
  {
    "text": "All right, so here I mentioned\nit for random variables. If you're interested,\nthere's also random vectors. A random vector is just a\ntable of random variables.",
    "start": "1935350",
    "end": "1943429"
  },
  {
    "text": "You can talk about\nrandom matrices. And you can talk about\nrandom whatever you want. Every time you have\nan object that's",
    "start": "1943430",
    "end": "1948919"
  },
  {
    "text": "just collecting real\nnumbers, you can just plug random variables in there. And so there's all these\ndefinitions that [? extend. ?]",
    "start": "1948920",
    "end": "1957050"
  },
  {
    "text": "So where I see you\nsee an absolute value, we'll see a norm. Things like this.",
    "start": "1957050",
    "end": "1963040"
  },
  {
    "text": "So I'm sure this might\nlook scary a little bit, but really what we are going to\nuse is only the last one, which",
    "start": "1963040",
    "end": "1969009"
  },
  {
    "text": "as you can see is\njust telling you that the probabilities\nconverge to the probabilities. But I'm going to need the other\nones every once in a while.",
    "start": "1969010",
    "end": "1975830"
  },
  {
    "text": "And the reason is,\nwell, OK, so here I'm actually going to the\nimportant characterizations",
    "start": "1975830",
    "end": "1982970"
  },
  {
    "text": "of the convergence\nin distribution, which is R convergence style.",
    "start": "1982970",
    "end": "1988110"
  },
  {
    "text": "So i converge in\ndistribution if and only if for any function that's\ncontinuous and bounded,",
    "start": "1988110",
    "end": "1994070"
  },
  {
    "text": "when I look at the\nexpectation of f of Tn, this converges to the\nexpectation of f of T. OK,",
    "start": "1994070",
    "end": "1999870"
  },
  {
    "text": "so this is just those two\nthings are actually equivalent.",
    "start": "1999870",
    "end": "2005127"
  },
  {
    "text": "Sometimes it's easier to check\none, easier to check the other, but in this class you won't\nhave to prove that something converges in distribution\nother than just combining",
    "start": "2005127",
    "end": "2013380"
  },
  {
    "text": "our existing\nconvergence results. And then the last one which\nis equivalent to the above two",
    "start": "2013380",
    "end": "2020020"
  },
  {
    "text": "is, anybody knows what the\nname of this quantity is? This expectation here?",
    "start": "2020020",
    "end": "2025120"
  },
  {
    "text": "What is it called? The characteristic\nfunction, right? And so this i is the complex\ni, and is the complex number.",
    "start": "2025120",
    "end": "2032679"
  },
  {
    "text": "And so it's\nessentially telling me that, well, rather\nthan actually looking at all bounded and continuous\nbut real functions,",
    "start": "2032680",
    "end": "2038620"
  },
  {
    "text": "I can actually look\nat one specific family",
    "start": "2038620",
    "end": "2043630"
  },
  {
    "text": "of complex functions, which\nare the functions that maps T to E to the ixT\nfor x and R. That's",
    "start": "2043630",
    "end": "2052980"
  },
  {
    "text": "a much smaller\nfamily of functions. All possible continuous\nembedded functions has many more elements\nthan just the real element.",
    "start": "2052980",
    "end": "2061590"
  },
  {
    "text": "And so now I can show that\nif I limit myself to do it, it's actually sufficient. ",
    "start": "2061590",
    "end": "2068138"
  },
  {
    "text": "So those three things are used\nall over the literature just to show things.",
    "start": "2068139",
    "end": "2073360"
  },
  {
    "text": "In particular, if you're\ninterested in deep digging a little more mathematically,\nthe central limit theorem",
    "start": "2073360",
    "end": "2079510"
  },
  {
    "text": "is going to be so important. Maybe you want to read\nabout how to prove it. We're not going to\nprove it in this class. There's probably at least five\ndifferent ways of proving it,",
    "start": "2079510",
    "end": "2089800"
  },
  {
    "text": "but the most canonical one, the\none that you find in textbooks, is the one that actually\nuses the third element.",
    "start": "2089800",
    "end": "2095980"
  },
  {
    "text": "So you just look at the\ncharacteristic function of the square root of\nn Xn bar minus say mu,",
    "start": "2095980",
    "end": "2104400"
  },
  {
    "text": "and you just expand the thing,\nand this is what you get. And you will see\nthat in the end, you will get the characteristic\nfunction of a Gaussian.",
    "start": "2104400",
    "end": "2113820"
  },
  {
    "text": "Why a Gaussian? Why does it kick in? Well, because what is the\ncharacteristic function of a Gaussian? Does anybody remember the\ncharacteristic function",
    "start": "2113820",
    "end": "2119760"
  },
  {
    "text": "of a standard Gaussian? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nYeah, well, I mean there's two pi's and stuff\nthat goes away, right?",
    "start": "2119760",
    "end": "2127760"
  },
  {
    "text": "A Gaussian is a random variable. A characteristic\nfunction is a function, and so it's not really itself.",
    "start": "2127760",
    "end": "2133039"
  },
  {
    "text": "It looks like itself. Anybody knows what\nthe actual formula is? Yeah. AUDIENCE: [INAUDIBLE]",
    "start": "2133040",
    "end": "2139584"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nE to the minus? AUDIENCE: E to the\nminus x squared over 2. PHILIPPE RIGOLLET: Exactly. E to the minus x squared over 2.",
    "start": "2139584",
    "end": "2144960"
  },
  {
    "text": "But this x squared\nover 2 is actually just the second order expansion\nin the Taylor expansion. And that's why the\nGaussian is so important.",
    "start": "2144960",
    "end": "2151534"
  },
  {
    "text": "It's just the second\norder Taylor expansion. And so you can check it out. I think Terry Tao has\nsome stuff on his blog,",
    "start": "2151534",
    "end": "2158349"
  },
  {
    "text": "and there's a bunch\nof different proofs. But if you want to prove\nconvergence in distribution, you very likely are going to\nuse one this three right here.",
    "start": "2158350",
    "end": "2167900"
  },
  {
    "text": "So let's move on. ",
    "start": "2167900",
    "end": "2173050"
  },
  {
    "text": "This is when I said\nthat this convergence is weaker than that convergence. This is what I meant.",
    "start": "2173050",
    "end": "2178869"
  },
  {
    "text": "If you have convergence\nin one style, it implies convergence\nin the other stuff. So the first [INAUDIBLE] is that\nif Tn converges almost surely,",
    "start": "2178870",
    "end": "2186505"
  },
  {
    "text": "this a dot s dot\nmeans almost surely, then it also converges\nin probability and actually the\ntwo limits, which",
    "start": "2186505",
    "end": "2192900"
  },
  {
    "text": "are this random variable\nT, are equal almost surely. Basically what it means is\nthat whatever you measure one",
    "start": "2192900",
    "end": "2199750"
  },
  {
    "text": "is going to be the same that\nyou measure on the other one. So that's very strong. So that means that\nconvergence almost surely",
    "start": "2199750",
    "end": "2207960"
  },
  {
    "text": "is stronger than\nconvergence in probability. If you're converge in Lp\nthen you also converge",
    "start": "2207960",
    "end": "2213570"
  },
  {
    "text": "in Lq for sum q less than p. So if you converge in L2,\nyou'll also converge in L1.",
    "start": "2213570",
    "end": "2219480"
  },
  {
    "text": "If you converge in L67,\nyou converge in L2. If you're converge\nin L infinity,",
    "start": "2219480",
    "end": "2224920"
  },
  {
    "text": "you converge in Lp for anything. And so, again, limits are equal.",
    "start": "2224920",
    "end": "2232390"
  },
  {
    "text": "And then when you\nconverge in distribution, when you converge\nin probability, you also converge\nin distribution.",
    "start": "2232390",
    "end": "2238860"
  },
  {
    "text": "OK, so almost surely\nimplies probability. Lp implies probability.",
    "start": "2238860",
    "end": "2244400"
  },
  {
    "text": "Probability implies\ndistribution. And here note that\nI did not write, and the limits are\nequal almost surely.",
    "start": "2244400",
    "end": "2250890"
  },
  {
    "text": "Why?  Because the convergence\nin distribution",
    "start": "2250890",
    "end": "2257070"
  },
  {
    "text": "is actually not telling you\nthat your random variable is converging to\nanother random variable. It's telling you\nthat the distribution",
    "start": "2257070",
    "end": "2262433"
  },
  {
    "text": "of your random variable is\nconverging to a distribution. And think of this, guys. x and minus x.",
    "start": "2262433",
    "end": "2269064"
  },
  {
    "text": "The central limit\ntheorem tells me that I'm converging to some\nstandard Gaussian distribution, but am I converging to x or\nam I converging to minus x?",
    "start": "2269064",
    "end": "2277334"
  },
  {
    "text": "It's not well identified. It's any random variable\nthat has this distribution. So there's no way\nthe limits are equal.",
    "start": "2277334",
    "end": "2284109"
  },
  {
    "text": "Their distributions are\ngoing to be the same, but they're not the same limit. Is that clear for everyone?",
    "start": "2284110",
    "end": "2289970"
  },
  {
    "text": "So in a way, convergence\nin distribution is really not a convergence\nof a random variable",
    "start": "2289970",
    "end": "2295177"
  },
  {
    "text": "towards another random variable. It's just telling you\nthe limiting distribution of your random\nvariable [INAUDIBLE]",
    "start": "2295177",
    "end": "2300390"
  },
  {
    "text": "which is enough for us. And one thing that's\nactually really nice is this continuous\nmapping theorem, which",
    "start": "2300390",
    "end": "2308790"
  },
  {
    "text": "essentially tells you that-- so this is one of the\ntheorems that we like, because they tell\nus you can do what",
    "start": "2308790",
    "end": "2313950"
  },
  {
    "text": "you feel like you want to do. So if I have Tn that goes to\nT, f of Tn goes to f of T,",
    "start": "2313950",
    "end": "2319829"
  },
  {
    "text": "and this is true for\nany of those convergence except for Lp.",
    "start": "2319830",
    "end": "2325650"
  },
  {
    "text": " But they have to have f,\nwhich is continuous, otherwise",
    "start": "2325650",
    "end": "2331490"
  },
  {
    "text": "weird stuff can happen. So this is going to be\nconvenient, because here I",
    "start": "2331490",
    "end": "2338150"
  },
  {
    "text": "don't have X to n minus p. I have a continuous function. It's between a linear\nfunction of Xn minus p, but I could think of like\neven crazier stuff to do,",
    "start": "2338150",
    "end": "2345800"
  },
  {
    "text": "and it would still be true. If I took the square, it would\nconverge to something that looks like its distribution.",
    "start": "2345800",
    "end": "2351600"
  },
  {
    "text": "It's the same as\nthe distribution of a square Gaussian. So this is a mouthful,\nthese two slides--",
    "start": "2351600",
    "end": "2358435"
  },
  {
    "text": "actually this particular\nslide is a mouthful. What I have in my head since\nI was pretty much where you're",
    "start": "2358435",
    "end": "2364620"
  },
  {
    "text": "sitting, is this diagram. So what it tells me-- so it's\nactually voluntarily cropped,",
    "start": "2364620",
    "end": "2372099"
  },
  {
    "text": "so you can start from\nany Lq you want large. And then as you\ndecrease the index,",
    "start": "2372100",
    "end": "2378030"
  },
  {
    "text": "you are actually\nimplying, implying, implying until you imply\nconvergence in probability. Convergence almost surely\nimplies convergence",
    "start": "2378030",
    "end": "2384849"
  },
  {
    "text": "in probability, and everything\ngoes to the [? sync, ?] that is convergence\nin distribution.",
    "start": "2384850",
    "end": "2392589"
  },
  {
    "text": "So everything implies\nconvergence in distribution. So that's basically rather than\nremembering those formulas,",
    "start": "2392590",
    "end": "2397800"
  },
  {
    "text": "this is really the diagram\nyou want to remember.  All right, so why do we bother\nlearning about those things.",
    "start": "2397800",
    "end": "2406589"
  },
  {
    "text": "That's because of this\nlimits and operations. Operations and limits. If I have a sequence\nof real numbers,",
    "start": "2406590",
    "end": "2413710"
  },
  {
    "text": "and I know that Xn converges\nto X and Yn converges to Y, then I can start doing all\nmy manipulations and things",
    "start": "2413710",
    "end": "2420051"
  },
  {
    "text": "are happy. I can add stuff. I can multiply stuff. But it's not true always for\nconvergence in distribution.",
    "start": "2420051",
    "end": "2428049"
  },
  {
    "text": "But it is, what's\nnice, it's actually true for convergence\nalmost surely. Convergence almost surely\neverything is true.",
    "start": "2428049",
    "end": "2435250"
  },
  {
    "text": "It's just impossible\nto make it fail. But convergence in probability\nis not always everything,",
    "start": "2435250",
    "end": "2441080"
  },
  {
    "text": "but at least you can actually\nadd stuff and multiply stuff. And it will still give\nyou the sum of the n,",
    "start": "2441080",
    "end": "2446600"
  },
  {
    "text": "and the product of the n. You can even take the ratio\nif V is not 0 of course.",
    "start": "2446600",
    "end": "2455590"
  },
  {
    "text": "If the limit is not\n0, then actually you need Vn to be not 0 as well. ",
    "start": "2455590",
    "end": "2461440"
  },
  {
    "text": "You can actually prove\nthis last statement, right? Because it's a combination\nof the first statement",
    "start": "2461440",
    "end": "2468620"
  },
  {
    "text": "of the second one, and the\ncontinuous mapping theorem. Because the function\nthat maps x to 1",
    "start": "2468620",
    "end": "2474770"
  },
  {
    "text": "over x on everything\nbut 0, is continuous. And so 1 over Vn\nconverges to 1 over V,",
    "start": "2474770",
    "end": "2484559"
  },
  {
    "text": "and then I can multiply\nthose two things. So you actually knew that one. But really this is\nnot what matters,",
    "start": "2484560",
    "end": "2490760"
  },
  {
    "text": "because this is something that\nyou will do whatever happens. If I don't tell you you cannot\ndo it, well, you will do it.",
    "start": "2490760",
    "end": "2497786"
  },
  {
    "text": "But in general\nthose things don't apply to convergence\nin distribution unless the pair itself is known\nto converge in distribution.",
    "start": "2497786",
    "end": "2504390"
  },
  {
    "text": "Remember when I said that\nthese things apply to vectors, then you need to actually\nsay that the vector converges",
    "start": "2504390",
    "end": "2511150"
  },
  {
    "text": "in distributions to\nthe limiting factor. Now this tells\nyou in particular, since the cumulative\ndistribution function is not",
    "start": "2511150",
    "end": "2517340"
  },
  {
    "text": "defined for vectors,\nI would have to actually use one of the\nother distributions, one",
    "start": "2517340",
    "end": "2522610"
  },
  {
    "text": "of the other criteria,\nwhich is convergence of characteristic\nfunctions or convergence of a function of bounded\ncontinuous function",
    "start": "2522610",
    "end": "2531099"
  },
  {
    "text": "of the random variable. 0.2 or 0.3, but 0.1 is not\ngoing get you anywhere.",
    "start": "2531100",
    "end": "2537154"
  },
  {
    "text": "But this is something\nthat's going to be too hard for us to\ndeal with, so we're actually going to rely on the\nfact that we have",
    "start": "2537154",
    "end": "2543742"
  },
  {
    "text": "something that's even better. There's something\nthat is waiting for us at the end of his lecture, which\nis called Slutsky's that says",
    "start": "2543742",
    "end": "2549163"
  },
  {
    "text": "that if V, in this case,\nconverges in probability but U converge in distribution,\nI can actually still do that.",
    "start": "2549163",
    "end": "2556040"
  },
  {
    "text": "I actually don't\nneed both of them to converge in probability. I actually need only one of\nthem to converge in probability",
    "start": "2556040",
    "end": "2561204"
  },
  {
    "text": "to make this statement. But two sum. So let's go to another example.",
    "start": "2561204",
    "end": "2567060"
  },
  {
    "text": "So I just want to make sure that\nwe keep on doing statistics. And every time we're going\nto just do a little bit too much probability, I'm\ngoing to reset the pressure,",
    "start": "2567060",
    "end": "2574202"
  },
  {
    "text": "and start doing\nstatistics again. All right, so assume\nyou observe the times",
    "start": "2574202",
    "end": "2579460"
  },
  {
    "text": "the inter-arrival time\nof the T at Kendall.",
    "start": "2579460",
    "end": "2584589"
  },
  {
    "text": "So this is not the arrival time. It's not like 7:56, 8:15.",
    "start": "2584590",
    "end": "2589980"
  },
  {
    "text": "No, it's really the\ninter-arrival time, right? So say the next T is\narriving in six minutes.",
    "start": "2589980",
    "end": "2597299"
  },
  {
    "text": "So let's say [INAUDIBLE] bound. And so you have this\ninter-arrival time.",
    "start": "2597300",
    "end": "2603250"
  },
  {
    "text": "So those are numbers say,\n3, 4, 5, 4, 3, et cetera. So I have this\nsequence of numbers.",
    "start": "2603250",
    "end": "2609490"
  },
  {
    "text": "So I'm going to\nobserve this, and I'm going to try to infer what\nis the rate of T's going out",
    "start": "2609490",
    "end": "2616050"
  },
  {
    "text": "of the station from this. So I'm going to assume\nthat these things are mutually independent.",
    "start": "2616050",
    "end": "2623160"
  },
  {
    "text": "That's probably not\ncompletely true. Again, it just means\nthat what it would mean is that two consecutive\ninter-arrival times are",
    "start": "2623160",
    "end": "2629100"
  },
  {
    "text": "independent. I mean, you can make it\nindependent if you want, but again, this\nindependent assumption is for us to be happy and safe.",
    "start": "2629100",
    "end": "2636180"
  },
  {
    "text": "Unless someone comes\nwith overwhelming proof that it's not independent and\nfar from being independent,",
    "start": "2636180",
    "end": "2641465"
  },
  {
    "text": "then yes, you have a problem. But it might be the fact\nthat it's actually-- if you have a T that's one hour late.",
    "start": "2641466",
    "end": "2649240"
  },
  {
    "text": "If an inter-arrival time is\none hour, then the other T, either they fixed\nit, and it's going",
    "start": "2649240",
    "end": "2654300"
  },
  {
    "text": "to be just 30 seconds behind,\nor they haven't fixed it, then it's going to be\nanother hour behind. So they're not\nexactly independent,",
    "start": "2654300",
    "end": "2660780"
  },
  {
    "text": "but they are when things\nwork well and approximate. And so now I need to model\na random variable that's",
    "start": "2660780",
    "end": "2667579"
  },
  {
    "text": "positive, maybe\nnot upper bounded. I mean, people complain\nenough that this thing can be really large. And so one thing that people\nlike for inter-arrival times",
    "start": "2667580",
    "end": "2674810"
  },
  {
    "text": "is exponential distribution. So that's a positive\nrandom variable. Looks like an exponential\non the right-hand slide,",
    "start": "2674810",
    "end": "2680463"
  },
  {
    "text": "on the positive line. And so it decays\nvery fast towards 0. The probability that\nyou have very large values exponentially small, and\nthere's a [INAUDIBLE] lambda",
    "start": "2680463",
    "end": "2689079"
  },
  {
    "text": "that controls how\nexponential is defined. It's exponential minus\nlambda times something. And so we're going\nto assume that they",
    "start": "2689080",
    "end": "2696270"
  },
  {
    "text": "have the same distribution,\nthe same random variable. So they're IID, because\nthey are independent, and they're identically\ndistributed.",
    "start": "2696270",
    "end": "2701810"
  },
  {
    "text": "They all have this exponential\nwith parameter lambda, and I'm going to try to\nlearn something about lambda. What is the estimated\nvalue of lambda,",
    "start": "2701810",
    "end": "2708790"
  },
  {
    "text": "and can I build a confidence\ninterval for lambda. So we observe n arrival times.",
    "start": "2708790",
    "end": "2716470"
  },
  {
    "text": "So as I said, the\nmutual independence is plausible, but not\ncompletely justified.",
    "start": "2716470",
    "end": "2724055"
  },
  {
    "text": "The fact that\nthey're exponential is actually something that\npeople like in all this what's called queuing theory. So exponentials\narise a lot when you",
    "start": "2724055",
    "end": "2731040"
  },
  {
    "text": "talk about inter-arrival times. It's not about\nthe bus, but where it's very important is call\ncenters, service, servers where",
    "start": "2731040",
    "end": "2741780"
  },
  {
    "text": "tasks come, and people\nwant to know how long it's going to take to serve a task.",
    "start": "2741780",
    "end": "2747450"
  },
  {
    "text": "So when I call at\na center, nobody knows how long I'm going to stay\non the phone with this person.",
    "start": "2747450",
    "end": "2752710"
  },
  {
    "text": "But it turns out that\nempirically exponential distributions have been\nvery good at modeling this. And what it means is\nthat they're actually--",
    "start": "2752710",
    "end": "2758630"
  },
  {
    "text": "you have this\nmemoryless property. It's kind of crazy if\nyou think about it. What does that thing say?",
    "start": "2758630",
    "end": "2764611"
  },
  {
    "text": "Let's parse it. That's the probability. So this is condition on the\nfact that T1 is larger than T.",
    "start": "2764611",
    "end": "2772619"
  },
  {
    "text": "So T1 is just say the\nfirst arrival time. That means that\nconditionally on the fact that I've been waiting\nfor the first T, well,",
    "start": "2772620",
    "end": "2779700"
  },
  {
    "text": "the first [INAUDIBLE]. Well, I should probably-- the\nfirst subway for more than T",
    "start": "2779700",
    "end": "2787440"
  },
  {
    "text": "conditionally-- so I've been\nthere T minutes already. Then the probability that\nI wait for s more minutes.",
    "start": "2787440",
    "end": "2793126"
  },
  {
    "text": "So that's the probability\nthat T1 is learned, and the time that we've\nalready waited plus x.",
    "start": "2793126",
    "end": "2798439"
  },
  {
    "text": "Given that I've been\nwaiting for T minutes, really I wait for\ns more minutes, is actually the probability\nthat I wait for s minutes total.",
    "start": "2798439",
    "end": "2806416"
  },
  {
    "text": "It's completely memoryless. It doesn't remember how\nlong have you been waiting. The probability does not change. You can have waited for\ntwo hours, the probability",
    "start": "2806416",
    "end": "2813450"
  },
  {
    "text": "that it takes\nanother 10 minutes is going to be the\nsame as if you had been waiting for zero minutes.",
    "start": "2813450",
    "end": "2819250"
  },
  {
    "text": "And that's something\nthat's actually part of your problem set. Very easy to compute. This is just an\nanalytical property.",
    "start": "2819250",
    "end": "2825730"
  },
  {
    "text": "And you just\nmanipulate functions, and you see that this thing\njust happen to be true, and that's something\nthat people like.",
    "start": "2825730",
    "end": "2831840"
  },
  {
    "text": "Because that's also\nsomething that benefit. And also what we like is\nthat this thing is positive",
    "start": "2831840",
    "end": "2837539"
  },
  {
    "text": "almost surely, which is good\nwhen you model arrival times. To be fair, we're not\ngoing to be that careful.",
    "start": "2837540",
    "end": "2843132"
  },
  {
    "text": "Because sometimes\nwe are just going to assume that something\nfollows a normal distribution.",
    "start": "2843132",
    "end": "2849010"
  },
  {
    "text": "And in particular,\nI mean, I don't know if we're going to\ngo into that details, but a good thing that you\ncan model with a Gaussian",
    "start": "2849010",
    "end": "2854830"
  },
  {
    "text": "distribution are\nheights of students. But technically with\npositive probability,",
    "start": "2854830",
    "end": "2860720"
  },
  {
    "text": "you can have a negative\nGaussian random variable, right? And the probability being it's\nprobably 10 to the minus 25,",
    "start": "2860720",
    "end": "2868550"
  },
  {
    "text": "but it's positive. But it's good enough\nfor us for our modeling. So this thing is nice, but this\nis not going to be required.",
    "start": "2868550",
    "end": "2874242"
  },
  {
    "text": "When you're modeling\npositive random variables, you don't always have to use\npositive distributions that are supported on positive numbers.",
    "start": "2874242",
    "end": "2881465"
  },
  {
    "text": "You can use distributions\nlike Gaussian.  So now this exponential\ndistribution of T1, Tn",
    "start": "2881465",
    "end": "2889817"
  },
  {
    "text": "they have the same\nparameter, and that means that in average they have\nthe same inter-arrival time. So this lambda is\nactually the expectation.",
    "start": "2889817",
    "end": "2896890"
  },
  {
    "text": "And what I'm just saying\nis that they're identically distributed means\nthat I mean some sort of a stationary regime,\nand it's not always true.",
    "start": "2896890",
    "end": "2904279"
  },
  {
    "text": "I have to look at a\nshorter period of time, because at rush\nhour and 11:00 PM clearly those average\ninter-arrival times",
    "start": "2904279",
    "end": "2911200"
  },
  {
    "text": "are going to be different\nSo it means that I am really focusing maybe on rush hour. ",
    "start": "2911200",
    "end": "2918567"
  },
  {
    "text": "Sorry, I said it's lambda. It's actually 1 over lambda. I always mix the two. All right, so you have\nthe density of T1.",
    "start": "2918567",
    "end": "2924300"
  },
  {
    "text": "So f of T is this. So it's on the\npositive real line.",
    "start": "2924300",
    "end": "2929400"
  },
  {
    "text": "The fact that I have strictly\npositive or larger [INAUDIBLE] to 0 doesn't make\nany difference.",
    "start": "2929400",
    "end": "2934542"
  },
  {
    "text": "So this is the density. So it's lambda E to the minus\nlambda T. The lambda in front just ensures that\nwhen I integrate",
    "start": "2934542",
    "end": "2939690"
  },
  {
    "text": "this function between 0\nand infinity, I get 1. And you can see, it decays like\nexponential minus lambda T.",
    "start": "2939690",
    "end": "2946160"
  },
  {
    "text": "So if I were to draw it, it\nwould just look like this. ",
    "start": "2946160",
    "end": "2953630"
  },
  {
    "text": "So at 0, what\nvalue does it take? Lambda.",
    "start": "2953630",
    "end": "2959750"
  },
  {
    "text": "And then I decay like\nexponential minus lambda T. So this is 0, and\nthis is f of T.",
    "start": "2959750",
    "end": "2970839"
  },
  {
    "text": "So very small probability\nof being very large. Of course, it depends on lambda. Now the expectation, you\ncan compute the expectation",
    "start": "2970840",
    "end": "2977916"
  },
  {
    "text": "of this thing, right? So you integrate T\ntimes f of T. This is part of the little sheet\nthat I gave you last time.",
    "start": "2977916",
    "end": "2984130"
  },
  {
    "text": "This is one of the\nthings you should be able to do blindfolded. And then you get the expectation\nof T1 is 1 over lambda.",
    "start": "2984130",
    "end": "2991276"
  },
  {
    "text": "That's what comes out. So as I actually tell many of\nmy students, 99% of statistics",
    "start": "2991276",
    "end": "2997630"
  },
  {
    "text": "is replacing\nexpectations by averages. And so what you're tempted to do\nis say, well, if in average I'm",
    "start": "2997630",
    "end": "3002940"
  },
  {
    "text": "supposed to see 1 over lambda,\nI have 15 observations. I'm just going to average\nthose observations, and I'm going to see something\nthat should be close to 1",
    "start": "3002940",
    "end": "3010143"
  },
  {
    "text": "over lambda. So statistics is about\nreplacing averages, expectations with\naverages, and that's we do.",
    "start": "3010143",
    "end": "3017950"
  },
  {
    "text": "So Tn bar here, which is\nthe average of the Ti's, is a pretty good estimator\nfor 1 over lambda.",
    "start": "3017950",
    "end": "3025060"
  },
  {
    "text": "So if I want an\nestimate for lambda, then I need to\ntake 1 over Tn bar.",
    "start": "3025060",
    "end": "3030190"
  },
  {
    "text": "So here is one estimator. I did it without much\nprinciple except that I just",
    "start": "3030190",
    "end": "3036340"
  },
  {
    "text": "want to replace\nexpectations by averages, and then I fixed the problem\nthat I was actually estimating 1 over lambda by lambda.",
    "start": "3036340",
    "end": "3043030"
  },
  {
    "text": "But you could come up with\nother estimators, right? But let's say this is my way\nof getting to that estimator.",
    "start": "3043030",
    "end": "3049730"
  },
  {
    "text": "Just like I didn't give you\nany principled way of getting p hat, which is Xn bar\nin the kiss example.",
    "start": "3049730",
    "end": "3054770"
  },
  {
    "text": "But that's the\nnatural way to do it. Everybody is completely\nshocked by this approach?",
    "start": "3054770",
    "end": "3061380"
  },
  {
    "text": "All right, so let's do this. So what can I say about the\nproperties of this estimator lambda hat?",
    "start": "3061380",
    "end": "3068130"
  },
  {
    "text": "Well, I know that Tn bar\nis going to 1 over lambda by the law of large number.",
    "start": "3068130",
    "end": "3074214"
  },
  {
    "text": "It's an average. It converges to the\nexpectation both almost surely, and in probability. So the first one is the\nstrong law of large number,",
    "start": "3074214",
    "end": "3081310"
  },
  {
    "text": "the second one is the\nweak law of large number. I can apply the strong one. I have enough conditions.",
    "start": "3081310",
    "end": "3086799"
  },
  {
    "text": "And hence, what do I apply\nso that 1 over Tn bar actually goes to lambda?",
    "start": "3086800",
    "end": "3095110"
  },
  {
    "text": "So I said hence. What is hence? What is it based on? AUDIENCE: [INAUDIBLE]",
    "start": "3095110",
    "end": "3103455"
  },
  {
    "text": "PHILIPPE RIGOLLET Yeah,\ncontinuous mapping theorem, right? So I have this\nfunction 1 over x. I just apply this function.",
    "start": "3103455",
    "end": "3109180"
  },
  {
    "text": "So if it was 1 over\nlambda squared, I would have the\nsame thing that would happen just because\nthe function 1 over x",
    "start": "3109180",
    "end": "3114688"
  },
  {
    "text": "is continuous away from 0. And now the central\nlimit theorem",
    "start": "3114688",
    "end": "3120299"
  },
  {
    "text": "is also telling me\nsomething about lambda. About Tn bar, right? It's telling me that if\nI look at my average, I remove the expectation here.",
    "start": "3120300",
    "end": "3128519"
  },
  {
    "text": "So if I do Tn bar\nminus my expectation, rescale by this guy here,\nthen this thing is going",
    "start": "3128520",
    "end": "3135820"
  },
  {
    "text": "to converge to some\nGaussian random variable, but here I have this\nlambda to the negative 1--",
    "start": "3135820",
    "end": "3141260"
  },
  {
    "text": "to the negative 2\nhere, and that's because they did not\ntell you that if you compute the variance--",
    "start": "3141260",
    "end": "3148730"
  },
  {
    "text": "so from this, you\ncan probably extract. ",
    "start": "3148730",
    "end": "3154307"
  },
  {
    "text": "So if I have X that follows\nsome exponential distribution with parameter lambda.",
    "start": "3154308",
    "end": "3160350"
  },
  {
    "text": "Well, let's call it T. So we know that T in\nexpectation, the expectation",
    "start": "3160350",
    "end": "3166540"
  },
  {
    "text": "of T is 1 over lambda. What is the variance of T? ",
    "start": "3166540",
    "end": "3176690"
  },
  {
    "text": "You should be able to read\nit from the thing here. ",
    "start": "3176690",
    "end": "3189984"
  },
  {
    "text": "1 over lambda squared. That's what you actually\nread in the variance, because the central limit\ntheorem is really telling you",
    "start": "3189984",
    "end": "3196530"
  },
  {
    "text": "the distribution\ngoes through this n. But this numbers and this\nnumber you can read, right?",
    "start": "3196530",
    "end": "3203739"
  },
  {
    "text": "If you look at the expectation\nof this guy it's-- of this guy comes out. This is 1 over lambda\nminus 1 over lambda. That's why you read the 0.",
    "start": "3203739",
    "end": "3210359"
  },
  {
    "text": "And if you look at the\nvariance of the dot, you get n times the\nvariance of this average.",
    "start": "3210360",
    "end": "3216330"
  },
  {
    "text": "Variance of the average is\npicking up a factor 1 over n. So the n cancels. And then I'm left with only\none of the variances, which",
    "start": "3216330",
    "end": "3222881"
  },
  {
    "text": "is 1 over lambda squared. OK, so we're not going\nto do that in details,",
    "start": "3222881",
    "end": "3228130"
  },
  {
    "text": "because, again, this is just\na pure calculus exercise. But this is if you compute\nintegral of lambda e",
    "start": "3228130",
    "end": "3234700"
  },
  {
    "text": "to the minus t lambda\ntimes t squared. Actually t minus 1\nover lambda squared",
    "start": "3234700",
    "end": "3241754"
  },
  {
    "text": "dt between 0 and infinity. You will see that this thing\nis 1 over lambda squared.",
    "start": "3241754",
    "end": "3247774"
  },
  {
    "start": "3247774",
    "end": "3254157"
  },
  {
    "text": "How would I do this? ",
    "start": "3254157",
    "end": "3260539"
  },
  {
    "text": "Configuration by\n[INAUDIBLE] or you know it. All right.",
    "start": "3260540",
    "end": "3266100"
  },
  {
    "text": "So this is what the central\nlimit theorem tells me. So this gives me\nif I solve this,",
    "start": "3266100",
    "end": "3271619"
  },
  {
    "text": "and I plug in so I can\nmultiply by lambda and solve, it would give me somewhat\na confidence interval for 1",
    "start": "3271620",
    "end": "3280100"
  },
  {
    "text": "over lambda. If we just think\nof 1 over lambda as being the p\nthat I had before,",
    "start": "3280100",
    "end": "3286590"
  },
  {
    "text": "this would give me a\ncentral limit theorem for-- ",
    "start": "3286590",
    "end": "3291663"
  },
  {
    "text": "sorry, a confidence\ninterval for 1 over lambda. So I'm hiding a little\nbit under the rug the fact that I have\nto still define it.",
    "start": "3291664",
    "end": "3298540"
  },
  {
    "text": "Let's just actually\ngo through this. I see some of you are\nuncomfortable with this, so let's just do it.",
    "start": "3298540",
    "end": "3304883"
  },
  {
    "text": "So what we've just proved\nby the central limit theorem is that the\nprobability, that's square root of n Tn minus 1 over\nlambda exceeds q alpha over 2",
    "start": "3304884",
    "end": "3321180"
  },
  {
    "text": "is approximately\nequal to alpha, right? That's just the statement of\nthe central limit theorem,",
    "start": "3321180",
    "end": "3327180"
  },
  {
    "text": "and by approximately equal I\nmean as n goes to infinity. ",
    "start": "3327180",
    "end": "3334230"
  },
  {
    "text": "Sorry I did not\nwrite it correctly. I still have to divide\nby square root of 1",
    "start": "3334230",
    "end": "3339440"
  },
  {
    "text": "over lambda squared, which is\nthe standard deviation, right? And we said that\nthis is a bit ugly.",
    "start": "3339440",
    "end": "3344620"
  },
  {
    "text": "So let's just do it\nthe way it should be. So multiply all these\nthings by lambda.",
    "start": "3344620",
    "end": "3350290"
  },
  {
    "text": "So that means now that\nthe absolute value, so",
    "start": "3350290",
    "end": "3356020"
  },
  {
    "text": "with probability 1 minus\nalpha asymptotically, I have that square root of\nn times lambda Tn minus 1",
    "start": "3356020",
    "end": "3367870"
  },
  {
    "text": "is less than or equal\nto q alpha over 2. ",
    "start": "3367870",
    "end": "3374930"
  },
  {
    "text": "So what it means is that, oh,\nI have negative q alpha over 2",
    "start": "3374930",
    "end": "3380020"
  },
  {
    "text": "less than square root of n. Let me divide by\nsquare root of n here.",
    "start": "3380020",
    "end": "3385224"
  },
  {
    "text": "lambda Tn minus\n1 q alpha over 2.",
    "start": "3385224",
    "end": "3394620"
  },
  {
    "text": "And so now what I have is that\nI get that lambda is between--",
    "start": "3394620",
    "end": "3401891"
  },
  {
    "text": "that's Tn bar-- is between\n1 plus q alpha over 2",
    "start": "3401891",
    "end": "3410410"
  },
  {
    "text": "divided by root n. And the whole thing\nis divided by Tn bar,",
    "start": "3410410",
    "end": "3417470"
  },
  {
    "text": "and same thing on the other side\nexcept I have 1 minus q alpha",
    "start": "3417470",
    "end": "3424010"
  },
  {
    "text": "over 2 divided by root\nn divided by Tn bar. ",
    "start": "3424010",
    "end": "3432980"
  },
  {
    "text": "So it's kind of a weird\nshape, but it's still of the form 1 over Tn bar\nplus or minus something.",
    "start": "3432980",
    "end": "3440238"
  },
  {
    "text": "But this something\ndepends on Tn bar itself. And that's actually normal,\nbecause Tn bar is not only",
    "start": "3440238",
    "end": "3446230"
  },
  {
    "text": "giving me information\nabout the mean, but it's also giving me\ninformation about the variance.",
    "start": "3446230",
    "end": "3451360"
  },
  {
    "text": "So it should definitely come\nin the size of my error bars.",
    "start": "3451360",
    "end": "3457570"
  },
  {
    "text": "And that's the way it comes\nin this fairly natural way. Everybody agrees?",
    "start": "3457570",
    "end": "3463810"
  },
  {
    "text": "So now I have actually\nbuilt a confidence interval. But what I want to show\nyou with this example is,",
    "start": "3463810",
    "end": "3470769"
  },
  {
    "text": "can I translate this\nin a central limit theorem for something that\nconverges to lambda, right?",
    "start": "3470770",
    "end": "3477520"
  },
  {
    "text": "I know that Tn bar\nconverges to 1 over lambda, but I also know that 1 over\nTn bar converges to lambda.",
    "start": "3477520",
    "end": "3485260"
  },
  {
    "text": "So do I have a central limit\ntheorem for 1 over Tn bar? Technically no, right?",
    "start": "3485260",
    "end": "3491490"
  },
  {
    "text": "Central limit theorems are about\naverages, and 1 over an average is not an average. But there's something that\nstatisticians like a lot,",
    "start": "3491490",
    "end": "3500520"
  },
  {
    "text": "and it's called\nthe Delta method. The Delta method\nis really something that's telling you\nthat you can actually",
    "start": "3500520",
    "end": "3507200"
  },
  {
    "text": "take a function of\nan average, and let it go to the function\nof the limit,",
    "start": "3507200",
    "end": "3512569"
  },
  {
    "text": "and you still have a\ncentral limit theorem. And the factor or the\nprice to pay for this is something which depends on\nthe derivative of the function.",
    "start": "3512570",
    "end": "3524040"
  },
  {
    "text": "And so let's just\ngo through this, and it's, again, just like\nthe proof of the central limit theorem.",
    "start": "3524040",
    "end": "3529640"
  },
  {
    "text": "And actually in many of those\nasymptotic statistics results, this is actually just\na Taylor expansion,",
    "start": "3529640",
    "end": "3535834"
  },
  {
    "text": "and here it's not\neven the second order, it's actually the\nfirst order, all right? So I'm just going to do linear\napproximation of this function.",
    "start": "3535834",
    "end": "3542183"
  },
  {
    "text": " So let's do it. So I have that g of Tn bar--",
    "start": "3542183",
    "end": "3552950"
  },
  {
    "text": "actually let's use the\nnotation of this slide, which is Zn and theta. So what I know is that Zn\nminus theta square root of n",
    "start": "3552950",
    "end": "3564250"
  },
  {
    "text": "goes to some Gaussian,\nthis standard Gaussian.",
    "start": "3564250",
    "end": "3569454"
  },
  {
    "text": "No, not standard. OK, so that's the assumptions. And what I want to show is\nsome convergence of g of Zn",
    "start": "3569454",
    "end": "3580502"
  },
  {
    "text": "to g of theta. So I'm not going to\nmultiply by root n just yet.",
    "start": "3580502",
    "end": "3586350"
  },
  {
    "text": "So I'm going to do a first\norder Taylor expansion. So what it is telling me is that\nthis is equal to Zn minus theta",
    "start": "3586350",
    "end": "3597040"
  },
  {
    "text": "times g prime of,\nlet's call it theta bar where theta bar is\nsomewhere between say",
    "start": "3597040",
    "end": "3606200"
  },
  {
    "text": "Zn and theta, for sum. ",
    "start": "3606200",
    "end": "3613980"
  },
  {
    "text": "OK, so if theta is less than\nZn you just permute those two. So that's what the\nTaylor first order Taylor",
    "start": "3613980",
    "end": "3621169"
  },
  {
    "text": "expansion tells me. There exists a theta bar\nthat's between the two values at which I'm expanding\nso that those two things are",
    "start": "3621169",
    "end": "3626911"
  },
  {
    "text": "equal. Is everybody shocked? No? So that's standard\nTaylor expansion.",
    "start": "3626912",
    "end": "3636350"
  },
  {
    "text": "Now I'm going to\nmultiply by root n. ",
    "start": "3636350",
    "end": "3644519"
  },
  {
    "text": "And so that's going to be what? That's going to be\nroot n Zn minus theta.",
    "start": "3644519",
    "end": "3650200"
  },
  {
    "text": "Ah-ha, that's something I like. Times g prime of theta bar.",
    "start": "3650200",
    "end": "3657130"
  },
  {
    "text": " Now the central limit\ntheorem tells me that this goes to what?",
    "start": "3657130",
    "end": "3662904"
  },
  {
    "text": " Well, this goes to sum n\n0 sigma squared, right?",
    "start": "3662904",
    "end": "3672370"
  },
  {
    "text": "That was the first\nline over there. This guy here, well,\nit's not clear, right?",
    "start": "3672370",
    "end": "3680520"
  },
  {
    "text": "Actually it is. Let's start with this guy. What does theta bar go to?",
    "start": "3680520",
    "end": "3688450"
  },
  {
    "text": "Well, I know that Zn\nis going to theta. ",
    "start": "3688450",
    "end": "3693660"
  },
  {
    "text": "Just because, well, that's\nmy law of large numbers. Zn is going to\ntheta, which means",
    "start": "3693660",
    "end": "3701010"
  },
  {
    "text": "that theta bar is sandwiched\nbetween two values that converge to theta.",
    "start": "3701010",
    "end": "3706910"
  },
  {
    "text": "So that means that theta bar\nconverges to theta itself as n goes to infinity. That's just the law\nof large numbers.",
    "start": "3706910",
    "end": "3714940"
  },
  {
    "text": "Everybody agrees? Just because it's\nsandwiched, right? So I have Zn.",
    "start": "3714940",
    "end": "3721180"
  },
  {
    "text": "I have theta, and theta\nbar is somewhere here. The picture might be reversed.",
    "start": "3721180",
    "end": "3726900"
  },
  {
    "text": "It might be that Zn end\nis larger than theta. But the law of large\nnumber tells me that this guy is not moving,\nbut this guy is moving that way.",
    "start": "3726900",
    "end": "3734050"
  },
  {
    "text": "So you know when\nn is [INAUDIBLE],, there's very little\nwiggle room for theta bar, and it can only get to theta.",
    "start": "3734050",
    "end": "3739975"
  },
  {
    "text": " And I call it the\nsandwich theorem,",
    "start": "3739975",
    "end": "3745310"
  },
  {
    "text": "or just find your\nfavorite food in there. So this guy goes\nto theta, and now I",
    "start": "3745310",
    "end": "3751716"
  },
  {
    "text": "need to make an extra\nassumption, which is that g prime is continuous.",
    "start": "3751716",
    "end": "3758601"
  },
  {
    "text": "And if g prime is continuous,\nthen g prime of theta bar goes to g prime of theta.",
    "start": "3758601",
    "end": "3764630"
  },
  {
    "text": "So this thing goes\nto g prime of theta. ",
    "start": "3764630",
    "end": "3772580"
  },
  {
    "text": "But I have an issue here. Is that now I have\nsomething that converges in distribution\nand something",
    "start": "3772580",
    "end": "3777859"
  },
  {
    "text": "that converges in say-- I mean, this converges almost\nsurely or saying probability",
    "start": "3777860",
    "end": "3784200"
  },
  {
    "text": "just to be safe. And this one converges\nin distribution.",
    "start": "3784200",
    "end": "3789820"
  },
  {
    "text": "And I want to combine them. But I don't have a\nslide that tells me I'm allowed to take the product\nof something that converges",
    "start": "3789820",
    "end": "3795660"
  },
  {
    "text": "in distribution, and something\nthat converges in probability. This does not exist. Actually, if\nanything it told me,",
    "start": "3795660",
    "end": "3801450"
  },
  {
    "text": "do not do anything with things\nthat converge in distribution. And so that gets us to our--",
    "start": "3801450",
    "end": "3812770"
  },
  {
    "text": "OK, so I'll come back\nto this in a second. And that gets us to something\ncalled Slutsky's theorem.",
    "start": "3812770",
    "end": "3819560"
  },
  {
    "text": "And Slutsky's theorem tells us\nthat in very specific cases, you can do just that.",
    "start": "3819560",
    "end": "3824740"
  },
  {
    "text": "So you have two sequences\nof random variables, Xn bar, that's Xn that converges to\nX. And Yn that converges to Y,",
    "start": "3824740",
    "end": "3833370"
  },
  {
    "text": "but Y is not anything. Y is not any random variable. So X converges in\nthis distribution.",
    "start": "3833370",
    "end": "3839089"
  },
  {
    "text": "Sorry, I forgot to mention,\nthis is very important. Xn converges in distribution,\nY converges in probability.",
    "start": "3839090",
    "end": "3844920"
  },
  {
    "text": "And we know that in generality\nwe cannot combine those two things, but Slutsky tells\nus that if the limit of Y is",
    "start": "3844920",
    "end": "3851272"
  },
  {
    "text": "a constant, meaning it's\nnot a random variable, but it's a\ndeterministic number 2, just a fixed number that's\nnot a random variable,",
    "start": "3851272",
    "end": "3858940"
  },
  {
    "text": "then you can combine them. Then you can sum them, and\nthen you can multiply them.",
    "start": "3858940",
    "end": "3864869"
  },
  {
    "text": " I mean, actually you can do\nwhatever combination you want,",
    "start": "3864869",
    "end": "3871290"
  },
  {
    "text": "because it actually implies\nthat X, the vector Xn, Yn converges to the vector Xc.",
    "start": "3871290",
    "end": "3879250"
  },
  {
    "text": "OK, so here I just\ntook two combinations. They are very convenient for\nus, the sum and the product so I could do other\nstuff like the ratio",
    "start": "3879250",
    "end": "3885850"
  },
  {
    "text": "if c is not 0, things like that. ",
    "start": "3885850",
    "end": "3891190"
  },
  {
    "text": "So that's what\nSlutsky does for us. So what you're going to have to\nwrite a lot in your homework, in your mid-terms, by Slutsky.",
    "start": "3891190",
    "end": "3898880"
  },
  {
    "text": "I know some people are very\ngenerous with their by Slutsky. They just do numerical\napplications,",
    "start": "3898880",
    "end": "3905940"
  },
  {
    "text": "mu is equal to 6, and\ntherefore by Slutsky mu square is equal to 36. All right, so don't do that.",
    "start": "3905940",
    "end": "3911690"
  },
  {
    "text": "Just use, write Slutsky when\nyou're actually using Slutsky. But this is something that's\nvery important for us,",
    "start": "3911690",
    "end": "3917540"
  },
  {
    "text": "and it turns out\nthat you're going to feel like you can write\nby Slutsky all the time, because that's going to\nwork for us all the time.",
    "start": "3917540",
    "end": "3923362"
  },
  {
    "text": "Everything we're going\nto see is actually going to be where we're going\nto have to combine stuff. Since we only rely on\nconvergence from distribution",
    "start": "3923362",
    "end": "3930260"
  },
  {
    "text": "arising from the\ncentral limit theorem, we're actually going to have\nto rely on something that allows us to combine them,\nand the only thing we know",
    "start": "3930260",
    "end": "3936920"
  },
  {
    "text": "is Slutsky. So we better hope\nthat this thing works. So why Slutsky works for us. Can somebody tell\nme why Slutsky works",
    "start": "3936920",
    "end": "3943640"
  },
  {
    "text": "to combine those two guys? So this one is converging\nin distribution.",
    "start": "3943640",
    "end": "3948820"
  },
  {
    "text": "This one is converging\nin probability, but to a deterministic number.",
    "start": "3948820",
    "end": "3954710"
  },
  {
    "text": "g prime of theta is a\ndeterministic number. I don't know what theta is, but\nit's certainly deterministic.",
    "start": "3954710",
    "end": "3962200"
  },
  {
    "text": "All right, so I can combine\nthem, multiply them. So that's just the second\nline of that in particular.",
    "start": "3962200",
    "end": "3968740"
  },
  {
    "text": "All right, everybody is with me? So now I'm allowed to do this. You can actually--\nyou will see something",
    "start": "3968740",
    "end": "3975048"
  },
  {
    "text": "like counterexample\nquestions in your problem set just so that you\ncan convince yourself. It's always a good thing. I don't like to\ngive them, because I",
    "start": "3975048",
    "end": "3981150"
  },
  {
    "text": "think it's much better\nfor you to actually come to the counterexample yourself. Like what can go wrong\nif Y is not a random--",
    "start": "3981150",
    "end": "3995670"
  },
  {
    "text": "sorry, if Y is not a-- sorry, if c is not the constant,\nbut it's a random variable.",
    "start": "3995670",
    "end": "4002572"
  },
  {
    "text": "You can figure that out. All right, so let's go back. So we have now this Delta\nmethod that tells us",
    "start": "4002572",
    "end": "4009040"
  },
  {
    "text": "that now I have a\ncentral limit theorem for functions of averages,\nand not just for averages.",
    "start": "4009040",
    "end": "4015500"
  },
  {
    "text": "So the only price to pay\nis this derivative there. ",
    "start": "4015500",
    "end": "4020600"
  },
  {
    "text": "So, for example, if g is\njust a linear function, then I'm going to have a\nconstant multiplication.",
    "start": "4020600",
    "end": "4027860"
  },
  {
    "text": "If g is a quadratic\nfunction, then I'm going to have theta squared\nthat shows up there.",
    "start": "4027860",
    "end": "4033710"
  },
  {
    "text": "Things like that. So just think of what\nkind of applications you could have for this. Here are the functions\nthat we're interested in,",
    "start": "4033710",
    "end": "4039769"
  },
  {
    "text": "is x maps to 1 over x. What is the derivative\nof this guy? ",
    "start": "4039769",
    "end": "4045947"
  },
  {
    "text": "What is the derivative\nof 1 over x? Negative 1 over\nx squared, right?",
    "start": "4045947",
    "end": "4051120"
  },
  {
    "text": "That's the thing we're going\nto have to put in there. And so this is what we get.",
    "start": "4051120",
    "end": "4057630"
  },
  {
    "text": "So now when I'm actually\ngoing to write this,",
    "start": "4057630",
    "end": "4064259"
  },
  {
    "text": "so if I want to show square root\nof n lambda hat minus lambda.",
    "start": "4064260",
    "end": "4071272"
  },
  {
    "text": "That's my application, right? This is actually 1 over Tn, and\nthis is 1 over 1 over lambda.",
    "start": "4071272",
    "end": "4079150"
  },
  {
    "text": "So the function g of x\nis 1 over x in this case.",
    "start": "4079150",
    "end": "4085510"
  },
  {
    "text": "So now I have this thing. So I know that by\nthe Delta method-- oh, and I knew\nthat Tn, remember,",
    "start": "4085510",
    "end": "4091240"
  },
  {
    "text": "square root of Tn\nminus 1 over lambda",
    "start": "4091240",
    "end": "4096790"
  },
  {
    "text": "was going to sum\nnormal with mean 0 and variance 1 over\nlambda squared, right?",
    "start": "4096790",
    "end": "4101932"
  },
  {
    "text": "So the sigma square over there\nis 1 over lambda squared. So now this thing goes to what?",
    "start": "4101932",
    "end": "4107370"
  },
  {
    "text": "Sum normal. What is going to be the mean? 0.",
    "start": "4107370",
    "end": "4112690"
  },
  {
    "text": " And what is the variance? So the variance is going--",
    "start": "4112690",
    "end": "4118270"
  },
  {
    "text": "I'm going to pick up\nthis guy, 1 over lambda squared, and then I'm going to\nhave to take g prime of what?",
    "start": "4118270",
    "end": "4126930"
  },
  {
    "text": "Of 1 over lambda, right? That's my theta. ",
    "start": "4126930",
    "end": "4132839"
  },
  {
    "text": "So I have g of theta,\nwhich is 1 over theta. So I'm going to have g\nprime of 1 over lambda.",
    "start": "4132840",
    "end": "4138405"
  },
  {
    "text": "And what is g prime\nof 1 over lambda? ",
    "start": "4138406",
    "end": "4145028"
  },
  {
    "text": "So we said that g prime is 1\nover negative 1 over x squared. So it's negative 1 over\n1 over lambda squared--",
    "start": "4145029",
    "end": "4153884"
  },
  {
    "text": " sorry, squared. ",
    "start": "4153885",
    "end": "4161870"
  },
  {
    "text": "Which is nice, because\ng can be decreasing. So that would be annoying\nto have a negative variance. And so g prime is\nnegative 1 over, and so",
    "start": "4161870",
    "end": "4169250"
  },
  {
    "text": "what I get eventually is\nlambda squared up here, but then I square it again.",
    "start": "4169250",
    "end": "4176369"
  },
  {
    "text": "So this whole thing\nhere becomes what? Can somebody tell me\nwhat the final result is?",
    "start": "4176370",
    "end": "4181688"
  },
  {
    "text": " Lambda squared right? So it's lambda 4\ndivided by lambda 2.",
    "start": "4181688",
    "end": "4187322"
  },
  {
    "start": "4187323",
    "end": "4195179"
  },
  {
    "text": "So that's what's written there. And now I can just do my\ngood old computation for a--",
    "start": "4195179",
    "end": "4204460"
  },
  {
    "start": "4204460",
    "end": "4210610"
  },
  {
    "text": "I can do a good computation\nfor a confidence interval. All right, so let's just\ngo from the second line.",
    "start": "4210610",
    "end": "4217880"
  },
  {
    "text": "So we know that lambda\nhat minus lambda is less than, we've done\nthat several times already.",
    "start": "4217880",
    "end": "4223980"
  },
  {
    "text": "So it's q alpha over 2-- sorry, I should put alpha\nover 2 over this thing, right? So that's really the quintile\nof what our alpha over 2 times",
    "start": "4223980",
    "end": "4231025"
  },
  {
    "text": "lambda divided by\nsquare root of n. All right, and so that means\nthat my confidence interval",
    "start": "4231025",
    "end": "4239610"
  },
  {
    "text": "should be this, lambda hat. Lambda belongs to lambda\nplus or minus q alpha",
    "start": "4239610",
    "end": "4247670"
  },
  {
    "text": "over 2 lambda divided\nby root n, right? So that's my\nconfidence interval.",
    "start": "4247670",
    "end": "4253640"
  },
  {
    "text": "But again, it's not\nvery suitable, because-- sorry, that's lambda hat.",
    "start": "4253640",
    "end": "4259292"
  },
  {
    "text": "Because they don't\nknow how to compute it. So now I'm going to\nrequest from the audience",
    "start": "4259292",
    "end": "4264510"
  },
  {
    "text": "some remedies for this. What do you suggest we do? ",
    "start": "4264510",
    "end": "4272860"
  },
  {
    "text": "What is the laziest\nthing I can do? ",
    "start": "4272860",
    "end": "4278272"
  },
  {
    "text": "Anybody? Yeah. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET Replace\nlambda by lambda hat.",
    "start": "4278272",
    "end": "4283290"
  },
  {
    "text": "What justifies\nfor me to do this? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET\nYeah, and Slutsky",
    "start": "4283290",
    "end": "4289060"
  },
  {
    "text": "tells me I can actually do\nit, because Slutsky tells me, where does this lambda\ncome from, right?",
    "start": "4289060",
    "end": "4295210"
  },
  {
    "text": "This lambda comes from here. That's the one that's here. So actually I could\nrewrite this entire thing",
    "start": "4295210",
    "end": "4301809"
  },
  {
    "text": "as square root of n lambda hat\nminus lambda divided by lambda",
    "start": "4301810",
    "end": "4307000"
  },
  {
    "text": "converges to sum n 0, 1. Now if I replace this by\nlambda hat, what I have is",
    "start": "4307000",
    "end": "4315560"
  },
  {
    "text": "that this is actually really\nthe original one times",
    "start": "4315560",
    "end": "4321600"
  },
  {
    "text": "lambda divided by lambda hat. And this converges\nto n 0, 1, right?",
    "start": "4321600",
    "end": "4327510"
  },
  {
    "text": "And now what you're telling\nme is, well, this guy I know it converges to n 0, 1,\nand this guy is converging to 1",
    "start": "4327510",
    "end": "4335360"
  },
  {
    "text": "by the law of large number. But this one is converging to 1,\nwhich happens to be a constant. It converges in probability,\nso by Slutsky I can actually",
    "start": "4335360",
    "end": "4342860"
  },
  {
    "text": "take the product and still\nmaintain my conversion to distribution to\na standard Gaussian.",
    "start": "4342860",
    "end": "4349070"
  },
  {
    "text": "So you can always do this. Every time you replace\nsome p by p hat,",
    "start": "4349070",
    "end": "4354080"
  },
  {
    "text": "as long as their\nratio goes to 1, which is going to be guaranteed\nby the law of large number, you're actually\ngoing to be fine.",
    "start": "4354080",
    "end": "4360381"
  },
  {
    "text": "And that's where we're\ngoing to use Slutsky a lot. When we do plug in, Slutsky\nis going to be our friend.",
    "start": "4360381",
    "end": "4366639"
  },
  {
    "text": "OK, so we can do this.  And that's one way.",
    "start": "4366640",
    "end": "4372110"
  },
  {
    "text": "And then other\nways to just solve for lambda like we did before. So the first one we\ngot is actually--",
    "start": "4372110",
    "end": "4378200"
  },
  {
    "text": "I don't know if I still\nhave it somewhere. Yeah, that was the one, right?",
    "start": "4378200",
    "end": "4383679"
  },
  {
    "text": "So we had 1 over Tn q, and\nthat's exactly the same that we have here.",
    "start": "4383680",
    "end": "4389180"
  },
  {
    "text": "So your solution is actually\ngiving us exactly this guy when we actually solve for lambda.",
    "start": "4389180",
    "end": "4394367"
  },
  {
    "text": " So this is what we get.",
    "start": "4394368",
    "end": "4400690"
  },
  {
    "text": "Lambda hat. We replace lambda by\nlambda hat, and we have our asymptotic\nconvergence theorem.",
    "start": "4400690",
    "end": "4407750"
  },
  {
    "text": "And that's exactly what we\ndid in Slutsky's theorem. Now we're getting to it at\nthis point is just telling us",
    "start": "4407750",
    "end": "4412817"
  },
  {
    "text": "that we can actually do this. Are there any questions\nabout what we did here?",
    "start": "4412817",
    "end": "4419680"
  },
  {
    "text": "So this derivation right\nhere is exactly what I did on the board I showed you. So let me just show you\nwith a little more space",
    "start": "4419680",
    "end": "4426690"
  },
  {
    "text": "just so that we all\nunderstand, right? So we know that square root of n\nlambda hat minus lambda divided",
    "start": "4426690",
    "end": "4438570"
  },
  {
    "text": "by lambda, the\ntrue lambda defined converges to sum n 0, 1.",
    "start": "4438570",
    "end": "4444099"
  },
  {
    "text": "So that was CLT\nplus Delta method. ",
    "start": "4444100",
    "end": "4451699"
  },
  {
    "text": "Applying those two,\nwe got to here. And we know that\nlambda hat converges",
    "start": "4451700",
    "end": "4457400"
  },
  {
    "text": "to lambda in probability and\nalmost surely, and that's what? That was law of large number\nplus continued mapping theorem,",
    "start": "4457400",
    "end": "4464980"
  },
  {
    "text": "right? Because we only knew that\none of our lambda hat converges to 1 over lambda. So we had to flip\nthose things around.",
    "start": "4464980",
    "end": "4471590"
  },
  {
    "text": "And now what I said is\nthat I apply Slutsky, so I write square root of n\nlambda hat minus lambda divided",
    "start": "4471590",
    "end": "4478210"
  },
  {
    "text": "by lambda hat, which is the\nsuggestion that was made to me. They said, I want\nthis, but I would",
    "start": "4478210",
    "end": "4484159"
  },
  {
    "text": "want to show that it\nconverges to sum n 0, 1 so I can legitimately use\nq alpha over 2 in this one",
    "start": "4484160",
    "end": "4489970"
  },
  {
    "text": "though. And the way we said is like,\nwell, this thing is actually really q divided by lambda times\nlambda divided by lambda hat.",
    "start": "4489970",
    "end": "4500737"
  },
  {
    "text": "So this thing that\nwas proposed to me, I can decompose\nit in the product of those two random variables.",
    "start": "4500737",
    "end": "4505980"
  },
  {
    "text": "The first one here converges\nthrough the Gaussian from the central limit theorem. And the second one converges\nto 1 from this guy,",
    "start": "4505980",
    "end": "4514718"
  },
  {
    "text": "but in probability this time. ",
    "start": "4514718",
    "end": "4520620"
  },
  {
    "text": "That was the ratio of two\nthings in probability, we can actually get it. And so now I apply Slutsky.",
    "start": "4520620",
    "end": "4526753"
  },
  {
    "text": " And Slutsky tells me that\nI can actually do that.",
    "start": "4526753",
    "end": "4534537"
  },
  {
    "text": "But when I take the product\nof this thing that converges to some standard Gaussian,\nand this thing that converges",
    "start": "4534537",
    "end": "4540010"
  },
  {
    "text": "in probability to 1, then\ntheir product actually converges to still this\nstandard Gaussian [INAUDIBLE]",
    "start": "4540010",
    "end": "4548617"
  },
  {
    "start": "4548618",
    "end": "4555370"
  },
  {
    "text": "Well, that's exactly\nwhat's done here, and I think I'm getting there.",
    "start": "4555370",
    "end": "4562340"
  },
  {
    "text": "So in our case, OK, so just a\nremark for Slutsky's theorem.",
    "start": "4562340",
    "end": "4567570"
  },
  {
    "text": "So that's the last line. So in the first example we used\nthe problem dependent trick, which was to say,\nwell, turns out",
    "start": "4567570",
    "end": "4573980"
  },
  {
    "text": "that we knew that p\nis between 0 and 1. So we have this p 1 minus\np that was annoying to us. We just said, let's\njust bound it by 1/4,",
    "start": "4573980",
    "end": "4581239"
  },
  {
    "text": "because that's going to be\ntrue for any value of p. But here, lambda takes any\nvalue between 0 and infinity,",
    "start": "4581240",
    "end": "4586310"
  },
  {
    "text": "so we didn't have such a trick. It's something like we could\nsee that lambda was less than something. Maybe we know it, in which\ncase we could use that.",
    "start": "4586310",
    "end": "4594070"
  },
  {
    "text": "But then in this case,\nwe could actually also have used Slutsky's theorem\nby doing plug in, right? So here this is my p 1 minus\np that's replaced by p hat 1",
    "start": "4594070",
    "end": "4601889"
  },
  {
    "text": "minus p hat. And Slutsky justify,\nso we did that without really\nthinking last time. But Slutsky actually\njustifies the fact",
    "start": "4601890",
    "end": "4608700"
  },
  {
    "text": "that this is valid, and\nstill allows me to use this q alpha over 2 here. ",
    "start": "4608700",
    "end": "4616230"
  },
  {
    "text": "All right, so that's\nthe end of this lecture. Tonight I will post the next\nset of slides, chapter two.",
    "start": "4616230",
    "end": "4621300"
  },
  {
    "text": "And, well, hopefully the video. I'm not sure when it's\ngoing to come out.",
    "start": "4621300",
    "end": "4626810"
  },
  {
    "start": "4626810",
    "end": "4628780"
  }
]