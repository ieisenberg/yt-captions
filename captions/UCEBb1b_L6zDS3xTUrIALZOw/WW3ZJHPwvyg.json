[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "12720",
    "end": "74980"
  },
  {
    "text": "PHILIPPE RIGOLLET: --bunch\nof x's and a bunch of y's. The y's were univariate,\njust one real",
    "start": "74980",
    "end": "80140"
  },
  {
    "text": "valued random variable. And the x's were vectors that\ndescribed a bunch of attributes for each of our individuals\nor each of our observations.",
    "start": "80140",
    "end": "87730"
  },
  {
    "text": "Let's assume now that we're\ngiven essentially only the x's. This is sometimes referred\nto as unsupervised learning.",
    "start": "87730",
    "end": "93970"
  },
  {
    "text": "There is just the x's. Usually, supervision\nis done by the y's. And so what you're trying to do\nis to make sense of this data.",
    "start": "93970",
    "end": "101710"
  },
  {
    "text": "You're going to try to\nunderstand this data, represent this data,\nvisualize this data,",
    "start": "101710",
    "end": "107062"
  },
  {
    "text": "try to understand\nsomething, right? So, if I give you a\nd-dimensional random vectors,",
    "start": "107062",
    "end": "112196"
  },
  {
    "text": "and you're going to have\nn independent copies of this individual-- of\nthis random vector, OK?",
    "start": "112196",
    "end": "117310"
  },
  {
    "text": "So you will see that\nI'm going to have-- I'm going to very quickly\nrun into some limitations about what I can actually\ndraw on the board",
    "start": "117310",
    "end": "124270"
  },
  {
    "text": "because I'm using\n[? boldface ?] here. I'm also going to use the\nblackboard [? boldface. ?] So it's going to\nbe a bit difficult.",
    "start": "124270",
    "end": "129819"
  },
  {
    "text": "So tell me if you're actually\na little confused by what",
    "start": "129820",
    "end": "135430"
  },
  {
    "text": "is a vector, what is a\nnumber, and what is a matrix. But we'll get there. So I have X in Rd, and\nthat's a random vector.",
    "start": "135430",
    "end": "142450"
  },
  {
    "text": " And I have X1 to\nXn that are IID.",
    "start": "142450",
    "end": "150650"
  },
  {
    "text": "They're independent\ncopies of X. OK,",
    "start": "150650",
    "end": "157635"
  },
  {
    "text": "so you can think\nof those as being-- the realization\nof these guys are going to be a cloud of\nn points in R to the d.",
    "start": "157635",
    "end": "171090"
  },
  {
    "text": "And we're going to think\nof d as being fairly large. And for this to\nstart to make sense, we're going to think of d\nas being at least 4, OK?",
    "start": "171090",
    "end": "179760"
  },
  {
    "text": "And meaning that you're\ngoing to have a hard time visualizing those things. If it was 3 or 2, you would\nbe able to draw these points.",
    "start": "179760",
    "end": "186530"
  },
  {
    "text": "And that's pretty\nmuch as much sense you're going to be\nmaking about those guys, just looking at the [INAUDIBLE]",
    "start": "186530",
    "end": "192030"
  },
  {
    "text": "All right, so I'm going to\nwrite each of those X's, right? So this vector, X,\nhas d coordinate.",
    "start": "192030",
    "end": "200520"
  },
  {
    "text": "And I'm going to write\nthem as X1, to Xd.",
    "start": "200520",
    "end": "205650"
  },
  {
    "start": "205650",
    "end": "210730"
  },
  {
    "text": "And I'm going to stack\nthem into a matrix, OK? So once I have those guys,\nI'm going to have a matrix.",
    "start": "210730",
    "end": "218100"
  },
  {
    "text": "But here, I'm going\nto use the double bar. And it's X1 transpose,\nXn transpose.",
    "start": "218100",
    "end": "227879"
  },
  {
    "text": "So what it means is that\nthe coordinates of this guy, of course, are X1,1.",
    "start": "227880",
    "end": "233040"
  },
  {
    "text": "Here, I have-- I'm of size d, so I have X1d. And here, I have Xn1.",
    "start": "233040",
    "end": "241290"
  },
  {
    "text": "Xnd. And so the i-th, j-th--",
    "start": "241290",
    "end": "246660"
  },
  {
    "text": "i-th row and j-th column\nis the matrix, Xij, right-- is the entry, Xi to-- sorry.",
    "start": "246660",
    "end": "252780"
  },
  {
    "start": "252780",
    "end": "263540"
  },
  {
    "text": "OK, so each-- so the rows\nhere are the observations. And the columns are the\ncovariance over attributes.",
    "start": "263540",
    "end": "272040"
  },
  {
    "text": "OK? So this is an n by d matrix. ",
    "start": "272040",
    "end": "279220"
  },
  {
    "text": "All right, this is really\njust some bookkeeping. How do we store\nthis data somehow? And the fact that we use a\nmatrix just like for regression",
    "start": "279220",
    "end": "286257"
  },
  {
    "text": "is going to be convenient\nbecause we're going to able to talk about projections-- going to be able to talk\nabout things like this.",
    "start": "286257",
    "end": "293310"
  },
  {
    "text": "All right, so everything\nI'm going to say now is about variances\nor covariances",
    "start": "293310",
    "end": "299190"
  },
  {
    "text": "of those things, which means\nthat I need two moments, OK? If the variance does\nnot exist, there's nothing I can say\nabout this problem.",
    "start": "299190",
    "end": "305320"
  },
  {
    "text": "So I'm going to assume\nthat the variance exists. And one way to\njust put it to say that the two norm\nof those guys is",
    "start": "305320",
    "end": "312389"
  },
  {
    "text": "finite, which is another\nway to say that each of them is finite. I mean, you can think\nof it the way you want.",
    "start": "312390",
    "end": "318210"
  },
  {
    "text": "All right, so now,\nthe mean of X, right? So I have a random vector. So I can talk about\nthe expectation of X.",
    "start": "318210",
    "end": "326430"
  },
  {
    "text": "That's a vector that's in Rd. And that's just taking\nthe expectation entrywise.",
    "start": "326430",
    "end": "333828"
  },
  {
    "text": "Sorry. ",
    "start": "333828",
    "end": "342265"
  },
  {
    "text": "X1, Xd. OK, so I should say it out loud.",
    "start": "342265",
    "end": "349640"
  },
  {
    "text": "For this, the purpose\nof this class, I will denote by\nsubscripts the indices that",
    "start": "349640",
    "end": "355850"
  },
  {
    "text": "corresponds to observations. And superscripts, the\nindices that correspond to",
    "start": "355850",
    "end": "362690"
  },
  {
    "text": "coordinates of a variable. And I think that's the\nsame convention that we took for the regression case.",
    "start": "362690",
    "end": "370599"
  },
  {
    "text": "Of course, you could\nuse whatever you want. If you want to put\ncommas, et cetera, it becomes just a\nbit more complicated.",
    "start": "370599",
    "end": "376072"
  },
  {
    "text": "All right, and so\nnow, once I have this, so this tells me where my cloud\nof point is centered, right?",
    "start": "376072",
    "end": "381380"
  },
  {
    "text": "So if I have a bunch of points-- OK, so now I have a\ndistribution on Rd,",
    "start": "381380",
    "end": "387440"
  },
  {
    "text": "so maybe I should\ntalk about this-- I'll talk about\nthis when we talk about the empirical version.",
    "start": "387440",
    "end": "392960"
  },
  {
    "text": "But if you think\nthat you have, say, a two-dimensional\nGaussian random variable, then you have a center\nin two dimension, which",
    "start": "392960",
    "end": "398930"
  },
  {
    "text": "is where it peaks, basically. And that's what we're\ntalking about here. But the other thing\nwe want to know",
    "start": "398930",
    "end": "404738"
  },
  {
    "text": "is how much does it spread\nin every direction, right? So in every direction of\nthe two dimensional thing, I can then try to understand\nhow much spread I'm getting.",
    "start": "404738",
    "end": "412220"
  },
  {
    "text": "And the way you measure this\nis by using covariance, right? So the covariance\nmatrix, sigma--",
    "start": "412220",
    "end": "422150"
  },
  {
    "text": "that's a matrix which is d by d. And it records-- in\nthe j, k-th entry,",
    "start": "422150",
    "end": "428150"
  },
  {
    "text": "it records the covariance\nbetween the j-th coordinate of X and the k-th\ncoordinate of X, OK?",
    "start": "428150",
    "end": "433490"
  },
  {
    "text": "So with entries-- ",
    "start": "433490",
    "end": "441300"
  },
  {
    "text": "OK, so I have sigma, which is\nsigma 1,1, sigma dd, sigma 1d,",
    "start": "441300",
    "end": "450509"
  },
  {
    "text": "sigma d1.  OK, and here I have\nsigma jk And sigma jk",
    "start": "450510",
    "end": "459690"
  },
  {
    "text": "is just the covariance between\nXj, the j-th coordinate",
    "start": "459690",
    "end": "468930"
  },
  {
    "text": "and the k-th coordinate. OK? So in particular, it's\nsymmetric because the covariance",
    "start": "468930",
    "end": "475159"
  },
  {
    "text": "between Xj and Xk is the same\nas the covariance between Xk and Xj. I should not put those\nparentheses here.",
    "start": "475160",
    "end": "481230"
  },
  {
    "text": "I do not use them in this, OK? Just the covariance matrix.",
    "start": "481230",
    "end": "486900"
  },
  {
    "text": "So that's just something\nthat records everything. And so what's nice about\nthe covariance matrix is that if I actually\ngive you X as a vector,",
    "start": "486900",
    "end": "493040"
  },
  {
    "text": "you actually can\nbuild the matrix just by looking at vectors\ntimes vectors transpose,",
    "start": "493040",
    "end": "498140"
  },
  {
    "text": "rather than actually\nthinking about building it coordinate by coordinate. So for example, if you're\nused to using MATLAB,",
    "start": "498140",
    "end": "503840"
  },
  {
    "text": "that's the way you want to\nbuild a covariance matrix because MATLAB is good\nat manipulating vectors",
    "start": "503840",
    "end": "509600"
  },
  {
    "text": "and matrices rather than just\nentering it entry by entry. OK, so, right?",
    "start": "509600",
    "end": "514820"
  },
  {
    "text": "So, what is the covariance\nbetween Xj and Xk?",
    "start": "514820",
    "end": "522590"
  },
  {
    "text": "Well by definition, it's\nthe expectation of Xj and Xk",
    "start": "522590",
    "end": "531360"
  },
  {
    "text": "minus the expectation of Xj\ntimes the expectation of Xk,",
    "start": "531360",
    "end": "541329"
  },
  {
    "text": "right? That's the definition\nof the covariance. I hope everybody's seeing that. And so, in particular,\nI can actually",
    "start": "541330",
    "end": "548280"
  },
  {
    "text": "see that this thing\ncan be written as-- sigma can now be written\nas the expectation",
    "start": "548280",
    "end": "554339"
  },
  {
    "text": "of XX transpose minus\nthe expectation of X",
    "start": "554340",
    "end": "561040"
  },
  {
    "text": "times the expectation\nof X transpose. Why?",
    "start": "561040",
    "end": "566500"
  },
  {
    "text": "Well, let's look at the jk-th\ncoefficient of this guy, right? So here, if I look at the\njk-th coefficient, I see what?",
    "start": "566500",
    "end": "575649"
  },
  {
    "text": "Well, I see that\nit's the expectation of XX transpose jk, which is\nequal to the expectation of XX",
    "start": "575650",
    "end": "590839"
  },
  {
    "text": "transpose jk. And what are the\nentries of XX transpose?",
    "start": "590840",
    "end": "596570"
  },
  {
    "text": "Well, they're of the\nform, Xj times Xk exactly. So this is actually equal to\nthe expectation of Xj times Xk.",
    "start": "596570",
    "end": "602940"
  },
  {
    "start": "602940",
    "end": "609060"
  },
  {
    "text": "And this is actually not\nthe way I want to write it. I want to write it-- ",
    "start": "609060",
    "end": "615530"
  },
  {
    "text": "OK? Is that clear? That when I have a rank 1 matrix\nof this form, XX transpose, the entries are of\nthis form, right?",
    "start": "615530",
    "end": "621950"
  },
  {
    "text": "Because if I take-- for example, think\nabout x, y, z, and then",
    "start": "621950",
    "end": "628865"
  },
  {
    "text": "I multiply by x, y, z. What I'm getting here is x--",
    "start": "628865",
    "end": "636380"
  },
  {
    "text": "maybe I should actually\nuse indices here. x1, x2, x3.",
    "start": "636380",
    "end": "642735"
  },
  {
    "text": "x1, x2, x3. The entries are x1x1, x1x2,\nx1x3; x2x1, x2x2, x2x3; x3x1,",
    "start": "642735",
    "end": "657018"
  },
  {
    "text": "x3x2, x3x3, OK?",
    "start": "657018",
    "end": "664770"
  },
  {
    "text": "So indeed, this is exactly of\nthe form if you look at jk, you get exactly Xj times Xk, OK?",
    "start": "664770",
    "end": "672566"
  },
  {
    "text": "So that's the beauty\nof those matrices. So now, once I have this, I\ncan do exactly the same thing,",
    "start": "672566",
    "end": "679380"
  },
  {
    "text": "except that here, if I\ntake the jk-th entry, I will get exactly\nthe same thing,",
    "start": "679380",
    "end": "685044"
  },
  {
    "text": "except that it's not going to be\nthe expectation of the product, but the product of the\nexpectation, right? So I get that the jk-th entry\nof E of X, E of X transpose,",
    "start": "685044",
    "end": "696810"
  },
  {
    "text": "is just the j-th entry of E of X\ntimes the k-th entry of E of X.",
    "start": "696810",
    "end": "708310"
  },
  {
    "text": "So if I put those two together,\nit's actually telling me that if I look at the\nj, k-th entry of sigma,",
    "start": "708310",
    "end": "716990"
  },
  {
    "text": "which I called\nlittle sigma jk, then this is actually equal to what? It's equal to the first\nterm minus the second term.",
    "start": "716990",
    "end": "724170"
  },
  {
    "text": "The first term is the\nexpectation of Xj, Xk",
    "start": "724170",
    "end": "731420"
  },
  {
    "text": "minus the expectation of Xj,\nexpectation of Xk, which--",
    "start": "731420",
    "end": "738899"
  },
  {
    "text": "oh, by the way, I forgot\nto say this is actually equal to the expectation of\nXj times the expectation of Xk",
    "start": "738900",
    "end": "746022"
  },
  {
    "text": "because that's just the\ndefinition of the expectation of random vectors. So my j and my k are now inside.",
    "start": "746022",
    "end": "751460"
  },
  {
    "text": "And that's by definition the\ncovariance between Xj and Xk,",
    "start": "751460",
    "end": "757175"
  },
  {
    "text": "OK? So just if you've seen those\nmanipulations between vectors,",
    "start": "757175",
    "end": "763360"
  },
  {
    "text": "hopefully you're bored\nout of your mind. And if you have not,\nthen that's something you just need to get\ncomfortable with, right?",
    "start": "763360",
    "end": "771010"
  },
  {
    "text": "So one thing that's\ngoing to be useful is to know very\nquickly what's called the outer product of a\nvector with itself, which",
    "start": "771010",
    "end": "777850"
  },
  {
    "text": "is the vector of times\nthe vector transpose, what the entries of these things are. And that's what we've been using\non this second set of boards.",
    "start": "777850",
    "end": "786510"
  },
  {
    "text": "OK, so everybody\nagrees now that we've sort of showed that the\ncovariance matrix can",
    "start": "786510",
    "end": "791860"
  },
  {
    "text": "be written in this vector form. So expectation of XX\ntranspose minus expectation",
    "start": "791860",
    "end": "797500"
  },
  {
    "text": "of X, expectation\nof X transpose.  OK, just like the covariance\ncan be written in two ways,",
    "start": "797500",
    "end": "808060"
  },
  {
    "text": "right we know that the\ncovariance can also be written as the expectation\nof Xj minus expectation of Xj",
    "start": "808060",
    "end": "819460"
  },
  {
    "text": "times Xk minus\nexpectation of Xk, right?",
    "start": "819460",
    "end": "825500"
  },
  {
    "text": "That's the-- sometimes, this\nis the original definition of covariance.",
    "start": "825500",
    "end": "830850"
  },
  {
    "text": "This is the second\ndefinition of covariance. Just like you have\nthe variance which is the expectation of the\nsquare of X minus c of X,",
    "start": "830850",
    "end": "837240"
  },
  {
    "text": "or the expectation X squared\nminus the expectation of X squared. It's the same thing\nfor covariance.",
    "start": "837240",
    "end": "843420"
  },
  {
    "text": "And you can actually see this\nin terms of vectors, right?",
    "start": "843420",
    "end": "851190"
  },
  {
    "text": "So this actually implies that\nyou can also rewrite sigma as the expectation of X\nminus expectation of X",
    "start": "851190",
    "end": "861780"
  },
  {
    "text": "times the same thing transpose. ",
    "start": "861780",
    "end": "872191"
  },
  {
    "text": "Right? And the reason is because if\nyou just distribute those guys, this is just the\nexpectation of XX transpose",
    "start": "872191",
    "end": "883760"
  },
  {
    "text": "minus X, expectation of X\ntranspose minus expectation",
    "start": "883760",
    "end": "894800"
  },
  {
    "text": "of XX transpose. And then I have plus\nexpectation of X,",
    "start": "894800",
    "end": "903608"
  },
  {
    "text": "expectation of X transpose. ",
    "start": "903608",
    "end": "909930"
  },
  {
    "text": "Now, things could go wrong\nbecause the main difference between matrices slash\nvectors and numbers is",
    "start": "909930",
    "end": "918660"
  },
  {
    "text": "that multiplication\ndoes not commute, right? So in particular, those two\nthings are not the same thing.",
    "start": "918660",
    "end": "925610"
  },
  {
    "text": "And so that's the main\ndifference that we have before, but it actually does not\nmatter for our problem. It's because what's\nhappening is that if when",
    "start": "925610",
    "end": "932210"
  },
  {
    "text": "I take the expectation\nof this guy, then it's actually the same as the\nexpectation of this guy, OK?",
    "start": "932210",
    "end": "938939"
  },
  {
    "text": "And so just because the\nexpectation is linear-- ",
    "start": "938940",
    "end": "948230"
  },
  {
    "text": "so what we have\nis that sigma now becomes equal to the\nexpectation of XX transpose",
    "start": "948230",
    "end": "955560"
  },
  {
    "text": "minus the expectation\nof X, expectation of X transpose minus\nexpectation of X,",
    "start": "955560",
    "end": "963170"
  },
  {
    "text": "expectation of X transpose. And then I have--",
    "start": "963170",
    "end": "970029"
  },
  {
    "text": "well, really, what\nI have is this guy. And then I have\nplus the expectation",
    "start": "970030",
    "end": "975990"
  },
  {
    "text": "of X, expectation\nof X transpose. ",
    "start": "975990",
    "end": "983970"
  },
  {
    "text": "And now, those three things are\nactually equal to each other just because the\nexpectation of X transpose",
    "start": "983970",
    "end": "990700"
  },
  {
    "text": "is the same as the\nexpectation of X transpose. And so what I'm\nleft with is just the expectation of XX transpose\nminus the expectation of X,",
    "start": "990700",
    "end": "1004364"
  },
  {
    "text": "expectation of X transpose, OK?",
    "start": "1004364",
    "end": "1009650"
  },
  {
    "text": "So same thing that's\nhappening when you want to prove\nthat you can write the covariance either\nthis way or that way.",
    "start": "1009650",
    "end": "1017760"
  },
  {
    "text": "The same thing happens for\nmatrices, or for vectors, right, or a covariance matrix. They go together.",
    "start": "1017760",
    "end": "1024609"
  },
  {
    "text": "Is there any questions so far? And if you have some, please\ntell me, because I want to-- I don't know to which extent you\nguys are comfortable with this",
    "start": "1024609",
    "end": "1032490"
  },
  {
    "text": "at all or not.  OK, so let's move on.",
    "start": "1032490",
    "end": "1039809"
  },
  {
    "text": "All right, so of\ncourse, this is what I'm describing in terms of\nthe distribution right here.",
    "start": "1039810",
    "end": "1046419"
  },
  {
    "text": "I took expectations. Covariances are\nalso expectations. So those depend on some\ndistribution of X, right?",
    "start": "1046420",
    "end": "1052560"
  },
  {
    "text": "If I wanted to compute\nthat, I would basically need to know what the\ndistribution of X is. Now, we're doing\nstatistics, so I",
    "start": "1052560",
    "end": "1057975"
  },
  {
    "text": "need to [INAUDIBLE] my question\nis going to be to say, well, how well can I estimate the\ncovariance matrix itself,",
    "start": "1057975",
    "end": "1064380"
  },
  {
    "text": "or some properties of\nthis covariance matrix based on data? All right, so if I\nwant to understand",
    "start": "1064380",
    "end": "1070140"
  },
  {
    "text": "what my covariance matrix\nlooks like based on data, I'm going to have\nto basically form its empirical\ncounterparts, which",
    "start": "1070140",
    "end": "1077759"
  },
  {
    "text": "I can do by doing the age-old\nstatistical trick, which is replace your expectation\nby an average, all right?",
    "start": "1077760",
    "end": "1084700"
  },
  {
    "text": "So let's just-- everything\nthat's on the board, you see expectation, just\nreplace it by an average. OK, so, now I'm going\nto be given X1, Xn.",
    "start": "1084700",
    "end": "1094230"
  },
  {
    "text": "So, I'm going to define\nthe empirical mean. ",
    "start": "1094230",
    "end": "1099780"
  },
  {
    "text": "OK so, really, the idea\nis take your expectation and replace it by 1\nover n sum, right?",
    "start": "1099780",
    "end": "1104970"
  },
  {
    "text": "And so the empirical\nmean is just 1 over n. Some of the Xi's--",
    "start": "1104970",
    "end": "1111510"
  },
  {
    "text": "I'm guessing everybody knows\nhow to average vectors. It's just the average\nof the coordinates. So I will write this as X bar.",
    "start": "1111510",
    "end": "1119730"
  },
  {
    "text": "And the empirical covariance\nmatrix, often called",
    "start": "1119730",
    "end": "1131440"
  },
  {
    "text": "sample covariance matrix,\nhence the notation, S.",
    "start": "1131440",
    "end": "1137519"
  },
  {
    "text": "Well, this is my\ncovariance matrix, right? Let's just replace the\nexpectations by averages.",
    "start": "1137520",
    "end": "1142650"
  },
  {
    "text": "1 over n, sum from i equal 1 to\nn, of Xi, Xi transpose, minus--",
    "start": "1142650",
    "end": "1152160"
  },
  {
    "text": "this is the expectation\nof X. I will replace it by the average, which I just\ncalled X bar, X bar transpose,",
    "start": "1152160",
    "end": "1161380"
  },
  {
    "text": "OK? And that's when I\nwant to use the-- that's when I want\nto use the notation--",
    "start": "1161380",
    "end": "1168429"
  },
  {
    "text": "the second definition,\nbut I could actually do exactly the same thing\nusing this definition here.",
    "start": "1168430",
    "end": "1175530"
  },
  {
    "text": "Sorry, using this\ndefinition right here. So this is actually\n1 over n, sum from i",
    "start": "1175530",
    "end": "1182340"
  },
  {
    "text": "equal 1 to n, of Xi minus X\nbar, Xi minus X bar transpose.",
    "start": "1182340",
    "end": "1195240"
  },
  {
    "text": "And those are actually-- I mean, in a way,\nit looks like I could define two\ndifferent estimators, but you can actually check.",
    "start": "1195240",
    "end": "1201630"
  },
  {
    "text": "And I do encourage\nyou to do this. If you're not comfortable\nmaking those manipulations, you can actually check that\nthose two things are actually",
    "start": "1201630",
    "end": "1208294"
  },
  {
    "text": "exactly the same, OK?",
    "start": "1208294",
    "end": "1215216"
  },
  {
    "start": "1215216",
    "end": "1220539"
  },
  {
    "text": "So now, I'm going to want\nto talk about matrices, OK? And remember, we defined\nthis big matrix, X,",
    "start": "1220540",
    "end": "1227259"
  },
  {
    "text": "with the double bar. And the question\nis, can I express both X bar and the\nsample covariance matrix",
    "start": "1227260",
    "end": "1235360"
  },
  {
    "text": "in terms of this big matrix, X? Because right now,\nit's still expressed in terms of the vectors.",
    "start": "1235360",
    "end": "1240820"
  },
  {
    "text": "I'm summing those vectors,\nvectors transpose. The question is, can I just\ndo that in a very compact way,",
    "start": "1240820",
    "end": "1246049"
  },
  {
    "text": "in a way that I can actually\nremove this sum term, all right? That's going to be the goal.",
    "start": "1246050",
    "end": "1252990"
  },
  {
    "text": "I mean, that's not\na notational goal. That's really something\nthat we want--",
    "start": "1252990",
    "end": "1258091"
  },
  {
    "text": "that's going to be\nconvenient for us just like it was convenient\nto talk about matrices when we did linear regression.",
    "start": "1258091",
    "end": "1264199"
  },
  {
    "start": "1264199",
    "end": "1283179"
  },
  {
    "text": "OK, X bar. We just said it's 1 over\nn, sum from I equal 1 to n",
    "start": "1283180",
    "end": "1290000"
  },
  {
    "text": "of Xi, right? Now remember, what does\nthis matrix look like?",
    "start": "1290000",
    "end": "1295100"
  },
  {
    "text": "We said that X bar-- X is this guy.",
    "start": "1295100",
    "end": "1300270"
  },
  {
    "text": "So if I look at X transpose,\nthe columns of this guy",
    "start": "1300270",
    "end": "1305930"
  },
  {
    "text": "becomes X1, my first\nobservation, X2,",
    "start": "1305930",
    "end": "1311430"
  },
  {
    "text": "my second observation, all the\nway to Xn, my last observation, right? Agreed?",
    "start": "1311430",
    "end": "1316850"
  },
  {
    "text": "That's what X transpose is. So if I want to\nsum those guys, I can multiply by the\nall-ones vector.",
    "start": "1316850",
    "end": "1322700"
  },
  {
    "text": " All right, so that's what the\ndefinition of the all-ones 1",
    "start": "1322700",
    "end": "1328700"
  },
  {
    "text": "vector is.  Well, it's just a bunch of\n1's in Rn, in this case.",
    "start": "1328700",
    "end": "1339870"
  },
  {
    "text": "And so when I do X transpose 1,\nwhat I get is just the sum from i equal 1 to n of the Xi's.",
    "start": "1339870",
    "end": "1347690"
  },
  {
    "text": "So if I divide by n,\nI get my average, OK?",
    "start": "1347690",
    "end": "1356460"
  },
  {
    "text": "So here, I definitely\nremoved the sum term.",
    "start": "1356460",
    "end": "1363200"
  },
  {
    "text": "Let's see if with the covariance\nmatrix, we can do the same. Well, and that's actually a\nlittle more difficult to see,",
    "start": "1363200",
    "end": "1373280"
  },
  {
    "text": "I guess. But let's use this\ndefinition for S, OK?",
    "start": "1373280",
    "end": "1385510"
  },
  {
    "text": "And one thing that's\nactually going to be-- so, let's see for\none second, what-- so it's going to be\nsomething that involves X,",
    "start": "1385510",
    "end": "1392510"
  },
  {
    "text": "multiplying X with itself, OK? And the question is,\nis it going to be multiplying X with X transpose,\nor X tranpose with X?",
    "start": "1392510",
    "end": "1399032"
  },
  {
    "text": "To answer this\nquestion, you can go the easy route, which says,\nwell, my covariance matrix is of size, what?",
    "start": "1399032",
    "end": "1404870"
  },
  {
    "text": "What is the size of S? AUDIENCE: d by d. PHILIPPE RIGOLLET: d by d, OK?",
    "start": "1404870",
    "end": "1410260"
  },
  {
    "text": "X is of size n by d. So if I do X times\nX transpose, I'm",
    "start": "1410260",
    "end": "1415760"
  },
  {
    "text": "going to have something\nwhich is of size n by n. If I do X transpose\nX, I'm going to have something which is d by d.",
    "start": "1415760",
    "end": "1420794"
  },
  {
    "text": "That's the easy route. And there's basically\none of the two guys. You can actually open\nthe box a little bit",
    "start": "1420794",
    "end": "1426130"
  },
  {
    "text": "and see what's\ngoing on in there. If you do X transpose X, which\nwe know gives you a d by d,",
    "start": "1426130",
    "end": "1432760"
  },
  {
    "text": "you'll see that X is\ngoing to have vectors that are of the form,\nXi, and X transpose is going to have vectors that\nare of the form, Xi transpose,",
    "start": "1432760",
    "end": "1442230"
  },
  {
    "text": "right? And so, this is actually\nprobably the right way to go. So let's look at what's X\ntranspose X is giving us.",
    "start": "1442230",
    "end": "1451690"
  },
  {
    "text": "So I claim that it's actually\ngoing to give us what we want,",
    "start": "1451690",
    "end": "1456850"
  },
  {
    "text": "but rather than actually\ngoing there, let's-- to actually-- I mean, we\ncould check it entry by entry,",
    "start": "1456850",
    "end": "1462700"
  },
  {
    "text": "but there's actually a\nnice thing we can do. Before we go there,\nlet's write X transpose",
    "start": "1462700",
    "end": "1468090"
  },
  {
    "text": "as the following sum of\nvariables, X1 and then",
    "start": "1468090",
    "end": "1473260"
  },
  {
    "text": "just a bunch of 0's\neverywhere else. So it's still d by n.",
    "start": "1473260",
    "end": "1479410"
  },
  {
    "text": "So n minus 1 of the columns\nare equal to 0 here. Then I'm going to put\na 0 and then put X2.",
    "start": "1479410",
    "end": "1485860"
  },
  {
    "text": "And then just a\nbunch of 0's, right? So that's just 0, 0 plus 0,\n0, all the way to Xn, OK?",
    "start": "1485860",
    "end": "1499940"
  },
  {
    "text": "Everybody agrees with it? See what I'm doing here? I'm just splitting it into\na sum of matrices that",
    "start": "1499940",
    "end": "1506150"
  },
  {
    "text": "only have one nonzero columns. But clearly, that's true.",
    "start": "1506150",
    "end": "1511210"
  },
  {
    "text": "Now let's look at the product\nof this guy with itself. So, let's call these\nmatrices M1, M2, Mn.",
    "start": "1511210",
    "end": "1523396"
  },
  {
    "text": " So when I do X\ntranspose X, what I",
    "start": "1523396",
    "end": "1530750"
  },
  {
    "text": "do is the sum of the\nMi's for i equal 1 to n,",
    "start": "1530750",
    "end": "1537970"
  },
  {
    "text": "times the sum of the\nMi transpose, right?",
    "start": "1537970",
    "end": "1548620"
  },
  {
    "text": "Now, the sum of\nthe Mi's transpose is just the sum of each\nof the Mi's transpose, OK?",
    "start": "1548620",
    "end": "1555273"
  },
  {
    "text": " So now I just have this\nproduct of two sums,",
    "start": "1555274",
    "end": "1560620"
  },
  {
    "text": "so I'm just going to\nre-index the second one by j. So this is sum for i equal\n1 to n, j equal 1 to n of Mi",
    "start": "1560620",
    "end": "1572650"
  },
  {
    "text": "Mj transpose. OK? ",
    "start": "1572650",
    "end": "1579036"
  },
  {
    "text": "And now what we\nwant to notice is that if i is different\nfrom j, what's happening?",
    "start": "1579036",
    "end": "1586000"
  },
  {
    "text": "Well if i is different from j,\nlet's look at say, M1 times XM2",
    "start": "1586000",
    "end": "1594380"
  },
  {
    "text": "transpose. ",
    "start": "1594380",
    "end": "1614067"
  },
  {
    "text": "So what is the product\nbetween those two matrices? ",
    "start": "1614067",
    "end": "1624404"
  },
  {
    "text": "AUDIENCE: It's a new\nentry and [INAUDIBLE]",
    "start": "1624404",
    "end": "1629870"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nThere's an entry? AUDIENCE: Well, it's an entry. It's like a dot product in that\nform next to [? transpose. ?]",
    "start": "1629870",
    "end": "1637116"
  },
  {
    "text": "PHILIPPE RIGOLLET: You mean\na dot product is just getting [INAUDIBLE] number, right? So I want-- this is\ngoing to be a matrix. It's the product of\ntwo matrices, right?",
    "start": "1637116",
    "end": "1644550"
  },
  {
    "text": "This is a matrix times a matrix. So this should be a matrix,\nright, of size d by d.",
    "start": "1644550",
    "end": "1651210"
  },
  {
    "text": " Yeah, I should\nsee a lot of hands",
    "start": "1651210",
    "end": "1657610"
  },
  {
    "text": "that look like this, right? Because look at this. So let's multiply the first-- let's look at what's going\non in the first column here.",
    "start": "1657610",
    "end": "1665215"
  },
  {
    "text": "I'm multiplying this column\nwith each of those rows. The only nonzero\ncoefficient is here,",
    "start": "1665215",
    "end": "1670480"
  },
  {
    "text": "and it only hits\nthis column of 0's. So every time, this is going\nto give you 0, 0, 0, 0.",
    "start": "1670480",
    "end": "1677035"
  },
  {
    "text": "And it's going to be the same\nfor every single one of them. So this matrix is just\nfull of 0's, right?",
    "start": "1677036",
    "end": "1684420"
  },
  {
    "text": "They never hit each\nother when I do the matrix-matrix\nmultiplication. There's no-- every\nnon-zero hits a 0.",
    "start": "1684420",
    "end": "1691811"
  },
  {
    "text": "So what it means is--\nand this, of course, you can check for every\ni different from j. So this means that Mi times\nMj transpose is actually",
    "start": "1691811",
    "end": "1702290"
  },
  {
    "text": "equal to 0 when i is\ndifferent from j, Right? Everybody is OK with this?",
    "start": "1702290",
    "end": "1709370"
  },
  {
    "text": "So what that means is that when\nI do this double sum, really, it's a simple sum. There's only just the\nsum from i equal 1",
    "start": "1709370",
    "end": "1717310"
  },
  {
    "text": "to n of Mi Mi transpose. Because this is the only\nterms in this double sum",
    "start": "1717310",
    "end": "1724820"
  },
  {
    "text": "that are not going to be 0 when\n[INAUDIBLE] [? M1 ?] with M1 itself.",
    "start": "1724820",
    "end": "1730492"
  },
  {
    "text": "Now, let's see\nwhat's going on when I do M1 times M1 transpose. Well, now, if I do Mi\ntimes and Mi transpose,",
    "start": "1730492",
    "end": "1737890"
  },
  {
    "text": "now this guy becomes [? X1 ?]\n[INAUDIBLE] it's here. And so now, I really have\nX1 times X1 transpose.",
    "start": "1737890",
    "end": "1743830"
  },
  {
    "text": "So this is really\njust the sum from i equal 1 to n of Xi Xi transpose,\njust because Mi Mi transpose",
    "start": "1743830",
    "end": "1760080"
  },
  {
    "text": "is Xi Xi transpose. There's nothing else there. ",
    "start": "1760080",
    "end": "1766190"
  },
  {
    "text": "So that's the good news, right? This term here is really just\nX transpose X divided by n.",
    "start": "1766190",
    "end": "1777100"
  },
  {
    "start": "1777100",
    "end": "1783460"
  },
  {
    "text": "OK, I can use that\nguy again, I guess. Well, no. Let's just-- OK, so\nlet me rewrite S.",
    "start": "1783460",
    "end": "1808602"
  },
  {
    "text": "All right, that's the\ndefinition we have. And we know that this guy\nalready is equal to 1 over n X",
    "start": "1808602",
    "end": "1814990"
  },
  {
    "text": "transpose X. x bar\nx bar transpose--",
    "start": "1814990",
    "end": "1820960"
  },
  {
    "text": "we know that x bar-- we\njust proved that x bar-- sorry, little x\nbar was equal to 1",
    "start": "1820960",
    "end": "1831080"
  },
  {
    "text": "over n X bar transpose\ntimes the all-ones vector.",
    "start": "1831080",
    "end": "1836652"
  },
  {
    "text": "So I'm just going to do that. So that's just\ngoing to be minus. I'm going to pull\nmy two 1 over n's-- one from this guy,\none from this guy.",
    "start": "1836652",
    "end": "1842540"
  },
  {
    "text": "So I'm going to get\n1 over n squared. And then I'm going\nto get X bar-- sorry, there's no X bar here.",
    "start": "1842540",
    "end": "1848690"
  },
  {
    "text": "It's just X. Yeah. X transpose all ones times X\ntranspose all ones transpose,",
    "start": "1848690",
    "end": "1859861"
  },
  {
    "text": "right?  And X transpose all\nones transpose--",
    "start": "1859861",
    "end": "1867580"
  },
  {
    "text": " right, the rule-- if I\nhave A times B transpose,",
    "start": "1867580",
    "end": "1874200"
  },
  {
    "text": "it's B transpose times\nA transpose, right? ",
    "start": "1874200",
    "end": "1883460"
  },
  {
    "text": "That's just the rule\nof transposition. So this is 1\ntranspose X transpose.",
    "start": "1883460",
    "end": "1891400"
  },
  {
    "text": "And so when I put all\nthese guys together, this is actually equal to 1\nover n X transpose X minus one",
    "start": "1891400",
    "end": "1898365"
  },
  {
    "text": "over n squared X transpose\n1, 1 transpose X. Because X",
    "start": "1898365",
    "end": "1907670"
  },
  {
    "text": "transpose transposes X, OK? ",
    "start": "1907670",
    "end": "1913700"
  },
  {
    "text": "So now, I can actually-- I have something which is\nof the form, X transpose X--",
    "start": "1913700",
    "end": "1919434"
  },
  {
    "text": "[INAUDIBLE] to the left, X\ntranspose; to the right, X. Here, I have X transpose to\nthe left, X to the right.",
    "start": "1919435",
    "end": "1924930"
  },
  {
    "text": "So it can factor out\nwhatever's in there. So I can write S as 1 over n--",
    "start": "1924930",
    "end": "1931639"
  },
  {
    "text": "sorry, X transpose times 1 over\nn times the identity of Rd.",
    "start": "1931640",
    "end": "1937230"
  },
  {
    "text": " And then I have minus 1\nover n, 1, 1 transpose X.",
    "start": "1937230",
    "end": "1953110"
  },
  {
    "text": "OK, because if you-- I mean, you can\ndistribute it back, right? So here, I'm going to get what? X transpose identity times X,\nthe whole thing divided by n.",
    "start": "1953110",
    "end": "1961809"
  },
  {
    "text": "That's this term. And then the second one is\ngoing to be-- sorry, 1 over n squared. And then I'm going to get 1 over\nn squared times X transpose 1,",
    "start": "1961810",
    "end": "1970840"
  },
  {
    "text": "1 transpose which is\nthis guy, times X, and that's the [? right ?]\n[? thing, ?] OK?",
    "start": "1970840",
    "end": "1978580"
  },
  {
    "text": "So, the way it's written, I\nfactored out one of the 1 over n's. So I'm just going to do the\nsame thing as on this slide.",
    "start": "1978580",
    "end": "1985500"
  },
  {
    "text": "So I'm just factoring\nout this 1 over n here. So it's 1 over n times\nX transpose identity",
    "start": "1985500",
    "end": "1996280"
  },
  {
    "text": "of our d divided by n\ndivided by 1 this time, minus 1 over n 1, 1\ntranspose times X, OK?",
    "start": "1996280",
    "end": "2006780"
  },
  {
    "text": "So that's just\nwhat's on the slides.  What does the matrix, 1,\n1 transpose, look like?",
    "start": "2006780",
    "end": "2015874"
  },
  {
    "text": "AUDIENCE: All 1's. PHILIPPE RIGOLLET: It's\njust all 1's, right? Because the entries are the\nproducts of the all-ones--",
    "start": "2015874",
    "end": "2021060"
  },
  {
    "text": "of the coordinates of\nthe all-ones vectors with the coordinates of the all-ones\nvectors, so I only get 1's. So it's a d by d\nmatrix with only 1's.",
    "start": "2021060",
    "end": "2029610"
  },
  {
    "text": "So this matrix, I can\nactually write exactly, right? H, this matrix that\nI called H which",
    "start": "2029610",
    "end": "2035710"
  },
  {
    "text": "is what's sandwiched in-between\nthis X transpose and X. By definition, I said this\nis the definition of H. Then",
    "start": "2035710",
    "end": "2042760"
  },
  {
    "text": "this thing, I can write\nits coordinates exactly. ",
    "start": "2042760",
    "end": "2058879"
  },
  {
    "text": "We know it's identity\ndivided by n minus-- sorry, I don't know\nwhy I keep [INAUDIBLE]..",
    "start": "2058880",
    "end": "2065330"
  },
  {
    "text": "Minus 1 over n 1, 1 transpose-- so it's this matrix\nwith the only 1's",
    "start": "2065330",
    "end": "2070940"
  },
  {
    "text": "on the diagonals and 0's and\nelsewhere-- minus a matrix that only has 1 over n everywhere.",
    "start": "2070940",
    "end": "2076486"
  },
  {
    "text": " OK, so the whole thing is 1\nminus 1 over n on the diagonals",
    "start": "2076487",
    "end": "2089820"
  },
  {
    "text": "and then minus 1\nover n here, OK?",
    "start": "2089820",
    "end": "2097430"
  },
  {
    "text": "And now I claim that this matrix\nis an orthogonal projector. Now, I'm writing this, but\nit's completely useless.",
    "start": "2097430",
    "end": "2105579"
  },
  {
    "text": "This is just a way for you to\nsee that it's actually very convenient now to think\nabout this problem",
    "start": "2105580",
    "end": "2111430"
  },
  {
    "text": "as being a matrix\nproblem, because things are much nicer when you\nthink about the actual form",
    "start": "2111430",
    "end": "2117890"
  },
  {
    "text": "of your matrices, right? They could tell you,\nhere is the matrix. I mean, imagine you're\nsitting at a midterm,",
    "start": "2117890",
    "end": "2123340"
  },
  {
    "text": "and I say, here's the\nmatrix that has 1 minus 1 over n on the diagonals\nand minus 1 over n",
    "start": "2123340",
    "end": "2128640"
  },
  {
    "text": "on the [INAUDIBLE] diagonal. Prove to me that it's\na projector matrix. You're going to\nhave to basically",
    "start": "2128640",
    "end": "2134230"
  },
  {
    "text": "take this guy times itself. It's going to be really\ncomplicated, right? So we know it's symmetric. That's for sure.",
    "start": "2134230",
    "end": "2139930"
  },
  {
    "text": "But the fact that it\nhas this particular way of writing it is\ngoing to make my life super easy to check this.",
    "start": "2139930",
    "end": "2145599"
  },
  {
    "text": "That's the definition\nof a projector. It has to be\nsymmetric and it has to square to itself\nbecause we just",
    "start": "2145599",
    "end": "2151270"
  },
  {
    "text": "said in the chapter\non linear regression that once you project, if you\napply the projection again,",
    "start": "2151270",
    "end": "2157360"
  },
  {
    "text": "you're not moving because\nyou're already there. OK, so why is H\nsquared equal to H?",
    "start": "2157360",
    "end": "2164469"
  },
  {
    "text": "Well let's just write H square. It's the identity\nminus 1 over n 1, 1 transpose times the\nidentity minus 1 over n 1, 1",
    "start": "2164469",
    "end": "2176610"
  },
  {
    "text": "transpose, right? Let's just expand this now.",
    "start": "2176610",
    "end": "2182490"
  },
  {
    "text": "This is equal to\nthe identity minus-- well, the identity times 1, 1\ntranspose is just the identity.",
    "start": "2182490",
    "end": "2189280"
  },
  {
    "text": "So it's 1, 1 transpose, sorry. So 1 over n 1, 1 transpose\nminus 1 over n 1, 1 transpose.",
    "start": "2189280",
    "end": "2198839"
  },
  {
    "text": "And then there's\ngoing to be what makes the deal is that\nI get this 1 over n squared this time.",
    "start": "2198840",
    "end": "2204750"
  },
  {
    "text": "And then I get the product\nof 1 over n trans-- oh, let's write it completely. I get 1, 1 transpose\ntimes 1, 1 transpose, OK?",
    "start": "2204750",
    "end": "2218010"
  },
  {
    "text": "But this thing here-- what is this?",
    "start": "2218010",
    "end": "2223840"
  },
  {
    "text": "n, right, is the end product\nof the all-ones vector with the all-ones vector. So I'm just summing n times\n1 squared, which is n.",
    "start": "2223840",
    "end": "2230740"
  },
  {
    "text": "So this is equal to n. So I pull it out,\ncancel one of the ends, and I'm back to\nwhat I had before.",
    "start": "2230740",
    "end": "2235869"
  },
  {
    "text": "So I had identity minus 2\nover n 1, 1 transpose plus 1",
    "start": "2235870",
    "end": "2241720"
  },
  {
    "text": "over n 1, 1 transpose\nwhich is equal to H.",
    "start": "2241720",
    "end": "2247530"
  },
  {
    "text": "Because one of the 1\nover n's cancel, OK? ",
    "start": "2247530",
    "end": "2256264"
  },
  {
    "text": "So it's a projection matrix. It's projecting onto\nsome linear space, right? It's taking a matrix.",
    "start": "2256264",
    "end": "2262450"
  },
  {
    "text": "Sorry, it's taking\na vector and it's projecting onto a\ncertain space of vectors. ",
    "start": "2262450",
    "end": "2269255"
  },
  {
    "text": "What is this space?  Right, so, how do\nyou-- so I'm only",
    "start": "2269255",
    "end": "2274920"
  },
  {
    "text": "asking the answer to this\nquestion in words, right? So how would you\ndescribe the vectors onto which this\nmatrix is projecting?",
    "start": "2274920",
    "end": "2282950"
  },
  {
    "text": "Well, if you want to\nanswer this question, the way you would tackle\nit is first by saying, OK, what does a vector which is of\nthe form, H times something,",
    "start": "2282950",
    "end": "2293690"
  },
  {
    "text": "look like, right? What can I say about\nthis vector that's going to be definitely\ngiving me something",
    "start": "2293690",
    "end": "2299540"
  },
  {
    "text": "about the space on\nwhich it projects? I need to know a little more to\nknow that it projects exactly",
    "start": "2299540",
    "end": "2304800"
  },
  {
    "text": "onto this. But one way we can\ndo this is just see how it acts on a vector.",
    "start": "2304800",
    "end": "2310440"
  },
  {
    "text": "What does it do to a\nvector to apply H, right? So I take v. And let's see what\ntaking v and applying H to it",
    "start": "2310440",
    "end": "2324550"
  },
  {
    "text": "looks like. Well, it's the identity\nminus something. So it takes v and\nit removes something",
    "start": "2324550",
    "end": "2330640"
  },
  {
    "text": "from v. What does it remove? Well, it's 1 over n\ntimes v transpose 1 times",
    "start": "2330640",
    "end": "2340589"
  },
  {
    "text": "the all-ones vector, right? Agreed? I just wrote v transpose 1\ninstead of 1 transpose v,",
    "start": "2340590",
    "end": "2353570"
  },
  {
    "text": "which are the same thing. What is this thing? ",
    "start": "2353570",
    "end": "2365160"
  },
  {
    "text": "What should I call it in\nmathematical notation? ",
    "start": "2365160",
    "end": "2370720"
  },
  {
    "text": "v bar, right? I should all it v bar because\nthis is exactly the average of the entries of v, agreed?",
    "start": "2370720",
    "end": "2378839"
  },
  {
    "text": "This is summing the entries\nof v's, and this is dividing by the number of those v's. Sorry, now v is in our--",
    "start": "2378840",
    "end": "2384859"
  },
  {
    "text": " sorry, why do I divide by--",
    "start": "2384860",
    "end": "2391074"
  },
  {
    "text": " I'm just-- OK, I need to check\nwhat my dimensions are now.",
    "start": "2391074",
    "end": "2399070"
  },
  {
    "text": "No, it's in Rd, right? So why do I divide by n? ",
    "start": "2399070",
    "end": "2405520"
  },
  {
    "text": "So it's not really v bar. It's the sum of the\nv's divided by--",
    "start": "2405520",
    "end": "2413910"
  },
  {
    "text": "right, so it's v bar. ",
    "start": "2413910",
    "end": "2424024"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] [INTERPOSING VOICES] AUDIENCE: Yeah, v\nhas to be [INAUDIBLE] PHILIPPE RIGOLLET: Oh, yeah.",
    "start": "2424024",
    "end": "2429450"
  },
  {
    "text": "OK, thank you. So everywhere I wrote\nHd, that was actually Hn.",
    "start": "2429450",
    "end": "2434750"
  },
  {
    "text": "Oh, man. I wish I had a computer now. All right. So-- yeah, because the--",
    "start": "2434750",
    "end": "2443230"
  },
  {
    "text": "yeah, right? So why it's not-- well, why I thought it was\nthis is because I was thinking about the outer\ndimension of X, really",
    "start": "2443230",
    "end": "2449890"
  },
  {
    "text": "of X transpose, which is\nreally the inner dimension, didn't matter to me, right? So the thing that I can\nsandwich between X transpose",
    "start": "2449890",
    "end": "2455080"
  },
  {
    "text": "and X has to be n by n. So this was actually n by n. And so that's actually n by n.",
    "start": "2455080",
    "end": "2460480"
  },
  {
    "text": "Everything is n by n. Sorry about that. ",
    "start": "2460480",
    "end": "2468220"
  },
  {
    "text": "So this is n. This is n. This is-- well, I\ndidn't really tell you what the all-ones vector\nwas, but it's also in our n.",
    "start": "2468220",
    "end": "2476289"
  },
  {
    "text": "Yeah, OK. ",
    "start": "2476290",
    "end": "2482190"
  },
  {
    "text": "Thank you. And n-- actually, I used the\nfact that this was of size n",
    "start": "2482190",
    "end": "2487939"
  },
  {
    "text": "here already.  OK, and so that's indeed v bar.",
    "start": "2487939",
    "end": "2493339"
  },
  {
    "start": "2493340",
    "end": "2498996"
  },
  {
    "text": "So what is this projection\ndoing to a vector? ",
    "start": "2498996",
    "end": "2507470"
  },
  {
    "text": "It's removing its average\non each coordinate, right? And the effect of this\nis that v is a vector.",
    "start": "2507470",
    "end": "2514569"
  },
  {
    "text": "What is the average of Hv? AUDIENCE: 0. PHILIPPE RIGOLLET:\nRight, so it's 0.",
    "start": "2514570",
    "end": "2520840"
  },
  {
    "text": "It's the average of v, which\nis v bar, minus the average of something that only has v\nbar's entry, which is v bar.",
    "start": "2520840",
    "end": "2527230"
  },
  {
    "text": "So this thing is actually 0.  So let me repeat my question.",
    "start": "2527230",
    "end": "2532840"
  },
  {
    "text": "Onto what subspace\ndoes H project? ",
    "start": "2532840",
    "end": "2542700"
  },
  {
    "text": "Onto the subspace of\nvectors that have mean 0. A vector that has\nmean 0 is a vector.",
    "start": "2542700",
    "end": "2550010"
  },
  {
    "text": "So if you want to talk more\nlinear algebra, v bar-- for a vector you\nhave mean 0, it means",
    "start": "2550010",
    "end": "2556750"
  },
  {
    "text": "that v is orthogonal to the\nspan of the all-ones vector.",
    "start": "2556750",
    "end": "2563440"
  },
  {
    "text": "That's it. It projects to this space. So in words, it\nprojects onto the space of vectors that have 0 mean.",
    "start": "2563440",
    "end": "2569880"
  },
  {
    "text": "In linear algebra,\nit says it projects onto the hyperplane\nwhich is orthogonal",
    "start": "2569880",
    "end": "2575759"
  },
  {
    "text": "to the all-ones vector, OK? So that's all.",
    "start": "2575760",
    "end": "2581860"
  },
  {
    "text": "Can you guys still\nsee the screen? Are you good over there? OK.",
    "start": "2581860",
    "end": "2587420"
  },
  {
    "text": "All right, so now, what it\nmeans is that, well, I'm doing this weird thing, right?",
    "start": "2587420",
    "end": "2593280"
  },
  {
    "text": "I'm taking the inner product-- so S is taking X. And then\nit's removing its mean of each",
    "start": "2593280",
    "end": "2600030"
  },
  {
    "text": "of the columns of X, right? When I take H times X, I'm\nbasically applying this projection which consists\nin removing the mean of all",
    "start": "2600030",
    "end": "2606780"
  },
  {
    "text": "the X's. And then I multiply\nby H transpose. But what's actually\nnice is that, remember,",
    "start": "2606780",
    "end": "2613550"
  },
  {
    "text": "H is a projector. Sorry, I don't\nwant to keep that. Which means that when I\nlook at X transpose HX,",
    "start": "2613550",
    "end": "2627010"
  },
  {
    "text": "it's the same as looking\nat X transpose H squared X.",
    "start": "2627010",
    "end": "2632410"
  },
  {
    "text": "But since H is equal\nto its transpose, this is actually the same\nas looking at X transpose H",
    "start": "2632410",
    "end": "2638020"
  },
  {
    "text": "transpose HX, which is the\nsame as looking at HX transpose",
    "start": "2638020",
    "end": "2647146"
  },
  {
    "text": "HX, OK? So what it's doing, it's\nfirst applying this projection",
    "start": "2647146",
    "end": "2654300"
  },
  {
    "text": "matrix, H, which removes the\nmean of each of your columns, and then looks at the inner\nproducts between those guys,",
    "start": "2654300",
    "end": "2663000"
  },
  {
    "text": "right? Each entry of this guy\nis just the covariance between those centered things. That's all it's doing.",
    "start": "2663000",
    "end": "2668910"
  },
  {
    "text": "All right, so those are actually\ngoing to be the key statements.",
    "start": "2668910",
    "end": "2675450"
  },
  {
    "text": "So everything we've\ndone so far is really mainly linear algebra, right? I mean, looking at expectations\nand covariances was just--",
    "start": "2675450",
    "end": "2681950"
  },
  {
    "text": "we just used the fact that\nthe expectation was linear. We didn't do much. But now there's a nice\nthing that's happening.",
    "start": "2681950",
    "end": "2687450"
  },
  {
    "text": "And that's why we're\ngoing to switch from the language\nof linear algebra to more statistical,\nbecause what's happening",
    "start": "2687450",
    "end": "2693710"
  },
  {
    "text": "is that if I look at this\nquadratic form, right? So I take sigma.",
    "start": "2693710",
    "end": "2699080"
  },
  {
    "text": "So I take a vector, u.  And I'm going to look at\nu-- so let's say, in Rd.",
    "start": "2699080",
    "end": "2709180"
  },
  {
    "text": "And I'm going to look\nat u transpose sigma u.",
    "start": "2709180",
    "end": "2714796"
  },
  {
    "text": "OK?  What is this doing? Well, we know that u transpose\nsigma u is equal to what?",
    "start": "2714796",
    "end": "2724630"
  },
  {
    "text": "Well, sigma is the\nexpectation of XX transpose",
    "start": "2724630",
    "end": "2731720"
  },
  {
    "text": "minus the expectation of X\nexpectation of X transpose, right? ",
    "start": "2731720",
    "end": "2739460"
  },
  {
    "text": "So I just substitute in there. ",
    "start": "2739460",
    "end": "2746100"
  },
  {
    "text": "Now, u is deterministic. So in particular, I can push\nit inside the expectation",
    "start": "2746100",
    "end": "2752250"
  },
  {
    "text": "here, agreed? And I can do the\nsame from the right. So here, when I push u\ntranspose here, and u here,",
    "start": "2752250",
    "end": "2760800"
  },
  {
    "text": "what I'm left with is the\nexpectation of u transpose X",
    "start": "2760800",
    "end": "2766170"
  },
  {
    "text": "times X transpose u. OK?",
    "start": "2766170",
    "end": "2771436"
  },
  {
    "text": "And now, I can do the\nsame thing for this guy. And this tells me that this is\nthe expectation of u transpose",
    "start": "2771436",
    "end": "2777410"
  },
  {
    "text": "X times the expectation\nof X transpose u. ",
    "start": "2777410",
    "end": "2784640"
  },
  {
    "text": "Of course, u transpose X\nis equal to X transpose u. And u-- yeah.",
    "start": "2784640",
    "end": "2791330"
  },
  {
    "text": "So what it means is\nthat this is actually equal to the expectation\nof u transpose X squared",
    "start": "2791330",
    "end": "2803700"
  },
  {
    "text": "minus the expectation\nof u transpose X, the whole thing squared.",
    "start": "2803700",
    "end": "2809065"
  },
  {
    "start": "2809065",
    "end": "2816900"
  },
  {
    "text": "But this is something\nthat should look familiar. This is really just the variance\nof this particular random variable which is of\nthe form, u transpose X,",
    "start": "2816900",
    "end": "2823359"
  },
  {
    "text": "right? u transpose\nX is a number. It involves a random vector,\nso it's a random variable.",
    "start": "2823360",
    "end": "2830109"
  },
  {
    "text": "And so it has a variance. And this variance is exactly\ngiven by this formula.",
    "start": "2830110",
    "end": "2835430"
  },
  {
    "text": "So this is just the\nvariance of u transpose X. So what we've proved is\nthat if I look at this guy,",
    "start": "2835430",
    "end": "2841720"
  },
  {
    "text": "this is really just the\nvariance of u transpose X, OK?",
    "start": "2841720",
    "end": "2849772"
  },
  {
    "start": "2849772",
    "end": "2857580"
  },
  {
    "text": "I can do the same thing\nfor the sample variance. So let's do this. ",
    "start": "2857580",
    "end": "2868240"
  },
  {
    "text": "And as you can\nsee, spoiler alert, this is going to be\nthe sample variance.",
    "start": "2868240",
    "end": "2876334"
  },
  {
    "text": " OK, so remember, S is 1 over n,\nsum of Xi Xi transpose minus X",
    "start": "2876334",
    "end": "2889430"
  },
  {
    "text": "bar X bar transpose. So when I do u\ntranspose, Su, what",
    "start": "2889430",
    "end": "2896060"
  },
  {
    "text": "it gives me is 1 over\nn sum from i equal 1 to n of u transpose Xi times\nXi transpose u, all right?",
    "start": "2896060",
    "end": "2905779"
  },
  {
    "text": "So those are two numbers\nthat multiply each other and that happen to be\nequal to each other, minus u transpose X\nbar X bar transpose u,",
    "start": "2905780",
    "end": "2916430"
  },
  {
    "text": "which is also the product\nof two numbers that happen to be equal to each other. So I can rewrite\nthis with squares.",
    "start": "2916430",
    "end": "2921455"
  },
  {
    "start": "2921455",
    "end": "2935120"
  },
  {
    "text": "So we're almost there. All I need to know to check\nis that this thing is actually",
    "start": "2935120",
    "end": "2940359"
  },
  {
    "text": "the average of\nthose guys, right? So u transpose X bar. What is it? It's 1 over n sum from i equal\n1 to n of u transpose Xi.",
    "start": "2940360",
    "end": "2950980"
  },
  {
    "text": "So it's really something that I\ncan write as u transpose X bar,",
    "start": "2950980",
    "end": "2957050"
  },
  {
    "text": "right? That's the average of\nthose random variables of the form, u transpose Xi. ",
    "start": "2957050",
    "end": "2963880"
  },
  {
    "text": "So what it means is that u\ntranspose Su, I can write as 1",
    "start": "2963880",
    "end": "2969910"
  },
  {
    "text": "over n sum from i equal 1 to\nn of u transpose Xi squared",
    "start": "2969910",
    "end": "2978059"
  },
  {
    "text": "minus u transpose X\nbar squared, which",
    "start": "2978060",
    "end": "2986720"
  },
  {
    "text": "is the empirical variance\nthat we need noted by small s squared, right?",
    "start": "2986720",
    "end": "2994599"
  },
  {
    "text": "So that's the empirical variance\nof u transpose X1 all the way",
    "start": "2994600",
    "end": "3006850"
  },
  {
    "text": "to u transpose Xn. ",
    "start": "3006850",
    "end": "3012430"
  },
  {
    "text": "OK, and here, same thing. I use exactly the same thing. I just use the fact that here,\nthe only thing I use is really",
    "start": "3012430",
    "end": "3017990"
  },
  {
    "text": "the linearity of this\nguy, of 1 over n sum or the linearity of expectation,\nthat I can push things",
    "start": "3017990",
    "end": "3024020"
  },
  {
    "text": "in there, OK? ",
    "start": "3024020",
    "end": "3030224"
  },
  {
    "text": "AUDIENCE: So what\nyou have written at the end of that\nsum for uT Su? PHILIPPE RIGOLLET: This one? AUDIENCE: Yeah.",
    "start": "3030224",
    "end": "3035380"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah, I\nsaid it's equal to small s, and I want to make a\ndifference between the big S that I'm using here.",
    "start": "3035380",
    "end": "3040660"
  },
  {
    "text": "So this is equal to small-- I don't know, I'm\ntrying to make it look like a calligraphic s squared.",
    "start": "3040660",
    "end": "3047550"
  },
  {
    "start": "3047550",
    "end": "3056870"
  },
  {
    "text": "OK, so this is nice, right? This covariance matrix-- so\nlet's look at capital sigma",
    "start": "3056870",
    "end": "3064120"
  },
  {
    "text": "itself right now. This covariance matrix,\nwe know that if we read its entries, what\nwe get is the covariance",
    "start": "3064120",
    "end": "3071690"
  },
  {
    "text": "between the coordinates\nof the X's, right, of the random vector, X.\nAnd the coordinates, well,",
    "start": "3071690",
    "end": "3079140"
  },
  {
    "text": "by definition, are attached\nto a coordinate system. So I only know\nwhat the covariance",
    "start": "3079140",
    "end": "3085830"
  },
  {
    "text": "of X in of those two things are,\nor the covariance of those two things are.",
    "start": "3085830",
    "end": "3091320"
  },
  {
    "text": "But what if I want to find\ncoordinates between linear combination of the X's? Sorry, if I want to find\ncovariances between linear",
    "start": "3091320",
    "end": "3097200"
  },
  {
    "text": "combination of those X's. And that's exactly what\nthis allows me to do. It says, well, if I pre-\nand post-multiply by u,",
    "start": "3097200",
    "end": "3104640"
  },
  {
    "text": "this is actually telling\nme what the variance of X along direction u is, OK?",
    "start": "3104640",
    "end": "3111950"
  },
  {
    "text": "So there's a lot of\ninformation in there, and it's just really\nexploiting the fact that there is some linearity\ngoing on in the covariance.",
    "start": "3111950",
    "end": "3120600"
  },
  {
    "text": "So, why variance? Why is variance\ninteresting for us, right? Why? I started by saying,\nhere, we're going",
    "start": "3120600",
    "end": "3125760"
  },
  {
    "text": "to be interested\nin having something to do dimension reduction. We have-- think of your points\nas [? being in a ?] dimension larger than 4, and we're going\nto try to reduce the dimension.",
    "start": "3125760",
    "end": "3133990"
  },
  {
    "text": "So let's just think\nfor one second, what do we want about a\ndimension reduction procedure?",
    "start": "3133990",
    "end": "3139320"
  },
  {
    "text": "If I have all my points that\nlive in, say, three dimensions, and I have one point\nhere and one point here",
    "start": "3139320",
    "end": "3145260"
  },
  {
    "text": "and one point here and one\npoint here and one point here, and I decide to project\nthem onto some plane-- that I take a plane that's\njust like this, what's",
    "start": "3145260",
    "end": "3152132"
  },
  {
    "text": "going to happen is that those\npoints are all going to project to the same point, right? I'm just going to\nnot see anything.",
    "start": "3152132",
    "end": "3158070"
  },
  {
    "text": "However, if I take a\nplane which is like this, they're all going to\nproject into some nice line. Maybe I can even\nproject them onto a line",
    "start": "3158070",
    "end": "3164640"
  },
  {
    "text": "and they will still be\nfar apart from each other. So that's what you want. You want to be able to\nsay, when I take my points",
    "start": "3164640",
    "end": "3171930"
  },
  {
    "text": "and I say I project them\nonto lower dimensions, I do not want them to collapse\ninto one single point.",
    "start": "3171930",
    "end": "3177270"
  },
  {
    "text": "I want them to be spread as\npossible in the direction on which I project. And this is what we're\ngoing to try to do.",
    "start": "3177270",
    "end": "3184000"
  },
  {
    "text": "And of course, measuring\nspread between points can be done in many ways, right? I mean, you could\nlook at, I don't know,",
    "start": "3184000",
    "end": "3189960"
  },
  {
    "text": "sum of pairwise distances\nbetween those guys. You could look at\nsome sort of energy. You can look at\nmany ways to measure",
    "start": "3189960",
    "end": "3196380"
  },
  {
    "text": "of spread in a direction. But variance is a\ngood way to measure of spread between points. If you have a lot of\nvariance between your points,",
    "start": "3196380",
    "end": "3203726"
  },
  {
    "text": "then chances are they're\ngoing to be spread. Now, this is not\nalways the case, right? If I have a direction in which\nall my points are clumped",
    "start": "3203727",
    "end": "3210480"
  },
  {
    "text": "onto one big point and\none other big point, it's going to choose\nthis because that's the direction that\nhas a lot of variance.",
    "start": "3210480",
    "end": "3217180"
  },
  {
    "text": "But hopefully, the\nvariance is going to spread things out nicely. So the idea of principal\ncomponent analysis",
    "start": "3217180",
    "end": "3227730"
  },
  {
    "text": "is going to try to\nidentify those variances-- those directions along which\nwe have a lot of variance.",
    "start": "3227730",
    "end": "3235740"
  },
  {
    "text": "Reciprocally, we're\ngoing to try to eliminate the directions along which we do\nnot have a lot of variance, OK?",
    "start": "3235740",
    "end": "3241890"
  },
  {
    "text": "And let's see why. Well, if-- so here's\nthe first claim.",
    "start": "3241890",
    "end": "3248130"
  },
  {
    "text": "If you transpose Su is equal\nto 0, what's happening?",
    "start": "3248130",
    "end": "3254000"
  },
  {
    "text": "Well, I know that an empirical\nvariance is equal to 0. What does it mean for\nan empirical variance to be equal to 0?",
    "start": "3254000",
    "end": "3262056"
  },
  {
    "text": "So I give you a bunch\nof points, right? So those points are those\npoints-- u transpose X1, u transpose-- those\nare a bunch of numbers.",
    "start": "3262056",
    "end": "3269089"
  },
  {
    "text": "What does it mean to have\nthe empirical variance of those points\nbeing equal to 0? AUDIENCE: They're all the same.",
    "start": "3269090",
    "end": "3274570"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nThey're all the same. So what it means is that\nwhen I have my points, right?",
    "start": "3274570",
    "end": "3283680"
  },
  {
    "text": "So, can you find a direction\nfor those points in which they project to all the same point?",
    "start": "3283680",
    "end": "3288849"
  },
  {
    "text": " No, right? There's no such thing. For this to happen, you have\nto have your points which",
    "start": "3288850",
    "end": "3295870"
  },
  {
    "text": "are perfectly aligned. And then when you're\ngoing to project onto the orthogonal\nof this guy, they're",
    "start": "3295870",
    "end": "3301829"
  },
  {
    "text": "going to all project\nto the same point here, which means that\nthe empirical variance is going to be 0.",
    "start": "3301830",
    "end": "3308790"
  },
  {
    "text": "Now, this is an extreme case. This will never\nhappen in practice, because if that\nhappens, well, I mean,",
    "start": "3308790",
    "end": "3313840"
  },
  {
    "text": "you can basically figure\nthat out very quickly. So in the same way,\nit's very unlikely",
    "start": "3313840",
    "end": "3321520"
  },
  {
    "text": "that you're going to have\nu transpose sigma u, which is equal to 0, which means\nthat, essentially, all your points are [INAUDIBLE]\nor let's say all of them",
    "start": "3321520",
    "end": "3328510"
  },
  {
    "text": "are orthogonal to u, right? So it's exactly the same thing. It just says that in\nthe population case, there's no probability that your\npoints deviate from this guy",
    "start": "3328510",
    "end": "3336960"
  },
  {
    "text": "here. This happens with\nzero probability, OK? And that's just\nbecause if you look",
    "start": "3336960",
    "end": "3342600"
  },
  {
    "text": "at the variance of this\nguy, it's going to be 0. And then that means that\nthere's no deviation.",
    "start": "3342600",
    "end": "3348910"
  },
  {
    "text": "By the way, I'm using\nthe name projection when I talk about u\ntranspose X, right?",
    "start": "3348910",
    "end": "3355510"
  },
  {
    "text": "So let's just be\nclear about this. If you-- so let's say I\nhave a bunch of points,",
    "start": "3355510",
    "end": "3364089"
  },
  {
    "text": "and u is a vector\nin this direction. And let's say that u has the-- so this is 0.",
    "start": "3364090",
    "end": "3370119"
  },
  {
    "text": "This is u. And let's say that\nu has norm, 1, OK?",
    "start": "3370120",
    "end": "3377560"
  },
  {
    "text": "When I look, what is the\ncoordinate of the projection? So what is the length\nof this guy here?",
    "start": "3377560",
    "end": "3383859"
  },
  {
    "text": "Let's call this guy X1. What is the length of this guy? ",
    "start": "3383860",
    "end": "3391150"
  },
  {
    "text": "In terms of inner products?  This is exactly u transpose X1.",
    "start": "3391150",
    "end": "3399678"
  },
  {
    "text": "This length here,\nif this is X2, this is exactly u transpose X2, OK?",
    "start": "3399678",
    "end": "3406580"
  },
  {
    "text": "So those-- u transpose X\nmeasure exactly the distance",
    "start": "3406580",
    "end": "3412430"
  },
  {
    "text": "to the origin of those-- I mean, it's really--",
    "start": "3412430",
    "end": "3418310"
  },
  {
    "text": "think of it as being\njust an x-axis thing. You just have a bunch of points. You have an origin. And it's really just\ntelling you what",
    "start": "3418310",
    "end": "3424520"
  },
  {
    "text": "the coordinate on this\naxis is going to be, right? So in particular, if the\nempirical variance is 0,",
    "start": "3424520",
    "end": "3430819"
  },
  {
    "text": "it means that all\nthese points project to the same point, which\nmeans that they have to be orthogonal to this guy.",
    "start": "3430820",
    "end": "3436912"
  },
  {
    "text": "And you can think of it as\nbeing also maybe an entire plane that's orthogonal\nto this line, OK?",
    "start": "3436912",
    "end": "3443990"
  },
  {
    "text": "So that's why I talk\nabout projection, because the inner\nproducts, u transpose X,",
    "start": "3443990",
    "end": "3449560"
  },
  {
    "text": "is really measuring\nthe coordinates of X",
    "start": "3449560",
    "end": "3456220"
  },
  {
    "text": "when u becomes the x-axis. Now, if u does not have\nnorm 1, then you just",
    "start": "3456220",
    "end": "3462819"
  },
  {
    "text": "have a change of scale here. You just have a\nchange of unit, right? So this is really u times X1.",
    "start": "3462820",
    "end": "3471560"
  },
  {
    "text": "The coordinates should really\nbe divided by the norm of u. ",
    "start": "3471560",
    "end": "3479150"
  },
  {
    "text": "OK, so now, just in\nthe same way-- so",
    "start": "3479150",
    "end": "3484970"
  },
  {
    "text": "we're never going\nto have exactly 0. But if we [INAUDIBLE]\nthe other end, if u transpose Su is\nlarge, what does it mean?",
    "start": "3484970",
    "end": "3492050"
  },
  {
    "text": " It means that when\nI look at my points",
    "start": "3492050",
    "end": "3497740"
  },
  {
    "text": "as projected onto the\naxis generated by u, they're going to have\na lot of variance.",
    "start": "3497740",
    "end": "3503860"
  },
  {
    "text": "They're going to be far away\nfrom each other in average, right? That's what large variance\nmeans, or at least",
    "start": "3503860",
    "end": "3508900"
  },
  {
    "text": "large empirical variance means. And same thing for u.",
    "start": "3508900",
    "end": "3514690"
  },
  {
    "text": "So what we're going\nto try to find is a u that maximizes this.",
    "start": "3514690",
    "end": "3519869"
  },
  {
    "text": "If I can find a u\nthat maximizes this so I can look in\nevery direction, and suddenly I find a direction\nin which the spread is massive,",
    "start": "3519870",
    "end": "3528320"
  },
  {
    "text": "then that's a point\non which I'm basically the less likely\nto have my points project onto each other\nand collide, right?",
    "start": "3528320",
    "end": "3534824"
  },
  {
    "text": "At least I know they're\ngoing to project at least onto two points. So the idea now is\nto say, OK, let's try",
    "start": "3534824",
    "end": "3542290"
  },
  {
    "text": "to maximize this spread, right? So we're going to try to\nfind the maximum over all u's",
    "start": "3542290",
    "end": "3549130"
  },
  {
    "text": "of u transpose Su. And that's going to be the\ndirection that maximizes",
    "start": "3549130",
    "end": "3555010"
  },
  {
    "text": "the empirical variance. Now of course, if I read it\nlike that for all u's in Rd,",
    "start": "3555010",
    "end": "3562075"
  },
  {
    "text": "what is the value\nof this maximum? ",
    "start": "3562075",
    "end": "3568060"
  },
  {
    "text": "It's infinity, right? Because I can always\nmultiply u by 10, and this entire thing is\ngoing to multiplied by 100.",
    "start": "3568060",
    "end": "3574661"
  },
  {
    "text": "So I'm just going to take\nu as large as I want, and this thing is going\nto be as large as I want, and so I need to constrain u.",
    "start": "3574662",
    "end": "3580049"
  },
  {
    "text": "And as I said, I need\nto have u of size 1 to talk about coordinates\nin the system generated",
    "start": "3580050",
    "end": "3585990"
  },
  {
    "text": "by u like this. So I'm just going to\nconstrain u to have Euclidean norm equal to 1, OK?",
    "start": "3585990",
    "end": "3595467"
  },
  {
    "text": "So that's going to\nbe my goal-- trying to find the largest\npossible u transpose Su,",
    "start": "3595467",
    "end": "3601100"
  },
  {
    "text": "or in other words, empirical\nvariance of the points projected onto the direction\nu when u is of norm 1,",
    "start": "3601100",
    "end": "3607520"
  },
  {
    "text": "which justifies to use\nthe word, \"direction,\" and because there's no\nmagnitude to this u.",
    "start": "3607520",
    "end": "3612830"
  },
  {
    "text": " OK, so how am I\ngoing to do this?",
    "start": "3612830",
    "end": "3622410"
  },
  {
    "text": "I could just fold and\nsay, let's just optimize this thing, right? Let's just take this problem.",
    "start": "3622410",
    "end": "3628540"
  },
  {
    "text": "It says maximize a function\nonto some constraints. Immediately, the constraint\nis sort of nasty.",
    "start": "3628540",
    "end": "3634125"
  },
  {
    "text": "I'm on a sphere, and I'm trying\nto move points on the sphere. And I'm maximizing\nthis thing which actually happens to be convex.",
    "start": "3634125",
    "end": "3640182"
  },
  {
    "text": "And we know we know how to\nminimize convex functions, but maximize them is\na different question.",
    "start": "3640182",
    "end": "3645279"
  },
  {
    "text": "And so this problem\nmight be super hard. So I can just say,\nOK, here's what I want to do, and let me\ngive that to an optimizer",
    "start": "3645280",
    "end": "3652950"
  },
  {
    "text": "and just hope that the optimizer\ncan solve this problem for me. That's one thing we can do. Now as you can imagine, PCA\nis so well spread, right?",
    "start": "3652950",
    "end": "3660091"
  },
  {
    "text": "Principal component\nanalysis is something that people do constantly. And so that means that we\nknow how to do this fast.",
    "start": "3660092",
    "end": "3666190"
  },
  {
    "text": "So that's one thing. The other thing that you should\nprobably question about why-- if this thing is actually\ndifficult, why in the world",
    "start": "3666190",
    "end": "3673110"
  },
  {
    "text": "would you even choose the\nvariance as a measure of spread if there's so many\nmeasures of spread, right?",
    "start": "3673110",
    "end": "3679020"
  },
  {
    "text": "The variance is one\nmeasure of spread. It's not guaranteed\nthat everything is going to project nicely\nfar apart from each other.",
    "start": "3679020",
    "end": "3686366"
  },
  {
    "text": "So we could choose\nthe variance, but we could choose something else. If the variance does\nnot help, why choose it? Turns out the variance helps.",
    "start": "3686366",
    "end": "3692520"
  },
  {
    "text": "So this is indeed a\nnon-convex problem. I'm maximizing, so\nit's actually the same.",
    "start": "3692520",
    "end": "3698339"
  },
  {
    "text": "I can make this\nconstraint convex because I'm maximizing\na convex function,",
    "start": "3698340",
    "end": "3703920"
  },
  {
    "text": "so it's clear that\nthe maximum is going to be attained at the boundary. So I can actually just fill\nthis ball into some convex ball.",
    "start": "3703920",
    "end": "3711540"
  },
  {
    "text": "However, I'm still\nmaximizing, so this is a non-convex problem. And this turns out to be the\nfanciest non-convex problem",
    "start": "3711540",
    "end": "3717549"
  },
  {
    "text": "we know how to solve. And the reason why we\nknow how to solve it is not because of optimization\nor using gradient-type things",
    "start": "3717550",
    "end": "3724410"
  },
  {
    "text": "or anything of the\nalgorithms that I mentioned during the maximum likelihood. It's because of linear algebra.",
    "start": "3724410",
    "end": "3731000"
  },
  {
    "text": "Linear algebra guarantees that\nwe know how to solve this. And to understand this, we\nneed to go a little deeper",
    "start": "3731000",
    "end": "3737885"
  },
  {
    "text": "in linear algebra, and we\nneed to understand the concept of diagonalization of a matrix.",
    "start": "3737885",
    "end": "3744590"
  },
  {
    "text": "So who has ever seen the\nconcept of an eigenvalue?",
    "start": "3744590",
    "end": "3749850"
  },
  {
    "text": "Oh, that's beautiful. And if you're not\nraising your hand, you're just playing\n\"Candy Crush,\" right? All right, so, OK.",
    "start": "3749850",
    "end": "3755930"
  },
  {
    "start": "3755930",
    "end": "3764930"
  },
  {
    "text": "This is great. Everybody's seen it. For my live audience of\nmillions, maybe you have not,",
    "start": "3764930",
    "end": "3771230"
  },
  {
    "text": "so I will still go through it. All right, so one\nof the basic facts--",
    "start": "3771230",
    "end": "3778840"
  },
  {
    "text": "and I remember when\nI learned this in-- I mean, when I was\nan undergrad, I",
    "start": "3778840",
    "end": "3784090"
  },
  {
    "text": "learned about the\nspectral decomposition and this diagonalization\nof matrices. And for me, it was just\na structural property of matrices, but it turns out\nthat it's extremely useful,",
    "start": "3784090",
    "end": "3791445"
  },
  {
    "text": "and it's useful for\nalgorithmic purposes. And so what this\ntheorem tells you is that if you take\na symmetric matrix--",
    "start": "3791445",
    "end": "3796765"
  },
  {
    "start": "3796765",
    "end": "3802859"
  },
  {
    "text": "well, with real\nentries, but that really does not matter so much.",
    "start": "3802860",
    "end": "3808220"
  },
  {
    "text": "And here, I'm\ngoing to actually-- so I take a symmetric matrix,\nand actually S and sigma are two such symmetric\nmatrices, right?",
    "start": "3808220",
    "end": "3816190"
  },
  {
    "text": "Then there exists P\nand D, which are both--",
    "start": "3816190",
    "end": "3824500"
  },
  {
    "text": "so let's say d by d. Which are both d by d\nsuch that P is orthogonal.",
    "start": "3824500",
    "end": "3835960"
  },
  {
    "text": " That means that P transpose\nP is equal to PP transpose",
    "start": "3835960",
    "end": "3842420"
  },
  {
    "text": "is equal to the identity. And D is diagonal.",
    "start": "3842420",
    "end": "3847630"
  },
  {
    "text": " And sigma, let's say, is\nequal to PDP transpose, OK?",
    "start": "3847630",
    "end": "3860130"
  },
  {
    "text": "So it's a diagonalization\nbecause it's finding a nice transformation. P has some nice properties.",
    "start": "3860130",
    "end": "3865260"
  },
  {
    "text": "It's really just the change\nof coordinates in which your matrix is diagonal, right?",
    "start": "3865260",
    "end": "3871044"
  },
  {
    "text": "And the way you\nwant to see this-- and I think it sort of helps\nto think about this problem as being--",
    "start": "3871044",
    "end": "3876720"
  },
  {
    "text": "sigma being a covariance matrix. What does a covariance\nmatrix tell you? Think of a\nmultivariate Gaussian. Can everybody visualize a\nthree-dimensional Gaussian",
    "start": "3876720",
    "end": "3883660"
  },
  {
    "text": "density? Right, so it's going to be some\nsort of a bell-shaped curve, but it might be more elongated\nin one direction than another.",
    "start": "3883660",
    "end": "3891869"
  },
  {
    "text": "And then going to chop\nit like that, all right? So I'm going to chop it off. And I'm going to look at\nhow it bleeds, all right?",
    "start": "3891870",
    "end": "3900070"
  },
  {
    "text": "So I'm just going to look\nat where the blood is. And what it's going to look at-- it's going to look like some\nsort of ellipsoid, right?",
    "start": "3900070",
    "end": "3908720"
  },
  {
    "text": "In high dimension, it's\njust going to be an olive. And that is just going\nto be bigger and bigger. And then I chop it\noff a little lower,",
    "start": "3908720",
    "end": "3916460"
  },
  {
    "text": "and I get something a\nlittle bigger like this. And so it turns out that sigma\nis capturing exactly this,",
    "start": "3916460",
    "end": "3923069"
  },
  {
    "text": "right? The matrix sigma-- so the\ncenter of your covariance matrix of your Gaussian is\ngoing to be this thing.",
    "start": "3923070",
    "end": "3929240"
  },
  {
    "text": "And sigma is going to tell you\nwhich direction it's elongated. And so in particular, if you\nlook, if you knew an ellipse,",
    "start": "3929240",
    "end": "3936140"
  },
  {
    "text": "you know there's something\ncalled principal axis, right? So you could actually\ndefine something that looks like this, which is\nthis axis, the one along which",
    "start": "3936140",
    "end": "3943190"
  },
  {
    "text": "it's the most elongated. Then the axis along which\nis orthogonal to it, along which it's\nslightly less elongated,",
    "start": "3943190",
    "end": "3949369"
  },
  {
    "text": "and you go again and again\nalong the orthogonal ones. It turns out that\nthose things here",
    "start": "3949370",
    "end": "3956500"
  },
  {
    "text": "is the new coordinate system\nin which this transformation, P and P transpose, is\nputting you into.",
    "start": "3956500",
    "end": "3963190"
  },
  {
    "text": "And D has entries\non the diagonal which are exactly this length\nand this length, right?",
    "start": "3963190",
    "end": "3969979"
  },
  {
    "text": "So that's just what it's doing. It's just telling\nyou, well, if you think of having this Gaussian\nor this high-dimensional",
    "start": "3969979",
    "end": "3976760"
  },
  {
    "text": "ellipsoid, it's elongated\nalong certain directions. And these directions are\nactually maybe not well aligned",
    "start": "3976760",
    "end": "3983020"
  },
  {
    "text": "with your original coordinate\nsystem, which might just be the usual one, right-- north, south, and east, west.",
    "start": "3983020",
    "end": "3989740"
  },
  {
    "text": "Maybe I need to turn it. And that's exactly what this\northogonal transformation is doing for you, all right?",
    "start": "3989740",
    "end": "3996820"
  },
  {
    "text": "So, in a way, this is actually\ntelling you even more. It's telling you that any\nmatrix that's symmetric, you can actually\nturn it somewhere.",
    "start": "3996820",
    "end": "4005190"
  },
  {
    "text": "And that'll start to dilate\nthings in the directions that you have, and\nthen turn it back to what you originally had.",
    "start": "4005190",
    "end": "4010800"
  },
  {
    "text": "And that's actually\nexactly the effect of applying a symmetric matrix\nthrough a vector, right?",
    "start": "4010800",
    "end": "4017180"
  },
  {
    "text": "And it's pretty impressive. It says if I take sigma\ntimes v. Any sigma that's",
    "start": "4017180",
    "end": "4024650"
  },
  {
    "text": "of this form, what I'm\ndoing is-- that's symmetric. What I'm really\ndoing to v is I'm changing its coordinate\nsystem, so I'm rotating it.",
    "start": "4024650",
    "end": "4032150"
  },
  {
    "text": "Then I'm changing-- I'm\nmultiplying its coordinates, and then I'm rotating it back. That's all it's\ndoing, and that's",
    "start": "4032150",
    "end": "4038330"
  },
  {
    "text": "what all symmetric\nmatrices do, which means that this is doing a lot.",
    "start": "4038330",
    "end": "4044070"
  },
  {
    "text": "All right, so OK. So, what do I know?",
    "start": "4044070",
    "end": "4049236"
  },
  {
    "text": "So I'm not going to\nprove that this is the so-called spectral theorem. ",
    "start": "4049237",
    "end": "4059269"
  },
  {
    "text": "And the diagonal entries of\nD is of the form, lambda 1,",
    "start": "4059270",
    "end": "4065850"
  },
  {
    "text": "lambda 2, lambda d, 0, 0. And the lambda j's are\ncalled eigenvalues of D.",
    "start": "4065850",
    "end": "4081800"
  },
  {
    "text": "Now in general, those numbers\ncan be positive, negative, or equal to 0. But here, I know that\nsigma and S are--",
    "start": "4081800",
    "end": "4092000"
  },
  {
    "text": "well, they're\nsymmetric for sure, but they are positive\nsemidefinite.",
    "start": "4092000",
    "end": "4097466"
  },
  {
    "start": "4097467",
    "end": "4103939"
  },
  {
    "text": "What does it mean? It means that when I take u\ntranspose sigma u for example,",
    "start": "4103939",
    "end": "4110929"
  },
  {
    "text": "this number is\nalways non-negative.  Why is this true?",
    "start": "4110930",
    "end": "4116720"
  },
  {
    "start": "4116720",
    "end": "4122770"
  },
  {
    "text": "What is this number?  It's the variance of--\nand actually, I don't even",
    "start": "4122770",
    "end": "4129850"
  },
  {
    "text": "need to finish this sentence. As soon as I say that\nthis is a variance, well, it has to be non-negative.",
    "start": "4129850",
    "end": "4135040"
  },
  {
    "text": "We know that a variance\nis not negative. And so, that's also a\nnice way you can use that.",
    "start": "4135040",
    "end": "4140532"
  },
  {
    "text": "So it's just to say,\nwell, OK, this thing is positive semidefinite because\nit's a covariance matrix. So I know it's a variance, OK?",
    "start": "4140532",
    "end": "4146920"
  },
  {
    "text": "So I get this. Now, if I had some\nnegative numbers-- so the effect of that is that\nwhen I draw this picture,",
    "start": "4146920",
    "end": "4155350"
  },
  {
    "text": "those axes are always positive,\nwhich is kind of a weird thing to say. But what it means is that when\nI take a vector, v, I rotate it,",
    "start": "4155350",
    "end": "4163839"
  },
  {
    "text": "and then I stretch it in the\ndirections of the coordinate, I cannot flip it.",
    "start": "4163840",
    "end": "4170259"
  },
  {
    "text": "I can only stretch or shrink,\nbut I cannot flip its sign, all right? But in general, for\nany symmetric matrices,",
    "start": "4170260",
    "end": "4177369"
  },
  {
    "text": "I could do this. But when it's positive\nsymmetric definite, actually what turns out\nis that all the lambda",
    "start": "4177370",
    "end": "4183020"
  },
  {
    "text": "j's are non-negative.",
    "start": "4183020",
    "end": "4188350"
  },
  {
    "text": "I cannot flip it, OK? So all the eigenvalues\nare non-negative.",
    "start": "4188350",
    "end": "4193777"
  },
  {
    "text": " That's a property\nof positive semidef. So when it's symmetric,\nyou have the eigenvalues.",
    "start": "4193778",
    "end": "4200510"
  },
  {
    "text": "They can be any number. And when it's positive\nsemidefinite, in particular that's the case of\nthe covariance matrix and the empirical\ncovariance matrix, right?",
    "start": "4200510",
    "end": "4207110"
  },
  {
    "text": "Because the empirical\ncovariance matrix is an empirical variance,\nwhich itself is non-negative.",
    "start": "4207110",
    "end": "4212150"
  },
  {
    "text": "And so I get that the\neigenvalues are non-negative.",
    "start": "4212150",
    "end": "4217900"
  },
  {
    "text": "All right, so principal\ncomponent analysis is saying,",
    "start": "4217900",
    "end": "4223030"
  },
  {
    "text": "OK, I want to find\nthe direction, u,",
    "start": "4223030",
    "end": "4232369"
  },
  {
    "text": "that maximizes u\ntranspose Su, all right?",
    "start": "4232370",
    "end": "4238830"
  },
  {
    "text": "I've just introduced\nin one slide something about eigenvalues. So hopefully, they should help.",
    "start": "4238830",
    "end": "4244739"
  },
  {
    "text": "So what is it that I'm\ngoing to be getting? Well, let's just\nsee what happens.",
    "start": "4244740",
    "end": "4251446"
  },
  {
    "text": "Oh, I forgot to mention\nthat-- and I will use this. So the lambda j's are\ncalled eigenvectors. And then the matrix, P,\nhas columns v1 to vd, OK?",
    "start": "4251446",
    "end": "4268690"
  },
  {
    "text": "The fact that it's orthogonal--\nthat P transpose P is equal to the identity--",
    "start": "4268690",
    "end": "4275470"
  },
  {
    "text": "means that those guys\nsatisfied that vi transpose",
    "start": "4275470",
    "end": "4280810"
  },
  {
    "text": "vj is equal to 0 if i\nis different from j.",
    "start": "4280810",
    "end": "4287485"
  },
  {
    "text": "And vi transpose vi is\nactually equal to 1, right, because the\nentries of PP transpose",
    "start": "4287485",
    "end": "4293920"
  },
  {
    "text": "are exactly going to be of\nthe form, vi transpose vj, OK?",
    "start": "4293920",
    "end": "4298989"
  },
  {
    "text": "So those v's are\ncalled eigenvectors. ",
    "start": "4298990",
    "end": "4306000"
  },
  {
    "text": "And v1 is attached to lambda 1,\nand v2 is attached to lambda 2,",
    "start": "4306000",
    "end": "4312020"
  },
  {
    "text": "OK? So let's see what's\nhappening with those things. What happens if I take sigma--",
    "start": "4312020",
    "end": "4318045"
  },
  {
    "text": "so if you know eigenvalues,\nyou know exactly what's going to happen. If I look at, say, sigma\ntimes v1, well, what is sigma?",
    "start": "4318045",
    "end": "4326920"
  },
  {
    "text": "We know that sigma\nis PDP transpose v1.",
    "start": "4326920",
    "end": "4335440"
  },
  {
    "text": "What is P transpose times v1? Well, P transpose has\nrows v1 transpose,",
    "start": "4335440",
    "end": "4341560"
  },
  {
    "text": "v2 transpose, all the\nway to vd transpose.",
    "start": "4341560",
    "end": "4346850"
  },
  {
    "text": "So when I multiply\nthis by v1, what I'm left with is\nthe first coordinate",
    "start": "4346850",
    "end": "4352820"
  },
  {
    "text": "is going to be equal to 1\nand the second coordinate is",
    "start": "4352820",
    "end": "4358010"
  },
  {
    "text": "going to be equal to 0, right? Because they're\northogonal to each other-- 0 all the way to the end.",
    "start": "4358010",
    "end": "4365810"
  },
  {
    "text": "So that's when I\ndo P transpose v1. Now I multiply by\nD. Well, I'm just",
    "start": "4365810",
    "end": "4375250"
  },
  {
    "text": "multiplying this guy by lambda\n1, this guy by lambda 2, and this guy by lambda d, so\nthis is really just lambda 1.",
    "start": "4375250",
    "end": "4382150"
  },
  {
    "text": " And now I need to\npost-multiply by P.",
    "start": "4382150",
    "end": "4392080"
  },
  {
    "text": "So what is P times this guy? Well, P is v1 all the way to vd.",
    "start": "4392080",
    "end": "4399730"
  },
  {
    "text": "And now I multiply\nby a vector that only has 0's except\nlambda 1 on the first guy. So this is just\nlambda 1 times v1.",
    "start": "4399730",
    "end": "4406510"
  },
  {
    "text": " So what we've proved is that\nsigma times v1 is lambda 1 v1,",
    "start": "4406510",
    "end": "4414630"
  },
  {
    "text": "and that's probably the\nnotion of eigenvalue you're most comfortable with, right? So just when I\nmultiply by v1, I get",
    "start": "4414630",
    "end": "4421619"
  },
  {
    "text": "v1 back multiplied by something,\nwhich is the eigenvalue. So in particular, if I look\nat v1, transpose sigma v1,",
    "start": "4421620",
    "end": "4434449"
  },
  {
    "text": "what do I get? Well, I get lambda\n1 v1 transpose v1, which is 1, right?",
    "start": "4434450",
    "end": "4440179"
  },
  {
    "text": "So this is actually\nlambda 1 v1 transpose v1, which is lambda 1, OK?",
    "start": "4440180",
    "end": "4448360"
  },
  {
    "text": "And if I do the same\nwith v2, clearly I'm going to get v2 transpose sigma.",
    "start": "4448360",
    "end": "4453449"
  },
  {
    "text": "v2 is equal to lambda 2. So for each of the\nvj's, I know that if I",
    "start": "4453450",
    "end": "4459910"
  },
  {
    "text": "look at the variance\nalong the vj, it's actually exactly given by\nthose eigenvalues, all right?",
    "start": "4459910",
    "end": "4467760"
  },
  {
    "text": "Which proves this, because the\nvariance along the eigenvectors",
    "start": "4467760",
    "end": "4478489"
  },
  {
    "text": "is actually equal\nto the eigenvalues. So since they're variances,\nthey have to be non-negative.",
    "start": "4478490",
    "end": "4483760"
  },
  {
    "text": "So now, I'm looking for\nthe one direction that has the most variance, right?",
    "start": "4483760",
    "end": "4490450"
  },
  {
    "text": "But that's not only\namong the eigenvectors. That's also among\nthe other directions",
    "start": "4490450",
    "end": "4495520"
  },
  {
    "text": "that are in-between\nthe eigenvectors. If I were to look only\nat the eigenvectors, it would just tell me, well,\njust pick the eigenvector, vj,",
    "start": "4495520",
    "end": "4502420"
  },
  {
    "text": "that's associated to the\nlargest of the lambda j's. But it turns out that that's\nalso true for any vector--",
    "start": "4502420",
    "end": "4509080"
  },
  {
    "text": "that the maximum direction is\nactually one direction which is among the eigenvectors. And among the eigenvectors,\nwe know that the one that's",
    "start": "4509080",
    "end": "4516100"
  },
  {
    "text": "the largest-- that carries the\nlargest variance is the one that's associated to the\nlargest eigenvalue, all right?",
    "start": "4516100",
    "end": "4523780"
  },
  {
    "text": "And so this is what PCA is\ngoing to try to do for me. So in practice, that's what\nI mentioned already, right?",
    "start": "4523780",
    "end": "4529420"
  },
  {
    "text": "We're trying to\nproject the point cloud onto a low-dimensional\nspace, D prime,",
    "start": "4529420",
    "end": "4534730"
  },
  {
    "text": "by keeping as much\ninformation as possible. And by \"as much information,\"\nI mean we do not want points to collide.",
    "start": "4534730",
    "end": "4541540"
  },
  {
    "text": "And so what PCA is\ngoing to do is just going to try to project\n[? on two ?] directions.",
    "start": "4541540",
    "end": "4548231"
  },
  {
    "text": "So there's going\nto be a u, and then there's going to be something\northogonal to u, and then the third one, et cetera, so\nthat once we project on those,",
    "start": "4548231",
    "end": "4555550"
  },
  {
    "text": "we're keeping as much of the\ncovariance as possible, OK? And in particular,\nthose directions",
    "start": "4555550",
    "end": "4562859"
  },
  {
    "text": "that we're going to\npick are actually a subset of the vj's that\nare associated to the largest eigenvalues.",
    "start": "4562859",
    "end": "4568580"
  },
  {
    "text": "So I'm going to\nstop here for today. We'll finish this on Tuesday.",
    "start": "4568580",
    "end": "4575020"
  },
  {
    "text": "But basically, the idea is\nit's just the following. You're just going to--\nwell, let me skip one more.",
    "start": "4575020",
    "end": "4582590"
  },
  {
    "text": "Yeah, this is the idea. You're first going to pick\nthe eigenvector associated to the largest eigenvalue.",
    "start": "4582590",
    "end": "4590290"
  },
  {
    "text": "Then you're going to pick\nthe direction that orthogonal to the vector that\nyou've picked,",
    "start": "4590290",
    "end": "4597130"
  },
  {
    "text": "and that's carrying\nthe most variance. And that's actually\nthe second largest-- the eigenvector associated to\nthe second largest eigenvalue.",
    "start": "4597130",
    "end": "4604030"
  },
  {
    "text": "And you're going to go all\nthe way to the number of them that you actually want to pick,\nwhich is in this case, d, OK?",
    "start": "4604030",
    "end": "4610120"
  },
  {
    "text": "And wherever you choose\nto chop this process, not going all the way to d,\nis going to actually give you",
    "start": "4610120",
    "end": "4616390"
  },
  {
    "text": "a lower-dimensional\nrepresentation in the coordinate system\nthat's given by v1, v2, v3, et cetera, OK?",
    "start": "4616390",
    "end": "4622420"
  },
  {
    "text": "So we'll see that in\nmore details on Tuesday. But I don't want\nto get into it now. We don't have enough time.",
    "start": "4622420",
    "end": "4627500"
  },
  {
    "text": "Are there any questions? ",
    "start": "4627500",
    "end": "4631079"
  }
]