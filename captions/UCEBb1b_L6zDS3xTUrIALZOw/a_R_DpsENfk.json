[
  {
    "start": "0",
    "end": "61000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  JULIAN SHUN: Good\nafternoon, everyone.",
    "start": "18140",
    "end": "23390"
  },
  {
    "text": "So let's get started. So today, we're\ngoing to be talking about races and parallelism.",
    "start": "23390",
    "end": "31130"
  },
  {
    "text": "And you'll be doing a lot\nof parallel programming for the next homework\nassignment and project.",
    "start": "31130",
    "end": "38173"
  },
  {
    "text": "One thing I want to point\nout is that it's important to meet with your MITPOSSE\nas soon as possible,",
    "start": "38173",
    "end": "43800"
  },
  {
    "text": "if you haven't done so\nalready, since that's going to be part of the\nevaluation for the Project 1",
    "start": "43800",
    "end": "49430"
  },
  {
    "text": "grade. And if you have trouble\nreaching your MITPOSSE members, please contact your TA and\nalso make a post on Piazza",
    "start": "49430",
    "end": "57087"
  },
  {
    "text": "as soon as possible.  So as a reminder, let's\nlook at the basics of Cilk.",
    "start": "57087",
    "end": "65209"
  },
  {
    "start": "61000",
    "end": "61000"
  },
  {
    "text": "So we have cilk_spawn\nand cilk_sync statements. In Cilk, this was\nthe code that we",
    "start": "65209",
    "end": "72080"
  },
  {
    "text": "saw in last lecture, which\ncomputes the nth Fibonacci number. So when we say\ncilk_spawn, it means",
    "start": "72080",
    "end": "80509"
  },
  {
    "text": "that the named child\nfunction, the function right after the cilk_spawn keyword,\ncan execute in parallel",
    "start": "80510",
    "end": "86450"
  },
  {
    "text": "with the parent caller. So it says that fib\nof n minus 1 can execute in parallel with the\nfib function that called it.",
    "start": "86450",
    "end": "95420"
  },
  {
    "text": "And then cilk_sync says that\ncontrol cannot pass this point until all of this spawned\nchildren have returned.",
    "start": "95420",
    "end": "102870"
  },
  {
    "text": "So this is going to wait\nfor fib of n minus 1 to finish before it goes on\nand returns the sum of x and y.",
    "start": "102870",
    "end": "113240"
  },
  {
    "text": "And recall that the Cilk\nkeywords grant permission for parallel execution,\nbut they don't actually",
    "start": "113240",
    "end": "118280"
  },
  {
    "text": "force parallel execution. So this code here says that we\ncan execute fib of n minus 1",
    "start": "118280",
    "end": "123800"
  },
  {
    "text": "in parallel with\nthis parent caller, but it doesn't say that\nwe necessarily have to execute them in parallel.",
    "start": "123800",
    "end": "130310"
  },
  {
    "text": "And it's up to\nthe runtime system to decide whether these\ndifferent functions will",
    "start": "130310",
    "end": "136040"
  },
  {
    "text": "be executed in parallel. We'll talk more about\nthe runtime system today.",
    "start": "136040",
    "end": "141980"
  },
  {
    "start": "141000",
    "end": "141000"
  },
  {
    "text": "And also, we talked\nabout this example, where we wanted to do an\nin-place matrix transpose.",
    "start": "141980",
    "end": "148310"
  },
  {
    "text": "And this used the\ncilk_for keyword. And this says that\nwe can execute",
    "start": "148310",
    "end": "154100"
  },
  {
    "text": "the iterations of this\ncilk_for loop in parallel.",
    "start": "154100",
    "end": "159260"
  },
  {
    "text": "And again, this says\nthat the runtime system is allowed to schedule these\niterations in parallel,",
    "start": "159260",
    "end": "164347"
  },
  {
    "text": "but doesn't necessarily\nsay that they have to execute in parallel.",
    "start": "164348",
    "end": "169940"
  },
  {
    "text": "And under the hood,\ncilk_for statements are translated into nested\ncilk_spawn and cilk_sync calls.",
    "start": "169940",
    "end": "178620"
  },
  {
    "text": "So the compiler is going\nto divide the iteration space in half, do a cilk_spawn\non one of the two halves,",
    "start": "178620",
    "end": "186689"
  },
  {
    "text": "call the other\nhalf, and then this is done recursively\nuntil we reach",
    "start": "186690",
    "end": "192200"
  },
  {
    "text": "a certain size for the\nnumber of iterations in a loop, at\nwhich point it just creates a single task for that.",
    "start": "192200",
    "end": "199730"
  },
  {
    "text": "So any questions on\nthe Cilk constructs? Yes? AUDIENCE: Is Cilk smart\nenough to recognize issues",
    "start": "199730",
    "end": "207680"
  },
  {
    "text": "with reading and writing\nfor matrix transpose? JULIAN SHUN: So\nit's actually not going to figure out\nwhether the iterations are",
    "start": "207680",
    "end": "216950"
  },
  {
    "text": "independent for you. The programmer actually\nhas to reason about that. But Cilk does have a nice\ntool, which we'll talk about,",
    "start": "216950",
    "end": "224090"
  },
  {
    "text": "that will tell you which\nplaces your code might possibly be reading and writing\nthe same memory location,",
    "start": "224090",
    "end": "230540"
  },
  {
    "text": "and that allows you to\nlocalize any possible race bugs in your code. So we'll actually\ntalk about races.",
    "start": "230540",
    "end": "237020"
  },
  {
    "text": "But if you just\ncompile this code, Cilk isn't going to know whether\nthe iterations are independent.",
    "start": "237020",
    "end": "243462"
  },
  {
    "text": " So determinacy races--\nso race conditions",
    "start": "243462",
    "end": "253000"
  },
  {
    "start": "252000",
    "end": "252000"
  },
  {
    "text": "are the bane of concurrency. So you don't want to have\nrace conditions in your code.",
    "start": "253000",
    "end": "258670"
  },
  {
    "text": "And there are these two famous\nrace bugs that cause disaster. So there is this Therac-25\nradiation therapy machine,",
    "start": "258670",
    "end": "267850"
  },
  {
    "text": "and there was a race\ncondition in the software. And this led to three\npeople being killed and many more being\nseriously injured.",
    "start": "267850",
    "end": "276099"
  },
  {
    "text": "The North American\nblackout of 2003 was also caused by a\nrace bug in the software,",
    "start": "276100",
    "end": "281530"
  },
  {
    "text": "and this left 50 million\npeople without power. So these are very bad.",
    "start": "281530",
    "end": "287050"
  },
  {
    "text": "And they're notoriously\ndifficult to discover by conventional testing. So race bugs aren't going\nto appear every time",
    "start": "287050",
    "end": "292870"
  },
  {
    "text": "you execute your program. And in fact, the hardest ones to\nfind, which cause these events,",
    "start": "292870",
    "end": "299980"
  },
  {
    "text": "are actually very rare events. So most of the times when\nyou run your program, you're not going to\nsee the race bug.",
    "start": "299980",
    "end": "305212"
  },
  {
    "text": "Only very rarely\nwill you see it. So this makes it very hard\nto find these race bugs.",
    "start": "305212",
    "end": "310512"
  },
  {
    "text": "And furthermore, when\nyou see a race bug, it doesn't necessarily\nalways happen in the same place in your code.",
    "start": "310512",
    "end": "315662"
  },
  {
    "text": "So that makes it even harder.  So what is a race?",
    "start": "315662",
    "end": "320919"
  },
  {
    "start": "320000",
    "end": "320000"
  },
  {
    "text": "So a determinacy race is one of\nthe most basic forms of races. And a determinacy\nrace occurs when",
    "start": "320920",
    "end": "327550"
  },
  {
    "text": "two logically\nparallel instructions access the same memory\nlocation, and at least one",
    "start": "327550",
    "end": "332560"
  },
  {
    "text": "of these instructions performs\na write to that location. So let's look at\na simple example.",
    "start": "332560",
    "end": "339500"
  },
  {
    "text": "So in this code here, I'm\nfirst setting x equal to 0. And then I have a cilk_for\nloop with two iterations,",
    "start": "339500",
    "end": "345790"
  },
  {
    "text": "and each of the\ntwo iterations are incrementing this variable x. And then at the end, I'm going\nto assert that x is equal to 2.",
    "start": "345790",
    "end": "355090"
  },
  {
    "text": "So there's actually a\nrace in this program here. So in order to understand\nwhere the race occurs,",
    "start": "355090",
    "end": "361540"
  },
  {
    "text": "let's look at the\nexecution graph here. So I'm going to label each of\nthese statements with a letter.",
    "start": "361540",
    "end": "368200"
  },
  {
    "text": "The first statement, a, is\njust setting x equal to 0. And then after\nthat, we're actually",
    "start": "368200",
    "end": "374500"
  },
  {
    "text": "going to have two\nparallel paths, because we have two iterations of\nthis cilk_for loop, which can execute in parallel.",
    "start": "374500",
    "end": "381190"
  },
  {
    "text": "And each of these paths are\ngoing to increment x by 1.",
    "start": "381190",
    "end": "386840"
  },
  {
    "text": "And then finally, we're going\nto assert that x is equal to 2 at the end.",
    "start": "386840",
    "end": "393010"
  },
  {
    "text": "And this sort of graph is\nknown as a dependency graph. It tells you what\ninstructions have",
    "start": "393010",
    "end": "398620"
  },
  {
    "text": "to finish before you execute\nthe next instruction. So here it says\nthat B and C must",
    "start": "398620",
    "end": "403840"
  },
  {
    "text": "wait for A to execute\nbefore they proceed, but B and C can actually happen\nin parallel, because there is no dependency among them.",
    "start": "403840",
    "end": "409840"
  },
  {
    "text": "And then D has to happen\nafter B and C finish.",
    "start": "409840",
    "end": "415300"
  },
  {
    "text": "So to understand why\nthere's a race bug here, we actually need to\ntake a closer look at this dependency graph.",
    "start": "415300",
    "end": "421640"
  },
  {
    "text": "So let's take a closer look. So when you run this\ncode, x plus plus",
    "start": "421640",
    "end": "428620"
  },
  {
    "start": "422000",
    "end": "422000"
  },
  {
    "text": "is actually going to be\ntranslated into three steps. So first, we're going\nto load the value",
    "start": "428620",
    "end": "434530"
  },
  {
    "text": "of x into some\nprocessor's register, r1. And then we're going\nto increment r1,",
    "start": "434530",
    "end": "440980"
  },
  {
    "text": "and then we're going to set\nx equal to the result of r1. And the same thing for r2. We're going to load x into\nregister r2, increment r2,",
    "start": "440980",
    "end": "450160"
  },
  {
    "text": "and then set x equal to r2. ",
    "start": "450160",
    "end": "455620"
  },
  {
    "text": "So here, we have a race,\nbecause both of these stores,",
    "start": "455620",
    "end": "463990"
  },
  {
    "text": "x1 equal to r1 and\nx2 equal to r2, are actually writing to\nthe same memory location.",
    "start": "463990",
    "end": "469840"
  },
  {
    "start": "469000",
    "end": "469000"
  },
  {
    "text": "So let's look at one possible\nexecution of this computation graph. And we're going to keep track\nof the values of x, r1 and r2.",
    "start": "469840",
    "end": "478194"
  },
  {
    "text": " So the first instruction\nwe're going to execute is x equal to 0.",
    "start": "478195",
    "end": "484120"
  },
  {
    "text": "So we just set x equal to 0,\nand everything's good so far. And then next, we can actually\npick one of two instructions",
    "start": "484120",
    "end": "491560"
  },
  {
    "text": "to execute, because both\nof these two instructions have their predecessors\nsatisfied already.",
    "start": "491560",
    "end": "499090"
  },
  {
    "text": "Their predecessors\nhave already executed. So let's say I pick r1\nequal to x to execute.",
    "start": "499090",
    "end": "506090"
  },
  {
    "text": "And this is going to place\nthe value 0 into register r1. Now I'm going to\nincrement r1, so this",
    "start": "506090",
    "end": "513460"
  },
  {
    "text": "changes the value in r1 to 1. Then now, let's say I\nexecute r2 equal to x.",
    "start": "513460",
    "end": "521140"
  },
  {
    "text": "So that's going to read\nx, which has a value of 0. It's going to place\nthe value of 0 into r2.",
    "start": "521140",
    "end": "526700"
  },
  {
    "text": "It's going to increment r2. That's going to change\nthat value to 1. And then now, let's say\nI write r2 back to x.",
    "start": "526700",
    "end": "534550"
  },
  {
    "text": "So I'm going to place\na value of 1 into x. Then now, when I execute this\ninstruction, x1 equal to r1,",
    "start": "534550",
    "end": "542250"
  },
  {
    "text": "it's also placing a\nvalue of 1 into x. And then finally, when\nI do the assertion,",
    "start": "542250",
    "end": "549190"
  },
  {
    "text": "this value here is not equal\nto 2, and that's wrong. Because if you executed\nthis sequentially,",
    "start": "549190",
    "end": "554590"
  },
  {
    "text": "you would get a value of 2 here. And the reason-- as I said,\nthe reason why this occurs",
    "start": "554590",
    "end": "560529"
  },
  {
    "text": "is because we have\nmultiple writes to the same shared\nmemory location, which could execute in parallel.",
    "start": "560530",
    "end": "567910"
  },
  {
    "text": "And one of the nasty\nthings about this example here is that the race bug\ndoesn't necessarily always",
    "start": "567910",
    "end": "574850"
  },
  {
    "text": "occur. So does anyone see why this\nrace bug doesn't necessarily always show up? ",
    "start": "574850",
    "end": "582730"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] ",
    "start": "582730",
    "end": "588748"
  },
  {
    "text": "JULIAN SHUN: Right. So the answer is because if\none of these two branches",
    "start": "588748",
    "end": "593750"
  },
  {
    "text": "executes all three\nof its instructions before we start the other one,\nthen the final result in x",
    "start": "593750",
    "end": "599330"
  },
  {
    "text": "is going to be 2,\nwhich is correct. So if I executed\nthese instructions in order of 1, 2, 3, 7, 4, 5, 6,\nand then, finally, 8, the value",
    "start": "599330",
    "end": "608690"
  },
  {
    "text": "is going to be 2 in x. So the race bug here doesn't\nnecessarily always occur.",
    "start": "608690",
    "end": "615960"
  },
  {
    "text": "And this is one thing that\nmakes these bugs hard to find. So any questions?",
    "start": "615960",
    "end": "621500"
  },
  {
    "start": "621500",
    "end": "630030"
  },
  {
    "start": "627000",
    "end": "627000"
  },
  {
    "text": "So there are two different\ntypes of determinacy races. And they're shown\nin this table here.",
    "start": "630030",
    "end": "636990"
  },
  {
    "text": "So let's suppose that\ninstruction A and instruction B both access some location x,\nand suppose A is parallel to B.",
    "start": "636990",
    "end": "644660"
  },
  {
    "text": "So both of the instructions\ncan execute in parallel. So if A and B are just\nreading that location,",
    "start": "644660",
    "end": "651439"
  },
  {
    "text": "then that's fine. You don't actually\nhave a race here. But if one of the\ntwo instructions is writing to that location,\nwhereas the other one is",
    "start": "651440",
    "end": "659150"
  },
  {
    "text": "reading to that\nlocation, then you have what's called a read race. And the program might have\na non-deterministic result",
    "start": "659150",
    "end": "666950"
  },
  {
    "text": "when you have a read race,\nbecause the final answer might depend on whether you\nread A first before B",
    "start": "666950",
    "end": "673250"
  },
  {
    "text": "updated the value, or\nwhether A read the updated value before B reads it.",
    "start": "673250",
    "end": "679680"
  },
  {
    "text": "So the order of the\nexecution of A and B can affect the final\nresult that you see.",
    "start": "679680",
    "end": "686420"
  },
  {
    "text": "And finally, if\nboth A and B write to the same shared location,\nthen you have a write race.",
    "start": "686420",
    "end": "692420"
  },
  {
    "text": "And again, this will cause\nnon-deterministic behavior in your program, because the\nfinal answer could depend on",
    "start": "692420",
    "end": "697610"
  },
  {
    "text": "whether A did the write first\nor B did the write first. And we say that two\nsections of code",
    "start": "697610",
    "end": "704180"
  },
  {
    "text": "are independent if there are no\ndeterminacy races between them.",
    "start": "704180",
    "end": "709200"
  },
  {
    "text": "So the two pieces of code\ncan't have a shared location, where one computation\nwrites to it",
    "start": "709200",
    "end": "715490"
  },
  {
    "text": "and another computation\nreads from it, or if both computations\nwrite to that location.",
    "start": "715490",
    "end": "723199"
  },
  {
    "text": "Any questions on the definition? ",
    "start": "723200",
    "end": "729660"
  },
  {
    "text": "So races are really bad,\nand you should avoid having races in your program.",
    "start": "729660",
    "end": "736590"
  },
  {
    "text": "So here are some tips\non how to avoid races. So I can tell you not to\nwrite races in your program,",
    "start": "736590",
    "end": "742140"
  },
  {
    "text": "and you know that races\nare bad, but sometimes, when you're writing\ncode, you just have races in your program,\nand you can't help it.",
    "start": "742140",
    "end": "748740"
  },
  {
    "text": "But here are some tips on\nhow you can avoid races. So first, the iterations\nof a cilk_for loop",
    "start": "748740",
    "end": "756733"
  },
  {
    "text": "should be independent. So you should make sure that\nthe different iterations of a cilk_for loop aren't\nwriting to the same memory",
    "start": "756733",
    "end": "764095"
  },
  {
    "text": "location.  Secondly, between a\ncilk_spawn statement",
    "start": "764095",
    "end": "770070"
  },
  {
    "start": "770000",
    "end": "770000"
  },
  {
    "text": "and a corresponding cilk_sync,\nthe code of the spawn child should be independent of\nthe code of the parent.",
    "start": "770070",
    "end": "777150"
  },
  {
    "text": "And this includes code that's\nexecuted by additional spawned or called children\nby the spawned child.",
    "start": "777150",
    "end": "784348"
  },
  {
    "text": "So you should make sure\nthat these pieces of code are independent--\nthere's no read or write races between them. ",
    "start": "784348",
    "end": "792370"
  },
  {
    "text": "One thing to note is that the\narguments to a spawn function are evaluated in the parent\nbefore the spawn actually",
    "start": "792370",
    "end": "797820"
  },
  {
    "text": "occurs. So you can't get a race in\nthe argument evaluation, because the parent is going\nto evaluate these arguments.",
    "start": "797820",
    "end": "805620"
  },
  {
    "text": "And there's only one\nthread that's doing this, so it's fine.",
    "start": "805620",
    "end": "812100"
  },
  {
    "text": "And another thing to note\nis that the machine word size matters. So you need to\nwatch out for races",
    "start": "812100",
    "end": "818160"
  },
  {
    "text": "when you're reading and writing\nto packed data structures. So here's an example.",
    "start": "818160",
    "end": "824250"
  },
  {
    "text": "I have a struct x with\ntwo chars, a and b. And updating x.a and x.b\nmay possibly cause a race.",
    "start": "824250",
    "end": "834990"
  },
  {
    "text": "And this is a nasty\nrace, because it depends on the compiler\noptimization level.",
    "start": "834990",
    "end": "840790"
  },
  {
    "text": "Fortunately, this is safe\non the Intel machines that we're using in this class. You can't get a race\nin this example.",
    "start": "840790",
    "end": "846450"
  },
  {
    "text": "But there are\nother architectures that might have a race when\nyou're updating the two",
    "start": "846450",
    "end": "852779"
  },
  {
    "text": "variables a and b in this case. So with the Intel\nmachines that we're using, if you're using standard\ndata types like chars, shorts,",
    "start": "852780",
    "end": "860580"
  },
  {
    "text": "ints, and longs inside a\nstruct, you won't get races. But if you're using\nnon-standard types--",
    "start": "860580",
    "end": "867750"
  },
  {
    "text": "for example, you're using\nthe C bit fields facilities, and the sizes of the fields are\nnot one of the standard sizes,",
    "start": "867750",
    "end": "875220"
  },
  {
    "text": "then you could\npossibly get a race. In particular, if you're\nupdating individual bits",
    "start": "875220",
    "end": "882900"
  },
  {
    "text": "inside a word in parallel, then\nyou might see a race there. So you need to be careful.",
    "start": "882900",
    "end": "888510"
  },
  {
    "text": " Questions? ",
    "start": "888510",
    "end": "899510"
  },
  {
    "text": "So fortunately,\nthe Cilk platform has a very nice\ntool called the--",
    "start": "899510",
    "end": "904690"
  },
  {
    "text": "yes, question? AUDIENCE: [INAUDIBLE] was going\nto ask, what causes that race?",
    "start": "904690",
    "end": "909970"
  },
  {
    "text": "JULIAN SHUN: Because the\narchitecture might actually be updating this struct\nat the granularity of more",
    "start": "909970",
    "end": "918700"
  },
  {
    "text": "than 1 byte. So if you're updating single\nbytes inside this larger word,",
    "start": "918700",
    "end": "925440"
  },
  {
    "text": "then that might cause a race.  But fortunately, this doesn't\nhappen on Intel machines.",
    "start": "925440",
    "end": "932380"
  },
  {
    "text": " So the Cilksan race detector--",
    "start": "932380",
    "end": "938950"
  },
  {
    "text": "if you compile your\ncode using this flag, minus f sanitize\nequal to cilk, then",
    "start": "938950",
    "end": "945820"
  },
  {
    "start": "942000",
    "end": "942000"
  },
  {
    "text": "it's going to generate a\nCilksan instrumentive program. And then if an ostensibly\ndeterministic Cilk program",
    "start": "945820",
    "end": "953950"
  },
  {
    "text": "run on a given input could\npossibly behave any differently than its serial\nelision, then Cilksan",
    "start": "953950",
    "end": "960250"
  },
  {
    "text": "is going to guarantee\nto report and localize the offending race. So Cilksan is going to tell\nyou which memory location there",
    "start": "960250",
    "end": "968769"
  },
  {
    "text": "might be a race on and\nwhich of the instructions were involved in this race.",
    "start": "968770",
    "end": "975100"
  },
  {
    "text": "So Cilksan employs a\nregression test methodology where the programmer provides\nit different test inputs.",
    "start": "975100",
    "end": "980740"
  },
  {
    "text": "And for each test input,\nif there could possibly be a race in the program, then\nit will report these races.",
    "start": "980740",
    "end": "988630"
  },
  {
    "text": "And it identifies the\nfile names, the lines, the variables\ninvolved in the races,",
    "start": "988630",
    "end": "994779"
  },
  {
    "text": "including the stack traces. So it's very helpful when\nyou're trying to debug your code and find out where there's\na race in your program.",
    "start": "994780",
    "end": "1003930"
  },
  {
    "text": "One thing to note\nis that you should ensure that all of your\nprogram files are instrumented. Because if you only instrument\nsome of your files and not",
    "start": "1003930",
    "end": "1011220"
  },
  {
    "text": "the other ones, then\nyou'll possibly miss out on some of these race bugs. ",
    "start": "1011220",
    "end": "1018510"
  },
  {
    "text": "And one of the nice things\nabout the Cilksan race detector is that it's always going\nto report a race if there",
    "start": "1018510",
    "end": "1024420"
  },
  {
    "text": "is possibly a race, unlike many\nother race detectors, which are best efforts.",
    "start": "1024420",
    "end": "1029520"
  },
  {
    "text": "So they might report a\nrace some of the times when the race actually occurs,\nbut they don't necessarily",
    "start": "1029520",
    "end": "1034650"
  },
  {
    "text": "report a race all the time. Because in some executions,\nthe race doesn't occur. But the Cilksan race\ndetector is going",
    "start": "1034650",
    "end": "1040949"
  },
  {
    "text": "to always report the race,\nif there is potentially a race in there. ",
    "start": "1040950",
    "end": "1048520"
  },
  {
    "text": "Cilksan is your best friend. So use this when you're\ndebugging your homeworks",
    "start": "1048520",
    "end": "1053720"
  },
  {
    "text": "and projects. Here's an example of the output\nthat's generated by Cilksan.",
    "start": "1053720",
    "end": "1059900"
  },
  {
    "text": "So you can see that it's saying\nthat there's a race detected at this memory address here.",
    "start": "1059900",
    "end": "1066410"
  },
  {
    "text": "And the line of code\nthat caused this race is shown here, as\nwell as the file name.",
    "start": "1066410",
    "end": "1073940"
  },
  {
    "text": "So this is a matrix\nmultiplication example. And then it also tells you\nhow many races it detected.",
    "start": "1073940",
    "end": "1079110"
  },
  {
    "start": "1079110",
    "end": "1084540"
  },
  {
    "text": "So any questions on\ndeterminacy races? ",
    "start": "1084540",
    "end": "1096630"
  },
  {
    "text": "So let's now talk\nabout parallelism. So what is parallelism? Can we quantitatively\ndefine what parallelism is?",
    "start": "1096630",
    "end": "1105716"
  },
  {
    "text": "So what does it mean\nwhen somebody tells you that their code is\nhighly parallel?",
    "start": "1105717",
    "end": "1110900"
  },
  {
    "text": "So to have a formal\ndefinition of parallelism, we first need to look at\nthe Cilk execution model.",
    "start": "1110900",
    "end": "1118230"
  },
  {
    "start": "1117000",
    "end": "1117000"
  },
  {
    "text": "So this is a code that we\nsaw before for Fibonacci.",
    "start": "1118230",
    "end": "1123480"
  },
  {
    "text": "Let's now look at what a\ncall to fib of 4 looks like.",
    "start": "1123480",
    "end": "1129669"
  },
  {
    "text": "So here, I've color coded the\ndifferent lines of code here so that I can refer\nto them when I'm",
    "start": "1129670",
    "end": "1135750"
  },
  {
    "text": "drawing this computation graph. So now, I'm going to draw this\ncomputation graph corresponding",
    "start": "1135750",
    "end": "1141179"
  },
  {
    "text": "to how the computation\nunfolds during execution. So the first thing\nI'm going to do",
    "start": "1141180",
    "end": "1147210"
  },
  {
    "text": "is I'm going to call fib of 4. And that's going to\ngenerate this magenta node here corresponding to\nthe call to fib of 4,",
    "start": "1147210",
    "end": "1155070"
  },
  {
    "text": "and that's going to represent\nthis pink code here. ",
    "start": "1155070",
    "end": "1160740"
  },
  {
    "text": "And this illustration is similar\nto the computation graphs that you saw in the\nprevious lecture,",
    "start": "1160740",
    "end": "1167100"
  },
  {
    "text": "but this is happening\nin parallel. And I'm only labeling\nthe argument here,",
    "start": "1167100",
    "end": "1172300"
  },
  {
    "text": "but you could actually also\nwrite the local variables there. But I didn't do it, because\nI want to fit everything",
    "start": "1172300",
    "end": "1177990"
  },
  {
    "text": "on this slide.  So what happens when\nyou call fib of 4?",
    "start": "1177990",
    "end": "1184020"
  },
  {
    "text": "It's going to get to this\ncilk_spawn statement, and then it's going\nto call fib of 3.",
    "start": "1184020",
    "end": "1189360"
  },
  {
    "text": "And when I get to a cilk_spawn\nstatement, what I do is I'm going to create\nanother node that corresponds",
    "start": "1189360",
    "end": "1194700"
  },
  {
    "text": "to the child that I spawned. So this is this magenta\nnode here in this blue box.",
    "start": "1194700",
    "end": "1201840"
  },
  {
    "text": "And then I also\nhave a continue edge going to a green node that\nrepresents the computation",
    "start": "1201840",
    "end": "1207240"
  },
  {
    "text": "after the cilk_spawn statement. So this green node here\ncorresponds to the green line",
    "start": "1207240",
    "end": "1212400"
  },
  {
    "text": "of code in the code snippet. ",
    "start": "1212400",
    "end": "1218040"
  },
  {
    "text": "Now I can unfold this\ncomputation graph one more step. So we see that fib 3 is\ngoing to call fib of 2,",
    "start": "1218040",
    "end": "1225130"
  },
  {
    "text": "so I created another node here. And the green node\nhere, which corresponds to this green line\nof code-- it's",
    "start": "1225130",
    "end": "1232679"
  },
  {
    "text": "also going to make\na function call. It's going to call fib of 2. And that's also going\nto create a new node.",
    "start": "1232680",
    "end": "1240190"
  },
  {
    "text": "So in general,\nwhen I do a spawn, I'm going to have two outgoing\nedges out of a magenta node.",
    "start": "1240190",
    "end": "1247320"
  },
  {
    "text": "And when I do a call, I'm going\nto have one outgoing edge out of a green node. So this green node,\nthe outgoing edge",
    "start": "1247320",
    "end": "1253950"
  },
  {
    "text": "corresponds to a function call. And for this magenta node,\nits first outgoing edge",
    "start": "1253950",
    "end": "1259410"
  },
  {
    "text": "corresponds to spawn, and\nthen its second outgoing edge goes to the continuation strand.",
    "start": "1259410",
    "end": "1266790"
  },
  {
    "text": "So I can unfold\nthis one more time. And here, I see that I'm\ncreating some more spawns",
    "start": "1266790",
    "end": "1276090"
  },
  {
    "text": "and calls to fib. And if I do this\none more time, I've actually reached the base case.",
    "start": "1276090",
    "end": "1281370"
  },
  {
    "text": "Because once n is\nequal to 1 or 0, I'm not going to make\nany more recursive calls.",
    "start": "1281370",
    "end": "1288960"
  },
  {
    "text": "And by the way, the color of\nthese boxes that I used here correspond to whether\nI called that function",
    "start": "1288960",
    "end": "1295530"
  },
  {
    "text": "or whether I spawned it. So a box with white background\ncorresponds to a function that I called, whereas a\nbox with blue background",
    "start": "1295530",
    "end": "1303347"
  },
  {
    "text": "corresponds to a\nfunction that I spawned. ",
    "start": "1303347",
    "end": "1308630"
  },
  {
    "text": "So now I've gotten\nto the base case, I need to now execute\nthis blue statement, which",
    "start": "1308630",
    "end": "1315930"
  },
  {
    "text": "sums up x and y and returns the\nresult to the parent caller. ",
    "start": "1315930",
    "end": "1324070"
  },
  {
    "text": "So here I have a blue node. So this is going to take\nthe results of the two",
    "start": "1324070",
    "end": "1329920"
  },
  {
    "text": "recursive calls,\nsum them together. And I have another\nblue node here. And then it's going\nto pass its value",
    "start": "1329920",
    "end": "1336910"
  },
  {
    "text": "to the parent that called it. So I'm going to pass\nthis up to its parent,",
    "start": "1336910",
    "end": "1342880"
  },
  {
    "text": "and then I'm going to\npass this one up as well. And finally, I have a blue\nnode at the top level, which",
    "start": "1342880",
    "end": "1349480"
  },
  {
    "text": "is going to compute\nmy final result, and that's going to be\nthe output of the program. ",
    "start": "1349480",
    "end": "1356810"
  },
  {
    "text": "So one thing to note is\nthat this computation dag unfolds dynamically\nduring the execution.",
    "start": "1356810",
    "end": "1364240"
  },
  {
    "text": "So the runtime\nsystem isn't going to create this graph\nat the beginning. It's actually going to\ncreate this on the fly",
    "start": "1364240",
    "end": "1371570"
  },
  {
    "text": "as you run the program. So this graph here\nunfolds dynamically.",
    "start": "1371570",
    "end": "1378649"
  },
  {
    "text": "And also, this graph here\nis processor-oblivious. So nowhere in this\ncomputation dag",
    "start": "1378650",
    "end": "1383990"
  },
  {
    "text": "did I mention the\nnumber of processors I had for the computation. And similarly, in the\ncode here, I never",
    "start": "1383990",
    "end": "1390860"
  },
  {
    "text": "mentioned the number of\nprocessors that I'm using. So the runtime system\nis going to figure out how to map these tasks to\nthe number of processors",
    "start": "1390860",
    "end": "1398060"
  },
  {
    "text": "that you give to the computation\ndynamically at runtime. So for example, I can run this\non any number of processors.",
    "start": "1398060",
    "end": "1404390"
  },
  {
    "text": "If I run it on one\nprocessor, it's just going to execute\nthese tasks in parallel. In fact, it's going\nto execute them",
    "start": "1404390",
    "end": "1410240"
  },
  {
    "text": "in a depth-first order,\nwhich corresponds to the what the sequential\nalgorithm would do.",
    "start": "1410240",
    "end": "1415610"
  },
  {
    "text": "So I'm going to start with fib\nof 4, go to fib of 3, fib of 2, fib of 1, and go pop back\nup and then do fib of 0",
    "start": "1415610",
    "end": "1423679"
  },
  {
    "text": "and go back up and so on. So if I use one\nprocessor, it's going",
    "start": "1423680",
    "end": "1429200"
  },
  {
    "text": "to create and execute\nthis computation dag in the depth-first manner. And if I have more\nthan one processor,",
    "start": "1429200",
    "end": "1435765"
  },
  {
    "text": "it's not necessarily going to\nfollow a depth-first order, because I could have multiple\ncomputations going on. ",
    "start": "1435765",
    "end": "1445640"
  },
  {
    "text": "Any questions on this example? I'm actually going to\nformally define some terms",
    "start": "1445640",
    "end": "1450919"
  },
  {
    "text": "on the next slide so that\nwe can formalize the notion of a computation dag.",
    "start": "1450920",
    "end": "1457340"
  },
  {
    "start": "1456000",
    "end": "1456000"
  },
  {
    "text": "So dag stands for\ndirected acyclic graph, and this is a directed\nacyclic graph. So we call it a computation dag.",
    "start": "1457340",
    "end": "1464780"
  },
  {
    "text": "So a parallel\ninstruction stream is a dag G with vertices\nV and edges E.",
    "start": "1464780",
    "end": "1471830"
  },
  {
    "text": "And each vertex in this dag\ncorresponds to a strand. And a strand is a\nsequence of instructions",
    "start": "1471830",
    "end": "1478940"
  },
  {
    "text": "not containing a spawn, a\nsync, or a return from a spawn. So the instructions\ninside a strand",
    "start": "1478940",
    "end": "1484910"
  },
  {
    "text": "are executed sequentially. There's no parallelism\nwithin a strand. We call the first strand\nthe initial strand,",
    "start": "1484910",
    "end": "1492830"
  },
  {
    "text": "so this is the\nmagenta node up here. The last strand-- we\ncall it the final strand.",
    "start": "1492830",
    "end": "1498110"
  },
  {
    "text": "And then everything else,\nwe just call it a strand. And then there are\nfour types of edges.",
    "start": "1498110",
    "end": "1505010"
  },
  {
    "text": "So there are spawn edges,\ncall edges, return edges, or continue edges. And a spawn edge corresponds\nto an edge to a function",
    "start": "1505010",
    "end": "1514460"
  },
  {
    "text": "that you spawned. So these spawn edges are\ngoing to go to a magenta node.",
    "start": "1514460",
    "end": "1522670"
  },
  {
    "text": "A call edge corresponds to an\nedge that goes to a function that you called. So in this example, these are\ncoming out of the green nodes",
    "start": "1522670",
    "end": "1530659"
  },
  {
    "text": "and going to a magenta node. A return edge corresponds\nto an edge going back up",
    "start": "1530660",
    "end": "1538520"
  },
  {
    "text": "to the parent caller. So here, it's going into\none of these blue nodes.",
    "start": "1538520",
    "end": "1544970"
  },
  {
    "text": "And then finally, a continue\nedge is just the other edge when you spawn a function.",
    "start": "1544970",
    "end": "1550140"
  },
  {
    "text": "So this is the edge that\ngoes to the green node. It's representing\nthe computation after you spawn something.",
    "start": "1550140",
    "end": "1556793"
  },
  {
    "text": " And notice that in\nthis computation dag,",
    "start": "1556793",
    "end": "1563420"
  },
  {
    "text": "we never explicitly\nrepresented cilk_for, because as I said\nbefore, cilk_fors are converted to\nnested cilk_spawns",
    "start": "1563420",
    "end": "1571370"
  },
  {
    "text": "and cilk_sync statements. So we don't actually need to\nexplicitly represent cilk_fors in the computation DAG.",
    "start": "1571370",
    "end": "1576920"
  },
  {
    "text": " Any questions on\nthis definition?",
    "start": "1576920",
    "end": "1582638"
  },
  {
    "text": "So we're going to be\nusing this computation dag throughout this lecture to\nanalyze how much parallelism there is in a program.",
    "start": "1582638",
    "end": "1588775"
  },
  {
    "start": "1588775",
    "end": "1599070"
  },
  {
    "start": "1595000",
    "end": "1595000"
  },
  {
    "text": "So assuming that each of these\nstrands executes in unit time--",
    "start": "1599070",
    "end": "1604462"
  },
  {
    "text": "this assumption isn't\nalways true in practice. In practice, strands will take\ndifferent amounts of time. But let's assume,\nfor simplicity,",
    "start": "1604463",
    "end": "1610470"
  },
  {
    "text": "that each strand\nhere takes unit time. Does anyone want to guess\nwhat the parallelism",
    "start": "1610470",
    "end": "1615960"
  },
  {
    "text": "of this computation is? ",
    "start": "1615960",
    "end": "1624100"
  },
  {
    "text": "So how parallel do\nyou think this is? What's the maximum speedup you\nmight get on this computation?",
    "start": "1624100",
    "end": "1629760"
  },
  {
    "text": "AUDIENCE: 5. JULIAN SHUN: 5. Somebody said 5. Any other guesses?",
    "start": "1629760",
    "end": "1634920"
  },
  {
    "text": "Who thinks this is going\nto be less than five? ",
    "start": "1634920",
    "end": "1640490"
  },
  {
    "text": "A couple people. Who thinks it's going\nto be more than five? ",
    "start": "1640490",
    "end": "1646478"
  },
  {
    "text": "A couple of people. Who thinks there's\nany parallelism at all in this computation?",
    "start": "1646478",
    "end": "1651485"
  },
  {
    "text": " Yeah, seems like a lot of people\nthink there is some parallelism",
    "start": "1651485",
    "end": "1659190"
  },
  {
    "text": "here. So we're actually going to\nanalyze how much parallelism is in this computation. So I'm not going to\ntell you the answer now,",
    "start": "1659190",
    "end": "1665730"
  },
  {
    "text": "but I'll tell you in\na couple of slides. First need to go over\nsome terminology.",
    "start": "1665730",
    "end": "1673170"
  },
  {
    "start": "1671000",
    "end": "1671000"
  },
  {
    "text": "So whenever you start\ntalking about parallelism, somebody is almost always\ngoing to bring up Amdahl's Law.",
    "start": "1673170",
    "end": "1680250"
  },
  {
    "text": "And Amdahl's Law says that\nif 50% of your application is parallel and the\nother 50% is serial,",
    "start": "1680250",
    "end": "1688410"
  },
  {
    "text": "then you can't get more\nthan a factor of 2 speedup, no matter how many processors\nyou run the computation on.",
    "start": "1688410",
    "end": "1696600"
  },
  {
    "text": "Does anyone know why\nthis is the case? ",
    "start": "1696600",
    "end": "1702320"
  },
  {
    "text": "Yes? AUDIENCE: Because you need it\nto execute for at least 50% of the time in order to get\nthrough the serial portion.",
    "start": "1702320",
    "end": "1707870"
  },
  {
    "text": "JULIAN SHUN: Right. So you have to\nspend at least 50% of the time in the\nserial portion.",
    "start": "1707870",
    "end": "1713000"
  },
  {
    "text": "So in the best\ncase, if I gave you an infinite number\nof processors, and you can reduce the\nparallel portion of your code",
    "start": "1713000",
    "end": "1720559"
  },
  {
    "text": "to 0 running time, you still\nhave the 50% of the serial time that you have to execute. And therefore, the best speedup\nyou can get is a factor of 2.",
    "start": "1720560",
    "end": "1731390"
  },
  {
    "text": "And in general, if a fraction\nalpha of an application must be run serially, then\nthe speedup can be at most 1",
    "start": "1731390",
    "end": "1739130"
  },
  {
    "text": "over alpha. So if 1/3 of your program has\nto be executed sequentially,",
    "start": "1739130",
    "end": "1744500"
  },
  {
    "text": "then the speedup\ncan be, at most, 3. Because even if you reduce the\nparallel portion of your code",
    "start": "1744500",
    "end": "1750799"
  },
  {
    "text": "to tab a running\ntime of 0, you still have the sequential part of your\ncode that you have to wait for.",
    "start": "1750800",
    "end": "1756320"
  },
  {
    "start": "1756320",
    "end": "1761380"
  },
  {
    "start": "1760000",
    "end": "1760000"
  },
  {
    "text": "So let's try to quantify the\nparallelism in this computation here.",
    "start": "1761380",
    "end": "1766600"
  },
  {
    "text": "So how many of these nodes have\nto be executed sequentially? ",
    "start": "1766600",
    "end": "1780710"
  },
  {
    "text": "Yes? AUDIENCE: 9 of them. JULIAN SHUN: So it turns\nout to be less than 9.",
    "start": "1780710",
    "end": "1786140"
  },
  {
    "start": "1786140",
    "end": "1793288"
  },
  {
    "text": "Yes? AUDIENCE: 7. JULIAN SHUN: 7. It turns out to be less than 7. ",
    "start": "1793288",
    "end": "1802472"
  },
  {
    "text": "Yes? AUDIENCE: 6. JULIAN SHUN: So it turns\nout to be less than 6. ",
    "start": "1802472",
    "end": "1809407"
  },
  {
    "text": "AUDIENCE: 4. JULIAN SHUN: Turns\nout to be less than 4. You're getting close.",
    "start": "1809407",
    "end": "1814750"
  },
  {
    "text": "AUDIENCE: 2. JULIAN SHUN: 2. So turns out to be more than 2. ",
    "start": "1814750",
    "end": "1824762"
  },
  {
    "text": "AUDIENCE: 2.5. JULIAN SHUN: What's left? AUDIENCE: 3. JULIAN SHUN: 3. OK. ",
    "start": "1824762",
    "end": "1831960"
  },
  {
    "text": "So 3 of these nodes have to\nbe executed sequentially. Because when you're\nexecuting these nodes,",
    "start": "1831960",
    "end": "1838330"
  },
  {
    "text": "there's nothing else that\ncan happen in parallel. For all of the remaining nodes,\nwhen you're executing them,",
    "start": "1838330",
    "end": "1843899"
  },
  {
    "text": "you can potentially\nbe executing some of the other nodes in parallel. But for these three nodes\nthat I've colored in yellow,",
    "start": "1843900",
    "end": "1852060"
  },
  {
    "text": "you have to execute\nthose sequentially, because there's nothing else\nthat's going on in parallel.",
    "start": "1852060",
    "end": "1857940"
  },
  {
    "text": "So according to\nAmdahl's Law, this says that the serial fraction\nof the program is 3 over 18.",
    "start": "1857940",
    "end": "1864910"
  },
  {
    "text": "So there's 18 nodes\nin this graph here. So therefore, the serial\nfactor is 1 over 6,",
    "start": "1864910",
    "end": "1871890"
  },
  {
    "text": "and the speedup is upper bound\nby 1 over that, which is 6.",
    "start": "1871890",
    "end": "1877170"
  },
  {
    "text": "So Amdahl's Law tells us that\nthe maximum speedup we can get is 6.",
    "start": "1877170",
    "end": "1883470"
  },
  {
    "text": "Any questions on how I\ngot this number here? ",
    "start": "1883470",
    "end": "1891450"
  },
  {
    "text": "So it turns out that Amdahl's\nLaw actually gives us a pretty loose upper\nbound on the parallelism,",
    "start": "1891450",
    "end": "1898190"
  },
  {
    "text": "and it's not that useful\nin many practical cases. So we're actually going\nto look at a better definition of parallelism\nthat will give us",
    "start": "1898190",
    "end": "1905270"
  },
  {
    "text": "a better upper bound on the\nmaximum speedup we can get. ",
    "start": "1905270",
    "end": "1912059"
  },
  {
    "start": "1910000",
    "end": "1910000"
  },
  {
    "text": "So we're going to define T\nsub P to be the execution time of the program on P processors.",
    "start": "1912060",
    "end": "1919770"
  },
  {
    "text": "And T sub 1 is just the work. So T sub 1 is if you executed\nthis program on one processor,",
    "start": "1919770",
    "end": "1925910"
  },
  {
    "text": "how much stuff do\nyou have to do? And we define that\nto be the work. Recall in lecture 2,\nwe looked at many ways",
    "start": "1925910",
    "end": "1932690"
  },
  {
    "text": "to optimize the work. This is the work term. ",
    "start": "1932690",
    "end": "1940450"
  },
  {
    "text": "So in this example,\nthe number of nodes here is 18, so the work\nis just going to be 18.",
    "start": "1940450",
    "end": "1946635"
  },
  {
    "text": " We also define T of\ninfinity to be the span.",
    "start": "1946635",
    "end": "1955049"
  },
  {
    "text": "The span is also called\nthe critical path length, or the computational\ndepth, of the graph.",
    "start": "1955050",
    "end": "1961020"
  },
  {
    "text": "And this is equal to the\nlongest directed path you can find in this graph.",
    "start": "1961020",
    "end": "1968750"
  },
  {
    "text": "So in this example,\nthe longest path is 9. So one of the students\nanswered 9 earlier,",
    "start": "1968750",
    "end": "1974179"
  },
  {
    "text": "and this is actually\nthe span of this graph. So there are 9 nodes\nalong this path here,",
    "start": "1974180",
    "end": "1981228"
  },
  {
    "text": "and that's the longest\none you can find. ",
    "start": "1981228",
    "end": "1988790"
  },
  {
    "text": "And we call this T of infinity\nbecause that's actually the execution time\nof this program",
    "start": "1988790",
    "end": "1994700"
  },
  {
    "text": "if you had an infinite\nnumber of processors. So there are two\nlaws that are going",
    "start": "1994700",
    "end": "2000370"
  },
  {
    "text": "to relate these quantities. So the work law\nsays that T sub P",
    "start": "2000370",
    "end": "2006519"
  },
  {
    "text": "is greater than or equal\nto T sub 1 divided by P. So this says that the\nexecution time on P processors",
    "start": "2006520",
    "end": "2013480"
  },
  {
    "text": "has to be greater than\nor equal to the work of the program divided by the\nnumber of processors you have.",
    "start": "2013480",
    "end": "2020020"
  },
  {
    "text": "Does anyone see why\nthe work law is true? So the answer is that if you\nhave P processors, on each time",
    "start": "2020020",
    "end": "2027279"
  },
  {
    "text": "stub, you can do,\nat most, P work. So if you multiply\nboth sides by P,",
    "start": "2027280",
    "end": "2033020"
  },
  {
    "text": "you get P times T sub P is\ngreater than or equal to T1. If P times T sub P\nwas less than T1, then",
    "start": "2033020",
    "end": "2040780"
  },
  {
    "text": "that means you're not\ndone with the computation, because you haven't\ndone all the work yet. So the work law\nsays that T sub P",
    "start": "2040780",
    "end": "2047559"
  },
  {
    "text": "has to be greater than\nor equal to T1 over P. Any questions on the work law?",
    "start": "2047560",
    "end": "2053770"
  },
  {
    "text": " So let's look at another law. This is called the span law.",
    "start": "2053770",
    "end": "2060350"
  },
  {
    "text": "It says that T sub P has to be\ngreater than or equal to T sub infinity.",
    "start": "2060350",
    "end": "2065449"
  },
  {
    "text": "So the execution\ntime on P processors has to be at least execution\ntime on an infinite number",
    "start": "2065449",
    "end": "2071119"
  },
  {
    "text": "of processors. Anyone know why the\nspan law has to be true?",
    "start": "2071120",
    "end": "2076780"
  },
  {
    "text": "So another way to see\nthis is that if you had an infinite\nnumber of processors, you can actually simulate\na P processor system.",
    "start": "2076780",
    "end": "2083799"
  },
  {
    "text": "You just use P of the\nprocessors and leave all the remaining processors idle. And that can't slow\ndown your program.",
    "start": "2083800",
    "end": "2091000"
  },
  {
    "text": "So therefore, you\nhave that T sub P has to be greater than or\nequal to T sub infinity.",
    "start": "2091000",
    "end": "2096940"
  },
  {
    "text": "If you add more\nprocessors to it, the running time can't go up. ",
    "start": "2096940",
    "end": "2103569"
  },
  {
    "text": "Any questions? ",
    "start": "2103570",
    "end": "2109756"
  },
  {
    "text": "So let's see how we\ncan compose the work and the span quantities\nof different computations.",
    "start": "2109756",
    "end": "2114890"
  },
  {
    "start": "2111000",
    "end": "2111000"
  },
  {
    "text": "So let's say I have two\ncomputations, A and B. And let's say that A\nhas to execute before B.",
    "start": "2114890",
    "end": "2122780"
  },
  {
    "text": "So everything in\nA has to be done before I start the\ncomputation in B. Let's say",
    "start": "2122780",
    "end": "2128120"
  },
  {
    "text": "I know what the work of A and\nthe work of B individually are. What would be the\nwork of A union B?",
    "start": "2128120",
    "end": "2135440"
  },
  {
    "start": "2135440",
    "end": "2144720"
  },
  {
    "text": "Yes? AUDIENCE: I guess it\nwould be T1 A plus T1 B. JULIAN SHUN: Yeah.",
    "start": "2144720",
    "end": "2150230"
  },
  {
    "text": "So why is that? AUDIENCE: Well, you have\nto execute sequentially. So then you just take the time\nand [INAUDIBLE] execute A,",
    "start": "2150230",
    "end": "2157866"
  },
  {
    "text": "then it'll execute B after that. JULIAN SHUN: Yeah. So the work is just going to\nbe the sum of the work of A",
    "start": "2157866",
    "end": "2163460"
  },
  {
    "text": "and the work of B. Because you\nhave to do all of the work of A and then do all\nof the work of B,",
    "start": "2163460",
    "end": "2169280"
  },
  {
    "text": "so you just add them together. What about the span? So let's say I\nknow the span of A",
    "start": "2169280",
    "end": "2175720"
  },
  {
    "text": "and I know the span of B.\nWhat's the span of A union B? So again, it's just a sum of\nthe span of A and the span of B.",
    "start": "2175720",
    "end": "2185230"
  },
  {
    "text": "This is because I have\nto execute everything in A before I start B. So I\njust sum together the spans.",
    "start": "2185230",
    "end": "2193840"
  },
  {
    "text": "So this is series composition. What if I do\nparallel composition? So let's say here,\nI'm executing the two",
    "start": "2193840",
    "end": "2201070"
  },
  {
    "start": "2196000",
    "end": "2196000"
  },
  {
    "text": "computations in parallel. What's the work of A union B?",
    "start": "2201070",
    "end": "2206620"
  },
  {
    "start": "2206620",
    "end": "2214305"
  },
  {
    "text": "So it's not it's not\ngoing to be the maximum.  Yes?",
    "start": "2214305",
    "end": "2219670"
  },
  {
    "text": "AUDIENCE: It should still\nbe T1 of A plus T1 of B. JULIAN SHUN: Yeah,\nso it's still going to be the sum of T1\nof A and T1 of B.",
    "start": "2219670",
    "end": "2226640"
  },
  {
    "text": "Because you still have\nthe same amount of work that you have to do. It's just that you're\ndoing it in parallel.",
    "start": "2226640",
    "end": "2233120"
  },
  {
    "text": "But the work is just the time\nif you had one processor. So if you had one\nprocessor, you wouldn't",
    "start": "2233120",
    "end": "2238370"
  },
  {
    "text": "be executing these in parallel. What about the span? So if I know the span\nof A and the span of B,",
    "start": "2238370",
    "end": "2244040"
  },
  {
    "text": "what's the span of the parallel\ncomposition of the two? ",
    "start": "2244040",
    "end": "2254310"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] JULIAN SHUN: Yeah, so\nthe span of A union B",
    "start": "2254310",
    "end": "2261410"
  },
  {
    "text": "is going to be the max of the\nspan of A and the span of B, because I'm going\nto be bottlenecked",
    "start": "2261410",
    "end": "2267590"
  },
  {
    "text": "by the slower of the\ntwo computations. So I just take the one\nthat has longer span,",
    "start": "2267590",
    "end": "2272960"
  },
  {
    "text": "and that gives me\nthe overall span.  Any questions?",
    "start": "2272960",
    "end": "2279340"
  },
  {
    "start": "2279340",
    "end": "2285150"
  },
  {
    "start": "2284000",
    "end": "2284000"
  },
  {
    "text": "So here's another definition. So T1 divided by TP is the\nspeedup on P processors.",
    "start": "2285150",
    "end": "2294190"
  },
  {
    "text": "If I have T1 divided\nby TP less than P, then this means that I have\nsub-linear speedup.",
    "start": "2294190",
    "end": "2300010"
  },
  {
    "text": "I'm not making use of\nall the processors. Because I'm using P\nprocessors, but I'm not getting a speedup of P.",
    "start": "2300010",
    "end": "2307650"
  },
  {
    "text": "If T1 over TP is\nequal to P, then I'm getting perfect linear speedup.",
    "start": "2307650",
    "end": "2312820"
  },
  {
    "text": "I'm making use of\nall of my processors. I'm putting P times as many\nresources into my computation,",
    "start": "2312820",
    "end": "2318880"
  },
  {
    "text": "and it becomes P times faster. So this is the good case. And finally, if T1 over\nTP is greater than P,",
    "start": "2318880",
    "end": "2326680"
  },
  {
    "text": "we have something called\nsuperlinear speedup. In our simple\nperformance model, this can't actually happen,\nbecause of the work law.",
    "start": "2326680",
    "end": "2333799"
  },
  {
    "text": "The work law says that TP has\nto be at least T1 divided by P.",
    "start": "2333800",
    "end": "2338848"
  },
  {
    "text": "So if you rearrange\nthe terms, you'll see that we get a\ncontradiction in our model. In practice, you might sometimes\nsee that you have a superlinear",
    "start": "2338848",
    "end": "2347140"
  },
  {
    "text": "speedup, because when you're\nusing more processors, you might have\naccess to more cache,",
    "start": "2347140",
    "end": "2352569"
  },
  {
    "text": "and that could improve the\nperformance of your program. But in general, you might see\na little bit of superlinear",
    "start": "2352570",
    "end": "2358329"
  },
  {
    "text": "speedup, but not that much. And in our simplified\nmodel, we're just going to assume that\nyou can't have a superlinear",
    "start": "2358330",
    "end": "2364880"
  },
  {
    "text": "speedup. And getting perfect linear\nspeedup is already very good. ",
    "start": "2364880",
    "end": "2374220"
  },
  {
    "text": "So because the span law says\nthat TP has to be at least T",
    "start": "2374220",
    "end": "2380010"
  },
  {
    "text": "infinity, the maximum\npossible speedup is just going to be T1\ndivided by T infinity,",
    "start": "2380010",
    "end": "2385830"
  },
  {
    "text": "and that's the parallelism\nof your computation. This is a maximum possible\nspeedup you can get.",
    "start": "2385830",
    "end": "2392610"
  },
  {
    "text": "Another way to view\nthis is that it's equal to the average\namount of work",
    "start": "2392610",
    "end": "2398100"
  },
  {
    "text": "that you have to do per\nstep along the span. So for every step\nalong the span,",
    "start": "2398100",
    "end": "2403980"
  },
  {
    "text": "you're doing this much work. And after all the steps, then\nyou've done all of the work. ",
    "start": "2403980",
    "end": "2411500"
  },
  {
    "text": "So what's the parallelism of\nthis computation dag here? ",
    "start": "2411500",
    "end": "2425807"
  },
  {
    "text": "AUDIENCE: 2. JULIAN SHUN: 2. Why is it 2? AUDIENCE: T1 is 18\nand T infinity is 9.",
    "start": "2425807",
    "end": "2431560"
  },
  {
    "text": "JULIAN SHUN: Yeah. So T1 is 18. There are 18 nodes\nin this graph. T infinity is 9.",
    "start": "2431560",
    "end": "2438780"
  },
  {
    "text": "And the last time I checked,\n18 divided by 9 is 2. So the parallelism here is 2.",
    "start": "2438780",
    "end": "2445000"
  },
  {
    "text": " So now we can go back to\nour Fibonacci example,",
    "start": "2445000",
    "end": "2451130"
  },
  {
    "start": "2448000",
    "end": "2448000"
  },
  {
    "text": "and we can also analyze the\nwork and the span of this and compute the\nmaximum parallelism.",
    "start": "2451130",
    "end": "2458730"
  },
  {
    "text": "So again, for\nsimplicity, let's assume that each of these strands\ntakes unit time to execute.",
    "start": "2458730",
    "end": "2463800"
  },
  {
    "text": "Again, in practice, that's\nnot necessarily true. But for simplicity,\nlet's just assume that.",
    "start": "2463800",
    "end": "2470570"
  },
  {
    "text": "So what's the work\nof this computation? ",
    "start": "2470570",
    "end": "2480282"
  },
  {
    "text": "AUDIENCE: 17. JULIAN SHUN: 17. Right. So the work is just\nthe number of nodes",
    "start": "2480282",
    "end": "2486510"
  },
  {
    "text": "you have in this graph. And you can just count\nthat up, and you get 17.",
    "start": "2486510",
    "end": "2491580"
  },
  {
    "text": "What about the span? ",
    "start": "2491580",
    "end": "2497150"
  },
  {
    "text": "Somebody said 8. Yeah, so the span is 8. And here's the longest path.",
    "start": "2497150",
    "end": "2504950"
  },
  {
    "text": "So this is the path\nthat has 8 nodes in it, and that's the longest\none you can find here.",
    "start": "2504950",
    "end": "2510570"
  },
  {
    "text": "So therefore, the\nparallelism is just 17 divided by 8, which is 2.125.",
    "start": "2510570",
    "end": "2518300"
  },
  {
    "text": "And so for all of you who\nguessed that the parallelism was 2, you were very close.",
    "start": "2518300",
    "end": "2524900"
  },
  {
    "text": "This tells us that using\nmany more than two processors can only yield us marginal\nperformance gains.",
    "start": "2524900",
    "end": "2532490"
  },
  {
    "text": "Because the maximum speedup\nwe can get is 2.125. So we throw eight processors\nat this computation,",
    "start": "2532490",
    "end": "2538370"
  },
  {
    "text": "we're not going to get\na speedup beyond 2.125.",
    "start": "2538370",
    "end": "2547530"
  },
  {
    "text": "So to figure out\nhow much parallelism is in your computation,\nyou need to analyze",
    "start": "2547530",
    "end": "2553079"
  },
  {
    "text": "the work of your computation\nand the span of your computation and then take the ratio\nbetween the two quantities.",
    "start": "2553080",
    "end": "2559820"
  },
  {
    "text": "But for large computations,\nit's actually pretty tedious to analyze this by hand. You don't want to\ndraw these things out",
    "start": "2559820",
    "end": "2565590"
  },
  {
    "text": "by hand for a very\nlarge computation. And fortunately, Cilk has\na tool called the Cilkscale",
    "start": "2565590",
    "end": "2571440"
  },
  {
    "text": "Scalability Analyzer. So this is integrated into\nthe Tapir/LLVM compiler",
    "start": "2571440",
    "end": "2577140"
  },
  {
    "start": "2574000",
    "end": "2574000"
  },
  {
    "text": "that you'll be using\nfor this course. And Cilkscale uses\ncompiler instrumentation",
    "start": "2577140",
    "end": "2584670"
  },
  {
    "text": "to analyze a serial\nexecution of a program, and it's going to generate the\nwork and the span quantities",
    "start": "2584670",
    "end": "2590010"
  },
  {
    "text": "and then use those\nquantities to derive upper bounds on the parallel\nspeedup of your program.",
    "start": "2590010",
    "end": "2596737"
  },
  {
    "text": "So you'll have a\nchance to play around with Cilkscale in homework 4. ",
    "start": "2596737",
    "end": "2603640"
  },
  {
    "start": "2602000",
    "end": "2602000"
  },
  {
    "text": "So let's try to analyze the\nparallelism of quicksort.",
    "start": "2603640",
    "end": "2608799"
  },
  {
    "text": "And here, we're using a\nparallel quicksort algorithm. The function quicksort\nhere takes two inputs.",
    "start": "2608800",
    "end": "2615670"
  },
  {
    "text": "These are two pointers. Left points to the beginning of\nthe array that we want to sort.",
    "start": "2615670",
    "end": "2620750"
  },
  {
    "text": "Right points to one element\nafter the end of the array. And what we do is we first\ncheck if left is equal to right.",
    "start": "2620750",
    "end": "2630880"
  },
  {
    "text": "If so, then we just return,\nbecause there are no elements to sort. Otherwise, we're going to\ncall this partition function.",
    "start": "2630880",
    "end": "2637750"
  },
  {
    "text": "The partition function is\ngoing to pick a random pivot-- so this is a randomized\nquicksort algorithm--",
    "start": "2637750",
    "end": "2644830"
  },
  {
    "text": "and then it's going to\nmove everything that's less than the pivot to\nthe left part of the array",
    "start": "2644830",
    "end": "2651190"
  },
  {
    "text": "and everything\nthat's greater than or equal to the pivot to\nthe right part of the array.",
    "start": "2651190",
    "end": "2656369"
  },
  {
    "text": "It's also going to return\nus a pointer to the pivot. And then now we can execute\ntwo recursive calls.",
    "start": "2656370",
    "end": "2662890"
  },
  {
    "text": "So we do quicksort on the\nleft side and quicksort on the right side. And this can happen in parallel.",
    "start": "2662890",
    "end": "2668450"
  },
  {
    "text": "So we use the cilk_spawn here\nto spawn off one of these calls to quicksort in parallel. And therefore, the two\nrecursive calls are parallel.",
    "start": "2668450",
    "end": "2676030"
  },
  {
    "text": "And then finally,\nwe sync up before we return from the function. ",
    "start": "2676030",
    "end": "2684640"
  },
  {
    "text": "So let's say we wanted\nto sort 1 million numbers with this quicksort algorithm.",
    "start": "2684640",
    "end": "2691600"
  },
  {
    "text": "And let's also assume that\nthe partition function here is written sequentially,\nso you have",
    "start": "2691600",
    "end": "2696910"
  },
  {
    "text": "to go through all of the\nelements, one by one. Can anyone guess\nwhat the parallelism is in this computation?",
    "start": "2696910",
    "end": "2705406"
  },
  {
    "text": "AUDIENCE: 1 million. JULIAN SHUN: So the\nguess was 1 million.",
    "start": "2705406",
    "end": "2710589"
  },
  {
    "text": "Any other guesses? ",
    "start": "2710590",
    "end": "2719468"
  },
  {
    "text": "AUDIENCE: 50,000. JULIAN SHUN: 50,000. Any other guesses?",
    "start": "2719468",
    "end": "2724970"
  },
  {
    "text": "Yes? AUDIENCE: 2. JULIAN SHUN: 2. It's a good guess.",
    "start": "2724970",
    "end": "2731255"
  },
  {
    "text": "AUDIENCE: Log 2 of a million. JULIAN SHUN: Log\nbase 2 of a million. ",
    "start": "2731255",
    "end": "2737500"
  },
  {
    "text": "Any other guesses? So log base 2 of a million,\n2, 50,000, and 1 million.",
    "start": "2737500",
    "end": "2745270"
  },
  {
    "text": "Anyone think it's\nmore than 1 million? No. So no takers on\nmore than 1 million.",
    "start": "2745270",
    "end": "2751000"
  },
  {
    "text": " So if you run this\nprogram using Cilkscale,",
    "start": "2751000",
    "end": "2757819"
  },
  {
    "start": "2753000",
    "end": "2753000"
  },
  {
    "text": "it will generate a plot\nthat looks like this. And there are several\nlines on this plot.",
    "start": "2757820",
    "end": "2763260"
  },
  {
    "text": "So let's talk about what\neach of these lines mean. So this purple line\nhere is the speedup",
    "start": "2763260",
    "end": "2771470"
  },
  {
    "text": "that you observe\nin your computation when you're running it. And you can get that by\ntaking the single processor",
    "start": "2771470",
    "end": "2778910"
  },
  {
    "text": "running time and dividing\nit by the running time on P processors. So this is the observed speedup.",
    "start": "2778910",
    "end": "2784160"
  },
  {
    "text": "That's the purple line. The blue line here is the line\nthat you get from the span law.",
    "start": "2784160",
    "end": "2792860"
  },
  {
    "text": "So this is T1 over T infinity. And here, this gives us a bound\nof about 6 for the parallelism.",
    "start": "2792860",
    "end": "2801950"
  },
  {
    "text": "The green line is the\nbound from the work law. So this is just a linear\nline with a slope of 1.",
    "start": "2801950",
    "end": "2810800"
  },
  {
    "text": "It says that on\nP processors, you can't get more than a\nfactor of P speedup. So therefore, the maximum\nspeedup you can get",
    "start": "2810800",
    "end": "2818450"
  },
  {
    "text": "has to be below the green\nline and below the blue line. So you're in this lower\nright quadrant of the plot.",
    "start": "2818450",
    "end": "2827780"
  },
  {
    "text": "There's also this\norange line, which is the speedup you would get\nif you used a greedy scheduler.",
    "start": "2827780",
    "end": "2832910"
  },
  {
    "text": "We'll talk more about\nthe greedy scheduler later on in this lecture.",
    "start": "2832910",
    "end": "2838140"
  },
  {
    "text": "So this is the plot\nthat you would get. And we see here that the\nmaximum speedup is about 5.",
    "start": "2838140",
    "end": "2847190"
  },
  {
    "text": "So for those of you who guessed\n2 and log base 2 of a million, you were the closest. ",
    "start": "2847190",
    "end": "2855500"
  },
  {
    "text": "You can also\ngenerate a plot that just tells you the execution\ntime versus the number",
    "start": "2855500",
    "end": "2860630"
  },
  {
    "text": "of processors. And you can get\nthis quite easily just by doing a\nsimple transformation",
    "start": "2860630",
    "end": "2867260"
  },
  {
    "text": "from the previous plot. So Cilkscale is going to give\nyou these useful plots that you",
    "start": "2867260",
    "end": "2872750"
  },
  {
    "text": "can use to figure out how much\nparallelism is in your program.",
    "start": "2872750",
    "end": "2878090"
  },
  {
    "start": "2876000",
    "end": "2876000"
  },
  {
    "text": "And let's see why the\nparallelism here is so low.",
    "start": "2878090",
    "end": "2886130"
  },
  {
    "text": "So I said that we were going\nto execute this partition function sequentially,\nand it turns out",
    "start": "2886130",
    "end": "2891980"
  },
  {
    "text": "that that's actually the\nbottleneck to the parallelism. ",
    "start": "2891980",
    "end": "2898610"
  },
  {
    "text": "So the expected work of\nquicksort is order n log n. So some of you\nmight have seen this",
    "start": "2898610",
    "end": "2904580"
  },
  {
    "text": "in your previous\nalgorithms courses. If you haven't seen\nthis yet, then you can take a look at your\nfavorite textbook, Introduction",
    "start": "2904580",
    "end": "2911539"
  },
  {
    "text": "to Algorithms. It turns out that the\nparallel version of quicksort",
    "start": "2911540",
    "end": "2917240"
  },
  {
    "text": "also has an expected work\nbound of order n log n, if you pick a random pivot. So the analysis is similar.",
    "start": "2917240",
    "end": "2923119"
  },
  {
    "text": " The expected span bound\nturns out to be at least n.",
    "start": "2923120",
    "end": "2930530"
  },
  {
    "text": "And this is because on the\nfirst level of recursion, we have to call this\npartition function, which",
    "start": "2930530",
    "end": "2936049"
  },
  {
    "text": "is going to go through\nthe elements one by one. So that already\nhas a linear span.",
    "start": "2936050",
    "end": "2941579"
  },
  {
    "text": "And it turns out that the\noverall span is also order n, because the span\nactually works out",
    "start": "2941580",
    "end": "2947690"
  },
  {
    "text": "to be a geometrically decreasing\nsequence and sums to order n.",
    "start": "2947690",
    "end": "2953980"
  },
  {
    "text": "And therefore, the maximum\nparallelism you can get is order log n.",
    "start": "2953980",
    "end": "2959210"
  },
  {
    "text": "So you just take the\nwork divided by the span. So for the student who guessed\nthat the parallelism is log",
    "start": "2959210",
    "end": "2965390"
  },
  {
    "text": "base 2 of n, that's very good. Turns out that it's\nnot exactly log base",
    "start": "2965390",
    "end": "2970728"
  },
  {
    "text": "2 of n, because there are\nconstants in these work and span bounds, so it's\non the order of log of n.",
    "start": "2970728",
    "end": "2977330"
  },
  {
    "text": "That's the parallelism. And it turns out that order log\nn parallelism is not very high.",
    "start": "2977330",
    "end": "2982898"
  },
  {
    "text": "In general, you want the\nparallelism to be much higher, something polynomial in n.",
    "start": "2982898",
    "end": "2989599"
  },
  {
    "text": "And in order to get\nmore parallelism in this algorithm,\nwhat you have to do",
    "start": "2989600",
    "end": "2998060"
  },
  {
    "text": "is you have to\nparallelize this partition function, because\nright now I I'm just executing\nthis sequentially.",
    "start": "2998060",
    "end": "3004540"
  },
  {
    "text": "But you can actually indeed\nwrite a parallel partition function that takes linear\nyour work in order log n span.",
    "start": "3004540",
    "end": "3012519"
  },
  {
    "text": "And then this would give you\nan overall span bound of log squared n. And then if you take n log\nn divided by log squared n,",
    "start": "3012520",
    "end": "3018340"
  },
  {
    "text": "that gives you an\noverall parallelism of n over log n, which is much\nhigher than order log n here.",
    "start": "3018340",
    "end": "3024532"
  },
  {
    "text": "And similarly, if you were\nto implement a merge sort, you would also need to make\nsure that the merging routine is",
    "start": "3024532",
    "end": "3029830"
  },
  {
    "text": "implemented in\nparallel, if you want to see significant speedup. So not only do you have to\nexecute the two recursive calls",
    "start": "3029830",
    "end": "3035165"
  },
  {
    "text": "in parallel, you also\nneed to make sure that the merging portion of\nthe code is done in parallel.",
    "start": "3035165",
    "end": "3041790"
  },
  {
    "text": "Any questions on this example? ",
    "start": "3041790",
    "end": "3049019"
  },
  {
    "text": "AUDIENCE: In the graph\nthat you had, sometimes when you got to higher processor\nnumbers, it got jagged,",
    "start": "3049019",
    "end": "3055610"
  },
  {
    "text": "and so sometimes adding a\nprocessor was making it slower. What are some\nreasons [INAUDIBLE]??",
    "start": "3055610",
    "end": "3060960"
  },
  {
    "text": "JULIAN SHUN: Yeah so I believe\nthat's just due to noise, because there's some noise\ngoing on in the machine.",
    "start": "3060960",
    "end": "3066680"
  },
  {
    "text": "So if you ran it\nenough times and took the average or the median,\nit should be always going up,",
    "start": "3066680",
    "end": "3072109"
  },
  {
    "text": "or it shouldn't be\ndecreasing, at least. ",
    "start": "3072110",
    "end": "3077380"
  },
  {
    "text": "Yes? AUDIENCE: So [INAUDIBLE]\nis also [INAUDIBLE]??",
    "start": "3077380",
    "end": "3082740"
  },
  {
    "text": " JULIAN SHUN: So at one\nlevel of recursion,",
    "start": "3082740",
    "end": "3089650"
  },
  {
    "text": "the partition function\ntakes order log n span. You can show that there are\nlog n levels of recursion",
    "start": "3089650",
    "end": "3095580"
  },
  {
    "text": "in this quicksort algorithm. I didn't go over the\ndetails of this analysis, but you can show that.",
    "start": "3095580",
    "end": "3102690"
  },
  {
    "text": "And then therefore,\nthe overall span is going to be\norder log squared. And I can show you on\nthe board after class,",
    "start": "3102690",
    "end": "3107819"
  },
  {
    "text": "if you're interested, or I\ncan give you a reference. ",
    "start": "3107820",
    "end": "3113090"
  },
  {
    "text": "Other questions? ",
    "start": "3113090",
    "end": "3119640"
  },
  {
    "text": "So it turns out that in\naddition to quicksort, there are also many other\ninteresting practical parallel",
    "start": "3119640",
    "end": "3126540"
  },
  {
    "start": "3120000",
    "end": "3120000"
  },
  {
    "text": "algorithms out there. So here, I've listed\na few of them. And by practical, I mean\nthat the Cilk program running",
    "start": "3126540",
    "end": "3132480"
  },
  {
    "text": "on one processor is\ncompetitive with the best sequential program\nfor that problem.",
    "start": "3132480",
    "end": "3137640"
  },
  {
    "text": "And so you can see that I've\nlisted the work and the span of merge sort here.",
    "start": "3137640",
    "end": "3143880"
  },
  {
    "text": "And if you implement\nthe merge and parallel, the span of the\noverall computation would be log cubed n.",
    "start": "3143880",
    "end": "3149370"
  },
  {
    "text": "And log n divided by log cubed\nn is n over log squared n. That's the parallelism,\nwhich is pretty high.",
    "start": "3149370",
    "end": "3154780"
  },
  {
    "text": "And in general, all\nof these computations have pretty high parallelism. Another thing to note is that\nthese algorithms are practical,",
    "start": "3154780",
    "end": "3162060"
  },
  {
    "text": "because their work\nbound is asymptotically equal to the work of the\ncorresponding sequential",
    "start": "3162060",
    "end": "3168360"
  },
  {
    "text": "algorithm. That's known as a work-efficient\nparallel algorithm. It's actually one of the goals\nof parallel algorithm design,",
    "start": "3168360",
    "end": "3174540"
  },
  {
    "text": "to come up with work-efficient\nparallel algorithms. Because this means\nthat even if you have a small number\nof processors,",
    "start": "3174540",
    "end": "3180420"
  },
  {
    "text": "you can still be competitive\nwith a sequential algorithm running on one processor.",
    "start": "3180420",
    "end": "3186410"
  },
  {
    "text": "And in the next\nlecture, we actually",
    "start": "3186410",
    "end": "3192329"
  },
  {
    "text": "see some examples of\nthese other algorithms, and possibly even ones\nnot listed on this slide,",
    "start": "3192330",
    "end": "3197550"
  },
  {
    "text": "and we'll go over the\nwork and span analysis and figure out the parallelism. ",
    "start": "3197550",
    "end": "3206020"
  },
  {
    "text": "So now I want to move on to talk\nabout some scheduling theory. So I talked about these\ncomputation dags earlier,",
    "start": "3206020",
    "end": "3212675"
  },
  {
    "text": "analyzed the work\nand the span of them, but I never talked about how\nthese different strands are actually mapped to\nprocessors at running time.",
    "start": "3212675",
    "end": "3221140"
  },
  {
    "text": "So let's talk a little bit\nabout scheduling theory. And it turns out that\nscheduling theory is actually very general. It's not just limited\nto parallel programming.",
    "start": "3221140",
    "end": "3229900"
  },
  {
    "text": "It's used all over the place\nin computer science, operations research, and math.",
    "start": "3229900",
    "end": "3238060"
  },
  {
    "text": "So as a reminder, Cilk\nallows the program to express potential\nparallelism in an application.",
    "start": "3238060",
    "end": "3243460"
  },
  {
    "text": "And a Cilk scheduler is\ngoing to map these strands onto the processors that you\nhave available dynamically",
    "start": "3243460",
    "end": "3250750"
  },
  {
    "text": "at runtime. Cilk actually uses a\ndistributed scheduler.",
    "start": "3250750",
    "end": "3256900"
  },
  {
    "text": "But since the theory of\ndistributed schedulers is a little bit\ncomplicated, we'll actually explore the\nideas of scheduling first",
    "start": "3256900",
    "end": "3263590"
  },
  {
    "text": "using a centralized scheduler. And a centralized\nscheduler knows everything",
    "start": "3263590",
    "end": "3269230"
  },
  {
    "text": "about what's going on\nin the computation, and it can use that to\nmake a good decision.",
    "start": "3269230",
    "end": "3274490"
  },
  {
    "text": "So let's first look at what\na centralized scheduler does, and then I'll talk a\nlittle bit about the Cilk",
    "start": "3274490",
    "end": "3279580"
  },
  {
    "text": "distributed scheduler. And we'll learn more about that\nin a future lecture as well. ",
    "start": "3279580",
    "end": "3287240"
  },
  {
    "start": "3285000",
    "end": "3285000"
  },
  {
    "text": "So we're going to look\nat a greedy scheduler. And an idea of a\ngreedy scheduler is to just do as\nmuch as possible",
    "start": "3287240",
    "end": "3293770"
  },
  {
    "text": "in every step of\nthe computation. So has anyone seen\ngreedy algorithms before?",
    "start": "3293770",
    "end": "3299480"
  },
  {
    "text": "Right. So many of you have seen\ngreedy algorithms before. So the idea is similar here. We're just going to\ndo as much as possible",
    "start": "3299480",
    "end": "3304970"
  },
  {
    "text": "at the current time step. We're not going to think\ntoo much about the future. ",
    "start": "3304970",
    "end": "3311820"
  },
  {
    "text": "So we're going to\ndefine a ready strand to be a strand where all of its\npredecessors in the computation",
    "start": "3311820",
    "end": "3317490"
  },
  {
    "text": "dag have already executed. So in this example\nhere, let's say",
    "start": "3317490",
    "end": "3322560"
  },
  {
    "text": "I already executed all\nof these blue strands. Then the ones\nshaded in yellow are",
    "start": "3322560",
    "end": "3328740"
  },
  {
    "text": "going to be my ready\nstrands, because they have all of their\npredecessors executed already.",
    "start": "3328740",
    "end": "3335540"
  },
  {
    "text": "And there are two types of\nsteps in a greedy scheduler. The first kind of step is\ncalled a complete step.",
    "start": "3335540",
    "end": "3344160"
  },
  {
    "text": "And in a complete step, we\nhave at least P strands ready.",
    "start": "3344160",
    "end": "3350250"
  },
  {
    "text": "So if we had P equal to 3, then\nwe have a complete step now, because we have 5 strands\nready, which is greater than 3.",
    "start": "3350250",
    "end": "3358410"
  },
  {
    "text": "So what are we going to\ndo in a complete step? What would a greedy\nscheduler do? ",
    "start": "3358410",
    "end": "3364520"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] JULIAN SHUN: Yeah, so a\ngreedy scheduler would just",
    "start": "3364520",
    "end": "3370120"
  },
  {
    "text": "do as much as it can. So it would just run any 3 of\nthese, or any P in general.",
    "start": "3370120",
    "end": "3376190"
  },
  {
    "text": "So let's say I picked\nthese 3 to run. So it turns out that these are\nactually the worst 3 to run,",
    "start": "3376190",
    "end": "3383920"
  },
  {
    "text": "because they don't enable\nany new strands to be ready. But I can pick those 3.",
    "start": "3383920",
    "end": "3390040"
  },
  {
    "text": "And then the\nincomplete step is one where I have fewer\nthan P strands ready. So here, I have 2 strands\nready, and I have 3 processors.",
    "start": "3390040",
    "end": "3399069"
  },
  {
    "text": "So what would I do in\nan incomplete step? AUDIENCE: Just run through\nthe strands that are ready.",
    "start": "3399070",
    "end": "3406010"
  },
  {
    "text": "JULIAN SHUN: Yeah, so\njust run all of them. So here, I'm going to\nexecute these two strands. ",
    "start": "3406010",
    "end": "3412980"
  },
  {
    "text": "And then we're going\nto use complete steps and incomplete steps to\nanalyze the performance of the greedy scheduler.",
    "start": "3412980",
    "end": "3419350"
  },
  {
    "start": "3417000",
    "end": "3417000"
  },
  {
    "text": "There's a famous\ntheorem which was first shown by Ron Graham\nin 1968 that says",
    "start": "3419350",
    "end": "3426009"
  },
  {
    "text": "that any greedy\nscheduler achieves the following time bound-- T sub P is less than or equal\nto T1 over P plus T infinity.",
    "start": "3426010",
    "end": "3435610"
  },
  {
    "text": "And you might recognize the\nterms on the right hand side-- T1 is the work, and T\ninfinity is the span",
    "start": "3435610",
    "end": "3442930"
  },
  {
    "text": "that we saw earlier. And here's a simple proof for\nwhy this time bound holds.",
    "start": "3442930",
    "end": "3449755"
  },
  {
    "text": " So we can upper bound the\nnumber of complete steps",
    "start": "3449755",
    "end": "3455810"
  },
  {
    "text": "in the computation by\nT1 over P. And this is because each complete step\nis going to perform P work.",
    "start": "3455810",
    "end": "3463060"
  },
  {
    "text": "So after T1 over\nP completes steps, we'll have done all the\nwork in our computation.",
    "start": "3463060",
    "end": "3469010"
  },
  {
    "text": "So that means that the\nnumber of complete steps can be at most T1 over P.",
    "start": "3469010",
    "end": "3474619"
  },
  {
    "text": "So any questions on this? ",
    "start": "3474620",
    "end": "3482750"
  },
  {
    "text": "So now, let's look at the\nnumber of incomplete steps we can have.",
    "start": "3482750",
    "end": "3488619"
  },
  {
    "text": "So the number of incomplete\nsteps we can have is upper bounded by the\nspan, or T infinity.",
    "start": "3488620",
    "end": "3495890"
  },
  {
    "text": "And the reason why is that if\nyou look at the unexecuted dag",
    "start": "3495890",
    "end": "3501910"
  },
  {
    "text": "right before you execute\nan incomplete step, and you measure the span\nof that unexecuted dag,",
    "start": "3501910",
    "end": "3508090"
  },
  {
    "text": "you'll see that once you\nexecute an incomplete step, it's going to reduce the\nspan of that dag by 1.",
    "start": "3508090",
    "end": "3514240"
  },
  {
    "text": "So here, this is the span\nof our unexecuted dag",
    "start": "3514240",
    "end": "3519310"
  },
  {
    "text": "that contains just\nthese seven nodes. The span of this is 5. And when we execute\nan incomplete step,",
    "start": "3519310",
    "end": "3525070"
  },
  {
    "text": "we're going to process all the\nroots of this unexecuted dag, delete them from the\ndag, and therefore, we're",
    "start": "3525070",
    "end": "3531369"
  },
  {
    "text": "going to reduce the length\nof the longest path by 1. So when we execute\nan incomplete step, it decreases the\nspan from 5 to 4.",
    "start": "3531370",
    "end": "3538480"
  },
  {
    "text": " And then the time\nbound up here, T sub P,",
    "start": "3538480",
    "end": "3545040"
  },
  {
    "text": "is just upper bounded by the\nsum of these two types of steps. Because after you execute\nT1 over P complete steps",
    "start": "3545040",
    "end": "3553370"
  },
  {
    "text": "and T infinity\nincomplete steps, you must have finished the\nentire computation.",
    "start": "3553370",
    "end": "3559705"
  },
  {
    "text": "So any questions? ",
    "start": "3559705",
    "end": "3568590"
  },
  {
    "text": "A corollary of this theorem\nis that any greedy scheduler achieves within a factor of 2\nof the optimal running time.",
    "start": "3568590",
    "end": "3575250"
  },
  {
    "start": "3572000",
    "end": "3572000"
  },
  {
    "text": "So this is the optimal\nrunning time of a scheduler that knows everything and can\npredict the future and so on.",
    "start": "3575250",
    "end": "3583680"
  },
  {
    "text": "So let's let TP star be\nthe execution time produced by an optimal scheduler.",
    "start": "3583680",
    "end": "3591780"
  },
  {
    "text": "We know that TP star has to\nbe at least the max of T1 over P and T infinity.",
    "start": "3591780",
    "end": "3597690"
  },
  {
    "text": "This is due to the\nwork and span laws. So it has to be at least\na max of these two terms.",
    "start": "3597690",
    "end": "3604530"
  },
  {
    "text": "Otherwise, we wouldn't have\nfinished the computation. So now we can take\nthe inequality",
    "start": "3604530",
    "end": "3612270"
  },
  {
    "text": "we had before for the\ngreedy scheduler bound-- so TP is less than or equal\nto T1 over P plus T infinity.",
    "start": "3612270",
    "end": "3620500"
  },
  {
    "text": "And this is upper bounded by\n2 times the max of these two terms. So A plus B is upper bounded\nby 2 times the max of A and B.",
    "start": "3620500",
    "end": "3630150"
  },
  {
    "text": "And then now, the max\nof T1 over P and T infinity is just upper\nbounded by TP star.",
    "start": "3630150",
    "end": "3636960"
  },
  {
    "text": "So we can substitute\nthat in, and we get that TP is upper\nbounded by 2 times",
    "start": "3636960",
    "end": "3642809"
  },
  {
    "text": "TP star, which is the running\ntime of the optimal scheduler. So the greedy scheduler\nachieves within a factor",
    "start": "3642810",
    "end": "3649230"
  },
  {
    "text": "of 2 of the optimal scheduler. ",
    "start": "3649230",
    "end": "3657000"
  },
  {
    "text": "Here's another corollary. This is a more\ninteresting corollary. It says that any greedy\nscheduler achieves",
    "start": "3657000",
    "end": "3662849"
  },
  {
    "text": "near-perfect linear speedup\nwhenever T1 divided by T infinity is greater\nthan or equal to P.",
    "start": "3662850",
    "end": "3672849"
  },
  {
    "text": "To see why this is true-- if we have that\nT1 over T infinity is much greater than P--",
    "start": "3672850",
    "end": "3680350"
  },
  {
    "text": "so the double arrows here\nmean that the left hand",
    "start": "3680350",
    "end": "3685612"
  },
  {
    "text": "side is much greater than\nthe right hand side-- then this means that the span\nis much less than T1 over P.",
    "start": "3685612",
    "end": "3692500"
  },
  {
    "text": "And the greedy scheduling\ntheorem gives us that TP is less than or equal\nto T1 over P plus T infinity,",
    "start": "3692500",
    "end": "3700630"
  },
  {
    "text": "but T infinity is much\nless than T1 over P, so the first term\ndominates, and we have that TP is approximately\nequal to T1 over P.",
    "start": "3700630",
    "end": "3708940"
  },
  {
    "text": "And therefore, the speedup you\nget is T1 over P, which is P.",
    "start": "3708940",
    "end": "3714760"
  },
  {
    "text": "And this is linear speedup. ",
    "start": "3714760",
    "end": "3722040"
  },
  {
    "text": "The quantity T1\ndivided by P times T infinity is known as\nthe parallel slackness.",
    "start": "3722040",
    "end": "3728270"
  },
  {
    "text": "So this is basically\nmeasuring how much more parallelism you have in a\ncomputation than the number",
    "start": "3728270",
    "end": "3733550"
  },
  {
    "text": "of processors you have. And if parallel\nslackness is very high, then this corollary\nis going to hold,",
    "start": "3733550",
    "end": "3740000"
  },
  {
    "text": "and you're going to see\nnear-linear speedup. As a rule of thumb, you usually\nwant the parallel slackness",
    "start": "3740000",
    "end": "3746270"
  },
  {
    "text": "of your program\nto be at least 10. Because if you have a\nparallel slackness of just 1,",
    "start": "3746270",
    "end": "3753590"
  },
  {
    "text": "you can't actually amortize\nthe overheads of the scheduling mechanism. So therefore, you want\nthe parallel slackness",
    "start": "3753590",
    "end": "3760130"
  },
  {
    "text": "to be at least 10 when\nyou're programming in Cilk. ",
    "start": "3760130",
    "end": "3770990"
  },
  {
    "text": "So that was the\ngreedy scheduler. Let's talk a little bit\nabout the Cilk scheduler.",
    "start": "3770990",
    "end": "3776650"
  },
  {
    "text": "So Cilk uses a\nwork-stealing scheduler, and it achieves an\nexpected running time",
    "start": "3776650",
    "end": "3782630"
  },
  {
    "text": "of TP equal to T1 over\nP plus order T infinity.",
    "start": "3782630",
    "end": "3788150"
  },
  {
    "text": "So instead of just\nsumming the two terms, we actually have a big O\nin front of the T infinity, and this is used to account for\nthe overheads of scheduling.",
    "start": "3788150",
    "end": "3796820"
  },
  {
    "text": "The greedy scheduler\nI presented earlier-- I didn't account for any of\nthe overheads of scheduling. I just assumed that it could\nfigure out which of the tasks",
    "start": "3796820",
    "end": "3803720"
  },
  {
    "text": "to execute. So this Cilk\nwork-stealing scheduler has this expected\ntime provably, so you",
    "start": "3803720",
    "end": "3811730"
  },
  {
    "text": "can prove this using\nrandom variables and tail bounds of distribution.",
    "start": "3811730",
    "end": "3817050"
  },
  {
    "text": "So Charles Leiserson\nhas a paper that talks about how to prove this.",
    "start": "3817050",
    "end": "3822140"
  },
  {
    "text": "And empirically, we usually\nsee that TP is more like T1 over P plus T infinity.",
    "start": "3822140",
    "end": "3828830"
  },
  {
    "text": "So we usually don't see any\nbig constant in front of the T infinity term in practice.",
    "start": "3828830",
    "end": "3836090"
  },
  {
    "text": "And therefore, we can get\nnear-perfect linear speedup, as long as the number of\nprocessors is much less than T1",
    "start": "3836090",
    "end": "3844250"
  },
  {
    "text": "over T infinity, the\nmaximum parallelism. And as I said earlier, the\ninstrumentation in Cilkscale",
    "start": "3844250",
    "end": "3851780"
  },
  {
    "text": "will allow you to\nmeasure the work and span terms so that you can figure\nout how much parallelism",
    "start": "3851780",
    "end": "3857059"
  },
  {
    "text": "is in your program. Any questions? ",
    "start": "3857060",
    "end": "3868730"
  },
  {
    "text": "So let's talk a little bit\nabout how the Cilk runtime system works. ",
    "start": "3868730",
    "end": "3876140"
  },
  {
    "text": "So in the Cilk runtime system,\neach worker or processor maintains a work deque.",
    "start": "3876140",
    "end": "3882349"
  },
  {
    "text": "Deque stands for\ndouble-ended queue, so it's just short for\ndouble-ended queue. It maintains a work\ndeque of ready strands,",
    "start": "3882350",
    "end": "3889280"
  },
  {
    "text": "and it manipulates the\nbottom of the deck, just like you would in a\nstack of a sequential program.",
    "start": "3889280",
    "end": "3896060"
  },
  {
    "text": "So here, I have four\nprocessors, and each one of them have their own deques, and they\nhave these things on the stack,",
    "start": "3896060",
    "end": "3903900"
  },
  {
    "text": "these function calls,\nsaves the return address to local variables, and so on.",
    "start": "3903900",
    "end": "3909859"
  },
  {
    "text": "So a processor can\ncall a function, and when it calls\na function, it just places that function's frame\nat the bottom of its stack.",
    "start": "3909860",
    "end": "3919790"
  },
  {
    "text": "You can also spawn things, so\nthen it places a spawn frame at the bottom of its stack.",
    "start": "3919790",
    "end": "3925575"
  },
  {
    "text": "And then these things\ncan happen in parallel, so multiple processes can\nbe spawning and calling things in parallel.",
    "start": "3925575",
    "end": "3930710"
  },
  {
    "text": " And you can also return\nfrom a spawn or a call.",
    "start": "3930710",
    "end": "3938330"
  },
  {
    "text": "So here, I'm going to\nreturn from a call. Then I return from a spawn. And at this point,\nI don't actually",
    "start": "3938330",
    "end": "3944870"
  },
  {
    "text": "have anything left to do\nfor the second processor. So what do I do now, when\nI'm left with nothing to do?",
    "start": "3944870",
    "end": "3952339"
  },
  {
    "text": " Yes? AUDIENCE: Take a [INAUDIBLE].",
    "start": "3952340",
    "end": "3959720"
  },
  {
    "text": "JULIAN SHUN: Yeah,\nso the idea here is to steal some work\nfrom another processor.",
    "start": "3959720",
    "end": "3965640"
  },
  {
    "text": "So when a worker runs\nout of work to do, it's going to steal from the\ntop of a random victim's deque.",
    "start": "3965640",
    "end": "3971640"
  },
  {
    "text": "So it's going to pick one of\nthese processors at random. It's going to roll some dice\nto determine who to steal from.",
    "start": "3971640",
    "end": "3979140"
  },
  {
    "text": "And let's say that it\npicked the third processor. Now it's going to\ntake all of the stuff",
    "start": "3979140",
    "end": "3986369"
  },
  {
    "text": "at the top of the deque\nup until the next spawn and place it into its own deque.",
    "start": "3986370",
    "end": "3992160"
  },
  {
    "text": "And then now it has\nstuff to do again. So now it can continue\nexecuting this code. It can spawn stuff,\ncall stuff, and so on.",
    "start": "3992160",
    "end": "4002190"
  },
  {
    "text": "So the idea is that whenever a\nworker runs out of work to do, it's going to start\nstealing some work",
    "start": "4002190",
    "end": "4007430"
  },
  {
    "text": "from other processors. But if it always has enough\nwork to do, then it's happy,",
    "start": "4007430",
    "end": "4012710"
  },
  {
    "text": "and it doesn't need to steal\nthings from other processors. And this is why MIT gives\nus so much work to do,",
    "start": "4012710",
    "end": "4019310"
  },
  {
    "text": "so we don't have to steal\nwork from other people.  So a famous theorem says that\nwith sufficient parallelism,",
    "start": "4019310",
    "end": "4028010"
  },
  {
    "text": "workers steal very\ninfrequently, and this gives us near-linear speedup.",
    "start": "4028010",
    "end": "4033200"
  },
  {
    "text": "So with sufficient\nparallelism, the first term in our running bound is going\nto dominate the T1 over P term,",
    "start": "4033200",
    "end": "4039540"
  },
  {
    "text": "and that gives us\nnear-linear speedup. ",
    "start": "4039540",
    "end": "4046430"
  },
  {
    "text": "Let me actually show you a\npseudoproof of this theorem.",
    "start": "4046430",
    "end": "4052069"
  },
  {
    "text": "And I'm allowed to\ndo a pseudoproof. It's not actually a real\nproof, but a pseudoproof. So I'm allowed to do\nthis, because I'm not",
    "start": "4052070",
    "end": "4057998"
  },
  {
    "text": "the author of an\nalgorithms textbook.  So here's a pseudo proof.",
    "start": "4057998",
    "end": "4063753"
  },
  {
    "text": "AUDIENCE: Yet. JULIAN SHUN: Yet.  So a processor is either working\nor stealing at every time step.",
    "start": "4063753",
    "end": "4073170"
  },
  {
    "text": "And the total time that all\nprocessors spend working is just T1, because that's the\ntotal work that you have to do.",
    "start": "4073170",
    "end": "4081240"
  },
  {
    "text": "And then when it's not\ndoing work, it's stealing. And each steal has\na 1 over P chance",
    "start": "4081240",
    "end": "4086869"
  },
  {
    "text": "of reducing the span by 1,\nbecause one of the processors is contributing to the longest\npath in the compilation dag.",
    "start": "4086870",
    "end": "4094187"
  },
  {
    "text": "And there's a 1 over\nP chance that I'm going to pick that\nprocessor and steal some work from that\nprocessor and reduce",
    "start": "4094187",
    "end": "4099589"
  },
  {
    "text": "the span of my remaining\ncomputation by 1. And therefore, the\nexpected cost of all steals",
    "start": "4099590",
    "end": "4106040"
  },
  {
    "text": "is going to be order\nP times T infinity, because I have to steal P\nthings in expectation before I",
    "start": "4106040",
    "end": "4111259"
  },
  {
    "text": "get to the processor that\nhas the critical path.",
    "start": "4111260",
    "end": "4117739"
  },
  {
    "text": "And therefore, my overall costs\nfor stealing is order P times T",
    "start": "4117740",
    "end": "4122839"
  },
  {
    "text": "infinity, because I'm going\nto do this T infinity times. And since there\nare P processors,",
    "start": "4122840",
    "end": "4128810"
  },
  {
    "text": "I'm going to divide\nthe expected time by P, so T1 plus O of P times\nT infinity divided by P,",
    "start": "4128810",
    "end": "4137915"
  },
  {
    "text": "and that's going to\ngive me the bound-- T1 over P plus order T infinity.",
    "start": "4137915",
    "end": "4143670"
  },
  {
    "text": "So this pseudoproof here ignores\nissues with independence, but it still gives\nyou an intuition",
    "start": "4143670",
    "end": "4150140"
  },
  {
    "text": "of why we get this\nexpected running time. If you want to actually\nsee the full proof,",
    "start": "4150140",
    "end": "4156407"
  },
  {
    "text": "it's actually quite interesting. It uses random variables and\ntail bounds of distributions.",
    "start": "4156407",
    "end": "4161910"
  },
  {
    "text": "And this is the\npaper that has this. This is by Blumofe\nand Charles Leiserson.",
    "start": "4161910",
    "end": "4168115"
  },
  {
    "start": "4168115",
    "end": "4174189"
  },
  {
    "text": "So another thing I\nwant to talk about is that Cilk supports\nC's rules for pointers.",
    "start": "4174189",
    "end": "4180540"
  },
  {
    "text": "So a pointer to a stack space\ncan be passed from a parent to a child, but not from\na child to a parent.",
    "start": "4180540",
    "end": "4187450"
  },
  {
    "text": "And this is the same as the\nstack rule for sequential C programs.",
    "start": "4187450",
    "end": "4193170"
  },
  {
    "text": "So let's say I have this\ncomputation on the left here. So A is going to spawn\noff B, and then it's",
    "start": "4193170",
    "end": "4200440"
  },
  {
    "text": "going to continue\nexecuting C. In then C is going to spawn\noff D and execute E.",
    "start": "4200440",
    "end": "4207160"
  },
  {
    "text": "So we see on the right hand\nside the views of the stacks for each of the tasks here.",
    "start": "4207160",
    "end": "4212800"
  },
  {
    "text": "So A sees its own stack. B sees its own\nstack, but it also sees A's stack, because\nA is its parent.",
    "start": "4212800",
    "end": "4220989"
  },
  {
    "text": "C will see its own\nstack, but again, it sees A's stack, because\nA is its parent. And then finally, D and E,\nthey see the stack of C,",
    "start": "4220990",
    "end": "4228940"
  },
  {
    "text": "and they also see\nthe stack of A. So in general, a task\ncan see the stack of all of its ancestors\nin this computation graph.",
    "start": "4228940",
    "end": "4236770"
  },
  {
    "text": " And we call this a\ncactus stack, because it",
    "start": "4236770",
    "end": "4243010"
  },
  {
    "text": "sort of looks like a cactus,\nif you draw this upside down. And Cilk's cactus stack\nsupports multiple views",
    "start": "4243010",
    "end": "4250179"
  },
  {
    "text": "of the stacks in\nparallel, and this is what makes the parallel\ncalls to functions work in C.",
    "start": "4250180",
    "end": "4259010"
  },
  {
    "text": "We can also bound the stack\nspace used by a Cilk program.",
    "start": "4259010",
    "end": "4264199"
  },
  {
    "text": "So let's let S sub 1 be\nthe stack space required by the serial execution\nof a Cilk program.",
    "start": "4264200",
    "end": "4271760"
  },
  {
    "text": "Then the stack space required\nby a P-processor execution is going to be\nbounded by P times S1.",
    "start": "4271760",
    "end": "4279050"
  },
  {
    "text": "So SP is the stack\nspace required by a P-processor execution. That's less than or\nequal to P times S1.",
    "start": "4279050",
    "end": "4287900"
  },
  {
    "text": "Here's a high-level proof\nof why this is true. So it turns out that the\nwork-stealing algorithm in Cilk",
    "start": "4287900",
    "end": "4293480"
  },
  {
    "text": "maintains what's called\nthe busy leaves property. And this says that each of the\nexisting leaves that are still",
    "start": "4293480",
    "end": "4301670"
  },
  {
    "text": "active in the computation\ndag have some work they're executing on it.",
    "start": "4301670",
    "end": "4307280"
  },
  {
    "text": "So in this example\nhere, the vertices shaded in blue and purple--",
    "start": "4307280",
    "end": "4312330"
  },
  {
    "text": "these are the ones that are in\nmy remaining computation dag. And all of the gray nodes\nhave already been finished.",
    "start": "4312330",
    "end": "4319650"
  },
  {
    "text": "And here-- for\neach of the leaves here, I have one processor\non that leaf executing",
    "start": "4319650",
    "end": "4325130"
  },
  {
    "text": "the task associated with it. So Cilk guarantees this\nbusy leaves property. ",
    "start": "4325130",
    "end": "4331650"
  },
  {
    "text": "And now, for each\nof these processors, the amount of stack\nspace it needs is it needs the stack\nspace for its own task",
    "start": "4331650",
    "end": "4338420"
  },
  {
    "text": "and then everything above\nit in this computation dag. And we can actually bound\nthat by the stack space needed",
    "start": "4338420",
    "end": "4345170"
  },
  {
    "text": "by a single processor execution\nof the Cilk program, S1,",
    "start": "4345170",
    "end": "4350360"
  },
  {
    "text": "because S1 is just the\nmaximum stack space we need, which is basically the\nlongest path in this graph.",
    "start": "4350360",
    "end": "4359900"
  },
  {
    "text": "And we do this for\nevery processor. So therefore, the upper\nbound on the stack space",
    "start": "4359900",
    "end": "4365000"
  },
  {
    "text": "required by P-processor\nexecution is just P times S1. And in general, this is a\nquite loose upper bound,",
    "start": "4365000",
    "end": "4371960"
  },
  {
    "text": "because you're not\nnecessarily going all the way all the way\ndown in this competition dag",
    "start": "4371960",
    "end": "4378380"
  },
  {
    "text": "every time. Usually you'll be much higher\nin this computation dag.",
    "start": "4378380",
    "end": "4385320"
  },
  {
    "text": "So any questions? Yes? AUDIENCE: In practice,\nhow much work is stolen? JULIAN SHUN: In practice, if you\nhave enough parallelism, then",
    "start": "4385320",
    "end": "4393643"
  },
  {
    "text": "you're not actually\ngoing to steal that much in your algorithm. So if you guarantee that\nthere's a lot of parallelism,",
    "start": "4393643",
    "end": "4400520"
  },
  {
    "text": "then each processor is going\nto have a lot of its own work to do, and it doesn't need\nto steal very frequently.",
    "start": "4400520",
    "end": "4408650"
  },
  {
    "text": "But if your\nparallelism is very low compared to the\nnumber of processors-- if it's equal to the\nnumber of processors,",
    "start": "4408650",
    "end": "4414980"
  },
  {
    "text": "then you're going to spend\na significant amount of time stealing, and the overheads\nof the work-stealing algorithm",
    "start": "4414980",
    "end": "4421750"
  },
  {
    "text": "are going to show up\nin your running time. AUDIENCE: So I\nmeant in one steal-- like do you take\nhalf of the deque,",
    "start": "4421750",
    "end": "4428250"
  },
  {
    "text": "or do you take one\nelement of the deque? JULIAN SHUN: So the standard\nCilk work-stealing scheduler takes everything at\nthe top of the deque up",
    "start": "4428250",
    "end": "4435800"
  },
  {
    "text": "until the next spawn. So basically that's a strand. So it takes that. There are variants that\ntake more than that,",
    "start": "4435800",
    "end": "4441680"
  },
  {
    "text": "but the Cilk\nwork-stealing scheduler that we'll be\nusing in this class just takes the top strand. ",
    "start": "4441680",
    "end": "4449510"
  },
  {
    "text": "Any other questions?  So that's actually\nall I have for today.",
    "start": "4449510",
    "end": "4456508"
  },
  {
    "text": "If you have any\nadditional questions, you can come talk\nto us after class. And remember to meet with\nyour MITPOSSE mentors soon.",
    "start": "4456508",
    "end": "4465170"
  },
  {
    "start": "4465170",
    "end": "4478571"
  }
]