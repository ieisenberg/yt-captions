[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13330"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13330",
    "end": "22369"
  },
  {
    "text": "PROFESSOR: All right. Let's see. We're going to start\ntoday with a wrap up of our discussion of\nunivariate time series",
    "start": "22370",
    "end": "29130"
  },
  {
    "text": "analysis. And last time we went through\nthe Wold representation",
    "start": "29130",
    "end": "35140"
  },
  {
    "text": "theorem, which\napplies to covariance stationary processes, a\nvery powerful theorem.",
    "start": "35140",
    "end": "41090"
  },
  {
    "text": "And implementations of\nthe covariance stationary processes with ARMA models.",
    "start": "41090",
    "end": "47700"
  },
  {
    "text": "And we discussed\nestimation of those models with maximum likelihood.",
    "start": "47700",
    "end": "54840"
  },
  {
    "text": "And here in this\nslide I just wanted to highlight how when\nwe estimate models",
    "start": "54840",
    "end": "61070"
  },
  {
    "text": "with maximum likelihood\nwe need to have an assumption of a probability\ndistribution for what's random,",
    "start": "61070",
    "end": "67280"
  },
  {
    "text": "and in the ARMA structure\nwe consider the simple case where the innovations,\nthe eta_t,",
    "start": "67280",
    "end": "74119"
  },
  {
    "text": "are normally\ndistributed white noise. So they're independent and\nidentically distributed",
    "start": "74120",
    "end": "79520"
  },
  {
    "text": "normal random variables. And the likelihood\nfunction can be maximized at the maximum\nlikelihood parameters.",
    "start": "79520",
    "end": "88240"
  },
  {
    "text": "And it's simple to implement\nthe limited information maximum",
    "start": "88240",
    "end": "94450"
  },
  {
    "text": "likelihood where one conditions\non the first few observations in the time series.",
    "start": "94450",
    "end": "100590"
  },
  {
    "text": "If you look at the likelihood\nstructure for ARMA models,",
    "start": "100590",
    "end": "106869"
  },
  {
    "text": "the density of an outcome\nat a given time point depends on lags of that\ndependent variable.",
    "start": "106870",
    "end": "113700"
  },
  {
    "text": "So if those are unavailable,\nthen that can be a problem. One can implement limited\ninformation maximum likelihood",
    "start": "113700",
    "end": "121930"
  },
  {
    "text": "where you're just conditioning\non those initial values, or there are full information\nmaximum likelihood methods",
    "start": "121930",
    "end": "127740"
  },
  {
    "text": "that you can apply as well. Generally though the\nlimited information case",
    "start": "127740",
    "end": "133170"
  },
  {
    "text": "is what's applied. Then the issue is\nmodel selection.",
    "start": "133170",
    "end": "138800"
  },
  {
    "text": "And with model\nselection the issues that arise with time\nseries are issues",
    "start": "138800",
    "end": "143870"
  },
  {
    "text": "that arise in fitting any\nkind of statistical model. Ordinarily one will\nhave multiple candidates",
    "start": "143870",
    "end": "150040"
  },
  {
    "text": "for the model you\nwant to fit to data. And the issue is how\ndo you judge which ones are better than others.",
    "start": "150040",
    "end": "156390"
  },
  {
    "text": "Why would you prefer\none over the other? And if we're considering a\ncollection of different ARMA",
    "start": "156390",
    "end": "163230"
  },
  {
    "text": "models then we could say, fit\nall ARMA models of order p,q",
    "start": "163230",
    "end": "169670"
  },
  {
    "text": "with p and q varying\nover some range. p from 0 up to p_max,\nq from q up to q_max.",
    "start": "169670",
    "end": "177130"
  },
  {
    "text": "And evaluate those\np,q different models.",
    "start": "177130",
    "end": "182270"
  },
  {
    "text": "And if we consider sigma\ntilde squared of p, q being the MLE of\nthe error variance,",
    "start": "182270",
    "end": "189270"
  },
  {
    "text": "then there are these\nmodel selection criteria that are very popular. Akaike information criterion,\nand Bayes information",
    "start": "189270",
    "end": "196450"
  },
  {
    "text": "criterion, and Hannan-Quinn. Now these criteria all\nhave the same term,",
    "start": "196450",
    "end": "202849"
  },
  {
    "text": "log of the MLE of\nthe error variance. So these criteria don't\nvary at all with that.",
    "start": "202850",
    "end": "210519"
  },
  {
    "text": "They just vary with\nthis second term, but let's focus first\non the AIC criterion.",
    "start": "210520",
    "end": "215780"
  },
  {
    "text": "A given model is\ngoing to be better if the log of the MLE for the\nerror variance is smaller.",
    "start": "215780",
    "end": "224760"
  },
  {
    "text": "Now is that a good thing? Meaning, what is\nthe interpretation",
    "start": "224760",
    "end": "230186"
  },
  {
    "text": "of that practically when you're\nfitting different models? ",
    "start": "230186",
    "end": "235720"
  },
  {
    "text": "Well, the practical\ninterpretation is the variability of the\nmodel about where you're",
    "start": "235720",
    "end": "242980"
  },
  {
    "text": "predicting things, our\nestimate of the error variance is smaller. So we have essentially a\nmodel with a smaller error",
    "start": "242980",
    "end": "249940"
  },
  {
    "text": "variance is better. So we're trying to minimize\nthe log of that variance.",
    "start": "249940",
    "end": "255720"
  },
  {
    "text": "Minimizing that is a good thing. Now what happens when\nyou have many sort",
    "start": "255720",
    "end": "263095"
  },
  {
    "text": "of independent variables\nto include in a model? Well, if you were doing a\nTaylor series approximation",
    "start": "263095",
    "end": "268930"
  },
  {
    "text": "of a continuous function,\neventually you'd sort of get to probably\nthe smooth function",
    "start": "268930",
    "end": "274150"
  },
  {
    "text": "with enough terms, but suppose\nthat the actual model, it does",
    "start": "274150",
    "end": "279800"
  },
  {
    "text": "have a finite number\nof parameters. And you're considering\nnew factors,",
    "start": "279800",
    "end": "284990"
  },
  {
    "text": "new lags of\nindependent variables in the autoregressions. As you add more\nand more variables,",
    "start": "284990",
    "end": "291190"
  },
  {
    "text": "well, there really\nshould be a penalty for adding extra variables\nthat aren't adding",
    "start": "291190",
    "end": "300140"
  },
  {
    "text": "real value to the model in terms\nof reducing the error variance. So the Akaike\ninformation criterion",
    "start": "300140",
    "end": "306190"
  },
  {
    "text": "is penalizing different\nmodels by a factor that depends on the size of the model\nin terms of the dimensionality",
    "start": "306190",
    "end": "314490"
  },
  {
    "text": "of the model parameters. So p plus q is\nthe dimensionality of the autoregression model.",
    "start": "314490",
    "end": "324020"
  },
  {
    "text": "So let's see.",
    "start": "324020",
    "end": "329770"
  },
  {
    "text": "With the BIC criterion the\ndifference between that and the AIC criterion is\nthat this factor two is",
    "start": "329770",
    "end": "339030"
  },
  {
    "text": "replaced by log n. So rather than having a sort\nof unit increment of penalty",
    "start": "339030",
    "end": "349120"
  },
  {
    "text": "for adding an extra parameter,\nthe Bayes information criterion",
    "start": "349120",
    "end": "354460"
  },
  {
    "text": "is adding a log n penalty\ntimes the number of parameters.",
    "start": "354460",
    "end": "359729"
  },
  {
    "text": "And so as the sample size\ngets larger and larger, that penalty gets\nhigher and higher.",
    "start": "359730",
    "end": "367030"
  },
  {
    "text": "Now the practical interpretation\nof the Akaike information",
    "start": "367030",
    "end": "372610"
  },
  {
    "text": "criterion is that it is\nvery similar to applying",
    "start": "372610",
    "end": "378139"
  },
  {
    "text": "a rule which says, we're\ngoing to include variables in our model if the square of\nthe t statistic for estimating",
    "start": "378140",
    "end": "388740"
  },
  {
    "text": "the additional parameter in the\nmodel is greater than 2 or not.",
    "start": "388740",
    "end": "394849"
  },
  {
    "text": "So in terms of when does the\nAkaike information criterion",
    "start": "394850",
    "end": "401710"
  },
  {
    "text": "become lower from adding\nadditional terms to a model? If you're considering two models\nthat differ by just one factor,",
    "start": "401710",
    "end": "409150"
  },
  {
    "text": "it's basically if the t\nstatistic for the model coefficient on that factor is a\nsquared value greater than two",
    "start": "409150",
    "end": "416889"
  },
  {
    "text": "or not. Now many of you who have\nseen regression models before",
    "start": "416890",
    "end": "423310"
  },
  {
    "text": "and applied them, in\nparticular applications would probably\nsay, I really don't believe in the value\nof an additional factor",
    "start": "423310",
    "end": "431590"
  },
  {
    "text": "unless the t statistic\nis greater than 1.96, or 2 or something.",
    "start": "431590",
    "end": "438410"
  },
  {
    "text": "But the Akaike\ninformation criterion says the t statistic\nshould be greater than the square root of 2.",
    "start": "438410",
    "end": "444450"
  },
  {
    "text": "So it's sort of a weaker\nconstraint for adding variables into the model. And now why is it called\nan information criterion?",
    "start": "444450",
    "end": "451551"
  },
  {
    "text": "I won't go into\nthis in the lecture. I am happy to go into\nit during office hours, but there's notions\nof information theory",
    "start": "451551",
    "end": "458085"
  },
  {
    "text": "and Kullback-Leibler\ninformation of the model versus the true\nmodel, and trying",
    "start": "458085",
    "end": "463500"
  },
  {
    "text": "to basically maximize the\ncloseness of our fitted model to that.",
    "start": "463500",
    "end": "468960"
  },
  {
    "text": "Now the Hannan-Quinn\ncriterion, let's just look at how that differs. Well, that basically has a\npenalty midway between the log",
    "start": "468960",
    "end": "477479"
  },
  {
    "text": "n and two. It's 2*log(log n). So this has a penalty that's\nincreasing with size n,",
    "start": "477480",
    "end": "483890"
  },
  {
    "text": "but not as fast as log n. This becomes\nrelevant when we have",
    "start": "483890",
    "end": "491479"
  },
  {
    "text": "models that get to be very large\nbecause we have a lot of data. Basically the more\ndata you have,",
    "start": "491480",
    "end": "497780"
  },
  {
    "text": "the more parameters\nyou should be able to incorporate in\nthe model if they're sort of statistically valid\nfactors, important factors.",
    "start": "497780",
    "end": "507360"
  },
  {
    "text": "And the Hannan-Quinn\ncriterion basically allows for modeling processes\nwhere really an infinite number",
    "start": "507360",
    "end": "516179"
  },
  {
    "text": "of variables might\nbe appropriate, but you need larger\nand larger sample sizes to effectively estimate those.",
    "start": "516179",
    "end": "524640"
  },
  {
    "text": "So those are the criteria that\ncan be applied with time series",
    "start": "524640",
    "end": "531530"
  },
  {
    "text": "models. And I should point\nout that, let's see, if you took sort of\nthis factor 2 over n",
    "start": "531530",
    "end": "539720"
  },
  {
    "text": "and inverted it to n over\ntwo log sigma squared, that term is basically one of\nthe terms in the likelihood",
    "start": "539720",
    "end": "547560"
  },
  {
    "text": "function of the fitted model. So you can see how this\ncriterion is basically manipulating the\nmaximum likelihood",
    "start": "547560",
    "end": "556280"
  },
  {
    "text": "value by adjusting it for a\npenalty for extra parameters.",
    "start": "556280",
    "end": "561845"
  },
  {
    "start": "561845",
    "end": "568079"
  },
  {
    "text": "Let's see. OK. Next topic is just\ntest for stationarity and non-stationarity. ",
    "start": "568080",
    "end": "575540"
  },
  {
    "text": "There's a famous test called\nthe Dickey-Fuller test, which",
    "start": "575540",
    "end": "580870"
  },
  {
    "text": "is essentially to evaluate\nthe time series to see if it's",
    "start": "580870",
    "end": "587550"
  },
  {
    "text": "consistent with a random walk. We know that we've been\ndiscussing sort of lecture after lecture how simple random\nwalks are non-stationary.",
    "start": "587550",
    "end": "596880"
  },
  {
    "text": "And the simple random walk is\ngiven by the model up here,",
    "start": "596880",
    "end": "602960"
  },
  {
    "text": "x_t equals phi\nx_(t-1) plus eta_t. If phi is equal\nto 1, right, that",
    "start": "602960",
    "end": "609345"
  },
  {
    "text": "is a non-stationary process. Well, in the\nDickey-Fuller test we want to test whether\nphi equals 1 or not.",
    "start": "609345",
    "end": "617640"
  },
  {
    "text": "And so we can fit the AR(1)\nmodel by least squares",
    "start": "617640",
    "end": "623090"
  },
  {
    "text": "and define the test statistic to\nbe the estimate of phi minus 1",
    "start": "623090",
    "end": "629110"
  },
  {
    "text": "over its standard error where\nphi is the least squares estimate and the standard error\nis the least squares estimate,",
    "start": "629110",
    "end": "636250"
  },
  {
    "text": "the standard error of that. If our coefficient phi is\nless than 1 in modulus,",
    "start": "636250",
    "end": "644970"
  },
  {
    "text": "so this really is a\nstationary series, then the estimate phi converges\nin distribution to a normal 0,",
    "start": "644970",
    "end": "656080"
  },
  {
    "text": "1 minus phi squared. And let's see.",
    "start": "656080",
    "end": "664920"
  },
  {
    "text": "But if phi is equal\nto 1, OK, so just",
    "start": "664920",
    "end": "669990"
  },
  {
    "text": "to recap that second\nto last bullet point is basically the property that\nwhen norm phi is less than 1,",
    "start": "669990",
    "end": "677580"
  },
  {
    "text": "then our least squares\nestimates are asymptotically normally distributed\nwith mean 0 if we",
    "start": "677580",
    "end": "686420"
  },
  {
    "text": "normalize by the true value,\nand 1 minus phi squared. If phi is equal to\n1, then it turns out",
    "start": "686420",
    "end": "694220"
  },
  {
    "text": "that phi hat is super-consistent\nwith rate 1 over t. Now this super-consistency\nis related",
    "start": "694220",
    "end": "704519"
  },
  {
    "text": "to statistics converging\nto some value,",
    "start": "704520",
    "end": "710900"
  },
  {
    "text": "and what is the rate of\nconvergence of those statistics to different values.",
    "start": "710900",
    "end": "716490"
  },
  {
    "text": "So in normal samples we can\nestimate sort of the mean",
    "start": "716490",
    "end": "724839"
  },
  {
    "text": "by the sample mean. And that will converge to\nthe true mean at rate of 1",
    "start": "724840",
    "end": "733960"
  },
  {
    "text": "over root n.  When we have a\nnon-stationary random walk,",
    "start": "733960",
    "end": "743069"
  },
  {
    "text": "the independent\nvariables matrix is such",
    "start": "743070",
    "end": "748890"
  },
  {
    "text": "that X transpose X over\nn grows without bound.",
    "start": "748890",
    "end": "755250"
  },
  {
    "text": "So if we have y is equal\nto X beta plus epsilon,",
    "start": "755250",
    "end": "762270"
  },
  {
    "text": "and beta hat is equal to\nX transpose X inverse X transpose y, the\nproblem is-- well,",
    "start": "762270",
    "end": "775940"
  },
  {
    "text": "and beta hat is\ndistributed as ultimately",
    "start": "775940",
    "end": "781080"
  },
  {
    "text": "normal with mean beta\nand variance sigma squared, X transpose X inverse.",
    "start": "781080",
    "end": "787430"
  },
  {
    "text": "This X transpose\nX inverse matrix, when the process is\nnon-stationary, a random walk,",
    "start": "787430",
    "end": "794750"
  },
  {
    "text": "it grows infinitely.  X transpose X over\nn actually grows",
    "start": "794750",
    "end": "803980"
  },
  {
    "text": "to infinity in magnitude just\nbecause it becomes unbounded.",
    "start": "803980",
    "end": "811279"
  },
  {
    "text": "Whereas X transpose X over\nn, when it's stationary is bounded. So anyway, so that leads\nto the super-consistency,",
    "start": "811280",
    "end": "819210"
  },
  {
    "text": "meaning that it converges\nto the value much faster and so this normal\ndistribution isn't appropriate.",
    "start": "819210",
    "end": "824589"
  },
  {
    "text": "And it turns out there's\nDickey-Fuller distribution for this test statistic,\nwhich is based on integrals",
    "start": "824590",
    "end": "830910"
  },
  {
    "text": "of diffusions and one\ncan read about that in the literature on unit roots\nand test for non-stationarity.",
    "start": "830910",
    "end": "841990"
  },
  {
    "text": "So there's a very rich\nliterature on this problem. If you're into econometrics,\nbasically a lot of time's",
    "start": "841990",
    "end": "852470"
  },
  {
    "text": "been spent in that\nfield on this topic. And the mathematics gets\nvery, very involved,",
    "start": "852470",
    "end": "862370"
  },
  {
    "text": "but good results are available. So let's see an application\nof some of these time series",
    "start": "862370",
    "end": "870230"
  },
  {
    "text": "methods. ",
    "start": "870230",
    "end": "877550"
  },
  {
    "text": "Let me go to the\ndesktop here if I can. In this supplemental material\nthat'll be on the website,",
    "start": "877550",
    "end": "886690"
  },
  {
    "text": "I just wanted you\nto be able to work with time series,\nreal time series and implement these\nautoregressive moving",
    "start": "886690",
    "end": "893250"
  },
  {
    "text": "average fits and understand\nbasically how things work. So in this, it introduces\nloading the R libraries",
    "start": "893250",
    "end": "903439"
  },
  {
    "text": "and Federal Reserve data into\nR, basically collecting it off the web. Creating weekly and monthly\ntime series from a daily series,",
    "start": "903440",
    "end": "911300"
  },
  {
    "text": "and it's a trivial thing to do,\nbut when you sit down and try to do it gets involved.",
    "start": "911300",
    "end": "916459"
  },
  {
    "text": "So there's some nice\ntools that are available. There's the ACF\nand the PACF, which",
    "start": "916460",
    "end": "922080"
  },
  {
    "text": "is the auto-correlation\nfunction and the partial auto-correlation\nfunction, which are",
    "start": "922080",
    "end": "929030"
  },
  {
    "text": "used for interpreting series. Then we conduct Dickey-Fuller\ntest for unit roots",
    "start": "929030",
    "end": "935350"
  },
  {
    "text": "and determine, evaluate\nstationarity, non-stationarity of the 10-year yield.",
    "start": "935350",
    "end": "942170"
  },
  {
    "text": "And then we evaluate\nstationarity and cyclicality",
    "start": "942170",
    "end": "948010"
  },
  {
    "text": "in the fitted autoregressive\nmodel of order 2 to monthly data.",
    "start": "948010",
    "end": "953350"
  },
  {
    "text": "And actually 1.7 there,\nthat cyclicality issue, relates to one of the\nproblems on the problem set",
    "start": "953350",
    "end": "959980"
  },
  {
    "text": "for time series,\nwhich is looking at, with second order\nautoregressive models,",
    "start": "959980",
    "end": "966029"
  },
  {
    "text": "is there cyclicality\nin the process? And then finally\nlooking at identifying",
    "start": "966030",
    "end": "972890"
  },
  {
    "text": "the best autoregressive model\nusing the AIC criterion. So let me just page through\nand show you a couple of plots",
    "start": "972890",
    "end": "981500"
  },
  {
    "text": "here. OK. Well, there's the\noriginal 10-year yield collected directly from\nthe Federal Reserve",
    "start": "981500",
    "end": "988170"
  },
  {
    "text": "website over a 10 year period. And, oh, here we go.",
    "start": "988170",
    "end": "994600"
  },
  {
    "text": "This is nice. OK. ",
    "start": "994600",
    "end": "1002579"
  },
  {
    "text": "OK. Let's see, this\nsection 1.4 conducts the Dickey-Fuller test.",
    "start": "1002580",
    "end": "1009930"
  },
  {
    "text": "And it basically\ndetermines that the p-value",
    "start": "1009930",
    "end": "1023080"
  },
  {
    "text": "for non-stationarity\nis not rejected. And so, with the augmented\nDickey-Fuller test,",
    "start": "1023080",
    "end": "1032819"
  },
  {
    "text": "the test statistic is computed. Its significance is\nevaluated by the distribution",
    "start": "1032819",
    "end": "1039849"
  },
  {
    "text": "for that statistic. And the p-value tells\nyou how extreme the value of the statistic is,\nmeaning how unusual is it.",
    "start": "1039849",
    "end": "1048910"
  },
  {
    "text": "The smaller the p-value, the\nmore unlikely the value is.",
    "start": "1048910",
    "end": "1053950"
  },
  {
    "text": "The p-value is what's\nthe likelihood of getting as extreme or more extreme a\nvalue of the test statistic,",
    "start": "1053950",
    "end": "1059690"
  },
  {
    "text": "and the test\nstatistic is evidence against the null hypothesis. So in this case the p-values\nrange basically 0.2726",
    "start": "1059690",
    "end": "1068850"
  },
  {
    "text": "for the monthly data, which\nsays that basically there",
    "start": "1068850",
    "end": "1080760"
  },
  {
    "text": "is evidence of a unit\nroot in the process. ",
    "start": "1080760",
    "end": "1086530"
  },
  {
    "text": "Let's see. OK. There's a section\non understanding partial auto-correlation\ncoefficients.",
    "start": "1086530",
    "end": "1092815"
  },
  {
    "text": " And let me just state what\nthe partial correlation",
    "start": "1092815",
    "end": "1100179"
  },
  {
    "text": "coefficients are. You have the\nauto-correlation functions, which are simply the\ncorrelations of the time",
    "start": "1100180",
    "end": "1105850"
  },
  {
    "text": "series with lags of its values. The partial\nauto-correlation coefficient is the correlation that's\nbetween the time series",
    "start": "1105850",
    "end": "1116640"
  },
  {
    "text": "and say, it's p-th lag that is\nnot explained by all lags lower",
    "start": "1116640",
    "end": "1122180"
  },
  {
    "text": "than p. So it's basically the\nincremental correlation of the time series variable with\nthe p-th lag after controlling",
    "start": "1122180",
    "end": "1130460"
  },
  {
    "text": "for the others. ",
    "start": "1130460",
    "end": "1135650"
  },
  {
    "text": "And then let's see. With this, in section\neight here there's",
    "start": "1135650",
    "end": "1141480"
  },
  {
    "text": "a function in R called ar, for\nautoregressive, which basically",
    "start": "1141480",
    "end": "1147220"
  },
  {
    "text": "will fit all autoregressive\nmodels up to a given order and provide diagnostic\nstatistics for that.",
    "start": "1147220",
    "end": "1154230"
  },
  {
    "text": "And here is a plot of the\nrelative AIC statistic for models of the monthly data.",
    "start": "1154230",
    "end": "1160640"
  },
  {
    "text": "And you can see that basically\nit takes all the AIC statistics and subtracts the smallest\none from all the others.",
    "start": "1160640",
    "end": "1168950"
  },
  {
    "text": "So one can see that according\nto the AIC statistic a model of order seven is\nsuggested for this treasury",
    "start": "1168950",
    "end": "1180110"
  },
  {
    "text": "yield data.  OK.",
    "start": "1180110",
    "end": "1186140"
  },
  {
    "text": "Then finally because these\nautoregressive models are implemented with\nregression models,",
    "start": "1186140",
    "end": "1192920"
  },
  {
    "text": "one can apply\nregression diagnostics that we had introduced earlier\nto look at those data as well.",
    "start": "1192920",
    "end": "1202180"
  },
  {
    "text": "All right. So let's go down now.",
    "start": "1202180",
    "end": "1207495"
  },
  {
    "start": "1207495",
    "end": "1214978"
  },
  {
    "text": "[INAUDIBLE] OK. ",
    "start": "1214978",
    "end": "1225770"
  },
  {
    "text": "[INAUDIBLE] Full screen. Here we go.",
    "start": "1225770",
    "end": "1231170"
  },
  {
    "text": "All right. ",
    "start": "1231170",
    "end": "1236700"
  },
  {
    "text": "So let's move on to the\ntopic of volatility modeling. ",
    "start": "1236700",
    "end": "1244350"
  },
  {
    "text": "The discussion in\nthis section is",
    "start": "1244350",
    "end": "1250289"
  },
  {
    "text": "going to begin with just\ndefining volatility. So we know what\nwe're talking about.",
    "start": "1250290",
    "end": "1256450"
  },
  {
    "text": "And then measuring volatility\nwith historical data",
    "start": "1256450",
    "end": "1261740"
  },
  {
    "text": "where we don't really apply sort\nof statistical models so much, but we're concerned with\njust historical measures",
    "start": "1261740",
    "end": "1267809"
  },
  {
    "text": "of volatility and\ntheir prediction. Then there are formal models. We'll introduce Geometric\nBrownian Motion, of course.",
    "start": "1267810",
    "end": "1274230"
  },
  {
    "text": "That's one of the standard\nmodels in finance. But also Poisson\njump-diffusions, which is an extension of\nGeometric Brownian Motion",
    "start": "1274230",
    "end": "1282240"
  },
  {
    "text": "to allow for discontinuities. And then there's a property\nof these Brownian motion",
    "start": "1282240",
    "end": "1288410"
  },
  {
    "text": "and jump-diffusion\nmodels which is models with independent increments. Basically you have disjoint\nincrements of the process,",
    "start": "1288410",
    "end": "1303620"
  },
  {
    "text": "basically are independent\nof each other, which is a key property when there's\ntime dependence in the models.",
    "start": "1303620",
    "end": "1311270"
  },
  {
    "text": "There can be time dependence\nactually in the volatility. And ARCH models were\nintroduced initially to try and capture that.",
    "start": "1311270",
    "end": "1317084"
  },
  {
    "text": "And were extended\nto GARCH models, and these are the\nsort of simplest cases of time-dependent\nvolatility models",
    "start": "1317084",
    "end": "1323530"
  },
  {
    "text": "that we can work\nwith and introduce. And in all of these the sort\nof mathematical framework",
    "start": "1323530",
    "end": "1331630"
  },
  {
    "text": "for defining these models\nand the statistical framework for estimating their parameters\nis going to be highlighted.",
    "start": "1331630",
    "end": "1338049"
  },
  {
    "text": "And while it's a\nvery simple setting in terms of what\nthese models are,",
    "start": "1338050",
    "end": "1344710"
  },
  {
    "text": "these issues that\nwe'll be covering relate to virtually all\nstatistical modeling as well.",
    "start": "1344710",
    "end": "1353200"
  },
  {
    "text": "So let's define volatility. OK. In finance it's defined as the\nannualized standard deviation",
    "start": "1353200",
    "end": "1360480"
  },
  {
    "text": "of the change in price or\nvalue of a financial security, or an index. So we're interested\nin the variability",
    "start": "1360480",
    "end": "1369630"
  },
  {
    "text": "of this process, a price\nprocess or a value process.",
    "start": "1369630",
    "end": "1375220"
  },
  {
    "text": "And we consider it on an\nannualized time scale. Now because of that, when\nyou talk about volatility",
    "start": "1375220",
    "end": "1383910"
  },
  {
    "text": "it really is meaningful to\ncommunicate, levels of 10%.",
    "start": "1383910",
    "end": "1390550"
  },
  {
    "text": "If you think of, at what level\ndo sort of absolute bond yields",
    "start": "1390550",
    "end": "1397500"
  },
  {
    "text": "vary over a year?  It's probably less than 5%.",
    "start": "1397500",
    "end": "1405120"
  },
  {
    "text": "Bond yields don't-- When you think of\ncurrencies, how much do those vary over a year.",
    "start": "1405120",
    "end": "1410860"
  },
  {
    "text": "Maybe 10%. With equity markets,\nhow do those vary? Well, maybe 30%, 40% or more.",
    "start": "1410860",
    "end": "1419700"
  },
  {
    "text": "With the estimation and\nprediction approaches, OK, these are what\nwe'll be discussing.",
    "start": "1419700",
    "end": "1426030"
  },
  {
    "text": "There's different cases. So let's go on to\nhistorical volatility.",
    "start": "1426030",
    "end": "1432830"
  },
  {
    "text": "In terms of computing\nthe historical volatility we'll be considering\nbasically a price",
    "start": "1432830",
    "end": "1439350"
  },
  {
    "text": "series of T plus 1 points. And then we can get\nT period returns",
    "start": "1439350",
    "end": "1446810"
  },
  {
    "text": "corresponding to\nthose prices, which is the difference in\nthe logs of the prices,",
    "start": "1446811",
    "end": "1452450"
  },
  {
    "text": "or the log of the\nprice relatives. So R_t is going to be\nthe return for the asset.",
    "start": "1452450",
    "end": "1458370"
  },
  {
    "text": "And one could use\nother definitions, like sort of the absolute\nreturn, not take logs.",
    "start": "1458370",
    "end": "1466340"
  },
  {
    "text": "It's convenient in much\nempirical analysis, I guess, to work with the\nlogs because if you sum",
    "start": "1466340",
    "end": "1474250"
  },
  {
    "text": "logs you get sort of\nlog of the product. And so total cumulative\nreturns can be computed easily",
    "start": "1474250",
    "end": "1481830"
  },
  {
    "text": "with sums of logs. But anyway, we'll work\nwith that scale for now.",
    "start": "1481830",
    "end": "1487140"
  },
  {
    "text": "OK. Now the process R_t, the\nreturn series process, is going to be assumed to\nbe covariance stationary,",
    "start": "1487140",
    "end": "1495399"
  },
  {
    "text": "meaning that it does\nhave a finite variance. And the sample estimate\nof that is just",
    "start": "1495400",
    "end": "1504899"
  },
  {
    "text": "given by the square root\nof the sample variance.",
    "start": "1504900",
    "end": "1510730"
  },
  {
    "text": "And we're also considering\nan unbiased estimate of that. ",
    "start": "1510730",
    "end": "1516360"
  },
  {
    "text": "And if we want to\nbasically convert these to annualized\nvalues so that we're",
    "start": "1516360",
    "end": "1522570"
  },
  {
    "text": "dealing with a\nvolatility, then if we have daily prices of\nwhich in financial markets",
    "start": "1522570",
    "end": "1528672"
  },
  {
    "text": "they're usually--\nin the US they're open roughly 252 days\na year on average. We multiply that sigma\nhat by 252 square root.",
    "start": "1528672",
    "end": "1537580"
  },
  {
    "text": "And for weekly, root 52, and\nroot 12 for monthly data.",
    "start": "1537580",
    "end": "1544110"
  },
  {
    "text": "So regardless of the\nperiodicity of our original data we can get them onto\nthat volatility scale.",
    "start": "1544110",
    "end": "1551700"
  },
  {
    "text": " Now in terms of\nprediction methods",
    "start": "1551700",
    "end": "1560960"
  },
  {
    "text": "that one can make with\nhistorical volatility,",
    "start": "1560960",
    "end": "1565980"
  },
  {
    "text": "and there's a lot of work\ndone in finance by people",
    "start": "1565980",
    "end": "1572230"
  },
  {
    "text": "who aren't sort of\ntrained as econometricians or statisticians, they basically\njust work with the data.",
    "start": "1572230",
    "end": "1578570"
  },
  {
    "text": "And there's a standard for\nrisk analysis called the risk",
    "start": "1578570",
    "end": "1583840"
  },
  {
    "text": "metrics approach, where the\napproach defines volatility",
    "start": "1583840",
    "end": "1590779"
  },
  {
    "text": "and volatility estimates,\nhistorical estimates, just using simple methodologies. And so that's just go\nthrough what those are here.",
    "start": "1590780",
    "end": "1599870"
  },
  {
    "text": "One can-- basically\nfor any period t,",
    "start": "1599870",
    "end": "1606940"
  },
  {
    "text": "one can define the\nsample volatility, just to be the sample standard\ndeviation of the period t",
    "start": "1606940",
    "end": "1613669"
  },
  {
    "text": "returns. And so with daily\ndata that might just be the square of\nthat daily return.",
    "start": "1613670",
    "end": "1620800"
  },
  {
    "text": "With monthly data it could be\nthe sample standard deviation of the returns over the\nmonth and with yearly it",
    "start": "1620800",
    "end": "1628240"
  },
  {
    "text": "would be the sample\nover the year. Also with intraday data, it\ncould be the sample standard",
    "start": "1628240",
    "end": "1635150"
  },
  {
    "text": "deviation over intraday periods\nof say, half hours or hours.",
    "start": "1635150",
    "end": "1642900"
  },
  {
    "text": "And the historical\naverage is simply the mean of those\nestimates, which",
    "start": "1642900",
    "end": "1650320"
  },
  {
    "text": "uses all the available data. One can consider the\nsimple moving average of these realized volatilities.",
    "start": "1650320",
    "end": "1658279"
  },
  {
    "text": "And so that basically is using\nthe last m, for some finite m,",
    "start": "1658280",
    "end": "1664330"
  },
  {
    "text": "values to average. And one could also consider\nan exponential moving average",
    "start": "1664330",
    "end": "1673296"
  },
  {
    "text": "of these sample\nvolatilities where we have-- our estimate of the\nvolatility is 1 minus beta",
    "start": "1673296",
    "end": "1682060"
  },
  {
    "text": "times the current\nperiod volatility plus beta times the\nprevious estimate.",
    "start": "1682060",
    "end": "1688549"
  },
  {
    "text": "And these exponential\nmoving averages are really very nice\nways to estimate",
    "start": "1688550",
    "end": "1695990"
  },
  {
    "text": "processes that change over time. And they're able to track\nthe changes quite well",
    "start": "1695990",
    "end": "1703440"
  },
  {
    "text": "and they will tend to\ncome up again and again. This exponential\nmoving average actually",
    "start": "1703440",
    "end": "1708880"
  },
  {
    "text": "uses all available data. And there can be discrete\nversions of those where",
    "start": "1708880",
    "end": "1714770"
  },
  {
    "text": "you say, well let's use not\nan equal weighted average like the simple moving\naverage, but let's use a geometric average of the last\nm values in an exponential way.",
    "start": "1714770",
    "end": "1724220"
  },
  {
    "text": "And that's the exponential\nweighted moving average that uses the last m. ",
    "start": "1724220",
    "end": "1734191"
  },
  {
    "text": "OK. There we go. ",
    "start": "1734191",
    "end": "1743108"
  },
  {
    "text": "OK.  Well, with these different\nmeasures of sample volatility,",
    "start": "1743109",
    "end": "1751870"
  },
  {
    "text": "one can basically build\nmodels to estimate them",
    "start": "1751870",
    "end": "1757610"
  },
  {
    "text": "with regression\nmodels and evaluate.",
    "start": "1757610",
    "end": "1763990"
  },
  {
    "text": "And in terms of the\nrisk metrics benchmark, they consider a variety\nof different methodologies",
    "start": "1763990",
    "end": "1770140"
  },
  {
    "text": "for estimating volatility. And sort of determine\nwhat methods are best for different kinds of\nfinancial instruments.",
    "start": "1770140",
    "end": "1778320"
  },
  {
    "text": "And different financial indexes. And there are different\nperformance measures",
    "start": "1778320",
    "end": "1784140"
  },
  {
    "text": "one can apply. Sort of mean squared\nerror of prediction, mean absolute error\nof prediction,",
    "start": "1784140",
    "end": "1791020"
  },
  {
    "text": "mean absolute prediction\nerror, and so forth to evaluate different\nmethodologies. And on the web you can actually\nlook at the technical documents",
    "start": "1791020",
    "end": "1800640"
  },
  {
    "text": "for risk metrics and they\ngo through these analyses and if your interest is in a\nparticular area of finance,",
    "start": "1800640",
    "end": "1806700"
  },
  {
    "text": "whether it's fixed income\nor equities, commodities, or currencies,\nreviewing their work",
    "start": "1806700",
    "end": "1813220"
  },
  {
    "text": "there is very\ninteresting because it does highlight different\naspects of those markets.",
    "start": "1813220",
    "end": "1820740"
  },
  {
    "text": "And it turns out that basically\nthe exponential moving average is generally a very good\nmethod for many instruments.",
    "start": "1820740",
    "end": "1830039"
  },
  {
    "text": "And the sort of discounting\nof the values over time",
    "start": "1830040",
    "end": "1838050"
  },
  {
    "text": "corresponds to having roughly\nbetween, I guess, a 45 and a 90 day period in\nestimating your volatility.",
    "start": "1838050",
    "end": "1845909"
  },
  {
    "text": "And in these approaches\nwhich are, I guess, they're a bit ad hoc.",
    "start": "1845910",
    "end": "1852929"
  },
  {
    "text": "There's the formalism. And defining them is\nbasically just empirically what has worked in the past.",
    "start": "1852930",
    "end": "1858750"
  },
  {
    "start": "1858750",
    "end": "1863760"
  },
  {
    "text": "Let's see.  While these things are\nad hoc, they actually",
    "start": "1863760",
    "end": "1872170"
  },
  {
    "text": "have been very, very effective. So let's move on to\nformal statistical models",
    "start": "1872170",
    "end": "1883970"
  },
  {
    "text": "of volatility. And the first class is-- model\nis the Geometric Brownian",
    "start": "1883970",
    "end": "1890740"
  },
  {
    "text": "Motion. So here we have basically\na stochastic differential",
    "start": "1890740",
    "end": "1897700"
  },
  {
    "text": "equation defining the model\nfor Geometric Brownian Motion. And Choongbum will be\ngoing in some detail",
    "start": "1897700",
    "end": "1904950"
  },
  {
    "text": "about stochastic\ndifferential equations, and stochastic calculus\nfor representing",
    "start": "1904950",
    "end": "1912299"
  },
  {
    "text": "different processes,\ncontinuous processes. And the formulation\nis basically looking",
    "start": "1912300",
    "end": "1920909"
  },
  {
    "text": "at increments of the price\nprocess S is equal to basically",
    "start": "1920910",
    "end": "1928470"
  },
  {
    "text": "a mu S of t, sort of a drift\nterm, plus a sigma S of t,",
    "start": "1928470",
    "end": "1934909"
  },
  {
    "text": "a multiple of d W\nof t, where sigma is the volatility of\nthe security price,",
    "start": "1934910",
    "end": "1941130"
  },
  {
    "text": "mu is the mean return\nper unit time, d W of t is the increment of a standard\nBrownian motion processor,",
    "start": "1941130",
    "end": "1949830"
  },
  {
    "text": "Wiener process. And this W process is\nsuch that it's increments,",
    "start": "1949830",
    "end": "1958210"
  },
  {
    "text": "basically the change in value\nof the process between two time points is normally\ndistributed, with mean 0",
    "start": "1958210",
    "end": "1966410"
  },
  {
    "text": "and variance equal to the\nlength of the interval.",
    "start": "1966410",
    "end": "1971720"
  },
  {
    "text": " And increments on disjoint\ntime intervals are independent.",
    "start": "1971720",
    "end": "1976770"
  },
  {
    "text": " And well, if you\ndivide both sides",
    "start": "1976770",
    "end": "1990809"
  },
  {
    "text": "of that equation by S of t then\nyou have d S of t over S of t",
    "start": "1990810",
    "end": "1996535"
  },
  {
    "text": "is equal to mu dt\nplus sigma d W of t. And so the increments d S\nof t normalized by S of t",
    "start": "1996535",
    "end": "2005495"
  },
  {
    "text": "are a standard Brownian motion\nwith drift mu and volatility sigma. ",
    "start": "2005495",
    "end": "2016200"
  },
  {
    "text": "Now with sample data\nfrom this process,",
    "start": "2016200",
    "end": "2024570"
  },
  {
    "text": "now suppose we have\nprices observed at times t_0 up to t_n.",
    "start": "2024570",
    "end": "2030820"
  },
  {
    "text": "And for now we're not going\nto make any assumptions about what those time increments\nare, what those times are.",
    "start": "2030820",
    "end": "2037950"
  },
  {
    "text": "They could be equally spaced. They could be unequally spaced. ",
    "start": "2037950",
    "end": "2043550"
  },
  {
    "text": "The returns, the log of the\nrelative price change from time",
    "start": "2043550",
    "end": "2050419"
  },
  {
    "text": "t_(j-1) to t_j are\nindependent random variables.",
    "start": "2050420",
    "end": "2055879"
  },
  {
    "text": "And they are independent. Their distribution is\nnormally distributed",
    "start": "2055880",
    "end": "2061800"
  },
  {
    "text": "with mean given by mu times the\nlength of the time increment,",
    "start": "2061800",
    "end": "2067330"
  },
  {
    "text": "and variance sigma squared times\nthe length of the increment. And these properties will\nbe covered by Choongbum",
    "start": "2067330",
    "end": "2075908"
  },
  {
    "text": "in some later lectures. So for now what we can\njust know that this is true",
    "start": "2075909",
    "end": "2081750"
  },
  {
    "text": "and apply this result.\nIf we fix various time points for the observation\nand compute returns this way.",
    "start": "2081750",
    "end": "2089129"
  },
  {
    "text": "If it's a Geometric\nBrownian Motion we know that this is the\ndistribution of the returns.",
    "start": "2089130",
    "end": "2095610"
  },
  {
    "text": "Now knowing that\ndistribution we can now engage in maximum\nlikelihood estimation.",
    "start": "2095610",
    "end": "2101619"
  },
  {
    "text": "OK. If the increments are\nall just equal to 1, so we're thinking\nof daily data, say.",
    "start": "2101620",
    "end": "2109140"
  },
  {
    "text": "Then the maximum likelihood\nestimates are simple. It's basically the sample mean\nand the sample variance with 1",
    "start": "2109140",
    "end": "2117570"
  },
  {
    "text": "over n instead of 1 over\nn minus 1 in the MLE's. If delta_j varies\nthen, well, that's",
    "start": "2117570",
    "end": "2126520"
  },
  {
    "text": "actually a case\nin the exercises. Now does anyone,\nin terms of, well,",
    "start": "2126520",
    "end": "2139100"
  },
  {
    "text": "in the class exercise the issue\nthat is important to think",
    "start": "2139100",
    "end": "2146730"
  },
  {
    "text": "about is if you consider a given\ninterval of time over which",
    "start": "2146730",
    "end": "2153640"
  },
  {
    "text": "we're observing this Geometric\nBrownian Motion process, if we increase the sampling\nrate of prices over a given",
    "start": "2153640",
    "end": "2163440"
  },
  {
    "text": "interval, how does that\nchange the properties of our estimates?",
    "start": "2163440",
    "end": "2169400"
  },
  {
    "text": "Basically, do we obtain\nmore accurate estimates of the underlying parameters?",
    "start": "2169400",
    "end": "2174450"
  },
  {
    "text": "And as you increase\nthe sampling frequency, it turns out that some\nparameters are estimated much,",
    "start": "2174450",
    "end": "2181829"
  },
  {
    "text": "much better and you\nget basically much lower standard errors\non those estimates.",
    "start": "2181830",
    "end": "2188730"
  },
  {
    "text": "With other parameters\nyou don't necessarily. And the exercise is\nto evaluate that.",
    "start": "2188730",
    "end": "2195140"
  },
  {
    "text": "Now another issue\nthat's important is the issue of sort of what\nis the appropriate time scale",
    "start": "2195140",
    "end": "2202550"
  },
  {
    "text": "for Geometric Brownian Motion. Right now we're\nthinking of, you collect",
    "start": "2202550",
    "end": "2208750"
  },
  {
    "text": "data, whatever the\nperiodicity is of the data is you think that's your period\nfor your Brownian Motion.",
    "start": "2208750",
    "end": "2214430"
  },
  {
    "text": "Let's evaluate that. Let me go to another example.",
    "start": "2214430",
    "end": "2221654"
  },
  {
    "start": "2221655",
    "end": "2228200"
  },
  {
    "text": "Let's see here. ",
    "start": "2228200",
    "end": "2233515"
  },
  {
    "text": "Yep. OK. Let's go control-minus here. ",
    "start": "2233515",
    "end": "2244830"
  },
  {
    "text": "OK. All right. ",
    "start": "2244830",
    "end": "2251026"
  },
  {
    "text": "Let's see. With this second\ncase study there was data on exchange rates,\nlooking for regime changes",
    "start": "2251026",
    "end": "2261200"
  },
  {
    "text": "in exchange rate relationships. And so we have data\nfrom that case study",
    "start": "2261200",
    "end": "2266559"
  },
  {
    "text": "on different foreign\nexchange rates. And here in the top panel\nI've graphed the euro/dollar",
    "start": "2266560",
    "end": "2277390"
  },
  {
    "text": "exchange rate from\nthe beginning of 1999 through just a few months ago.",
    "start": "2277390",
    "end": "2285369"
  },
  {
    "text": "And the second panel is a\nplot of the daily returns",
    "start": "2285370",
    "end": "2292830"
  },
  {
    "text": "for that series. And here is a histogram\nof those daily returns.",
    "start": "2292830",
    "end": "2301859"
  },
  {
    "text": "And a fit of the Gaussian\ndistribution for the daily",
    "start": "2301860",
    "end": "2308990"
  },
  {
    "text": "returns if our sort of\ntime scale is correct. Basically daily returns\nare normally distributed.",
    "start": "2308990",
    "end": "2317350"
  },
  {
    "text": "Days are disjoint in\nterms of the price change. And so they're independent\nand identically distributed",
    "start": "2317350",
    "end": "2325160"
  },
  {
    "text": "under the model. And they all have the\nsame normal distribution with mean mu and\nvariance sigma squared.",
    "start": "2325160",
    "end": "2332330"
  },
  {
    "text": " OK. This analysis assumes\nbasically that we're",
    "start": "2332330",
    "end": "2340000"
  },
  {
    "text": "dealing with trading days for\nthe appropriate time scale, the Geometric Brownian Motion. ",
    "start": "2340000",
    "end": "2349640"
  },
  {
    "text": "Let's see. One can ask, well, what\nif trading dates really",
    "start": "2349640",
    "end": "2355300"
  },
  {
    "text": "isn't the right time scale,\nbut it's more calendar time. The change in value\nover the weekends",
    "start": "2355300",
    "end": "2362060"
  },
  {
    "text": "maybe correspond to price\nchanges, or value changes over a longer period of time.",
    "start": "2362060",
    "end": "2368150"
  },
  {
    "text": "And so this model\nreally needs to be adjusted for that time scale.",
    "start": "2368150",
    "end": "2375270"
  },
  {
    "text": "The exercise that\nallows you to consider",
    "start": "2375270",
    "end": "2381190"
  },
  {
    "text": "different delta t's shows you\nwhat the maximum likelihood estimates-- you'll\nbe deriving maximum",
    "start": "2381190",
    "end": "2387429"
  },
  {
    "text": "likely estimates if we\nhave different definitions of time scale there. But if you apply the calendar\ntime scale to this euro,",
    "start": "2387429",
    "end": "2402992"
  },
  {
    "text": "let me just show you what\nthe different estimates are of the annualized mean return\nand the annualized volatility.",
    "start": "2402992",
    "end": "2409589"
  },
  {
    "text": "So if we consider trading days\nfor euro it's 10.25% or 0.1025.",
    "start": "2409590",
    "end": "2416030"
  },
  {
    "text": "If you consider clock time, it\nactually turns out to be 12.2%.",
    "start": "2416030",
    "end": "2422390"
  },
  {
    "text": "So depending on how\nyou specify the model you get a different\ndefinition of volatility here.",
    "start": "2422390",
    "end": "2428640"
  },
  {
    "text": "And it's important to\nbasically understand",
    "start": "2428640",
    "end": "2436170"
  },
  {
    "text": "sort of what the assumptions\nare of your model and whether perhaps things\nought to be different.",
    "start": "2436170",
    "end": "2447480"
  },
  {
    "text": "In stochastic modeling,\nthere's an area",
    "start": "2447480",
    "end": "2453700"
  },
  {
    "text": "called subordinated\nstochastic processes. And basically the idea is, if\nyou have a stochastic process",
    "start": "2453700",
    "end": "2464220"
  },
  {
    "text": "like Geometric Brownian Motion\nof simple Brownian motion, maybe you're observing that\non the wrong time scale.",
    "start": "2464220",
    "end": "2474005"
  },
  {
    "text": "You may fit the Geometric\nBrownian Motion model and it doesn't look right. But it could be that\nthere's a different time",
    "start": "2474005",
    "end": "2479740"
  },
  {
    "text": "scale that's appropriate. And it's really Brownian\nmotion on that time scale.",
    "start": "2479740",
    "end": "2484990"
  },
  {
    "text": "And so formally it's called\na subordinated stochastic process.",
    "start": "2484990",
    "end": "2490330"
  },
  {
    "text": "You have a different\ntime function for how to model the\nstochastic process.",
    "start": "2490330",
    "end": "2495970"
  },
  {
    "text": "And the evaluation of\nsubordinated stochastic processes leads to consideration\nof different time scales.",
    "start": "2495970",
    "end": "2503750"
  },
  {
    "text": "With, say, equity markets,\nand futures markets, sort of the volume of trading,\nsort of cumulative volume",
    "start": "2503750",
    "end": "2510987"
  },
  {
    "text": "of training might be really\nan appropriate measure of the real time scale. Because that's a\nmeasure of, in a sense,",
    "start": "2510987",
    "end": "2516819"
  },
  {
    "text": "information flow\ncoming into the market through the level of activity.",
    "start": "2516820",
    "end": "2521870"
  },
  {
    "text": "So anyway I wanted to highlight\nhow with different time scales you can get different results.",
    "start": "2521870",
    "end": "2528320"
  },
  {
    "text": "And so that's something\nto be evaluated. In looking at these\ndifferent models,",
    "start": "2528320",
    "end": "2533619"
  },
  {
    "text": "OK, these first few\ngraphs here show the fit of the normal model\nwith the trading day time scale.",
    "start": "2533620",
    "end": "2538880"
  },
  {
    "text": " Let's see. Those of you who've ever taken\na statistics class before,",
    "start": "2538880",
    "end": "2545340"
  },
  {
    "text": "or an applied statistics, may\nknow about normal q-q plots. Basically if you\nwant to evaluate",
    "start": "2545340",
    "end": "2553830"
  },
  {
    "text": "the consistency of\nthe returns here with a Gaussian\ndistribution, what we can do",
    "start": "2553830",
    "end": "2561619"
  },
  {
    "text": "is plot the observed\nordered, sorted returns",
    "start": "2561620",
    "end": "2569410"
  },
  {
    "text": "against what we would\nexpect the sorted returns to be if it were from\na Gaussian sample.",
    "start": "2569410",
    "end": "2576200"
  },
  {
    "text": "So under the Geometric\nBrownian Motion model the daily returns are a sample,\nindependent and identically",
    "start": "2576200",
    "end": "2584530"
  },
  {
    "text": "distributed random variable\nsampled from a Gaussian distribution. So the smallest return should\nbe consistent with the smallest",
    "start": "2584530",
    "end": "2591510"
  },
  {
    "text": "of the sample size n. And what's being plotted here\nis the theoretical quantiles",
    "start": "2591510",
    "end": "2598870"
  },
  {
    "text": "or percentiles versus\nthe actual ones. And one would expect that\nto lie along a straight line",
    "start": "2598870",
    "end": "2604930"
  },
  {
    "text": "if the theoretical quantiles\nwere well-predicting",
    "start": "2604930",
    "end": "2610099"
  },
  {
    "text": "the actual extreme values. What we see here is that as the\ntheoretical quantiles get high,",
    "start": "2610100",
    "end": "2617760"
  },
  {
    "text": "and it's in units of\nstandard deviation units, the realized sample\nreturns are in fact",
    "start": "2617760",
    "end": "2625080"
  },
  {
    "text": "much higher than would be\npredicted by the Gaussian distribution. And similarly, on\nthe low end side.",
    "start": "2625080",
    "end": "2632400"
  },
  {
    "text": "So there's a normal\nq-q plot that's used often in the\ndiagnostics of these models.",
    "start": "2632400",
    "end": "2637800"
  },
  {
    "text": "Then down here I've actually\nplotted a fitted percentile",
    "start": "2637800",
    "end": "2644910"
  },
  {
    "text": "distribution. Now what's been done here\nis if we modeled the series",
    "start": "2644910",
    "end": "2652470"
  },
  {
    "text": "as a series of Gaussian\nrandom variables then we can evaluate\nthe percentile",
    "start": "2652470",
    "end": "2664960"
  },
  {
    "text": "of the fitted Gaussian\ndistribution that was realized by every point. So if we have a return of say\nnegative 2%, what percentile",
    "start": "2664960",
    "end": "2678780"
  },
  {
    "text": "is the normal fit of that? ",
    "start": "2678780",
    "end": "2685720"
  },
  {
    "text": "And you can evaluate the\ncumulative distribution function of the fitted model at\nthat value to get that point.",
    "start": "2685720",
    "end": "2694750"
  },
  {
    "text": "And what should the\ndistribution of percentiles be for fitted percentiles if\nwe have a really good model?",
    "start": "2694750",
    "end": "2704410"
  },
  {
    "text": "OK. Well, OK. Let's think.",
    "start": "2704410",
    "end": "2709859"
  },
  {
    "text": "If you consider the 50th\npercentile you would expect,",
    "start": "2709860",
    "end": "2714890"
  },
  {
    "text": "I guess, 50% of the data to\nlie above the 50th percentile and 50% to lie below the\n50th percentile, right?",
    "start": "2714890",
    "end": "2721930"
  },
  {
    "text": "OK. Let's consider,\nhere I divided up into 100 bins\nbetween zero and one",
    "start": "2721930",
    "end": "2727839"
  },
  {
    "text": "so this bin is the\n99th percentile. ",
    "start": "2727840",
    "end": "2738630"
  },
  {
    "text": "How many observations\nwould you expect to find in between the\n99th and 100 percentile?",
    "start": "2738630",
    "end": "2745589"
  },
  {
    "text": " This is an easy question.",
    "start": "2745590",
    "end": "2751170"
  },
  {
    "text": "AUDIENCE: 1%. PROFESSOR: 1%. Right. And so in any of\nthese bins we would expect to see 1% if the\nGaussian model were fitting.",
    "start": "2751170",
    "end": "2761450"
  },
  {
    "text": "And what we see is that,\nwell, at the extremes",
    "start": "2761450",
    "end": "2766690"
  },
  {
    "text": "they're more extreme values. And actually inside there\nare some fewer values.",
    "start": "2766690",
    "end": "2773720"
  },
  {
    "text": "And actually this is exhibiting\na leptokurtic distribution for the actually\nrealized samples;",
    "start": "2773720",
    "end": "2780069"
  },
  {
    "text": "basically the middle\nof the distribution is a little thinner\nand it's compensated for by fatter tails.",
    "start": "2780070",
    "end": "2786660"
  },
  {
    "text": "But with this\nparticular model we can basically expect to\nsee a uniform distribution",
    "start": "2786660",
    "end": "2793900"
  },
  {
    "text": "of percentiles in this graph.",
    "start": "2793900",
    "end": "2799690"
  },
  {
    "text": "If we compare this with\na fit of the clock time",
    "start": "2799690",
    "end": "2806990"
  },
  {
    "text": "we actually see\nthat clock time does a bit of a better job at getting\nthe extreme values closer",
    "start": "2806990",
    "end": "2819490"
  },
  {
    "text": "to what we would\nexpect them to be. So in terms of being a better\nmodel for the returns process,",
    "start": "2819490",
    "end": "2827506"
  },
  {
    "text": "if we're concerned with\nthese extreme values, we're actually getting\na slightly better value with those.",
    "start": "2827506",
    "end": "2833720"
  },
  {
    "text": "So all right. Let's move on back to the notes.",
    "start": "2833720",
    "end": "2840890"
  },
  {
    "text": "And talk about the\nGarman-Klass Estimator.",
    "start": "2840890",
    "end": "2848410"
  },
  {
    "text": "So let me do this. ",
    "start": "2848410",
    "end": "2854625"
  },
  {
    "text": "All right. View full screen. ",
    "start": "2854625",
    "end": "2863040"
  },
  {
    "text": "OK. All right. So, OK.",
    "start": "2863040",
    "end": "2868090"
  },
  {
    "text": "The Garman-Klass\nEstimator is one where we consider the\nsituation where we actually",
    "start": "2868090",
    "end": "2875410"
  },
  {
    "text": "have much more information\nthan simply sort of closing prices at different intervals.",
    "start": "2875410",
    "end": "2881980"
  },
  {
    "text": "Basically all transaction\ndata's collected in a financial market. So really we have\nvirtually all of the data",
    "start": "2881980",
    "end": "2888150"
  },
  {
    "text": "available if we want\nit, or can pay for it. But let's consider\na case where we",
    "start": "2888150",
    "end": "2894270"
  },
  {
    "text": "expand upon just having\nclosing prices to having additional information over\nincrements of time that",
    "start": "2894270",
    "end": "2902190"
  },
  {
    "text": "include the open,\nhigh, and low price",
    "start": "2902190",
    "end": "2907230"
  },
  {
    "text": "over the different periods. ",
    "start": "2907230",
    "end": "2913500"
  },
  {
    "text": "So those of you who are\nfamiliar with bar data graphs that you see whenever you\nplot stock prices over periods",
    "start": "2913500",
    "end": "2921000"
  },
  {
    "text": "of weeks or months you'll\nbe familiar with having",
    "start": "2921000",
    "end": "2926870"
  },
  {
    "text": "seen those. Now the Garman-Klass\npaper addressed",
    "start": "2926870",
    "end": "2932550"
  },
  {
    "text": "how can we exploit this\nadditional information to improve upon our estimates\nof the close-to-close.",
    "start": "2932550",
    "end": "2940290"
  },
  {
    "text": "And so we'll just\nuse this notation. Well, let's make some\nassumptions and notation.",
    "start": "2940290",
    "end": "2946200"
  },
  {
    "text": "So we'll assume that mu is equal\nto 0 in our Geometric Brownian Motion model. So we don't have to\nworry about the mean.",
    "start": "2946200",
    "end": "2952190"
  },
  {
    "text": "We're just concerned\nwith volatility. We'll assume that the\nincrements are one for daily,",
    "start": "2952190",
    "end": "2958530"
  },
  {
    "text": "corresponding to daily. And we'll let little f,\nbetween zero and one,",
    "start": "2958530",
    "end": "2963640"
  },
  {
    "text": "correspond to the time of day\nat which the market opens.",
    "start": "2963640",
    "end": "2975130"
  },
  {
    "text": "So over a day, from\nday zero to day one at f we assume that\nthe market opens",
    "start": "2975130",
    "end": "2985420"
  },
  {
    "text": "and basically the Geometric\nBrownian Motion process",
    "start": "2985420",
    "end": "2990780"
  },
  {
    "text": "might have closed\non day zero here. So this would be C_0 and it\nmay have opened on day one",
    "start": "2990780",
    "end": "3000790"
  },
  {
    "text": "at this value. So this would be O_1.",
    "start": "3000790",
    "end": "3005990"
  },
  {
    "text": "Might have gone up\nand down and then closed here with the\nBrownian Motion process.",
    "start": "3005990",
    "end": "3014801"
  },
  {
    "text": "OK. This value here would\ncorrespond to the high value.",
    "start": "3014801",
    "end": "3022240"
  },
  {
    "text": "This value here would correspond\nto the low value on day one. And then the closing\nvalue here would be C_1.",
    "start": "3022240",
    "end": "3030839"
  },
  {
    "text": "So the model is we have this\nunderlying Brownian Motion process is actually working\nover continuous time,",
    "start": "3030840",
    "end": "3041220"
  },
  {
    "text": "but we just observe it over\nthe time when the markets open. And so it can move between when\nthe market closes and opens",
    "start": "3041220",
    "end": "3048839"
  },
  {
    "text": "on any given day and we have\nthe additional information. Instead of just the close, we\nalso have the high and low.",
    "start": "3048840",
    "end": "3056380"
  },
  {
    "text": "So let's look at how we might\nexploit that information to estimate volatility.",
    "start": "3056380",
    "end": "3063070"
  },
  {
    "text": "OK. Using data from the first period\nas we've graphed here, let's first just highlight what\nthe close-to-close return is.",
    "start": "3063070",
    "end": "3076369"
  },
  {
    "text": "And that basically\nis an estimate of the one-period variance. And so sigma hat 0 squared is\na single period squared return.",
    "start": "3076370",
    "end": "3085750"
  },
  {
    "text": " C_1 minus C_0 has a distribution\nwhich is normal with mean 0,",
    "start": "3085750",
    "end": "3093130"
  },
  {
    "text": "and variance sigma squared. And if we consider\nsquaring that, what's",
    "start": "3093130",
    "end": "3105429"
  },
  {
    "text": "the distribution of that? That's the square of a\nnormal random variable, which",
    "start": "3105429",
    "end": "3110482"
  },
  {
    "text": "is chi squared, but it's a\nmultiple of a chi squared. It's sigma squared times a chi\nsquared one random variable.",
    "start": "3110482",
    "end": "3117250"
  },
  {
    "text": "And with a chi squared\nrandom variable the expected value is 1.",
    "start": "3117250",
    "end": "3123150"
  },
  {
    "text": "The variance of a chi squared\nrandom variable is equal to 2.",
    "start": "3123150",
    "end": "3129140"
  },
  {
    "text": "So just knowing\nthose facts gives us the fact we have an unbiased\nestimate of the volatility",
    "start": "3129140",
    "end": "3137119"
  },
  {
    "text": "parameter sigma and the variance\nis 2 sigma to the fourth.",
    "start": "3137120",
    "end": "3145420"
  },
  {
    "text": "So that's basically\nthe precision of close-to-close returns.",
    "start": "3145420",
    "end": "3151510"
  },
  {
    "text": " Let's look at two\nother estimates. ",
    "start": "3151510",
    "end": "3160170"
  },
  {
    "text": "Basically the\nopen-to-close return, sigma_1 squared,\nnormalized by f,",
    "start": "3160170",
    "end": "3166950"
  },
  {
    "text": "the length of the interval f. So we have sigma_1 is equal\nto O_1 minus C_0 squared.",
    "start": "3166950",
    "end": "3182820"
  },
  {
    "text": "OK.  Actually why don't\nI just do this?",
    "start": "3182821",
    "end": "3187917"
  },
  {
    "text": "I'll just write down\na few facts and then you can see that the\nresults are clear. Basically O_1 minus C_0 is\ndistributed normal with mean 0",
    "start": "3187917",
    "end": "3197400"
  },
  {
    "text": "and variance f sigma squared. And C_1 minus O_1 is\ndistributed normal with mean 0",
    "start": "3197400",
    "end": "3205100"
  },
  {
    "text": "in variance 1 minus\nf sigma squared. OK. This is simply using the\nproperties of the diffusion",
    "start": "3205100",
    "end": "3211410"
  },
  {
    "text": "process over different\nperiods of time. So if we normalize\nthe squared values",
    "start": "3211410",
    "end": "3217070"
  },
  {
    "text": "by the length of\nthe interval we get estimates of the volatility.",
    "start": "3217070",
    "end": "3222150"
  },
  {
    "text": "And what's particularly\nsignificant about these\nestimates one and two",
    "start": "3222150",
    "end": "3229250"
  },
  {
    "text": "is that they're independent. So we actually\nhave two estimates",
    "start": "3229250",
    "end": "3234660"
  },
  {
    "text": "of the same\nunderlying parameter, which are independent.",
    "start": "3234660",
    "end": "3241089"
  },
  {
    "text": "And actually they both\nhave the same mean and they both have\nthe same variance.",
    "start": "3241090",
    "end": "3248359"
  },
  {
    "text": "So if we consider\na new estimate, which is basically\naveraging those two.",
    "start": "3248360",
    "end": "3255789"
  },
  {
    "text": "Then this new estimate has the\nsame-- is unbiased as well,",
    "start": "3255790",
    "end": "3261050"
  },
  {
    "text": "but it's variance is basically\nthe variance of this sum.",
    "start": "3261050",
    "end": "3266340"
  },
  {
    "text": "So it's 1/2 squared times\nthis variance plus 1/2 squared times this variance, which is\na half of the variance of each",
    "start": "3266340",
    "end": "3273640"
  },
  {
    "text": "of them. So this estimate\nhas lower variance than our close-to-close.",
    "start": "3273640",
    "end": "3280200"
  },
  {
    "text": "And we can define the efficiency\nof this particular estimate",
    "start": "3280200",
    "end": "3286560"
  },
  {
    "text": "relative to the\nclose-to-close estimate as 2. Basically we get\ndouble the precision.",
    "start": "3286560",
    "end": "3294175"
  },
  {
    "text": " Suppose you had the open,\nhigh, close for one day.",
    "start": "3294175",
    "end": "3301940"
  },
  {
    "text": "How many days of\nclose-to-close data would you need to have the\nsame variance as this estimate?",
    "start": "3301940",
    "end": "3308343"
  },
  {
    "start": "3308343",
    "end": "3313860"
  },
  {
    "text": "No. AUDIENCE: [INAUDIBLE]. Because of the three\ndata points [INAUDIBLE]. PROFESSOR: No. No.",
    "start": "3313860",
    "end": "3320590"
  },
  {
    "text": "Anyone else? One more. Four. OK.",
    "start": "3320590",
    "end": "3325760"
  },
  {
    "text": "Basically if the\nvariance is 1/2, basically to get the standard\ndeviation, or the variance",
    "start": "3325760",
    "end": "3336040"
  },
  {
    "text": "to be-- I'm sorry. The ratio of the\nvariance is two. So no.",
    "start": "3336040",
    "end": "3341430"
  },
  {
    "text": "So it actually is close to two. ",
    "start": "3341430",
    "end": "3346570"
  },
  {
    "text": "Let's see. Our 1/n is-- so it\nactually is two. OK. I was thinking standard\ndeviation units instead of squared units.",
    "start": "3346571",
    "end": "3352540"
  },
  {
    "text": "So I was trying to\nbe clever there. So it actually is\nbasically two days.",
    "start": "3352540",
    "end": "3359270"
  },
  {
    "text": "So sampling this\nwith this information gives you as much as two\ndays worth of information.",
    "start": "3359270",
    "end": "3365841"
  },
  {
    "text": "So what does that mean? Well, if you want\nsomething that's as efficient as daily\nestimates you'll need to look back one\nday instead of two days",
    "start": "3365842",
    "end": "3374390"
  },
  {
    "text": "to get the same efficiency\nwith the estimate. All right. ",
    "start": "3374390",
    "end": "3379869"
  },
  {
    "text": "The motivation for\nthe Garman-Klass paper was actually a paper\nwritten by Parkinson",
    "start": "3379870",
    "end": "3386035"
  },
  {
    "text": "in 1976, which dealt with using\nthe extremes of a Brownian",
    "start": "3386035",
    "end": "3391210"
  },
  {
    "text": "Motion to estimate the\nunderlying parameters. And when Choongbum talks about\nBrownian Motion a bit later,",
    "start": "3391210",
    "end": "3399590"
  },
  {
    "text": "I don't know if you'll\nderive this result, but in courses on\nstochastic processes one does derive properties\nof the maximum of a Brownian",
    "start": "3399590",
    "end": "3407340"
  },
  {
    "text": "Motion over a given\ninterval and the minimum. And it turns out\nthat if you look",
    "start": "3407340",
    "end": "3413349"
  },
  {
    "text": "at the difference between the\nhigh and low squared divided by 4 log 2, this is an\nestimate of the volatility",
    "start": "3413350",
    "end": "3422140"
  },
  {
    "text": "of the process. And the efficiency\nof this estimate turns out to be 5.2,\nwhich is better yet.",
    "start": "3422140",
    "end": "3432589"
  },
  {
    "text": "Well, Garman and Klass\nwere excited by that and wanted to find\neven better ones. So they wrote a paper that\nevaluated all different kinds",
    "start": "3432589",
    "end": "3442349"
  },
  {
    "text": "of estimates. And I encourage you\nto Google that paper and read it because\nit's very accessible.",
    "start": "3442350",
    "end": "3447480"
  },
  {
    "text": "And it sort of highlights the\nstatistical and probability issues associated\nwith these problems.",
    "start": "3447480",
    "end": "3453150"
  },
  {
    "text": "But what they did\nwas they derived the best analytic\nscale-invariant estimator, which has this sort of bizarre\ncombination of different terms,",
    "start": "3453150",
    "end": "3466130"
  },
  {
    "text": "but basically we're\nusing normalized values of the high, low, close\nnormalized by the open.",
    "start": "3466130",
    "end": "3472270"
  },
  {
    "text": "And they're able to get\nan efficiency of 7.4 with this combination.",
    "start": "3472270",
    "end": "3479390"
  },
  {
    "text": "Now scale-invariant estimates,\nin statistical theory,",
    "start": "3479390",
    "end": "3486700"
  },
  {
    "text": "they're different\nprinciples that guide the development of\ndifferent methodologies. And one kind of principle is\nissues of scale invariance.",
    "start": "3486700",
    "end": "3496420"
  },
  {
    "text": "If you're estimating a scale\nparameter, and in this case volatility is telling\nyou essentially",
    "start": "3496420",
    "end": "3502210"
  },
  {
    "text": "how large is the\nvariability of this process, if you were to say multiply your\noriginal data all by a given",
    "start": "3502210",
    "end": "3509750"
  },
  {
    "text": "constant, then a\nscale-invariant estimator should be such that your\nestimator changes in that case",
    "start": "3509750",
    "end": "3517560"
  },
  {
    "text": "only by that same scale factor. So sort of the\nestimator doesn't depend",
    "start": "3517560",
    "end": "3522710"
  },
  {
    "text": "on how you scale the data. So that's the notion\nof scale invariance.",
    "start": "3522710",
    "end": "3530230"
  },
  {
    "text": "The Garman-Klass paper\nactually goes to the nth degree and actually finds a\nparticular estimator",
    "start": "3530230",
    "end": "3536800"
  },
  {
    "text": "that has an efficiency\nof 8.4, which is really highly significant.",
    "start": "3536800",
    "end": "3542470"
  },
  {
    "text": "So if you are working\nwith a modeling process",
    "start": "3542470",
    "end": "3548349"
  },
  {
    "text": "where you believe that the\nunderlying parameters may be reasonably assumed\nto be constant",
    "start": "3548350",
    "end": "3556079"
  },
  {
    "text": "over short periods\nof time, well, over those short periods\nof time if you use",
    "start": "3556080",
    "end": "3561870"
  },
  {
    "text": "these extended\nestimators like this, you'll get much more\nprecise measures of the underlying parameters\nthan from just using",
    "start": "3561870",
    "end": "3568970"
  },
  {
    "text": "simple close-to-close data. All right. Let's introduce Poisson\nJump Diffusions.",
    "start": "3568970",
    "end": "3578790"
  },
  {
    "text": "With Poisson Jump\nDiffusions we have",
    "start": "3578790",
    "end": "3584480"
  },
  {
    "text": "basically a stochastic\ndifferential equation",
    "start": "3584480",
    "end": "3590560"
  },
  {
    "text": "for representing this model. And it's just like the\nGeometric Brownian Motion model,",
    "start": "3590560",
    "end": "3595750"
  },
  {
    "text": "except we have this additional\nterm, gamma sigma Z d pi of t.",
    "start": "3595750",
    "end": "3600930"
  },
  {
    "text": "Now that's a lot of\ndifferent variables, but essentially what\nwe're thinking about",
    "start": "3600930",
    "end": "3607369"
  },
  {
    "text": "is over time a Brownian Motion\nprocess is fully continuous.",
    "start": "3607370",
    "end": "3620300"
  },
  {
    "text": "There are basically no jumps in\nthis Brownian Motion process. In order to allow\nfor jumps, we assume",
    "start": "3620300",
    "end": "3627609"
  },
  {
    "text": "that there's some process pi of\nt, which is a Poisson process.",
    "start": "3627610",
    "end": "3633960"
  },
  {
    "text": "It's counting process that\ncounts when jumps occur, how many jumps have occurred.",
    "start": "3633960",
    "end": "3640160"
  },
  {
    "text": "So that might start\nat 0 at the value 0. Then if there's a jump\nhere it goes up by one.",
    "start": "3640160",
    "end": "3650680"
  },
  {
    "text": "And then if there's another\njump here, it goes up by one, and so forth.",
    "start": "3650680",
    "end": "3655890"
  },
  {
    "text": "And so the Poisson Jump\nDiffusion model says,",
    "start": "3655890",
    "end": "3662000"
  },
  {
    "text": "this diffusion\nprocess is actually going to experience\nsome shocks to it. Those shocks are\ngoing to be arriving",
    "start": "3662000",
    "end": "3668940"
  },
  {
    "text": "according to a Poisson process. If you've taken\nstochastic modeling you know that that's a sort\nof a purely random process.",
    "start": "3668940",
    "end": "3677925"
  },
  {
    "text": "Basically exponential\narrival rate of shocks occur. You can't predict them.",
    "start": "3677925",
    "end": "3683740"
  },
  {
    "text": "And when those\noccur, d pi of t is going to change from 0\nup to the unit increment.",
    "start": "3683740",
    "end": "3691690"
  },
  {
    "text": "So d pi of t is 1. And then we'll realize\ngamma sigma Z of t. So at this point we're\ngoing to have shocks.",
    "start": "3691690",
    "end": "3700975"
  },
  {
    "text": " Here this is going to be gamma\nsigma Z_1 And at this point,",
    "start": "3700975",
    "end": "3710900"
  },
  {
    "text": "maybe it's a negative\nshock, gamma sigma Z_2.",
    "start": "3710900",
    "end": "3717960"
  },
  {
    "text": "This is 0. And so with this overall\nprocess we basically",
    "start": "3717960",
    "end": "3723400"
  },
  {
    "text": "have a shift in the\ndiffusion, up or down, according to these values. And so this model allows for\nthe arrival of these processes",
    "start": "3723400",
    "end": "3731746"
  },
  {
    "text": "to be random according to\nthe Poisson distribution, and for the magnitude of the\nshocks to be random as well.",
    "start": "3731746",
    "end": "3737770"
  },
  {
    "text": " Now like the Geometric\nBrownian Motion model",
    "start": "3737770",
    "end": "3744740"
  },
  {
    "text": "this process sort of has\nindependent increments, which",
    "start": "3744740",
    "end": "3751350"
  },
  {
    "text": "helps with this estimation. One could estimate this\nmodel by maximum likelihood,",
    "start": "3751350",
    "end": "3759549"
  },
  {
    "text": "but it does get tricky\nin that basically over different increments\nof time the change",
    "start": "3759550",
    "end": "3765970"
  },
  {
    "text": "in the process corresponds\nto the diffusion increment, plus the sum of the\njumps that have occurred",
    "start": "3765970",
    "end": "3773550"
  },
  {
    "text": "over that same increment. And so the model ultimately\nis a Poisson mixture",
    "start": "3773550",
    "end": "3781170"
  },
  {
    "text": "of Gaussian distributions. And in order to evaluate this\nmodel, model's properties,",
    "start": "3781170",
    "end": "3791039"
  },
  {
    "text": "moment generating functions\ncan be computed rather directly with that. And so one can understand how\nthe moments of the process",
    "start": "3791040",
    "end": "3799650"
  },
  {
    "text": "vary with these different\nmodel parameters. The likelihood function is\na product of Poisson sums.",
    "start": "3799650",
    "end": "3808410"
  },
  {
    "text": "And there's a closed form\nfor the EM algorithm, which can be used to\nimplement the estimation",
    "start": "3808410",
    "end": "3814280"
  },
  {
    "text": "of the unknown parameters. And if you think about observing\na Poisson Jump Diffusion",
    "start": "3814280",
    "end": "3824430"
  },
  {
    "text": "process, if you knew\nwhere the jumps occurred,",
    "start": "3824430",
    "end": "3832069"
  },
  {
    "text": "so you knew where\nthe jumps occurred and how many there were\nper increment in your data,",
    "start": "3832070",
    "end": "3837080"
  },
  {
    "text": "then the maximum\nlikelihood estimation would all be very, very simple.",
    "start": "3837080",
    "end": "3843230"
  },
  {
    "text": "And because this sort\nof is a separation of the estimation of\nthe Gaussian parameters from the Poisson parameters.",
    "start": "3843230",
    "end": "3850940"
  },
  {
    "text": "When you haven't observed\nthose values then you need to deal with methods\nappropriate for missing data.",
    "start": "3850940",
    "end": "3858880"
  },
  {
    "text": "And the EM algorithm is a very\nfamous algorithm developed by the people up at Harvard,\nRubin, Laird, and Dempster,",
    "start": "3858880",
    "end": "3868730"
  },
  {
    "text": "which deals with, basically if\nthe problem is much simpler,",
    "start": "3868730",
    "end": "3875720"
  },
  {
    "text": "if you could posit there\nbeing unobserved variables",
    "start": "3875720",
    "end": "3881240"
  },
  {
    "text": "that you would observe,\nthen you sort of expand the problem to\ninclude your observed",
    "start": "3881240",
    "end": "3886960"
  },
  {
    "text": "data, plus the missing\ndata, in this case where the jumps have occurred.",
    "start": "3886960",
    "end": "3892310"
  },
  {
    "text": "And you then do\nconditional expectations of estimating those jumps.",
    "start": "3892310",
    "end": "3899849"
  },
  {
    "text": "And then assuming that\nthose jumps had those-- occurred with those frequencies,\nestimating the underlying",
    "start": "3899850",
    "end": "3906960"
  },
  {
    "text": "parameters. So the EM algorithm\nis very powerful and has extensive\napplications in all kinds",
    "start": "3906960",
    "end": "3914550"
  },
  {
    "text": "of different models. I'll put up on the\nwebsite a paper that I wrote with David\nPickard and his student",
    "start": "3914550",
    "end": "3924240"
  },
  {
    "text": "Arshad Zakaria, which goes\nthrough the maximum likelihood",
    "start": "3924240",
    "end": "3930470"
  },
  {
    "text": "methodology for this. But looking at that,\nyou can see how with an extended model,\nhow maximum likelihood gets",
    "start": "3930470",
    "end": "3939109"
  },
  {
    "text": "implemented and I think\nthat's useful to see.",
    "start": "3939110",
    "end": "3944390"
  },
  {
    "text": "All right. So let's turn next\nto ARCH models.",
    "start": "3944390",
    "end": "3951059"
  },
  {
    "text": "And OK. Just as a bit of motivation, the\nGeometric Brownian Motion model",
    "start": "3951060",
    "end": "3963250"
  },
  {
    "text": "and also the Poisson\nJump Diffusion model are models which assume\nthat volatility over time",
    "start": "3963250",
    "end": "3970750"
  },
  {
    "text": "is essentially stationary. And with the sort of independent\nincrements of those processes,",
    "start": "3970750",
    "end": "3980470"
  },
  {
    "text": "the volatility over\ndifferent increments is essentially the same. So the ARCH models\nwere introduced",
    "start": "3980470",
    "end": "3987650"
  },
  {
    "text": "to accommodate the\npossibility that there's time dependence in volatility.",
    "start": "3987650",
    "end": "3993025"
  },
  {
    "text": " And so let's see.",
    "start": "3993025",
    "end": "4000220"
  },
  {
    "start": "4000220",
    "end": "4007041"
  },
  {
    "text": "Let's see. Let me go. OK. At the very end, I'll go through\nan example showing that time",
    "start": "4007041",
    "end": "4015190"
  },
  {
    "text": "dependence with our\neuro/dollar exchange rates.",
    "start": "4015190",
    "end": "4024910"
  },
  {
    "text": "So the set up for this\nmodel is basically we look at the log of\nthe price relatives y_t",
    "start": "4024910",
    "end": "4033369"
  },
  {
    "text": "and we model the\nresiduals to not",
    "start": "4033370",
    "end": "4039610"
  },
  {
    "text": "be of constant volatility,\nbut to be multiples of sort",
    "start": "4039610",
    "end": "4047400"
  },
  {
    "text": "of white noise with mean 0\nand variance 1, where sigma_t",
    "start": "4047400",
    "end": "4052900"
  },
  {
    "text": "is given by this essentially\nARCH function, which",
    "start": "4052900",
    "end": "4058920"
  },
  {
    "text": "says that the volatility\nat a given period t is a weighted average\nof the squared residuals",
    "start": "4058920",
    "end": "4068020"
  },
  {
    "text": "over the last p lags. And so if there's a\nlarge residual then",
    "start": "4068020",
    "end": "4076400"
  },
  {
    "text": "that could persist and\nmake the next observation",
    "start": "4076400",
    "end": "4082809"
  },
  {
    "text": "have a large variance. And so this accommodates\nsome time dependence.",
    "start": "4082810",
    "end": "4090309"
  },
  {
    "text": "Now this model actually\nhas parameter constraints,",
    "start": "4090310",
    "end": "4098229"
  },
  {
    "text": "which are never a\nnice thing to have when you're fitting models.",
    "start": "4098229",
    "end": "4104410"
  },
  {
    "text": "In this case the parameters\nalpha_one through alpha_p all have to be positive.",
    "start": "4104410",
    "end": "4110278"
  },
  {
    "text": "And why do they\nhave to be positive? ",
    "start": "4110279",
    "end": "4116166"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PROFESSOR: Right. Variance is positive. So if any of these\nalphas were negative,",
    "start": "4116166",
    "end": "4122088"
  },
  {
    "text": "then there would be a\npossibility that under this model that you could\nhave negative volatility, which you can't. So if we estimate this\nmodel to estimate them",
    "start": "4122089",
    "end": "4135109"
  },
  {
    "text": "with the constraint that\nall these parameter values are non-negative.",
    "start": "4135109",
    "end": "4140979"
  },
  {
    "text": "So that does complicate\nthe estimation a bit.",
    "start": "4140979",
    "end": "4146528"
  },
  {
    "text": "In terms of understanding\nhow this process works",
    "start": "4146529",
    "end": "4152579"
  },
  {
    "text": "one can actually see how\nthe ARCH model implies",
    "start": "4152580",
    "end": "4157880"
  },
  {
    "text": "an autoregressive model for\nthe squared residuals, which",
    "start": "4157880",
    "end": "4163491"
  },
  {
    "text": "turns out to be useful. So the top line there\nis the ARCH model saying that the variance\nof the t period return",
    "start": "4163492",
    "end": "4171149"
  },
  {
    "text": "is this weighted average\nof the past residuals. And then if we simply add\na new variable u_t, which",
    "start": "4171149",
    "end": "4181229"
  },
  {
    "text": "is our squared residual minus\nits variance, to both sides",
    "start": "4181229",
    "end": "4191399"
  },
  {
    "text": "we get the next line, which says\nthat epsilon_t squared follows",
    "start": "4191399",
    "end": "4199020"
  },
  {
    "text": "an autoregression on itself,\nwith the u_t value being",
    "start": "4199020",
    "end": "4207230"
  },
  {
    "text": "the disturbance in\nthat autoregression. Now u_t, which is epsilon_t\nsquared minus sigma squared t,",
    "start": "4207230",
    "end": "4217230"
  },
  {
    "text": "what is the mean of that? The mean is 0.",
    "start": "4217230",
    "end": "4223219"
  },
  {
    "text": "So it's almost white noise. But its variance is maybe\ngoing to change over time.",
    "start": "4223220",
    "end": "4228920"
  },
  {
    "text": "So it's not sort of\nstandard white noise, but it basically\nhas expectation 0.",
    "start": "4228920",
    "end": "4234949"
  },
  {
    "text": "It's also conditional\nindependent, but there's some possible\nvariability there.",
    "start": "4234950",
    "end": "4240850"
  },
  {
    "text": "But what this implies\nis that there basically is an autoregressive\nmodel where we just have time-varying variances\nin the underlying process.",
    "start": "4240850",
    "end": "4250190"
  },
  {
    "text": "Now because of that\none can sort of quickly evaluate whether there's\nARCH structure in data",
    "start": "4250190",
    "end": "4257119"
  },
  {
    "text": "by simply fitting an\nautoregressive model to the squared residuals. And testing whether\nthat regression",
    "start": "4257120",
    "end": "4262750"
  },
  {
    "text": "is significant or not. And that formally is a\nLagrange multiplier test.",
    "start": "4262750",
    "end": "4268940"
  },
  {
    "text": "Some of the original papers by\nEngle go through that analysis. And the test statistic\nturns out to just",
    "start": "4268940",
    "end": "4277880"
  },
  {
    "text": "be the multiple of the r\nsquared for that regression fit.",
    "start": "4277880",
    "end": "4284620"
  },
  {
    "text": "And basically under,\nsay, a null hypothesis",
    "start": "4284620",
    "end": "4293690"
  },
  {
    "text": "that there isn't\nany ARCH structure, then this regression model\nshould have no predictability.",
    "start": "4293690",
    "end": "4300290"
  },
  {
    "text": "This ARCH model\nin the residuals, basically if there's no time\ndependence in those residuals,",
    "start": "4300290",
    "end": "4305920"
  },
  {
    "text": "that's evidence of there being\nan absence of ARCH structure.",
    "start": "4305920",
    "end": "4312620"
  },
  {
    "text": "And so under the null\nhypothesis of no ARCH structure that r squared statistic\nshould be small.",
    "start": "4312620",
    "end": "4319580"
  },
  {
    "text": "It turns out that sort of n\ntimes the r squared statistic with p variables\nis asymptotically",
    "start": "4319580",
    "end": "4328230"
  },
  {
    "text": "a chi-square distribution\nwith p degrees of freedom. So that's where that test\nstatistic comes into play.",
    "start": "4328230",
    "end": "4337760"
  },
  {
    "text": "And in implementing this, the\nfact that we were applying",
    "start": "4337760",
    "end": "4343179"
  },
  {
    "text": "essentially least squares\nwith the autoregression to implement this Lagrange\nmultiplier test, but we were",
    "start": "4343180",
    "end": "4350900"
  },
  {
    "text": "assuming, well,\nwe're not assuming, we're implicitly assuming the\nassumptions of Gauss-Markov",
    "start": "4350900",
    "end": "4355980"
  },
  {
    "text": "in fitting that. This corresponds to the notion\nof quasi-maximum likelihood",
    "start": "4355980",
    "end": "4362920"
  },
  {
    "text": "estimates for\nunknown parameters. And quasi-maximum\nlikelihood estimates",
    "start": "4362920",
    "end": "4371870"
  },
  {
    "text": "are used extensively in some\nstochastic volatility models. And so essentially situations\nwhere you sort of use",
    "start": "4371870",
    "end": "4379139"
  },
  {
    "text": "the normal approximation, or\nthe second order approximation,",
    "start": "4379140",
    "end": "4384180"
  },
  {
    "text": "to get your estimates,\nand they turn out to be consistent and decent.",
    "start": "4384180",
    "end": "4391930"
  },
  {
    "text": " All right.",
    "start": "4391930",
    "end": "4398590"
  },
  {
    "text": "Let's go to Maximum\nLikelihood Estimation. OK Maximum Likelihood\nEstimation basically",
    "start": "4398590",
    "end": "4403800"
  },
  {
    "text": "involves-- the hard part\nis defining the likelihood function, which is the\ndensity of the data given",
    "start": "4403800",
    "end": "4413700"
  },
  {
    "text": "the unknown parameters. In this case, the data are\nconditionally independent.",
    "start": "4413700",
    "end": "4420743"
  },
  {
    "start": "4420743",
    "end": "4430630"
  },
  {
    "text": "The joint density is the product\nof the density of y_t given the information at t minus 1.",
    "start": "4430630",
    "end": "4436570"
  },
  {
    "text": "So basically the joint\nprobability density is the density at each time\npoint conditional on the past,",
    "start": "4436570",
    "end": "4443479"
  },
  {
    "text": "and then the density times the\ndensity of the next time point conditional on the past. And those are all\nnormal random variables.",
    "start": "4443479",
    "end": "4449730"
  },
  {
    "text": "So these are the normal\nPDFs coming into play here. And so what we want\nto do is basically",
    "start": "4449730",
    "end": "4456170"
  },
  {
    "text": "maximize this\nlikelihood function subject to these constraints. ",
    "start": "4456170",
    "end": "4462040"
  },
  {
    "text": "And we already went\nthrough the fact that the alpha_i's have\nto be greater than zero. And it turns out you\nalso have to have",
    "start": "4462040",
    "end": "4469080"
  },
  {
    "text": "that the sum of the\nalphas is less than one. Now what would happen\nif the sum of the alphas",
    "start": "4469080",
    "end": "4475040"
  },
  {
    "text": "was not less than one?  AUDIENCE: [INAUDIBLE].",
    "start": "4475040",
    "end": "4481852"
  },
  {
    "text": "PROFESSOR: Right. And you basically could have\nthe process start diverging. Basically these\nautoregressions can explode.",
    "start": "4481852",
    "end": "4490570"
  },
  {
    "text": "So let's go through and see. ",
    "start": "4490570",
    "end": "4497580"
  },
  {
    "text": "Let's see.  Actually, we're going to\ngo to GARCH models next.",
    "start": "4497580",
    "end": "4506570"
  },
  {
    "text": "OK. Let's see. ",
    "start": "4506570",
    "end": "4517464"
  },
  {
    "text": "Let me just go\nback here a second. OK. Very good. OK. In the remaining few minutes\nlet me just introduce you",
    "start": "4517464",
    "end": "4523780"
  },
  {
    "text": "to the GARCH models. ",
    "start": "4523780",
    "end": "4536560"
  },
  {
    "text": "The GARCH model is\nbasically a series",
    "start": "4536560",
    "end": "4543960"
  },
  {
    "text": "of past values of the\nsquared volatilities, basically the q sum of\npast squared volatilities",
    "start": "4543960",
    "end": "4555860"
  },
  {
    "text": "for the equation for the\nvolatility sigma t squared. And so it may be\nthat very high order",
    "start": "4555860",
    "end": "4566300"
  },
  {
    "text": "ARCH models are\nactually important. Or very high order ARCH terms\nare found to be significant",
    "start": "4566300",
    "end": "4577409"
  },
  {
    "text": "when you fit ARCH models. It could be that\nmuch of that need",
    "start": "4577410",
    "end": "4585610"
  },
  {
    "text": "is explained by adding\nthese GARCH terms. And so let's just consider\na simple GARCH model where",
    "start": "4585610",
    "end": "4593660"
  },
  {
    "text": "we have only a first order ARCH\nterm and a first order GARCH",
    "start": "4593660",
    "end": "4600020"
  },
  {
    "text": "term. So we're basically\nsaying that this is a weighted average of\nthe previous volatility,",
    "start": "4600020",
    "end": "4608470"
  },
  {
    "text": "the new squared residual. And this is a very\nparsimonious representation",
    "start": "4608470",
    "end": "4618060"
  },
  {
    "text": "that actually ends up fitting\ndata quite, quite well. And there are various\nproperties of this GARCH model",
    "start": "4618060",
    "end": "4627710"
  },
  {
    "text": "which we'll go\nthrough next time, but I want to just\nclose this lecture",
    "start": "4627710",
    "end": "4634260"
  },
  {
    "text": "by showing you fits of the ARCH\nmodels and of this GARCH model",
    "start": "4634260",
    "end": "4640750"
  },
  {
    "text": "to the euro/dollar\nexchange rate process. So let's just look at that here.",
    "start": "4640750",
    "end": "4651080"
  },
  {
    "text": " OK. ",
    "start": "4651080",
    "end": "4660700"
  },
  {
    "text": "OK. With the euro/dollar\nexchange rate, actually there's\nthe graph here which",
    "start": "4660700",
    "end": "4667110"
  },
  {
    "text": "shows the\nauto-correlation function and the partial\nauto-correlation function",
    "start": "4667110",
    "end": "4674070"
  },
  {
    "text": "of the squared returns. So is there dependence in\nthese daily volatilities?",
    "start": "4674070",
    "end": "4682600"
  },
  {
    "text": "And basically these blue\nlines are plus or minus two standard deviations of\nthe correlation coefficient.",
    "start": "4682600",
    "end": "4692580"
  },
  {
    "text": "Basically we have highly\nsignificant auto-correlations and very highly significant\npartial auto-correlations,",
    "start": "4692580",
    "end": "4700900"
  },
  {
    "text": "which suggests if you're\nfamiliar with ARMA process that you would need a very\nhigh order ARMA process",
    "start": "4700900",
    "end": "4707820"
  },
  {
    "text": "to fit the squared residuals.",
    "start": "4707820",
    "end": "4714360"
  },
  {
    "text": "But this highlights how\nwith the statistical tools you can actually identify this\ntime dependence quite quickly.",
    "start": "4714360",
    "end": "4725610"
  },
  {
    "text": "And here's a plot of the ARCH\norder one model and the ARCH",
    "start": "4725610",
    "end": "4732530"
  },
  {
    "text": "order two model. And on each of\nthese I've actually drawn a solid line where the\nsort of constant variance model",
    "start": "4732530",
    "end": "4740940"
  },
  {
    "text": "would be. So ARCH is saying that we\nhave a lot of variability about that constant mean.",
    "start": "4740940",
    "end": "4749270"
  },
  {
    "text": "And a property, I guess,\nof these ARCH models is that they all have\nsort of a minimum value",
    "start": "4749270",
    "end": "4758340"
  },
  {
    "text": "for the volatility that\nthey're estimating. If you look at\nthe ARCH function, that alpha_0 now is--\nthe constant term",
    "start": "4758340",
    "end": "4766770"
  },
  {
    "text": "is basically the minimum\nvalue, which that can be. So there's a constraint\nsort of on the lower value.",
    "start": "4766770",
    "end": "4775790"
  },
  {
    "text": "Then here's an\nARCH(10) fit which,",
    "start": "4775790",
    "end": "4788080"
  },
  {
    "text": "it doesn't look like it sort of\nhas quite as much of a uniform lower bound in it, but one could\ngo on and on with higher order",
    "start": "4788080",
    "end": "4794940"
  },
  {
    "text": "ARCH terms, but rather than\ndoing that one can also fit just a GARCH(1,1) model.",
    "start": "4794940",
    "end": "4801030"
  },
  {
    "text": "And this is what it looks like. So basically the time varying\nvolatility in this process",
    "start": "4801030",
    "end": "4810170"
  },
  {
    "text": "is captured really,\nreally well with just this two-parameter GARCH model\nas compared with a high order",
    "start": "4810170",
    "end": "4816040"
  },
  {
    "text": "autoregressive model. And it sort of\nhighlights the issues with the Wold decomposition\nwhere a potentially infinite",
    "start": "4816040",
    "end": "4825090"
  },
  {
    "text": "order autoregressive\nmodel will effectively fit most time series.",
    "start": "4825090",
    "end": "4830769"
  },
  {
    "text": "Well, that's nice\nto know, but it's nice to have a parsimonious\nway of defining that infinite\ncollection of parameters",
    "start": "4830770",
    "end": "4836389"
  },
  {
    "text": "and with the GARCH model\na couple of parameters do a good job. And then finally here's\njust a simultaneous plot",
    "start": "4836390",
    "end": "4842820"
  },
  {
    "text": "of all those volatility\nestimates on the same graph. And so one can see the\nincreased flexibility basically",
    "start": "4842820",
    "end": "4853010"
  },
  {
    "text": "of the GARCH models compared to\nthe ARCH models for capturing time-varying volatility.",
    "start": "4853010",
    "end": "4859280"
  },
  {
    "text": "So all right. I'll stop there for today. And let's see.",
    "start": "4859280",
    "end": "4865080"
  },
  {
    "text": "Next Tuesday is a presentation\nfrom Morgan Stanley so.",
    "start": "4865080",
    "end": "4870290"
  },
  {
    "text": "And today's the last day to\nsign up for a field trip.",
    "start": "4870290",
    "end": "4875570"
  }
]