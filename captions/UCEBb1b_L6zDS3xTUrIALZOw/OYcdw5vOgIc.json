[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at oct.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "start": "17880",
    "end": "32680"
  },
  {
    "text": "PHILIPPE RIGOLLET: All right. So let's continue talking about\nmaximum likelihood estimation",
    "start": "32680",
    "end": "41180"
  },
  {
    "text": "in the context of generalized\nlinear models, all right? So in those generalized\nlinear models, what we spent most of the\npast lectures working on",
    "start": "41180",
    "end": "49730"
  },
  {
    "text": "is the conditional\ndistribution of Y given",
    "start": "49730",
    "end": "55680"
  },
  {
    "text": "X. And we're going\nto assume that this follows some distribution\nin the exponential family.",
    "start": "55680",
    "end": "68990"
  },
  {
    "text": " OK. And so what it means is that if\nwe look at the density, say--",
    "start": "68990",
    "end": "78170"
  },
  {
    "text": "or the PMF, but let's\ntalk about density to make things clearer--",
    "start": "78170",
    "end": "83440"
  },
  {
    "text": "we're going to assume that\nY given X has distribution.",
    "start": "83440",
    "end": "89020"
  },
  {
    "text": "So X is now fixed, because\nwe're conditioning on it. And it has a density, which\nis of this form, c of Yi phi.",
    "start": "89020",
    "end": "110240"
  },
  {
    "text": "OK. So this c, again, we don't\nreally need to think about it. This is something that's\ngoing to come up naturally",
    "start": "110240",
    "end": "115415"
  },
  {
    "text": "as soon as you need\nnormalization factor. And so here what it means, if\nthis is the distribution of Y",
    "start": "115415",
    "end": "122710"
  },
  {
    "text": "given Xi, so that's\nthe density of Yi",
    "start": "122710",
    "end": "128080"
  },
  {
    "text": "given Xi is equal to little xi. So if it's the conditional\ndistribution of Yi given Xi,",
    "start": "128080",
    "end": "135760"
  },
  {
    "text": "it should depend on xi somehow. And it does not appear\nto depend on Xi.",
    "start": "135760",
    "end": "140980"
  },
  {
    "text": "And here, the model is going\nto be on theta i, which is just",
    "start": "140980",
    "end": "146640"
  },
  {
    "text": "a function, theta i of Xi. And we're going to take\na very specific one. It's going to be a function\nof a linear form of the Xi.",
    "start": "146640",
    "end": "158430"
  },
  {
    "text": "So really we're going to take\nsomething which is of the form theta I, which is really\njust-- as theta does not depend on Xi--",
    "start": "158430",
    "end": "164080"
  },
  {
    "text": "of Xi transposed from beta. OK? So all these parts here,\nthis is really some modeling",
    "start": "164080",
    "end": "170800"
  },
  {
    "text": "assumptions that we're\nmaking once we've agreed on what distribution we want. OK.",
    "start": "170800",
    "end": "176240"
  },
  {
    "text": "So to do that, our\ngoal, of course, is going to try to\nunderstand what this beta is. There's one beta here.",
    "start": "176240",
    "end": "182810"
  },
  {
    "text": "What's important is that this\nbeta does not depend on i.",
    "start": "182810",
    "end": "192660"
  },
  {
    "text": "So if they observe\npairs Xi, Yi-- let's say I observe n of\nthem, i equals 1 to n--",
    "start": "192660",
    "end": "202459"
  },
  {
    "text": "the hope is that as a\naccumulate more and more pairs of this form where there's\nalways the same parameter that",
    "start": "202460",
    "end": "209360"
  },
  {
    "text": "links Xi to Yi, that's\nthis parameter beta, that I should have a better and\nbetter estimation of this beta.",
    "start": "209360",
    "end": "215960"
  },
  {
    "text": "Because it's always the same. And that's essentially\nwith couples all of our distribution. If I did not assume\nthis, then I could",
    "start": "215960",
    "end": "222620"
  },
  {
    "text": "have a different distribution\nfor each pair Xi given YI. And I would not be able\nto do any statistics.",
    "start": "222620",
    "end": "228830"
  },
  {
    "text": "Nothing would\naverage in the end. But here I have the\nsame beta, which means that I can hope to\ndo statistics and average",
    "start": "228830",
    "end": "234349"
  },
  {
    "text": "errors in them. OK. So I'm going to collect,\nso I'll come back to this.",
    "start": "234350",
    "end": "240540"
  },
  {
    "text": "But as usual in the\nlinear regression model, we're going to collect\nall our observations Yi.",
    "start": "240540",
    "end": "245740"
  },
  {
    "text": "So I'm going to assume\nthat they're real valued and that my Xi's\ntakes value in Rp",
    "start": "245740",
    "end": "250940"
  },
  {
    "text": "just like in the\nregression model. And I'm going to collect all my\nYi's into one big vector of Y",
    "start": "250940",
    "end": "256410"
  },
  {
    "text": "in our n and all my X's into\none big matrix in Rn times p",
    "start": "256410",
    "end": "263480"
  },
  {
    "text": "just like for the\nlinear regression model. All right, so, again, what\nI'm interested in here",
    "start": "263480",
    "end": "270020"
  },
  {
    "text": "is the conditional\ndistribution of Yi given Xi.",
    "start": "270020",
    "end": "276361"
  },
  {
    "text": "OK. I said this is\nthis distribution. When we're talking\nabout regression, I defined last time\nwhat the definition",
    "start": "276361",
    "end": "282470"
  },
  {
    "text": "of regression function was. It's just one\nparticular aspect of this conventional distribution.",
    "start": "282470",
    "end": "288240"
  },
  {
    "text": "It's the conditional\nexpectation of Yi given Xi. OK. And so this conditional\nexpectation,",
    "start": "288240",
    "end": "293780"
  },
  {
    "text": "I will denote it by-- so I talk about the\nconditional, I'm",
    "start": "293780",
    "end": "306949"
  },
  {
    "text": "going to call it,\nsay, mu i, which is the conditional\nexpectation of Yi given Xi equals some little xi, say.",
    "start": "306950",
    "end": "315217"
  },
  {
    "text": "You can forget about this\npart if you find it confusing. It really doesn't matter. It's just that this\nmeans that this",
    "start": "315217",
    "end": "321590"
  },
  {
    "text": "is a function of little xi. But if I only had the\nexpectation of Yi given big Xi,",
    "start": "321590",
    "end": "329885"
  },
  {
    "text": "this would be just a\nfunction of big Xi. So it really doesn't\nchange anything. It's just a matter of notation.",
    "start": "329885",
    "end": "336240"
  },
  {
    "text": "OK. So just forget about this part. But I'll just do\nit like that here.",
    "start": "336240",
    "end": "341720"
  },
  {
    "text": "OK. So this is just the conditional\nexpectation of Yi given Xi. It just depends on Xi, so\nI think it depends on i,",
    "start": "341720",
    "end": "350390"
  },
  {
    "text": "and so I will call it mu i. But I know that since in a\ncanonical exponential family,",
    "start": "350390",
    "end": "355880"
  },
  {
    "text": "then I know that mu i is\nactually B prime of theta i. OK.",
    "start": "355880",
    "end": "361010"
  },
  {
    "text": "So there's a 1 to 1 link\nbetween the canonical parameter of my exponential\nfamily and the mean mu",
    "start": "361010",
    "end": "367362"
  },
  {
    "text": "i, the conditional expectation. And the modeling assumption\nwe're going to make",
    "start": "367362",
    "end": "372630"
  },
  {
    "text": "is not directly-- remember, that was\nthe second aspect of the generalized linear model. We're not going to assume\nthat theta i itself directly",
    "start": "372630",
    "end": "380600"
  },
  {
    "text": "depends on Xi. We're going to\nassume that mu i has a particular dependence on\nXi through the link function.",
    "start": "380600",
    "end": "388190"
  },
  {
    "text": "So, again, we're\nback to modeling. So we have a link function g. ",
    "start": "388190",
    "end": "398020"
  },
  {
    "text": "And we assume that mu i\ndepends on Xi as follows.",
    "start": "398020",
    "end": "411840"
  },
  {
    "text": " g of mu i--",
    "start": "411840",
    "end": "418625"
  },
  {
    "text": "and remember, all g\ndoes for us is really map the space in which\nmu i lives, which",
    "start": "418625",
    "end": "425259"
  },
  {
    "text": "could be just the interval\n0, 1 to the entire real line, all right? And we're going to assume\nthat this thing that",
    "start": "425260",
    "end": "431379"
  },
  {
    "text": "lives in the real line is\njust Xi transpose beta. I should maybe put a small\none, Xi transpose beta.",
    "start": "431380",
    "end": "437235"
  },
  {
    "text": "OK?  So we're making, indeed,\nsome modeling assumption.",
    "start": "437235",
    "end": "443039"
  },
  {
    "text": "But compared to in the\nlinear regression model, we only assume that mu\ni was Xi transpose beta.",
    "start": "443040",
    "end": "449250"
  },
  {
    "text": "So if you want to\nmake a parallel between generalized\nlinear models and linear model is\nthe only difference",
    "start": "449250",
    "end": "455090"
  },
  {
    "text": "is that g is not the identity\nnecessarily in this case. And all the g does\nfor us is to just",
    "start": "455090",
    "end": "462759"
  },
  {
    "text": "make this thing\ncompatible, that those two things on the left and\nthe right of equality",
    "start": "462760",
    "end": "468100"
  },
  {
    "text": "live in the same space. So in a way, we're not making\na much bigger leap of faith",
    "start": "468100",
    "end": "476169"
  },
  {
    "text": "by assuming a linear model. The linear link is already here. We're just making things\ncompatible, all right?",
    "start": "476170",
    "end": "484020"
  },
  {
    "text": "And so it's always the\nsame link function.",
    "start": "484020",
    "end": "489139"
  },
  {
    "text": "So now if I want to\ngo back to beta-- right, because I'm going to\nwant to express my likelihood--",
    "start": "489140",
    "end": "496510"
  },
  {
    "text": "if I were to express my\nlikelihood from this, it would just be a\nfunction of theta, right? And so if I want to\nmaximize my likelihood,",
    "start": "496510",
    "end": "502960"
  },
  {
    "text": "I don't want to\nmaximize it in theta. I want to maximize it in beta. So if I can write my density\nas a function of beta,",
    "start": "502960",
    "end": "509910"
  },
  {
    "text": "then I will be able\nto write my likelihood as a function of\nbeta, and then talk about my maximum\nlikelihood estimator.",
    "start": "509910",
    "end": "515168"
  },
  {
    "text": "And so all they need to\ndo is to just say, OK, how do I replace theta by-- I know that theta is a\nfunction of beta, right?",
    "start": "515169",
    "end": "522820"
  },
  {
    "text": "I wrote it here. So the question is,\nwhat is this function? And I actually have\naccess to all of this.",
    "start": "522820",
    "end": "529399"
  },
  {
    "text": "So what I know is that theta-- right, so mu is\nb prime of theta,",
    "start": "529400",
    "end": "534990"
  },
  {
    "text": "which means that theta i\nis b prime inverse of mu i.",
    "start": "534990",
    "end": "542279"
  },
  {
    "text": "OK. So that's what we've got from\nthis derivative of the log likelihood equal to 0.",
    "start": "542280",
    "end": "547850"
  },
  {
    "text": "That give us this guy inverted. And now I know that mu i\nis g inverse of Xi beta.",
    "start": "547850",
    "end": "553670"
  },
  {
    "start": "553670",
    "end": "563790"
  },
  {
    "text": "So this composition of b\nprime inverse and g inverse is actually just the\ncomposition of g with b prime.",
    "start": "563790",
    "end": "570960"
  },
  {
    "start": "570960",
    "end": "578960"
  },
  {
    "text": "Everybody's comfortable\nwith this notation, the little circle? Any question about this? It just means that I\nfirst applied b prime.",
    "start": "578960",
    "end": "585999"
  },
  {
    "text": "Well, actually, it's D inverse. But if I look at a function\ng composed with b prime, I first applied the g b prime\nof x, is just g of b prime of x.",
    "start": "585999",
    "end": "599230"
  },
  {
    "text": "OK. And then I take the\ninverse of this function, which is first take g inverse,\nand then take b prime inverse.",
    "start": "599230",
    "end": "605644"
  },
  {
    "text": " OK. So now I have\neverywhere I saw theta,",
    "start": "605645",
    "end": "612810"
  },
  {
    "text": "now I see this function of beta. So I could technically\nplug that in.",
    "start": "612810",
    "end": "618010"
  },
  {
    "text": "Of course, it's a\nlittle painful to have to write g circle beta\nprime all the time. So I'm going to give\nthis guy a name.",
    "start": "618010",
    "end": "624699"
  },
  {
    "text": "And so you're just\ngoing to define",
    "start": "624700",
    "end": "629800"
  },
  {
    "text": "h, which is g b prime inverse\nso that theta i is simply",
    "start": "629800",
    "end": "639089"
  },
  {
    "text": "h of Xi transpose beta. OK. I could give it\na name, you know.",
    "start": "639090",
    "end": "646150"
  },
  {
    "text": "But let's just call\nthat the h function. ",
    "start": "646150",
    "end": "652170"
  },
  {
    "text": "And something which is\nnice about this h function is that if g is the\ncanonical link--",
    "start": "652170",
    "end": "661360"
  },
  {
    "text": " what is the canonical link? ",
    "start": "661360",
    "end": "672010"
  },
  {
    "text": "So what is it canonical to? A canonical link, it's canonical\nto a particular distribution",
    "start": "672010",
    "end": "679860"
  },
  {
    "text": "in the canonical\nexponential family, right? A canonical exponential family\nis completely characterized",
    "start": "679860",
    "end": "686490"
  },
  {
    "text": "by the function b. Which means that if I want to\ntalk about the canonical link,",
    "start": "686490",
    "end": "691830"
  },
  {
    "text": "all I need to tell you\nis how it depends on b. So what is g as a function of b?",
    "start": "691830",
    "end": "697069"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: b inverse.",
    "start": "697070",
    "end": "702485"
  },
  {
    "text": " b prime inverse, right?",
    "start": "702485",
    "end": "707490"
  },
  {
    "text": "So this is g is equal to\nb prime inverse, which",
    "start": "707490",
    "end": "712709"
  },
  {
    "text": "means that if g is composed\nwith b prime that means that this is just the identity.",
    "start": "712710",
    "end": "718120"
  },
  {
    "text": " So h is the identity.",
    "start": "718120",
    "end": "724020"
  },
  {
    "start": "724020",
    "end": "731700"
  },
  {
    "text": "So h of Xi transpose beta\nis simply Xi transpose beta.",
    "start": "731700",
    "end": "738870"
  },
  {
    "text": "And it's true that the way we\nintroduce the canonical link was just the function for\nwhich we model directly theta i",
    "start": "738870",
    "end": "746130"
  },
  {
    "text": "as Xi transpose beta, which\nwe can read off here, right? So theta i is simply\nXi transpose beta.",
    "start": "746130",
    "end": "754830"
  },
  {
    "text": "So now, for example, if I go\nback to my log-likelihood,",
    "start": "754830",
    "end": "763450"
  },
  {
    "text": "so if I look log-likelihood,\nthe log-likelihood",
    "start": "763450",
    "end": "771650"
  },
  {
    "text": "is sum of the log\nof the densities. So it's sum from i equal 1 to n\nof log of exponential Yi theta",
    "start": "771650",
    "end": "786740"
  },
  {
    "text": "i minus b theta i divided\nby phi plus c of Yi phi.",
    "start": "786740",
    "end": "794740"
  },
  {
    "text": "So this term does\nnot depend on theta. So I have two things.",
    "start": "794740",
    "end": "799940"
  },
  {
    "text": "First of all, the log\nand the exponential are going to cancel each other. And second, I actually\nknow that theta is just",
    "start": "799940",
    "end": "808230"
  },
  {
    "text": "a function of beta. And it has this form. Theta i is h of\nXi transpose beta. And that's my\nmodeling assumption.",
    "start": "808230",
    "end": "814740"
  },
  {
    "text": "So this is actually equal to the\nsum from i equal 1 to n of Yi.",
    "start": "814740",
    "end": "821300"
  },
  {
    "text": "And then here I'm going to\nwrite h of Xi transpose beta minus b of h of Xi transpose\nbeta divided by phi.",
    "start": "821300",
    "end": "834010"
  },
  {
    "text": "And then I have,\nagain, this function c of Yi phi, which\nagain won't matter.",
    "start": "834010",
    "end": "840710"
  },
  {
    "text": "Because when I'm going to\ntry to maximize this thing, this is just playing\nthe role of a constant that's shifting the\nentire function.",
    "start": "840710",
    "end": "847100"
  },
  {
    "text": "In particular, your max is\ngoing to be exactly what it was. OK?",
    "start": "847100",
    "end": "852480"
  },
  {
    "text": "So this thing is really\nnot going to matter for me. I'm keeping track of it. And actually, if you look\nhere, it's gone, right?",
    "start": "852480",
    "end": "860400"
  },
  {
    "text": "It's gone, because\nit does not matter. So let's just pretend\nit's not here,",
    "start": "860400",
    "end": "866865"
  },
  {
    "text": "because it won't\nmatter when I'm trying to maximize the likelihood. OK?",
    "start": "866865",
    "end": "873839"
  },
  {
    "text": "While it's here up to\nconstant term, it says. That's the constant term. ",
    "start": "873840",
    "end": "880340"
  },
  {
    "text": "All right, any question?  All I'm doing here is replacing\nmy likelihood as a function",
    "start": "880340",
    "end": "889120"
  },
  {
    "text": "of theta i's. So if I had one theta\ni per observation, again, this would not\nhelp me very much.",
    "start": "889120",
    "end": "896200"
  },
  {
    "text": "But if I assume that they\nare all linked together by saying that theta i is of\nthe form Xi transpose beta",
    "start": "896200",
    "end": "902560"
  },
  {
    "text": "or h of Xi transpose beta if I'm\nnot using the canonical link, then I can hope to\nmake some estimation.",
    "start": "902560",
    "end": "910510"
  },
  {
    "text": "And so, again, if I\nhave the canonical link, h is the identity. So I'm left only with Yi\ntimes Xi transpose beta.",
    "start": "910510",
    "end": "919060"
  },
  {
    "text": "And then I have b\nof Xi transpose beta and not b composed with h,\nbecause h is the identity,",
    "start": "919060",
    "end": "926590"
  },
  {
    "text": "which is fairly simple, right? Why is it simple? Well, let's actually focus\non this guy for one second.",
    "start": "926590",
    "end": "935489"
  },
  {
    "text": "So let me write it down, so we\nknow what we're talking about. ",
    "start": "935489",
    "end": "947300"
  },
  {
    "text": "So we just showed that the\nlog-likelihood when I use",
    "start": "947300",
    "end": "952420"
  },
  {
    "text": "the canonical link-- so that h is equal\nto the identity,",
    "start": "952420",
    "end": "957500"
  },
  {
    "text": "the log-likelihood\nactually takes the form ln. And it depends on\na bunch of stuff. But let's just make it\ndepend only on the parameter",
    "start": "957500",
    "end": "964085"
  },
  {
    "text": "that we care about,\nwhich is beta, all right? So this is of the form l of\nbeta and that's equal to what?",
    "start": "964085",
    "end": "969700"
  },
  {
    "text": "It's the sum from i equal 1 to n\nof Yi Xi transpose beta minus--",
    "start": "969700",
    "end": "976390"
  },
  {
    "text": "let me put the phi here. And then I'm going to have\nminus b of Xi transpose beta.",
    "start": "976390",
    "end": "984060"
  },
  {
    "text": " OK.",
    "start": "984060",
    "end": "989120"
  },
  {
    "text": "And phi we know is some\nknown positive term. So again, optimizing a\nfunction plus some constant",
    "start": "989120",
    "end": "997855"
  },
  {
    "text": "or optimizing of function\ntime as a constant, that's not going to\nchange much either. So it won't really matter to\nthink about whether this phi is",
    "start": "997856",
    "end": "1004350"
  },
  {
    "text": "here or not. But let's just think about\nwhat this function looks like. I'm trying to\nmaximize a function.",
    "start": "1004350",
    "end": "1010410"
  },
  {
    "text": "I'm trying to maximize\na log-likelihood. If it looked like this, that\nwould be a serious problem.",
    "start": "1010410",
    "end": "1017080"
  },
  {
    "text": "But we can do like a\nbasic, you know, back of the envelope guess of what\nthe variations of this function",
    "start": "1017080",
    "end": "1023700"
  },
  {
    "text": "is. This first term here is-- as a function of beta, what\nkind of function is it?",
    "start": "1023700",
    "end": "1031302"
  },
  {
    "text": "AUDIENCE: Linear. PHILIPPE RIGOLLET:\nIt's linear, right? This is just Xi transpose beta.",
    "start": "1031302",
    "end": "1036400"
  },
  {
    "text": "If I multiply beta\nby 2, I get twice. If I add something to\nbeta, it just gets added, so it's a linear\nfunction of beta.",
    "start": "1036400",
    "end": "1043030"
  },
  {
    "text": "And so this thing is\nboth convex and concave. In the one-dimensional\ncase-- so think about p",
    "start": "1043030",
    "end": "1048178"
  },
  {
    "text": "as being one-dimensional--\nso if beta is a one-dimensional\nthing, those are just the function that\nlooks like this, right?",
    "start": "1048179",
    "end": "1054070"
  },
  {
    "text": "Those are linear functions. They are both\nconvex and concave.",
    "start": "1054070",
    "end": "1060220"
  },
  {
    "text": "So this is not\ngoing to matter when it comes to the convexity\nof my overall function, because I'm just adding\nsomething which is just a line.",
    "start": "1060220",
    "end": "1066950"
  },
  {
    "text": "And so if I started with convex,\nit's going to stay convex. If I started with concave,\nit's going to stay concave. And if I started with\nsomething which is both,",
    "start": "1066950",
    "end": "1073376"
  },
  {
    "text": "it's going to stay\nboth, meaning neither. It cannot be both. Yeah. So if you're neither convex or\nconcave, adding this linear--",
    "start": "1073376",
    "end": "1080619"
  },
  {
    "text": "so this will not really matter. If I want to understand\nwhen my function looks like, I need to understand would\nb of Xi transpose beta does.",
    "start": "1080619",
    "end": "1087540"
  },
  {
    "text": "Begin, the Xi transpose beta-- no impact. It's a linear function. In terms of convexity, it's\nnot going to play any role.",
    "start": "1087540",
    "end": "1095220"
  },
  {
    "text": "So I really need to understand\nwhat my function b looks like. What do we know about b again? ",
    "start": "1095220",
    "end": "1102930"
  },
  {
    "text": "So we know that b prime of\ntheta is equal to mu, right?",
    "start": "1102930",
    "end": "1110610"
  },
  {
    "text": "Well, the mean of\na random variable in a canonical\nexponential family",
    "start": "1110610",
    "end": "1116069"
  },
  {
    "text": "can be a positive\nor negative number. This really does not\ntell me anything. That can be really anything.",
    "start": "1116069",
    "end": "1121289"
  },
  {
    "text": "However, if I look at the\nsecond the derivative of b,",
    "start": "1121290",
    "end": "1128080"
  },
  {
    "text": "I know that this is what? This is the variance\nof Y divided by phi.",
    "start": "1128080",
    "end": "1139190"
  },
  {
    "text": "That was my\ndispersion parameter. The variance was equal to\nphi times b prime prime. So we know that if\ntheta is not degenerate,",
    "start": "1139190",
    "end": "1146630"
  },
  {
    "text": "meaning that the\ndensity does not take value infinity\nat only one point, this thing is actually positive.",
    "start": "1146630",
    "end": "1152570"
  },
  {
    "text": "And clearly, when\nyou have something that looks like this, unless you\nhave some crazy stuff happening with phi being equal to 0 or\nanything that's not normal,",
    "start": "1152570",
    "end": "1160755"
  },
  {
    "text": "then you will see that\nyou're not degenerate. So this think is\nstrictly positive. And we've said several times\nthat if b prime prime is",
    "start": "1160755",
    "end": "1167059"
  },
  {
    "text": "positive, then that means that's\nthe derivative of b prime,",
    "start": "1167060",
    "end": "1172080"
  },
  {
    "text": "meaning that b\nprime is increasing. And b prime is\nincreasing is just the same thing as saying\nthat b is convex, All right?",
    "start": "1172080",
    "end": "1179299"
  },
  {
    "text": "So that implies that\nb is strictly convex.",
    "start": "1179300",
    "end": "1184850"
  },
  {
    "text": "And the strictly\ncomes from the fact that this is a strict sign.",
    "start": "1184850",
    "end": "1190180"
  },
  {
    "text": "Well, I should not do that,\nbecause now it's no longer. So it's just a\nstrict sign, meaning that the function, this\nis not strictly convex,",
    "start": "1190180",
    "end": "1196890"
  },
  {
    "text": "because it's linear. Strictly convex\nmeans there's always some curvature everywhere.",
    "start": "1196890",
    "end": "1202479"
  },
  {
    "text": "So now I have this thing that's\nlinear minus something that's convex. Something that's negative,\nsomething convex, is concave.",
    "start": "1202479",
    "end": "1210240"
  },
  {
    "text": "So this thing is\nlinear plus concave. So it is concave. So I know just by looking at\nthis that ln of beta, which,",
    "start": "1210240",
    "end": "1218279"
  },
  {
    "text": "of course, is something\nthat lives in Rp, but if I saw it living in\nR1 it would look like this.",
    "start": "1218280",
    "end": "1223940"
  },
  {
    "text": "And if I saw it\nliving in R2, it would look like a dome like this. And the fact that\nit's strict is also",
    "start": "1223940",
    "end": "1230290"
  },
  {
    "text": "telling me that it is\nactually a unique maximizer. So there's unique maximizer\nin Xi transpose beta,",
    "start": "1230290",
    "end": "1237400"
  },
  {
    "text": "but not in beta necessarily. We're going to need extra\nassumptions for this. OK. So this is what I say here.",
    "start": "1237400",
    "end": "1244080"
  },
  {
    "text": "The log-likelihood\nis strictly concave. And so as a consequence under\nextra assumptions on the Xi",
    "start": "1244080",
    "end": "1249175"
  },
  {
    "text": "is because, of course, if the\nXi's are all the same, right?",
    "start": "1249175",
    "end": "1255240"
  },
  {
    "text": "So if the entries of Xi's-- so if Xi is equal\nto 1, 1, 1, 1, 1,",
    "start": "1255240",
    "end": "1261856"
  },
  {
    "text": "then Xi transpose beta is\njust the sum of the betas. And of the beta i's, I will be\nstrictly concaving those guys,",
    "start": "1261857",
    "end": "1270300"
  },
  {
    "text": "but certainly not in\nthe individual entries. OK. So I need extra thing on my\nXi, so that this happens,",
    "start": "1270300",
    "end": "1277320"
  },
  {
    "text": "just like we needed\nthe matrix capital X in the linear regression\ncase to be a full rank,",
    "start": "1277320",
    "end": "1283060"
  },
  {
    "text": "so we could actually\nidentify would beta was. OK. It's going to be\nexactly the same thing. ",
    "start": "1283060",
    "end": "1292350"
  },
  {
    "text": "So here, this is when we\nhave this very specific parametrization. And the question is--",
    "start": "1292350",
    "end": "1299000"
  },
  {
    "text": "but it may not be the case\nif we change the parameter beta into something else. OK. So here, the fact that we use\nthe canonical link, et cetera,",
    "start": "1299000",
    "end": "1306800"
  },
  {
    "text": "everything actually works\nreally to our advantage, so that everything\nbecomes strictly concave. And we know exactly\nwhat's happening.",
    "start": "1306800",
    "end": "1313320"
  },
  {
    "text": " All right, so I understand\nI went a bit fast",
    "start": "1313321",
    "end": "1318470"
  },
  {
    "text": "on playing with convex\nand concave functions. This is not the purpose. You know, I could\nspend a lecture",
    "start": "1318470",
    "end": "1324590"
  },
  {
    "text": "telling you, oh, if I add\ntwo concave functions, then the result remains concave. If I had a concave and\na strictly concave,",
    "start": "1324590",
    "end": "1331500"
  },
  {
    "text": "then the result still\nremains strictly concave. And we could spend\ntime proving this. This was just for you\nto get an intuition",
    "start": "1331500",
    "end": "1337850"
  },
  {
    "text": "as to why this is correct. But we don't really have time\nto go into too much detail. One thing you can do--",
    "start": "1337850",
    "end": "1343971"
  },
  {
    "text": "a strictly concave function,\nif it's in one dimension, all I need to have is that the\nsecond derivative is strictly",
    "start": "1343971",
    "end": "1352010"
  },
  {
    "text": "negative, right? That's a strictly\nconcave function. That was the analytic definition\nwe had for strict concavity.",
    "start": "1352010",
    "end": "1358340"
  },
  {
    "text": "So if this was in\none dimension, it would look like this,\nYi times Xi times beta.",
    "start": "1358340",
    "end": "1365669"
  },
  {
    "text": "Now, beta is just one number. And then I would have\nminus beta Xi times b.",
    "start": "1365670",
    "end": "1372500"
  },
  {
    "text": "And this is all over phi. You take second derivatives. The fact that this is linear in\nbeta, this is going to go away.",
    "start": "1372500",
    "end": "1379310"
  },
  {
    "text": "And here, I'm just going\nto be left with minus-- so if I take the second\nderivative with respect",
    "start": "1379310",
    "end": "1387900"
  },
  {
    "text": "to beta, this is going to be\nequal to minus b prime prime Xi",
    "start": "1387900",
    "end": "1393490"
  },
  {
    "text": "beta times Xi squared\ndivided by phi.",
    "start": "1393490",
    "end": "1398520"
  },
  {
    "text": "So this is clearly positive. If Xi is 0, this is degenerate,\nso I would not get it.",
    "start": "1398520",
    "end": "1405126"
  },
  {
    "text": "Then I have the\nsecond derivative of b prime, which\nI know is positive, because of the variance\nthing that I have here,",
    "start": "1405126",
    "end": "1410840"
  },
  {
    "text": "divided by phi. And so that would all be fine. That's for one dimension. If I wanted to do this\nin higher dimensions,",
    "start": "1410840",
    "end": "1417210"
  },
  {
    "text": "I would have to say\nthat the Hessian is a positive definite matrix. And that's maybe a bit\nbeyond what this course is.",
    "start": "1417210",
    "end": "1424351"
  },
  {
    "start": "1424352",
    "end": "1430760"
  },
  {
    "text": "So in the rest of\nthis chapter, I will do what I did\nnot do when we talked",
    "start": "1430760",
    "end": "1436530"
  },
  {
    "text": "about maximum likelihood. And what we're\ngoing to do is we're going to actually show how to\ndo this maximization, right?",
    "start": "1436530",
    "end": "1443130"
  },
  {
    "text": "So here, we know that\nthe function is concave. But what it looks\nlike specifically",
    "start": "1443130",
    "end": "1448890"
  },
  {
    "text": "depends on what b is. And for different b's, I'm going\nto have different things to do,",
    "start": "1448890",
    "end": "1455562"
  },
  {
    "text": "Just like when I was talking\nabout maximum likelihood estimation, if it had a concave\nlog-likelihood function,",
    "start": "1455562",
    "end": "1462120"
  },
  {
    "text": "I could optimize it. But depending on\nwhat the function is, I would actually\nneed some algorithms that may be working better on\nsome functions than others.",
    "start": "1462120",
    "end": "1469400"
  },
  {
    "text": "Now, here I don't\nhave random things. I have the b is the\ncumulant generating",
    "start": "1469400",
    "end": "1474960"
  },
  {
    "text": "function of a canonical\nexponential family. And there is a way for me\nto sort of leverage that.",
    "start": "1474960",
    "end": "1482280"
  },
  {
    "text": "So not only is there the\nb part, but there's also the linear part. And if I start\ntrying to use that,",
    "start": "1482280",
    "end": "1489000"
  },
  {
    "text": "I'm actually going to be\nable to devise very specific optimization algorithms. And the way I'm going\nto be able to do this",
    "start": "1489000",
    "end": "1494930"
  },
  {
    "text": "is by thinking of\nsimple black box optimization to which I can\nactually technically feed any function.",
    "start": "1494930",
    "end": "1500434"
  },
  {
    "text": "But it's going to turn\nout that the iterations of this iterative\nalgorithms are going to look very\nfamiliar when we just",
    "start": "1500434",
    "end": "1510450"
  },
  {
    "text": "plug in the particular values\nof b, of the log-likelihood that we have for this problem.",
    "start": "1510450",
    "end": "1515500"
  },
  {
    "text": "And so the three methods we're\ngoing to talk about going from more black box-- meaning you can\nbasically stuff in any function",
    "start": "1515500",
    "end": "1522199"
  },
  {
    "text": "that's going to work, any\nconcave function that's going to work, all the way to\nthis is working specifically",
    "start": "1522199",
    "end": "1527850"
  },
  {
    "text": "for generalized linear models-- are Newton-Raphson method. Who's already heard about\nthe Newton-Raphson method?",
    "start": "1527850",
    "end": "1535100"
  },
  {
    "text": "So there's probably some people\nactually learned this algorithm",
    "start": "1535100",
    "end": "1540929"
  },
  {
    "text": "without even knowing the\nword algorithm, right? It's a function. Typically, it's supposed to\nbe finding roots of functions.",
    "start": "1540930",
    "end": "1546930"
  },
  {
    "text": "But finding the root of a\nfunction of the derivative is the same as finding\nthe minimum of a function.",
    "start": "1546930",
    "end": "1553150"
  },
  {
    "text": "So that's the first\nblack box method. I mean, it's pretty old. And then there's\nsomething that's",
    "start": "1553150",
    "end": "1559320"
  },
  {
    "text": "very specific to what we're\ndoing, which is called-- so this Newton-Raphson\nmethod is going to involve the Hessian\nof our log-likelihood.",
    "start": "1559320",
    "end": "1566820"
  },
  {
    "text": "And since we know\nsomething about the Hessian for a particular problem,\nwe're going to be able move one to Fisher-scoring.",
    "start": "1566820",
    "end": "1573030"
  },
  {
    "text": "And the word Fisher here\nis actually exactly coming from Fisher information. So the Hessian is going to\ninvolve the Fisher information.",
    "start": "1573030",
    "end": "1580450"
  },
  {
    "text": "And finally, we will talk about\niteratively re-weighted least",
    "start": "1580450",
    "end": "1585600"
  },
  {
    "text": "squares. And that's not for any function. It's really when\nwe're trying to use the fact that there is this\nlinear dependence on the Xi.",
    "start": "1585600",
    "end": "1595075"
  },
  {
    "text": "And this is essentially going\nto tell us, well, you know, you can use least squares\nfor linear regression. Here, you can use least\nsquares, but locally,",
    "start": "1595076",
    "end": "1601530"
  },
  {
    "text": "and you have to iterate. OK. And this last part is\nessentially a trick by statisticians\nto be able to solve",
    "start": "1601530",
    "end": "1609060"
  },
  {
    "text": "the Newton-Raphson\nupdates without actually having a dedicated\nsoftware for this,",
    "start": "1609060",
    "end": "1614549"
  },
  {
    "text": "but just being able to reuse\nsome least squares software. OK.",
    "start": "1614550",
    "end": "1619860"
  },
  {
    "text": "So you know, we've talked\nabout this many times. I just want to make sure that\nwe're all on the same page",
    "start": "1619860",
    "end": "1625740"
  },
  {
    "text": "here. We have a function f. We're going to assume that\nit has two derivatives.",
    "start": "1625740",
    "end": "1632100"
  },
  {
    "text": "And it's a function\nfrom Rm to R. So it's fist derivative\nis called gradient. That's the vector that collects\nall the partial derivatives",
    "start": "1632100",
    "end": "1640590"
  },
  {
    "text": "with respect to each\nof the coordinates. It's dimension m, of course. And the second derivative\nis an m by m matrix.",
    "start": "1640590",
    "end": "1648520"
  },
  {
    "text": "It's called the Hessian. And ith row and\njth column, you see",
    "start": "1648520",
    "end": "1654900"
  },
  {
    "text": "the second partial\nderivative with respect to the ith component\nand the jth component. OK.",
    "start": "1654900",
    "end": "1660620"
  },
  {
    "text": "We've seen that several times. This is just\nmulti-variable calculus. But really the point here\nis to maybe the notation",
    "start": "1660620",
    "end": "1668490"
  },
  {
    "text": "is slightly different, because\nI want to keep track of f. So when I write the gradient,\nI write nabla sub f.",
    "start": "1668490",
    "end": "1675090"
  },
  {
    "text": "And when I write Hessian,\nI write nabla H sub f.",
    "start": "1675090",
    "end": "1680190"
  },
  {
    "text": "And as I said, if f\nis strictly concave, then Hf of x is\nnegative definite.",
    "start": "1680190",
    "end": "1685950"
  },
  {
    "text": "What it means is that if\nI take any x in Rm, then x",
    "start": "1685950",
    "end": "1694250"
  },
  {
    "text": "transpose Hf, well,\nthat's for any X0 X,",
    "start": "1694250",
    "end": "1700420"
  },
  {
    "text": "this is actually\nstrictly negative. That's what it means to\nbe negative definite.",
    "start": "1700420",
    "end": "1705650"
  },
  {
    "text": "OK? So every time I do x transpose-- so this is like\na quadratic form.",
    "start": "1705650",
    "end": "1711940"
  },
  {
    "text": "And I want it to be negative\nfor all values of X0 and X, both of them. That's very strong, clearly.",
    "start": "1711940",
    "end": "1719180"
  },
  {
    "text": "But for us, actually,\nthis is what happens just because of the properties of b. ",
    "start": "1719180",
    "end": "1725540"
  },
  {
    "text": "Well, at least\nthe fact that it's negative, less than\nor equal to, if I",
    "start": "1725540",
    "end": "1730850"
  },
  {
    "text": "want it to be strictly less\nI need some properties on X. And then I will call the\nHessian map the function",
    "start": "1730850",
    "end": "1736630"
  },
  {
    "text": "that maps X to this\nmatrix Hf of X.",
    "start": "1736630",
    "end": "1742270"
  },
  {
    "text": "So that's just the\nsecond derivative at x. Yeah. AUDIENCE: When you\nwhat are [INAUDIBLE]??",
    "start": "1742270",
    "end": "1749750"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nWhere do [INAUDIBLE]?? Oh, yeah. I mean, you know, you need to\nbe able to apply Schwarz lemma.",
    "start": "1749750",
    "end": "1756130"
  },
  {
    "text": "Let's say two continue\nderivatives that's smooth. AUDIENCE: [INAUDIBLE]",
    "start": "1756130",
    "end": "1761542"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nNo, that's fine. ",
    "start": "1761542",
    "end": "1769190"
  },
  {
    "text": "OK. So how does the\nNewton-Raphson method work? Well, what it does is that it\nforms a quadratic approximation",
    "start": "1769190",
    "end": "1775180"
  },
  {
    "text": "to your function. And that's the one it optimizes\nat every single point. OK.",
    "start": "1775180",
    "end": "1780370"
  },
  {
    "text": "And the reason is\nbecause we have a closed-form solution\nto defining the minimum of a quadratic function.",
    "start": "1780370",
    "end": "1786393"
  },
  {
    "text": "So if I give you\na function that's of the form ax squared\nplus b x plus c, you know exactly a closed\nform for its minimum.",
    "start": "1786394",
    "end": "1794350"
  },
  {
    "text": "But if I give you any\nfunction or, let's say-- yeah, yeah. So here, it's all about maximum.",
    "start": "1794350",
    "end": "1799793"
  },
  {
    "text": "I'm sorry. If you're confused with\nme using the word minimum, just assume that it\nwas the word maximum.",
    "start": "1799793",
    "end": "1806890"
  },
  {
    "text": "So this is how it works. OK. If I give you a function which\nis concave, that's quadratic.",
    "start": "1806890",
    "end": "1814070"
  },
  {
    "text": "OK. So it's going to look like this.  So that's of the\nform ax squared--",
    "start": "1814070",
    "end": "1821630"
  },
  {
    "text": "where a is negative, of course-- plus bx plus c. Then you can solve\nyour whatever.",
    "start": "1821630",
    "end": "1828550"
  },
  {
    "text": "You can take the derivative of\nthis guy, set it equal to 0, and you will have\nan exact equation into what the value of x is\nthat it realizes this maximum.",
    "start": "1828550",
    "end": "1837530"
  },
  {
    "text": "If I give you any\nfunction that's concave,",
    "start": "1837530",
    "end": "1842840"
  },
  {
    "text": "that's all clear, right? I mean, if I tell you the\nfunction that we have here is that the form\nax minus b of x,",
    "start": "1842840",
    "end": "1850389"
  },
  {
    "text": "then I'm just going to have\nsomething that inverts b prime. But how do I do that exactly?",
    "start": "1850390",
    "end": "1856049"
  },
  {
    "text": "It's not clear. And so what we do is we do a\nquadratic approximation, which should be true approximately\neverywhere, right?",
    "start": "1856050",
    "end": "1863170"
  },
  {
    "text": "So I'm at this point\nhere, I'm going to say, oh, I'm close to\nbeing that function.",
    "start": "1863170",
    "end": "1868964"
  },
  {
    "text": "And if I'm at this\npoint here, I'm going to be close to\nbeing that function. And for this function,\nI can actually optimize.",
    "start": "1868964",
    "end": "1874820"
  },
  {
    "text": "And so if I'm not moving too\nfar from one to the other, I should actually get something. So here's how the quadratic\napproximation works.",
    "start": "1874820",
    "end": "1881960"
  },
  {
    "text": "I'm going to write the second\norder Taylor expansion.",
    "start": "1881960",
    "end": "1887791"
  },
  {
    "text": " OK. And so that's just going to\nbe my quadratic approximation.",
    "start": "1887791",
    "end": "1895160"
  },
  {
    "text": "It's going to say, oh,\nf of x, when x is close to some point x0,\nis going to close",
    "start": "1895160",
    "end": "1900710"
  },
  {
    "text": "to f of x0 plus the gradient of\nf at x0 transpose x minus x0.",
    "start": "1900710",
    "end": "1908870"
  },
  {
    "text": "And then I'm going to have\nplus 1/2x minus x0 transpose Hf at 0x x minus x\ntranspose, right--",
    "start": "1908870",
    "end": "1918020"
  },
  {
    "text": "x minus x0. So that's just my second\norder Taylor expansion multi-variate 1.",
    "start": "1918020",
    "end": "1923820"
  },
  {
    "text": "And let's say x0 is this guy. ",
    "start": "1923820",
    "end": "1928840"
  },
  {
    "text": "Now, what I'm going\nto do is say, OK, if I wanted to set this\nderivative of this guy",
    "start": "1928840",
    "end": "1935290"
  },
  {
    "text": "equal to 0, I would\njust have to solve, well, you know, f\nprime of x equals 0,",
    "start": "1935290",
    "end": "1941260"
  },
  {
    "text": "meaning that X has to\nbe f prime inverse of 0.",
    "start": "1941260",
    "end": "1946300"
  },
  {
    "text": "And really apart from like being\nsome notation manipulation, this is really not helping me.",
    "start": "1946300",
    "end": "1951670"
  },
  {
    "text": "OK. Because I don't know\nwhat f prime inverse of 0 is in many instances.",
    "start": "1951670",
    "end": "1956880"
  },
  {
    "text": "However if f has a very\nspecific form which is something that depends\non x in a very specific way,",
    "start": "1956880",
    "end": "1963340"
  },
  {
    "text": "there's just a linear term\nand then a quadratic term, then I can actually\ndo something. So let's forget\nabout this approach.",
    "start": "1963340",
    "end": "1969610"
  },
  {
    "text": "And rather than\nminimizing f, let's just minimize the right-hand side. OK.",
    "start": "1969610",
    "end": "1975090"
  },
  {
    "text": "So sorry- maximize. So maximize the right-hand side.",
    "start": "1975090",
    "end": "1986820"
  },
  {
    "text": "And so how do I get this? Well, I just set the\ngradient equal to 0. So what is the gradient?",
    "start": "1986820",
    "end": "1992470"
  },
  {
    "text": "The first term does\nnot depend on x. So that means that this\nis going to be 0 plus--",
    "start": "1992470",
    "end": "2001419"
  },
  {
    "text": "what is the gradient of this\nthing, of the gradient of f at x0 transpose x minus x0?",
    "start": "2001420",
    "end": "2007275"
  },
  {
    "text": "What is the gradient\nof this guy? ",
    "start": "2007275",
    "end": "2020300"
  },
  {
    "text": "So I have a function of\nthe form b transpose x. What is the gradient\nof this thing?",
    "start": "2020300",
    "end": "2026648"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: I'm sorry? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: I'm writing\neverything in two-column form,",
    "start": "2026648",
    "end": "2032866"
  },
  {
    "text": "right? So it's just b. OK. So here, what is b? Well, it's gradient of f at x0.",
    "start": "2032866",
    "end": "2039964"
  },
  {
    "text": " OK. And this term here gradient\nof f at x0 transpose x0",
    "start": "2039964",
    "end": "2047119"
  },
  {
    "text": "is just a constant. This thing is\ngoing away as well. And then I'm looking at the\nderivative of this guy here.",
    "start": "2047119",
    "end": "2053850"
  },
  {
    "text": "And this is like\na quadratic term. It's like H times\nx minus x0 squared.",
    "start": "2053850",
    "end": "2058930"
  },
  {
    "text": "So when I'm going to\ntake the derivative, I'm going to have\na factor 2 that's going to pop out and\ncancel this one half. And then I'm going\nto be left only",
    "start": "2058931",
    "end": "2065340"
  },
  {
    "text": "with this part times this part. OK. So that's plus Hf x minus x0.",
    "start": "2065340",
    "end": "2076320"
  },
  {
    "text": "OK. So that's just a gradient. And I want it to be equal to 0. So I'm just going to\nsolve this equal to 0.",
    "start": "2076320",
    "end": "2084909"
  },
  {
    "text": "OK? So that means that if I\nwant to find the minimum, this is just going to be\nthe x* that satisfies this.",
    "start": "2084910",
    "end": "2093399"
  },
  {
    "text": "So that's actually equivalent\nto Hf times x* is equal to Hf x0",
    "start": "2093400",
    "end": "2107859"
  },
  {
    "text": "minus gradient f at x0.",
    "start": "2107860",
    "end": "2114010"
  },
  {
    "text": "Now, this is a much\neasier thing to solve. What is this? ",
    "start": "2114010",
    "end": "2121410"
  },
  {
    "text": "This is just a system of\nlinear equations, right? I just need to find the x* such\nthat when I pre-multiply it",
    "start": "2121410",
    "end": "2129170"
  },
  {
    "text": "by a matrix I get this vector\non the right-hand side. This is just something\nof the form ax equals b.",
    "start": "2129170",
    "end": "2137890"
  },
  {
    "text": "And I have many\nways I can do this. I could do Gaussian\nelimination, or I could use Spielman's\nfast Laplacian solvers",
    "start": "2137890",
    "end": "2146200"
  },
  {
    "text": "if I had some particular\nproperties of H. I mean, there's huge activity in terms\nof how to solve those systems.",
    "start": "2146200",
    "end": "2154190"
  },
  {
    "text": "But let's say I have some time. It's not a huge problem. I can actually just\nuse linear algebra.",
    "start": "2154190",
    "end": "2159910"
  },
  {
    "text": "And linear algebra just tells me\nthat x* is equal to Hf inverse",
    "start": "2159910",
    "end": "2166589"
  },
  {
    "text": "times this guy, which those\ntwo guys are going to cancel.",
    "start": "2166590",
    "end": "2176010"
  },
  {
    "text": "So this is actually equal to\nx0 minus Hf inverse gradient f",
    "start": "2176010",
    "end": "2183630"
  },
  {
    "text": "at x0. And that's just what's\ncalled a Newton iteration.",
    "start": "2183630",
    "end": "2188920"
  },
  {
    "text": "I started at some x0. I'm at some x0, where I\nmake my approximation.",
    "start": "2188920",
    "end": "2194610"
  },
  {
    "text": "And it's telling me\nstarting from this x0, I wanted to fully optimize\na quadratic approximation,",
    "start": "2194610",
    "end": "2200890"
  },
  {
    "text": "I would just have\nto take the x*. That's this guy. And then I could just use this\nguy as my x0 and do it again,",
    "start": "2200890",
    "end": "2208800"
  },
  {
    "text": "and again, and again, and again. And those are called\nNewton iterations. And they're basically\nthe workhorse",
    "start": "2208800",
    "end": "2214320"
  },
  {
    "text": "of interior point methods,\nfor example, a lot of optimization algorithms.",
    "start": "2214320",
    "end": "2219930"
  },
  {
    "text": "And that's what\nyou can see here. x* is equal to x0 minus\nthe inverse Hessian times",
    "start": "2219930",
    "end": "2225480"
  },
  {
    "text": "the gradient. We briefly mentioned\ngradient descent.",
    "start": "2225480",
    "end": "2230844"
  },
  {
    "text": "We briefly mentioned gradient\ndecent, at some point, to optimize the convex\nfunction, right? And if I wanted to use gradient\ndescent, again, H is a matrix.",
    "start": "2230844",
    "end": "2242680"
  },
  {
    "text": "But if I wanted to think\nof H as being a scalar, would it be a positive\nor negative number? ",
    "start": "2242680",
    "end": "2251897"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Why? AUDIENCE: [INAUDIBLE]",
    "start": "2251897",
    "end": "2259870"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah. So that would be this. So I want to move against\nthe gradient to do what? AUDIENCE: [INAUDIBLE]",
    "start": "2259870",
    "end": "2265390"
  },
  {
    "text": "PHILIPPE RIGOLLET: To minimize. But I'm maximizing here, right? Everything is maximized, right? So I know that H is\nactually negative definite.",
    "start": "2265390",
    "end": "2272950"
  },
  {
    "text": "So it's a negative number. So you have the same\nconfusions they do.",
    "start": "2272950",
    "end": "2279040"
  },
  {
    "text": "We're maximizing a\nconcave function here. So H is negative. So this is something of\nthe form x0 plus something",
    "start": "2279040",
    "end": "2287710"
  },
  {
    "text": "times the gradient. And this is what your gradient\nascent, rather than descent,",
    "start": "2287710",
    "end": "2293270"
  },
  {
    "text": "would look like. And all it's saying,\nNewton is telling you don't take the\ngradient for granted",
    "start": "2293270",
    "end": "2299769"
  },
  {
    "text": "as a direction in\nwhich you want to go. It says, do a slight change\nof coordinates before you",
    "start": "2299770",
    "end": "2305410"
  },
  {
    "text": "do this according to what your\nHessian looks like, all right?",
    "start": "2305410",
    "end": "2310839"
  },
  {
    "text": "And those are called second\norder methods that require knowing what the Hessian is.",
    "start": "2310840",
    "end": "2316410"
  },
  {
    "text": "But those are actually\nmuch more powerful than the gradient\ndescent, because they're using all of the local\ngeometry of the problem.",
    "start": "2316410",
    "end": "2323690"
  },
  {
    "text": "All of the local\ngeometry of your function is completely encoded\nin this Hessian. And in particular it\nimplies that it tells you",
    "start": "2323690",
    "end": "2330290"
  },
  {
    "text": "where to switch and not to\ngo slower in some places or go faster in other places.",
    "start": "2330290",
    "end": "2335550"
  },
  {
    "text": "Now, this in practice for,\nsay, modern large scale machine learning problems,\ninverting this matrix H",
    "start": "2335550",
    "end": "2342170"
  },
  {
    "text": "is extremely painful. It takes too much time. The matrix is too big, and\ncomputers cannot do it.",
    "start": "2342170",
    "end": "2348450"
  },
  {
    "text": "And people resort\nto what's called pseudo-Newton method,\nwhich essentially tries",
    "start": "2348450",
    "end": "2353810"
  },
  {
    "text": "to emulate what this guy is. And there's many\nways you can do this. Some of them is\nby using gradients",
    "start": "2353810",
    "end": "2360170"
  },
  {
    "text": "that you've collected\nin the past. Some of them just say,\nwell let's just pretend H",
    "start": "2360170",
    "end": "2367435"
  },
  {
    "text": "is diagonal. There's a lot of\nthings you can do to just play around this\nand not actually have",
    "start": "2367435",
    "end": "2372980"
  },
  {
    "text": "to invert this matrix. OK? So once you have this,\nyou started from edge 0.",
    "start": "2372980",
    "end": "2378840"
  },
  {
    "text": "It tells you which H* you can\nget as a maximizer of the local",
    "start": "2378840",
    "end": "2384630"
  },
  {
    "text": "quadratic approximation\nto your function. You can actually just\niterate that, all right? So you start at\nsome x0 somewhere.",
    "start": "2384630",
    "end": "2392940"
  },
  {
    "text": "And then once you\nget to some xk, you just do the iteration\nwhich is described, which is just find\na k plus 1, which",
    "start": "2392940",
    "end": "2399690"
  },
  {
    "text": "is the maximizer of the\nlocal quadratic approximation to your function at xk and\nrepeat until convergence.",
    "start": "2399690",
    "end": "2409011"
  },
  {
    "text": "OK. So if this was an\noptimization class, we would prove that convergence\nactually, eventually,",
    "start": "2409011",
    "end": "2416530"
  },
  {
    "text": "happens for a strictly\nconcave function. This is a stats\nclass, so you're just",
    "start": "2416530",
    "end": "2422320"
  },
  {
    "text": "going to have to trust\nme that this is the case. And it's globally\nconvergent, meaning that you can start\nwherever you want, and it's",
    "start": "2422320",
    "end": "2430390"
  },
  {
    "text": "going to work for under\nminor conditions on f.",
    "start": "2430390",
    "end": "2435472"
  },
  {
    "text": "And in particular,\nthose conditions are satisfied for the\nlog-likelihood functions we have in mind.",
    "start": "2435472",
    "end": "2441120"
  },
  {
    "text": "OK. And it converges at an\nextremely fast rate. Usually it's\nquadratic convergence,",
    "start": "2441120",
    "end": "2446470"
  },
  {
    "text": "which means that every\ntime you make one step, you improve the accuracy of\nyour solution by two digits.",
    "start": "2446470",
    "end": "2452154"
  },
  {
    "text": " If that's something you're\nvaguely interested in,",
    "start": "2452154",
    "end": "2458960"
  },
  {
    "text": "I highly recommend that\nyou take a class on them in your optimization. It's a fascinating topic.",
    "start": "2458960",
    "end": "2464175"
  },
  {
    "text": "Unfortunately, we\ndon't have much time, but it starts being more\nand more intertwined with high dimensional\nstatistics and machine learning.",
    "start": "2464175",
    "end": "2471680"
  },
  {
    "text": " I mean, it's an algorithms\nclass, typically.",
    "start": "2471680",
    "end": "2477000"
  },
  {
    "text": "But it's very much\nmore principled.",
    "start": "2477000",
    "end": "2482047"
  },
  {
    "text": "It's not a bunch of algorithms\nthat solve a bunch of problems. There's basically\none basic idea, which is if I have\na convex function,",
    "start": "2482047",
    "end": "2488250"
  },
  {
    "text": "I can actually minimize it. If I have a concave\nfunction, I can maximize it. And it evolves around\na similar thing.",
    "start": "2488250",
    "end": "2495930"
  },
  {
    "text": "So let's stare at this iterative\nstep for a second and pause. And let me know if you\nhave any questions.",
    "start": "2495930",
    "end": "2503317"
  },
  {
    "text": " OK. So, of course, in a\nsecond we will plug in",
    "start": "2503318",
    "end": "2510930"
  },
  {
    "text": "for the log-likelihood. This is just a general thing\nfor a general function f. But then in a second,\nf is going to be ln.",
    "start": "2510930",
    "end": "2517550"
  },
  {
    "text": "OK. So if I wanted to\nimplement that for real, I would have to compute the\ngradient of ln at a point xk.",
    "start": "2517550",
    "end": "2524220"
  },
  {
    "text": "And I would have to compute\nthe Hessian at a given point and invert it. OK. So this is just the\nbasic algorithm.",
    "start": "2524220",
    "end": "2531519"
  },
  {
    "text": "And this, as you can\ntell, used in no place the fact that ln was the\nlog-likelihood associated",
    "start": "2531520",
    "end": "2537150"
  },
  {
    "text": "to some canonical\nexponential family in a generalized linear model. This never showed up.",
    "start": "2537150",
    "end": "2543550"
  },
  {
    "text": "So can we use that somehow? Optimization for longest time\nwas about making your problems",
    "start": "2543550",
    "end": "2548920"
  },
  {
    "text": "as general as possible\naccumulating maybe in the interior point method\ntheory in Koenig programming",
    "start": "2548920",
    "end": "2554680"
  },
  {
    "text": "in the mid-'90s. And now what\noptimization is doing is that it's [INAUDIBLE]\nvery general. It' says, OK, if I want\nto start to go fast,",
    "start": "2554680",
    "end": "2560619"
  },
  {
    "text": "I need to exploit as much\nstructure about my problem as I can. And the beauty is\nthat as statisticians",
    "start": "2560620",
    "end": "2565936"
  },
  {
    "text": "are a machine\nlearning people, we do have a bunch of\nvery specific problem that we want\noptimizers to solve.",
    "start": "2565936",
    "end": "2570940"
  },
  {
    "text": "And they can make\nthings run much faster. But this did not require to\nwait until the 21st century.",
    "start": "2570940",
    "end": "2576850"
  },
  {
    "text": "Problems with very\nspecific structure arose already in this\ngeneralized linear model.",
    "start": "2576850",
    "end": "2581900"
  },
  {
    "text": "So what do we know? Well, we know that this\nlog-likelihood is really",
    "start": "2581900",
    "end": "2587770"
  },
  {
    "text": "one thing that comes\nwhen we're trying to replace an expectation\nby an average, and then doing\nsomething fancy, right?",
    "start": "2587770",
    "end": "2594310"
  },
  {
    "text": "That was our statistical hammer. And remember when we introduced\nlikelihood maximization we just said, what do we\nreally want to do",
    "start": "2594310",
    "end": "2601780"
  },
  {
    "text": "is to minimize the KL, right? That's the thing we\nwanted to minimize, the KL divergence between two\ndistributions, the true one",
    "start": "2601780",
    "end": "2609640"
  },
  {
    "text": "and the one that's parameterized\nby some unknown theta. And we're trying to\nminimize that over theta. And we said, well,\nI don't know what",
    "start": "2609640",
    "end": "2616179"
  },
  {
    "text": "this is, because it's an\nexpectation with respect to some known distribution. So let me just replace the\nexpectation with respect",
    "start": "2616179",
    "end": "2622900"
  },
  {
    "text": "to my unknown distribution by\nan average over my data points. And that's how we\njustified the existence",
    "start": "2622900",
    "end": "2629800"
  },
  {
    "text": "of the log-likelihood\nmaximization problem. But here, actually, I\nmight be able to compute",
    "start": "2629800",
    "end": "2640690"
  },
  {
    "text": "this expectation, at least\npartially where I need it. And what we're going to do\nis we're going to say, OK,",
    "start": "2640690",
    "end": "2647690"
  },
  {
    "text": "since at a given point xk,\nsay, let me call it here theta, I'm trying to find the\ninverse of the Hessian",
    "start": "2647690",
    "end": "2656079"
  },
  {
    "text": "of my log-likelihood, right? So if you look at the\nprevious one, as I said, we're going to have to compute\nthe Hessian H sub l n of xk,",
    "start": "2656080",
    "end": "2663659"
  },
  {
    "text": "and then invert it. But let's forget about the\ninversion step for a second. We have to compute the Hessian.",
    "start": "2663659",
    "end": "2670102"
  },
  {
    "text": "This is the Hessian\nof the function we're trying to minimize. But if I could\nactually replace it not by the function I'm\ntrying to minimize to maximize",
    "start": "2670102",
    "end": "2677099"
  },
  {
    "text": "or the log-likelihood,\nbut really by the function I wish I\nwas actually minimizing,",
    "start": "2677100",
    "end": "2682390"
  },
  {
    "text": "which is the KL, right? Then that would be really nice. And what happens is\nthat since I'm actually",
    "start": "2682390",
    "end": "2688360"
  },
  {
    "text": "trying to find\nthis at a given xk, I can always pretend\nthat this xk that I have",
    "start": "2688360",
    "end": "2693759"
  },
  {
    "text": "in my current iteration\nis the true one and compute my expectation\nwith respect to that guy.",
    "start": "2693760",
    "end": "2698902"
  },
  {
    "text": "And what happens is\nthat I know that when I compute the expectation of the\nHessian of the log-likelihood",
    "start": "2698902",
    "end": "2705190"
  },
  {
    "text": "at a given theta and when I take\nthe expectation with respect to the same theta,\nwhat I get out",
    "start": "2705190",
    "end": "2710870"
  },
  {
    "text": "is negative Fisher information. The Fisher information\nwas defined in two ways--",
    "start": "2710870",
    "end": "2718240"
  },
  {
    "text": "as the expectation of the square\nof the derivative or negative",
    "start": "2718240",
    "end": "2725509"
  },
  {
    "text": "of the expectation of\nthe second derivative of the log-likelihood. And so now, I'm doing some\nsort of a leap of faith here.",
    "start": "2725509",
    "end": "2735090"
  },
  {
    "text": "Because there's no way the\ntheta, which is the current xk,",
    "start": "2735090",
    "end": "2740390"
  },
  {
    "text": "that's the current theta\nat which I'm actually doing this optimization-- I'm actually pretending\nthat this is the right one.",
    "start": "2740390",
    "end": "2746334"
  },
  {
    "text": " But what's going to\nchange by doing this is that it's going to\nmake my life easier.",
    "start": "2746334",
    "end": "2752980"
  },
  {
    "text": "Because when I\ntake expectations, we'll see that when we\nlook at the Hessian,",
    "start": "2752980",
    "end": "2760630"
  },
  {
    "text": "the Hessian as essentially the\nderivative of, say, a product",
    "start": "2760630",
    "end": "2767200"
  },
  {
    "text": "is going to be the sum\nof two terms, right? The derivative of u times v\nis u prime v plus uv prime.",
    "start": "2767200",
    "end": "2773224"
  },
  {
    "text": "One of those two\nterms is actually going to have expectation 0. And that's going to make\nmy life very easy when",
    "start": "2773224",
    "end": "2778420"
  },
  {
    "text": "I take expectations\nand basically just have one term that's\ngoing to go away. And so in particular,\nmy formula,",
    "start": "2778420",
    "end": "2784300"
  },
  {
    "text": "just by the virtue of\ntaking this expectation before inverting the\nHessian, is going",
    "start": "2784300",
    "end": "2789339"
  },
  {
    "text": "to just shrink the size\nof my formulas by half. OK. So let's see how this works.",
    "start": "2789340",
    "end": "2795619"
  },
  {
    "text": "You don't have to believe me. Is there any question\nabout this slide? You guys remember\nwhen we were doing",
    "start": "2795620",
    "end": "2801050"
  },
  {
    "text": "maximum estimation\nand Fisher information and the KL\ndivergence, et cetera?",
    "start": "2801050",
    "end": "2807228"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Because\nthat's what we're really",
    "start": "2807228",
    "end": "2813300"
  },
  {
    "text": "trying to minimize. AUDIENCE: [INAUDIBLE]",
    "start": "2813300",
    "end": "2819539"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah. So there's something\nyou need to trust me",
    "start": "2819540",
    "end": "2825100"
  },
  {
    "text": "with, which is that the\nexpectation of H of ln is actually H of the\nexpectation of ln, all right?",
    "start": "2825100",
    "end": "2834975"
  },
  {
    "text": "Yeah, it's true, right? Because taking derivative\nis a linear operator.",
    "start": "2834975",
    "end": "2840387"
  },
  {
    "text": "And we actually used\nthat several times when we said expectation of partial\nof l with respect to theta",
    "start": "2840387",
    "end": "2850530"
  },
  {
    "text": "is equal to 0. Remember we did that? That's basically\nwhat we used, right? ",
    "start": "2850530",
    "end": "2859069"
  },
  {
    "text": "AUDIENCE: ln is the likelihood. PHILIPPE RIGOLLET: It's\nthe log-likelihood. AUDIENCE: Log-likelihood,\n[INAUDIBLE] OK.",
    "start": "2859070",
    "end": "2865453"
  },
  {
    "text": "When we did Fisher\n[INAUDIBLE],, we did the likelihood of\n[INAUDIBLE] observation. PHILIPPE RIGOLLET: Yeah.",
    "start": "2865453",
    "end": "2871836"
  },
  {
    "text": "AUDIENCE: Why is\nit ln in this case? PHILIPPE RIGOLLET: So actually,\nln is typically not normalized.",
    "start": "2871836",
    "end": "2881410"
  },
  {
    "text": "So I really should\ntalk about ln over n. OK. But let's see that, OK? So if I have IID observations,\nthat should be pretty obvious.",
    "start": "2881410",
    "end": "2890790"
  },
  {
    "text": "OK. So if I have IID x1, xn, with\ndensity f theta and if I look",
    "start": "2890790",
    "end": "2902320"
  },
  {
    "text": "at log f theta of Xi, sum from\ni equal 1 to n, as I said,",
    "start": "2902320",
    "end": "2908290"
  },
  {
    "text": "I need to actually\nhave a 1 over n here. When I look at the\nexpectation, they all have the same expectation, right?",
    "start": "2908290",
    "end": "2914350"
  },
  {
    "text": "So this is actually,\nindeed, equal to negative KL",
    "start": "2914350",
    "end": "2921250"
  },
  {
    "text": "plus a constant. OK? And negative KL\nis because this-- sorry, if I look\nat the expectation.",
    "start": "2921250",
    "end": "2926710"
  },
  {
    "text": "So the expectation of this guy\nis just the expectation of one of them, all right?",
    "start": "2926710",
    "end": "2936430"
  },
  {
    "text": "So I just do expectation theta. OK? Agree?",
    "start": "2936430",
    "end": "2942109"
  },
  {
    "text": "Remember, the KL was expectation\ntheta log f theta divided by f.",
    "start": "2942110",
    "end": "2949060"
  },
  {
    "text": "So that's between p\ntheta and p theta prime. ",
    "start": "2949060",
    "end": "2954714"
  },
  {
    "text": "Well, no, sorry. That's the true p. And let's call it f. p theta, right?",
    "start": "2954714",
    "end": "2960619"
  },
  {
    "text": "So that's what showed\nup, which is, indeed, equal to minus\nexpectation theta log",
    "start": "2960620",
    "end": "2967075"
  },
  {
    "text": "f theta plus log of f, which\nis just a constant with respect",
    "start": "2967075",
    "end": "2973339"
  },
  {
    "text": "to theta. It's just the thing\nthat's up doesn't matter. OK?",
    "start": "2973340",
    "end": "2978890"
  },
  {
    "text": "So this is what shows up here. And just the fact that\nI have this 1 over n doesn't change,\nbecause they're IID.",
    "start": "2978890",
    "end": "2984614"
  },
  {
    "text": "Now, when I have things that are\nnot IID-- because what I really had was Y1 Yn, and Yi at\ndensity f theta i, which is just",
    "start": "2984614",
    "end": "2994570"
  },
  {
    "text": "the conditional\ndensity given Xi, then I could still write this. And now when I look at the\nexpectation of this guy, what",
    "start": "2994570",
    "end": "3002220"
  },
  {
    "text": "I'm going to be left with\nis just 1 over n sum from i",
    "start": "3002220",
    "end": "3008130"
  },
  {
    "text": "equal 1 to n of the expectation\nof log f theta i of Yi.",
    "start": "3008130",
    "end": "3017390"
  },
  {
    "text": " And it's basically the same\nthing, except that I have a 1 over n expectation in front.",
    "start": "3017391",
    "end": "3024772"
  },
  {
    "text": "And I didn't tell you this,\nbecause I only showed you what the KL divergence was\nfor between two distributions.",
    "start": "3024772",
    "end": "3032830"
  },
  {
    "text": "But here, I'm telling\nyou what the KL is between two products\nof distributions",
    "start": "3032830",
    "end": "3039012"
  },
  {
    "text": "that are independent,\nbut not necessarily identically distributed. ",
    "start": "3039012",
    "end": "3048411"
  },
  {
    "text": "But that's what's going to\nshow up, just because it's a product of things. So when you have the log,\nit's just going to be a sum.",
    "start": "3048412",
    "end": "3053930"
  },
  {
    "text": " Other questions?",
    "start": "3053930",
    "end": "3061706"
  },
  {
    "text": "All right, so what\ndo we do here? Well, as I said, now we know\nthat the expectation of H",
    "start": "3061706",
    "end": "3068849"
  },
  {
    "text": "is negative Fisher information. So rather than putting\nH inverse in my iterates for Newton-Raphson, I'm just\ngoing to put the inverse Fisher",
    "start": "3068850",
    "end": "3079050"
  },
  {
    "text": "information. And remember, it had\na minus sign in front. So I'm just going to\npick up a plus sign now,",
    "start": "3079050",
    "end": "3085950"
  },
  {
    "text": "just because i is negative,\nthe expectation of the Hessian.",
    "start": "3085950",
    "end": "3091329"
  },
  {
    "text": "And this guy has, essentially,\nthe same convergence properties. And it just happens\nthat it's easier",
    "start": "3091330",
    "end": "3097330"
  },
  {
    "text": "to compute the i than H Ln. And that's it. That's really why\nyou want to do this.",
    "start": "3097330",
    "end": "3103430"
  },
  {
    "text": "Now, you might say that, well,\nif I use more information,",
    "start": "3103430",
    "end": "3110180"
  },
  {
    "text": "I should do better, right? But it's actually\nnot necessarily true for several reasons. But let's say that one is\nprobably the fact that I",
    "start": "3110180",
    "end": "3116990"
  },
  {
    "text": "did not use more information. Every step when I was\ncomputing this thing at xk,",
    "start": "3116990",
    "end": "3122029"
  },
  {
    "text": "I actually pretended\nthat at theta k the true distribution\nwas the one distributed",
    "start": "3122030",
    "end": "3127780"
  },
  {
    "text": "according to theta k. And that was not true. This is only true\nwhen theta k becomes close to the true theta.",
    "start": "3127780",
    "end": "3133830"
  },
  {
    "text": " And so in a way, what\nI gained I lost again by making this thing.",
    "start": "3133830",
    "end": "3140380"
  },
  {
    "text": "It's just really a matter\nof simple computation. So let's just see it on\na particular example.",
    "start": "3140380",
    "end": "3145620"
  },
  {
    "text": "Actually, in this example, it's\nnot going to look much simpler. It's actually going\nto be the same.",
    "start": "3145620",
    "end": "3150700"
  },
  {
    "text": "All right, so I'm going to\nhave the Bernoulli example. All right, so we\nknow that Bernoulli",
    "start": "3150700",
    "end": "3156560"
  },
  {
    "text": "belongs to the canonical\nexponential family. And essentially, all I\nneed to tell you what b is.",
    "start": "3156560",
    "end": "3166700"
  },
  {
    "text": "And b of theta for Bernoulli\nis log 1 plus e theta, right?",
    "start": "3166700",
    "end": "3173670"
  },
  {
    "text": "We computed that. OK. And so when I look\nat my log-likelihood,",
    "start": "3173670",
    "end": "3182060"
  },
  {
    "text": "it is going to look like the sum\nfrom i equal 1 to n of Yi of--",
    "start": "3182060",
    "end": "3190079"
  },
  {
    "text": "OK, so here I'm\ngoing to actually use the canonical link. So it's going to be\nXi transpose beta",
    "start": "3190079",
    "end": "3195609"
  },
  {
    "text": "minus log 1 plus exponential\nXi transpose beta.",
    "start": "3195610",
    "end": "3200750"
  },
  {
    "text": " And phi for this\nguy is equal to 1.",
    "start": "3200750",
    "end": "3207620"
  },
  {
    "text": "Is it clear for\neveryone what I did? OK. So remember the density,\nso that was really just--",
    "start": "3207620",
    "end": "3215780"
  },
  {
    "text": "so the PMF was exponential Y\ntheta minus log 1 plus e theta.",
    "start": "3215780",
    "end": "3225070"
  },
  {
    "text": "There was actually\nno normalization. That's just the\ndensity of a Bernoulli.",
    "start": "3225070",
    "end": "3231140"
  },
  {
    "text": "And the theta is actually\nlog p over 1 minus p.",
    "start": "3231140",
    "end": "3240329"
  },
  {
    "text": "And so that's what\nactually gives me what my-- since p is the expectation,\nthis is actually giving me also my\ncanonical link,",
    "start": "3240330",
    "end": "3246750"
  },
  {
    "text": "which is the log [? at link. ?]\nWe saw that last time. And so if I start taking the log\nof this guy and summing over n",
    "start": "3246750",
    "end": "3252960"
  },
  {
    "text": "and replacing theta by\nXi transpose beta, which is what the canonical link\ntells me to do, I get this guy.",
    "start": "3252960",
    "end": "3259980"
  },
  {
    "text": "Is that clear for everyone? If it's not, please redo\nthis step on your own.",
    "start": "3259980",
    "end": "3265119"
  },
  {
    "text": " OK. ",
    "start": "3265120",
    "end": "3271100"
  },
  {
    "text": "So I want to maximize\nthis function. Sorry. So I want to maximize\nthis function over there",
    "start": "3271100",
    "end": "3276256"
  },
  {
    "text": "on the first line as\na function of beta. And so to do this, I want\nto use either Newton-Raphson",
    "start": "3276256",
    "end": "3284280"
  },
  {
    "text": "or what I call Fisher-scoring. So Fisher-scoring\nis the second one, when you replace the Hessian\nby negative Fisher information.",
    "start": "3284280",
    "end": "3291980"
  },
  {
    "text": "So I replace these two things. And so I first\ntake the gradient. OK. So let's take the\ngradient of ln.",
    "start": "3291980",
    "end": "3297160"
  },
  {
    "start": "3297160",
    "end": "3304359"
  },
  {
    "text": "So the gradient of ln is\ngoing to be, well, sum-- so here, this is of\nthe form Yi, which",
    "start": "3304360",
    "end": "3311470"
  },
  {
    "text": "is a scalar, times\na vector, Xi beta. That's what I erased from here.",
    "start": "3311470",
    "end": "3318400"
  },
  {
    "text": "The gradient of b\ntranspose x is just b. So here, I have just Yi Xi.",
    "start": "3318400",
    "end": "3323619"
  },
  {
    "text": "So that's of the form Yi,\nwhich is a scalar, times Xi, which is a vector.",
    "start": "3323620",
    "end": "3330028"
  },
  {
    "text": "Now, what about this guy? Well, here I have a function. So I'm going to have just the\nusual rule, the chain rule, right?",
    "start": "3330029",
    "end": "3335490"
  },
  {
    "text": "So that's just going\nto be 1 over this guy. ",
    "start": "3335490",
    "end": "3340690"
  },
  {
    "text": "And then I need to find\nthe Hessian of this thing. So the 1 is going away. And then I apply the\nchain rule again.",
    "start": "3340690",
    "end": "3346910"
  },
  {
    "text": "So I get e of Xi\ntranspose beta, and then the Hessian of this\nthing, which is Xi.",
    "start": "3346910",
    "end": "3353274"
  },
  {
    "text": " So my Hessian--\nmy radiant, sorry,",
    "start": "3353274",
    "end": "3360450"
  },
  {
    "text": "I can actually factor\nout all my Xi's. And it's going to\nlook like this. ",
    "start": "3360450",
    "end": "3375230"
  },
  {
    "text": "My gradient is a weighted\naverage or weighted sum",
    "start": "3375230",
    "end": "3381180"
  },
  {
    "text": "of the Xi's.  This will always\nhappen when you have",
    "start": "3381180",
    "end": "3390140"
  },
  {
    "text": "a generalized linear model. And that's pretty clear. Where did the Xi show up?",
    "start": "3390140",
    "end": "3395359"
  },
  {
    "text": "Whether it's from\nthis guy or that guy, the Xi came from\nthe fact that when I take the gradient\nof Xi transpose beta,",
    "start": "3395360",
    "end": "3401270"
  },
  {
    "text": "I have this vector\nXi that comes out. It's always going to be\nthe thing that comes out.",
    "start": "3401270",
    "end": "3406740"
  },
  {
    "text": "So I will always have something\nthat looks like with some sum with some weights\nhere of the Xi's.",
    "start": "3406740",
    "end": "3413590"
  },
  {
    "text": "Now, when I look at\nthe second derivative-- ",
    "start": "3413590",
    "end": "3424350"
  },
  {
    "text": "so same thing, I'm just going\nto take the derivative this guy. Since nothing depends\non beta here or here,",
    "start": "3424350",
    "end": "3431460"
  },
  {
    "text": "I'm just going to have to take\nthe derivative of this thing. And so it's going to be equal. So if I look now at the Hessian\nln as a function of beta,",
    "start": "3431460",
    "end": "3441110"
  },
  {
    "text": "I'm going to have sum from i\nequal 1 to n of, well, Yi-- what is a derivative of\nYi with respect to beta?",
    "start": "3441110",
    "end": "3448590"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: What?",
    "start": "3448590",
    "end": "3454374"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Yeah. 0.",
    "start": "3454374",
    "end": "3459660"
  },
  {
    "text": "OK? It doesn't depend on data. I mean, this distribution does. But Y itself is just\na number, right?",
    "start": "3459660",
    "end": "3466290"
  },
  {
    "text": "So this is 0. So I'm going to get the minus. And then I'm going to\nhave, again, the chain rule that shows up.",
    "start": "3466290",
    "end": "3472030"
  },
  {
    "text": "So I need to find the\nderivative of x over 1 plus x. What is the derivative\nof x over 1 plus x.",
    "start": "3472030",
    "end": "3479289"
  },
  {
    "text": "Actually don't even know.  So that gives me--",
    "start": "3479289",
    "end": "3484510"
  },
  {
    "start": "3484510",
    "end": "3492491"
  },
  {
    "text": "OK. So that's 1 over\n1 plus x squared. So that's minus e\nXi transpose beta--",
    "start": "3492491",
    "end": "3499380"
  },
  {
    "text": "sorry, 1 divided by 1 plus e\nXi transpose beta squared times",
    "start": "3499380",
    "end": "3507390"
  },
  {
    "text": "the derivative of the\nexponential, which is e Xi transpose beta and again, Xi.",
    "start": "3507390",
    "end": "3515280"
  },
  {
    "text": " And then I have this\nXi that shows up. But since I'm\nlooking for a matrix,",
    "start": "3515280",
    "end": "3521138"
  },
  {
    "text": "I'm going to have Xi,\nXi transpose, right? ",
    "start": "3521138",
    "end": "3533079"
  },
  {
    "text": "OK? AUDIENCE: [INAUDIBLE]",
    "start": "3533080",
    "end": "3538599"
  },
  {
    "text": "PHILIPPE RIGOLLET: So I know\nI'm going to need something that looks like a matrix in the end.",
    "start": "3538600",
    "end": "3544360"
  },
  {
    "text": "And so one way you want\nto think about it is this is going to spit out an Xi.",
    "start": "3544360",
    "end": "3549850"
  },
  {
    "text": "There's already an Xi here. So I'm going to have\nsomething that looks like Xi. And I'm going to have to\nmultiply by it another vector",
    "start": "3549850",
    "end": "3555850"
  },
  {
    "text": "Xi. And I want it to form a matrix. And so what you need to do\nis to take an outer product.",
    "start": "3555850",
    "end": "3562264"
  },
  {
    "text": "And that's it. ",
    "start": "3562264",
    "end": "3576420"
  },
  {
    "text": "So now as a result, the\nupdating rule is this.",
    "start": "3576420",
    "end": "3582510"
  },
  {
    "text": "Honestly, this is not\na result of anything. I actually rewrote everything\nthat I had before with a theta",
    "start": "3582510",
    "end": "3587950"
  },
  {
    "text": "replaced by beta,\nbecause it's just painful to rewrite this entire\nthing, put some big parenthesis",
    "start": "3587950",
    "end": "3594289"
  },
  {
    "text": "and put minus 1 here.  And then I would have to\nput the gradient, which",
    "start": "3594290",
    "end": "3600920"
  },
  {
    "text": "is this thing here. So as you can imagine,\nthis is not super nice.",
    "start": "3600920",
    "end": "3607339"
  },
  {
    "text": "Actually, what's\ninteresting is at some point I mentioned there's a\npseudo-Newton method.",
    "start": "3607340",
    "end": "3615079"
  },
  {
    "text": "They're actually\ndoing exactly this. They're saying, oh,\nat each iteration,",
    "start": "3615080",
    "end": "3622359"
  },
  {
    "text": "I'm actually going to\njust take those guys. If I'm at iteration\nk, I'm actually just going to sum\nthose guys up to k",
    "start": "3622360",
    "end": "3628225"
  },
  {
    "text": "rather than going all the way\nto n and look at every one. So you're just looking at your\nobservations one at a time based on where you were before.",
    "start": "3628225",
    "end": "3635851"
  },
  {
    "text": "OK. So you have a matrix. You need to invert it. So if you want to be\nable to invert it,",
    "start": "3635851",
    "end": "3641185"
  },
  {
    "text": "you need to make\nsure that the sum with those weights of Xi\nouter, Xi, or Xi, Xi transpose",
    "start": "3641185",
    "end": "3646810"
  },
  {
    "text": "is invertible. So that's a condition\nthat you need to have. And well, you don't have\nto, because technically you",
    "start": "3646810",
    "end": "3653826"
  },
  {
    "text": "don't need to invert. You just need to solve\nthe linear system. But that's actually guaranteed\nin most of the cases",
    "start": "3653826",
    "end": "3660910"
  },
  {
    "text": "if n is large enough. All right, so everybody\nsees what we're doing here? OK.",
    "start": "3660910",
    "end": "3666380"
  },
  {
    "text": "So that's for the\nNewton-Raphson.",
    "start": "3666380",
    "end": "3671480"
  },
  {
    "text": "If I wanted to actually\ndo the Fisher-scoring,",
    "start": "3671480",
    "end": "3677740"
  },
  {
    "text": "all I would need to do is\nto replace the Hessian here by its expectation when I\npretend that the beta have,",
    "start": "3677740",
    "end": "3685100"
  },
  {
    "text": "iteration k, is the true one. What is the expectation\nof this thing?",
    "start": "3685100",
    "end": "3691510"
  },
  {
    "text": "And when I say\nexpectation here, I'm always talking about conditional\nexpectation of Y given X.",
    "start": "3691510",
    "end": "3697810"
  },
  {
    "text": "The only distributions that\nmatter, that have mattered in this entire chapter, are\nconditional expectation of Y",
    "start": "3697810",
    "end": "3704530"
  },
  {
    "text": "given X. The conditional expectation\nof this thing given X is what?",
    "start": "3704530",
    "end": "3710810"
  },
  {
    "start": "3710810",
    "end": "3715870"
  },
  {
    "text": "It's itself. It does not depend on Y.\nIt only depends on the X's. So conditionally\non x, this thing",
    "start": "3715870",
    "end": "3721520"
  },
  {
    "text": "as far as we're concerned,\nis completely deterministic. So it's actually equal\nto it's expectation.",
    "start": "3721520",
    "end": "3727700"
  },
  {
    "text": "And so in this\nparticular example, there's no difference\nbetween Fisher-scoring",
    "start": "3727700",
    "end": "3736450"
  },
  {
    "text": "and Newton-Raphson. And the reason is because\nthe gradient no longer",
    "start": "3736450",
    "end": "3744190"
  },
  {
    "text": "depends on Yi-- I'm sorry. The Hessian no\nlonger depends on Yi.",
    "start": "3744190",
    "end": "3750172"
  },
  {
    "text": "OK? ",
    "start": "3750172",
    "end": "3761630"
  },
  {
    "text": "This slide is just repeating\nsome stuff that I've said. ",
    "start": "3761630",
    "end": "3769466"
  },
  {
    "text": "OK.  So I think this is probably--",
    "start": "3769466",
    "end": "3775800"
  },
  {
    "text": " OK, let's go through\nthis actually. ",
    "start": "3775800",
    "end": "3784980"
  },
  {
    "text": "At some point, I said\nthat Newton-Raphson-- do you have a question? AUDIENCE: Yeah. When would the gradient-- sorry,\nthe Hessian ever depend on Yi?",
    "start": "3784980",
    "end": "3792290"
  },
  {
    "text": "Because it seems like\nYi is just-- or at least when you have a canonical link,\nthat the log-likelihood is just",
    "start": "3792290",
    "end": "3802601"
  },
  {
    "text": "[INAUDIBLE] to Yi Xi [INAUDIBLE]\ntheta and that's the only place Y shows up.",
    "start": "3802601",
    "end": "3808002"
  },
  {
    "text": "So [INAUDIBLE] derivative\n[INAUDIBLE] never depend on Y? PHILIPPE RIGOLLET: Not when\nyou have a canonical link.",
    "start": "3808002",
    "end": "3813134"
  },
  {
    "text": "AUDIENCE: So if it's not a\n[INAUDIBLE] there's is no difference between-- PHILIPPE RIGOLLET: No. AUDIENCE: OK. PHILIPPE RIGOLLET: Yeah.",
    "start": "3813134",
    "end": "3818880"
  },
  {
    "text": "So yeah, maybe I wanted you to\nfigure that out for yourself.",
    "start": "3818880",
    "end": "3825349"
  },
  {
    "text": "OK. So Yi times Xi transpose beta. So essentially, when I\nhave a general family, what",
    "start": "3825350",
    "end": "3835390"
  },
  {
    "text": "he's referring to is that this\nis just b of Xi transpose beta. So I'm going to take\nsome derivatives.",
    "start": "3835390",
    "end": "3841750"
  },
  {
    "text": "And there's going\nto be something complicated coming out of this. But I'm certainly not going\nto have some Yi showing up. The only place where\nYi shows up is here.",
    "start": "3841750",
    "end": "3849670"
  },
  {
    "text": "Now, if I take two\nderivatives, this thing is gone, because it's linear. The first one is going\nto keep on like this guy.",
    "start": "3849670",
    "end": "3855550"
  },
  {
    "text": "And the second one is\ngoing to make it go on. The only way this actually shows\nup is when to have an H here.",
    "start": "3855550",
    "end": "3861730"
  },
  {
    "text": "And if I have an H, then I\ncan take second derivatives. And this thing is\nnot going to be",
    "start": "3861730",
    "end": "3866980"
  },
  {
    "text": "completely independent of beta. Sorry. Yeah, this thing is\nstill going to depend",
    "start": "3866980",
    "end": "3873128"
  },
  {
    "text": "on beta, which means\nthat this Yi term is not going to disappear. ",
    "start": "3873128",
    "end": "3879127"
  },
  {
    "text": "I believe we'll see an example\nof that, or maybe I removed it. I'm not sure, actually. I think we will see an example.",
    "start": "3879127",
    "end": "3884626"
  },
  {
    "text": " So let us do a Iteratively\nRe-weighted Least",
    "start": "3884626",
    "end": "3895390"
  },
  {
    "text": "Squares, or IRLS, which I've\nactually recently learned is a term that even though\nit was defined in the '50s,",
    "start": "3895390",
    "end": "3903490"
  },
  {
    "text": "people still feel\nfree to use to define to call their new algorithms\nwhich have nothing",
    "start": "3903490",
    "end": "3908530"
  },
  {
    "text": "to do with this. This is really something where\nyou actually do iteratively re-weighted least squares.",
    "start": "3908530",
    "end": "3913660"
  },
  {
    "text": " OK.",
    "start": "3913660",
    "end": "3918790"
  },
  {
    "text": "Let's just actually go\nthrough this quickly what is going to be iteratively\nre-weighted least squares. The way the steps that\nwe had here showed up--",
    "start": "3918790",
    "end": "3928250"
  },
  {
    "text": "let's say those guys, x*-- is this, is when were actually\nsolving this linear system,",
    "start": "3928250",
    "end": "3937430"
  },
  {
    "text": "right? That was the linear system\nwe were trying to solve.",
    "start": "3937430",
    "end": "3943215"
  },
  {
    "text": "But solving a\nlinear system can be done by just trying\nto minimize, right?",
    "start": "3943215",
    "end": "3948390"
  },
  {
    "text": "If I have x a and\nb, it's the same as minimizing the norm of\nax minus b squared over x.",
    "start": "3948390",
    "end": "3958819"
  },
  {
    "text": "If I can actually find\nan x for which it's 0, it means that I've\nactually solved my problem.",
    "start": "3958820",
    "end": "3964280"
  },
  {
    "text": "And so that means that I can\nsolve linear systems by solving",
    "start": "3964280",
    "end": "3970580"
  },
  {
    "text": "least square problems. And least square problems\nare things that statisticians are comfortable solving.",
    "start": "3970580",
    "end": "3975630"
  },
  {
    "text": "And so all I have to\ndo is to rephrase this as at least square problem. OK?",
    "start": "3975630",
    "end": "3980780"
  },
  {
    "text": "And you know, I could just\nwrite it directly like this. But there's a way to\nstreamline it a little bit.",
    "start": "3980780",
    "end": "3985850"
  },
  {
    "text": "And that's actually\nby using weights. OK.",
    "start": "3985850",
    "end": "3990859"
  },
  {
    "text": "So I've come in the weights-- well, not today, actually,\nbut very soon, all right?",
    "start": "3990860",
    "end": "3997580"
  },
  {
    "text": "So this is just a\nreminder of what we had. We have that's Yi give Xi as\na distribution distributed",
    "start": "3997580",
    "end": "4003670"
  },
  {
    "text": "according to some distribution\nin the canonical exponential family. So that means that the\nlog-likelihood looks like this.",
    "start": "4003670",
    "end": "4010730"
  },
  {
    "text": "Again, this does\nnot matter to us. This is the form that matters. And we have a bunch\nof relationships",
    "start": "4010730",
    "end": "4015940"
  },
  {
    "text": "that we actually spent\nsome time computing. The first one is that mu\nis b prime of theta i.",
    "start": "4015940",
    "end": "4021250"
  },
  {
    "text": "The second one is that\nif I take g of mu i, I get this systematic component,\nXi transpose beta that's",
    "start": "4021250",
    "end": "4028150"
  },
  {
    "text": "modeling. Now, if I look at the derivative\nmu i with respect to theta i, this is the derivative\nof b prime of theta i",
    "start": "4028150",
    "end": "4035510"
  },
  {
    "text": "with respect to theta i. So that's the second derivative. And I'm going to call it Vi. If phi is equal to 1, this\nis actually the variance.",
    "start": "4035510",
    "end": "4044430"
  },
  {
    "text": "And then I have this\nfunction H, which allows me to bypass altogether\nthe existence of this parameter",
    "start": "4044430",
    "end": "4050710"
  },
  {
    "text": "mu, which says if I want to\ngo from Xi transpose beta all the way to theta i, I\nhave to first do g inverse,",
    "start": "4050710",
    "end": "4057420"
  },
  {
    "text": "and then b prime inverse. If I stopped here, I\nwould just have mu. OK? ",
    "start": "4057420",
    "end": "4066029"
  },
  {
    "text": "OK. So now what I'm\ngoing to do is I'm going to apply the chain rule. And I'm going to try to\ncompute the derivative",
    "start": "4066030",
    "end": "4073070"
  },
  {
    "text": "of my log-likelihood\nwith respect to beta. So, again, the\nlog-likelihood is much nicer",
    "start": "4073070",
    "end": "4080516"
  },
  {
    "text": "when I read it as a function of\ntheta than a function of beta, but it's basically what\nwe've been doing by hand.",
    "start": "4080517",
    "end": "4085930"
  },
  {
    "text": "You can write it as a\nderivative with respect to theta first, and then\nmultiply by the derivative",
    "start": "4085930",
    "end": "4091390"
  },
  {
    "text": "of theta with respect to beta. OK. And we know that\ntheta depends on beta as H of Xi transpose beta.",
    "start": "4091390",
    "end": "4100020"
  },
  {
    "text": "OK? I mean, that's basically\nwhat we've been doing for the Bernoulli case.",
    "start": "4100020",
    "end": "4106009"
  },
  {
    "text": "I mean, we used the chain rule\nwithout actually saying it. But this is going to be\nconvenient to actually make it explicitly show up.",
    "start": "4106010",
    "end": "4112200"
  },
  {
    "text": "OK. So when I first take the\nderivative of my log-likelihood with respect to theta,\nI'm going to use the fact",
    "start": "4112200",
    "end": "4119689"
  },
  {
    "text": "that my canonical\nfamily is super simple. OK. So what I have is that\nmy log-likelihood ln",
    "start": "4119689",
    "end": "4130219"
  },
  {
    "text": "is the sum from i equal 1\nto n of Yi theta i minus b",
    "start": "4130220",
    "end": "4136019"
  },
  {
    "text": "of theta i divided by phi\nplus some constant, which will go away as\nsoon as I'm going",
    "start": "4136019",
    "end": "4141680"
  },
  {
    "text": "to take my first derivative. So if I take the derivative with\nrespect to theta i of this guy,",
    "start": "4141680",
    "end": "4148210"
  },
  {
    "text": "this is actually going to be\nequal to Yi minus b prime theta i divided by phi.",
    "start": "4148210",
    "end": "4156870"
  },
  {
    "text": "And then I need to multiply\nby the derivative of theta i with respect to beta. Remember, theta is H\nof Xi transpose beta.",
    "start": "4156870",
    "end": "4166318"
  },
  {
    "text": "So the derivative of theta\ni with respect to beta j,",
    "start": "4166319",
    "end": "4171389"
  },
  {
    "text": "this is equal to H prime\nof Xi transpose beta.",
    "start": "4171390",
    "end": "4176818"
  },
  {
    "text": "And then I have the\nderivative of this guy. Actually, let me just do\nthe gradient of theta I",
    "start": "4176819",
    "end": "4186028"
  },
  {
    "text": "at beta, right? That's what we did. I'm just thinking of theta i\nas being a function of theta.",
    "start": "4186029",
    "end": "4193028"
  },
  {
    "text": "So what should I add here? It was just the vector Xi, which\nis just the chain rule again.",
    "start": "4193029",
    "end": "4201812"
  },
  {
    "text": "That's Hi prime, right? You don't see it, but there's\na prime here that's derivative. OK. We've done that without\nsaying it explicitly.",
    "start": "4201812",
    "end": "4210030"
  },
  {
    "text": "So now if I multiply\nthose two things I have this Yi minus\nb prime of the theta i, which I call by its\ngood name, which is mu i.",
    "start": "4210030",
    "end": "4219020"
  },
  {
    "text": "b prime of theta i\nis the expectation of Yi conditionally on Xi. And then I multiply\nby this thing here.",
    "start": "4219020",
    "end": "4224910"
  },
  {
    "text": "So here, this thing is written\ncoordinate by coordinate. But I can write\nit as a big vector",
    "start": "4224910",
    "end": "4230630"
  },
  {
    "text": "when I stack them together. And so what I claim is\nthat this thing here",
    "start": "4230630",
    "end": "4236250"
  },
  {
    "text": "is of the form Y minus mu. But here I put some tildes. Because what I did is that\nfirst I multiplied everything",
    "start": "4236250",
    "end": "4246660"
  },
  {
    "text": "by g prime of mu for each mu. OK. So why not?",
    "start": "4246660",
    "end": "4252620"
  },
  {
    "text": "OK. Actually, on this slide it will\nmake no sense why I do this.",
    "start": "4252620",
    "end": "4258680"
  },
  {
    "text": "I basically multiply\nby g prime on one side and divide by g prime\non the other side.",
    "start": "4258680",
    "end": "4264080"
  },
  {
    "text": "So what I write so far is that\nthe gradient of ln with respect",
    "start": "4264080",
    "end": "4272090"
  },
  {
    "text": "to beta is the sum from i\nequal 1 to n of Yi minus mu i,",
    "start": "4272090",
    "end": "4279969"
  },
  {
    "text": "let's call it,\ndivide by phi times H prime of Xi transpose beta Xi.",
    "start": "4279970",
    "end": "4288591"
  },
  {
    "text": "OK. So I just stacked\neverything that's here. And now I'm going to\nstart calling things.",
    "start": "4288591",
    "end": "4293659"
  },
  {
    "text": "The first thing I'm going to\ndo is I'm going to divide. So this guy here I'm\ngoing to push here. ",
    "start": "4293660",
    "end": "4300840"
  },
  {
    "text": "Now, this guy here\nI'm actually going to multiply by g prime of mu i.",
    "start": "4300840",
    "end": "4308870"
  },
  {
    "text": "And this guy I'm going to\ndivide by g prime of mu i. So there's really nothing\nthat happened here.",
    "start": "4308870",
    "end": "4314780"
  },
  {
    "text": "I just took g prime and multiply\nand divide it by g prime. Why do I do this?",
    "start": "4314780",
    "end": "4320614"
  },
  {
    "text": "Well, that's actually\ngoing to be clear when we talk about iteratively\nre-weighted least squares.",
    "start": "4320615",
    "end": "4326320"
  },
  {
    "text": "But now, essentially I have\na new mu, a Y which is-- so this thing now is going\nto be Y tilde minus mu",
    "start": "4326320",
    "end": "4334619"
  },
  {
    "text": "tilde, so i, i. Now, this guy here\nI'm going to call Wi.",
    "start": "4334620",
    "end": "4343720"
  },
  {
    "text": " And I have the Xi\nthat's there, which",
    "start": "4343720",
    "end": "4351520"
  },
  {
    "text": "means that now the thing that\nI have here I can write as",
    "start": "4351520",
    "end": "4358120"
  },
  {
    "text": "follows. Gradient ln of beta\nis equal to what?",
    "start": "4358120",
    "end": "4366280"
  },
  {
    "text": "Well, I'm going to write\nit in matrix forms. So I have the sum over i of\nsomething multiplied by Xi.",
    "start": "4366280",
    "end": "4373750"
  },
  {
    "text": "So I'm going to write\nit as x transpose. Then I'm going to have\nthis matrix W1 Wn, and then",
    "start": "4373750",
    "end": "4382989"
  },
  {
    "text": "0 elsewhere. And then I'm going to\nhave my Y tilde minus mu.",
    "start": "4382990",
    "end": "4389520"
  },
  {
    "text": "And remember, X is the\nmatrix with-- sorry,",
    "start": "4389520",
    "end": "4395190"
  },
  {
    "text": "it should be a bit [INAUDIBLE]. I have n, and then p. And here I have my Xi j in this\nmatrix on row i and column j.",
    "start": "4395190",
    "end": "4406560"
  },
  {
    "text": "And this is just a matrix that\nhas the Wi's on the diagonal. And then I have\nY tilde minus mu.",
    "start": "4406560",
    "end": "4412830"
  },
  {
    "text": "So this is just the matrix\nwe're writing of this formula. All right. So it's just saying\nthat if I look",
    "start": "4412830",
    "end": "4418446"
  },
  {
    "text": "at the sum of weighted\nthings of my columns of Xi, it's basically the same thing.",
    "start": "4418446",
    "end": "4424480"
  },
  {
    "text": "When I'm going to multiply\nthis by my matrix, I'm going to get exactly\nthose terms, right? Yi minus mu i tilde times Wi.",
    "start": "4424480",
    "end": "4432900"
  },
  {
    "text": "And then when I actually\ntake this Xi transpose times this guy, I'm\nreally just getting",
    "start": "4432900",
    "end": "4438120"
  },
  {
    "text": "the sum of the columns\nwith the weights, right?",
    "start": "4438120",
    "end": "4444590"
  },
  {
    "text": "Agree? If I look at this\nthing here, this is a vector that has S\ncoordinates, Wi times Yi",
    "start": "4444590",
    "end": "4455915"
  },
  {
    "text": "tilde minus mu i tilde. And I have n of them.",
    "start": "4455915",
    "end": "4461020"
  },
  {
    "text": "So when I multiply X\ntranspose by this guy, I'm just looking at a weighted\nsum of the columns of X",
    "start": "4461020",
    "end": "4468060"
  },
  {
    "text": "transpose, which is a\nweighted sum of the rows of X, which are exactly my Xi's.",
    "start": "4468060",
    "end": "4474166"
  },
  {
    "text": "All right, and that's this\nweighted sum of the Xi's. ",
    "start": "4474166",
    "end": "4480110"
  },
  {
    "text": "OK. So here, as I said,\nthe fact that we decided to put this g prime of\nmu i here and g prime of mu i",
    "start": "4480110",
    "end": "4488050"
  },
  {
    "text": "here, we could have\nnot done this, right? We could have just said,\nI forget about the tilde",
    "start": "4488050",
    "end": "4494470"
  },
  {
    "text": "and just call it Yi minus mu i. And here, I just put everything\nI don't know into some Wi.",
    "start": "4494470",
    "end": "4500500"
  },
  {
    "text": "And so why do I do this? Well, it's because\nwhen I actually start looking at the Hessian,\nwhat's going to happen?",
    "start": "4500500",
    "end": "4506830"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PHILIPPE RIGOLLET: Yeah. We'll do that next time. But let's just look quickly at\nthe outcome of the computation",
    "start": "4506830",
    "end": "4514370"
  },
  {
    "text": "of my Hessian. So I compute a bunch\nof second derivatives.",
    "start": "4514370",
    "end": "4520160"
  },
  {
    "text": "And here, I have\ntwo terms, right? Well, he's gone. So I have two terms. And when I take the\nexpectation now,",
    "start": "4520160",
    "end": "4526569"
  },
  {
    "text": "it's going to actually\nchange, right? This thing is actually\ngoing to depend on Yi. Because I have an H which\nis not the identity.",
    "start": "4526569",
    "end": "4534070"
  },
  {
    "text": "Oh, no, you're here, sorry. So when I start looking\nat the expectation,",
    "start": "4534070",
    "end": "4539460"
  },
  {
    "text": "so I look at the conditional\nexpectation given Xi. The first term here has\na Yi minus expectation.",
    "start": "4539460",
    "end": "4546598"
  },
  {
    "text": "So when I take the\nconditional expectation, this is going to be 0. The first term is\ngoing away when I take the conditional expectation.",
    "start": "4546599",
    "end": "4552469"
  },
  {
    "text": "But this was\nactually gone already if we had the canonical term,\nbecause the second derivative of H when H is\nthe identity is 0.",
    "start": "4552470",
    "end": "4560480"
  },
  {
    "text": "But if H is not the identity,\nH prime prime may not be 0. And so I need that part\nto remove that term.",
    "start": "4560480",
    "end": "4567120"
  },
  {
    "text": "And so now, you know,\nI work a little bit, and I get this term. That's not very surprising. In the second derivative, I see\nI have terms in b prime prime.",
    "start": "4567120",
    "end": "4575150"
  },
  {
    "text": "I have term in H\nprime, but squared. And then I have my Xi\nouter Xi, Xi, Xi transpose,",
    "start": "4575150",
    "end": "4581700"
  },
  {
    "text": "which we know we would see. OK. So we'll go through\nthose things next time. But what I want to show you is\nthat now once I compute this,",
    "start": "4581700",
    "end": "4591850"
  },
  {
    "text": "I can actually show that if\nI look at this product that showed up, I had b prime\nprime times H prime squared.",
    "start": "4591850",
    "end": "4600700"
  },
  {
    "text": "One of those terms is\nactually 1 over g prime. And so I can rewrite it\nas one of the H primes,",
    "start": "4600700",
    "end": "4606390"
  },
  {
    "text": "because I had a square,\ndivided by g prime. And now, I have this\nXi Xi transpose.",
    "start": "4606390",
    "end": "4611730"
  },
  {
    "text": "So if I did not put\nthe g prime in the W",
    "start": "4611730",
    "end": "4617370"
  },
  {
    "text": "that I put here\ncompletely artificially, I would not be able to call\nthis guy Wi, which is exactly",
    "start": "4617370",
    "end": "4624659"
  },
  {
    "text": "what it is from this board. And now that this guy is Wi, I\ncan actually write this thing",
    "start": "4624660",
    "end": "4629969"
  },
  {
    "text": "here as X transpose WX. OK?",
    "start": "4629970",
    "end": "4635010"
  },
  {
    "text": "And that's why I\nreally wanted my W to have this g prime of\nmu i in the denominator.",
    "start": "4635010",
    "end": "4640920"
  },
  {
    "text": "Because now I can actually\nwrite a term that depends on W. Now, you might say, how do I\nreconcile those two things?",
    "start": "4640920",
    "end": "4646440"
  },
  {
    "text": "What the hell are you doing? And what the hell I'm\ndoing is essentially that I'm saying that if\nyou write beta k according",
    "start": "4646440",
    "end": "4655380"
  },
  {
    "text": "to the Fisher-scoring\niterations, you can actually write it\nas just this term here,",
    "start": "4655380",
    "end": "4661660"
  },
  {
    "text": "which is of the form X transpose\nX inverse X transpose Y. But I actually\nsqueezed in these W's.",
    "start": "4661660",
    "end": "4670119"
  },
  {
    "text": "And that's actually a\nweighted least square. And it's applied to\nthis particular guy. So we'll talk about those\nweighted least squares.",
    "start": "4670120",
    "end": "4676090"
  },
  {
    "text": "But remember, least\nsquares is of the form-- beta hat is X transpose\nX inverse X transpose Y.",
    "start": "4676090",
    "end": "4682725"
  },
  {
    "text": "And here it's basically\nthe same thing, except that I squeeze in\nsome W after my X transpose.",
    "start": "4682725",
    "end": "4689290"
  },
  {
    "text": "OK. So that's how we're\ngoing to solve it. I don't want to go\ninto the details now, mostly because we're\nrunning out of time.",
    "start": "4689290",
    "end": "4698190"
  },
  {
    "text": "Are there any questions?",
    "start": "4698190",
    "end": "4700940"
  }
]