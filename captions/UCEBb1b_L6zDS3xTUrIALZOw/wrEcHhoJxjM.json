[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation, or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  GILBERT STRANG: OK.",
    "start": "18140",
    "end": "23810"
  },
  {
    "text": "So what I promised, and\nnow I'm going to do it, to talk about gradient\ndescent and its descendants.",
    "start": "23810",
    "end": "34010"
  },
  {
    "text": "So from the basic\ngradient descent formula, which we all know-- let me just write that down--",
    "start": "34010",
    "end": "40640"
  },
  {
    "text": "the new point is the old point.",
    "start": "40640",
    "end": "47150"
  },
  {
    "text": "We're going downwards,\nso with a minus sign that's the step size.",
    "start": "47150",
    "end": "52520"
  },
  {
    "text": "And we compute the\ngradient at XK. So we're descending\nin the direction",
    "start": "52520",
    "end": "60109"
  },
  {
    "text": "of the negative gradient. And that's the basic formula,\nand is in every book studied.",
    "start": "60110",
    "end": "69470"
  },
  {
    "text": "So my main reference for\nsome of these lectures",
    "start": "69470",
    "end": "78980"
  },
  {
    "text": "is the book by Stephen Boyd\nand Lieven Vandenberghe.",
    "start": "78980",
    "end": "84910"
  },
  {
    "text": "And I mention again, Professor\nBoyd is talking, in this room,",
    "start": "84910",
    "end": "90350"
  },
  {
    "text": "next week Wednesday,\nThursday and he's speaking",
    "start": "90350",
    "end": "95360"
  },
  {
    "text": "somewhere on Friday at 4:30-- and of course,\nabout optimization.",
    "start": "95360",
    "end": "101310"
  },
  {
    "text": "And he's a good lecturer,\nyeah, very good. OK. So there's steepest descent,\nand I've redrawn my picture",
    "start": "101310",
    "end": "109789"
  },
  {
    "text": "from last time. Now I'll go over there\nand look at that picture. But let me say what's coming.",
    "start": "109790",
    "end": "116390"
  },
  {
    "text": "So that's pretty standard-- very standard, you could say.",
    "start": "116390",
    "end": "122799"
  },
  {
    "text": "Then this is the improvement\nthat is widely used.",
    "start": "122800",
    "end": "129889"
  },
  {
    "text": "Adding in something\ncalled momentum to avoid the zigzag that\nwe're going to see over there.",
    "start": "129889",
    "end": "135920"
  },
  {
    "text": "And there's another\nway to do it. There's a Russian\nguy named Nesterov. His papers are not easy\nto read, but they've",
    "start": "135920",
    "end": "142920"
  },
  {
    "text": "got serious content. And one thing he did\nwas find an alternative",
    "start": "142920",
    "end": "149370"
  },
  {
    "text": "to momentum that also\naccelerated the descent.",
    "start": "149370",
    "end": "157110"
  },
  {
    "text": "So this produces-- these\nboth produce faster descent",
    "start": "157110",
    "end": "162870"
  },
  {
    "text": "than the ordinary one. OK. And then you know,\nlooking ahead,",
    "start": "162870",
    "end": "168930"
  },
  {
    "text": "that for problems\nof machine learning, they're so large\nthat the gradient--",
    "start": "168930",
    "end": "176750"
  },
  {
    "text": "we have so many variables-- all those weights are variables. And that could-- hundreds of\nthousands is not uncommon.",
    "start": "176750",
    "end": "187420"
  },
  {
    "text": "So then the gradient becomes\na pretty big calculation,",
    "start": "187420",
    "end": "192690"
  },
  {
    "text": "and we just don't have\nto do it all at once. We don't have to change-- so XK is a vector of all the\nweights, or-- and using--",
    "start": "192690",
    "end": "202900"
  },
  {
    "text": "and our equations are\nmatching the training data.",
    "start": "202900",
    "end": "209739"
  },
  {
    "text": "So we don't have to use all\nthe training data at once, and we don't. We take a batch of\ntraining data, like one.",
    "start": "209740",
    "end": "218969"
  },
  {
    "text": "But that's sort of inefficient\nin the opposite direction, to do them one at a time. So we don't know want to\ndo them one at a time,",
    "start": "218970",
    "end": "226209"
  },
  {
    "text": "but we don't want to do\nall million at a time. So the compromise\nis a mini batch.",
    "start": "226210",
    "end": "233580"
  },
  {
    "text": "So stochastic gradient descent\ndoes a mini batch at a time--",
    "start": "233580",
    "end": "239490"
  },
  {
    "text": "a mini batch of\ntraining, of samples",
    "start": "239490",
    "end": "244710"
  },
  {
    "text": "training data each step. ",
    "start": "244710",
    "end": "252620"
  },
  {
    "text": "And it can choose\nthem stochastically-- meaning randomly, or\nmore systematically--",
    "start": "252620",
    "end": "259560"
  },
  {
    "text": "but we do a batch at a time. And that will come after the--",
    "start": "259560",
    "end": "266266"
  },
  {
    "text": "it'll come next week after a\nmarathon, of course, on Monday.",
    "start": "266266",
    "end": "272340"
  },
  {
    "text": "OK. So let me just go back to\nthat picture for a moment, but then the real\ncontent of today",
    "start": "272340",
    "end": "279419"
  },
  {
    "text": "is this one with momentum added. OK.",
    "start": "279420",
    "end": "284550"
  },
  {
    "text": "I just-- I probably haven't\ngot the picture perfect yet. I'm just not an artist,\nbut I think I'm closer.",
    "start": "284550",
    "end": "296620"
  },
  {
    "text": "So this is-- those\nare the level sets. Those are the sets f\nof x equal constant.",
    "start": "296620",
    "end": "306550"
  },
  {
    "text": "And in our model problem,\nf of x is x1 squared--",
    "start": "306550",
    "end": "311590"
  },
  {
    "text": "or let's say x squared plus\nb-y squared equal constant with",
    "start": "311590",
    "end": "316750"
  },
  {
    "text": "small b-- b below 1 and maybe far below 1.",
    "start": "316750",
    "end": "322240"
  },
  {
    "text": "So those are ellipses.  Those are the equations\nof an ellipse,",
    "start": "322240",
    "end": "327860"
  },
  {
    "text": "and that's what I tried to draw. And if b is small,\nthen the ellipses are long and thin like that.",
    "start": "327860",
    "end": "335300"
  },
  {
    "text": "And now, what's the picture? You start with a point x\nnought, and you descend",
    "start": "335300",
    "end": "342470"
  },
  {
    "text": "in the steepest direction. So the steepest direction is\nperpendicular to the level set,",
    "start": "342470",
    "end": "347740"
  },
  {
    "text": "right? Perpendicular to the ellipse. So you're down, down, down. You're passing\nthrough more ellipses,",
    "start": "347740",
    "end": "353099"
  },
  {
    "text": "more ellipses, more ellipses. Eventually, your tangent to a--",
    "start": "353100",
    "end": "358620"
  },
  {
    "text": "it seems to me it\nhas to be tangent. I didn't read this, but\nlooks reasonable to me that",
    "start": "358620",
    "end": "365180"
  },
  {
    "text": "the farthest in level\nset-- farthest in ellipse--",
    "start": "365180",
    "end": "371759"
  },
  {
    "text": "you're tangent to, and then\nyou start going up again. So that's the optimal point\nto stop to end that step.",
    "start": "371760",
    "end": "379440"
  },
  {
    "text": "And then where does\nthe next step go? Well, you're here. You're on an ellipse.",
    "start": "379440",
    "end": "385080"
  },
  {
    "text": "That's a level set. You want to move in\nthe gradient direction. That's perpendicular\nto the level set.",
    "start": "385080",
    "end": "391350"
  },
  {
    "text": "So you're going\ndown somewhere here, and you're passing again\nthrough more and more ellipses,",
    "start": "391350",
    "end": "397620"
  },
  {
    "text": "until you're tangent to\na smaller ellipse here.",
    "start": "397620",
    "end": "404760"
  },
  {
    "text": "And you see the zigzag pattern. And that zigzag\npattern is what we",
    "start": "404760",
    "end": "410460"
  },
  {
    "text": "see, by formula, in Boyd's book,\nand many other places, too.",
    "start": "410460",
    "end": "420240"
  },
  {
    "text": "The formula has those\npowers of the magic number.",
    "start": "420240",
    "end": "426990"
  },
  {
    "text": "So we start at the-- start at the point b1,\nand follow this path.",
    "start": "426990",
    "end": "441300"
  },
  {
    "text": "Then the X's are the same\nb times this quantity",
    "start": "441300",
    "end": "447120"
  },
  {
    "text": "to the kth power. And here is that quantity,\nb minus 1 over b plus 1.",
    "start": "447120",
    "end": "454180"
  },
  {
    "text": "So you see, for a small b,\nthat's a negative number. So it's flipping sine in the\nX's, as we saw in the picture.",
    "start": "454180",
    "end": "462720"
  },
  {
    "text": "At least that part of\nthe picture is correct. The y's don't flip sine.",
    "start": "462720",
    "end": "468390"
  },
  {
    "text": "So this was XK, and\nwhen k is 0, we got b.",
    "start": "468390",
    "end": "475380"
  },
  {
    "text": "YK is, I think, is\nnot flipping sine.",
    "start": "475380",
    "end": "483600"
  },
  {
    "text": "So that looks good. And then FK-- the value of f--",
    "start": "483600",
    "end": "489780"
  },
  {
    "text": "also was the same quantity. FK is that same quantity\nto the kth times f0.",
    "start": "489780",
    "end": "500910"
  },
  {
    "text": "So that quantity's\nall important. And so the purpose\nof today's lecture,",
    "start": "500910",
    "end": "507930"
  },
  {
    "text": "is to tell you what\nthe momentum term--",
    "start": "507930",
    "end": "513159"
  },
  {
    "text": "what improvement--\nwhat change that brings in the basic\nsteepest descent formula.",
    "start": "513159",
    "end": "519349"
  },
  {
    "text": "I'm going to add on\nanother term, which is going to have some-- give us some memory\nof the previous step.",
    "start": "519350",
    "end": "528120"
  },
  {
    "text": "And so when I do that, I want\nto track that kind of descent",
    "start": "528120",
    "end": "538080"
  },
  {
    "text": "for the new-- for the accelerated descent,\nand track it and see",
    "start": "538080",
    "end": "546410"
  },
  {
    "text": "what improvement the\nmomentum term brings. And so the final result\nwill be to tell you",
    "start": "546410",
    "end": "554450"
  },
  {
    "text": "the improvement in the-- produced by the momentum term. Maybe while I have\nyour attention,",
    "start": "554450",
    "end": "561740"
  },
  {
    "text": "I'll tell you what it is now. And then will come the\ndetails, the algebra.",
    "start": "561740",
    "end": "569430"
  },
  {
    "text": "And to me-- so this\nis as my own thought-- ",
    "start": "569430",
    "end": "576360"
  },
  {
    "text": "it's a miracle that\nthe algebra, which is straightforward-- you really\nsee the value of eigenvectors.",
    "start": "576360",
    "end": "583740"
  },
  {
    "text": "We explained eigenvectors in\nclass, but here you see why--",
    "start": "583740",
    "end": "589410"
  },
  {
    "text": "how to use them. That is really a good exercise. But to me it's a miracle that\nthe expression with momentum",
    "start": "589410",
    "end": "603940"
  },
  {
    "text": "is very much like that\nexpression, but different, of course. The decay-- the term that tells\nyou how fast the decay is--",
    "start": "603940",
    "end": "618100"
  },
  {
    "text": "is smaller. So you're taking kth power. So let me-- I'll write that\ndown, if that's all right.",
    "start": "618100",
    "end": "625070"
  },
  {
    "text": "I didn't plan to do-- to reveal the final result at\nthe beginning of the lecture.",
    "start": "625070",
    "end": "634010"
  },
  {
    "text": "But I think you want to\nsee where we're going. So with momentum-- and we\nhave to see what that means--",
    "start": "634010",
    "end": "648500"
  },
  {
    "text": "this term of 1 minus b\nover 1 plus b becomes--",
    "start": "648500",
    "end": "656810"
  },
  {
    "text": "changes to-- 1 minus\nsquare root of b over 1",
    "start": "656810",
    "end": "666090"
  },
  {
    "text": "plus square root of b. So I mentioned that\nbefore, but I don't think",
    "start": "666090",
    "end": "672930"
  },
  {
    "text": "I wrote it down as clearly. So the miracle to me is to\nget such a nice expression",
    "start": "672930",
    "end": "680940"
  },
  {
    "text": "for the-- because you'll see the algebra\nis-- it works, but it involves",
    "start": "680940",
    "end": "691030"
  },
  {
    "text": "more terms because\nof momentum, involves doing a minimization\nof eigenvalues,",
    "start": "691030",
    "end": "697420"
  },
  {
    "text": "and yet it comes out nicely. And then you have to see\nthe importance of that.",
    "start": "697420",
    "end": "702860"
  },
  {
    "text": "So let me-- I will just\ntake the same example that I mentioned before. If b is 1 over 100, then\nthis is 0.99 over 1.01.",
    "start": "702860",
    "end": "715750"
  },
  {
    "text": "I think that these-- there's a square here, 2k.",
    "start": "715750",
    "end": "725470"
  },
  {
    "text": "So if we're-- so I'll just\nkeep the square there,",
    "start": "725470",
    "end": "730829"
  },
  {
    "text": "no big change, but I'm looking\nat-- now here-- at the square. ",
    "start": "730830",
    "end": "737450"
  },
  {
    "text": "Maybe squares are everywhere. OK. So that's close to 1.",
    "start": "737450",
    "end": "746730"
  },
  {
    "text": "And now let's compare\nthat with what we have. So if b is 1 over 100--",
    "start": "746730",
    "end": "752509"
  },
  {
    "text": "so I'm taking b\nto be 1 over 100-- and square root\nof b is 1 over 10. So this is 0.9 over 1.1 squared.",
    "start": "752510",
    "end": "765090"
  },
  {
    "text": "And there's a\ntremendous-- that's a lot smaller than that is. Right.",
    "start": "765090",
    "end": "771180"
  },
  {
    "text": "9/10-- 9 over 11,\ncompared to 99 over 101.",
    "start": "771180",
    "end": "777120"
  },
  {
    "text": "This one is\ndefinitely-- oh, sorry.",
    "start": "777120",
    "end": "783690"
  },
  {
    "text": "Yeah, this reduction factor\nis well below that one.",
    "start": "783690",
    "end": "790560"
  },
  {
    "text": "So it's a good thing. It's worth doing. And now what does it involve?",
    "start": "790560",
    "end": "797019"
  },
  {
    "text": "So I'll write down the\nexpression for the stochastic--",
    "start": "797020",
    "end": "805240"
  },
  {
    "text": "here we go. OK. So here's one way to see it. The new X is the old\nX minus the gradient.",
    "start": "805240",
    "end": "816130"
  },
  {
    "text": " And now comes an extra term,\nwhich gives us a little memory.",
    "start": "816130",
    "end": "827300"
  },
  {
    "text": "Well, sorry. The algebra is slightly nicer\nif I write it a little bit",
    "start": "827300",
    "end": "834700"
  },
  {
    "text": "differently. I'll create a new quantity,\nZK, with a step size.",
    "start": "834700",
    "end": "846480"
  },
  {
    "text": "OK. ",
    "start": "846480",
    "end": "855800"
  },
  {
    "text": "So if I took ZK to\nbe just the gradient, that would be steepest descent. Nothing has changed.",
    "start": "855800",
    "end": "862370"
  },
  {
    "text": "But instead, I'm\ngoing to take ZK-- well, it's leading term\nwill be the gradient.",
    "start": "862370",
    "end": "868399"
  },
  {
    "text": " But here comes\nthe momentum term.",
    "start": "868400",
    "end": "874080"
  },
  {
    "text": "I add on a multiple beta. One way to do it is\nof the previous Z.",
    "start": "874080",
    "end": "880050"
  },
  {
    "text": "So the Z is the\nsearch direction. Z is the gradient\nyou're traveling. It is the direction\nyou're moving.",
    "start": "880050",
    "end": "887730"
  },
  {
    "text": "So it's different from\nthat direction there. That direction was the gradient.",
    "start": "887730",
    "end": "896430"
  },
  {
    "text": "This direction is the\ngradient corrected by a memory",
    "start": "896430",
    "end": "902310"
  },
  {
    "text": "term, a momentum term. And one way to interpret that\nis to say that that ball--",
    "start": "902310",
    "end": "909780"
  },
  {
    "text": "is to think of a heavy ball,\ninstead of just a point. I think of a heavy ball.",
    "start": "909780",
    "end": "917610"
  },
  {
    "text": "It, instead of bouncing back and\nforth as uselessly as this one,",
    "start": "917610",
    "end": "927060"
  },
  {
    "text": "it tends to-- it still bounces, of course,\non the sides of the level set--",
    "start": "927060",
    "end": "932640"
  },
  {
    "text": "but it comes down\nthe valley faster. And that's the effect of this.",
    "start": "932640",
    "end": "937840"
  },
  {
    "text": "So you could play with\ndifferent adjustment",
    "start": "937840",
    "end": "943710"
  },
  {
    "text": "terms, different corrections. So I'll follow through this one.",
    "start": "943710",
    "end": "948780"
  },
  {
    "text": "Nesterov had another way to\nmake a change in the formula, and there are certainly\nothers beyond that.",
    "start": "948780",
    "end": "957140"
  },
  {
    "text": "OK, so how do we\nanalyze that one? Well, the real point is,\nwe've sort of, by taking--",
    "start": "957140",
    "end": "967610"
  },
  {
    "text": "by involving the\nprevious step, we now have a three level method\ninstead of a two level method,",
    "start": "967610",
    "end": "976620"
  },
  {
    "text": "you could say. This involves only\nlevel K plus 1 and level K. The formulas\nnow involve K plus 1K,",
    "start": "976620",
    "end": "988320"
  },
  {
    "text": "and K minus 1. It's just like going from\na first order differential",
    "start": "988320",
    "end": "995490"
  },
  {
    "text": "equation to a second order\ndifferential equation. ",
    "start": "995490",
    "end": "1002540"
  },
  {
    "text": "I'm not really thinking\nthat K is a time variable. But in the analogy, K\ncould be a time variable.",
    "start": "1002540",
    "end": "1010290"
  },
  {
    "text": "So that here we had a\nfirst order equation. If I wanted to model\nthat, it's sort",
    "start": "1010290",
    "end": "1017149"
  },
  {
    "text": "of a DXDT coming in\nthere, equal gradient. And these models\nare highly useful",
    "start": "1017150",
    "end": "1024260"
  },
  {
    "text": "and developed for sort\nof a continuous model",
    "start": "1024260",
    "end": "1029390"
  },
  {
    "text": "of steepest descent-- a continuous motion instead\nof the discrete motion.",
    "start": "1029390",
    "end": "1039740"
  },
  {
    "text": "OK. So that would-- that\ncontinuous model for that guy",
    "start": "1039740",
    "end": "1045140"
  },
  {
    "text": "would be a first order in time. For this one, it'll be\nsecond order in time.",
    "start": "1045140",
    "end": "1050590"
  },
  {
    "text": "And second order\nequations, of course, and there'd be\nconstant coefficients",
    "start": "1050590",
    "end": "1055730"
  },
  {
    "text": "in our model problem. And the thing about a second\norder equation that we all know",
    "start": "1055730",
    "end": "1061730"
  },
  {
    "text": "is, there is a momentum term-- a damping term, you could say--",
    "start": "1061730",
    "end": "1069440"
  },
  {
    "text": "in multiplying the\nfirst derivative. So that's what a second\norder equation offers--",
    "start": "1069440",
    "end": "1079399"
  },
  {
    "text": "is the inclusion\nof a damping term which isn't present in\nthe original first order.",
    "start": "1079400",
    "end": "1087800"
  },
  {
    "text": "OK. So how do we analyze this? ",
    "start": "1087800",
    "end": "1093590"
  },
  {
    "text": "I have to-- so how do\nyou analyze second order differential equations?",
    "start": "1093590",
    "end": "1099080"
  },
  {
    "text": "You write them as a system\nof two first order equations. So that's exactly what\nwe're going to do here,",
    "start": "1099080",
    "end": "1105170"
  },
  {
    "text": "in the discrete case. We're going to see-- because we have two equations.",
    "start": "1105170",
    "end": "1111890"
  },
  {
    "text": "And they're first\norder, and we can-- let me play with them for\na moment to make them good.",
    "start": "1111890",
    "end": "1118940"
  },
  {
    "text": "OK. So I'm going to have-- so this will go to two\nfirst order equations,",
    "start": "1118940",
    "end": "1126680"
  },
  {
    "text": "in which the first one-- I'm just going to\ncopy, XK plus 1",
    "start": "1126680",
    "end": "1131720"
  },
  {
    "text": "is XK minus that step size ZK. ",
    "start": "1131720",
    "end": "1140330"
  },
  {
    "text": "Yeah. OK. Yeah. OK. Time the previous\ntimes step here--",
    "start": "1140330",
    "end": "1147150"
  },
  {
    "text": "the next time step on the left. OK. So I just copied that. Now this one I'm going\nto increase K by 1.",
    "start": "1147150",
    "end": "1157090"
  },
  {
    "text": "So in order to have that\nlooking to match this, I'll write that as ZK plus 1,\nand I'll bring the K, saying,",
    "start": "1157090",
    "end": "1167159"
  },
  {
    "text": "grad FK plus 1 equal beta ZK.",
    "start": "1167160",
    "end": "1175920"
  },
  {
    "text": "That work with you? I just, in this thing,\ninstead of looking at it at K,",
    "start": "1175920",
    "end": "1182850"
  },
  {
    "text": "I went to K plus 1. And I put the K plus\n1 terms on one side.",
    "start": "1182850",
    "end": "1190080"
  },
  {
    "text": "OK. So now I have a-- ",
    "start": "1190080",
    "end": "1196600"
  },
  {
    "text": "let's see. Let's remember, we're doing-- the model we're doing is F\nequal a half X transpose SX.",
    "start": "1196600",
    "end": "1204060"
  },
  {
    "text": "So the gradient of F is SX.",
    "start": "1204060",
    "end": "1209250"
  },
  {
    "text": "So what I've written there,\nfor gradient, is really-- I know what that gradient is.",
    "start": "1209250",
    "end": "1215980"
  },
  {
    "text": "So that's really SX K plus 1. ",
    "start": "1215980",
    "end": "1223190"
  },
  {
    "text": "OK.  How to analyze that.",
    "start": "1223190",
    "end": "1230039"
  },
  {
    "text": "What happens as K travels\nforward 1, 2, 3, 4, 5?",
    "start": "1230040",
    "end": "1236940"
  },
  {
    "text": "We have a constant coefficient\nproblem at every step. The XZ variable is getting\nmultiplied by a matrix.",
    "start": "1236940",
    "end": "1246190"
  },
  {
    "text": "So here's XZ at K plus 1.",
    "start": "1246190",
    "end": "1252480"
  },
  {
    "text": "And over here will\nbe XZ at step K.",
    "start": "1252480",
    "end": "1258750"
  },
  {
    "text": "And I just have to\nfigure out what matrix is multiplying here and here.",
    "start": "1258750",
    "end": "1265580"
  },
  {
    "text": "OK. And I guess here I see it. For the first equation\nhas a 1 and a minus S,",
    "start": "1265580",
    "end": "1272730"
  },
  {
    "text": "looks like, in the first row. And it has a beta\nin the second row. And here the first equation\nhas a 1, 0 in that row.",
    "start": "1272730",
    "end": "1284100"
  },
  {
    "text": "And then a minus\nS. So I'll put in minus S, multiplying\nXK plus 1, and then",
    "start": "1284100",
    "end": "1291840"
  },
  {
    "text": "the 1 that multiplies ZK plus 1. Is that all right? ",
    "start": "1291840",
    "end": "1299399"
  },
  {
    "text": "Sorry. I've got two S's, and I\ndidn't draw that one-- didn't write that\none in large enough, and I'd planned to\nerase it anyway.",
    "start": "1299400",
    "end": "1307370"
  },
  {
    "text": "This is the step sizes. This is the matrix. But it's not quite\nfitting its place.",
    "start": "1307370",
    "end": "1317050"
  },
  {
    "text": "This is the point where I'm\ngoing to use eigenvalues. I'm going to follow\neach eigenvalue.",
    "start": "1317050",
    "end": "1325450"
  },
  {
    "text": "That's the whole point. When I follow each\neigenvalue-- each eigenvector, I should say--",
    "start": "1325450",
    "end": "1331000"
  },
  {
    "text": "I'll follow each eigenvector\nof S. So let's do that.",
    "start": "1331000",
    "end": "1337510"
  },
  {
    "text": "So eigenvectors of S-- what\nare we going to call those?",
    "start": "1337510",
    "end": "1343450"
  },
  {
    "text": "Lambda, probably. So SX equal lambda X. I\nthink that's what's coming.",
    "start": "1343450",
    "end": "1350850"
  },
  {
    "text": " Or Q. To do things\nright, I want to remember",
    "start": "1350850",
    "end": "1361560"
  },
  {
    "text": "that S is a positive,\ndefinite symmetric matrix. That's why I call\nit S, instead of A.",
    "start": "1361560",
    "end": "1367620"
  },
  {
    "text": "So I really should\ncall the eigen-- it doesn't matter,\nbut to be on the ball,",
    "start": "1367620",
    "end": "1374940"
  },
  {
    "text": "let me call the eigenvector\nQ, and the eigenvalue lambda. ",
    "start": "1374940",
    "end": "1382224"
  },
  {
    "text": "OK.  So now I want to follow\nthis eigenvector.",
    "start": "1382224",
    "end": "1390720"
  },
  {
    "text": "So I'm supposing that\nXK is sum CK times Q.",
    "start": "1390720",
    "end": "1397929"
  },
  {
    "text": "I'm assuming that X is in the-- tracking this eigenvector.",
    "start": "1397930",
    "end": "1403389"
  },
  {
    "text": "And I'm going to assume that ZK\nis some other constant times Q.",
    "start": "1403390",
    "end": "1410350"
  },
  {
    "text": "Everybody, do you see? That's a vector and\nthat's a vector. And I want scalars. I want to attract\njust scalar CK and DK.",
    "start": "1410350",
    "end": "1420580"
  },
  {
    "text": "So that's really\nwhat I have here. This was a little tricky,\nbecause X here is a vector,",
    "start": "1420580",
    "end": "1427630"
  },
  {
    "text": "and two components\nare N components. I didn't want that. I really wanted just to\ntrack an eigenvector.",
    "start": "1427630",
    "end": "1436030"
  },
  {
    "text": "Once I've settled\non the direction Q, everything is-- all vectors\nare in the direction of Q.",
    "start": "1436030",
    "end": "1442690"
  },
  {
    "text": "So we just have numbers\nC and D to track. OK. So I'm going to rewrite\nthis correctly, as, yeah.",
    "start": "1442690",
    "end": "1455659"
  },
  {
    "text": "Well, let me keep going\nwith this little formula. Then what will-- I needed an SX.",
    "start": "1455660",
    "end": "1462320"
  },
  {
    "text": "What will SXK be? If XK is in the direction of\nthe eigenvector Q, and it's CK--",
    "start": "1462320",
    "end": "1473000"
  },
  {
    "text": "what happens when\nI multiply by S?  Q was an eigenvector.",
    "start": "1473000",
    "end": "1479170"
  },
  {
    "text": "So the multiplying\nby S gives me a-- AUDIENCE: Eigenvalue. GILBERT STRANG:\nEigenvalue, right?",
    "start": "1479170",
    "end": "1484327"
  },
  {
    "text": "So it's CK lambda Q.\nEverything is a multiple of Q.",
    "start": "1484327",
    "end": "1490810"
  },
  {
    "text": "And it's only those\nmultiples I'm looking for, the C's and the D's.",
    "start": "1490810",
    "end": "1496000"
  },
  {
    "text": "And then the lambda\ncomes into the S term. Yeah. I think that's probably\nall I need to do this.",
    "start": "1496000",
    "end": "1505970"
  },
  {
    "text": "And then the gradient-- yeah. So that's the\ngradient, of course. This is the gradient of F at K--",
    "start": "1505970",
    "end": "1515169"
  },
  {
    "text": "is that one. OK. So instead of this,\nlet me just write what's happening if I'm tracking\nthe coefficients CK plus 1",
    "start": "1515170",
    "end": "1526400"
  },
  {
    "text": "and DK plus 1. Then what I really meant\nto have there is 1, 0.",
    "start": "1526400",
    "end": "1534200"
  },
  {
    "text": "And minus S is a minus lambda. ",
    "start": "1534200",
    "end": "1543450"
  },
  {
    "text": "Is that right? Yeah. When I multiply the eigenvector\nby S, I'm just getting--",
    "start": "1543450",
    "end": "1550150"
  },
  {
    "text": "oh, it's a lambda times a CK. Yeah. Lambda times the\nCK-- that's good.",
    "start": "1550150",
    "end": "1557280"
  },
  {
    "text": "I think that that's the left\nhand side of my equation. And on the right hand\nside, I have here.",
    "start": "1557280",
    "end": "1570540"
  },
  {
    "text": "That's 1. And this was the\nscalar, the step size. And this was the\nother coefficient.",
    "start": "1570540",
    "end": "1576299"
  },
  {
    "text": "It's the beta. So I want to choose-- what's my purpose now?",
    "start": "1576300",
    "end": "1582550"
  },
  {
    "text": "That gives me the-- what happens at every\nstep to the C and D.",
    "start": "1582550",
    "end": "1590769"
  },
  {
    "text": "So I want to choose the\ntwo things that I have-- I'm free to choose\nare S and beta.",
    "start": "1590770",
    "end": "1596740"
  },
  {
    "text": "So that's my big job-- choose S and beta. ",
    "start": "1596740",
    "end": "1604140"
  },
  {
    "text": "OK. Now I-- to make-- oh, let me just shape this\nby multiplying the inverse",
    "start": "1604140",
    "end": "1612350"
  },
  {
    "text": "of that, and get it over here. So that will really-- you'll see everything. So CK plus 1, DK plus 1.",
    "start": "1612350",
    "end": "1622520"
  },
  {
    "text": "What's the inverse of 1, 0? Oh, I don't think\nI want to-- that would have a tough time\nfinding an inverse.",
    "start": "1622520",
    "end": "1632030"
  },
  {
    "text": "It was a 1, wasn't it? ",
    "start": "1632030",
    "end": "1639180"
  },
  {
    "text": "Yeah. OK. So I'm going to multiply by\nthe inverse of that matrix to get it over here.",
    "start": "1639180",
    "end": "1646330"
  },
  {
    "text": "And what's the inverse\nof 1, 1 minus lambda? It's 1, 1 plus lambda.",
    "start": "1646330",
    "end": "1652419"
  },
  {
    "text": "So that the inverse\nbrought it over here, times this matrix, 1, 0 beta,\nand minus the step size.",
    "start": "1652420",
    "end": "1661040"
  },
  {
    "text": "That's what multiply CK DK.  So we have these\nsimple, beautiful steps",
    "start": "1661040",
    "end": "1669310"
  },
  {
    "text": "which come from tracking\none eigenvector-- makes the whole problem scalar.",
    "start": "1669310",
    "end": "1675789"
  },
  {
    "text": "So I multiply those two\nmatrices and I finally get the matrix that I\nreally have to think about.",
    "start": "1675790",
    "end": "1681610"
  },
  {
    "text": "1, 0 times that'll be 1 minus\nS. Lambda 1 times that'll be a lambda there.",
    "start": "1681610",
    "end": "1688030"
  },
  {
    "text": "And minus lambda S plus beta. Beta minus lambda\nS. That's the matrix",
    "start": "1688030",
    "end": "1695950"
  },
  {
    "text": "that we see at every step. Let me call that matrix R.",
    "start": "1695950",
    "end": "1707422"
  },
  {
    "text": "So I've done some algebra--\nmore than I would always do in a lecture--",
    "start": "1707422",
    "end": "1713440"
  },
  {
    "text": "but it's really my-- I wouldn't do it if it\nwasn't nice algebra. What's the conclusion?",
    "start": "1713440",
    "end": "1719680"
  },
  {
    "text": "That conclusion is that\nwith the momentum term-- with this number beta available\nto choose, as well as S,",
    "start": "1719680",
    "end": "1729610"
  },
  {
    "text": "the step-- the coefficient\nof the eigenvector",
    "start": "1729610",
    "end": "1737080"
  },
  {
    "text": "is multiplied at every\nstep by that matrix R. R is that matrix.",
    "start": "1737080",
    "end": "1744200"
  },
  {
    "text": "And of course, that matrix\ninvolves the eigenvalue. ",
    "start": "1744200",
    "end": "1750120"
  },
  {
    "text": "So we have to think about-- what do we want to do now?",
    "start": "1750120",
    "end": "1757110"
  },
  {
    "text": "We want to choose\nbeta and S to make",
    "start": "1757110",
    "end": "1763780"
  },
  {
    "text": "R as small as possible, right? We want to make R as\nsmall as possible.",
    "start": "1763780",
    "end": "1769350"
  },
  {
    "text": "And we are free to choose beta\nand S, but R depends on lambda. ",
    "start": "1769350",
    "end": "1776780"
  },
  {
    "text": "So I'm going to make\nit as small as possible over the whole range\nof possible lambdas.",
    "start": "1776780",
    "end": "1782240"
  },
  {
    "text": "So let me-- so now\nhere we really go. ",
    "start": "1782240",
    "end": "1789409"
  },
  {
    "text": "So we have lambda between sum.",
    "start": "1789410",
    "end": "1795740"
  },
  {
    "text": "These are the eigenvalue\nof S. And what we know--",
    "start": "1795740",
    "end": "1804520"
  },
  {
    "text": "what's reasonable to\nknow-- is a lower bound. It's a positive.",
    "start": "1804520",
    "end": "1810160"
  },
  {
    "text": "This is a symmetric\npositive definite matrix. A lower bound and an upper\nbound, for example, m was B,",
    "start": "1810160",
    "end": "1820880"
  },
  {
    "text": "and M was 1, in\nthat 2 by 2 problem. And this is what we know,\nthat the eigenvalues",
    "start": "1820880",
    "end": "1828310"
  },
  {
    "text": "are between m and M. And\nthe ratio of m to M--",
    "start": "1828310",
    "end": "1838850"
  },
  {
    "text": "well, if I write-- ",
    "start": "1838850",
    "end": "1845380"
  },
  {
    "text": "this is the key quantity.",
    "start": "1845380",
    "end": "1850880"
  },
  {
    "text": "And what's it called? Lambda max divided by\nlambda min is the-- AUDIENCE: Condition number.",
    "start": "1850880",
    "end": "1856800"
  },
  {
    "text": "GILBERT STRANG:\nCondition number. Right. This is all sometimes\nwritten kappa-- Greek letter kappa-- the\ncondition number of S.",
    "start": "1856800",
    "end": "1870420"
  },
  {
    "text": "And when that's big, then the\nproblem is going to be harder. When that's 1, then my\nmatrix is just a multiple",
    "start": "1870420",
    "end": "1879779"
  },
  {
    "text": "of the identity matrix. And the problem is trivial. When capital M and\nsmall m are the same,",
    "start": "1879780",
    "end": "1887710"
  },
  {
    "text": "then that's saying that\nthe largest and smallest eigenvalues are identical,\nthat the matrix is",
    "start": "1887710",
    "end": "1894840"
  },
  {
    "text": "a multiple of the identity. That's the condition number one. But the bad one is when it's\n1 over b, in our example,",
    "start": "1894840",
    "end": "1907980"
  },
  {
    "text": "and that could be very large. OK. That's where we\nhave our problem.",
    "start": "1907980",
    "end": "1916680"
  },
  {
    "text": "Let me just insert about the\nordinary gradient descent.",
    "start": "1916680",
    "end": "1925830"
  },
  {
    "text": "Of course, the textbooks find a\nestimate for how fast that is.",
    "start": "1925830",
    "end": "1931470"
  },
  {
    "text": "And of course, it\ndepends on that number. Yeah. So it depends on that\nnumber, and you exactly",
    "start": "1931470",
    "end": "1939809"
  },
  {
    "text": "saw how it depended\non that number. Right.",
    "start": "1939810",
    "end": "1945210"
  },
  {
    "text": "But now we have a\ndifferent problem. And we're going to finish it. OK. So what's my job?",
    "start": "1945210",
    "end": "1951000"
  },
  {
    "text": "I'm going to choose S and beta\nto keep the eigenvalues of R.",
    "start": "1951000",
    "end": "1958650"
  },
  {
    "text": "So let's give the\neigenvalues of R a name. So R-- let's say R has\neigenvalues e1, that",
    "start": "1958650",
    "end": "1970490"
  },
  {
    "text": "depends on the lambda and\nthe S and the beta and e2.",
    "start": "1970490",
    "end": "1976840"
  },
  {
    "text": " So those are the\neigenvalues of R--",
    "start": "1976840",
    "end": "1983700"
  },
  {
    "text": "just giving a letter to them. So what's our job?",
    "start": "1983700",
    "end": "1989429"
  },
  {
    "text": "We want to choose S and beta\nto make those eigenvalues as",
    "start": "1989430",
    "end": "1994680"
  },
  {
    "text": "small as possible. Right? Small eigenvalues-- if R has\nsmall eigenvalues, its powers--",
    "start": "1994680",
    "end": "2004770"
  },
  {
    "text": "every step multiplies by\nR. So the convergence rate",
    "start": "2004770",
    "end": "2009930"
  },
  {
    "text": "with momentum is-- depends on the powers\nof R getting small fast.",
    "start": "2009930",
    "end": "2016409"
  },
  {
    "text": "It depends on the\neigenvalues being small. We want to minimize\nthe largest eigenvalue.",
    "start": "2016410",
    "end": "2028500"
  },
  {
    "text": "So I'll say the\nmaximum of e1 and e2--",
    "start": "2028500",
    "end": "2036000"
  },
  {
    "text": "that's our job. Minimize-- we want to choose\nS and beta to minimize",
    "start": "2036000",
    "end": "2041430"
  },
  {
    "text": "the largest eigenvalue. Because if there's\none small eigenvalue, but the other is big, then the\nother one is going to kill us.",
    "start": "2041430",
    "end": "2048679"
  },
  {
    "text": "So we have to get\nboth eigenvalues down. And of course, those\ndepend on lambda.",
    "start": "2048679",
    "end": "2056239"
  },
  {
    "text": "E1 depends on lambda. So we have a little\nalgebra problem. And this is what I\ndescribed as a miracle--",
    "start": "2056239",
    "end": "2063679"
  },
  {
    "text": "the fact that this\nlittle algebra problem-- the eigenvalues of that\nmatrix, e1 and e2, which",
    "start": "2063679",
    "end": "2070969"
  },
  {
    "text": "depend on lambda in some way. And we want to make\nboth e1 and e2 small--",
    "start": "2070969",
    "end": "2079158"
  },
  {
    "text": "the maximum of those-- of them. And we have to do it for\nall the eigenvalues lambda,",
    "start": "2079159",
    "end": "2087050"
  },
  {
    "text": "because we have to-- we're now thinking-- we've\nbeen tracking each eigenvector.",
    "start": "2087050",
    "end": "2094369"
  },
  {
    "text": "So that gave us 1-- so this is for all\npossible lambda.",
    "start": "2094370",
    "end": "2099930"
  },
  {
    "text": "So we have to decide, what do\nI mean by all possible lambda? And I mean all lambda that\nare between some m and M.",
    "start": "2099930",
    "end": "2112910"
  },
  {
    "text": "There is a beautiful problem. You have a 2 by 2 matrix.",
    "start": "2112910",
    "end": "2118790"
  },
  {
    "text": "You can find its eigenvalues. They depend on lambda.",
    "start": "2118790",
    "end": "2124609"
  },
  {
    "text": "And what we-- all we know\nabout lambda is it's between m and cap M. And also, they\nalso depend on S and beta--",
    "start": "2124610",
    "end": "2132920"
  },
  {
    "text": "the two parameters\nwe can choose. And we want to choose\nthose parameters, so that for all the\npossible eigenvalues,",
    "start": "2132920",
    "end": "2143060"
  },
  {
    "text": "the larger of the\ntwo eigenvalues will be as small as possible. That's-- it's a\nlittle bit of algebra,",
    "start": "2143060",
    "end": "2151040"
  },
  {
    "text": "but do you see that\nthat's the tricky-- that-- I shouldn't say\ntricky, because it comes out--",
    "start": "2151040",
    "end": "2159680"
  },
  {
    "text": "this is the one that is a\nmiracle in the simplicity of the solution.",
    "start": "2159680",
    "end": "2165270"
  },
  {
    "text": "OK. And I'm going to-- in fact, maybe I'll move over\nhere to write the answer. ",
    "start": "2165270",
    "end": "2173930"
  },
  {
    "text": "OK. And I just want to\nsay that miracles",
    "start": "2173930",
    "end": "2179690"
  },
  {
    "text": "don't happen so often in math. There is-- all of mathematics--\nthe whole point of math",
    "start": "2179690",
    "end": "2186470"
  },
  {
    "text": "is to explain miracles. So there is something\nto explain here,",
    "start": "2186470",
    "end": "2193850"
  },
  {
    "text": "and I don't have my\nfinger on it yet. Because-- anyway, it happens.",
    "start": "2193850",
    "end": "2201230"
  },
  {
    "text": "So let me tell you what the\nright S, and the right beta, and the resulting\nminimum eigenvalue are.",
    "start": "2201230",
    "end": "2213500"
  },
  {
    "text": "So again, they depend\non little m and big M.",
    "start": "2213500",
    "end": "2220300"
  },
  {
    "text": "That's a very nice\nfeature, which we expect. And they depend on the ratio.",
    "start": "2220300",
    "end": "2227680"
  },
  {
    "text": "OK. So that ratio-- all right. Let's see it. OK. So the best S--",
    "start": "2227680",
    "end": "2233275"
  },
  {
    "start": "2233275",
    "end": "2238750"
  },
  {
    "text": "the S optimal has the formula 2\nover square root of lambda max.",
    "start": "2238750",
    "end": "2249470"
  },
  {
    "text": "That's the square root of M\nand the squared of m squared.",
    "start": "2249470",
    "end": "2257290"
  },
  {
    "text": "Amazing OK. And beta optimal turns out\nto be the square root of M",
    "start": "2257290",
    "end": "2269020"
  },
  {
    "text": "minus the square of little\nm, over the square root of M plus the square root of\nlittle m, all squared.",
    "start": "2269020",
    "end": "2277592"
  },
  {
    "text": "And of course, we know\nwhat these numbers are-- 1 and beta, in\nour model problem. That's where I'm going to\nget this square root of--",
    "start": "2277592",
    "end": "2286720"
  },
  {
    "text": "this is 1 minus the\nsquare root-- oh sorry, b. This is 1 minus the\nsquare root of b.",
    "start": "2286720",
    "end": "2293049"
  },
  {
    "text": "In fact, for our example-- well, let me just write\nwhat they would be.",
    "start": "2293050",
    "end": "2299670"
  },
  {
    "text": "2 over 1 plus square\nroot of b squared,",
    "start": "2299670",
    "end": "2305079"
  },
  {
    "text": "and 1 minus square root\nof b over 1 plus square-- you see where this is--",
    "start": "2305080",
    "end": "2313530"
  },
  {
    "text": "1 minus square root of b is\nbeginning to appear in that. It appears in this\nsolution to this problem.",
    "start": "2313530",
    "end": "2318910"
  },
  {
    "text": "And then I have to\ntell you what the-- ",
    "start": "2318910",
    "end": "2325089"
  },
  {
    "text": "how small do these\noptimal choices make the eigenvalues\nof R, right?",
    "start": "2325090",
    "end": "2332520"
  },
  {
    "text": "This is what we're really\npaying attention to, because",
    "start": "2332520",
    "end": "2337600"
  },
  {
    "text": "if the eigenvalues-- that matrix tells us what\nhappens at every step. And its eigenvalues have to be\nsmall to get fast convergence.",
    "start": "2337600",
    "end": "2346860"
  },
  {
    "text": "So how small are they? Well they involve this-- ",
    "start": "2346860",
    "end": "2353480"
  },
  {
    "text": "yeah. So it's the number\nthat I've seen. So in this case, the e's--",
    "start": "2353480",
    "end": "2361630"
  },
  {
    "text": "the eigenvalues of R--",
    "start": "2361630",
    "end": "2369299"
  },
  {
    "text": "that's the iterating matrix-- are below-- now you're going\nto see the 1 minus square root",
    "start": "2369300",
    "end": "2376560"
  },
  {
    "text": "of b over 1 plus\nsquare root of b-- I think, maybe, squared.",
    "start": "2376560",
    "end": "2383220"
  },
  {
    "text": "Let me just see. Yeah. It happens to come\nout that number again.",
    "start": "2383220",
    "end": "2390450"
  },
  {
    "text": "So that's the conclusion. That with the right\nchoice of S and beta,",
    "start": "2390450",
    "end": "2397790"
  },
  {
    "text": "by adding this look back\nterm-- look back one step--",
    "start": "2397790",
    "end": "2403490"
  },
  {
    "text": "you get this improvement. And it happens, and you see\nit in practice, of course.",
    "start": "2403490",
    "end": "2413490"
  },
  {
    "text": "You'll see it exactly. And so you do the\njob to use momentum.",
    "start": "2413490",
    "end": "2426309"
  },
  {
    "text": "Now I'm going to mention\nwhat the Nesterov-- Nesterov had a slightly\ndifferent way to do it,",
    "start": "2426310",
    "end": "2433600"
  },
  {
    "text": "and I'll tell you what that is. But it's the same idea--\nget a second thing.",
    "start": "2433600",
    "end": "2440319"
  },
  {
    "text": "So let's see if I can find that. Yeah, Nesterov. OK. ",
    "start": "2440320",
    "end": "2451250"
  },
  {
    "text": "Here we go. So let me bring\nNesterov's name down. ",
    "start": "2451250",
    "end": "2461740"
  },
  {
    "text": "So that's basically what I\nwanted to say about number 1.",
    "start": "2461740",
    "end": "2467320"
  },
  {
    "text": "And when you see\nNesterov, you'll see that it's a similar idea\nof involving the previous time",
    "start": "2467320",
    "end": "2474910"
  },
  {
    "text": "value. OK. There are very popular\nmethods in use now",
    "start": "2474910",
    "end": "2484720"
  },
  {
    "text": "for machine learning\nthat involve-- by a simple formula--",
    "start": "2484720",
    "end": "2489940"
  },
  {
    "text": "all the previous\nvalues, by sort of a-- just by an addition\nof a bunch of terms.",
    "start": "2489940",
    "end": "2496970"
  },
  {
    "text": "So it's really-- so it\ngoes under the names",
    "start": "2496970",
    "end": "2504160"
  },
  {
    "text": "adagrad, or others.",
    "start": "2504160",
    "end": "2510970"
  },
  {
    "text": "Those of you who already\nknow about machine learning will know what I'm\nspeaking about.",
    "start": "2510970",
    "end": "2515980"
  },
  {
    "text": "And I'll say more about those. Yeah. But it doesn't involve\na separate coefficient",
    "start": "2515980",
    "end": "2522789"
  },
  {
    "text": "for each previous\nvalue, or that would be a momentous amount of work.",
    "start": "2522790",
    "end": "2528880"
  },
  {
    "text": "So now I just want to tell\nyou what Nesterov is, and then we're good. OK. Nesterov's idea.",
    "start": "2528880",
    "end": "2534880"
  },
  {
    "text": " Let me bring that down.",
    "start": "2534880",
    "end": "2540820"
  },
  {
    "text": "Shoot this up. Bring down Nesterov. ",
    "start": "2540820",
    "end": "2551059"
  },
  {
    "text": "Because he had an idea that\nyou might not have thought of. Somehow the momentum\nidea was pretty natural--",
    "start": "2551060",
    "end": "2558790"
  },
  {
    "text": "to use that previous value. And actually, I\nwould like to know what happens if you use two\nprevious values, or three",
    "start": "2558790",
    "end": "2566810"
  },
  {
    "text": "previous values. Can you then get improvements\non this convergence rate",
    "start": "2566810",
    "end": "2577309"
  },
  {
    "text": "by going back two\nsteps or three steps? If I'd use the analogy\nwith ordinary differential",
    "start": "2577310",
    "end": "2585170"
  },
  {
    "text": "equations, maybe you know. So there are backward\ndifference formulas.",
    "start": "2585170",
    "end": "2592720"
  },
  {
    "text": "Do you know about those for-- those would be in\nMATLAB software,",
    "start": "2592720",
    "end": "2598380"
  },
  {
    "text": "and all other software. Backward differences--\nso maybe you go back two steps or four steps.",
    "start": "2598380",
    "end": "2607040"
  },
  {
    "text": "If you're doing\nplanetary calculations, if you're an astronomer, you go\nback maybe seven or eight steps",
    "start": "2607040",
    "end": "2613460"
  },
  {
    "text": "to get super high accuracy. So that doesn't seem\nto have happened yet,",
    "start": "2613460",
    "end": "2620050"
  },
  {
    "text": "but it's should happen here-- to go back more. But Nesterov has this\ndifferent way to go back.",
    "start": "2620050",
    "end": "2628010"
  },
  {
    "text": "So his formula is XK\nplus 1-- the new X-- is YK-- so he's introducing\nsomething a little different--",
    "start": "2628010",
    "end": "2638360"
  },
  {
    "text": "minus S gradient f at YK.",
    "start": "2638360",
    "end": "2643790"
  },
  {
    "start": "2643790",
    "end": "2649100"
  },
  {
    "text": "I'm a little surprised\nabout that YK, but this is the point, here-- that the gradient\nis being evaluated",
    "start": "2649100",
    "end": "2655940"
  },
  {
    "text": "at some different point. And then he has to give a\nformula for that to track those",
    "start": "2655940",
    "end": "2662750"
  },
  {
    "text": "Y's. So the Y's are like\nthe X's, but they",
    "start": "2662750",
    "end": "2667760"
  },
  {
    "text": "are shifted a little bit by some\nterm-- and beta would be fine.",
    "start": "2667760",
    "end": "2673230"
  },
  {
    "text": "Oh no. Yeah-- beta-- have\nwe got Nesterov here?",
    "start": "2673230",
    "end": "2679830"
  },
  {
    "text": "Yes. Nesterov has a factor gamma in.",
    "start": "2679830",
    "end": "2685150"
  },
  {
    "text": "Yeah. So all right. Let me try to get this right.",
    "start": "2685150",
    "end": "2690170"
  },
  {
    "text": "OK. All right. On a previous line, I've written\nthe whole Nesterov thing.",
    "start": "2690170",
    "end": "2696890"
  },
  {
    "text": "Here, let's see a\nNesterov completely. And then it'll break-- then this is the step that\nbreaks it into two first order.",
    "start": "2696890",
    "end": "2704010"
  },
  {
    "text": "But you'll see the\nmain formula here. XK plus 1 is XK. ",
    "start": "2704010",
    "end": "2710750"
  },
  {
    "text": "And then a beta times\nXK minus XK minus 1.",
    "start": "2710750",
    "end": "2719600"
  },
  {
    "text": "So that's a momentum term. And then a typical gradient.",
    "start": "2719600",
    "end": "2726560"
  },
  {
    "text": "But now here is\nNesterov speaking up. Nesterov evaluates the gradient\nnot at XK, not at XK minus 1.",
    "start": "2726560",
    "end": "2735710"
  },
  {
    "text": "But it his own, Nesterov point. So this is Nesterov's\nfavorite point.",
    "start": "2735710",
    "end": "2741950"
  },
  {
    "text": "Gamma XK minus XK minus 1. Some point, part\nway along that step.",
    "start": "2741950",
    "end": "2754950"
  },
  {
    "text": "So this point-- because gamma is\ngoing to be some non-integer--",
    "start": "2754950",
    "end": "2761190"
  },
  {
    "text": "this evaluation point\nfor the gradient of f is a little\nunexpected and weird,",
    "start": "2761190",
    "end": "2767569"
  },
  {
    "text": "because it's not a mesh point. It's somewhere between.",
    "start": "2767570",
    "end": "2773470"
  },
  {
    "text": "OK. Yeah. And then that-- so that involves\nXK plus 1, XK, and XK minus 1.",
    "start": "2773470",
    "end": "2789410"
  },
  {
    "text": "So it's a second order-- there's a second\norder method here.",
    "start": "2789410",
    "end": "2795580"
  },
  {
    "text": "We're going to-- to analyze it,\nwe're going to go through this same process of writing it\nas two first order steps--",
    "start": "2795580",
    "end": "2805260"
  },
  {
    "text": "two first-- two single step-- two one step from K to K plus\n1 coupled with one step thing.",
    "start": "2805260",
    "end": "2818460"
  },
  {
    "text": "Follow that same thing\nthrough, and then the result is, the same factor\nappears for him.",
    "start": "2818460",
    "end": "2828280"
  },
  {
    "text": "The same factor-- this is also-- so the point is, this is\nfor momentum and Nesterov,",
    "start": "2828280",
    "end": "2844140"
  },
  {
    "text": "with some constant--\ndifferent by some constant.",
    "start": "2844140",
    "end": "2853529"
  },
  {
    "text": "But the key quantity is that\none and that appears in both.",
    "start": "2853530",
    "end": "2861840"
  },
  {
    "text": "So I don't propose, of\ncourse, to repeat these steps",
    "start": "2861840",
    "end": "2869550"
  },
  {
    "text": "for Nesterov. But you see what you could do.",
    "start": "2869550",
    "end": "2874770"
  },
  {
    "text": "You see that it involves\nK minus 1, KNK plus 1. You write it as--",
    "start": "2874770",
    "end": "2881550"
  },
  {
    "text": "you follow an eigenvector. You write it as a coupled\nsystem of-- that's a one step.",
    "start": "2881550",
    "end": "2888900"
  },
  {
    "text": "That has a matrix. You find the matrix. You find the eigenvalues\nof the matrix.",
    "start": "2888900",
    "end": "2894840"
  },
  {
    "text": "You make those eigenvalues\nas small as possible. And you have optimized the\ncoefficients in Nesterov.",
    "start": "2894840",
    "end": "2902320"
  },
  {
    "text": "OK. That's sort of a lot\nof algebra that's",
    "start": "2902320",
    "end": "2907800"
  },
  {
    "text": "at the heart of accelerated\ngradient descent.",
    "start": "2907800",
    "end": "2912840"
  },
  {
    "text": "And of course, it's\nworth doing because it's a tremendous saving in\nthe convergence rate.",
    "start": "2912840",
    "end": "2922589"
  },
  {
    "text": "OK. Anybody running in the\nmarathon or just watching?",
    "start": "2922590",
    "end": "2929640"
  },
  {
    "text": "It's possible to run, you know. Anyway, I'll see you after\nthe marathon, next Wednesday.",
    "start": "2929640",
    "end": "2937350"
  },
  {
    "text": "And Professor Boyd\nwill also see you.",
    "start": "2937350",
    "end": "2941300"
  }
]