[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  CHARLES E. LEISERSON: Hi,\nit's my great pleasure",
    "start": "18140",
    "end": "23860"
  },
  {
    "text": "to introduce, again, TB Schardl. TB is not only a fabulous,\nworld-class performance",
    "start": "23860",
    "end": "36640"
  },
  {
    "text": "engineer, he is a world-class\nperformance meta engineer.",
    "start": "36640",
    "end": "44020"
  },
  {
    "text": "In other words, building the\ntools and such to make it",
    "start": "44020",
    "end": "52030"
  },
  {
    "text": "so that people can\nengineer fast code. And he's the author\nof the technology",
    "start": "52030",
    "end": "59559"
  },
  {
    "text": "that we're using in\nour compiler, the taper technology that's in the open\ncompiler for parallelism.",
    "start": "59560",
    "end": "65810"
  },
  {
    "text": "So he implemented all of that,\nand all the optimizations, and so forth, which has\ngreatly improved the quality",
    "start": "65810",
    "end": "72159"
  },
  {
    "text": "of the programming environment. So today, he's going to talk\nabout something near and dear",
    "start": "72160",
    "end": "78310"
  },
  {
    "text": "to his heart,\nwhich is compilers, and what they can and cannot do.",
    "start": "78310",
    "end": "84778"
  },
  {
    "text": "TAO B. SCHARDL:\nGreat, thank you very much for that introduction. Can everyone hear\nme in the back?",
    "start": "84778",
    "end": "90700"
  },
  {
    "text": "Yes, great. All right, so as\nI understand it, last lecture you talked about\nmulti-threaded algorithms.",
    "start": "90700",
    "end": "97780"
  },
  {
    "text": "And you spent the lecture\nstudying those algorithms, analyzing them in a\ntheoretical sense,",
    "start": "97780",
    "end": "102970"
  },
  {
    "text": "essentially analyzing their\nasymptotic running times, work and span complexity.",
    "start": "102970",
    "end": "108520"
  },
  {
    "text": "This lecture is not that at all. We're not going to\ndo that kind of math",
    "start": "108520",
    "end": "113650"
  },
  {
    "text": "anywhere in the course\nof this lecture. Instead, this lecture is going\nto take a look at compilers,",
    "start": "113650",
    "end": "119920"
  },
  {
    "text": "as professor mentioned, and what\ncompilers can and cannot do. ",
    "start": "119920",
    "end": "126440"
  },
  {
    "text": "So the last time, you\nsaw me standing up here was back in lecture five.",
    "start": "126440",
    "end": "131500"
  },
  {
    "text": "And during that\nlecture we talked about LLVM IR and\nx8664 assembly,",
    "start": "131500",
    "end": "137530"
  },
  {
    "text": "and how C code got translated\ninto assembly code via LLVM IR.",
    "start": "137530",
    "end": "145750"
  },
  {
    "text": "In this lecture,\nwe're going to talk more about what happens between\nthe LLVM IR and assembly",
    "start": "145750",
    "end": "152050"
  },
  {
    "text": "stages. And, essentially, that's what\nhappens when the compiler is allowed to edit and optimize the\ncode in its IR representation,",
    "start": "152050",
    "end": "161200"
  },
  {
    "text": "while it's producing\nthe assembly. So last time, we were\ntalking about this IR,",
    "start": "161200",
    "end": "167379"
  },
  {
    "text": "and the assembly. And this time, they called\nthe compiler guy back, I suppose, to tell you about\nthe boxes in the middle.",
    "start": "167380",
    "end": "175260"
  },
  {
    "text": "Now, even though you're\npredominately dealing with C code within this class, I hope\nthat some of the lessons from",
    "start": "175260",
    "end": "182080"
  },
  {
    "text": "today's lecture you will be able\nto take away into any job that you pursue in the future,\nbecause there are a lot",
    "start": "182080",
    "end": "190060"
  },
  {
    "text": "of languages today that do end\nup being compiled, C and C++,",
    "start": "190060",
    "end": "196000"
  },
  {
    "text": "Rust, Swift, even\nHaskell, Julia, Halide, the list goes on and on. And those languages\nall get compiled",
    "start": "196000",
    "end": "201640"
  },
  {
    "text": "for a variety of\ndifferent what we call backends, different machine\narchitectures, not just x86-64.",
    "start": "201640",
    "end": "209080"
  },
  {
    "text": "And, in fact, a lot\nof those languages get compiled using very\nsimilar compilation technology",
    "start": "209080",
    "end": "217450"
  },
  {
    "text": "to what you have in\nthe Clang LLVM compiler that you're using in this class. In fact, many of\nthose languages today",
    "start": "217450",
    "end": "225040"
  },
  {
    "text": "are optimized by LLVM itself. LLVM is the internal\nengine within the compiler",
    "start": "225040",
    "end": "230590"
  },
  {
    "text": "that actually does all\nof the optimization. So that's my hope, that the\nlessons you'll learn here today",
    "start": "230590",
    "end": "237100"
  },
  {
    "text": "don't just apply to 172. They'll, in fact,\napply to software that you use and develop\nfor many years on the road.",
    "start": "237100",
    "end": "245740"
  },
  {
    "text": "But let's take a step\nback, and ask ourselves, why bother studying the\ncompiler optimizations at all?",
    "start": "245740",
    "end": "251950"
  },
  {
    "text": "Why should we take\na look at what's going on within this, up to this\npoint, black box of software?",
    "start": "251950",
    "end": "259810"
  },
  {
    "text": "Any ideas? Any suggestions? ",
    "start": "259810",
    "end": "267909"
  },
  {
    "text": "In the back? AUDIENCE: [INAUDIBLE] ",
    "start": "267910",
    "end": "273607"
  },
  {
    "text": "TAO B. SCHARDL: You\ncan avoid manually trying to optimize things\nthat the compiler will do for you, great answer.",
    "start": "273607",
    "end": "278910"
  },
  {
    "text": "Great, great answer. Any other answers? ",
    "start": "278910",
    "end": "287449"
  },
  {
    "text": "AUDIENCE: You learn\nhow to best write your code to take advantages\nof the compiler optimizations.",
    "start": "287450",
    "end": "293565"
  },
  {
    "text": "TAO B. SCHARDL:\nYou can learn how to write your code to take\nadvantage of the compiler optimizations, how to suggest\nto the compiler what it should",
    "start": "293565",
    "end": "302260"
  },
  {
    "text": "or should not do as\nyou're constructing your program, great\nanswer as well.",
    "start": "302260",
    "end": "307720"
  },
  {
    "text": "Very good, in the front. AUDIENCE: It might\nhelp for debugging if the compiler has bugs.",
    "start": "307720",
    "end": "313306"
  },
  {
    "text": " TAO B. SCHARDL:\nIt can absolutely help for debugging when the\ncompiler itself has bugs.",
    "start": "313306",
    "end": "319630"
  },
  {
    "text": "The compiler is a big\npiece of software. And you may have noticed that a\nlot of software contains bugs.",
    "start": "319630",
    "end": "325120"
  },
  {
    "text": "The compiler is no exception. And it helps to understand where\nthe compiler might have made",
    "start": "325120",
    "end": "330520"
  },
  {
    "text": "a mistake, or where the\ncompiler simply just didn't do what you thought\nit should be able to do.",
    "start": "330520",
    "end": "337420"
  },
  {
    "text": "Understanding more of what\nhappens in the compiler can demystify some\nof those oddities.",
    "start": "337420",
    "end": "344860"
  },
  {
    "text": "Good answer. Any other thoughts? AUDIENCE: It's fun. ",
    "start": "344860",
    "end": "350897"
  },
  {
    "text": "TAO B. SCHARDL: It's fun. Well, OK, so in my\ncompletely biased opinion,",
    "start": "350898",
    "end": "355930"
  },
  {
    "text": "I would agree that\nit's fun to understand what the compiler does. You may have different opinions.",
    "start": "355930",
    "end": "361870"
  },
  {
    "text": "That's OK. I won't judge. So I put together\na list of reasons",
    "start": "361870",
    "end": "368650"
  },
  {
    "text": "why, in general, we\nmay care about what goes on inside the compiler.",
    "start": "368650",
    "end": "374070"
  },
  {
    "text": "I highlighted that last\npoint from this list, my bad. Compilers can have a really\nbig impact on software.",
    "start": "374070",
    "end": "383572"
  },
  {
    "text": "It's kind of like this. Imagine that you're working\non some software project. And you have a\nteammate on your team",
    "start": "383572",
    "end": "390050"
  },
  {
    "text": "he's pretty quiet\nbut extremely smart. And what that teammate does\nis whenever that teammate gets",
    "start": "390050",
    "end": "396220"
  },
  {
    "text": "access to some\ncode, they jump in and immediately start trying\nto make that code work faster.",
    "start": "396220",
    "end": "403509"
  },
  {
    "text": "And that's really cool, because\nthat teammate does good work. And, oftentimes, you see that\nwhat the teammate produces",
    "start": "403510",
    "end": "409509"
  },
  {
    "text": "is, indeed, much faster\ncode than what you wrote. Now, in other industries,\nyou might just sit back",
    "start": "409510",
    "end": "415390"
  },
  {
    "text": "and say, this teammate\ndoes fantastic work. Maybe they don't\ntalk very often. But that's OK.",
    "start": "415390",
    "end": "421420"
  },
  {
    "text": "Teammate, you do you. But in this class, we're\nperformance engineers. We want to understand what that\nteammate did to the software.",
    "start": "421420",
    "end": "429190"
  },
  {
    "text": "How did that teammate get\nso much performance out of the code? The compiler is kind\nof like that teammate.",
    "start": "429190",
    "end": "436330"
  },
  {
    "text": "And so understanding\nwhat the compiler does is valuable in that sense.",
    "start": "436330",
    "end": "441670"
  },
  {
    "text": "As mentioned before,\ncompilers can save you performance engineering work. If you understand\nthat the compiler can",
    "start": "441670",
    "end": "448300"
  },
  {
    "text": "do some optimization\nfor you, then you don't have to do it yourself. And that means that\nyou can continue",
    "start": "448300",
    "end": "454030"
  },
  {
    "text": "writing simple, and readable,\nand maintainable code without sacrificing performance.",
    "start": "454030",
    "end": "460780"
  },
  {
    "text": "You can also understand the\ndifferences between the source code and whatever you might\nsee show up in either the LLVM",
    "start": "460780",
    "end": "466600"
  },
  {
    "text": "IR or the assembly,\nif you have to look at the assembly language\nproduced for your executable.",
    "start": "466600",
    "end": "476260"
  },
  {
    "text": "And compilers can make mistakes. Sometimes, that's because of\na genuine bug in the compiler. And other times, it's\nbecause the compiler just",
    "start": "476260",
    "end": "483400"
  },
  {
    "text": "couldn't understand something\nabout what was going on. And having some insight into how\nthe compiler reasons about code",
    "start": "483400",
    "end": "490300"
  },
  {
    "text": "can help you understand why\nthose mistakes were made, or figure out ways to work\naround those mistakes,",
    "start": "490300",
    "end": "497620"
  },
  {
    "text": "or let you write meaningful\nbug reports to the compiler developers.",
    "start": "497620",
    "end": "502955"
  },
  {
    "text": "And, of course,\nunderstanding computers can help you use them\nmore effectively. Plus, I think it's fun.",
    "start": "502955",
    "end": "510010"
  },
  {
    "text": "So the first thing to\nunderstand about a compiler is a basic anatomy of\nhow the compiler works.",
    "start": "510010",
    "end": "515440"
  },
  {
    "text": "The compiler takes\nas input LLVM IR. And up until this\npoint, we thought of it",
    "start": "515440",
    "end": "520900"
  },
  {
    "text": "as just a big black box. That does stuff to the IR,\nand out pops more LLVM IR,",
    "start": "520900",
    "end": "527740"
  },
  {
    "text": "but it's somehow optimized. In fact, what's going\non within that black box",
    "start": "527740",
    "end": "533350"
  },
  {
    "text": "the compiler is\nexecuting a sequence of what we call transformation\npasses on the code.",
    "start": "533350",
    "end": "538990"
  },
  {
    "text": "Each transformation pass\ntakes a look at its input, and analyzes that\ncode, and then tries",
    "start": "538990",
    "end": "545380"
  },
  {
    "text": "to edit the code in\nan effort to optimize the code's performance. Now, a transformation pass might\nend up running multiple times.",
    "start": "545380",
    "end": "553459"
  },
  {
    "text": "And those passes\nrun in some order. That order ends up being\na predetermined order",
    "start": "553460",
    "end": "559990"
  },
  {
    "text": "that the compiler\nwriters found to work pretty well on their tests.",
    "start": "559990",
    "end": "565240"
  },
  {
    "text": "That's about the\nlevel of insight that went into picking the order. It seems to work well.",
    "start": "565240",
    "end": "572990"
  },
  {
    "text": "Now, some good news,\nin terms of trying to understand what\nthe compiler does, you can actually just ask the\ncompiler, what did you do?",
    "start": "572990",
    "end": "580709"
  },
  {
    "text": "And you've already used\nthis functionality, as I understand, in some\nof your assignments. You've already\nasked the compiler",
    "start": "580710",
    "end": "586960"
  },
  {
    "text": "to give you a\nreport specifically about whether or not it\ncould vectorize some code.",
    "start": "586960",
    "end": "592300"
  },
  {
    "text": "But, in fact, LLVM, the\ncompiler you have access to, can produce reports not\njust for factorization,",
    "start": "592300",
    "end": "598870"
  },
  {
    "text": "but for a lot of the\ndifferent transformation passes that it tries to perform.",
    "start": "598870",
    "end": "603898"
  },
  {
    "text": "And there's some\nsyntax that you have to pass to the compiler,\nsome compiler flags that you have to specify in\norder to get those reports.",
    "start": "603898",
    "end": "610995"
  },
  {
    "text": "Those are described\non the slide. I won't walk you\nthrough that text. You can look at the\nslides afterwards.",
    "start": "610995",
    "end": "616540"
  },
  {
    "text": "At the end of the day, the\nstring that you're passing is actually a\nregular expression. If you know what\nregular expressions are,",
    "start": "616540",
    "end": "621910"
  },
  {
    "text": "great, then you can\nuse that to narrow down the search for your report.",
    "start": "621910",
    "end": "627140"
  },
  {
    "text": "If you don't, and you just\nwant to see the whole report, just provide dot star as a\nstring and you're good to go.",
    "start": "627140",
    "end": "632945"
  },
  {
    "text": "That's the good news. You can get the compiler to\ntell you exactly what it did. The bad news is that when you\nask the compiler what it did,",
    "start": "632945",
    "end": "641220"
  },
  {
    "text": "it will give you a report. And the report looks\nsomething like this.",
    "start": "641220",
    "end": "646957"
  },
  {
    "text": "In fact, I've highlighted\nmost of the report for this particular\npiece of code, because the report ends\nup being very long.",
    "start": "646957",
    "end": "653403"
  },
  {
    "text": "And as you might\nhave noticed just from reading some of the\ntexts, there are definitely English words in this text.",
    "start": "653403",
    "end": "660970"
  },
  {
    "text": "And there are pointers to pieces\nof code that you've compiled.",
    "start": "660970",
    "end": "666129"
  },
  {
    "text": "But it is very jargon,\nand hard to understand. ",
    "start": "666130",
    "end": "671780"
  },
  {
    "text": "This isn't the easiest\nreport to make sense of. OK, so that's some good\nnews and some bad news",
    "start": "671780",
    "end": "678782"
  },
  {
    "text": "about these compiler reports. The good news is, you\ncan ask the compiler. And it'll happily tell you all\nabout the things that it did.",
    "start": "678782",
    "end": "685000"
  },
  {
    "text": "It can tell you about which\ntransformation passes were successfully able to\ntransform the code.",
    "start": "685000",
    "end": "691300"
  },
  {
    "text": "It can tell you\nconclusions that it drew about its analysis of the code.",
    "start": "691300",
    "end": "697079"
  },
  {
    "text": "But the bad news\nis, these reports are kind of complicated. They can be long.",
    "start": "697080",
    "end": "702670"
  },
  {
    "text": "They use a lot of internal\ncompiler jargon, which if you're not familiar\nwith that jargon,",
    "start": "702670",
    "end": "708100"
  },
  {
    "text": "it makes it hard to understand. It also turns out that not\nall of the transformation",
    "start": "708100",
    "end": "713380"
  },
  {
    "text": "passes in the compiler give\nyou these nice reports. So you don't get to\nsee the whole picture.",
    "start": "713380",
    "end": "718820"
  },
  {
    "text": "And, in general, the\nreports don't really tell you the whole story\nabout what the compiler did or did not do.",
    "start": "718820",
    "end": "724430"
  },
  {
    "text": "And we'll see another\nexample of that later on. So part of the goal\nof today's lecture is to get some context for\nunderstanding the reports",
    "start": "724430",
    "end": "732839"
  },
  {
    "text": "that you might see if you pass\nthose flags to the compiler. And the structure\nof today's lecture",
    "start": "732840",
    "end": "739550"
  },
  {
    "text": "is basically divided\nup into two parts. First, I want to give\nyou some examples of compiler optimizations,\njust simple examples",
    "start": "739550",
    "end": "745839"
  },
  {
    "text": "so you get a sense as to how a\ncompiler mechanically reasons about the code it's given, and\ntries to optimize that code.",
    "start": "745840",
    "end": "754141"
  },
  {
    "text": "We'll take a look at how\nthe compiler optimizes a single scalar value, how\nit can optimize a structure,",
    "start": "754142",
    "end": "759459"
  },
  {
    "text": "how it can optimize\nfunction calls, and how it can optimize\nloops, just simple examples to give some flavor.",
    "start": "759460",
    "end": "766060"
  },
  {
    "text": "And then the second\nhalf of lecture, I have a few case\nstudies for you which get into diagnosing ways\nin which the compiler failed,",
    "start": "766060",
    "end": "774220"
  },
  {
    "text": "not due to bugs,\nper se, but simply didn't do an optimization you\nmight have expected it to do.",
    "start": "774220",
    "end": "780420"
  },
  {
    "text": "But, to be frank, I\nthink all those case studies are really cool. But it's not totally\ncrucial that we",
    "start": "780420",
    "end": "786519"
  },
  {
    "text": "get through every single case\nstudy during today's lecture. The slides will be\navailable afterwards.",
    "start": "786520",
    "end": "791860"
  },
  {
    "text": "So when we get to\nthat part, we'll just see how many case\nstudies we can cover. Sound good? Any questions so far?",
    "start": "791860",
    "end": "797125"
  },
  {
    "text": " All right, let's get to it.",
    "start": "797125",
    "end": "804010"
  },
  {
    "text": "Let's start with\na quick overview of compiler optimizations. So here is a summary\nof the various--",
    "start": "804010",
    "end": "810750"
  },
  {
    "text": "oh, I forgot that I\njust copied this slide",
    "start": "810750",
    "end": "816210"
  },
  {
    "text": "from a previous lecture\ngiven in this class. You might recognize this slide\nI think from lecture two.",
    "start": "816210",
    "end": "824700"
  },
  {
    "text": "Sorry about that. That's OK. We can fix this. We'll just go ahead and\nadd this slide right now.",
    "start": "824700",
    "end": "833310"
  },
  {
    "text": "We need to change the title. So let's cross that out\nand put in our new title.",
    "start": "833310",
    "end": "839400"
  },
  {
    "text": "OK, so, great, and now we\nshould double check these lists",
    "start": "839400",
    "end": "845730"
  },
  {
    "text": "and make sure that\nthey're accurate. Data structures, we'll come\nback to data structures.",
    "start": "845730",
    "end": "850980"
  },
  {
    "text": "Loops, hoisting, yeah, the\ncompiler can do hoisting. Sentinels, not\nreally, the compiler",
    "start": "850980",
    "end": "857260"
  },
  {
    "text": "is not good at sentinels. Loop unrolling, yeah, it\nabsolutely does loop unrolling. Loop fusion, yeah,\nit can, but there are",
    "start": "857260",
    "end": "865680"
  },
  {
    "text": "some restrictions that apply. Your mileage might vary. Eliminate waste iterations,\nsome restrictions might apply.",
    "start": "865680",
    "end": "873389"
  },
  {
    "text": "OK, logic, constant folding\nand propagation, yeah, it's good on that. Common subexpression\nelimination, yeah, I",
    "start": "873390",
    "end": "878880"
  },
  {
    "text": "can find common subexpressions,\nyou're fin there. It knows algebra, yeah good. Short circuiting,\nyes, absolutely.",
    "start": "878880",
    "end": "885390"
  },
  {
    "text": "Ordering tests,\ndepends on the tests-- I'll give it to the compiler.",
    "start": "885390",
    "end": "890850"
  },
  {
    "text": "But I'll say,\nrestrictions apply. Creating a fast path,\ncompilers aren't",
    "start": "890850",
    "end": "897209"
  },
  {
    "text": "that smart about fast paths. They come up with really\nboring fast paths. I'm going to take\nthat off the list.",
    "start": "897210",
    "end": "902580"
  },
  {
    "text": "Combining tests, again, it\nkind of depends on the tests. Functions, compilers are\npretty good at functions.",
    "start": "902580",
    "end": "907590"
  },
  {
    "text": "So inling, it can do that. Tail recursion elimination,\nyes, absolutely. Coarsening, not so much.",
    "start": "907590",
    "end": "915150"
  },
  {
    "text": "OK, great. Let's come back to\ndata structures, which we skipped before.",
    "start": "915150",
    "end": "920370"
  },
  {
    "text": "Packing, augmentation--\nOK, honestly, the compiler does a lot with data\nstructures but really",
    "start": "920370",
    "end": "927540"
  },
  {
    "text": "none of those things. The compiler isn't smart\nabout data structures in that particular way.",
    "start": "927540",
    "end": "933515"
  },
  {
    "text": "Really, the way\nthat the compiler is smart about data\nstructures is shown here,",
    "start": "933515",
    "end": "938730"
  },
  {
    "text": "if we expand this list to\ninclude even more compiler optimizations. Bottom line with data\nstructures, the compiler",
    "start": "938730",
    "end": "945779"
  },
  {
    "text": "knows a lot about architecture. And it really has\nput a lot of effort into figuring out how to use\nregisters really effectively.",
    "start": "945780",
    "end": "954450"
  },
  {
    "text": "Reading and writing and\nregister is super fast. Touching memory is not so fast.",
    "start": "954450",
    "end": "959530"
  },
  {
    "text": "And so the compiler works really\nhard to allocate registers, put anything that lives in memory\nordinarily into registers,",
    "start": "959530",
    "end": "968460"
  },
  {
    "text": "manipulate aggregate\ntypes to use registers, as we'll see in a couple\nof slides, align data",
    "start": "968460",
    "end": "973800"
  },
  {
    "text": "that has to live in memory. Compilers are good at that. Compilers are also\ngood at loops. We already saw some\nexample optimization",
    "start": "973800",
    "end": "980949"
  },
  {
    "text": "on the previous slide. It can vectorize. It does a lot of\nother cool stuff. Unswitching is a\ncool optimization",
    "start": "980950",
    "end": "986610"
  },
  {
    "text": "that I won't cover here. Idiom replacement, it\nfinds common patterns, and does something\nsmart with those.",
    "start": "986610",
    "end": "993000"
  },
  {
    "text": "Vision, skewing, tiling,\ninterchange, those all try to process the iterations\nof the loop in some clever way",
    "start": "993000",
    "end": "1001430"
  },
  {
    "text": "to make stuff go fast. And some restrictions apply. Those are really in\ndevelopment in LLVM.",
    "start": "1001430",
    "end": "1007750"
  },
  {
    "text": "Logic, it does a lot more with\nlogic than what we saw before. It can eliminate instructions\nthat aren't necessary.",
    "start": "1007750",
    "end": "1013480"
  },
  {
    "text": "It can do strength reduction,\nand other cool optimization. I think we saw that one\nin the Bentley slides.",
    "start": "1013480",
    "end": "1019340"
  },
  {
    "text": "It gets rid of dead code. It can do more\nidiom replacement. Branch reordering is kind\nlike reordering tests.",
    "start": "1019340",
    "end": "1025817"
  },
  {
    "text": "Global value numbering,\nanother cool optimization that we won't talk about today. Functions, it can do\nmore on switching.",
    "start": "1025817",
    "end": "1031550"
  },
  {
    "text": "It can eliminate arguments\nthat aren't necessary. So the compiler can do\na lot of stuff for you.",
    "start": "1031550",
    "end": "1036762"
  },
  {
    "text": "And at the end the day,\nwriting down this whole list is kind of a futile activity\nbecause it changes over time.",
    "start": "1036763",
    "end": "1042880"
  },
  {
    "text": "Compilers are a moving target. Compiler developers,\nthey're software engineers like you and me.",
    "start": "1042880",
    "end": "1048470"
  },
  {
    "text": "And they're clever. And they're trying to apply\nall their clever software engineering practice to\nthis compiler code base",
    "start": "1048470",
    "end": "1055220"
  },
  {
    "text": "to make it do more stuff. And so they are constantly\nadding new optimizations",
    "start": "1055220",
    "end": "1060830"
  },
  {
    "text": "to the compiler, new clever\nanalyses, all the time. So, really, what we're\ngoing to look at today",
    "start": "1060830",
    "end": "1066860"
  },
  {
    "text": "is just a couple examples\nto get a flavor for what the compiler does internally.",
    "start": "1066860",
    "end": "1072378"
  },
  {
    "text": "Now, if you want to follow along\nwith how the compiler works, the good news is,\nby and large, you can take a look at\nthe LLVM IR to see",
    "start": "1072378",
    "end": "1081080"
  },
  {
    "text": "what happens as the compiler\nprocesses your code. You don't need to\nlook out the assembly.",
    "start": "1081080",
    "end": "1086570"
  },
  {
    "text": "That's generally true. But there are some exceptions.",
    "start": "1086570",
    "end": "1091799"
  },
  {
    "text": "So, for example, if we have\nthese three snippets of C code on the left, and we look at what\nyour LLVM compiler generates,",
    "start": "1091800",
    "end": "1101300"
  },
  {
    "text": "in terms of the IR,\nwe can see that there are some optimizations\nreflected, but not too many interesting ones.",
    "start": "1101300",
    "end": "1108445"
  },
  {
    "text": "The multiply by 8 turns into\na shift left operation by 3, because 8 is a power of 2.",
    "start": "1108445",
    "end": "1113779"
  },
  {
    "text": "That's straightforward. Good, we can see that in the IR. The multiply by 15 still\nlooks like a multiply by 15.",
    "start": "1113780",
    "end": "1120440"
  },
  {
    "text": "No changes there. The divide by 71 looks\nlike a divide by 71.",
    "start": "1120440",
    "end": "1125610"
  },
  {
    "text": "Again, no changes there. Now, with arithmetic\nops, the difference",
    "start": "1125610",
    "end": "1131090"
  },
  {
    "text": "between what you\nsee in the LLVM IR and what you see\nin the assembly, this is where it's\nmost pronounced,",
    "start": "1131090",
    "end": "1136580"
  },
  {
    "text": "at least in my\nexperience, because if we take a look at these\nsame snippets of C code,",
    "start": "1136580",
    "end": "1142120"
  },
  {
    "text": "and we look at the corresponding\nx86 assembly for it, we get the stuff on the right.",
    "start": "1142120",
    "end": "1149179"
  },
  {
    "text": "And this looks different. Let's pick through\nwhat this assembly",
    "start": "1149180",
    "end": "1154280"
  },
  {
    "text": "code does one line at a time. So the first one in the C\ncode, takes the argument n,",
    "start": "1154280",
    "end": "1159500"
  },
  {
    "text": "and multiplies it by 8. And then the assembly, we\nhave this LEA instruction. Anyone remember what the\nLEA instruction does?",
    "start": "1159500",
    "end": "1166259"
  },
  {
    "text": "I see one person\nshaking their head. That's a perfectly\nreasonable response. Yeah, go for it? Load effective address,\nwhat does that mean?",
    "start": "1166260",
    "end": "1172940"
  },
  {
    "start": "1172940",
    "end": "1178039"
  },
  {
    "text": "Load the address, but don't\nactually access memory. Another way to phrase that,\ndo this address calculation.",
    "start": "1178040",
    "end": "1184750"
  },
  {
    "text": "And give me the result of\nthe address calculation. Don't read or write\nmemory at that address. Just do the calculation.",
    "start": "1184750",
    "end": "1191330"
  },
  {
    "text": "That's what loading an effective\naddress means, essentially.",
    "start": "1191330",
    "end": "1196340"
  },
  {
    "text": "But you're exactly right. The LEA instruction does\nan address calculation,",
    "start": "1196340",
    "end": "1201445"
  },
  {
    "text": "and stores the result in\nthe register on the right. Anyone remember enough about\nx86 address calculations",
    "start": "1201445",
    "end": "1208070"
  },
  {
    "text": "to tell me how that LEA in\nparticular works, the first LEA on the slide?",
    "start": "1208070",
    "end": "1213200"
  },
  {
    "text": " Yeah? AUDIENCE: [INAUDIBLE]",
    "start": "1213200",
    "end": "1221500"
  },
  {
    "text": " TAO B. SCHARDL: But before\nthe first comma, in this case nothing, gets added to the\nproduct of the second two",
    "start": "1221500",
    "end": "1229100"
  },
  {
    "text": "arguments in those parens. You're exactly right. So this LEA takes the value\n8, multiplies it by whatever",
    "start": "1229100",
    "end": "1236850"
  },
  {
    "text": "is in register RDI,\nwhich holds the value n. And it stores the\nresult into AX.",
    "start": "1236850",
    "end": "1242960"
  },
  {
    "text": "So, perfect, it does\na multiply by 8. The address calculator is\nonly capable of a small range",
    "start": "1242960",
    "end": "1250429"
  },
  {
    "text": "of operations. It can do additions. And it can multiply\nby 1, 2, 4, or 8.",
    "start": "1250430",
    "end": "1255980"
  },
  {
    "text": "That's it. So it's a really simple\ncircuit in the hardware. But it's fast.",
    "start": "1255980",
    "end": "1261409"
  },
  {
    "text": "It's optimized heavily\nby modern processors. And so if the\ncompiler can use it,",
    "start": "1261410",
    "end": "1267260"
  },
  {
    "text": "they tend to try to use\nthese LEA instructions. So good job. How about the next one?",
    "start": "1267260",
    "end": "1272390"
  },
  {
    "text": "Multiply by 15 turns into\nthese two LEA instructions. Can anyone tell\nme how these work?",
    "start": "1272390",
    "end": "1279035"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] TAO B. SCHARDL: You're\nbasically multiplying by 5",
    "start": "1279035",
    "end": "1285780"
  },
  {
    "text": "and multiplying by\n3, exactly right. We can step through\nthis as well.",
    "start": "1285780",
    "end": "1291039"
  },
  {
    "text": "If we look at the\nfirst LEA instruction, we take RDI, which\nstores the value n. We multiply that by 4.",
    "start": "1291040",
    "end": "1298200"
  },
  {
    "text": "We add it to the\noriginal value of RDI. And so that computes 4 times n,\nplus n, which is five times n.",
    "start": "1298200",
    "end": "1307590"
  },
  {
    "text": "And that result\ngets stored into AX. Could, we've effectively\nmultiplied by 5.",
    "start": "1307590",
    "end": "1312960"
  },
  {
    "text": "The next instruction\ntakes whatever is in REX, which is now 5n,\nmultiplies that by 2, adds it",
    "start": "1312960",
    "end": "1321179"
  },
  {
    "text": "to whatever is currently in\nREX, which is once again 5n. So that computes 2\ntimes 5n, plus 5n, which",
    "start": "1321180",
    "end": "1330570"
  },
  {
    "text": "is 3 times 5n, which is 15n. So just like that,\nwe've done our multiply",
    "start": "1330570",
    "end": "1336750"
  },
  {
    "text": "with two LEA instructions. How about the last one? In this last piece of code,\nwe take the arguments in RDI.",
    "start": "1336750",
    "end": "1346230"
  },
  {
    "text": "We move it into EX. We then move the\nvalue 3,871,519,817,",
    "start": "1346230",
    "end": "1356940"
  },
  {
    "text": "and put that into\nECX, as you do. We multiply those\ntwo values together.",
    "start": "1356940",
    "end": "1363980"
  },
  {
    "text": "And then we shift the\nproduct right by 38. ",
    "start": "1363980",
    "end": "1369180"
  },
  {
    "text": "So, obviously,\nthis divides by 71.  Any guesses as to\nhow this performs",
    "start": "1369180",
    "end": "1377370"
  },
  {
    "text": "the division operation we want? Both of you answered.",
    "start": "1377370",
    "end": "1383669"
  },
  {
    "text": "I might still call on you. give a little more\ntime for someone else to raise their hand.",
    "start": "1383670",
    "end": "1389223"
  },
  {
    "start": "1389223",
    "end": "1395460"
  },
  {
    "text": "Go for it. AUDIENCE: [INAUDIBLE] TAO B. SCHARDL: It has a lot to\ndo with 2 to the 38, very good.",
    "start": "1395460",
    "end": "1402420"
  },
  {
    "text": " Yeah, all right,\nany further guesses",
    "start": "1402420",
    "end": "1409049"
  },
  {
    "text": "before I give the answer away? Yeah, in the back? AUDIENCE: [INAUDIBLE]",
    "start": "1409050",
    "end": "1416653"
  },
  {
    "start": "1416654",
    "end": "1422760"
  },
  {
    "text": "TAO B. SCHARDL: Kind of. So this is what's technically\ncalled a magic number.",
    "start": "1422760",
    "end": "1428620"
  },
  {
    "text": "And, yes, it's technically\ncalled a magic number. And this magic\nnumber is equal to 2 to the 38, divided by 71, plus\n1 to deal with some rounding",
    "start": "1428620",
    "end": "1438070"
  },
  {
    "text": "effects. What this code does\nis it says, let's",
    "start": "1438070",
    "end": "1443200"
  },
  {
    "text": "compute n divided by 71, by\nfirst computing n divided by 71, times 2 to the 38, and\nthen shifting off the lower 38",
    "start": "1443200",
    "end": "1453640"
  },
  {
    "text": "bits with that shift\nright operation. And by converting the\noperation into this,",
    "start": "1453640",
    "end": "1463150"
  },
  {
    "text": "it's able to replace\nthe division operation with a multiply.",
    "start": "1463150",
    "end": "1468270"
  },
  {
    "text": "And if you remember, hopefully,\nfrom the architecture lecture, multiply operations, they're\nnot the cheapest things",
    "start": "1468270",
    "end": "1474040"
  },
  {
    "text": "in the world. But they're not too bad. Division is really expensive. If you want fast\ncode, never divide.",
    "start": "1474040",
    "end": "1481060"
  },
  {
    "text": "Also, never compute\nmodulus, or access memory. Yeah, question? AUDIENCE: Why did you choose 38?",
    "start": "1481060",
    "end": "1486550"
  },
  {
    "text": "TAO B. SCHARDL: Why\ndid I choose 38?  I think it shows 38\nbecause 38 works.",
    "start": "1486550",
    "end": "1494740"
  },
  {
    "text": "There's actually a formula for-- pretty much it\ndoesn't want to choose a value that's too large,\nor else it'll overflow.",
    "start": "1494740",
    "end": "1502408"
  },
  {
    "text": "And it doesn't want to choose\na value that's too small, or else you lose precision. So it's able to find\na balancing point.",
    "start": "1502408",
    "end": "1510130"
  },
  {
    "text": "If you want to know more\nabout magic numbers, I recommend checking out this\nbook called Hackers Delight.",
    "start": "1510130",
    "end": "1516370"
  },
  {
    "text": "For any of you who are\nfamiliar with this book, it is a book full of bit tricks. Seriously, that's\nthe entire book.",
    "start": "1516370",
    "end": "1522550"
  },
  {
    "text": "It's just a book\nfull of bit tricks. And there's a whole\nsection in there describing how you do division by various\nconstants using multiplication,",
    "start": "1522550",
    "end": "1531790"
  },
  {
    "text": "either signed or unsigned. It's very cool. But magic number to\nconvert a division",
    "start": "1531790",
    "end": "1538810"
  },
  {
    "text": "into a multiply, that's\nthe kind of thing that you might see\nfrom the assembly. That's one of these examples\nof arithmetic operations",
    "start": "1538810",
    "end": "1546390"
  },
  {
    "text": "that are really optimized\nat the very last step. But for the rest of\nthe optimizations, fortunately we can\nfocus on the IR.",
    "start": "1546390",
    "end": "1553476"
  },
  {
    "text": "Any questions about that so far?  Cool.",
    "start": "1553477",
    "end": "1559590"
  },
  {
    "text": "OK, so for the next\npart of the lecture, I want to show you a couple\nexample optimizations in terms",
    "start": "1559590",
    "end": "1565790"
  },
  {
    "text": "of the LLVM IR. And to show you\nthese optimizations, we'll have a little bit of\ncode that we'll work through,",
    "start": "1565790",
    "end": "1572870"
  },
  {
    "text": "a running example, if you will. And this running example\nwill be some code that I stole from I think it was\na serial program that simulates",
    "start": "1572870",
    "end": "1582680"
  },
  {
    "text": "the behavior of n massive\nbodies in 2D space under the law of gravitation.",
    "start": "1582680",
    "end": "1589010"
  },
  {
    "text": "So we've got a whole\nbunch of point masses. Those point masses\nhave varying masses. And we just want\nto simulate what",
    "start": "1589010",
    "end": "1594710"
  },
  {
    "text": "happens due to gravity as these\nmasses interact in the plane.",
    "start": "1594710",
    "end": "1602510"
  },
  {
    "text": "At a high level, the n\nbody code is pretty simple. We have a top level\nsimulate routine,",
    "start": "1602510",
    "end": "1608990"
  },
  {
    "text": "which just loops\nover all the time steps, during which we want\nto perform this simulation.",
    "start": "1608990",
    "end": "1615050"
  },
  {
    "text": "And at each time step, it\ncalculates the various forces acting on those\ndifferent bodies.",
    "start": "1615050",
    "end": "1620630"
  },
  {
    "text": "And then it updates the\nposition of each body, based on those forces. In order to do that calculation.",
    "start": "1620630",
    "end": "1626520"
  },
  {
    "text": "It has some internal\ndata structures, one to represent each\nbody, which contains a couple of vector types.",
    "start": "1626520",
    "end": "1632650"
  },
  {
    "text": "And we define our\nown vector type to store to double precision\nfloating point values.",
    "start": "1632650",
    "end": "1638227"
  },
  {
    "text": "Now, we don't need to see\nthe entire code in order to look at some\ncompiler optimizations.",
    "start": "1638227",
    "end": "1643910"
  },
  {
    "text": "The one routine that\nwe will take a look at is this one to\nupdate the positions. This is a simple loop that\ntakes each body, one at a time,",
    "start": "1643910",
    "end": "1653750"
  },
  {
    "text": "computes the new\nvelocity on that body, based on the forces\nacting on that body, and uses vector\noperations to do that.",
    "start": "1653750",
    "end": "1661780"
  },
  {
    "text": "Then it updates the\nposition of that body, again using these vector\noperations that we've defined.",
    "start": "1661780",
    "end": "1667800"
  },
  {
    "text": "And then it stores the results\ninto the data structure for that body. ",
    "start": "1667800",
    "end": "1674200"
  },
  {
    "text": "So all these methods\nwith this code make use of these basic routines\non 2D vectors, points in x, y,",
    "start": "1674200",
    "end": "1680770"
  },
  {
    "text": "or points in 2D space. And these routines\nare pretty simple. There is one to add two vectors.",
    "start": "1680770",
    "end": "1686600"
  },
  {
    "text": "There's another to scale a\nvector by a scalar value. And there's a third to compute\nthe length, which we won't",
    "start": "1686600",
    "end": "1693100"
  },
  {
    "text": "actually look at too much.  Everyone good so far?",
    "start": "1693100",
    "end": "1700230"
  },
  {
    "text": "OK, so let's try\nto start simple. Let's take a look at just\none of these one line vector",
    "start": "1700230",
    "end": "1707440"
  },
  {
    "text": "operations, vec scale. All vec scale does is it takes\none of these vector inputs",
    "start": "1707440",
    "end": "1716260"
  },
  {
    "text": "at a scalar value a. And it multiplies x by a, and\ny by a, and stores the results",
    "start": "1716260",
    "end": "1723190"
  },
  {
    "text": "into a vector type,\nand return to it. Great, couldn't be simpler.",
    "start": "1723190",
    "end": "1729340"
  },
  {
    "text": "If we compile this with no\noptimizations whatsoever, and we take a look\nat the LLVM IR,",
    "start": "1729340",
    "end": "1734530"
  },
  {
    "text": "we get that, which is a\nlittle more complicated",
    "start": "1734530",
    "end": "1741250"
  },
  {
    "text": "than you might imagine. ",
    "start": "1741250",
    "end": "1746260"
  },
  {
    "text": "The good news, though, is that\nif you turn on optimizations, and you just turn on the first\nlevel of optimization, just 01,",
    "start": "1746260",
    "end": "1754630"
  },
  {
    "text": "whereas we got this code before,\nnow we get this, which is far,",
    "start": "1754630",
    "end": "1760420"
  },
  {
    "text": "far simpler, and so simple I\ncan blow up the font size so you can actually read the\ncode on the slide.",
    "start": "1760420",
    "end": "1769180"
  },
  {
    "text": "So to see, again, no\noptimizations, optimizations.",
    "start": "1769180",
    "end": "1775520"
  },
  {
    "text": "So a lot of stuff happened to\noptimize this simple function.",
    "start": "1775520",
    "end": "1781990"
  },
  {
    "text": "We're going to see what those\noptimizations actually were. But, first, let's\npick apart what's",
    "start": "1781990",
    "end": "1787240"
  },
  {
    "text": "going on in this function. We have our vec scale\nroutine in LLVM IR.",
    "start": "1787240",
    "end": "1792279"
  },
  {
    "text": "It takes a structure\nas its first argument. And that's represented\nusing two doubles.",
    "start": "1792280",
    "end": "1797680"
  },
  {
    "text": "It takes a scalar as\nthe second argument. And what the operation does\nis it multiplies those two",
    "start": "1797680",
    "end": "1805809"
  },
  {
    "text": "fields by the third\nargument, the double A. It then packs those values\ninto a struct that'll return.",
    "start": "1805810",
    "end": "1816220"
  },
  {
    "text": "And, finally, it\nreturns that struct. So that's what the\noptimized code does.",
    "start": "1816220",
    "end": "1821679"
  },
  {
    "text": "Let's see actually how we\nget to this optimized code. And we'll do this\none step at a time.",
    "start": "1821680",
    "end": "1828710"
  },
  {
    "text": "Let's start by optimizing the\noperations on a single scalar value. That's why I picked\nthis example.",
    "start": "1828710",
    "end": "1834850"
  },
  {
    "text": "So we go back to the 00 code. And we just pick out\nthe operations that dealt with that scalar value.",
    "start": "1834850",
    "end": "1841450"
  },
  {
    "text": "We our scope down\nto just these lines. So the argument double A is\nthe final argument in the list.",
    "start": "1841450",
    "end": "1851110"
  },
  {
    "text": "And what we see is that\nwithin the vector scale routine, compiler to 0, we\nallocate some local storage.",
    "start": "1851110",
    "end": "1859010"
  },
  {
    "text": "We store that double A\ninto the local storage. And then later on,\nwe'll load the value out",
    "start": "1859010",
    "end": "1864490"
  },
  {
    "text": "of the local storage\nbefore the multiply. And then we load it again\nbefore the other multiply.",
    "start": "1864490",
    "end": "1872470"
  },
  {
    "text": "OK, any ideas how we could\nmake this code faster? ",
    "start": "1872470",
    "end": "1881400"
  },
  {
    "text": "Don't store in memory,\nwhat a great idea. How do we get around not\nstoring it in memory? ",
    "start": "1881400",
    "end": "1888440"
  },
  {
    "text": "Saving a register. In particular, what property of\nLLVM IR makes that really easy?",
    "start": "1888440",
    "end": "1894750"
  },
  {
    "text": " There are infinite registers. And, in fact, the argument\nis already in a register.",
    "start": "1894750",
    "end": "1904050"
  },
  {
    "text": "It's already in the register\npercent two, if I recall. So we don't need to\nmove it into a register.",
    "start": "1904050",
    "end": "1910830"
  },
  {
    "text": "It's already there. So how do we go about optimizing\nthat code in this case?",
    "start": "1910830",
    "end": "1916530"
  },
  {
    "text": "Well, let's find the places\nwhere we're using the value. And we're using the\nvalue loaded from memory.",
    "start": "1916530",
    "end": "1924750"
  },
  {
    "text": "And what we're going to do\nis just replace those loads from memory with the\noriginal argument.",
    "start": "1924750",
    "end": "1930090"
  },
  {
    "text": "We know exactly what\noperation we're trying to do. We know we're trying\nto do a multiply",
    "start": "1930090",
    "end": "1935670"
  },
  {
    "text": "by the original parameter. So we just find those two uses.",
    "start": "1935670",
    "end": "1940950"
  },
  {
    "text": "We cross them out. And we put in the input\nparameter in its place.",
    "start": "1940950",
    "end": "1947010"
  },
  {
    "text": "That make sense? Questions so far? Cool.",
    "start": "1947010",
    "end": "1953040"
  },
  {
    "text": "So now, those multipliers\naren't using the values returned by the loads.",
    "start": "1953040",
    "end": "1958247"
  },
  {
    "text": "How further can we\noptimize this code? ",
    "start": "1958247",
    "end": "1965899"
  },
  {
    "text": "Delete the loads. What else can we delete? ",
    "start": "1965900",
    "end": "1975980"
  },
  {
    "text": "So there's no address\ncalculation here just because the code is so\nsimple, but good insight.",
    "start": "1975980",
    "end": "1983090"
  },
  {
    "text": "The allocation and\nthe store, great. So those loads are dead code.",
    "start": "1983090",
    "end": "1989870"
  },
  {
    "text": "The store is dead code. The allocation is dead code. We eliminate all that dead code.",
    "start": "1989870",
    "end": "1995690"
  },
  {
    "text": "We got rid of those loads. We just used the value\nliving in the register. And we've already eliminated\na bunch of instructions.",
    "start": "1995690",
    "end": "2003080"
  },
  {
    "text": "So the net effect of that was\nto turn the code optimizer at 00 that we had in the background\ninto the code we have",
    "start": "2003080",
    "end": "2009730"
  },
  {
    "text": "in the foreground, which\nis slightly shorter, but not that much.",
    "start": "2009730",
    "end": "2016190"
  },
  {
    "text": "So it's a little bit faster,\nbut not that much faster. How do we optimize\nthis function further?",
    "start": "2016190",
    "end": "2022350"
  },
  {
    "text": "Do it for every\nvariable we have. In particular, the only\nother variable we have is a structure that\nwe're passing in.",
    "start": "2022350",
    "end": "2030130"
  },
  {
    "text": "So we want to do this kind of\noptimization on the structure.",
    "start": "2030130",
    "end": "2035300"
  },
  {
    "text": "Make sense? So let's see how we\noptimize this structure.",
    "start": "2035300",
    "end": "2042130"
  },
  {
    "text": "Now, the problem\nis that structures are harder to handle than\nindividual scalar values,",
    "start": "2042130",
    "end": "2047350"
  },
  {
    "text": "because, in general, you can't\nstore the whole structure in just a single register. It's more complicated\nto juggle all the data",
    "start": "2047350",
    "end": "2054969"
  },
  {
    "text": "within a structure. But, nevertheless,\nlet's take a look at the code that operates\non the structure,",
    "start": "2054969",
    "end": "2061239"
  },
  {
    "text": "or at least operates\non the structure that we pass in to the function.",
    "start": "2061239",
    "end": "2066620"
  },
  {
    "text": "So when we eliminate\nall the other code, we see that we've\ngot an allocation. See if I animations\nhere, yeah, I do.",
    "start": "2066620",
    "end": "2072989"
  },
  {
    "text": "We have an allocation. So we can store the\nstructure onto the stack.",
    "start": "2072989",
    "end": "2078560"
  },
  {
    "text": "Then we have an\naddress calculation that lets us store the\nfirst part of the structure onto the stack.",
    "start": "2078560",
    "end": "2085449"
  },
  {
    "text": "We have a second\naddress calculation to store the second\nfield on the stack. And later on, when\nwe need those values,",
    "start": "2085449",
    "end": "2092469"
  },
  {
    "text": "we load the first\nfield out of memory. And we load the second\nfield out of memory.",
    "start": "2092469",
    "end": "2098020"
  },
  {
    "text": "It's a very similar pattern\nto what we had before, except we've got more\ngoing on in this case.",
    "start": "2098020",
    "end": "2103990"
  },
  {
    "text": " So how do we go about\noptimizing this structure?",
    "start": "2103990",
    "end": "2112340"
  },
  {
    "text": "Any ideas, high level ideas? Ultimately, we want to get\nrid of all of the memory",
    "start": "2112340",
    "end": "2119690"
  },
  {
    "text": "references and all that\nstorage for the structure.",
    "start": "2119690",
    "end": "2126170"
  },
  {
    "text": "How do we reason through\neliminating all that stuff in a mechanical fashion, based\non what we've seen so far?",
    "start": "2126170",
    "end": "2133640"
  },
  {
    "text": "Go for it. AUDIENCE: [INAUDIBLE]",
    "start": "2133640",
    "end": "2139794"
  },
  {
    "text": " TAO B. SCHARDL: They are passed\nin using separate parameters,",
    "start": "2139794",
    "end": "2146000"
  },
  {
    "text": "separate registers if you will,\nas a quirk of how LLVM does it. So given that insight,\nhow would you optimize it?",
    "start": "2146000",
    "end": "2155158"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "2155158",
    "end": "2161599"
  },
  {
    "text": "TAO B. SCHARDL: Cross out\npercent 12, percent 6, and put in the relevant field.",
    "start": "2161600",
    "end": "2167640"
  },
  {
    "text": "Cool. Let me phrase that a\nlittle bit differently. Let's do this one\nfield at a time.",
    "start": "2167640",
    "end": "2173680"
  },
  {
    "text": "We've got a structure,\nwhich has multiple fields. Let's just take it\none step at a time.",
    "start": "2173680",
    "end": "2178900"
  },
  {
    "text": " All right, so let's\nlook at the first field.",
    "start": "2178900",
    "end": "2185980"
  },
  {
    "text": "And let's look at the operations\nthat deal with the first field. We have, in our code, in\nour LLVM IR, some address",
    "start": "2185980",
    "end": "2194710"
  },
  {
    "text": "calculations that refer to the\nsame field of the structure. In this case, I believe\nit's the first field, yes.",
    "start": "2194710",
    "end": "2200870"
  },
  {
    "text": " And, ultimately, we end up\nloading from this location",
    "start": "2200870",
    "end": "2209220"
  },
  {
    "text": "in local memory. So what value is this\nload going to retrieve?",
    "start": "2209220",
    "end": "2214485"
  },
  {
    "text": "How do we know that both\naddress calculations refer to the same field? Good question. What we do in this case\nis very careful analysis",
    "start": "2214485",
    "end": "2222060"
  },
  {
    "text": "of the math that's going on. We know that the alga, the\nlocation in local memory,",
    "start": "2222060",
    "end": "2230640"
  },
  {
    "text": "that's just a fixed location. And from that, we can interpret\nwhat each of the instructions",
    "start": "2230640",
    "end": "2235830"
  },
  {
    "text": "does in terms of an\naddress calculation. And we can determine that\nthey're the same value.",
    "start": "2235830",
    "end": "2241859"
  },
  {
    "start": "2241860",
    "end": "2249340"
  },
  {
    "text": "So we have this location in\nmemory that we operate on.",
    "start": "2249340",
    "end": "2255410"
  },
  {
    "text": "And before you do a\nmultiply, we end up loading from that\nlocation in memory.",
    "start": "2255410",
    "end": "2263069"
  },
  {
    "text": "So what value do we know is\ngoing to be loaded by that load instruction?",
    "start": "2263070",
    "end": "2269098"
  },
  {
    "text": "Go for it. ",
    "start": "2269098",
    "end": "2274818"
  },
  {
    "text": "AUDIENCE: So what we're doing\nright now is taking some value, and then storing it, and\nthen getting it back out, and putting it back.",
    "start": "2274818",
    "end": "2282552"
  },
  {
    "text": "TAO B. SCHARDL: Not putting\nit back, but we don't you worry about putting it back. AUDIENCE: So we don't need\nto put it somewhere just",
    "start": "2282552",
    "end": "2288944"
  },
  {
    "text": "to take it back out? TAO B. SCHARDL: Correct. Correct. So what are we multiplying in\nthat multiply, which value?",
    "start": "2288944",
    "end": "2297315"
  },
  {
    "start": "2297315",
    "end": "2302369"
  },
  {
    "text": "First element of the struct. It's percent zero. It's the value that\nwe stored right there.",
    "start": "2302370",
    "end": "2309042"
  },
  {
    "text": "That makes sense? Everyone see that? Any questions about that? ",
    "start": "2309042",
    "end": "2319040"
  },
  {
    "text": "All right, so we're storing\nthe first element of the struct into this location. Later, we load it out\nof that same location.",
    "start": "2319040",
    "end": "2326670"
  },
  {
    "text": "Nothing else happened\nto that location. So let's go ahead and\noptimize it just the same way",
    "start": "2326670",
    "end": "2332070"
  },
  {
    "text": "we optimize the scalar. We see that we use the result\nof the load right there. But we know that load is going\nto return the first field",
    "start": "2332070",
    "end": "2340230"
  },
  {
    "text": "of our struct input. So we'll just cross it out,\nand replace it with that field.",
    "start": "2340230",
    "end": "2347512"
  },
  {
    "text": "So now we're not using\nthe result of that load. What do we get to\ndo as the compiler? ",
    "start": "2347512",
    "end": "2357547"
  },
  {
    "text": "I can tell you know the answer. ",
    "start": "2357548",
    "end": "2363790"
  },
  {
    "text": "Delete the dead code,\ndelete all of it. Remove the now dead code,\nwhich is all those address",
    "start": "2363790",
    "end": "2370450"
  },
  {
    "text": "calculations, as well as the\nload operation, and the store operation. And that's pretty much it.",
    "start": "2370450",
    "end": "2376930"
  },
  {
    "text": "Yeah, good. So we replace that operation.",
    "start": "2376930",
    "end": "2382030"
  },
  {
    "text": "And we got rid of a bunch of\nother code from our function. We've now optimized one of\nthe two fields in our struct.",
    "start": "2382030",
    "end": "2390970"
  },
  {
    "text": "What do we do next?  Optimize the next one.",
    "start": "2390970",
    "end": "2398190"
  },
  {
    "text": "That happened similarly. I won't walk you through\nthat a second time. We find where we're using\nthe result of that load.",
    "start": "2398190",
    "end": "2404760"
  },
  {
    "text": "We can cross it out, and replace\nit with the appropriate input, and then delete all\nthe relevant dead code.",
    "start": "2404760",
    "end": "2411030"
  },
  {
    "text": "And now, we get to delete\nthe original allocation because nothing's getting\nstored to that memory.",
    "start": "2411030",
    "end": "2416133"
  },
  {
    "text": "That make sense? Any questions about that? Yeah? AUDIENCE: So when we first\ncompile it to LLVM IR,",
    "start": "2416133",
    "end": "2423690"
  },
  {
    "text": "does it unpack the\nstruct and just put in separate parameters? TAO B. SCHARDL: When we\nfirst compiled LLVM IR,",
    "start": "2423690",
    "end": "2430530"
  },
  {
    "text": "do we unpack the struct and\npass in the separate parameters? AUDIENCE: Like, how we\nhave three parameters here that are doubled. Wasn't our original C code\njust a struct of vectors in",
    "start": "2430530",
    "end": "2439721"
  },
  {
    "text": "the double? TAO B. SCHARDL: So LLVM IR in\nthis case, when we compiled it",
    "start": "2439721",
    "end": "2444780"
  },
  {
    "text": "as zero, decided to pass\nit as separate parameters,",
    "start": "2444780",
    "end": "2450360"
  },
  {
    "text": "just as it's representation. So in that sense, yes.",
    "start": "2450360",
    "end": "2456660"
  },
  {
    "text": "But it was still\ndoing the standard, create some local storage,\nstore the parameters",
    "start": "2456660",
    "end": "2462869"
  },
  {
    "text": "on to local storage, and\nthen all operations just read out of local storage. It's the standard thing that\nthe compiler generates when",
    "start": "2462870",
    "end": "2471810"
  },
  {
    "text": "it's asked to compile C code. And with no other optimizations,\nthat's what you get.",
    "start": "2471810",
    "end": "2477180"
  },
  {
    "text": "That makes sense? Yeah? AUDIENCE: What are\nall the align eights?",
    "start": "2477180",
    "end": "2482430"
  },
  {
    "text": "TAO B. SCHARDL: What are all\nthe aligned eights doing? The align eights\nare attributes that",
    "start": "2482430",
    "end": "2487770"
  },
  {
    "text": "specify the alignment of\nthat location in memory. This is alignment\ninformation that the compiler",
    "start": "2487770",
    "end": "2494340"
  },
  {
    "text": "either determines by\nanalysis, or implements as part of a standard.",
    "start": "2494340",
    "end": "2501180"
  },
  {
    "text": "So they're specifying how\nvalues are aligned in memory. That matters a lot more for\nultimate code generation,",
    "start": "2501180",
    "end": "2507180"
  },
  {
    "text": "unless we're able to\njust delete the memory references altogether. Make sense? Cool.",
    "start": "2507180",
    "end": "2513670"
  },
  {
    "text": "Any other questions?  All right, so we\noptimized the first field.",
    "start": "2513670",
    "end": "2522940"
  },
  {
    "text": "We optimize the second\nfield in a similar way. Turns out, there's\nadditional optimizations",
    "start": "2522940",
    "end": "2528609"
  },
  {
    "text": "that need to happen\nin order to return a structure from this function.",
    "start": "2528610",
    "end": "2534610"
  },
  {
    "text": "Those operations can be\noptimized in a similar way. They're shown here. We're not going to go through\nexactly how that works.",
    "start": "2534610",
    "end": "2541150"
  },
  {
    "text": "But at the end of\nthe day, after we've optimized all of that\ncode we end up with this.",
    "start": "2541150",
    "end": "2547210"
  },
  {
    "text": "We end up with our\nfunction compiled at 01. And it's far simpler.",
    "start": "2547210",
    "end": "2552477"
  },
  {
    "text": "I think it's far more intuitive. This is what I would\nimagine the code should look like when I wrote the\nC code in the first place.",
    "start": "2552477",
    "end": "2560920"
  },
  {
    "text": "Take your input. Do a couple of multiplications. And then it does them operations\nto create the return value,",
    "start": "2560920",
    "end": "2568310"
  },
  {
    "text": "and ultimately\nreturn that value. So, in summary,\nthe compiler works",
    "start": "2568310",
    "end": "2574330"
  },
  {
    "text": "hard to transform data\nstructures and scalar values to store as\nmuch as it possibly",
    "start": "2574330",
    "end": "2579370"
  },
  {
    "text": "can purely within\nregisters, and avoid using any local storage, if possible.",
    "start": "2579370",
    "end": "2586064"
  },
  {
    "text": "Everyone good with that so far? Cool.",
    "start": "2586064",
    "end": "2591250"
  },
  {
    "text": "Let's move on to\nanother optimization. Let's talk about function calls. Let's take a look\nat how the compiler",
    "start": "2591250",
    "end": "2597790"
  },
  {
    "text": "can optimize function calls. By and large,\nthese optimizations will occur if you pass\noptimization level 2 or higher,",
    "start": "2597790",
    "end": "2609510"
  },
  {
    "text": "just FYI. So from our original\nC code, we had some lines that performed a\nbunch of vector operations.",
    "start": "2609510",
    "end": "2617150"
  },
  {
    "text": "We had a vec add that added two\nvectors together, one of which was the result of\na vec scale, which",
    "start": "2617150",
    "end": "2622880"
  },
  {
    "text": "scaled the result of a vec\nadd by some scalar value. So we had this chain\nof calls in our code.",
    "start": "2622880",
    "end": "2632353"
  },
  {
    "text": "And if we take a look\nat the code compile that was 0, what we end up\nwith is this snippet shown on the bottom, which performs\nsome operations on these vector",
    "start": "2632353",
    "end": "2641720"
  },
  {
    "text": "structures, does this\nmultiply operation, and then calls this\nvector scale routine,",
    "start": "2641720",
    "end": "2647000"
  },
  {
    "text": "the vector scale routine that\nwe decide to focus on first.",
    "start": "2647000",
    "end": "2652230"
  },
  {
    "text": "So any ideas for how we\ngo about optimizing this?",
    "start": "2652230",
    "end": "2658340"
  },
  {
    "text": " So to give you a little bit of\na hint, what the compiler sees",
    "start": "2658340",
    "end": "2665809"
  },
  {
    "text": "when it looks at that call is\nit sees a snippet containing the call instruction.",
    "start": "2665810",
    "end": "2670920"
  },
  {
    "text": "And in our example, it also\nsees the code for the vec scale",
    "start": "2670920",
    "end": "2676730"
  },
  {
    "text": "function that we\nwere just looking at. And we're going to suppose\nthat it's already optimized vec scale as best as it can.",
    "start": "2676730",
    "end": "2682280"
  },
  {
    "text": "It's produced this code\nfor the vec scale routine. And so it sees that\ncall instruction.",
    "start": "2682280",
    "end": "2687830"
  },
  {
    "text": "And it sees this code for the\nfunction that's being called. So what could the\ncompiler do at this point",
    "start": "2687830",
    "end": "2694789"
  },
  {
    "text": "to try to make the\ncode above even faster?",
    "start": "2694790",
    "end": "2701570"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]",
    "start": "2701570",
    "end": "2708402"
  },
  {
    "text": " TAO B. SCHARDL:\nYou're exactly right. Remove the call, and just put\nthe body of the vec scale code",
    "start": "2708402",
    "end": "2715020"
  },
  {
    "text": "right there in\nplace of the call. It takes a little bit of\neffort to pull that off.",
    "start": "2715020",
    "end": "2720130"
  },
  {
    "text": "But, roughly\nspeaking, yeah, we're just going to copy and paste\nthis code in our function",
    "start": "2720130",
    "end": "2725220"
  },
  {
    "text": "into the place where we're\ncalling the function. And so if we do that\nsimple copy paste,",
    "start": "2725220",
    "end": "2730620"
  },
  {
    "text": "we end up with some garbage\ncode as an intermediate. We had to do a little\nbit of renaming",
    "start": "2730620",
    "end": "2735900"
  },
  {
    "text": "to make everything work out. But at this point,\nwe have the code from our function in\nthe place of that call.",
    "start": "2735900",
    "end": "2743910"
  },
  {
    "text": "And now, we can observe\nthat to restore correctness, we don't want to do the call. And we don't want to do\nthe return that we just",
    "start": "2743910",
    "end": "2751980"
  },
  {
    "text": "pasted in place. So we'll just go\nahead and remove both that call and the return.",
    "start": "2751980",
    "end": "2758370"
  },
  {
    "text": "That is called\nfunction inlining. We identify some function\ncall, or the compiler identifies some function call.",
    "start": "2758370",
    "end": "2764790"
  },
  {
    "text": "And it takes the\nbody of the function, and just pastes it right\nin place of that call.",
    "start": "2764790",
    "end": "2771359"
  },
  {
    "text": "Sound good? Make sense? Anyone confused? ",
    "start": "2771360",
    "end": "2781472"
  },
  {
    "text": "Raise your hand if\nyou're confused. ",
    "start": "2781472",
    "end": "2789369"
  },
  {
    "text": "Now, once you've done some\namount of function inlining, we can actually do some\nmore optimizations.",
    "start": "2789370",
    "end": "2795680"
  },
  {
    "text": "So here, we have the\ncode after we got rid of the unnecessary\ncall and return. And we have a couple multiply\noperations sitting in place.",
    "start": "2795680",
    "end": "2802840"
  },
  {
    "text": "That looks fine. But if we expand our\nscope just a little bit, what we see, so we\nhave some operations",
    "start": "2802840",
    "end": "2809500"
  },
  {
    "text": "happening that were\nsitting there already after the original call.",
    "start": "2809500",
    "end": "2816214"
  },
  {
    "text": "What the compiler\ncan do is it can take a look at these instructions. And long story\nshort, it realizes",
    "start": "2816215",
    "end": "2822940"
  },
  {
    "text": "that all these\ninstructions do is pack some data into a structure,\nand then immediately unpack",
    "start": "2822940",
    "end": "2828279"
  },
  {
    "text": "the structure. So it's like you put a\nbunch of stuff into a bag, and then immediately\ndump out the bag.",
    "start": "2828280",
    "end": "2835539"
  },
  {
    "text": "That was kind of\na waste of time. That's kind of a waste of code. Let's get rid of it. ",
    "start": "2835540",
    "end": "2843540"
  },
  {
    "text": "Those operations are useless. Let's delete them. The compiler has a great\ntime deleting dead code.",
    "start": "2843540",
    "end": "2849252"
  },
  {
    "text": "It's like it's what\nit lives to do.  All right, now, in fact,\nin the original code,",
    "start": "2849252",
    "end": "2856410"
  },
  {
    "text": "we didn't just have\none function call. We had a whole sequence\nof function calls. And if we expand our LLVM IR\nsnippet even a little further,",
    "start": "2856410",
    "end": "2864180"
  },
  {
    "text": "we can include\nthose two function calls, the original call to\nvec ad, followed by the code",
    "start": "2864180",
    "end": "2869730"
  },
  {
    "text": "that we've now\noptimized by inlining, ultimately followed by yet\nanother call to vec add.",
    "start": "2869730",
    "end": "2876960"
  },
  {
    "text": "Minor spoiler, the vec add\nroutine, once it's optimized, looks pretty similar to\nthe vec scalar routine.",
    "start": "2876960",
    "end": "2884420"
  },
  {
    "text": "And, in particular,\nit has comparable size to the vector scale routine. So what's the compiler is going\nto do to those to call sites?",
    "start": "2884420",
    "end": "2891620"
  },
  {
    "start": "2891620",
    "end": "2900710"
  },
  {
    "text": "Inline it, do more\ninlining, inlining is great. We'll inline these\nfunctions as well,",
    "start": "2900710",
    "end": "2908839"
  },
  {
    "text": "and then remove all of the\nadditional, now-useless instructions. We'll walk through that process.",
    "start": "2908840",
    "end": "2914220"
  },
  {
    "text": "The result of that process\nlooks something like this. So in the original C\ncode, we had this vec",
    "start": "2914220",
    "end": "2920040"
  },
  {
    "text": "add, which called\nthe vec scale as one of its arguments, which\ncalled the vec add is one of its arguments.",
    "start": "2920040",
    "end": "2925500"
  },
  {
    "text": "And what we end up with\nin the optimized IR is just a bunch of straight\nline code that performs",
    "start": "2925500",
    "end": "2930600"
  },
  {
    "text": "floating point operations. It's almost as if the compiler\ntook the original C code,",
    "start": "2930600",
    "end": "2937860"
  },
  {
    "text": "and transformed it into\nthe equivalency code shown on the bottom, where\nit just operates",
    "start": "2937860",
    "end": "2943740"
  },
  {
    "text": "on a whole bunch of doubles, and\njust does primitive operations. So function inlining, as well as\nthe additional transformations",
    "start": "2943740",
    "end": "2952230"
  },
  {
    "text": "it was able to\nperform as a result, together those were\nable to eliminate all of those function calls.",
    "start": "2952230",
    "end": "2958360"
  },
  {
    "text": "It was able to\ncompletely eliminate any costs associated with the\nfunction call abstraction,",
    "start": "2958360",
    "end": "2965130"
  },
  {
    "text": "at least in this code. Make sense? ",
    "start": "2965130",
    "end": "2970500"
  },
  {
    "text": "I think that's pretty cool. You write code that has a\nbunch of function calls, because that's how you've\nconstructed your interfaces.",
    "start": "2970500",
    "end": "2977250"
  },
  {
    "text": "But you're not really paying\nfor those function calls. Function calls aren't\nthe cheapest operation in the world,\nespecially if you think",
    "start": "2977250",
    "end": "2982830"
  },
  {
    "text": "about everything\nthat goes on in terms of the registers and the stack. But the compiler is able to\navoid all of that overhead,",
    "start": "2982830",
    "end": "2990420"
  },
  {
    "text": "and just perform the floating\npoint operations we care about. OK, well, if function\ninlining is so great,",
    "start": "2990420",
    "end": "2997380"
  },
  {
    "text": "and it enables so many\ngreat optimizations, why doesn't the compiler just\ninline every function call?",
    "start": "2997380",
    "end": "3003248"
  },
  {
    "text": " Go for it. Recursion, it's really hard\nto inline a recursive call.",
    "start": "3003248",
    "end": "3012630"
  },
  {
    "text": "In general, you can't inline\na function into itself, although it turns out\nthere are some exceptions.",
    "start": "3012630",
    "end": "3017940"
  },
  {
    "text": "So, yes, recursion\ncreates problems with function inlining. Any other thoughts?",
    "start": "3017940",
    "end": "3023670"
  },
  {
    "text": "In the back. AUDIENCE: [INAUDIBLE]",
    "start": "3023670",
    "end": "3029505"
  },
  {
    "start": "3029505",
    "end": "3038057"
  },
  {
    "text": "TAO B. SCHARDL: You're\ndefinitely on to something. So we had to do a bunch\nof this renaming stuff",
    "start": "3038057",
    "end": "3043170"
  },
  {
    "text": "when we inlined the\nfirst time, and when we inlined every single time. And even though LLVM IR has an\ninfinite number of registers,",
    "start": "3043170",
    "end": "3051870"
  },
  {
    "text": "the machine doesn't. And so all of that renaming\ndoes create a problem. But there are other\nproblems as well of",
    "start": "3051870",
    "end": "3059370"
  },
  {
    "text": "a similar nature when you start\ninlining all those functions. For example, you copy\npasted a bunch of code.",
    "start": "3059370",
    "end": "3066100"
  },
  {
    "text": "And that made the original call\nsite even bigger, and bigger, and bigger, and bigger. And programs, we generally\ndon't think about the space",
    "start": "3066100",
    "end": "3073950"
  },
  {
    "text": "they take in memory. But they do take\nspace in memory. And that does have an\nimpact on performance.",
    "start": "3073950",
    "end": "3079120"
  },
  {
    "text": "So great answer,\nany other thoughts? ",
    "start": "3079120",
    "end": "3085056"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "3085056",
    "end": "3095486"
  },
  {
    "text": "TAO B. SCHARDL: If your\nfunction becomes too long, then it may not fit\nin instruction cache. And that can increase\nthe amount of time",
    "start": "3095487",
    "end": "3101110"
  },
  {
    "text": "it takes just to\nexecute the function. Right, because you're now\nnot getting hash hits,",
    "start": "3101110",
    "end": "3107367"
  },
  {
    "text": "exactly right. That's one of the problems\nwith this code size blow up from inlining everything.",
    "start": "3107367",
    "end": "3112630"
  },
  {
    "text": "Any other thoughts? Any final thoughts? ",
    "start": "3112630",
    "end": "3123290"
  },
  {
    "text": "So there are three main\nreasons why the compiler won't inline every function. I think we touched\non two of them here.",
    "start": "3123290",
    "end": "3131069"
  },
  {
    "text": "For some function calls,\nlike recursive calls, it's impossible to inline\nthem, because you can't inline a function into itself.",
    "start": "3131070",
    "end": "3138450"
  },
  {
    "text": "But there are exceptions\nto that, namely recursive tail calls. If the last thing in a\nfunction is a function call,",
    "start": "3138450",
    "end": "3146280"
  },
  {
    "text": "then it turns out\nyou can effectively inline that function\ncall as an optimization.",
    "start": "3146280",
    "end": "3151860"
  },
  {
    "text": "We're not going to talk too\nmuch about how that works. But there are corner cases.",
    "start": "3151860",
    "end": "3156940"
  },
  {
    "text": "But, in general, you can't\ninline a recursive call.",
    "start": "3156940",
    "end": "3162119"
  },
  {
    "text": "The compiler has\nanother problem. Namely, if the function\nthat you're calling",
    "start": "3162120",
    "end": "3167570"
  },
  {
    "text": "is in a different castle, if\nit's in a different compilation unit, literally in\na different file",
    "start": "3167570",
    "end": "3174240"
  },
  {
    "text": "that's compiled independently,\nthen the compiler can't very well\ninline that function,",
    "start": "3174240",
    "end": "3180238"
  },
  {
    "text": "because it doesn't know\nabout the function. It doesn't have access\nto that function's code.",
    "start": "3180238",
    "end": "3185280"
  },
  {
    "text": "There is a way to get\naround that problem with modern compiler technology\nthat involves whole program optimization.",
    "start": "3185280",
    "end": "3191040"
  },
  {
    "text": "And I think there's some backup\nslides that will tell you how to do that with LLVM.",
    "start": "3191040",
    "end": "3196260"
  },
  {
    "text": "But, in general, if it's in\na different compilation unit, it can't be inline.",
    "start": "3196260",
    "end": "3201390"
  },
  {
    "text": "And, finally, as touched\non, function inlining can increase code size,\nwhich can hurt performance.",
    "start": "3201390",
    "end": "3208200"
  },
  {
    "text": "OK, so some functions\nare OK to inline. Other functions could create\nthis performance problem,",
    "start": "3208200",
    "end": "3214110"
  },
  {
    "text": "because you've\nincreased code size. So how does the compiler\nknow whether or not inlining any particular\nfunction at a call site",
    "start": "3214110",
    "end": "3222660"
  },
  {
    "text": "could hurt performance? Any guesses?",
    "start": "3222660",
    "end": "3227780"
  },
  {
    "text": "Yeah? AUDIENCE: [INAUDIBLE] ",
    "start": "3227780",
    "end": "3235975"
  },
  {
    "text": "TAO B. SCHARDL: Yeah. So the compiler has some\ncost model, which gives it some information\nabout, how much will it",
    "start": "3235975",
    "end": "3242740"
  },
  {
    "text": "cost to inline that function? Is the cost model always right? ",
    "start": "3242740",
    "end": "3250560"
  },
  {
    "text": "It is not. So the answer, how\ndoes the compiler know, is, really, it doesn't know.",
    "start": "3250560",
    "end": "3257400"
  },
  {
    "text": "It makes a best guess\nusing that cost model, and other heuristics,\nto determine,",
    "start": "3257400",
    "end": "3264000"
  },
  {
    "text": "when does it make sense to\ntry to inline a function? And because it's\nmaking a best guess,",
    "start": "3264000",
    "end": "3269820"
  },
  {
    "text": "sometimes the compiler\nguesses wrong. So to wrap up this\npart, here are just",
    "start": "3269820",
    "end": "3275430"
  },
  {
    "text": "a couple of tips for\ncontrolling function inlining in your own programs. If there's a function that you\nknow must always be inlined,",
    "start": "3275430",
    "end": "3282809"
  },
  {
    "text": "no matter what happens,\nyou can mark that function with a special attribute, namely\nthe always inline attribute.",
    "start": "3282810",
    "end": "3289963"
  },
  {
    "text": "For example, if you\nhave a function that does some complex\naddress calculation, and it should be inlined\nrather than called,",
    "start": "3289963",
    "end": "3297329"
  },
  {
    "text": "you may want to mark that with\nan always inline attribute. Similarly, if you have a\nfunction that really should",
    "start": "3297330",
    "end": "3302580"
  },
  {
    "text": "never be inlined, it's\nnever cost effective to inline that function,\nyou can mark that function",
    "start": "3302580",
    "end": "3308160"
  },
  {
    "text": "with the no inline attribute. And, finally, if you want to\nenable more function inlining",
    "start": "3308160",
    "end": "3315150"
  },
  {
    "text": "in the compiler, you can use\nlink time optimization, or LTO, to enable whole\nprogram optimization.",
    "start": "3315150",
    "end": "3322380"
  },
  {
    "text": "Won't go into that\nduring these slides. Let's move on, and talk\nabout loop optimizations.",
    "start": "3322380",
    "end": "3328170"
  },
  {
    "text": "Any questions so\nfar, before continue? Yeah? AUDIENCE: [INAUDIBLE]",
    "start": "3328170",
    "end": "3335460"
  },
  {
    "text": "TAO B. SCHARDL: Sorry? AUDIENCE: [INAUDIBLE]",
    "start": "3335460",
    "end": "3340520"
  },
  {
    "text": " TAO B. SCHARDL:\nDoes static inline guarantee you the compiler\nwill always inline it?",
    "start": "3340520",
    "end": "3347099"
  },
  {
    "text": "It actually doesn't. The inline keyword will\nprovide a hint to the compiler",
    "start": "3347100",
    "end": "3354420"
  },
  {
    "text": "that it should think about\ninlining the function. But it doesn't provide\nany guarantees. If you want a strong guarantee,\nuse the always inline",
    "start": "3354420",
    "end": "3361230"
  },
  {
    "text": "attribute. Good question, though. ",
    "start": "3361230",
    "end": "3368060"
  },
  {
    "text": "All right, loop optimizations-- you've already seen\nsome loop optimizations. You've seen vectorization,\nfor example.",
    "start": "3368060",
    "end": "3377010"
  },
  {
    "text": "It turns out, the compiler\ndoes a lot of work to try to optimize loops. So first, why is that?",
    "start": "3377010",
    "end": "3384230"
  },
  {
    "text": "Why would the compiler\nengineers invest so much effort into optimizing loops?",
    "start": "3384230",
    "end": "3390480"
  },
  {
    "text": "Why loops in particular? ",
    "start": "3390480",
    "end": "3402470"
  },
  {
    "text": "They're extremely\ncommon control structure that also has a branch. Both things are true.",
    "start": "3402470",
    "end": "3408930"
  },
  {
    "text": "I think there's a higher\nlevel reason, though, or more fundamental\nreason, if you will.",
    "start": "3408930",
    "end": "3415854"
  },
  {
    "text": "Yeah? AUDIENCE: Most of the time, the\nloop takes up the most time. TAO B. SCHARDL: Most of\nthe time the loop takes up",
    "start": "3415854",
    "end": "3422870"
  },
  {
    "text": "the most time. You got it. Loops account for a lot of the\nexecution time of programs.",
    "start": "3422870",
    "end": "3429830"
  },
  {
    "text": "The way I like to\nthink about this is with a really simple\nthought experiment. Let's imagine that you've got\na machine with a two gigahertz",
    "start": "3429830",
    "end": "3436790"
  },
  {
    "text": "processor. We've chosen these\nvalues to be easier to think about\nusing mental math.",
    "start": "3436790",
    "end": "3443412"
  },
  {
    "text": "Suppose you've got\na two gigahertz processor with 16 cores. Each core executes one\ninstruction per cycle.",
    "start": "3443413",
    "end": "3449570"
  },
  {
    "text": "And suppose you've\ngot a program which contains a trillion instructions\nand ample parallelism",
    "start": "3449570",
    "end": "3455900"
  },
  {
    "text": "for those 16 cores. But all of those instructions\nare simple, straight line code.",
    "start": "3455900",
    "end": "3461560"
  },
  {
    "text": "There are no branches. There are no loops. There no complicated\noperations like IO.",
    "start": "3461560",
    "end": "3466760"
  },
  {
    "text": "It's just a bunch of really\nsimple straight line code. Each instruction takes\na cycle to execute.",
    "start": "3466760",
    "end": "3472309"
  },
  {
    "text": "The processor executes\none instruction per cycle. How long does it take to\nrun this code, to execute",
    "start": "3472310",
    "end": "3481640"
  },
  {
    "text": "the entire terabyte binary? ",
    "start": "3481640",
    "end": "3495740"
  },
  {
    "text": "2 to the 40th cycles for\n2 to the 40 instructions. But you're using a two gigahertz\nprocessor and 16 cores.",
    "start": "3495740",
    "end": "3504610"
  },
  {
    "text": "And you've got ample\nparallelism in the program to keep them all saturated. So how much time?",
    "start": "3504610",
    "end": "3510304"
  },
  {
    "text": " AUDIENCE: 32 seconds.",
    "start": "3510304",
    "end": "3518110"
  },
  {
    "text": "TAO B. SCHARDL: 32\nseconds, nice job.",
    "start": "3518110",
    "end": "3523210"
  },
  {
    "text": "This one has mastered power\nof 2 arithmetic in one's head. It's a good skill to have,\nespecially in core six.",
    "start": "3523210",
    "end": "3530860"
  },
  {
    "text": "Yeah, so if you have\njust a bunch of simple, straight line code, and\nyou have a terabyte of it.",
    "start": "3530860",
    "end": "3537610"
  },
  {
    "text": "That's a lot of code. That is a big binary. And, yet, the program,\nthis processor,",
    "start": "3537610",
    "end": "3544035"
  },
  {
    "text": "this relatively\nsimple processor, can execute the whole thing\nin just about 30 seconds. Now, in your experience\nworking with software,",
    "start": "3544035",
    "end": "3551290"
  },
  {
    "text": "you might have\nnoticed that there are some programs that take\nlonger than 30 seconds to run.",
    "start": "3551290",
    "end": "3557480"
  },
  {
    "text": "And some of those programs don't\nhave terabyte size binaries. The reason that those\nprograms take longer to run,",
    "start": "3557480",
    "end": "3565720"
  },
  {
    "text": "by and large, is loops. So loops account for\na lot of the execution time in real programs.",
    "start": "3565720",
    "end": "3571960"
  },
  {
    "text": " Now, you've already seen\nsome loop optimizations. We're just going to take\na look at one other loop",
    "start": "3571960",
    "end": "3578802"
  },
  {
    "text": "optimization today, namely\ncode hoisting, also known as loop invariant code motion.",
    "start": "3578802",
    "end": "3584360"
  },
  {
    "text": "To look at that,\nwe're going to take a look at a different\nsnippet of code from the end body simulation.",
    "start": "3584360",
    "end": "3590500"
  },
  {
    "text": "This code calculates\nthe forces going on each of the end bodies.",
    "start": "3590500",
    "end": "3595980"
  },
  {
    "text": "And it does it with\na doubly nested loop. For all the zero to\nnumber of bodies,",
    "start": "3595980",
    "end": "3601943"
  },
  {
    "text": "for all body zero\nnumber bodies, as long as you're not looking\nat the same body, call this add force routine,\nwhich calculates to--",
    "start": "3601943",
    "end": "3610210"
  },
  {
    "text": "calculate the force\nbetween those two bodies. And add that force\nto one of the bodies.",
    "start": "3610210",
    "end": "3616599"
  },
  {
    "text": "That's all that's\ngoing on in this code. If we translate this\ncode into LLVM IR,",
    "start": "3616600",
    "end": "3622330"
  },
  {
    "text": "we end up with,\nhopefully unsurprisingly, a doubly nested loop.",
    "start": "3622330",
    "end": "3628210"
  },
  {
    "text": "It looks something like this. The body of the code, the\nbody of the innermost loop, has been lighted, just so\nthings can fit on the slide.",
    "start": "3628210",
    "end": "3635170"
  },
  {
    "text": "But we can see the\noverall structure. On the outside, we have\nsome outer loop control.",
    "start": "3635170",
    "end": "3641069"
  },
  {
    "text": "This should look familiar\nfrom lecture five, hopefully. Inside of that outer loop,\nwe have an inner loop.",
    "start": "3641070",
    "end": "3648278"
  },
  {
    "text": "And at the top and the\nbottom of that inner loop, we have the inner loop control. And within that\ninner loop, we do",
    "start": "3648278",
    "end": "3654670"
  },
  {
    "text": "have one branch, which\ncan skip a bunch of code if you're looking at the\nsame body for i and j.",
    "start": "3654670",
    "end": "3661930"
  },
  {
    "text": "But, otherwise, we have the loop\nbody of the inner most loop, basic structure.",
    "start": "3661930",
    "end": "3668589"
  },
  {
    "text": "Now, if we just zoom\nin on the top part of this doubly-nested loop, just\nthe topmost three basic blocks,",
    "start": "3668590",
    "end": "3675910"
  },
  {
    "text": "take a look at more of the\ncode that's going on here, we end up with something\nthat looks like this.",
    "start": "3675910",
    "end": "3682200"
  },
  {
    "text": "And if you remember\nsome of the discussion from lecture five about the\nloop induction variables, and what that looks like\nin LLVM IR, what you find",
    "start": "3682200",
    "end": "3689829"
  },
  {
    "text": "is that for the outer loop\nwe have an induction variable at the very top. It's that weird fee\ninstruction, once again.",
    "start": "3689830",
    "end": "3697270"
  },
  {
    "text": "Inside that outer loop,\nwe have the loop control for the inner loop, which has\nits own induction variable.",
    "start": "3697270",
    "end": "3703089"
  },
  {
    "text": "Once again, we have\nanother fee node. That's how we can spot it. And then we have the body\nof the innermost loop.",
    "start": "3703090",
    "end": "3710360"
  },
  {
    "text": "And this is just the start of. It it's just a couple\naddress calculations. But can anyone tell me\nsome interesting property",
    "start": "3710360",
    "end": "3716920"
  },
  {
    "text": "about just a couple of\nthese address calculations that could lead to\nan optimization?",
    "start": "3716920",
    "end": "3722532"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]",
    "start": "3722532",
    "end": "3727670"
  },
  {
    "text": "TAO B. SCHARDL: The first\ntwo address calculations only depend on the outermost\nloop variable, the iteration",
    "start": "3727670",
    "end": "3734600"
  },
  {
    "text": "variable for the outer\nloop, exactly right. So what can we do with\nthose instructions?",
    "start": "3734600",
    "end": "3741614"
  },
  {
    "start": "3741614",
    "end": "3751460"
  },
  {
    "text": "Bring them out of\nthe inner loop. Why should we keep\ncomputing these addresses in the innermost loop when we\ncould just compute them once",
    "start": "3751460",
    "end": "3758750"
  },
  {
    "text": "in the outer loop? That optimization is called\ncode hoisting, or loop invariant",
    "start": "3758750",
    "end": "3765119"
  },
  {
    "text": "code motion. Those instructions are\ninvariant to the code in the innermost loop. So you hoist them out.",
    "start": "3765120",
    "end": "3771430"
  },
  {
    "text": "And once you hoist\nthem out, you end up with a transformed loop that\nlooks something like this.",
    "start": "3771430",
    "end": "3777260"
  },
  {
    "text": "What we have is the same outer\nloop control at the very top. But now, we're doing some\naddress calculations there.",
    "start": "3777260",
    "end": "3784410"
  },
  {
    "text": "And we no longer have\nthose address calculations on the inside. ",
    "start": "3784410",
    "end": "3790310"
  },
  {
    "text": "And as a result, those\nhoisted calculations are performed just once per\niteration of the outer loop,",
    "start": "3790310",
    "end": "3797150"
  },
  {
    "text": "rather than once per\niteration of the inner loop. And so those instructions\nare run far fewer times.",
    "start": "3797150",
    "end": "3803110"
  },
  {
    "text": "You get to save a\nlot of running time. ",
    "start": "3803110",
    "end": "3808450"
  },
  {
    "text": "So the effect of\nthis optimization in terms of C code,\nbecause it can be a little tedious\nto look at LLVM IR,",
    "start": "3808450",
    "end": "3814079"
  },
  {
    "text": "is essentially like this. We took this\ndoubly-nested loop in C. We're calling add force of blah,\nblah, blah, calculate force,",
    "start": "3814080",
    "end": "3823390"
  },
  {
    "text": "blah, blah, blah. And now, we just move\nthe address calculation to get the ith body\nthat we care about.",
    "start": "3823390",
    "end": "3831130"
  },
  {
    "text": "We move that to the outer. Now, this was an example of loop\ninvariant code motion on just",
    "start": "3831130",
    "end": "3836410"
  },
  {
    "text": "a couple address calculations. In general, the\ncompiler will try to prove that some calculation\nis invariant across all",
    "start": "3836410",
    "end": "3844630"
  },
  {
    "text": "the iterations of a loop. And whenever it\ncan prove that, it will try to hoist that\ncode out of the loop.",
    "start": "3844630",
    "end": "3850030"
  },
  {
    "text": "If it can get code out\nof the body of a loop, that reduces the running\ntime of the loop,",
    "start": "3850030",
    "end": "3855250"
  },
  {
    "text": "saves a lot of execution time. Huge bang for the buck.",
    "start": "3855250",
    "end": "3860550"
  },
  {
    "text": "Make sense? Any questions about that so far? All right, so just to\nsummarize this part,",
    "start": "3860550",
    "end": "3867190"
  },
  {
    "text": "what can the compiler do? The compiler optimizes code\nby performing a sequence of transformation passes.",
    "start": "3867190",
    "end": "3873100"
  },
  {
    "text": "All those passes are\npretty mechanical. The compiler goes\nthrough the code. It tries to find some property,\nlike this address calculation",
    "start": "3873100",
    "end": "3880675"
  },
  {
    "text": "is the same as that\naddress calculation. And so this load will return\nthe same value as that store,",
    "start": "3880675",
    "end": "3886620"
  },
  {
    "text": "and so on, and so forth. And based on that\nanalysis, it tries to get rid of some dead code,\nand replace certain register",
    "start": "3886620",
    "end": "3895180"
  },
  {
    "text": "values with other\nregister values, replace things that live\nin memory with things that just live in registers.",
    "start": "3895180",
    "end": "3900900"
  },
  {
    "text": "A lot of the transformations\nresemble Bentley-rule work optimizations that you've\nseen in lecture two.",
    "start": "3900900",
    "end": "3906609"
  },
  {
    "text": "So as you're studying\nfor your upcoming quiz, you can kind of get\ntwo for one by looking at those Bentley-rule\noptimizations.",
    "start": "3906610",
    "end": "3915410"
  },
  {
    "text": "And one transformation pass, in\nparticular function inlining, was a good example of this. One transformation can\nenable other transformations.",
    "start": "3915410",
    "end": "3922630"
  },
  {
    "text": "And those together can\ncompound to give you fast code. In general, compilers perform\na lot more transformations",
    "start": "3922630",
    "end": "3928960"
  },
  {
    "text": "than just the ones we saw today. But there are things that\nthe compiler can't do. Here's one very simple example.",
    "start": "3928960",
    "end": "3934750"
  },
  {
    "text": " In this case, we're\ntaking another look at this calculate\nforces routine.",
    "start": "3934750",
    "end": "3940900"
  },
  {
    "text": "Although the compiler\ncan optimize the code by moving address\ncalculations out of loop,",
    "start": "3940900",
    "end": "3947050"
  },
  {
    "text": "one thing that I can't\ndo is exploit symmetry in the problem. So in this problem,\nwhat's going on",
    "start": "3947050",
    "end": "3954099"
  },
  {
    "text": "is we're computing the\nforces on any pair of bodies using the law of gravitation.",
    "start": "3954100",
    "end": "3959349"
  },
  {
    "text": "And it turns out that the force\nacting on one body by another is exactly the opposite the\nforce acting on the other body",
    "start": "3959350",
    "end": "3967210"
  },
  {
    "text": "by the one. So F of 12 is equal\nto minus F of 21.",
    "start": "3967210",
    "end": "3972910"
  },
  {
    "text": "The compiler will\nnot figure that out. The compiler knows algebra. It doesn't know physics.",
    "start": "3972910",
    "end": "3978760"
  },
  {
    "text": "So it won't be\nable to figure out that there's symmetry\nin this problem, and it can avoid\nwasted operations.",
    "start": "3978760",
    "end": "3986880"
  },
  {
    "text": "Make sense?  All right, so that\nwas an overview of some simple\ncompiler optimizations.",
    "start": "3986880",
    "end": "3993600"
  },
  {
    "text": "We now have some examples\nof some case studies to see where the compiler\ncan get tripped up.",
    "start": "3993600",
    "end": "4002080"
  },
  {
    "text": "And it doesn't matter if we get\nthrough all of these or not. You'll have access to\nthe slides afterwards. But I think these\nare kind of cool.",
    "start": "4002080",
    "end": "4007908"
  },
  {
    "text": "So shall we take a look? ",
    "start": "4007908",
    "end": "4012950"
  },
  {
    "text": "Simple question-- does the\ncompiler vectorize this loop?",
    "start": "4012950",
    "end": "4018200"
  },
  {
    "start": "4018200",
    "end": "4024290"
  },
  {
    "text": "So just to go over what this\nloop does, it's a simple loop. The function takes\ntwo vectors as inputs,",
    "start": "4024290",
    "end": "4033099"
  },
  {
    "text": "or two arrays as\ninputs, I should say-- an array called y, of like then,\nand an array x of like then,",
    "start": "4033100",
    "end": "4041920"
  },
  {
    "text": "and some scalar value a. And all that this\nfunction does is it loops over each element of\nthe vector, multiplies x of i",
    "start": "4041920",
    "end": "4050200"
  },
  {
    "text": "by the input scalar, adds\nthe product into y's of i. So does the loop vectorize?",
    "start": "4050200",
    "end": "4056380"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE]",
    "start": "4056380",
    "end": "4061500"
  },
  {
    "text": " TAO B. SCHARDL: y\nand x could overlap. And there is no information\nabout whether they overlap.",
    "start": "4061500",
    "end": "4066869"
  },
  {
    "text": "So do they vectorize? We have a vote for no.",
    "start": "4066870",
    "end": "4071990"
  },
  {
    "text": "Anyone think that\nit does vectorize? You made a very\nconvincing argument.",
    "start": "4071990",
    "end": "4077360"
  },
  {
    "text": "So everyone believes that\nthis loop does not vectorize.",
    "start": "4077360",
    "end": "4084850"
  },
  {
    "text": "Is that true? Anyone uncertain?",
    "start": "4084850",
    "end": "4090860"
  },
  {
    "text": "Anyone unwilling to commit\nto yes or no right here? ",
    "start": "4090860",
    "end": "4096401"
  },
  {
    "text": "All right, a bunch of people\nare unwilling to commit to yes or no. All right, let's\nresolve this question.",
    "start": "4096402",
    "end": "4101989"
  },
  {
    "text": "Let's first ask for the report. Let's look at the\nvectorization report. We compile it.",
    "start": "4101990",
    "end": "4107389"
  },
  {
    "text": "We pass the flags to get\nthe vectorization report. And the vectorization\nreport says, yes, it",
    "start": "4107390",
    "end": "4113750"
  },
  {
    "text": "does vectorize this loop,\nwhich is interesting, because we have this\ngreat argument that says,",
    "start": "4113750",
    "end": "4120460"
  },
  {
    "text": "but you don't know how these\naddresses fit in memory. You don't know if x and y\noverlap with each other.",
    "start": "4120460",
    "end": "4126920"
  },
  {
    "text": "How can you possibly vectorize? Kind of a mystery.",
    "start": "4126920",
    "end": "4132720"
  },
  {
    "text": "Well, if we take a look at the\nactual compiled code when we optimize this at 02, turns\nout you can pass certain flags",
    "start": "4132720",
    "end": "4141210"
  },
  {
    "text": "to the compiler, and get it to\nprint out not just the LLVM IR, but the LLVM IR formatted\nas a control flow graph.",
    "start": "4141210",
    "end": "4148489"
  },
  {
    "text": "And the control flow graph for\nthis simple two line function is the thing on the\nright, which you obviously",
    "start": "4148490",
    "end": "4157609"
  },
  {
    "text": "can't, read because\nit's a little bit small, in terms of its text. And it seems have\na lot going on.",
    "start": "4157609",
    "end": "4166520"
  },
  {
    "text": "So I took the liberty of\nredrawing that control flow graph with none of\nthe code inside,",
    "start": "4166520",
    "end": "4172520"
  },
  {
    "text": "just get a picture of\nwhat the structure looks like for this compiled function.",
    "start": "4172520",
    "end": "4177739"
  },
  {
    "text": "And, structurally speaking,\nit looks like this. And with a bit of practice\nstaring at control flow graphs,",
    "start": "4177740",
    "end": "4185312"
  },
  {
    "text": "which you might get if you\nspend way too much time working on compilers, you might look\nat this control flow graph,",
    "start": "4185312",
    "end": "4190818"
  },
  {
    "text": "and think, this graph looks\na little too complicated for the two line function\nthat we gave as input.",
    "start": "4190819",
    "end": "4199010"
  },
  {
    "text": "So what's going on here? Well, we've got three\ndifferent loops in this code.",
    "start": "4199010",
    "end": "4204782"
  },
  {
    "text": "And it turns out that\none of those loops is full of vector operations. OK, the other two loops are\nnot full of vector operations.",
    "start": "4204783",
    "end": "4213100"
  },
  {
    "text": "That's unvectorized code. And then there's this\nbasic block right at the top that has\na conditional branch",
    "start": "4213100",
    "end": "4220460"
  },
  {
    "text": "at the end of it, branching\nto either the vectorized loop or the unvectorized loop. And, yeah, there's a lot of\nother control flow going on",
    "start": "4220460",
    "end": "4227280"
  },
  {
    "text": "as well. But we can focus on just these\ncomponents for the time being.",
    "start": "4227280",
    "end": "4232610"
  },
  {
    "text": "So what's that\nconditional branch doing? Well, we can zoom in on\njust this one basic block,",
    "start": "4232610",
    "end": "4238400"
  },
  {
    "text": "and actually show it to\nbe readable on the slide.",
    "start": "4238400",
    "end": "4243590"
  },
  {
    "text": "And the basic block\nlooks like this. So let's just study\nthis LLVM IR code.",
    "start": "4243590",
    "end": "4249530"
  },
  {
    "text": "In this case, we have got the\naddress y stored in register 0. The address of x is\nstored in register 2.",
    "start": "4249530",
    "end": "4256940"
  },
  {
    "text": "And register 3 stores\nthe value of n. So one instruction\nat a time, who can tell me what the first\ninstruction in this code does?",
    "start": "4256940",
    "end": "4265010"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] TAO B. SCHARDL: Gets\nthe address of y.",
    "start": "4265010",
    "end": "4271455"
  },
  {
    "text": " Is that what you said? ",
    "start": "4271455",
    "end": "4279090"
  },
  {
    "text": "So it does use the address of y. It's an address calculation that\noperates on register 0, which",
    "start": "4279090",
    "end": "4284790"
  },
  {
    "text": "stores the address of y. But it's not just\ncomputing the address of y.",
    "start": "4284790",
    "end": "4291302"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] TAO B. SCHARDL: It's\ngetting me the address of the nth element of y.",
    "start": "4291302",
    "end": "4296830"
  },
  {
    "text": "It's adding in whatever is in\nregister 3, which is the value n, into the address of y.",
    "start": "4296830",
    "end": "4302860"
  },
  {
    "text": "So that computes the\naddress y plus n. This is testing your memory\nof pointer arithmetic",
    "start": "4302860",
    "end": "4310130"
  },
  {
    "text": "in C just a little bit but. Don't worry. It won't be too rough. So that's what the first\naddress calculation does.",
    "start": "4310130",
    "end": "4317290"
  },
  {
    "text": "What does the next\ninstruction do? AUDIENCE: It does x plus n. TAO B. SCHARDL: That\ncomputes x plus, very good.",
    "start": "4317290",
    "end": "4324387"
  },
  {
    "text": "How about the next one? ",
    "start": "4324388",
    "end": "4332991"
  },
  {
    "text": "AUDIENCE: It compares\nwhether x plus n and y plus n are the same.",
    "start": "4332992",
    "end": "4338880"
  },
  {
    "text": "TAO B. SCHARDL: It compares\nx plus n, versus y plus n. AUDIENCE: [INAUDIBLE] compares\nthe 33, which is x plus n,",
    "start": "4338880",
    "end": "4349250"
  },
  {
    "text": "and compares it to y. So if x plus n is bigger\nthan y, there's overlap.",
    "start": "4349250",
    "end": "4355590"
  },
  {
    "text": "TAO B. SCHARDL: Right,\nso it does a comparison. We'll take that a\nlittle more slowly. It does a comparison of x\nplus n, versus y in checks.",
    "start": "4355590",
    "end": "4362489"
  },
  {
    "text": "Is x plus n greater than y? Perfect. How about the next instruction?",
    "start": "4362490",
    "end": "4367644"
  },
  {
    "text": " Yeah?",
    "start": "4367644",
    "end": "4373050"
  },
  {
    "text": "AUDIENCE: It compares\ny plus n versus x. TAO B. SCHARDL: It\ncompares y plus n, versus x, is y plus n\neven greater than x.",
    "start": "4373050",
    "end": "4379930"
  },
  {
    "text": "How would the last\ninstruction before the branch? ",
    "start": "4379930",
    "end": "4394335"
  },
  {
    "text": "Yep, go for it? AUDIENCE: [INAUDIBLE] TAO B. SCHARDL: [INAUDIBLE]\none of the results.",
    "start": "4394335",
    "end": "4399420"
  },
  {
    "text": "So this computes the\ncomparison, is x plus n greater than y, bit-wise? And is y plus n greater than x.",
    "start": "4399420",
    "end": "4408330"
  },
  {
    "text": "Fair enough. So what does the result\nof that condition mean? I think we've pretty much\nalready spoiled the answer.",
    "start": "4408330",
    "end": "4414700"
  },
  {
    "text": "Anyone want to hear\nit one last time? ",
    "start": "4414700",
    "end": "4420326"
  },
  {
    "text": "We had this whole setup. ",
    "start": "4420326",
    "end": "4425710"
  },
  {
    "text": "Go for it. AUDIENCE: They overlap. TAO B. SCHARDL: Checks\nif they overlap. So let's look at this\ncondition in a couple",
    "start": "4425710",
    "end": "4431010"
  },
  {
    "text": "of different situations. If we have x living in\none place in memory, and y living in another\nplace in memory,",
    "start": "4431010",
    "end": "4437790"
  },
  {
    "text": "then no matter how we\nresolve this condition, if we check is both y\nplus n greater than x,",
    "start": "4437790",
    "end": "4445740"
  },
  {
    "text": "and x plus n greater than y,\nthe results will be false.",
    "start": "4445740",
    "end": "4451300"
  },
  {
    "text": "But if we have this\nsituation, where x and y overlap in memory\nsome portion of memory,",
    "start": "4451300",
    "end": "4460600"
  },
  {
    "text": "then it turns out that\nregardless of whether x or y is first, x plus n will\nbe greater than y. y",
    "start": "4460600",
    "end": "4465910"
  },
  {
    "text": "plus n will be greater than x. And the condition\nwill return true. In other words, the\ncondition returns true,",
    "start": "4465910",
    "end": "4472090"
  },
  {
    "text": "if and only if these portions\nof memory pointed by x and y alias.",
    "start": "4472090",
    "end": "4478469"
  },
  {
    "text": "So going back to our\noriginal looping code, we have a situation where\nwe have a branch based on",
    "start": "4478470",
    "end": "4484810"
  },
  {
    "text": "whether or not they alias. And in one case, it executes\nthe vectorized loop.",
    "start": "4484810",
    "end": "4490900"
  },
  {
    "text": "And in another case, it\nexecutes a non-vectorized code. So returning to our original\nquestion, in particular",
    "start": "4490900",
    "end": "4497620"
  },
  {
    "text": "is a vectorized loop\nif they don't alias. So returning to our\noriginal question,",
    "start": "4497620",
    "end": "4504130"
  },
  {
    "text": "does this code get vectorized? The answer is yes and no.",
    "start": "4504130",
    "end": "4509800"
  },
  {
    "text": "So if you voted yes,\nyou're actually right. If you voted no, and you were\npersuaded, you were right.",
    "start": "4509800",
    "end": "4515949"
  },
  {
    "text": "And if you didn't commit to\nan answer, I can't help you. ",
    "start": "4515950",
    "end": "4521472"
  },
  {
    "text": "But that's interesting. The compiler actually generated\nmultiple versions of this loop,",
    "start": "4521472",
    "end": "4527560"
  },
  {
    "text": "due to uncertainty\nabout memory aliasing. Yeah, question? AUDIENCE: [INAUDIBLE]",
    "start": "4527560",
    "end": "4536342"
  },
  {
    "start": "4536342",
    "end": "4547179"
  },
  {
    "text": "TAO B. SCHARDL: So the\nquestion is, could the compiler figure out this\ncondition statically while it's compiling\nthe function?",
    "start": "4547180",
    "end": "4553630"
  },
  {
    "text": "Because we know the\nfunction is going to get called from somewhere. The answer is, sometimes it can.",
    "start": "4553630",
    "end": "4561100"
  },
  {
    "text": "A lot of times it can't. If it's not capable of\ninlining this function, for example, then it probably\ndoesn't have enough information",
    "start": "4561100",
    "end": "4568660"
  },
  {
    "text": "to tell whether or not these\ntwo pointers will alias. For example, you're\njust building a library with a bunch of vector routines.",
    "start": "4568660",
    "end": "4577417"
  },
  {
    "text": "You don't know the code\nthat's going to call this routine eventually.",
    "start": "4577417",
    "end": "4583090"
  },
  {
    "text": "Now, in general,\nmemory aliasing, this will be the last point\nbefore we wrap up, in general, memory aliasing can\ncause a lot of issues",
    "start": "4583090",
    "end": "4590925"
  },
  {
    "text": "when it comes to\ncompiler optimization. It can cause the compiler\nto act very conservatively.",
    "start": "4590925",
    "end": "4596320"
  },
  {
    "text": "In this example, we have\na simple serial base case for a matrix multiply routine.",
    "start": "4596320",
    "end": "4601555"
  },
  {
    "text": "But we don't know anything\nabout the pointers to the C, A, or B matrices. And when we try to compile\nthis and optimize it,",
    "start": "4601555",
    "end": "4608620"
  },
  {
    "text": "the compiler complains that it\ncan't do loop invariant code motion, because it doesn't know\nanything about these pointers.",
    "start": "4608620",
    "end": "4615310"
  },
  {
    "text": "It could be that\nthe pointer changes within the innermost loop. So it can't move\nsome calculation out",
    "start": "4615310",
    "end": "4622120"
  },
  {
    "text": "to an outer loop.  Compilers try to deal with this\nstatically using an analysis",
    "start": "4622120",
    "end": "4630070"
  },
  {
    "text": "technique called alias analysis. And they do try very\nhard to figure out, when are these pointers\ngoing to alias?",
    "start": "4630070",
    "end": "4638739"
  },
  {
    "text": "Or when are they\nguaranteed to not alias? Now, in general, it turns\nout that alias analysis",
    "start": "4638740",
    "end": "4645219"
  },
  {
    "text": "isn't just hard. It's undecidable. If only it were hard,\nmaybe we'd have some hope.",
    "start": "4645220",
    "end": "4650940"
  },
  {
    "text": "But compilers, in\npractice, are faced with this undecidable question. And they try a variety of tricks\nto get useful alias analysis",
    "start": "4650940",
    "end": "4657550"
  },
  {
    "text": "results in practice. For example, based on\ninformation in the source code,",
    "start": "4657550",
    "end": "4662570"
  },
  {
    "text": "the compiler might\nannotate instructions with various metadata to track\nthis aliasing information.",
    "start": "4662570",
    "end": "4668860"
  },
  {
    "text": "For example, TBAA is aliasing\ninformation based on types.",
    "start": "4668860",
    "end": "4674139"
  },
  {
    "text": "There's some scoping\ninformation for aliasing. There is some\ninformation that says it's guaranteed not to alias\nwith this other operation,",
    "start": "4674140",
    "end": "4681639"
  },
  {
    "text": "all kinds of metadata. Now, what can you\ndo as a programmer to avoid these issues\nof memory aliasing?",
    "start": "4681640",
    "end": "4688330"
  },
  {
    "text": "Always annotate\nyour pointers, kids. Always annotate your pointers. The restrict keyword\nyou've seen before.",
    "start": "4688330",
    "end": "4695170"
  },
  {
    "text": "It tells the compiler,\naddress calculations based off this pointer won't alias\nwith address calculations",
    "start": "4695170",
    "end": "4701830"
  },
  {
    "text": "based off other pointers. The const keyword provides\na little more information. It says, these addresses\nwill only be read from.",
    "start": "4701830",
    "end": "4709739"
  },
  {
    "text": "They won't be written to. And that can enable a lot\nmore compiler optimizations.",
    "start": "4709740",
    "end": "4715030"
  },
  {
    "text": "Now, that's all the\ntime that we have. There are a couple of other\ncool case studies in the slides. You're welcome to peruse\nthe slides afterwards.",
    "start": "4715030",
    "end": "4722390"
  },
  {
    "text": "Thanks for listening. ",
    "start": "4722390",
    "end": "4725538"
  }
]