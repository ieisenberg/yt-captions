[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13320",
    "end": "18369"
  },
  {
    "text": " JOHN W. ROBERTS:\nOur investigation",
    "start": "18370",
    "end": "23859"
  },
  {
    "text": "of stochastic gradient descent. ",
    "start": "23860",
    "end": "29398"
  },
  {
    "text": "This time, we'll be talking\nabout the stochastic policy interpretation. And I have some examples.",
    "start": "29398",
    "end": "35750"
  },
  {
    "text": "Well, a very simple\nsort of system example. But a whole bunch of ways of\nlooking and fooling with it in policy parameters that\nhopefully give you some",
    "start": "35750",
    "end": "41922"
  },
  {
    "text": "feel for how to actually\ngo about using this, if you want to use in your\nproject or something like that.",
    "start": "41922",
    "end": "48040"
  },
  {
    "start": "48040",
    "end": "59380"
  },
  {
    "text": "So here.",
    "start": "59380",
    "end": "67480"
  },
  {
    "start": "67480",
    "end": "79980"
  },
  {
    "text": "OK. ",
    "start": "79980",
    "end": "86080"
  },
  {
    "text": "So I gave a little\nintroduction to the idea",
    "start": "86080",
    "end": "93820"
  },
  {
    "text": "of a stochastic\npolicy last time. But this time, I'll give maybe\na little review of the things",
    "start": "93820",
    "end": "100540"
  },
  {
    "text": "and also a sketch\nof the algorithm. I think someone asked,\nJoe asked for pseudocode.",
    "start": "100540",
    "end": "106180"
  },
  {
    "text": "To get a feel of the process\nyou need to go through. ",
    "start": "106180",
    "end": "112850"
  },
  {
    "text": "So just in sort of words,\nthis is the process.",
    "start": "112850",
    "end": "117900"
  },
  {
    "text": "I'll just get right into it. Set. Alpha.",
    "start": "117900",
    "end": "123003"
  },
  {
    "text": "And again, there's\na bunch of ways of actually implementing\nthis and going through the details of it. But this is a very simple\nway and easy understand.",
    "start": "123003",
    "end": "128590"
  },
  {
    "text": "And most of the other\nforms are pretty clearly derived from this. We can sort of tweak it\nmaybe to get a little bit",
    "start": "128590",
    "end": "134920"
  },
  {
    "text": "better performance or fill in\nthe details in different ways. But set alpha to\nan initial guess.",
    "start": "134920",
    "end": "140710"
  },
  {
    "start": "140710",
    "end": "147730"
  },
  {
    "text": "Wow. Sorry. So this is what you\nstart your policy off as.",
    "start": "147730",
    "end": "153460"
  },
  {
    "text": "You can start it as zero or\na bunch of random numbers. And this can affect your\nresults a lot actually.",
    "start": "153460",
    "end": "158620"
  },
  {
    "text": "Good initial guesses will\nobviously converge faster. But also they can be a bit\ndifferent base of of attraction for different local minima.",
    "start": "158620",
    "end": "165079"
  },
  {
    "text": "So it also makes sense sometimes\nto run it several times from different initial guesses\nand just see where it goes.",
    "start": "165080",
    "end": "170200"
  },
  {
    "text": "And if they all converge\nto the same thing, that means you're at least\nin a local minima that has a large base of attraction. While, if they are go to\nvery different things,",
    "start": "170200",
    "end": "177020"
  },
  {
    "text": "that means that you sort of\nhave a local minima problem. And you can take the\nminimum of all those runs or you can try to\ninvestigate exactly why you",
    "start": "177020",
    "end": "184330"
  },
  {
    "text": "keep on getting stuck. So the initial guess\nisn't just like come up with the best thing you can. It also means sort of map out\nthe space of policy parameters",
    "start": "184330",
    "end": "190660"
  },
  {
    "text": "somewhat so that, when you\nget stuck in a local minima, you have some confidence about\nhow good that policy really is.",
    "start": "190660",
    "end": "196558"
  },
  {
    "text": "So then this is again,\nyou don't have to do this. But run a policy pi alpha.",
    "start": "196558",
    "end": "207739"
  },
  {
    "text": "So this is the policy with your\nparameter set of your alpha. And store the cost\nas your baseline.",
    "start": "207740",
    "end": "217952"
  },
  {
    "text": " So that gives you\nyour first baseline.",
    "start": "217952",
    "end": "223780"
  },
  {
    "text": "Then let's see. The Do While loop here.",
    "start": "223780",
    "end": "231489"
  },
  {
    "text": "So do draw noise z.",
    "start": "231490",
    "end": "244280"
  },
  {
    "text": "So if you remember before, maybe\nI should sketch this first one and fill both these\nboards with the sketch,",
    "start": "244280",
    "end": "249620"
  },
  {
    "text": "z is the noise you\nadd to your policy. So when you're searching,\nwhen you sample, that's the vector you add.",
    "start": "249620",
    "end": "255690"
  },
  {
    "text": "I'll write out the\ndetails of it over there. So draw your noise z\nfrom some distribution.",
    "start": "255690",
    "end": "261629"
  },
  {
    "start": "261629",
    "end": "269460"
  },
  {
    "text": "Now, yesterday we talked about\nthe example of a Gaussian. ",
    "start": "269460",
    "end": "277760"
  },
  {
    "text": "That's a common one. But there's a number\nof other ways. And actually, today we'll talk\nabout a different distribution",
    "start": "277760",
    "end": "283139"
  },
  {
    "text": "and sort of why\nyou'd want to use it.  Run system with pi alpha plus z.",
    "start": "283140",
    "end": "299973"
  },
  {
    "text": "So this is sort of\nwhen we're evaluating how well that policy does. ",
    "start": "299973",
    "end": "307930"
  },
  {
    "text": "And then we'll say we'll store\nthis just in J. All right.",
    "start": "307930",
    "end": "316699"
  },
  {
    "text": "Now, we do the update.  Remember?",
    "start": "316700",
    "end": "321800"
  },
  {
    "text": "That we were talking about. So alpha gets alpha plus\nnegative eta J minus b.",
    "start": "321800",
    "end": "338315"
  },
  {
    "text": "b is our baseline up here. ",
    "start": "338315",
    "end": "347030"
  },
  {
    "text": "So J minus b z. ",
    "start": "347030",
    "end": "353720"
  },
  {
    "text": "And then update your baseline. ",
    "start": "353720",
    "end": "359270"
  },
  {
    "text": "Now you can do this\nwith a second run",
    "start": "359270",
    "end": "365680"
  },
  {
    "text": "where you run your new policy. That's sort of the\nmore expensive way. But you can have confidence\nin that baseline, as long as it's not too random\nwhen you evaluate the system.",
    "start": "365680",
    "end": "376327"
  },
  {
    "text": "You could do a decaying average. You could have,\nfor example, b gets 0.2 times your J plus 0.8\ntimes your old baseline.",
    "start": "376327",
    "end": "388790"
  },
  {
    "text": "Right? And so, this one is sort of an\nexponentially decaying average where I take my new\none, smooth it out",
    "start": "388790",
    "end": "395360"
  },
  {
    "text": "by averaging with\nthe previous one. And then if you think\nabout what this does, sort of every\nmeasurement gets sort",
    "start": "395360",
    "end": "403550"
  },
  {
    "text": "of exponentially decayed\naway and its significance as you go through time.",
    "start": "403550",
    "end": "408635"
  },
  {
    "text": "That's the update. And then you can do this\nwhile not converged.",
    "start": "408635",
    "end": "416210"
  },
  {
    "text": " So many times, you can\njust do a four loop, too.",
    "start": "416210",
    "end": "424090"
  },
  {
    "text": "And you can just run\nit for 100 iterations or several iterations\nand see whether or not it looks to have converged.",
    "start": "424090",
    "end": "429202"
  },
  {
    "text": "That's what I often do. But this is really what\nI guess you're doing. It's just the four looping\nstuff is sometimes easier",
    "start": "429202",
    "end": "434690"
  },
  {
    "text": "and prevents you\nfrom getting stuck where your convergence\nconditions ever met and you have to Control C.\nSo hopefully this",
    "start": "434690",
    "end": "443419"
  },
  {
    "text": "is clear enough that, if you\nwanted to implement this, you can go do it and\ntry for your project if that's what\nyou interested in.",
    "start": "443420",
    "end": "450160"
  },
  {
    "text": "All right. So-- STUDENT: [INAUDIBLE].",
    "start": "450160",
    "end": "456035"
  },
  {
    "text": "JOHN W. ROBERTS: I'm sorry. This is et cetera. There's different ways you\ncould update your baseline. You could do it with a-- yeah. Sorry. ",
    "start": "456035",
    "end": "463310"
  },
  {
    "text": "I mean, these are\ntwo common ones. I mean, these are the\ntwo that I usually use.",
    "start": "463310",
    "end": "468770"
  },
  {
    "text": "But you could have critics\nand stuff like that, too, where you have more\ncomplicated updates for having more complicated baseline.",
    "start": "468770",
    "end": "475060"
  },
  {
    "text": " All right. So when should you use this?",
    "start": "475060",
    "end": "485030"
  },
  {
    "start": "485030",
    "end": "490250"
  },
  {
    "text": "We'll just say stochastic\ngradient descent. ",
    "start": "490250",
    "end": "496460"
  },
  {
    "text": "Well, it's important\nto remember, one, that it never does better\nthan true gradient descent.",
    "start": "496460",
    "end": "509810"
  },
  {
    "start": "509810",
    "end": "515292"
  },
  {
    "text": "So if it's cheap to\nevaluate the true gradient, you can follow that\ntrue gradient down. And you're in pretty good shape.",
    "start": "515292",
    "end": "520490"
  },
  {
    "text": "Now the fact that\nit's random maybe gives you some robustness\nto very tiny local minima. But you could add that\nin your gradient descent,",
    "start": "520490",
    "end": "526150"
  },
  {
    "text": "too, if you're just [INAUDIBLE]\nyourself around a bit. So stochastic gradient descent,\nif you have an easy way",
    "start": "526150",
    "end": "531290"
  },
  {
    "text": "to compute the gradients,\nyou can do gradient descent using those. Or you can even do something\nlike SNOPT or some high order method.",
    "start": "531290",
    "end": "537150"
  },
  {
    "text": "So if it's easy to compute\nthe gradients, exactly. It doesn't\nnecessarily make sense",
    "start": "537150",
    "end": "542180"
  },
  {
    "text": "to do stochastic\ngradient descent. If those gradients are\nimpossible to compute or extremely\nexpensive to compute,",
    "start": "542180",
    "end": "547822"
  },
  {
    "text": "then it can make sense. ",
    "start": "547822",
    "end": "552920"
  },
  {
    "text": "Then also, if you have\na good parameterization,",
    "start": "552920",
    "end": "566240"
  },
  {
    "text": "it can be quite efficient. Again, I guess this isn't\na prerequisite because it",
    "start": "566240",
    "end": "571940"
  },
  {
    "text": "will work with high dimensions.",
    "start": "571940",
    "end": "578372"
  },
  {
    "start": "578372",
    "end": "583790"
  },
  {
    "text": "And just sort of like in\nnaive parameterizations. ",
    "start": "583790",
    "end": "596839"
  },
  {
    "text": "Wow. You just leave params.",
    "start": "596840",
    "end": "603200"
  },
  {
    "text": "But it'll be really slow. But is slow.",
    "start": "603200",
    "end": "610408"
  },
  {
    "text": "And it can get stuck\nin local minima. ",
    "start": "610408",
    "end": "622860"
  },
  {
    "text": "So if you have a good\nparameterization, it can be much more\nreasonable to use this. An example is, in our\nlab, we have this glider",
    "start": "622860",
    "end": "628628"
  },
  {
    "text": "we're trying to have perch. So we launch it\nout this catapult. It flies through,\nexecutes a maneuver, and then tries to\ncatch this line.",
    "start": "628629",
    "end": "634700"
  },
  {
    "text": "Right? Now, in that case, if\nyour parameterization were the kind of\nopen loop tapes we've previously used, where just\nwhat elevator angle did",
    "start": "634700",
    "end": "641870"
  },
  {
    "text": "you try to serve\nit to, that could be a bad parameterization. It's a course that's going to\ndemand sort of rough things.",
    "start": "641870",
    "end": "647520"
  },
  {
    "text": "And so, trying to do learning\non that can be very slow. Especially because\nevaluation is so expensive. So if we can come up with a\ngood parameterization though,",
    "start": "647520",
    "end": "654890"
  },
  {
    "text": "suddenly it becomes\nreasonable to do this. If we can knock down to five\nparameters that parameterize a very sort of nice\nclass of policies,",
    "start": "654890",
    "end": "660645"
  },
  {
    "text": "now maybe we don't\nneed that much data. And we can actually get\nit just by launching it. So if we can come up with a good\nparameterization, doing this",
    "start": "660645",
    "end": "667260"
  },
  {
    "text": "should work. Yeah? STUDENT: What if you had\na good estimate of what your trajectory should be? Do you parameterize [INAUDIBLE]?",
    "start": "667260",
    "end": "672644"
  },
  {
    "text": " JOHN W. ROBERTS:\nWell, I mean, you could start with a\ngood initial guess,",
    "start": "672644",
    "end": "677750"
  },
  {
    "text": "which maybe means that you\nconverge reasonably quickly. But the thing is\nthat the learning is going to struggle to\nget a lot of improvement. It will get some improvement.",
    "start": "677750",
    "end": "683900"
  },
  {
    "text": "It'll show in these examples\nyou have ugly parameterizations, it'll still learn a bit. But the problem is that, if you\nparameters in this open loop",
    "start": "683900",
    "end": "691190"
  },
  {
    "text": "tape, let's say we have some\nreally nice smooth trajectory like this.",
    "start": "691190",
    "end": "696620"
  },
  {
    "text": "And we parameterize it. Is this all clear? Is this is sort of\noff on a tangent?",
    "start": "696620",
    "end": "702829"
  },
  {
    "text": " But this is just sort of\nabout bad parameterizations.",
    "start": "702830",
    "end": "709004"
  },
  {
    "text": " So do you want me to go\nthrough all this first? And I can talk about\nthe points in detail?",
    "start": "709004",
    "end": "715250"
  },
  {
    "text": " STUDENT: Do you\nneed a microphone?",
    "start": "715250",
    "end": "720487"
  },
  {
    "text": "JOHN W. ROBERTS: I believe\nI have a microphone. [INTERPOSING VOICES] Thank you. ",
    "start": "720487",
    "end": "729070"
  },
  {
    "text": "All right. So are we all on board here? Or are we-- yeah. OK.",
    "start": "729070",
    "end": "734370"
  },
  {
    "text": "Good. Yeah. This is sort of disorganized. So if you parameterize by\nsetting all these numbers,",
    "start": "734370",
    "end": "745463"
  },
  {
    "text": "you have a nice\nsmooth trajectory. That's the kind\nof stuff you want. Now if you\nparameterize by saying each one of these numbers,\nstochastic gradient descent",
    "start": "745463",
    "end": "751200"
  },
  {
    "text": "when it bounces around\nis going to be like, OK. Send this guy up a bit. Send this guy down a bit. These two guys up. This guy down, this guy down.",
    "start": "751200",
    "end": "757960"
  },
  {
    "text": "Up. This guy back in the same place. You know? Something like that. And you get some param\npolicy like this. Right?",
    "start": "757960",
    "end": "763800"
  },
  {
    "text": "Now your physical system is\ngoing to smooth that out. It's going to filter it. Right? You're not going to\nexecute something like that necessarily. But it's very\nunlikely that that's",
    "start": "763800",
    "end": "769838"
  },
  {
    "text": "the right kind of policy. Right? I mean, it's very sort of\nfine-grained tick-tick-tick doesn't really make sense.",
    "start": "769838",
    "end": "775480"
  },
  {
    "text": "So if you assume you are going\nto have something smooth, it's nice to do something that's\ngoing to be sort of always relatively smooth. If you were to use fewer\npoints parameters as a spline,",
    "start": "775480",
    "end": "782430"
  },
  {
    "text": "that could be a win. So that's what I mean by a good\nparameterization is something",
    "start": "782430",
    "end": "788130"
  },
  {
    "text": "that ideally captures\nthe kind of behaviors you want in your system. ",
    "start": "788130",
    "end": "795180"
  },
  {
    "text": "So yeah. I think the main\npoint is don't--",
    "start": "795180",
    "end": "802330"
  },
  {
    "text": "well, this is not\nthe main point. This is a big point that I think\na lot of people tend to do, and I can be guilty\nof myself, is",
    "start": "802330",
    "end": "808070"
  },
  {
    "text": "don't discard Back prop\nthrough time, RTRL with SNOPT.",
    "start": "808070",
    "end": "821900"
  },
  {
    "text": " Many times, because you can\nsee how simple this update is,",
    "start": "821900",
    "end": "830381"
  },
  {
    "text": "it's very tempting\njust to be like, OK, well, I'll put\nthis loop and run it. And I'll be in good shape. I mean, because it's\ntrivial to write that code.",
    "start": "830382",
    "end": "837320"
  },
  {
    "text": "I mean, you can see it\nfrom pseudocode here. So when you're working\non your project, if you're like oh well,\nI want to optimize. I'll just throw this on there.",
    "start": "837320",
    "end": "843019"
  },
  {
    "text": "Bam. I mean, it'll take you 30\nminutes to get that code ready.",
    "start": "843020",
    "end": "848270"
  },
  {
    "text": "But maybe you think back prop\nthrough time is more confusing, you have to do with all these\njoint equations and stuff",
    "start": "848270",
    "end": "853760"
  },
  {
    "text": "like that. But these give you\nthe true gradient. If your gradients are cheap,\ndoing this can be a win. And then you can\nget a better policy.",
    "start": "853760",
    "end": "860060"
  },
  {
    "text": "You can solve it a lot faster. You can check richer\nclasses of parameterizations because it will solve\nthem so quickly.",
    "start": "860060",
    "end": "865473"
  },
  {
    "text": "So when you're thinking\nabout, OK, we're trying to solve this problem\nand you have your project, don't choose this without\nbeing very conscious that it",
    "start": "865473",
    "end": "873800"
  },
  {
    "text": "has a lot of limitations. And there's a lot\nof alternatives which could be better. So first think, are\nthere very solid",
    "start": "873800",
    "end": "879589"
  },
  {
    "text": "sort of things you\ncan do with these using SNOPT converged nicely? If you can't, if you don't have\na good model, if your system is",
    "start": "879590",
    "end": "885275"
  },
  {
    "text": "too complicated,\nstochastic range descent maybe is what you want. ",
    "start": "885275",
    "end": "890966"
  },
  {
    "text": "All right. So that's sort of a discussion\ncoming back from Tuesday",
    "start": "890966",
    "end": "897920"
  },
  {
    "text": "on when to do these things and\nalso hopefully code that now that you have it in your notes.",
    "start": "897920",
    "end": "903770"
  },
  {
    "text": "Now that you have\nit in your notes, you'll be able to implement\nthis pretty easily.",
    "start": "903770",
    "end": "911190"
  },
  {
    "text": "So now getting onto\nnew stuff for today,",
    "start": "911190",
    "end": "917561"
  },
  {
    "text": "we're talking about\nstochastic policies. ",
    "start": "917561",
    "end": "929240"
  },
  {
    "text": "And in particular, we are\ntalking about a certain class of these kind of algorithms.",
    "start": "929240",
    "end": "934250"
  },
  {
    "text": "Reinforced algorithms. ",
    "start": "934250",
    "end": "943780"
  },
  {
    "text": "All right. And I think that these were\nintroduced by Williams. ",
    "start": "943780",
    "end": "951080"
  },
  {
    "text": "He's at Northeastern\nin '92 or something.",
    "start": "951080",
    "end": "958260"
  },
  {
    "text": "So once again, trying\nto get across the idea",
    "start": "958260",
    "end": "963690"
  },
  {
    "text": "of the stochastic policy. So this interpretation\nis very different. We're not going\nto linearization. We're not going to assume\nwe have this nominal policy,",
    "start": "963690",
    "end": "971670"
  },
  {
    "text": "we sample and try\nsomething else. We're going to\nassume that we have some distribution of policy. Our policy is a distribution.",
    "start": "971670",
    "end": "986670"
  },
  {
    "text": "So we can think about it as, we\nparameterize our distribution with alpha.",
    "start": "986670",
    "end": "992720"
  },
  {
    "text": "Our policy then has some\nstochasticity to it, which is going to give us what\nused to be our alpha plus z.",
    "start": "992720",
    "end": "1000730"
  },
  {
    "text": "Right? So we know that this\nis not a sample. This is the output\nof our policy.",
    "start": "1000730",
    "end": "1007750"
  },
  {
    "text": "This is how we\nrepresent our policy. Not as the nominal action,\nbut as some parameterization of the behavior.",
    "start": "1007750",
    "end": "1014029"
  },
  {
    "text": "Right. So this would be,\nfor example, if I were to play a game where\nI was flipping a coin",
    "start": "1014030",
    "end": "1019540"
  },
  {
    "text": "and I bet on something,\nit could be like, OK. My parameter here\nis the percentage of times I'm going to\nbet money on it coming up",
    "start": "1019540",
    "end": "1027121"
  },
  {
    "text": "heads or something like that. I mean, that doesn't\nmake sense because you're [INAUDIBLE] is zero or net zero\nif you're playing a fair game.",
    "start": "1027121",
    "end": "1032740"
  },
  {
    "text": "But the point is\nthat it's something like, this is the\nprobability of doing this. And so here, we're\ngoing to think about this as the mean\nof a bunch of Gaussians.",
    "start": "1032740",
    "end": "1040900"
  },
  {
    "text": "So our policy is do\nall these actions with the probabilities described\nby a multivariate Gaussian",
    "start": "1040900",
    "end": "1047770"
  },
  {
    "text": "with means described\nby this vector. All right. ",
    "start": "1047770",
    "end": "1055310"
  },
  {
    "text": "OK. [INTERPOSING VOICES] STUDENT: So you just\nhave alpha plus z, is the policy just to add\nnoise to your parameters?",
    "start": "1055310",
    "end": "1063980"
  },
  {
    "text": "JOHN W. ROBERTS: I\nmean, these are the two different interpretations. Right? So I mean, it is\neffectively doing that. I mean over here, we\nhave our nominal thing.",
    "start": "1063980",
    "end": "1070980"
  },
  {
    "text": "And then we add this noise. Right? Then we have our nominal policy. We add noise with\ndistribution like this.",
    "start": "1070980",
    "end": "1077870"
  },
  {
    "text": "But this one, what\nwe can have is this is parameterizing\na distribution.",
    "start": "1077870",
    "end": "1083000"
  },
  {
    "text": "And our policy is to do these\nthings with this probability. Right? STUDENT: And you said\nthat alpha is the mean--",
    "start": "1083000",
    "end": "1088983"
  },
  {
    "text": "JOHN W. ROBERTS: For us, it's\nthe mean of these Gaussians. But you could parameterize\nit any way you wanted.",
    "start": "1088983",
    "end": "1095058"
  },
  {
    "text": "I mean, you could have a\npolicy parameterized by, if you have discrete\nactions, it could be the probability\nof all these actions and force them to sum up to one.",
    "start": "1095058",
    "end": "1101390"
  },
  {
    "text": "It could be some\nparameterization of a PDF or something like that. So we're going to talk\nabout it in a limited case.",
    "start": "1101390",
    "end": "1106790"
  },
  {
    "text": "And I think that thinking about\nthat way, at least for me, is easier to think\nabout where we're going. But remember, that's\na lot more general.",
    "start": "1106790",
    "end": "1111997"
  },
  {
    "text": "This is just parameterizing\nsome distribution of actions. And so, it could be\nsomething very general.",
    "start": "1111997",
    "end": "1117210"
  },
  {
    "text": "But if you're having trouble\nputting your head around what all that means, you can\nthink about it as just like, this is describing the means\nof a bunch of Gaussians.",
    "start": "1117210",
    "end": "1124010"
  },
  {
    "text": "And then our policy\nis do these actions with the probabilities\ndescribed by that.",
    "start": "1124010",
    "end": "1129890"
  },
  {
    "text": "Does that make sense? I think that this is\nsort of, I feel like I'm",
    "start": "1129890",
    "end": "1135519"
  },
  {
    "text": "running around in circles here. All right. I just want to be clear. So in that case, it\ndoesn't make sense to say,",
    "start": "1135520",
    "end": "1142809"
  },
  {
    "text": "so this goes through our\nsystem and produces cost J.",
    "start": "1142810",
    "end": "1153160"
  },
  {
    "text": "But now J is a random variable. ",
    "start": "1153160",
    "end": "1164620"
  },
  {
    "text": "All right. So it no longer makes sense to\nsay, what is J for this policy? Because running\nthat policy is going",
    "start": "1164620",
    "end": "1170600"
  },
  {
    "text": "to produce all sorts\nof different Js. Right? So when you evaluate\nthe policy, I mean you could have a question\nof, what is the right way?",
    "start": "1170600",
    "end": "1178300"
  },
  {
    "text": "Should you do the min of J? Should you do, what is the\nworst J it's going to produce? Or what is the maximum\nJ it's going to produce?",
    "start": "1178300",
    "end": "1185019"
  },
  {
    "text": "Well, the thing that\nwe generally choose",
    "start": "1185020",
    "end": "1191050"
  },
  {
    "text": "in this world-- well, not Earth,\nbut our discipline--",
    "start": "1191050",
    "end": "1196810"
  },
  {
    "text": "is you want to look at\nthe expected value of J.",
    "start": "1196810",
    "end": "1203480"
  },
  {
    "text": "And so, this is a philosophical\ndecision to some degree. I think Russ may have\ntalked about this a bit.",
    "start": "1203480",
    "end": "1209090"
  },
  {
    "text": "But when you're an\nairliner, you probably want to look at the min of J.\nWhen you're building a gambling",
    "start": "1209090",
    "end": "1215410"
  },
  {
    "text": "robot, maybe you do want\nthe expected value of J. So we have a lot of money. Right?",
    "start": "1215410",
    "end": "1220992"
  },
  {
    "text": "And so, this is a\nphilosophical decision. But it's analytically tractable. And it makes sense\nin many cases.",
    "start": "1220992",
    "end": "1227080"
  },
  {
    "text": "And again, like\nRuss says, animals aren't doing robust control. I mean, if they were, we\nwouldn't be sprinting around",
    "start": "1227080",
    "end": "1232495"
  },
  {
    "text": "and jumping, doing gymnastics. Well, some people would\nbe doing gymnastics. I'm probably not\ndoing them anyway. But my expected value of\ntrying to do gymnastics",
    "start": "1232495",
    "end": "1239230"
  },
  {
    "text": "would not be high. I can guarantee that much. All right. So what we're going to look\nat is our stochastic gradient",
    "start": "1239230",
    "end": "1246110"
  },
  {
    "text": "set now. Since we can't just\ndJ d alpha, we're going to have to look at d\nexpected value of J d alpha.",
    "start": "1246110",
    "end": "1257920"
  },
  {
    "text": "Right? So this is now our metric. And so we have to\nlook at the gradient of that expected value.",
    "start": "1257920",
    "end": "1265309"
  },
  {
    "text": "Now, this then is the\ndefinition of expected value.",
    "start": "1265310",
    "end": "1274570"
  },
  {
    "text": "Right? We can call, I'm going to call\nthis sort of like the policy parameters that you're\nrunning on under this trial.",
    "start": "1274570",
    "end": "1283269"
  },
  {
    "text": "I'm going to call it\nbeta in an attempt to be as unoriginal as possible.",
    "start": "1283270",
    "end": "1288919"
  },
  {
    "text": "So we're going to\ncall this beta.  So the expected\nvalue for J we want",
    "start": "1288920",
    "end": "1295540"
  },
  {
    "text": "to integrate over all beta. ",
    "start": "1295540",
    "end": "1301030"
  },
  {
    "text": "And your expected\nvalue is going to be-- ",
    "start": "1301030",
    "end": "1311990"
  },
  {
    "text": "I'm sorry. I've got an error in my notes.  So J of beta,\nprobability of beta.",
    "start": "1311990",
    "end": "1321950"
  },
  {
    "text": " So this is just definition\nof expected value. ",
    "start": "1321950",
    "end": "1329490"
  },
  {
    "text": "Now, if you push the\nd, d alpha through--",
    "start": "1329490",
    "end": "1337050"
  },
  {
    "text": " yeah, sorry. If you push the\nd, d alpha through",
    "start": "1337050",
    "end": "1347770"
  },
  {
    "text": "for a beta, what you're going\nto find is that J of beta",
    "start": "1347770",
    "end": "1357550"
  },
  {
    "text": "is not a function of alpha. We're saying beta\nup here written up here is a function of alpha.",
    "start": "1357550",
    "end": "1362830"
  },
  {
    "text": "I mean, that's how\nwe've derived it. But remember that beta is\njust the actions you've chosen from a distribution.",
    "start": "1362830",
    "end": "1368290"
  },
  {
    "text": "So if you look at running your\nsystem with these parameters, that's not a random variable.",
    "start": "1368290",
    "end": "1373300"
  },
  {
    "text": "That is just your number. Right? So the thing that varies\nthough is the probability",
    "start": "1373300",
    "end": "1381130"
  },
  {
    "text": "of getting beta. Right? This is the function,\nthis distribution. So this is what\ndepends on alpha.",
    "start": "1381130",
    "end": "1386350"
  },
  {
    "start": "1386350",
    "end": "1398549"
  },
  {
    "text": "All right. Now, this is where a bit\nof cleverness comes in.",
    "start": "1398550",
    "end": "1408980"
  },
  {
    "text": "You can write this then as a\nhandle over beta, J beta P.",
    "start": "1408980",
    "end": "1422780"
  },
  {
    "text": "So you get the\ndistribution beta. Then d, d alpha of\nln of p of beta.",
    "start": "1422780",
    "end": "1437570"
  },
  {
    "text": "d beta. Right? So this is just\nsaying, when you take the derivative of\nyour natural log, you're going to\ncancel out this one.",
    "start": "1437570",
    "end": "1443360"
  },
  {
    "text": "Right? You'll get the derivative\nof the one inside. That makes sense? When you do a\nchain rule on this, you're going to get one over\nP beta d P beta, d alpha.",
    "start": "1443360",
    "end": "1454270"
  },
  {
    "text": "So this is sort of the\ntrick that you need. So you should see this. All right.",
    "start": "1454270",
    "end": "1459580"
  },
  {
    "text": " I can erase the pseudocode.",
    "start": "1459580",
    "end": "1465476"
  },
  {
    "start": "1465477",
    "end": "1476690"
  },
  {
    "text": "So once you do that, if you look\nat the expression over there, what you can see is that it's\nthe same as the expected value",
    "start": "1476690",
    "end": "1487880"
  },
  {
    "text": "of J beta d ln P beta d alpha.",
    "start": "1487880",
    "end": "1500030"
  },
  {
    "text": " Right? Because we're\nintegrating overall beta,",
    "start": "1500030",
    "end": "1506330"
  },
  {
    "text": "probability of beta. Right? So what's the expected\nvalue of this?",
    "start": "1506330",
    "end": "1512900"
  },
  {
    "text": "That means that, if we can make\nthis our update, if we can make",
    "start": "1512900",
    "end": "1523190"
  },
  {
    "text": "effectively this our update,\nthen the expected value of our update is equal\nto the derivative",
    "start": "1523190",
    "end": "1532130"
  },
  {
    "text": "of the expected value of\nour performance with respect to our policy parameterization.",
    "start": "1532130",
    "end": "1537140"
  },
  {
    "text": "Do you see? That's sort of cool. Because this seems like\nsort of a complicated thing,",
    "start": "1537140",
    "end": "1542600"
  },
  {
    "text": "the derivative of your\nexpected value with respect to your policies, other things. It's this big random thing\nover a very complicated J.",
    "start": "1542600",
    "end": "1548810"
  },
  {
    "text": "Like a general J. We haven't\nassumed anything about J. We didn't do a linearization. We didn't do anything like that. Right? So J is still however\ngeneral you want it to be.",
    "start": "1548810",
    "end": "1555750"
  },
  {
    "text": "It's not local anything. The derivative this with respect\nalpha, we can make this update.",
    "start": "1555750",
    "end": "1562820"
  },
  {
    "text": "We follow that derivative. So that's pretty cool. Right? ",
    "start": "1562820",
    "end": "1570200"
  },
  {
    "text": "So the question is, now\nwhat does this update look like in practice?",
    "start": "1570200",
    "end": "1575460"
  },
  {
    "text": "This is kind of an ugly term. So what happens when we actually\ntry to do an update like this?",
    "start": "1575460",
    "end": "1583940"
  },
  {
    "text": "Well, we can write\nthis as our update",
    "start": "1583940",
    "end": "1594872"
  },
  {
    "text": "in the direction\nof the gradient is going to be delta alpha again. Now we want a learning rate\nbecause this is again sort",
    "start": "1594872",
    "end": "1601160"
  },
  {
    "text": "of a gradient following thing. So generally, in there, you\nwant some sort of learning rate. And you're just going to\ndo a gradient descent.",
    "start": "1601160",
    "end": "1608750"
  },
  {
    "text": "You get this J of\nbeta, which again now",
    "start": "1608750",
    "end": "1614720"
  },
  {
    "text": "we can write this\nis alpha plus z. ",
    "start": "1614720",
    "end": "1620980"
  },
  {
    "text": "All right. And then something to represent\nthat d ln d alpha P beta.",
    "start": "1620980",
    "end": "1629140"
  },
  {
    "text": "So we'll call that E. And\nthat's called the eligibility. I think that term may\ncome from neural networks.",
    "start": "1629140",
    "end": "1635990"
  },
  {
    "text": "But you can think of it as\nthis eligibility captures how sensitive each parameter\nshould be to an update.",
    "start": "1635990",
    "end": "1642760"
  },
  {
    "text": "So let's say we have 10 alphas. And the eligibility for one\nof those alphas is very high.",
    "start": "1642760",
    "end": "1649900"
  },
  {
    "text": "That means that, if we do\nwell, that eligibility should go much more in that direction.",
    "start": "1649900",
    "end": "1656472"
  },
  {
    "text": "The eligibility is\nsort of just capturing the weights of\nhow much you think each parameter is responsible\nfor affecting that output.",
    "start": "1656472",
    "end": "1664360"
  },
  {
    "text": "Right? So a big eligibility\non a parameter means that that parameter\nis going to move a lot. If the eligibility\nis small, it's",
    "start": "1664360",
    "end": "1670510"
  },
  {
    "text": "not going to move much at all.  So does that makes sense? So that's the way to think\nabout the eligibility.",
    "start": "1670510",
    "end": "1677450"
  },
  {
    "text": "All right. So eligibility is\njust capturing what we expect the significance to be. Yeah? STUDENT: Is it the\nquantity inside",
    "start": "1677450",
    "end": "1683715"
  },
  {
    "text": "the expected value brackets? Or is it the expected value? ",
    "start": "1683715",
    "end": "1688965"
  },
  {
    "text": "JOHN W. ROBERTS: It's this. It's just this quantity. It's not the expected value. We want our update to be this\ninside the expected value.",
    "start": "1688965",
    "end": "1695310"
  },
  {
    "text": "Then the expected value of\nour update is the gradient. So it's just the\nquantity inside.",
    "start": "1695310",
    "end": "1701730"
  },
  {
    "text": "Right. So I think I remember,\nwhen I first learned this,",
    "start": "1701730",
    "end": "1711294"
  },
  {
    "text": "Russ was talking\nabout the eligibility. And I had no idea\nhow to interpret it. And if you still are\nconfused about it,",
    "start": "1711295",
    "end": "1717780"
  },
  {
    "text": "I can try to describe it in\nmore detail maybe with a picture or something like\nthat if you'd like. Do people think they have a good\nidea of what the eligibility",
    "start": "1717780",
    "end": "1724942"
  },
  {
    "text": "means intuitively? OK. Great. ",
    "start": "1724942",
    "end": "1732390"
  },
  {
    "text": "STUDENT: [INAUDIBLE]? JOHN W. ROBERTS: Pardon? STUDENT: I know\neligibility [INAUDIBLE]?? JOHN W. ROBERTS: I'm going to\ngo through how you calculate it",
    "start": "1732390",
    "end": "1739660"
  },
  {
    "text": "right now. Because E, you see,\nis going to be this. It's going to depend on\nwhat our distribution is.",
    "start": "1739660",
    "end": "1746070"
  },
  {
    "text": "So I said, if we\nparameterize our policy by the means of a\nbunch of Gaussians, eligibility is going\nto be one thing.",
    "start": "1746070",
    "end": "1751620"
  },
  {
    "text": "If you parameterize it as a\nbunch of Bernoulli variables or something like that,\nit's going to be different.",
    "start": "1751620",
    "end": "1757240"
  },
  {
    "text": "So it depends on the kind of\ndistribution your policy uses. ",
    "start": "1757240",
    "end": "1763040"
  },
  {
    "text": "All right. ",
    "start": "1763040",
    "end": "1768200"
  },
  {
    "text": "So-- yeah? STUDENT: If alpha isn't the\nmeans of a bunch of Gaussians, then what does\nalpha plus z mean?",
    "start": "1768200",
    "end": "1776809"
  },
  {
    "text": "JOHN W. ROBERTS: z, alpha plus\nz is always the action you take.",
    "start": "1776810",
    "end": "1786040"
  },
  {
    "text": "Sort of the-- we actually\nhad some notes on this. If you think about it, let's\nsay that the way you represented",
    "start": "1786040",
    "end": "1797260"
  },
  {
    "text": "your controller, there are\nsome subtle differences here. But yeah. I think it's worth\ngoing through.",
    "start": "1797260",
    "end": "1803840"
  },
  {
    "text": "So let's say I parameterize my\ncontroller by three numbers. ",
    "start": "1803840",
    "end": "1810010"
  },
  {
    "text": "Like gain on position,\ngain on the speed,",
    "start": "1810010",
    "end": "1816790"
  },
  {
    "text": "and an interval term. So this is a PID controller. Right?",
    "start": "1816790",
    "end": "1821980"
  },
  {
    "text": "So this is something\nthat most of you I think are probably familiar with. So this is a very simple\nparameterization of a policy.",
    "start": "1821980",
    "end": "1829539"
  },
  {
    "text": "Right? Now, this how we control it. Now the thing is that this is\nsort of what the actions are. So this would be, a beta\nwould be one of these.",
    "start": "1829540",
    "end": "1836559"
  },
  {
    "text": "An alpha-- and this terminology\nisn't necessarily standard. I think the alpha and z are.",
    "start": "1836560",
    "end": "1842620"
  },
  {
    "text": "My beta just came\nup when I was trying to make these notes so I could\nhave a simple term to write down all these expressions.",
    "start": "1842620",
    "end": "1847929"
  },
  {
    "text": "But so this is your beta. This is the thing you're\nrunning your system under. Your alpha is the\nsame size as that.",
    "start": "1847930",
    "end": "1856929"
  },
  {
    "text": "But what it is is\nparameterization for some distribution\nof how you select these.",
    "start": "1856930",
    "end": "1865299"
  },
  {
    "text": "All right? So in a Gaussian, I mean\nI give you a simple one. A Bernoulli\ndistribution, this could be-- let's say KP was\neither one or five,",
    "start": "1865300",
    "end": "1874179"
  },
  {
    "text": "then my alpha one could be the\nprobability of picking one. And then the probability\nof picking five",
    "start": "1874180",
    "end": "1879730"
  },
  {
    "text": "is one minus alpha one. STUDENT: So in this case,\nit's a specific evaluation",
    "start": "1879730",
    "end": "1885680"
  },
  {
    "text": "of your stochastic policy? JOHN W. ROBERTS: This is\na specific evaluation. This is the parameterization\nthat defines the distribution.",
    "start": "1885680",
    "end": "1892970"
  },
  {
    "text": "So once you tell me alpha, I\ncan tell you the probability of getting any of these.",
    "start": "1892970",
    "end": "1898100"
  },
  {
    "text": "Right? But when I run it, I have\nthese numbers in there. So this is sort of like a\nsample from a distribution.",
    "start": "1898100",
    "end": "1903820"
  },
  {
    "text": "And this is what I\nactually get the cost of.  Yeah.",
    "start": "1903820",
    "end": "1909980"
  },
  {
    "text": "All right. And so, if you look at this\nminus this, that's the noise we talk about in the\nother interpretation.",
    "start": "1909980",
    "end": "1916365"
  },
  {
    "start": "1916365",
    "end": "1924972"
  },
  {
    "text": "STUDENT: And the\nsystem you're trying to solve is not, [INAUDIBLE]\nhow do you decide [INAUDIBLE]??",
    "start": "1924972",
    "end": "1934443"
  },
  {
    "text": " JOHN W. ROBERTS: Yeah. So we're thinking about this. This is another thing\nI have a note on.",
    "start": "1934443",
    "end": "1941200"
  },
  {
    "text": "We're going about this\nin the context of trials. So you run the\nsystem under a trial. Right?",
    "start": "1941200",
    "end": "1947450"
  },
  {
    "text": "And then you evaluate the cost. What if your system is\njust running constantly? Right. Well, you can think about\na short period of time.",
    "start": "1947450",
    "end": "1955340"
  },
  {
    "text": "As a trial, you could\nrun it for a while. You can look at discounted cost. And then there's also the\naverage cost or average reward",
    "start": "1955340",
    "end": "1962728"
  },
  {
    "text": "formulation where you\ncan look at how long, what reward you expect to get\nrunning it off to infinity. Like, what sort of reward rate.",
    "start": "1962728",
    "end": "1969710"
  },
  {
    "text": "I think about it right\nnow in the context of, let's just say we can\ndo a trial of something. And if you want to\nworry about what",
    "start": "1969710",
    "end": "1975430"
  },
  {
    "text": "happens if this is\nconstantly running, there are ways you\ncan still do a trial. And then you can keep track\nof an eligibility trace, which",
    "start": "1975430",
    "end": "1980720"
  },
  {
    "text": "is different than\nthis eligibility. So that's a confusing thing. What you can do is you\ncan be running constantly.",
    "start": "1980720",
    "end": "1985880"
  },
  {
    "text": "You can just sort of keep\niterating and take into account the sort of coupling\nor [INAUDIBLE].. You can imagine\nit's sort of like,",
    "start": "1985880",
    "end": "1992240"
  },
  {
    "text": "if I study for my test tonight,\nI don't have a good night. But I do well my test tomorrow. So I was happy.",
    "start": "1992240",
    "end": "1997437"
  },
  {
    "text": "And so, it's sort of like,\nthen my action didn't give me an immediate reward. It gave me a reward later. That's sort of\nthe delayed reward",
    "start": "1997437",
    "end": "2003365"
  },
  {
    "text": "problem, which is very common. And there are ways of\ndealing with this here where you can still just look at\nyour accruing award right now.",
    "start": "2003365",
    "end": "2009220"
  },
  {
    "text": "And you can still\nsort of give yourself a reward for doing good\nthings in the past. You can still handle that.",
    "start": "2009220",
    "end": "2015350"
  },
  {
    "text": "So I mean, that is\na good question. And there's a number of\nways to deal with this. But right now, just think\nabout executing a trial.",
    "start": "2015350",
    "end": "2021029"
  },
  {
    "text": "Right? ",
    "start": "2021030",
    "end": "2026590"
  },
  {
    "text": "So then our E here-- again, to make it\nlook like what we want it to look like, to make it--",
    "start": "2026590",
    "end": "2033299"
  },
  {
    "text": "well, that tells us\nexactly what our E is. So we can write\nit as this vector. d d alpha one of ln\nprobability of beta.",
    "start": "2033300",
    "end": "2045789"
  },
  {
    "text": " And this beta is\nstill this vector. But the probability\nof that is a scalar.",
    "start": "2045790",
    "end": "2052239"
  },
  {
    "text": "Right? So this element of the\nprobability is just a scalar. And that's our\nentire thing here.",
    "start": "2052239",
    "end": "2058310"
  },
  {
    "text": "Right? So then let's say again, this\nis where the distribution you",
    "start": "2058310",
    "end": "2066270"
  },
  {
    "text": "choose comes into play. Let's say that we\nchose our distribution to be a bunch of Gaussians.",
    "start": "2066270",
    "end": "2073469"
  },
  {
    "text": "All the different\nparameters ID, the noise ID.",
    "start": "2073469",
    "end": "2079507"
  },
  {
    "text": "But we'll just think\nabout as an independent with the same sigma. So the probability\nof getting a beta,",
    "start": "2079507",
    "end": "2084540"
  },
  {
    "text": "because they're\nall independent, is the product over all the\ndifferent parameters.",
    "start": "2084540",
    "end": "2092760"
  },
  {
    "text": "1 over square root of\n2 pi sigma squared.",
    "start": "2092760",
    "end": "2097870"
  },
  {
    "text": "This is going to\ncramp it if I do that. So I think I have the opposite\nproblem of anyone else.",
    "start": "2097870",
    "end": "2104890"
  },
  {
    "text": "I write way too big. Video people will be happy. So let just me\nsquish it in here.",
    "start": "2104890",
    "end": "2112019"
  },
  {
    "text": " So probability of beta\nis equal to product",
    "start": "2112020",
    "end": "2123770"
  },
  {
    "text": "over all my different\nparameters with 1 over-- well, you know the\nGaussian distribution.",
    "start": "2123770",
    "end": "2131345"
  },
  {
    "text": "It's like this. ",
    "start": "2131345",
    "end": "2144300"
  },
  {
    "text": "Right. So if you give me a beta\nand I have the alpha, I can tell you the probability\nof us picking that beta.",
    "start": "2144300",
    "end": "2151300"
  },
  {
    "text": "All right. So our ln, the\nnatural log of this,",
    "start": "2151300",
    "end": "2161960"
  },
  {
    "text": "will then allow you to turn\nthis product into a sum. And you'll get the sum\nof over i equals 1 to n.",
    "start": "2161960",
    "end": "2170600"
  },
  {
    "start": "2170600",
    "end": "2187240"
  },
  {
    "text": "Now we can take this derivative. This is a constant. That won't show up. And we'll just have\nthe derivative of--",
    "start": "2187240",
    "end": "2194290"
  },
  {
    "text": " yeah. ",
    "start": "2194290",
    "end": "2200860"
  },
  {
    "text": "Yes. So the derivative, now\nwe'll get rid of this. This one is simple to do now. It's just a product.",
    "start": "2200860",
    "end": "2206460"
  },
  {
    "text": "So the derivative of\nln P beta d alpha--",
    "start": "2206460",
    "end": "2218150"
  },
  {
    "text": "I did this again-- equals. ",
    "start": "2218150",
    "end": "2230230"
  },
  {
    "text": "So beta i minus alpha\ni over sigma squared. Now if you remember how we\ntalked about it over there,",
    "start": "2230230",
    "end": "2241750"
  },
  {
    "text": "it ends up being [INAUDIBLE].",
    "start": "2241750",
    "end": "2247320"
  },
  {
    "text": " So here is our eligibility.",
    "start": "2247320",
    "end": "2257880"
  },
  {
    "text": "z over sigma squared. Now what does that\nmean our update is?",
    "start": "2257880",
    "end": "2263960"
  },
  {
    "start": "2263960",
    "end": "2279080"
  },
  {
    "text": "So hopefully, this\nlooks familiar. My voice is struggling today.",
    "start": "2279080",
    "end": "2285510"
  },
  {
    "text": "I'm going to come in next\ntime talking like this. It'll be great. So this is our update.",
    "start": "2285510",
    "end": "2292400"
  },
  {
    "text": "Does this look familiar? [INTERPOSING VOICES] Yeah. Actually, let me-- yeah.",
    "start": "2292400",
    "end": "2298940"
  },
  {
    "text": "I guess so. Does that look familiar? ",
    "start": "2298940",
    "end": "2304130"
  },
  {
    "text": "Is this no, it does not\nlook at all familiar? Or is this yes, it looks so\nfamiliar, I don't waste my time and embarrass myself\nby saying it does?",
    "start": "2304130",
    "end": "2311940"
  },
  {
    "text": "It does. Right? It's the exact same update. Right? Now this one, we\ndon't have a baseline. And we have sigma squared.",
    "start": "2311940",
    "end": "2319500"
  },
  {
    "text": "But that sigma squared\nis just a scalar. And we already said, baseline or\nno baseline, whatever you want,",
    "start": "2319500",
    "end": "2324650"
  },
  {
    "text": "it doesn't affect it. Right? Now, is that true for this case? It didn't affect the other one.",
    "start": "2324650",
    "end": "2330172"
  },
  {
    "text": "This one, it also does. If you think about\nyour reward as-- ",
    "start": "2330172",
    "end": "2337340"
  },
  {
    "text": "if you think about your\nreward as one everywhere, then your derivative, let's see.",
    "start": "2337340",
    "end": "2347600"
  },
  {
    "start": "2347600",
    "end": "2361788"
  },
  {
    "text": "Where is this exactly? ",
    "start": "2361788",
    "end": "2389080"
  },
  {
    "text": "OK. All right. So if this is constant, then\nour derivative expected value",
    "start": "2389080",
    "end": "2399520"
  },
  {
    "text": "is zero. That means that the expected\nvalue delta alpha over here",
    "start": "2399520",
    "end": "2409150"
  },
  {
    "text": "has to be zero because it's\nequal to that expected value. That means the expected value\nof our eligibility is zero.",
    "start": "2409150",
    "end": "2415030"
  },
  {
    "text": "Right? So the expected value of the\neligibility is always zero.",
    "start": "2415030",
    "end": "2421002"
  },
  {
    "text": "So that means that\nsome constant in there is not going to affect it. Right? When we do this expected value,\nwhen we put in our baseline,",
    "start": "2421002",
    "end": "2427898"
  },
  {
    "text": "that's going to turn into zero. Because the expected\nvalue of E is zero. The exact same thing\nwe had previously. Right?",
    "start": "2427898",
    "end": "2433390"
  },
  {
    "text": "And you can see it's the case\nfor this specific example of z. The expected value\nof z is zero again. So it's not going to affect\nthe direction of our update",
    "start": "2433390",
    "end": "2440440"
  },
  {
    "text": "and expectation. So yes, you can do anything\nlike this with eligibilities. You can still use a baseline.",
    "start": "2440440",
    "end": "2447580"
  },
  {
    "text": "Right? So this is really the exact same\nupdate that we already found.",
    "start": "2447580",
    "end": "2452859"
  },
  {
    "text": " All right. ",
    "start": "2452860",
    "end": "2459319"
  },
  {
    "text": "So I think that's pretty cool. ",
    "start": "2459320",
    "end": "2465020"
  },
  {
    "text": "Update that we\noriginally interpreted as locally following\nthe gradient",
    "start": "2465020",
    "end": "2470210"
  },
  {
    "text": "is also following the\ntrue gradient, not some sort of local\nbut the true gradient",
    "start": "2470210",
    "end": "2476330"
  },
  {
    "text": "of the expected value of the\nperformance of that policy. All right? So if you have some really\nugly value function,",
    "start": "2476330",
    "end": "2483620"
  },
  {
    "text": "you throw this\ndistribution on top of it and you move it around. It's going to be\nsome, it's going",
    "start": "2483620",
    "end": "2491510"
  },
  {
    "text": "to follow in the direction\nof true improvement there. And a little test case. I remember when I was trying\nto think about interpreting",
    "start": "2491510",
    "end": "2496650"
  },
  {
    "text": "these things back when\nwe were originally doing some of this analysis\non the signal to noise ratio,",
    "start": "2496650",
    "end": "2502730"
  },
  {
    "text": "if you think of little cusp, if\nyou have a value function that has a little plateau on it, I\ncan write this on a fixed board",
    "start": "2502730",
    "end": "2511760"
  },
  {
    "text": "because it's not going to stay. If you think you have\na value function that's",
    "start": "2511760",
    "end": "2516770"
  },
  {
    "text": "sitting around here, my drawing\nisn't going to be too clear.",
    "start": "2516770",
    "end": "2524370"
  },
  {
    "text": "But here, we'll do it in 1D. If you have a value\nfunction like this,",
    "start": "2524370",
    "end": "2536450"
  },
  {
    "text": "you sit here and follow\nthe local true gradient. It is very small. The other one says that\nyou'll follow that gradient,",
    "start": "2536450",
    "end": "2544280"
  },
  {
    "text": "no matter how big it is. So what it's sort of doing\nwhen you have this Gaussian is it smooths out the kinks and\nstuff like this in your policy.",
    "start": "2544280",
    "end": "2551990"
  },
  {
    "text": "Right? Because you think my\nlocal analysis says, OK, well, I'll just\nget local enough that these little kinks\nin my value function",
    "start": "2551990",
    "end": "2558533"
  },
  {
    "text": "aren't a problem anymore. But here, we say we\ndidn't say it was local. We said it was global. And so, it's following\nthis gradient,",
    "start": "2558533",
    "end": "2564110"
  },
  {
    "text": "even if we have ugly stuff\nin our value function. The reason is because\nthis distribution sort of smooths that stuff out.",
    "start": "2564110",
    "end": "2569900"
  },
  {
    "text": "And now it's able to follow it. So that gradient is\nstill well-defined. It takes the stuff and\nsmooths it into something.",
    "start": "2569900",
    "end": "2576020"
  },
  {
    "text": "Does that makes sense?  I think it's just the big sort\nof difference in interpretation",
    "start": "2576020",
    "end": "2585100"
  },
  {
    "text": "right here. There's the one where\nit's this local analysis and follows the local true\ngradient but only locally.",
    "start": "2585100",
    "end": "2590550"
  },
  {
    "text": "And then we have the other\none which follows even, it doesn't require a small sigma\nor anything like that anymore.",
    "start": "2590550",
    "end": "2596490"
  },
  {
    "text": "It'll follow the expected\nvalue, the true gradient of the expected\nvalue of your reward.",
    "start": "2596490",
    "end": "2604578"
  },
  {
    "text": "And it's too big of\na mouthful for it to be clear just by saying it. But these are the two\ndifferent interpretations.",
    "start": "2604578",
    "end": "2611200"
  },
  {
    "text": "All right?  Good.",
    "start": "2611200",
    "end": "2617178"
  },
  {
    "text": "STUDENT: That function\nis basically the policy. JOHN W. ROBERTS: Oh yes,\nI'll draw this better.",
    "start": "2617178",
    "end": "2623010"
  },
  {
    "text": " So here is sort of our alpha.",
    "start": "2623010",
    "end": "2630750"
  },
  {
    "text": "This is our reward. Here we have our\n[INAUDIBLE] function.",
    "start": "2630750",
    "end": "2638920"
  },
  {
    "text": "Our policy is taking actions\nwith this probability.",
    "start": "2638920",
    "end": "2644250"
  },
  {
    "text": "All right? That's our policy. So you can see that this-- even\nthough this has a kink in it,",
    "start": "2644250",
    "end": "2649642"
  },
  {
    "text": "as it moves that over a bit,\nthis is varying smoothly. So even though this is a\nlittle disconnected like that,",
    "start": "2649642",
    "end": "2655297"
  },
  {
    "text": "this is going to slide\nover still smoothly. Right? So the gradient is always\ngoing to be nicely well defined and everything. And this also goes\nout to infinity.",
    "start": "2655298",
    "end": "2662098"
  },
  {
    "text": "So even if there's some\nugly thing way out there, you're never going to\nsuddenly go and start seeing stuff you never saw before. You always sort of saw it.",
    "start": "2662098",
    "end": "2667799"
  },
  {
    "text": "Right? So you'll be following the sort\nof smooth one all the time.",
    "start": "2667800",
    "end": "2673480"
  },
  {
    "text": "And that's nice\nbecause maybe you won't be following when\nyou get really broad.",
    "start": "2673480",
    "end": "2680498"
  },
  {
    "text": "One interpretation says, well,\nwe're not in the linear regime anymore. So we're not following\nthat local true gradient.",
    "start": "2680498",
    "end": "2685590"
  },
  {
    "text": "This is sort of the\nissue that brought it up. Let's say this one says\nit's the linear analysis. And it says, OK.",
    "start": "2685590",
    "end": "2690785"
  },
  {
    "text": "If my sigma is\nsmall enough, so I sample close enough so it\nlooks approximately linear, I'm going to follow\nthat gradient.",
    "start": "2690785",
    "end": "2696810"
  },
  {
    "text": "But instead, when\nit breaks down, you get this really\nbroad Gaussian. So what does that mean?",
    "start": "2696810",
    "end": "2702160"
  },
  {
    "text": "Well, this one is still\nfollowing that gradient. So that means that you're\nseeing stuff all over. And it's following that gradient\ninstead of this local one.",
    "start": "2702160",
    "end": "2708540"
  },
  {
    "text": "Right? Now the problem is that, if\nyou follow that gradient,",
    "start": "2708540",
    "end": "2714059"
  },
  {
    "text": "the stochastic policy\nisn't necessarily actually the optimal one to follow. When you're actually\nrunning your policy",
    "start": "2714060",
    "end": "2719850"
  },
  {
    "text": "in the end, certain\nsystems you do. But in many mechanical\nsystems, you don't actually want these random actions. Right?",
    "start": "2719850",
    "end": "2726210"
  },
  {
    "text": "There's the action that\nactually does minimize cost. And so, the sort of\nlocal interpretation",
    "start": "2726210",
    "end": "2731820"
  },
  {
    "text": "has some value because\nyou don't necessarily want to stick with the\nstochastic policy forever. That's why you sort\nof reduce your sigma.",
    "start": "2731820",
    "end": "2737310"
  },
  {
    "text": "And you get tighter and\ntighter and tighter. And you follow the gradient\nso it's more and more biased to the right spot.",
    "start": "2737310",
    "end": "2742320"
  },
  {
    "text": "Because you can imagine, if\nyou have a value function",
    "start": "2742320",
    "end": "2747780"
  },
  {
    "text": "that's a global\nmin here, and then say this is all extremely high.",
    "start": "2747780",
    "end": "2754890"
  },
  {
    "text": "And this goes down and\nit's like a low plateau. Now, if you have a\nreally broad Gaussian,",
    "start": "2754890",
    "end": "2761670"
  },
  {
    "text": "it may say, OK, well,\neven though this is the best spot to execute, I'm\ngoing to slide way over here.",
    "start": "2761670",
    "end": "2767970"
  },
  {
    "text": "Because I have a sort\nof lower expected cost. Right? That put you in the wrong spot. If it was really tight,\nthen I'd say, OK.",
    "start": "2767970",
    "end": "2775770"
  },
  {
    "text": "Sit right here. So do you see that\nsort of distinction? That this broad guy could push\nyou in the wrong direction",
    "start": "2775770",
    "end": "2782590"
  },
  {
    "text": "if it's really broad. And you may not,\nin the end, want to execute a stochastic\npolicy for this exact reason.",
    "start": "2782590",
    "end": "2788478"
  },
  {
    "text": "Your stochastic\npolicy could have a much higher expected\ncost than a small sigma",
    "start": "2788478",
    "end": "2794820"
  },
  {
    "text": "stochastic policy. Down to where, if\nsigma is zero, that's when you actually have\nthe minimum expected cost. Yeah?",
    "start": "2794820",
    "end": "2799830"
  },
  {
    "text": "STUDENT: So when is this used? It's used when you're\nstarting and you",
    "start": "2799830",
    "end": "2805320"
  },
  {
    "text": "don't know about systems? JOHN W. ROBERTS: When\nyou don't have a model, it's used in the same\ncontext as the other one. Are you saying stochastic\ngradient descent as a whole?",
    "start": "2805320",
    "end": "2811860"
  },
  {
    "text": "Or-- STUDENT: I mean\nstochastic policies. [INTERPOSING VOICES]",
    "start": "2811860",
    "end": "2819007"
  },
  {
    "text": "JOHN W. ROBERTS: It's\nused for exploration. Right? Because our other one, where\nwe had this nominal policy and we add a noise, that\ndoesn't look any different,",
    "start": "2819008",
    "end": "2826809"
  },
  {
    "text": "except from this sort of\ninterpretation, than this. Right? So-- STUDENT: So for the [INAUDIBLE]\nand learning about systems--",
    "start": "2826810",
    "end": "2834470"
  },
  {
    "text": "JOHN W. ROBERTS: Yeah. It's for exploration. And you need it\nto get more data. Right? Because if I run the\nsame policy all the time,",
    "start": "2834470",
    "end": "2841017"
  },
  {
    "text": "I don't know how\nthings are sensitive. So my stochastic policy\ngives me this information to where I can learn.",
    "start": "2841017",
    "end": "2846030"
  },
  {
    "text": "Right? So in the end many\ntimes, yes, you'll want a deterministic policy.",
    "start": "2846030",
    "end": "2852420"
  },
  {
    "text": "But for the development\nof that policy, you'll want to execute\nall sorts of things that could be suboptimal\nto give you information.",
    "start": "2852420",
    "end": "2858450"
  },
  {
    "text": "And so, that is the\ninterpretation here. It's for that learning. And if your system\nis evolving and stuff",
    "start": "2858450",
    "end": "2864359"
  },
  {
    "text": "like that, if it's\nnot constant in time, you may want to always\nrun a stochastic policy. Because you'll be able to\nconstantly learn, if that makes",
    "start": "2864360",
    "end": "2871440"
  },
  {
    "text": "sense. So you may never\nwant to converge.  But I think it's-- yes?",
    "start": "2871440",
    "end": "2877542"
  },
  {
    "text": "STUDENT: How do you\nobtain the parameters of that stochastic policy\nwith that probability?",
    "start": "2877542",
    "end": "2883170"
  },
  {
    "text": "It's very sensitive\nto sigma, right? JOHN W. ROBERTS: How do I\nget the parameters of what? STUDENT: Of that distribution.",
    "start": "2883170",
    "end": "2888190"
  },
  {
    "text": "JOHN W. ROBERTS: Distribution? Well, I mean that's\nwhat I've set. Right? I said I'm going to act with\nthe probability described",
    "start": "2888190",
    "end": "2894119"
  },
  {
    "text": "by a certain distribution. And I describe the\ndistribution here. So I know these probabilities. Because this, I set. That's from my controller.",
    "start": "2894120",
    "end": "2899463"
  },
  {
    "text": "I designed that myself. What I don't know is this,\nthis value function aspect.",
    "start": "2899463",
    "end": "2905010"
  },
  {
    "text": "STUDENT: So that it should\nbe based on intuition? JOHN W. ROBERTS: No. No. This up here? Oh, how do you decide\nhow broad this?",
    "start": "2905010",
    "end": "2912270"
  },
  {
    "text": "Intuition has something\nto do with it. Also, you can imagine sampling.",
    "start": "2912270",
    "end": "2917330"
  },
  {
    "text": "I think I mentioned\nthis on Tuesday. You could sample. You can imagine I\nsample a few points",
    "start": "2917330",
    "end": "2924289"
  },
  {
    "text": "and I'd look at them here. And they look straight to me. And I'm like, OK. Well, what if I sample bigger? And I get these points.",
    "start": "2924290",
    "end": "2930830"
  },
  {
    "text": "And it looks relatively rough. Then I go, OK. Well, I want to be sampling\non probably the regime.",
    "start": "2930830",
    "end": "2936198"
  },
  {
    "text": "We're going to see this\nroughness to some degree. I don't want to be\nstuck where it's going to take me forever\nto go anywhere because I'm in the linear regime.",
    "start": "2936198",
    "end": "2941642"
  },
  {
    "text": "So you can do some\nsampling and get sort of the coarseness\nof your value function.",
    "start": "2941642",
    "end": "2947220"
  },
  {
    "text": "And so, I mean that's\nsort of before you run it, you do some of those things. And you figure out how\nbig these parameters are. Because the alternative\nwould be you set it,",
    "start": "2947220",
    "end": "2953150"
  },
  {
    "text": "you see if it how it does,\nand you fool around with them all the time. That's another way to do it. But this way, you sort of\nhave a more direct process",
    "start": "2953150",
    "end": "2960890"
  },
  {
    "text": "for trying to be like, OK. I want to be sampling\nwhere I actually get some interesting information. And so, you want to be\nsampling where you're probably",
    "start": "2960890",
    "end": "2968450"
  },
  {
    "text": "getting distributions\nthat cover linear-- it's fine if it's linear. But you want to\nactually sample to where",
    "start": "2968450",
    "end": "2974090"
  },
  {
    "text": "you're going to move around\nto these different features. You want to sort\nof be able to get,",
    "start": "2974090",
    "end": "2979730"
  },
  {
    "text": "you want your updates when\nyou change your policy to be on the scale, too. Because it was really\nsmall, it could take forever",
    "start": "2979730",
    "end": "2985093"
  },
  {
    "text": "to get down to the minimum. Or if I was over here\nand I had small sigma, I may never sample over here.",
    "start": "2985093",
    "end": "2991130"
  },
  {
    "text": "So I'd never know\nto move that way. Right? And so, if you have\na bigger sigma, you might be to get\nout of this local min.",
    "start": "2991130",
    "end": "2997615"
  },
  {
    "text": "So I sample over here\nand see it's better. And move in that direction. Despite the fact that,\nlocally, it was bad. So there's a number\nof considerations.",
    "start": "2997615",
    "end": "3004685"
  },
  {
    "text": "But yeah, doing some\nsampling and stuff like that combined\nwith some intuition. That's probably the\nbest I can give you as",
    "start": "3004685",
    "end": "3013029"
  },
  {
    "text": "to how to set these parameters. I mean, that's a tricky thing. That's something that's\nnice about SNOPT. There's no eta to set.",
    "start": "3013030",
    "end": "3018580"
  },
  {
    "text": "This one also has\na stigma to set. ",
    "start": "3018580",
    "end": "3029780"
  },
  {
    "text": "So please, Russ emphasized\nthat I was to make all of you understand this\nstuff really well.",
    "start": "3029780",
    "end": "3036227"
  },
  {
    "text": "That's why I had two days. And if you do it on your\nproject and you apply it at the wrong system,\nI'm going to be blamed.",
    "start": "3036227",
    "end": "3042710"
  },
  {
    "text": "[LAUGHTER] Yeah. So if you don't\nhave a model and you don't have anything like\nthat, try that back prop first",
    "start": "3042710",
    "end": "3050990"
  },
  {
    "text": "if you can. Try SNOPT first if\nyou can, I think. If you can't, this\nmaybe won't do as--",
    "start": "3050990",
    "end": "3058309"
  },
  {
    "text": "this can solve the\nproblem, too, I guess. But sort of appreciate\nits limitations.",
    "start": "3058310",
    "end": "3064406"
  },
  {
    "text": "STUDENT: So with the stochastic\ngradient and stochastic policy, you also have to be\ncareful about how",
    "start": "3064406",
    "end": "3069589"
  },
  {
    "text": "you parameterize everything. Right? JOHN W. ROBERTS: Oh, very much. STUDENT: Because, I mean, you\nare taking the expectation over these betas, which\nare in fact instantiations",
    "start": "3069590",
    "end": "3077690"
  },
  {
    "text": "of your parameters. JOHN W. ROBERTS: That\nis a very good point. STUDENT: Are you still having\nthat problem regardless",
    "start": "3077690",
    "end": "3084200"
  },
  {
    "text": "of which two you use? JOHN W. ROBERTS: What\ndo you mean, which two? Oh, whether you're using back\nprop or this or something",
    "start": "3084200",
    "end": "3089510"
  },
  {
    "text": "like that? STUDENT: Well, no. Between the stochastic policy\nand the stochastic gradient descent. JOHN W. ROBERTS: OK.",
    "start": "3089510",
    "end": "3094520"
  },
  {
    "text": "I really want to emphasize\nsomething right now. They're not different. OK? They're not, they're\nthe same thing.",
    "start": "3094520",
    "end": "3101030"
  },
  {
    "text": "All right? You're correct that\nyour parameterization",
    "start": "3101030",
    "end": "3106036"
  },
  {
    "text": "is going to affect things. Because you're going\nto get to a minimum, with respect to your\nparameterization. STUDENT: Right. JOHN W. ROBERTS:\nYou're going to be sampling over your parameters.",
    "start": "3106037",
    "end": "3111450"
  },
  {
    "text": "And so, the cost\nfunction is going to be a function\nof your parameters. So if you imagine, if you\nhave different parameters, the cost function will\nlook completely different.",
    "start": "3111450",
    "end": "3118713"
  },
  {
    "text": "Does that makes sense? If I were to parameterize with\nan open loop tape or feedback policy, how would\nthe value function",
    "start": "3118713",
    "end": "3125090"
  },
  {
    "text": "dependent on those parameters\nbe completely different? So it's very important. That's true. It's over these\ninstantiated parameters.",
    "start": "3125090",
    "end": "3130777"
  },
  {
    "text": "So picking that\naffects everything. And picking a sensible\none can be a hard problem.",
    "start": "3130777",
    "end": "3136145"
  },
  {
    "text": "STUDENT: So there is\nalmost like another, instead of just having\nthe sigma and the rate,",
    "start": "3136145",
    "end": "3142140"
  },
  {
    "text": "you also have to have\nessentially these alphas and [INAUDIBLE]. JOHN W. ROBERTS: You\nhave it for both, right? STUDENT: Right. Exactly.",
    "start": "3142140",
    "end": "3147589"
  },
  {
    "text": "It's like-- JOHN W. ROBERTS: Yeah. STUDENT: It's almost you're\nover-parameterizing everything just so you can get an answer. JOHN W. ROBERTS:\nWell, I mean you",
    "start": "3147590",
    "end": "3152809"
  },
  {
    "text": "need to parameterize\nyour policy either way. I mean, whether you choose to\nparameterize it as an open loop tape where you have 250\nparameters or something",
    "start": "3152810",
    "end": "3159200"
  },
  {
    "text": "like that, you've still\nchosen parameterization. I mean, you still\npicked a lot of them. But you need to\nrepresent it some way.",
    "start": "3159200",
    "end": "3167070"
  },
  {
    "text": "And so, I mean, whether you\ndo back prop or anything, you're still going to have\nthat exact same problem.",
    "start": "3167070",
    "end": "3172620"
  },
  {
    "text": "I mean, you could do\nback prop and make it more efficient if you\nhad five parameters, too. But it also can handle these\nbig open loop tapes and stuff",
    "start": "3172620",
    "end": "3177860"
  },
  {
    "text": "like that. So there's less of a pressure\nto find concise and sort of compact representations.",
    "start": "3177860",
    "end": "3184550"
  },
  {
    "text": "But yeah. But I want to emphasize that\nthese are different ways of looking at it. But update's the same.",
    "start": "3184550",
    "end": "3190130"
  },
  {
    "text": "The algorithm's the same. The only difference\nis, am I following this local true gradient?",
    "start": "3190130",
    "end": "3195230"
  },
  {
    "text": "Am I following the\nexpected value of my cost? ",
    "start": "3195230",
    "end": "3201920"
  },
  {
    "text": "So a toy example here to show\nsome of the practical issues",
    "start": "3201920",
    "end": "3208579"
  },
  {
    "text": "of some of these things. And please, if you\nhave any more questions about any of these things,\neven if it seems tangential,",
    "start": "3208580",
    "end": "3214702"
  },
  {
    "text": "just let me know.  OK. ",
    "start": "3214702",
    "end": "3223630"
  },
  {
    "text": "So here, we have what we just\ntalked about implemented.",
    "start": "3223630",
    "end": "3232420"
  },
  {
    "text": "You can see how simple\nit is right here. I'm doing a four loop instead\nof that while converged. But we have a loop through\nsome number of iterations.",
    "start": "3232420",
    "end": "3241720"
  },
  {
    "text": "I get my sigma and eta. So this is so I can make\nit dependent on the number of iterations or\nsomething like that.",
    "start": "3241720",
    "end": "3247252"
  },
  {
    "text": "So I can make them decrease. I generate the noise according\nto a Gaussian distribution here.",
    "start": "3247252",
    "end": "3253278"
  },
  {
    "text": "I get my reward.  And the rest of\nthis is just sort of plotting stuff and\neverything like that.",
    "start": "3253278",
    "end": "3259622"
  },
  {
    "text": "And then here. Here's my update. And that's where\nI've got my baseline.",
    "start": "3259622",
    "end": "3264820"
  },
  {
    "text": "Right? So you can see that this is just\nsort of that pseudocode right over there. There's nothing more. All these functions\nare just so that I",
    "start": "3264820",
    "end": "3270790"
  },
  {
    "text": "can switch between different\nthings really quickly. But it's very easy to implement. STUDENT: So you do the baseline. And then the\nequations, [INAUDIBLE]??",
    "start": "3270790",
    "end": "3279312"
  },
  {
    "text": "JOHN W. ROBERTS: Right. I mean, this is\nwhat pops out when you look at that expectation. But as we said, the\nexpected value of E is 0.",
    "start": "3279312",
    "end": "3286000"
  },
  {
    "text": "So if I have a constant in\nthere, my expected value if I subtract off that\nconstant the same way, it's not going to affect the\nexpected direction of my update",
    "start": "3286000",
    "end": "3293380"
  },
  {
    "text": "at all. So these say you\ndon't need a baseline. But a baseline has\nperformance benefits.",
    "start": "3293380",
    "end": "3300220"
  },
  {
    "text": "Right? So that's sort of\nanother, if you go these different\ndirections, this one says, oh you don't need a baseline. You can put it and it helps.",
    "start": "3300220",
    "end": "3305320"
  },
  {
    "text": "The other one's more\nnatural to think about starting with a baseline. Then you can say, oh well,\nyou actually don't need it. So again, they both have\nthe same effect there.",
    "start": "3305320",
    "end": "3311140"
  },
  {
    "text": "Maybe I should have\nbeen more clear. But yeah.",
    "start": "3311140",
    "end": "3316300"
  },
  {
    "text": " So the problem that\nwe're looking at",
    "start": "3316300",
    "end": "3321640"
  },
  {
    "text": "is simply, we have a spline.",
    "start": "3321640",
    "end": "3327910"
  },
  {
    "text": "So I make a spline. And then we're\ntrying to figure out a curve that gets as close\nto that spline as possible.",
    "start": "3327910",
    "end": "3333225"
  },
  {
    "text": "So it's really\njust sort of we're trying to fit a curve\nto this random spline. Right? Now, obviously we don't need\nto do anything fancy for this.",
    "start": "3333225",
    "end": "3341260"
  },
  {
    "text": "But it's very visual. And so, I think it's\nprobably a good example here. ",
    "start": "3341260",
    "end": "3348700"
  },
  {
    "text": "So we'll start. The cost function we're\nusing is the square distance",
    "start": "3348700",
    "end": "3354160"
  },
  {
    "text": "between our sort of guessed\ncurve and the actual curve.",
    "start": "3354160",
    "end": "3359410"
  },
  {
    "text": "And try running it. ",
    "start": "3359410",
    "end": "3382240"
  },
  {
    "text": "All right. So on the left,\nthis is our cost. Well, we have it formulated\nas our reward here.",
    "start": "3382240",
    "end": "3388840"
  },
  {
    "text": "So it's negative. And then over here, the blue\nthing is the thing we want.",
    "start": "3388840",
    "end": "3394720"
  },
  {
    "text": "The red is our nominal. And the green is our noise. You see our noise\nis pretty small. I started it at L zero.",
    "start": "3394720",
    "end": "3401050"
  },
  {
    "text": "You can see it climbing here. Right? And it is getting better. It's bouncing around a bit.",
    "start": "3401050",
    "end": "3409210"
  },
  {
    "text": "Yeah. And so, this is actually\nparameterized, this spline. So it could be exactly\ncorrect if we let it.",
    "start": "3409210",
    "end": "3415400"
  },
  {
    "text": "I mean, it's capable of\nrepresenting the curve exactly. So the cost could be\nzero, effectively. Right?",
    "start": "3415400",
    "end": "3420490"
  },
  {
    "text": "I think actually,\nif I have run it, it gets stuck in a\nlocal minimum, which is a good cautionary tale.",
    "start": "3420490",
    "end": "3426490"
  },
  {
    "text": "Even if a problem\nis simple like this, you can still get\nstuck in local minima. So always be careful for that.",
    "start": "3426490",
    "end": "3431925"
  },
  {
    "text": "But see, it's actually\ndoing a reasonably good job. Right? Now you might be\nlike, oh well, it",
    "start": "3431925",
    "end": "3437680"
  },
  {
    "text": "has an ideal parameterization. It's the minimum\nnumber of parameters",
    "start": "3437680",
    "end": "3442810"
  },
  {
    "text": "you need to have a spline\nthat can represent it. And this was the\nsame form of spline. So that's a pretty\nsolid parameterization.",
    "start": "3442810",
    "end": "3449410"
  },
  {
    "text": "But we can try a different one. ",
    "start": "3449410",
    "end": "3455740"
  },
  {
    "text": "This computer is\nshowing its age.",
    "start": "3455740",
    "end": "3460810"
  },
  {
    "text": "When I ran these tests on\nmy desktop, it went pfft. And so, I had my\nnumber of iterations, like several thousand.",
    "start": "3460810",
    "end": "3466780"
  },
  {
    "text": "And then when I realized\nit was going to do this, I was like oh. All right.",
    "start": "3466780",
    "end": "3472060"
  },
  {
    "text": "So here. What parameterization\ndo you want? Linear, tripolated,\nnearest neighbor?",
    "start": "3472060",
    "end": "3478230"
  },
  {
    "text": "A polynomial?  All right. ",
    "start": "3478230",
    "end": "3489230"
  },
  {
    "text": "OK. ",
    "start": "3489230",
    "end": "3501430"
  },
  {
    "text": "[INAUDIBLE] There we go. ",
    "start": "3501430",
    "end": "3519650"
  },
  {
    "text": "Sorry about this. ",
    "start": "3519650",
    "end": "3529857"
  },
  {
    "text": "It's not done. Here we go. Here, you see it's\ncapable of learning.",
    "start": "3529857",
    "end": "3536130"
  },
  {
    "text": "It has this is obviously\ninferior parameterization. It can't represent\nthe right answer. But you can see it is\ngetting closer here.",
    "start": "3536130",
    "end": "3541830"
  },
  {
    "text": "It's not doing a bad job. Right? And the question of\nwhat the optimum is here",
    "start": "3541830",
    "end": "3547453"
  },
  {
    "text": "is not going to look\njust like the curve because it has to\nsort of balance these competing things of\ngoing over and going under.",
    "start": "3547453",
    "end": "3552490"
  },
  {
    "text": "So this is a good\nexample of where your policy\nparameterization doesn't capture the true optimum.",
    "start": "3552490",
    "end": "3559342"
  },
  {
    "text": "And so, when this thing\nconverges to something, it maybe is the best. But it's only the\nbest with respect to that way of\nparameterizing a policy.",
    "start": "3559342",
    "end": "3565590"
  },
  {
    "text": "Right? So if you guessed that nearest\nneighbor is rich enough, then you put it in here. You're going to see\nthat it's not optimal.",
    "start": "3565590",
    "end": "3573900"
  },
  {
    "text": "Right? That you've limited yourself. And sometimes I mean, the\nother one with the spline,",
    "start": "3573900",
    "end": "3579470"
  },
  {
    "text": "you put yourself in the\nregime where you have a very good parameterization. You get very close\nto the right answer. Here, we have poor\nparameterization.",
    "start": "3579470",
    "end": "3586003"
  },
  {
    "text": "And so, it's still\ngoing to learn. It's still going to converge. But it's not going\nto be as good. ",
    "start": "3586003",
    "end": "3599410"
  },
  {
    "text": "So lin interp, we could\nprobably do better. But here. Now, we'll go back\nto-- well, we'll",
    "start": "3599410",
    "end": "3609405"
  },
  {
    "text": "keep it in nearest\nneighbor for now. You can see lin interp. ",
    "start": "3609405",
    "end": "3616300"
  },
  {
    "text": "All right. Now I said previously that\nthis is partially nice because it's robust to noise.",
    "start": "3616300",
    "end": "3622250"
  },
  {
    "text": "So here we're going\nto look at, we're going to add Gaussian noise with\na standard distribution of 20.",
    "start": "3622250",
    "end": "3629230"
  },
  {
    "start": "3629230",
    "end": "3634480"
  },
  {
    "text": "Let's see. Here, it's not doing well.",
    "start": "3634480",
    "end": "3642240"
  },
  {
    "text": "I probably have to\nreduce my sigma eta.",
    "start": "3642240",
    "end": "3650400"
  },
  {
    "start": "3650400",
    "end": "3663980"
  },
  {
    "text": "Let's see if we can get\nback to where it started. Yeah.",
    "start": "3663980",
    "end": "3669908"
  },
  {
    "text": "STUDENT: I think that's the\nsame thing as [INAUDIBLE].. ",
    "start": "3669908",
    "end": "3675890"
  },
  {
    "text": "JOHN W. ROBERTS: You\nmultiply by sigma. The variance is squared.",
    "start": "3675890",
    "end": "3681890"
  },
  {
    "text": "So this is not\ndoing a great job. You see noise can break it.",
    "start": "3681890",
    "end": "3687750"
  },
  {
    "text": "If we start with a\nbigger error though, it's effectively\nstandard deviation.",
    "start": "3687750",
    "end": "3694250"
  },
  {
    "text": "Also maybe our eta\ncould have been too big. Because we're getting\nbig measurements just from this error. So like a big eta, let's\ntry reducing our eta.",
    "start": "3694250",
    "end": "3701480"
  },
  {
    "start": "3701480",
    "end": "3717500"
  },
  {
    "text": "So it didn't get worse. Learning is going to be\nslow though, obviously. Because it has such\na small signal.",
    "start": "3717500",
    "end": "3723780"
  },
  {
    "text": "But at least we prevented it\nfrom doing that terrible thing.",
    "start": "3723780",
    "end": "3731270"
  },
  {
    "text": "That's because it was getting\nthese giant measurements of error and updating\nbased on them.",
    "start": "3731270",
    "end": "3738350"
  },
  {
    "text": "But an expected value will still\ngo where you want it to go. If you did a bunch of\nupdates, you'd be fine. But that just gives\nyou everything out.",
    "start": "3738350",
    "end": "3744505"
  },
  {
    "text": " If we reduce that\ndown to, let's say,",
    "start": "3744505",
    "end": "3767372"
  },
  {
    "text": "learning will be really slow. So let me turn off plotting. ",
    "start": "3767372",
    "end": "3782130"
  },
  {
    "text": "I'm sorry about this. ",
    "start": "3782130",
    "end": "3791969"
  },
  {
    "text": "I don't know what just happened. OK. ",
    "start": "3791969",
    "end": "3800290"
  },
  {
    "text": "So let's see how this does. ",
    "start": "3800290",
    "end": "3807859"
  },
  {
    "text": "Ah, this is smoothed. I ran a boxcar filter on it.",
    "start": "3807860",
    "end": "3813490"
  },
  {
    "text": "But it's not necessarily\nlearning quickly. But with that noise,\nit still learns. Now if we're going\nto be doing this,",
    "start": "3813490",
    "end": "3819190"
  },
  {
    "text": "we might as well kick\nourselves back to 20 and see if we can do anything. ",
    "start": "3819190",
    "end": "3837559"
  },
  {
    "text": "Yeah. So there, it's got enough\nnoise that it's sort of been walking a bit much. Maybe.",
    "start": "3837560",
    "end": "3842580"
  },
  {
    "text": "If you make your eta\nsmaller and stuff like that, maybe make your\nsigma bigger so it gets a bigger signal when\nit measures the change,",
    "start": "3842580",
    "end": "3848260"
  },
  {
    "text": "you might be able to be OK. But I mean, it's still suffered\nfrom these things definitely.",
    "start": "3848260",
    "end": "3855730"
  },
  {
    "text": "Now, another point here. I talked about cost\nfunctions a bit yesterday. Here, we have one where\nit's the square distance.",
    "start": "3855730",
    "end": "3861820"
  },
  {
    "text": "Let's say we-- I think I'm going to\ngive away whether it's going to do better or worse\nby having named it poor grad.",
    "start": "3861820",
    "end": "3868809"
  },
  {
    "text": "But this one, we're going\nto get a reward of one for each point that's within\nof 0.05 of the desired point.",
    "start": "3868810",
    "end": "3876605"
  },
  {
    "text": "So if you're far away at zero\nand if you're close enough, it's one. Right?",
    "start": "3876605",
    "end": "3881994"
  },
  {
    "text": "STUDENT: [INAUDIBLE]. JOHN W. ROBERTS: Right. ",
    "start": "3881994",
    "end": "3890070"
  },
  {
    "text": "Where did my-- ",
    "start": "3890070",
    "end": "3896240"
  },
  {
    "text": "Oh. ",
    "start": "3896240",
    "end": "3905280"
  },
  {
    "text": "So you see? This is a much\nshallower performance.",
    "start": "3905280",
    "end": "3910672"
  },
  {
    "text": "It still looks\nlike it's learning. And it is. Because I mean, even the bad\ncost functions can learn. But now I'm going\nto show you what",
    "start": "3910673",
    "end": "3915780"
  },
  {
    "text": "that looks like when\nyou run it and actually look at the plotting. ",
    "start": "3915780",
    "end": "3932980"
  },
  {
    "text": "You see? ",
    "start": "3932980",
    "end": "3938540"
  },
  {
    "text": "Now we can give it our-- we're still giving it\nthat linear interp. Let's see if we give it\nthe better parameterization",
    "start": "3938540",
    "end": "3944310"
  },
  {
    "text": "and see how that does. ",
    "start": "3944310",
    "end": "3964450"
  },
  {
    "text": "You can see it's not\nlearning nearly as quickly as the other cost function did. Right?",
    "start": "3964450",
    "end": "3971480"
  },
  {
    "text": "Let's get all our things back\nto make eta a bit bigger. ",
    "start": "3971480",
    "end": "4007260"
  },
  {
    "text": "See? So here the only thing that\nchanged between our runs that looked pretty nice and this run\nright now is the cost function.",
    "start": "4007260",
    "end": "4013680"
  },
  {
    "text": " You see, it is over the course\nof-- that's a 300 something,",
    "start": "4013680",
    "end": "4021039"
  },
  {
    "text": "it is getting up there-- but\nif you remember the other one, it's fitting a lot nicer.",
    "start": "4021040",
    "end": "4026470"
  },
  {
    "text": "Right?  And the thing is\nthat, if you look",
    "start": "4026470",
    "end": "4032100"
  },
  {
    "text": "at probably the cost\nof the other one, even using this cost\nfunction, once you solve it, the other one would\nbe lower as well.",
    "start": "4032100",
    "end": "4039420"
  },
  {
    "text": "Do you see what I mean? The problem that,\nif you solved it with the other\none then evaluated",
    "start": "4039420",
    "end": "4044970"
  },
  {
    "text": "this cost function\non the solution, that would be better than\nthis after some number of parameters. And it's because learning\nbased on this is hard.",
    "start": "4044970",
    "end": "4051180"
  },
  {
    "text": "Right? You can see that\nthis is very coarse. It's very easy just to\nlose one or gain one",
    "start": "4051180",
    "end": "4056369"
  },
  {
    "text": "if you don't have good\ngradient information. And so, this is a bad cost\nfunction for learning. Right?",
    "start": "4056370",
    "end": "4061680"
  },
  {
    "text": "The same task, if this were the\ntask you really cared about, formulating it with the other\ncost function and solving it,",
    "start": "4061680",
    "end": "4067170"
  },
  {
    "text": "you actually probably do\nbetter than solving with this directly. Right? And that's because the\ngradients in this one are poor.",
    "start": "4067170",
    "end": "4073829"
  },
  {
    "text": "It's not easy for it\nto define the direction it's supposed to move in. And it's even worse. I was talking before about how\nyou could be in regions where",
    "start": "4073830",
    "end": "4081090"
  },
  {
    "text": "you're getting zero cost\nor zero reward for a while or there's no gradient at all.",
    "start": "4081090",
    "end": "4087730"
  },
  {
    "text": "So if we start ourselves\nwith an error here, it's not getting\nany measurement.",
    "start": "4087730",
    "end": "4093940"
  },
  {
    "text": "You see?  Again, it's not changing\nat all because it",
    "start": "4093940",
    "end": "4100290"
  },
  {
    "text": "can't measure anything. So if we go to--",
    "start": "4100290",
    "end": "4108930"
  },
  {
    "text": "I said the way to solve that,\nif you were stuck with it, could be to use more\nviolent kind of sampling.",
    "start": "4108930",
    "end": "4124120"
  },
  {
    "text": "And there, you see we actually\nare able to get some reward. And it takes a while. But you see that\nthere's more violent sampling at least getting\nout of that region",
    "start": "4124120",
    "end": "4130873"
  },
  {
    "text": "where we don't get information. Right? Still not the best. But if you're stuck\nwith that problem,",
    "start": "4130873",
    "end": "4137439"
  },
  {
    "text": "that's one way to deal with it. But you can see that, with the\nother one where it doesn't have any regions like that-- if\nyou go back to our nice grad--",
    "start": "4137439",
    "end": "4143559"
  },
  {
    "start": "4143560",
    "end": "4155009"
  },
  {
    "text": "sorry, I need to switch my eta.  Yeah. ",
    "start": "4155010",
    "end": "4162250"
  },
  {
    "text": "Whoops. ",
    "start": "4162250",
    "end": "4168649"
  },
  {
    "text": "We can go back to-- ",
    "start": "4168649",
    "end": "4177455"
  },
  {
    "text": "See how much nicer that is?  So there you go.",
    "start": "4177455",
    "end": "4185818"
  },
  {
    "text": "So here you can see\nthe parameterization can affect it a lot. The cost function\ncan affect it a lot.",
    "start": "4185819",
    "end": "4190979"
  },
  {
    "text": "The eta and sigma\ncan affect it a lot. There's a lot of concerns. Right? And that's why that should\nmake SNOPT and stuff more",
    "start": "4190979",
    "end": "4197910"
  },
  {
    "text": "appealing to you. Because if you can complete\nthose gradients using back prop, there's no\nguesswork in that. Right?",
    "start": "4197910",
    "end": "4203460"
  },
  {
    "text": "You give it to SNOPT,\nthere's no parameters to set. I mean, I guess there's some\nconvergence criteria and stuff.",
    "start": "4203460",
    "end": "4210300"
  },
  {
    "text": "But you don't have\nto play these games. There's a lot of ways\nto get this wrong. Right?",
    "start": "4210300",
    "end": "4216800"
  },
  {
    "text": "So-- STUDENT: So the reason\nthat we introduce the [INAUDIBLE] policy was a\nway to explore [INAUDIBLE]??",
    "start": "4216800",
    "end": "4225554"
  },
  {
    "text": " JOHN W. ROBERTS: For here,\nI mean they provide you the exploration.",
    "start": "4225555",
    "end": "4230697"
  },
  {
    "text": "Yeah. There's certain situations-- we\ntalk about gambling and stuff like that-- where\nstochasticity, if there's",
    "start": "4230697",
    "end": "4236202"
  },
  {
    "text": "an adversarial\nkind of component, stochasticity could be necessary\nfor optimality sometimes.",
    "start": "4236202",
    "end": "4242250"
  },
  {
    "text": "Right? But in these kind of cases where\nwe have a dynamical system,",
    "start": "4242250",
    "end": "4249440"
  },
  {
    "text": "I don't see how stochasticity\nin a deterministic system could make you more\noptimal than deterministic.",
    "start": "4249440",
    "end": "4258960"
  },
  {
    "text": "Right? That's because the\nsystem is deterministic and not trying to guess\nwhat you're doing and stuff.",
    "start": "4258960",
    "end": "4265790"
  },
  {
    "text": "So that's pretty nice\nconvergence right there. ",
    "start": "4265790",
    "end": "4273800"
  },
  {
    "text": "And you see, if we\nevaluated this solution with our cost function of\none of the poor gradients,",
    "start": "4273800",
    "end": "4282110"
  },
  {
    "text": "I mean this is going\nto be a lot nicer. Right? And that's even though we\nwouldn't solve it explicitly.",
    "start": "4282110",
    "end": "4287119"
  },
  {
    "text": "It's just because the gradients\nof this one are a lot nicer. So you can solve it a lot nicer. So remember that,\nthat cost functions--",
    "start": "4287120",
    "end": "4294380"
  },
  {
    "text": "even if it's not explicitly\nwhat you maybe most care about-- for example, we care\nabout catching the perch.",
    "start": "4294380",
    "end": "4300530"
  },
  {
    "text": "Right? But if you want to\ncatch the perch, it could be that, by learning\nwith something with a distance",
    "start": "4300530",
    "end": "4306559"
  },
  {
    "text": "in it, maybe we'd learn\nbetter and actually end up catching\nthe perch more than if our reward was just one\nthat could catch the perch.",
    "start": "4306560",
    "end": "4312260"
  },
  {
    "text": "Right? So cost function design\ncan matter a lot.",
    "start": "4312260",
    "end": "4318920"
  },
  {
    "text": "Policy parameterization\nmatters a lot. So hopefully you all feel\nreally comfortable using",
    "start": "4318920",
    "end": "4330619"
  },
  {
    "text": "stochastic gradient descent\nnow in the situations where it's warranted. ",
    "start": "4330620",
    "end": "4336330"
  },
  {
    "text": "STUDENT: Can you talk a\nlittle bit about the baseline? How do you get the baseline? JOHN W. ROBERTS: Yeah.",
    "start": "4336330",
    "end": "4341380"
  },
  {
    "text": "My baseline here. I think I have two limitations,\nlet's see something we can do.",
    "start": "4341380",
    "end": "4347199"
  },
  {
    "text": "So my baseline here, right now\nI'm using a second evaluation.",
    "start": "4347200",
    "end": "4352210"
  },
  {
    "text": "Right? We can do an average baseline. ",
    "start": "4352210",
    "end": "4365010"
  },
  {
    "text": "I screwed something up. ",
    "start": "4365010",
    "end": "4411010"
  },
  {
    "text": "Yeah. I probably-- so\nthat's gone crazy. But we'll try a smaller eta.",
    "start": "4411010",
    "end": "4417312"
  },
  {
    "start": "4417312",
    "end": "4426050"
  },
  {
    "text": "There.  So this is an average baseline.",
    "start": "4426050",
    "end": "4431880"
  },
  {
    "text": "See, it's still improving. It's not doing as nice. And it's also because we have\nto turn down the eta and stuff. I don't know that error was.",
    "start": "4431880",
    "end": "4437550"
  },
  {
    "text": "I'm not going to try\nto debug it right now. But you can see\nit's still learning. Now the real question is,\nwhat if we have no baseline?",
    "start": "4437550",
    "end": "4444490"
  },
  {
    "text": "Right? ",
    "start": "4444490",
    "end": "4468930"
  },
  {
    "text": "I made a really smart eta\nthere because now it's not getting rid of anything. ",
    "start": "4468930",
    "end": "4504830"
  },
  {
    "text": "Yeah. You don't really have to\nturn it down even more.",
    "start": "4504830",
    "end": "4510510"
  },
  {
    "text": "But that's a good example of why\na good baseline is so critical.",
    "start": "4510510",
    "end": "4516019"
  },
  {
    "text": "Right? I mean, you can\nsee that all we did was change how the baseline,\nthe average baseline was working fine. And it didn't cost us any more.",
    "start": "4516020",
    "end": "4521640"
  },
  {
    "text": "You have two extra evaluations\nthat we just got rid of. We put in that. And you see how much\nit struggled there. So I mean, if you just\ndo an average baseline,",
    "start": "4521640",
    "end": "4530617"
  },
  {
    "text": "you get it for free. And it's still a huge\nadvantage over what",
    "start": "4530617",
    "end": "4535972"
  },
  {
    "text": "we're trying to do without\nany baseline at all. ",
    "start": "4535972",
    "end": "4547681"
  },
  {
    "text": "STUDENT: But we did-- I mean, it will\neventually work it out. The baseline would\njust be a lot slower? JOHN W. ROBERTS: Yeah.",
    "start": "4547681",
    "end": "4553500"
  },
  {
    "text": "I mean, it should. I mean, the thing is\nthat, in practice, it can randomly walk around\nand get stuck in places. Like right here, you\nsee this is that.",
    "start": "4553500",
    "end": "4559290"
  },
  {
    "text": "It's a big run of\nimprovement there. And it did better than\nit originally did. You see it's just\nwalking all around.",
    "start": "4559290",
    "end": "4566050"
  },
  {
    "text": "And so, yes. I mean there's still this\nbias towards improvement. And it will work. And if you were to calculate\na whole bunch of delta alphas",
    "start": "4566050",
    "end": "4573510"
  },
  {
    "text": "and then add them all\ntogether, then you probably wouldn't walk in the\nwrong direction as often. But here's the thing. You have to remember that\nit's sort of always moving",
    "start": "4573510",
    "end": "4579667"
  },
  {
    "text": "in the direction. Right? It's just it's moving\nmore or less, depending on how good it is. And so, yes.",
    "start": "4579667",
    "end": "4587160"
  },
  {
    "text": "This should still work. But in practice, a good baseline\nhelps a lot, as you've seen.",
    "start": "4587160",
    "end": "4594870"
  },
  {
    "text": "So hopefully, all\nthose examples-- STUDENT: Is it also the case\nthat that baseline will not",
    "start": "4594870",
    "end": "4600480"
  },
  {
    "text": "hurt you? JOHN W. ROBERTS: Well, how bad? I mean, this could\nbe considered bad. Right? A zero baseline did that.",
    "start": "4600480",
    "end": "4605640"
  },
  {
    "text": "[INTERPOSING VOICES] STUDENT: Or can you actually\nhave a baseline that gives you worse results than no baseline?",
    "start": "4605640",
    "end": "4612810"
  },
  {
    "text": "JOHN W. ROBERTS:\nYou probably could. Because you could just\nhave it be a bigger error. Right? Just more in the\nwrong direction.",
    "start": "4612810",
    "end": "4617960"
  },
  {
    "text": "STUDENT: Right. JOHN W. ROBERTS: So Yeah. But I mean that's\nwhat I'm saying. If you average over\nthe past several trials",
    "start": "4617960",
    "end": "4623362"
  },
  {
    "text": "like we did and then reduce\nthe rate of your learning so that those averages are\nrelatively representative and you don't move\ntoo much on your own--",
    "start": "4623362",
    "end": "4629580"
  },
  {
    "text": "STUDENT: [INAUDIBLE]. JOHN W. ROBERTS: Yeah. But I'm saying yeah. I mean, an average\nbaseline is free. There's no extra\npolicy evaluations.",
    "start": "4629580",
    "end": "4635070"
  },
  {
    "text": "And it still helps a lot. So an average\nbaseline makes sense. And I mean, in something like\nthis where it's so cheap,",
    "start": "4635070",
    "end": "4642230"
  },
  {
    "text": "if you want those big\njumps and stuff like that, the average baseline is\nif the big jump can struggle.",
    "start": "4642230",
    "end": "4648060"
  },
  {
    "text": "Because the cost is so\ndifferent when you've updated. Right? So your average is going\nto make a lot of sense. But so they're sometimes\ngoing to be better rather than",
    "start": "4648060",
    "end": "4654700"
  },
  {
    "text": "reducing your updates. Like when we're really far\naway from the right policy and we have a long\nway to move and we could move in all sorts of\nways, that'd still be good.",
    "start": "4654700",
    "end": "4662520"
  },
  {
    "text": "The average baseline,\nif we turn everything down to the average baseline,\nand it still works nicely,",
    "start": "4662520",
    "end": "4668550"
  },
  {
    "text": "that probably wouldn't be\nas fast as doing two policy evaluations and doing big jumps. Right?",
    "start": "4668550",
    "end": "4674160"
  },
  {
    "text": "Where every time, you\njust re-evaluate it. STUDENT: It just resets it. JOHN W. ROBERTS: And so, yeah. To move it smoothly with\nan average baseline, we may need more than\ntwice as many iterations",
    "start": "4674160",
    "end": "4681630"
  },
  {
    "text": "as the one does when you\ndo an ideal baseline. That's something that\nI've seen in practices. When you're doing\nthis big updates,",
    "start": "4681630",
    "end": "4687270"
  },
  {
    "text": "a real evaluation can be\nfewer evaluations in the end. So anything else or anything\nelse you'd like to see",
    "start": "4687270",
    "end": "4696840"
  },
  {
    "text": "run on the spline fitting\nproblem right here? ",
    "start": "4696840",
    "end": "4704010"
  },
  {
    "text": "OK. No one has any questions? We're all good? I'm going to tell Russ that you\nunderstand stochastic gradient",
    "start": "4704010",
    "end": "4712070"
  },
  {
    "text": "descent perfectly. And it's going to be extremely\nuseful in your research in the problems to\nwhich it is well suited.",
    "start": "4712070",
    "end": "4719659"
  },
  {
    "text": "All right. Great. ",
    "start": "4719660",
    "end": "4723000"
  }
]