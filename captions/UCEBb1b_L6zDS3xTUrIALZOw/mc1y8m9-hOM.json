[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "12720",
    "end": "20340"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nWe're talking about generalized linear models. And in generalized\nlinear models,",
    "start": "20340",
    "end": "25764"
  },
  {
    "text": "we generalize linear\nmodels in two ways. The first one is to allow\nfor a different distribution",
    "start": "25764",
    "end": "31170"
  },
  {
    "text": "for the response variables. And the distributions\nthat we wanted was the exponential family.",
    "start": "31170",
    "end": "37230"
  },
  {
    "start": "37230",
    "end": "44239"
  },
  {
    "text": "And this is a family\nthat can be generalized over random variables\nthat are defined",
    "start": "44240",
    "end": "49340"
  },
  {
    "text": "on c or q in general,\nwith parameters rk. But we're going to focus in\na very specific case when",
    "start": "49340",
    "end": "58309"
  },
  {
    "text": "y is a real valued\nresponse variable, which is the one you're used to when\nyou're doing linear regression.",
    "start": "58310",
    "end": "64040"
  },
  {
    "text": "And the parameter\ntheta also lives in r.",
    "start": "64040",
    "end": "69500"
  },
  {
    "text": "And so we're going to talk\nabout the canonical case. So that's the canonical\nexponential family,",
    "start": "69500",
    "end": "75920"
  },
  {
    "text": "where you have a density,\ntheta of x, which is of the form, exponential plus.",
    "start": "75920",
    "end": "85280"
  },
  {
    "text": "And then, we have y,\nwhich interacts with theta only by taking a product. Then, there's a term that\ndepends only on theta,",
    "start": "85280",
    "end": "92990"
  },
  {
    "text": "some dispersion parameter phi. And then, we have some\nnormalization factor. Let's call it c of y phi.",
    "start": "92990",
    "end": "104899"
  },
  {
    "text": "So it really should not matter\ntoo much, so it's c of y phi, and that's really just the\nnormal position factor.",
    "start": "104900",
    "end": "110520"
  },
  {
    "text": "And here, we're going to\nassume that phi is known. ",
    "start": "110520",
    "end": "117480"
  },
  {
    "text": "I have no idea what I write. I don't know if\nyou guys can read. I don't know what chalk\nhas been used today, but I just can't see it.",
    "start": "117480",
    "end": "125480"
  },
  {
    "text": "That's not my fault. All\nright, so we're going to assume that phi is known. And so we saw that\nseveral distributions",
    "start": "125480",
    "end": "132021"
  },
  {
    "text": "that we know well, including\nthe Gaussian for example, belong to this family. And there's other\nones, such as Poisson--",
    "start": "132021",
    "end": "137319"
  },
  {
    "text": " Poisson and Bernoulli. So if the PMF has\nthis form, if you",
    "start": "137320",
    "end": "144040"
  },
  {
    "text": "have a discrete random\nvariable, this is also valid. And the reason why we\nintroduced this family",
    "start": "144040",
    "end": "149779"
  },
  {
    "text": "is because there are going to\nbe some properties that we know that this thing here,\nthis function, b of theta,",
    "start": "149779",
    "end": "154960"
  },
  {
    "text": "is essentially what\ncompletely characterizes your distribution. So if phi is fixed, we know that\nthe interaction is the form.",
    "start": "154960",
    "end": "162504"
  },
  {
    "text": "And this really just\ncomes from the fact that we want the function\nto integrate to one. So this b here in\nthe canonical form",
    "start": "162504",
    "end": "169180"
  },
  {
    "text": "encodes everything\nwe want to know. If I tell you what\nb of theta is-- and of course, I\ntell you what phi",
    "start": "169180",
    "end": "174580"
  },
  {
    "text": "is, but let's say for a second\nthat phi is equal to one. If I tell you this\nb of theta, you know exactly what distribution\nI'm talking about.",
    "start": "174580",
    "end": "180420"
  },
  {
    "text": "So it should encode\neverything that's specific to this distribution,\nsuch as mean, variance,",
    "start": "180420",
    "end": "185920"
  },
  {
    "text": "all the moments\nthat you would want. And we'll see how we can\ncompute from this thing",
    "start": "185920",
    "end": "192310"
  },
  {
    "text": "the mean and the\nvariance, for example. So today, we're going to\ntalk about likelihood, and we're going to start\nwith the likelihood",
    "start": "192310",
    "end": "198400"
  },
  {
    "text": "function or the log likelihood\nfor one observation. From this, we're going\nto do some computations,",
    "start": "198400",
    "end": "203440"
  },
  {
    "text": "and then, we'll move on to the\nactual log likelihood based on n independent observations.",
    "start": "203440",
    "end": "208750"
  },
  {
    "text": "And here, as we will\nsee, the observations are not going to be\nidentically distributed, because we're going\nto want each of them,",
    "start": "208750",
    "end": "215080"
  },
  {
    "text": "conditionally on x to be a\ndifferent function of x, where theta is just a\ndifferent function of x",
    "start": "215080",
    "end": "221200"
  },
  {
    "text": "for each of the observation. So remember, the\nlog likelihood-- ",
    "start": "221200",
    "end": "230050"
  },
  {
    "text": "and this is for\none observation-- is just the log of\nthe density, right? ",
    "start": "230050",
    "end": "239090"
  },
  {
    "text": "And we have this\nidentity that I mentioned at the end of the\nclass on Tuesday.",
    "start": "239090",
    "end": "244530"
  },
  {
    "text": "And this identity is\njust that the expectation of the derivative of this\nguy with respect to theta is equal to 0.",
    "start": "244530",
    "end": "250430"
  },
  {
    "text": "So let's see why. So if I take the derivative\nwith respect to theta, of log f,",
    "start": "250430",
    "end": "255610"
  },
  {
    "text": "theta of x, what I\nget is the derivative with respect to\ntheta of f, theta",
    "start": "255610",
    "end": "261930"
  },
  {
    "text": "of x, divided by f theta of x. Now, if I take the\nexpectation of this guy,",
    "start": "261930",
    "end": "270820"
  },
  {
    "text": "with respect to this\ntheta as well, what I get",
    "start": "270820",
    "end": "277810"
  },
  {
    "text": "is that this thing--\nwhat is the expectation? Well, it's just the\nintegral against f theta.",
    "start": "277810",
    "end": "282970"
  },
  {
    "text": "Or if I'm in a\ndiscrete case, I just have the sum against f\ntheta, if it's a pmf.",
    "start": "282970",
    "end": "288590"
  },
  {
    "text": "Just the definition,\nthe expectation of x, is either the integral--\nwell, let's say of h of x--",
    "start": "288590",
    "end": "296790"
  },
  {
    "text": "is integral of h of x. F theta of x-- if this is discrete\nor is just the sum",
    "start": "296790",
    "end": "304090"
  },
  {
    "text": "of h of x, f theta of x. If x is discrete--",
    "start": "304090",
    "end": "309880"
  },
  {
    "text": "so if it's continuous,\nyou put this soft sum.",
    "start": "309880",
    "end": "315115"
  },
  {
    "text": "This guy is the\nsame thing, right? So I'm just going to illustrate\nthe case when it's continuous.",
    "start": "315115",
    "end": "320449"
  },
  {
    "text": "So this is what? Well, this is the integral of\npartial derivative with respect to theta, of f theta of\nx, divided by f theta",
    "start": "320450",
    "end": "329740"
  },
  {
    "text": "of x, all time f theta of x--",
    "start": "329740",
    "end": "335060"
  },
  {
    "text": "dx. And now, this f\ntheta is canceled, so I'm actually left\nwith the integral",
    "start": "335060",
    "end": "340210"
  },
  {
    "text": "of the derivative,\nwhich I'm going to write as the derivative\nof the integral. ",
    "start": "340210",
    "end": "350690"
  },
  {
    "text": "But f theta being density\nfor any value of theta",
    "start": "350690",
    "end": "361640"
  },
  {
    "text": "that I can take,\nthis is the function. As a function of\ntheta, this function",
    "start": "361640",
    "end": "367720"
  },
  {
    "text": "is constantly equal to 1. For any theta that I take\nit, it takes value of 1.",
    "start": "367720",
    "end": "373710"
  },
  {
    "text": "So this is constantly\nequal to 1. I put three bars to see\nthat for any value of theta, this is 1, which actually\ntells me that the derivative is",
    "start": "373710",
    "end": "381830"
  },
  {
    "text": "equal to 0. OK, yes? ",
    "start": "381830",
    "end": "390455"
  },
  {
    "text": "AUDIENCE: What is\nthe first [INAUDIBLE] that you wrote on the board? ",
    "start": "390455",
    "end": "398666"
  },
  {
    "text": "PHILIPPE RIGOLLET: That's\njust the definition of the derivative of\nthe log of a function?",
    "start": "398666",
    "end": "404396"
  },
  {
    "text": "AUDIENCE: OK. ",
    "start": "404396",
    "end": "409720"
  },
  {
    "text": "PHILIPPE RIGOLLET: Log of\nf prime is f prime over f. That's a log, yeah.",
    "start": "409720",
    "end": "416060"
  },
  {
    "text": "Just by elimination. AUDIENCE: [INAUDIBLE]",
    "start": "416060",
    "end": "421651"
  },
  {
    "text": "PHILIPPE RIGOLLET: I'm sorry. AUDIENCE: When you write a\nsquiggle that starts with an l, I assume it's lambda.",
    "start": "421652",
    "end": "426680"
  },
  {
    "text": "PHILIPPE RIGOLLET: And you do\ngood, because that's probably how my mind processes. And so I'm like, yeah, l.",
    "start": "426680",
    "end": "433310"
  },
  {
    "text": "Here is enough information. OK, everybody is good with this?",
    "start": "433310",
    "end": "439290"
  },
  {
    "text": "So that was convenient. So it just said\nthat the expectation of the derivative of the log\nlikelihood is equal to 0.",
    "start": "439290",
    "end": "446969"
  },
  {
    "text": "That's going to be\nour first identity. Let's move onto the\nsecond identity, using exactly the same\ntrick, which is let's hope",
    "start": "446970",
    "end": "454139"
  },
  {
    "text": "that at some point,\nwe have the integral of this function\nthat's constantly equal to 1 as a function of\ntheta, and then use the fact",
    "start": "454140",
    "end": "461550"
  },
  {
    "text": "that its derivative\nis equal to 0. So if I start taking the\nsecond derivative of the log",
    "start": "461550",
    "end": "474850"
  },
  {
    "text": "of f theta, so what is this? Well, it's the\nderivative of this guy",
    "start": "474850",
    "end": "480600"
  },
  {
    "text": "here, so I'm going\nto go straight to it. So it's second derivative,\nf theta of x, times",
    "start": "480600",
    "end": "488830"
  },
  {
    "text": "f theta of x, minus first\nderivative of f theta of x,",
    "start": "488830",
    "end": "499810"
  },
  {
    "text": "times first derivative\nof f theta of x. ",
    "start": "499810",
    "end": "506360"
  },
  {
    "text": "Here is some super\nimportant stuff-- no, I'm kidding.",
    "start": "506360",
    "end": "511740"
  },
  {
    "text": "So you can still see\nthat guy over there? So it's just the square. And then, I divide by\nf theta of x squared.",
    "start": "511740",
    "end": "518099"
  },
  {
    "start": "518100",
    "end": "523889"
  },
  {
    "text": "So here I have the second\nderivative, times f itself. And here, I have the product\nof the first derivative",
    "start": "523890",
    "end": "531630"
  },
  {
    "text": "with itself. So that's the square. So now, I'm going to\nintegrate this guy. So if I take the expectation\nof this thing here, what I get",
    "start": "531630",
    "end": "541550"
  },
  {
    "text": "is the integral. So here, the only\nthing that's going to happen when I'm going\nto take my integral",
    "start": "541550",
    "end": "547073"
  },
  {
    "text": "is that one of those\nsquares is going to cancel against f theta, right? So I'm going to get\nthe second derivative",
    "start": "547073",
    "end": "562430"
  },
  {
    "text": "minus the second\nderivative squared. ",
    "start": "562430",
    "end": "572950"
  },
  {
    "text": "And then, I'm\ndivided by f theta. ",
    "start": "572950",
    "end": "578120"
  },
  {
    "text": "And I know that this\nthing is equal to 0. ",
    "start": "578120",
    "end": "584459"
  },
  {
    "text": "Now, one of these guys here-- sorry, why do I have-- so I have this guy here. So this guy here\nis going to cancel.",
    "start": "584460",
    "end": "590865"
  },
  {
    "text": " So this is what\nthis is equal to--",
    "start": "590865",
    "end": "597320"
  },
  {
    "text": " the integral of the partial,\nso the second derivative of f",
    "start": "597320",
    "end": "605595"
  },
  {
    "text": "theta of x, because\nthose two guys cancel-- minus the integral of\nthe second derivative.",
    "start": "605595",
    "end": "614156"
  },
  {
    "start": "614156",
    "end": "624380"
  },
  {
    "text": "And this is telling me what? ",
    "start": "624380",
    "end": "655480"
  },
  {
    "text": "Yeah, I'm losing one, because\nI have some weird sequences. Thank you. ",
    "start": "655480",
    "end": "663210"
  },
  {
    "text": "OK, this is still positive.",
    "start": "663210",
    "end": "672282"
  },
  {
    "text": "I want to say that this\nthing is actually equal to 0.  But then, it gives\nme some weird things,",
    "start": "672282",
    "end": "679410"
  },
  {
    "text": "which are that I\nhave an integral of a positive function,\nwhich is equal to 0.",
    "start": "679410",
    "end": "686080"
  },
  {
    "start": "686080",
    "end": "692814"
  },
  {
    "text": "Yeah, that's what I'm\nthinking of doing. But I'm going to get 0 for\nthis entire integral, which means that I have the integral\nof a positive function, which",
    "start": "692814",
    "end": "699729"
  },
  {
    "text": "is equal to 0, which means that\nthis function is equal to 0, which sounds a little bad--",
    "start": "699729",
    "end": "704940"
  },
  {
    "text": "basically, tells me that this\nfunction, f theta, is linear. ",
    "start": "704940",
    "end": "712710"
  },
  {
    "text": "So I went a little\ntoo far, I believe, because I only want to\nprove that the expectation of the second derivative--",
    "start": "712710",
    "end": "718937"
  },
  {
    "start": "718937",
    "end": "744190"
  },
  {
    "text": "Yes, so I want to pull this out. ",
    "start": "744190",
    "end": "751019"
  },
  {
    "text": "So let's see, if I keep rolling\nwith this, I'm going to get-- well, no because the fact\nthat it's divided by f theta,",
    "start": "751020",
    "end": "757520"
  },
  {
    "text": "means that, indeed, the second\nderivative is equal to 0. So it cannot do this here. ",
    "start": "757520",
    "end": "769446"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "769446",
    "end": "779920"
  },
  {
    "text": "PHILIPPE RIGOLLET: OK, but\nlet's write it like this. You're right, so this is what?",
    "start": "779920",
    "end": "785020"
  },
  {
    "text": "This is the expectation of\nthe partial with respect",
    "start": "785020",
    "end": "792200"
  },
  {
    "text": "to theta of f\ntheta of x, divided by f theta of x squared.",
    "start": "792200",
    "end": "801360"
  },
  {
    "text": "And this is exactly the\nderivative of the log, right? So indeed, this thing is equal\nto the expectation with respect",
    "start": "801360",
    "end": "808830"
  },
  {
    "text": "to theta of the partial of l--",
    "start": "808830",
    "end": "814980"
  },
  {
    "text": "of log f theta, divided\nby partial theta.",
    "start": "814980",
    "end": "821160"
  },
  {
    "text": "All right, so this is one of\nthe guys that I want squared. This is one of the\nguys that I want.",
    "start": "821160",
    "end": "827509"
  },
  {
    "text": "And this is actually\nequal, so this will be equal to the expectation--",
    "start": "827510",
    "end": "833270"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] ",
    "start": "833270",
    "end": "839110"
  },
  {
    "text": "PHILIPPE RIGOLLET: Oh, right, so\nthis term should be equal to 0. This was not 0. You're absolutely right.",
    "start": "839110",
    "end": "844990"
  },
  {
    "text": "So at some point,\nI got confused, because I thought\nputting this equal to 0 would mean that this is 0. But this thing is\nnot equal to 0.",
    "start": "844990",
    "end": "850840"
  },
  {
    "text": "So this thing, you're right. I take the same trick as\nbefore, and this is actually equal to 0, which\nmeans that now I have",
    "start": "850840",
    "end": "856899"
  },
  {
    "text": "what's on the left-hand side,\nwhich is equal to what's on the right-hand side. And if I recap, I\nget that e theta",
    "start": "856900",
    "end": "863220"
  },
  {
    "text": "of the second derivative\nof the log of f theta",
    "start": "863220",
    "end": "870750"
  },
  {
    "text": "is equal to minus-- because I had a\nminus sign here-- to the expectation with respect\nto theta, of log of f theta,",
    "start": "870750",
    "end": "879300"
  },
  {
    "text": "divided by theta squared. Thank you for being on watch\nwhen I'm falling apart.",
    "start": "879300",
    "end": "888720"
  },
  {
    "text": "All right, so this\nis exactly what you have here, except\nthat both terms have been put on the same side.",
    "start": "888720",
    "end": "894180"
  },
  {
    "text": "All right, so those things\nare going to be useful to us, so maybe, we should write\nthem somewhere here.",
    "start": "894180",
    "end": "899880"
  },
  {
    "start": "899880",
    "end": "913820"
  },
  {
    "text": "And then, we have\nthat the expectation of the second\nderivative of the log",
    "start": "913820",
    "end": "926090"
  },
  {
    "text": "is equal to minus the\nexpectation of the square",
    "start": "926090",
    "end": "933170"
  },
  {
    "text": "of the first derivative. ",
    "start": "933170",
    "end": "940019"
  },
  {
    "text": "And this is, indeed,\nmy Fisher information. This is just telling me what is\nthe second derivative of my log",
    "start": "940020",
    "end": "948030"
  },
  {
    "text": "likelihood at theta, right? So everything is\nwith respect to theta when I take these expectations. And so it tells me\nthat the expectation",
    "start": "948030",
    "end": "955140"
  },
  {
    "text": "of the second derivative--\nat least first of all, what it's telling me is\nthat it's concave,",
    "start": "955140",
    "end": "960150"
  },
  {
    "text": "because the second\nderivative of this thing, which is the second\nderivative of kl divergence,",
    "start": "960150",
    "end": "965910"
  },
  {
    "text": "is actually minus something\nwhich is must be non-negative. And so it's telling me\nthat it's concave here",
    "start": "965910",
    "end": "971310"
  },
  {
    "text": "at this [INAUDIBLE]. And in particular,\nit's also telling me that it has to be strictly\npositive, unless the derivative",
    "start": "971310",
    "end": "979240"
  },
  {
    "text": "of f is equal to 0. Unless f is constant, then\nit's not going to change.",
    "start": "979240",
    "end": "987699"
  },
  {
    "text": "All right, do you\nhave a question?",
    "start": "987700",
    "end": "992920"
  },
  {
    "text": "So now, let's use this. So what does my\nlog likelihood look like when I actually compute it\nfor this canonical exponential",
    "start": "992920",
    "end": "1001389"
  },
  {
    "text": "family. We have this exponential\nfunction, so taking the log should make my life much\neasier, and indeed, it does.",
    "start": "1001390",
    "end": "1008260"
  },
  {
    "text": "So if I look at the\ncanonical, what I have",
    "start": "1008260",
    "end": "1016250"
  },
  {
    "text": "is that the log of f\ntheta of x, it's equal",
    "start": "1016250",
    "end": "1024339"
  },
  {
    "text": "simply to y theta minus b\nof theta, divided by phi,",
    "start": "1024339",
    "end": "1030849"
  },
  {
    "text": "plus this function that\ndoes not depend on theta. ",
    "start": "1030849",
    "end": "1038790"
  },
  {
    "text": "So let's see what this tells me. Let's just plug-in those\nequalities in there. I can take the derivative\nof the right-hand side",
    "start": "1038790",
    "end": "1045329"
  },
  {
    "text": "and just say that in\nexpectation, it's equal to 0. So if I start looking\nat the derivative,",
    "start": "1045329",
    "end": "1052300"
  },
  {
    "text": "this is equal to what? Well, here I'm going\nto pick up only y.",
    "start": "1052300",
    "end": "1057820"
  },
  {
    "text": "Sorry, this is a function of y. ",
    "start": "1057820",
    "end": "1066585"
  },
  {
    "text": "I was talking about\nlikelihood, so I actually need to put the\nrandom variable here. So I get y minus the\nderivative of b of theta.",
    "start": "1066585",
    "end": "1074750"
  },
  {
    "text": "Since it's only a\nfunction of theta, I'm just going to write\nb prime, is at OK-- rather than having the\npartial with respect to theta.",
    "start": "1074750",
    "end": "1081450"
  },
  {
    "text": "Then, this is a constant. This does not depend on\ntheta, so it goes away. ",
    "start": "1081450",
    "end": "1090200"
  },
  {
    "text": "So if I start taking the\nexpectation of this guy,",
    "start": "1090200",
    "end": "1095970"
  },
  {
    "text": "I get the expectation\nof this guy, which is the expectation\nof y, minus-- well,",
    "start": "1095970",
    "end": "1104960"
  },
  {
    "text": "this does not depend on\ny, so it's just itself-- b prime of theta. And the whole thing\nis divided by phi.",
    "start": "1104960",
    "end": "1110460"
  },
  {
    "text": "But from my first\nequality over there, I know that this thing\nis actually equal to 0. ",
    "start": "1110460",
    "end": "1118680"
  },
  {
    "text": "We just proved that. So in particular, it means\nthat since phi is non-zero, it means that this guy\nmust be equal to this guy--",
    "start": "1118680",
    "end": "1125550"
  },
  {
    "text": "or phi is not infinity. And so that implies\nthat the expectation with respect to theta of y\nis equal to b prime of theta.",
    "start": "1125550",
    "end": "1136310"
  },
  {
    "start": "1136310",
    "end": "1142322"
  },
  {
    "text": "I'm sorry, you're not\nregistered in this class. I'm going to have\nto ask you to leave. I'm not kidding.",
    "start": "1142322",
    "end": "1149150"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: You are? I've never seen you here. I saw you for the first lecture.",
    "start": "1149150",
    "end": "1155861"
  },
  {
    "text": "OK.  All right, so e theta of y\nis equal to b prime of theta.",
    "start": "1155861",
    "end": "1163105"
  },
  {
    "text": "Everybody agrees with that?  So this is actually nice,\nbecause if I give you",
    "start": "1163105",
    "end": "1171130"
  },
  {
    "text": "an exponential family, the only\nthing I really need to tell you is what b theta is.",
    "start": "1171130",
    "end": "1176210"
  },
  {
    "text": "And if I give you b of theta,\nthen computing a derivative is actually much easier\nthan having to integrate y",
    "start": "1176210",
    "end": "1182470"
  },
  {
    "text": "against the density itself. You could really\nhave fun and try to compute this, which you\nwould be able to do, right?",
    "start": "1182470",
    "end": "1188310"
  },
  {
    "start": "1188310",
    "end": "1194080"
  },
  {
    "text": "And then, there's the plus c\nof y phi, blah, blah, blah-- dy.",
    "start": "1194080",
    "end": "1199385"
  },
  {
    "text": "And that's the way you would\nactually compute this thing. ",
    "start": "1199385",
    "end": "1205040"
  },
  {
    "text": "Sorry, this guy is here. That would be painful. I don't know what this\nnormalization looks like,",
    "start": "1205040",
    "end": "1210309"
  },
  {
    "text": "so it would have to\nalso explicit that, so I can actually\ncompute this thing. And you know, just\nthe same way, if you",
    "start": "1210310",
    "end": "1215415"
  },
  {
    "text": "want to compute the\nexpectation of a Gaussian-- well, the expectation\nof a Gaussian is not the most difficult one.",
    "start": "1215415",
    "end": "1220750"
  },
  {
    "text": "But even if you compute the\nexpectation of a Poisson, you start to have to\nwork in a little bit. There's a few things that\nyou have to work through.",
    "start": "1220750",
    "end": "1227200"
  },
  {
    "text": "Here, I'm just telling\nyou, all you have to know is what b of theta\nis, and then, you can just take the derivative.",
    "start": "1227200",
    "end": "1233190"
  },
  {
    "text": "Let's see what the second\nequality is going to give us. ",
    "start": "1233190",
    "end": "1256490"
  },
  {
    "text": "OK, so what is the\nsecond equality? It's telling me that if I\nlook at the second derivative,",
    "start": "1256490",
    "end": "1263850"
  },
  {
    "text": "and then I take its\nexpectation, I'm going to have something which\nis equal to negative this guy",
    "start": "1263850",
    "end": "1271190"
  },
  {
    "text": "squared. Sorry, that was the log, right? ",
    "start": "1271190",
    "end": "1279970"
  },
  {
    "text": "We've already computed\nthis first derivative of the likelihood. It's just the expectation of\nthe square of this thing here.",
    "start": "1279970",
    "end": "1289390"
  },
  {
    "text": "So expectation of\nthe derivative, with respect to theta of\nlog, f theta of x, divided",
    "start": "1289390",
    "end": "1298899"
  },
  {
    "text": "by partial theta squared.",
    "start": "1298900",
    "end": "1304130"
  },
  {
    "text": "This is equal to the\nexpectation of the square of y,",
    "start": "1304130",
    "end": "1310040"
  },
  {
    "text": "minus b theta, divided\nby phi squared--",
    "start": "1310040",
    "end": "1318580"
  },
  {
    "text": "b prime, theta squared. ",
    "start": "1318580",
    "end": "1324375"
  },
  {
    "text": "OK, sorry, I'm actually\ngoing to move on with the-- ",
    "start": "1324375",
    "end": "1331100"
  },
  {
    "text": "so if I start computing,\nwhat is this thing? Well, we just agreed\nthat this was what?",
    "start": "1331100",
    "end": "1336350"
  },
  {
    "text": " The expectation of theta, right?",
    "start": "1336350",
    "end": "1342919"
  },
  {
    "text": "So that's just the\nexpectation of y. We just computed it here.",
    "start": "1342920",
    "end": "1348230"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] ",
    "start": "1348230",
    "end": "1355744"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYeah, that's b prime. There's a derivative here. ",
    "start": "1355744",
    "end": "1364760"
  },
  {
    "text": "So now, this is what? This is simply-- anyone?",
    "start": "1364760",
    "end": "1377680"
  },
  {
    "text": " I'm sorry?",
    "start": "1377680",
    "end": "1382809"
  },
  {
    "text": "Variance of y, but you're\nscaling by phi squared. ",
    "start": "1382810",
    "end": "1391040"
  },
  {
    "text": "OK, so this is negative\nof the right-hand side of our inequality.",
    "start": "1391040",
    "end": "1397250"
  },
  {
    "text": "And now, I just have to take\none more derivative to this guy. So now, if I look at\nthe left-hand side now,",
    "start": "1397250",
    "end": "1407420"
  },
  {
    "text": "I have that the\nsecond derivative of log, of f theta of y, divided\nby partial of theta squared.",
    "start": "1407420",
    "end": "1418890"
  },
  {
    "text": "So this thing is equal to-- well, no, I'm not\nleft with much. The white part is\ngoing to go away,",
    "start": "1418890",
    "end": "1424630"
  },
  {
    "text": "and I'm left only with the\nsecond derivative of theta, minus the second derivative\ntheta, divided by phi.",
    "start": "1424630",
    "end": "1429929"
  },
  {
    "start": "1429930",
    "end": "1440540"
  },
  {
    "text": "So if I take expectation-- well, it just doesn't change. ",
    "start": "1440540",
    "end": "1448010"
  },
  {
    "text": "This is deterministic. So now, what I've\nestablished is that this guy is equal to negative this guy.",
    "start": "1448010",
    "end": "1454010"
  },
  {
    "text": "So those two things, the\nsigns are going to go away. And so this implies\nthat the variance of y",
    "start": "1454010",
    "end": "1460910"
  },
  {
    "text": "is equal to b prime prime theta. And then, I have a phi\nsquare in denominator",
    "start": "1460910",
    "end": "1470240"
  },
  {
    "text": "that cancels only one of the\nphi squares, so it's time phi. ",
    "start": "1470240",
    "end": "1477480"
  },
  {
    "text": "So now, I have that my second\nderivative-- since I know phi is completely\ndetermining the variance.",
    "start": "1477480",
    "end": "1486280"
  },
  {
    "text": "So basically, that's why b is\ncalled the cumulant generating",
    "start": "1486280",
    "end": "1492470"
  },
  {
    "text": "function. It's not generating\nmoments, but cumulants. But cumulants, in this\ncase, correspond, basically,",
    "start": "1492470",
    "end": "1499180"
  },
  {
    "text": "to the moments, at\nleast for the first two. If I start going\nfarther, I'm going to have more combinations of\nthe expectation of y3, y2,",
    "start": "1499180",
    "end": "1508090"
  },
  {
    "text": "and y itself. ",
    "start": "1508090",
    "end": "1513150"
  },
  {
    "text": "But as we know,\nthose are the ones that are usually the\nmost useful, at least if we're interested in\nasymptotic performance.",
    "start": "1513150",
    "end": "1519384"
  },
  {
    "text": "The central limit\ntheorem tells us that all that matters are\nthe first two moments, and then, the rest is just\ngoing to go and say well,",
    "start": "1519384",
    "end": "1525580"
  },
  {
    "text": "it doesn't matter. It's all going to\n[INAUDIBLE] anyway. So let's go to a\nPoisson for example.",
    "start": "1525580",
    "end": "1531290"
  },
  {
    "text": "So if I had a Poisson\ndistribution-- ",
    "start": "1531290",
    "end": "1539909"
  },
  {
    "text": "so this is a discrete\ndistribution. And what I know is that f--",
    "start": "1539910",
    "end": "1546390"
  },
  {
    "text": "let me call mu the\nparameter of y.",
    "start": "1546390",
    "end": "1551580"
  },
  {
    "text": " So it's mu to the y, divided\nby y factorial, exponential",
    "start": "1551580",
    "end": "1561870"
  },
  {
    "text": "minus mu. OK so mu is usually\ncalled lambda, and y is usually\ncalled x, that's why it takes me to a\nlittle bit of time.",
    "start": "1561870",
    "end": "1567960"
  },
  {
    "text": "But it usually it's\nlambda to the x over factorial x, exponential\nminus lambda.",
    "start": "1567960",
    "end": "1573809"
  },
  {
    "text": "Since this is just the series\nexpansion of the exponential when I sum those things\nfrom 0 to infinity,",
    "start": "1573810",
    "end": "1579230"
  },
  {
    "text": "this thing sums to 1. But then, if I wanted to\nstart understanding what the expectation\nof this thing is--",
    "start": "1579230",
    "end": "1585900"
  },
  {
    "text": "so if I want to understand\nthe expectation with respect to mu of y, then, I would\nhave to compute the sum",
    "start": "1585900",
    "end": "1593820"
  },
  {
    "text": "from k equals 0 to infinity\nof k, times mu to the k,",
    "start": "1593820",
    "end": "1608279"
  },
  {
    "text": "over factorial of k,\nexponential minus mu-- which means that I\nwould, essentially, have to take the derivative\nof my series in the end.",
    "start": "1608280",
    "end": "1626090"
  },
  {
    "text": "So I can do this. This is a standard exercise. You've probably done it\nwhen you took probability. But let's see if we can\nactually just read it off",
    "start": "1626090",
    "end": "1632900"
  },
  {
    "text": "from the first derivative of b. So to do that, we\nneed to write this in the form of an\nexponential, where there",
    "start": "1632900",
    "end": "1638850"
  },
  {
    "text": "is one parameter that captures\nmu, that interacts with y, just doing this parameter times\ny, and then something that",
    "start": "1638850",
    "end": "1645860"
  },
  {
    "text": "depends only on y, and then\nsomething that depends only on mu.",
    "start": "1645860",
    "end": "1652979"
  },
  {
    "text": "That's the important one. That's going to be\nour B. And then, there's going to be something\nthat depends only on y.",
    "start": "1652979",
    "end": "1659150"
  },
  {
    "text": "So let's write this and\ncheck that this f mu, indeed, belongs to this canonical\nexponential family.",
    "start": "1659150",
    "end": "1666510"
  },
  {
    "text": "So I definitely\nhave an exponential that comes from this guy. So I have minus mu. And then, this thing is\ngoing to give me what?",
    "start": "1666510",
    "end": "1673370"
  },
  {
    "text": "It's going to give\nme plus y log mu. And then, I'm going to have\nminus log of y factorial.",
    "start": "1673370",
    "end": "1682166"
  },
  {
    "text": " So clearly, I have a\nterm that depends only",
    "start": "1682166",
    "end": "1688790"
  },
  {
    "text": "on mu, terms that\ndepend only on y, and I have a product of y and\nsomething that depends on mu.",
    "start": "1688790",
    "end": "1695299"
  },
  {
    "text": "If I want to be\ncanonical, I must have this to be exactly\nthe parameter theta itself.",
    "start": "1695300",
    "end": "1703649"
  },
  {
    "text": "So I'm going to\ncall this guy theta. So theta is log mu,\nwhich means that mu",
    "start": "1703650",
    "end": "1710750"
  },
  {
    "text": "is equal to e to the theta. And so wherever I\nsee mu, I'm going to replace it by [INAUDIBLE] the\ntheta, because my new parameter",
    "start": "1710750",
    "end": "1716716"
  },
  {
    "text": "now, is theta. So this is what? This is equal to\nexponential y times theta.",
    "start": "1716716",
    "end": "1723490"
  },
  {
    "text": "And then, I'm going to\nhave minus e of theta. And then, who cares, something\nthat depends only on mu.",
    "start": "1723490",
    "end": "1731630"
  },
  {
    "text": "So this is my c of y, and phi\nis equal to 1 in this case.",
    "start": "1731630",
    "end": "1738330"
  },
  {
    "text": "So that's all I care about. So let's use it. ",
    "start": "1738330",
    "end": "1745000"
  },
  {
    "text": "So this is my canonical\nexponential family. Y interacts with theta\nexactly like this.",
    "start": "1745000",
    "end": "1751680"
  },
  {
    "text": "And then, I have this function. So this function here\nmust be b of theta.",
    "start": "1751680",
    "end": "1757410"
  },
  {
    "text": " So from this function,\nexponential theta,",
    "start": "1757410",
    "end": "1762779"
  },
  {
    "text": "I'm supposed to be able\nto read what the mean is. ",
    "start": "1762780",
    "end": "1779820"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: Because\nsince in this course",
    "start": "1779820",
    "end": "1786990"
  },
  {
    "text": "I always know what\nthe dispersion is, I can actually always absorb\nit into theta from one.",
    "start": "1786990",
    "end": "1792450"
  },
  {
    "text": "But here, it's really of\nthe form y times something divided by 1, right? ",
    "start": "1792450",
    "end": "1801030"
  },
  {
    "text": "If it was like log\nof mu divided by phi, that would be the\nquestion of whether I",
    "start": "1801030",
    "end": "1806430"
  },
  {
    "text": "want to call phi my\ndispersion, or if I want to just have it in there.",
    "start": "1806430",
    "end": "1812070"
  },
  {
    "text": " This makes no\ndifference in practice.",
    "start": "1812070",
    "end": "1818740"
  },
  {
    "text": "But the real thing\nis it's never going to happen that this\nthing, this version, is going to be an exact number.",
    "start": "1818740",
    "end": "1823960"
  },
  {
    "text": "If it's an actual\nnumerical number, this just means that this\nnumber should be absorbed in the definition of theta.",
    "start": "1823960",
    "end": "1832120"
  },
  {
    "text": "But if it's something\nthat is called sigma, say, and I will assume\nthat sigma is known, then it's probably preferable\nto keep it in the dispersion,",
    "start": "1832120",
    "end": "1839162"
  },
  {
    "text": "so you can see that\nthere's this parameter here that you can,\nessentially, play with.",
    "start": "1839162",
    "end": "1844450"
  },
  {
    "text": "It doesn't make any\ndifference when you know phi. So now, if I look at the\nexpectation of some y-- so now,",
    "start": "1844450",
    "end": "1855050"
  },
  {
    "text": "I'm going to have y, which\nfollows my Poisson mu.",
    "start": "1855050",
    "end": "1860419"
  },
  {
    "text": "I'm going to look\nat the expectation, and I know that the expectation\nis b prime of theta.",
    "start": "1860419",
    "end": "1869210"
  },
  {
    "text": "Agreed? That's what I just\nerased, I think.",
    "start": "1869210",
    "end": "1874779"
  },
  {
    "text": "Agreed with this,\nthe derivative? So what is this? Well, it's the derivative\nof e to the theta, which",
    "start": "1874780",
    "end": "1883050"
  },
  {
    "text": "is e to the theta, which is mu. So my Poisson is\nparametrized by its mean.",
    "start": "1883050",
    "end": "1890030"
  },
  {
    "text": "I can also compute\nthe variance, which is equal to minus the\nsecond derivative of--",
    "start": "1890030",
    "end": "1900580"
  },
  {
    "text": "no, it's equal to the\nsecond derivative of b. ",
    "start": "1900580",
    "end": "1907169"
  },
  {
    "text": "Dispersion is equal to 1. Again, if I took phi elsewhere,\nI would see it here as well.",
    "start": "1907170",
    "end": "1915000"
  },
  {
    "text": "So if I just absorbed phi here,\nI would see it divided here, so it would not\nmake any difference.",
    "start": "1915000",
    "end": "1920040"
  },
  {
    "text": "And what is the second\nderivative of the exponential? ",
    "start": "1920040",
    "end": "1926570"
  },
  {
    "text": "Still the exponential--\nso it's still equal to mu. ",
    "start": "1926570",
    "end": "1934759"
  },
  {
    "text": "So that certainly\nmakes our life easier. Just one quick from remark-- ",
    "start": "1934760",
    "end": "1951130"
  },
  {
    "text": "here's the function. I am giving you problem-- can the b function--",
    "start": "1951130",
    "end": "1956710"
  },
  {
    "text": " can it ever be equal\nto log of theta?",
    "start": "1956710",
    "end": "1966549"
  },
  {
    "start": "1966550",
    "end": "1975840"
  },
  {
    "text": "Who says yes? Who says no? Why?",
    "start": "1975840",
    "end": "1982858"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "1982858",
    "end": "1989680"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah, so\nwhat I've learned from this-- it's sort of completely\nanalytic, right?",
    "start": "1989680",
    "end": "1996610"
  },
  {
    "text": "So we just took derivatives,\nand this thing just happened. This thing actually allowed us\nto relate the second derivative",
    "start": "1996610",
    "end": "2002490"
  },
  {
    "text": "of b to the variance. And one thing that we\nknow about a variance is that this is non-negative.",
    "start": "2002490",
    "end": "2007919"
  },
  {
    "text": "And in particular,\nit's always positive. If they give you a canonical\nexponential family that",
    "start": "2007920",
    "end": "2015330"
  },
  {
    "text": "has zero variance, trust\nme, you will see it. That means that\nthis thing is not",
    "start": "2015330",
    "end": "2020919"
  },
  {
    "text": "going to look like\nsomething that's finite, and it's going to\nhave a point mass. It's going to take value\ninfinity at one point.",
    "start": "2020919",
    "end": "2026280"
  },
  {
    "text": "So this will,\nbasically, never happen. This thing is, actually,\nstrictly positive, which means that this thing\nis always strictly concave.",
    "start": "2026280",
    "end": "2032600"
  },
  {
    "text": "It means that the second\nderivative of this function, b, has to be strictly positive, and\nso that the function is convex.",
    "start": "2032600",
    "end": "2040440"
  },
  {
    "text": "So this is concave, so this\nis definitely not working. I need to have something\nthat looks like this when I talk about my b.",
    "start": "2040440",
    "end": "2047920"
  },
  {
    "text": "So f theta squared-- we'll see a bunch of\nexponential theta.",
    "start": "2047920",
    "end": "2053190"
  },
  {
    "text": "And there's a bunch of them. But if you started writing\nsomething, and you find b--",
    "start": "2053190",
    "end": "2058280"
  },
  {
    "text": "try to think of the\nplot of b in your mind, and you find that b looks like\nit's going to become concave,",
    "start": "2058280",
    "end": "2063980"
  },
  {
    "text": "you've made a sign\nmistake somewhere. ",
    "start": "2063980",
    "end": "2070110"
  },
  {
    "text": "All right, so we've done\na pretty big parenthesis to try to characterize\nwhat the distribution of y",
    "start": "2070110",
    "end": "2077040"
  },
  {
    "text": "was going to be. We wanted to extend from, say,\nGaussian to something else. But when we're doing\nregression, which",
    "start": "2077040",
    "end": "2083908"
  },
  {
    "text": "means generalized\nlinear models, we are not interested in\nthe distribution of y but really the conditional\ndistribution of y given x.",
    "start": "2083909",
    "end": "2091649"
  },
  {
    "text": "So I need now to couple\nthose back together. So what I know is that\nthis same mu, in this case,",
    "start": "2091650",
    "end": "2099702"
  },
  {
    "text": "which is the expectation--\nwhat I want to say is that the conditional\nexpectation of y given x--",
    "start": "2099702",
    "end": "2109740"
  },
  {
    "text": " this is some mu of x.",
    "start": "2109740",
    "end": "2115869"
  },
  {
    "text": "When we did linear\nmodels, we said well, this thing was some x transpose\nbeta for linear models.",
    "start": "2115870",
    "end": "2121869"
  },
  {
    "text": " And the whole premise\nof this chapter",
    "start": "2121870",
    "end": "2127680"
  },
  {
    "text": "is to say well, this\nmight make no sense, because x transpose beta\ncan take the entire range",
    "start": "2127680",
    "end": "2132930"
  },
  {
    "text": "of real values. Whereas, this mu can take\nonly a partial range. So even if you actually focus\non the Poisson, for example,",
    "start": "2132930",
    "end": "2140550"
  },
  {
    "text": "we know that the expectation\nof a Poisson has to be a non-negative number--",
    "start": "2140550",
    "end": "2145910"
  },
  {
    "text": "actually, a positive\nnumber as soon as you have a little bit of variance. It's mu itself-- mu\nis a positive number.",
    "start": "2145910",
    "end": "2152590"
  },
  {
    "text": "And so it's not going\nto make any sense to assume that mu of x is\nequal to x transpose beta,",
    "start": "2152590",
    "end": "2157710"
  },
  {
    "text": "because you might find some x's\nfor which this value ends up being negative. And so we're going\nto need, what we",
    "start": "2157710",
    "end": "2163760"
  },
  {
    "text": "call, the link\nfunction that relates, that transforms mu,\nmaps onto the real line, so that you can now express it\nof the form x transpose beta.",
    "start": "2163760",
    "end": "2173210"
  },
  {
    "text": "So we're going to take\nnot this, but we're going to assume\nthat g of mu of x",
    "start": "2173210",
    "end": "2181250"
  },
  {
    "text": "is not equal to\nx transpose beta, and that's the\ngeneralized linear models. ",
    "start": "2181250",
    "end": "2193220"
  },
  {
    "text": "So as I said, it's weird to\ntransform x transpose beta--",
    "start": "2193220",
    "end": "2200650"
  },
  {
    "text": "a mu to make it\ntake the real line. At least to me, it\nfeels a bit more natural to take x\ntranspose beta and make",
    "start": "2200650",
    "end": "2207530"
  },
  {
    "text": "it fit to the particular\ndistribution that I want. And so I'm going to want to\ntalk about g and g inverse",
    "start": "2207530",
    "end": "2213890"
  },
  {
    "text": "at the same time. So I'm going to\nactually take always g.",
    "start": "2213890",
    "end": "2219070"
  },
  {
    "text": "So g is my link\nfunction, and I'm",
    "start": "2219070",
    "end": "2224920"
  },
  {
    "text": "going to want g to be\ncontinuous differentiable.",
    "start": "2224920",
    "end": "2230036"
  },
  {
    "start": "2230036",
    "end": "2236980"
  },
  {
    "text": "OK, let's say that\nit has a derivative, and its derivative\nis continuous.",
    "start": "2236980",
    "end": "2242630"
  },
  {
    "text": "And I'm going to want g\nto be strictly increasing.",
    "start": "2242630",
    "end": "2248398"
  },
  {
    "start": "2248398",
    "end": "2254770"
  },
  {
    "text": "And that actually implies\nthat g inverse exists.",
    "start": "2254770",
    "end": "2259930"
  },
  {
    "text": "Actually, that's not true. What I'm also going to want\nis that g of mu spans--",
    "start": "2259930",
    "end": "2274505"
  },
  {
    "text": " how do I do this? ",
    "start": "2274505",
    "end": "2286089"
  },
  {
    "text": "So I want the g, as I arrange\nfor all possible values of mu, whether they're all\npositive values,",
    "start": "2286090",
    "end": "2291220"
  },
  {
    "text": "or whether they're\nvalues that are limited between the intervals 0,\n1, I want those to span the entire real line, so that\nwhen I want to talk about g",
    "start": "2291220",
    "end": "2298340"
  },
  {
    "text": "inverses define over\nthe entire real line, I know where I started. ",
    "start": "2298340",
    "end": "2304396"
  },
  {
    "text": "So this implies that\ngene inverse exists. ",
    "start": "2304396",
    "end": "2310200"
  },
  {
    "text": "What else does it\nimply about g inverse? ",
    "start": "2310200",
    "end": "2319500"
  },
  {
    "text": "So for a function\nto be invertible, I only need for it to\nbe strictly monotone. I don't need it to be\nstrictly increasing.",
    "start": "2319500",
    "end": "2325605"
  },
  {
    "text": "So in particular, the fact\nthat I picked increasing implies that this guy\nis actually increasing.",
    "start": "2325605",
    "end": "2333360"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nThat's the image. ",
    "start": "2333360",
    "end": "2343470"
  },
  {
    "text": "So this is my link function, and\nthis slide is just telling me I want my function\nto be invertible, so I can talk about g inverse.",
    "start": "2343470",
    "end": "2349890"
  },
  {
    "text": "I'm going to switch\nbetween the two. So what link functions\nam I going to get?",
    "start": "2349890",
    "end": "2355450"
  },
  {
    "text": "So for linear\nmodels, we just said there's no link\nfunction, which is the same as saying that the\nlink function is identity,",
    "start": "2355450",
    "end": "2360962"
  },
  {
    "text": "which certainly satisfies\nall these conditions. It's invertible. It has all these\nnice properties, but might as well\nnot talk about it.",
    "start": "2360962",
    "end": "2367540"
  },
  {
    "text": "For Poisson data,\nwhen we assume that the conditional distribution\nof y given x is Poisson, the mu, as I just said, is\nrequired to be positive.",
    "start": "2367540",
    "end": "2377200"
  },
  {
    "text": "So I need a g that goes\nfrom the interval 0 infinity",
    "start": "2377200",
    "end": "2383650"
  },
  {
    "text": "to the entire real line. I need a function that\nstarts from one end and just takes-- not\nonly the positive values",
    "start": "2383650",
    "end": "2391720"
  },
  {
    "text": "are split between positive\nand negative values. And here, for example, I\ncould take the log link.",
    "start": "2391720",
    "end": "2396820"
  },
  {
    "text": "So the log is defined\non this entire interval. And as I range from\n0 to plus infinity,",
    "start": "2396820",
    "end": "2404050"
  },
  {
    "text": "the log is ranging from negative\ninfinity to plus infinity. ",
    "start": "2404050",
    "end": "2410381"
  },
  {
    "text": "You can probably think\nof other functions that do that, like 2 times log.",
    "start": "2410382",
    "end": "2415510"
  },
  {
    "text": "That's another one. But there's many others\nyou can think of. But let's say the\nlog is one of them",
    "start": "2415510",
    "end": "2421752"
  },
  {
    "text": "that you might want\nto think about. ",
    "start": "2421752",
    "end": "2432680"
  },
  {
    "text": "It is unnatural in\nthe sense that it's one of the first\nfunction we can think of. We will see, also, that it has\nanother canonical property that",
    "start": "2432680",
    "end": "2439839"
  },
  {
    "text": "makes it a natural choice. The other one is\nthe other example, where we had an even stronger\ncondition on what mu could be.",
    "start": "2439840",
    "end": "2447519"
  },
  {
    "text": "Mu could only be a\nnumber between 0 and 1, that was the probability\nof success of a coin flip--",
    "start": "2447520",
    "end": "2452780"
  },
  {
    "text": "probability of success of a\nBernoulli random variable. And now, I need g to map 0,\n1 to the entire real line.",
    "start": "2452780",
    "end": "2459310"
  },
  {
    "text": "And so here are\na bunch of things that you can come up\nwith, because now you",
    "start": "2459310",
    "end": "2464980"
  },
  {
    "text": "start to have maybe-- I will soon claim that\nthis one, log of mu,",
    "start": "2464980",
    "end": "2471340"
  },
  {
    "text": "divided by 1 minus mu\nis the most natural one. But maybe, if you had\nnever thought of this,",
    "start": "2471340",
    "end": "2476770"
  },
  {
    "text": "that might not be\nthe first function you would come up with, right? You mentioned trigonometric\nfunctions, for example,",
    "start": "2476770",
    "end": "2483670"
  },
  {
    "text": "so maybe, you can\ncome up with something that comes from hyperbolic\ntrigonometry or something.",
    "start": "2483670",
    "end": "2490960"
  },
  {
    "text": "So what does this function do? Well, we'll see a picture,\nbut this function does map the interval 0, 1\nto the entire real line.",
    "start": "2490960",
    "end": "2496990"
  },
  {
    "text": "We also discuss the fact that\nif we think reciprocally-- ",
    "start": "2496990",
    "end": "2503740"
  },
  {
    "text": "what I want if I want to\nthink about g inverse, I want a function that maps the\nentire real line into the unit",
    "start": "2503740",
    "end": "2509140"
  },
  {
    "text": "interval. And as we said, if I'm not\na very creative statistician or probabilist, I can just\npick my favorite continuous,",
    "start": "2509140",
    "end": "2515960"
  },
  {
    "text": "strictly increasing cumulative\ndistribution function, which as we know,\nwill arise as soon",
    "start": "2515960",
    "end": "2521349"
  },
  {
    "text": "as I have a density\nthat has support on the entire real line. If I have support everywhere,\nthen it means that my--",
    "start": "2521350",
    "end": "2527350"
  },
  {
    "text": " it is strictly positive\neverywhere, then,",
    "start": "2527350",
    "end": "2534140"
  },
  {
    "text": "it means that my community\ndistribution function has to be strictly increasing. And of course, it has to go\nfrom 0 to 1, because that's just",
    "start": "2534140",
    "end": "2541450"
  },
  {
    "text": "the nature of those things. And so for example, I\ncan take the Gaussian, that's one such function. But I could also take\nthe double exponential",
    "start": "2541450",
    "end": "2548591"
  },
  {
    "text": "that looks like an\nexponential on one end, and then an exponential\non the other end. And basically, if you\ntake capital phi, which",
    "start": "2548591",
    "end": "2559930"
  },
  {
    "text": "is the standard Gaussian\ncumulative distribution function, it does work for you,\nand you can take its inverse.",
    "start": "2559930",
    "end": "2567460"
  },
  {
    "text": "And in this case,\nwe don't talk about, so this guy is called\nlogit or logit. And this guy is called probit.",
    "start": "2567460",
    "end": "2573172"
  },
  {
    "text": "And you see it,\nusually, every time you have a package on\ngeneralized linear models.",
    "start": "2573172",
    "end": "2578534"
  },
  {
    "text": "You are trying to implement. You have this choice. And for what's called logistic\nregression-- so it's funny",
    "start": "2578534",
    "end": "2584009"
  },
  {
    "text": "that it's called\nlogistic regression, but you can actually\nuse the probit link, which in this case, is\ncalled probit regression.",
    "start": "2584009",
    "end": "2590619"
  },
  {
    "text": "But those things are\nessentially equivalent, and it's really a\nmatter of taste. Maybe of communities--\nsome communities",
    "start": "2590620",
    "end": "2596440"
  },
  {
    "text": "might prefer one over the other. We'll see that\nagain, as I claimed before, the logistic,\nthe logit one",
    "start": "2596440",
    "end": "2604810"
  },
  {
    "text": "has a slightly more compelling\nargument for its reason to exist.",
    "start": "2604810",
    "end": "2610151"
  },
  {
    "text": "I guess this one, the\ncompelling argument is that it involved the\nstandard Gaussian, which of course, is something that\nshould show up everywhere.",
    "start": "2610152",
    "end": "2617470"
  },
  {
    "text": "And then, you can think\nabout crazy stuff. Even crazy gets name--",
    "start": "2617470",
    "end": "2623770"
  },
  {
    "text": "complimentary log, log, which is\nthe log of minus, log 1 minus. Why not?",
    "start": "2623770",
    "end": "2629170"
  },
  {
    "text": " So I guess you can\niterate that thing.",
    "start": "2629170",
    "end": "2636450"
  },
  {
    "text": "You can just put a log 1\nminus in front of this thing, and it's still going to go. So that's not true.",
    "start": "2636450",
    "end": "2647809"
  },
  {
    "text": "I have to put a minus and take-- no, that's not true. ",
    "start": "2647810",
    "end": "2653707"
  },
  {
    "text": "So you can think of\nwhatever you want. ",
    "start": "2653707",
    "end": "2659320"
  },
  {
    "text": "So I claimed that the logit\nlink is the natural choice, so here's a picture. I should have actually\nplotted the other one,",
    "start": "2659320",
    "end": "2665590"
  },
  {
    "text": "so we can actually compare it. To be fair, I don't even\nremember how it would actually fit into those two functions.",
    "start": "2665590",
    "end": "2672010"
  },
  {
    "text": "So the blue one, which is\nthis one, for those of you don't see the difference\nbetween blue and red--",
    "start": "2672010",
    "end": "2677670"
  },
  {
    "text": "sorry about that. So this the blue one\nis the logistic one.",
    "start": "2677670",
    "end": "2685320"
  },
  {
    "text": "So this guy is the function that\ndoes e to the x, over 1 plus e to the x.",
    "start": "2685320",
    "end": "2690480"
  },
  {
    "text": "As you can see,\nthis is a function that's supposed to map\nthe entire real line into the intervals, 0, 1.",
    "start": "2690480",
    "end": "2695970"
  },
  {
    "text": "So that's supposed to be the\ninverse of your function, and I claimed that this is\nthe inverse of the logistic of the logit function.",
    "start": "2695970",
    "end": "2702090"
  },
  {
    "text": "And the blue one, well,\nthis is the Gaussian CDF, so you know it's clearly\nthe inverse of the inverse of the Gaussian CDF.",
    "start": "2702090",
    "end": "2707732"
  },
  {
    "text": "And that's the red one. That's the one that goes here.  I would guess that the\ncomplimentary log, log is",
    "start": "2707732",
    "end": "2715320"
  },
  {
    "text": "something that's probably\ngoing above here, and for which the\nslope is, actually, even a little flatter\nas you cross 0.",
    "start": "2715320",
    "end": "2722840"
  },
  {
    "text": " So of course, this is\nnot our link functions.",
    "start": "2722840",
    "end": "2729119"
  },
  {
    "text": "These are the inverse\nof our link function. So what do they look\nlike when actually, basically, flip my\nthing like this?",
    "start": "2729119",
    "end": "2736670"
  },
  {
    "text": "So this is what I see. And so I can see that in blue,\nthis is my logistic link.",
    "start": "2736670",
    "end": "2742600"
  },
  {
    "text": "So it crosses 0 with a\nslightly faster rate. Remember, if we could\nuse the identity, that",
    "start": "2742600",
    "end": "2749829"
  },
  {
    "text": "would be very nice to us. We would just want\nto take the identity. The problem is that\nif I start having",
    "start": "2749830",
    "end": "2755145"
  },
  {
    "text": "the identity that\ngoes here, it's going to start being a problem. And this is the probit link,\nthe phi verse that you see here.",
    "start": "2755145",
    "end": "2766419"
  },
  {
    "text": "It's a little flatter.  You can compute the derivative\nat zero of those guys.",
    "start": "2766419",
    "end": "2776599"
  },
  {
    "text": "What is the derivative of the--  So I'm taking the derivative\nof log of x over 1 minus x.",
    "start": "2776599",
    "end": "2784380"
  },
  {
    "text": "So it's 1 over x,\nminus 1 over 1 minus x.",
    "start": "2784380",
    "end": "2792009"
  },
  {
    "text": " So if I look at 0.5--",
    "start": "2792010",
    "end": "2799120"
  },
  {
    "text": "sorry, this is\nthe interval 0, 1. So I'm interested\nin the slope at 0.5.",
    "start": "2799120",
    "end": "2808070"
  },
  {
    "text": "Yes, it's plus, thank you. So at 0.5, what I\nget is 2 plus 2.",
    "start": "2808070",
    "end": "2813230"
  },
  {
    "text": " Yeah, so that's the\nslope that we get,",
    "start": "2813230",
    "end": "2822650"
  },
  {
    "text": "and if you compute\nfor the derivative-- what is the derivative\nof a phi inverse?",
    "start": "2822650",
    "end": "2829100"
  },
  {
    "text": "Well, it's a little\nphi of x, divided by little phi of capital\nphi, inverse of x.",
    "start": "2829100",
    "end": "2840640"
  },
  {
    "text": "So little phi at 1/2-- I don't know. ",
    "start": "2840640",
    "end": "2849450"
  },
  {
    "text": "Yeah, I guess I can\nprobably compute the derivative of\nthe capital phi at 0, which is going\nto be just that.",
    "start": "2849450",
    "end": "2854460"
  },
  {
    "text": "1 over square root of 2\npi, and then just say well, the slope has to be 1 over that. ",
    "start": "2854460",
    "end": "2862972"
  },
  {
    "text": "Square root 2 pi.  So that's just a comparison,\nbut again, so far, we",
    "start": "2862972",
    "end": "2870309"
  },
  {
    "text": "do not have any reason to\nprefer one to the other. And so now, I'm going to\nstart giving you some reasons",
    "start": "2870310",
    "end": "2876400"
  },
  {
    "text": "to prefer one to the other. And one of those two-- and actually for each\ncanonical family,",
    "start": "2876400",
    "end": "2883570"
  },
  {
    "text": "there is something which is\ncalled the canonical link. And when you don't\nhave any other reason to choose anything else, why\nnot choose the canonical one?",
    "start": "2883570",
    "end": "2890386"
  },
  {
    "text": "And the canonical\nlink is the one that says OK, what I want is g\nto map mu onto the real line.",
    "start": "2890386",
    "end": "2899580"
  },
  {
    "text": " But mu is not the parameter\nof my canonical family.",
    "start": "2899580",
    "end": "2908550"
  },
  {
    "text": "Here for example,\nmu is e of theta, but the canonical\nparameter is theta. ",
    "start": "2908550",
    "end": "2916050"
  },
  {
    "text": "But the parameter of a\ncanonical exponential family is something that lives\nin the entire real line.",
    "start": "2916050",
    "end": "2922650"
  },
  {
    "text": "It was defined for all thetas. And so in particular,\nI can just take theta",
    "start": "2922650",
    "end": "2930250"
  },
  {
    "text": "to be the one that's\nx transpose beta. And so in particular,\nI'm just going to try to find the link\nthat just says OK, when",
    "start": "2930250",
    "end": "2937180"
  },
  {
    "text": "I take g of mu,\nI'm going to map, so that's what's going to be. So I know that g of mu is\ngoing to be equal to x beta.",
    "start": "2937180",
    "end": "2945499"
  },
  {
    "text": "And now, what I'm\ngoing to say is OK, let's just take the g that\nmakes this guy equal to theta, so that this is theta\nthat actually model,",
    "start": "2945499",
    "end": "2951600"
  },
  {
    "text": "like x transpose beta. Feels pretty canonical, right?",
    "start": "2951600",
    "end": "2957960"
  },
  {
    "text": "What else? What other central, easy\nchoice would you take? This was pretty easy.",
    "start": "2957960",
    "end": "2963650"
  },
  {
    "text": "There is a natural parameter\nfor this canonical family, and it takes value on\nthe entire real line.",
    "start": "2963650",
    "end": "2969780"
  },
  {
    "text": "I have a function that maps\nmu onto the entire real line, so let's just map it to\nthe actual parameter.",
    "start": "2969780",
    "end": "2976260"
  },
  {
    "text": "So now, OK, why do I have this? Well, we've already\nfigured that out.",
    "start": "2976260",
    "end": "2981960"
  },
  {
    "text": "The canonical link function\nis strictly increasing. Sorry, so I said that\nnow I want this guy--",
    "start": "2981960",
    "end": "2989670"
  },
  {
    "text": "so I want g of mu to\nbe equal to theta,",
    "start": "2989670",
    "end": "2997470"
  },
  {
    "text": "which is equivalent to\nsaying that I want mu to be equal to g inverse of theta.",
    "start": "2997470",
    "end": "3003860"
  },
  {
    "text": "But we know that mu is what-- b prime of theta.",
    "start": "3003860",
    "end": "3009160"
  },
  {
    "start": "3009160",
    "end": "3015640"
  },
  {
    "text": "So that means that b prime is\nthe same function as g inverse.",
    "start": "3015640",
    "end": "3021089"
  },
  {
    "text": "And I claimed that this is\nactually giving me, indeed, a function that has the\nproperties that I want,",
    "start": "3021090",
    "end": "3027930"
  },
  {
    "text": "because before I said,\njust pick any function that has these properties. And now, I'm giving\nyou a very hard rule",
    "start": "3027930",
    "end": "3033102"
  },
  {
    "text": "to pick this, though\nyou need still to check that it satisfies\nthose conditions and particular, that it's increasing\nand invertible.",
    "start": "3033102",
    "end": "3039050"
  },
  {
    "text": "And so for this to be\nincreasing and invertible, strictly increasing\nand invertible, really what I need is that\nthe inverse is strictly",
    "start": "3039050",
    "end": "3044880"
  },
  {
    "text": "increasing and invertible,\nwhich is the case here, because b prime as we said--",
    "start": "3044880",
    "end": "3051220"
  },
  {
    "text": "well, b prime is the derivative\nof a strictly convex function.",
    "start": "3051220",
    "end": "3056609"
  },
  {
    "text": "A strictly convex function\nhas a second derivative that's strictly positive. We just figured that\nout using the fact",
    "start": "3056610",
    "end": "3061770"
  },
  {
    "text": "that the variance was\nstrictly positive. And if phi is strictly\npositive, then this thing has to be strictly positive.",
    "start": "3061770",
    "end": "3068530"
  },
  {
    "text": "So if b prime, prime\nis strictly positive-- this is the derivative of\na function called b prime.",
    "start": "3068530",
    "end": "3073604"
  },
  {
    "text": "If your derivative\nis strictly positive, you are strictly increasing. And so we know that b prime is,\nindeed, strictly increasing.",
    "start": "3073604",
    "end": "3082809"
  },
  {
    "text": "And what I need also\nto check-- well, I guess this is already\nchecked on its own,",
    "start": "3082810",
    "end": "3088010"
  },
  {
    "text": "because b prime is\nactually mapping all of our",
    "start": "3088010",
    "end": "3093560"
  },
  {
    "text": "into the possible values. When theta ranges on\nthe entire real line,",
    "start": "3093560",
    "end": "3098870"
  },
  {
    "text": "then b prime ranges\nin the entire interval of the mean values\nthat it can take.",
    "start": "3098870",
    "end": "3105440"
  },
  {
    "text": "And so now, I have this thing\nthat's completely defined. B prime inverse is a valid link.",
    "start": "3105440",
    "end": "3110490"
  },
  {
    "start": "3110490",
    "end": "3116030"
  },
  {
    "text": "And it's called\na canonical link. ",
    "start": "3116030",
    "end": "3122470"
  },
  {
    "text": "OK, so again, if I give you\nan exponential family, which is another way of saying I'll\ngive you a convex function, b,",
    "start": "3122470",
    "end": "3129350"
  },
  {
    "text": "which gives you some\nexponential family, then if you just\ntake b prime inverse,",
    "start": "3129350",
    "end": "3135160"
  },
  {
    "text": "this gives you the\nassociated canonical link for this canonical\nexponential family.",
    "start": "3135160",
    "end": "3141590"
  },
  {
    "text": "So clearly there's\nan advantage of doing this, which is I don't\nhave to actually think",
    "start": "3141590",
    "end": "3148070"
  },
  {
    "text": "about which one to pick if I\ndon't want to think about it. But there's other\nadvantages that come to it,",
    "start": "3148070",
    "end": "3154220"
  },
  {
    "text": "and we'll see that in\nthe representations. There's, basically, going to be\nsome light cancellations that show up.",
    "start": "3154220",
    "end": "3159290"
  },
  {
    "text": "So before we go\nthere, let's just compute the canonical link for\nthe Bernoulli distribution. So remember, the\nBernoulli distribution",
    "start": "3159290",
    "end": "3166360"
  },
  {
    "text": "has a PMF, which is part of the\ncanonical exponential family.",
    "start": "3166360",
    "end": "3175510"
  },
  {
    "text": "So the PMF of the\nBernoulli is f theta of x. ",
    "start": "3175510",
    "end": "3186529"
  },
  {
    "text": "Let me just write it like this. So it's p to the y,\nlet's say-- one minus p",
    "start": "3186529",
    "end": "3192470"
  },
  {
    "text": "to the 1 minus y,\nwhich I will write as exponential y log p, plus\n1 minus y, log 1 minus p.",
    "start": "3192470",
    "end": "3208910"
  },
  {
    "text": "OK, we've done that last time. Now, I'm going to\ngroup my terms in y to see how y interacts\nwith this parameter p.",
    "start": "3208910",
    "end": "3217530"
  },
  {
    "text": "And what I'm getting is\ny, which is times log p divided by 1 minus p.",
    "start": "3217530",
    "end": "3222540"
  },
  {
    "text": "And then, the only term that\nremains is log, 1 minus p. ",
    "start": "3222540",
    "end": "3230370"
  },
  {
    "text": "Now, I want this to be a\ncanonical exponential family, which means that I just\nneed to call this guy,",
    "start": "3230370",
    "end": "3236880"
  },
  {
    "text": "so it is part of the\nexponential family. You can read that. If I want it to be canonical,\nthis guy must be theta itself.",
    "start": "3236880",
    "end": "3244520"
  },
  {
    "text": "So I have that theta is\nequal to log p, 1 minus p.",
    "start": "3244520",
    "end": "3251180"
  },
  {
    "text": "If I invert this\nthing, it tells me that p is e to the theta,\ndivided by 1, plus e",
    "start": "3251180",
    "end": "3256880"
  },
  {
    "text": "to the theta. It's just inverting\nthis function. ",
    "start": "3256880",
    "end": "3263550"
  },
  {
    "text": "In particular, it means\nthat log, 1 minus p, is equal to log, 1\nminus this thing.",
    "start": "3263550",
    "end": "3271900"
  },
  {
    "text": "So the exponential\nthetas go away. So in the numerator,\nthis is what I get.",
    "start": "3271900",
    "end": "3279350"
  },
  {
    "text": "That's the log 1 minus this guy,\nwhich is equal to minus log 1,",
    "start": "3279350",
    "end": "3284870"
  },
  {
    "text": "plus e to the theta. ",
    "start": "3284870",
    "end": "3290790"
  },
  {
    "text": "So I'm going a bit too\nfast, but these are very elementary manipulations--",
    "start": "3290790",
    "end": "3296230"
  },
  {
    "text": "maybe, it requires one more\nline to convince yourself. But just do it in the\ncomfort of your room.",
    "start": "3296230",
    "end": "3305940"
  },
  {
    "text": "And then, what you have is the\nexponential of y times theta,",
    "start": "3305940",
    "end": "3311210"
  },
  {
    "text": "and then, I have minus\nlog, 1 plus e theta.",
    "start": "3311210",
    "end": "3316849"
  },
  {
    "text": "So this is the\nrepresentation of the p and f of a Bernoulli\ndistribution,",
    "start": "3316850",
    "end": "3323990"
  },
  {
    "text": "as part of a member of the\ncanonical exponential family. And it tells me that b of\ntheta is equal to log 1,",
    "start": "3323990",
    "end": "3333530"
  },
  {
    "text": "plus e of theta. That's what I have there. From there, I can compute the\nexpectation, which hopefully,",
    "start": "3333530",
    "end": "3341789"
  },
  {
    "text": "I'm going to get p as\nthe mean and p times 1, minus p as the variance.",
    "start": "3341790",
    "end": "3347759"
  },
  {
    "text": "Otherwise, that would be weird.  So let's just do this.",
    "start": "3347759",
    "end": "3355839"
  },
  {
    "text": "B prime of theta should\ngive me the mean.",
    "start": "3355840",
    "end": "3360950"
  },
  {
    "text": "And indeed, b prime of\ntheta is e to the theta, divided by 1, plus e to\nthe theta, which is exactly",
    "start": "3360950",
    "end": "3368060"
  },
  {
    "text": "this p that I had there. ",
    "start": "3368060",
    "end": "3374850"
  },
  {
    "text": "OK just for fun-- well, I don't know. Maybe, that's not part of it.",
    "start": "3374850",
    "end": "3380520"
  },
  {
    "text": "Yeah, let's not compute\nthe second derivative. That's probably going to be on\nyour homework at some point--",
    "start": "3380520",
    "end": "3385800"
  },
  {
    "text": "if not, on the final. So b prime now--",
    "start": "3385800",
    "end": "3392890"
  },
  {
    "text": "oh, I erased it, of course.  G, the canonical link\nis b prime inverse.",
    "start": "3392890",
    "end": "3399380"
  },
  {
    "text": " And I claim that this\nis going to give me",
    "start": "3399380",
    "end": "3404770"
  },
  {
    "text": "the logit function, log\nof mu, over 1 minus mu. So let's check that.",
    "start": "3404770",
    "end": "3410480"
  },
  {
    "text": "So b prime is this\nthing, so now, I want to find the inverse. ",
    "start": "3410480",
    "end": "3422180"
  },
  {
    "text": "Well, I should really call\nmy inverse a function of p. And I've done it before-- all I have to do is to\nsolve this equation, which",
    "start": "3422180",
    "end": "3428930"
  },
  {
    "text": "I've actually just\ndone it, that's where I'm actually coming from. So it's actually telling me\nthat the solution of this thing",
    "start": "3428930",
    "end": "3434510"
  },
  {
    "text": "is equal to log of\np over 1 minus p. ",
    "start": "3434510",
    "end": "3445809"
  },
  {
    "text": "We just solve this\nthing both ways. And this is, indeed, logit\nof p by definition of logit.",
    "start": "3445810",
    "end": "3458520"
  },
  {
    "text": "So b prime inverse,\nthis function that seemed to come out\nof nowhere, is really just the inverse of b\nprime, which we know",
    "start": "3458520",
    "end": "3465030"
  },
  {
    "text": "is the canonical link. And canonical is some\nsort of ad hoc choices that we've made by saying let's\njust take the link, such that d",
    "start": "3465030",
    "end": "3473040"
  },
  {
    "text": "of mu is giving me the actual\ncanonical parameter of theta. Yeah?",
    "start": "3473040",
    "end": "3478785"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: You're right. ",
    "start": "3478785",
    "end": "3488520"
  },
  {
    "text": "Now, of course, I'm going\nthrough all this trouble, but you could see\nit immediately. I know this is\ngoing to be theta.",
    "start": "3488520",
    "end": "3496550"
  },
  {
    "text": "We also have prior\nknowledge, hopefully, that the expectation of\na Bernoulli is p itself.",
    "start": "3496550",
    "end": "3503519"
  },
  {
    "text": "So right at this\nstep, when I say that I'm going to take\ntheta to be this guy, already knew that the\ncanonical link was the logit--",
    "start": "3503520",
    "end": "3512959"
  },
  {
    "text": "because I just said\noh, here's theta. And it's just this\nfunction of mu [INAUDIBLE].. ",
    "start": "3512959",
    "end": "3521099"
  },
  {
    "text": "OK, so you can do that\nfor a bunch of examples, and this is what they're\ngoing to give you. So the Gaussian\ncase, b of theta--",
    "start": "3521100",
    "end": "3527820"
  },
  {
    "text": "we've actually computed\nit, actually, once. This is theta squared over 2. So the derivative of\nthis thing is really",
    "start": "3527820",
    "end": "3533130"
  },
  {
    "text": "just theta, which means that\ng or g inverse is actually equal to the identity.",
    "start": "3533130",
    "end": "3539280"
  },
  {
    "text": "And again, sanity check-- when I'm in the\nGaussian case, there's",
    "start": "3539280",
    "end": "3544410"
  },
  {
    "text": "nothing general about\ngeneral linear models if you don't have a link. The Poisson case-- you\ncan actually check.",
    "start": "3544410",
    "end": "3552390"
  },
  {
    "text": "Did we do this, actually? Yes we did. So that's when we\nhad this e of theta. And so b is e of theta, which\nmeans that the natural link is",
    "start": "3552390",
    "end": "3559960"
  },
  {
    "text": "the inverse, which is log, which\nis the inverse of exponential. And so that's logarithm\nlink, which as I said,",
    "start": "3559960",
    "end": "3569680"
  },
  {
    "text": "I used the word natural. You can also use\nthe word canonical if you want to describe\nthis function as being",
    "start": "3569680",
    "end": "3575740"
  },
  {
    "text": "the right function to map\nthe positive real line to the entire real line.",
    "start": "3575740",
    "end": "3580959"
  },
  {
    "text": "The Bernoulli-- we just did it. So b-- the cumulative\nenduring function is log of 1,",
    "start": "3580959",
    "end": "3586930"
  },
  {
    "text": "plus e of theta, which is\nlog of mu over 1 minus mu.",
    "start": "3586930",
    "end": "3592990"
  },
  {
    "text": "And gamma function--\nwhere you have the thing you're going to see is\nminus log of minus [INAUDIBLE]..",
    "start": "3592990",
    "end": "3600700"
  },
  {
    "text": "You see the reciprocal link\nis the link that actually shows up, so minus 1 over mu.",
    "start": "3600700",
    "end": "3608045"
  },
  {
    "text": "That maps. ",
    "start": "3608045",
    "end": "3635690"
  },
  {
    "text": "So are there any questions\nabout the canonical links, canonical families?",
    "start": "3635690",
    "end": "3642532"
  },
  {
    "text": "I use the word canonical a lot. But is everything fitting\ntogether right now?",
    "start": "3642532",
    "end": "3648929"
  },
  {
    "text": "So we have this function. We have canonical exponential\nfamily, by assumption. It has a function,\nb, which contains",
    "start": "3648929",
    "end": "3654800"
  },
  {
    "text": "every information we want. At the beginning\nof the lecture, we established that\nit has information about the mean in\nthe first derivative,",
    "start": "3654800",
    "end": "3661310"
  },
  {
    "text": "about the variance in\nthe second derivative. And it's also giving\nus a canonical link. So just cherish this b\nonce you've found it,",
    "start": "3661310",
    "end": "3668035"
  },
  {
    "text": "because it's\neverything you need. Yeah? AUDIENCE: [INAUDIBLE] ",
    "start": "3668035",
    "end": "3675962"
  },
  {
    "text": "PHILIPPE RIGOLLET: I don't\nknow, a political preference? ",
    "start": "3675962",
    "end": "3684710"
  },
  {
    "text": "I don't know, honestly. If I were a serious\npractitioner, I probably would have a\nbetter answer for you.",
    "start": "3684710",
    "end": "3691700"
  },
  {
    "text": "At this point, I just don't. I think it's a\nmatter of practice and actual preferences.",
    "start": "3691700",
    "end": "3696859"
  },
  {
    "text": "You can also try both. We didn't mention\nit, but there's this idea of\ncross-validation-- well, we mentioned it without\ngoing too much into detail.",
    "start": "3696860",
    "end": "3703610"
  },
  {
    "text": "But you could try both\nand see which one performs best on a yet unseen data set.",
    "start": "3703610",
    "end": "3708617"
  },
  {
    "text": "In terms of prediction, just say\nI prefer this one of the two, because this actually comes\nas part of your modeling assumption, right?",
    "start": "3708617",
    "end": "3716089"
  },
  {
    "text": "Not only did you decide\nto model the image of mu through the link function as\na linear model, but really",
    "start": "3716090",
    "end": "3723057"
  },
  {
    "text": "what you're saying-- your model is saying\nwell, you have two pieces of [INAUDIBLE],,\nthe distribution of y. But you also have\nthe fact that mu",
    "start": "3723057",
    "end": "3730340"
  },
  {
    "text": "is modeled as g inverse\nof x transpose beta. And for different g's, this\nis just different modeling",
    "start": "3730340",
    "end": "3737120"
  },
  {
    "text": "assumptions, right? So why should this be linear--",
    "start": "3737120",
    "end": "3745930"
  },
  {
    "text": "I don't know.  My authority as a\nperson who has not",
    "start": "3745930",
    "end": "3752740"
  },
  {
    "text": "examined the\n[INAUDIBLE] data sets for both things would be that\nthe changes are fairly minor.",
    "start": "3752740",
    "end": "3758050"
  },
  {
    "text": " OK, so this was all\nfor one observation.",
    "start": "3758050",
    "end": "3765420"
  },
  {
    "text": "We just, basically,\ndid probability. We described some density, some\nproperties of the densities,",
    "start": "3765420",
    "end": "3772620"
  },
  {
    "text": "how to compute expectations. That was really\njust probability. There was no data\ninvolved at any point. We did a bit of modeling, but\nit was all for one observation.",
    "start": "3772620",
    "end": "3780330"
  },
  {
    "text": "What we're going\nto try to do now is given the reverse\nengineering to probability",
    "start": "3780330",
    "end": "3786359"
  },
  {
    "text": "that is statistics,\ngiven data, what can I infer about my model? Now remember, there's\nthree parameters",
    "start": "3786360",
    "end": "3792370"
  },
  {
    "text": "that are floating\naround in this model. There is one that was theta.",
    "start": "3792370",
    "end": "3798190"
  },
  {
    "text": "There is one that was mu, and\nthere is one that is beta. OK, so those are\nthe three parameters",
    "start": "3798190",
    "end": "3803230"
  },
  {
    "text": "that are floating around. What we said is that the\nexpectation of y, given x,",
    "start": "3803230",
    "end": "3812550"
  },
  {
    "text": "is mu of x. So if I estimate mu, I know the\nconditional expectation of y,",
    "start": "3812550",
    "end": "3817950"
  },
  {
    "text": "given x, which definitely\ngives me theta of x.",
    "start": "3817950",
    "end": "3824960"
  },
  {
    "text": "How do I go from mu\nof x to theta of x? ",
    "start": "3824960",
    "end": "3835080"
  },
  {
    "text": "The inverse of what-- of the arrow? Yeah, sure, but how do I go\nfrom this guy to this guy?",
    "start": "3835080",
    "end": "3847289"
  },
  {
    "text": "So theta as a function of mu is? ",
    "start": "3847290",
    "end": "3852556"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nYeah, so we just computed that mu was\nb prime of theta.",
    "start": "3852556",
    "end": "3858760"
  },
  {
    "text": "So it means that theta is\njust b prime inverse of mu. So those two things\nare the same as far",
    "start": "3858760",
    "end": "3864910"
  },
  {
    "text": "as we're concerned, because we\nknow that b prime is strictly increasing it's invertible. So it's just a matter\nof re-parametrization,",
    "start": "3864910",
    "end": "3871560"
  },
  {
    "text": "and we just can switch from one\nto the other whenever we want. But why we go through\nmu, because so far",
    "start": "3871560",
    "end": "3876754"
  },
  {
    "text": "for the entire\nsemester I told you there was one\nparameter that's theta. It does not have to be the\nmean, and that's the parameter that we care about.",
    "start": "3876754",
    "end": "3882130"
  },
  {
    "text": "It's the one on which we\nwant to do interference. That's the one for which we're\ngoing to compute the Fisher information. This was the parameter that\nwas our object of worship.",
    "start": "3882130",
    "end": "3889572"
  },
  {
    "text": "And now, I'm saying\noh, I'm going to have mu that's coming around. And why we have mu,\nbecause this is the mu",
    "start": "3889572",
    "end": "3895270"
  },
  {
    "text": "that we use to go to beta. So I can go freely from theta\nto mu using b prime or b",
    "start": "3895270",
    "end": "3906359"
  },
  {
    "text": "prime inverse. And now, I can go\nfrom mu to beta, because I have that g of mu\nof x is beta transpose x.",
    "start": "3906360",
    "end": "3919120"
  },
  {
    "text": "So in the end,\nnow, this is going to be my object of worship. This is going to be the\nparameter that matters.",
    "start": "3919120",
    "end": "3924318"
  },
  {
    "text": "Because once I set beta,\nI set everything else through this chain.",
    "start": "3924318",
    "end": "3930289"
  },
  {
    "text": "So the question is if I\nstart stacking up this pile of parameters-- so I\nstart with my beta,",
    "start": "3930290",
    "end": "3936260"
  },
  {
    "text": "which in turns give me\na mu, which in turn, gives me a theta-- can I just have a\nlong, streamlined--",
    "start": "3936260",
    "end": "3943720"
  },
  {
    "text": "what is the outcome\nwhen I actually start writing my likelihood,\nnot as a function of theta, not as a function of mu,\nbut as a function of beta,",
    "start": "3943720",
    "end": "3950140"
  },
  {
    "text": "which is the one at\nthe end of the chain? And hopefully, things are\ngoing to happen nicely,",
    "start": "3950140",
    "end": "3955540"
  },
  {
    "text": "and they might no. Yeah? AUDIENCE: [INAUDIBLE] ",
    "start": "3955540",
    "end": "3962076"
  },
  {
    "text": "PHILIPPE RIGOLLET: Is G-- that's my link. G of mu of x-- now, its mu is a function of x,\nbecause its conditional on x.",
    "start": "3962076",
    "end": "3969320"
  },
  {
    "text": " So this is really\ntheta of x, mu of x,",
    "start": "3969320",
    "end": "3977000"
  },
  {
    "text": "but b is not a function of x,\nbecause it's just something to tells me what the\nfunction of x is.",
    "start": "3977000",
    "end": "3982965"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: Mu is\nthe conditional expectation",
    "start": "3982965",
    "end": "3988240"
  },
  {
    "text": "of y, given x. It has, actually, a fancy name\nin the statistics literature. It's called-- anybody knows\nthe name of the function, mu",
    "start": "3988240",
    "end": "3996989"
  },
  {
    "text": "of x, which is a conditional\nexpectation of y, given x? ",
    "start": "3996989",
    "end": "4002116"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: That's\nthe regression function. That's actual definition.",
    "start": "4002116",
    "end": "4007230"
  },
  {
    "text": "If I tell you what is the\ndefinition of the regression function, that's just the\nconditional expectation of why, given x.",
    "start": "4007230",
    "end": "4012970"
  },
  {
    "text": "And I could look at any property\nof the conditional distribution",
    "start": "4012970",
    "end": "4018720"
  },
  {
    "text": "of y given x. I could look at the\nconditional 95th percentile. I can look at the\nconditional median.",
    "start": "4018720",
    "end": "4024180"
  },
  {
    "text": "I can look at the conditional\n[INAUDIBLE] range. I can look at the\nconditional variance. But I decide to look at the\nconditional expectation, which",
    "start": "4024180",
    "end": "4032300"
  },
  {
    "text": "is called the\nregression function. ",
    "start": "4032300",
    "end": "4038363"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] ",
    "start": "4038363",
    "end": "4044231"
  },
  {
    "text": "PHILIPPE RIGOLLET: Oh,\nthere's no transpose here. Actually, only Victor-Emmanuel\nused this prime for transpose, and I found it confusing\nwith the derivatives.",
    "start": "4044231",
    "end": "4050710"
  },
  {
    "text": "So primes here is\nonly a derivative. AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: Oh, yeah,\nsorry, beta transpose x.",
    "start": "4050710",
    "end": "4058640"
  },
  {
    "text": "So you said what? I said that g of mu of\nx is beta transpose x? AUDIENCE: [INAUDIBLE]",
    "start": "4058640",
    "end": "4065145"
  },
  {
    "text": " PHILIPPE RIGOLLET: Isn't\nthat the same thing? ",
    "start": "4065145",
    "end": "4072510"
  },
  {
    "text": "X is a vector here, right? AUDIENCE: Yeah. PHILIPPE RIGOLLET:\nSo x transpose beta, and beta transpose x\nare of the same thing.",
    "start": "4072510",
    "end": "4080348"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PHILIPPE RIGOLLET: So\nbeta looks like this.",
    "start": "4080348",
    "end": "4085770"
  },
  {
    "text": "X looks like this. It's just a simple number.",
    "start": "4085770",
    "end": "4092420"
  },
  {
    "text": "Yeah, you're right. I'm going to start\nto look at matrices. I'm going to have to be slightly\nmore careful when I do this.",
    "start": "4092420",
    "end": "4098189"
  },
  {
    "text": "OK so let's do the\nreverse engineering. I'm giving you data. From this data,\nhopefully, you should",
    "start": "4098189",
    "end": "4103740"
  },
  {
    "text": "be able to get what the\nconditional-- if I give you an infinite amount of data,\nyou would know exactly,",
    "start": "4103740",
    "end": "4109630"
  },
  {
    "text": "of pairs xy, what the\nconditional distribution of y given x is.",
    "start": "4109630",
    "end": "4116130"
  },
  {
    "text": "And in particular,\nyou would know what the conditional\nexpectation of y given x is, which means that\nyou would know mu,",
    "start": "4116130",
    "end": "4122359"
  },
  {
    "text": "which means that you\nwould know theta, which means that you would know beta. Now, when I have a finite\nnumber of observations,",
    "start": "4122359",
    "end": "4128600"
  },
  {
    "text": "I'm going to try to\nestimate mu of x. But really, I'm going to\ngo the other way around. Because the fact that I assume,\nspecifically, that mu of x",
    "start": "4128600",
    "end": "4136278"
  },
  {
    "text": "is of the form g of mu of x\nis x transpose beta, then that means that I only have\nto estimate beta, which",
    "start": "4136279",
    "end": "4142849"
  },
  {
    "text": "is a much simpler object than\nthe entire regression function. So that's what I'm\ngoing to go for.",
    "start": "4142850",
    "end": "4147890"
  },
  {
    "text": "I'm going to try to represent\nthe likelihood, the log likelihood, of my data as\na function, not of theta, not of mu, but of beta--",
    "start": "4147890",
    "end": "4155390"
  },
  {
    "text": "and then, maximize that guy. So now, rather than thinking\nof just one observation,",
    "start": "4155390",
    "end": "4161870"
  },
  {
    "text": "I'm going to have a\nbunch of observations. ",
    "start": "4161870",
    "end": "4167100"
  },
  {
    "text": "So this might actually\nlook a little confusing, but let's just make sure\nthat we understand each other",
    "start": "4167100",
    "end": "4172189"
  },
  {
    "text": "before we go any further. So I'm going to\nhave observations,",
    "start": "4172189",
    "end": "4178509"
  },
  {
    "text": "x1, y1, all the\nway to xn, yn, just like in a natural\nregression problem,",
    "start": "4178510",
    "end": "4185310"
  },
  {
    "text": "except that here my y's\nmight be 0 one valued. They might be positive valued.",
    "start": "4185310",
    "end": "4190649"
  },
  {
    "text": "They might be exponential. They might be anything in the\ncanonical exponential family. ",
    "start": "4190649",
    "end": "4197840"
  },
  {
    "text": "OK so I have this\nthing, and now, what I have is that my\nobservations are x1, y1, xn, yn.",
    "start": "4197840",
    "end": "4203310"
  },
  {
    "text": "And what I want\nis that I'm going to assume that the conditional\nexpectation of yi, given--",
    "start": "4203310",
    "end": "4211640"
  },
  {
    "text": " the conditional distribution\nof yi, given xi,",
    "start": "4211640",
    "end": "4218710"
  },
  {
    "text": "is something that has density. ",
    "start": "4218710",
    "end": "4230070"
  },
  {
    "text": "Did I put an i on y-- yeah. ",
    "start": "4230070",
    "end": "4242820"
  },
  {
    "text": "I'm not going to deal with\nthe phi and the c now. And why do I have\ntheta i and not theta",
    "start": "4242820",
    "end": "4248610"
  },
  {
    "text": "is because theta i is\nreally a function of xi.",
    "start": "4248610",
    "end": "4261350"
  },
  {
    "text": "So it's really theta i of xi. But what do I know\nabout theta i of xi,",
    "start": "4261350",
    "end": "4267240"
  },
  {
    "text": "it's actually equal to b-- I did this error twice--",
    "start": "4267240",
    "end": "4273920"
  },
  {
    "text": "b prime inverse of mu of xi. ",
    "start": "4273920",
    "end": "4290620"
  },
  {
    "text": "And I'm going to assume that\nthis is of the form beta transpose xi.",
    "start": "4290620",
    "end": "4296190"
  },
  {
    "text": "And this is why I have theta i-- is because this theta\ni is a function of xi, and I'm going to assume a very\nsimple form for this thing.",
    "start": "4296190",
    "end": "4302830"
  },
  {
    "text": " Sorry, sorry, sorry, sorry--",
    "start": "4302830",
    "end": "4308746"
  },
  {
    "text": "I should not write it like this. This is only when I\nhave the canonical link. So this is actually equal\nto b prime inverse of g,",
    "start": "4308747",
    "end": "4317310"
  },
  {
    "text": "of xi transpose beta. ",
    "start": "4317310",
    "end": "4325010"
  },
  {
    "text": "Sorry, g inverse--\nthose two things are actually\ncanceling each other. ",
    "start": "4325010",
    "end": "4333760"
  },
  {
    "text": "So as before, I'm going to\nstack everything into some-- well, actually, I'm not going to\nstack anything for the moment.",
    "start": "4333760",
    "end": "4340360"
  },
  {
    "text": "I'm just going to give\nyou a peek at what's happening next week, rather\nthan just manipulating the data.",
    "start": "4340360",
    "end": "4348010"
  },
  {
    "text": "So here is how we're going\nto proceed at this point.",
    "start": "4348010",
    "end": "4353809"
  },
  {
    "text": "Well now, I want to write\nmy likelihood function, not as a function of theta,\nbut as a function of beta,",
    "start": "4353810",
    "end": "4359270"
  },
  {
    "text": "because that's the parameter\nI'm actually trying to maximize. So if I have a link--",
    "start": "4359270",
    "end": "4367050"
  },
  {
    "text": "so this thing that matters\nhere, I'm going to call h. ",
    "start": "4367050",
    "end": "4373600"
  },
  {
    "text": "By definition, this is going\nto be h of xi transpose beta. Helena, you have a question?",
    "start": "4373600",
    "end": "4380080"
  },
  {
    "text": "AUDIENCE: Uh, no [INAUDIBLE] PHILIPPE RIGOLLET: So this\nis just all the things that we know. Theta is just the, by\ndefinition of the fact that mu",
    "start": "4380080",
    "end": "4389150"
  },
  {
    "text": "is b prime of theta, the\nmean is b prime of theta-- it means that theta is\nb prime inverse of mu.",
    "start": "4389150",
    "end": "4394250"
  },
  {
    "text": "And then, mu is modeled from\nthe systematic component. G of mu is xi transpose\nbeta, so this is",
    "start": "4394250",
    "end": "4401940"
  },
  {
    "text": "g inverse of xi transpose beta. So I want to have b prime\ninverse of g inverse.",
    "start": "4401940",
    "end": "4407810"
  },
  {
    "text": "This function is a\nbit annoying to say, so I'm just going to call it h. And when I do the\ncomposition of two inverses,",
    "start": "4407810",
    "end": "4414837"
  },
  {
    "text": "the inverse of the composition\nof those two things in the reverse order-- so h is really the inverse\nof g, composed with b",
    "start": "4414837",
    "end": "4422139"
  },
  {
    "text": "prime, g of b prime inverse. And now, if I have\nthe canonical link,",
    "start": "4422140",
    "end": "4428260"
  },
  {
    "text": "since I know that g\nis b prime inverse, this is really\njust the identity.",
    "start": "4428260",
    "end": "4434180"
  },
  {
    "text": "As you can imagine,\nthis entire thing, which is actually\nquite complicated--",
    "start": "4434180",
    "end": "4439650"
  },
  {
    "text": "would just say oh, this thing,\nactually, does not show up when I have the canonical link. I really just have that theta\ncan be replaced by xi of beta.",
    "start": "4439650",
    "end": "4446370"
  },
  {
    "text": "So think about going\nback to this guy here. Now, theta becomes\nonly xi transpose beta.",
    "start": "4446370",
    "end": "4455159"
  },
  {
    "text": "That's going to be much\nmore simple to optimize, because remember, when I'm\ngoing to log likelihood,",
    "start": "4455160",
    "end": "4460550"
  },
  {
    "text": "this thing is going to go away. I'm going to sum those guys. And so what I'm going to\nhave is something which is essentially linear in beta.",
    "start": "4460550",
    "end": "4466140"
  },
  {
    "text": "And then, I'm going\nto have this minus b, which is just minus the sum\nof convex functions of beta.",
    "start": "4466140",
    "end": "4471760"
  },
  {
    "text": "And so I'm going to have to\nbring in the tools of convex optimization. Now, it's not just going to be\ntake the gradient, set it to 0.",
    "start": "4471760",
    "end": "4477566"
  },
  {
    "text": "It's going to be more\ncomplicated to do that. I'm going to have to do that\nin an iterative fashion. And so that's what\nI'm telling you,",
    "start": "4477566",
    "end": "4483800"
  },
  {
    "text": "when you look at your\nlog likelihood for all those functions. You sum, the exponential goes\naway because you had the log,",
    "start": "4483800",
    "end": "4490062"
  },
  {
    "text": "and then, you have\nall these things here. I kept the b. I kept the h. But if h is the identity,\nthis is the linear function,",
    "start": "4490062",
    "end": "4496690"
  },
  {
    "text": "the linear part, yi\ntimes xi transpose beta, minus b of my theta, which\nis now only xi transpose beta.",
    "start": "4496690",
    "end": "4503776"
  },
  {
    "text": "And that's the function I\nwant to maximize in beta. ",
    "start": "4503776",
    "end": "4510370"
  },
  {
    "text": "It's a convex function. When I know what b is, I have\nan explicit formula for this, and I want to just bring\nin some optimization.",
    "start": "4510370",
    "end": "4518230"
  },
  {
    "text": "And that's what\nwe're going to do, and we're going to see three\ndifferent methods, which are really, basically,\nthe same method.",
    "start": "4518230",
    "end": "4524110"
  },
  {
    "text": "It's just an adaptation\nor specialization of the so-called Newton-Raphson\nmethod, which is essentially",
    "start": "4524110",
    "end": "4531550"
  },
  {
    "text": "telling you do iterative\nlocal quadratic approximations through your function--\nso second order [INAUDIBLE] expansion,\nminimize this guy,",
    "start": "4531550",
    "end": "4538480"
  },
  {
    "text": "and then do it again\nfrom where you were. And we'll see that\nthis can be, actually, implemented using what's called\niteratively re-weighted least",
    "start": "4538480",
    "end": "4547210"
  },
  {
    "text": "squares, which means\nthat every step-- since it's just\na quadratic, it's going to be just\nsquares in there--",
    "start": "4547210",
    "end": "4553090"
  },
  {
    "text": "can actually be solved\nby using a weighted least squares version of the problem.",
    "start": "4553090",
    "end": "4559420"
  },
  {
    "text": "So I'm going to\nstop here for today. So we'll continue and probably\nnot finish this chapter,",
    "start": "4559420",
    "end": "4565930"
  },
  {
    "text": "but finish next week. And then, I think\nthere's only one lecture. Actually, for the last lecture,\nwhat do you guys want to do?",
    "start": "4565930",
    "end": "4573310"
  },
  {
    "text": " Do you want to have\ndoughnuts and cider?",
    "start": "4573310",
    "end": "4578460"
  },
  {
    "text": "Do you want to just have\nsome more outlooking lecture",
    "start": "4578460",
    "end": "4585620"
  },
  {
    "text": "on what's happening\npost 1975 in statistics?",
    "start": "4585620",
    "end": "4591390"
  },
  {
    "text": "Do you want to have a\nreview for the final exam-- pragmatic people.",
    "start": "4591390",
    "end": "4598969"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\ninteresting, advanced topics. PHILIPPE RIGOLLET: You want\nto do interesting, advanced--",
    "start": "4598970",
    "end": "4606099"
  },
  {
    "text": "for the last lecture? AUDIENCE: Something that\nwe haven't thought of yet. PHILIPPE RIGOLLET: Yeah, that's,\nbasically, what I'm asking,",
    "start": "4606100",
    "end": "4613920"
  },
  {
    "text": "right-- interesting\nadvanced topics, versus ask me any\nquestion you want.",
    "start": "4613920",
    "end": "4620694"
  },
  {
    "text": "Those questions can be about\ninteresting, advanced topics, though. Like, what are interesting,\nadvanced topics?",
    "start": "4620694",
    "end": "4626020"
  },
  {
    "text": "I'm sorry? AUDIENCE: Interesting with\ndoughnuts-- is that OK? PHILIPPE RIGOLLET: Yeah, we\ncan always do the doughnuts. [LAUGHTER]",
    "start": "4626020",
    "end": "4631838"
  },
  {
    "text": "AUDIENCE: As long as\nthere are doughnuts. PHILIPPE RIGOLLET: All\nright, so we'll do that. So you guys have a good weekend.",
    "start": "4631838",
    "end": "4639500"
  }
]