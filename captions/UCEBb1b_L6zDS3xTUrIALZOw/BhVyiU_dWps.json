[
  {
    "text": " The following\ncontent is provided under a Creative\nCommons license. Your support will help MIT\nOpenCourseWare continue",
    "start": "0",
    "end": "6870"
  },
  {
    "text": "to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6870",
    "end": "13330"
  },
  {
    "text": "from hundreds of\nMIT courses, visit MIT OpenCourseWare\nat ocw.mit.edu",
    "start": "13330",
    "end": "20720"
  },
  {
    "text": "PROFESSOR: So we started by\ntalking about thermodynamics. And then switched off to\ntalking about probability.",
    "start": "20720",
    "end": "28500"
  },
  {
    "text": "And you may well ask, what's\nthe connection between these? And we will eventually try\nto build that connection",
    "start": "28500",
    "end": "35030"
  },
  {
    "text": "through statistical physics. And maybe this lecture\ntoday will sort of provide you with why these\nelements of probability",
    "start": "35030",
    "end": "44670"
  },
  {
    "text": "are important and essential\nto making this bridge. So last time, I started with\ntalking about the Central Limit",
    "start": "44670",
    "end": "51920"
  },
  {
    "text": "Theorem which pertains to\nadding lots of variables",
    "start": "51920",
    "end": "62949"
  },
  {
    "text": "together to form a sum. And the control parameter\nthat we will use",
    "start": "62950",
    "end": "70100"
  },
  {
    "text": "is this number of\nterms in the sum. ",
    "start": "70100",
    "end": "75170"
  },
  {
    "text": "So in principle,\nthere's a joint PDF",
    "start": "75170",
    "end": "81189"
  },
  {
    "text": "that determines how these\nvariables are distributed. ",
    "start": "81190",
    "end": "89380"
  },
  {
    "text": "And using that, we can calculate\nvarious characteristics",
    "start": "89380",
    "end": "94600"
  },
  {
    "text": "of this sum. If I were to raise the\nsum to some power m,",
    "start": "94600",
    "end": "101310"
  },
  {
    "text": "I could do that by doing a sum\nover i running from let's say",
    "start": "101310",
    "end": "106840"
  },
  {
    "text": "i1 running from 1 to N, i2\nrunning from-- im running",
    "start": "106840",
    "end": "115780"
  },
  {
    "text": "from 1 to N, so basically\nspeaking this sum. And then I have x of\ni1, x of i2, x of im.",
    "start": "115780",
    "end": "125880"
  },
  {
    "text": "So basically I multiplied m\ncopies of the original sum together.",
    "start": "125880",
    "end": "131690"
  },
  {
    "text": "And if I were to calculate\nsome moment of this,",
    "start": "131690",
    "end": "137560"
  },
  {
    "text": "basically the moment of a sum\nis the sum of the moments. I could do this.",
    "start": "137560",
    "end": "144319"
  },
  {
    "text": "Now the last thing\nthat we did last time was to look at some\ncharacteristic function",
    "start": "144320",
    "end": "150050"
  },
  {
    "text": "for the sum related to the\ncharacteristic function of this joint\nprobability distribution,",
    "start": "150050",
    "end": "156210"
  },
  {
    "text": "and conclude that actually\nexactly the same relation holds if I were to put index\nc for a cumulant.",
    "start": "156210",
    "end": "164020"
  },
  {
    "text": "And that is basically, say the\nmean is the sum of the means, the variance is sum of\nall possible variances",
    "start": "164020",
    "end": "171190"
  },
  {
    "text": "and covariances. And this holds to all orders.",
    "start": "171190",
    "end": "176390"
  },
  {
    "text": "OK? Fine. So where do we go from here? We are going to gradually\nsimplify the problem in order",
    "start": "176390",
    "end": "184370"
  },
  {
    "text": "to get some final\nresult that we want. But that result eventually\nis a little bit more general",
    "start": "184370",
    "end": "191780"
  },
  {
    "text": "than the simplification. The first simplification\nthat we do is to look at\nindependent variables.",
    "start": "191780",
    "end": "197780"
  },
  {
    "start": "197780",
    "end": "202870"
  },
  {
    "text": "And what happened when we\nhad the independent variables was that the\nprobability distribution",
    "start": "202870",
    "end": "208700"
  },
  {
    "text": "could be written as the product\nof probability distributions pertaining to different ones.",
    "start": "208700",
    "end": "213990"
  },
  {
    "text": "I would have a p1 acting\non x1, a p2 acting on x2,",
    "start": "213990",
    "end": "221290"
  },
  {
    "text": "a pn acting on the xn. ",
    "start": "221290",
    "end": "229430"
  },
  {
    "text": "Now, when we did that,\nwe saw that actually one of the conditions that would\nthen follow from this if we",
    "start": "229430",
    "end": "237185"
  },
  {
    "text": "were to Fourier transform and\nthen try to expand in powers of k, is we would never get in\nthe expansion of the log terms",
    "start": "237185",
    "end": "244159"
  },
  {
    "text": "that were coupling\ndifferent k's. Essentially all of the joint\ncumulants involving things",
    "start": "244160",
    "end": "253190"
  },
  {
    "text": "other than one variable\nby itself would vanish. So essentially in that\nlimit, the only terms",
    "start": "253190",
    "end": "261208"
  },
  {
    "text": "in this that would\nsurvive we're the ones in which all of the\nindices were the same.",
    "start": "261209",
    "end": "267270"
  },
  {
    "text": "So basically in that\ncase, I would write this as a sum i running from one\nto N, xi to the power of N.",
    "start": "267270",
    "end": "282830"
  },
  {
    "text": "So basically for\nindependent variables, let's say, the variance is\nthe sum of the variances,",
    "start": "282830",
    "end": "290010"
  },
  {
    "text": "the third cumulant is the sum of\nthe third cumulants, et cetera. ",
    "start": "290010",
    "end": "296050"
  },
  {
    "text": "One more simplification. Again not necessary\nfor the final thing",
    "start": "296050",
    "end": "302139"
  },
  {
    "text": "that we want to have in mind. But let's just assume\nthat all of these are identically distributed.",
    "start": "302140",
    "end": "312959"
  },
  {
    "text": " By that I mean that this is\nbasically the same probability",
    "start": "312959",
    "end": "320920"
  },
  {
    "text": "that I would use for\neach one of them. So this I could write\nas a product over i one",
    "start": "320920",
    "end": "327350"
  },
  {
    "text": "to N, the same p for each xi.",
    "start": "327350",
    "end": "332360"
  },
  {
    "text": "Just to make sure you sum\nnotation that you may see every",
    "start": "332360",
    "end": "341189"
  },
  {
    "text": "now and then, variables that\nare independent and identically",
    "start": "341190",
    "end": "346660"
  },
  {
    "text": "distributed are\nsometimes called IID's. ",
    "start": "346660",
    "end": "353849"
  },
  {
    "text": "And if I focus my attention\nto these IID's, then",
    "start": "353850",
    "end": "360010"
  },
  {
    "text": "all of these things are\nclearly the same thing. And the answer would be\nsimply N times the cumulant",
    "start": "360010",
    "end": "368160"
  },
  {
    "text": "that I would have\nfor one of them. This-- actually some\nversion of this,",
    "start": "368160",
    "end": "374310"
  },
  {
    "text": "we already saw for the\nbinomial distribution in which the same\ncoin, let's say,",
    "start": "374310",
    "end": "379340"
  },
  {
    "text": "was thrown N independent times. And all of the cumulants for\nthe sum of the number of heads,",
    "start": "379340",
    "end": "387000"
  },
  {
    "text": "let's say, were related to\nthe cumulants in one trial that you would get.",
    "start": "387000",
    "end": "393520"
  },
  {
    "text": "OK? So fine. Nothing so far here.",
    "start": "393520",
    "end": "400180"
  },
  {
    "text": "However let's imagine now that\nI construct a variable that I will call y, which\nis the variable",
    "start": "400180",
    "end": "408170"
  },
  {
    "text": "x, this sum that I have. From it I subtract\nN times the mean,",
    "start": "408170",
    "end": "420130"
  },
  {
    "text": "and then I divide\nby square root of N.",
    "start": "420130",
    "end": "427925"
  },
  {
    "text": "I can certainly choose to do so. Then what we observe here\nis that the average of y",
    "start": "427925",
    "end": "436620"
  },
  {
    "text": "by this construction is 0. Because essentially, I make\nsure that the average of x",
    "start": "436620",
    "end": "442530"
  },
  {
    "text": "is subtracted. No problem. Average of y squared--\nnot average of y squared,",
    "start": "442530",
    "end": "451350"
  },
  {
    "text": "but the variance. Surely it's easy to show\nthe variance doesn't really",
    "start": "451350",
    "end": "456860"
  },
  {
    "text": "depend on the subtraction. It is the same thing\nas the variance of x.",
    "start": "456860",
    "end": "463750"
  },
  {
    "text": "So it is going to\nbe essentially x squared c divided\nby square of this.",
    "start": "463750",
    "end": "471479"
  },
  {
    "text": "So I will have N.\nAnd x squared, big x",
    "start": "471480",
    "end": "476720"
  },
  {
    "text": "squared cumulant,\naccording to this rule, is N times small x\nsquared cumulant.",
    "start": "476720",
    "end": "481850"
  },
  {
    "text": "And I get something like this. Still nothing interesting.",
    "start": "481850",
    "end": "487930"
  },
  {
    "text": "But now let's look\nat the m-th cumulant. So let's look at y m c for\nm that is greater than 2.",
    "start": "487930",
    "end": "500650"
  },
  {
    "text": "And then what do I get? I will get to N times x m c\ndivided by N to the m over 2.",
    "start": "500650",
    "end": "512340"
  },
  {
    "text": "The N to the power\nof m over 2 just came from raising this\nto the power of m,",
    "start": "512340",
    "end": "518400"
  },
  {
    "text": "since I'm looking at y to the m. And x to the m c, according\nto this, is N times x1.",
    "start": "518400",
    "end": "526720"
  },
  {
    "text": " Now we see that\nthis is something that is proportional to the N to\nthe power of 1 minus m over 2.",
    "start": "526720",
    "end": "535600"
  },
  {
    "text": "And since I chose m\nto be greater than 2, in the limit that N becomes\nmuch, much larger than 1,",
    "start": "535600",
    "end": "544260"
  },
  {
    "text": "this goes to 0.  So if I look at the limit where\nthe number of terms in the sum",
    "start": "544260",
    "end": "553120"
  },
  {
    "text": "is much larger than\n1, what I conclude is that the probability\ndistribution for this variable",
    "start": "553120",
    "end": "559920"
  },
  {
    "text": "that I have constructed has\n0 mean, a finite variance,",
    "start": "559920",
    "end": "565170"
  },
  {
    "text": "and all the other\nhigher order cumulants are asymptotically vanishing. So I know that the\nprobability of y,",
    "start": "565170",
    "end": "574630"
  },
  {
    "text": "which is this variable that\nI have given you up there, is given by the one distribution\nthat we know is completely",
    "start": "574630",
    "end": "581570"
  },
  {
    "text": "characterized by its first\nand second cumulant, which is the Gaussian. So it is exponential of minus y\nsquared, two times its variance",
    "start": "581570",
    "end": "591220"
  },
  {
    "text": "divided, appropriately\nnormalized. ",
    "start": "591220",
    "end": "603520"
  },
  {
    "text": "Essentially this sum is\nGaussian distributed. ",
    "start": "603520",
    "end": "610029"
  },
  {
    "text": "And this result\nis true for things",
    "start": "610030",
    "end": "615660"
  },
  {
    "text": "that are not IID's so long\nas this sum i1 to im, one",
    "start": "615660",
    "end": "634829"
  },
  {
    "text": "to N, xi1 to xim goes as\nN goes to infinity, much,",
    "start": "634830",
    "end": "645670"
  },
  {
    "text": "much less than 1,\nas long as it is less than-- less than strictly\nthan N to the m over 2.",
    "start": "645670",
    "end": "654165"
  },
  {
    "text": " So basically, what\nI want to do is",
    "start": "654165",
    "end": "660870"
  },
  {
    "text": "to ensure that when I\nconstruct the analog of this, I would have something that when\nI divide by N to the m over 2,",
    "start": "660870",
    "end": "669379"
  },
  {
    "text": "I will asymptotically go to 0. So in the case of IID's,\nthe numerator goes like N,",
    "start": "669380",
    "end": "676300"
  },
  {
    "text": "it could be that I\nhave correlations among the variables et\ncetera, so that there",
    "start": "676300",
    "end": "681520"
  },
  {
    "text": "are other terms in the sum\nbecause of the correlations as long as the sum total\nof them asymptotically",
    "start": "681520",
    "end": "688770"
  },
  {
    "text": "grows less than N\nto the m over 2, this statement that\nthe sum is Gaussian",
    "start": "688770",
    "end": "694580"
  },
  {
    "text": "distributed it is\ngoing to be valid. Yes. AUDIENCE: Question--\nhow can you compare",
    "start": "694580",
    "end": "700536"
  },
  {
    "text": "a value of [INAUDIBLE] with\nnumber of variables that you",
    "start": "700536",
    "end": "706497"
  },
  {
    "text": "[INAUDIBLE]? Because this is\na-- just, if, say, your random value is\nset [? in advance-- ?]",
    "start": "706497",
    "end": "713305"
  },
  {
    "text": "PROFESSOR: So basically,\nyou choose a probability distribution-- at least in\nthis case, it is obvious.",
    "start": "713305",
    "end": "721320"
  },
  {
    "text": "In this case, basically\nwhat we want to know is that there is a\nprobability distribution for individual variables.",
    "start": "721320",
    "end": "728310"
  },
  {
    "text": "And I repeat it\nmany, many times. So it is like the coin.",
    "start": "728310",
    "end": "733450"
  },
  {
    "text": "So for the coin I\nwill ensure that I will throw it hundreds of times. Now suppose that for\nsome reason, if I throw",
    "start": "733450",
    "end": "740510"
  },
  {
    "text": "the coin once, the\nnext five times it is much more likely to be the\nsame thing that I had before.",
    "start": "740510",
    "end": "747480"
  },
  {
    "text": "Kind of some strange\ncoin, or whatever. Then there is some\ncorrelation up to five.",
    "start": "747480",
    "end": "753430"
  },
  {
    "text": "So when I'm calculating\nthings up to five, there all kinds of\nresults over here.",
    "start": "753430",
    "end": "758670"
  },
  {
    "text": "But as long as that's five\nis independent of the length of the sequence, if I throw\nthings 1,000 times, still only",
    "start": "758670",
    "end": "766959"
  },
  {
    "text": "groups of five that\nare correlated, then this result still holds. Because I have the additional\nparameter N to play with.",
    "start": "766960",
    "end": "774450"
  },
  {
    "text": "So I want to have a\nparameter N to play with to go to infinity which\nis independent of what",
    "start": "774450",
    "end": "781840"
  },
  {
    "text": "characterizes the\ndistribution of my variable. AUDIENCE: I was mainly\nconcerned with the fact that you compare\nthe cumulant which",
    "start": "781840",
    "end": "790222"
  },
  {
    "text": "has the same dimension\nas your random variable. So if my random variable is--\nI measure length or something.",
    "start": "790222",
    "end": "797620"
  },
  {
    "text": "I do it many, many times\nlength is measured in meters,",
    "start": "797620",
    "end": "803365"
  },
  {
    "text": "and you try to compare it\nto a number of measurements. So, shouldn't there be\nsome dimensionful constant",
    "start": "803365",
    "end": "809662"
  },
  {
    "text": "on the right? PROFESSOR: So\nhere, this quantity has dimensions of\nmeter to m-th power,",
    "start": "809662",
    "end": "816949"
  },
  {
    "text": "this quantity has dimensions\nof meter to the m-th power. This quantity is dimensionless.",
    "start": "816950",
    "end": "823630"
  },
  {
    "text": "Right? So what I want is\nthe N dependence to be such that when I go\nto large N, it goes to 0.",
    "start": "823630",
    "end": "831670"
  },
  {
    "text": "It is true that this\nis still multiplying something that has-- so it is.",
    "start": "831670",
    "end": "838073"
  },
  {
    "text": "AUDIENCE: It's like less than\nsomething of order of N to m/2? OK.",
    "start": "838073",
    "end": "843170"
  },
  {
    "text": "PROFESSOR: Oh this\nis what you-- order. Thank you. ",
    "start": "843170",
    "end": "853631"
  },
  {
    "text": "AUDIENCE: The last time\n[INAUDIBLE] cumulant",
    "start": "853631",
    "end": "859547"
  },
  {
    "text": "[INAUDIBLE]? PROFESSOR: Yes, thank you. ",
    "start": "859547",
    "end": "867800"
  },
  {
    "text": "Any other correction,\nclarification? ",
    "start": "867800",
    "end": "873270"
  },
  {
    "text": "OK. So again but we will\nsee that essentially in statistical\nphysics, we will have,",
    "start": "873270",
    "end": "880529"
  },
  {
    "text": "always, to deal with\nsome analog of this N, like the part number of\nmolecules of gas in this room,",
    "start": "880530",
    "end": "887089"
  },
  {
    "text": "et cetera, that enables us\nto use something like this.",
    "start": "887090",
    "end": "894370"
  },
  {
    "text": "I mean, it is clear\nthat in this case, I chose to subtract the mean\nand divide by N to the 1/2.",
    "start": "894370",
    "end": "902450"
  },
  {
    "text": "But suppose I didn't have\nthe division by N to the 1/2.",
    "start": "902450",
    "end": "908020"
  },
  {
    "text": "Then what happens is that I\ncould have divided for example by N. Then my\ndistribution for something",
    "start": "908020",
    "end": "916490"
  },
  {
    "text": "that has a well-defined,\nindependent mean would have gone to something\nlike a delta function",
    "start": "916490",
    "end": "924540"
  },
  {
    "text": "in the limit of N\ngoing to infinity. But I kind of sort\nof change my scale",
    "start": "924540",
    "end": "931210"
  },
  {
    "text": "by dividing by N to\nthe 1/2 rather than N to sort of emphasize that\nthe scale of fluctuations",
    "start": "931210",
    "end": "937940"
  },
  {
    "text": "is of the order of\nsquare root of N. This is again something\nthat generically happens.",
    "start": "937940",
    "end": "944120"
  },
  {
    "text": "So let's say, we know the\nenergy of the gas in this room to be proportional to\nvolume or whatever.",
    "start": "944120",
    "end": "950810"
  },
  {
    "text": "The amount of\nuncertainty that we have will be of the order of\nsquare root of volume.",
    "start": "950810",
    "end": "956830"
  },
  {
    "text": "So it's clear that we\nare kind of building results that have to do with\ndependencies on N. So let's",
    "start": "956830",
    "end": "965580"
  },
  {
    "text": "sort of look at some other\nthings that happen when we are",
    "start": "965580",
    "end": "976010"
  },
  {
    "text": "dealing with large number\nof degrees of freedom. So already we've\nspoken about things",
    "start": "976010",
    "end": "984520"
  },
  {
    "text": "that intensive, variables such\nas temperature, pressure, et",
    "start": "984520",
    "end": "994240"
  },
  {
    "text": "cetera. And their characteristic\nis that if we express them",
    "start": "994240",
    "end": "1000020"
  },
  {
    "text": "in terms of, say, the\nnumber of constituents, they are independent\nof that number.",
    "start": "1000020",
    "end": "1008689"
  },
  {
    "text": "As opposed to extensive\nquantities, such as the energy",
    "start": "1008690",
    "end": "1014490"
  },
  {
    "text": "or the volume, et cetera,\nthat are proportional to this.",
    "start": "1014490",
    "end": "1022060"
  },
  {
    "text": "We can certainly\nimagine things that would increase [INAUDIBLE]\nthe polynomial, order of N",
    "start": "1022060",
    "end": "1033079"
  },
  {
    "text": "to some power. If I have N molecules\nof gas, and I",
    "start": "1033079",
    "end": "1038150"
  },
  {
    "text": "ask how many pairs of\ninteractions I have, you would say it's N, N\nminus 1 over 2, for example.",
    "start": "1038150",
    "end": "1044560"
  },
  {
    "text": "That would be\nsomething like this. But most importantly, when we\ndeal with statistical physics,",
    "start": "1044560",
    "end": "1051540"
  },
  {
    "text": "we will encounter\nquantities that have exponential dependence. That is, they will\nbe something like e",
    "start": "1051540",
    "end": "1058670"
  },
  {
    "text": "to the N with some something\nthat will appear after.",
    "start": "1058670",
    "end": "1064280"
  },
  {
    "text": "An example of that is when we\nwere, for example, calculating the phase space\nof gas particles.",
    "start": "1064280",
    "end": "1071390"
  },
  {
    "text": "A gas particle by itself can\nbe in a volume V. Two of them, jointly, can occupy\na volume V squared.",
    "start": "1071390",
    "end": "1078880"
  },
  {
    "text": "Three of them, V\ncubed, et cetera. Eventually you hit V to\nthe N for N particles.",
    "start": "1078880",
    "end": "1084370"
  },
  {
    "text": "So that's a kind of\nexponential dependence. So this is e g V\nto the N that you",
    "start": "1084370",
    "end": "1091740"
  },
  {
    "text": "would have for joined\nvolume of N particles.",
    "start": "1091740",
    "end": "1097890"
  },
  {
    "text": "OK?  So some curious\nthings happen when",
    "start": "1097890",
    "end": "1103900"
  },
  {
    "text": "you have these\nkinds of variables. And one thing that\nyou may not realize",
    "start": "1103900",
    "end": "1114120"
  },
  {
    "text": "is what happens when you\nsumming exponentials. ",
    "start": "1114120",
    "end": "1122250"
  },
  {
    "text": "So let's imagine that\nI have a sum composed of a number of terms i running\nfrom one to script N-- script n",
    "start": "1122250",
    "end": "1132860"
  },
  {
    "text": "is the number of terms\nin the sum-- that are of these exponential types.",
    "start": "1132860",
    "end": "1138500"
  },
  {
    "text": "So let's actually sometimes I\nwill call this-- never mind. So let's call these\ne to the N phi--",
    "start": "1138500",
    "end": "1148260"
  },
  {
    "text": "Let me write it in this fashion. Epsilon i where epsilon i\nsatisfies two conditions.",
    "start": "1148260",
    "end": "1161010"
  },
  {
    "text": "One of them, it is positive. And the other is\nthat it has this kind",
    "start": "1161010",
    "end": "1166520"
  },
  {
    "text": "of exponential dependence. It is order of e to\nthe N phi i where",
    "start": "1166520",
    "end": "1173770"
  },
  {
    "text": "there could be some prefactor or\nsomething else in front to give you dimension and stuff like\nthat that you were discussing.",
    "start": "1173770",
    "end": "1179995"
  },
  {
    "text": " I assume that the\nnumber of terms",
    "start": "1179995",
    "end": "1186799"
  },
  {
    "text": "is less than or of the\norder of some polynomial. ",
    "start": "1186800",
    "end": "1193669"
  },
  {
    "text": "OK? ",
    "start": "1193670",
    "end": "1199300"
  },
  {
    "text": "Then my claim is\nthat, in some sense,",
    "start": "1199300",
    "end": "1205910"
  },
  {
    "text": "the sum S is the largest term.",
    "start": "1205910",
    "end": "1211140"
  },
  {
    "text": " OK? ",
    "start": "1211140",
    "end": "1218350"
  },
  {
    "text": "So let's sort of put\nthis graphically. What I'm telling you is that\nwe have a whole bunch of terms",
    "start": "1218350",
    "end": "1227370"
  },
  {
    "text": "that are these epsilons i's. They're all positive, so I\ncan sort of indicate them",
    "start": "1227370",
    "end": "1232830"
  },
  {
    "text": "by bars of different lengths\nthat are positive and so forth.",
    "start": "1232830",
    "end": "1240789"
  },
  {
    "text": "So let's say this is epsilon\n1, epsilon 2 all the way to epsilon N. And let's say\nthat this guy is the largest.",
    "start": "1240790",
    "end": "1247730"
  },
  {
    "text": " And my task is to add up the\nlength of all of these things.",
    "start": "1247730",
    "end": "1258110"
  },
  {
    "text": "So how do I claim that the\nlength is just the largest one. It's in the following sense.",
    "start": "1258110",
    "end": "1264590"
  },
  {
    "text": "You would agree that\nthis sum you say is certainly larger\nthan the largest term,",
    "start": "1264590",
    "end": "1271700"
  },
  {
    "text": "because I have added\nlots of other things to the largest term, and\nthey are all positive.",
    "start": "1271700",
    "end": "1278030"
  },
  {
    "text": "I say, fine, what\nI'm going to do is I'm going to raise the\nlength of everybody else",
    "start": "1278030",
    "end": "1284600"
  },
  {
    "text": "to be the same thing\nas epsilon max. ",
    "start": "1284600",
    "end": "1291360"
  },
  {
    "text": "And then I would say\nthat the sum is certainly less than this artificial sum\nwhere I have raised everybody",
    "start": "1291360",
    "end": "1302420"
  },
  {
    "text": "to epsilon max. OK?",
    "start": "1302420",
    "end": "1307510"
  },
  {
    "text": "So then what I will do is I will\ntake log off this expression, and it will be bounded by log\nof epsilon max and log of N",
    "start": "1307510",
    "end": "1318040"
  },
  {
    "text": "epsilon max, which is the same\nthing as log of epsilon max plus log of N. And\nthen I divide by N.",
    "start": "1318040",
    "end": "1337240"
  },
  {
    "text": "And then note that the\nconditions that I have set up are such that in the limit\nthat N goes to infinity,",
    "start": "1337240",
    "end": "1348800"
  },
  {
    "text": "script N would be\nP log N over N. And the limit of this as N\nbecomes much less than 1 is 0.",
    "start": "1348800",
    "end": "1359670"
  },
  {
    "text": "Log N over N goes to 0\nas N goes to infinity. So basically this sum\nis bounded on both sides",
    "start": "1359670",
    "end": "1368149"
  },
  {
    "text": "by the same thing. So what we've established\nis that essentially log of S",
    "start": "1368150",
    "end": "1373210"
  },
  {
    "text": "over N, its limit as\nN goes to infinity, is the same thing as a\nlog of epsilon max over N,",
    "start": "1373210",
    "end": "1385140"
  },
  {
    "text": "which is what? If I say my epsilon max's have\nthis exponential dependence,",
    "start": "1385140",
    "end": "1390700"
  },
  {
    "text": "is phi max.  And actually this is again\nthe reason for something",
    "start": "1390700",
    "end": "1398190"
  },
  {
    "text": "that you probably have seen. That using statistical\nphysics let's",
    "start": "1398190",
    "end": "1403610"
  },
  {
    "text": "say a micro-canonical\nensemble when you say exactly\nwhat the energy is. Or you look at the\ncanonical ensemble",
    "start": "1403610",
    "end": "1410450"
  },
  {
    "text": "where the energy can\nbe all over the place, why do you get the same result? This is why.",
    "start": "1410450",
    "end": "1415610"
  },
  {
    "text": " Any questions on this?",
    "start": "1415610",
    "end": "1423360"
  },
  {
    "text": "Everybody's happy, obviously. Good. AUDIENCE: [INAUDIBLE]\na question?",
    "start": "1423360",
    "end": "1428700"
  },
  {
    "text": "PROFESSOR: Yes. AUDIENCE: The N on\nthe end, [INAUDIBLE]? PROFESSOR: There's a script N,\nwhich is the number of terms.",
    "start": "1428700",
    "end": "1434660"
  },
  {
    "text": "And there's the Roman N,\nwhich is the parameter that is the analog of the number\nof degrees of freedom.",
    "start": "1434660",
    "end": "1441659"
  },
  {
    "text": "The one that we usually\ndeal in statistical physics would be, say, the\nnumber of particles. AUDIENCE: So number of\nmeasurements [INAUDIBLE] number",
    "start": "1441660",
    "end": "1448832"
  },
  {
    "text": "of particles. PROFESSOR: Number\nof measurements? AUDIENCE: So the\nscript N is what?",
    "start": "1448832",
    "end": "1454755"
  },
  {
    "text": "PROFESSOR: The script N\ncould be, for example, I'm summing over all\npairs of interactions.",
    "start": "1454755",
    "end": "1460380"
  },
  {
    "text": "So the number of pairs\nwould go like N squared. Now in reality\npracticality in all cases",
    "start": "1460380",
    "end": "1466480"
  },
  {
    "text": "that you will deal with,\nthis P would be one. So the number of terms\nthat we would be dealing",
    "start": "1466480",
    "end": "1472640"
  },
  {
    "text": "would be of the order of the\nnumber of degrees of freedom. So, we will see some\nexamples of that later on.",
    "start": "1472640",
    "end": "1483002"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nscript N might be N squared? PROFESSOR: If I'm forced\nto come up with a situation",
    "start": "1483002",
    "end": "1489520"
  },
  {
    "text": "where script N is\nN squared, I would say count the number of pairs. ",
    "start": "1489520",
    "end": "1497810"
  },
  {
    "text": "Number of pairs if\nI have N [? sides ?] is N, N minus 1 over 2.",
    "start": "1497810",
    "end": "1502880"
  },
  {
    "text": "So this is something that\ngoes like N squared over 2. Can I come up with\na physical situation",
    "start": "1502880",
    "end": "1509230"
  },
  {
    "text": "where I'm summing over\nthe number of terms? Not obviously, but it could\nbe something like that.",
    "start": "1509230",
    "end": "1514890"
  },
  {
    "text": "The situations in statistical\nphysics that we come up with is typically, let's say, in\ngoing from the micro-canonical",
    "start": "1514890",
    "end": "1521500"
  },
  {
    "text": "to the canonical\nensemble, you would be summing over energy levels.",
    "start": "1521500",
    "end": "1526820"
  },
  {
    "text": "And typically, let's\nsay, in a system that is bounded the\nnumber of energy levels is proportional to the\nnumber of particles.",
    "start": "1526820",
    "end": "1535350"
  },
  {
    "start": "1535350",
    "end": "1544539"
  },
  {
    "text": "Now there cases that actually,\nin going from micro-canonical to canonical, like the energy\nof the gas in this room,",
    "start": "1544540",
    "end": "1553020"
  },
  {
    "text": "the energy axis goes all\nthe way from 0 to infinity. So there is a continuous version\nof the summation procedure",
    "start": "1553020",
    "end": "1562200"
  },
  {
    "text": "that we have that is\nthen usually applied which is in mathematics\nis called the saddle point",
    "start": "1562200",
    "end": "1570846"
  },
  {
    "text": "integration. ",
    "start": "1570846",
    "end": "1578679"
  },
  {
    "text": "So basically there, rather\nthan having to deal with a sum,",
    "start": "1578680",
    "end": "1583810"
  },
  {
    "text": "I deal with an integral.  The integration is over\nsome variable, let's say x.",
    "start": "1583810",
    "end": "1593930"
  },
  {
    "text": "Could be energy, whatever. And then I have a quantity that\nhas this exponential character.",
    "start": "1593930",
    "end": "1602120"
  },
  {
    "start": "1602120",
    "end": "1607780"
  },
  {
    "text": "And then again, in\nsome specific sense, I can just look at the largest\nvalue and replace this with e",
    "start": "1607780",
    "end": "1617250"
  },
  {
    "text": "to the N phi evaluated at x max. I should really write\nthis as a proportionality,",
    "start": "1617250",
    "end": "1623221"
  },
  {
    "text": "but we'll see what\nthat means shortly.",
    "start": "1623221",
    "end": "1628490"
  },
  {
    "text": "So basically it's\nthe above picture,",
    "start": "1628490",
    "end": "1633740"
  },
  {
    "text": "I have a continuous variable. And this continuous\nvariable, let's",
    "start": "1633740",
    "end": "1642770"
  },
  {
    "text": "say I have to sum a quantity\nthat is e to the N phi. So maybe I will have to\nnot sum, but integrate over",
    "start": "1642770",
    "end": "1652375"
  },
  {
    "text": "a function such as this. And let's say this is the\nplace where the maximums occur.",
    "start": "1652375",
    "end": "1658280"
  },
  {
    "text": " So the procedure\nof saddle point is",
    "start": "1658280",
    "end": "1666620"
  },
  {
    "text": "to expand phi\naround its maximum.",
    "start": "1666620",
    "end": "1676285"
  },
  {
    "text": " And then I can write i as an\nintegral over x, exponential",
    "start": "1676285",
    "end": "1687690"
  },
  {
    "text": "of N, phi evaluated\nat the maximum.",
    "start": "1687690",
    "end": "1696039"
  },
  {
    "text": "Now if I'm doing\na Taylor series, then next term in\nthe Taylor series typically would involve\nthe first derivative.",
    "start": "1696040",
    "end": "1703000"
  },
  {
    "text": "But around the maximum,\nthe first derivative is 0. Again if it is a maximum,\nthe second derivative phi",
    "start": "1703000",
    "end": "1711960"
  },
  {
    "text": "double prime evaluated at\nthis xm, would be negative. And that's why I indicate\nit in this fashion.",
    "start": "1711960",
    "end": "1719885"
  },
  {
    "text": "To sort of emphasize that it\nis a negative thing, x minus xm squared.",
    "start": "1719885",
    "end": "1726630"
  },
  {
    "text": "And then I would have higher\norder terms, N minus xm cubed, et cetera.",
    "start": "1726630",
    "end": "1731850"
  },
  {
    "text": "Actually what I will do is I\nwill expand all of those things separately.",
    "start": "1731850",
    "end": "1737010"
  },
  {
    "text": "So I have e to the minus\nN over 6 phi triple prime.",
    "start": "1737010",
    "end": "1743910"
  },
  {
    "text": "N plus N over 6 phi triple\nprime, evaluated at xm,",
    "start": "1743910",
    "end": "1750110"
  },
  {
    "text": "x minus xm cubed, and then the\nfourth order term and so forth.",
    "start": "1750110",
    "end": "1756330"
  },
  {
    "text": "So basically there is\na series such as this that I would have to look at.",
    "start": "1756330",
    "end": "1761700"
  },
  {
    "start": "1761700",
    "end": "1772010"
  },
  {
    "text": "So the first term you can\ntake outside the integral. ",
    "start": "1772010",
    "end": "1781679"
  },
  {
    "text": "And the integration\nagainst the one of this is simply a Gaussian.",
    "start": "1781680",
    "end": "1789340"
  },
  {
    "text": "So what I would\nget is square root of 2 pi divided by the variance,\nwhich is N phi double prime.",
    "start": "1789340",
    "end": "1800500"
  },
  {
    "text": " So that's the first term\nI have taken care of.",
    "start": "1800500",
    "end": "1809420"
  },
  {
    "text": "Now the next term\nactually the way that I have it, since I'm\nexpanding something that",
    "start": "1809420",
    "end": "1816919"
  },
  {
    "text": "is third order around a\npotential that is symmetric.",
    "start": "1816920",
    "end": "1821930"
  },
  {
    "text": "That would give me 0. The next order term, which is\nx minus xm to the fourth power,",
    "start": "1821930",
    "end": "1828210"
  },
  {
    "text": "you already know how\nto calculate averages of various powers with the\nGaussian using Wick's Theorem.",
    "start": "1828210",
    "end": "1837060"
  },
  {
    "text": "And it would be\nrelated to essentially to the square of the variance.",
    "start": "1837060",
    "end": "1842420"
  },
  {
    "text": "The square of the variance\nwould be essentially the square of this quantity out here.",
    "start": "1842420",
    "end": "1848070"
  },
  {
    "text": "So I will get a correction\nthat is order of 1 over N.",
    "start": "1848070",
    "end": "1856830"
  },
  {
    "text": "So if you have\nsufficient energy, you can actually numerically\ncalculate what this is",
    "start": "1856830",
    "end": "1862510"
  },
  {
    "text": "and the higher order\nterms, et cetera. Yes. AUDIENCE: Could\nyou, briefly remind",
    "start": "1862510",
    "end": "1869150"
  },
  {
    "text": "what the second term\nin the bracket means? PROFESSOR: This?",
    "start": "1869150",
    "end": "1874264"
  },
  {
    "text": "This? AUDIENCE: The whole thing,\non the second bracket. PROFESSOR: In the numerator,\nI would have N phi m, N phi",
    "start": "1874264",
    "end": "1883980"
  },
  {
    "text": "prime. Let's call the deviation y y. But phi prime is 0\naround the maximum.",
    "start": "1883980",
    "end": "1892929"
  },
  {
    "text": "So the next order\nterm will be phi double prime y squared over 2. The next order term will be phi\ntriple prime y cubed over 6.",
    "start": "1892930",
    "end": "1903700"
  },
  {
    "text": "e to the minus N phi triple\nprime y cubed over 6,",
    "start": "1903700",
    "end": "1910630"
  },
  {
    "text": "I can expand as 1 minus N phi\ntriple prime y cubed over 6,",
    "start": "1910630",
    "end": "1919150"
  },
  {
    "text": "which is what this is. And then you can go and do that\nwith all of the other terms. ",
    "start": "1919150",
    "end": "1929520"
  },
  {
    "text": "Yes. AUDIENCE: Isn't it\nthen you can also expand as N the local maximum? PROFESSOR: Excellent.",
    "start": "1929520",
    "end": "1934670"
  },
  {
    "text": "Good. So you are saying, why didn't\nI expand around this maximum, around this maximum.",
    "start": "1934670",
    "end": "1940450"
  },
  {
    "text": "So let's do that. xm prime xm double prime.",
    "start": "1940450",
    "end": "1946040"
  },
  {
    "text": "So I would have a series\naround the other maxima. So the next one would be N to\nthe phi of xm prime, root 2",
    "start": "1946040",
    "end": "1957030"
  },
  {
    "text": "pi N phi double\nprime at xm prime.",
    "start": "1957030",
    "end": "1962690"
  },
  {
    "text": "And then one plus order of 1\nover N And then the next one,",
    "start": "1962690",
    "end": "1967909"
  },
  {
    "text": "and so forth.  Now we are interested in the\nlimit where N goes to infinity.",
    "start": "1967910",
    "end": "1977920"
  },
  {
    "text": "Or N is much, much\nlarger than 1. In the limit where N\nis much larger than 1,",
    "start": "1977920",
    "end": "1985190"
  },
  {
    "text": "Let's imagine that\nthese two phi's if I were to plot not e\nto the phi but phi itself.",
    "start": "1985190",
    "end": "1993559"
  },
  {
    "text": "Let's imagine that\nthese two phi's are different by I don't know,\n0.1, 10 to the minus 4.",
    "start": "1993560",
    "end": "2000740"
  },
  {
    "text": "It doesn't matter. I'm multiplying\ntwo things with N,",
    "start": "2000740",
    "end": "2006560"
  },
  {
    "text": "and then I'm comparing\ntwo exponentials. So if this maximum was at 1,\nI would have here e to the N.",
    "start": "2006560",
    "end": "2015080"
  },
  {
    "text": "If this one was at\n1 minus epsilon, over here I would have e\nto the N minus N epsilon.",
    "start": "2015080",
    "end": "2023440"
  },
  {
    "text": "And so I can always ignore\nthis compared to that. ",
    "start": "2023440",
    "end": "2031050"
  },
  {
    "text": "And so basically, this\nis the leading term. And if I were to take its log\nand divide by N, what do I get?",
    "start": "2031050",
    "end": "2042190"
  },
  {
    "text": "I will get phi of xm. And then I would get\nfrom this something",
    "start": "2042190",
    "end": "2049199"
  },
  {
    "text": "like minus 1/2 log of N phi\ndouble prime xm over 2 pi.",
    "start": "2049199",
    "end": "2059000"
  },
  {
    "text": "And I divided by N,\nso this is 1 over N. And the next term would be\norder of 1 over N squared.",
    "start": "2059000",
    "end": "2066600"
  },
  {
    "start": "2066600",
    "end": "2072379"
  },
  {
    "text": "So systematically,\nin the large N limit, there is a series\nfor the quantity log",
    "start": "2072380",
    "end": "2079879"
  },
  {
    "text": "i divided by N that\nstarts with phi of xm. And then subsequent terms\nto it, you can calculate.",
    "start": "2079880",
    "end": "2088379"
  },
  {
    "text": "Actually I was kind\nof hesitant in writing this as asymptotically\nequal because you",
    "start": "2088380",
    "end": "2095020"
  },
  {
    "text": "may have worried\nabout the dimensions. There should be something\nthat has dimensions of x here.",
    "start": "2095020",
    "end": "2102050"
  },
  {
    "text": "Now when I take the log it\ndoesn't matter that much. But the dimension\nappears over here.",
    "start": "2102050",
    "end": "2107470"
  },
  {
    "text": "It's really the size of the\ninterval that contributes which is of the order of N to the 1/2.",
    "start": "2107470",
    "end": "2113250"
  },
  {
    "text": "And that's where\nthe log N comes. ",
    "start": "2113250",
    "end": "2135490"
  },
  {
    "text": "Questions? ",
    "start": "2135490",
    "end": "2144630"
  },
  {
    "text": "Now let me do one example of\nthis because we will need it.",
    "start": "2144630",
    "end": "2150710"
  },
  {
    "text": " We can easily show\nthat N factorial",
    "start": "2150710",
    "end": "2156920"
  },
  {
    "text": "you can write as 0 to infinity\ndx x to N, e to the minus x.",
    "start": "2156920",
    "end": "2167839"
  },
  {
    "text": "And if you don't\nbelieve this, you can start with the integral\n0 to infinity of dx",
    "start": "2167840",
    "end": "2176704"
  },
  {
    "text": "e to the minus alpha\nx being one over alpha",
    "start": "2176705",
    "end": "2181790"
  },
  {
    "text": "and taking many derivatives. If you take N\nderivatives on this side,",
    "start": "2181790",
    "end": "2192520"
  },
  {
    "text": "you would have 0 to N dx x to\nthe N, e to the minus alpha x,",
    "start": "2192520",
    "end": "2198360"
  },
  {
    "text": "because every time, you\nbring down a factor of x. On the other side, if you\ntake derivatives, 1 over alpha",
    "start": "2198360",
    "end": "2204920"
  },
  {
    "text": "becomes 1 over\nalpha squared, then goes to 2 over alpha cubed, then\ngo c over alpha to the fourth.",
    "start": "2204920",
    "end": "2210090"
  },
  {
    "text": "So basically we will N\nfactorial alpha to the N plus 1.",
    "start": "2210090",
    "end": "2215280"
  },
  {
    "text": "So I just set alpha equals to 1. Now if you look at the thing\nthat I have to integrate,",
    "start": "2215280",
    "end": "2225759"
  },
  {
    "text": "it is something that\nhas a function of x,",
    "start": "2225760",
    "end": "2231630"
  },
  {
    "text": "the quantity that I should\nintegrate starts as x to the N,",
    "start": "2231630",
    "end": "2237220"
  },
  {
    "text": "and then decays exponentially. So over here, I have\nx to the N. Out here I",
    "start": "2237220",
    "end": "2243562"
  },
  {
    "text": "have e to the minus x. It is not quite of the\nform that I had before.",
    "start": "2243562",
    "end": "2250890"
  },
  {
    "text": "Part of it is proportional to\nN in the exponent, part of it is not.",
    "start": "2250890",
    "end": "2256210"
  },
  {
    "text": "But you can still use\nexactly the saddle point approach for even this function.",
    "start": "2256210",
    "end": "2261589"
  },
  {
    "text": "And so that's what we will do. I will write this as\nintegral 0 to infinity dx e",
    "start": "2261590",
    "end": "2268700"
  },
  {
    "text": "to some function of x where\nthis function of x is N",
    "start": "2268700",
    "end": "2277220"
  },
  {
    "text": "log x minus x.  And then I will follow that\nprocedure despite this is not",
    "start": "2277220",
    "end": "2285830"
  },
  {
    "text": "being quite entirely\nproportional to N. I will find its maximum\nby setting phi prime to 0.",
    "start": "2285830",
    "end": "2293820"
  },
  {
    "text": "phi prime is N over x minus 1. So clearly, phi prime\nto 0 will give me",
    "start": "2293820",
    "end": "2302580"
  },
  {
    "text": "that x max is N. So the\nlocation of this maximum",
    "start": "2302580",
    "end": "2309880"
  },
  {
    "text": "that I have is in fact N.",
    "start": "2309880",
    "end": "2315890"
  },
  {
    "text": "And the second derivative,\nphi double prime, is minus N over x squared, which\nif I evaluate at the maximum,",
    "start": "2315890",
    "end": "2330019"
  },
  {
    "text": "is going to be minus 1 over\nN. Because the maximum occurs",
    "start": "2330020",
    "end": "2335480"
  },
  {
    "text": "at the N. So if I'm were to make a\nsaddle point expansion of this,",
    "start": "2335480",
    "end": "2342590"
  },
  {
    "text": "I would say that N factorial\nis integral 0 to infinity, dx",
    "start": "2342590",
    "end": "2350256"
  },
  {
    "text": "e to the phi evaluated at x\nmax, which is N log N minus N.",
    "start": "2350256",
    "end": "2358940"
  },
  {
    "text": "First derivative is 0. The second derivative\nwill give me minus 1",
    "start": "2358940",
    "end": "2363950"
  },
  {
    "text": "over N with a factor\nof 2 because I'm expanding second order.",
    "start": "2363950",
    "end": "2370780"
  },
  {
    "text": "And then I have x\nminus this location of the maximum squared.",
    "start": "2370780",
    "end": "2377520"
  },
  {
    "text": "And there would be higher order\nterms from the higher order derivatives. ",
    "start": "2377520",
    "end": "2383790"
  },
  {
    "text": "So I can clearly take e to\nthe N log N minus N out front.",
    "start": "2383790",
    "end": "2390880"
  },
  {
    "text": "And then the\nintegration that I have is just a standard Gaussian\nwith a variance that",
    "start": "2390880",
    "end": "2397930"
  },
  {
    "text": "is just proportional to N.\nSo I would get a root 2 pi N.",
    "start": "2397930",
    "end": "2403790"
  },
  {
    "text": "And then I would have\nhigher order corrections that if you are energetic,\nyou can actually calculate.",
    "start": "2403790",
    "end": "2411320"
  },
  {
    "text": "It's not that difficult. So you get this Stirling's\nFormula that limit of large N,",
    "start": "2411320",
    "end": "2426890"
  },
  {
    "text": "let's do log of N factorial is N\nlog N minus N. And if you want,",
    "start": "2426890",
    "end": "2434599"
  },
  {
    "text": "you can go one step further,\nand you have 1/2 log of 2 pi N.",
    "start": "2434600",
    "end": "2441320"
  },
  {
    "text": "And the next order term\nwould be order of 1/N.",
    "start": "2441320",
    "end": "2474572"
  },
  {
    "text": "Any questions? ",
    "start": "2474572",
    "end": "2480240"
  },
  {
    "text": "OK? Where do I need to use this? Next part, we are going to talk\nabout entropy, information,",
    "start": "2480240",
    "end": "2493910"
  },
  {
    "text": "and estimation. ",
    "start": "2493910",
    "end": "2505600"
  },
  {
    "text": "So the first four\ntopics of the course thermodynamics, probability,\nthis kinetic theory of gases,",
    "start": "2505600",
    "end": "2515910"
  },
  {
    "text": "and basic of\nstatistical physics. In each one of them, you will\ndefine some version of entropy.",
    "start": "2515910",
    "end": "2523960"
  },
  {
    "text": "We already saw the\nthermodynamic one as dQ divided by T meaning dS.",
    "start": "2523960",
    "end": "2531420"
  },
  {
    "text": "Now just thinking\nabout probability will also enable you to\ndefine some form of entropy.",
    "start": "2531420",
    "end": "2537520"
  },
  {
    "text": "So let's see how we go about it. So also information,\nwhat does that mean?",
    "start": "2537520",
    "end": "2546230"
  },
  {
    "text": "It goes back to\nwork off Shannon. And the idea is as\nfollows, suppose",
    "start": "2546230",
    "end": "2553650"
  },
  {
    "text": "you want to send a\nmessage of N characters. ",
    "start": "2553650",
    "end": "2568200"
  },
  {
    "text": "The characters\nthemselves are taken",
    "start": "2568200",
    "end": "2573560"
  },
  {
    "text": "from some kind of\nalphabet, if you like, x1 through xM\nthat has M characters.",
    "start": "2573560",
    "end": "2588365"
  },
  {
    "text": " So, for example\nif you're sending",
    "start": "2588365",
    "end": "2595720"
  },
  {
    "text": "a message in English\nlanguage, you would be using the\nletters A through Z. So you have M off 26.",
    "start": "2595720",
    "end": "2601720"
  },
  {
    "text": " Maybe if you want\nto include space, punctuation, it would\nbe larger than that.",
    "start": "2601720",
    "end": "2608783"
  },
  {
    "text": " But let's say if you're\ndealing with English language,",
    "start": "2608783",
    "end": "2617580"
  },
  {
    "text": "the probabilities of\nthe different characters are not the same. So S and P, you are going to\nencounter much more frequently",
    "start": "2617580",
    "end": "2626510"
  },
  {
    "text": "than, say, Z or X. So let's say\nthat the frequencies with which",
    "start": "2626510",
    "end": "2633820"
  },
  {
    "text": "we expect these characters\nto occur are things like P1 through PM.",
    "start": "2633820",
    "end": "2640226"
  },
  {
    "text": "OK?  Now how many possible\nmessages are there?",
    "start": "2640226",
    "end": "2649680"
  },
  {
    "text": "So number of possible\nmessages that's are composed",
    "start": "2649680",
    "end": "2658559"
  },
  {
    "text": "of N occurrences of alphabet\nof M letters you would say",
    "start": "2658560",
    "end": "2667650"
  },
  {
    "text": "is M to the N. Now, Shannon was\nsort of concerned with sending",
    "start": "2667650",
    "end": "2676130"
  },
  {
    "text": "the information\nabout this message, let's say, over a line\nwhere you have converted it",
    "start": "2676130",
    "end": "2684780"
  },
  {
    "text": "to, say, a binary code. And then you would say\nthat the number of bits",
    "start": "2684780",
    "end": "2691710"
  },
  {
    "text": "that would correspond to M to\nthe N is the N log base 2 of M.",
    "start": "2691710",
    "end": "2702900"
  },
  {
    "text": "That is, if you really\nhad the simpler case",
    "start": "2702900",
    "end": "2709849"
  },
  {
    "text": "where your selections was just\nhead or tail, it was binary.",
    "start": "2709850",
    "end": "2715680"
  },
  {
    "text": "And you wanted to\nsend to somebody else the outcome of\n500 throws of a coin.",
    "start": "2715680",
    "end": "2723230"
  },
  {
    "text": "It would be a sequence of\n500 0's and 1's corresponding to head or tails.",
    "start": "2723230",
    "end": "2729280"
  },
  {
    "text": "So you would have to send\nfor the binary case, one",
    "start": "2729280",
    "end": "2735130"
  },
  {
    "text": "bit per outcome. If it is something\nlike a base of DNA and there are four things,\nyou would have two per base.",
    "start": "2735130",
    "end": "2743910"
  },
  {
    "text": "So that would be log 4 base 2. And for English, it would\nbe log 26 or whatever",
    "start": "2743910",
    "end": "2753470"
  },
  {
    "text": "the appropriate number is\nwith punctuation-- maybe comes to 32-- possible\ncharacters than five",
    "start": "2753470",
    "end": "2760138"
  },
  {
    "text": "per [? element ?]. OK. But you know that\nif you sort of were",
    "start": "2760138",
    "end": "2768430"
  },
  {
    "text": "to look at all possible\nmessages, most of them would be junk.",
    "start": "2768430",
    "end": "2774910"
  },
  {
    "text": "And in particular, if you had\nused this simple substitution code, for example, to\nmix up your message,",
    "start": "2774910",
    "end": "2783000"
  },
  {
    "text": "you replaced A by\nsomething else, et cetera, the frequencies\nwould be preserved.",
    "start": "2783000",
    "end": "2790310"
  },
  {
    "text": "So sort of clearly a nice way to\ndecode this substitution code, if you have a long\nenough text, you sort of",
    "start": "2790310",
    "end": "2796940"
  },
  {
    "text": "look at how many\nrepetitions they are and match them with\ntheir frequencies that you expect for\na real language.",
    "start": "2796940",
    "end": "2805010"
  },
  {
    "text": "So the number of\npossible messages-- So in a typical\nmessage, what you",
    "start": "2805010",
    "end": "2818400"
  },
  {
    "text": "expect Ni, which is Pi\nN occurrences, of xi.",
    "start": "2818400",
    "end": "2835954"
  },
  {
    "text": " So if you know for example, what\nthe frequencies of the letters",
    "start": "2835955",
    "end": "2844840"
  },
  {
    "text": "in the alphabet are, in\na long enough message, you expect that typically\nyou would get that number.",
    "start": "2844840",
    "end": "2852080"
  },
  {
    "text": "Of course, what\nthat really means is that you're going\nto get correction because not all\nmessages are the same.",
    "start": "2852080",
    "end": "2858890"
  },
  {
    "text": "But the deviation\nthat you would get from getting something that is\nproportional to the probability",
    "start": "2858890",
    "end": "2866810"
  },
  {
    "text": "through the frequency in the\nlimit of a very long message would be of the order\nof N to the 1/2.",
    "start": "2866810",
    "end": "2874620"
  },
  {
    "text": "So ignoring this\nN to the 1/2, you would say that the typical\nmessage that I expect",
    "start": "2874620",
    "end": "2880420"
  },
  {
    "text": "to receive will have characters\naccording to these proportions.",
    "start": "2880420",
    "end": "2886540"
  },
  {
    "text": "So if I asked the\nfollowing question, not what are the number\nof all possible messages,",
    "start": "2886540",
    "end": "2893360"
  },
  {
    "text": "but what is the number\nof typical messages? ",
    "start": "2893360",
    "end": "2901980"
  },
  {
    "text": "I will call that g. The number of typical\nmessages would",
    "start": "2901980",
    "end": "2907039"
  },
  {
    "text": "be always of distributing\nthese number of characters",
    "start": "2907040",
    "end": "2913080"
  },
  {
    "text": "in a message of length N. Again\nthere are clearly correlations. But for the time\nbeing, forgetting all",
    "start": "2913080",
    "end": "2919149"
  },
  {
    "text": "of the correlations, if\n[? we ?] [? do ?] correlations, we only reduce this number. ",
    "start": "2919149",
    "end": "2938870"
  },
  {
    "text": "So this number is much,\nmuch less time M to the N.",
    "start": "2938870",
    "end": "2970120"
  },
  {
    "text": "Now here is I'm going to make an\nexcursion to so far everything was clear.",
    "start": "2970120",
    "end": "2975400"
  },
  {
    "text": "Now I'm going to say something\nthat is kind of theoretically correct, but\npractically not so much.",
    "start": "2975400",
    "end": "2983210"
  },
  {
    "text": "You could, for\nexample, have some way of labeling all possible\ntypical messages.",
    "start": "2983210",
    "end": "2990640"
  },
  {
    "text": "So you would have-- this would\nbe typical message number one, number two, all the way\nto typical message number g.",
    "start": "2990640",
    "end": "2998970"
  },
  {
    "text": "This is the number\nof typical message. Suppose I could point out to\none of these messages and say,",
    "start": "2998970",
    "end": "3004990"
  },
  {
    "text": "this is the message\nthat was actually sent. How many bits of\ninformation would I",
    "start": "3004990",
    "end": "3011200"
  },
  {
    "text": "have to that indicate\none number out of g?",
    "start": "3011200",
    "end": "3017530"
  },
  {
    "text": "The number of bits\nof information",
    "start": "3017530",
    "end": "3029620"
  },
  {
    "text": "for a typical message, rather\nthan being this object,",
    "start": "3029620",
    "end": "3039820"
  },
  {
    "text": "would simply be log g. ",
    "start": "3039820",
    "end": "3049240"
  },
  {
    "text": "So let's see what this log g is. And for the time being,\nlet's forget the basis. I can always change\nbasis by dividing",
    "start": "3049240",
    "end": "3056420"
  },
  {
    "text": "by log of whatever quantity\nI'm looking at the basis. This is they log of N factorial\ndivided by these product over i",
    "start": "3056420",
    "end": "3072024"
  },
  {
    "text": "of Ni factorials which\nare these Pi N's.",
    "start": "3072024",
    "end": "3078890"
  },
  {
    "text": "And in the limit of\nlarge N, what I can use",
    "start": "3078890",
    "end": "3083990"
  },
  {
    "text": "is the Stirling's Formula\nthat we had over there. So what I have is N log N\nminus N in the numerator.",
    "start": "3083990",
    "end": "3092390"
  },
  {
    "text": " Minus sum over i Ni\nlog of Ni minus Ni.",
    "start": "3092390",
    "end": "3104664"
  },
  {
    "text": " Of course the sum over\nNi's cancels this N,",
    "start": "3104664",
    "end": "3113370"
  },
  {
    "text": "so I don't need to\nworry about that. And I can rearrange this.",
    "start": "3113370",
    "end": "3119240"
  },
  {
    "text": "I can write this as\nthis N as sum over i Ni.",
    "start": "3119240",
    "end": "3125860"
  },
  {
    "text": "Put the terms that are\nproportional to Ni together. You can see that\nI get Ni log of Ni",
    "start": "3125860",
    "end": "3132340"
  },
  {
    "text": "over N, which\nwould be log of Pi. And I can actually then\ntake out a factor of N,",
    "start": "3132340",
    "end": "3140180"
  },
  {
    "text": "and write it as sum\nover i Pi log of Pi.",
    "start": "3140180",
    "end": "3145438"
  },
  {
    "start": "3145438",
    "end": "3159140"
  },
  {
    "text": "And just as a excursion,\nthis is something that you've already\nseen hopefully.",
    "start": "3159140",
    "end": "3165770"
  },
  {
    "text": "This is also called\nmixing entropy. ",
    "start": "3165770",
    "end": "3177400"
  },
  {
    "text": "And we will see\nit later on, also. That is, if I had\ninitially a bunch of,",
    "start": "3177400",
    "end": "3186349"
  },
  {
    "text": "let's say, things that\nwere of color red, and separately in a\nbox a bunch of things",
    "start": "3186350",
    "end": "3192819"
  },
  {
    "text": "that are color green, and\nthen bunch of things that are a different color,\nand I knew initially",
    "start": "3192820",
    "end": "3203470"
  },
  {
    "text": "where they were in\neach separate box, and I then mix them up\ntogether so that they're",
    "start": "3203470",
    "end": "3210360"
  },
  {
    "text": "putting all possible\nrandom ways, and I don't know\nwhich is where, I",
    "start": "3210360",
    "end": "3216740"
  },
  {
    "text": "have done something\nthat is irreversible.",
    "start": "3216740",
    "end": "3222430"
  },
  {
    "text": "It is very easy to take\nthese boxes of marbles of different colors\nand mix them up.",
    "start": "3222430",
    "end": "3228470"
  },
  {
    "text": "You have to do more work\nto separate them out. And so this increase\nin entropy is",
    "start": "3228470",
    "end": "3236580"
  },
  {
    "text": "given by precisely\nthe same formula here. And it's called\nthe mixing entropy.",
    "start": "3236580",
    "end": "3243840"
  },
  {
    "text": "So what we can see\nnow that we sort of rather than thinking\nof these as particles,",
    "start": "3243840",
    "end": "3250820"
  },
  {
    "text": "we were thinking of\nthese as letters. And then we mixed up the\nletters in all possible ways to make our messages.",
    "start": "3250820",
    "end": "3257560"
  },
  {
    "text": "But quite generally for\nany discrete probability,",
    "start": "3257560",
    "end": "3268850"
  },
  {
    "text": "so a probability that has a\nset of possible outcomes Pi,",
    "start": "3268850",
    "end": "3275920"
  },
  {
    "text": "we can define an\nentropy S associated",
    "start": "3275920",
    "end": "3289180"
  },
  {
    "text": "with these set of probabilities,\nwhich is given by this formula.",
    "start": "3289180",
    "end": "3295160"
  },
  {
    "text": "Minus sum over i Pi log of Pi. If you like, it is also this--\nnot quite, doesn't makes",
    "start": "3295160",
    "end": "3303680"
  },
  {
    "text": "sense-- but it's some kind\nof an average of log P.",
    "start": "3303680",
    "end": "3314190"
  },
  {
    "text": "So anytime we see a\ndiscrete probability, we can certainly do that.",
    "start": "3314190",
    "end": "3320950"
  },
  {
    "text": "It turns out that also we will\nencounter in cases later on, where rather than having\na discrete probability,",
    "start": "3320950",
    "end": "3330350"
  },
  {
    "text": "we have a probability\ndensity function. And we would be very tempted\nto define an entropy associated",
    "start": "3330350",
    "end": "3340400"
  },
  {
    "text": "with a PDF to be something\nlike minus an integral dx",
    "start": "3340400",
    "end": "3347646"
  },
  {
    "text": "P of x log of P of x. ",
    "start": "3347646",
    "end": "3353060"
  },
  {
    "text": "But this is kind of undefined. Because probability density\ndepends on some quantity",
    "start": "3353060",
    "end": "3361790"
  },
  {
    "text": "x that has units. If this was probability\nalong a line, and I changed my units\nfrom meters to centimeters,",
    "start": "3361790",
    "end": "3371190"
  },
  {
    "text": "then this log will\ngain a factor that will be associated with\nthe change in scale",
    "start": "3371190",
    "end": "3378080"
  },
  {
    "text": "So this is kind of undefined. ",
    "start": "3378080",
    "end": "3384200"
  },
  {
    "text": "One of the miracles\nof statistical physics is that we will find\nthe exact measure",
    "start": "3384200",
    "end": "3393059"
  },
  {
    "text": "to make this probability\nin the continuum unique and independent of\nthe choice of-- I mean,",
    "start": "3393060",
    "end": "3401840"
  },
  {
    "text": "there is a very\nprecise choice of units for measuring things that\nwould make this well-defined.",
    "start": "3401840",
    "end": "3407310"
  },
  {
    "text": "Yes. AUDIENCE: But that would be\nundefined up to some sort of [INAUDIBLE]. PROFESSOR: After\nyou [INAUDIBLE].",
    "start": "3407310",
    "end": "3412670"
  },
  {
    "text": "AUDIENCE: So you can still\nextract dependencies from it. PROFESSOR: You can\nstill calculate things like differences, et cetera.",
    "start": "3412670",
    "end": "3418440"
  },
  {
    "text": "But there is a certain\nlack of definition. ",
    "start": "3418440",
    "end": "3426136"
  },
  {
    "text": "Yes. AUDIENCE: [INAUDIBLE] the\nrelation between this entropy defined here with the\nentropy defined earlier,",
    "start": "3426136",
    "end": "3432160"
  },
  {
    "text": "you notice the parallel. PROFESSOR: We find\nthat all you have to do",
    "start": "3432160",
    "end": "3437359"
  },
  {
    "text": "is to multiply by\na Boltzmann factor, and they would become identical.",
    "start": "3437360",
    "end": "3443330"
  },
  {
    "text": "So we will see that. It turns out that the heat\ndefinition of entropy,",
    "start": "3443330",
    "end": "3450570"
  },
  {
    "text": "once you look at\nthe right variables to define probability with, then\nthe entropy of a probability",
    "start": "3450570",
    "end": "3457770"
  },
  {
    "text": "distribution is\nexactly the entropy that comes from the\nheat calculation. So up to here, there is a\nmeasured numerical constant",
    "start": "3457770",
    "end": "3466612"
  },
  {
    "text": "that we have to define. ",
    "start": "3466612",
    "end": "3479290"
  },
  {
    "text": "All right. But what does this have to\ndo with this Shannon story? ",
    "start": "3479290",
    "end": "3500910"
  },
  {
    "text": "Going back to the story, if I\ndidn't know the probabilities,",
    "start": "3500910",
    "end": "3507228"
  },
  {
    "text": "if I didn't know\nthis, I would say that I need to pass on\nthis amount of information.",
    "start": "3507228",
    "end": "3516119"
  },
  {
    "text": "But if I somehow constructed\nthe right scheme, and the person that\nI'm sending the message",
    "start": "3516120",
    "end": "3523020"
  },
  {
    "text": "knows the probabilities,\nthen I need to send this amount of\ninformation, which is actually",
    "start": "3523020",
    "end": "3532569"
  },
  {
    "text": "less than N log M. So clearly having knowledge\nof the probabilities",
    "start": "3532570",
    "end": "3541170"
  },
  {
    "text": "gives you some ability,\nsome amount of information, so that you have\nto send less bits.",
    "start": "3541170",
    "end": "3549980"
  },
  {
    "text": "OK. So the reduction in number\nof bits due to knowledge of P",
    "start": "3549980",
    "end": "3572440"
  },
  {
    "text": "is the difference between N log\nM, which I had to do before,",
    "start": "3572440",
    "end": "3579880"
  },
  {
    "text": "and what I have to\ndo now, which is N Pi sum over i Pi log of Pi.",
    "start": "3579880",
    "end": "3590000"
  },
  {
    "start": "3590000",
    "end": "3595500"
  },
  {
    "text": "So which is N log M plus\nsum over i Pi log of Pi.",
    "start": "3595500",
    "end": "3608710"
  },
  {
    "text": "I can evaluate\nthis in any basis. If I wanted to really count in\nterms of the number of bits,",
    "start": "3608710",
    "end": "3616220"
  },
  {
    "text": "I would do both of these\nthings in log base 2.",
    "start": "3616220",
    "end": "3621619"
  },
  {
    "text": "It is clearly something that\nis proportional to the length",
    "start": "3621620",
    "end": "3626860"
  },
  {
    "text": "of the message. That is, if I want to send a\nbook that these twice as big,",
    "start": "3626860",
    "end": "3632700"
  },
  {
    "text": "the amount of bits will\nbe reduced proportionately by this amount.",
    "start": "3632700",
    "end": "3637930"
  },
  {
    "text": "So you can define\na quantity that is basically the\ninformation per bit. ",
    "start": "3637930",
    "end": "3645550"
  },
  {
    "text": "And this is given the\nknowledge of the probabilities,",
    "start": "3645550",
    "end": "3654590"
  },
  {
    "text": "you really have gained\nan information per bit",
    "start": "3654590",
    "end": "3660670"
  },
  {
    "text": "which is the difference of log\nM and sum over i Pi log Pi.",
    "start": "3660670",
    "end": "3666650"
  },
  {
    "start": "3666650",
    "end": "3672880"
  },
  {
    "text": "Up to a sign and this\nadditional factor of log N, the entropy-- because I can\nactually get rid of this N--",
    "start": "3672880",
    "end": "3681705"
  },
  {
    "text": "the entropy and the information\nare really the same thing",
    "start": "3681705",
    "end": "3688430"
  },
  {
    "text": "up to a sign. And just to sort of\nmake sure that we understand the\nappropriate limits.",
    "start": "3688430",
    "end": "3697760"
  },
  {
    "text": "If I have something\nlike the case where I have a\nuniform distribution.",
    "start": "3697760",
    "end": "3706330"
  },
  {
    "text": "Let's say that I say that\nall characters in my message",
    "start": "3706330",
    "end": "3711430"
  },
  {
    "text": "are equally likely to occur. If it's a coin, it's unbiased\ncoin, it's as likely in a throw",
    "start": "3711430",
    "end": "3718180"
  },
  {
    "text": "to be head or tail. You would say that if\nit's an unbiased coin, I really should send one\nbit per throw of the coin.",
    "start": "3718180",
    "end": "3726740"
  },
  {
    "text": "And indeed, that will\nfollow from this. Because in this\ncase, you can see that the information\ncontained is",
    "start": "3726740",
    "end": "3734440"
  },
  {
    "text": "going to be log M. And then\nI have plus 1 over M log of 1",
    "start": "3734440",
    "end": "3744200"
  },
  {
    "text": "over M. And there are M\nsuch terms that are uniform. And this gives me 0.",
    "start": "3744200",
    "end": "3751240"
  },
  {
    "text": "There is no information here. If I ask what's the\nentropy in this case.",
    "start": "3751240",
    "end": "3757869"
  },
  {
    "text": "The entropy is M terms. Each one of them have\na factor of 1 over M.",
    "start": "3757870",
    "end": "3764750"
  },
  {
    "text": "And then I have a\nlog of 1 over M. And there is a minus\nsign here overall.",
    "start": "3764750",
    "end": "3772400"
  },
  {
    "text": " So this is log of M.",
    "start": "3772400",
    "end": "3780150"
  },
  {
    "text": "So you've probably seen this\nversion of the entropy before.",
    "start": "3780150",
    "end": "3786309"
  },
  {
    "text": "That if you have M\nequal possibilities, the entropy is\nrelated to log M. This",
    "start": "3786310",
    "end": "3793609"
  },
  {
    "text": "is the case where all of\noutcomes are equally likely.",
    "start": "3793610",
    "end": "3799380"
  },
  {
    "text": "So basically this is\na uniform probability. Everything is equally likely.",
    "start": "3799380",
    "end": "3805539"
  },
  {
    "text": "You have no information. You have this maximal\npossible entropy.",
    "start": "3805540",
    "end": "3811520"
  },
  {
    "text": "The other extreme\nof it would be where you have a definite result.",
    "start": "3811520",
    "end": "3817870"
  },
  {
    "text": "You have a coin that\nalways gives you heads. And if the other\nperson knows that,",
    "start": "3817870",
    "end": "3824300"
  },
  {
    "text": "you don't need to\nsend any information. No matter thousand times,\nit will be thousand heads.",
    "start": "3824300",
    "end": "3829560"
  },
  {
    "text": "So here, Pi is a delta function. Let's say i equals to five\nor whatever number is.",
    "start": "3829560",
    "end": "3837720"
  },
  {
    "text": "So one of the\nvariables in the list carries all the probability.",
    "start": "3837720",
    "end": "3842840"
  },
  {
    "text": "All the others\ncarry 0 probability. How much information\ndo I have here?",
    "start": "3842840",
    "end": "3848680"
  },
  {
    "text": "I have log M. Now when I\ngo and looked at the list,",
    "start": "3848680",
    "end": "3854230"
  },
  {
    "text": "in the list, either P is 0, or\nP is one, but the log of 1 and M",
    "start": "3854230",
    "end": "3861840"
  },
  {
    "text": "is 0. So this is basically\ngoing to give me 0. Entropy in this case is 0.",
    "start": "3861840",
    "end": "3868150"
  },
  {
    "text": "The information is maximum. You don't need to\npass any information. So anything else is in between.",
    "start": "3868150",
    "end": "3874230"
  },
  {
    "text": "So you sort of think\nof a probability that is some big thing, some\nsmall things, et cetera,",
    "start": "3874230",
    "end": "3883050"
  },
  {
    "text": "you can figure out\nwhat its entropy is and what is\ninformation content is.",
    "start": "3883050",
    "end": "3889460"
  },
  {
    "text": "So actually I don't\nknow the answer. But presume it's very\neasy to figure out",
    "start": "3889460",
    "end": "3894720"
  },
  {
    "text": "what's the information\nper character of the text in English language.",
    "start": "3894720",
    "end": "3900400"
  },
  {
    "text": "Once you know the\nfrequencies of the characters you can go and calculate this. ",
    "start": "3900400",
    "end": "3910549"
  },
  {
    "text": "Questions. Yes. AUDIENCE: Just to\nclarify the terminology, so the information\nmeans the [INAUDIBLE]?",
    "start": "3910550",
    "end": "3917187"
  },
  {
    "text": "PROFESSOR: The number\nof bits that you have to transmit to\nthe other person.",
    "start": "3917187",
    "end": "3922860"
  },
  {
    "text": "So the other person\nknows the probability. Given that they know\nthe probabilities,",
    "start": "3922860",
    "end": "3928095"
  },
  {
    "text": "how many fewer\nbits of information should I send to them?",
    "start": "3928095",
    "end": "3933160"
  },
  {
    "text": "So their knowledge\ncorresponds to a gain in number of bits, which\nis given by this formula.",
    "start": "3933160",
    "end": "3939742"
  },
  {
    "start": "3939742",
    "end": "3947310"
  },
  {
    "text": "If you know that the\ncoin that I'm throwing is biased so that it\nalways comes heads,",
    "start": "3947310",
    "end": "3955660"
  },
  {
    "text": "then I don't have to\nsend you any information. So per every time\nI throw the coin,",
    "start": "3955660",
    "end": "3961480"
  },
  {
    "text": "you have one bit of information. ",
    "start": "3961480",
    "end": "3972940"
  },
  {
    "text": "Other questions? AUDIENCE: The equation, the\ntop equation, so natural",
    "start": "3972940",
    "end": "3981235"
  },
  {
    "text": "log [INAUDIBLE] natural\nlog of 2, [INAUDIBLE]? ",
    "start": "3981235",
    "end": "3988720"
  },
  {
    "text": "PROFESSOR: I initially\ncalculated my standing formula",
    "start": "3988720",
    "end": "3994510"
  },
  {
    "text": "as log of N factorial\nis N log N minus N.",
    "start": "3994510",
    "end": "4000970"
  },
  {
    "text": "So since I had done\neverything in natural log, I maintained that.",
    "start": "4000970",
    "end": "4006650"
  },
  {
    "text": "And then I used this\nsymbol that log, say, 5 2",
    "start": "4006650",
    "end": "4014049"
  },
  {
    "text": "is the same thing that maybe\nare used with this notation. I don't know. ",
    "start": "4014050",
    "end": "4021920"
  },
  {
    "text": "So if I don't indicate a number\nhere, it's the natural log. It's base e.",
    "start": "4021920",
    "end": "4027280"
  },
  {
    "text": "If I put a number so log,\nlet's say, base 2 of 5",
    "start": "4027280",
    "end": "4033240"
  },
  {
    "text": "is log 5 divided by log 2. ",
    "start": "4033240",
    "end": "4040378"
  },
  {
    "text": "AUDIENCE: So [INAUDIBLE]? PROFESSOR: Log 2, log 2. Information. ",
    "start": "4040378",
    "end": "4047289"
  },
  {
    "text": "AUDIENCE: Oh.  PROFESSOR: Or if you like, I\ncould have divided by log 2",
    "start": "4047289",
    "end": "4054290"
  },
  {
    "text": "here. AUDIENCE: But so\nthere [INAUDIBLE] all of the other\nplaces, and you just",
    "start": "4054290",
    "end": "4060121"
  },
  {
    "text": "[? write ?] all\nthis [INAUDIBLE]. All right, thank\nyou, [? Michael. ?]",
    "start": "4060122",
    "end": "4066345"
  },
  {
    "text": "PROFESSOR: Right. Yeah. So this is the general\nway to transfer",
    "start": "4066345",
    "end": "4074970"
  },
  {
    "text": "between log, natural\nlog, and any log. In the language of\nelectrical engineering,",
    "start": "4074970",
    "end": "4081339"
  },
  {
    "text": "where Shannon worked, it is\ncommon to express everything in terms of the number of bits.",
    "start": "4081340",
    "end": "4087270"
  },
  {
    "text": "So whenever I'm\nexpressing things in terms of the number\nof bits, I really should use the log of 2.",
    "start": "4087270",
    "end": "4093040"
  },
  {
    "text": "So I really, if I want\nto use information, I really should use log of 2.",
    "start": "4093040",
    "end": "4098439"
  },
  {
    "text": "Whereas in statistical\nphysics, we usually use the natural log\nin expressing entropy.",
    "start": "4098439",
    "end": "4104214"
  },
  {
    "text": "AUDIENCE: Oh, so it doesn't\nreally matter [INAUDIBLE]. PROFESSOR: It's just\nan overall coefficient. As I said that\neventually, if I want",
    "start": "4104215",
    "end": "4110710"
  },
  {
    "text": "to calculate to the heat\nversion of the entropy, I have to multiply by\nyet another number, which",
    "start": "4110710",
    "end": "4116770"
  },
  {
    "text": "is the Boltzmann constant. So really the\nconceptual part is more",
    "start": "4116770",
    "end": "4122410"
  },
  {
    "text": "important than the\noverall numerical factor. ",
    "start": "4122410",
    "end": "4130979"
  },
  {
    "text": "OK? ",
    "start": "4130979",
    "end": "4142589"
  },
  {
    "text": "I had the third item in my list\nhere, which we can finish with,",
    "start": "4142590",
    "end": "4148167"
  },
  {
    "text": "which is estimation. ",
    "start": "4148167",
    "end": "4160920"
  },
  {
    "text": "So frequently you are\nfaced with the task",
    "start": "4160920",
    "end": "4166449"
  },
  {
    "text": "of assigning probabilities. So there's a situation.",
    "start": "4166450",
    "end": "4172849"
  },
  {
    "text": "You know that there's\na number of outcomes. And you want to\nassign probabilities for these outcomes.",
    "start": "4172850",
    "end": "4179170"
  },
  {
    "text": "And the procedure\nthat we will use is summarized by the\nfollowing sentence",
    "start": "4179170",
    "end": "4186900"
  },
  {
    "text": "that I have to then define. The most unbiased--\nlet's actually",
    "start": "4186900",
    "end": "4199420"
  },
  {
    "text": "just say it's the\ndefinition if you like-- the unbiased assignment\nof probabilities",
    "start": "4199420",
    "end": "4214940"
  },
  {
    "text": "maximizes the entropy\nsubject to constraints.",
    "start": "4214940",
    "end": "4227560"
  },
  {
    "text": " Known constraints. ",
    "start": "4227560",
    "end": "4240810"
  },
  {
    "text": "What do I mean by that? So suppose I had told you\nthat we are throwing a dice.",
    "start": "4240810",
    "end": "4248080"
  },
  {
    "text": "Or let's say a coin, but\nlet's go back to the dice. And the dice has possibilities\n1, 2, 3, 4, 5, 6.",
    "start": "4248080",
    "end": "4257210"
  },
  {
    "text": "And this is the only\nthing that I know. So if somebody says\nthat I'm throwing a dice",
    "start": "4257210",
    "end": "4263360"
  },
  {
    "text": "and you don't know\nanything else, there's no reason for you to\nprivilege 6 with respect to 4,",
    "start": "4263360",
    "end": "4268675"
  },
  {
    "text": "or 3 with respect to 5. So as far as I know, at this\nmoment in time, all of these",
    "start": "4268675",
    "end": "4274310"
  },
  {
    "text": "are equally likely. So I will assign each one of\nthem for probability of 1/6.",
    "start": "4274310",
    "end": "4282159"
  },
  {
    "text": "But we also saw over\nhere what was happening. The uniform\nprobability was the one",
    "start": "4282160",
    "end": "4288580"
  },
  {
    "text": "that had the largest entropy. If I were to change\nthe probability so that something goes up\nand something goes down,",
    "start": "4288580",
    "end": "4296199"
  },
  {
    "text": "then I calculate that formula. And I find that the--\nsorry-- the uniform one has the largest entropy.",
    "start": "4296200",
    "end": "4302489"
  },
  {
    "text": "This has less entropy\ncompared to the uniform one. So what we have done in\nassigning uniform probability",
    "start": "4302490",
    "end": "4312420"
  },
  {
    "text": "is really to maximize the\nentropy subject to the fact that I don't know anything\nexcept that the probabilities",
    "start": "4312420",
    "end": "4319260"
  },
  {
    "text": "should add up to 1. But now suppose\nthat somebody threw",
    "start": "4319260",
    "end": "4326230"
  },
  {
    "text": "the dice many, many times. And each time they\nwere throwing the dice, they were calculating\nthe number.",
    "start": "4326230",
    "end": "4334050"
  },
  {
    "text": "But they didn't give us\nthe number and frequency is what they told us was that\nat the end of many, many run,",
    "start": "4334050",
    "end": "4340770"
  },
  {
    "text": "the average number that\nwe were coming up was 3.2,",
    "start": "4340770",
    "end": "4347990"
  },
  {
    "text": "4.7, whatever. So we know the average of M.",
    "start": "4347990",
    "end": "4353650"
  },
  {
    "text": "So I know now some\nother constraint. I've added to the\ninformation that I had.",
    "start": "4353650",
    "end": "4359810"
  },
  {
    "text": "So if I want to reassign\nthe probabilities given that somebody told me that\nin a large number of runs,",
    "start": "4359810",
    "end": "4367090"
  },
  {
    "text": "the average value of\nthe faces that showed up was some particular value. What do I do?",
    "start": "4367090",
    "end": "4373380"
  },
  {
    "text": "I say, well, I maximize S which\ndepends on these Pi's, which",
    "start": "4373380",
    "end": "4381290"
  },
  {
    "text": "is minus sum over i Pi\nlog of Pi, subjected",
    "start": "4381290",
    "end": "4387190"
  },
  {
    "text": "to constraints that I know. Now one constraint\nyou already used previously is that the\nsum of the probabilities",
    "start": "4387190",
    "end": "4396070"
  },
  {
    "text": "is equal to 1.  This I introduce here through\na Lagrange multiplier,",
    "start": "4396070",
    "end": "4407440"
  },
  {
    "text": "alpha, which I will adjust later\nto make sure that this holds.",
    "start": "4407440",
    "end": "4414330"
  },
  {
    "text": "And in general, what we do if\nwe have multiple constraints is",
    "start": "4414330",
    "end": "4419710"
  },
  {
    "text": "we can add more and more\nLagrange multipliers.",
    "start": "4419710",
    "end": "4424969"
  },
  {
    "text": "And the average of M is\nsum over, let's say, i Pi.",
    "start": "4424970",
    "end": "4433892"
  },
  {
    "text": "So 1 times P of\n1, 2 times P of 2, et cetera, will give you\nwhatever the average value is.",
    "start": "4433892",
    "end": "4442650"
  },
  {
    "text": "So these are the two constraints\nthat I specified for you here. There could've been other\nconstraints, et cetera.",
    "start": "4442650",
    "end": "4450300"
  },
  {
    "text": "So then, if you have a\nfunction with constraint that you have to extremize, you\nadd these Lagrange multipliers.",
    "start": "4450300",
    "end": "4459070"
  },
  {
    "text": "Then you do dS by dPi. Why did I do this? dS by\ndPi, which is minus log of Pi",
    "start": "4459070",
    "end": "4474100"
  },
  {
    "text": "from here. Derivative of log P is 1 over P,\nwith this will give me minus 1.",
    "start": "4474100",
    "end": "4480400"
  },
  {
    "text": "There is a minus alpha here. And then there's a minus\nbeta times i from here.",
    "start": "4480400",
    "end": "4492770"
  },
  {
    "text": "And extremizing means I\nhave to set this to 0.",
    "start": "4492770",
    "end": "4498570"
  },
  {
    "text": "So you can see that the\nsolution to this is Pi--",
    "start": "4498570",
    "end": "4504770"
  },
  {
    "text": "or actually log of Pi,\nlet's say, is minus 1 plus alpha minus beta i.",
    "start": "4504770",
    "end": "4515260"
  },
  {
    "text": "So that Pi is e to\nthe minus 1 plus alpha",
    "start": "4515260",
    "end": "4523659"
  },
  {
    "text": "e to the minus beta times i. ",
    "start": "4523660",
    "end": "4529980"
  },
  {
    "text": "I haven't completed the story. I really have to\nsolve the equations",
    "start": "4529980",
    "end": "4536300"
  },
  {
    "text": "in terms of alpha and\nbeta that would give me the final results in terms\nof the expectation value of i",
    "start": "4536300",
    "end": "4544980"
  },
  {
    "text": "as well as some\nother quantities. But this is the procedure\nthat you would normally",
    "start": "4544980",
    "end": "4551420"
  },
  {
    "text": "use to give you the unbiased\nassignment of probability.",
    "start": "4551420",
    "end": "4557520"
  },
  {
    "text": "Now this actually goes back to\nwhat I said at the beginning. That there's two ways of\nassigning probabilities,",
    "start": "4557520",
    "end": "4564940"
  },
  {
    "text": "either objectively by actually\ndoing lots of measurement, or subjectivity.",
    "start": "4564940",
    "end": "4569980"
  },
  {
    "text": "So this is really\nformalizing what this objective procedure means. So you put in all\nof the information",
    "start": "4569980",
    "end": "4576810"
  },
  {
    "text": "that you have, the number\nof states, any constraints. And then you maximize\nentropy that we",
    "start": "4576810",
    "end": "4583310"
  },
  {
    "text": "defined what it\nwas to get the best",
    "start": "4583310",
    "end": "4589010"
  },
  {
    "text": "maximal entropy for the\nassignment of probabilities consistent with\nthings that you know.",
    "start": "4589010",
    "end": "4596122"
  },
  {
    "text": " You probably recognize this form\nas kind of a Boltzmann weight",
    "start": "4596122",
    "end": "4603350"
  },
  {
    "text": "that comes up again and\nagain in statistical physics. And that is again natural,\nbecause there are constraints,",
    "start": "4603350",
    "end": "4610570"
  },
  {
    "text": "such as the average\nvalue of energy, average value of the\nnumber of particles, et cetera, that consistent\nwith maximizing their entropy,",
    "start": "4610570",
    "end": "4619470"
  },
  {
    "text": "give you forms such as this. So you can see that\na lot of concepts",
    "start": "4619470",
    "end": "4624850"
  },
  {
    "text": "that we will later on be\nusing in statistical physics",
    "start": "4624850",
    "end": "4629960"
  },
  {
    "text": "are already embedded in these\ndiscussions of probability. And we've also seen how the\nlarge N aspect comes about,",
    "start": "4629960",
    "end": "4638170"
  },
  {
    "text": "et cetera. So we now have the\nprobabilistic tools. And from next\ntime, we will go on",
    "start": "4638170",
    "end": "4646185"
  },
  {
    "text": "to define the\ndegrees of freedom. What are the units that we\nare going to be talking about?",
    "start": "4646185",
    "end": "4653110"
  },
  {
    "text": "And how to assign them some\nkind of a probabilistic picture. And then build on into\nstatistical mechanics.",
    "start": "4653110",
    "end": "4660180"
  },
  {
    "text": "Yes. AUDIENCE: So here,\nyou write the letter i to represent, in this case, the\nresults of a random die roll,",
    "start": "4660180",
    "end": "4666558"
  },
  {
    "text": "that you can replace it with any\nfunction of a random variable. PROFESSOR: Exactly. So I could have, maybe\nrather than giving me",
    "start": "4666558",
    "end": "4674775"
  },
  {
    "text": "the average value of the number\nthat was appearing on the face, they would have given\nme the average inverse.",
    "start": "4674775",
    "end": "4680144"
  },
  {
    "text": " And then I would have had this.",
    "start": "4680145",
    "end": "4685290"
  },
  {
    "text": " I could have had\nmultiple things. So maybe somebody else\nmeasures something else.",
    "start": "4685290",
    "end": "4692530"
  },
  {
    "text": "And then my general\nform would be e to the minus beta\nmeasurement of type one,",
    "start": "4692530",
    "end": "4699460"
  },
  {
    "text": "minus beta 2 measurement\nof type two, et cetera. And the rest of thing over\nhere is clearly just a constant",
    "start": "4699460",
    "end": "4705975"
  },
  {
    "text": "of proportionality\nthat I would need to adjust for the normalization. ",
    "start": "4705975",
    "end": "4713970"
  },
  {
    "text": "OK? So that's it for today. ",
    "start": "4713970",
    "end": "4718711"
  }
]