[
  {
    "text": " PROFESSOR: We are still\nin chapter 12 on the sum-product algorithm.",
    "start": "0",
    "end": "5779"
  },
  {
    "text": "I hope to get through that\ntoday, fairly expeditiously, and to start with chapter 13.",
    "start": "5780",
    "end": "13720"
  },
  {
    "text": "The handouts for today\nwill be chapter 13. And, as usual, problem set 9\nwith problem 8.3 has been",
    "start": "13720",
    "end": "24550"
  },
  {
    "text": "moved up into problem set 9. Problem set 8 solutions\nfor 8.1 and 8.2.",
    "start": "24550",
    "end": "33870"
  },
  {
    "text": "So I'm going to start off on a\ndifferent tack of presenting",
    "start": "33870",
    "end": "39449"
  },
  {
    "text": "the sum-product algorithm. Where is everybody? 1, 2, 3, 4, 5, 6, 7, 8, 9.",
    "start": "39450",
    "end": "46460"
  },
  {
    "text": "Is there any competitive event\nthat I should know about? I know the traffic was bad and\nit's a rainy, late morning.",
    "start": "46460",
    "end": "56870"
  },
  {
    "text": "Problem getting here myself. Well, this is improving. ",
    "start": "56870",
    "end": "67125"
  },
  {
    "text": "The write up of the sum-product\nin the notes is in terms of equations.",
    "start": "67125",
    "end": "72220"
  },
  {
    "text": "And I've never been terribly\nsatisfied with this write up.",
    "start": "72220",
    "end": "79940"
  },
  {
    "text": "It's concise, but we have to\ninvent some new notation,",
    "start": "79940",
    "end": "85150"
  },
  {
    "text": "which I don't consider to be\ncompletely transparent. I've never found it particularly\neasy to present",
    "start": "85150",
    "end": "92950"
  },
  {
    "text": "in class, and it's been a little\nfrustrating to me. So I'd like to try a\ndifferent approach.",
    "start": "92950",
    "end": "102020"
  },
  {
    "text": "I'd like in class just to go\nover through a very explicit example, our favorite\n844 code.",
    "start": "102020",
    "end": "110720"
  },
  {
    "text": "What we're trying to achieve\nwith the sum-product algorithm and how the sum-product\nalgorithm helps us to achieve",
    "start": "110720",
    "end": "119110"
  },
  {
    "text": "it in an efficient way. So it's proof by example. It won't be very satisfying to\nthose of you who like to see",
    "start": "119110",
    "end": "126260"
  },
  {
    "text": "real proofs, but that you can\nget by going back and re-reading the notes.",
    "start": "126260",
    "end": "132140"
  },
  {
    "text": "So I'm going to try doing it\nthis way and you will let me",
    "start": "132140",
    "end": "137190"
  },
  {
    "text": "know by your looks of gladness\nor of frustration whether you",
    "start": "137190",
    "end": "142380"
  },
  {
    "text": "like it or not. So let's start at the\nbeginning again. What are we trying to do?",
    "start": "142380",
    "end": "149820"
  },
  {
    "text": "We're trying to compute the a\nposteriori probability of every possible value of every\nsymbol, internal and external,",
    "start": "149820",
    "end": "157810"
  },
  {
    "text": "that appears in a graphical\nrealization for a code. And we will consider these\nto be APP vectors.",
    "start": "157810",
    "end": "171080"
  },
  {
    "text": "In other words, if a symbol is\neight valued, there will be eight APP values\nin the vector.",
    "start": "171080",
    "end": "177002"
  },
  {
    "text": " And we're going to assume that\nwe have a cycle-free graph",
    "start": "177003",
    "end": "183510"
  },
  {
    "text": "realization, so that we can\nproceed by the cut-set idea to",
    "start": "183510",
    "end": "190209"
  },
  {
    "text": "solve independent parts\nof the graph. And at the very end, we'll relax\nthat and see whether we",
    "start": "190210",
    "end": "197030"
  },
  {
    "text": "can still do OK. All right, one concept that\nseems very simple in theory",
    "start": "197030",
    "end": "205540"
  },
  {
    "text": "but that some people don't get\ntill they really try to do it in practice is that an APP\nvector is really a likelihood",
    "start": "205540",
    "end": "215670"
  },
  {
    "text": "weight vector normalized so\nthat the sums of all the probabilities are equal to 1.",
    "start": "215670",
    "end": "223590"
  },
  {
    "text": "For instance, if we send plus\n1 or minus 1 through an",
    "start": "223590",
    "end": "229500"
  },
  {
    "text": "additive white Gaussian Noise\nchannel with sigma squared",
    "start": "229500",
    "end": "235220"
  },
  {
    "text": "equal to 1 or whatever, then\nthe probability of--",
    "start": "235220",
    "end": "242180"
  },
  {
    "text": "we call this y and this r, the\nprobability of r given y equals whatever.",
    "start": "242180",
    "end": "248510"
  },
  {
    "text": "This is probability of r given\ny, is 1 over square root of 2 pi sigma squared e to the minus\ny minus r-squared over 2",
    "start": "248510",
    "end": "261200"
  },
  {
    "text": "sigma-squared. And so we can compute that\nfor y equals 1 and y",
    "start": "261200",
    "end": "268020"
  },
  {
    "text": "equals minus 1. And we'll get a vector\nof two things.",
    "start": "268020",
    "end": "273690"
  },
  {
    "text": "The APP vector would then\nconsist of this evaluated for",
    "start": "273690",
    "end": "280280"
  },
  {
    "text": "y equals 1. Let me just take this part of\nit, a to the minus r minus 1",
    "start": "280280",
    "end": "287440"
  },
  {
    "text": "squared over 2 sigma squared\nand e to the minus r plus 1",
    "start": "287440",
    "end": "294090"
  },
  {
    "text": "squared over 2 sigma-squared.",
    "start": "294090",
    "end": "299370"
  },
  {
    "text": "Something proportional to these\ntwo vectors normalized and that's why I continue to\nuse this proportional to",
    "start": "299370",
    "end": "307050"
  },
  {
    "text": "symbol, which you may not\nhave seen before. Or maybe you have. So the APP vector in this case\nwould be proportional to--",
    "start": "307050",
    "end": "315900"
  },
  {
    "text": "well, actually 1 over square\nroot of 2 pi sigma-squared times each of these\ntwo things.",
    "start": "315900",
    "end": "322080"
  },
  {
    "text": "But this would basically tell\nyou how much weight to give to",
    "start": "322080",
    "end": "327789"
  },
  {
    "text": "plus 1 and minus 1 as possible\ntransmitted symbols. And all this matters --",
    "start": "327790",
    "end": "333490"
  },
  {
    "text": "there's one extra degree\nof freedom in here. You can scale this, the whole\nvector, by any scale factor",
    "start": "333490",
    "end": "340740"
  },
  {
    "text": "you like and it gives you\nthe same information. Because you can always normalize\nthe two terms to be equal to 1.",
    "start": "340740",
    "end": "346949"
  },
  {
    "text": "So if this, for instance, was\n0.12 and this was 0.03, that's",
    "start": "346950",
    "end": "356260"
  },
  {
    "text": "equivalent to 0.8 and 0.2.",
    "start": "356260",
    "end": "363770"
  },
  {
    "text": "Two probabilities. Actual a-posteriori\nprobabilities that sum to 1, all you've got to do is\nkeep the ratio right.",
    "start": "363770",
    "end": "370380"
  },
  {
    "text": "This is four times as large as\nthat, so what that really means is 0.8 and 0.2.",
    "start": "370380",
    "end": "376260"
  },
  {
    "text": "And in implementation, so the\nsum-product algorithm, we never really worry about\nconstant scaling factors.",
    "start": "376260",
    "end": "384020"
  },
  {
    "text": "As long as they appear in every\nterm we can just forget",
    "start": "384020",
    "end": "391009"
  },
  {
    "text": "about scaling. I mean we don't want things to\nget too large or too small, so we can apply an overall\nscaling factor.",
    "start": "391010",
    "end": "397120"
  },
  {
    "text": "But as long as we've got\nsomething, the relative weights, then we have what\nwe need to know. ",
    "start": "397120",
    "end": "407160"
  },
  {
    "text": "Let's go to the 844 code since\nwe know that very well and ask",
    "start": "407160",
    "end": "413570"
  },
  {
    "text": "ourselves what it is we're\ntrying to compute. We're given APP vectors for\neach of the eight received",
    "start": "413570",
    "end": "422240"
  },
  {
    "text": "bits, let's say. We transmit y, we receive some\nr, perhaps over an additive white Gaussian noise channel.",
    "start": "422240",
    "end": "428970"
  },
  {
    "text": "So we get a little 2 term APP\nvector for each of these y's, telling us the relative\nprobability that a 0 was sent",
    "start": "428970",
    "end": "437045"
  },
  {
    "text": "or a 1 was sent. That's called the intrinsic\ninformation. ",
    "start": "437045",
    "end": "443560"
  },
  {
    "text": "Let me just call that. So for each one we\nget p0 and-- ",
    "start": "443560",
    "end": "450080"
  },
  {
    "text": "let me make it p0 and q0, the\ntwo terms for y0; p1 and q1",
    "start": "450080",
    "end": "457000"
  },
  {
    "text": "for y1 and so forth. So we have that information\nfor each of the received symbols.",
    "start": "457000",
    "end": "462280"
  },
  {
    "start": "462280",
    "end": "469520"
  },
  {
    "text": "What's the likelihood of each of\nthese code words then given",
    "start": "469520",
    "end": "474870"
  },
  {
    "text": "these individual bitwise\nlikelihoods, the probability of the all zero code word.",
    "start": "474870",
    "end": "481390"
  },
  {
    "text": "It's likelihood, let's say, is\nthen proportional to p0, p1,",
    "start": "481390",
    "end": "491330"
  },
  {
    "text": "p2, p3, p4, p5, p6, p7.",
    "start": "491330",
    "end": "499000"
  },
  {
    "text": "Whereas the probability at 1, 1,\n1, 1, 0, 0, 0, 0 is q0, q1,",
    "start": "499000",
    "end": "505900"
  },
  {
    "text": "q2, q3, p4, p5, p6,\np7 and so forth.",
    "start": "505900",
    "end": "512909"
  },
  {
    "text": "Each one is a product of the\neight corresponding terms.",
    "start": "512909",
    "end": "518190"
  },
  {
    "text": "Yes? AUDIENCE: You're assuming that\nthe bits are independent.",
    "start": "518190",
    "end": "524000"
  },
  {
    "text": "PROFESSOR: Yes, it's a\nmemoryless channel. So the noise in every symbol\ntransmission is independent.",
    "start": "524000",
    "end": "530940"
  },
  {
    "text": "And that's important too. AUDIENCE: So basically once\nagain, the second bit is independent of the first\nbit in transmission?",
    "start": "530940",
    "end": "539180"
  },
  {
    "text": "PROFESSOR: I'm sorry,\nI was writing and I missed what you said. ",
    "start": "539180",
    "end": "544565"
  },
  {
    "text": "AUDIENCE: Well, I mean, if the\nfirst bit is 0, maybe that is a code where if the first\nbit is 0, the second",
    "start": "544565",
    "end": "552010"
  },
  {
    "text": "bit has to be 0. ",
    "start": "552010",
    "end": "557330"
  },
  {
    "text": "PROFESSOR: Yeah, there's\ncertainly dependencies among the code words. That's what we're trying\nto take into account.",
    "start": "557330",
    "end": "564070"
  },
  {
    "text": "This is how to take\nit into account. So I'm assuming a memoryless\nchannel. But I'm of course, assuming\ndependencies within the code.",
    "start": "564070",
    "end": "573110"
  },
  {
    "text": "How do I do that? One way is maximum likelihood\ndecoding.",
    "start": "573110",
    "end": "580000"
  },
  {
    "text": "What do we do when\nwe do maximum likelihood decoding in principle? We exhaustively computed each of\nthese 16 products, each of",
    "start": "580000",
    "end": "590630"
  },
  {
    "text": "these 16 likelihoods for each\nof the 16 code words. We'd simply pick the one that's\ngreatest and that's the",
    "start": "590630",
    "end": "597329"
  },
  {
    "text": "code word that is most likely\nto have been sent. So that's what exhaustive\nmaximum likelihood decoding",
    "start": "597330",
    "end": "605070"
  },
  {
    "text": "consists of, finding the maximum\nlikelihood word. And that clearly takes into\naccount the dependencies.",
    "start": "605070",
    "end": "612269"
  },
  {
    "text": "We're only considering\nvalid code sequences when we do that. ",
    "start": "612270",
    "end": "618920"
  },
  {
    "text": "In APP decoding we're doing\nsomething else, but it can be",
    "start": "618920",
    "end": "623959"
  },
  {
    "text": "characterized quite simply. What's the a-posteriori\nprobability that say, the",
    "start": "623960",
    "end": "629410"
  },
  {
    "text": "first bit, is equal to a\n0 or is equal to a 1?",
    "start": "629410",
    "end": "636170"
  },
  {
    "text": "We can get the APP by summing up\nthe likelihoods of all the",
    "start": "636170",
    "end": "643350"
  },
  {
    "text": "code words that are associated\nwith the value of the bit that we want to see. So we take the eight code words\nthat have a 0 in this",
    "start": "643350",
    "end": "650900"
  },
  {
    "text": "place and we'd sum up these\nlikelihoods for those eight code words, and that would be\nthe weight, the likelihood",
    "start": "650900",
    "end": "658009"
  },
  {
    "text": "weight of y0 being 0. And the likelihood weight of y0\nbeing 1 would be the sum of",
    "start": "658010",
    "end": "665920"
  },
  {
    "text": "the other eight. And again you see that\nscale factors aren't going to matter here.",
    "start": "665920",
    "end": "671240"
  },
  {
    "text": "All we want is the relative\nweights of these things. So that's what we're trying to\ndo in APP decoding, but we're",
    "start": "671240",
    "end": "678110"
  },
  {
    "text": "trying to do it now for\nevery single variable. At this point, I've only shown\nyou the symbol variables, the",
    "start": "678110",
    "end": "684490"
  },
  {
    "text": "external variables. So a brute force way of doing\nthat would be to fill out this",
    "start": "684490",
    "end": "690040"
  },
  {
    "text": "table, all 16 likelihoods, do\nthese sums and then we'd have",
    "start": "690040",
    "end": "695600"
  },
  {
    "text": "the APP vector for\neach of these. ",
    "start": "695600",
    "end": "700990"
  },
  {
    "text": "Now we're looking for a more\nefficient way of doing this. The efficient way of doing this\nis going to be based on",
    "start": "700990",
    "end": "707700"
  },
  {
    "text": "the fact we have a cycle-free\ngraph realization. Which in this case, is a\ntrellis realization.",
    "start": "707700",
    "end": "713810"
  },
  {
    "text": "I'm going to use this two\nsection trellis where each",
    "start": "713810",
    "end": "720300"
  },
  {
    "text": "section is a four-tuple. We've drawn it two\ndifferent ways. This is a very explicit way\nshowing the two parallel",
    "start": "720300",
    "end": "728940"
  },
  {
    "text": "transitions that go from the\ninitial node to the central state, and then two\nmore down here.",
    "start": "728940",
    "end": "735650"
  },
  {
    "text": "The set of code words\nis the set of all possible paths through-- corresponds to the set of all\npaths through this trellis.",
    "start": "735650",
    "end": "743080"
  },
  {
    "text": "So for instance, includes the\nall zero word, (0,0,1,1,1), (1,1,1,0,0,0), (1,1,1,1,1,1,1),\nand so forth.",
    "start": "743080",
    "end": "751740"
  },
  {
    "text": "And these I happened to have\nlisted as the first four here. ",
    "start": "751740",
    "end": "757829"
  },
  {
    "text": "Or we now have this more\nabstract way of writing this",
    "start": "757830",
    "end": "763210"
  },
  {
    "text": "same thing. This says there are four\nexternal variables and two",
    "start": "763210",
    "end": "768660"
  },
  {
    "text": "state variables or a vector\nspace of dimension 2 here and",
    "start": "768660",
    "end": "774029"
  },
  {
    "text": "a vector space with\ndimension 4 here. That the dimension of this\nconstraint is 3.",
    "start": "774030",
    "end": "781370"
  },
  {
    "text": "That means there are eight\npossible values for these",
    "start": "781370",
    "end": "788240"
  },
  {
    "text": "combinations of 4 and 2. They're shown explicitly\nup here. And they form a linear\nvector space.",
    "start": "788240",
    "end": "796315"
  },
  {
    "start": "796315",
    "end": "802370"
  },
  {
    "text": "Either way, this is our\ntrellis realization.",
    "start": "802370",
    "end": "807470"
  },
  {
    "text": "Now, we know that to do maximum\nlikelihood decoding,",
    "start": "807470",
    "end": "812920"
  },
  {
    "text": "it's useful to have this trellis\nrealization, because, for instance, we can do\nViterbi decoding.",
    "start": "812920",
    "end": "818839"
  },
  {
    "text": "And that's a more efficient\nway of finding what the maximum likelihood\nsequence is.",
    "start": "818840",
    "end": "824410"
  },
  {
    "text": "Basically, because we go through\nand at this point we can make a decision between\n0, 0, 0, and 1, 1, 1, 1.",
    "start": "824410",
    "end": "832760"
  },
  {
    "text": "We only have to do that based on\nthis four-tuple and then we can live with that decision\nregardless of what else we see",
    "start": "832760",
    "end": "839110"
  },
  {
    "text": "in the other half. And likewise in the\nother direction. ",
    "start": "839110",
    "end": "844570"
  },
  {
    "text": "So now we'd like to use this\nsame structure to simplify the APP decoding calculation,\nwhich looks like a more",
    "start": "844570",
    "end": "852910"
  },
  {
    "text": "complicated calculation. So how do we do that? We introduce two state\nbits here.",
    "start": "852910",
    "end": "862970"
  },
  {
    "text": "All of these have state\nbit 0, 0, 0, 0.",
    "start": "862970",
    "end": "868230"
  },
  {
    "text": "Sorry, this point, we've\ncalled this state 0, 1. ",
    "start": "868230",
    "end": "875290"
  },
  {
    "text": "Just to keep it visually\nstraight, 0, 1, 0, 1, 0, 1,",
    "start": "875290",
    "end": "880430"
  },
  {
    "text": "and then there are eight more\ncorresponding to the other two possible values of the\nstate bits over here.",
    "start": "880430",
    "end": "887210"
  },
  {
    "start": "887210",
    "end": "894520"
  },
  {
    "text": "This is a quaternary variable. We want it to assume that it's\na four-valued variable and to",
    "start": "894520",
    "end": "900940"
  },
  {
    "text": "make the realization cycle-free,\nI don't want to consider it to be two\nindependent bits.",
    "start": "900940",
    "end": "906000"
  },
  {
    "text": "So there are four\npossible values for this state variable. So maybe I just ought to call\nthis my state space --",
    "start": "906000",
    "end": "913345"
  },
  {
    "text": " which is a four-valued\nstate space. ",
    "start": "913345",
    "end": "920970"
  },
  {
    "text": "Now, what's the a-posteriori\nprobability of the state",
    "start": "920970",
    "end": "928329"
  },
  {
    "text": "being say, 0, 0? I compute that in\nthe same way.",
    "start": "928330",
    "end": "933930"
  },
  {
    "text": "I compute that simply by\nsumming up the four likelihoods of the code for the\nexternal variables, the",
    "start": "933930",
    "end": "943170"
  },
  {
    "text": "code words that are associated\nwith state variable 0, 0 --",
    "start": "943170",
    "end": "949404"
  },
  {
    "text": "the first possible values of\nthis quaternary variable. So it'd be the sum of\nthese four things.",
    "start": "949405",
    "end": "956660"
  },
  {
    "text": "Yes? AUDIENCE:\n[UNINTELLIGIBLE PHRASE] ",
    "start": "956660",
    "end": "961990"
  },
  {
    "text": "PROFESSOR: I've only listed\nhalf the code words. This is only 8 of the\n16 code words.",
    "start": "961990",
    "end": "967630"
  },
  {
    "text": "You want me to write\nthe other 8? ",
    "start": "967630",
    "end": "975010"
  },
  {
    "text": "And by the way, there's at least\nan implicit assumption",
    "start": "975010",
    "end": "980030"
  },
  {
    "text": "here that the code sequence\ndetermines the state sequence. As we found when we did minimal\ntrellis realizations,",
    "start": "980030",
    "end": "989629"
  },
  {
    "text": "the code sequence always does\ndetermine the state sequence. So if I know the entire code\nword that also tells me what",
    "start": "989630",
    "end": "997110"
  },
  {
    "text": "the values of all the\nstate variables are. There's a 1:1 correspondence\nbetween code words and",
    "start": "997110",
    "end": "1002240"
  },
  {
    "text": "trajectories in a\nminimal trellis. And I believe I'm sometimes\nimplicitly using that",
    "start": "1002240",
    "end": "1012750"
  },
  {
    "text": "assumption. But anyway, it holds whenever\nwe have a cycle-free graph. In a cycle-free graph, we have\na well-defined minimal",
    "start": "1012750",
    "end": "1019690"
  },
  {
    "text": "realization. And the minimal realization does\nhave this 1:1 property. So the state variables\nare determined",
    "start": "1019690",
    "end": "1026619"
  },
  {
    "text": "by the symbol variables. ",
    "start": "1026619",
    "end": "1033410"
  },
  {
    "text": "Suppose I want to compute the\nvalues of these state variables here.",
    "start": "1033410",
    "end": "1038800"
  },
  {
    "text": "Let me write this out. This will be p0, p1,\np2, p3, q1--",
    "start": "1038800",
    "end": "1046874"
  },
  {
    "text": "sorry --- q4, q5, q6, q7.",
    "start": "1046874",
    "end": "1052330"
  },
  {
    "text": "And then there's one\nthat's all q's. q0, q1, q2, q3, q4,\nq5, q6, q7.",
    "start": "1052330",
    "end": "1061370"
  },
  {
    "text": "So basically, what I want to\ndo is take the sum of these four things, and that's just a\nsum of products and I could",
    "start": "1061370",
    "end": "1068440"
  },
  {
    "text": "just do it. But the Cartesian product lemma\ngives us a simpler way",
    "start": "1068440",
    "end": "1074910"
  },
  {
    "text": "of doing it. We notice that I can have\neither of these two",
    "start": "1074910",
    "end": "1082980"
  },
  {
    "text": "four-tuples as the first half\nand either of these two four-tuples as a second half.",
    "start": "1082980",
    "end": "1088159"
  },
  {
    "text": "In fact, I see over here,\nexplicitly, these four code words are the Cartesian product\nof two four-tuple",
    "start": "1088160",
    "end": "1097380"
  },
  {
    "text": "codes, the repetition\ncode on 4. So I can use the Cartesian\nproduct lemma to",
    "start": "1097380",
    "end": "1106480"
  },
  {
    "text": "write this as -- again, let me write it\nvery explicitly.",
    "start": "1106480",
    "end": "1111970"
  },
  {
    "text": "p0, p1, p2, p3, which you\nrecognize as the probability",
    "start": "1111970",
    "end": "1119140"
  },
  {
    "text": "of this four-tuple plus\nq0, q1, q2, q3.",
    "start": "1119140",
    "end": "1125780"
  },
  {
    "text": "The probability of this\nfour-tuple times p4, p5, p6,",
    "start": "1125780",
    "end": "1135060"
  },
  {
    "text": "p7 plus q4, q5, q6, q7.",
    "start": "1135060",
    "end": "1145840"
  },
  {
    "text": "Now do you agree that the\nproduct of those two terms is equal to this, the sum\nof these four terms?",
    "start": "1145840",
    "end": "1153250"
  },
  {
    "text": "Yes, there are four terms\nin this sum and they're what we want. And it's because we have this\nCartesian product structure,",
    "start": "1153250",
    "end": "1162190"
  },
  {
    "text": "which we always have\nin the state space. This is the basic Markov\nproperty of a state variable.",
    "start": "1162190",
    "end": "1167905"
  },
  {
    "text": " So that's a simpler\ncalculation. ",
    "start": "1167905",
    "end": "1177510"
  },
  {
    "text": "This requires four\nmultiplications, each one is seven-fold multiplication.",
    "start": "1177510",
    "end": "1184420"
  },
  {
    "text": "This only requires one multip --\nwell, it requires one, two,",
    "start": "1184420",
    "end": "1191320"
  },
  {
    "text": "three, four-fold\nmultiplications, plus one overall multiplication.",
    "start": "1191320",
    "end": "1197060"
  },
  {
    "text": "And it's certainly a simpler\nway to compute that. ",
    "start": "1197060",
    "end": "1204080"
  },
  {
    "text": "And I can similarly do that for\neach of these down here. ",
    "start": "1204080",
    "end": "1213530"
  },
  {
    "text": "What this leads me to is what\nI called last time the past future decomposition.",
    "start": "1213530",
    "end": "1223890"
  },
  {
    "text": "We have one APP vector over here\nthat corresponds to the--",
    "start": "1223890",
    "end": "1231656"
  },
  {
    "text": "let's call it the APP vector\nof the state given the past",
    "start": "1231656",
    "end": "1244770"
  },
  {
    "text": "received symbols. We had something like that. In other words, given what was\nreceived for y0, y1, y2, y3,",
    "start": "1244770",
    "end": "1253850"
  },
  {
    "text": "what's the probability, the\npartial a-posteriori probability for each of the four\npossible state variables",
    "start": "1253850",
    "end": "1262540"
  },
  {
    "text": "given this past? And similarly, we have another\nvector which I will consider",
    "start": "1262540",
    "end": "1270860"
  },
  {
    "text": "to be a message coming in this\ndirection, a message consisting of a vector\nof four values.",
    "start": "1270860",
    "end": "1277370"
  },
  {
    "text": "Which is what's the probability\nthe state vector given these four received\nsimple, which is computed in",
    "start": "1277370",
    "end": "1286105"
  },
  {
    "text": "the same way? That's this here. And the past future rule is\nthat just the overall",
    "start": "1286105",
    "end": "1292500"
  },
  {
    "text": "probability for any of these\ninternal state variables, the overall APP vector, is just\nobtained by a component-wise",
    "start": "1292500",
    "end": "1303330"
  },
  {
    "text": "multiplication.  This would be the APP of the\nstate vector being equal to 0,",
    "start": "1303330",
    "end": "1311850"
  },
  {
    "text": "0 given the past. This would be APP of the\nstate vector given",
    "start": "1311850",
    "end": "1317080"
  },
  {
    "text": "0, 0 given the future. We'd have the same thing\nfor 0, 1, 1, 0, 1, 1.",
    "start": "1317080",
    "end": "1322789"
  },
  {
    "text": "And to get the overall APP\nvector given all of r, the rule is just multiply\nthem component-wise.",
    "start": "1322790",
    "end": "1330415"
  },
  {
    "text": " We take the likelihood weight\nfor 0, 0 given the past and",
    "start": "1330415",
    "end": "1337150"
  },
  {
    "text": "multiply it by likelihood\nweight of 0, 0 given the future. And that gives us the total\nlikelihood weight up to a",
    "start": "1337150",
    "end": "1343840"
  },
  {
    "text": "scale factor. ",
    "start": "1343840",
    "end": "1351340"
  },
  {
    "text": "The notes derive this\nin equation terms. You see it basically comes from\nthis Cartesian product",
    "start": "1351340",
    "end": "1358929"
  },
  {
    "text": "decomposition, which we\nalways get for any internal state variable.",
    "start": "1358930",
    "end": "1364250"
  },
  {
    "text": "The possible code words\nconsistent with that state consist of a certain set of\npossible past code words",
    "start": "1364250",
    "end": "1373400"
  },
  {
    "text": "consistent with that state. Cartesian product with a set of\npossible future code words consistent with that state.",
    "start": "1373400",
    "end": "1379920"
  },
  {
    "text": "And as a result of that\nCartesian product decomposition, we always get\nthis APP vector decomposition.",
    "start": "1379920",
    "end": "1386530"
  },
  {
    "text": "That's what the proof\nsays in the notes. Do you like this\nway of arguing?",
    "start": "1386530",
    "end": "1391555"
  },
  {
    "text": " A couple people are\nnodding at least. It's an experiment.",
    "start": "1391555",
    "end": "1398540"
  },
  {
    "text": "All right, so that tells us what\nwe're going to want to do, at least, for the\ninternal variables.",
    "start": "1398540",
    "end": "1404330"
  },
  {
    "text": "And actually, the same\nis true up here. The variables going\nin are easy.",
    "start": "1404330",
    "end": "1409500"
  },
  {
    "text": "That's just p0, p1, p2,\np3, or whatever. The ones coming out are--",
    "start": "1409500",
    "end": "1415700"
  },
  {
    "text": "this is called the intrinsic\nvariable, the one that we get from direct observation,\nthe intrinsic APP",
    "start": "1415700",
    "end": "1422700"
  },
  {
    "text": "vector, excuse me. The extrinsic APP vector is the\none that we get from all",
    "start": "1422700",
    "end": "1428179"
  },
  {
    "text": "the rest of the inputs and\nsymbols and the other parts of the graph. And we combine these\ncomponent-wise to get the",
    "start": "1428180",
    "end": "1435450"
  },
  {
    "text": "overall APP vector.",
    "start": "1435450",
    "end": "1440919"
  },
  {
    "text": "All right, but we now\nhave to do this for",
    "start": "1440920",
    "end": "1447280"
  },
  {
    "text": "every part of the graph. ",
    "start": "1447280",
    "end": "1453410"
  },
  {
    "text": "Let me go through another\ncalculation, which will--",
    "start": "1453410",
    "end": "1458540"
  },
  {
    "text": "how do we, for instance, find\nthe APP vector for y7, the",
    "start": "1458540",
    "end": "1463880"
  },
  {
    "text": "extrinsic APP vector for y7,\ngiven that we have a trellis like this?",
    "start": "1463880",
    "end": "1469840"
  },
  {
    "text": " In general, for y7, we're going\nto be adding up all the",
    "start": "1469840",
    "end": "1477910"
  },
  {
    "text": "zeros, these guys. And four more down here, and\nwe'll make that the extrinsic",
    "start": "1477910",
    "end": "1485059"
  },
  {
    "text": "APP vector of 0. So we want to add up this and\nthis and this and this and so",
    "start": "1485060",
    "end": "1493550"
  },
  {
    "text": "forth, eight of those. We want to do that in\nan efficient way. ",
    "start": "1493550",
    "end": "1509180"
  },
  {
    "text": "This defines a code word with\neight possible components.",
    "start": "1509180",
    "end": "1514465"
  },
  {
    "text": "Let me move this up now. So let's talk about\nthis 6, 3 code.",
    "start": "1514465",
    "end": "1520640"
  },
  {
    "text": "What does it look like? With state vectors 0, 0,\nI can have y4, y5, y6,",
    "start": "1520640",
    "end": "1531289"
  },
  {
    "text": "y7 be 0, 0, 0, 0. Or I can have it\nbe 1, 1, 1, 1.",
    "start": "1531290",
    "end": "1536919"
  },
  {
    "text": "The state vector is 0, 1. I can have 0, 1, 0, 1 or I\ncould have 1, 0, 1, 0.",
    "start": "1536920",
    "end": "1546210"
  },
  {
    "text": "With state vectors variable\n1, 0, I can have 0, 0, 1,",
    "start": "1546210",
    "end": "1551830"
  },
  {
    "text": "1 or 1, 1, 0, 0. And with 1, 1, I can have 0, 1,\n1, 0 or 1, 1, 1, 0, 0, 1.",
    "start": "1551830",
    "end": "1567550"
  },
  {
    "text": "All right, so that's\nprecisely the 6, 3 code I'm talking about. ",
    "start": "1567550",
    "end": "1574440"
  },
  {
    "text": "Now suppose I have the APP\nvector for each of these.",
    "start": "1574440",
    "end": "1579649"
  },
  {
    "text": "The APP vector for each of\nthese, 0, 0 we've already calculated.",
    "start": "1579650",
    "end": "1585090"
  },
  {
    "text": "It'd be p0, p1, p2, p3\nplus q0, q1, q2, q3.",
    "start": "1585090",
    "end": "1592510"
  },
  {
    "text": "That's the likelihood vector for\n0, 0 coming from the past.",
    "start": "1592510",
    "end": "1600070"
  },
  {
    "text": "So that's one part of\nthis message here. For either of these I get\np0, q1, p2, q2, q3",
    "start": "1600070",
    "end": "1615830"
  },
  {
    "text": "plus q0, p1, q2, p3.",
    "start": "1615830",
    "end": "1621830"
  },
  {
    "text": "Reflecting the two possible ways\nI can get to this value. ",
    "start": "1621830",
    "end": "1630059"
  },
  {
    "text": "I'm going to get four terms\nthat I've computed for the past message.",
    "start": "1630060",
    "end": "1637600"
  },
  {
    "text": "I'm going to have four\nlikelihood weights corresponding to each\nof the four values",
    "start": "1637600",
    "end": "1643100"
  },
  {
    "text": "of this state variable. So I've got that vector. I already computed it as part\nof the computation to do the",
    "start": "1643100",
    "end": "1650490"
  },
  {
    "text": "APP for here. It's the past part of it. ",
    "start": "1650490",
    "end": "1656730"
  },
  {
    "text": "So now, I want to compute the\nprobability that y7 is a 0.",
    "start": "1656730",
    "end": "1665315"
  },
  {
    "text": " The rule here is to take every\ncombination in this code and",
    "start": "1665315",
    "end": "1676750"
  },
  {
    "text": "again, I'm going to get a-- well, let me write\nthis part of it. This is going to be\np4, p5, p6, p7.",
    "start": "1676750",
    "end": "1685990"
  },
  {
    "text": "That's the probability\nof this four-tuple. This is q4, q5, q6, q7.",
    "start": "1685990",
    "end": "1693710"
  },
  {
    "text": "This is p4, q5, p6, q7, q4,\np5, q6, p7 and so forth.",
    "start": "1693710",
    "end": "1706659"
  },
  {
    "text": "That's the weights I want to\nassign to each of these. So now I get a weight for each\nof these six-tuples.",
    "start": "1706660",
    "end": "1716940"
  },
  {
    "text": "I multiply this by this to get\nthe weight, the likelihood weight, for this code word.",
    "start": "1716940",
    "end": "1723440"
  },
  {
    "text": "This by this to get the\nlikelihood weight for this code word and so forth. And I add these to\nbuckets for--",
    "start": "1723440",
    "end": "1734460"
  },
  {
    "text": "I simply go through this and\nI'll add this one to the bucket for y7 equals 0. And I'll add this to the\nbucket for y7 equals 1.",
    "start": "1734460",
    "end": "1742030"
  },
  {
    "text": "And this to the bucket\nfor y7 equals 1. And this to the bucket for\ny7 equals 0 and so forth.",
    "start": "1742030",
    "end": "1748450"
  },
  {
    "start": "1748450",
    "end": "1755750"
  },
  {
    "text": "It's clear that in this bucket\nI'm always going to get the same value for p--",
    "start": "1755750",
    "end": "1761800"
  },
  {
    "text": "I'm always going to get a\np7 for 0 and a q7 for 1.",
    "start": "1761800",
    "end": "1767310"
  },
  {
    "text": "So just looking at y7, I don't\nhave to compute the seventh",
    "start": "1767310",
    "end": "1773060"
  },
  {
    "text": "value here. So if I did this separately as a\nlittle y7 variable, then the",
    "start": "1773060",
    "end": "1782430"
  },
  {
    "text": "incoming message, the intrinsic\ninformation, would just be this p7, q7 likelihood\nweight vector.",
    "start": "1782430",
    "end": "1791070"
  },
  {
    "text": " And what I really want to\ncompute is the outgoing part,",
    "start": "1791070",
    "end": "1800919"
  },
  {
    "text": "which has to do with\ny4 through y6. So let me draw it that way.",
    "start": "1800920",
    "end": "1807720"
  },
  {
    "text": "This I don't think is very\ngood exposition. Bear with me.",
    "start": "1807720",
    "end": "1813200"
  },
  {
    "text": "So now I'm going to put this\ntimes this in my bucket for 0.",
    "start": "1813200",
    "end": "1820250"
  },
  {
    "text": "This times this in my bucket\nfor 1, and so forth.",
    "start": "1820250",
    "end": "1825950"
  },
  {
    "text": "The question here is, does this\nmethod give me the same answer as my exhaustive\nmethod up here?",
    "start": "1825950",
    "end": "1834220"
  },
  {
    "text": "And again, because of the\nCartesian product character of the states, you can convince\nyourself that it does.",
    "start": "1834220",
    "end": "1841980"
  },
  {
    "text": "In other words, I can use this\nas a summary of everything in the past that got to the state\nvalue equal to 0, 0, 0.",
    "start": "1841980",
    "end": "1850880"
  },
  {
    "text": "Any time I got to 0, 0,\n0, I got through one of these two things.",
    "start": "1850880",
    "end": "1856690"
  },
  {
    "text": "And so for any possible\ncontinuation over here, I",
    "start": "1856690",
    "end": "1862320"
  },
  {
    "text": "could have got to it in\none of these two ways. And I'm going to find terms,\ne.g., these two terms.",
    "start": "1862320",
    "end": "1868995"
  },
  {
    "text": " Let's see. What do I want?",
    "start": "1868995",
    "end": "1874540"
  },
  {
    "text": "I want two ways of getting\nto 0, 0, 0. I'm sorry, these two.",
    "start": "1874540",
    "end": "1880940"
  },
  {
    "text": "I claim that this times\nthese is equal to the sum of these two.",
    "start": "1880940",
    "end": "1888310"
  },
  {
    "text": "And it is. I can get to 0, 0, 0, 0, 0\neither through this or this.",
    "start": "1888310",
    "end": "1894260"
  },
  {
    "text": "And I get to 1, 1, 1, 1, which\nis this one, either through",
    "start": "1894260",
    "end": "1899320"
  },
  {
    "text": "this or this. And again, it's a fundamental\nproperty of the state that whenever I get one of these, I'm\ngoing to get both of them",
    "start": "1899320",
    "end": "1906850"
  },
  {
    "text": "in equal combination. So I can summarize things in\nthis way, which leads to an",
    "start": "1906850",
    "end": "1914210"
  },
  {
    "text": "efficiency of computation. So what I'm basically explaining\nto you now is the",
    "start": "1914210",
    "end": "1921550"
  },
  {
    "text": "sum-product update rule, which\nis how to propagate messages.",
    "start": "1921550",
    "end": "1932865"
  },
  {
    "text": " Propagating messages, which\nare APP vectors.",
    "start": "1932865",
    "end": "1944424"
  },
  {
    "start": "1944425",
    "end": "1949860"
  },
  {
    "text": "In general, my situation is\ngoing to be like this. I'm going to have some\nconstraint code, (n,k), I'm",
    "start": "1949860",
    "end": "1956680"
  },
  {
    "text": "going to have some incoming\nmessages over here which tell",
    "start": "1956680",
    "end": "1962160"
  },
  {
    "text": "me about everything that's\nhappened in this past, p prime, and in this\npast, p prime.",
    "start": "1962160",
    "end": "1968400"
  },
  {
    "text": "Give me a summary of all the\nweights corresponding to that. I'm going to have some output\nsymbol over here and I want to",
    "start": "1968400",
    "end": "1976980"
  },
  {
    "text": "compute now what the APP vector\nis for the possible",
    "start": "1976980",
    "end": "1982530"
  },
  {
    "text": "values of this output symbol,\nwhich is going to be some aggregate of this down here.",
    "start": "1982530",
    "end": "1988010"
  },
  {
    "text": "What I do is I go through all\n2 to the k code words, call this constraint code.",
    "start": "1988010",
    "end": "1993900"
  },
  {
    "text": "Sorry, I'm using k again. That's 2 to the k code words.",
    "start": "1993900",
    "end": "1998930"
  },
  {
    "text": "And compute the product\nof the inputs. ",
    "start": "1998930",
    "end": "2010130"
  },
  {
    "text": "Of input APP vectors according\nto the possible combinations",
    "start": "2010130",
    "end": "2019620"
  },
  {
    "text": "of the input variables and the\noutput variables that are allowed by this constraint. ",
    "start": "2019620",
    "end": "2026390"
  },
  {
    "text": "That's explicitly what\nI'm doing here. Here are all the possible\ncombinations of state",
    "start": "2026390",
    "end": "2032410"
  },
  {
    "text": "variables and symbol variables\nthat are allowed by this 6, 3 constraint code. There are eight of them.",
    "start": "2032410",
    "end": "2039410"
  },
  {
    "text": "For each one, I'm going to\ncompute the relevant product",
    "start": "2039410",
    "end": "2045420"
  },
  {
    "text": "of APP vectors and I'm going\nto add it to a bin. So add to a bin.",
    "start": "2045420",
    "end": "2053888"
  },
  {
    "text": "Bins which correspond to-- draw it like that--",
    "start": "2053889",
    "end": "2060580"
  },
  {
    "text": "output. I'm calling this the output\nvariable values. ",
    "start": "2060580",
    "end": "2074840"
  },
  {
    "text": "OK, and that'll give me the\nsummary APP vector of the outputs for now the past, which\nconsists of everything",
    "start": "2074840",
    "end": "2085960"
  },
  {
    "text": "that could have happened in\nthis side of the graph. Again, I'm using the cycle-free\nassumption in",
    "start": "2085960",
    "end": "2094219"
  },
  {
    "text": "several ways. I'm saying that everything\nthat happens up here is independent of everything\nthat happens here.",
    "start": "2094219",
    "end": "2101770"
  },
  {
    "text": "Everything that happens\ncompletely in this past is just the sum of what\nhappens here.",
    "start": "2101770",
    "end": "2107140"
  },
  {
    "text": "What happens here are subject\nto this constraint and has nothing to do with what happens",
    "start": "2107140",
    "end": "2112920"
  },
  {
    "text": "over here in the future. Eventually I'm going to do the\nsame thing in the future.",
    "start": "2112920",
    "end": "2119000"
  },
  {
    "text": "I'm going to compute a future\ncomponent that's based on all this stuff. I'm going to get messages going\nin each direction and",
    "start": "2119000",
    "end": "2126130"
  },
  {
    "text": "I'm going to sum them up.  OK, now I'm seeing a fair\namount of puzzlement on",
    "start": "2126130",
    "end": "2134130"
  },
  {
    "text": "people's faces. ",
    "start": "2134130",
    "end": "2140410"
  },
  {
    "text": "Do you get abstractly what\nI'm trying to do here? Does anyone want to ask\na clarifying question?",
    "start": "2140410",
    "end": "2147415"
  },
  {
    "start": "2147415",
    "end": "2162450"
  },
  {
    "text": "So now-- AUDIENCE:\n[UNINTELLIGIBLE PHRASE] the bottom right,\noutput variable?",
    "start": "2162450",
    "end": "2168569"
  },
  {
    "text": "PROFESSOR: Output variable\nvalues, I'm sorry. ",
    "start": "2168570",
    "end": "2179080"
  },
  {
    "text": "I think there's a little bit\nof confusion here because sometimes I've considered\nthis output to",
    "start": "2179080",
    "end": "2184240"
  },
  {
    "text": "be a 16-valued output. Here, in fact, well, it has\neight actually valid values.",
    "start": "2184240",
    "end": "2191840"
  },
  {
    "text": "And if I was considering it that\nway, then I would just have one bin for each of\nthese likelihoods.",
    "start": "2191840",
    "end": "2198599"
  },
  {
    "text": "But now if I consider these as\nfour binary variables, then I",
    "start": "2198600",
    "end": "2203630"
  },
  {
    "text": "would aggregate these things\naccording to the values of each of the binary values.",
    "start": "2203630",
    "end": "2210560"
  },
  {
    "text": "And I'd get four vectors\nof length 2 for",
    "start": "2210560",
    "end": "2216330"
  },
  {
    "text": "the individual APP. Which is ultimately what I want\nhere, so I'm sorry I've",
    "start": "2216330",
    "end": "2221900"
  },
  {
    "text": "gone back and forth between\nsubtly different realizations.",
    "start": "2221900",
    "end": "2228339"
  },
  {
    "text": "I do find this very\nhard to explain. Perhaps there's someone else\nwho can do a better job.",
    "start": "2228340",
    "end": "2235119"
  },
  {
    "text": "You have a homework problem\nwhich asks you to do this, and I think having done that you'll\nthen get the idea.",
    "start": "2235120",
    "end": "2243410"
  },
  {
    "text": "But it's a matter of going back\nand forth between the gross structure and the\nactual equations and",
    "start": "2243410",
    "end": "2248770"
  },
  {
    "text": "just working it out. ",
    "start": "2248770",
    "end": "2257090"
  },
  {
    "text": "All right, so the key things in\nthe sum-product algorithm.",
    "start": "2257090",
    "end": "2263980"
  },
  {
    "text": "I've already explained\nsome level to them.",
    "start": "2263980",
    "end": "2269820"
  },
  {
    "text": "We have this sum-product\nupdate. ",
    "start": "2269820",
    "end": "2275400"
  },
  {
    "text": "That basically explains given\nmessages coming into a node,",
    "start": "2275400",
    "end": "2282779"
  },
  {
    "text": "how do we compute the message\ngoing out of the node? We take contributions, products\ncorresponding to what",
    "start": "2282780",
    "end": "2295090"
  },
  {
    "text": "the code allows. We dump them into the\nappropriate bins in the output vector and that's the\nsum-product update rule.",
    "start": "2295090",
    "end": "2302901"
  },
  {
    "text": " We have the past future\ndecomposition",
    "start": "2302902",
    "end": "2310063"
  },
  {
    "text": "or combination rule. And that says at the end of the\nday, you're finally going",
    "start": "2310063",
    "end": "2317140"
  },
  {
    "text": "to get messages going\nin both directions. And you just combine them\ncomponent-wise as",
    "start": "2317140",
    "end": "2325200"
  },
  {
    "text": "we started out with. And finally, we need to talk\nabout the overall schedule of",
    "start": "2325200",
    "end": "2331414"
  },
  {
    "text": "the algorithm. ",
    "start": "2331414",
    "end": "2336480"
  },
  {
    "text": "So let me draw an arbitrary\ncycle-free graph. ",
    "start": "2336480",
    "end": "2354060"
  },
  {
    "text": "OK, suppose I have an arbitrary\ncycle-free graph. It consists of external\nvariables out here, internal",
    "start": "2354060",
    "end": "2363290"
  },
  {
    "text": "variables and constraint\nnodes. How am I going to schedule these\ncomputations in order to",
    "start": "2363290",
    "end": "2374470"
  },
  {
    "text": "do APP decoding of this graph? Well, what do I have\nat the beginning?",
    "start": "2374470",
    "end": "2380640"
  },
  {
    "text": "At the beginning, I measure\nthe received variables corresponding to each symbol,\nand that gives me the",
    "start": "2380640",
    "end": "2388019"
  },
  {
    "text": "intrinsic information, which\nis incoming message you can",
    "start": "2388020",
    "end": "2393090"
  },
  {
    "text": "consider from every\nsymbol variable. Basically, what did I\nsee on the channel?",
    "start": "2393090",
    "end": "2398540"
  },
  {
    "text": "That's what that likelihood\nweight vector corresponds to.",
    "start": "2398540",
    "end": "2405210"
  },
  {
    "text": "All right.  What do I need in order to do\nthe sum-product update rule?",
    "start": "2405210",
    "end": "2414119"
  },
  {
    "text": "I need the inputs. Think of this now as\na directed graph. I need the incoming messages on\nall of the incident edges",
    "start": "2414120",
    "end": "2427070"
  },
  {
    "text": "on this node except for one. If I have all but one, I can\ncompute the last one as an",
    "start": "2427070",
    "end": "2432579"
  },
  {
    "text": "outgoing message. So that's the structure\nof that update.",
    "start": "2432580",
    "end": "2439340"
  },
  {
    "text": "So at this point, can I\ncompute anything here? No, I would need the inputs.",
    "start": "2439340",
    "end": "2444870"
  },
  {
    "text": "But I can compute this\noutgoing message. I can compute this\noutgoing message.",
    "start": "2444870",
    "end": "2450470"
  },
  {
    "text": "I can compute this\noutgoing message. I haven't drawn these\nof very high degree.",
    "start": "2450470",
    "end": "2456260"
  },
  {
    "text": "Normally we have degree\nhigher than two. If I had another input here,\nhowever, I could still get",
    "start": "2456260",
    "end": "2464190"
  },
  {
    "text": "that output.  All right, so that's time one.",
    "start": "2464190",
    "end": "2471270"
  },
  {
    "text": "Think of there being a clock\nand at time one I can propagate messages\ninto the graph to",
    "start": "2471270",
    "end": "2478410"
  },
  {
    "text": "depth one if you like. There's going to be a systematic\nway that we can",
    "start": "2478410",
    "end": "2485260"
  },
  {
    "text": "assign depths. All right, now at time two\nwhat can I compute? I can compute this guy because\nI have this input.",
    "start": "2485260",
    "end": "2493300"
  },
  {
    "text": " Now I have both inputs\ncoming in here, I can",
    "start": "2493300",
    "end": "2499130"
  },
  {
    "text": "compute this guy. Anything else?",
    "start": "2499130",
    "end": "2504609"
  },
  {
    "text": "So maybe 0, 1, 2. I should put times\non each of these, so I don't get confused.",
    "start": "2504610",
    "end": "2510660"
  },
  {
    "text": "0, 0, 1, 0. So this is 2 at the output.",
    "start": "2510660",
    "end": "2516750"
  },
  {
    "text": " Now I'm at time three,\nwhat can I compute?",
    "start": "2516750",
    "end": "2525060"
  },
  {
    "text": "At time three, I now have two\ninputs coming in here.",
    "start": "2525060",
    "end": "2530210"
  },
  {
    "text": "I can compute this output. Actually, from these two inputs,\nI can compute this",
    "start": "2530210",
    "end": "2536930"
  },
  {
    "text": "output at time three. And from these two, I can\ncompute this outgoing message at time three.",
    "start": "2536930",
    "end": "2543160"
  },
  {
    "start": "2543160",
    "end": "2548170"
  },
  {
    "text": "Are there other things\nI can compute? Can I compute this? No, I don't have this yet. ",
    "start": "2548170",
    "end": "2556360"
  },
  {
    "text": "That's probably it. Does anyone else see anything\nelse I can compute at time three based on the time zero,\none, and two messages?",
    "start": "2556360",
    "end": "2564049"
  },
  {
    "text": "No.  All right, but I'm\nmaking progress.",
    "start": "2564050",
    "end": "2569530"
  },
  {
    "text": "And should be clear that I'm\nalways going to be able to make some progress. At time four I can compute this",
    "start": "2569530",
    "end": "2576130"
  },
  {
    "text": "message in that direction. Let's see, I now have everything\nI need to compute",
    "start": "2576130",
    "end": "2582559"
  },
  {
    "text": "this message in that\ndirection. And I'm almost done.",
    "start": "2582560",
    "end": "2588859"
  },
  {
    "text": "And finally, I can compute this\nmessage in this direction at time four, the extrinsic",
    "start": "2588860",
    "end": "2595030"
  },
  {
    "text": "information for those variables. I guess I can do this\nat time four.",
    "start": "2595030",
    "end": "2600720"
  },
  {
    "text": "I had that at time three. And now at time five, I can get",
    "start": "2600720",
    "end": "2607260"
  },
  {
    "text": "everything else that I need. Time five I get that out.",
    "start": "2607260",
    "end": "2612900"
  },
  {
    "text": "I get this out based on this\ninput and this input.",
    "start": "2612900",
    "end": "2619700"
  },
  {
    "text": "And I get this out based on\nthis input and this input.",
    "start": "2619700",
    "end": "2625470"
  },
  {
    "text": "So in a finite amount of time,\nin this case, five computational cycles, I can\ncompute all of the messages in",
    "start": "2625470",
    "end": "2634250"
  },
  {
    "text": "both directions for every\nvariable in the graph.",
    "start": "2634250",
    "end": "2639860"
  },
  {
    "text": "Now as a final cleanup step, I\nsimply component-wise multiply",
    "start": "2639860",
    "end": "2645190"
  },
  {
    "text": "the vectors in the two\ndirections, and that gives me the APPs for all\nthe variables. ",
    "start": "2645190",
    "end": "2651920"
  },
  {
    "text": "This is a kind of parallel\ncomputation of all the APPs for all variables\nin the graph.",
    "start": "2651920",
    "end": "2657550"
  },
  {
    "text": " And I claim two things.",
    "start": "2657550",
    "end": "2662960"
  },
  {
    "text": "It's finite and it's exact. And the exactness is proved\nfrom the equations and the",
    "start": "2662960",
    "end": "2670819"
  },
  {
    "text": "cycle-free assumptions. So we can always decompose\nthings as Cartesian products. Independent components.",
    "start": "2670820",
    "end": "2679340"
  },
  {
    "text": "The finiteness, well, I assumed this is a finite graph.",
    "start": "2679340",
    "end": "2684529"
  },
  {
    "text": "What is this number 5? It's really the maximum length\nof time it takes me to get",
    "start": "2684530",
    "end": "2691710"
  },
  {
    "text": "from any of these external\nsymbol variables",
    "start": "2691710",
    "end": "2698160"
  },
  {
    "text": "to any other external-- from leaf to leaf in\ngraph theory terms. And that's called the diameter\nof the graph.",
    "start": "2698160",
    "end": "2704849"
  },
  {
    "text": "And again, some people count\nthe last little symbol node and some people don't count the\nlast little symbol node.",
    "start": "2704850",
    "end": "2711670"
  },
  {
    "text": "But it's clear that a finite\ngraph is going to have finite diameter and that the\npropagation time is basically",
    "start": "2711670",
    "end": "2718180"
  },
  {
    "text": "going to be equal\nto the diameter. That's how long it takes to\nget across the graph.",
    "start": "2718180",
    "end": "2723660"
  },
  {
    "text": "So each of these messages\nin each direction has a certain depth.",
    "start": "2723660",
    "end": "2729210"
  },
  {
    "text": "That's the distance in\nthe graph to the most distant leaf node.",
    "start": "2729210",
    "end": "2737619"
  },
  {
    "text": "And the maximum sum of these\nis always going to be 5 in this case.",
    "start": "2737620",
    "end": "2744320"
  },
  {
    "text": "Notice that it is. A tiny little graph\ntheory theorem.",
    "start": "2744320",
    "end": "2750415"
  },
  {
    "text": "AUDIENCE:\n[UNINTELLIGIBLE PHRASE]  you see why if you sum these\nmessages flying through those",
    "start": "2750415",
    "end": "2759534"
  },
  {
    "text": "past and future of the single\nedge, you would get the overall APP because you're\nsumming the contribution from",
    "start": "2759534",
    "end": "2765849"
  },
  {
    "text": "both the set of outputs and\nthose [UNINTELLIGIBLE PHRASE].",
    "start": "2765850",
    "end": "2770981"
  },
  {
    "text": "How would you prove that that\nshould be overall the APP of the symbols?",
    "start": "2770981",
    "end": "2777012"
  },
  {
    "text": " PROFESSOR: Recall the\nstate space theorem.",
    "start": "2777012",
    "end": "2782750"
  },
  {
    "text": "Let's take any edge in here. Suppose we think about\ncutting it.",
    "start": "2782750",
    "end": "2788099"
  },
  {
    "text": "A cut-set through that edge. Suppose we consider\nagglomerating everything back",
    "start": "2788100",
    "end": "2795110"
  },
  {
    "text": "here and calling\nthat the past. Agglomerating everything\nout here and calling that the future.",
    "start": "2795110",
    "end": "2802390"
  },
  {
    "text": "This is past and future. The idea of the state space\ntheorem was that this is",
    "start": "2802390",
    "end": "2809230"
  },
  {
    "text": "exactly the same situation\nas a two section trellis. ",
    "start": "2809230",
    "end": "2816610"
  },
  {
    "text": "Because everything is linear\nyou get the state space",
    "start": "2816610",
    "end": "2824020"
  },
  {
    "text": "theorem basically, which says\nthat you get a Cartesian",
    "start": "2824020",
    "end": "2829160"
  },
  {
    "text": "product of a certain past,\na certain future. You get a minimal state\nspace, which is kind",
    "start": "2829160",
    "end": "2834500"
  },
  {
    "text": "of the total code. The modulo, the part of the code\nthat lives purely on the",
    "start": "2834500",
    "end": "2840080"
  },
  {
    "text": "past, and the part that lives\npurely on the future. That quotient space is really\nwhat it is, corresponds to the",
    "start": "2840080",
    "end": "2848400"
  },
  {
    "text": "state space at this time. Because every edge in a\ncycle-free graph is by itself",
    "start": "2848400",
    "end": "2855920"
  },
  {
    "text": "a cut-set, you can do this\nfor every edge in any cycle-free graph.",
    "start": "2855920",
    "end": "2862550"
  },
  {
    "text": "You can make this\nsame argument. So you get a well-defined\nminimal state space. You get a Cartesian product\ndecomposition, which has a",
    "start": "2862550",
    "end": "2871220"
  },
  {
    "text": "number of terms equal to the\nsize of the state space, due to the dimension of the state\nspace over binary field.",
    "start": "2871220",
    "end": "2878260"
  },
  {
    "text": "And so everything goes through\njust as it did in the trellis case. That was the argument.",
    "start": "2878260",
    "end": "2884700"
  },
  {
    "text": "So mushing all this\ntogether, you get the sum-product algorithm.",
    "start": "2884700",
    "end": "2891260"
  },
  {
    "text": "This is the fundamental reason\nwhy I took the time to go through minimal trellis\nrealizations of block codes.",
    "start": "2891260",
    "end": "2902390"
  },
  {
    "text": "Not because that's actually the\nway we decode block codes all by themselves, or even\nuse block codes.",
    "start": "2902390",
    "end": "2908880"
  },
  {
    "text": "We use convolutional codes. It's so that we could prove\nthese very fundamental theorems about graph decoding.",
    "start": "2908880",
    "end": "2916450"
  },
  {
    "text": "And we aren't even going to\ndecode cycle-free graphs. But that's the intellectual\nline here.",
    "start": "2916450",
    "end": "2922270"
  },
  {
    "start": "2922270",
    "end": "2928490"
  },
  {
    "text": "Thank you for that question,\nit was very well timed. AUDIENCE:\n[UNINTELLIGIBLE PHRASE] ",
    "start": "2928490",
    "end": "2940450"
  },
  {
    "text": "PROFESSOR: Let's take a look\nat the structure of this. Where do the computations\noccur?",
    "start": "2940450",
    "end": "2945990"
  },
  {
    "text": "Every node there's\na computation. What's the complexity\nof that computation?",
    "start": "2945990",
    "end": "2952910"
  },
  {
    "text": "It's somehow approximately\nequal to the size of the constraint code.",
    "start": "2952910",
    "end": "2957950"
  },
  {
    "text": "We do one thing for every\npossible combination of legitimate combination of\nvariables, which is what we",
    "start": "2957950",
    "end": "2964410"
  },
  {
    "text": "mean by the size of the\nconstraint code. So if this is a 6, 3 code, we\nneed to do eight things here.",
    "start": "2964410",
    "end": "2970780"
  },
  {
    "text": "Basically, eight\nmultiplications. So this, in more general\ncycle-free graphs, that's what",
    "start": "2970780",
    "end": "2979540"
  },
  {
    "text": "corresponds to branch complexity\nand trellis graphs.",
    "start": "2979540",
    "end": "2985380"
  },
  {
    "text": "We found that the branch space\ncorresponded to a constraint code in trellises.",
    "start": "2985380",
    "end": "2991290"
  },
  {
    "text": "So the computation is basically\ndetermined by the constraint code complexity and\nreally, overall what we'd like",
    "start": "2991290",
    "end": "2998970"
  },
  {
    "text": "to do is minimize the constraint\ncode, the maximum size of any constraint code\nbecause that's going to",
    "start": "2998970",
    "end": "3005740"
  },
  {
    "text": "dominate computation. So we try to keep\nthe k in these",
    "start": "3005740",
    "end": "3012200"
  },
  {
    "text": "things as small as possible. Yes? AUDIENCE:\n[UNINTELLIGIBLE PHRASE]",
    "start": "3012200",
    "end": "3017320"
  },
  {
    "text": " and you get larger dimension\nconstraint for",
    "start": "3017320",
    "end": "3026901"
  },
  {
    "text": "[UNINTELLIGIBLE PHRASE].  PROFESSOR: Yeah, we're actually\nnot too concerned",
    "start": "3026901",
    "end": "3033890"
  },
  {
    "text": "about diameter.  Because things go up\nexponentially with constraint",
    "start": "3033890",
    "end": "3039750"
  },
  {
    "text": "code dimension, just that single\nparameter tends to be the thing we want to focus on.",
    "start": "3039750",
    "end": "3046230"
  },
  {
    "text": "But we remember that's hard\nto minimize in a trellis. We showed that basically our\nonly trick there after",
    "start": "3046230",
    "end": "3055240"
  },
  {
    "text": "ordering the coordinates was\nsectionalization and that we could not reduce the constraint\ncode complexity,",
    "start": "3055240",
    "end": "3062070"
  },
  {
    "text": "the branch complexity,\nby sectionalization. And then again, we build on that\nand we went through this",
    "start": "3062070",
    "end": "3068799"
  },
  {
    "text": "argument that said, gee, this\ncorresponds to some trellis. We could draw a trellis where\nthis, literally was the past",
    "start": "3068800",
    "end": "3077130"
  },
  {
    "text": "and this was the future. And the minimal size state space\nin this cycle-free graph would be the same as\nin that trellis.",
    "start": "3077130",
    "end": "3084165"
  },
  {
    "text": " What I guess I didn't show\nis you can't reduce the",
    "start": "3084165",
    "end": "3091500"
  },
  {
    "text": "constraint code complexity\nby significantly --",
    "start": "3091500",
    "end": "3097320"
  },
  {
    "text": "actually, I'm not sure there's\na crisp theorem. But the fact that you now are\nconstrained to size of state",
    "start": "3097320",
    "end": "3104619"
  },
  {
    "text": "space, this is certainly going\nto be at least as large as state space sizes. Tends to indicate that you're\nnot going to be able to do a",
    "start": "3104620",
    "end": "3112580"
  },
  {
    "text": "lot about constraint code\ncomplexity by going to more elaborate cycle-free graphs than\njust the chain trellis",
    "start": "3112580",
    "end": "3119820"
  },
  {
    "text": "graph either.  That's, I think, a missing\ntheorem that",
    "start": "3119820",
    "end": "3125902"
  },
  {
    "text": "would be nice to have. I had a paper about a year or\ntwo ago, I'm not sure it ever",
    "start": "3125902",
    "end": "3132380"
  },
  {
    "text": "quite gets to that theorem. But that's the theorem\nyou'd like to see. You really can't improve this.",
    "start": "3132380",
    "end": "3139829"
  },
  {
    "text": "So ultimately, we're bound by\nwhat you can do in a trellis. And in a trellis you can't do\ntoo much about this branch",
    "start": "3139830",
    "end": "3147520"
  },
  {
    "text": "complexity parameter once\nyou've ordered the coordinates. ",
    "start": "3147520",
    "end": "3155319"
  },
  {
    "text": "A bit of hand-waving here, but\nwe're going to need to go away from cycle-free graphs. ",
    "start": "3155320",
    "end": "3161430"
  },
  {
    "text": "One reason I like normal graphs\nis that you get this very nice separation\nof function.",
    "start": "3161430",
    "end": "3167869"
  },
  {
    "text": "You get the idea that nodes correspond to little computers. You could actually realize this\nwith little computers.",
    "start": "3167870",
    "end": "3175650"
  },
  {
    "text": "Put a computer in here\nfor each node. Its job is to do the sum-product\nupdate equation.",
    "start": "3175650",
    "end": "3182410"
  },
  {
    "text": "And then, well, what\nare the edges? They're for communication.",
    "start": "3182410",
    "end": "3188609"
  },
  {
    "text": "These are the wires that run\naround on your chip. And so when we talk about state\nspace complexity, we're",
    "start": "3188610",
    "end": "3196280"
  },
  {
    "text": "really talking about something\nlike bandwidth. How wide do you have to\nmake these wires? Do you have 6 bits, or\n9 bits, or whatever?",
    "start": "3196280",
    "end": "3204410"
  },
  {
    "text": "So state space really has\nto do with communication complexity. Constraint or branch has to\ndo with the computational",
    "start": "3204410",
    "end": "3211839"
  },
  {
    "text": "complexity, which sometimes\none's more",
    "start": "3211840",
    "end": "3216910"
  },
  {
    "text": "important than the other. Computational tends to be\nthe thing we focus on. But we know more and more as\nwe get to more complicated",
    "start": "3216910",
    "end": "3223150"
  },
  {
    "text": "chips, communications\ncomplexity tends to become dominant.",
    "start": "3223150",
    "end": "3230410"
  },
  {
    "text": "And this, of course, is the\nI/O. These are your paths",
    "start": "3230410",
    "end": "3235780"
  },
  {
    "text": "which go to the outside\nworld, your I/O paths. So it's really a good way\nto think about this.",
    "start": "3235780",
    "end": "3241650"
  },
  {
    "text": "And in fact, some people like\nAndy [UNINTELLIGIBLE] and his students have\nexperimented with building",
    "start": "3241650",
    "end": "3248290"
  },
  {
    "text": "analog sum-product decoders. ",
    "start": "3248290",
    "end": "3253650"
  },
  {
    "text": "The general idea here is that\nyou can compute an output as soon as you have the two\ncorresponding inputs.",
    "start": "3253650",
    "end": "3261510"
  },
  {
    "text": "Well, imagine another schedule\nwhere this thing just computes all the time based on whatever\ninputs it has.",
    "start": "3261510",
    "end": "3268724"
  },
  {
    "text": " In a cycle-free graph, say\nyou're in the interior.",
    "start": "3268725",
    "end": "3276500"
  },
  {
    "text": "Initially, what you have on\nyour inputs is garbage. But eventually, you're going to\nget the right things on all",
    "start": "3276500",
    "end": "3282930"
  },
  {
    "text": "your inputs. And therefore, you're going to\nput the right things on all your outputs. So again, up to some propagation\ntimes, just",
    "start": "3282930",
    "end": "3291050"
  },
  {
    "text": "putting in non-clocked little\ncomputers here, whose function is to generate the right outputs\ngiven the inputs in",
    "start": "3291050",
    "end": "3298720"
  },
  {
    "text": "each direction, is eventually\ngoing to be right in a cycle-free graph.",
    "start": "3298720",
    "end": "3305330"
  },
  {
    "text": "And in fact, there are very\nnice little analog implementations of this where\nit turns out the sum-product",
    "start": "3305330",
    "end": "3312300"
  },
  {
    "text": "update rule is extremely\namenable to transistor implementation nonlinear rule.",
    "start": "3312300",
    "end": "3320540"
  },
  {
    "text": "So you just put these in, little\ncomputing nodes, and",
    "start": "3320540",
    "end": "3326590"
  },
  {
    "text": "you put whatever you received\non here as the received symbols, and you let it go.",
    "start": "3326590",
    "end": "3331900"
  },
  {
    "text": "And it computes incredibly\nquickly for a cycle-free graph.",
    "start": "3331900",
    "end": "3338260"
  },
  {
    "text": " Where we're going to go is we're\ngoing to say, well, gee,",
    "start": "3338260",
    "end": "3347160"
  },
  {
    "text": "we got this nice little local\nrule that we can implement with local computers for the\nsum-product algorithm.",
    "start": "3347160",
    "end": "3353160"
  },
  {
    "text": "Maybe it'll work if we do it\non a graph with cycles. And on a graph with cycles\nthis kind of parallel or",
    "start": "3353160",
    "end": "3360240"
  },
  {
    "text": "flooding schedule, where every\nlittle computer is computing something at every instant of\ntime, is probably the most",
    "start": "3360240",
    "end": "3367520"
  },
  {
    "text": "popular schedule. And again, you build an analog\nimplementation of that and it goes extremely fast and it\nconverges to something.",
    "start": "3367520",
    "end": "3376270"
  },
  {
    "text": "It just relaxes to something,\nthere's a local equilibrium in all these constraint nodes.",
    "start": "3376270",
    "end": "3383080"
  },
  {
    "text": "And therefore, some global APP\nvector that satisfies the whole thing.",
    "start": "3383080",
    "end": "3389130"
  },
  {
    "start": "3389130",
    "end": "3394640"
  },
  {
    "text": "I think this is a nice attribute\nof this kind of graphical realization.",
    "start": "3394640",
    "end": "3400670"
  },
  {
    "text": "All right, a particular\nexample of how",
    "start": "3400670",
    "end": "3405720"
  },
  {
    "text": "this schedule works. The cycle-free realization\nthat we care most",
    "start": "3405720",
    "end": "3411570"
  },
  {
    "text": "about is the trellis. The BCJR algorithm is simply\nthe implementation of the",
    "start": "3411570",
    "end": "3419920"
  },
  {
    "text": "sum-product algorithm\non a trellis. SP on a trellis.",
    "start": "3419920",
    "end": "3427310"
  },
  {
    "text": "So generically, what does\na trellis look like?",
    "start": "3427310",
    "end": "3433010"
  },
  {
    "text": "I guess I should allow for some\nnontrivial code here.",
    "start": "3433010",
    "end": "3438610"
  },
  {
    "text": "So all trellises have this\nsame boring graph. ",
    "start": "3438610",
    "end": "3453330"
  },
  {
    "text": "What is this schedule? It's a cycle-free graph, so we\ncould operate according to",
    "start": "3453330",
    "end": "3459350"
  },
  {
    "text": "this nice controlled schedule,\ncompute everything just once.",
    "start": "3459350",
    "end": "3464670"
  },
  {
    "text": "And how does that operate? We get intrinsic information\nthat's measured at each of",
    "start": "3464670",
    "end": "3471320"
  },
  {
    "text": "these symbols here. What did we receive\non the channel?",
    "start": "3471320",
    "end": "3476400"
  },
  {
    "text": "We put that through a constraint\ncode and we get",
    "start": "3476400",
    "end": "3482279"
  },
  {
    "text": "here, let's give this a name. This is called intrinsic 0. Here is alpha 1, which is just\na function of the intrinsic",
    "start": "3482280",
    "end": "3493440"
  },
  {
    "text": "information at time zero. And we combine that with the\nintrinsic information at time one and we get alpha\n2, and so forth.",
    "start": "3493440",
    "end": "3502460"
  },
  {
    "text": "So we get a forward-going\npath down the trellis. If any of you knows what a\nKalman filter or a smoother",
    "start": "3502460",
    "end": "3511170"
  },
  {
    "text": "does, this is really exactly\nthe same thing. This is finding the conditional\nAPP probabilities",
    "start": "3511170",
    "end": "3519690"
  },
  {
    "text": "of this state vector given\neverything in the past. ",
    "start": "3519690",
    "end": "3525335"
  },
  {
    "text": "In Kalman filtering or\nsmoothing, it's done for Gaussian vectors. Here we're talking about\ndiscrete vectors.",
    "start": "3525335",
    "end": "3531480"
  },
  {
    "text": "But in principle, it's doing\nexactly the same thing. It's computing conditional\nprobabilities given everything",
    "start": "3531480",
    "end": "3537880"
  },
  {
    "text": "observed in the past. And so the diameter here\nis just the length of the trellis basically.",
    "start": "3537880",
    "end": "3544080"
  },
  {
    "text": " So this is i n minus 1, i n.",
    "start": "3544080",
    "end": "3550740"
  },
  {
    "text": "And here we have alpha n. ",
    "start": "3550740",
    "end": "3558210"
  },
  {
    "text": "When we have alpha n finally\nin here, we can compute the extrinsic information based on\nalpha n as just extrinsic n.",
    "start": "3558210",
    "end": "3567330"
  },
  {
    "text": "We combine these two and we're\ndone for that guy.",
    "start": "3567330",
    "end": "3573310"
  },
  {
    "text": "Meanwhile, we have to compute\nthe backward-going information",
    "start": "3573310",
    "end": "3578600"
  },
  {
    "text": "on each of these branches. So we compute first\nb n based on i n.",
    "start": "3578600",
    "end": "3584370"
  },
  {
    "text": "Then we get b n minus 1\nbased on i n minus 1 and b n, and so forth.",
    "start": "3584370",
    "end": "3590910"
  },
  {
    "text": "We get a backward-going\npropagation of conditional",
    "start": "3590910",
    "end": "3596650"
  },
  {
    "text": "information. Each of these betas represents\nthe a-posteriori probability vector given the\nfuture from it.",
    "start": "3596650",
    "end": "3604640"
  },
  {
    "text": "And finally, when we get down\nto here we can compute extrinsic 0 from data 1.",
    "start": "3604640",
    "end": "3612850"
  },
  {
    "text": "And by this time, when we have\nboth alpha 1 and beta 2 coming into here, we can, of course,\nget extrinsic 1.",
    "start": "3612850",
    "end": "3621100"
  },
  {
    "text": "When we get both the alpha and\nbeta coming in here, we can get extrinsic 2, and so forth.",
    "start": "3621100",
    "end": "3626380"
  },
  {
    "start": "3626380",
    "end": "3632799"
  },
  {
    "text": "Of course, you can write\nthis down as equations. If you want to read the\nequations, you look in the",
    "start": "3632800",
    "end": "3639060"
  },
  {
    "text": "original Bahl, Cocke, Jelinek,\nand Raviv article in 1973.",
    "start": "3639060",
    "end": "3647190"
  },
  {
    "text": "This was again, a kind of lost\npaper because just for",
    "start": "3647190",
    "end": "3654300"
  },
  {
    "text": "decoding a trellis, the\nViterbi algorithm is much less complex.",
    "start": "3654300",
    "end": "3660640"
  },
  {
    "text": "And gives you approximately\nthe same thing. I mean, I talked last time how\nprobably what you really want",
    "start": "3660640",
    "end": "3669590"
  },
  {
    "text": "to do in decoding a block code\nis maximum likelihood. That's minimum probability of\nerror in a block-wise basis.",
    "start": "3669590",
    "end": "3676089"
  },
  {
    "text": " What APP decoding gives you\nminimum probability of error.",
    "start": "3676090",
    "end": "3682650"
  },
  {
    "text": "If you then make a decision\nbased on each of these extrinsic information, or on the\ncombination of these two",
    "start": "3682650",
    "end": "3689620"
  },
  {
    "text": "to get the overall APP vector,\nthat's called a map algorithm, maximum a-posteriori\nprobability.",
    "start": "3689620",
    "end": "3695600"
  },
  {
    "text": "If you make a decision based\non that, that will give you the minimal bit error\nprobability, but it may not necessarily give you a code\nword, and it will not minimize",
    "start": "3695600",
    "end": "3702839"
  },
  {
    "text": "the probability of making any\nerror, which is generally what you want to minimize. So for trellis decoding\nof block codes",
    "start": "3702840",
    "end": "3711180"
  },
  {
    "text": "this was not favored. However, a block code as a\ncomponent of a larger code,",
    "start": "3711180",
    "end": "3717640"
  },
  {
    "text": "you want the APP vector to feed\noff to something else. And so with the advent of much\nlarger codes, of which block",
    "start": "3717640",
    "end": "3730349"
  },
  {
    "text": "codes are components or\nconvolutional codes, BCJR is the way you want to go.",
    "start": "3730350",
    "end": "3735800"
  },
  {
    "text": "And some people say it's about\nthree times as complicated as the Viterbi algorithm. ",
    "start": "3735800",
    "end": "3742450"
  },
  {
    "text": "All right, so you'll have an\nexercise with doing that in the homework. ",
    "start": "3742450",
    "end": "3749280"
  },
  {
    "text": "There is a closely related\nalgorithm called the min-sum algorithm, or the max-product\nalgorithm.",
    "start": "3749280",
    "end": "3758420"
  },
  {
    "text": "And the idea of that is that you\ncan really carry through",
    "start": "3758420",
    "end": "3765880"
  },
  {
    "text": "the same logical computations\nexcept when you get to one of",
    "start": "3765880",
    "end": "3772009"
  },
  {
    "text": "these combining operations,\nrather than doing a sum of products, you can do\na max of products.",
    "start": "3772010",
    "end": "3780750"
  },
  {
    "text": "And that the Cartesian product\nlaw still holds, the same kind of decompositions that\nwe have still hold.",
    "start": "3780750",
    "end": "3787950"
  },
  {
    "text": "For instance, in this example,\nto get the state 0, 0, 0",
    "start": "3787950",
    "end": "3801329"
  },
  {
    "text": "before what we said we're going\nto get an APP vector where this is the past APP for\nthe 0, 0 value of the middle",
    "start": "3801330",
    "end": "3816490"
  },
  {
    "text": "state vector. We simply combine the likelihood\nweights of the two",
    "start": "3816490",
    "end": "3822950"
  },
  {
    "text": "possible ways of getting to that\nstate and we sum them. ",
    "start": "3822950",
    "end": "3829270"
  },
  {
    "text": "Instead of doing sum,\nlet's just take max. ",
    "start": "3829270",
    "end": "3835470"
  },
  {
    "text": "The maximum probability\nof getting-- in other words, what's the\nbest way of getting here?",
    "start": "3835470",
    "end": "3842339"
  },
  {
    "text": "Same question as we ask in\nthe Viterbi algorithm. ",
    "start": "3842340",
    "end": "3848520"
  },
  {
    "text": "We'll let that be the likelihood\nweight for the",
    "start": "3848520",
    "end": "3853610"
  },
  {
    "text": "state vector. Otherwise, the logic is\nexactly the same. Similarly, coming this way,\nwhat's the max of these two?",
    "start": "3853610",
    "end": "3862040"
  },
  {
    "text": "OK, what's the best way of\ngetting to the state vector in terms of likelihood\nfrom the future?",
    "start": "3862040",
    "end": "3867970"
  },
  {
    "text": "And what do you know, if we\ncombine these things at this point, we now have a past vector\nand a future vector.",
    "start": "3867970",
    "end": "3876130"
  },
  {
    "text": "Let's take the max of this\ntimes the max of that.",
    "start": "3876130",
    "end": "3882890"
  },
  {
    "text": "That gives us the maximum\nlikelihood way of going through the 0, 0 value\nof the state.",
    "start": "3882890",
    "end": "3888175"
  },
  {
    "start": "3888175",
    "end": "3894720"
  },
  {
    "text": "If we do exactly the same\nlogic for decoding this trellis, we get a two-way\nalgorithm.",
    "start": "3894720",
    "end": "3901220"
  },
  {
    "text": "It's like this. It's like the BCJR algorithm,\nwhere at every point we're",
    "start": "3901220",
    "end": "3907350"
  },
  {
    "text": "computing the maximum\nlikelihood. It's not hard to show that what\nthis gives you is that",
    "start": "3907350",
    "end": "3915020"
  },
  {
    "text": "each of these symbols out here,\nit gives you the symbol that belongs to the maximum\nlikelihood code word.",
    "start": "3915020",
    "end": "3920660"
  },
  {
    "text": " How does it differ from\nthe Viterbi algorithm?",
    "start": "3920660",
    "end": "3927075"
  },
  {
    "text": "It gives you the same answer. It gives you the symbols, one\nby one, of the maximum",
    "start": "3927075",
    "end": "3932550"
  },
  {
    "text": "likelihood code word. It doesn't have to remember\nanything.",
    "start": "3932550",
    "end": "3937760"
  },
  {
    "text": "It doesn't store survivors. That's its advantage. It's disadvantage is it's\na two-way algorithm.",
    "start": "3937760",
    "end": "3945220"
  },
  {
    "text": "You have to start from both\nends and propagate your information in the same way.",
    "start": "3945220",
    "end": "3951310"
  },
  {
    "text": "And for block codes,\nmaybe that's OK. For convolutional codes that's\ncertainly not something you want to do because the code word\nmay go on indefinitely.",
    "start": "3951310",
    "end": "3959770"
  },
  {
    "text": "So the Viterbi algorithm gets\nrid of the backward step by",
    "start": "3959770",
    "end": "3965350"
  },
  {
    "text": "always remembering\nthe survivor. It remembers the history of\nhow it got to where it is.",
    "start": "3965350",
    "end": "3972309"
  },
  {
    "text": "So that once you get the max,\nyou don't have to--",
    "start": "3972310",
    "end": "3978100"
  },
  {
    "text": "you simply read it out\nrather than having-- this backward part amounts to\na trace back operation.",
    "start": "3978100",
    "end": "3987270"
  },
  {
    "text": "That's probably much too\nquick to be absorbed. But I just wanted to mention\nthere is a max product",
    "start": "3987270",
    "end": "3996460"
  },
  {
    "text": "version, which if you take log\nlikelihoods becomes a max-sum. Or if you take negative log",
    "start": "3996460",
    "end": "4002480"
  },
  {
    "text": "likelihoods it becomes min-sum. So this is called the\nmin-sum algorithm. And it does maximum likelihood\ndecoding",
    "start": "4002480",
    "end": "4011030"
  },
  {
    "text": "rather than APP decoding.  And sometimes it's used as\nan approximation or very",
    "start": "4011030",
    "end": "4019000"
  },
  {
    "text": "intermediate approximations\nbetween these two.",
    "start": "4019000",
    "end": "4024378"
  },
  {
    "text": "AUDIENCE:\n[UNINTELLIGIBLE PHRASE] ",
    "start": "4024378",
    "end": "4031099"
  },
  {
    "text": "PROFESSOR: No, if you use the\nmin-sum, basically everybody",
    "start": "4031100",
    "end": "4036830"
  },
  {
    "text": "will converge on the same\nmaximum likelihood code word. So all these constraints will\nbe consistent with--",
    "start": "4036830",
    "end": "4044790"
  },
  {
    "text": "you'd be taking a single\nmax at each point. But at each point you will\nsolve for the maximum",
    "start": "4044790",
    "end": "4050130"
  },
  {
    "text": "likelihood code word. And what you'll see up here is\nthe symbol that belongs to the maximum likelihood code word.",
    "start": "4050130",
    "end": "4056330"
  },
  {
    "text": "This is maybe a bit of a miracle\nwhen you first see it, but you'll independently get\neach of the bits in the",
    "start": "4056330",
    "end": "4064060"
  },
  {
    "text": "maximum likelihood code word. Check it out at home. AUDIENCE: Why not\nalways do this?",
    "start": "4064060",
    "end": "4071430"
  },
  {
    "text": "PROFESSOR: Why not\nalways do that? It turns out that we want softer\ndecisions when this is",
    "start": "4071430",
    "end": "4080450"
  },
  {
    "text": "part of a big graph. We actually want the APPs, the\nrelative probabilities of each",
    "start": "4080450",
    "end": "4087820"
  },
  {
    "text": "of the values. The max, in effect, is making\na hard decision.",
    "start": "4087820",
    "end": "4093710"
  },
  {
    "text": "AUDIENCE:\n[UNINTELLIGIBLE PHRASE] PROFESSOR: Yeah, all right. It does give reliability\ninformation.",
    "start": "4093710",
    "end": "4098818"
  },
  {
    "text": " I'm not sure I can give you a\nconclusive answer to that",
    "start": "4098819",
    "end": "4105619"
  },
  {
    "text": "question except APP\nworks better. That's a more empirical\nanswer.",
    "start": "4105620",
    "end": "4111568"
  },
  {
    "text": "Yes? AUDIENCE:\n[UNINTELLIGIBLE PHRASE] ",
    "start": "4111569",
    "end": "4122088"
  },
  {
    "text": "Where does that flexibility\nof the input symbols being [UNINTELLIGIBLE PHRASE].",
    "start": "4122089",
    "end": "4127770"
  },
  {
    "text": " We should be able to take\nthat into account",
    "start": "4127770",
    "end": "4135744"
  },
  {
    "text": "[UNINTELLIGIBLE PHRASE]. ",
    "start": "4135745",
    "end": "4144979"
  },
  {
    "text": "PROFESSOR: Yeah. OK, well, if the input symbols\nare independently not",
    "start": "4144979",
    "end": "4158089"
  },
  {
    "text": "equiprobable, you can\nfeed that in as--",
    "start": "4158090",
    "end": "4166818"
  },
  {
    "text": "you know, once they came out\nof the channel, once we see the channel information,\nthe two symbols are not",
    "start": "4166819",
    "end": "4175399"
  },
  {
    "text": "equiprobable anymore. So if somehow they a priori\nindependently were not equally",
    "start": "4175399",
    "end": "4183219"
  },
  {
    "text": "probable, you could just feed\nthat in as part of this intrinsic information vector.",
    "start": "4183220",
    "end": "4188880"
  },
  {
    "text": "That's almost never the\ncase in coding.",
    "start": "4188880",
    "end": "4194719"
  },
  {
    "text": "The intrinsic information is\nsometimes regarded as a-priori and this has a-posteriori, or\nthe combination of the two of",
    "start": "4194720",
    "end": "4204360"
  },
  {
    "text": "them is then a-posteriori when\nthis goes off somewhere else. ",
    "start": "4204360",
    "end": "4211510"
  },
  {
    "text": "But it doesn't really mean that\nthe symbol itself was--",
    "start": "4211510",
    "end": "4217539"
  },
  {
    "text": "it means that the evidence is\nbiased one way or the other. And when this appears somewhere\nelse in the graph,",
    "start": "4217540",
    "end": "4225980"
  },
  {
    "text": "this will now appear somewhere\ndown in some other graph. It's called the a-priori\ninformation, but what it is is",
    "start": "4225980",
    "end": "4232659"
  },
  {
    "text": "the a-posteriori information\ngiven all of the received symbols in this part\nof the graph.",
    "start": "4232660",
    "end": "4239018"
  },
  {
    "text": "AUDIENCE:\n[UNINTELLIGIBLE PHRASE]  Does the i0 of that other\ngraph contain this table",
    "start": "4239018",
    "end": "4249320"
  },
  {
    "text": "sharing information\nadded to it? PROFESSOR: Yeah, the i0 for\nanother graph is just the e0",
    "start": "4249320",
    "end": "4256720"
  },
  {
    "text": "of this graph.  I mean, if you consider it\nall as one big graph,",
    "start": "4256720",
    "end": "4263969"
  },
  {
    "text": "that's what you want. You want the message that goes\nin this direction, which basically summarizes all of the\ninputs from this graph.",
    "start": "4263970",
    "end": "4271210"
  },
  {
    "text": " Generally, you'd have a little\nequals node here.",
    "start": "4271210",
    "end": "4278460"
  },
  {
    "text": "You'd have the actual intrinsic\ninformation coming from the outside world there.",
    "start": "4278460",
    "end": "4285100"
  },
  {
    "text": "You would compute the\ne0 from this graph. And at this point, you would\ncombine e0 and i0, and that",
    "start": "4285100",
    "end": "4292975"
  },
  {
    "text": "would go around here. ",
    "start": "4292975",
    "end": "4300400"
  },
  {
    "text": "So the very last page of chapter\n12 just says, OK,",
    "start": "4300400",
    "end": "4305460"
  },
  {
    "text": "we've got a nice, clean, finite\nexact algorithm that will compute all these APPs\non a cycle-free graph.",
    "start": "4305460",
    "end": "4312120"
  },
  {
    "text": "Now, suppose we're faced with\na graph that has cycles. ",
    "start": "4312120",
    "end": "4317650"
  },
  {
    "text": "And here's the situation. Suppose the graph has\ncycles in it. ",
    "start": "4317650",
    "end": "4324219"
  },
  {
    "text": "Then first thing that might\noccur to you, at least after",
    "start": "4324220",
    "end": "4330290"
  },
  {
    "text": "this development is, well, we\nnow have a local rule for updating at each of these\ncomputational nodes, each of",
    "start": "4330290",
    "end": "4338220"
  },
  {
    "text": "these constraint nodes, in\nthis representation. ",
    "start": "4338220",
    "end": "4344150"
  },
  {
    "text": "Why don't we just let it rip? See what happens. ",
    "start": "4344150",
    "end": "4350640"
  },
  {
    "text": "We're going to put in some\nintrinsic information at the outputs as before.",
    "start": "4350640",
    "end": "4357360"
  },
  {
    "text": "We can compute some output\nmessage here based on-- well,",
    "start": "4357360",
    "end": "4362489"
  },
  {
    "text": "eventually we're going to get\nsome input message from here. And we can compute some output\nmessage here based on all of",
    "start": "4362490",
    "end": "4370520"
  },
  {
    "text": "these and we'll pass\nthat over.",
    "start": "4370520",
    "end": "4375940"
  },
  {
    "text": "Let's assume what's called\na parallel schedule or a flooding schedule, where in each\ncycle every one of these",
    "start": "4375940",
    "end": "4383769"
  },
  {
    "text": "little guys does its thing. It takes all the inputs that it\nhas available at that time",
    "start": "4383770",
    "end": "4389739"
  },
  {
    "text": "and it generates outputs.  Well, hope for the best.",
    "start": "4389740",
    "end": "4399160"
  },
  {
    "text": "It will compute something. You can sort of intuitively\nsee that it'll settle.",
    "start": "4399160",
    "end": "4406340"
  },
  {
    "text": "It'll converge, perhaps, into\nsome kind of equilibrium. Or hopefully it will converge\ninto some kind of equilibrium.",
    "start": "4406340",
    "end": "4413685"
  },
  {
    "text": " There is a basic fallacy here,\nwhich is that the information",
    "start": "4413685",
    "end": "4421800"
  },
  {
    "text": "that comes in here is used to\ncompute part of this message.",
    "start": "4421800",
    "end": "4426960"
  },
  {
    "text": "Which is used to compute\npart of this message. Which ultimately comes back and\nis used to compute part of",
    "start": "4426960",
    "end": "4433540"
  },
  {
    "text": "this message. And then part of this message\nagain, so that information",
    "start": "4433540",
    "end": "4440400"
  },
  {
    "text": "goes around in cycles and it\ntends to be used again and",
    "start": "4440400",
    "end": "4446590"
  },
  {
    "text": "again and again. Of course, this tends\nto reinforce previously computed messages.",
    "start": "4446590",
    "end": "4452190"
  },
  {
    "text": "It tends to make you\noverconfident.  You heard some rumor and then\nthe rumor goes all around the",
    "start": "4452190",
    "end": "4463810"
  },
  {
    "text": "class and it comes back\nand you hear it again. And that tends to confirm\nthe rumor.",
    "start": "4463810",
    "end": "4470010"
  },
  {
    "text": "So it's that kind of telephone\nsituation, and it's not really independent information.",
    "start": "4470010",
    "end": "4477190"
  },
  {
    "text": "Intuitively, makes sense\nthat if all of the cycles are very large--",
    "start": "4477190",
    "end": "4482869"
  },
  {
    "text": " in other words, the rumor has to\ngo to China and back before",
    "start": "4482870",
    "end": "4491970"
  },
  {
    "text": "you get it. That it probably is highly\nattenuated by the time you get.",
    "start": "4491970",
    "end": "4497489"
  },
  {
    "text": "It's mixed up with all kinds of\nother information, so maybe it doesn't hurt you too much if\nthe cycles are very large.",
    "start": "4497490",
    "end": "4505590"
  },
  {
    "text": "And in fact, that's the way\nthese capacity approaching",
    "start": "4505590",
    "end": "4512099"
  },
  {
    "text": "codes are designed. Whereas, as I mentioned last\ntime, if you're in a physical",
    "start": "4512100",
    "end": "4517630"
  },
  {
    "text": "situation where basically your\ngraph looks something like",
    "start": "4517630",
    "end": "4523010"
  },
  {
    "text": "this, you don't have the freedom\nto make your cycles large, then this is probably\na bad idea.",
    "start": "4523010",
    "end": "4530520"
  },
  {
    "text": "And this isn't why in fields,\nlike vision, for instance, people try to use this--",
    "start": "4530520",
    "end": "4535980"
  },
  {
    "text": "the sum-product algorithm is\nthe belief propagation algorithm, if you've\never heard of that. Widely used in inference\non Bayesian networks.",
    "start": "4535980",
    "end": "4544070"
  },
  {
    "text": "And the religion in belief\npropagation used to be that",
    "start": "4544070",
    "end": "4549429"
  },
  {
    "text": "you simply can't use belief\npropagation on graphs that have cycles. Why? Because most of the\ngraphs they dealt",
    "start": "4549430",
    "end": "4558680"
  },
  {
    "text": "with look like this. And in fact, it works\nterribly on that. ",
    "start": "4558680",
    "end": "4564850"
  },
  {
    "text": "It was a great shock on that\nfield when the coding people came along and said, well, we\nuse belief propagation on",
    "start": "4564850",
    "end": "4571639"
  },
  {
    "text": "graphs with cycles and\nit works great. And this, as I understand it,\nreally had an enormous impact",
    "start": "4571640",
    "end": "4579580"
  },
  {
    "text": "on the belief propagation\ncommunity. But it was realized that gee,\ncoding people have it nice.",
    "start": "4579580",
    "end": "4586150"
  },
  {
    "text": "You make your own graphs.  And you make them so this is\ngoing to be a very low order,",
    "start": "4586150",
    "end": "4594230"
  },
  {
    "text": "low impact event. And that's right. But coding we do have\nthis freedom to",
    "start": "4594230",
    "end": "4601020"
  },
  {
    "text": "design our own graphs.  We realize from the basic\narguments that I put up before",
    "start": "4601020",
    "end": "4608489"
  },
  {
    "text": "that we're really going to\nneed to have cycles. As long as we have cycle-free\ngraphs, we're going to get",
    "start": "4608490",
    "end": "4614720"
  },
  {
    "text": "this kind of exponentially\nincreasing complexity, which is characteristic of\ntrellis graphs.",
    "start": "4614720",
    "end": "4620250"
  },
  {
    "text": "So we're going to need cycles,\nbut as long as the cycles are very long and attenuated, as\nlong as the girth of the graph",
    "start": "4620250",
    "end": "4627610"
  },
  {
    "text": "is great, maybe we can\nget away with it. And that, in fact, is the way\nthe story has developed.",
    "start": "4627610",
    "end": "4634030"
  },
  {
    "text": "And Gallager basically saw this\nback in 1961, but it took a long time for it to catch.",
    "start": "4634030",
    "end": "4642400"
  },
  {
    "text": "So I didn't even get\ninto chapter 13. Chapter 13 we're going to first\njust introduce several",
    "start": "4642400",
    "end": "4651219"
  },
  {
    "text": "classes, the most popular\nclasses of capacity approaching codes. And then I'm going to give\nyou some real performance",
    "start": "4651220",
    "end": "4662370"
  },
  {
    "text": "analysis, how you would design, simulate, in these codes.",
    "start": "4662370",
    "end": "4670060"
  },
  {
    "text": "And it's what people\nactually use. OK, see you next time. ",
    "start": "4670060",
    "end": "4683135"
  }
]