[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "text": " PHILIPPE RIGOLLET: --124. If I were to repeat\nthis 1,000 times,",
    "start": "17880",
    "end": "24350"
  },
  {
    "text": "so every one of\nthose 1,000 times they collect 124\ndata points and then I'd do it again and\ndo it again and again,",
    "start": "24350",
    "end": "31430"
  },
  {
    "text": "then in average, the\nnumber I should get should be close to the true\nparameter that I'm looking for.",
    "start": "31430",
    "end": "37222"
  },
  {
    "text": "The fluctuations that\nare due to the fact that I get different\nsamples every time should somewhat vanish. And so what I want is to have a\nsmall bias, hopefully a 0 bias.",
    "start": "37222",
    "end": "46309"
  },
  {
    "text": "If this thing is 0, then we see\nthat the estimator is unbiased. ",
    "start": "46310",
    "end": "66470"
  },
  {
    "text": "So this is definitely\na property that we are going to be looking\nfor in an estimator, trying to find them\nto be unbiased.",
    "start": "66470",
    "end": "71570"
  },
  {
    "text": "But we'll see that it's\nactually maybe not enough. So unbiasedness should\nnot be something you lose your sleep over.",
    "start": "71570",
    "end": "78140"
  },
  {
    "text": "Something that's slightly\nbetter is the risk, really the quadratics risk,\nwhich is expectation of--",
    "start": "78140",
    "end": "93050"
  },
  {
    "text": "so if I have an\nestimator, theta hat, I'm going to look at the\nexpectation of theta hat n",
    "start": "93050",
    "end": "98360"
  },
  {
    "text": "minus theta squared. And what we showed last time\nis that we can actually--",
    "start": "98360",
    "end": "104130"
  },
  {
    "text": "by inserting in there,\nadding and removing the expectation of\ntheta hat, we actually",
    "start": "104130",
    "end": "109374"
  },
  {
    "text": "get something where\nthis thing can be decomposed as the square\nof the bias plus the variance,",
    "start": "109374",
    "end": "119160"
  },
  {
    "text": "which is just the expectation of\ntheta hat minus its expectation squared.",
    "start": "119160",
    "end": "126986"
  },
  {
    "text": "That came from\nthe fact that when I added and removed the\nexpectation of theta hat in there, the\ncross-terms cancel.",
    "start": "126986",
    "end": "133731"
  },
  {
    "text": "All right. So that was the bias squared,\nand this is the variance.",
    "start": "133731",
    "end": "139556"
  },
  {
    "start": "139556",
    "end": "145410"
  },
  {
    "text": "And so for example, if the\nquadratic risk goes to 0, then that means that\ntheta hat converges",
    "start": "145410",
    "end": "151470"
  },
  {
    "text": "to theta in the L2 sense. And here we know that if\nwe want this to go to 0,",
    "start": "151470",
    "end": "158050"
  },
  {
    "text": "since it's the sum of\ntwo positive terms, we need to have both\nthe bias that goes to 0 and the variance\nthat goes to 0, so we",
    "start": "158050",
    "end": "164459"
  },
  {
    "text": "need to control both\nof those things. And so there is usually\nan inherent trade-off between getting a small bias\nand getting a small variance.",
    "start": "164460",
    "end": "173550"
  },
  {
    "text": "If you reduce one too much, then\nthe variance of the other one is going to-- then the other one is going\nto increase, or the opposite.",
    "start": "173550",
    "end": "179470"
  },
  {
    "text": "That happens a lot, but not so\nmuch, actually, in this class. So let's just look at\na couple of examples.",
    "start": "179470",
    "end": "187113"
  },
  {
    "text": "So am I planning-- yeah. So examples.",
    "start": "187113",
    "end": "199040"
  },
  {
    "text": "So if I do, for example, X1,\nXn, there are iid Bernoulli.",
    "start": "199040",
    "end": "206163"
  },
  {
    "text": "And I'm going to\nwrite it theta so that we keep the same notation. Then theta hat, what\nis the theta hat",
    "start": "206164",
    "end": "212360"
  },
  {
    "text": "that we proposed many times? It's just X bar, Xn bar,\nthe average of Xi's.",
    "start": "212360",
    "end": "218530"
  },
  {
    "text": "So what is the bias of this guy? Well, to know the bias, I\njust have to remove theta",
    "start": "218530",
    "end": "224340"
  },
  {
    "text": "from the expectation. What is the\nexpectation of Xn bar? Well, by linearity\nof the expectation,",
    "start": "224340",
    "end": "231300"
  },
  {
    "text": "it's just the average\nof the expectations. ",
    "start": "231300",
    "end": "237950"
  },
  {
    "text": "But since all my Xi's are\nBernouilli with the same theta, then each of this guy is\nactually equal to theta.",
    "start": "237950",
    "end": "243740"
  },
  {
    "text": "So this thing is actually\ntheta, which means that this isn't biased, right? ",
    "start": "243740",
    "end": "254660"
  },
  {
    "text": "Now, what is the\nvariance of this guy? ",
    "start": "254660",
    "end": "262440"
  },
  {
    "text": "So if you forgot the\nproperties of the variance",
    "start": "262440",
    "end": "267774"
  },
  {
    "text": "for sum of independent\nrandom variables, now it's time to wake up. So we have the\nvariance of something",
    "start": "267774",
    "end": "274060"
  },
  {
    "text": "that looks like 1 over n, the\nsum from i equal 1 to n of Xi. ",
    "start": "274060",
    "end": "281460"
  },
  {
    "text": "So it's of the form\nvariance of a constant times a random variable. So the first thing I'm going\nto do is pull out the constant.",
    "start": "281460",
    "end": "289140"
  },
  {
    "text": "But we know that the variance\nleaves on the square scale, so when I pull out a constant\noutside of the variance,",
    "start": "289140",
    "end": "294449"
  },
  {
    "text": "it comes out with a square. The variance of a\ntimes X is a-squared times the variance of\nX, so this is equal to 1",
    "start": "294450",
    "end": "302550"
  },
  {
    "text": "over n squared times\nthe variance of the sum. ",
    "start": "302550",
    "end": "310580"
  },
  {
    "text": "So now we want to always\ndo what we want to do. So we have the\nvariance of the sum.",
    "start": "310580",
    "end": "316060"
  },
  {
    "text": "We would like somehow\nto say that this is the sum of the variances. And in general, we are\nnot allowed to say that,",
    "start": "316060",
    "end": "322320"
  },
  {
    "text": "but we are because my Xi's\nare actually independent. So this is actually equal to 1\nover n squared sum from i equal",
    "start": "322320",
    "end": "330660"
  },
  {
    "text": "1 to n of the variance\nof each of the Xi's.",
    "start": "330660",
    "end": "336320"
  },
  {
    "text": "And that's by independence,\nso this is basic probability.",
    "start": "336320",
    "end": "342760"
  },
  {
    "text": "And now, what is the variance\nof Xi's where again they're all the same distribution,\nso the variance of Xi is the same as the\nvariance of X1.",
    "start": "342760",
    "end": "349400"
  },
  {
    "text": "And so each of those\nguys has variance what? What is the variance\nof a Bernoulli? We've said it once. It's theta times 1 minus theta.",
    "start": "349400",
    "end": "355770"
  },
  {
    "text": " And so now I'm going to have\nthe sum of n times a constant,",
    "start": "355770",
    "end": "363390"
  },
  {
    "text": "so I get n times the constant\ndivided by n squared, so one of the n's\nis going to cancel. And so the whole\nthing here is actually",
    "start": "363390",
    "end": "370210"
  },
  {
    "text": "equal to theta, 1 minus\ntheta divided by n. ",
    "start": "370210",
    "end": "378500"
  },
  {
    "text": "So if I'm interested\nin the quadratic risk-- ",
    "start": "378500",
    "end": "387434"
  },
  {
    "text": "and again, I should\njust say risk, because this is the\nonly risk we're going to be actually looking at. Yeah.",
    "start": "387434",
    "end": "392510"
  },
  {
    "text": "This parenthesis should\nreally stop here. ",
    "start": "392510",
    "end": "398000"
  },
  {
    "text": "I really wanted to put\nquadratic in parenthesis. So the risk of this guy is what?",
    "start": "398000",
    "end": "403350"
  },
  {
    "text": "Well, it's the expectation of\nx bar n minus theta squared.",
    "start": "403350",
    "end": "410890"
  },
  {
    "text": "And we know it's the\nsquare of the variance, so it's the square\nof the bias, which",
    "start": "410890",
    "end": "416389"
  },
  {
    "text": "we know is 0, so it's 0 squared\nplus the variance, which is theta, 1 plus theta--",
    "start": "416390",
    "end": "423430"
  },
  {
    "text": "1 minus theta divided by n. So it's just theta, 1\nminus theta divided by n.",
    "start": "423430",
    "end": "434620"
  },
  {
    "text": "So this is just summarizing the\nperformance of an estimator, which is the random variable. I mean, it's complicated.",
    "start": "434620",
    "end": "439761"
  },
  {
    "text": "If I really wanted\nto describe it, I would just tell you\nthe entire distribution",
    "start": "439761",
    "end": "445400"
  },
  {
    "text": "of this random variable. But now what I'm doing\nis I'm saying, well, let's just take this random\nvariable, remove theta from it,",
    "start": "445400",
    "end": "452710"
  },
  {
    "text": "and see how small the\nfluctuations around theta-- the squared fluctuations around\ntheta are in expectation.",
    "start": "452710",
    "end": "461120"
  },
  {
    "text": "So that's what the\nquadratic risk is doing. And in a way, this\ndecomposition, as the sum of the bias\nsquare and the variance,",
    "start": "461120",
    "end": "466508"
  },
  {
    "text": "is really telling you that-- it is really accounting for\nthe bias, which is, well, even if I had an infinite\namount of observations,",
    "start": "466508",
    "end": "472530"
  },
  {
    "text": "is this thing doing\nthe right thing? And the other thing is\nactually the variance, so for finite number\nof observations,",
    "start": "472530",
    "end": "477581"
  },
  {
    "text": "what are the fluctuations? All right. Then you can see that those\nthings, bias and variance, are actually very different.",
    "start": "477581",
    "end": "485740"
  },
  {
    "text": "So I don't have any\ncolors here, so you're going to have to really\nfollow the speed--",
    "start": "485740",
    "end": "492360"
  },
  {
    "text": "the order in which\nI draw those curves. All right. So let's find-- I'm going to give you three\ncandidate estimators, so--",
    "start": "492360",
    "end": "499530"
  },
  {
    "start": "499530",
    "end": "509980"
  },
  {
    "text": "estimators for theta. ",
    "start": "509980",
    "end": "515349"
  },
  {
    "text": "So the first one is\ndefinitely Xn bar. That will be a good\ncandidate estimator.",
    "start": "515350",
    "end": "520899"
  },
  {
    "text": "The second one is going to\nbe 0.5, because after all, why should I bother if\nit's actually going to be--",
    "start": "520900",
    "end": "527260"
  },
  {
    "text": "right? So for example, if\nI ask you to predict the score of some\ncandidate in some election,",
    "start": "527260",
    "end": "534510"
  },
  {
    "text": "then since you know it's\ngoing to be very close to 0.5, you might as well just throw\n0.5 and you're not going",
    "start": "534510",
    "end": "539680"
  },
  {
    "text": "to be very far from reality. And it's actually going\nto cost you 0 time and $0 to come up with that. So sometimes maybe\njust a good old guess",
    "start": "539680",
    "end": "546460"
  },
  {
    "text": "is actually doing\nthe job for you. Of course, for\npresidential elections or something like this,\nit's not very helpful",
    "start": "546460",
    "end": "552890"
  },
  {
    "text": "if your prediction\nis telling you this. But if it was\nsomething different, that would be a good way to\ngenerate some close to 1/2.",
    "start": "552890",
    "end": "561112"
  },
  {
    "text": "For a coin, for example,\nif I give you a coin, you never know. Maybe it's slightly biased. But the good guess, just\nlooking at it, inspecting it,",
    "start": "561112",
    "end": "567970"
  },
  {
    "text": "maybe there's something\ncrazy happening with the structure\nof it, you're going to guess that it's 0.5 without\ntrying to collect information.",
    "start": "567970",
    "end": "574522"
  },
  {
    "text": "And let's find another one,\nwhich is, well, you know, I have a lot of observations. But I'm recording couples\nkissing, but I'm on a budget.",
    "start": "574522",
    "end": "583810"
  },
  {
    "text": "I don't have time to\ntravel all around the world and collect some people. So really, I'm just going\nto look at the first couple",
    "start": "583810",
    "end": "589330"
  },
  {
    "text": "and go home. So my other estimator\nis just going to be X1. I just take the first\nobservation, 0, 1,",
    "start": "589330",
    "end": "595709"
  },
  {
    "text": "and that's it. So now I'm going-- I want to actually understand\nwhat the behavior of those guys",
    "start": "595710",
    "end": "601079"
  },
  {
    "text": "is. All right. So we know-- and so we know\nthat for this guy, the bias is 0",
    "start": "601080",
    "end": "609240"
  },
  {
    "text": "and the variance\nis equal to theta,",
    "start": "609240",
    "end": "614279"
  },
  {
    "text": "1 minus theta divided by n.",
    "start": "614280",
    "end": "619610"
  },
  {
    "text": "What is the bias\nof this guy, 0.5? ",
    "start": "619610",
    "end": "628100"
  },
  {
    "text": "AUDIENCE: 0.5. AUDIENCE: 0.5 minus theta? PHILIPPE RIGOLLET: 0.5\nminus theta, right. ",
    "start": "628100",
    "end": "635360"
  },
  {
    "text": "So the bias, 0.5 minus theta. What is the variance\nof this guy?",
    "start": "635360",
    "end": "640510"
  },
  {
    "text": " What is the variance of 0.5?",
    "start": "640510",
    "end": "646701"
  },
  {
    "text": "AUDIENCE: It's 0. PHILIPPE RIGOLLET: 0. Right. It's just a\ndeterministic number, so there's no\nfluctuations for this guy.",
    "start": "646702",
    "end": "653110"
  },
  {
    "text": "What is the bias? Well, X1 is actually-- just for simplicity,\nI can think of it",
    "start": "653110",
    "end": "658210"
  },
  {
    "text": "as being X1 bar, the\naverage of itself, so that wherever I saw an n for\nthis guy, I can replace it by 1",
    "start": "658210",
    "end": "663940"
  },
  {
    "text": "and that will give\nme my formula. So the bias is\nstill going to be 0. And the variance is going to be\nequal to theta, 1 minus theta.",
    "start": "663940",
    "end": "670190"
  },
  {
    "text": " So now I have those\nthree estimators.",
    "start": "670190",
    "end": "676180"
  },
  {
    "text": "Well, if I compare\nX1 and Xn bar, then clearly I have 0\nbias in both cases.",
    "start": "676180",
    "end": "682480"
  },
  {
    "text": "That's good. And I have the variance that's\nactually n times smaller when I use my n observations\nthan when I don't.",
    "start": "682480",
    "end": "689556"
  },
  {
    "text": "So those two guys,\non these two fronts, you can actually look\nat the two numbers and say, well, the first\nnumber is the same.",
    "start": "689556",
    "end": "695264"
  },
  {
    "text": "The second number is\nbetter for the other guy, so I will definitely go for\nthis guy compared to this guy.",
    "start": "695264",
    "end": "700550"
  },
  {
    "text": "So this guy is gone. But not this guy. Well, if I look at the\nbias, the variance is 0.",
    "start": "700550",
    "end": "707079"
  },
  {
    "text": "It's always beating the\nvariance of this guy. And if I look at the bias, it's\nactually really not that bad.",
    "start": "707080",
    "end": "712269"
  },
  {
    "text": "It's 0.5 minus theta. In particular, if theta\nis 0.5, then this guy is strictly better.",
    "start": "712270",
    "end": "717930"
  },
  {
    "text": "And so you can actually\nnow look at what the quadratic risk looks like.",
    "start": "717930",
    "end": "725100"
  },
  {
    "text": "So here, what I'm\ngoing to do is I'm going to take my\ntrue theta-- so it's going to range between 0 and 1. And we know that those two\nthings are functions of theta,",
    "start": "725100",
    "end": "732080"
  },
  {
    "text": "so I can only understand\nthem if I plot them as functions of theta. And so now I'm going\nto actually plot--",
    "start": "732080",
    "end": "738590"
  },
  {
    "text": "the y-axis is going\nto be the risk. ",
    "start": "738590",
    "end": "743860"
  },
  {
    "text": "So what is the risk of\nthe estimator of 0.5? This one is easy. Well, it's 0 plus the\nsquare of 0.5 minus theta.",
    "start": "743860",
    "end": "753330"
  },
  {
    "text": "So we know that at theta,\nit's actually going to be 0. And then it's going\nto be a square.",
    "start": "753330",
    "end": "759640"
  },
  {
    "text": "So at 0, it's going to be 0.25.",
    "start": "759640",
    "end": "764800"
  },
  {
    "text": "And at 1, it's going\nto be 0.25 as well. So it looks like this.",
    "start": "764800",
    "end": "769940"
  },
  {
    "text": "Well, actually, sorry. Let me put the 0.5\nwhere it should be. ",
    "start": "769940",
    "end": "776680"
  },
  {
    "text": "OK. So this here is the risk of 0.5.",
    "start": "776680",
    "end": "783690"
  },
  {
    "text": "And we'll write it like this. So when theta is very close\nto 0.5, I'm very happy.",
    "start": "783690",
    "end": "789490"
  },
  {
    "text": "When theta gets farther,\nit's a little bit annoying. And then here, I want to\nplot the risk of this guy.",
    "start": "789490",
    "end": "796680"
  },
  {
    "text": "So now the thing with\nthe risk of this guy is that it will depend on n. So I will just pick some\nn that I'm happy with just",
    "start": "796680",
    "end": "804040"
  },
  {
    "text": "so that I can\nactually draw a curve. Otherwise, I'm going to have to\nplot one curve per value of n. So let's just say, for\nexample, that n is equal to 10.",
    "start": "804040",
    "end": "811900"
  },
  {
    "text": "And so now I need to plot\nthe function theta, 1 minus theta divided by 10.",
    "start": "811900",
    "end": "817600"
  },
  {
    "text": "We know that theta,\n1 minus theta is a curve that goes like this. It takes value at 1/2. It thinks value 1/4.",
    "start": "817600",
    "end": "823480"
  },
  {
    "text": "That's the maximum. And then it's 0 at the end. So really, if n is\nequal to 1, this",
    "start": "823480",
    "end": "832557"
  },
  {
    "text": "is what the variance looks like. The bias doesn't\ncount in the risk. Yeah. AUDIENCE: [INAUDIBLE]",
    "start": "832557",
    "end": "840020"
  },
  {
    "text": "PHILIPPE RIGOLLET: Sure. Can you move? All right. Are you guys good?",
    "start": "840020",
    "end": "845065"
  },
  {
    "text": " All right. So now I have this picture. And I know I'm going up to 25.",
    "start": "845065",
    "end": "852279"
  },
  {
    "text": "And there's a place\nwhere those curves cross. So if you're sure-- let's say you're talking\nabout presidential election,",
    "start": "852280",
    "end": "858340"
  },
  {
    "text": "you know that those things\nare going to be really close. Maybe you're actually\nbetter by predicting 0.5",
    "start": "858340",
    "end": "863620"
  },
  {
    "text": "if you know it's not\ngoing to go too far. But that's for one observation,\nso that's the risk of X1.",
    "start": "863620",
    "end": "872670"
  },
  {
    "text": "But if I look at the\nrisk of Xn, all I'm doing is just crushing\nthis curve down to 0.",
    "start": "872670",
    "end": "878940"
  },
  {
    "text": "So as n increases, it's going\nto look more and more like this. It's the same\ncurve divided by n.",
    "start": "878940",
    "end": "884396"
  },
  {
    "text": " And so now I can just\nstart to understand",
    "start": "884396",
    "end": "890649"
  },
  {
    "text": "that for different\nvalues of thetas, now I'm going to have to be very\nclose to theta is equal to 1/2",
    "start": "890650",
    "end": "896240"
  },
  {
    "text": "if I want to start saying\nthat Xn bar is worse than the naive estimator 0.5.",
    "start": "896240",
    "end": "903225"
  },
  {
    "text": "Yeah. AUDIENCE: Sorry. I know you explained a little\nbit before, but can you just--",
    "start": "903226",
    "end": "908528"
  },
  {
    "text": "what is an intuitive\ndefinition of risk? What is it actually describing?",
    "start": "908528",
    "end": "913839"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nSo either you can-- well, when you have an unbiased\nestimator, it's simple.",
    "start": "913840",
    "end": "918924"
  },
  {
    "text": "It's just telling you\nit's the variance, because the theta that you\nhave over there is really-- so in the definition of\nthe risk, the theta",
    "start": "918924",
    "end": "926500"
  },
  {
    "text": "that you have here\nif you're unbiased is really the\nexpectation of theta hat. So that's really\njust the variance.",
    "start": "926500",
    "end": "933230"
  },
  {
    "text": "So the risk is\nreally telling you how much fluctuations I\nhave around my expectation",
    "start": "933230",
    "end": "939160"
  },
  {
    "text": "if unbiased. But actually here, it's telling\nyou how much fluctuations I have in average around theta.",
    "start": "939160",
    "end": "944420"
  },
  {
    "text": "So if you understand the\nnotion of variance as being-- AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: What? AUDIENCE: Like\nvariance on average.",
    "start": "944420",
    "end": "949780"
  },
  {
    "text": "PHILIPPE RIGOLLET: No. AUDIENCE: No. PHILIPPE RIGOLLET: It's\njust like variance. AUDIENCE: Oh, OK. PHILIPPE RIGOLLET: So when you-- I mean, if you claim you\nunderstand what variance is,",
    "start": "949780",
    "end": "956140"
  },
  {
    "text": "it's telling you\nwhat is the expected squared fluctuation\naround the expectation of my random variable.",
    "start": "956140",
    "end": "961640"
  },
  {
    "text": "It's just telling you on\naverage how far I'm going to be. And you take the square because\nyou want to cancel the signs. Otherwise, you're\ngoing to get 0.",
    "start": "961640",
    "end": "967250"
  },
  {
    "text": "AUDIENCE: Oh, OK. PHILIPPE RIGOLLET: And\nhere it's saying, well, I really don't care what the\nexpectation of theta hat is. What I want to get\nto is theta, so I'm",
    "start": "967250",
    "end": "973029"
  },
  {
    "text": "looking at the expectation\nof the squared fluctuations around theta itself. If I'm unbiased, it\ncoincides with the variance.",
    "start": "973030",
    "end": "979610"
  },
  {
    "text": "But if I'm biased, then I\nhave to account for the fact that I'm really\nnot computing the-- AUDIENCE: OK. OK.",
    "start": "979610",
    "end": "984640"
  },
  {
    "text": "Thanks. PHILIPPE RIGOLLET: OK? All right. Are there any questions?",
    "start": "984640",
    "end": "989670"
  },
  {
    "text": "So here, what I really\nwant to illustrate is that the risk\nitself is a function of theta most of the times. And so for different\nthetas, some estimators",
    "start": "989670",
    "end": "995620"
  },
  {
    "text": "are going to be\nbetter than others. But there's also\nthe entire range of estimators, those\nthat are really biased,",
    "start": "995620",
    "end": "1001960"
  },
  {
    "text": "but the bias can\ncompletely vanish. And so here, you see\nyou have no bias,",
    "start": "1001960",
    "end": "1007269"
  },
  {
    "text": "but the variance can be large. Or you have 0 bias-- you have a bias, but\nthe variance is 0.",
    "start": "1007270",
    "end": "1012430"
  },
  {
    "text": "So you can actually\nhave this trade-off and you can find things that are\nin the entire range in general.",
    "start": "1012430",
    "end": "1018220"
  },
  {
    "text": " So those things are\nactually-- those trade-offs",
    "start": "1018220",
    "end": "1025939"
  },
  {
    "text": "between bias and variance are\nusually much better illustrated if we're talking about\nmultivariate parameters.",
    "start": "1025940",
    "end": "1032564"
  },
  {
    "text": "If I actually look\nat a parameter which is the mean of some multivariate\nGaussian, so an entire vector,",
    "start": "1032565",
    "end": "1039024"
  },
  {
    "text": "then the bias is going to-- I can make the bias\nbigger by, for example, forcing all the coordinates of\nmy estimator to be the same.",
    "start": "1039025",
    "end": "1046189"
  },
  {
    "text": "So here, I'm going\nto get some bias, but the variance\nis actually going to be much better, because\nI get to average all",
    "start": "1046190",
    "end": "1051200"
  },
  {
    "text": "the coordinates for this guy. And so really, the\nbias/variance trade-off is when you have multiple\nparameters to estimate,",
    "start": "1051200",
    "end": "1058790"
  },
  {
    "text": "so you have a vector\nof parameters, a multivariate\nparameter, the bias increases when you're trying\nto pull more information",
    "start": "1058790",
    "end": "1065450"
  },
  {
    "text": "across the different\ncomponents to actually have a lower variance.",
    "start": "1065450",
    "end": "1070669"
  },
  {
    "text": "So the more you average,\nthe lower the variance. That's exactly what\nwe've illustrated. As n increases, the\nvariance decreases,",
    "start": "1070670",
    "end": "1076700"
  },
  {
    "text": "like 1 over n or theta,\n1 minus theta over n. And so this is how it\nhappens in general. In this class, it's mostly\none-dimensional parameter",
    "start": "1076700",
    "end": "1083840"
  },
  {
    "text": "estimation, so it's going to be\na little harder to illustrate that. But if you do, for example,\nnon-parametric estimation,",
    "start": "1083840",
    "end": "1089210"
  },
  {
    "text": "that's all you do. There's just bias/variance\ntrade-offs all the time.",
    "start": "1089210",
    "end": "1094590"
  },
  {
    "text": "And in between, when you have\nhigh-dimensional parametric estimation, that\nhappens a lot as well.",
    "start": "1094590",
    "end": "1100110"
  },
  {
    "text": "OK. So I'm just going to go quickly\nthrough those two remaining slides, because we've\nactually seen them.",
    "start": "1100110",
    "end": "1106730"
  },
  {
    "text": "But I just wanted you to have\nsomewhere a formal definition of what a confidence\ninterval is.",
    "start": "1106730",
    "end": "1112700"
  },
  {
    "text": "And so we fixed a statistical\nmodel for n observations, X1",
    "start": "1112700",
    "end": "1117830"
  },
  {
    "text": "to Xn. The parameter theta\nhere is one-dimensional. Theta is a subset\nof the real line,",
    "start": "1117830",
    "end": "1124010"
  },
  {
    "text": "and that's why I\ntalk about intervals. An interval is a\nsubset of the line. If I had a subset\nof R2, for example,",
    "start": "1124010",
    "end": "1131480"
  },
  {
    "text": "that would no longer be called\nan interval, but a region, just because-- well, that's\njust we can say a set,",
    "start": "1131480",
    "end": "1137570"
  },
  {
    "text": "a confidence set. But people like to\nsay confidence region. So an interval is just a\none-dimensional conference",
    "start": "1137570",
    "end": "1144170"
  },
  {
    "text": "region. And it has to be an\ninterval as well. So a confidence interval\nof level 1 minus alpha--",
    "start": "1144170",
    "end": "1151820"
  },
  {
    "text": "so we refer to the quality\nof a confidence interval is actually called it's level.",
    "start": "1151820",
    "end": "1158120"
  },
  {
    "text": "It takes value 1 minus alpha\nfor some positive alpha. And so the confidence level-- the level of the confidence\ninterval is between 0 and 1.",
    "start": "1158120",
    "end": "1166760"
  },
  {
    "text": "The closer to 1 it is, the\nbetter the confidence interval. The closer to 0,\nthe worse it is.",
    "start": "1166760",
    "end": "1172040"
  },
  {
    "text": "And so for any\nrandom interval-- so a confidence interval\nis a random interval.",
    "start": "1172040",
    "end": "1177830"
  },
  {
    "text": "The bounds of this interval\ndepends on random data. Just like we had\nX bar plus/minus",
    "start": "1177830",
    "end": "1184650"
  },
  {
    "text": "1 over square root of\nn, for example, or 2 over square root\nof n, this X bar was the random thing that would\nmake fluctuate those guys.",
    "start": "1184650",
    "end": "1193020"
  },
  {
    "text": "And so now I have an interval. And now I have its boundaries,\nbut now the boundaries are not allowed to depend\non my unknown parameter.",
    "start": "1193020",
    "end": "1198830"
  },
  {
    "text": "Otherwise, it's not a\nconfidence interval, just like an\nestimator that depends on the unknown parameter\nis not an estimator.",
    "start": "1198830",
    "end": "1204929"
  },
  {
    "text": "The confidence interval\nhas to be something that I can compute\nonce I collect data.",
    "start": "1204929",
    "end": "1210360"
  },
  {
    "text": "And so what I want is that--\nso there's this weird notation. The fact that I write theta--",
    "start": "1210360",
    "end": "1217800"
  },
  {
    "text": "that's the probability\nthat I contains theta. You're used to seeing\ntheta belongs to I.",
    "start": "1217800",
    "end": "1223080"
  },
  {
    "text": "But here, I really\nwant to emphasize that the randomness is\nin I. And so the way you actually say\nit when you read",
    "start": "1223081",
    "end": "1228540"
  },
  {
    "text": "this formula is the probability\nthat I contains theta is at least 1 minus alpha.",
    "start": "1228540",
    "end": "1236929"
  },
  {
    "text": "So it better be close to 1. You want 1 minus alpha\nto be very close to 1, because it's really\ntelling you that whatever",
    "start": "1236930",
    "end": "1243724"
  },
  {
    "text": "random variable I'm giving\nyou, my error bars are actually covering the right theta.",
    "start": "1243724",
    "end": "1249190"
  },
  {
    "text": "And I want this to be true. But I want this--\nsince I don't know what my confidence--\nmy parameter of theta",
    "start": "1249190",
    "end": "1254340"
  },
  {
    "text": "is, I want this to hold\ntrue for all possible values of the parameters that nature\nmay have come up with from.",
    "start": "1254340",
    "end": "1262860"
  },
  {
    "text": "So I want this-- so there's\ntheta that changes here, so the distribution\nof the interval is actually changing\nwith theta hopefully.",
    "start": "1262860",
    "end": "1268860"
  },
  {
    "text": "And theta is changing\nwith this guy. So regardless of the value\nof theta that I'm getting, I want that the probability\nthat it contains the theta",
    "start": "1268860",
    "end": "1277350"
  },
  {
    "text": "is actually larger\nthan 1 minus alpha. So I'll come back\nto it in a second. I just want to say\nthat here, we can",
    "start": "1277350",
    "end": "1283600"
  },
  {
    "text": "talk about asymptotic level. And that's typically when\nyou use central limit theorem to compute this guy.",
    "start": "1283600",
    "end": "1289750"
  },
  {
    "text": "Then you're not guaranteed\nthat the value is at least 1 minus\nalpha for every n,",
    "start": "1289750",
    "end": "1295840"
  },
  {
    "text": "but it's actually in the limit\nlarger than 1 minus alpha. So maybe for each fixed n\nit's going to be not true.",
    "start": "1295840",
    "end": "1303140"
  },
  {
    "text": "But for as no goes\nto infinity, it's actually going to become true. If you want this to\nhold for every n,",
    "start": "1303140",
    "end": "1309230"
  },
  {
    "text": "you actually need to use things\nsuch as Hoeffding's inequality that we described at some\npoint, that hold for every n.",
    "start": "1309230",
    "end": "1315169"
  },
  {
    "text": "So as a rule of thumb, if you\nuse the central limit theorem, you're dealing with\na confidence interval",
    "start": "1315170",
    "end": "1321710"
  },
  {
    "text": "with asymptotic\nlevel 1 minus alpha. And the reason is\nbecause you actually want to get the quintiles\nof the normal-- the Gaussian",
    "start": "1321710",
    "end": "1330260"
  },
  {
    "text": "distribution that comes from\nthe central limit theorem. And if you want to use\nHoeffding's, for example,",
    "start": "1330260",
    "end": "1335579"
  },
  {
    "text": "you might actually get away with\na confidence interval that's actually true even\nnon-asymptotically. It's just the regular\nconfidence interval.",
    "start": "1335579",
    "end": "1342030"
  },
  {
    "text": " So this is the\nformal definition. It's a bit of a mouthful.",
    "start": "1342030",
    "end": "1348009"
  },
  {
    "text": "But we actually-- the best\nway to understand them is to build them. Now, at some point I said--",
    "start": "1348009",
    "end": "1353929"
  },
  {
    "text": "and I think it was\npart of the homework--  so here, I really\nsay the probability",
    "start": "1353930",
    "end": "1359970"
  },
  {
    "text": "the true parameter belongs\nto the confidence interval is actually 1 minus alpha. And so that's because here,\nthis confidence interval",
    "start": "1359970",
    "end": "1367096"
  },
  {
    "text": "is still a random variable. Now, if I start plugging\nin numbers instead of the random\nvariables X1 to Xn, I start putting 1,\n0, 0, 1, 0, 0, 1,",
    "start": "1367096",
    "end": "1375240"
  },
  {
    "text": "like I did for the kiss\nexample, then in this case, the random interval is actually\ngoing to be 0.42, 0.65.",
    "start": "1375240",
    "end": "1383320"
  },
  {
    "text": "And this guy, the probability\nthat theta belongs to it is not 1 minus alpha. It's either 0 if\nit's not in there",
    "start": "1383321",
    "end": "1390000"
  },
  {
    "text": "or it's 1 if it's in there. ",
    "start": "1390000",
    "end": "1396870"
  },
  {
    "text": "So here is the\nexample that we had. So just let's look at back into\nour favorite example, which",
    "start": "1396870",
    "end": "1404220"
  },
  {
    "text": "is the average of\nBernoulli random variables, so we studied that maybe\nthat's the third time already.",
    "start": "1404220",
    "end": "1410280"
  },
  {
    "text": "So the sample average, Xn\nbar, is a strongly consistent estimator of p. That was one of the\nproperties that we wanted.",
    "start": "1410280",
    "end": "1417480"
  },
  {
    "text": "Strongly consistent means\nthat as n goes to infinity, it converges almost surely\nto the true parameter.",
    "start": "1417480",
    "end": "1422940"
  },
  {
    "text": "That's the strong\nlaw of large number. It is consistent also, because\nit's strongly consistent, so it also converges\nin probability,",
    "start": "1422940",
    "end": "1429910"
  },
  {
    "text": "which makes it consistent. It's unbiased. We've seen that. We've actually computed\nits quadratic risk.",
    "start": "1429910",
    "end": "1437780"
  },
  {
    "text": "And now what I have\nis that if I look at-- thanks to the central limit\ntheorem, we actually did this. We built a confidence interval\nat level 1 minus alpha--",
    "start": "1437780",
    "end": "1448850"
  },
  {
    "text": "asymptotic level, sorry,\nasymptotic level 1 minus alpha. And so here, this\nis how we did it.",
    "start": "1448850",
    "end": "1455679"
  },
  {
    "text": "Let me just go through it again. So we know from the\ncentral limit theorem-- ",
    "start": "1455680",
    "end": "1468240"
  },
  {
    "text": "so the central limit\ntheorem tells us that Xn bar minus p divided\nby square root of p1 minus p,",
    "start": "1468240",
    "end": "1478040"
  },
  {
    "text": "square root of n converges\nin distribution as n goes to infinity to some\nstandard normal distribution.",
    "start": "1478040",
    "end": "1487269"
  },
  {
    "text": "So what it means is that if\nI look at the probability under the true p, that's\nsquare root of n, Xn bar",
    "start": "1487270",
    "end": "1493990"
  },
  {
    "text": "minus p divided by square\nroot of p1 minus p,",
    "start": "1493990",
    "end": "1503130"
  },
  {
    "text": "it's less than Q alpha\nover 2, where this is the definition of the quintile. Then this guy-- and I'm actually\ngoing to use the same notation,",
    "start": "1503130",
    "end": "1511980"
  },
  {
    "text": "limit as n goes to infinity,\nthis is the same thing.",
    "start": "1511980",
    "end": "1517320"
  },
  {
    "text": "So this is actually going to\nbe equal to 1 minus alpha.",
    "start": "1517320",
    "end": "1522720"
  },
  {
    "text": "That's exactly what\nI did last time. This is by definition of the\nquintile of a standard Gaussian",
    "start": "1522720",
    "end": "1528690"
  },
  {
    "text": "and of a limit in distribution. So the probabilities computed on\nthis guy in the limit converges",
    "start": "1528690",
    "end": "1536920"
  },
  {
    "text": "to the probability\ncomputed on this guy. And we know that this\nis just the probability that the absolute\nvalue of sum n 0, 1",
    "start": "1536920",
    "end": "1542279"
  },
  {
    "text": "is less than Q alpha over 2. ",
    "start": "1542280",
    "end": "1547750"
  },
  {
    "text": "And so in particular,\nif it's equal, then I can put some\nlarger than or equal to,",
    "start": "1547750",
    "end": "1554180"
  },
  {
    "text": "which guarantees my\nasymptotic confidence level. And I just solve for p.",
    "start": "1554180",
    "end": "1559700"
  },
  {
    "text": "So this is equivalent\nto the limit as n goes to infinity\nof the probability",
    "start": "1559700",
    "end": "1567110"
  },
  {
    "text": "that theta is between\nXn bar minus Q",
    "start": "1567110",
    "end": "1575990"
  },
  {
    "text": "alpha over 2 divided by--",
    "start": "1575990",
    "end": "1581210"
  },
  {
    "text": "times square root of p1 minus p\ndivided by square root of n, Xn",
    "start": "1581210",
    "end": "1586809"
  },
  {
    "text": "bar plus q alpha over 2,\nsquare root of p1 minus p",
    "start": "1586810",
    "end": "1593980"
  },
  {
    "text": "divided by square root of\nn is larger than or equal to 1 minus alpha.",
    "start": "1593980",
    "end": "1599030"
  },
  {
    "text": "And so there you go. I have my confidence interval. Except that's not, right?",
    "start": "1599030",
    "end": "1605750"
  },
  {
    "text": "We just said that the bounds\nof a confidence interval may not depend on the\nunknown parameter. And here, they do.",
    "start": "1605750",
    "end": "1612440"
  },
  {
    "text": "And so we actually\ncame up with two ways of getting rid of this. Since we only need this thing--\nso this thing, as we said,",
    "start": "1612440",
    "end": "1618290"
  },
  {
    "text": "is really equal. Every time I'm going to\nmake this guy smaller and this guy larger,\nI'm only going",
    "start": "1618290",
    "end": "1623450"
  },
  {
    "text": "to increase the probability. And so what we do is\nwe actually just take the largest possible\nvalue for p1 minus",
    "start": "1623450",
    "end": "1628940"
  },
  {
    "text": "p, which makes the interval\nas large as possible. And so now I have this.",
    "start": "1628940",
    "end": "1635419"
  },
  {
    "text": "I just do one of the two tricks. I replace p1 minus p by their\nupper bound, which is 1/4.",
    "start": "1635420",
    "end": "1642399"
  },
  {
    "text": " As we said, p1 minus p, the\nfunction looks like this.",
    "start": "1642400",
    "end": "1648255"
  },
  {
    "text": "So I just take the\nvalue here at 1/2. Or, I can use Slutsky and say\nthat if I replace p by Xn bar,",
    "start": "1648255",
    "end": "1657800"
  },
  {
    "text": "that's the same as just\nreplacing p by Xn bar here. ",
    "start": "1657800",
    "end": "1665640"
  },
  {
    "text": "And by Slutsky, we know that\nthis is actually converging also to some standard Gaussian.",
    "start": "1665640",
    "end": "1670650"
  },
  {
    "start": "1670650",
    "end": "1679630"
  },
  {
    "text": "We've seen that when we\nsaw Slutsky as an example. And so those two\nthings-- actually,",
    "start": "1679630",
    "end": "1685620"
  },
  {
    "text": "just because I'm\ntaking the limit and I'm only caring about the\nasymptotic confidence level, I can actually just plug in\nconsistent quantities in there,",
    "start": "1685620",
    "end": "1693419"
  },
  {
    "text": "such as Xn bar where\nI don't have a p. And that gives me another\nconfidence interval.",
    "start": "1693420",
    "end": "1698789"
  },
  {
    "text": "All right. So this by now, hopefully\nafter doing it three times,",
    "start": "1698790",
    "end": "1704510"
  },
  {
    "text": "you should really, really be\ncomfortable with just creating this confidence interval.",
    "start": "1704510",
    "end": "1709880"
  },
  {
    "text": "We did it three times in class. I think you probably did\nit another couple times in your homework. So just make sure you're\ncomfortable with this.",
    "start": "1709880",
    "end": "1716570"
  },
  {
    "text": "All right. That's one of the basic\nthings you would want to know. Are there any questions? Yes.",
    "start": "1716570",
    "end": "1722121"
  },
  {
    "text": "AUDIENCE: So Slutsky holds\nfor any single response set p. But Xn converges [INAUDIBLE].",
    "start": "1722121",
    "end": "1728504"
  },
  {
    "text": " PHILIPPE RIGOLLET: So\nthat's not Slutsky, right?",
    "start": "1728504",
    "end": "1735075"
  },
  {
    "text": "AUDIENCE: That's [INAUDIBLE]. PHILIPPE RIGOLLET: So Slutsky\ntells you that if you--",
    "start": "1735076",
    "end": "1744040"
  },
  {
    "text": "Slutsky's about combining\ntwo types of convergence. So Slutsky tells you\nthat if you actually have one Xn that converges\nto X in distribution and Yn",
    "start": "1744040",
    "end": "1753910"
  },
  {
    "text": "that converges to Y\nin probability, then you can actually\nmultiply Xn and Yn and get that the\nlimit in distribution",
    "start": "1753910",
    "end": "1760450"
  },
  {
    "text": "is the product of X and Y,\nwhere X is now a constant.",
    "start": "1760450",
    "end": "1768460"
  },
  {
    "text": "And here we have the\nconstant, which is 1. But I did that already, right?",
    "start": "1768460",
    "end": "1775160"
  },
  {
    "text": "Using Slutsky to\nreplace it for the-- to replace P by\nXn bar, we've done",
    "start": "1775160",
    "end": "1780890"
  },
  {
    "text": "that last time, maybe a\ncouple of times ago, actually. Yeah. AUDIENCE: So I guess these\nstatements are [INAUDIBLE]..",
    "start": "1780890",
    "end": "1789802"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nThat's correct. AUDIENCE: So could we like\nfigure out [INAUDIBLE] can we set a finite [INAUDIBLE].",
    "start": "1789802",
    "end": "1798220"
  },
  {
    "text": "PHILIPPE RIGOLLET: So of\ncourse, the short answer is no. ",
    "start": "1798220",
    "end": "1804279"
  },
  {
    "text": "So here's how you\nwould go about thinking about which method is better. So there's always the\nmore conservative method.",
    "start": "1804280",
    "end": "1810760"
  },
  {
    "text": "The first one, the only\nthing you're losing is the rate of convergence\nof the central limit theorem.",
    "start": "1810760",
    "end": "1816430"
  },
  {
    "text": "So if n is large enough so\nthat the central limit theorem approximation is very good,\nthen that's all you're",
    "start": "1816430",
    "end": "1822700"
  },
  {
    "text": "going to be losing. Of course, the price you pay is\nthat your confidence interval is wider than it\nwould be if you were",
    "start": "1822700",
    "end": "1828700"
  },
  {
    "text": "to use Slutsky for this\nparticular problem, typically wider. Actually, it is always\nwider, because Xn bar--",
    "start": "1828700",
    "end": "1837140"
  },
  {
    "text": "1 minus Xn bar is always\nless than 1/4 as well. And so that's the\nfirst thing you--",
    "start": "1837140",
    "end": "1845920"
  },
  {
    "text": "so Slutsky basically adds your\nrelying on the central limit--",
    "start": "1845920",
    "end": "1851380"
  },
  {
    "text": "your relying on the\nasymptotics again. Now of course, you don't\nwant to be conservative, because you actually want to\nsqueeze as much from your data",
    "start": "1851380",
    "end": "1859060"
  },
  {
    "text": "as you can. So it depends on how comfortable\nand how critical it is for you to put valid error bars.",
    "start": "1859060",
    "end": "1866410"
  },
  {
    "text": "If they're valid\nin the asymptotics, then maybe you're actually\ngoing to go with Slutsky so it actually gives you\nslightly narrower confidence",
    "start": "1866410",
    "end": "1871918"
  },
  {
    "text": "intervals and so you feel\nlike you're a little more-- you have a more precise answer.",
    "start": "1871918",
    "end": "1877869"
  },
  {
    "text": "Now, if you really need\nto be super-conservative, then you're actually going\nto go with the P1 minus P.",
    "start": "1877869",
    "end": "1883390"
  },
  {
    "text": "Actually, if you need to\nbe even more conservative, you are going to go with\nHoeffding's so you don't even",
    "start": "1883390",
    "end": "1888850"
  },
  {
    "text": "have to rely on the\nasymptotics level at all. But then you're\nconfidence interval becomes twice as wide\nand twice as wide",
    "start": "1888850",
    "end": "1895000"
  },
  {
    "text": "and it becomes wider\nand wider as you go. So depends on-- I mean, there's a lot\nof data in statistics",
    "start": "1895000",
    "end": "1901650"
  },
  {
    "text": "which is gauging how critical\nit is for you to output valid error bounds or if\nthey're really just here",
    "start": "1901650",
    "end": "1908380"
  },
  {
    "text": "to be indicative of the\nprecision of the estimator you gave from a more\nqualitative perspective.",
    "start": "1908380",
    "end": "1915396"
  },
  {
    "text": "AUDIENCE: So the error\nthere is [INAUDIBLE]?? PHILIPPE RIGOLLET: Yeah. So here, there's basically\na bunch of errors.",
    "start": "1915396",
    "end": "1921220"
  },
  {
    "text": "There's one that's-- so there's\na theorem called Berry-Esseen that quantifies how far this\nprobability is from 1 minus",
    "start": "1921220",
    "end": "1929830"
  },
  {
    "text": "alpha, but the\nconstants are terrible. So it's not very\nhelpful, but it tells you as n grows how smaller\nthis thing grows--",
    "start": "1929830",
    "end": "1937330"
  },
  {
    "text": "becomes smaller. And then for\nSlutsky, again you're multiplying something that\nconverges by something that",
    "start": "1937330",
    "end": "1942789"
  },
  {
    "text": "fluctuates around 1, so\nyou need to understand how this thing fluctuates. Now, there's something\nthat shows up.",
    "start": "1942790",
    "end": "1948070"
  },
  {
    "text": "Basically, what is the\nslope of the function 1 over square root of X1\nminus X around the value",
    "start": "1948070",
    "end": "1956220"
  },
  {
    "text": "you're interested in? And so if this function\nis super-sharp, then small fluctuations of Xn\nbar around this expectation",
    "start": "1956220",
    "end": "1963200"
  },
  {
    "text": "are going to lead to\nreally high fluctuations of the function itself. So if you're looking at--",
    "start": "1963200",
    "end": "1969570"
  },
  {
    "text": "if you have f of Xn bar and\nf around say the true P,",
    "start": "1969570",
    "end": "1975615"
  },
  {
    "text": "if f is really sharp\nlike that, then if you move a little\nbit here, then you're",
    "start": "1975615",
    "end": "1980730"
  },
  {
    "text": "going to move really\na lot on the y-axis. So that's what the function\nhere-- the function you're interested in is 1 over\nsquare root of X1 minus X.",
    "start": "1980730",
    "end": "1989205"
  },
  {
    "text": "So what does this function look\nlike around the point where you think P is the true parameter?",
    "start": "1989205",
    "end": "1994260"
  },
  {
    "text": " Its derivative really\nis what matters.",
    "start": "1994260",
    "end": "1999850"
  },
  {
    "text": "OK? Any other question.  OK.",
    "start": "1999850",
    "end": "2005165"
  },
  {
    "text": "So it's important,\nbecause now we're going to switch to the\nreal let's do some hardcore computation type of things.",
    "start": "2005165",
    "end": "2011928"
  },
  {
    "text": "All right.  So in this chapter, we're\ngoing to talk about maximum",
    "start": "2011928",
    "end": "2019550"
  },
  {
    "text": "likelihood estimation.  Who has already seen maximum\nlikelihood estimation?",
    "start": "2019550",
    "end": "2029320"
  },
  {
    "text": "OK. And who knows what a\nconvex function is?",
    "start": "2029320",
    "end": "2035380"
  },
  {
    "text": "OK. So we'll do a little bit of\nreminders on those things.",
    "start": "2035380",
    "end": "2040909"
  },
  {
    "text": "So those things are when we do\nmaximum likelihood estimation, likelihood is the function, so\nwe need to maximize a function.",
    "start": "2040910",
    "end": "2047470"
  },
  {
    "text": "That's basically\nwhat we need to do. And if I give you\na function, you need to know how to\nmaximize this function.",
    "start": "2047470",
    "end": "2052658"
  },
  {
    "text": "Sometimes, you have\nclosed-form solutions. You can take the derivative and\nset it equal to 0 and solve it.",
    "start": "2052659",
    "end": "2058219"
  },
  {
    "text": "But sometimes, you actually\nneed to resort to algorithms to do that. And there's an entire\nindustry doing that.",
    "start": "2058219",
    "end": "2065020"
  },
  {
    "text": "And we'll briefly touch upon\nit, but this is definitely not the focus of this class.",
    "start": "2065020",
    "end": "2070370"
  },
  {
    "text": "OK. So before diving directly\ninto the definition of the likelihood and\nwhat is the definition",
    "start": "2070370",
    "end": "2076519"
  },
  {
    "text": "of the maximum likelihood\nestimator, what I'm going to try to\ndo is to give you",
    "start": "2076520",
    "end": "2081840"
  },
  {
    "text": "an insight for what we're\nactually doing when we do maximum likelihood estimation.",
    "start": "2081840",
    "end": "2088869"
  },
  {
    "text": "So remember, we have a\nmodel on a sample space E and some candidate\ndistributions P theta.",
    "start": "2088870",
    "end": "2097600"
  },
  {
    "text": "And really, your goal is\nto estimate a true theta star, the one that generated\nsome data, X1 to Xn,",
    "start": "2097600",
    "end": "2104079"
  },
  {
    "text": "in an iid fashion. But this theta star is\nreally a proxy for us to know that we\nactually understand",
    "start": "2104080",
    "end": "2110740"
  },
  {
    "text": "the distribution itself. The goal of knowing theta star\nis so that you can actually know what P theta star.",
    "start": "2110740",
    "end": "2117790"
  },
  {
    "text": "Otherwise, it has--\nwell, sometimes we said it has some meaning\nitself, but really you want to know what\nthe distribution is.",
    "start": "2117790",
    "end": "2123490"
  },
  {
    "text": "And so your goal is to actually\ncome up with the distribution-- hopefully that comes\nfrom the family P theta--",
    "start": "2123490",
    "end": "2130270"
  },
  {
    "text": "that's close to P theta star. So in a way, what does it mean\nto have two distributions that",
    "start": "2130270",
    "end": "2138710"
  },
  {
    "text": "are close? It means that when you\ncompute probabilities on one distribution,\nyou should have the same probability on the\nother distribution pretty much.",
    "start": "2138710",
    "end": "2146870"
  },
  {
    "text": "So what we can do\nis say, well, now I have two candidate\ndistributions. ",
    "start": "2146870",
    "end": "2159010"
  },
  {
    "text": "So if theta hat leads to a\ncandidate distribution P theta hat, and this is\nthe true theta star,",
    "start": "2159010",
    "end": "2166210"
  },
  {
    "text": "it leads to the true\ndistribution P theta star according to which\nmy data was drawn.",
    "start": "2166210",
    "end": "2171940"
  },
  {
    "text": "That's my candidate.  As a statistician, I'm\nsupposed to come up",
    "start": "2171940",
    "end": "2178100"
  },
  {
    "text": "with a good candidate,\nand this is the truth. ",
    "start": "2178100",
    "end": "2183940"
  },
  {
    "text": "And what I want is that\nif you actually give me the distribution,\nthen I want when",
    "start": "2183940",
    "end": "2190030"
  },
  {
    "text": "I'm computing\nprobabilities for this guy, I know what the probabilities\nfor the other guys are. And so really what I want is\nthat if I compute a probability",
    "start": "2190030",
    "end": "2200040"
  },
  {
    "text": "under theta hat of\nsome interval a, b, it should be pretty\nclose to the probability",
    "start": "2200040",
    "end": "2206580"
  },
  {
    "text": "under theta star of a, b.",
    "start": "2206580",
    "end": "2211660"
  },
  {
    "text": "And more generally,\nif I want to take the union of two intervals,\nI want this to be true. If I take just 1/2 lines, I\nwant this to be true from 0",
    "start": "2211660",
    "end": "2218500"
  },
  {
    "text": "to infinity, for example,\nthings like this. I want this to be true\nfor all of them at once.",
    "start": "2218500",
    "end": "2223549"
  },
  {
    "text": "And so what I do is that I\nwrite A for a probability event. And I want that P hat of\nA is close to P star of A",
    "start": "2223550",
    "end": "2231520"
  },
  {
    "text": "for any event A in\nthe sample space. Does that sound like\na reasonable goal",
    "start": "2231520",
    "end": "2237100"
  },
  {
    "text": "for a statistician? So in particular, if I\nwant those to be close, I want the absolute\nvalue of their difference",
    "start": "2237100",
    "end": "2242784"
  },
  {
    "text": "to be close to 0.  And this turns out to be--",
    "start": "2242784",
    "end": "2248140"
  },
  {
    "text": "if I want this to hold\nfor all possible A's, I have all possible events, so I'm\ngoing to actually maximize over",
    "start": "2248140",
    "end": "2255460"
  },
  {
    "text": "these events. And I'm going to\nlook at the worst possible event on which theta\nhat can depart from theta star.",
    "start": "2255460",
    "end": "2261160"
  },
  {
    "text": "And so rather than\ndefining it specifically for theta hat and\ntheta star, I'm just going to say, well, if\nyou give me two probability",
    "start": "2261160",
    "end": "2267910"
  },
  {
    "text": "measures, P theta\nand P theta prime, I want to know how\nclose they are.",
    "start": "2267910",
    "end": "2273099"
  },
  {
    "text": "Well, if I want to\nmeasure how close they are by how they can\ndiffer when I measure",
    "start": "2273100",
    "end": "2278980"
  },
  {
    "text": "the probability\nof some event, I'm just looking at the absolute\nvalue of the difference",
    "start": "2278980",
    "end": "2284799"
  },
  {
    "text": "of the probabilities\nand I'm just maximizing over the worst\npossible event that might actually make them differ.",
    "start": "2284800",
    "end": "2291380"
  },
  {
    "text": "Agreed? That's a pretty strong notion. So if the total variation\nbetween theta and theta prime",
    "start": "2291380",
    "end": "2297720"
  },
  {
    "text": "is small, it means that for all\npossible A's that you give me, then P theta of A is\ngoing to be close to P",
    "start": "2297720",
    "end": "2305589"
  },
  {
    "text": "theta prime of A, because if--",
    "start": "2305590",
    "end": "2310820"
  },
  {
    "text": "let's say I just found the\nbound on the total variation distance, which is 0.01.",
    "start": "2310820",
    "end": "2321911"
  },
  {
    "text": "All right. So that means that this\nis going to be larger than the max over A of P theta\nminus P theta prime of A,",
    "start": "2321911",
    "end": "2340940"
  },
  {
    "text": "which means that for any A-- actually, let me write P\ntheta hat and P theta star,",
    "start": "2340940",
    "end": "2346990"
  },
  {
    "text": "like we said, theta\nhat and theta star. And so if I have a bound,\nsay, on the total variation,",
    "start": "2346990",
    "end": "2352860"
  },
  {
    "text": "which is 0.01, that\nmeans that P theta hat--",
    "start": "2352860",
    "end": "2359270"
  },
  {
    "text": "every time I compute a\nprobability on P theta hat, it's basically in the\ninterval P theta star of A,",
    "start": "2359270",
    "end": "2369295"
  },
  {
    "text": "the one that I really wanted\nto compute, plus or minus 0.01.",
    "start": "2369295",
    "end": "2374790"
  },
  {
    "text": "This has nothing to do\nwith confidence interval. This is just\ntelling me how far I am from the value of\nactually trying to compute.",
    "start": "2374790",
    "end": "2381280"
  },
  {
    "text": "And that's true for\nall A. And that's key. That's where this\nmax comes into play.",
    "start": "2381280",
    "end": "2387400"
  },
  {
    "text": "It just says, I want\nthis bound to hold for all possible A's at once. ",
    "start": "2387400",
    "end": "2395300"
  },
  {
    "text": "So this is actually a\nvery well-known distance between probability measures. It's the total\nvariation distance.",
    "start": "2395300",
    "end": "2400766"
  },
  {
    "text": "It's extremely central to\nprobabilistic analysis. And it essentially tells\nyou that every time--",
    "start": "2400766",
    "end": "2407119"
  },
  {
    "text": "if two probability\ndistributions are close, then it means that every\ntime I compute a probability under P theta but\nI really actually",
    "start": "2407120",
    "end": "2415160"
  },
  {
    "text": "have data from P\ntheta prime, then the error is no larger\nthan the total variation.",
    "start": "2415160",
    "end": "2421710"
  },
  {
    "text": "OK. So this is maybe not\nthe most convenient way",
    "start": "2421710",
    "end": "2429460"
  },
  {
    "text": "of finding a distance. I mean, how are you going-- in reality, how are you\nto compute this maximum",
    "start": "2429460",
    "end": "2434500"
  },
  {
    "text": "over all possible events? I mean, it's just crazy, right? There's an infinite\nnumber of them. It's much larger than the number\nof intervals, for example,",
    "start": "2434500",
    "end": "2441339"
  },
  {
    "text": "so it's a bit annoying. And so there's actually\na way to compress it",
    "start": "2441340",
    "end": "2446800"
  },
  {
    "text": "by just looking at the basically\nfunction distance or vector distance between probability\nmass functions or probability",
    "start": "2446800",
    "end": "2453250"
  },
  {
    "text": "density functions. So I'm going to start\nwith the discrete version of the total variation.",
    "start": "2453250",
    "end": "2459279"
  },
  {
    "text": "So throughout this\nchapter, I will make the difference between\ndiscrete random variables",
    "start": "2459280",
    "end": "2465490"
  },
  {
    "text": "and continuous random variables. It really doesn't matter. All it means is that when\nI talk about discrete,",
    "start": "2465490",
    "end": "2470650"
  },
  {
    "text": "I will talk about\nprobability mass functions. And when I talk\nabout continuous, I will talk about probability\ndensity functions.",
    "start": "2470650",
    "end": "2476600"
  },
  {
    "text": "When I talk about\nprobability mass functions, I talk about sums. When I talk about probability\ndensity functions,",
    "start": "2476600",
    "end": "2484900"
  },
  {
    "text": "I talk about integrals. But they're all the\nsame thing, really.",
    "start": "2484900",
    "end": "2490090"
  },
  {
    "text": "So let's start with the\nprobability mass function. Everybody remembers what\nthe probability mass function of a discrete\nrandom variable is.",
    "start": "2490090",
    "end": "2497980"
  },
  {
    "text": "This is the function that tells\nme for each possible value that it can take,\nthe probability",
    "start": "2497980",
    "end": "2503720"
  },
  {
    "text": "that it takes this value. So the Probability\nMass Function, PMF,",
    "start": "2503720",
    "end": "2513200"
  },
  {
    "text": "is just the function for\nall x in the sample space tells me the probability\nthat my random variable is",
    "start": "2513200",
    "end": "2521420"
  },
  {
    "text": "equal to this little value. And I will denote it\nby P sub theta of X.",
    "start": "2521420",
    "end": "2529091"
  },
  {
    "text": "So what I want is, of\ncourse, that the sum of the probabilities is 1. ",
    "start": "2529091",
    "end": "2537620"
  },
  {
    "text": "And I want them to\nbe non-negative. Actually, typically we will\nassume that they are positive.",
    "start": "2537620",
    "end": "2543109"
  },
  {
    "text": "Otherwise, we can just remove\nthis x from the sample space. And so then I have the total\nvariation distance, I mean,",
    "start": "2543110",
    "end": "2551700"
  },
  {
    "text": "it's supposed to be the\nmaximum overall sets of-- of subsets of E, such\nthat the probability",
    "start": "2551700",
    "end": "2559849"
  },
  {
    "text": "of A minus probability\nof theta prime of A-- it's complicated,\nbut really there's this beautiful\nformula that tells me",
    "start": "2559850",
    "end": "2566130"
  },
  {
    "text": "that if I look at the total\nvariation between P theta and P theta prime, it's\nactually equal to just 1/2",
    "start": "2566130",
    "end": "2574520"
  },
  {
    "text": "of the sum for all X in E of the\nabsolute difference between P",
    "start": "2574520",
    "end": "2584402"
  },
  {
    "text": "theta X and P theta prime of X.",
    "start": "2584402",
    "end": "2592151"
  },
  {
    "text": "So that's something\nyou can compute. If I give you two\nprobability mass functions, you can compute\nthis immediately.",
    "start": "2592151",
    "end": "2599660"
  },
  {
    "text": "But if I give you\njust the densities and the original distribution,\nthe original definition",
    "start": "2599660",
    "end": "2606460"
  },
  {
    "text": "where you have to max\nover all possible events, it's not clear\nyou're going to be able to do that very quickly. So this is really the\none you can work with.",
    "start": "2606460",
    "end": "2615335"
  },
  {
    "text": "But the other one is\nreally telling you what it is doing for you. It's controlling the\ndifference of probabilities you can compute on any event.",
    "start": "2615335",
    "end": "2621077"
  },
  {
    "text": "But here, it's just\ntelling you, well, if you do it for each\nsimple event, it's little x.",
    "start": "2621077",
    "end": "2626369"
  },
  {
    "text": "It's actually simple. Now, if we have continuous\nrandom variables-- so",
    "start": "2626370",
    "end": "2633150"
  },
  {
    "text": "by the way, I didn't mention,\nbut discrete means Bernoulli. Binomial, but not only those\nthat have finite support,",
    "start": "2633150",
    "end": "2639420"
  },
  {
    "text": "like Bernoulli has\nsupport of size 2, binomial NP has\nsupport of size n--",
    "start": "2639420",
    "end": "2645760"
  },
  {
    "text": "there's n possible values it\ncan take-- but also Poisson. Poisson distribution can\ntake an infinite number of values, all the\npositive integers,",
    "start": "2645760",
    "end": "2653510"
  },
  {
    "text": "non-negative integers. And so now we have also\nthe continuous ones, such as Gaussian, exponential.",
    "start": "2653510",
    "end": "2659384"
  },
  {
    "text": "And what characterizes\nthose guys is that they have a probability density. So the density,\nremember the way I",
    "start": "2659384",
    "end": "2666630"
  },
  {
    "text": "use my density is\nwhen I want to compute the probability of\nbelonging to some event A.",
    "start": "2666630",
    "end": "2671910"
  },
  {
    "text": "The probability of X falling to\nsome subset of the real line A",
    "start": "2671910",
    "end": "2677010"
  },
  {
    "text": "is simply the integral of\nthe density on this set. That's the famous area\nunder the curve thing.",
    "start": "2677010",
    "end": "2683940"
  },
  {
    "text": "So since for each possible\nvalue, the probability at X--",
    "start": "2683940",
    "end": "2689196"
  },
  {
    "text": "so I hope you\nremember that stuff. That's just probably\nsomething that you",
    "start": "2689196",
    "end": "2697890"
  },
  {
    "text": "must remember from probability. But essentially, we know that\nthe probability that X is equal to little x is 0 for a\ncontinuous random variable,",
    "start": "2697890",
    "end": "2704820"
  },
  {
    "text": "for all possible X's. There's just none of them\nthat actually gets weight. So what we have to do is to\ndescribe the fact that it's",
    "start": "2704820",
    "end": "2711321"
  },
  {
    "text": "in some little region. So the probability that it's in\nsome interval, say, a, b, this",
    "start": "2711321",
    "end": "2718830"
  },
  {
    "text": "is the integral between A\nand B of f theta of X, dx.",
    "start": "2718830",
    "end": "2725550"
  },
  {
    "text": "So I have this density,\nsuch as the Gaussian one. And the probability that I\nbelong to the interval a, b is just the area under\nthe curve between A and B.",
    "start": "2725550",
    "end": "2736920"
  },
  {
    "text": "If you don't remember that,\nplease take immediate remedy.",
    "start": "2736920",
    "end": "2743880"
  },
  {
    "text": "So this function f, just\nlike P, is non-negative.",
    "start": "2743880",
    "end": "2748920"
  },
  {
    "text": "And rather than summing\nto 1, it integrates to 1 when I integrate it over\nthe entire sample space E.",
    "start": "2748920",
    "end": "2755119"
  },
  {
    "text": "And now the total\nvariation, well, it takes basically the same form. I said that you\nessentially replace sums",
    "start": "2755119",
    "end": "2760230"
  },
  {
    "text": "by integrals when you're\ndealing with densities. And here, it's just\nsaying, rather than having 1/2 of the sum of\nthe absolute values,",
    "start": "2760230",
    "end": "2767220"
  },
  {
    "text": "you have 1/2 of the integral\nof the absolute value of the difference. Again, if I give\nyou two densities",
    "start": "2767220",
    "end": "2775310"
  },
  {
    "text": "and if you're not too bad at\ncalculus, which you will often be, because there's lots of them\nyou can actually not compute.",
    "start": "2775310",
    "end": "2781490"
  },
  {
    "text": "But if I gave you, for example,\ntwo Gaussian densities, exponential minus x squared,\nblah, blah, blah, and I say,",
    "start": "2781490",
    "end": "2787329"
  },
  {
    "text": "just compute the total\nvariation distance, you could actually\nwrite it as an integral. Now, whether you can\nactually reduce this integral",
    "start": "2787330",
    "end": "2793040"
  },
  {
    "text": "to some particular\nnumber is another story. But you could technically do it.",
    "start": "2793040",
    "end": "2798859"
  },
  {
    "text": "So now, you have actually\na handle on this thing and you could technically\nask Mathematica, whereas asking\nMathematica to take",
    "start": "2798860",
    "end": "2805280"
  },
  {
    "text": "the max over all possible\nevents is going to be difficult. All right. So the total variation\nhas some properties.",
    "start": "2805280",
    "end": "2815240"
  },
  {
    "text": "So let's keep on the\nboard the definition that involves, say, the densities.",
    "start": "2815240",
    "end": "2825410"
  },
  {
    "text": "So think Gaussian in your mind. And you have two Gaussians,\none with mean theta and one with mean theta prime.",
    "start": "2825410",
    "end": "2830810"
  },
  {
    "text": "And I'm looking at the total\nvariation between those two guys. So if I look at P theta minus--",
    "start": "2830810",
    "end": "2840030"
  },
  {
    "text": "sorry. TV between P theta and\nP theta prime, this",
    "start": "2840030",
    "end": "2845800"
  },
  {
    "text": "is equal to 1/2 of the integral\nbetween f theta, f theta prime.",
    "start": "2845800",
    "end": "2851110"
  },
  {
    "text": "And when I don't write it-- so I don't write the\nX, dx but it's there. And then I integrate over E.",
    "start": "2851110",
    "end": "2858432"
  },
  {
    "text": "So what is this\nthing doing for me? It's just saying,\nwell, if I have-- so think of two Gaussians. For example, I have one that's\nhere and one that's here.",
    "start": "2858432",
    "end": "2864940"
  },
  {
    "text": " So this is let's say f\ntheta, f theta prime.",
    "start": "2864940",
    "end": "2871670"
  },
  {
    "text": "This guy is doing what? It's computing the absolute\nvalue of the difference between f and f theta prime.",
    "start": "2871670",
    "end": "2877910"
  },
  {
    "text": "You can check for yourself\nthat graphically, this I can represent as an area\nnot under the curve,",
    "start": "2877910",
    "end": "2885931"
  },
  {
    "text": "but between the curves. So this is this guy.",
    "start": "2885931",
    "end": "2891760"
  },
  {
    "text": " Now, this guy is really the\nintegral of the absolute value.",
    "start": "2891760",
    "end": "2900040"
  },
  {
    "text": "So this thing here,\nthis area, this is 2 times the total variation.",
    "start": "2900040",
    "end": "2905224"
  },
  {
    "text": " The scaling 1/2\nreally doesn't matter. It's just if I want to have\nan actual correspondence",
    "start": "2905224",
    "end": "2912790"
  },
  {
    "text": "between the maximum and the\nother guy, I have to do this. ",
    "start": "2912790",
    "end": "2919630"
  },
  {
    "text": "So this is what it looks like. So we have this definition. And so we have a couple of\nproperties that come into this.",
    "start": "2919630",
    "end": "2928278"
  },
  {
    "text": "The first one is\nthat it's symmetric. TV of P theta and\nP theta prime is the same as the TV between\nP theta prime and P theta.",
    "start": "2928279",
    "end": "2935970"
  },
  {
    "text": "Well, that's pretty obvious\nfrom this definition. I just flip those two,\nI get the same number.",
    "start": "2935970",
    "end": "2942089"
  },
  {
    "text": "It's actually also true\nif I take the maximum. Those things are completely\nsymmetric in theta and theta",
    "start": "2942090",
    "end": "2947630"
  },
  {
    "text": "prime. You can just flip them. It's non-negative. Is that clear to everyone that\nthis thing is non-negative?",
    "start": "2947630",
    "end": "2955640"
  },
  {
    "text": "I integrate an absolute\nvalue, so this thing is going to give me some\nnon-negative number.",
    "start": "2955640",
    "end": "2962640"
  },
  {
    "text": "And so if I integrate\nthis non-negative number, it's going to be a\nnon-negative number. The fact also that\nit's an area tells me",
    "start": "2962640",
    "end": "2969230"
  },
  {
    "text": "that it's going to\nbe non-negative. The nice thing is that if\nTV is equal to zero, then",
    "start": "2969230",
    "end": "2976900"
  },
  {
    "text": "the two distributions, the two\nprobabilities are the same.",
    "start": "2976900",
    "end": "2982490"
  },
  {
    "text": "That means that for\nevery A, P theta of A is equal to P theta\nprime of A. Now,",
    "start": "2982490",
    "end": "2989050"
  },
  {
    "text": "there's two ways to see that. The first one is to say\nthat if this integral is equal to 0, that means\nthat for almost all X,",
    "start": "2989050",
    "end": "2996650"
  },
  {
    "text": "f theta is equal\nto f theta prime. The only way I can integrate\na non-negative and get 0 is that it's 0 pretty\nmuch everywhere.",
    "start": "2996650",
    "end": "3005390"
  },
  {
    "text": "And so what it means is\nthat the two densities have to be the same\npretty much everywhere, which means that the\ndistributions are the same.",
    "start": "3005390",
    "end": "3011546"
  },
  {
    "text": "But this is not really the\nway you want to do this, because you have\nto understand what pretty much everywhere means--",
    "start": "3011546",
    "end": "3016849"
  },
  {
    "text": "which I should really\nsay almost everywhere. That's the formal\nway of saying it. But let's go to\nthis definition--",
    "start": "3016850",
    "end": "3022280"
  },
  {
    "text": " which is gone. Yeah. That's the one here.",
    "start": "3022280",
    "end": "3028670"
  },
  {
    "text": "The max of those two guys, if\nthis maximum is equal to 0--",
    "start": "3028670",
    "end": "3035230"
  },
  {
    "text": "I have a maximum of non-negative\nnumbers, their absolute values. Their maximum is\nequal to 0, well,",
    "start": "3035230",
    "end": "3042090"
  },
  {
    "text": "they better be all equal\nto 0, because if one is not equal to 0, then the\nmaximum is not equal to 0.",
    "start": "3042090",
    "end": "3047470"
  },
  {
    "text": "So those two guys,\nfor those two things to be-- for the maximum\nto be equal to 0, then each of the\nindividual absolute values",
    "start": "3047470",
    "end": "3054220"
  },
  {
    "text": "have to be equal to 0, which\nmeans that the probability here is equal to this probability\nhere for every event A.",
    "start": "3054220",
    "end": "3063730"
  },
  {
    "text": "So those two things-- this is nice, right? That's called definiteness. The total variation equal\nto 0 implies that P theta",
    "start": "3063730",
    "end": "3070900"
  },
  {
    "text": "is equal to P theta prime. So that's really some\nnotion of distance, right? That's what we want.",
    "start": "3070900",
    "end": "3076060"
  },
  {
    "text": "If this thing\nbeing small implied that P theta could be all\nover the place compared to P theta prime, that\nwould not help very much.",
    "start": "3076060",
    "end": "3084270"
  },
  {
    "text": "Now, there's also the\ntriangle inequality that follows immediately\nfrom the triangle inequality inside this guy.",
    "start": "3084270",
    "end": "3092730"
  },
  {
    "text": "If I squeeze in some f\ntheta prime prime in there, I'm going to use the\ntriangle inequality and get the triangle\ninequality for the whole thing.",
    "start": "3092730",
    "end": "3099486"
  },
  {
    "text": " Yeah? AUDIENCE: The fact that\nyou need two definitions",
    "start": "3099486",
    "end": "3105287"
  },
  {
    "text": "of the [INAUDIBLE],,\nis it something obvious or is it complete? PHILIPPE RIGOLLET:\nI'll do it for you now.",
    "start": "3105287",
    "end": "3112930"
  },
  {
    "text": "So let's just prove that\nthose two things are actually giving me the same definition.",
    "start": "3112930",
    "end": "3118756"
  },
  {
    "text": " So what I'm going to do\nis I'm actually going to start with the second one.",
    "start": "3118756",
    "end": "3124420"
  },
  {
    "text": "And I'm going to write-- I'm going to start with\nthe density version. But as an exercise, you can\ndo it for the PMF version",
    "start": "3124420",
    "end": "3130300"
  },
  {
    "text": "if you prefer. So I'm going to start\nwith the fact that f-- ",
    "start": "3130300",
    "end": "3140240"
  },
  {
    "text": "so I'm going to write f of g so\nI don't have to write f and g. So think of this as being f sub\ntheta, and think of this guy",
    "start": "3140240",
    "end": "3147490"
  },
  {
    "text": "as being f sub theta prime. I just don't want to have to\nwrite indices all the time. So I'm going to start with\nthis thing, the integral of f",
    "start": "3147490",
    "end": "3154970"
  },
  {
    "text": "of X minus g of X dx. The first thing I'm going to do\nis this is an absolute value,",
    "start": "3154970",
    "end": "3161910"
  },
  {
    "text": "so either the number in the\nabsolute value is positive and I actually kept it\nlike that, or it's negative",
    "start": "3161910",
    "end": "3167390"
  },
  {
    "text": "and I flipped its sign. So let's just split\nbetween those two cases. So this thing is equal\nto 1/2 the integral of--",
    "start": "3167390",
    "end": "3175460"
  },
  {
    "text": "so let me actually\nwrite the set A star as being the set of X's such that\nf of X is larger than g of X.",
    "start": "3175460",
    "end": "3189240"
  },
  {
    "text": "So that's the set on\nwhich the difference is going to be positive\nor the difference is going to be negative.",
    "start": "3189240",
    "end": "3194370"
  },
  {
    "text": "So this, again,\nis equivalent to f of X minus g of X is positive.",
    "start": "3194370",
    "end": "3203280"
  },
  {
    "text": "OK. Everybody agrees? So this is the set\nI'm interested in. ",
    "start": "3203280",
    "end": "3209040"
  },
  {
    "text": "So now I'm going to split\nmy integral into two parts, in A, A star, so on A\nstar, f is larger than g,",
    "start": "3209040",
    "end": "3218250"
  },
  {
    "text": "so the absolute value is\njust the difference itself. ",
    "start": "3218250",
    "end": "3225150"
  },
  {
    "text": "So here I put parenthesis\nrather than absolute value. And then I have plus 1/2 of\nthe integral on the complement.",
    "start": "3225150",
    "end": "3234330"
  },
  {
    "text": "What are you guys used to to\nwrite the complement, to the C or the bar?",
    "start": "3234330",
    "end": "3241005"
  },
  {
    "text": "To the C?  And so here on the complement,\nthen f is less than g,",
    "start": "3241005",
    "end": "3248320"
  },
  {
    "text": "so this is actually really\ng of X minus f of X, dx.",
    "start": "3248320",
    "end": "3257810"
  },
  {
    "text": "Everybody's with me here? So I just said-- I mean, those are just\nrewriting what the definition",
    "start": "3257810",
    "end": "3263390"
  },
  {
    "text": "of the absolute value is. ",
    "start": "3263390",
    "end": "3273290"
  },
  {
    "text": "OK. So now there's nice things\nthat I know about f and g. And the two nice things is that\nthe integral of f is equal to 1",
    "start": "3273290",
    "end": "3280880"
  },
  {
    "text": "and the integral\nof g is equal to 1. ",
    "start": "3280880",
    "end": "3286270"
  },
  {
    "text": "This implies that the integral\nof f minus g is equal to what?",
    "start": "3286270",
    "end": "3293614"
  },
  {
    "text": "AUDIENCE: 0. PHILIPPE RIGOLLET: 0. And so now that\nmeans that if I want",
    "start": "3293614",
    "end": "3299059"
  },
  {
    "text": "to just go from the integral\nhere on A complement",
    "start": "3299060",
    "end": "3304130"
  },
  {
    "text": "to the integral on A-- or on A star, complement\nto the integral of A star, I just have to flip the sign.",
    "start": "3304130",
    "end": "3311700"
  },
  {
    "text": "So that implies that\nan integral on A star complement of g\nof X minus f of X,",
    "start": "3311700",
    "end": "3321198"
  },
  {
    "text": "dx, this is simply equal\nto the integral on A star of f of X minus g of X, dx.",
    "start": "3321198",
    "end": "3330250"
  },
  {
    "start": "3330250",
    "end": "3340880"
  },
  {
    "text": "All right. So now this guy becomes\nthis guy over there.",
    "start": "3340880",
    "end": "3346100"
  },
  {
    "text": "So I have 1/2 of this\nplus 1/2 of the same guy, so that means that 1/2 half\nof the integral between of f",
    "start": "3346100",
    "end": "3355720"
  },
  {
    "text": "minus g absolute value-- so that was my\noriginal definition, this thing is actually equal\nto the integral on A star",
    "start": "3355720",
    "end": "3363890"
  },
  {
    "text": "of f of X minus g of X, dx.",
    "start": "3363890",
    "end": "3370379"
  },
  {
    "text": " And this is simply\nequal to P of A star--",
    "start": "3370379",
    "end": "3381440"
  },
  {
    "text": "so say Pf of A start\nminus Pg of A star. ",
    "start": "3381440",
    "end": "3394160"
  },
  {
    "text": "Which one is larger\nthan the other one? ",
    "start": "3394160",
    "end": "3401610"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: It is. Just look at this board. AUDIENCE: [INAUDIBLE]",
    "start": "3401610",
    "end": "3407406"
  },
  {
    "text": "PHILIPPE RIGOLLET: What? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nThe first one has to be larger, because\nthis thing is actually equal to a non-negative number.",
    "start": "3407406",
    "end": "3413271"
  },
  {
    "start": "3413271",
    "end": "3419590"
  },
  {
    "text": "So now I have this absolute\nvalue of two things, and so I'm closer to\nthe actual definition. But I still need to show\nyou that this thing is",
    "start": "3419590",
    "end": "3426910"
  },
  {
    "text": "the maximum value. So this is definitely at\nmost the maximum over A of Pf",
    "start": "3426910",
    "end": "3437710"
  },
  {
    "text": "of A minus Pg of A. That's certainly true.",
    "start": "3437710",
    "end": "3444290"
  },
  {
    "text": "Right? We agree with this? Because this is just\nfor one specific A,",
    "start": "3444290",
    "end": "3450620"
  },
  {
    "text": "and I'm bounding it by the\nmaximum over all possible A. So that's clearly true.",
    "start": "3450620",
    "end": "3456932"
  },
  {
    "text": "So now I have to go\nthe other way around. I have to show you that the max\nis actually this guy, A star.",
    "start": "3456932",
    "end": "3464369"
  },
  {
    "text": "So why would that be true? Well, let's just inspect\nthis thing over there. So we want to show\nthat if I take",
    "start": "3464370",
    "end": "3470730"
  },
  {
    "text": "any other A in this integral\nthan this guy A star, it's actually got to\ndecrease its value.",
    "start": "3470730",
    "end": "3476580"
  },
  {
    "text": "So we have this function. I'm going to call\nthis function delta. ",
    "start": "3476580",
    "end": "3482314"
  },
  {
    "text": "And what we have\nis-- so let's say this function looks like this. Now it's the difference\nbetween two densities. It doesn't have to\nintegrate-- it doesn't",
    "start": "3482314",
    "end": "3489500"
  },
  {
    "text": "have to be non-negative. But it certainly has\nto integrate to 0. ",
    "start": "3489500",
    "end": "3495510"
  },
  {
    "text": "And so now I take this thing. And the A star, what\nis the set A star here?",
    "start": "3495510",
    "end": "3502126"
  },
  {
    "text": "The set A star is the set\nover which the function delta is non-negative.",
    "start": "3502126",
    "end": "3507644"
  },
  {
    "start": "3507645",
    "end": "3516340"
  },
  {
    "text": "So that's just the definition. A star was the set over\nwhich f minus g was positive,",
    "start": "3516340",
    "end": "3521660"
  },
  {
    "text": "and f minus g was\njust called delta. So what it means is that\nwhat I'm really integrating",
    "start": "3521660",
    "end": "3527720"
  },
  {
    "text": "is delta on this set. So it's this area\nunder the curve,",
    "start": "3527720",
    "end": "3533570"
  },
  {
    "text": "just on the positive things. Agreed? So now let's just make some\ntiny variations around this guy.",
    "start": "3533570",
    "end": "3543290"
  },
  {
    "text": "If I take A to be\nlarger than A star-- so let me add, for\nexample, this part here.",
    "start": "3543290",
    "end": "3550279"
  },
  {
    "text": " That means that when\nI compute my integral,",
    "start": "3550280",
    "end": "3555680"
  },
  {
    "text": "I'm removing this\narea under the curve. It's negative. The integral here is negative. So if I start adding something\nto A, the value goes lower.",
    "start": "3555680",
    "end": "3565160"
  },
  {
    "text": "If I start removing something\nfrom A, like say this guy, I'm actually removing this\nvalue from the integral.",
    "start": "3565160",
    "end": "3572450"
  },
  {
    "text": "So there's no way. I'm actually stuck. This A star is the one\nthat actually maximizes the integral of this function.",
    "start": "3572450",
    "end": "3579830"
  },
  {
    "text": "So we used the fact\nthat for any function,",
    "start": "3579830",
    "end": "3589470"
  },
  {
    "text": "say delta, the integral\nover A of delta",
    "start": "3589470",
    "end": "3599180"
  },
  {
    "text": "is less than the integral\nover the set of X's such that delta of X is\nnon-negative of delta of X, dx.",
    "start": "3599180",
    "end": "3607670"
  },
  {
    "text": " And that's an obvious\nfact, just by picture, say.",
    "start": "3607670",
    "end": "3613518"
  },
  {
    "text": " And that's true for all A. Yeah?",
    "start": "3613518",
    "end": "3624972"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\ncould you use like a portion under the\naxis as like less than",
    "start": "3624972",
    "end": "3633106"
  },
  {
    "text": "or equal to the\nportion above the axis? PHILIPPE RIGOLLET:\nIt's actually equal. We know that the\nintegral of f minus g--",
    "start": "3633106",
    "end": "3639005"
  },
  {
    "text": "the integral of delta is 0. So there's actually exactly\nthe same area above and below.",
    "start": "3639005",
    "end": "3647344"
  },
  {
    "text": "But yeah, you're right. You could go to\nthe extreme cases. You're right. ",
    "start": "3647344",
    "end": "3657470"
  },
  {
    "text": "No. It's actually still be\ntrue, even if there was-- if this was a constant,\nthat would still be true.",
    "start": "3657470",
    "end": "3662720"
  },
  {
    "text": "Here, I never use the fact that\nthe integral is equal to 0. ",
    "start": "3662720",
    "end": "3671380"
  },
  {
    "text": "I could shift this function by\n1 so that the integral of delta is equal to 1,\nand it would still",
    "start": "3671380",
    "end": "3678230"
  },
  {
    "text": "be true that it's maximized\nwhen I take A to be the set where it's positive.",
    "start": "3678230",
    "end": "3684892"
  },
  {
    "text": "Just need to make sure that\nthere is someplace where it is, but that's about it. ",
    "start": "3684892",
    "end": "3693390"
  },
  {
    "text": "Of course, we used this before,\nwhen we made this thing. But just the last\nargument, this last fact",
    "start": "3693390",
    "end": "3698730"
  },
  {
    "text": "does not require that. ",
    "start": "3698730",
    "end": "3703820"
  },
  {
    "text": "All right. So now we have this notion of-- I need the-- ",
    "start": "3703820",
    "end": "3712531"
  },
  {
    "text": "OK. So we have this\nnotion of distance between probability measures.",
    "start": "3712531",
    "end": "3718829"
  },
  {
    "text": "I mean, these things\nare exactly what-- if I were to be in a formal\nmath class and I said, here are the axioms that\na distance should satisfy,",
    "start": "3718830",
    "end": "3726060"
  },
  {
    "text": "those are exactly those things. If it's not\nsatisfying this thing, it's called pseudo-distance or\nquasi-distance or just metric",
    "start": "3726060",
    "end": "3733799"
  },
  {
    "text": "or nothing at all, honestly. So it's a distance. It's symmetric,\nnon-negative, equal to 0,",
    "start": "3733800",
    "end": "3738930"
  },
  {
    "text": "if and only if the two\narguments are equal, then it satisfies the\ntriangle inequality.",
    "start": "3738930",
    "end": "3745869"
  },
  {
    "text": "And so that means that we have\nthis actual total variation distance between\nprobability distributions.",
    "start": "3745870",
    "end": "3751140"
  },
  {
    "text": "And here is now a statistical\nstrategy to implement our goal.",
    "start": "3751140",
    "end": "3756510"
  },
  {
    "text": "Remember, our goal\nwas to spit out a theta hat, which was\nclose such that P theta",
    "start": "3756510",
    "end": "3761940"
  },
  {
    "text": "hat was close to P theta star. So hopefully, we were trying\nto minimize the total variation",
    "start": "3761940",
    "end": "3768940"
  },
  {
    "text": "distance between P theta\nhat and P theta star. Now, we cannot do that, because\njust by this fact, this slide,",
    "start": "3768940",
    "end": "3775090"
  },
  {
    "text": "if we wanted to do that\ndirectly, we would just take-- well, let's take theta hat\nequals theta star and that will give me the value 0.",
    "start": "3775090",
    "end": "3780880"
  },
  {
    "text": "And that's the minimum\npossible value we can take. The problem is\nthat we don't know what the total variation is to\nsomething that we don't know.",
    "start": "3780880",
    "end": "3787342"
  },
  {
    "text": "We know how to compute total\nvariations if I give you the two arguments. But here, one of the\narguments is not known.",
    "start": "3787342",
    "end": "3792559"
  },
  {
    "text": "P theta star is not known to\nus, so we need to estimate it. And so here is the strategy.",
    "start": "3792560",
    "end": "3798910"
  },
  {
    "text": "Just build an estimator\nof the total variation distance between P\ntheta and P theta star",
    "start": "3798910",
    "end": "3804580"
  },
  {
    "text": "for all candidate theta,\nall possible theta in capital theta.",
    "start": "3804580",
    "end": "3810240"
  },
  {
    "text": "Now, if this is a good estimate,\nthen when I minimize it, I should get something\nthat's close to P theta star.",
    "start": "3810240",
    "end": "3817230"
  },
  {
    "text": "So here's the strategy. This is my function\nthat maps theta to the total variation between\nP theta and P theta star.",
    "start": "3817230",
    "end": "3824340"
  },
  {
    "text": "I know it's minimized\nat theta star. That's definitely TV of P--\nand the value here, the y-axis",
    "start": "3824340",
    "end": "3831090"
  },
  {
    "text": "should say 0. And so I don't know\nthis guy, so I'm going to estimate it\nby some estimator that",
    "start": "3831090",
    "end": "3836810"
  },
  {
    "text": "comes from my data. Hopefully, the more data I have,\nthe better this estimator is. And I'm going to try to\nminimize this estimator now.",
    "start": "3836810",
    "end": "3843391"
  },
  {
    "text": "And if the two things are\nclose, then the minima should be close. That's a pretty good\nestimation strategy.",
    "start": "3843391",
    "end": "3849560"
  },
  {
    "text": "The problem is that\nit's very unclear how you would build\nthis estimator of TV, of the Total Variation.",
    "start": "3849560",
    "end": "3858710"
  },
  {
    "text": "So building\nestimators, as I said, typically consists in replacing\nexpectations by averages.",
    "start": "3858710",
    "end": "3865160"
  },
  {
    "text": "But there's no simple way of\nexpressing the total variation distance as the\nexpectations with respect",
    "start": "3865160",
    "end": "3871230"
  },
  {
    "text": "to theta star of anything. So what we're going\nto do is we're going to move from\ntotal variation distance",
    "start": "3871230",
    "end": "3878190"
  },
  {
    "text": "to another notion of\ndistance that sort of has the same properties\nand the same feeling and the same motivations as\nthe total variation distance.",
    "start": "3878190",
    "end": "3887040"
  },
  {
    "text": "But for this guy, we\nwill be able to build an estimate for it,\nbecause it's actually going to be of the form\nexpectation of something.",
    "start": "3887040",
    "end": "3893929"
  },
  {
    "text": "And we're going to\nbe able to replace the expectation by an average\nand then minimize this average.",
    "start": "3893929",
    "end": "3900280"
  },
  {
    "text": "So this surrogate for\ntotal variation distance is actually called the\nKullback-Leibler divergence.",
    "start": "3900280",
    "end": "3907510"
  },
  {
    "text": "And why we call it divergence\nis because it's actually not a distance. It's not going to be\nsymmetric to start with.",
    "start": "3907510",
    "end": "3914760"
  },
  {
    "text": "So this Kullback-Leibler\nor even KL divergence-- I will just refer to it as KL--",
    "start": "3914760",
    "end": "3920789"
  },
  {
    "text": "is actually just\nmore convenient. But it has some roots coming\nfrom information theory, which",
    "start": "3920790",
    "end": "3927480"
  },
  {
    "text": "I will not delve into. But if any of you is\nactually a Core 6 student, I'm sure you've\nseen that in some--",
    "start": "3927480",
    "end": "3932970"
  },
  {
    "text": "I don't know-- course that\nhas any content on information",
    "start": "3932970",
    "end": "3937980"
  },
  {
    "text": "theory. All right. So the KL divergence between two\nprobability measures, P theta and P theta prime--",
    "start": "3937980",
    "end": "3943789"
  },
  {
    "text": "and here, as I said, it's not\ngoing to be the symmetric, so it's very important\nfor you to specify",
    "start": "3943790",
    "end": "3949680"
  },
  {
    "text": "which order you say it is,\nbetween P theta and P theta prime. It's different from saying\nbetween P theta prime and P",
    "start": "3949680",
    "end": "3955060"
  },
  {
    "text": "theta. And so we denote it by KL. And so remember, before we had\neither the sum or the integral",
    "start": "3955060",
    "end": "3964010"
  },
  {
    "text": "of 1/2 of the distance--\nabsolute value of the distance between the PMFs and 1/2\nof the absolute values",
    "start": "3964010",
    "end": "3970549"
  },
  {
    "text": "of the distances between the\nprobability density functions.",
    "start": "3970550",
    "end": "3977900"
  },
  {
    "text": "And then we replace\nthis absolute value of the distance divided by\n2 by this weird function.",
    "start": "3977900",
    "end": "3984740"
  },
  {
    "text": "This function is P\ntheta, log P theta, divided by P theta prime.",
    "start": "3984740",
    "end": "3990290"
  },
  {
    "text": "That's the function. That's a weird function. OK. So this was what we had.",
    "start": "3990290",
    "end": "3998359"
  },
  {
    "text": " That's the TV. ",
    "start": "3998360",
    "end": "4004670"
  },
  {
    "text": "And the KL, if I use the\nsame notation, f and g, is integral of f of X, log\nof f of X over g of X, dx.",
    "start": "4004670",
    "end": "4017315"
  },
  {
    "text": " It's a bit different.",
    "start": "4017315",
    "end": "4024279"
  },
  {
    "text": "And I go from discrete to\ncontinuous using an integral. Everybody can read this.",
    "start": "4024280",
    "end": "4030240"
  },
  {
    "text": "Everybody's fine with this. Is there any uncertainty about\nthe actual definition here?",
    "start": "4030240",
    "end": "4035780"
  },
  {
    "text": "So here I go straight\nto the definition, which is just\nplugging the functions into some integral and compute.",
    "start": "4035780",
    "end": "4042190"
  },
  {
    "text": "So I don't bother with\nmaxima or anything. I mean, there is\nsomething like that, but it's certainly not as\nnatural as the total variation.",
    "start": "4042190",
    "end": "4049885"
  },
  {
    "text": "Yes? AUDIENCE: The total\nvariation, [INAUDIBLE].. ",
    "start": "4049885",
    "end": "4058732"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYes, just because it's hard to build anything\nfrom total variation, because I don't know it. So it's very difficult.\nBut if you can actually--",
    "start": "4058732",
    "end": "4065835"
  },
  {
    "text": "and even computing it\nbetween two Gaussians, just try it for yourself. And please stop doing it\nafter at most six minutes,",
    "start": "4065835",
    "end": "4072740"
  },
  {
    "text": "because you won't\nbe able to do it. And so it's just very\nhard to manipulate, like this integral of\nabsolute values of differences",
    "start": "4072740",
    "end": "4079070"
  },
  {
    "text": "between probability\ndensity function, at least for the probability\ndensity functions we're used to manipulate\nis actually a nightmare.",
    "start": "4079070",
    "end": "4084860"
  },
  {
    "text": "And so people prefer KL,\nbecause for the Gaussian, this is going to be theta\nminus theta prime squared.",
    "start": "4084860",
    "end": "4090770"
  },
  {
    "text": "And then we're\ngoing to be happy. And so those things are\nmuch easier to manipulate. But it's really--\nthe total variation",
    "start": "4090770",
    "end": "4098028"
  },
  {
    "text": "is telling you how\nfar in the worst case the two probabilities can be. This is really the\nintrinsic notion",
    "start": "4098029",
    "end": "4103220"
  },
  {
    "text": "of closeness between\nprobabilities. So that's really the\none-- if we could, that's the one we\nwould go after.",
    "start": "4103220",
    "end": "4110202"
  },
  {
    "text": "Sometimes people will\ncompute them numerically, so that they can say, oh, here's\nthe total variation distance I have between those two things.",
    "start": "4110202",
    "end": "4116899"
  },
  {
    "text": "And then you actually\nknow that that means they are close, because\nthe absolute value-- if I tell you total variation is\n0.01, like we did here,",
    "start": "4116899",
    "end": "4124370"
  },
  {
    "text": "it has a very specific meaning. If I tell you the KL\ndivergence is 0.01,",
    "start": "4124370",
    "end": "4129761"
  },
  {
    "text": "it's not clear what it means. ",
    "start": "4129762",
    "end": "4135130"
  },
  {
    "text": "OK. So what are the properties? The KL divergence between\nP theta and P theta prime",
    "start": "4135130",
    "end": "4140870"
  },
  {
    "text": "is different from the KL\ndivergence between P theta prime and P theta in general. Of course, in general,\nbecause if theta",
    "start": "4140870",
    "end": "4147639"
  },
  {
    "text": "is equal to theta prime,\nthen this certainly is true. So there's cases\nwhen it's not true.",
    "start": "4147640",
    "end": "4154599"
  },
  {
    "text": "The KL divergence\nis non-negative. Who knows the Jensen's\ninequality here?",
    "start": "4154600",
    "end": "4159741"
  },
  {
    "text": "That should be a subset\nof the people who raised their hand when I asked\nwhat a convex function is.",
    "start": "4159742",
    "end": "4165310"
  },
  {
    "text": "All right. So you know what\nJensen's inequality is. This is Jensen's-- the\nproof is just one step",
    "start": "4165310",
    "end": "4170490"
  },
  {
    "text": "Jensen's inequality, which\nwe will not go into details. But that's basically\nan inequality",
    "start": "4170490",
    "end": "4175568"
  },
  {
    "text": "involving expectation\nof a convex function of a random variable compared\nto the convex function of the expectation\nof a random variable.",
    "start": "4175569",
    "end": "4182065"
  },
  {
    "text": " If you know Jensen,\nhave fun and prove it.",
    "start": "4182065",
    "end": "4188580"
  },
  {
    "text": "What's really nice is that\nif the KL is equal to 0, then the two distributions\nare the same.",
    "start": "4188580",
    "end": "4195220"
  },
  {
    "text": "And that's something\nwe're looking for. Everything else we're\nhappy to throw out. And actually, if\nyou pay attention,",
    "start": "4195220",
    "end": "4200478"
  },
  {
    "text": "we're actually really\nthrowing out everything else. So they're not symmetric. It does satisfy the triangle\ninequality in general.",
    "start": "4200478",
    "end": "4208530"
  },
  {
    "text": "But it's non-negative and\nit's 0 if and only if the two distributions are the same.",
    "start": "4208530",
    "end": "4213922"
  },
  {
    "text": "And that's all we care about. And that's what we call\na divergence rather than a distance, and divergence will\nbe enough for our purposes.",
    "start": "4213922",
    "end": "4221909"
  },
  {
    "text": "And actually, this\nasymmetry, the fact that it's not flipping--\nthe first time I saw it, I was just annoyed.",
    "start": "4221910",
    "end": "4227380"
  },
  {
    "text": "I was like, can we\njust like, I don't know, take the average\nof the KL between P theta and P theta prime and P\ntheta prime and P theta,",
    "start": "4227380",
    "end": "4234270"
  },
  {
    "text": "you would think maybe\nyou could do this. You just symmatrize it by just\ntaking the average of the two",
    "start": "4234270",
    "end": "4239590"
  },
  {
    "text": "possible values it can take. The problem is that this will\nstill not satisfy the triangle",
    "start": "4239590",
    "end": "4244930"
  },
  {
    "text": "inequality. And there's no way basically\nto turn it into something that is a distance. But the divergence is doing\na pretty good thing for us.",
    "start": "4244930",
    "end": "4252350"
  },
  {
    "text": "And this is what will allow us\nto estimate it and basically overcome what we could not\ndo with the total variation.",
    "start": "4252350",
    "end": "4263160"
  },
  {
    "text": "So the first thing\nthat you want to notice is the total\nvariation distance-- the KL divergence,\nsorry, is actually",
    "start": "4263160",
    "end": "4270130"
  },
  {
    "text": "an expectation of something. Look at what it is here.",
    "start": "4270130",
    "end": "4275260"
  },
  {
    "text": "It's the integral of some\nfunction against a density.",
    "start": "4275260",
    "end": "4280420"
  },
  {
    "text": "That's exactly the definition\nof an expectation, right? So this is the expectation\nof this particular function",
    "start": "4280420",
    "end": "4289950"
  },
  {
    "text": "with respect to this density f. So in particular, if I call\nthis is density f-- if I say,",
    "start": "4289950",
    "end": "4295650"
  },
  {
    "text": "I want the true distribution\nto be the first argument, this is an expectation\nwith respect to the true distribution from\nwhich my data is actually",
    "start": "4295650",
    "end": "4302310"
  },
  {
    "text": "drawn of the log of this ratio. So ha ha. I'm a statistician.",
    "start": "4302310",
    "end": "4307700"
  },
  {
    "text": "Now I have an expectation. I can replace it by an\naverage, because I have data from this distribution. And I could actually replace\nthe expectation by an average",
    "start": "4307700",
    "end": "4314940"
  },
  {
    "text": "and try to minimize here. The problem is that-- actually the star here should\nbe in front of the theta,",
    "start": "4314940",
    "end": "4320250"
  },
  {
    "text": "not of the P, right? That's P theta star,\nnot P star theta. But here, I still\ncannot compute it,",
    "start": "4320250",
    "end": "4325960"
  },
  {
    "text": "because I have this P\ntheta star that shows up. I don't know what it is. And that's now where\nthe log plays a role.",
    "start": "4325960",
    "end": "4333500"
  },
  {
    "text": "If you actually pay\nattention, I said you can use Jensen to\nprove all this stuff. You could actually replace the\nlog by any concave function.",
    "start": "4333500",
    "end": "4341110"
  },
  {
    "text": "That would be f divergent. That's called an f divergence. But the log itself is a\nvery, very specific property,",
    "start": "4341110",
    "end": "4346949"
  },
  {
    "text": "which allows us to say\nthat the log of the ratio is the ratio of the log.",
    "start": "4346950",
    "end": "4353290"
  },
  {
    "text": "Now, this thing here\ndoes not depend on theta.",
    "start": "4353290",
    "end": "4358620"
  },
  {
    "text": "If I think of this KL divergence\nas a function of theta, then the first part is\nactually a constant.",
    "start": "4358620",
    "end": "4365239"
  },
  {
    "text": "If I change theta, this thing\nis never going to change. It depends only on theta star. So if I look at\nthis function KL--",
    "start": "4365239",
    "end": "4371480"
  },
  {
    "start": "4371480",
    "end": "4383200"
  },
  {
    "text": "so if I look at the\nfunction, theta maps to KL P theta\nstar, P theta, it's",
    "start": "4383200",
    "end": "4391450"
  },
  {
    "text": "of the form expectation\nwith respect to theta star, log of P theta star\nof X. And then I",
    "start": "4391450",
    "end": "4403780"
  },
  {
    "text": "have minus expectation with\nrespect to theta star of log",
    "start": "4403780",
    "end": "4409610"
  },
  {
    "text": "of P theta of x. Now as I said, this thing\nhere, this second expectation",
    "start": "4409610",
    "end": "4418900"
  },
  {
    "text": "is a function of theta. When theta changes, this\nthing is going to change. And that's a good thing. We want something that reflects\nhow close theta and theta",
    "start": "4418900",
    "end": "4425754"
  },
  {
    "text": "star are. But this thing is\nnot going to change. This is a fixed value. Actually, it's the negative\nentropy of P theta star.",
    "start": "4425754",
    "end": "4433125"
  },
  {
    "text": "And if you've\nheard of KL, you've probably heard of entropy. And that's what-- it's\nbasically minus the entropy.",
    "start": "4433125",
    "end": "4438820"
  },
  {
    "text": "And that's a quantity that\njust depends on theta star. But it's just the number. I could compute this\nnumber if I told",
    "start": "4438820",
    "end": "4445030"
  },
  {
    "text": "you this is n theta star 1. You could compute this. So now I'm going\nto try to minimize",
    "start": "4445030",
    "end": "4451640"
  },
  {
    "text": "the estimate of this function. And minimizing a function or\na function plus a constant",
    "start": "4451640",
    "end": "4456870"
  },
  {
    "text": "is the same thing. I'm just shifting the\nfunction here or here, but it's the same minimizer.",
    "start": "4456870",
    "end": "4463560"
  },
  {
    "text": "OK. So the function that maps\ntheta to KL of P theta star",
    "start": "4463560",
    "end": "4468910"
  },
  {
    "text": "to P theta is of the form\nconstant minus this expectation of a log of P theta.",
    "start": "4468910",
    "end": "4475810"
  },
  {
    "text": "Everybody agrees? Are there any\nquestions about this? Are there any\nremarks, including I",
    "start": "4475810",
    "end": "4482739"
  },
  {
    "text": "have no idea what's\nhappening right now? OK. We're good? Yeah.",
    "start": "4482740",
    "end": "4488200"
  },
  {
    "text": "AUDIENCE: So when you're\nactually employing this method, how do you know which theta\nto use as theta star and which isn't? PHILIPPE RIGOLLET: So this is\nnot a method just yet, right?",
    "start": "4488200",
    "end": "4495600"
  },
  {
    "text": "I'm just describing to\nyou what the KL divergence between two distributions is. If you really wanted\nto compute it, you would need to know\nwhat P theta star is",
    "start": "4495600",
    "end": "4501929"
  },
  {
    "text": "and what P theta is. AUDIENCE: Right. PHILIPPE RIGOLLET: And so here,\nI'm just saying at some point, we still-- so here, you see--",
    "start": "4501930",
    "end": "4507650"
  },
  {
    "text": "so now let's move onto one step. I don't know expectation\nof theta star. But I have data that comes\nfrom distribution P theta star.",
    "start": "4507650",
    "end": "4515904"
  },
  {
    "text": "So the expectation by\nthe law of large numbers should be close to the average. And so what I'm doing\nis I'm replacing any--",
    "start": "4515904",
    "end": "4523670"
  },
  {
    "text": "I can actually-- this is a very\nstandard estimation method. You write something as an\nexpectation with respect",
    "start": "4523670",
    "end": "4530360"
  },
  {
    "text": "to the data-generating\nprocess of some function. And then you replace this by\nthe average of this function.",
    "start": "4530360",
    "end": "4537349"
  },
  {
    "text": "And the law of large\nnumbers tells me that those two quantities\nshould actually be close. Now, it doesn't mean that's\ngoing to be the end of the day,",
    "start": "4537349",
    "end": "4543820"
  },
  {
    "text": "right. When we did Xn bar, that\nwas the end of the day. We had an expectation. We replaced it by an average.",
    "start": "4543820",
    "end": "4549850"
  },
  {
    "text": "And then we were gone. But here, we still\nhave to do something, because this is not\ntelling me what theta is.",
    "start": "4549850",
    "end": "4555250"
  },
  {
    "text": "Now I still have to\nminimize this average. So this is now my candidate\nestimator for KL, KL hat.",
    "start": "4555250",
    "end": "4564369"
  },
  {
    "text": "And that's the one\nwhere I said, well, it's going to be of the\nform of constant. And this constant, I don't know. You're right.",
    "start": "4564370",
    "end": "4569771"
  },
  {
    "text": "I have no idea what\nthis constant is. It depends on P theta star. But then I have minus something\nthat I can completely compute.",
    "start": "4569771",
    "end": "4576310"
  },
  {
    "text": "If you give me data and theta,\nI can compute this entire thing. And now what I claim is that\nthe minimizer of f or f plus--",
    "start": "4576310",
    "end": "4585670"
  },
  {
    "text": "f of X or f of X plus\n4 are the same thing, or say 4 plus f of\nX. I'm just shifting",
    "start": "4585670",
    "end": "4592200"
  },
  {
    "text": "the plot of my\nfunction up and down, but the minimizer stays\nexactly where it is. ",
    "start": "4592200",
    "end": "4599590"
  },
  {
    "text": "If I have a function--  so now I have a\nfunction of theta.",
    "start": "4599590",
    "end": "4605284"
  },
  {
    "start": "4605284",
    "end": "4611620"
  },
  {
    "text": "This is KL hat of P\ntheta star, P theta. And it's of the form--\nit's a function like this.",
    "start": "4611620",
    "end": "4618831"
  },
  {
    "text": "I don't know where\nthis function is. It might very well be this\nfunction or this function.",
    "start": "4618831",
    "end": "4626880"
  },
  {
    "text": "Every time it's a translation\non the y-axis of all these guys. And the value that I translated\nby depends on theta star.",
    "start": "4626880",
    "end": "4634690"
  },
  {
    "text": "I don't know what it is. But what I claim is that the\nminimizer is always this guy, regardless of what the value is.",
    "start": "4634690",
    "end": "4642428"
  },
  {
    "text": "OK? So when I say constant, it's a\nconstant with respect to theta.",
    "start": "4642428",
    "end": "4648560"
  },
  {
    "text": "It's an unknown constant. But it's with respect to theta,\nso without loss of generality, I can assume that this\nconstant is 0 for my purposes,",
    "start": "4648560",
    "end": "4656840"
  },
  {
    "text": "or 25 if you prefer.  All right. So we'll just keep going\non this property next time.",
    "start": "4656840",
    "end": "4666420"
  },
  {
    "text": "And we'll see how from\nhere we can move on to-- the likelihood is actually going\nto come out of this formula.",
    "start": "4666420",
    "end": "4671900"
  },
  {
    "text": "Thanks. ",
    "start": "4671900",
    "end": "4676335"
  }
]