[
  {
    "text": "hey folks I'm Eric at Lang chain and today we're going to be talking about using mongodb semantic caching in order",
    "start": "2360",
    "end": "8000"
  },
  {
    "text": "to cut our llm costs today mongodb launched their integration package with Lang chain so you can use mongodb as a",
    "start": "8000",
    "end": "16278"
  },
  {
    "text": "vector store or as a cache or even a semantic cache which we're going to go through today uh we're going to be using",
    "start": "16279",
    "end": "22160"
  },
  {
    "text": "a few different providers in our solution um so the first thing that we're going to go through is kind of who",
    "start": "22160",
    "end": "27920"
  },
  {
    "text": "who each of them are uh then we're going to go through two examples from our cookbook one just being a basic prompt",
    "start": "27920",
    "end": "33239"
  },
  {
    "text": "in llm chain with semantic caching and one with rag where we'll actually discuss some of the challenges of using",
    "start": "33239",
    "end": "39160"
  },
  {
    "text": "semantic caching with a retrieval based system so without further Ado let's dive",
    "start": "39160",
    "end": "45079"
  },
  {
    "text": "in the first orchestration framework that we're going to be using is Lang chain the company that I work at UH",
    "start": "45079",
    "end": "51840"
  },
  {
    "text": "these are the Lang chain docs we allow you to connect to any llm any Vector store uh different kinds of tools and",
    "start": "51840",
    "end": "59320"
  },
  {
    "text": "we're kind of a One-Stop shop for all those different Integrations and then we provide some standard abstractions uh",
    "start": "59320",
    "end": "65280"
  },
  {
    "text": "for those Integrations as well as some other approaches uh we have lots of cookbooks that allow you to implement academic papers or just kind of basic",
    "start": "65280",
    "end": "72080"
  },
  {
    "text": "Solutions which we're going to be going through today um the next up is mongodb",
    "start": "72080",
    "end": "77119"
  },
  {
    "text": "specifically mongodb is a nosql database provider that also provides Atlas which",
    "start": "77119",
    "end": "83400"
  },
  {
    "text": "is a cloud database available in a few different uh environments it has native",
    "start": "83400",
    "end": "89040"
  },
  {
    "text": "Vector search uh which is the reason that we're going to be using it today cement de caching involves uh kind of an",
    "start": "89040",
    "end": "94439"
  },
  {
    "text": "embedding lookup very akin to a vector store retrieval step so we're going to be leveraging the atlas Vector search",
    "start": "94439",
    "end": "101200"
  },
  {
    "text": "instance there uh and we're also going to be using uh some other providers as well so we're going to be using",
    "start": "101200",
    "end": "107079"
  },
  {
    "text": "anthropics uh Cloud 3 llm in order to uh actually generate some responses from",
    "start": "107079",
    "end": "113439"
  },
  {
    "text": "our application we're going to be using open AIS embeddings uh for this",
    "start": "113439",
    "end": "118600"
  },
  {
    "text": "particular application and when we get to the rag step we're going to be using X's search retriever in order to",
    "start": "118600",
    "end": "124840"
  },
  {
    "text": "retrieve some documents to actually base our responses on so let's dive in the",
    "start": "124840",
    "end": "131959"
  },
  {
    "text": "first chain that we're going to implement is the basic prompt and llm chain from our cookbook it's the first entry and rag conveniently is the second",
    "start": "131959",
    "end": "138840"
  },
  {
    "text": "one as well the idea uh behind this is it's it's really the simplest thing that you can do for getting started and so",
    "start": "138840",
    "end": "145720"
  },
  {
    "text": "we're going to make something kind of similar to that so let's go into our editor here um so the first thing you're",
    "start": "145720",
    "end": "153519"
  },
  {
    "text": "going to want to do is you're going to want to install some packages so today we're going to be using um four main",
    "start": "153519",
    "end": "160319"
  },
  {
    "text": "providers mongod DB anthropic open Ai and EXA so we're going to want to install those for partner packages so we",
    "start": "160319",
    "end": "167760"
  },
  {
    "text": "can see those up here where you're going to pip install Lang chain mongodb Lang chain anthropic Lang chain open Ai and",
    "start": "167760",
    "end": "173319"
  },
  {
    "text": "Lang chain XA uh the next step is we're going to want to set some environment variables in order to get those set up I",
    "start": "173319",
    "end": "179920"
  },
  {
    "text": "already have those set up in my environment so we're not going to have to worry about those um we're going to",
    "start": "179920",
    "end": "185400"
  },
  {
    "text": "be using open AI for embeddings in this particular example uh and then last but not least uh Lang Smith is our kind of",
    "start": "185400",
    "end": "192640"
  },
  {
    "text": "first party observability and debugging tool at Lang chain um and if you want to use that in order to debug traces um we",
    "start": "192640",
    "end": "200200"
  },
  {
    "text": "can actually just quickly open that to see what that looks like um and just briefly showing what lsmith",
    "start": "200200",
    "end": "207519"
  },
  {
    "text": "looks like we'll look at this a little bit more later with some of the further examples it allows us to trace all of",
    "start": "207519",
    "end": "212920"
  },
  {
    "text": "our different um applications in order to see kind of how the prompt is constructed and which documents are",
    "start": "212920",
    "end": "219280"
  },
  {
    "text": "actually retrieved as part of different retrieval steps uh as well as uh other things such as kind of llm pricing uh",
    "start": "219280",
    "end": "226720"
  },
  {
    "text": "token counting those kinds of things so we'll be using that um down here and you",
    "start": "226720",
    "end": "232599"
  },
  {
    "text": "just need to set these two environment variables to activate that throughout kind of your entire Lane chain application which is quite nice so once",
    "start": "232599",
    "end": "240079"
  },
  {
    "text": "we have all of the that set up we can dive in so first thing we're going to do is we're going to want to set up some",
    "start": "240079",
    "end": "246360"
  },
  {
    "text": "sort of prompt into model chain um so",
    "start": "246360",
    "end": "252400"
  },
  {
    "text": "the kind of way that we compose prompts and models as well as everything in L",
    "start": "252400",
    "end": "258320"
  },
  {
    "text": "chain is kind of through this runnable interface where we can pipe the output of a prompt into a",
    "start": "258320",
    "end": "265759"
  },
  {
    "text": "model now let's configure what that prompt and model actually are so first",
    "start": "267160",
    "end": "272360"
  },
  {
    "text": "we're going to want to set up our model uh and we're going to use a chat anthropic model using their new Cloud 3",
    "start": "272360",
    "end": "278960"
  },
  {
    "text": "uh Sonet model so we'll say from blank chain anthropic import chat",
    "start": "278960",
    "end": "286000"
  },
  {
    "text": "anthropic um and actually let's use Opus instead just so we can show off some of the speed gains as well from caching so",
    "start": "286000",
    "end": "295120"
  },
  {
    "text": "uh Opus is anthropics largest and slowest Cloud 3 model um if you want",
    "start": "295120",
    "end": "300639"
  },
  {
    "text": "faster ones you can obviously use Sonet and Haiku as well so we'll use cloud 3",
    "start": "300639",
    "end": "306280"
  },
  {
    "text": "uh Opus 24229 they have their nice leape date in",
    "start": "306280",
    "end": "313160"
  },
  {
    "text": "their model uh and then we'll also want to set up our prompt so in order to make a prompt we'll do from Lang chain",
    "start": "313160",
    "end": "320280"
  },
  {
    "text": "core. prompt import prompt template and we'll set our",
    "start": "320280",
    "end": "327039"
  },
  {
    "text": "prompt to be uh promp template. from",
    "start": "327039",
    "end": "333000"
  },
  {
    "text": "template and we'll give it a nice little string here and we'll do actually we'll",
    "start": "333000",
    "end": "338479"
  },
  {
    "text": "start with just a classic rag prompt where we'll say answer the following question about the given",
    "start": "338479",
    "end": "346280"
  },
  {
    "text": "context and we'll give some kind of XML surrounded context so",
    "start": "346280",
    "end": "354240"
  },
  {
    "text": "the squiggly bracket context field is what we're actually going to substitute in as part of of uh our prompt template",
    "start": "354240",
    "end": "362160"
  },
  {
    "text": "and then these XML tags are actually just there to indicate to the llm that that is what the context is and then",
    "start": "362160",
    "end": "368360"
  },
  {
    "text": "we'll give a question below there um so that should form our prompt and our",
    "start": "368360",
    "end": "375039"
  },
  {
    "text": "model and we'll see if that runs looks like we successfully did that and then we can now try invoking it um with some",
    "start": "375039",
    "end": "383840"
  },
  {
    "text": "sort of context and some sort of question",
    "start": "383840",
    "end": "390440"
  },
  {
    "text": "where we are essentially substituting in those two kind of blue variables from up",
    "start": "390440",
    "end": "396400"
  },
  {
    "text": "here um from whatever we set it to so we could for example give it something like",
    "start": "396400",
    "end": "402599"
  },
  {
    "text": "Eric works at Lang chain with a question of where does Eric work and we can see",
    "start": "402599",
    "end": "411560"
  },
  {
    "text": "what we get back from that and in 2.6 seconds again Opus is",
    "start": "411560",
    "end": "416960"
  },
  {
    "text": "kind of a larger model we can see that BAS on the given context Eric works at L chain so looks like our our basic chain",
    "start": "416960",
    "end": "423879"
  },
  {
    "text": "is working um if I were to run this again it should still take two or",
    "start": "423879",
    "end": "429120"
  },
  {
    "text": "seconds uh because it's going to actually send that API call to anthropic every single time that we do this uh as",
    "start": "429120",
    "end": "435960"
  },
  {
    "text": "well as rack up token usage as well as our our good anthropic Bill there um so",
    "start": "435960",
    "end": "441599"
  },
  {
    "text": "in order to kind of cut some of that let's go into semantic caching um so",
    "start": "441599",
    "end": "448039"
  },
  {
    "text": "caching in general um and I'll actually draw a bit of a diagram here",
    "start": "448039",
    "end": "455720"
  },
  {
    "text": "um semantic caching in general uh intends to kind of uh prevent us from",
    "start": "458039",
    "end": "465879"
  },
  {
    "text": "sending too many API calls to our model provider so in the chain that we just set up where we",
    "start": "465879",
    "end": "472199"
  },
  {
    "text": "have a prompt going into a",
    "start": "472199",
    "end": "478199"
  },
  {
    "text": "model and giving some kind of output what we're essentially going to do is we're",
    "start": "478440",
    "end": "483919"
  },
  {
    "text": "going to create a kind of bypass path around the model",
    "start": "483919",
    "end": "489479"
  },
  {
    "text": "step where if we've seen kind of the formatted",
    "start": "489479",
    "end": "496120"
  },
  {
    "text": "prompt before we're actually just going to grab that from our cache so in this",
    "start": "498280",
    "end": "503919"
  },
  {
    "text": "case uh we're going to be doing that with kind of a  DB",
    "start": "503919",
    "end": "511240"
  },
  {
    "text": "based cache um and we're going to just get",
    "start": "511240",
    "end": "516880"
  },
  {
    "text": "whatever response we had last time from that um and there's kind of two main ways that we can do that so the the",
    "start": "516880",
    "end": "523120"
  },
  {
    "text": "first is with a regular",
    "start": "523120",
    "end": "526760"
  },
  {
    "text": "cache where we're going to be looking for an exact",
    "start": "529720",
    "end": "535200"
  },
  {
    "text": "match of the formatted prompt",
    "start": "535399",
    "end": "541440"
  },
  {
    "text": "and this is good for instances where we know we're going to get very similar",
    "start": "546279",
    "end": "551480"
  },
  {
    "text": "prompts uh from whatever system we're using so in our particular example this",
    "start": "551480",
    "end": "556640"
  },
  {
    "text": "actually likely won't work particularly well because it's passing in uh a user's",
    "start": "556640",
    "end": "561959"
  },
  {
    "text": "question as part of it and then likely that context is coming from some sort of retrieval step which we're going to add",
    "start": "561959",
    "end": "567360"
  },
  {
    "text": "in a second um and so if I asked even something just slightly different such",
    "start": "567360",
    "end": "573800"
  },
  {
    "text": "as uh where did Eric work uh period instead of a question mark it's actually",
    "start": "573800",
    "end": "579399"
  },
  {
    "text": "going to not be able to look that up because it needs that exact match in order to look it up and so the approach",
    "start": "579399",
    "end": "584800"
  },
  {
    "text": "that we're going to be using today in order to make that work is called semantic",
    "start": "584800",
    "end": "591240"
  },
  {
    "text": "caching where instead of indexing by the kind of exact formatted prompt string",
    "start": "591240",
    "end": "597360"
  },
  {
    "text": "we're going to index by an embedding of it so we're going to",
    "start": "597360",
    "end": "602680"
  },
  {
    "text": "use embeddings",
    "start": "602680",
    "end": "609480"
  },
  {
    "text": "plus a vector store uh so we're going to use Vector",
    "start": "609480",
    "end": "614760"
  },
  {
    "text": "search uh to look up",
    "start": "614760",
    "end": "621560"
  },
  {
    "text": "matches and we're going to set some sort of",
    "start": "621560",
    "end": "626120"
  },
  {
    "text": "threshold because an important part of of these systems is that if there isn't something with sufficient similarity we",
    "start": "627120",
    "end": "633760"
  },
  {
    "text": "actually do still want to call the model um and then of course at the end of this we're also going to want to insert our",
    "start": "633760",
    "end": "640519"
  },
  {
    "text": "record back into our database uh such that we can actually access that the next time that we call it so today we're",
    "start": "640519",
    "end": "647320"
  },
  {
    "text": "going to be focused on the semantic cache implementation of um and we also have documentation on how to use",
    "start": "647320",
    "end": "653000"
  },
  {
    "text": "the regular cache if that works better for your use case so let's get started on adding semantic caching to our kind",
    "start": "653000",
    "end": "660959"
  },
  {
    "text": "of basic prompt and model chain um so to do that we're going",
    "start": "660959",
    "end": "669360"
  },
  {
    "text": "to first create our semantic cache instance so we're going to import that",
    "start": "669360",
    "end": "675680"
  },
  {
    "text": "from the Lang chain mongodb package and it's called the mongod DB Atlas semantic",
    "start": "675680",
    "end": "683839"
  },
  {
    "text": "cache um is what we're going to import uh this relies on mongod DBS hosted",
    "start": "683839",
    "end": "689320"
  },
  {
    "text": "Atlas solution in order to enable Vector search so note that if you're using open source uh you might run into some",
    "start": "689320",
    "end": "695959"
  },
  {
    "text": "problems using this um and then we're also going to want to configure which embeddings we want to use so in our case",
    "start": "695959",
    "end": "701959"
  },
  {
    "text": "we're going to import that from Lang chain open AI import open AI",
    "start": "701959",
    "end": "707519"
  },
  {
    "text": "embeddings um you can use lots of local ones from hugging face nomic has some",
    "start": "707519",
    "end": "713120"
  },
  {
    "text": "pretty good embeddings um Voyage AI has a new embedding integration with us that",
    "start": "713120",
    "end": "718839"
  },
  {
    "text": "really excited about so you can kind of pick your embedding provider we're just use open AI for convenience today um and",
    "start": "718839",
    "end": "725000"
  },
  {
    "text": "then we're going to also import our good OS package so we can access some environment",
    "start": "725000",
    "end": "730600"
  },
  {
    "text": "variables um I'm actually going to paste in some of the configuration that we're going to use for our uh Vector search",
    "start": "730600",
    "end": "738800"
  },
  {
    "text": "today uh the first is we're going to load in kind of my secret atas URI uh as",
    "start": "738800",
    "end": "744639"
  },
  {
    "text": "our connection string this is Mongo's way of configuring kind of with Atlas",
    "start": "744639",
    "end": "749880"
  },
  {
    "text": "instance to connect to and it also encodes the password used um to connect to it so I have that set as an",
    "start": "749880",
    "end": "755399"
  },
  {
    "text": "environment variable already um and then we're going to configure an index database and uh collection to connect to",
    "start": "755399",
    "end": "761800"
  },
  {
    "text": "as well so we can try running that to make sure we can actually load that and then we're going to want to",
    "start": "761800",
    "end": "768279"
  },
  {
    "text": "create our semantic cache as our semantic cache instance um we'll create our",
    "start": "768279",
    "end": "776639"
  },
  {
    "text": "embedding as opening embeddings and we'll use their uh text embedding three",
    "start": "776639",
    "end": "783880"
  },
  {
    "text": "small model in order to do that we don't want to add any more dashes to that and",
    "start": "783880",
    "end": "790120"
  },
  {
    "text": "then we'll have our connection string as con string we'll have our collection",
    "start": "790120",
    "end": "796160"
  },
  {
    "text": "name as collection our database name and our",
    "start": "796160",
    "end": "801839"
  },
  {
    "text": "index name in order to kind of wrap that up and then last but definitely not least we're also going to want to",
    "start": "801839",
    "end": "807480"
  },
  {
    "text": "configure a threshold in order to make sure we're not returning things that are potentially the most relevant uh entry",
    "start": "807480",
    "end": "815720"
  },
  {
    "text": "even though it's not relevant at all um so that's something that we do in classic retrieval where we actually",
    "start": "815720",
    "end": "822519"
  },
  {
    "text": "always want to get kind of the three most relevant documents from our Vector store um but with semantic caching even",
    "start": "822519",
    "end": "829800"
  },
  {
    "text": "if there are three entries that are the most relevant to our query um it's",
    "start": "829800",
    "end": "834839"
  },
  {
    "text": "possible that those three entries are actually not relevant at all to us so uh this particular index uses a cosine",
    "start": "834839",
    "end": "841320"
  },
  {
    "text": "distance um or a cosine relevant score and so we'll set um our threshold to",
    "start": "841320",
    "end": "848040"
  },
  {
    "text": "make sure it's greater than 0.99 in order to um get that set up so we can",
    "start": "848040",
    "end": "853880"
  },
  {
    "text": "try running this make sure I didn't have any typos um and then we should be good to",
    "start": "853880",
    "end": "859160"
  },
  {
    "text": "go so the first thing that we'll want to do is we'll actually just want to clear that cache um so the llm cache",
    "start": "859160",
    "end": "865639"
  },
  {
    "text": "abstraction in Lang chain allows you to clear uh kind of all the entries in it and so we'll do that just to make sure",
    "start": "865639",
    "end": "871560"
  },
  {
    "text": "there's there's nothing in there to start um and then we'll want to uh set",
    "start": "871560",
    "end": "877480"
  },
  {
    "text": "our llm cache so in Lang chain there's a kind of convenient way of setting that globally so you don't actually have to",
    "start": "877480",
    "end": "883560"
  },
  {
    "text": "configure this on each and every llm call that you have so we can have",
    "start": "883560",
    "end": "889600"
  },
  {
    "text": "from Lang chain core globals import set llm cache and",
    "start": "889600",
    "end": "897240"
  },
  {
    "text": "here we'll just set our LM cache to our semantic cache object um and so this",
    "start": "897240",
    "end": "902920"
  },
  {
    "text": "will allow us to cach our responses um in that as well as kind of",
    "start": "902920",
    "end": "909000"
  },
  {
    "text": "get our responses back out whenever we've seen um ones that are sufficiently similar through the semantic cache um",
    "start": "909000",
    "end": "916800"
  },
  {
    "text": "lookup so we can try our nice question",
    "start": "916800",
    "end": "921920"
  },
  {
    "text": "about where I work again uh as expected the first time we run it uh this takes some time where um",
    "start": "921920",
    "end": "929519"
  },
  {
    "text": "again based on the given context Eric works at L chain and then if we run it again we should already have an entry in",
    "start": "929519",
    "end": "937399"
  },
  {
    "text": "uh our mongodb Atlas semantic cache such that it should be pretty quick so let's try running it again and we get our",
    "start": "937399",
    "end": "943600"
  },
  {
    "text": "response much faster um so now let's try changing our query slightly um so let's",
    "start": "943600",
    "end": "952279"
  },
  {
    "text": "add some comments here to make that clear same query exact query and then almost same query so we",
    "start": "952279",
    "end": "961000"
  },
  {
    "text": "can try an example where um instead where does Eric work question mark we'll do where does Eric work period hopefully",
    "start": "961000",
    "end": "968560"
  },
  {
    "text": "that will also be able to retrieve it and then now let's try one that is",
    "start": "968560",
    "end": "975920"
  },
  {
    "text": "different um so here we're going to instead ask something",
    "start": "975920",
    "end": "982639"
  },
  {
    "text": "about um the mayor of San Francisco is London and breed and here",
    "start": "982639",
    "end": "991000"
  },
  {
    "text": "we basically want to make sure um",
    "start": "991000",
    "end": "996639"
  },
  {
    "text": "that we are not retrieving our Eric works at Lang chain response um because",
    "start": "998920",
    "end": "1005240"
  },
  {
    "text": "in that case that would that would really represent a failure of our chain so here hopefully it takes some time so",
    "start": "1005240",
    "end": "1011480"
  },
  {
    "text": "far so good um and it looks like it's successfully making that llm call and now if we ask um some along those lines",
    "start": "1011480",
    "end": "1019839"
  },
  {
    "text": "again it should be quick because it's already cached from this prior step so",
    "start": "1019839",
    "end": "1025360"
  },
  {
    "text": "looks like our cement to cach is working on our basic example um we went through",
    "start": "1025360",
    "end": "1030558"
  },
  {
    "text": "kind of what semantic caching is and why it's powerful um and now let's actually augment our or create a new chain that",
    "start": "1030559",
    "end": "1037880"
  },
  {
    "text": "uh implements rag instead of like explicitly plumbing through the exact context that we have up here so let's go",
    "start": "1037880",
    "end": "1045760"
  },
  {
    "text": "through that and we're going to use our uh EXA search retriever from this so",
    "start": "1045760",
    "end": "1051679"
  },
  {
    "text": "we'll import uh from linkchain EXA import EXA",
    "start": "1051679",
    "end": "1056960"
  },
  {
    "text": "search Retriever and uh we'll also import a a",
    "start": "1056960",
    "end": "1062320"
  },
  {
    "text": "configuration option called text contents options um we can actually pull",
    "start": "1062320",
    "end": "1068480"
  },
  {
    "text": "up the ex search retriever documentation and the reason we're importing that",
    "start": "1068799",
    "end": "1076440"
  },
  {
    "text": "is um because we want to configure kind of a max length uh for each of the documents that it returns so EXA is an",
    "start": "1076440",
    "end": "1083440"
  },
  {
    "text": "integration that searches uh the internet much like Google um and returns",
    "start": "1083440",
    "end": "1089200"
  },
  {
    "text": "documents from their index and so some of the documents on the internet are quite long so we're going to want to set",
    "start": "1089200",
    "end": "1094520"
  },
  {
    "text": "kind of a max length for that so let's create our retriever um I have my exit API key",
    "start": "1094520",
    "end": "1101640"
  },
  {
    "text": "already set and we're going to want to set our text content options to text",
    "start": "1101640",
    "end": "1109320"
  },
  {
    "text": "content options of Max characters as uh let's",
    "start": "1109320",
    "end": "1114600"
  },
  {
    "text": "start with 300 for now um depending on the use case and the types of questions we expect uh that might be a little bit",
    "start": "1114600",
    "end": "1121000"
  },
  {
    "text": "short but we can give it a go for now and then we'll kind of create our uh",
    "start": "1121000",
    "end": "1127880"
  },
  {
    "text": "chain from scratch again so we'll actually use the um the prompt we",
    "start": "1127880",
    "end": "1134919"
  },
  {
    "text": "already had because it plums in context and question um um but we will start",
    "start": "1134919",
    "end": "1140720"
  },
  {
    "text": "with a different step where we actually need to populate our question which we",
    "start": "1140720",
    "end": "1146559"
  },
  {
    "text": "can do as a runnable pass through and we'll have to import that from linkchain",
    "start": "1146559",
    "end": "1151919"
  },
  {
    "text": "core runnables um so that's just going to pass through",
    "start": "1151919",
    "end": "1158280"
  },
  {
    "text": "the exact um string that we invoke our chain with through the question and then",
    "start": "1158280",
    "end": "1164159"
  },
  {
    "text": "context we're actually just going to pass that string into our Retriever and then we can pass pass that into a kind",
    "start": "1164159",
    "end": "1171240"
  },
  {
    "text": "of format documents function which we'll Define up here um where that'll take in docks and",
    "start": "1171240",
    "end": "1180200"
  },
  {
    "text": "then we'll return um well first we can get the",
    "start": "1180200",
    "end": "1186120"
  },
  {
    "text": "contents of all those docs so we'll get doc. page content for Doc and docs and",
    "start": "1186120",
    "end": "1193960"
  },
  {
    "text": "then we will join those with the lines so we'll just have a double new",
    "start": "1193960",
    "end": "1200520"
  },
  {
    "text": "line and we'll join all of those kind of content strings uh so this is just kind of",
    "start": "1200520",
    "end": "1207200"
  },
  {
    "text": "explicitly formatting that into a string if you were to just pass in the documents directly you might get some of",
    "start": "1207200",
    "end": "1213840"
  },
  {
    "text": "the metadata that XO returns kind of plumbed into your llm as well um which in this case is probably not as",
    "start": "1213840",
    "end": "1220000"
  },
  {
    "text": "desirable because that could be ordered differently and especially since we're dealing with caching it's better to just have kind of the whole string in there",
    "start": "1220000",
    "end": "1227919"
  },
  {
    "text": "so let's have that as the first part of our chain and then we'll pass that into",
    "start": "1227919",
    "end": "1232960"
  },
  {
    "text": "our prompt that we had above and then we'll actually use the same model as well so as a reminder this uh chain and",
    "start": "1232960",
    "end": "1241400"
  },
  {
    "text": "we can actually draw this that might be a little bit more clear",
    "start": "1241400",
    "end": "1248320"
  },
  {
    "text": "um this chain is basically adding to the beginning of this one where before this",
    "start": "1249280",
    "end": "1257240"
  },
  {
    "text": "we want to populate both a",
    "start": "1257240",
    "end": "1262679"
  },
  {
    "text": "question as well as some",
    "start": "1262679",
    "end": "1267360"
  },
  {
    "text": "context where the input before that is going to be coming in as",
    "start": "1269000",
    "end": "1276240"
  },
  {
    "text": "just the question so the first thing we're going to do is we're just going to",
    "start": "1277080",
    "end": "1282960"
  },
  {
    "text": "Plum our question straight through that's all the runnable pass through",
    "start": "1282960",
    "end": "1291400"
  },
  {
    "text": "does and down here we're going to pass our question to our",
    "start": "1293200",
    "end": "1298240"
  },
  {
    "text": "XA search Retriever and we're going to pass the",
    "start": "1298240",
    "end": "1304960"
  },
  {
    "text": "output of that into format docs in order to get our context",
    "start": "1304960",
    "end": "1313000"
  },
  {
    "text": "so here again we're kind of passing our question in as input to exess search retriever and then in the middle here",
    "start": "1313000",
    "end": "1320880"
  },
  {
    "text": "this is going to be our kind of list of",
    "start": "1320880",
    "end": "1328720"
  },
  {
    "text": "documents is what's going to be kind of passed through in this middle step and then format docs is going to just turn",
    "start": "1329120",
    "end": "1334840"
  },
  {
    "text": "this into a string where it just has all the contents kind of put together uh our prompt is going to turn that into our",
    "start": "1334840",
    "end": "1341440"
  },
  {
    "text": "string of uh answer the following question based on the given context that will become our formatted",
    "start": "1341440",
    "end": "1347840"
  },
  {
    "text": "prompt um and then we're actually just going to use the exact same cache in order to uh get this working so in order",
    "start": "1347840",
    "end": "1355480"
  },
  {
    "text": "to debug this at first a little bit",
    "start": "1355480",
    "end": "1360520"
  },
  {
    "text": "let's um let's actually make the LM cache none",
    "start": "1360520",
    "end": "1368480"
  },
  {
    "text": "um just to kind of make sure our chain is working before we start relying on the cache so down here we",
    "start": "1368480",
    "end": "1375480"
  },
  {
    "text": "will um have this one thing I just realized I",
    "start": "1375480",
    "end": "1381279"
  },
  {
    "text": "forgot to configure is we never configured the number of documents to",
    "start": "1381279",
    "end": "1387120"
  },
  {
    "text": "return so we'll configure that here we'll have it return three documents for each of our queries um and let's see if I defined",
    "start": "1387120",
    "end": "1394919"
  },
  {
    "text": "everything right and now we should be able to invoke it with something like who is the mayor of San",
    "start": "1394919",
    "end": "1402919"
  },
  {
    "text": "Francisco and ideally that will return some sort of document",
    "start": "1402919",
    "end": "1409360"
  },
  {
    "text": "that will be used to answer that question so it looks like we're successfully getting that London breed",
    "start": "1409360",
    "end": "1414760"
  },
  {
    "text": "is the current and 45th mayor of San Francisco um and if we look at our Langs Smith",
    "start": "1414760",
    "end": "1420679"
  },
  {
    "text": "Trace we should be able to see uh some of the documents that actually get returned from that",
    "start": "1420679",
    "end": "1426799"
  },
  {
    "text": "retriever um where hopefully so we can see the first one",
    "start": "1426799",
    "end": "1431880"
  },
  {
    "text": "seems to be getting a little bit of a tabular bio uh this looks like it might",
    "start": "1431880",
    "end": "1437559"
  },
  {
    "text": "be coming from Wikipedia which looks like correct from down here um here's a document that is coming from",
    "start": "1437559",
    "end": "1445600"
  },
  {
    "text": "sf.gov and here's one that's also coming from sf.gov and they kind of unanimously",
    "start": "1445600",
    "end": "1451000"
  },
  {
    "text": "declare that London breed is the air of San Francisco which is good and correct",
    "start": "1451000",
    "end": "1456559"
  },
  {
    "text": "for our cases um so now let's add back our semantic",
    "start": "1456559",
    "end": "1463320"
  },
  {
    "text": "cache um of our semantic cache and let's actually let's to clear it just to be",
    "start": "1463440",
    "end": "1469799"
  },
  {
    "text": "safe uh because we don't want it to Cache some of the prior responses that we had um and now let's try running",
    "start": "1469799",
    "end": "1477200"
  },
  {
    "text": "this again where we can ask who is the mayor of San Francisco um it's going to run the",
    "start": "1477200",
    "end": "1483440"
  },
  {
    "text": "retrieval step which is not cash one of the key um things to remember is that",
    "start": "1483440",
    "end": "1489360"
  },
  {
    "text": "the um the retrieval step is not cached as part of the LM cache it is only the",
    "start": "1489360",
    "end": "1494600"
  },
  {
    "text": "input and output to the model um and let me make sure I'm not going to completely run out of battery um so we get that our",
    "start": "1494600",
    "end": "1505440"
  },
  {
    "text": "mayor is lon breed and then if we run that again uh so this will not experience as much of a speed of a",
    "start": "1505440",
    "end": "1512440"
  },
  {
    "text": "performance speed up as our prior example because we still need to run that retrieval step because that one is",
    "start": "1512440",
    "end": "1517919"
  },
  {
    "text": "not cached so if we run that again um it actually looks like the retrieval step is pretty quick but then we're still",
    "start": "1517919",
    "end": "1523799"
  },
  {
    "text": "spending uh 08 seconds on that retrieval step and then the",
    "start": "1523799",
    "end": "1528840"
  },
  {
    "text": "uh output of anthropic is uh going to be exactly the same just because our",
    "start": "1528840",
    "end": "1535039"
  },
  {
    "text": "retriever is getting kind of those exact three documents again because it's the exact same query um but then our Chad",
    "start": "1535039",
    "end": "1541559"
  },
  {
    "text": "anthropic step is going to take 32 seconds instead of 6 seconds which it",
    "start": "1541559",
    "end": "1548320"
  },
  {
    "text": "did in the prior step so that is kind of llm caching in a nutshell um now let's",
    "start": "1548320",
    "end": "1555320"
  },
  {
    "text": "talk about some of the challenges with a rag B application and using that in",
    "start": "1555320",
    "end": "1562039"
  },
  {
    "text": "um in an application like this so we can actually change our query here so in our",
    "start": "1562039",
    "end": "1567960"
  },
  {
    "text": "prior example we were able to change our query from where does Eric work to kind",
    "start": "1567960",
    "end": "1574279"
  },
  {
    "text": "of where does Eric work with a period um we can try that one again this one might actually work um so let's see if that",
    "start": "1574279",
    "end": "1581480"
  },
  {
    "text": "does anything yeah so that one still is retrieving the same set of documents but you could also imagine us using some",
    "start": "1581480",
    "end": "1587960"
  },
  {
    "text": "sort of query like um",
    "start": "1587960",
    "end": "1593080"
  },
  {
    "text": "who leads well I guess that's a little bit ambiguous but the mayor of San",
    "start": "1593080",
    "end": "1601840"
  },
  {
    "text": "Francisco is whom um which semantically is very",
    "start": "1601840",
    "end": "1607240"
  },
  {
    "text": "similar and so if we were to Just Produce an embedding of this string alone it would likely get a lookup but",
    "start": "1607240",
    "end": "1614320"
  },
  {
    "text": "in this case we're actually going to retrieve a different set of documents so if we look at our Langs Smith Trace",
    "start": "1614320",
    "end": "1619960"
  },
  {
    "text": "we're going to see that instead of the kind of Wikipedia article sfgv sfgv our",
    "start": "1619960",
    "end": "1625000"
  },
  {
    "text": "retriever is getting um a different section of the Wikipedia article so it's",
    "start": "1625000",
    "end": "1631919"
  },
  {
    "text": "kind of leading with from Wikipedia um it's the second entry is actually also from Wikipedia looks like",
    "start": "1631919",
    "end": "1639760"
  },
  {
    "text": "this is actually kind of similar to what the first document was in the prior example and then the third document has",
    "start": "1639760",
    "end": "1645760"
  },
  {
    "text": "actually become the second so even slight different ref es where now when we format our prompt template we're",
    "start": "1645760",
    "end": "1652480"
  },
  {
    "text": "going to get this as the input to our llm versus we can actually open the old",
    "start": "1652480",
    "end": "1660799"
  },
  {
    "text": "one uh which is going to be slightly different",
    "start": "1660799",
    "end": "1667919"
  },
  {
    "text": "where here our prompt is going to get formatted",
    "start": "1667919",
    "end": "1673279"
  },
  {
    "text": "as like these entries in that order and because",
    "start": "1673279",
    "end": "1679159"
  },
  {
    "text": "those two docu those two inputs are sufficiently different the semantic",
    "start": "1679159",
    "end": "1684519"
  },
  {
    "text": "cache is not going to be able to look those up so the really important thing to remember there and I'll kind of",
    "start": "1684519",
    "end": "1689760"
  },
  {
    "text": "emphasize this on my diagram is that the semantic",
    "start": "1689760",
    "end": "1696120"
  },
  {
    "text": "cache is only caching over here it's only caching the model inputs and",
    "start": "1696120",
    "end": "1705080"
  },
  {
    "text": "outputs and even though your question might be similar so you could also Imagine a",
    "start": "1705080",
    "end": "1711320"
  },
  {
    "text": "world in which we had a cache that's actually caching this whole",
    "start": "1711320",
    "end": "1717080"
  },
  {
    "text": "flow where if you had a similar enough question it would kind of look up the entire response of the entire chain this",
    "start": "1717080",
    "end": "1724519"
  },
  {
    "text": "is not what llm caching does um this is not llm",
    "start": "1724519",
    "end": "1732919"
  },
  {
    "text": "caching uh this is something you can Implement yourself as well um but the really important thing to remember is",
    "start": "1733159",
    "end": "1739279"
  },
  {
    "text": "because we're relying on the retrieval step to return the exact same set of",
    "start": "1739279",
    "end": "1744399"
  },
  {
    "text": "documents we actually have kind of another point of uh getting a cach Miss",
    "start": "1744399",
    "end": "1750519"
  },
  {
    "text": "uh than if we were to be indexing the entire flow all over um so that's kind",
    "start": "1750519",
    "end": "1755559"
  },
  {
    "text": "of one of the gotchas of using llm caching in a rag system like this um but",
    "start": "1755559",
    "end": "1761600"
  },
  {
    "text": "it is still kind of a cool approach and it works very well with mongodb so kind of inside",
    "start": "1761600",
    "end": "1769039"
  },
  {
    "text": "uh today we went through kind of using semantic caching uh in uh kind of two",
    "start": "1769039",
    "end": "1774679"
  },
  {
    "text": "simple applications the first just being a prompt in llm kind of mimicking some sort of chat GPT or kind of custom",
    "start": "1774679",
    "end": "1781960"
  },
  {
    "text": "prompt chatbot uh and then we went down into a rag implementation where the",
    "start": "1781960",
    "end": "1787720"
  },
  {
    "text": "retrieval step was able to produce uh some cash hits if we gave the same query",
    "start": "1787720",
    "end": "1793960"
  },
  {
    "text": "again but actually slight differences in the query uh even though they were semantically similar resulted in",
    "start": "1793960",
    "end": "1800360"
  },
  {
    "text": "different documents being retrieved and still produced a cash Miss um as one of",
    "start": "1800360",
    "end": "1805519"
  },
  {
    "text": "the gchas of semantic caching thanks for listening uh and have a great rest of",
    "start": "1805519",
    "end": "1812200"
  },
  {
    "text": "your day",
    "start": "1812200",
    "end": "1816919"
  }
]