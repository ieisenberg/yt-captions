[
  {
    "text": "hi this is Lance from Lang chain this is",
    "start": "1480",
    "end": "3919"
  },
  {
    "text": "the 13th part of our rag from scratch",
    "start": "3919",
    "end": "5839"
  },
  {
    "text": "series focused on a technique called",
    "start": "5839",
    "end": "8760"
  },
  {
    "text": "Raptor so Raptor sits within kind of an",
    "start": "8760",
    "end": "11960"
  },
  {
    "text": "array of different indexing techniques",
    "start": "11960",
    "end": "14839"
  },
  {
    "text": "that can be applied on Vector Stores um",
    "start": "14839",
    "end": "17640"
  },
  {
    "text": "we just talked about",
    "start": "17640",
    "end": "18359"
  },
  {
    "text": "multi-representation indexing um we I",
    "start": "18359",
    "end": "21359"
  },
  {
    "text": "prived a link to a video that's very",
    "start": "21359",
    "end": "23640"
  },
  {
    "text": "good talking about the different means",
    "start": "23640",
    "end": "25039"
  },
  {
    "text": "of chunking so I encourage you to look",
    "start": "25039",
    "end": "26560"
  },
  {
    "text": "at that and we're going to talk today",
    "start": "26560",
    "end": "28519"
  },
  {
    "text": "about a technique called Raptor which",
    "start": "28519",
    "end": "30599"
  },
  {
    "text": "you can kind of think about as a",
    "start": "30599",
    "end": "31759"
  },
  {
    "text": "technique for hierarchical",
    "start": "31759",
    "end": "34040"
  },
  {
    "text": "indexing so the high LEL intuition is",
    "start": "34040",
    "end": "37600"
  },
  {
    "text": "this some questions require very",
    "start": "37600",
    "end": "40800"
  },
  {
    "text": "detailed information from a corpus to",
    "start": "40800",
    "end": "43160"
  },
  {
    "text": "answer like pertain to a single document",
    "start": "43160",
    "end": "45719"
  },
  {
    "text": "or single chunk so like we can call",
    "start": "45719",
    "end": "48039"
  },
  {
    "text": "those lowlevel",
    "start": "48039",
    "end": "49360"
  },
  {
    "text": "questions some questions require",
    "start": "49360",
    "end": "51480"
  },
  {
    "text": "consolidation across kind of broad swast",
    "start": "51480",
    "end": "54120"
  },
  {
    "text": "of a document so across like many",
    "start": "54120",
    "end": "56480"
  },
  {
    "text": "documents or many chunks within a",
    "start": "56480",
    "end": "58600"
  },
  {
    "text": "document and all those like higher level",
    "start": "58600",
    "end": "61640"
  },
  {
    "text": "questions and so there's kind of this",
    "start": "61640",
    "end": "64158"
  },
  {
    "text": "challenge in retrieval and that",
    "start": "64159",
    "end": "66880"
  },
  {
    "text": "typically we do like K nearest neighbor",
    "start": "66880",
    "end": "68880"
  },
  {
    "text": "retrieval like we've been talking about",
    "start": "68880",
    "end": "70840"
  },
  {
    "text": "you're fishing out some number of chunks",
    "start": "70840",
    "end": "73680"
  },
  {
    "text": "but what if you have a question that",
    "start": "73680",
    "end": "75320"
  },
  {
    "text": "requires information across like five",
    "start": "75320",
    "end": "77439"
  },
  {
    "text": "six you know or a number of different",
    "start": "77439",
    "end": "79560"
  },
  {
    "text": "chunks which may exceed you know the K",
    "start": "79560",
    "end": "82680"
  },
  {
    "text": "parameter in your retrieval so again",
    "start": "82680",
    "end": "84479"
  },
  {
    "text": "when you typically do retrieval you",
    "start": "84479",
    "end": "86520"
  },
  {
    "text": "might set a k parameter of three which",
    "start": "86520",
    "end": "88600"
  },
  {
    "text": "means you're retrieving three chunk",
    "start": "88600",
    "end": "89880"
  },
  {
    "text": "junks from your vector store um and",
    "start": "89880",
    "end": "92280"
  },
  {
    "text": "maybe you have a high very high level",
    "start": "92280",
    "end": "93880"
  },
  {
    "text": "question that could benefit from",
    "start": "93880",
    "end": "95240"
  },
  {
    "text": "infation across more than three so this",
    "start": "95240",
    "end": "98280"
  },
  {
    "text": "technique called raptor is basically a",
    "start": "98280",
    "end": "100640"
  },
  {
    "text": "way to build a hierarchical index U of",
    "start": "100640",
    "end": "104000"
  },
  {
    "text": "document summaries and the intuition is",
    "start": "104000",
    "end": "106600"
  },
  {
    "text": "this you start with a set of documents",
    "start": "106600",
    "end": "108880"
  },
  {
    "text": "as your Leafs here on the left you",
    "start": "108880",
    "end": "111479"
  },
  {
    "text": "cluster them and then you Summarize each",
    "start": "111479",
    "end": "114759"
  },
  {
    "text": "cluster so each cluster of similar",
    "start": "114759",
    "end": "117680"
  },
  {
    "text": "documents um will consult information",
    "start": "117680",
    "end": "120280"
  },
  {
    "text": "from across your context which is you",
    "start": "120280",
    "end": "123079"
  },
  {
    "text": "know your context could be a bunch of",
    "start": "123079",
    "end": "124640"
  },
  {
    "text": "different splits or could even be across",
    "start": "124640",
    "end": "126759"
  },
  {
    "text": "a bunch of different documents you're",
    "start": "126759",
    "end": "128599"
  },
  {
    "text": "basically capturing similar ones and",
    "start": "128599",
    "end": "130319"
  },
  {
    "text": "you're consolidating the information",
    "start": "130319",
    "end": "131599"
  },
  {
    "text": "across them in a summary and here's the",
    "start": "131599",
    "end": "134200"
  },
  {
    "text": "interesting thing you do that",
    "start": "134200",
    "end": "135599"
  },
  {
    "text": "recursively until either you hit like a",
    "start": "135599",
    "end": "137800"
  },
  {
    "text": "limit or you end up with one single",
    "start": "137800",
    "end": "139840"
  },
  {
    "text": "cluster that's like kind of very high",
    "start": "139840",
    "end": "141160"
  },
  {
    "text": "level summary of all of your",
    "start": "141160",
    "end": "143400"
  },
  {
    "text": "documents and what the paper shows is",
    "start": "143400",
    "end": "145760"
  },
  {
    "text": "that if you basically just collapse all",
    "start": "145760",
    "end": "148560"
  },
  {
    "text": "these and index them together as a big",
    "start": "148560",
    "end": "150319"
  },
  {
    "text": "pool you end up with a really nice array",
    "start": "150319",
    "end": "152599"
  },
  {
    "text": "of chunks that span the abstraction",
    "start": "152599",
    "end": "155239"
  },
  {
    "text": "hierarchy like you have a bunch of",
    "start": "155239",
    "end": "157040"
  },
  {
    "text": "chunks from Individual documents that",
    "start": "157040",
    "end": "159959"
  },
  {
    "text": "are just like more detailed chunks",
    "start": "159959",
    "end": "161720"
  },
  {
    "text": "pertaining to that you know single",
    "start": "161720",
    "end": "163319"
  },
  {
    "text": "document but you also have chunks from",
    "start": "163319",
    "end": "165440"
  },
  {
    "text": "the summaries or I would say like you",
    "start": "165440",
    "end": "167920"
  },
  {
    "text": "know maybe not chunks but in this case",
    "start": "167920",
    "end": "169480"
  },
  {
    "text": "the summary is like a distillation so",
    "start": "169480",
    "end": "171560"
  },
  {
    "text": "you know raw chunks on the left that",
    "start": "171560",
    "end": "173599"
  },
  {
    "text": "represent your leaves are kind of like",
    "start": "173599",
    "end": "175120"
  },
  {
    "text": "the rawest form of information either",
    "start": "175120",
    "end": "177680"
  },
  {
    "text": "raw chunks or raw documents and then you",
    "start": "177680",
    "end": "180080"
  },
  {
    "text": "have these higher level summaries which",
    "start": "180080",
    "end": "182360"
  },
  {
    "text": "are all indexed together so if you have",
    "start": "182360",
    "end": "184560"
  },
  {
    "text": "higher level questions they should",
    "start": "184560",
    "end": "186440"
  },
  {
    "text": "basically be more similar uh in semantic",
    "start": "186440",
    "end": "189239"
  },
  {
    "text": "search for example to these higher level",
    "start": "189239",
    "end": "191000"
  },
  {
    "text": "summary chunks if you have lower level",
    "start": "191000",
    "end": "193239"
  },
  {
    "text": "questions then they'll retrieve these",
    "start": "193239",
    "end": "195120"
  },
  {
    "text": "more lower level chunks and so you have",
    "start": "195120",
    "end": "196680"
  },
  {
    "text": "better semantic coverage across like the",
    "start": "196680",
    "end": "199400"
  },
  {
    "text": "abstraction hierarchy of question types",
    "start": "199400",
    "end": "201480"
  },
  {
    "text": "that's the intuition they do a bunch of",
    "start": "201480",
    "end": "202959"
  },
  {
    "text": "nice studies to show that this works",
    "start": "202959",
    "end": "204560"
  },
  {
    "text": "pretty well um I actually did a deep",
    "start": "204560",
    "end": "207480"
  },
  {
    "text": "dive video just on this which I link",
    "start": "207480",
    "end": "209599"
  },
  {
    "text": "below low um I did want to cover it",
    "start": "209599",
    "end": "212239"
  },
  {
    "text": "briefly just at a very high level um so",
    "start": "212239",
    "end": "215000"
  },
  {
    "text": "let's actually just do kind of a code",
    "start": "215000",
    "end": "216319"
  },
  {
    "text": "walk through and I've added it to this",
    "start": "216319",
    "end": "218840"
  },
  {
    "text": "rack from scratch course notebook but I",
    "start": "218840",
    "end": "221080"
  },
  {
    "text": "link over to my deep dive video as well",
    "start": "221080",
    "end": "223239"
  },
  {
    "text": "as the paper and the the full code",
    "start": "223239",
    "end": "226159"
  },
  {
    "text": "notebook which is already checked in and",
    "start": "226159",
    "end": "228120"
  },
  {
    "text": "is discussed at more length in the Deep",
    "start": "228120",
    "end": "230799"
  },
  {
    "text": "dive the technique is a little bit",
    "start": "230799",
    "end": "232879"
  },
  {
    "text": "detailed so I only want to give you very",
    "start": "232879",
    "end": "235040"
  },
  {
    "text": "high levels kind of overview here and",
    "start": "235040",
    "end": "237439"
  },
  {
    "text": "you can look at the Deep dive video if",
    "start": "237439",
    "end": "239000"
  },
  {
    "text": "you want to go more depth again we",
    "start": "239000",
    "end": "241200"
  },
  {
    "text": "talked through this abstraction",
    "start": "241200",
    "end": "243159"
  },
  {
    "text": "hierarchy um I appli this to a large set",
    "start": "243159",
    "end": "247239"
  },
  {
    "text": "of Lang chain documents um so this is me",
    "start": "247239",
    "end": "250159"
  },
  {
    "text": "loading basically all of our Lang chain",
    "start": "250159",
    "end": "252040"
  },
  {
    "text": "expression language docs so this is on",
    "start": "252040",
    "end": "254079"
  },
  {
    "text": "the order of 30 documents you can see I",
    "start": "254079",
    "end": "256040"
  },
  {
    "text": "do a histogram here of the token counts",
    "start": "256040",
    "end": "257799"
  },
  {
    "text": "per document some are pretty big most",
    "start": "257799",
    "end": "260400"
  },
  {
    "text": "are fairly small less than you know",
    "start": "260400",
    "end": "262360"
  },
  {
    "text": "4,000 tokens um and what I did is I",
    "start": "262360",
    "end": "265840"
  },
  {
    "text": "indexed all of them um individually so",
    "start": "265840",
    "end": "269720"
  },
  {
    "text": "so all those raw documents you can kind",
    "start": "269720",
    "end": "271400"
  },
  {
    "text": "of Imagine are here on the left and then",
    "start": "271400",
    "end": "274400"
  },
  {
    "text": "I do um I do embedding I do clustering",
    "start": "274400",
    "end": "278960"
  },
  {
    "text": "summarization and I do that recursively",
    "start": "278960",
    "end": "281720"
  },
  {
    "text": "um until I end up with in this case I",
    "start": "281720",
    "end": "284560"
  },
  {
    "text": "believe I only set like three levels of",
    "start": "284560",
    "end": "287280"
  },
  {
    "text": "recursion and then I save them all my",
    "start": "287280",
    "end": "289400"
  },
  {
    "text": "Vector store so that's like the high",
    "start": "289400",
    "end": "291240"
  },
  {
    "text": "level idea I'm applying this Raptor",
    "start": "291240",
    "end": "293280"
  },
  {
    "text": "technique to a whole bunch of Lang chain",
    "start": "293280",
    "end": "295840"
  },
  {
    "text": "documents um that have fairly large",
    "start": "295840",
    "end": "298199"
  },
  {
    "text": "number of tokens um so I do that um and",
    "start": "298199",
    "end": "304960"
  },
  {
    "text": "yeah I use actually use both Claude as",
    "start": "304960",
    "end": "307360"
  },
  {
    "text": "well as open AI here um this talks",
    "start": "307360",
    "end": "310600"
  },
  {
    "text": "through the clustering method which they",
    "start": "310600",
    "end": "312160"
  },
  {
    "text": "that they use which is pretty",
    "start": "312160",
    "end": "313160"
  },
  {
    "text": "interesting you can kind of dig into",
    "start": "313160",
    "end": "314800"
  },
  {
    "text": "that on your own if if you're really um",
    "start": "314800",
    "end": "317080"
  },
  {
    "text": "interested this is a lot of their code",
    "start": "317080",
    "end": "319759"
  },
  {
    "text": "um which I cite accordingly um this is",
    "start": "319759",
    "end": "322120"
  },
  {
    "text": "basically implemented the clustering",
    "start": "322120",
    "end": "323400"
  },
  {
    "text": "method that they use um and this is just",
    "start": "323400",
    "end": "328080"
  },
  {
    "text": "simply the document embedding state page",
    "start": "328080",
    "end": "330919"
  },
  {
    "text": "um this is like basically embedding uh",
    "start": "330919",
    "end": "334199"
  },
  {
    "text": "and clustering that's really it some",
    "start": "334199",
    "end": "337560"
  },
  {
    "text": "text formatting um summarizing of the",
    "start": "337560",
    "end": "340639"
  },
  {
    "text": "clusters right here um and then this is",
    "start": "340639",
    "end": "344080"
  },
  {
    "text": "just running that whole process",
    "start": "344080",
    "end": "345199"
  },
  {
    "text": "recursively that's really it um this is",
    "start": "345199",
    "end": "349199"
  },
  {
    "text": "tree building so basically I have the",
    "start": "349199",
    "end": "351960"
  },
  {
    "text": "rod the rod docks let's just go back and",
    "start": "351960",
    "end": "353919"
  },
  {
    "text": "look at Doc texts so this should be all",
    "start": "353919",
    "end": "355720"
  },
  {
    "text": "my raw documents uh so that's right you",
    "start": "355720",
    "end": "358120"
  },
  {
    "text": "can see it here doc text is basically",
    "start": "358120",
    "end": "360160"
  },
  {
    "text": "just the text and all those Lang chain",
    "start": "360160",
    "end": "362160"
  },
  {
    "text": "documents that I",
    "start": "362160",
    "end": "363440"
  },
  {
    "text": "pulled",
    "start": "363440",
    "end": "365199"
  },
  {
    "text": "um and so I run this process on them",
    "start": "365199",
    "end": "369840"
  },
  {
    "text": "right here uh so this is that recursive",
    "start": "369840",
    "end": "372880"
  },
  {
    "text": "embedding cluster basically runs and",
    "start": "372880",
    "end": "374560"
  },
  {
    "text": "produces that tree here's the results um",
    "start": "374560",
    "end": "378720"
  },
  {
    "text": "this is me just going through the",
    "start": "378720",
    "end": "379919"
  },
  {
    "text": "results and basically adding the result",
    "start": "379919",
    "end": "382199"
  },
  {
    "text": "text to this list of uh texts um oh okay",
    "start": "382199",
    "end": "387280"
  },
  {
    "text": "so here's what I do this Leaf text is",
    "start": "387280",
    "end": "389360"
  },
  {
    "text": "all the raw documents and I'm appending",
    "start": "389360",
    "end": "392199"
  },
  {
    "text": "to that all the summaries that's all",
    "start": "392199",
    "end": "394120"
  },
  {
    "text": "it's going on and then I'm indexing them",
    "start": "394120",
    "end": "396080"
  },
  {
    "text": "all together that's the key Point rag",
    "start": "396080",
    "end": "399039"
  },
  {
    "text": "chain and there you have it that's",
    "start": "399039",
    "end": "401120"
  },
  {
    "text": "really all you do um so anyway I",
    "start": "401120",
    "end": "403599"
  },
  {
    "text": "encourage you to look at this in depth",
    "start": "403599",
    "end": "404800"
  },
  {
    "text": "it's a pretty interesting technique it",
    "start": "404800",
    "end": "406440"
  },
  {
    "text": "works well long with long contexts so",
    "start": "406440",
    "end": "409120"
  },
  {
    "text": "for example one of the arguments I made",
    "start": "409120",
    "end": "410880"
  },
  {
    "text": "is that it's kind of a nice approach to",
    "start": "410880",
    "end": "412919"
  },
  {
    "text": "consulted information across like a span",
    "start": "412919",
    "end": "415160"
  },
  {
    "text": "of large",
    "start": "415160",
    "end": "416360"
  },
  {
    "text": "documents like in this particular case",
    "start": "416360",
    "end": "418720"
  },
  {
    "text": "my individual ments were L expression",
    "start": "418720",
    "end": "421000"
  },
  {
    "text": "language docs uh each each being",
    "start": "421000",
    "end": "423280"
  },
  {
    "text": "somewhere in the order of you know in",
    "start": "423280",
    "end": "424879"
  },
  {
    "text": "this case like you know most of them are",
    "start": "424879",
    "end": "426960"
  },
  {
    "text": "less than 4,000 tokens some pretty big",
    "start": "426960",
    "end": "429120"
  },
  {
    "text": "but I index them all I cluster them",
    "start": "429120",
    "end": "431800"
  },
  {
    "text": "without any splits uh embed them cluster",
    "start": "431800",
    "end": "434440"
  },
  {
    "text": "them build this tree um and go from",
    "start": "434440",
    "end": "436960"
  },
  {
    "text": "there and it all works because we now",
    "start": "436960",
    "end": "438720"
  },
  {
    "text": "have llms that can go out to you know",
    "start": "438720",
    "end": "441199"
  },
  {
    "text": "100 or 200,000 up to a million tokens",
    "start": "441199",
    "end": "443440"
  },
  {
    "text": "and context so you can actually just do",
    "start": "443440",
    "end": "445400"
  },
  {
    "text": "this process for big swats of documents",
    "start": "445400",
    "end": "448000"
  },
  {
    "text": "in place without any without need",
    "start": "448000",
    "end": "449840"
  },
  {
    "text": "splitting uh it's a pretty nice approach",
    "start": "449840",
    "end": "452160"
  },
  {
    "text": "so I encourage you to think about it",
    "start": "452160",
    "end": "453400"
  },
  {
    "text": "look at it watch the deep that video If",
    "start": "453400",
    "end": "454919"
  },
  {
    "text": "you really want to go deeper on this um",
    "start": "454919",
    "end": "458759"
  },
  {
    "text": "thanks",
    "start": "458759",
    "end": "461759"
  }
]