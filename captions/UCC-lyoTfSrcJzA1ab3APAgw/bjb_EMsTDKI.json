[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "hi this is Lance from Lang chain this",
    "start": "2679",
    "end": "4640"
  },
  {
    "text": "the second video in our series rack from",
    "start": "4640",
    "end": "6960"
  },
  {
    "text": "scratch focused on",
    "start": "6960",
    "end": "9400"
  },
  {
    "text": "indexing so in the past video you saw",
    "start": "9400",
    "end": "12280"
  },
  {
    "text": "the main kind of overall components of",
    "start": "12280",
    "end": "14440"
  },
  {
    "text": "rag pipelines indexing retrieval and",
    "start": "14440",
    "end": "18080"
  },
  {
    "text": "generation and here we're going to kind",
    "start": "18080",
    "end": "19760"
  },
  {
    "text": "of Deep dive on indexing and give just a",
    "start": "19760",
    "end": "22320"
  },
  {
    "text": "quick overview of it so the first aspect",
    "start": "22320",
    "end": "25920"
  },
  {
    "start": "23000",
    "end": "41000"
  },
  {
    "text": "of indexing is we have some external",
    "start": "25920",
    "end": "28279"
  },
  {
    "text": "documents that we actually want to load",
    "start": "28279",
    "end": "30320"
  },
  {
    "text": "and put into what we're tring to call a",
    "start": "30320",
    "end": "32439"
  },
  {
    "text": "retriever and the goal of this retriever",
    "start": "32440",
    "end": "34480"
  },
  {
    "text": "is simply given an input question I want",
    "start": "34480",
    "end": "37440"
  },
  {
    "text": "to fish out documents that are related",
    "start": "37440",
    "end": "39320"
  },
  {
    "text": "to my question in some",
    "start": "39320",
    "end": "41440"
  },
  {
    "start": "41000",
    "end": "72000"
  },
  {
    "text": "way now the way to establish that",
    "start": "41440",
    "end": "43920"
  },
  {
    "text": "relationship or relevance or similarity",
    "start": "43920",
    "end": "46559"
  },
  {
    "text": "is typically done using some kind of",
    "start": "46559",
    "end": "48320"
  },
  {
    "text": "numerical representation of documents",
    "start": "48320",
    "end": "51280"
  },
  {
    "text": "and the reason is that it's very easy to",
    "start": "51280",
    "end": "53520"
  },
  {
    "text": "compare vectors for example of numbers",
    "start": "53520",
    "end": "56520"
  },
  {
    "text": "uh relative to you know just free form",
    "start": "56520",
    "end": "59160"
  },
  {
    "text": "text",
    "start": "59160",
    "end": "60640"
  },
  {
    "text": "and so a lot of approaches have been",
    "start": "60640",
    "end": "62640"
  },
  {
    "text": "developed over the years to take text",
    "start": "62640",
    "end": "65239"
  },
  {
    "text": "documents and compress them down into a",
    "start": "65239",
    "end": "67640"
  },
  {
    "text": "numerical",
    "start": "67640",
    "end": "68759"
  },
  {
    "text": "representation that then can be very",
    "start": "68759",
    "end": "70799"
  },
  {
    "text": "easily",
    "start": "70799",
    "end": "72080"
  },
  {
    "start": "72000",
    "end": "124000"
  },
  {
    "text": "searched now there's a few ways to do",
    "start": "72080",
    "end": "75439"
  },
  {
    "text": "that so Google and others came up with",
    "start": "75439",
    "end": "77680"
  },
  {
    "text": "many interesting statistical methods",
    "start": "77680",
    "end": "79920"
  },
  {
    "text": "where you take a document you look at",
    "start": "79920",
    "end": "82040"
  },
  {
    "text": "the frequency of words and you build",
    "start": "82040",
    "end": "84240"
  },
  {
    "text": "what they call sparse vectors such that",
    "start": "84240",
    "end": "86960"
  },
  {
    "text": "the vector locations are you know a",
    "start": "86960",
    "end": "89159"
  },
  {
    "text": "large vocab of possible words each value",
    "start": "89159",
    "end": "92439"
  },
  {
    "text": "represents the number of occurrences of",
    "start": "92439",
    "end": "94280"
  },
  {
    "text": "that particular word and it's spse",
    "start": "94280",
    "end": "96840"
  },
  {
    "text": "because there's of course many zeros",
    "start": "96840",
    "end": "98520"
  },
  {
    "text": "it's a very large vocabulary relative to",
    "start": "98520",
    "end": "100399"
  },
  {
    "text": "what's present in the document and",
    "start": "100399",
    "end": "102439"
  },
  {
    "text": "there's very good search methods over",
    "start": "102439",
    "end": "104000"
  },
  {
    "text": "this this type of numerical",
    "start": "104000",
    "end": "106320"
  },
  {
    "text": "representation now a bit more recently",
    "start": "106320",
    "end": "109079"
  },
  {
    "text": "uh embedding methods that are machine",
    "start": "109079",
    "end": "110719"
  },
  {
    "text": "learned so you take a document and you",
    "start": "110719",
    "end": "112719"
  },
  {
    "text": "build a compressed fixed slank",
    "start": "112719",
    "end": "114600"
  },
  {
    "text": "representation of that",
    "start": "114600",
    "end": "116600"
  },
  {
    "text": "document um have been developed with",
    "start": "116600",
    "end": "120079"
  },
  {
    "text": "correspondingly very strong search",
    "start": "120079",
    "end": "121920"
  },
  {
    "text": "methods over",
    "start": "121920",
    "end": "123759"
  },
  {
    "text": "embeddings um so the intuition here is",
    "start": "123759",
    "end": "128080"
  },
  {
    "text": "that we take documents and we typically",
    "start": "128080",
    "end": "130679"
  },
  {
    "text": "split them because embedding models",
    "start": "130679",
    "end": "133080"
  },
  {
    "text": "actually have limited context windows so",
    "start": "133080",
    "end": "135840"
  },
  {
    "text": "you know on the order of maybe 512",
    "start": "135840",
    "end": "137879"
  },
  {
    "text": "tokens up to 8,000 tokens or Beyond but",
    "start": "137879",
    "end": "140959"
  },
  {
    "text": "they're not infinitely large so",
    "start": "140959",
    "end": "143000"
  },
  {
    "text": "documents are split and each document is",
    "start": "143000",
    "end": "145680"
  },
  {
    "text": "compressed into a vector and that Vector",
    "start": "145680",
    "end": "149120"
  },
  {
    "text": "captures a Mantic meaning of the",
    "start": "149120",
    "end": "150879"
  },
  {
    "text": "document itself the vectors are indexed",
    "start": "150879",
    "end": "154560"
  },
  {
    "text": "questions can be embedded in the exactly",
    "start": "154560",
    "end": "156480"
  },
  {
    "text": "same way and then a numerical kind of",
    "start": "156480",
    "end": "159640"
  },
  {
    "text": "comparison in some form you know using",
    "start": "159640",
    "end": "162120"
  },
  {
    "text": "very different types of methods can be",
    "start": "162120",
    "end": "163599"
  },
  {
    "text": "performed on these vectors to fish out",
    "start": "163599",
    "end": "166159"
  },
  {
    "text": "relevant documents relative to my",
    "start": "166159",
    "end": "169840"
  },
  {
    "start": "169000",
    "end": "291000"
  },
  {
    "text": "question um and let's just do a quick",
    "start": "169840",
    "end": "172040"
  },
  {
    "text": "code walkr on some of these",
    "start": "172040",
    "end": "175519"
  },
  {
    "text": "points so I have my notebook here I've",
    "start": "175519",
    "end": "180239"
  },
  {
    "text": "installed",
    "start": "180239",
    "end": "181280"
  },
  {
    "text": "here um now I've set a few API keys for",
    "start": "181280",
    "end": "184879"
  },
  {
    "text": "lsmith which are very useful for tracing",
    "start": "184879",
    "end": "187799"
  },
  {
    "text": "which we'll see",
    "start": "187799",
    "end": "189400"
  },
  {
    "text": "shortly um previously I walked through",
    "start": "189400",
    "end": "191959"
  },
  {
    "text": "this this kind of quick start that just",
    "start": "191959",
    "end": "193440"
  },
  {
    "text": "showed overall how to lay out these rag",
    "start": "193440",
    "end": "196080"
  },
  {
    "text": "pipelines and here what I'll do is I'll",
    "start": "196080",
    "end": "198239"
  },
  {
    "text": "Deep dive a little bit more on indexing",
    "start": "198239",
    "end": "200200"
  },
  {
    "text": "and I'm going to take a question and a",
    "start": "200200",
    "end": "202280"
  },
  {
    "text": "document and first I'm just going to",
    "start": "202280",
    "end": "205400"
  },
  {
    "text": "compute the number of tokens in for",
    "start": "205400",
    "end": "207159"
  },
  {
    "text": "example the question and this is",
    "start": "207159",
    "end": "208680"
  },
  {
    "text": "interesting because",
    "start": "208680",
    "end": "210280"
  },
  {
    "text": "embedding models and LMS more generally",
    "start": "210280",
    "end": "212599"
  },
  {
    "text": "operate on tokens and so it's kind of",
    "start": "212599",
    "end": "214599"
  },
  {
    "text": "nice to understand how large the",
    "start": "214599",
    "end": "216920"
  },
  {
    "text": "documents are that I'm trying to feed in",
    "start": "216920",
    "end": "219000"
  },
  {
    "text": "in this case it's obviously a very small",
    "start": "219000",
    "end": "220959"
  },
  {
    "text": "in this case question now I'm going to",
    "start": "220959",
    "end": "223599"
  },
  {
    "text": "specify open eye embeddings I specify an",
    "start": "223599",
    "end": "226480"
  },
  {
    "text": "embedding model here and I just say",
    "start": "226480",
    "end": "228319"
  },
  {
    "text": "embed embed query I can pass my question",
    "start": "228319",
    "end": "231080"
  },
  {
    "text": "my document and what you can see here is",
    "start": "231080",
    "end": "234439"
  },
  {
    "text": "that runs and this is mapped to now a",
    "start": "234439",
    "end": "237879"
  },
  {
    "text": "vector of length 153 6 and that fixed",
    "start": "237879",
    "end": "241159"
  },
  {
    "text": "length Vector",
    "start": "241159",
    "end": "242360"
  },
  {
    "text": "representation will be computed for both",
    "start": "242360",
    "end": "245720"
  },
  {
    "text": "documents and really for any documents",
    "start": "245720",
    "end": "247640"
  },
  {
    "text": "so you're always kind of computing this",
    "start": "247640",
    "end": "249120"
  },
  {
    "text": "fixed length Vector that encodes the",
    "start": "249120",
    "end": "251159"
  },
  {
    "text": "semantics of the text that you've passed",
    "start": "251159",
    "end": "254239"
  },
  {
    "text": "now I can do things like cosine",
    "start": "254239",
    "end": "255640"
  },
  {
    "text": "similarity to compare",
    "start": "255640",
    "end": "257720"
  },
  {
    "text": "them and as we'll see here I can load",
    "start": "257720",
    "end": "262560"
  },
  {
    "text": "some documents this is just like we saw",
    "start": "262560",
    "end": "264840"
  },
  {
    "text": "previously I can split",
    "start": "264840",
    "end": "267199"
  },
  {
    "text": "them and I can index them here",
    "start": "267199",
    "end": "270560"
  },
  {
    "text": "just like we did before but we can see",
    "start": "270560",
    "end": "272320"
  },
  {
    "text": "under the hood really what we're doing",
    "start": "272320",
    "end": "273880"
  },
  {
    "text": "is we're taking each split we're",
    "start": "273880",
    "end": "276000"
  },
  {
    "text": "embedding it using open eye embeddings",
    "start": "276000",
    "end": "277800"
  },
  {
    "text": "into this kind of this Vector",
    "start": "277800",
    "end": "279199"
  },
  {
    "text": "representation and that's stored with a",
    "start": "279199",
    "end": "281800"
  },
  {
    "text": "link to the rod document itself in our",
    "start": "281800",
    "end": "284120"
  },
  {
    "text": "Vector store and next we'll see how to",
    "start": "284120",
    "end": "287120"
  },
  {
    "text": "actually do retrieval using this Vector",
    "start": "287120",
    "end": "290400"
  },
  {
    "text": "store",
    "start": "290400",
    "end": "293400"
  }
]