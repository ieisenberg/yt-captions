[
  {
    "text": "in a day or two if you have questions during the webinar um there's there's so there's a chat box",
    "start": "0",
    "end": "6359"
  },
  {
    "text": "which probably most of you guys are in right now and then if you look below that there's a little box with a question mark on it if you guys put your",
    "start": "6359",
    "end": "12240"
  },
  {
    "text": "questions in there and then upload the ones that that you like the most and want to hear answer the most that's",
    "start": "12240",
    "end": "17880"
  },
  {
    "text": "where we're going to be answering a lot of the questions from after the fact so in terms of the schedule we'll do like a",
    "start": "17880",
    "end": "23160"
  },
  {
    "text": "quick round of intros we'll do quick um five to ten minute presentations from unstructured and then chroma and the",
    "start": "23160",
    "end": "29460"
  },
  {
    "text": "money chain and then we'll jump right into questions because we know there's a lot of questions there's already a lot",
    "start": "29460",
    "end": "34620"
  },
  {
    "text": "in that in in that in that channel um so just put even more there upload the ones that you want to hear answered",
    "start": "34620",
    "end": "40559"
  },
  {
    "text": "and we'll we'll get to those in that order um yeah there's uh you know there's a lot",
    "start": "40559",
    "end": "46980"
  },
  {
    "text": "of things in Lane chain today we'll be talking about retrieval so please try to keep questions uh relevant to retrieval",
    "start": "46980",
    "end": "53399"
  },
  {
    "text": "I think that's all the logistical stuff it's pretty uh it's pretty um yeah it's pretty lightweight maybe we",
    "start": "53399",
    "end": "60300"
  },
  {
    "text": "can do some quick intros uh now before getting started Matt do you want to go yeah um hi I'm Matt Robinson from",
    "start": "60300",
    "end": "66540"
  },
  {
    "text": "unstructured I'm one of the developers on our open source Library uh we focus on data pre-processing up for llms we",
    "start": "66540",
    "end": "74220"
  },
  {
    "text": "also have Ronnie husata floating around in the audience uh somewhere who's uh one of our red developer Advocates um so",
    "start": "74220",
    "end": "80040"
  },
  {
    "text": "I'm really excited for those awesome and Anton yeah hi uh I'm Anton I'm one of the",
    "start": "80040",
    "end": "86640"
  },
  {
    "text": "co-founders of chroma we build retrieval systems for AI",
    "start": "86640",
    "end": "91680"
  },
  {
    "text": "Perfect all right and I'm Harrison one of the co-founders of linkchain so with that we can get into some presentations",
    "start": "91680",
    "end": "97979"
  },
  {
    "text": "Matt you want to start all right uh let's kick off here um so uh share my screen here real quick",
    "start": "97979",
    "end": "107780"
  },
  {
    "text": "all right you got my screen yes all right perfect",
    "start": "108720",
    "end": "114299"
  },
  {
    "text": "um so we'll start with just a quick introduction uh to what unstructured is and what we do so like I mentioned uh",
    "start": "114299",
    "end": "120540"
  },
  {
    "text": "we're focused on data pre-processing for llm applications um so I know a lot of you are probably",
    "start": "120540",
    "end": "125939"
  },
  {
    "text": "here for all of the advanced retrieval techniques but before you get to the advanced retrieval techniques you got to",
    "start": "125939",
    "end": "131039"
  },
  {
    "text": "get your documents uh from raw format and into your vector database and so uh",
    "start": "131039",
    "end": "136860"
  },
  {
    "text": "that's what we focus on uh we've got a open source repo and uh we pre-process all sorts of different document types",
    "start": "136860",
    "end": "143280"
  },
  {
    "text": "you can see them over here uh so we're about to about 25 different document types now and then we'll also make that",
    "start": "143280",
    "end": "150720"
  },
  {
    "text": "available uh through an API so if you don't want to go through the trouble of getting things up and running locally you can you can use that",
    "start": "150720",
    "end": "157819"
  },
  {
    "text": "uh we've got lots of Integrations uh with with Lang chain um so uh to check out the the",
    "start": "157819",
    "end": "164099"
  },
  {
    "text": "Integrations uh page over on the uh the line chain site here you can see a whole bunch of them uh if you use the the file",
    "start": "164099",
    "end": "170760"
  },
  {
    "text": "loader uh we'll even uh detect the file type for you and uh route it to the uh appropriate partitioner and then get it",
    "start": "170760",
    "end": "177660"
  },
  {
    "text": "ready to uh to put into your vector database um so uh some of you might be familiar",
    "start": "177660",
    "end": "183360"
  },
  {
    "text": "with us I will uh go through uh a uh an example workflow of how to go from raw",
    "start": "183360",
    "end": "189360"
  },
  {
    "text": "documents to a promo Vector database uh here and then we'll also point out a",
    "start": "189360",
    "end": "194400"
  },
  {
    "text": "couple of new features that we've that we've recently added uh that might be helpful for you",
    "start": "194400",
    "end": "200040"
  },
  {
    "text": "uh so uh what we're going to do here is uh just write a very uh simple uh summarize the views of the day",
    "start": "200040",
    "end": "206400"
  },
  {
    "text": "application um so yeah previously without all this tooling this would have been you know like a really you know kind of thing",
    "start": "206400",
    "end": "212340"
  },
  {
    "text": "involved project uh with unstructured um you can use our partition uh HTML uh",
    "start": "212340",
    "end": "220200"
  },
  {
    "text": "brick from our quarter Library uh and then what we're going to do is just go to the CNN Lite website and then uh just",
    "start": "220200",
    "end": "227940"
  },
  {
    "text": "grab a bunch of links um so you can see uh this logic over here uh when we pre-process data we grab",
    "start": "227940",
    "end": "235860"
  },
  {
    "text": "a bunch of metadata that then gets added to the vector database so you can",
    "start": "235860",
    "end": "241319"
  },
  {
    "text": "condition searches and that sort of thing one of the pieces of metadata that",
    "start": "241319",
    "end": "246360"
  },
  {
    "text": "we recently added for websites in particular is cropping out links and so",
    "start": "246360",
    "end": "252720"
  },
  {
    "text": "um it's really easy to go go to that website uh find all of the links and now we can have a list of URLs that we can",
    "start": "252720",
    "end": "259320"
  },
  {
    "text": "crawl for our a little new summarization application here we're also grabbing other sorts of",
    "start": "259320",
    "end": "266520"
  },
  {
    "text": "metadata like like page numbers and we're starting to think about things like like section numbers and that sort of thing to make things uh easy to uh to",
    "start": "266520",
    "end": "274139"
  },
  {
    "text": "search in inductor databases uh so once we have uh all of that we can",
    "start": "274139",
    "end": "280080"
  },
  {
    "text": "go and grab one of our loaders from the langstain library uh so in this case",
    "start": "280080",
    "end": "286020"
  },
  {
    "text": "we'll grab the unstructured URL loader uh we can just pass it all of those URLs I'll let it rip and now it's just going",
    "start": "286020",
    "end": "292919"
  },
  {
    "text": "through uh finding all of the text in the news articles and then once that's done I will have a bunch of link chain",
    "start": "292919",
    "end": "301380"
  },
  {
    "text": "document objects uh once it's in that form uh really easy to upload it into a",
    "start": "301380",
    "end": "308880"
  },
  {
    "text": "vector database like chroma so uh we'll just use the uh the vector store from",
    "start": "308880",
    "end": "315360"
  },
  {
    "text": "the line chain Library uh we'll I will upload it and then we can run a uh we",
    "start": "315360",
    "end": "321840"
  },
  {
    "text": "can run a similarity search to find news topics of interest and then uh run them",
    "start": "321840",
    "end": "327060"
  },
  {
    "text": "through the uh the langching summarizer summarization chain to get a summary and",
    "start": "327060",
    "end": "333000"
  },
  {
    "text": "so uh with this with this workflow we're able to create a nice little news of the",
    "start": "333000",
    "end": "338400"
  },
  {
    "text": "day summarization app and um I kind of I think it was less than two dozen uh lines of code um so a really a nice a",
    "start": "338400",
    "end": "345180"
  },
  {
    "text": "nice simple workflow there um let's see I'll pause and see if",
    "start": "345180",
    "end": "350639"
  },
  {
    "text": "there's any questions before we move along I have I have one at a high level now yeah yeah go for it and this um this",
    "start": "350639",
    "end": "358800"
  },
  {
    "text": "may be a bit like this may sound a bit basic but I'm sure there's a lot of nuance here like what what does like the difference like partition HTML and",
    "start": "358800",
    "end": "365580"
  },
  {
    "text": "partition markdown like actually do like what are you pulling out like different types of elements like what like what is",
    "start": "365580",
    "end": "372120"
  },
  {
    "text": "the difference because all the like HTML markdown can both just be represented as like a string so what exactly are you",
    "start": "372120",
    "end": "377940"
  },
  {
    "text": "guys kind of like adding in and doing yeah I will have a logic that goes uh through the document and depending on",
    "start": "377940",
    "end": "383880"
  },
  {
    "text": "the document type we'll apply different logic to figure out uh different element types uh so and we'll go through and be",
    "start": "383880",
    "end": "390539"
  },
  {
    "text": "like hey you know like this looks like a title uh this looks like a list uh this looks like uh body text uh depending on",
    "start": "390539",
    "end": "397620"
  },
  {
    "text": "the document type we'll do that in different ways if you're talking about a PDF I will use uh document image",
    "start": "397620",
    "end": "404819"
  },
  {
    "text": "analysis models to go through and look for visual Clues like hey like this is bold and underlined probably a title we",
    "start": "404819",
    "end": "410759"
  },
  {
    "text": "see like the bullet ports here like probably uh probably a list um if we're using something like HTML",
    "start": "410759",
    "end": "416520"
  },
  {
    "text": "we're using like tags that we're looking at plain text we might actually even look at the the",
    "start": "416520",
    "end": "422460"
  },
  {
    "text": "structure of the text itself um to get to get some clues about that and what that allows us to do is",
    "start": "422460",
    "end": "428940"
  },
  {
    "text": "identify things like like sections and that sort of thing that make it easy to uh to chunk and uh and group text and",
    "start": "428940",
    "end": "436199"
  },
  {
    "text": "importantly what we do is uh for all of these document types uh we pre-process them and then",
    "start": "436199",
    "end": "442380"
  },
  {
    "text": "um output them all in the same format um so from your perspective it doesn't really matter if it's an HTML document a",
    "start": "442380",
    "end": "449340"
  },
  {
    "text": "markdown document a PDF goes into unstructured and then comes out",
    "start": "449340",
    "end": "454740"
  },
  {
    "text": "um all in the on the same structured format and then you mentioned PDFs I know those",
    "start": "454740",
    "end": "460080"
  },
  {
    "text": "are those are like the most popular types of documents that or those are the trickiest ones to often work with and so",
    "start": "460080",
    "end": "465660"
  },
  {
    "text": "they're the most popular ones that we get questions about what do you what are you guys using under the hood are you using a different package do you have your own kind of like technology",
    "start": "465660",
    "end": "472620"
  },
  {
    "text": "um so uh it's actually um yeah the logic is actually like a little bit uh nuanced um so we actually",
    "start": "472620",
    "end": "478139"
  },
  {
    "text": "use different PDF processing techniques uh depending on what uh the user's uh",
    "start": "478139",
    "end": "483180"
  },
  {
    "text": "trying to do um and so um we've got what we call our high-res option uh which is uh model driven so if",
    "start": "483180",
    "end": "490620"
  },
  {
    "text": "you use that it will go and uh kind of look for bounding boxes and that sort of",
    "start": "490620",
    "end": "496080"
  },
  {
    "text": "thing in the document I'll label them and then extract the underlying text",
    "start": "496080",
    "end": "501780"
  },
  {
    "text": "um one of the other features that is available uh with that is table extraction and so um as part of that",
    "start": "501780",
    "end": "508139"
  },
  {
    "text": "we've got something called a table Transformer model uh that'll go through look for tables extract the tables and",
    "start": "508139",
    "end": "514800"
  },
  {
    "text": "then um they'll be available as both plain text and any HTML some documents don't need all of that",
    "start": "514800",
    "end": "522360"
  },
  {
    "text": "and like some in some cases you want to just process them really fast and don't really need like the model driven stuff in that case I will use under the hood",
    "start": "522360",
    "end": "529740"
  },
  {
    "text": "logic from things like PDF Miner and that sort of thing and pull out text and",
    "start": "529740",
    "end": "536220"
  },
  {
    "text": "then and then you basically kind of like just process the uh process the the plain text",
    "start": "536220",
    "end": "541560"
  },
  {
    "text": "um so uh depending on what you're trying to get out of it and um you know what uh what your priorities are there are a",
    "start": "541560",
    "end": "547920"
  },
  {
    "text": "couple different a couple different options for processing PDF specifically cool",
    "start": "547920",
    "end": "554760"
  },
  {
    "text": "all right um any other uh questions from the um from the chat uh if that will",
    "start": "557640",
    "end": "564120"
  },
  {
    "text": "show I'm just gonna show one uh last thing here uh really quick uh so one of the things that we've been recently",
    "start": "564120",
    "end": "570360"
  },
  {
    "text": "adding to make it uh even easier to go from uh you know your raw documents to",
    "start": "570360",
    "end": "575459"
  },
  {
    "text": "uh documents that are ready for uh Downstream llm use cases uh is uh something that we're calling connectors",
    "start": "575459",
    "end": "582420"
  },
  {
    "text": "um so the idea here is a lot of people have their documents in SharePoint or S3 or Azure block storage and that sort of",
    "start": "582420",
    "end": "588959"
  },
  {
    "text": "thing and they just want to point uh the they just want to point point us at that that document source and process",
    "start": "588959",
    "end": "596459"
  },
  {
    "text": "everything and then just kind of have the output available and so we started working on this thing called connectors",
    "start": "596459",
    "end": "603600"
  },
  {
    "text": "and so this is showing an example with with S3 and so you can go and I'll point",
    "start": "603600",
    "end": "609720"
  },
  {
    "text": "it at ns3 bucket and that could have PDFs and HTML documents and Powerpoints",
    "start": "609720",
    "end": "614820"
  },
  {
    "text": "and Word Documents it doesn't matter you can just point at that I use our CLI tool and it'll go through and",
    "start": "614820",
    "end": "621300"
  },
  {
    "text": "pre-process all of those documents and then on the other side you'll get a",
    "start": "621300",
    "end": "628680"
  },
  {
    "text": "directory with olive via the clean outputs um so we can look at some like",
    "start": "628680",
    "end": "634680"
  },
  {
    "text": "docx ones here so you'll see a Json output for uh for each of the uh for each of the documents uh in there the",
    "start": "634680",
    "end": "642120"
  },
  {
    "text": "idea long term is to basically kind of have a CLI and eventually a UI driven",
    "start": "642120",
    "end": "647519"
  },
  {
    "text": "tool that'll take you from source with a bunch of documents to a database sync like something like a chroma to make it",
    "start": "647519",
    "end": "654779"
  },
  {
    "text": "really easy to get up and running with your retrieval based application um so I think I'm coming up on my 10",
    "start": "654779",
    "end": "661680"
  },
  {
    "text": "minutes uh here and now that we got our documents into our Vector database I think it's good a good transition point",
    "start": "661680",
    "end": "668399"
  },
  {
    "text": "over to over to chroma unless there's any lingering questions there's a bunch of questions but I think",
    "start": "668399",
    "end": "674399"
  },
  {
    "text": "we can take them in the time after all right sounds good thanks Matthew that was great um and you",
    "start": "674399",
    "end": "680579"
  },
  {
    "text": "know thanks Harrison for having us hello to everyone um I want to talk to you guys a little bit about more you know the future of",
    "start": "680579",
    "end": "686760"
  },
  {
    "text": "retrieval um we've you know we've seen I think I think most people using lion Chain by",
    "start": "686760",
    "end": "692160"
  },
  {
    "text": "now are familiar with the very classic you know chat your documents example but um I think it's important for the",
    "start": "692160",
    "end": "697560"
  },
  {
    "text": "community to know there's a lot of research being done in this direction I want to talk about some of it um today so let me just pull up my slides and",
    "start": "697560",
    "end": "704160"
  },
  {
    "text": "hopefully everything doesn't instantly Break um audio visual software is the hardest problem in computer science we've come",
    "start": "704160",
    "end": "710220"
  },
  {
    "text": "to realize um so let me just pull this up all right",
    "start": "710220",
    "end": "715800"
  },
  {
    "text": "cool um the slides should be visible if they're not um people should you know scream and yell",
    "start": "715800",
    "end": "721740"
  },
  {
    "text": "are we good great so yeah let's talk about you know Advanced retrieval architecture so these",
    "start": "721740",
    "end": "726779"
  },
  {
    "text": "are things that are coming down the pipe right now a lot of these are still very very experimental things but they're worth thinking about",
    "start": "726779",
    "end": "732839"
  },
  {
    "text": "um because they kind of really influence how we build the underlying retrieval system and how tightly we integrate it with the model so let's get started with",
    "start": "732839",
    "end": "739500"
  },
  {
    "text": "the basics Everyone by now who's using line chain is probably very familiar with this fairly basic retrieval",
    "start": "739500",
    "end": "744959"
  },
  {
    "text": "augmented generation Loop or you know this is what you do when you want to chat with your documents you take your",
    "start": "744959",
    "end": "751440"
  },
  {
    "text": "um you take your documents and you embed them into Chrome or you you know ETL them and then embed them and load them",
    "start": "751440",
    "end": "757680"
  },
  {
    "text": "into chroma um that generates a set of embeddings which stores which is store to manage by chroma for you and then you put in a",
    "start": "757680",
    "end": "764160"
  },
  {
    "text": "query um chroma will handle embedding that query and returning uh the nearest neighbor results for you you had your",
    "start": "764160",
    "end": "770279"
  },
  {
    "text": "query on those results over to the model and the model generates an answer based on your query either answering the query or performing the task you'd like the",
    "start": "770279",
    "end": "775860"
  },
  {
    "text": "model to perform here you know it's open AI I'm actually very bullish on the sort of the rise of Open Source models we're",
    "start": "775860",
    "end": "782160"
  },
  {
    "text": "actively playing with law material to see how well it works on the retrieval up into generation context",
    "start": "782160",
    "end": "787260"
  },
  {
    "text": "um very exciting because with something like chroma which is also open source and an open source large language model and Lang chain uh it means you can run",
    "start": "787260",
    "end": "793320"
  },
  {
    "text": "this fully locally there's no you don't have to egress your data to the cloud which is something I know quite a number",
    "start": "793320",
    "end": "798600"
  },
  {
    "text": "of organizations and a few developers have been worried about especially if you're working with private data so the",
    "start": "798600",
    "end": "804120"
  },
  {
    "text": "one that I always think of is like you want this like intimate conversational assistant but you probably don't want to",
    "start": "804120",
    "end": "809519"
  },
  {
    "text": "send a lot of that intimate information over to open AI or whoever else of the world you like",
    "start": "809519",
    "end": "815820"
  },
  {
    "text": "so that's the basic Loop right and you know it's really easy to try with chroma and chrome is built into Lang chain",
    "start": "815820",
    "end": "821160"
  },
  {
    "text": "um fairly straightforward you can get us from the vector stores page so what's coming next um we are really barely getting started",
    "start": "821160",
    "end": "828240"
  },
  {
    "text": "with retrieval in um Ai and and with large language models",
    "start": "828240",
    "end": "833519"
  },
  {
    "text": "the retrieval systems that we have today are really just based on",
    "start": "833519",
    "end": "839339"
  },
  {
    "text": "straightforward embeddings-based Vector search the same way that it's done in",
    "start": "839339",
    "end": "844860"
  },
  {
    "text": "web you know in semantic search and recommender systems but and it doesn't really take into account the fact that",
    "start": "844860",
    "end": "850079"
  },
  {
    "text": "there's an entire large language model here that's also performing the inference task we're essentially retrieving information and then loading",
    "start": "850079",
    "end": "855720"
  },
  {
    "text": "it just into the model's context window but we could be doing a lot more so there's a few different questions as",
    "start": "855720",
    "end": "860880"
  },
  {
    "text": "well right we're talking about what to retrieve and the kind of the default is to retrieve these text chunks so we take a document we chunk it up using various",
    "start": "860880",
    "end": "867839"
  },
  {
    "text": "data loaders and then we retrieve those text chunks and we stick those chunks in the talk in the context window of the",
    "start": "867839",
    "end": "873600"
  },
  {
    "text": "model but we could be retrieving at the token level we could be retrieving something else entirely and in fact we",
    "start": "873600",
    "end": "879480"
  },
  {
    "text": "could vary what we're retrieving depending on the task and then there's different points at which we could use",
    "start": "879480",
    "end": "884540"
  },
  {
    "text": "retrieval as well we can put for example the way we do it today is we take our retrieve chunks and we put it into the",
    "start": "884540",
    "end": "891060"
  },
  {
    "text": "context window into the input of the large language model but we could also inject retrieval steps into the LM",
    "start": "891060",
    "end": "897180"
  },
  {
    "text": "architecture itself for example using the the attention heads in the decoder",
    "start": "897180",
    "end": "903360"
  },
  {
    "text": "layer to figure out which tokens to retrieve or we could even inject retrieval results into the model output",
    "start": "903360",
    "end": "909240"
  },
  {
    "text": "as well at the output layer so we basically um inject additional embeddings weighted",
    "start": "909240",
    "end": "915600"
  },
  {
    "text": "by what the model has generated so far and actually use the model generation as a signal for what to retrieve which is",
    "start": "915600",
    "end": "921180"
  },
  {
    "text": "very interesting one example of that is the hypothetical document embeddings idea where we actually use a model",
    "start": "921180",
    "end": "926639"
  },
  {
    "text": "response to a query even though it doesn't have access to the underlying information to generate something that might be a response to that query and",
    "start": "926639",
    "end": "933000"
  },
  {
    "text": "then use that generation for retrieval so that's another way to look at it and then of course there's the question",
    "start": "933000",
    "end": "938820"
  },
  {
    "text": "about when to retrieve typically the way we do things today is one big bang retrieval step in other words you know",
    "start": "938820",
    "end": "943980"
  },
  {
    "text": "we perform retrieval once we put it all in the context window and off we go but there's no reason that we can't for",
    "start": "943980",
    "end": "949019"
  },
  {
    "text": "example retrieve as the generation is happening so as each new token is generated we perform a new retrieval step and then feed that into the model",
    "start": "949019",
    "end": "955339"
  },
  {
    "text": "or we can actually conditionally retrieve so we know that okay the model here is that for example a hyperplexity",
    "start": "955339",
    "end": "961380"
  },
  {
    "text": "in the next generated token maybe this is a good time to ground ourselves into the retrieved information so there's a",
    "start": "961380",
    "end": "967199"
  },
  {
    "text": "lot to explore there was a workshop on this at ACL very recently a lot of great papers were examined I've got the source",
    "start": "967199",
    "end": "974100"
  },
  {
    "text": "um up on the slide here if you're interested in this stuff I really recommend digging into this",
    "start": "974100",
    "end": "979620"
  },
  {
    "text": "um because a lot of this is coming down the pipe it's it's been demonstrated that this really helps the models perform very well on certain benchmarks",
    "start": "979620",
    "end": "986160"
  },
  {
    "text": "um and so working out how this works is is like going to be going to be very important",
    "start": "986160",
    "end": "991920"
  },
  {
    "text": "so again there's um there's tons of papers around this right now uh we can retrieve entities",
    "start": "991920",
    "end": "997800"
  },
  {
    "text": "rather than just document chunks we can retrieve tokens either directly uh using",
    "start": "997800",
    "end": "1003139"
  },
  {
    "text": "like this K nearest neighbors approach or else adaptively where we use the attention heads as I mentioned",
    "start": "1003139",
    "end": "1008899"
  },
  {
    "text": "um we can feed tokens retrieve tokens or retrieve embeddings of tokens directly into the intermediate layers as I",
    "start": "1008899",
    "end": "1015320"
  },
  {
    "text": "mentioned so into the decoder head rather than in the encoder head or in the top level of the decoder only",
    "start": "1015320",
    "end": "1020899"
  },
  {
    "text": "architecture or um we can also retrieve from basically",
    "start": "1020899",
    "end": "1026058"
  },
  {
    "text": "within the model's context window itself and feed that in uh appropriately as part of the um as part of the retrieval",
    "start": "1026059",
    "end": "1033438"
  },
  {
    "text": "strategy which is very interesting for long context window models which we know tend to get distracted and tend to sort",
    "start": "1033439",
    "end": "1039558"
  },
  {
    "text": "of forget Parts in the middle of the context if you have a look at some of the recent papers around large context windows and how well they actually",
    "start": "1039559",
    "end": "1045260"
  },
  {
    "text": "perform uh the attention mechanism is fairly limited and could be augmented over a longer context window using a",
    "start": "1045260",
    "end": "1050780"
  },
  {
    "text": "retrieval-based strategy and then finally you know what we talked about when to retrieve you can retrieve",
    "start": "1050780",
    "end": "1056539"
  },
  {
    "text": "everything at once we can retrieve every end tokens or we can retrieve as I mentioned it absolutely deciding when we actually want to retrieve and then",
    "start": "1056539",
    "end": "1062480"
  },
  {
    "text": "inject that into into the most context windows so this is all right now very",
    "start": "1062480",
    "end": "1068179"
  },
  {
    "text": "experimental um what's interesting about these in particular is typically where for at",
    "start": "1068179",
    "end": "1074120"
  },
  {
    "text": "least many of these strategies you can take an existing off the shelf large language model and add these",
    "start": "1074120",
    "end": "1080419"
  },
  {
    "text": "strategies to it so there are papers like unlimi former which actually take any underlying encoder decoder",
    "start": "1080419",
    "end": "1086020"
  },
  {
    "text": "Transformer and allow you to basically integrate um a retrieval retrieval setup using the",
    "start": "1086020",
    "end": "1091940"
  },
  {
    "text": "attention mechanism as the query vector so there's a lot to play with here uh it's showing early promise we think that",
    "start": "1091940",
    "end": "1098900"
  },
  {
    "text": "ultimately this sort of type integration is very likely and and very useful but it raises a lot of questions around the",
    "start": "1098900",
    "end": "1104720"
  },
  {
    "text": "architecture about how you build retrieval systems and how you actually manage to get these things into the GPU memory all of this all these parts have",
    "start": "1104720",
    "end": "1111320"
  },
  {
    "text": "to still be worked out so so yeah um a little bit different from the usual retrieval augmented generation",
    "start": "1111320",
    "end": "1116720"
  },
  {
    "text": "uh talk and demo but I thought it would be nice for the topic of this of this Advanced retrieval webinar",
    "start": "1116720",
    "end": "1123919"
  },
  {
    "text": "um I'll leave it either open to Q8 now or I'll just hand it over to you Harrison uh well I'm going to ask one",
    "start": "1123919",
    "end": "1129919"
  },
  {
    "text": "question I think there's a there's a this is great by the way I hadn't seen a lot of these uh I haven't seen them kind",
    "start": "1129919",
    "end": "1135799"
  },
  {
    "text": "of like presented in these like what to retrieve how to retrieve or when to retrieve thing",
    "start": "1135799",
    "end": "1141080"
  },
  {
    "text": "um what like I guess my question would be you know I think a big rise in the popularity of language models is that",
    "start": "1141080",
    "end": "1149000"
  },
  {
    "text": "it's really easy to use them and that they're behind an API and right now and yes there's the local models and I'm",
    "start": "1149000",
    "end": "1154340"
  },
  {
    "text": "also excited about those but you know a lot of people are still using models behind an API not only because they're better but also because it's generally",
    "start": "1154340",
    "end": "1160460"
  },
  {
    "text": "kind of like easier um a lot of these methods rely on kind of like access to the model that aren't",
    "start": "1160460",
    "end": "1166039"
  },
  {
    "text": "currently exposed by some of like the large API providers yes or no",
    "start": "1166039",
    "end": "1173419"
  },
  {
    "text": "so obviously the ones that require injecting tokens directly into the intermediate layers aren't available to",
    "start": "1173419",
    "end": "1180559"
  },
  {
    "text": "you if you're accessing models over an API but the sort of idea about for example retrieving as tokens are",
    "start": "1180559",
    "end": "1186320"
  },
  {
    "text": "generated is completely accessible restarting the generation or injecting additional context in the middle of the generation is absolutely available to",
    "start": "1186320",
    "end": "1192620"
  },
  {
    "text": "you um it's just you know it's more expensive it does require more API accesses but the other thing is is by",
    "start": "1192620",
    "end": "1200780"
  },
  {
    "text": "making retrieval controllable in the context of the model generation itself you can start driving some of those costs down",
    "start": "1200780",
    "end": "1207200"
  },
  {
    "text": "um which is a very interesting direction to take I really do wonder I know for example",
    "start": "1207200",
    "end": "1212480"
  },
  {
    "text": "that the AI research labs are very actively also pursuing retrieval",
    "start": "1212480",
    "end": "1217880"
  },
  {
    "text": "and so I wonder exactly where the interface is going to land should they provide that kind of retrieval",
    "start": "1217880",
    "end": "1223760"
  },
  {
    "text": "augmentation um it's pretty unclear and I think it's unclear to everybody right now but part of the fun in in building in this space",
    "start": "1223760",
    "end": "1230000"
  },
  {
    "text": "right now is so much isn't determined yet so much isn't figured out and there's a lot of room to influence basically what what gets built",
    "start": "1230000",
    "end": "1237200"
  },
  {
    "text": "by by working just by starting to work with open source and saying hey this works really really well and then obviously if it's working really great",
    "start": "1237200",
    "end": "1243260"
  },
  {
    "text": "then the labs will have no trace the API providers will have no choice but to provide some version of it too so we're",
    "start": "1243260",
    "end": "1248720"
  },
  {
    "text": "bullish on that future yeah I think it's interesting there because like I don't know maybe this is",
    "start": "1248720",
    "end": "1254539"
  },
  {
    "text": "the retrieving context paper or maybe it's a different one but there's the flare paper which does something around like when to retrieve and they use they",
    "start": "1254539",
    "end": "1261919"
  },
  {
    "text": "use like open AI but what they use is like the token probabilities yes like if it's low probability stop generating do",
    "start": "1261919",
    "end": "1268580"
  },
  {
    "text": "a retrieval step put it in but nope that's that's the Adaptive retrieval that's the Adaptive retrieval piece with",
    "start": "1268580",
    "end": "1274220"
  },
  {
    "text": "with end tokens yeah yeah and and so that's doable with an API if it Returns",
    "start": "1274220",
    "end": "1279440"
  },
  {
    "text": "the token probabilities and I think we've actually seen the chat models return less and less and so it's kind of like in terms of like information so you",
    "start": "1279440",
    "end": "1286700"
  },
  {
    "text": "know the issue I mean the reason that they started doing that of course is because it like it really AIDS distillation when you have the log probs",
    "start": "1286700",
    "end": "1293240"
  },
  {
    "text": "um and they really don't want you just filling their models um look skill issue in my opinion",
    "start": "1293240",
    "end": "1300020"
  },
  {
    "text": "cool awesome um I will chat very briefly about some",
    "start": "1300020",
    "end": "1306980"
  },
  {
    "text": "of the retrieval stuff that we're doing um in uh Lang chin um and then we'll",
    "start": "1306980",
    "end": "1312500"
  },
  {
    "text": "open it up to questions so this is another call to go add your questions to the QA sidebar on the right and upload",
    "start": "1312500",
    "end": "1318799"
  },
  {
    "text": "the ones that you're most interested in in hearing answers um I will share my screen very briefly",
    "start": "1318799",
    "end": "1327559"
  },
  {
    "text": "um you may need to stop sharing your example",
    "start": "1327559",
    "end": "1333280"
  },
  {
    "text": "cool all right so this is um so so I I'm I'll I won't spend too",
    "start": "1342500",
    "end": "1349039"
  },
  {
    "text": "long on this but basically I'll chat very briefly about um some of the edge cases of uh the",
    "start": "1349039",
    "end": "1354440"
  },
  {
    "text": "classic semantic search kind of like rag style retrieval thing that we see mostly",
    "start": "1354440",
    "end": "1359539"
  },
  {
    "text": "on um so it's a little bit different approach from from Anton still sticking",
    "start": "1359539",
    "end": "1364580"
  },
  {
    "text": "with kind of like rag in in terms of like injecting it into the prompt just and largely because that's what we see a",
    "start": "1364580",
    "end": "1370039"
  },
  {
    "text": "lot of Lane chain kind of like users doing but talking about kind of like when does like cosine similarity when it's just straight up sound like",
    "start": "1370039",
    "end": "1376220"
  },
  {
    "text": "semantic search kind of like fail and what are some interesting Alternatives that we're kind of like working on",
    "start": "1376220",
    "end": "1382220"
  },
  {
    "text": "um uh so so some edge cases of semantic search and I'll go through these in a",
    "start": "1382220",
    "end": "1387260"
  },
  {
    "text": "little bit more detail but and there's probably also a lot more um you know I think kind of like just",
    "start": "1387260",
    "end": "1392539"
  },
  {
    "text": "cosine similarity might get you like 80 of the way there but but as you start to",
    "start": "1392539",
    "end": "1398240"
  },
  {
    "text": "want higher and higher reliability there's a lot of issues that pop up one one of the things that we see",
    "start": "1398240",
    "end": "1404360"
  },
  {
    "text": "um is repeated information um basically there's some documents have the same set of information maybe",
    "start": "1404360",
    "end": "1409700"
  },
  {
    "text": "there's even duplicate documents this uh has a few downsides in that it basically",
    "start": "1409700",
    "end": "1415100"
  },
  {
    "text": "just takes up kind of like context and so you might not be retrieving the the kind you might like you might retrieve",
    "start": "1415100",
    "end": "1422419"
  },
  {
    "text": "only one type of document even if you retrieve kind of like two types of documents if there's like five of one it",
    "start": "1422419",
    "end": "1428240"
  },
  {
    "text": "will kind of like um overwhelm a little bit the or could cause some distraction",
    "start": "1428240",
    "end": "1433640"
  },
  {
    "text": "um so basically some of the one of the more basic one or one of the more basic solutions that we have implemented here",
    "start": "1433640",
    "end": "1439520"
  },
  {
    "text": "is kind of like Max smart general relevance this is just basically a kind of like wrapper a lot of these use",
    "start": "1439520",
    "end": "1445460"
  },
  {
    "text": "standard kind of like vector indexes under the hood um so it's a wrap around that it selects maybe like the top kind",
    "start": "1445460",
    "end": "1451340"
  },
  {
    "text": "of like 100 documents by cosine similarity and then does a separate kind of like almost re-ranking step where it",
    "start": "1451340",
    "end": "1457700"
  },
  {
    "text": "goes through first two and then basically chooses documents based on not only their similarity to the query but",
    "start": "1457700",
    "end": "1463340"
  },
  {
    "text": "also their uh uh difference from previous documents that have already been selected so you get some diversity",
    "start": "1463340",
    "end": "1469280"
  },
  {
    "text": "and you get a diverse set of information coming through um conflicting information is an",
    "start": "1469280",
    "end": "1475400"
  },
  {
    "text": "interesting one um so you can sometimes have like the same answer from multiple sources",
    "start": "1475400",
    "end": "1480559"
  },
  {
    "text": "um so an example I'd like to use here is in in kind of like your notion uh database for your company you might have",
    "start": "1480559",
    "end": "1486620"
  },
  {
    "text": "some um uh description of the vacation policy in the main home page and then you might",
    "start": "1486620",
    "end": "1492860"
  },
  {
    "text": "have some sentence message in the vacation policy in like your personal notes or something like that and uh very",
    "start": "1492860",
    "end": "1499700"
  },
  {
    "text": "related to this by the way is the idea of like temporality and information can update over time um and maybe be conflicting over time",
    "start": "1499700",
    "end": "1506299"
  },
  {
    "text": "and and the solutions are kind of similar one is kind of like uh you can do some importance weighting based on on",
    "start": "1506299",
    "end": "1512720"
  },
  {
    "text": "documents perhaps you can also include a lot of the metadata in the retrieved results and then and then tell the",
    "start": "1512720",
    "end": "1519380"
  },
  {
    "text": "language model to to reason and then it will generally be able to reason that you know more relevant or more recent",
    "start": "1519380",
    "end": "1525679"
  },
  {
    "text": "information is more relevant um if you have a higher importance you should maybe trust that and this is where I think a lot of the ingestion",
    "start": "1525679",
    "end": "1532220"
  },
  {
    "text": "stuff and adding metadata to documents appropriately that that Matt was talking",
    "start": "1532220",
    "end": "1538159"
  },
  {
    "text": "about and why and why I think there's a lot of nuance in loading documents and splitting documents and making sure that",
    "start": "1538159",
    "end": "1543919"
  },
  {
    "text": "you have the right metadata for documents all this becomes really really important um to give kind of like the appropriate",
    "start": "1543919",
    "end": "1549200"
  },
  {
    "text": "context on uh when a document occur and how important it was how much it should be trusted things like that",
    "start": "1549200",
    "end": "1557260"
  },
  {
    "text": "um this is this is one of my uh more uh this is one of my more favorite ones",
    "start": "1557779",
    "end": "1562940"
  },
  {
    "text": "that we have in link chain um it's called the solution for this is the self query retrieval but the basic idea is that questions that you get",
    "start": "1562940",
    "end": "1569059"
  },
  {
    "text": "might be more about the metadata than the content and so if you're doing kind of like a cosine similarity approach",
    "start": "1569059",
    "end": "1575779"
  },
  {
    "text": "um and you're just and you're just looking at the semantic meaning of a query if you have a query like movie",
    "start": "1575779",
    "end": "1581480"
  },
  {
    "text": "movies about aliens in 1980 or something like that what you really want to search",
    "start": "1581480",
    "end": "1586700"
  },
  {
    "text": "for assuming you have a database of kind of like movies you you really want to search on the descriptions for for",
    "start": "1586700",
    "end": "1592100"
  },
  {
    "text": "aliens or something like that and then the 1980 it should be kind of like a metadata attribute that you're kind of",
    "start": "1592100",
    "end": "1598100"
  },
  {
    "text": "like explicitly filtering against and so uh this idea is basically using a",
    "start": "1598100",
    "end": "1603200"
  },
  {
    "text": "language model to extract that metadata most Vector stories including chroma have kind of like metadata that you can",
    "start": "1603200",
    "end": "1609080"
  },
  {
    "text": "associate and you can filter on and they have a bunch of different operations you can do um equal greater than a bunch of other",
    "start": "1609080",
    "end": "1616100"
  },
  {
    "text": "ones like that and so the idea is to extract metadata from a query along with a semantic bit from the query and then",
    "start": "1616100",
    "end": "1622520"
  },
  {
    "text": "use those uh Downstream um and then the the last uh thing would",
    "start": "1622520",
    "end": "1630440"
  },
  {
    "text": "basically be questions or queries that require kind of like multiple queries um so it might be asking kind of like in",
    "start": "1630440",
    "end": "1636679"
  },
  {
    "text": "and I call this like a multi-hop question um and I think like an example uh uh of",
    "start": "1636679",
    "end": "1642200"
  },
  {
    "text": "this is there's a lot of the original kind of like self-ask um questions like who won or what is the",
    "start": "1642200",
    "end": "1648200"
  },
  {
    "text": "hometown of the US Open champion in 2017 or something like that and so first you",
    "start": "1648200",
    "end": "1654020"
  },
  {
    "text": "want to look up like you want to do some retrieval on like the US Open champion in 2017 and then you want to do some",
    "start": "1654020",
    "end": "1659600"
  },
  {
    "text": "retrieval on on uh the his hometown um and so the idea here is basically",
    "start": "1659600",
    "end": "1665720"
  },
  {
    "text": "don't do kind of like one query and and get everything but rather First Look up the answer to the first one and then do",
    "start": "1665720",
    "end": "1671600"
  },
  {
    "text": "a second one and some type of like agent-like or or looping like approach here",
    "start": "1671600",
    "end": "1677179"
  },
  {
    "text": "um I think uh is is uh appropriate um just showing a screenshot",
    "start": "1677179",
    "end": "1684260"
  },
  {
    "text": "um of the self query really quickly because I think that is a",
    "start": "1684260",
    "end": "1690140"
  },
  {
    "text": "um that's a fun one because it uses chroma we can see that over here where we load up a bunch of documents first",
    "start": "1690140",
    "end": "1699080"
  },
  {
    "text": "um or here I think we explicitly list some out just to show what's going on so we have the page content this is the movie database one we have like the",
    "start": "1699080",
    "end": "1705140"
  },
  {
    "text": "pagecon 110 which is a description of the movies and then we have a bunch of metadata that we can filter on so the year the rating the genre and so we can",
    "start": "1705140",
    "end": "1712279"
  },
  {
    "text": "load it into chroma and then we can see uh that we just basically provide some information about what the metadata",
    "start": "1712279",
    "end": "1718760"
  },
  {
    "text": "fields are um and then load this self query Retriever and then now for a given query",
    "start": "1718760",
    "end": "1724580"
  },
  {
    "text": "we can extract kind of like the query and the filter so what are some movies about dinosaurs the queries now",
    "start": "1724580",
    "end": "1729860"
  },
  {
    "text": "dinosaurs there's no really filter that's specified here I want to watch a movie rated higher than 8.5 this is the",
    "start": "1729860",
    "end": "1737000"
  },
  {
    "text": "opposite there's no real query here but there is a filter and so we get this comparator greater than equal to the",
    "start": "1737000",
    "end": "1742880"
  },
  {
    "text": "rating 8.5 and then there's a bunch that combine them so has Greta Gerwig",
    "start": "1742880",
    "end": "1748039"
  },
  {
    "text": "directed any movements about women query women and then this filter this can get like pretty complicated so now we have a",
    "start": "1748039",
    "end": "1753380"
  },
  {
    "text": "movie like or now we have Aquarius what's a movie after 1990 before 2005.",
    "start": "1753380",
    "end": "1758419"
  },
  {
    "text": "that's all about toys and preferably is animated we can see here we have the queries toys and then we have this",
    "start": "1758419",
    "end": "1763760"
  },
  {
    "text": "complex kind of like uh conjunction of all those other things",
    "start": "1763760",
    "end": "1769399"
  },
  {
    "text": "um there's a bunch of other retrievers uh here and and I would uh comparing this a",
    "start": "1769399",
    "end": "1775640"
  },
  {
    "text": "bit to kind of like what uh Anton presented on I would I would classify",
    "start": "1775640",
    "end": "1780799"
  },
  {
    "text": "these more as kind of like heuristics you can kind of like do on top of your rag architecture rather than like",
    "start": "1780799",
    "end": "1785960"
  },
  {
    "text": "fundamentally changing a lot of the rag architecture itself I think the closest one that we kind of have is we have an",
    "start": "1785960",
    "end": "1791659"
  },
  {
    "text": "implementation of flair um which is the one that determines to dynamically kind of like do uh retrieval",
    "start": "1791659",
    "end": "1797899"
  },
  {
    "text": "but a lot of these rely on not changing the underlying structure too much you're still injecting stuff into the content",
    "start": "1797899",
    "end": "1804559"
  },
  {
    "text": "of the of the of the prompt um and that's partially by Design in order to keep kind of like the",
    "start": "1804559",
    "end": "1810559"
  },
  {
    "text": "interaction simple a lot of people are just using those apis um and yeah so I think it will be",
    "start": "1810559",
    "end": "1817220"
  },
  {
    "text": "interesting to kind of like I don't know I think it'll be really interesting to see how those apis evolve",
    "start": "1817220",
    "end": "1823039"
  },
  {
    "text": "and what other kind of like techniques besides just these heuristics become more and more available that's basically all I have",
    "start": "1823039",
    "end": "1830779"
  },
  {
    "text": "I don't know if there's anything you guys wanted to add on that yeah I do I I have something um so you know I think",
    "start": "1830779",
    "end": "1837140"
  },
  {
    "text": "we're seeing increasingly people sort of looking at more like you said Harris and",
    "start": "1837140",
    "end": "1843320"
  },
  {
    "text": "more heuristics driven retrieval Pipelines from my experience in robotics uh of you",
    "start": "1843320",
    "end": "1849679"
  },
  {
    "text": "know around seven years anytime you have a hand tuned heuristic driven pipeline you are going to put yourself in a rat",
    "start": "1849679",
    "end": "1856640"
  },
  {
    "text": "race of dealing with edge cases and I think we're going to enter that era before long I think the long-term right",
    "start": "1856640",
    "end": "1862460"
  },
  {
    "text": "solution for this is figuring out how to build flexible retrieval oriented models",
    "start": "1862460",
    "end": "1868640"
  },
  {
    "text": "and couple these things more tightly together again in my experience it's possible to get something that kind of",
    "start": "1868640",
    "end": "1874279"
  },
  {
    "text": "works most of the time and that's kind of the Trap of adding more heuristics is it will it will seem to kind of work",
    "start": "1874279",
    "end": "1880820"
  },
  {
    "text": "more often but what you won't notice necessarily unless you're looking you know to look out for this is the variety",
    "start": "1880820",
    "end": "1886220"
  },
  {
    "text": "of edge cases actually increases as you add more heuristics so just something to watch out for",
    "start": "1886220",
    "end": "1892399"
  },
  {
    "text": "um I think that what we need really are like very general approaches and general methods that work in in all kinds of",
    "start": "1892399",
    "end": "1898940"
  },
  {
    "text": "different in kinds of different contexts rather than talking about like you know this particular thing works",
    "start": "1898940",
    "end": "1904580"
  },
  {
    "text": "here this particular thing works there but at the same time I think it's really worth exploring a lot of those heuristics because they might point us",
    "start": "1904580",
    "end": "1909860"
  },
  {
    "text": "in the right direction uh long run that's just yeah that's just something I I thought I'd mention",
    "start": "1909860",
    "end": "1915440"
  },
  {
    "text": "foreign do you have like what what's your estimate on kind of like a",
    "start": "1915440",
    "end": "1922399"
  },
  {
    "text": "what's your resume on like the timeline for kind of like these things like if we say there's kind of like let's maybe like throw like you know",
    "start": "1922399",
    "end": "1929960"
  },
  {
    "text": "in in like six months will people still largely just be doing kind of like cosine similarity things and like in six",
    "start": "1929960",
    "end": "1936679"
  },
  {
    "text": "months will the apis by the major model providers still be the same where they don't kind of like allow for a lot of the the kind of like token level",
    "start": "1936679",
    "end": "1943159"
  },
  {
    "text": "manipulation yeah if I was to if I was to make an educated guess and in the six month Horizon I think is really",
    "start": "1943159",
    "end": "1949100"
  },
  {
    "text": "interesting one I think in six months we're still gonna be living in the world where most organizations and developers",
    "start": "1949100",
    "end": "1955460"
  },
  {
    "text": "haven't adopted retrieval augmented generation at all yet right it's it's easy to forget just how like just",
    "start": "1955460",
    "end": "1964159"
  },
  {
    "text": "working on this stuff instantly puts you three or four months in the future of everybody else who's not actively",
    "start": "1964159",
    "end": "1970100"
  },
  {
    "text": "working on it or thinking about it all the time so I think in six months we're still going to be in that long tail of",
    "start": "1970100",
    "end": "1975140"
  },
  {
    "text": "adoption for even basic retrieval looking into generation which is very powerful as it is right I think it's you",
    "start": "1975140",
    "end": "1980419"
  },
  {
    "text": "know we're talking about these Advanced methods but I think it's important to remember the the basic idea here still is very powerful the ability to interact",
    "start": "1980419",
    "end": "1986600"
  },
  {
    "text": "conversationally and non-linearly with large corpuses of text is just fantastically useful across a variety of",
    "start": "1986600",
    "end": "1992000"
  },
  {
    "text": "Industries and use cases and we've seen that with chroma's adoption right um now that's it I think at the Leading",
    "start": "1992000",
    "end": "1999019"
  },
  {
    "text": "Edge we're going to in six months I would say at the Leading Edge we're going to see practical deployments the",
    "start": "1999019",
    "end": "2006100"
  },
  {
    "text": "first practical deployments of these more directly integrated retrieval strategies",
    "start": "2006100",
    "end": "2012399"
  },
  {
    "text": "um for organizations that are very forward about this and I could commit it to pursuing this as a strategy instead",
    "start": "2012399",
    "end": "2017740"
  },
  {
    "text": "of people who instead of the organizations who are kind of like we got to do something with AI and the first thing they think to do of course",
    "start": "2017740",
    "end": "2023799"
  },
  {
    "text": "is document based question answering which again not to not to denigrate that I think it's actually tremendous",
    "start": "2023799",
    "end": "2029019"
  },
  {
    "text": "um in terms of what I think the AI research labs are going to do I think look it's in their best interests and I know",
    "start": "2029019",
    "end": "2035799"
  },
  {
    "text": "they're actively working on this and it's a very interesting question for us as a business for us as a research group",
    "start": "2035799",
    "end": "2042700"
  },
  {
    "text": "to know what that interface might be like right um and I think the best way to influence",
    "start": "2042700",
    "end": "2048520"
  },
  {
    "text": "that is to show what works uh as quickly as possible which is why again this is why we're bullish in particular on the",
    "start": "2048520",
    "end": "2054760"
  },
  {
    "text": "open source models because we can figure out what works um but yeah I think I would suggest it in",
    "start": "2054760",
    "end": "2061000"
  },
  {
    "text": "six months I would be very surprised if we didn't see some retrieval interface",
    "start": "2061000",
    "end": "2067540"
  },
  {
    "text": "being offered by a API provider however I think that's going to look a lot more",
    "start": "2067540",
    "end": "2073060"
  },
  {
    "text": "like your data is in Azure or your data is in gcp and you could hook it up to a model",
    "start": "2073060",
    "end": "2079419"
  },
  {
    "text": "rather than providing an interface to an external retriever I think the long run based on",
    "start": "2079419",
    "end": "2086020"
  },
  {
    "text": "you know how software the software business really runs and especially at the Enterprise level especially at the",
    "start": "2086020",
    "end": "2091060"
  },
  {
    "text": "data product level which ultimately what retrieval systems are it's a data product um",
    "start": "2091060",
    "end": "2096700"
  },
  {
    "text": "Enterprises want control of their own data Enterprise is want their data to be sitting in infrastructure that fits",
    "start": "2096700",
    "end": "2102580"
  },
  {
    "text": "neatly with what they're actually doing as a business as opposed to what fits neatly with the API providers",
    "start": "2102580",
    "end": "2108480"
  },
  {
    "text": "infrastructure and so I think there's going to be a great demand for sort of being able to do",
    "start": "2108480",
    "end": "2113500"
  },
  {
    "text": "all of the different retrieval stuff inside their own VPC inside their own cloud so regardless of exactly how the",
    "start": "2113500",
    "end": "2120099"
  },
  {
    "text": "API providers give it to you I think that like running it like the question the question that everyone will have is",
    "start": "2120099",
    "end": "2126460"
  },
  {
    "text": "like okay well I want this entirely inside my own VPC how do I do that how do I not send my data over to somebody",
    "start": "2126460",
    "end": "2132280"
  },
  {
    "text": "else or egress it like regardless of Warriors around the labs training on your data which they promise not to do",
    "start": "2132280",
    "end": "2138579"
  },
  {
    "text": "but how are you really going to hold them to that can you prove they didn't train on your data I don't know",
    "start": "2138579",
    "end": "2143859"
  },
  {
    "text": "um right there's costs of egress there's",
    "start": "2143859",
    "end": "2149619"
  },
  {
    "text": "cost of network um and these things become very large and non-trivial when you're running an Enterprise scale",
    "start": "2149619",
    "end": "2155560"
  },
  {
    "text": "and one other like just general like",
    "start": "2155560",
    "end": "2161160"
  },
  {
    "text": "so so in the you know you had the I forgot what you had you had the what do I retrieve how do I retrieve how do I",
    "start": "2161160",
    "end": "2168099"
  },
  {
    "text": "use the retrieval of something like that there's still the so like even whether you're retrieving tax or uh tokens or",
    "start": "2168099",
    "end": "2175660"
  },
  {
    "text": "whether you're retrieving once or every step you're still doing some retrieval at some form",
    "start": "2175660",
    "end": "2181420"
  },
  {
    "text": "um and like like that right now is generally kind of like some cosine similarity thing but there could be kind",
    "start": "2181420",
    "end": "2187060"
  },
  {
    "text": "of like other retrieval mechanisms how do you see like that retrieval regardless of where it happens and",
    "start": "2187060",
    "end": "2192099"
  },
  {
    "text": "regardless of what it's retrieving um kind of like evolving because I think a lot of your talks was about like yeah",
    "start": "2192099",
    "end": "2197560"
  },
  {
    "text": "the different ways you can use the retrieved things yeah so here's the thing about cosine similarity it's a",
    "start": "2197560",
    "end": "2203380"
  },
  {
    "text": "fixed metric right but the whole point of AI is flexible and adaptable and I think ultimately",
    "start": "2203380",
    "end": "2209339"
  },
  {
    "text": "how we deal with relevance needs to be conditional on the query the task and",
    "start": "2209339",
    "end": "2214420"
  },
  {
    "text": "the target model which means that you need to adapt like if we're doing vector-based retrieval which I suspect",
    "start": "2214420",
    "end": "2220180"
  },
  {
    "text": "we will be in six months um because it's just insanely flexible what we will do is we will see the space",
    "start": "2220180",
    "end": "2226240"
  },
  {
    "text": "in which we're performing retrieval adapted on the Fly so it will still be one way to look at",
    "start": "2226240",
    "end": "2231700"
  },
  {
    "text": "this is like not cosine similarity versus L2 versus whatever but the fact that like let's",
    "start": "2231700",
    "end": "2237099"
  },
  {
    "text": "say which one example of what could happen and we're already seeing it now is",
    "start": "2237099",
    "end": "2242260"
  },
  {
    "text": "you can use the attention mechanism of say the decoder head as a query Vector into",
    "start": "2242260",
    "end": "2249820"
  },
  {
    "text": "um into our latent space and then it's basically like the dot product similarity there is finding the most",
    "start": "2249820",
    "end": "2254920"
  },
  {
    "text": "attended two tokens right but that's kind of like adaptively um changing the space another way to do",
    "start": "2254920",
    "end": "2261820"
  },
  {
    "text": "it is to have some model for example that conditions the uh basically it applies a",
    "start": "2261820",
    "end": "2268420"
  },
  {
    "text": "linear transform onto the vector space which is conditional on the query as it comes in so you train this model and it",
    "start": "2268420",
    "end": "2274300"
  },
  {
    "text": "like says okay for this kind of query let's say the example that I always used to give was like for I'm looking for",
    "start": "2274300",
    "end": "2279820"
  },
  {
    "text": "angry tweets okay I need to like expand the angry dimensionality of the",
    "start": "2279820",
    "end": "2285579"
  },
  {
    "text": "embedding vector and I need to reduce the other dimensions and that's when you're when you're expanding some dimensions and Contracting others it's",
    "start": "2285579",
    "end": "2291099"
  },
  {
    "text": "an affine transform so all we're doing is learning a model that can generate an affine transform which is very neat",
    "start": "2291099",
    "end": "2296800"
  },
  {
    "text": "right that's a very different you know I want to query I want to answer query like give me all the tweets about covid okay I'm gonna like change weight that",
    "start": "2296800",
    "end": "2303040"
  },
  {
    "text": "differently to give me all the angry tweets right and so we're changing the vector space on the Fly and it's actually very cheap to do",
    "start": "2303040",
    "end": "2309579"
  },
  {
    "text": "uh because the beauty of affine transforms especially when they're invertible is you can apply that transform to the query instead of",
    "start": "2309579",
    "end": "2315760"
  },
  {
    "text": "recalculating the entire Vector space itself uh so I think that that's very promising and on the horizon and again",
    "start": "2315760",
    "end": "2321160"
  },
  {
    "text": "one one way to do that when you think about what the attention mechanism in a Transformer actually does is it",
    "start": "2321160",
    "end": "2326500"
  },
  {
    "text": "re-weights the vectors as they propagate through the layers so some sort of like basic attention-based model could be",
    "start": "2326500",
    "end": "2333040"
  },
  {
    "text": "very useful on the on the query side um so I think that we'll see something there pretty quickly it's still cosine",
    "start": "2333040",
    "end": "2338560"
  },
  {
    "text": "similarity deep down under the hood right but it's what you do with it that changes",
    "start": "2338560",
    "end": "2344400"
  },
  {
    "text": "all right enough of enough of my bad questions let's move on to some audience questions",
    "start": "2346480",
    "end": "2352060"
  },
  {
    "text": "so um I saw a lot one question from Louise in the audience about uh if there's a",
    "start": "2352060",
    "end": "2358420"
  },
  {
    "text": "generic way to find uh titles and body text from the HTML um so I just wanted",
    "start": "2358420",
    "end": "2363520"
  },
  {
    "text": "to choose something on that real quick",
    "start": "2363520",
    "end": "2369660"
  },
  {
    "text": "um so when we pull out with uh unstructured that's actually really easy so um just now um I just took one of",
    "start": "2371920",
    "end": "2378579"
  },
  {
    "text": "these uh links uh that we were processing uh for the example and you can see here we're going through and",
    "start": "2378579",
    "end": "2384460"
  },
  {
    "text": "categorizing um for example uh title and narrative text which is um just like the",
    "start": "2384460",
    "end": "2389500"
  },
  {
    "text": "body text um so once you get the the elements uh really easy to uh to filter down uh to",
    "start": "2389500",
    "end": "2395619"
  },
  {
    "text": "those two things um so we just grabbed the title here um so this one's about um fighting over a",
    "start": "2395619",
    "end": "2401500"
  },
  {
    "text": "docking spot and then you can easily patch together all of the uh the narrative text items to get the other",
    "start": "2401500",
    "end": "2407740"
  },
  {
    "text": "body of the article um and again the the power of what we're doing with unstructured is that um this",
    "start": "2407740",
    "end": "2413920"
  },
  {
    "text": "is showing it for an HTML article um but it could be a PDF it could be a",
    "start": "2413920",
    "end": "2419020"
  },
  {
    "text": "PowerPoint it could be a Word document or or whatever else and it would still work the same way um you know structured",
    "start": "2419020",
    "end": "2425260"
  },
  {
    "text": "group so um thanks for the question there just wanted to show an example of that uh real quick and then Matt can I ask a",
    "start": "2425260",
    "end": "2431619"
  },
  {
    "text": "question about this how do you have a recommended and I think we do some of this in LinkedIn maybe and if not we",
    "start": "2431619",
    "end": "2437079"
  },
  {
    "text": "should probably do more but do you have a recommended way for like patching all those elements together into chunks of",
    "start": "2437079",
    "end": "2442960"
  },
  {
    "text": "text that you'd store in a vector database yeah so well we're actually uh working on um some stuff around that um",
    "start": "2442960",
    "end": "2449380"
  },
  {
    "text": "right now so like traditionally like we'd kind of like pull out um there's like kind of two moods to do",
    "start": "2449380",
    "end": "2454540"
  },
  {
    "text": "it in outline chain right now like one is um what's called single Moon that kind of grabs the whole document and and",
    "start": "2454540",
    "end": "2460839"
  },
  {
    "text": "tosses it in there and then we have what's called elements mode which will be like hey like it'll kind of put the",
    "start": "2460839",
    "end": "2467140"
  },
  {
    "text": "the title will be an individual entry for example and um like a narrative text",
    "start": "2467140",
    "end": "2472920"
  },
  {
    "text": "paragraph would be uh would be a section that goes in there one of the things that we're working on now is coming up",
    "start": "2472920",
    "end": "2478780"
  },
  {
    "text": "with using the the element types to come up with um like good chunks",
    "start": "2478780",
    "end": "2484839"
  },
  {
    "text": "um and so like what what you would wind up with on the other side of that is like hey like here's a here's here here",
    "start": "2484839",
    "end": "2490180"
  },
  {
    "text": "we found like a title or like a subheading or something like that we're gonna get that and then all of the body text under that like that's going to",
    "start": "2490180",
    "end": "2496540"
  },
  {
    "text": "become uh one chunk and kind of like go through documents and kind of break it down smartly uh that way",
    "start": "2496540",
    "end": "2502000"
  },
  {
    "text": "um so um that's not it right now but it's something that um that we're actively working on and we should be",
    "start": "2502000",
    "end": "2507339"
  },
  {
    "text": "rolling out something and something on that um here in the next couple weeks nice I'm excited I'm excited to see that",
    "start": "2507339",
    "end": "2513280"
  },
  {
    "text": "I think that'll be I think that'll be really good um so there's some okay so taking a look",
    "start": "2513280",
    "end": "2520480"
  },
  {
    "text": "at the top question and then there's another question down below and it's basically about kind of like doing",
    "start": "2520480",
    "end": "2525640"
  },
  {
    "text": "retrieval at scale in production so like the top question what are what are some",
    "start": "2525640",
    "end": "2531160"
  },
  {
    "text": "of the best practices for retrieval for very large databases for example a database with 40 million news articles",
    "start": "2531160",
    "end": "2537339"
  },
  {
    "text": "and then there's also another question down below which is how would you go about deploying everything to be used",
    "start": "2537339",
    "end": "2543040"
  },
  {
    "text": "for production so I'm going to jump lump these two together to basically production retrieval um yeah on both the vector database as well",
    "start": "2543040",
    "end": "2550720"
  },
  {
    "text": "as the ingestion side like how do you guys how do you guys think about and tackle that yeah uh do you want me to go first yeah",
    "start": "2550720",
    "end": "2556780"
  },
  {
    "text": "I mean we spend a lot of time right now actually chroma's main engineering Focus right now is productionizing retrieval",
    "start": "2556780",
    "end": "2562240"
  },
  {
    "text": "for AI applications is what we're really pushing for and that scale question is",
    "start": "2562240",
    "end": "2567339"
  },
  {
    "text": "the number one thing we're addressing we're rolling out our distributed systems version very soon which will you know effortlessly scale our effortlessly",
    "start": "2567339",
    "end": "2574079"
  },
  {
    "text": "scale horizontally um 40 million entries even chunked into",
    "start": "2574079",
    "end": "2579940"
  },
  {
    "text": "I mean their news articles so they you maybe get three four chunks out of each one that gives you on the order of 120",
    "start": "2579940",
    "end": "2587440"
  },
  {
    "text": "million it's a very reasonable scale um that's actually not unreasonable",
    "start": "2587440",
    "end": "2592599"
  },
  {
    "text": "whatsoever now you can even process that scale in single node chroma today if you",
    "start": "2592599",
    "end": "2597700"
  },
  {
    "text": "have a machine with a lot of RAM because you're currently on single node chroma memory Bound in terms of retrieval",
    "start": "2597700",
    "end": "2603280"
  },
  {
    "text": "efficiency the reason for that is because the vector search part of the database runs in memory it's the way to",
    "start": "2603280",
    "end": "2610000"
  },
  {
    "text": "do approximate nearest neighbor efficiently so you know 120 million chunks from 40",
    "start": "2610000",
    "end": "2615339"
  },
  {
    "text": "million news articles isn't unreasonable even today uh you know just a just as a",
    "start": "2615339",
    "end": "2620800"
  },
  {
    "text": "rule of thumb on my 16 gig M1 Mac which roughly has half of its memory used by random applications at any given time",
    "start": "2620800",
    "end": "2627220"
  },
  {
    "text": "anyway I can do a million vectors no sweat um there's also other strategies which are",
    "start": "2627220",
    "end": "2633940"
  },
  {
    "text": "coming down the pipe which will make it more efficient but the answer here is basically use something that will scale",
    "start": "2633940",
    "end": "2639040"
  },
  {
    "text": "multiple nodes to multiple nodes easily horizontally if that's what you need otherwise just grab a single big node",
    "start": "2639040",
    "end": "2644200"
  },
  {
    "text": "and do it that way so their efficiencies to win here there's dimensionality reduction which obviously means you need",
    "start": "2644200",
    "end": "2649660"
  },
  {
    "text": "less memory because the vectors you're actually searching among um have fewer Dimensions just fewer",
    "start": "2649660",
    "end": "2655359"
  },
  {
    "text": "floats to store in memory there's also you know writing part of the index to the disk and only",
    "start": "2655359",
    "end": "2660760"
  },
  {
    "text": "keeping a part of it in memory and you can do that adaptively in response to query patterns but the answer today is is basically just like more RAM is good",
    "start": "2660760",
    "end": "2668980"
  },
  {
    "text": "um the on the part of deploying to production I mean there's a few there's a lot of considerations around here",
    "start": "2668980",
    "end": "2676839"
  },
  {
    "text": "um chroma is deployed in production as a single node service today you can run it you can even run it like as a",
    "start": "2676839",
    "end": "2682599"
  },
  {
    "text": "virtualized um distributed system or virtualized multi-tenant system by having several Docker containers running on a single",
    "start": "2682599",
    "end": "2688599"
  },
  {
    "text": "machine just as an example we've made it as straightforward as we can at the moment to deploy into",
    "start": "2688599",
    "end": "2694359"
  },
  {
    "text": "production but everything you know it's a little it's a little two Bare Bones we're definitely working on making that more robust you",
    "start": "2694359",
    "end": "2700480"
  },
  {
    "text": "have to sort of you know put it behind your own server do your own authentication do your own token passing around but we're making it easier all",
    "start": "2700480",
    "end": "2707619"
  },
  {
    "text": "the time um but the main consideration to like making this work is just make sure you have a machine with enough RAM",
    "start": "2707619",
    "end": "2713760"
  },
  {
    "text": "and what about on the data ingestion side Matt how are you guys tackling that",
    "start": "2714280",
    "end": "2719920"
  },
  {
    "text": "um so right now we're actually like really simple we have a like it's actually an open source container that you can you know kind of pull and run",
    "start": "2719920",
    "end": "2726099"
  },
  {
    "text": "wherever um wherever you'd like um like really kind of the uh the the heaviest lift from an infrastructure",
    "start": "2726099",
    "end": "2732099"
  },
  {
    "text": "standpoint is on um the PDFs and images where some of the uh the workloads or our model model based um and so",
    "start": "2732099",
    "end": "2740020"
  },
  {
    "text": "um like availability of gpus uh can help uh for those um but we found uh though also like",
    "start": "2740020",
    "end": "2745359"
  },
  {
    "text": "running this internally is that you can kind of run a lot of um like instances of it against like using CPS and in",
    "start": "2745359",
    "end": "2752500"
  },
  {
    "text": "parallel and that that's done uh that's done pretty well uh but um like from our president we've been trying to just kind",
    "start": "2752500",
    "end": "2758020"
  },
  {
    "text": "of like keep it simple you know kind of create these containers that you can take and put wherever you you'd like and",
    "start": "2758020",
    "end": "2764440"
  },
  {
    "text": "then once that's up and running I'm trying to make it so that it's document in structured Json I'm out",
    "start": "2764440",
    "end": "2771640"
  },
  {
    "text": "um and again we have like sort of the Integrations to make that uh easy to use with um stuff like my client chain too",
    "start": "2771640",
    "end": "2778060"
  },
  {
    "text": "foreign I see one question down uh uh below",
    "start": "2778060",
    "end": "2785800"
  },
  {
    "text": "which I think would be interesting to get both your guys's perspectives on any advice for retrieval on context",
    "start": "2785800",
    "end": "2792160"
  },
  {
    "text": "dependent questions or time-based information what were recent developments in XYZ or and I I'd maybe",
    "start": "2792160",
    "end": "2799000"
  },
  {
    "text": "categorize this as like you know when when the query itself isn't just something that you maybe want to do",
    "start": "2799000",
    "end": "2804040"
  },
  {
    "text": "semantic look upon but has some kind of like mention to some some other thing how do you guys think about that both",
    "start": "2804040",
    "end": "2809440"
  },
  {
    "text": "from like what should people be storing in the vector databases and how do you help people extract that information",
    "start": "2809440",
    "end": "2815380"
  },
  {
    "text": "during ingest time yeah so uh we've kind of put a lot of thought into that around it basically",
    "start": "2815380",
    "end": "2820839"
  },
  {
    "text": "kind of the metadata component of a documented pre-processing and so like",
    "start": "2820839",
    "end": "2826000"
  },
  {
    "text": "especially for stuff that's time dependent um kind of like extracting stuff like yeah like when the document was last",
    "start": "2826000",
    "end": "2831280"
  },
  {
    "text": "modified um if we can pull a date out of the document and like store that um that sort of thing uh is uh is really",
    "start": "2831280",
    "end": "2837280"
  },
  {
    "text": "helpful um and then like one of the things that we kind of have experimented around with internally is basically just kind of",
    "start": "2837280",
    "end": "2844060"
  },
  {
    "text": "um cosine similarity uh with like a Time Decay basically uh we found um was um",
    "start": "2844060",
    "end": "2850900"
  },
  {
    "text": "pretty simple and kind of like easy to implement to get stuff you know that that's like yeah kind of like bias",
    "start": "2850900",
    "end": "2857260"
  },
  {
    "text": "Horizon more recent information I'm sure there's more complex retrieval methods that you could do um to get it the same",
    "start": "2857260",
    "end": "2864040"
  },
  {
    "text": "thing uh but like really from like the data preprocessing perspective uh it's",
    "start": "2864040",
    "end": "2869200"
  },
  {
    "text": "about pulling all the information out of the article or via the document that you",
    "start": "2869200",
    "end": "2875319"
  },
  {
    "text": "can use for that conditioning step uh that's really the other Crux of it for us",
    "start": "2875319",
    "end": "2881940"
  },
  {
    "text": "anything on your end Anton in with regards to how you see people use metadata and and yeah",
    "start": "2884740",
    "end": "2891160"
  },
  {
    "text": "question actually around like time based um stuff has also come up for us one of",
    "start": "2891160",
    "end": "2896260"
  },
  {
    "text": "our hackers in Residence was working on things that need to update over time and so it's a question we've been thinking a",
    "start": "2896260",
    "end": "2901660"
  },
  {
    "text": "lot about and you start thinking about like journaling in the context of vector Rich based retrieval",
    "start": "2901660",
    "end": "2907599"
  },
  {
    "text": "I think metadata filtering is like important um I think that it definitely helps you",
    "start": "2907599",
    "end": "2912880"
  },
  {
    "text": "Scope your query one interesting thing that we've seen people doing is like basically using a model to do entity",
    "start": "2912880",
    "end": "2918460"
  },
  {
    "text": "extraction and then injecting that into as an additional metadata filter for query relevance again to me it feels like a short-term",
    "start": "2918460",
    "end": "2926319"
  },
  {
    "text": "stop Gap the model should figure that out for you it should know oh these are the relevant entities therefore I'll wait those parts of the vectors",
    "start": "2926319",
    "end": "2931720"
  },
  {
    "text": "appropriately but in the meantime I think it's interesting to experiment with this stuff um sort of my short my short response",
    "start": "2931720",
    "end": "2937780"
  },
  {
    "text": "here would be like I would try as much of this different stuff as possible and make it as lightweight as possible to do",
    "start": "2937780",
    "end": "2942880"
  },
  {
    "text": "so in your application and see how well that works the time thing where we're actively thinking about obviously chroma",
    "start": "2942880",
    "end": "2948819"
  },
  {
    "text": "supports like time fields in metadata and you can search inside time ranges",
    "start": "2948819",
    "end": "2955119"
  },
  {
    "text": "but when you think about the way you interact in natural language you don't say like oh I want to know information between the 26th of January or in blah",
    "start": "2955119",
    "end": "2962140"
  },
  {
    "text": "blah blah I want to know like it's that you're you want your system to know that your brother's birthday is coming up and you're within two weeks of",
    "start": "2962140",
    "end": "2968319"
  },
  {
    "text": "that so now I should you know prompt you to to remind you to get him a present for example right",
    "start": "2968319",
    "end": "2975220"
  },
  {
    "text": "um right now it just feels it just feels a little clunky it feels like we're not",
    "start": "2975220",
    "end": "2980380"
  },
  {
    "text": "we're either not trusting the model enough we haven't found the way to use them effectively yet or they just aren't",
    "start": "2980380",
    "end": "2986500"
  },
  {
    "text": "capable of doing this and so we're like patching over some of these gaps with with metadata",
    "start": "2986500",
    "end": "2992520"
  },
  {
    "text": "um long context Windows how are they changing how you see people do retrieval",
    "start": "2993940",
    "end": "3001260"
  },
  {
    "text": "so we haven't actually seen many people using long context with retrieval yet I think it's important to remember that",
    "start": "3001260",
    "end": "3007380"
  },
  {
    "text": "like long context doesn't actually mean you just stuff everything into the context possible",
    "start": "3007380",
    "end": "3013020"
  },
  {
    "text": "um there's a paper Harrison which I'll link you later which like again and what's interesting about the field today is there's a lot of folklore about what",
    "start": "3013020",
    "end": "3019680"
  },
  {
    "text": "works and what doesn't and eventually a paper comes out that like empirically supports or or refutes that conclusion but for a while",
    "start": "3019680",
    "end": "3027119"
  },
  {
    "text": "people have like intuitively known that longer context windows with a fixed detention mechanism kind of mean that the model can't really attend as",
    "start": "3027119",
    "end": "3033540"
  },
  {
    "text": "effectively to every token in that context window and it's been proven and it's also been proven that like distractors are really bad",
    "start": "3033540",
    "end": "3040680"
  },
  {
    "text": "um if you add like incidentally relevant information the model performance just falls off a cliff very quickly very",
    "start": "3040680",
    "end": "3046680"
  },
  {
    "text": "quickly for very little distraction um so what that means is that long context is great for retrieval",
    "start": "3046680",
    "end": "3053280"
  },
  {
    "text": "because it means we can fit more relevant information into the context and we can like appropriately present",
    "start": "3053280",
    "end": "3058980"
  },
  {
    "text": "that in a way that the model can understand and apprehend it but it means that the longer the context",
    "start": "3058980",
    "end": "3065339"
  },
  {
    "text": "the more information you can retrieve the more important relevance actually becomes because each time you retrieve a",
    "start": "3065339",
    "end": "3070980"
  },
  {
    "text": "distractor um the model performance falls off a cliff so it's not a question of just like tuning the K near the K and K",
    "start": "3070980",
    "end": "3077099"
  },
  {
    "text": "nearest neighbors to be a hundred instead of ten and just shoving it all in there you have to be very very",
    "start": "3077099",
    "end": "3082200"
  },
  {
    "text": "careful with relevance and that's something that we're that we're looking at right now I personally I think longer context numbers are great for retrieval",
    "start": "3082200",
    "end": "3087240"
  },
  {
    "text": "it means we no longer like cut off what we retrieve anymore and on this note one question I have for",
    "start": "3087240",
    "end": "3094920"
  },
  {
    "text": "Matt is like are you seeing uh basically with longer context Windows",
    "start": "3094920",
    "end": "3100500"
  },
  {
    "text": "does that change chunking at all like are now people doing longer chunks because they can maybe put longer chunks",
    "start": "3100500",
    "end": "3106079"
  },
  {
    "text": "in or do longer chunks kind of distract us Anton was saying and they they make the they make the retrieval stuff like",
    "start": "3106079",
    "end": "3112140"
  },
  {
    "text": "worse so how easy people or how are you seeing people balance that",
    "start": "3112140",
    "end": "3117180"
  },
  {
    "text": "yeah I think there's also kind of like the cost consideration too like if you're processing like a lot of",
    "start": "3117180",
    "end": "3122640"
  },
  {
    "text": "documents and it's like you're paying on like a per token basis and like throwing like a lot of stuff like into the",
    "start": "3122640",
    "end": "3128880"
  },
  {
    "text": "context window can get like more expensive um too so um I mean I think like with like the",
    "start": "3128880",
    "end": "3135059"
  },
  {
    "text": "models with like longer context Windows like coming out like I think we're seeing like that become more common",
    "start": "3135059",
    "end": "3141420"
  },
  {
    "text": "um but like like there's still kind of a use case for kind of getting that you're sort of like more targeted context",
    "start": "3141420",
    "end": "3148140"
  },
  {
    "text": "um as well if I put the reasons like Anton said and then just kind of like for similar kind of like nuts and volts is like you know like",
    "start": "3148140",
    "end": "3153720"
  },
  {
    "text": "if you're processing a lot of documents like you're you're going to want to be kind of careful about um yeah like how much your um you're",
    "start": "3153720",
    "end": "3160380"
  },
  {
    "text": "kind of how many tokens you're putting in the uh model and that sort of thing um if you're using one of the hosted ones",
    "start": "3160380",
    "end": "3167660"
  },
  {
    "text": "all right let's let's maybe um and depending on how long the answer to this one is on on a fun one",
    "start": "3168839",
    "end": "3176040"
  },
  {
    "text": "um which is basically going back to like oh you know different types of retrieval what do you think about combining graph",
    "start": "3176040",
    "end": "3181920"
  },
  {
    "text": "retrieval or what do you think about knowledge graph retrieval combined with Vector retrieval",
    "start": "3181920",
    "end": "3187559"
  },
  {
    "text": "um is is this a viable solution will everything in the long run end up happening kind of like in the embedding",
    "start": "3187559",
    "end": "3193380"
  },
  {
    "text": "space with Transformations and so this this is actually the same as like just up waiting certain tokens how do you",
    "start": "3193380",
    "end": "3200400"
  },
  {
    "text": "think about this app yeah interesting question um this kind of idea of like overlaying a",
    "start": "3200400",
    "end": "3205980"
  },
  {
    "text": "Knowledge Graph we've seen we've seen a little bit of this used as a re-ranking strategy so you find like related",
    "start": "3205980",
    "end": "3212280"
  },
  {
    "text": "entities through the knowledge graph after you've you've performed the semantic search I think there's something here",
    "start": "3212280",
    "end": "3218160"
  },
  {
    "text": "um natively speaking um embeddings you know vanilla embedding",
    "start": "3218160",
    "end": "3225240"
  },
  {
    "text": "models contrastively pre-trained embedding models don't encode relationships between entities",
    "start": "3225240",
    "end": "3230640"
  },
  {
    "text": "for you right it has no knowledge of like how you relate the two concepts of car and bird right but you know that",
    "start": "3230640",
    "end": "3237960"
  },
  {
    "text": "might be relevant to your particular task and application um one argument could be made is okay",
    "start": "3237960",
    "end": "3243540"
  },
  {
    "text": "well that's just that's just a relevance hack and in your particular application what you need is to sort of fold in and",
    "start": "3243540",
    "end": "3250079"
  },
  {
    "text": "squeeze space such that the embeddings that you need actually end up closer together so entities that are related in",
    "start": "3250079",
    "end": "3256500"
  },
  {
    "text": "your knowledge graph should just appear you know similar under the vector search but I don't know if there's a good answer here and I think this is really",
    "start": "3256500",
    "end": "3262680"
  },
  {
    "text": "one of the questions how do we encode um how do we encode these like binary",
    "start": "3262680",
    "end": "3268380"
  },
  {
    "text": "relationships or how do we encode even these like weighted relationships and in in just AI in general right we",
    "start": "3268380",
    "end": "3275760"
  },
  {
    "text": "don't we don't have a good way of that it feels like um it feels like in many ways just using",
    "start": "3275760",
    "end": "3281700"
  },
  {
    "text": "um just using kind of the the training distribution as the target for the loss is like",
    "start": "3281700",
    "end": "3288900"
  },
  {
    "text": "fighting with one hand type behind our backs it doesn't encode any of this of the sort of logical relationships that",
    "start": "3288900",
    "end": "3294359"
  },
  {
    "text": "only encodes the statistical ones and ultimately of course we see the emergent Behavior",
    "start": "3294359",
    "end": "3299520"
  },
  {
    "text": "that and if you play with these models you probably have the same sense that I do you see these logical relationships",
    "start": "3299520",
    "end": "3305220"
  },
  {
    "text": "emerge as a consequence of the statistical Behavior but the actual logical relationship isn't there which",
    "start": "3305220",
    "end": "3312180"
  },
  {
    "text": "is what causes them to hallucinate for example like that's why hallucinations are hallucinations are statistically likely Generations that ignore the",
    "start": "3312180",
    "end": "3318359"
  },
  {
    "text": "logical or the grounded representation of the real world so knowledge graphs may be one way ultimately it comes down",
    "start": "3318359",
    "end": "3325020"
  },
  {
    "text": "like it comes down to this question of like how do we how do we represent information in a way that's legible to AI that it can work with it and I think",
    "start": "3325020",
    "end": "3331380"
  },
  {
    "text": "that that's a really Broad and interesting research question that everybody's working on",
    "start": "3331380",
    "end": "3336920"
  },
  {
    "text": "the value is definitely there on stuff like uh kind of like augmenting with like knowledge graphs and stuff like",
    "start": "3339559",
    "end": "3345059"
  },
  {
    "text": "that uh the difficulty is that knowledge graphs are like really difficult to construct so it's a lot easier to get up",
    "start": "3345059",
    "end": "3350400"
  },
  {
    "text": "and running with like a kind of like a vector search powered application so I think one of the things you might see is",
    "start": "3350400",
    "end": "3357300"
  },
  {
    "text": "kind of people use Vector store based applications as v0 to kind of like get",
    "start": "3357300",
    "end": "3364859"
  },
  {
    "text": "off the ground and then over time is the the systems which were uh you add in those sort of like higher cost like",
    "start": "3364859",
    "end": "3370260"
  },
  {
    "text": "components like like a knowledge graph that take a little bit more time to get uh to get up and running",
    "start": "3370260",
    "end": "3375960"
  },
  {
    "text": "um so um yeah like it definitely is like I'm definitely like interested to see um see see how that that evolves um but um",
    "start": "3375960",
    "end": "3382500"
  },
  {
    "text": "yeah like at the same time I think um you know it's it's hard to it's it's hard to replace kind of like the ease of getting up and running with kind of like",
    "start": "3382500",
    "end": "3388800"
  },
  {
    "text": "the um yeah kind of like the the current like rag type architecture",
    "start": "3388800",
    "end": "3394700"
  },
  {
    "text": "all right that seems like a great place to end it so with that thank you both for joining this was a lot of fun I",
    "start": "3395339",
    "end": "3401220"
  },
  {
    "text": "learned a lot so I I this is there these are always uh you know when I learn fun stuff those are the most fun webinars so",
    "start": "3401220",
    "end": "3407579"
  },
  {
    "text": "I appreciate you guys coming on and talking um yeah yeah yeah",
    "start": "3407579",
    "end": "3412880"
  },
  {
    "text": "thanks for having us yeah I look forward to sort of continuing to to invent and experiment",
    "start": "3412880",
    "end": "3419220"
  },
  {
    "text": "here yeah I really liked what you said about like you know it's uh the the best thing to do is just try a bunch of stuff",
    "start": "3419220",
    "end": "3424559"
  },
  {
    "text": "like I think we're in just that phase um yeah exciting we agree strongly agree",
    "start": "3424559",
    "end": "3429839"
  },
  {
    "text": "and like you know everyone everyone here on this webinar is building tools ultimately for experimentation I think",
    "start": "3429839",
    "end": "3435000"
  },
  {
    "text": "it's a big part of the way Karma builds things the way line chain builds things um and so just making it easy for people",
    "start": "3435000",
    "end": "3441599"
  },
  {
    "text": "to try this stuff I think is a big deal right now yeah and I think yeah and there's the fine you know everyone in",
    "start": "3441599",
    "end": "3447480"
  },
  {
    "text": "the audience should try out everything and you know yes thank you guys for tuning in hopefully you got some ideas",
    "start": "3447480",
    "end": "3453020"
  },
  {
    "text": "uh fun time fun time to be alive yeah thanks very much Harrison see you guys",
    "start": "3453020",
    "end": "3458520"
  },
  {
    "text": "thanks foreign",
    "start": "3458520",
    "end": "3463760"
  }
]