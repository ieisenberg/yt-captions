[
  {
    "text": "what's up everyone it's Brace from",
    "start": "560",
    "end": "1760"
  },
  {
    "text": "Langchain and I'm super excited today to",
    "start": "1760",
    "end": "3439"
  },
  {
    "text": "show off our new project LLM manager",
    "start": "3439",
    "end": "5680"
  },
  {
    "text": "this is a langraph workflow for handling",
    "start": "5680",
    "end": "8240"
  },
  {
    "text": "approval requests from users it uses",
    "start": "8240",
    "end": "10639"
  },
  {
    "text": "memories and human the loop to generate",
    "start": "10639",
    "end": "12960"
  },
  {
    "text": "these memories to then use in future",
    "start": "12960",
    "end": "15040"
  },
  {
    "text": "iterations so that it can learn and",
    "start": "15040",
    "end": "16400"
  },
  {
    "text": "improve over time we built it in a way",
    "start": "16400",
    "end": "18720"
  },
  {
    "text": "to be generic to work for a wide variety",
    "start": "18720",
    "end": "20800"
  },
  {
    "text": "of approval requests such as poll",
    "start": "20800",
    "end": "22880"
  },
  {
    "text": "request reviews right saying yes you can",
    "start": "22880",
    "end": "24560"
  },
  {
    "text": "merge this no you need to make changes",
    "start": "24560",
    "end": "26560"
  },
  {
    "text": "uh reimbursement requests hey I want to",
    "start": "26560",
    "end": "28480"
  },
  {
    "text": "attend the interrupt conference from",
    "start": "28480",
    "end": "29679"
  },
  {
    "text": "Langchain can I go um something like hey",
    "start": "29679",
    "end": "32480"
  },
  {
    "text": "I want to start this project right",
    "start": "32480",
    "end": "33840"
  },
  {
    "text": "project requests and so on it's set up",
    "start": "33840",
    "end": "36160"
  },
  {
    "text": "to work with any type of approval",
    "start": "36160",
    "end": "38000"
  },
  {
    "text": "requests as long as you can specify some",
    "start": "38000",
    "end": "40000"
  },
  {
    "text": "approval and rejection criteria this LM",
    "start": "40000",
    "end": "42719"
  },
  {
    "text": "manager can take that and then learn",
    "start": "42719",
    "end": "44559"
  },
  {
    "text": "over time based on the requests it sees",
    "start": "44559",
    "end": "46640"
  },
  {
    "text": "and the feedback it's able to get from",
    "start": "46640",
    "end": "48480"
  },
  {
    "text": "those requests from the human and update",
    "start": "48480",
    "end": "51200"
  },
  {
    "text": "its memory bank with different types of",
    "start": "51200",
    "end": "52879"
  },
  {
    "text": "memory such as reflections few shots and",
    "start": "52879",
    "end": "54719"
  },
  {
    "text": "so on to then be used in further runs",
    "start": "54719",
    "end": "57120"
  },
  {
    "text": "where it generates some reasoning and",
    "start": "57120",
    "end": "58640"
  },
  {
    "text": "makes these decisions on whether to",
    "start": "58640",
    "end": "59920"
  },
  {
    "text": "approve or reject so if we look at this",
    "start": "59920",
    "end": "61760"
  },
  {
    "text": "high level diagram we can see the way it",
    "start": "61760",
    "end": "63199"
  },
  {
    "text": "works is we take in the user's request",
    "start": "63199",
    "end": "65198"
  },
  {
    "text": "we then reason about that request using",
    "start": "65199",
    "end": "66960"
  },
  {
    "text": "our memories and examples and whatever",
    "start": "66960",
    "end": "68880"
  },
  {
    "text": "else you want to store as your memories",
    "start": "68880",
    "end": "70960"
  },
  {
    "text": "pass that to an LM and we have it",
    "start": "70960",
    "end": "72640"
  },
  {
    "text": "generate a reasoning report we do this",
    "start": "72640",
    "end": "74560"
  },
  {
    "text": "before generating an answer because we",
    "start": "74560",
    "end": "76080"
  },
  {
    "text": "want to say \"Hey LM here are some",
    "start": "76080",
    "end": "78320"
  },
  {
    "text": "memories some context some uh the user's",
    "start": "78320",
    "end": "80880"
  },
  {
    "text": "request reason about whether or not you",
    "start": "80880",
    "end": "82880"
  },
  {
    "text": "should approve or reject this based on",
    "start": "82880",
    "end": "84640"
  },
  {
    "text": "this reasoning criteria or approval",
    "start": "84640",
    "end": "86799"
  },
  {
    "text": "criteria and these memories we've",
    "start": "86799",
    "end": "88799"
  },
  {
    "text": "generated from previous runs don't make",
    "start": "88799",
    "end": "91040"
  },
  {
    "text": "a decision just think about the pros and",
    "start": "91040",
    "end": "92720"
  },
  {
    "text": "the cons and generate a report then in a",
    "start": "92720",
    "end": "95119"
  },
  {
    "text": "second step we can say \"Hey LM here's",
    "start": "95119",
    "end": "97360"
  },
  {
    "text": "this report on whether or not to approve",
    "start": "97360",
    "end": "99119"
  },
  {
    "text": "or reject uh this user's request give me",
    "start": "99119",
    "end": "102000"
  },
  {
    "text": "an approval rejection and then an",
    "start": "102000",
    "end": "103439"
  },
  {
    "text": "explanation behind that.\" And we ask for",
    "start": "103439",
    "end": "105439"
  },
  {
    "text": "an explanation because we then interrupt",
    "start": "105439",
    "end": "106799"
  },
  {
    "text": "and wait for some human feedback here",
    "start": "106799",
    "end": "108640"
  },
  {
    "text": "the human can either accept what what",
    "start": "108640",
    "end": "110320"
  },
  {
    "text": "the LM generated right they say \"Yep you",
    "start": "110320",
    "end": "112479"
  },
  {
    "text": "want we want to accept this and I agree",
    "start": "112479",
    "end": "114079"
  },
  {
    "text": "with your explanation.\" It can totally",
    "start": "114079",
    "end": "116000"
  },
  {
    "text": "rewrite everything right it can say \"No",
    "start": "116000",
    "end": "117600"
  },
  {
    "text": "we want to reject this or we want to",
    "start": "117600",
    "end": "119040"
  },
  {
    "text": "accept instead of rejecting and change",
    "start": "119040",
    "end": "120880"
  },
  {
    "text": "the explanation or it can just change",
    "start": "120880",
    "end": "122799"
  },
  {
    "text": "the explanation right let's say uh the",
    "start": "122799",
    "end": "124799"
  },
  {
    "text": "user does want to approve this request",
    "start": "124799",
    "end": "126880"
  },
  {
    "text": "but the explanation the LM generated is",
    "start": "126880",
    "end": "128959"
  },
  {
    "text": "flawed it can then edit that and we can",
    "start": "128959",
    "end": "131120"
  },
  {
    "text": "take these edits right either both",
    "start": "131120",
    "end": "132959"
  },
  {
    "text": "fields or just one and perform some sort",
    "start": "132959",
    "end": "135440"
  },
  {
    "text": "of reflection step on that uh by default",
    "start": "135440",
    "end": "137840"
  },
  {
    "text": "this graph tries to extract some",
    "start": "137840",
    "end": "139440"
  },
  {
    "text": "memories right it first uh generates a",
    "start": "139440",
    "end": "141680"
  },
  {
    "text": "reflection summary saying where did",
    "start": "141680",
    "end": "143599"
  },
  {
    "text": "things go wrong and then from there it",
    "start": "143599",
    "end": "145280"
  },
  {
    "text": "says okay take this uh summary report on",
    "start": "145280",
    "end": "148160"
  },
  {
    "text": "how you messed up and extract some core",
    "start": "148160",
    "end": "150560"
  },
  {
    "text": "memories some core reflections to then",
    "start": "150560",
    "end": "152319"
  },
  {
    "text": "store and use uh in next iterations as",
    "start": "152319",
    "end": "155519"
  },
  {
    "text": "these memories are examples so the",
    "start": "155519",
    "end": "157519"
  },
  {
    "text": "really important part about this graph",
    "start": "157519",
    "end": "158959"
  },
  {
    "text": "is the way we generate and store",
    "start": "158959",
    "end": "160560"
  },
  {
    "text": "memories and the different types of",
    "start": "160560",
    "end": "161760"
  },
  {
    "text": "memories that we store in order to allow",
    "start": "161760",
    "end": "164239"
  },
  {
    "text": "the LM manager to improve and learn over",
    "start": "164239",
    "end": "166879"
  },
  {
    "text": "time based on this human feedback for",
    "start": "166879",
    "end": "169280"
  },
  {
    "text": "that very specific task so that is how",
    "start": "169280",
    "end": "172000"
  },
  {
    "text": "it works at a high level now we're going",
    "start": "172000",
    "end": "173519"
  },
  {
    "text": "to jump into the code and see all the",
    "start": "173519",
    "end": "175440"
  },
  {
    "text": "nitty-gritty details first things first",
    "start": "175440",
    "end": "177760"
  },
  {
    "text": "you want to go to the LM manager GitHub",
    "start": "177760",
    "end": "179840"
  },
  {
    "text": "repo and copy the git URL clone that and",
    "start": "179840",
    "end": "183200"
  },
  {
    "text": "open it up in your IDE once that's done",
    "start": "183200",
    "end": "185040"
  },
  {
    "text": "we can go into the very first file we're",
    "start": "185040",
    "end": "186319"
  },
  {
    "text": "going to look at and talk about all the",
    "start": "186319",
    "end": "187760"
  },
  {
    "text": "individual steps so once it's open your",
    "start": "187760",
    "end": "190560"
  },
  {
    "text": "ID you're going to want to go to the LM",
    "start": "190560",
    "end": "192480"
  },
  {
    "text": "manager directory and then the index.ts",
    "start": "192480",
    "end": "194720"
  },
  {
    "text": "file here is the main top level graph",
    "start": "194720",
    "end": "196800"
  },
  {
    "text": "for the LM manager where we call all the",
    "start": "196800",
    "end": "199120"
  },
  {
    "text": "nodes we see on the left so as you can",
    "start": "199120",
    "end": "201040"
  },
  {
    "text": "see the very first step is the reasoning",
    "start": "201040",
    "end": "202879"
  },
  {
    "text": "graph we're calling this as a subgraph",
    "start": "202879",
    "end": "205120"
  },
  {
    "text": "um and right now it's just a single node",
    "start": "205120",
    "end": "206879"
  },
  {
    "text": "initial reasoning but you could totally",
    "start": "206879",
    "end": "208959"
  },
  {
    "text": "expand this to be much more complex uh",
    "start": "208959",
    "end": "211280"
  },
  {
    "text": "fetch in different data sources do",
    "start": "211280",
    "end": "212879"
  },
  {
    "text": "different reasoning about different",
    "start": "212879",
    "end": "213920"
  },
  {
    "text": "parts of the request and so on before",
    "start": "213920",
    "end": "215920"
  },
  {
    "text": "generating that final report so we",
    "start": "215920",
    "end": "217680"
  },
  {
    "text": "implement it as a subgraph just to make",
    "start": "217680",
    "end": "219120"
  },
  {
    "text": "it easier to expand in the future right",
    "start": "219120",
    "end": "221040"
  },
  {
    "text": "now we just have this initial reasoning",
    "start": "221040",
    "end": "222480"
  },
  {
    "text": "node if we open this we can see there's",
    "start": "222480",
    "end": "224480"
  },
  {
    "text": "a prompt just calls the model uh passing",
    "start": "224480",
    "end": "226959"
  },
  {
    "text": "in the system prompt and then the user's",
    "start": "226959",
    "end": "228720"
  },
  {
    "text": "query this is the request and it returns",
    "start": "228720",
    "end": "230560"
  },
  {
    "text": "that let's look at how it does this so",
    "start": "230560",
    "end": "232480"
  },
  {
    "text": "first we call this build context",
    "start": "232480",
    "end": "234080"
  },
  {
    "text": "function if we open this up we can see",
    "start": "234080",
    "end": "236400"
  },
  {
    "text": "what it's doing is it's searching for",
    "start": "236400",
    "end": "237680"
  },
  {
    "text": "fshot examples and getting the",
    "start": "237680",
    "end": "239200"
  },
  {
    "text": "reflections for the fshot examples every",
    "start": "239200",
    "end": "241760"
  },
  {
    "text": "time the LLM uh takes in this request",
    "start": "241760",
    "end": "244239"
  },
  {
    "text": "goes through the whole flow and finishes",
    "start": "244239",
    "end": "246239"
  },
  {
    "text": "as long as the request was um approved",
    "start": "246239",
    "end": "248640"
  },
  {
    "text": "or rejected right and not just outright",
    "start": "248640",
    "end": "250560"
  },
  {
    "text": "ignored we save that in the langraph",
    "start": "250560",
    "end": "253360"
  },
  {
    "text": "store we're saving the initial request",
    "start": "253360",
    "end": "256239"
  },
  {
    "text": "the explanation and the final answer",
    "start": "256239",
    "end": "258320"
  },
  {
    "text": "that the LM generated we then embed all",
    "start": "258320",
    "end": "261840"
  },
  {
    "text": "of the queries so the all the requests",
    "start": "261840",
    "end": "264479"
  },
  {
    "text": "and then we can use semantic search to",
    "start": "264479",
    "end": "266479"
  },
  {
    "text": "fetch semantically similar past requests",
    "start": "266479",
    "end": "269120"
  },
  {
    "text": "from our fshot examples to ensure that",
    "start": "269120",
    "end": "271759"
  },
  {
    "text": "all of the fshot examples we provide for",
    "start": "271759",
    "end": "273840"
  },
  {
    "text": "this new request are going to be similar",
    "start": "273840",
    "end": "276000"
  },
  {
    "text": "to um what the actual request is uh",
    "start": "276000",
    "end": "279040"
  },
  {
    "text": "because we don't want to just be pulling",
    "start": "279040",
    "end": "280479"
  },
  {
    "text": "random fshot examples that aren't",
    "start": "280479",
    "end": "282160"
  },
  {
    "text": "relevant at all to this new request",
    "start": "282160",
    "end": "283520"
  },
  {
    "text": "because that's not going to help the",
    "start": "283520",
    "end": "284479"
  },
  {
    "text": "model so we pull semantically similar",
    "start": "284479",
    "end": "286240"
  },
  {
    "text": "requests so that the examples are highly",
    "start": "286240",
    "end": "288639"
  },
  {
    "text": "relevant to what the new request is you",
    "start": "288639",
    "end": "291600"
  },
  {
    "text": "could also expand on this to say right",
    "start": "291600",
    "end": "293600"
  },
  {
    "text": "now we pull 10 total requests but you",
    "start": "293600",
    "end": "295680"
  },
  {
    "text": "might want to pull five accepted",
    "start": "295680",
    "end": "297680"
  },
  {
    "text": "requests and five rejected requests that",
    "start": "297680",
    "end": "299440"
  },
  {
    "text": "are all semantically similar um which is",
    "start": "299440",
    "end": "301199"
  },
  {
    "text": "once again why we made this reasoning a",
    "start": "301199",
    "end": "303440"
  },
  {
    "text": "subgraph because there's a lot of ways",
    "start": "303440",
    "end": "304880"
  },
  {
    "text": "you can expand on this for reflections",
    "start": "304880",
    "end": "307759"
  },
  {
    "text": "super simple just get the list of",
    "start": "307759",
    "end": "309919"
  },
  {
    "text": "reflections from the store and return",
    "start": "309919",
    "end": "311360"
  },
  {
    "text": "that so that is our context builder once",
    "start": "311360",
    "end": "314880"
  },
  {
    "text": "we have our fshot examples and our",
    "start": "314880",
    "end": "316240"
  },
  {
    "text": "reflections we're going to format that",
    "start": "316240",
    "end": "317919"
  },
  {
    "text": "into the prompt basically just",
    "start": "317919",
    "end": "319440"
  },
  {
    "text": "stringifying it and wrapping it with XML",
    "start": "319440",
    "end": "321680"
  },
  {
    "text": "tags like we see here and pasting it all",
    "start": "321680",
    "end": "324000"
  },
  {
    "text": "in we see we have all the examples and",
    "start": "324000",
    "end": "325600"
  },
  {
    "text": "then for each uh individual example we",
    "start": "325600",
    "end": "328639"
  },
  {
    "text": "map that with the example the uh input",
    "start": "328639",
    "end": "331039"
  },
  {
    "text": "the request the explanation and the",
    "start": "331039",
    "end": "332639"
  },
  {
    "text": "answer uh the reflections and then we",
    "start": "332639",
    "end": "334400"
  },
  {
    "text": "also have this step for the approval and",
    "start": "334400",
    "end": "336080"
  },
  {
    "text": "rejecting rejection criteria these are",
    "start": "336080",
    "end": "338479"
  },
  {
    "text": "two configurable fields which you can",
    "start": "338479",
    "end": "340880"
  },
  {
    "text": "attach to your model just saying like at",
    "start": "340880",
    "end": "342720"
  },
  {
    "text": "a high level these are the kind of",
    "start": "342720",
    "end": "343840"
  },
  {
    "text": "requests you should accept or you should",
    "start": "343840",
    "end": "345759"
  },
  {
    "text": "expect and then these are the criteria",
    "start": "345759",
    "end": "348400"
  },
  {
    "text": "for approving and rejecting the model uh",
    "start": "348400",
    "end": "350479"
  },
  {
    "text": "these are all tied to assistance same",
    "start": "350479",
    "end": "352320"
  },
  {
    "text": "with the fshot examples and the",
    "start": "352320",
    "end": "354000"
  },
  {
    "text": "reflection so you can have the same",
    "start": "354000",
    "end": "356000"
  },
  {
    "text": "graph right deployed on Langraph",
    "start": "356000",
    "end": "357440"
  },
  {
    "text": "platform uh but you could have 10",
    "start": "357440",
    "end": "359039"
  },
  {
    "text": "different assistants for 10 different",
    "start": "359039",
    "end": "360400"
  },
  {
    "text": "approval pipelines like say pull request",
    "start": "360400",
    "end": "362639"
  },
  {
    "text": "reviews or new project requests or",
    "start": "362639",
    "end": "365919"
  },
  {
    "text": "expense requests and so on and they can",
    "start": "365919",
    "end": "367680"
  },
  {
    "text": "all be tied and namespaced these",
    "start": "367680",
    "end": "369520"
  },
  {
    "text": "specific assistance and that way you",
    "start": "369520",
    "end": "371360"
  },
  {
    "text": "won't have to worry about you know",
    "start": "371360",
    "end": "372960"
  },
  {
    "text": "having this uh go through and approve or",
    "start": "372960",
    "end": "375120"
  },
  {
    "text": "reject a pull request and then get",
    "start": "375120",
    "end": "376880"
  },
  {
    "text": "another request for approving or",
    "start": "376880",
    "end": "378400"
  },
  {
    "text": "rejecting an expense because those are",
    "start": "378400",
    "end": "379840"
  },
  {
    "text": "totally different things so you're going",
    "start": "379840",
    "end": "381199"
  },
  {
    "text": "to want to break that into uh different",
    "start": "381199",
    "end": "383440"
  },
  {
    "text": "areas so these are all namespaced by",
    "start": "383440",
    "end": "386120"
  },
  {
    "text": "assistant and that's how we format the",
    "start": "386120",
    "end": "388400"
  },
  {
    "text": "prompt so once we formatted that we get",
    "start": "388400",
    "end": "390560"
  },
  {
    "text": "the model uh you can configure this",
    "start": "390560",
    "end": "392400"
  },
  {
    "text": "model to be anything you want anthropic",
    "start": "392400",
    "end": "394240"
  },
  {
    "text": "or open AAI we then invoke it with the",
    "start": "394240",
    "end": "395919"
  },
  {
    "text": "system prompt and then the query and",
    "start": "395919",
    "end": "397680"
  },
  {
    "text": "return the generated reasoning which is",
    "start": "397680",
    "end": "399919"
  },
  {
    "text": "this reasoning report we ask it to",
    "start": "399919",
    "end": "401440"
  },
  {
    "text": "generate and the prompt context this is",
    "start": "401440",
    "end": "403600"
  },
  {
    "text": "the full uh formatted prompt containing",
    "start": "403600",
    "end": "406400"
  },
  {
    "text": "the approval rejection reflections and",
    "start": "406400",
    "end": "408880"
  },
  {
    "text": "fshot example and this is because we're",
    "start": "408880",
    "end": "411039"
  },
  {
    "text": "going to want to use this in the final",
    "start": "411039",
    "end": "412400"
  },
  {
    "text": "answer step and we don't want to have to",
    "start": "412400",
    "end": "413680"
  },
  {
    "text": "redo all of these different",
    "start": "413680",
    "end": "415400"
  },
  {
    "text": "computations so after we generate this",
    "start": "415400",
    "end": "417520"
  },
  {
    "text": "reasoning our subgraph ends and it",
    "start": "417520",
    "end": "419280"
  },
  {
    "text": "returns back to the top level LM manager",
    "start": "419280",
    "end": "421919"
  },
  {
    "text": "graph we then route our reasoning step",
    "start": "421919",
    "end": "424560"
  },
  {
    "text": "to the sorry we then route our reasoning",
    "start": "424560",
    "end": "427199"
  },
  {
    "text": "step to the final answer uh which takes",
    "start": "427199",
    "end": "430000"
  },
  {
    "text": "in as we can see takes in the generated",
    "start": "430000",
    "end": "433280"
  },
  {
    "text": "reasoning the query and the prompt",
    "start": "433280",
    "end": "435280"
  },
  {
    "text": "context and it says your highly advanced",
    "start": "435280",
    "end": "438160"
  },
  {
    "text": "uh LM or AI manager tasked with",
    "start": "438160",
    "end": "440240"
  },
  {
    "text": "approving or rejecting one of your",
    "start": "440240",
    "end": "441520"
  },
  {
    "text": "employees requests here's their request",
    "start": "441520",
    "end": "443919"
  },
  {
    "text": "u we then state some uh documentation",
    "start": "443919",
    "end": "446400"
  },
  {
    "text": "right here is uh the different pieces of",
    "start": "446400",
    "end": "449280"
  },
  {
    "text": "context you're going to be provided",
    "start": "449280",
    "end": "450479"
  },
  {
    "text": "here's the reasoning you just generated",
    "start": "450479",
    "end": "452000"
  },
  {
    "text": "Use all of it to ground your final",
    "start": "452000",
    "end": "453360"
  },
  {
    "text": "decision and then we restate the",
    "start": "453360",
    "end": "455280"
  },
  {
    "text": "employees request this is actually from",
    "start": "455280",
    "end": "457120"
  },
  {
    "text": "an OpenAI paper they just put out on",
    "start": "457120",
    "end": "458800"
  },
  {
    "text": "prompting where they say you should",
    "start": "458800",
    "end": "460720"
  },
  {
    "text": "always include the request at least in",
    "start": "460720",
    "end": "462720"
  },
  {
    "text": "the beginning and ideally at the end as",
    "start": "462720",
    "end": "465520"
  },
  {
    "text": "well and never just at the end so we",
    "start": "465520",
    "end": "467199"
  },
  {
    "text": "state the request at the beginning the",
    "start": "467199",
    "end": "468560"
  },
  {
    "text": "LM takes that in we then give it all the",
    "start": "468560",
    "end": "470720"
  },
  {
    "text": "context it should use it takes all that",
    "start": "470720",
    "end": "472639"
  },
  {
    "text": "in and then we restate the request once",
    "start": "472639",
    "end": "474639"
  },
  {
    "text": "again just to make sure it's top of mind",
    "start": "474639",
    "end": "476479"
  },
  {
    "text": "in the LM request",
    "start": "476479",
    "end": "478639"
  },
  {
    "text": "we then have a tool which is bound to",
    "start": "478639",
    "end": "480400"
  },
  {
    "text": "the LM and we're forcing it to call that",
    "start": "480400",
    "end": "482319"
  },
  {
    "text": "with an explanation and status field we",
    "start": "482319",
    "end": "484560"
  },
  {
    "text": "want to make sure the explanation field",
    "start": "484560",
    "end": "486160"
  },
  {
    "text": "is before the status field and that way",
    "start": "486160",
    "end": "488080"
  },
  {
    "text": "it's going to generate its explanation",
    "start": "488080",
    "end": "489840"
  },
  {
    "text": "first before generating a status uh",
    "start": "489840",
    "end": "492400"
  },
  {
    "text": "that's because we don't want it to",
    "start": "492400",
    "end": "493680"
  },
  {
    "text": "generate approved and then try and",
    "start": "493680",
    "end": "495680"
  },
  {
    "text": "explain that right so we want it to",
    "start": "495680",
    "end": "497360"
  },
  {
    "text": "explain its reasoning and then generate",
    "start": "497360",
    "end": "499520"
  },
  {
    "text": "the status and then we return that in",
    "start": "499520",
    "end": "501840"
  },
  {
    "text": "the answer field uh which is you know",
    "start": "501840",
    "end": "503759"
  },
  {
    "text": "just containing the explanation and",
    "start": "503759",
    "end": "505319"
  },
  {
    "text": "status so after generating the final",
    "start": "505319",
    "end": "507599"
  },
  {
    "text": "answer we go to the human node which is",
    "start": "507599",
    "end": "509440"
  },
  {
    "text": "going to interrupt uh I'll open that up",
    "start": "509440",
    "end": "511120"
  },
  {
    "text": "in the agent inbox in just a second and",
    "start": "511120",
    "end": "513360"
  },
  {
    "text": "the user can either ignore accept it or",
    "start": "513360",
    "end": "517039"
  },
  {
    "text": "edit it um and then we can take that and",
    "start": "517039",
    "end": "519279"
  },
  {
    "text": "process those responses and either route",
    "start": "519279",
    "end": "521839"
  },
  {
    "text": "to the end or go to the reflection step",
    "start": "521839",
    "end": "524720"
  },
  {
    "text": "we go to the end if no changes were made",
    "start": "524720",
    "end": "526800"
  },
  {
    "text": "right if the LM got it right in the",
    "start": "526800",
    "end": "528240"
  },
  {
    "text": "first try just finish we're all good if",
    "start": "528240",
    "end": "531120"
  },
  {
    "text": "the user did edit something then we're",
    "start": "531120",
    "end": "532880"
  },
  {
    "text": "going to want to go to the reflection",
    "start": "532880",
    "end": "533920"
  },
  {
    "text": "step and see where did we go wrong how",
    "start": "533920",
    "end": "535839"
  },
  {
    "text": "can we fix that for future processes so",
    "start": "535839",
    "end": "538160"
  },
  {
    "text": "once an LLM interrupts we can see it's",
    "start": "538160",
    "end": "540560"
  },
  {
    "text": "opened in the agent inbox right here we",
    "start": "540560",
    "end": "542399"
  },
  {
    "text": "have docs in the read me on how to set",
    "start": "542399",
    "end": "543920"
  },
  {
    "text": "up the agent inbox with the LM manager",
    "start": "543920",
    "end": "545920"
  },
  {
    "text": "but let's say you get a request you can",
    "start": "545920",
    "end": "547519"
  },
  {
    "text": "then open up that in the agent inbox and",
    "start": "547519",
    "end": "549519"
  },
  {
    "text": "we can see the LM decided should be",
    "start": "549519",
    "end": "550880"
  },
  {
    "text": "approved so the request was I'd like to",
    "start": "550880",
    "end": "553040"
  },
  {
    "text": "explore integrating AI into our customer",
    "start": "553040",
    "end": "555360"
  },
  {
    "text": "support chatbot and so on the LM said",
    "start": "555360",
    "end": "557680"
  },
  {
    "text": "yep I'm going to approve that and then",
    "start": "557680",
    "end": "559120"
  },
  {
    "text": "gave this explanation if I disagree with",
    "start": "559120",
    "end": "561440"
  },
  {
    "text": "this right I can say rejected and then",
    "start": "561440",
    "end": "564399"
  },
  {
    "text": "say we don't want any AI features in our",
    "start": "564399",
    "end": "571320"
  },
  {
    "text": "product now if I go and submit this",
    "start": "571320",
    "end": "574080"
  },
  {
    "text": "we're going to see it notice that I made",
    "start": "574080",
    "end": "575920"
  },
  {
    "text": "some changes to the LM's request and",
    "start": "575920",
    "end": "577839"
  },
  {
    "text": "it's going to route to the reflection",
    "start": "577839",
    "end": "579040"
  },
  {
    "text": "step and perform some reflection",
    "start": "579040",
    "end": "581040"
  },
  {
    "text": "comparing my response to the original",
    "start": "581040",
    "end": "583440"
  },
  {
    "text": "generated response so I'm going to",
    "start": "583440",
    "end": "585200"
  },
  {
    "text": "submit we can see response submitted",
    "start": "585200",
    "end": "586959"
  },
  {
    "text": "successfully it then routes to the full",
    "start": "586959",
    "end": "589040"
  },
  {
    "text": "reflection step right because I updated",
    "start": "589040",
    "end": "590880"
  },
  {
    "text": "both fields and not just one of them",
    "start": "590880",
    "end": "592880"
  },
  {
    "text": "once this is done generating this could",
    "start": "592880",
    "end": "594480"
  },
  {
    "text": "take a second because we're using a",
    "start": "594480",
    "end": "595519"
  },
  {
    "text": "thinking model it's going to then go to",
    "start": "595519",
    "end": "597760"
  },
  {
    "text": "a step which takes the thoughts on the",
    "start": "597760",
    "end": "599920"
  },
  {
    "text": "explanation and actually extracts the",
    "start": "599920",
    "end": "602240"
  },
  {
    "text": "specific reflection uh pieces from that",
    "start": "602240",
    "end": "604800"
  },
  {
    "text": "which I'll go into in a second right we",
    "start": "604800",
    "end": "606160"
  },
  {
    "text": "see the extract reflection step right",
    "start": "606160",
    "end": "607920"
  },
  {
    "text": "there and then finish it'll add it to",
    "start": "607920",
    "end": "609760"
  },
  {
    "text": "the store um and use that in future",
    "start": "609760",
    "end": "611760"
  },
  {
    "text": "requests so now that's done let's go",
    "start": "611760",
    "end": "613760"
  },
  {
    "text": "back to our IDE and let's open up the",
    "start": "613760",
    "end": "616480"
  },
  {
    "text": "reflection graph to see exactly how it",
    "start": "616480",
    "end": "618399"
  },
  {
    "text": "works so we have it open right here so",
    "start": "618399",
    "end": "620959"
  },
  {
    "text": "once we're inside the reflection graph",
    "start": "620959",
    "end": "622399"
  },
  {
    "text": "we can see that the very first node is",
    "start": "622399",
    "end": "624000"
  },
  {
    "text": "this route reflection node inside of",
    "start": "624000",
    "end": "625920"
  },
  {
    "text": "this we basically check to see if the",
    "start": "625920",
    "end": "627360"
  },
  {
    "text": "change type was only the explanation um",
    "start": "627360",
    "end": "629680"
  },
  {
    "text": "and if it's only the explanation was",
    "start": "629680",
    "end": "631040"
  },
  {
    "text": "changed we want to route to the",
    "start": "631040",
    "end": "632079"
  },
  {
    "text": "explanation reflection uh but if both",
    "start": "632079",
    "end": "634399"
  },
  {
    "text": "were changed we want to route to full",
    "start": "634399",
    "end": "635680"
  },
  {
    "text": "reflection we have two different nodes",
    "start": "635680",
    "end": "637680"
  },
  {
    "text": "for this because if you only change the",
    "start": "637680",
    "end": "639839"
  },
  {
    "text": "explanation but the overall answer right",
    "start": "639839",
    "end": "641839"
  },
  {
    "text": "approved or rejected was correct we",
    "start": "641839",
    "end": "644000"
  },
  {
    "text": "don't want to tell the LM to reflect on",
    "start": "644000",
    "end": "645680"
  },
  {
    "text": "everything but instead we want to say",
    "start": "645680",
    "end": "647120"
  },
  {
    "text": "hey you got the right answer but your",
    "start": "647120",
    "end": "649040"
  },
  {
    "text": "reasoning was not correct according to",
    "start": "649040",
    "end": "650720"
  },
  {
    "text": "the human look at the differences and",
    "start": "650720",
    "end": "652800"
  },
  {
    "text": "come up with some reflections based on",
    "start": "652800",
    "end": "654160"
  },
  {
    "text": "that so that's why that's one individual",
    "start": "654160",
    "end": "656320"
  },
  {
    "text": "node but if the LM got both wrong right",
    "start": "656320",
    "end": "658800"
  },
  {
    "text": "the answer accepted or rejected and the",
    "start": "658800",
    "end": "661040"
  },
  {
    "text": "explanation we want to say hey",
    "start": "661040",
    "end": "662560"
  },
  {
    "text": "everything went wrong in this step let's",
    "start": "662560",
    "end": "664320"
  },
  {
    "text": "take a step back and reflect about",
    "start": "664320",
    "end": "666079"
  },
  {
    "text": "exactly what happened what went wrong",
    "start": "666079",
    "end": "668160"
  },
  {
    "text": "and how we can improve it for next time",
    "start": "668160",
    "end": "670320"
  },
  {
    "text": "so we route the reflection and then we",
    "start": "670320",
    "end": "672240"
  },
  {
    "text": "call that node what these nodes do is",
    "start": "672240",
    "end": "674480"
  },
  {
    "text": "they say okay here is uh the original uh",
    "start": "674480",
    "end": "677440"
  },
  {
    "text": "generation that you produced here is",
    "start": "677440",
    "end": "679200"
  },
  {
    "text": "what the human corrected it to be here",
    "start": "679200",
    "end": "680959"
  },
  {
    "text": "is your reasoning generate a sort of",
    "start": "680959",
    "end": "683200"
  },
  {
    "text": "summary report on what went wrong don't",
    "start": "683200",
    "end": "685920"
  },
  {
    "text": "try to generate these very specific",
    "start": "685920",
    "end": "687760"
  },
  {
    "text": "reflections but instead just take a",
    "start": "687760",
    "end": "689279"
  },
  {
    "text": "highle look and talk about what went",
    "start": "689279",
    "end": "691120"
  },
  {
    "text": "wrong we do that because we want it to",
    "start": "691120",
    "end": "693120"
  },
  {
    "text": "first think about what went wrong and",
    "start": "693120",
    "end": "694880"
  },
  {
    "text": "then two extract that into very concise",
    "start": "694880",
    "end": "697760"
  },
  {
    "text": "specific reflections that we can use in",
    "start": "697760",
    "end": "699279"
  },
  {
    "text": "future steps kind of like the same",
    "start": "699279",
    "end": "700720"
  },
  {
    "text": "reason we separate out the initial uh",
    "start": "700720",
    "end": "703519"
  },
  {
    "text": "reasoning step from the answer",
    "start": "703519",
    "end": "705600"
  },
  {
    "text": "generation so we have it generate these",
    "start": "705600",
    "end": "708320"
  },
  {
    "text": "summary reflections right we can see we",
    "start": "708320",
    "end": "710480"
  },
  {
    "text": "just have this big old prompt uh telling",
    "start": "710480",
    "end": "712480"
  },
  {
    "text": "it to think carefully generate these",
    "start": "712480",
    "end": "714320"
  },
  {
    "text": "reflection summaries here is the",
    "start": "714320",
    "end": "716160"
  },
  {
    "text": "original incorrect explanation here's",
    "start": "716160",
    "end": "718480"
  },
  {
    "text": "the corrected one here's your reasoning",
    "start": "718480",
    "end": "720560"
  },
  {
    "text": "and then we also want to give it the",
    "start": "720560",
    "end": "721920"
  },
  {
    "text": "reflections it's already generated just",
    "start": "721920",
    "end": "723680"
  },
  {
    "text": "so it knows not to duplicate things that",
    "start": "723680",
    "end": "725600"
  },
  {
    "text": "it already has um generated in the past",
    "start": "725600",
    "end": "728800"
  },
  {
    "text": "once it's done that we then return this",
    "start": "728800",
    "end": "730480"
  },
  {
    "text": "reflection summary and then we call the",
    "start": "730480",
    "end": "733040"
  },
  {
    "text": "extract reflections node what this does",
    "start": "733040",
    "end": "735519"
  },
  {
    "text": "is we say okay here is um once again all",
    "start": "735519",
    "end": "738720"
  },
  {
    "text": "that context so the current reflections",
    "start": "738720",
    "end": "740560"
  },
  {
    "text": "you've generated uh the explanation that",
    "start": "740560",
    "end": "743839"
  },
  {
    "text": "you generated and then the one that the",
    "start": "743839",
    "end": "745920"
  },
  {
    "text": "human corrected it to be all of your",
    "start": "745920",
    "end": "747920"
  },
  {
    "text": "reflections take a look at all of this",
    "start": "747920",
    "end": "749920"
  },
  {
    "text": "and extract out these very specific",
    "start": "749920",
    "end": "752639"
  },
  {
    "text": "reflections from that so we tell it to",
    "start": "752639",
    "end": "754399"
  },
  {
    "text": "generate at least one but it can",
    "start": "754399",
    "end": "756079"
  },
  {
    "text": "generate multiple if it went wrong in",
    "start": "756079",
    "end": "758160"
  },
  {
    "text": "multiple places um and this is so we can",
    "start": "758160",
    "end": "760560"
  },
  {
    "text": "have you know once again break it down",
    "start": "760560",
    "end": "762079"
  },
  {
    "text": "into two steps make it a little bit",
    "start": "762079",
    "end": "763519"
  },
  {
    "text": "easier and don't have the LM try and do",
    "start": "763519",
    "end": "765279"
  },
  {
    "text": "everything in a single step so generate",
    "start": "765279",
    "end": "766959"
  },
  {
    "text": "the reflection summary and then from",
    "start": "766959",
    "end": "768639"
  },
  {
    "text": "that summary extract very specific",
    "start": "768639",
    "end": "771120"
  },
  {
    "text": "reflection pieces that we can then store",
    "start": "771120",
    "end": "772880"
  },
  {
    "text": "in the langraph store so we call the LM",
    "start": "772880",
    "end": "776320"
  },
  {
    "text": "and then we just put these reflections",
    "start": "776320",
    "end": "778079"
  },
  {
    "text": "putting in all of the old reflections",
    "start": "778079",
    "end": "779680"
  },
  {
    "text": "and all the new",
    "start": "779680",
    "end": "781160"
  },
  {
    "text": "reflections once again we made this into",
    "start": "781160",
    "end": "784639"
  },
  {
    "text": "a subgraph right because we know that",
    "start": "784639",
    "end": "786800"
  },
  {
    "text": "you're probably want to expand this for",
    "start": "786800",
    "end": "788560"
  },
  {
    "text": "your specific use case so one easy thing",
    "start": "788560",
    "end": "791040"
  },
  {
    "text": "would be to after you extract the",
    "start": "791040",
    "end": "792839"
  },
  {
    "text": "reflections you don't want to just go",
    "start": "792839",
    "end": "794720"
  },
  {
    "text": "save all the old ones and all the new",
    "start": "794720",
    "end": "796240"
  },
  {
    "text": "ones you could have a third step in this",
    "start": "796240",
    "end": "798560"
  },
  {
    "text": "process which says here are all the old",
    "start": "798560",
    "end": "800720"
  },
  {
    "text": "reflections here's the new ones I just",
    "start": "800720",
    "end": "802560"
  },
  {
    "text": "generated do you need to delete some old",
    "start": "802560",
    "end": "804800"
  },
  {
    "text": "reflections do you need to combine some",
    "start": "804800",
    "end": "806639"
  },
  {
    "text": "right if you generated one that's kind",
    "start": "806639",
    "end": "808000"
  },
  {
    "text": "of similar to one you've already",
    "start": "808000",
    "end": "808959"
  },
  {
    "text": "generated maybe you can combine them",
    "start": "808959",
    "end": "810480"
  },
  {
    "text": "into one slightly more elaborate but",
    "start": "810480",
    "end": "812880"
  },
  {
    "text": "still concise reflection and so on so we",
    "start": "812880",
    "end": "815920"
  },
  {
    "text": "made this a subgraph so that it's super",
    "start": "815920",
    "end": "817600"
  },
  {
    "text": "easy to add this customization to your",
    "start": "817600",
    "end": "820040"
  },
  {
    "text": "graph so that is how the LM manager",
    "start": "820040",
    "end": "822959"
  },
  {
    "text": "works i hope you all found this",
    "start": "822959",
    "end": "824320"
  },
  {
    "text": "interesting and learned something uh of",
    "start": "824320",
    "end": "825920"
  },
  {
    "text": "course this repo is open source and I'll",
    "start": "825920",
    "end": "827600"
  },
  {
    "text": "have all of the links to this repo and",
    "start": "827600",
    "end": "829279"
  },
  {
    "text": "the documentation in the description i",
    "start": "829279",
    "end": "831120"
  },
  {
    "text": "hope to see you all building some",
    "start": "831120",
    "end": "832639"
  },
  {
    "text": "interesting LM manager workflows with",
    "start": "832639",
    "end": "836240"
  }
]