[
  {
    "text": "all right we are live hello everyone welcome to a webinar on evaluating rag",
    "start": "3240",
    "end": "11400"
  },
  {
    "text": "applications um really excited for this one this has been in the works for a while",
    "start": "11400",
    "end": "17520"
  },
  {
    "text": "um we've we've had some uh we've got some awesome guests with we've been working on some collaboration with uh",
    "start": "17520",
    "end": "23100"
  },
  {
    "text": "with the ragus team for a few weeks now so so really excited about this and then added in Pedro last minute as well",
    "start": "23100",
    "end": "29400"
  },
  {
    "text": "because he has a really cool kind of like uh product that just launched it's very relevant to kind of like what we'll be talking about",
    "start": "29400",
    "end": "35760"
  },
  {
    "text": "um so minor Logistics before we get started um this is being recorded it'll be available at the link afterwards and",
    "start": "35760",
    "end": "42059"
  },
  {
    "text": "then we'll also put it up on YouTube tomorrow um we will the format that we'll do is",
    "start": "42059",
    "end": "48420"
  },
  {
    "text": "quick intros by everyone then we'll have a presentation from ragas on on what they're building",
    "start": "48420",
    "end": "54120"
  },
  {
    "text": "um we'll hear a little bit from will about what some of the stuff we're working on at linktrain and then we'll hit hand it over to pider to talk about building an application like what what",
    "start": "54120",
    "end": "61260"
  },
  {
    "text": "as an application Builder what is he actually looking at in terms of evaluation after that we will then just",
    "start": "61260",
    "end": "66840"
  },
  {
    "text": "go into General q a so if you guys have questions about kind of like rag evaluation",
    "start": "66840",
    "end": "73140"
  },
  {
    "text": "um please please please put them in the Q a box on the right so if you look to",
    "start": "73140",
    "end": "78659"
  },
  {
    "text": "the right you can see there's a little chat box that's probably where you're in by default and then if you look below that there's a little box with a",
    "start": "78659",
    "end": "83759"
  },
  {
    "text": "question mark please put any questions there we'll go through the ones that have the most votes so you can vote for ones that",
    "start": "83759",
    "end": "89280"
  },
  {
    "text": "you want to hear answered and and we'll answer them in that order I think that's all the logistics stuff",
    "start": "89280",
    "end": "95100"
  },
  {
    "text": "should be pretty straightforward I'm I'm really excited for that one or for the for this event so let's start it with",
    "start": "95100",
    "end": "100979"
  },
  {
    "text": "some quick intros Jason do you maybe want to go first yeah okay first of all thanks for",
    "start": "100979",
    "end": "106860"
  },
  {
    "text": "setting it up great to be talking with you guys and learning from you but yeah my name is I'm one of the co-main team",
    "start": "106860",
    "end": "114720"
  },
  {
    "text": "as a rakas I used to work as an ambulance here previously and then got",
    "start": "114720",
    "end": "120420"
  },
  {
    "text": "into the llm Fever by this year and that",
    "start": "120420",
    "end": "125640"
  },
  {
    "text": "every day I've been like from that point they have been focusing on how to make llm apps and especially lag apps more",
    "start": "125640",
    "end": "132000"
  },
  {
    "text": "much more reliable so yeah that's good awesome and and sure who will do you",
    "start": "132000",
    "end": "137280"
  },
  {
    "text": "want maybe you want to go now as well to round that up yeah hi I'm shahul I've been a data",
    "start": "137280",
    "end": "142500"
  },
  {
    "text": "science professional uh for about five years been training language models since 2018 and now full on imagining uh",
    "start": "142500",
    "end": "150080"
  },
  {
    "text": "so happy to be here great well yeah um hi I'm will I work",
    "start": "150080",
    "end": "157920"
  },
  {
    "text": "with Harrison at langchin been working a lot especially on links langsmith platform looking at our evaluations I'm",
    "start": "157920",
    "end": "163680"
  },
  {
    "text": "trying to integrate better with great pack packages and Frameworks like ragas um and really trying to connect the dots",
    "start": "163680",
    "end": "170280"
  },
  {
    "text": "between link chain link Smith and what everyone needs here um to evaluate any type of custom application",
    "start": "170280",
    "end": "177840"
  },
  {
    "text": "perfect and then and then Pedro hey guys thank you so much my name is Phaedra I'm the founder and CEO of",
    "start": "177840",
    "end": "183300"
  },
  {
    "text": "Tavern which is launched NOAA um yesterday uh we're just trying to really you know get all the cool",
    "start": "183300",
    "end": "188940"
  },
  {
    "text": "infrastructure and put in the hens users building workflows that are really easy and simple for them to get so we've been",
    "start": "188940",
    "end": "194700"
  },
  {
    "text": "building you know an AI and link chain since the beginning of the year so really excited too to dive deeper into it",
    "start": "194700",
    "end": "201540"
  },
  {
    "text": "and with those intros out of the way let's let's hear about our address what have you guys got for us today",
    "start": "201540",
    "end": "207739"
  },
  {
    "text": "yeah I guess I'll share the screen now perfect",
    "start": "208319",
    "end": "215480"
  },
  {
    "text": "okay I hope everyone can see the screen yeah good yep all right",
    "start": "219900",
    "end": "228900"
  },
  {
    "text": "all right so um open source evaluation framework that we",
    "start": "228900",
    "end": "234180"
  },
  {
    "text": "builted honestly for ourselves so we were we were testing out",
    "start": "234180",
    "end": "239280"
  },
  {
    "text": "um a bunch of like we were pretty passionate about electricity and augmented systems and we were building a",
    "start": "239280",
    "end": "244860"
  },
  {
    "text": "bunch of time ourselves but there was no proper way to bandwidth there were a couple of existing matrixes but like",
    "start": "244860",
    "end": "251340"
  },
  {
    "text": "blue score blooms uh blue squirrel roads Etc but they were had very few like very",
    "start": "251340",
    "end": "257699"
  },
  {
    "text": "poor correlation with human judgment and there were like many uh date assets and benchmarks to evaluate the retrieval",
    "start": "257699",
    "end": "264840"
  },
  {
    "text": "part but they were also not consistent with what with the data you see in production so that was also not very",
    "start": "264840",
    "end": "270600"
  },
  {
    "text": "good for evaluation so we were like focused very like we were trying to figure out how we can improve this",
    "start": "270600",
    "end": "276720"
  },
  {
    "text": "evaluation and we were thinking about that a bit and that said and all the learnings that we had was what we wrote",
    "start": "276720",
    "end": "283979"
  },
  {
    "text": "it up in ragas so but from the start having a very think very well thorough",
    "start": "283979",
    "end": "292020"
  },
  {
    "text": "and well tested evaluation pipeline is very important to build robust act while you can easily start off with a cool",
    "start": "292020",
    "end": "299040"
  },
  {
    "text": "demo without like libraries like language when you're going into production and you have to bring in",
    "start": "299040",
    "end": "305100"
  },
  {
    "text": "these engineering disciplines like who tests you and remove the valuation pipeline so that your your apps are",
    "start": "305100",
    "end": "312360"
  },
  {
    "text": "reliable when they go into production and continue to say so so that is one of the",
    "start": "312360",
    "end": "318600"
  },
  {
    "text": "importance of having a very good preparation Pipeline and that's the one of the reasons why we focused fully on",
    "start": "318600",
    "end": "324900"
  },
  {
    "text": "creating graph password um okay yeah",
    "start": "324900",
    "end": "332419"
  },
  {
    "text": "evaluation but before going to llm assistant evaluation we should be very",
    "start": "334759",
    "end": "340080"
  },
  {
    "text": "much aware of why and where we should not use llms and llms also comes with a",
    "start": "340080",
    "end": "345900"
  },
  {
    "text": "certain set of biases that we should be really aware of so that we can we can",
    "start": "345900",
    "end": "351600"
  },
  {
    "text": "design candidates that can bypass these spices so I'll just discuss some of these balances that we are aware of as",
    "start": "351600",
    "end": "357660"
  },
  {
    "text": "of today one of these either elements has a particular position bias because",
    "start": "357660",
    "end": "363180"
  },
  {
    "text": "of which it will prefer responses to a particular position compared to other positions so if you ask LM to compare",
    "start": "363180",
    "end": "371220"
  },
  {
    "text": "and select the best outputs from three given outputs it might just prefer the output and position one because it just",
    "start": "371220",
    "end": "378000"
  },
  {
    "text": "has a particular bias towards that position and the output and the output",
    "start": "378000",
    "end": "383039"
  },
  {
    "text": "from the llm can change whenever you change the positions of the outputs and",
    "start": "383039",
    "end": "388740"
  },
  {
    "text": "the algorithms also have also have shown to for individual scores when assigning",
    "start": "388740",
    "end": "394440"
  },
  {
    "text": "scores and it never almost prints out floating Point scores so this is also a",
    "start": "394440",
    "end": "401220"
  },
  {
    "text": "bias that llm has and when we used to compare answers even GPT 4 actually",
    "start": "401220",
    "end": "407280"
  },
  {
    "text": "preferred answers from its own output even when compared to answers from Human",
    "start": "407280",
    "end": "412680"
  },
  {
    "text": "orientated answers because it is due to a style bias that llm has and then it",
    "start": "412680",
    "end": "418500"
  },
  {
    "text": "also has a bias towards picking up for a particular number from",
    "start": "418500",
    "end": "424199"
  },
  {
    "text": "a set of numbers and then we will also see an example for that and llms in general are also strategic in nature",
    "start": "424199",
    "end": "431039"
  },
  {
    "text": "which due to which it can give different scores when invoked differently so using",
    "start": "431039",
    "end": "436740"
  },
  {
    "text": "elements directly is not really very good solution and here you can see that I have generated basically 100 random",
    "start": "436740",
    "end": "445020"
  },
  {
    "text": "numbers from GPD 3.5 and it picks up 7 most of the time and this",
    "start": "445020",
    "end": "452340"
  },
  {
    "text": "essentially shows a particular bias from of the llm that it is actually preferring the number seven compared to",
    "start": "452340",
    "end": "459660"
  },
  {
    "text": "other numbers and if this was an ideal system it should have actually preferred or should have produced some uniform",
    "start": "459660",
    "end": "466979"
  },
  {
    "text": "distribution so so yeah like I would say there are using",
    "start": "466979",
    "end": "473460"
  },
  {
    "text": "elements directly has a few biases but they are also very effective like when",
    "start": "473460",
    "end": "478740"
  },
  {
    "text": "evaluating in in a budget situation they have very good correlation with human",
    "start": "478740",
    "end": "484500"
  },
  {
    "text": "judgment if used in like first specific situations but so the goal with ragas is",
    "start": "484500",
    "end": "490880"
  },
  {
    "text": "certain prices but they also have some positives but what are some methodologies so that we can work around",
    "start": "490880",
    "end": "497220"
  },
  {
    "text": "these spices that they have and use it for evaluating little automated systems and we came up with a few paradings so",
    "start": "497220",
    "end": "504660"
  },
  {
    "text": "basically for any rag system there will select two components which is the generation answer to that and we need to",
    "start": "504660",
    "end": "510780"
  },
  {
    "text": "evaluate both of them separately so that it gives you whole holistic score what is happening uh Forget You gives",
    "start": "510780",
    "end": "518339"
  },
  {
    "text": "you a politics so for the generation part you need we have like faithfulness and answer the Legacy which",
    "start": "518339",
    "end": "525000"
  },
  {
    "text": "um which I I talk about in all these matrixes and how they work down the road but for travel we have context relevancy",
    "start": "525000",
    "end": "532320"
  },
  {
    "text": "and context record and faithfulness faithfulness is basically uh how we",
    "start": "532320",
    "end": "537600"
  },
  {
    "text": "formulated faithfulness is uh there are two we are using two random calls and the first one is we have given a",
    "start": "537600",
    "end": "544740"
  },
  {
    "text": "generated answer we are trying to figure out what is the number of statements can",
    "start": "544740",
    "end": "550080"
  },
  {
    "text": "meet the answer and we see if these statements are supported from the retrieved context so faithfulness is the",
    "start": "550080",
    "end": "556860"
  },
  {
    "text": "measure of the hallucination that is there that is generated answer and hallucination in our definition is uh if",
    "start": "556860",
    "end": "564600"
  },
  {
    "text": "the llm is generating anything that is not supported by the current and faith and with this faithfulness code we can",
    "start": "564600",
    "end": "571740"
  },
  {
    "text": "get a measure of that and the next one transfer relevancy is a measure of how",
    "start": "571740",
    "end": "577920"
  },
  {
    "text": "relevant the generated answer is to the given question so here you can see that for the same question first the last",
    "start": "577920",
    "end": "586380"
  },
  {
    "text": "answer is the most relevant and to the point and this is uh the answer relevancy score is a measure of time so",
    "start": "586380",
    "end": "593700"
  },
  {
    "text": "basically what we do is we try to reverse engineer the answer given uh the",
    "start": "593700",
    "end": "599339"
  },
  {
    "text": "question given an answer and we calculate the similarity between the generic we run this like three times and",
    "start": "599339",
    "end": "606540"
  },
  {
    "text": "we try to figure out the similarity between the generate questions and it's a similarity of the general question is",
    "start": "606540",
    "end": "611880"
  },
  {
    "text": "very high we know that the answer is also very correlated is also to the point",
    "start": "611880",
    "end": "619820"
  },
  {
    "text": "uh the next part that I will focus on is the context uh so uh now we have",
    "start": "619820",
    "end": "626459"
  },
  {
    "text": "evaluated we have our presented two matrices that can evaluate the generation but now we'll focus on uh two",
    "start": "626459",
    "end": "634080"
  },
  {
    "text": "matrices that can evaluate the root of your path so first thing is context 11c",
    "start": "634080",
    "end": "640080"
  },
  {
    "text": "which is used as a proxy for uh evaluate for precision of the retrieval contacts",
    "start": "640080",
    "end": "645600"
  },
  {
    "text": "so this helps in optimizing the sound size and to calculate this this the",
    "start": "645600",
    "end": "651420"
  },
  {
    "text": "problem is formulated as a candidate sentence extraction so it's pretty simple when a given a retrievable",
    "start": "651420",
    "end": "658380"
  },
  {
    "text": "contacts and question the llm X actually extracts a number of sentences that can",
    "start": "658380",
    "end": "665940"
  },
  {
    "text": "be used to answer the given question so the file score is the ratio of number of",
    "start": "665940",
    "end": "671279"
  },
  {
    "text": "extracted sentences from the given context divided by the total number of",
    "start": "671279",
    "end": "676320"
  },
  {
    "text": "sentences that was retrieved by the retriever so this is this can be used to optimize the channel size and decreases",
    "start": "676320",
    "end": "683399"
  },
  {
    "text": "chances which can lead to saving course and also latency next one is a convex recall this is a",
    "start": "683399",
    "end": "691019"
  },
  {
    "text": "very newly introduced metric and this is where a lot of problem happens and",
    "start": "691019",
    "end": "697260"
  },
  {
    "text": "estimating the recall without a reference is a tricky problem so we to run ragas contacts because we need a",
    "start": "697260",
    "end": "704579"
  },
  {
    "text": "ground truth annotated answer and this is also formulated as a combination of candidate sentence",
    "start": "704579",
    "end": "711240"
  },
  {
    "text": "extraction and natural language influence so given an annotated answer and retrieved a context we figure out",
    "start": "711240",
    "end": "718440"
  },
  {
    "text": "the data points that are present in the referral context and also are missing",
    "start": "718440",
    "end": "724200"
  },
  {
    "text": "from the retriever context and the final score is uh just the Precision from",
    "start": "724200",
    "end": "729720"
  },
  {
    "text": "these two points calculating record from these two points uh",
    "start": "729720",
    "end": "735120"
  },
  {
    "text": "then on top of all these matrices we apply self consistency effect to ensure",
    "start": "735120",
    "end": "740399"
  },
  {
    "text": "high reproducibility and consistency of ragas course so basically llms have",
    "start": "740399",
    "end": "746040"
  },
  {
    "text": "proven to give a consistent output when they are when they are highly confident",
    "start": "746040",
    "end": "753420"
  },
  {
    "text": "about srtale output so this conference can be measured and estimated by",
    "start": "753420",
    "end": "759480"
  },
  {
    "text": "invoking the llm multiple times and quantifying the agreement between the",
    "start": "759480",
    "end": "765779"
  },
  {
    "text": "generated answer in these multiple invocations we use this trick on top of these matrices to",
    "start": "765779",
    "end": "773040"
  },
  {
    "text": "ensure High consistency in rather scores so that he said and then we have a future",
    "start": "773040",
    "end": "780480"
  },
  {
    "text": "directions we are working on a tester generation Paradigm so we have seen that formulating a test set is kind of",
    "start": "780480",
    "end": "787800"
  },
  {
    "text": "difficult because it should follow certain characteristics such as it should have a distribution similar to",
    "start": "787800",
    "end": "793200"
  },
  {
    "text": "test set it should also have different levels of question questions in it with",
    "start": "793200",
    "end": "798959"
  },
  {
    "text": "the different levels of difficulties so we are generating a paradigm to generate",
    "start": "798959",
    "end": "804779"
  },
  {
    "text": "questions from a given set of documents and and if you have a seed test of",
    "start": "804779",
    "end": "811500"
  },
  {
    "text": "prompts that it will also consider those problems so apart from that we are also working on evaluating and evaluating and",
    "start": "811500",
    "end": "818160"
  },
  {
    "text": "adversarial testing LM agents which is mostly used in combination with configuration systems and then after",
    "start": "818160",
    "end": "825660"
  },
  {
    "text": "that we are looking forward to develop our own custom models for evaluation",
    "start": "825660",
    "end": "832579"
  },
  {
    "text": "thank you awesome thank you guys so okay I have a",
    "start": "846800",
    "end": "852540"
  },
  {
    "text": "few questions um and maybe and and we'll do those before maybe Moving on but if other",
    "start": "852540",
    "end": "857639"
  },
  {
    "text": "people have questions as well on the panel please jump in and then if people have questions in the chat drop them in the chat box because I think there's a",
    "start": "857639",
    "end": "863940"
  },
  {
    "text": "lot of stuff here that I want to unpack first of all though just like basic even setting the scene like I'd love to understand like are you guys working on",
    "start": "863940",
    "end": "870540"
  },
  {
    "text": "this full time is it the two of you is there is there a company behind this is just purely open source like what's the",
    "start": "870540",
    "end": "876300"
  },
  {
    "text": "what's what's the what's the story behind the repo",
    "start": "876300",
    "end": "881360"
  },
  {
    "text": "yeah so currently it is an open source report we are trying to for our company around it but",
    "start": "884339",
    "end": "892320"
  },
  {
    "text": "not necessarily around ragas as a notice",
    "start": "892320",
    "end": "897600"
  },
  {
    "text": "so yeah right now it's an open source report we are trying to see how the adoption goes and then",
    "start": "897600",
    "end": "904880"
  },
  {
    "text": "awesome well you've got a lot of fans I saw one person in the chat say that they use it in their company every day so",
    "start": "904880",
    "end": "910320"
  },
  {
    "text": "you've got a lot of fans for address um a more technical question I have is you presented like the four different",
    "start": "910320",
    "end": "916500"
  },
  {
    "text": "areas that you kind of like measure on when you guys see like applications that",
    "start": "916500",
    "end": "923220"
  },
  {
    "text": "are producing a wrong answer at the end which of the four like most contribute",
    "start": "923220",
    "end": "928800"
  },
  {
    "text": "to that do you have a or like or like yeah which of the four are most kind of like relevant or you see most people kind of like",
    "start": "928800",
    "end": "935160"
  },
  {
    "text": "stressing out about or that's where like most of the errors occur yeah it is mostly with uh the contacts",
    "start": "935160",
    "end": "943139"
  },
  {
    "text": "like the trigger part because um yeah building up a system that can",
    "start": "943139",
    "end": "948600"
  },
  {
    "text": "retrieve like there's a lot of things that go through like um all of the clients that like have been",
    "start": "948600",
    "end": "955440"
  },
  {
    "text": "using that we talked to are at the edge of what a simple Factor search and a similarity search stuff and can do so",
    "start": "955440",
    "end": "963120"
  },
  {
    "text": "that you need like they are trying to figure out much more comprehensive ways of trading the information that is",
    "start": "963120",
    "end": "969180"
  },
  {
    "text": "actually that can actually be used to answer the question so that is and that is where like context record that is",
    "start": "969180",
    "end": "975240"
  },
  {
    "text": "something that we added but yes yesterday you would feel it would really help",
    "start": "975240",
    "end": "980459"
  },
  {
    "text": "got it so context recall that makes sense um and then one interesting thing for",
    "start": "980459",
    "end": "985680"
  },
  {
    "text": "one of the uh for one of the ones not contextually at all but the",
    "start": "985680",
    "end": "991800"
  },
  {
    "text": "other one um in context you said like uh yeah context relevancy you said like changing",
    "start": "991800",
    "end": "997199"
  },
  {
    "text": "the chunk size um could help kind of like with optimizing that for the other ones like",
    "start": "997199",
    "end": "1002899"
  },
  {
    "text": "how would you recommend fixing all of them like if something's going wrong in context recall what can I do if something's going wrong in like",
    "start": "1002899",
    "end": "1009199"
  },
  {
    "text": "generation like what what do I do what are fixes for all these issues right so uh for especially for context",
    "start": "1009199",
    "end": "1017540"
  },
  {
    "text": "uh for contacts recall if we say in most cases it is either the wedding",
    "start": "1017540",
    "end": "1023240"
  },
  {
    "text": "it is the issue from the wedding that either the embedding is not really suitable to uh retrieve the kind of",
    "start": "1023240",
    "end": "1030319"
  },
  {
    "text": "information that is required for the given question or even most in some cases this can be also sold by kind of",
    "start": "1030319",
    "end": "1037520"
  },
  {
    "text": "query transformation so error of these methods Works many times uh but",
    "start": "1037520",
    "end": "1045140"
  },
  {
    "text": "basically generally improving and enhancing uh the retrieval techniques",
    "start": "1045140",
    "end": "1050540"
  },
  {
    "text": "and uh embedding usually works to improve context recall so that you don't",
    "start": "1050540",
    "end": "1056480"
  },
  {
    "text": "really miss any point that is necessary so this is very very common error after uh people we build a simple drag system",
    "start": "1056480",
    "end": "1064460"
  },
  {
    "text": "when we push towards production so if the reason that is very important regarding Road context relevancy it is a",
    "start": "1064460",
    "end": "1072320"
  },
  {
    "text": "matter of precision so the llm will answer the question correctly mostly",
    "start": "1072320",
    "end": "1078080"
  },
  {
    "text": "even if the system has no Precision but it will usually take more time because you are sending more land required",
    "start": "1078080",
    "end": "1084860"
  },
  {
    "text": "number of tokens to the llm and it is also going to cost much more so this can also in some cases cause some some form",
    "start": "1084860",
    "end": "1093140"
  },
  {
    "text": "of hallucination because you're presenting a whole lot of information that is more than required to the",
    "start": "1093140",
    "end": "1099200"
  },
  {
    "text": "learner and it can mess up some of the uh a long context it can go some of the",
    "start": "1099200",
    "end": "1105860"
  },
  {
    "text": "low contact issues and for the generator part uh regarding",
    "start": "1105860",
    "end": "1111260"
  },
  {
    "text": "faithfulness and relevancy adjusting the prompt and you know doing some kind of",
    "start": "1111260",
    "end": "1116840"
  },
  {
    "text": "transformation UCD Works to improve uh the problem we people also use some kind",
    "start": "1116840",
    "end": "1123020"
  },
  {
    "text": "of guardrails to ensure that none of the sources all of the sources which are",
    "start": "1123020",
    "end": "1129200"
  },
  {
    "text": "cited actually as the things that are actually generated so",
    "start": "1129200",
    "end": "1134299"
  },
  {
    "text": "yes these are the main points that we have seen yeah also I wanted to like mention the",
    "start": "1134299",
    "end": "1141200"
  },
  {
    "text": "role of langsmith and this is like one of the main reasons why we were like super excited with the collaboration because like",
    "start": "1141200",
    "end": "1147919"
  },
  {
    "text": "um okay because like I have very good like language has been like super useful for us even internally to figure out",
    "start": "1147919",
    "end": "1154700"
  },
  {
    "text": "where exactly because it gives you the old trace of the right from the evaluation and to the",
    "start": "1154700",
    "end": "1161179"
  },
  {
    "text": "the trace of the actual strength actual chain and that means you have like you",
    "start": "1161179",
    "end": "1167660"
  },
  {
    "text": "can act it is much more easier to pinpoint where exactly what where exactly and what exactly went wrong so",
    "start": "1167660",
    "end": "1173840"
  },
  {
    "text": "we like using some having a very good like tracing and monitoring setup with your evaluation pipeline as good as also",
    "start": "1173840",
    "end": "1181039"
  },
  {
    "text": "notification yeah and I know one of the big things that we're like you know struggling for",
    "start": "1181039",
    "end": "1188780"
  },
  {
    "text": "is good metrics to evaluate all these things yeah I think like uh I think linksmith provides pretty good kind of",
    "start": "1188780",
    "end": "1194720"
  },
  {
    "text": "like visibility into what's going on but I think when paired with some of these metrics I think it makes it even more powerful and actually that yeah so that",
    "start": "1194720",
    "end": "1202880"
  },
  {
    "text": "um leads me to another question which is like how much would you kind of like blindly trust these metrics like when",
    "start": "1202880",
    "end": "1209179"
  },
  {
    "text": "you guys are doing them do you like how how much do you look at the individual data points actually maybe this is a better way for",
    "start": "1209179",
    "end": "1215960"
  },
  {
    "text": "me when you guys look at the individual data points how often do you see that the grades are like correct and like you agree with the llm assisted evaluation",
    "start": "1215960",
    "end": "1223820"
  },
  {
    "text": "right uh so for our own evaluation we we have a set of three one annotated test",
    "start": "1223820",
    "end": "1230539"
  },
  {
    "text": "set and whenever we roll out the new metrics we uh measure the correlation",
    "start": "1230539",
    "end": "1236000"
  },
  {
    "text": "with human judgment against our metrics and if that is about a satisfiable level",
    "start": "1236000",
    "end": "1241340"
  },
  {
    "text": "that is around more than 0.8 we will only Roll Off The Matrix so we have this",
    "start": "1241340",
    "end": "1247640"
  },
  {
    "text": "internal testing mechanism through which we test these matrices and also like as",
    "start": "1247640",
    "end": "1253520"
  },
  {
    "text": "students and Central Language has been very useful when debugging because whenever the formulated methods some of",
    "start": "1253520",
    "end": "1259220"
  },
  {
    "text": "the data points or some situations The Matrix might mess up and we can use",
    "start": "1259220",
    "end": "1264260"
  },
  {
    "text": "language to identify why it messed up so it's been very useful but we generally",
    "start": "1264260",
    "end": "1270559"
  },
  {
    "text": "follow this this testing mechanism inside rather us to ensure the Matrix",
    "start": "1270559",
    "end": "1275660"
  },
  {
    "text": "actually works yeah and there there are times when it has messed up but I have been like",
    "start": "1275660",
    "end": "1281720"
  },
  {
    "text": "surprised with a number of times it actually called things right so that is that has been surprising but",
    "start": "1281720",
    "end": "1288320"
  },
  {
    "text": "that is something that we have to improve so that there is more cartwheels and these terrorists these like metrics",
    "start": "1288320",
    "end": "1295100"
  },
  {
    "text": "have are much more like but we do these like tests",
    "start": "1295100",
    "end": "1300279"
  },
  {
    "text": "so a somewhat related question for this that's in the Q a right now and um was",
    "start": "1300320",
    "end": "1305360"
  },
  {
    "text": "sort of matched one thing that that um I had personally um one of the nice things that you do is you take these four",
    "start": "1305360",
    "end": "1310580"
  },
  {
    "text": "metrics you take the harmonic mean for an overall score and you can make that a little bit easier to be filtering your results between models that because you",
    "start": "1310580",
    "end": "1316820"
  },
  {
    "text": "have that single combined float value um a lot of people coming maybe from the more traditional ml space will perhaps",
    "start": "1316820",
    "end": "1323240"
  },
  {
    "text": "already have implemented or be familiar with other metrics for retrieval like immune reciprocal rank where you have",
    "start": "1323240",
    "end": "1328340"
  },
  {
    "text": "like a labeled data set and you want to see if the retrieved documents or something or like show up there or maybe",
    "start": "1328340",
    "end": "1333500"
  },
  {
    "text": "like precision.k um how do you think about extending um like the the existing metrics set",
    "start": "1333500",
    "end": "1339559"
  },
  {
    "text": "within ragas to include other metrics either llm assisted or some of these traditional things and um like how would",
    "start": "1339559",
    "end": "1346039"
  },
  {
    "text": "you go about using that in order like for your overall workflow",
    "start": "1346039",
    "end": "1350980"
  },
  {
    "text": "yeah that's a that's a valuable question so like the like all these existing methodologies",
    "start": "1352580",
    "end": "1358700"
  },
  {
    "text": "and they work like really well we can be the one of the solutions of like",
    "start": "1358700",
    "end": "1364400"
  },
  {
    "text": "figuring like trying to leverage llm's work situations well like actually",
    "start": "1364400",
    "end": "1369559"
  },
  {
    "text": "building a test set to reflect so if you are using like um if you are running the",
    "start": "1369559",
    "end": "1375740"
  },
  {
    "text": "traditional Matrix for like information travel to calculate the previous code you have to have annotated annotated",
    "start": "1375740",
    "end": "1381679"
  },
  {
    "text": "data sets for the given for your given like description that you're trying to build and when people ask right starting",
    "start": "1381679",
    "end": "1389120"
  },
  {
    "text": "off and like even if they are moved forward is very complicated and that's why we",
    "start": "1389120",
    "end": "1395720"
  },
  {
    "text": "like focused on transference key reference free metacles but like if you",
    "start": "1395720",
    "end": "1401179"
  },
  {
    "text": "can leverage uh the existing like systems like access the accessing matrixes that is a very like valuable",
    "start": "1401179",
    "end": "1408320"
  },
  {
    "text": "point like little bit pretty useful",
    "start": "1408320",
    "end": "1413440"
  },
  {
    "text": "awesome I think we can probably there's a lot of other questions we'll get to them at the end I think there's a lot to",
    "start": "1417380",
    "end": "1423860"
  },
  {
    "text": "dive in here will do you maybe want to chat about some of the stuff we're doing on the lane",
    "start": "1423860",
    "end": "1429080"
  },
  {
    "text": "um chain I I guess we already mentioned a bit about how it integrates to ragus and everything like that but anything additional uh to add or any uh yeah",
    "start": "1429080",
    "end": "1436460"
  },
  {
    "text": "additional experiments we've done yeah I can um I can keep it pretty short I have I I can highlight two things one we have",
    "start": "1436460",
    "end": "1443539"
  },
  {
    "text": "a cookbook and I'll walk through maybe some code just briefly there to share some like some information of how you",
    "start": "1443539",
    "end": "1449240"
  },
  {
    "text": "can get started um actually maybe I'll pull up the The Notebook that you have in ragas so that",
    "start": "1449240",
    "end": "1454520"
  },
  {
    "text": "we can continue in this theme show how you're integrating there and then we can show a little bit of a glimpse in the UI",
    "start": "1454520",
    "end": "1459620"
  },
  {
    "text": "to see like maybe you have um something that goes wrong and then you can drill down into the retrieve pipeline in order to see what's happening",
    "start": "1459620",
    "end": "1467320"
  },
  {
    "text": "um so all right here I'll I'll share my screen",
    "start": "1467780",
    "end": "1473360"
  },
  {
    "text": "briefly foreign",
    "start": "1473360",
    "end": "1480340"
  },
  {
    "text": "so this notebook goes through a much simpler metric and it's not it's reference based so not reference free",
    "start": "1480340",
    "end": "1486620"
  },
  {
    "text": "we're just measuring the correctness of the overall end-to-end system response on your true system",
    "start": "1486620",
    "end": "1492260"
  },
  {
    "text": "um and I'll just walk through briefly how to connect it to langsmith there and then like go through a couple of the",
    "start": "1492260",
    "end": "1497539"
  },
  {
    "text": "data points there that maybe it gets wrong um overall this is an easy thing to get started if you have a small test set",
    "start": "1497539",
    "end": "1503720"
  },
  {
    "text": "that you have labeled and you know the answer it's less useful if you're still trying to mine that information you don't have um like the the actual",
    "start": "1503720",
    "end": "1510620"
  },
  {
    "text": "references that you want to build out and it um also maybe doesn't provide as much of a detailed analysis there by",
    "start": "1510620",
    "end": "1517760"
  },
  {
    "text": "breaking it down to those four different quadrants which is why we like to be able to be flexibly defining custom",
    "start": "1517760",
    "end": "1522919"
  },
  {
    "text": "evaluators and linking that to things such as rock s there um the overall process really is the",
    "start": "1522919",
    "end": "1528799"
  },
  {
    "text": "creating of the data sets and then defining the system and then you want to evaluate linksmith then",
    "start": "1528799",
    "end": "1534860"
  },
  {
    "text": "um then you can start iterating there and it creates some results that look something like this",
    "start": "1534860",
    "end": "1540860"
  },
  {
    "text": "um the thing that I think langsmith really helps with here is that it can help you",
    "start": "1540860",
    "end": "1547520"
  },
  {
    "text": "curate this data set it's often kind of hard to start from zero and you can do things such as like synthetic data",
    "start": "1547520",
    "end": "1554360"
  },
  {
    "text": "Generation Um or other things but by collecting traces of your actual system in like a",
    "start": "1554360",
    "end": "1559580"
  },
  {
    "text": "beta deployment then you can see real questions that your users are or that you are generating on the actual Corpus",
    "start": "1559580",
    "end": "1565100"
  },
  {
    "text": "you're doing and then you can go through and correct any retrieved answers and and create that really right there so in",
    "start": "1565100",
    "end": "1572600"
  },
  {
    "text": "that way you don't really have to start from this like zero and start like checking the performance there",
    "start": "1572600",
    "end": "1578659"
  },
  {
    "text": "um here we have an example where you just write out the questions and answers this",
    "start": "1578659",
    "end": "1584120"
  },
  {
    "text": "is over langsmith documentation um create the data set and this is I'll",
    "start": "1584120",
    "end": "1589340"
  },
  {
    "text": "share the link afterwards so I won't belabor the code too much um and then you evaluate the chain we just",
    "start": "1589340",
    "end": "1596659"
  },
  {
    "text": "use like a QA evaluator which essentially asks if it's correct or incorrect given the context and given the labels",
    "start": "1596659",
    "end": "1602600"
  },
  {
    "text": "um again very simple and I think it is important to bear in mind that those um those weaknesses and llm-based",
    "start": "1602600",
    "end": "1608900"
  },
  {
    "text": "evaluations um it does sometimes get biased towards like the position if you have specific",
    "start": "1608900",
    "end": "1614960"
  },
  {
    "text": "answers it's trying to select from and it also can have like systemically a systemic bias in certain things where",
    "start": "1614960",
    "end": "1622940"
  },
  {
    "text": "if it has some knowledge and it is parameter space it might be disagreeing if the actual ground truth answer you",
    "start": "1622940",
    "end": "1628760"
  },
  {
    "text": "have there especially if there's some areas in like the tokens or something there you can run the evalues and then",
    "start": "1628760",
    "end": "1634520"
  },
  {
    "text": "you can see the results in in langsmith and what this gives you is this",
    "start": "1634520",
    "end": "1639799"
  },
  {
    "text": "aggregate metrics there but then also it gives you an entire audit of each data point in the data set so you can see",
    "start": "1639799",
    "end": "1645500"
  },
  {
    "text": "what what which documents were retrieved for that data point um the the open Ai call or the other llm",
    "start": "1645500",
    "end": "1651860"
  },
  {
    "text": "call in order to synthesize the response the regeneration step if you want to be rephrasing the query into some other",
    "start": "1651860",
    "end": "1657740"
  },
  {
    "text": "steps there and then all those types of things and So then whenever you see that it is incorrect or this you see that it",
    "start": "1657740",
    "end": "1663679"
  },
  {
    "text": "is correct you can drill down into which part of the component is actually failing often it'll be that the documents you're retrieving are all too",
    "start": "1663679",
    "end": "1670520"
  },
  {
    "text": "similar or an insufficiently relevant and again using something like raw gas would help provide that a little bit",
    "start": "1670520",
    "end": "1676520"
  },
  {
    "text": "more Direction in that top level um but then you can actually go and then make tweakings tweaks to your system and",
    "start": "1676520",
    "end": "1682340"
  },
  {
    "text": "improve your metrics there um another pretty cool thing",
    "start": "1682340",
    "end": "1689120"
  },
  {
    "text": "or actually I'll I'll jump over to the the actual demo",
    "start": "1689120",
    "end": "1696679"
  },
  {
    "text": "I'll jump out it's actually a glank Smith now so that you can see what it looks like",
    "start": "1701120",
    "end": "1707260"
  },
  {
    "text": "so this is a run that we did I just generated a random project so that's not super informative but you can see the",
    "start": "1707260",
    "end": "1714200"
  },
  {
    "text": "results here on how accurate it is and it looks like it got most of the answers correct this is a relatively easy data",
    "start": "1714200",
    "end": "1721279"
  },
  {
    "text": "set let's see when it got incorrect here you can click through to each of these different steps like you can see which",
    "start": "1721279",
    "end": "1727340"
  },
  {
    "text": "documents it retrieved here you can see that perhaps like the format of this isn't the most useful for all of the",
    "start": "1727340",
    "end": "1734000"
  },
  {
    "text": "information um and then you can also go to the feedback run and you can see sort of the",
    "start": "1734000",
    "end": "1741260"
  },
  {
    "text": "the reasoning that it went through what information was passed through the evaluator and then how it was making the",
    "start": "1741260",
    "end": "1746360"
  },
  {
    "text": "decision here so you can see this whole prompt and if you disagree with it you can open it in",
    "start": "1746360",
    "end": "1752059"
  },
  {
    "text": "the playground you can make changes to the prompts there so that you can maybe improve your evaluator",
    "start": "1752059",
    "end": "1757400"
  },
  {
    "text": "um for the next time you're running it I think we provide some off-the-shelf evaluators within Lang chain I think",
    "start": "1757400",
    "end": "1763220"
  },
  {
    "text": "extending to things like ragas to other things can give you a lot more flexibility whenever you have special criteria you have special types of",
    "start": "1763220",
    "end": "1770020"
  },
  {
    "text": "user-specific evals or considerations you want to be including because not all retrieval augmented generation pipeline",
    "start": "1770020",
    "end": "1776120"
  },
  {
    "text": "is the same and not every system wants to have the exact same like decisions whenever you're doing that so I can try",
    "start": "1776120",
    "end": "1782299"
  },
  {
    "text": "this and then I can make changes and then you can submit it and see what types of results it returns",
    "start": "1782299",
    "end": "1789799"
  },
  {
    "text": "um and then you can use that next time whenever you want to be evaluating",
    "start": "1789799",
    "end": "1796000"
  },
  {
    "text": "do we have uh do we have any ragas evaluators built in",
    "start": "1798860",
    "end": "1804940"
  },
  {
    "text": "yeah do you have any shared results for it I haven't run it recently so I don't",
    "start": "1805279",
    "end": "1810320"
  },
  {
    "text": "have a link on hand maybe I could actually go through and and run your notebook",
    "start": "1810320",
    "end": "1816440"
  },
  {
    "text": "yeah I think we're probably a bit short on time to do that live but maybe we can drop a drop a link to that notebook and",
    "start": "1816440",
    "end": "1822500"
  },
  {
    "text": "and because I think uh yeah like the the metrics that that you guys kind of like",
    "start": "1822500",
    "end": "1828380"
  },
  {
    "text": "have pair super nicely I think with like the debuggability and inspectability so",
    "start": "1828380",
    "end": "1833779"
  },
  {
    "text": "I think it's a really powerful powerful combo um I think one one final thing before we",
    "start": "1833779",
    "end": "1839659"
  },
  {
    "text": "move on I think like the things that are really like are great about being able to integrate like ragas and langsmith is",
    "start": "1839659",
    "end": "1846500"
  },
  {
    "text": "that with fragus you can get started without having a label data set you get all these great metrics and you can look",
    "start": "1846500",
    "end": "1851840"
  },
  {
    "text": "through at the results and then from that you can use langsmith to help investigate things you can make",
    "start": "1851840",
    "end": "1857179"
  },
  {
    "text": "disagreements with that um and then continue to curate this data set so that's more reliable and that you",
    "start": "1857179",
    "end": "1862460"
  },
  {
    "text": "the types of evaluation results are more correlated with what your human judgment would be",
    "start": "1862460",
    "end": "1867860"
  },
  {
    "text": "um and so like with this it can be more of a flywheel and then you can also adjust it to be more specific to your domain like maybe your your generation",
    "start": "1867860",
    "end": "1875120"
  },
  {
    "text": "pipeline is very focused on the specific domain of of problems and maybe you want to be focused on",
    "start": "1875120",
    "end": "1881120"
  },
  {
    "text": "um like yeah specific information there maybe the hallucination metric you want",
    "start": "1881120",
    "end": "1886220"
  },
  {
    "text": "to include additional things in apart from the retrieved data that is very specific to your context you can use",
    "start": "1886220",
    "end": "1891860"
  },
  {
    "text": "that to have things that are very specific to whatever you want to be deploying",
    "start": "1891860",
    "end": "1896919"
  },
  {
    "text": "awesome and yeah Justin just shared a example or the notebook in the in the",
    "start": "1897279",
    "end": "1902539"
  },
  {
    "text": "chat so people should definitely check that out um all right so now let's move on to the",
    "start": "1902539",
    "end": "1908059"
  },
  {
    "text": "the final presenter Pedro who who is actually building something um he's the only one here building that",
    "start": "1908059",
    "end": "1913880"
  },
  {
    "text": "application instead of tools so maybe you can start chatting about uh Noah and and the product that you're building and",
    "start": "1913880",
    "end": "1919940"
  },
  {
    "text": "then also how do you think about evaluation like you know",
    "start": "1919940",
    "end": "1925279"
  },
  {
    "text": "there's all these different techniques what are you actually doing in in practice if if anything at all totally",
    "start": "1925279",
    "end": "1931880"
  },
  {
    "text": "know and I appreciate the time and the presentation was great um so for for starters um with Noah we",
    "start": "1931880",
    "end": "1937399"
  },
  {
    "text": "just try to bring the chat experience but with a lot more context on the user um I think right now users just go under",
    "start": "1937399",
    "end": "1944000"
  },
  {
    "text": "documents copy and paste and try to always bring context manually to chat",
    "start": "1944000",
    "end": "1949100"
  },
  {
    "text": "GPT um and you know I think retrievals and our infrastructure is very good enough",
    "start": "1949100",
    "end": "1954380"
  },
  {
    "text": "to create an automated system that we just get the documents that are relevant to any workflow and the user just talks",
    "start": "1954380",
    "end": "1961399"
  },
  {
    "text": "to the chat assuming that the chat can retrieve the best piece of context uh so",
    "start": "1961399",
    "end": "1966740"
  },
  {
    "text": "we build Noah to have an experience that users just come in and they integrate with Google drive or notion",
    "start": "1966740",
    "end": "1972860"
  },
  {
    "text": "um they populate in hundreds of documents and then they can just start chatting with them uh without necessarily knowing what where exactly",
    "start": "1972860",
    "end": "1979760"
  },
  {
    "text": "the documents are or certain keywords is just finally a copilot that can you know help people",
    "start": "1979760",
    "end": "1986179"
  },
  {
    "text": "um you know generate things or find things or you know find out of whatever use case that that application is",
    "start": "1986179",
    "end": "1991940"
  },
  {
    "text": "relevant um and for us you know evaluations is interesting because um it's a horizontal model so we're not",
    "start": "1991940",
    "end": "1998480"
  },
  {
    "text": "particularly optimizing for accountants or lawyers or um any other profession we want that to",
    "start": "1998480",
    "end": "2005500"
  },
  {
    "text": "be horizontal and multi-purpose so whenever um you know we try to optimize for a specific workflow or a query we usually",
    "start": "2005500",
    "end": "2012880"
  },
  {
    "text": "find that it hurts the the general uh performance of the system and then we",
    "start": "2012880",
    "end": "2018039"
  },
  {
    "text": "usually go back to well is this just a pure retrieval the best way to go about it um so I'm excited to share a bit of",
    "start": "2018039",
    "end": "2023860"
  },
  {
    "text": "the learnings that we we found in you know building the system and user input",
    "start": "2023860",
    "end": "2029159"
  },
  {
    "text": "uh and usually user input was a North star for us usually uh it's pretty",
    "start": "2029159",
    "end": "2034480"
  },
  {
    "text": "binary it's did they say either the answer is good or the answer is completely out of the way",
    "start": "2034480",
    "end": "2040539"
  },
  {
    "text": "um so this is um kind of a matrix that I oh sorry shirt wrong",
    "start": "2040539",
    "end": "2046179"
  },
  {
    "text": "um foreign",
    "start": "2046179",
    "end": "2052138"
  },
  {
    "text": "here so let's see yeah okay",
    "start": "2052859",
    "end": "2059378"
  },
  {
    "text": "um so the Matrix is it you know we built and we optimize we try to optimize for a bunch of the factors that are involved",
    "start": "2059379",
    "end": "2065679"
  },
  {
    "text": "in retrieval chunk size quantity putting a lot of um intermediary genes to try to get a",
    "start": "2065679",
    "end": "2072580"
  },
  {
    "text": "better answer uh and what we found is that usually the users will",
    "start": "2072580",
    "end": "2079358"
  },
  {
    "text": "um be happy with an answer when the model captures the right chunk uh and if",
    "start": "2079359",
    "end": "2084700"
  },
  {
    "text": "the chunk is big enough to provide a very good answer um so you know we tried with very",
    "start": "2084700",
    "end": "2091179"
  },
  {
    "text": "um we try to retrieval with very few uh big chunks and with a lot of chunks that",
    "start": "2091179",
    "end": "2097780"
  },
  {
    "text": "are medium-sized and there's usually a balance there that like the Chuck needs to be big enough but also there needs to",
    "start": "2097780",
    "end": "2105099"
  },
  {
    "text": "be some of them so that there's a complete context being captured in the uh in the llm and the answer is usually",
    "start": "2105099",
    "end": "2112180"
  },
  {
    "text": "going to be satisfactory so that we found that to be uh you know the best parameters to provide the best answer to",
    "start": "2112180",
    "end": "2118839"
  },
  {
    "text": "users as opposed to chains.educe what what needs to be done",
    "start": "2118839",
    "end": "2123940"
  },
  {
    "text": "and then that that agent or chain deciding documents to search through",
    "start": "2123940",
    "end": "2129880"
  },
  {
    "text": "um or or things to do so relevant chunks side and quantity uh were very important for us in a more peer retrieval system",
    "start": "2129880",
    "end": "2137160"
  },
  {
    "text": "uh and then of course um you know context size is really key we try not to partition our queries in a",
    "start": "2137160",
    "end": "2144640"
  },
  {
    "text": "lot of chains I usually just one or two chains with the user input uh the back",
    "start": "2144640",
    "end": "2150099"
  },
  {
    "text": "to retrieval stuffing that and into a chain um was very good to to get good answers",
    "start": "2150099",
    "end": "2156640"
  },
  {
    "text": "um intermediary genes are good sometimes because sometimes users are like compare uh you know information from this",
    "start": "2156640",
    "end": "2162880"
  },
  {
    "text": "document with information from this other document and then you kind of need a chain to understand that you need to",
    "start": "2162880",
    "end": "2169119"
  },
  {
    "text": "do two Vector retrievals to get um chunks from from different queries that was really good for optimizing for",
    "start": "2169119",
    "end": "2175720"
  },
  {
    "text": "this type of query but it wasn't it wasn't good for overall performance because you know every query would hit",
    "start": "2175720",
    "end": "2181660"
  },
  {
    "text": "into that chain and that would hurt our our latency and that would usually also",
    "start": "2181660",
    "end": "2187180"
  },
  {
    "text": "hurt the quality of the output because it would get repetitive chunks and then",
    "start": "2187180",
    "end": "2192220"
  },
  {
    "text": "it wouldn't be as good um so we honestly found that you know a more pure high quality retrieval is is",
    "start": "2192220",
    "end": "2199300"
  },
  {
    "text": "what right now use the best answers to users uh and then I think the factor to",
    "start": "2199300",
    "end": "2205900"
  },
  {
    "text": "optimize for is that when the user data set has outdated or conflicted information the LM is crazy because you",
    "start": "2205900",
    "end": "2212260"
  },
  {
    "text": "know one chunk says one thing and the other chunks is another thing um we just try to make sure that",
    "start": "2212260",
    "end": "2217960"
  },
  {
    "text": "whenever the user is uploading uh their documents uh that they don't take like",
    "start": "2217960",
    "end": "2223359"
  },
  {
    "text": "the best solution was to not take uh two old documents because usually you know if you're talking about product strategy",
    "start": "2223359",
    "end": "2229300"
  },
  {
    "text": "or sales metrics you're going to have documents from 2019 2020 2021",
    "start": "2229300",
    "end": "2234339"
  },
  {
    "text": "um but if you remove those documents out of the way they're not relevant anyways um you're going to get very good answers",
    "start": "2234339",
    "end": "2240099"
  },
  {
    "text": "so you just don't you just be sure to not upload older uh information that was better than to try to optimize that and",
    "start": "2240099",
    "end": "2246700"
  },
  {
    "text": "change it's like oh you have two you know different chunks uh one is newer one is older just for as high as the",
    "start": "2246700",
    "end": "2253300"
  },
  {
    "text": "newer and give a better weight for that um that was a good uh change to do and",
    "start": "2253300",
    "end": "2258400"
  },
  {
    "text": "that worked well for for those stories that had those conflict information going on but it wasn't good for the",
    "start": "2258400",
    "end": "2264880"
  },
  {
    "text": "overall model because again we're trying to optimize for you know people um in multiple Industries using it for",
    "start": "2264880",
    "end": "2270700"
  },
  {
    "text": "their own um use cases so um that wasn't a good optimization considering the overall",
    "start": "2270700",
    "end": "2277720"
  },
  {
    "text": "um uh performance and then just lastly users are very generous so you know if",
    "start": "2277720",
    "end": "2283000"
  },
  {
    "text": "if they ask something and it doesn't go they're trying to be more specific in their queries I think historically the",
    "start": "2283000",
    "end": "2288579"
  },
  {
    "text": "four LMS search and working with documents has always been a keyword",
    "start": "2288579",
    "end": "2295180"
  },
  {
    "text": "um type of effort so they're very used to being specific with their queries which really helps retrieval they're really like usually they're like go on",
    "start": "2295180",
    "end": "2301960"
  },
  {
    "text": "this document and and summarize it or they're very good with keywords which really helps uh performance of our of",
    "start": "2301960",
    "end": "2308200"
  },
  {
    "text": "our software because you know it just it just really helps retrieval",
    "start": "2308200",
    "end": "2313240"
  },
  {
    "text": "um and that memory is really hard to optimize I think you know I'm still trying to crack the code there but",
    "start": "2313240",
    "end": "2319180"
  },
  {
    "text": "memory is usually a prompt uh that that you pass and any word or any information that is weird it it usually messes with",
    "start": "2319180",
    "end": "2327880"
  },
  {
    "text": "the with the ultimate output uh so for us just getting the last message is is",
    "start": "2327880",
    "end": "2334480"
  },
  {
    "text": "optimal for memory um so whatever the user said before that's the only piece of context that we",
    "start": "2334480",
    "end": "2340660"
  },
  {
    "text": "would give that element the subsequent chain as memory if you add more it kind of goes into that issue of a lot of",
    "start": "2340660",
    "end": "2348040"
  },
  {
    "text": "conflict information that is weird and then the LM doesn't know what to do um and I think there's probably a sweet",
    "start": "2348040",
    "end": "2354040"
  },
  {
    "text": "spot uh memory prompt that charge PT uses that they don't share but that's usually probably going to be the gold",
    "start": "2354040",
    "end": "2359920"
  },
  {
    "text": "standard uh text for retrieval is amazing so tabular data notion HTML",
    "start": "2359920",
    "end": "2365680"
  },
  {
    "text": "you know I think it still depends on the on the quality of the um of the loading uh but that's you know",
    "start": "2365680",
    "end": "2372339"
  },
  {
    "text": "still um still not 100 and then you know the problem needs to be very pure it needs",
    "start": "2372339",
    "end": "2378640"
  },
  {
    "text": "to be very okay you retrieve the chunks and then you just ask based on that based on the context give an answer if",
    "start": "2378640",
    "end": "2385900"
  },
  {
    "text": "you add your helpful salesperson or if you're um a happy person or or anything there's",
    "start": "2385900",
    "end": "2393099"
  },
  {
    "text": "a black box um around that word that's going to hallucinate the final answer so being as",
    "start": "2393099",
    "end": "2398320"
  },
  {
    "text": "pure and as concise as possible in the prompt after the retrieval is done um is is really the best for for",
    "start": "2398320",
    "end": "2404619"
  },
  {
    "text": "preventing hallucination um so those are the like a few of the learnings that that we had and talking",
    "start": "2404619",
    "end": "2409960"
  },
  {
    "text": "to users and and building NOAA uh to ensure that the valuations are good but",
    "start": "2409960",
    "end": "2415660"
  },
  {
    "text": "um yeah it's it's been a very improval process of talking to users and making sure that chunks or Retreat answers were",
    "start": "2415660",
    "end": "2421720"
  },
  {
    "text": "satisfactory um to make sure the application works best for everybody this is super interesting and so yeah so",
    "start": "2421720",
    "end": "2427780"
  },
  {
    "text": "one of my questions was going to be exactly what it sounded like you were talking a little bit towards the end so you have all these learnings and when",
    "start": "2427780",
    "end": "2433960"
  },
  {
    "text": "you're when you're saying like oh we found this to be better than this how exactly did you find that was that like collecting thumbs up thumbs down feed",
    "start": "2433960",
    "end": "2440320"
  },
  {
    "text": "that was that purely kind of like running it with users and asking them what they thought like what's the what's",
    "start": "2440320",
    "end": "2445599"
  },
  {
    "text": "like how exactly are you doing kind of like yeah so it's just a mix of of all of the above uh so thumbs up and thumbs",
    "start": "2445599",
    "end": "2451780"
  },
  {
    "text": "down are really relevant to us so we can just you know users are open to it you know whenever we see a thumbs down",
    "start": "2451780",
    "end": "2457599"
  },
  {
    "text": "answer we hop on a call with a user and we look on the on the backlog and see okay like did it get the right chunk or",
    "start": "2457599",
    "end": "2464320"
  },
  {
    "text": "or why was the answer bad usually a bad answer came from a retrieval that didn't",
    "start": "2464320",
    "end": "2469660"
  },
  {
    "text": "get the right chunk either because it was stuffed somewhere hidden so so that",
    "start": "2469660",
    "end": "2476380"
  },
  {
    "text": "the retrieval wasn't you know smart enough um or it usually happen in data that",
    "start": "2476380",
    "end": "2482619"
  },
  {
    "text": "wasn't properly loaded so you know data that wasn't as CSV or data that was in a",
    "start": "2482619",
    "end": "2488380"
  },
  {
    "text": "slide or PDF uh for tax First Trade text it was really good but yeah so usually you know",
    "start": "2488380",
    "end": "2495339"
  },
  {
    "text": "we saw that you know a thumbs down or with shadowed users using it uh to make",
    "start": "2495339",
    "end": "2501220"
  },
  {
    "text": "sure that you know they they had you know we just wanted to see okay users are typing um you know go on on",
    "start": "2501220",
    "end": "2509140"
  },
  {
    "text": "um my product strategy um launch document and",
    "start": "2509140",
    "end": "2515260"
  },
  {
    "text": "um generate four more ideas and just like see how the users use it and if the retrieval was able to capture that was",
    "start": "2515260",
    "end": "2521560"
  },
  {
    "text": "really helpful in understanding okay like this is those are things to optimize for and usually the the change",
    "start": "2521560",
    "end": "2526839"
  },
  {
    "text": "that we try to optimize with the ideas that we have usually are not the right ones considering how the users use it",
    "start": "2526839",
    "end": "2533560"
  },
  {
    "text": "users are very keyword driven um so yeah that's what we found super",
    "start": "2533560",
    "end": "2538720"
  },
  {
    "text": "interesting more specific questions I have you say like text-based sources perform the best in Vector retriever is",
    "start": "2538720",
    "end": "2544119"
  },
  {
    "text": "this opposed to like PDFs or like uh are you guys doing you're not doing multimodal yet are you or or yeah so so",
    "start": "2544119",
    "end": "2552040"
  },
  {
    "text": "we are we're doing all types of uh of data sources so CSV and",
    "start": "2552040",
    "end": "2557560"
  },
  {
    "text": "um slides and everything uh but free text is the best just because it's the simple it's the simplest to load",
    "start": "2557560",
    "end": "2564460"
  },
  {
    "text": "um we usually the the most tricky to load for us is notion because it's a",
    "start": "2564460",
    "end": "2569740"
  },
  {
    "text": "huge HTML that you have that has so many components that you have to load properly and to ensure that it's",
    "start": "2569740",
    "end": "2575980"
  },
  {
    "text": "connected with the other components and that's a that's a full chunk so that's tricky for us and sometimes you know",
    "start": "2575980",
    "end": "2583000"
  },
  {
    "text": "very important context it's split across four or five chunks and the retrieval is not uh that great",
    "start": "2583000",
    "end": "2589780"
  },
  {
    "text": "um so that that's what I mean by that usually straight text uh when you vectorize a straight text that performs",
    "start": "2589780",
    "end": "2595960"
  },
  {
    "text": "really really well uh and it's still getting there for um like for csvs and",
    "start": "2595960",
    "end": "2601119"
  },
  {
    "text": "other types of data which would do them and we have good responses but it's it's",
    "start": "2601119",
    "end": "2606400"
  },
  {
    "text": "still not 100 uh and I do think that as agents evolve and you know as gpt4 gets",
    "start": "2606400",
    "end": "2612520"
  },
  {
    "text": "faster uh and then those things happen uh we're gonna see you know an ability",
    "start": "2612520",
    "end": "2618040"
  },
  {
    "text": "to really query information from from those as well awesome and then the last question I had",
    "start": "2618040",
    "end": "2624339"
  },
  {
    "text": "you had one bullet point that was like um generous context length is is key",
    "start": "2624339",
    "end": "2630700"
  },
  {
    "text": "or contact sizes key have you so there's been like some talk around like as the",
    "start": "2630700",
    "end": "2635740"
  },
  {
    "text": "as the context Windows gets longer and longer the llms kind of like forget what's in the middle and so maybe there's actually some diminishing",
    "start": "2635740",
    "end": "2641560"
  },
  {
    "text": "returns have you seen that in practice or yes yes we actually have so it's it goes",
    "start": "2641560",
    "end": "2648040"
  },
  {
    "text": "It goes as a parabola so you have to give it a context so it answers but if",
    "start": "2648040",
    "end": "2654280"
  },
  {
    "text": "you just if you just if you could put you know 9 000 tokens of context and ask",
    "start": "2654280",
    "end": "2659859"
  },
  {
    "text": "a question you wouldn't get like it would be better to have retrieved uh 4",
    "start": "2659859",
    "end": "2665440"
  },
  {
    "text": "000 token chunk and and gotten the answer from it so we actually found that really small chunk size is really really",
    "start": "2665440",
    "end": "2672220"
  },
  {
    "text": "bad because it's just kind of a keyword search um so you need to be able to retrieve at",
    "start": "2672220",
    "end": "2678579"
  },
  {
    "text": "least you know a Page worth of context that is around uh you know the the hot",
    "start": "2678579",
    "end": "2684460"
  },
  {
    "text": "spot of the answer uh but yeah as you as you really really um you know uh amplify that you do find",
    "start": "2684460",
    "end": "2692619"
  },
  {
    "text": "that that diminishing return happening so but it's important to have to strike that balance because very little or or",
    "start": "2692619",
    "end": "2699819"
  },
  {
    "text": "too much really hurt super interesting",
    "start": "2699819",
    "end": "2705880"
  },
  {
    "text": "um all right there's some good questions in the chat so let's maybe jump there and and some of them are uh maybe more",
    "start": "2705880",
    "end": "2712060"
  },
  {
    "text": "relevant for for ragus and some of them are probably more relevant for NOAA so I'll try to pick a combination of of all",
    "start": "2712060",
    "end": "2717460"
  },
  {
    "text": "of them for the next uh I guess we got like eight or nine minutes um I think this one's for the ragas team",
    "start": "2717460",
    "end": "2724000"
  },
  {
    "text": "so you guys mentioned that sometimes the embedding model is inappropriate for the corpus and and this is are there evaluation",
    "start": "2724000",
    "end": "2730960"
  },
  {
    "text": "metrics that point to that as a cause as opposed to some other defect in the rag Pipeline and maybe more generally like",
    "start": "2730960",
    "end": "2737020"
  },
  {
    "text": "when all these steps go wrong how are there how can you kind of like know what's the underlying cause of it",
    "start": "2737020",
    "end": "2742960"
  },
  {
    "text": "whether it's like the embedding model or the chunk size or anything like that are there any heuristics or best practices",
    "start": "2742960",
    "end": "2748599"
  },
  {
    "text": "that you guys have observed and no I think or uh Pedro I think you're still sharing your screen",
    "start": "2748599",
    "end": "2755319"
  },
  {
    "text": "um so yeah I think you turned off your video um instead of unsharing your screen oh",
    "start": "2755319",
    "end": "2762460"
  },
  {
    "text": "that's right oh stop sharing oh wow I see yeah there",
    "start": "2762460",
    "end": "2767740"
  },
  {
    "text": "you go if it's on the Chrome tab yeah yeah so to answer the question I",
    "start": "2767740",
    "end": "2775420"
  },
  {
    "text": "think uh when there is issues with the embedding the the recruitment uh uh",
    "start": "2775420",
    "end": "2783579"
  },
  {
    "text": "retream any or or the information which is required to answer the question so in",
    "start": "2783579",
    "end": "2788920"
  },
  {
    "text": "that case most cases the one to look at because the conductor",
    "start": "2788920",
    "end": "2794440"
  },
  {
    "text": "will be low because it doesn't really have the amount of information which can be used to which is necessary to answer",
    "start": "2794440",
    "end": "2800680"
  },
  {
    "text": "the question so yeah so uh another situation can be",
    "start": "2800680",
    "end": "2806980"
  },
  {
    "text": "again chunk says where you know context uh uh our respect across multiple chunks",
    "start": "2806980",
    "end": "2814060"
  },
  {
    "text": "and it uh it only um retrieves the chunk of theory or",
    "start": "2814060",
    "end": "2820180"
  },
  {
    "text": "maybe some of the chunks to answer the question so again in that case something which is very important in in case you",
    "start": "2820180",
    "end": "2826660"
  },
  {
    "text": "looked amazing uh metrics so yeah there's a series of questions around",
    "start": "2826660",
    "end": "2834579"
  },
  {
    "text": "basically like um what basically what traditional methods of assessment does ragus compare",
    "start": "2834579",
    "end": "2843220"
  },
  {
    "text": "slash compete with and can you even use them in in conjunction",
    "start": "2843220",
    "end": "2848680"
  },
  {
    "text": "um whether yeah how do you guys think about that like what are traditional",
    "start": "2848680",
    "end": "2853720"
  },
  {
    "text": "metrics for kind of like retrieval or question answering and how does ragus compare if you use them together",
    "start": "2853720",
    "end": "2858819"
  },
  {
    "text": "right so most of the financial metrics around generation is over was generated",
    "start": "2858819",
    "end": "2865240"
  },
  {
    "text": "or was you know implemented pre-gpt and generational quality itself was someone",
    "start": "2865240",
    "end": "2871560"
  },
  {
    "text": "and these matrixes mostly and also works mostly with only nitrated ground group",
    "start": "2871560",
    "end": "2878319"
  },
  {
    "text": "but in case of ragas it mostly focus on reference fee uh evaluation and hence",
    "start": "2878319",
    "end": "2884500"
  },
  {
    "text": "these things are actually a little bit different to compare Apple",
    "start": "2884500",
    "end": "2889540"
  },
  {
    "text": "sample so so we don't really compare it to other the traditional methods but",
    "start": "2889540",
    "end": "2895240"
  },
  {
    "text": "yeah I mean if you have got answers which are particularly short form one",
    "start": "2895240",
    "end": "2900700"
  },
  {
    "text": "can even go with traditional metrics instead of using llama system because short form access are particularly easy",
    "start": "2900700",
    "end": "2907180"
  },
  {
    "text": "to compare compared to long form answers",
    "start": "2907180",
    "end": "2911940"
  },
  {
    "text": "awesome Pedro I think there's two cool ones for you the first one you mentioned like",
    "start": "2912640",
    "end": "2919599"
  },
  {
    "text": "people would love to learn more about how you deal with stale and outdated information this is a very painful",
    "start": "2919599",
    "end": "2925300"
  },
  {
    "text": "problem so yeah you'd mentioned like in from what I understood you actually kind of like put the burden a little bit on",
    "start": "2925300",
    "end": "2931480"
  },
  {
    "text": "the user in terms of like trying to get them to upload only the most relevant things is that correct do you do",
    "start": "2931480",
    "end": "2937420"
  },
  {
    "text": "anything else to kind of like uh safeguard that did you try anything that that didn't work even",
    "start": "2937420",
    "end": "2943420"
  },
  {
    "text": "yeah so so we did try to uh wait out older so whatever retrieval happens and",
    "start": "2943420",
    "end": "2951160"
  },
  {
    "text": "chunks um are very disparate and and their in their date we really uh try to",
    "start": "2951160",
    "end": "2957700"
  },
  {
    "text": "give a better way to the newer chunk um it was just tricky to pass that on a",
    "start": "2957700",
    "end": "2963040"
  },
  {
    "text": "general chain of you have do you have to do that or not so whenever we did that it worked best for for older jobs for",
    "start": "2963040",
    "end": "2969880"
  },
  {
    "text": "for this issue when whenever you have those chunks happening but it hurt the overall performance of the whole system",
    "start": "2969880",
    "end": "2975880"
  },
  {
    "text": "because then now every query needed to um be passed into a chain they needed to",
    "start": "2975880",
    "end": "2981040"
  },
  {
    "text": "decide do you need to go into this chain and that hurt everything so um users are really good at okay like they ask the",
    "start": "2981040",
    "end": "2987819"
  },
  {
    "text": "question that are that is very directed right so they're like um find me the the product the day",
    "start": "2987819",
    "end": "2993099"
  },
  {
    "text": "usually set a time a timeline for for their query so the product document of",
    "start": "2993099",
    "end": "2998260"
  },
  {
    "text": "last year or the product document of this year for our uploads we try to restrict the very old documents so for",
    "start": "2998260",
    "end": "3005040"
  },
  {
    "text": "whenever you're uploading things to NOAA we only take documents up to January 2022 that way you avoid having documents",
    "start": "3005040",
    "end": "3013319"
  },
  {
    "text": "from 2015 you know those very old documents that have very different information so putting the burden user is is really good because you know you",
    "start": "3013319",
    "end": "3020460"
  },
  {
    "text": "don't have to overly optimize for it for up for outdated chunks because they're",
    "start": "3020460",
    "end": "3025980"
  },
  {
    "text": "not relevant to the user anyways right so the user can just choose okay we're going to put on only the relevant",
    "start": "3025980",
    "end": "3031260"
  },
  {
    "text": "documents that are usually the newer ones and then so you just don't even have to put them into into the pool of",
    "start": "3031260",
    "end": "3036480"
  },
  {
    "text": "retrieval anyways because that just caused a lot of problems um so yeah we try to optimize with the",
    "start": "3036480",
    "end": "3041700"
  },
  {
    "text": "chain uh it just hurt overall performance they were interesting and then the other",
    "start": "3041700",
    "end": "3047339"
  },
  {
    "text": "question that I think is really interesting is how can Noah answer questions that need",
    "start": "3047339",
    "end": "3052559"
  },
  {
    "text": "to aggregate data across different documents and and how do you handle that totally uh and and that's key uh when",
    "start": "3052559",
    "end": "3060059"
  },
  {
    "text": "you're setting up the number of chunks retrieve parameter um so everything is you know factorized",
    "start": "3060059",
    "end": "3066420"
  },
  {
    "text": "uh and and then the retrieval retrieves chunks that can that can be from a lot of multiple documents you're going to",
    "start": "3066420",
    "end": "3073500"
  },
  {
    "text": "have one or two chunks that are the hot spot of the answer that will come from one document but the more uh chunks you",
    "start": "3073500",
    "end": "3080099"
  },
  {
    "text": "retrieve the more likely they come from different documents and then they aggregate uh into a chain and then the",
    "start": "3080099",
    "end": "3087359"
  },
  {
    "text": "chain is multi-document um so um you know increasing the number of",
    "start": "3087359",
    "end": "3092700"
  },
  {
    "text": "chunks retrieved just maximizes the the chances of you getting chunks from multiple documents and then if they're",
    "start": "3092700",
    "end": "3098940"
  },
  {
    "text": "relevant the LM will be really good at including the context into the answer",
    "start": "3098940",
    "end": "3104819"
  },
  {
    "text": "um so yeah we just we just make sure to put a chunk size that is big enough so",
    "start": "3104819",
    "end": "3110579"
  },
  {
    "text": "that a chunk size retrieval that is big enough so that you get usually you know context from two three or four",
    "start": "3110579",
    "end": "3118500"
  },
  {
    "text": "um documents so usually when Noah provides an answer with say the sources and usually you know it comes from One",
    "start": "3118500",
    "end": "3124140"
  },
  {
    "text": "Source but a lot of times it comes from two or three yeah interesting do you have any like",
    "start": "3124140",
    "end": "3131099"
  },
  {
    "text": "sense of how how many questions are about multiple documents or are most",
    "start": "3131099",
    "end": "3136260"
  },
  {
    "text": "of them just about one document I'd say like 40 percent uh are are about you",
    "start": "3136260",
    "end": "3142319"
  },
  {
    "text": "know Q or more yeah so usually most are about one",
    "start": "3142319",
    "end": "3148440"
  },
  {
    "text": "document because the user is like okay like go on the document and ask as something and even if it retrieves jumps",
    "start": "3148440",
    "end": "3153780"
  },
  {
    "text": "from other documents not relevant it'll Cuts them off um but in a lot of times you know you",
    "start": "3153780",
    "end": "3159059"
  },
  {
    "text": "have you know two or three documents that like have something relevant that is sometimes like 10 of the chunk but",
    "start": "3159059",
    "end": "3165119"
  },
  {
    "text": "that really helps The Final Answer um so making sure that you're allowing you're retrieving",
    "start": "3165119",
    "end": "3170880"
  },
  {
    "text": "um you know five six seven chunks and then you know passing that burden of you",
    "start": "3170880",
    "end": "3176099"
  },
  {
    "text": "know deciding if they're relevant or not to that alignment is really is really good and then you have a multi-document",
    "start": "3176099",
    "end": "3181260"
  },
  {
    "text": "um answer awesome and then last question and this came up a few times so I think it's a good one to end on this is",
    "start": "3181260",
    "end": "3187440"
  },
  {
    "text": "probably for mostly for uh will and the ragus guys um around like the data that you",
    "start": "3187440",
    "end": "3194760"
  },
  {
    "text": "evaluate where where should this come from how many data points do you need how do you generate this how like a lot",
    "start": "3194760",
    "end": "3201240"
  },
  {
    "text": "of people like how how can I come up with data to actually evaluate and get these metrics on",
    "start": "3201240",
    "end": "3208460"
  },
  {
    "text": "um do you guys want to answer that first okay",
    "start": "3210720",
    "end": "3216780"
  },
  {
    "text": "yeah um yeah so I guess like so from the langsmith side I think one of our main",
    "start": "3216780",
    "end": "3223859"
  },
  {
    "text": "value props and like interest is that we want to get data that's as representative as possible to what",
    "start": "3223859",
    "end": "3229680"
  },
  {
    "text": "you're going to see in production and we wanted to cover the full distribution of use cases like whenever you're putting on a system into production yeah you",
    "start": "3229680",
    "end": "3236579"
  },
  {
    "text": "care about average performance you want to get that like P95 performance and similarly for the retrieval quality and",
    "start": "3236579",
    "end": "3241920"
  },
  {
    "text": "the answer quality you want to be trying to get as much cover of this tail distribution as possible I think this is really where llms are useful but also",
    "start": "3241920",
    "end": "3249420"
  },
  {
    "text": "where it's like becomes a little bit hard um and there's like obviously considerations around privacy and",
    "start": "3249420",
    "end": "3255119"
  },
  {
    "text": "everything so you can synthesize different things that are the representative there in terms of like",
    "start": "3255119",
    "end": "3260339"
  },
  {
    "text": "how like the like tactics to do this um there's definitely tools like nomic",
    "start": "3260339",
    "end": "3265920"
  },
  {
    "text": "and other things that use Vector distances um I'm not sure how useful those are what I've found and would love um yeah",
    "start": "3265920",
    "end": "3272579"
  },
  {
    "text": "the Rockers folks opinion on this but um I found it quite useful um to start with a small one and then",
    "start": "3272579",
    "end": "3278940"
  },
  {
    "text": "try to extend in different dimensions and see where the existing system fails and then I can manually annotate or use",
    "start": "3278940",
    "end": "3284579"
  },
  {
    "text": "like assisted annotations to then gradually build that out so that you have it still be pretty high quality",
    "start": "3284579",
    "end": "3290760"
  },
  {
    "text": "without just going for this huge quantity that can slow down your eval process",
    "start": "3290760",
    "end": "3295940"
  },
  {
    "text": "awesome yeah I yeah I agree to this point and uh as in the test set should",
    "start": "3297240",
    "end": "3304200"
  },
  {
    "text": "uh should be very reflective of what we see in production and there are some automated tools which we can do and",
    "start": "3304200",
    "end": "3310260"
  },
  {
    "text": "methods to filter problems or the test logs so that you can automatically",
    "start": "3310260",
    "end": "3315839"
  },
  {
    "text": "formulate a set of uh small a small test set from the ones you see in production",
    "start": "3315839",
    "end": "3321300"
  },
  {
    "text": "this can be something like automated reduplication and filtering out forms the internet persons data points from",
    "start": "3321300",
    "end": "3328440"
  },
  {
    "text": "different uh different data ports so yeah and then I think line Smith is",
    "start": "3328440",
    "end": "3334319"
  },
  {
    "text": "particularly useful in manually selected uh tested curation",
    "start": "3334319",
    "end": "3340579"
  },
  {
    "text": "all right awesome that about rats it wraps it up for today I think this was uh I think this was super interesting",
    "start": "3340800",
    "end": "3347099"
  },
  {
    "text": "lots of uh lots of lots of great learnings on on all the metrics that Rogers has and Pedro I really liked the",
    "start": "3347099",
    "end": "3353040"
  },
  {
    "text": "I think you had eight different bullet points those were awesome I think those are very real world kind of like learning so I think it was a great",
    "start": "3353040",
    "end": "3359460"
  },
  {
    "text": "combination of tooling and real world application so thank you guys so much for joining",
    "start": "3359460",
    "end": "3365280"
  },
  {
    "text": "um yeah that's all I got and thank you everyone for tuning in see at the next one thanks everyone thank you",
    "start": "3365280",
    "end": "3373819"
  },
  {
    "text": "foreign",
    "start": "3375480",
    "end": "3377720"
  }
]