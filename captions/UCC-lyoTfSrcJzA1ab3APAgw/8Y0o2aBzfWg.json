[
  {
    "text": "below everyone we're going live with an awesome uh awesome topic awesome speakers today",
    "start": "2700",
    "end": "11400"
  },
  {
    "text": "um but before we jump into it minor logistic things this is being recorded",
    "start": "11400",
    "end": "17100"
  },
  {
    "text": "um so it will be available at the link after the fact and then we'll also put it up on YouTube later in the week",
    "start": "17100",
    "end": "22920"
  },
  {
    "text": "um if you guys have questions during the during during the webinar put them not",
    "start": "22920",
    "end": "28320"
  },
  {
    "text": "in the normal chat box but there's a little box uh for questions and answers it's the it's on the side it's got the",
    "start": "28320",
    "end": "33840"
  },
  {
    "text": "one with the question mark in it if you put them there and then and then upload the ones that you like best",
    "start": "33840",
    "end": "39239"
  },
  {
    "text": "um and will basically answer those in terms of schedule we'll do is we'll we'll go uh we'll do quick and little",
    "start": "39239",
    "end": "45780"
  },
  {
    "text": "intros after this then we'll jump right into it with with uh James taking it over for a presentation then Lance and",
    "start": "45780",
    "end": "52440"
  },
  {
    "text": "then back to James and then open it up for audience QA so that's what we'll get that's when we'll get to all the uh",
    "start": "52440",
    "end": "58620"
  },
  {
    "text": "question answer stuff um in in the Box um",
    "start": "58620",
    "end": "63899"
  },
  {
    "text": "that's pretty much it for Logistics pretty simple pretty straightforward maybe we can do quick intros my name is",
    "start": "63899",
    "end": "69420"
  },
  {
    "text": "Harrison uh I work at Lane chain so trying to make it as easy as possible to build online applications uh Lance do",
    "start": "69420",
    "end": "76080"
  },
  {
    "text": "you want to do a quick intro I'm Lance I'm also on the line chain team and have been doing most recently a",
    "start": "76080",
    "end": "81540"
  },
  {
    "text": "bit of work with llama so looking forward to discussing uh more today and yeah and I'm James I'm at Pine Cone",
    "start": "81540",
    "end": "89340"
  },
  {
    "text": "so uh do like more events today base and",
    "start": "89340",
    "end": "94380"
  },
  {
    "text": "recently been playing around with llama too as well so pretty decides to share um what we've been working on",
    "start": "94380",
    "end": "101939"
  },
  {
    "text": "awesome all right James you want to take it away",
    "start": "101939",
    "end": "107539"
  },
  {
    "text": "second um",
    "start": "109200",
    "end": "115280"
  },
  {
    "text": "goes figuring out to share my screen on here",
    "start": "123540",
    "end": "128659"
  },
  {
    "text": "it is at the bottom next to the new audio turn off camera",
    "start": "128840",
    "end": "135260"
  },
  {
    "text": "okay so it should be this can you see that okay",
    "start": "139140",
    "end": "146180"
  },
  {
    "text": "yes it's a little small it's kind of assumed out yeah",
    "start": "146340",
    "end": "152220"
  },
  {
    "text": "um maybe if I just go",
    "start": "152220",
    "end": "157580"
  },
  {
    "text": "I mean I can do this otherwise it seems to get small yeah I",
    "start": "158040",
    "end": "163080"
  },
  {
    "text": "think uh I think just the PowerPoint slides are fine cool all right that's good",
    "start": "163080",
    "end": "171000"
  },
  {
    "text": "um so yeah we're gonna talk about retrieval augmented generation with the new llama",
    "start": "171000",
    "end": "177780"
  },
  {
    "text": "2 model so the problem that we're trying to solve here",
    "start": "177780",
    "end": "183000"
  },
  {
    "text": "is LMS they work very well we've all seen that",
    "start": "183000",
    "end": "190560"
  },
  {
    "text": "but in terms of what they do know it's limited to what they've learned during",
    "start": "190560",
    "end": "196080"
  },
  {
    "text": "training right so uh llama to the training cutoff is I",
    "start": "196080",
    "end": "202140"
  },
  {
    "text": "think for some of the fine-tuning like fine-tune chat model in June which is",
    "start": "202140",
    "end": "207420"
  },
  {
    "text": "pretty recent but that means if you're asking questions about for example line chain",
    "start": "207420",
    "end": "214560"
  },
  {
    "text": "which is a very fast-moving you know changing Library there's a good chance",
    "start": "214560",
    "end": "220379"
  },
  {
    "text": "at the information that the LM nodes is going to be very outdated dated which is kind of you know a bit of a",
    "start": "220379",
    "end": "228959"
  },
  {
    "text": "problem when you're working or wanting up-to-date information another issue is",
    "start": "228959",
    "end": "235440"
  },
  {
    "text": "you know the data that LMS like longitude been trained on is",
    "start": "235440",
    "end": "241500"
  },
  {
    "text": "not necessarily going to cover the data that you would like to ask questions about",
    "start": "241500",
    "end": "247379"
  },
  {
    "text": "so if you're working in an organization you might have like internal",
    "start": "247379",
    "end": "252659"
  },
  {
    "text": "organization documents that you would like to be considered by your llm",
    "start": "252659",
    "end": "259459"
  },
  {
    "text": "is no it's not going to work unfortunately so this is an example where this was the",
    "start": "259459",
    "end": "267840"
  },
  {
    "text": "first version of gpt4 and I asked it how do I use Airline chain line chain and it just told me",
    "start": "267840",
    "end": "275580"
  },
  {
    "text": "about this blockchain based decentralized AI language model which is not the line chain I was trying to ask",
    "start": "275580",
    "end": "282840"
  },
  {
    "text": "about and I'm also not sure about this information being correct anyway so",
    "start": "282840",
    "end": "289919"
  },
  {
    "text": "yeah this is the issue that we have when lens don't have access to the external",
    "start": "289919",
    "end": "295919"
  },
  {
    "text": "world so the solution is to give them access to that external world",
    "start": "295919",
    "end": "302340"
  },
  {
    "text": "um how we actually do that there's different ways so",
    "start": "302340",
    "end": "309479"
  },
  {
    "text": "let me come down to here what we have at the moment is this we have a lam the",
    "start": "309479",
    "end": "315540"
  },
  {
    "text": "knowledge solvent within it is called parametric knowledge that's everything from the train data it's kind of Frozen",
    "start": "315540",
    "end": "321840"
  },
  {
    "text": "in time all we want is this so we actually want to connect what we call a",
    "start": "321840",
    "end": "328199"
  },
  {
    "text": "knowledge base and more specifically in this case a Vex database to our LM so",
    "start": "328199",
    "end": "334740"
  },
  {
    "text": "it's going to have both like the um the processing power of the brain",
    "start": "334740",
    "end": "340740"
  },
  {
    "text": "that an llm has but we also have this database which allows us to actually manage the",
    "start": "340740",
    "end": "347699"
  },
  {
    "text": "information that Elm knows okay so it's like a traditional database right you",
    "start": "347699",
    "end": "353759"
  },
  {
    "text": "want to be able to add the update information but you want to connect that to your llm",
    "start": "353759",
    "end": "361340"
  },
  {
    "text": "so how do we feed external knowledge into the llm",
    "start": "362280",
    "end": "367919"
  },
  {
    "text": "so one thing like like the first approach that most people are going to",
    "start": "367919",
    "end": "373380"
  },
  {
    "text": "take is okay we're going to put everything into the context window um which",
    "start": "373380",
    "end": "379440"
  },
  {
    "text": "is okay it it depends on how much data you have basically so GT4 if you get",
    "start": "379440",
    "end": "386880"
  },
  {
    "text": "access to the 32k model has a maximum contact so under 32 000 tokens",
    "start": "386880",
    "end": "392759"
  },
  {
    "text": "uh the the cloud model goes up to a hundred thousand tokens which is pretty",
    "start": "392759",
    "end": "398940"
  },
  {
    "text": "big let's sell just 75 000 words",
    "start": "398940",
    "end": "404039"
  },
  {
    "text": "um or about 150 pages of text now if you're just wanting to kind of",
    "start": "404039",
    "end": "410819"
  },
  {
    "text": "like uh chat with a single document it's fine it'll work but if you're for",
    "start": "410819",
    "end": "416520"
  },
  {
    "text": "example chatting with your company documents then all of a sudden you",
    "start": "416520",
    "end": "421740"
  },
  {
    "text": "probably have more than 150 pages of company documents within your organization so things start to get a",
    "start": "421740",
    "end": "428639"
  },
  {
    "text": "little a lot more difficult because you exceed that maximum limit of the context you're",
    "start": "428639",
    "end": "434880"
  },
  {
    "text": "in there and even if we could fit everything into that context window",
    "start": "434880",
    "end": "440960"
  },
  {
    "text": "there's a lot of recent research that tells us that that's not actually a good idea so this um this paper from Stanford",
    "start": "440960",
    "end": "449400"
  },
  {
    "text": "called Lost In The Middle explored the idea of okay what what is",
    "start": "449400",
    "end": "454740"
  },
  {
    "text": "the performance of the model uh when we ask you about information that we put into the context window",
    "start": "454740",
    "end": "460620"
  },
  {
    "text": "and how does that performance vary over time or sorry not over time",
    "start": "460620",
    "end": "465960"
  },
  {
    "text": "um over the amount of text that we put into that context window and they found",
    "start": "465960",
    "end": "471120"
  },
  {
    "text": "that the more stuff you put into your content so in the less performance it is and they have another graph not this one",
    "start": "471120",
    "end": "476759"
  },
  {
    "text": "I'm showing here but another graph where you have basically like a a u-shape and",
    "start": "476759",
    "end": "482060"
  },
  {
    "text": "that is showing that the llm will pay attention to things that are at the",
    "start": "482060",
    "end": "488220"
  },
  {
    "text": "start of your context and at the end of the context but basically ignores everything in the middle",
    "start": "488220",
    "end": "494039"
  },
  {
    "text": "um particularly with longer context models so something you need to be wary of and",
    "start": "494039",
    "end": "500699"
  },
  {
    "text": "something that we kind of want to avoid we don't want to stuff everything into contest when they want to be more",
    "start": "500699",
    "end": "508620"
  },
  {
    "text": "efficient with what we're putting in there so is there a better way",
    "start": "508620",
    "end": "517219"
  },
  {
    "text": "ideally we need to very selectively feed just the high relevant bits of",
    "start": "517919",
    "end": "525660"
  },
  {
    "text": "information into our contacts to avoid this context-sipping issue",
    "start": "525660",
    "end": "531800"
  },
  {
    "text": "um lm's work in natural language so it would be great if our search could do",
    "start": "531839",
    "end": "537839"
  },
  {
    "text": "that too and using a vector database we kind of",
    "start": "537839",
    "end": "543480"
  },
  {
    "text": "satisfy both of those requirements so we can retrieve relevant dots with natural language",
    "start": "543480",
    "end": "549920"
  },
  {
    "text": "and that is something that we would call semantic search so what is semantic surge",
    "start": "549920",
    "end": "557820"
  },
  {
    "text": "a sort of semantic meaning so what is the semantic meaning of a word so it",
    "start": "557820",
    "end": "562860"
  },
  {
    "text": "moves first through examples here at the web Bank um but it's like in a traditional",
    "start": "562860",
    "end": "568980"
  },
  {
    "text": "keyword search all of those would mean the same thing with when you consider semantics they do not consider the same",
    "start": "568980",
    "end": "576540"
  },
  {
    "text": "uh mean the same thing because they have a different context they're being used in a different way the actual meaning of",
    "start": "576540",
    "end": "582899"
  },
  {
    "text": "those words is different and we as humans we understand that and top the bottom two sentences they",
    "start": "582899",
    "end": "590399"
  },
  {
    "text": "don't share any any keywords but in meaning they are basically the same",
    "start": "590399",
    "end": "596940"
  },
  {
    "text": "thing so what we want to do with semantic meaning is we want to put things that",
    "start": "596940",
    "end": "603660"
  },
  {
    "text": "in a human way have a similar meaning in a similar space and things that in a",
    "start": "603660",
    "end": "610800"
  },
  {
    "text": "human way have a different meaning in a different space so that's where semantic search comes in",
    "start": "610800",
    "end": "618240"
  },
  {
    "text": "semantic search works using language models but not generative language",
    "start": "618240",
    "end": "625019"
  },
  {
    "text": "models um so rather than generating text these language models actually generate",
    "start": "625019",
    "end": "631320"
  },
  {
    "text": "embeddings so you can think of them as essentially translating from Human readable text",
    "start": "631320",
    "end": "639959"
  },
  {
    "text": "like this query where is Normandy into machine readable vectors",
    "start": "639959",
    "end": "646740"
  },
  {
    "text": "okay and within that space relevant chunks of text will be",
    "start": "646740",
    "end": "652860"
  },
  {
    "text": "translated into very closely related vectors within that Vector space",
    "start": "652860",
    "end": "660200"
  },
  {
    "text": "and that's our Vector database essentially works so you have all of",
    "start": "660480",
    "end": "665519"
  },
  {
    "text": "these vectors within your vet space we saw them you can manage them",
    "start": "665519",
    "end": "671339"
  },
  {
    "text": "and what uh database like Pancham will do is",
    "start": "671339",
    "end": "676440"
  },
  {
    "text": "allow you to enter a new query Vector which is going to be your query",
    "start": "676440",
    "end": "682019"
  },
  {
    "text": "translated into a vector and very quickly search that space so you can you can do that over like",
    "start": "682019",
    "end": "688980"
  },
  {
    "text": "billions of items right so we don't have um the issue with only being able to put",
    "start": "688980",
    "end": "694920"
  },
  {
    "text": "in like 150 pages of text James I got a quick question for you and this is",
    "start": "694920",
    "end": "700740"
  },
  {
    "text": "jumping a little bit ahead but what do you think we're talking about um or we will be talking about is like",
    "start": "700740",
    "end": "705959"
  },
  {
    "text": "using kind of like llama too and like one of the benefits of that which is like open AI is is basically it's uh you",
    "start": "705959",
    "end": "712260"
  },
  {
    "text": "know it's an open source model you can run it locally you could there's a it's you know there's none of kind of like the Privacy concern stuff like that and",
    "start": "712260",
    "end": "719579"
  },
  {
    "text": "so like just as we use that for text generation there's open source yeah embedding",
    "start": "719579",
    "end": "725579"
  },
  {
    "text": "models as well um yeah so have you noticed like do you have any tips and tricks on like is",
    "start": "725579",
    "end": "731760"
  },
  {
    "text": "there any difference between using kind of like open source embeddings with Pinecone or open AI embeddings both in",
    "start": "731760",
    "end": "736800"
  },
  {
    "text": "terms of like I don't know I know the different sizes right so they have different size I know you guys have some",
    "start": "736800",
    "end": "742200"
  },
  {
    "text": "different search methods do one work to do some work better for open Ai and others for other ones or open AI is the",
    "start": "742200",
    "end": "748079"
  },
  {
    "text": "best out there like how should people be thinking about a lot of these like open source embedding models yeah so I'd say okay I'm betting is",
    "start": "748079",
    "end": "756000"
  },
  {
    "text": "probably one of the more performance ones out there but a lot of case you don't actually need a lot of performance",
    "start": "756000",
    "end": "763500"
  },
  {
    "text": "um so one of the in fact later on in demo one of or the embedding model that",
    "start": "763500",
    "end": "770700"
  },
  {
    "text": "we use is an open source model and it's actually a very small model like it's you can you can put you can run it on",
    "start": "770700",
    "end": "777720"
  },
  {
    "text": "CPU and it's it'll be fine um so yeah it it varies between the two but",
    "start": "777720",
    "end": "785760"
  },
  {
    "text": "with that tiny model we actually get performance that's good enough for our use case",
    "start": "785760",
    "end": "792300"
  },
  {
    "text": "um so you don't always need something like opening eyes um too which is a very good embedding",
    "start": "792300",
    "end": "798899"
  },
  {
    "text": "model but not 100 needed uh the other thing that you know you can also consider with",
    "start": "798899",
    "end": "806579"
  },
  {
    "text": "this is opening eyes embedding model is like uh 1500 dimension",
    "start": "806579",
    "end": "811680"
  },
  {
    "text": "this other one that we're going to demo is only like 380. so that means the",
    "start": "811680",
    "end": "818399"
  },
  {
    "text": "amount of storage space you get in Pinecone is like uh four times higher maybe even",
    "start": "818399",
    "end": "824459"
  },
  {
    "text": "more um so this sort of pros and cons to both",
    "start": "824459",
    "end": "829560"
  },
  {
    "text": "approaches um just kind of depends on what you what you want to do it's also to fine-tuning",
    "start": "829560",
    "end": "835740"
  },
  {
    "text": "aspects uh these open source models are actually pretty easy to fine-tune if you have your own your own data",
    "start": "835740",
    "end": "844160"
  },
  {
    "text": "cool so yeah um so going back to the retrieval augmented generation piece",
    "start": "845579",
    "end": "852779"
  },
  {
    "text": "um what we just explained is can these two components here so the we've got the query goes to the embedding model and it",
    "start": "852779",
    "end": "860040"
  },
  {
    "text": "goes to Pinecone and it retrieves relevant contents then what we want to do is feed those relevant contacts and",
    "start": "860040",
    "end": "868079"
  },
  {
    "text": "the initial query back into our llm so we're we're getting that external",
    "start": "868079",
    "end": "875760"
  },
  {
    "text": "knowledge from Pinecone and being into a lamp so it is now kind",
    "start": "875760",
    "end": "881279"
  },
  {
    "text": "of connected to the world or at least connected to the part of the world that we would like it to be connected to",
    "start": "881279",
    "end": "888120"
  },
  {
    "text": "so pros of this approach we like we're getting going to get retrieval highly",
    "start": "888120",
    "end": "894240"
  },
  {
    "text": "relevant dot using natural language search or semantic search you can scale that to billions of Records we get data",
    "start": "894240",
    "end": "901260"
  },
  {
    "text": "management like a traditional database and we don't need to do any context stuffing so we don't get that",
    "start": "901260",
    "end": "906360"
  },
  {
    "text": "performance uh degradation that we saw with the other examples so yeah that's it on the on the",
    "start": "906360",
    "end": "914220"
  },
  {
    "text": "retrieval piece um I'll hand it over to Once yup perfect segue",
    "start": "914220",
    "end": "920240"
  },
  {
    "text": "and you know building on all that blank chain is an application development framework that makes it very easy to",
    "start": "920240",
    "end": "926220"
  },
  {
    "text": "build kind of retrieval augmented generation applications and pipelines um it's a little bit hard to see but on",
    "start": "926220",
    "end": "932040"
  },
  {
    "text": "the left you can see kind of a sense for the different data connectors for Lang chain there's 135 and I actually link to our",
    "start": "932040",
    "end": "939839"
  },
  {
    "text": "integration sub at the bottom of these slides um but these allow you to connect data from all sorts of different places from",
    "start": "939839",
    "end": "946860"
  },
  {
    "text": "structured sources unstructured sources public and private James talked a lot",
    "start": "946860",
    "end": "952500"
  },
  {
    "text": "about the importance of of kind of operating on private or company data and of course Lane chain has connectors for",
    "start": "952500",
    "end": "959279"
  },
  {
    "text": "um really most of the types of data that you'd want to be working with the second thing langtune provides is",
    "start": "959279",
    "end": "964680"
  },
  {
    "text": "different embedding models so James talked about for example open eye embeddings being over a thousand Dimension versus some open source models",
    "start": "964680",
    "end": "971519"
  },
  {
    "text": "I mentioned gnomec here they have gpd4all embeddings that came out pretty recently uh hugging face and other",
    "start": "971519",
    "end": "977880"
  },
  {
    "text": "models and so we have over 20 over 20 Integrations with different embedding models of course we've integrated with",
    "start": "977880",
    "end": "984600"
  },
  {
    "text": "any Vector stores Pinecone being one of the primary Integrations we have and kind of finely and relevant to our",
    "start": "984600",
    "end": "991740"
  },
  {
    "text": "talk today with many llm Integrations and in particular we have Integrations with the Llama CPP Library which is",
    "start": "991740",
    "end": "999540"
  },
  {
    "text": "basically a framework along with python bindings to run llama models um that works with llama V2",
    "start": "999540",
    "end": "1006860"
  },
  {
    "text": "and um I provide some documentation at the bottom which kind of shows how this can be run in particular with Lama V2",
    "start": "1006860",
    "end": "1013759"
  },
  {
    "text": "but I can go to the next slide James I have a quick question here while we're",
    "start": "1013759",
    "end": "1019100"
  },
  {
    "text": "talking about kind of like just different model providers um there's also like uh you know hosting",
    "start": "1019100",
    "end": "1025040"
  },
  {
    "text": "services like replicate that that kind of like or replicate I don't actually know how to pronounce it that offer",
    "start": "1025040",
    "end": "1031220"
  },
  {
    "text": "um uh hosting of like llama2 how should people think about using llama CPP versus like replica or what are the pros",
    "start": "1031220",
    "end": "1037220"
  },
  {
    "text": "and cons and yeah this is a good point so to be honest running llama V2 locally",
    "start": "1037220",
    "end": "1045260"
  },
  {
    "text": "is a little bit tricky and for example I can run on my laptop but I have an M2 Mac",
    "start": "1045260",
    "end": "1051740"
  },
  {
    "text": "um uh Max 32 gigs I can run about 30 or 25 to 30 tokens a second",
    "start": "1051740",
    "end": "1058700"
  },
  {
    "text": "but many people don't have a computer that for example has that kind of CPU I'm actually also running on GPU take a",
    "start": "1058700",
    "end": "1065179"
  },
  {
    "text": "little bit of time to set that up so basically if you're an organization you have a lot of resources if you're an individual you have a higher performance",
    "start": "1065179",
    "end": "1071780"
  },
  {
    "text": "laptop you really care about privacy running these things locally is a great option but alternatively you know if you don't",
    "start": "1071780",
    "end": "1079520"
  },
  {
    "text": "want to if you don't have those resources or you want something that's faster using an endpoint like replicate is is very easy and we also have",
    "start": "1079520",
    "end": "1085520"
  },
  {
    "text": "integration there um so I think it's a trade-off between how much do you value privacy and the",
    "start": "1085520",
    "end": "1091400"
  },
  {
    "text": "ability to run it locally for example on your machine um versus ease of use and these",
    "start": "1091400",
    "end": "1097280"
  },
  {
    "text": "endpoints I replicate are quite easy to use it's just like hitting open AI or any other kind of external endpoint",
    "start": "1097280",
    "end": "1104259"
  },
  {
    "text": "but that's kind of important thing to highlight this is actually I just added a few figures from the paper maybe the",
    "start": "1105440",
    "end": "1111799"
  },
  {
    "text": "high level takeaway is that long V2 is about as good as GPD 3.5 or chat gbt for",
    "start": "1111799",
    "end": "1120440"
  },
  {
    "text": "language and math but it does lack a bit quite a bit on coding and this is like the Highlight in",
    "start": "1120440",
    "end": "1126200"
  },
  {
    "text": "the paper so that's kind of a way to think about it it's kind of like roughly as good as Charity BT except if you want",
    "start": "1126200",
    "end": "1133700"
  },
  {
    "text": "to do coding tasks then it it draw it's quite a drawback relative to other open source models it's quite strong and then",
    "start": "1133700",
    "end": "1140539"
  },
  {
    "text": "you can see relative to more performant closed Source models of gbd4 it does still lag",
    "start": "1140539",
    "end": "1148220"
  },
  {
    "text": "um and relative to the first llama it's notable that the context length has",
    "start": "1148220",
    "end": "1154100"
  },
  {
    "text": "doubled so that that's the figure at the top contact length went from around 2K to now 4K tokens",
    "start": "1154100",
    "end": "1159919"
  },
  {
    "text": "and it's trained on around 2x more data so that kind of gives you the landscape for the Llama model itself",
    "start": "1159919",
    "end": "1166760"
  },
  {
    "text": "one uh one interesting thing here I actually haven't seen this figure before but um when openai had their uh like their",
    "start": "1166760",
    "end": "1176059"
  },
  {
    "text": "code model um whatever it was called uh code or DaVinci Code whatever",
    "start": "1176059",
    "end": "1181820"
  },
  {
    "text": "um I think a lot of people are like using that over text DaVinci for some of the more like agent-like and like",
    "start": "1181820",
    "end": "1187820"
  },
  {
    "text": "reasoning things because there's a lot of like like code has a lot of like nice properties where there's pattern recognition and structure and stuff like",
    "start": "1187820",
    "end": "1194539"
  },
  {
    "text": "that and so I mean this is again jumping the head a little bit but I think we I've at least tried out llama two for",
    "start": "1194539",
    "end": "1200000"
  },
  {
    "text": "agents and it hasn't been amazing and so I wonder if like that like that Gap actually partially explains some of that",
    "start": "1200000",
    "end": "1206120"
  },
  {
    "text": "like even though it's good at like writing and you think like agents are about writing things like a big part of it is pattern recognition and kind of",
    "start": "1206120",
    "end": "1212539"
  },
  {
    "text": "like things related to code and so maybe that's actually a good uh yeah maybe like the coding Challengers are are good",
    "start": "1212539",
    "end": "1219980"
  },
  {
    "text": "ones to look at for like some indication of how good they'll be at like reasoning tasks or something like that yeah that's",
    "start": "1219980",
    "end": "1226280"
  },
  {
    "text": "an interesting point and actually James I I think you played a little bit as well with llama two and agents maybe we could speak about that later but um that",
    "start": "1226280",
    "end": "1233960"
  },
  {
    "text": "is that actually may be a reasonable hypothesis as to why it may lag a bit with agents um you can see his coding",
    "start": "1233960",
    "end": "1239059"
  },
  {
    "text": "capacity is is quite a bit worse even than GPD 3.5 um",
    "start": "1239059",
    "end": "1245179"
  },
  {
    "text": "and of course ubd4 kind of is state of the art across the board which is kind of expected",
    "start": "1245179",
    "end": "1251179"
  },
  {
    "text": "um but it's also notable one other thing I'll flag here is that the context like this is reasonably good now of course it",
    "start": "1251179",
    "end": "1256280"
  },
  {
    "text": "lags what you see with larger open source models but 4K tokens you know",
    "start": "1256280",
    "end": "1261380"
  },
  {
    "text": "time what is it four and a half characters per token that you're able to fit quite a bit of context in there for retrieval augmented generation assuming",
    "start": "1261380",
    "end": "1268400"
  },
  {
    "text": "say a chunk size of 1500 characters you know that's quite a few chunks so",
    "start": "1268400",
    "end": "1275120"
  },
  {
    "text": "um it is pretty reasonable for retrieval augmented generation in that respect",
    "start": "1275120",
    "end": "1280520"
  },
  {
    "text": "and considering has a language capacities of gpt35 there's a reasonable hypothesis that it",
    "start": "1280520",
    "end": "1287179"
  },
  {
    "text": "is a quite promising model for retrieval augmented generation um and maybe James you can go to the",
    "start": "1287179",
    "end": "1293419"
  },
  {
    "text": "next slide um you might actually have to play this video",
    "start": "1293419",
    "end": "1298700"
  },
  {
    "text": "um yeah this is more of just like a a fun one but this is showing this is",
    "start": "1298700",
    "end": "1304220"
  },
  {
    "text": "running locally my laptop uh on my Mac M2 um on GPU I can get like uh around you",
    "start": "1304220",
    "end": "1312260"
  },
  {
    "text": "know 25 tokens a second so not bad it shows the fact that it is pretty cool you can run these models yourself",
    "start": "1312260",
    "end": "1318500"
  },
  {
    "text": "locally on consumer grade um uh hardware and this is provided in",
    "start": "1318500",
    "end": "1325400"
  },
  {
    "text": "the notebook um this is an example of retrieval major generation with streaming in line chain",
    "start": "1325400",
    "end": "1330860"
  },
  {
    "text": "with uh the llama2 model um so maybe this is it sets the stage",
    "start": "1330860",
    "end": "1336020"
  },
  {
    "text": "maybe to dive in a little bit more to James's notebook in particular and then we can kind of just keep kind",
    "start": "1336020",
    "end": "1342080"
  },
  {
    "text": "of discussing and move on to questions yeah definitely so let's move on to the",
    "start": "1342080",
    "end": "1348320"
  },
  {
    "text": "demo um",
    "start": "1348320",
    "end": "1352600"
  },
  {
    "text": "so you should be able to see um so we'll just go through really quickly the yeah I can I can talk",
    "start": "1353900",
    "end": "1360919"
  },
  {
    "text": "through these these first components as well you know because it's already embedding component comes in",
    "start": "1360919",
    "end": "1367940"
  },
  {
    "text": "so as we kind of mentioned we're actually using this open source model so",
    "start": "1367940",
    "end": "1373700"
  },
  {
    "text": "it's called a sentence Transformer and we use this one here so it's record it's literally called mini language",
    "start": "1373700",
    "end": "1380240"
  },
  {
    "text": "model uh it's very small super easy to run and we load that in through hiking face",
    "start": "1380240",
    "end": "1388760"
  },
  {
    "text": "but the uh the line chain Library so that kind of just wraps everything up so we don't need to worry about actually",
    "start": "1388760",
    "end": "1395559"
  },
  {
    "text": "creating those embeddings with a few lines of Pi torch code",
    "start": "1395559",
    "end": "1400880"
  },
  {
    "text": "so we wrap it up within the home face embeddings class",
    "start": "1400880",
    "end": "1407480"
  },
  {
    "text": "um and then this is just an example of how we actually create those so we have these two like documents which is like",
    "start": "1407480",
    "end": "1412640"
  },
  {
    "text": "200 text and we just embed those like this and we can see that from that we get two",
    "start": "1412640",
    "end": "1417740"
  },
  {
    "text": "document embeddings and each of those has a dimensionality of 384. so you know",
    "start": "1417740",
    "end": "1423860"
  },
  {
    "text": "a lot smaller than the open AI embeddings which are you know 1 500 and",
    "start": "1423860",
    "end": "1430280"
  },
  {
    "text": "something um it's much bigger so once we've created our embedding model next thing we're",
    "start": "1430280",
    "end": "1436640"
  },
  {
    "text": "going to do is create our Vector index that's affect the database uh with that we need to get a pine cone API key and",
    "start": "1436640",
    "end": "1444980"
  },
  {
    "text": "then we just put those details in there and we'd create our index and this is",
    "start": "1444980",
    "end": "1450980"
  },
  {
    "text": "probably at least on the Pinecone side of things this is the only point where it's going to vary between",
    "start": "1450980",
    "end": "1458240"
  },
  {
    "text": "um how you would use like an open source model versus like open AI models so",
    "start": "1458240",
    "end": "1463940"
  },
  {
    "text": "index name it's the same there's no difference there but this is a different bit so the dimensionality and also",
    "start": "1463940",
    "end": "1469940"
  },
  {
    "text": "possibly the metrics so the dimensionality is rather than the 1500",
    "start": "1469940",
    "end": "1476299"
  },
  {
    "text": "that we use open now it's like a 384 um or so that we have this model",
    "start": "1476299",
    "end": "1482240"
  },
  {
    "text": "and then also the metric so with opening embeddings we're going",
    "start": "1482240",
    "end": "1487640"
  },
  {
    "text": "to either use cosine or dot product um with this model we have to use cosine",
    "start": "1487640",
    "end": "1495260"
  },
  {
    "text": "we can't use dot products as if I'm correct and there are also some models",
    "start": "1495260",
    "end": "1501679"
  },
  {
    "text": "out there that you would have to use euclidean so you just have to be careful when you have to usually it says in the",
    "start": "1501679",
    "end": "1508400"
  },
  {
    "text": "model card if you're looking at like hooking face models um you can usually see in the model card",
    "start": "1508400",
    "end": "1513740"
  },
  {
    "text": "what you need to use there so that's the only actual difference you need to make to your code in order to",
    "start": "1513740",
    "end": "1520760"
  },
  {
    "text": "get this working Okay so I quickly scraped a few archive papers I",
    "start": "1520760",
    "end": "1527059"
  },
  {
    "text": "relate to llama to um including the alarm 2 research paper",
    "start": "1527059",
    "end": "1532360"
  },
  {
    "text": "and I just pulled those in I embedded them so we have that embed",
    "start": "1532360",
    "end": "1538580"
  },
  {
    "text": "documents here and stored them all within python so at the end of that we can see we have it's",
    "start": "1538580",
    "end": "1544820"
  },
  {
    "text": "pretty small like I said we can have billions of documents in here in this case we just have like four thousand uh",
    "start": "1544820",
    "end": "1550940"
  },
  {
    "text": "just under five thousand um Okay cool so moving on",
    "start": "1550940",
    "end": "1558140"
  },
  {
    "text": "we now have that like everything embedded we have all our documents sold",
    "start": "1558140",
    "end": "1564200"
  },
  {
    "text": "in pinecan now what we'd want to do is like hookers out to a llama 2 model",
    "start": "1564200",
    "end": "1573140"
  },
  {
    "text": "so I'm using the 13B Lam 2 model here um we're using quantization so we can",
    "start": "1573140",
    "end": "1580039"
  },
  {
    "text": "fit this on to the statute of free version of collab so we've got the T4",
    "start": "1580039",
    "end": "1585440"
  },
  {
    "text": "GPU up here so you can convert this onto there which is I think is pretty cool",
    "start": "1585440",
    "end": "1592880"
  },
  {
    "text": "um to do that you do need access so you have to like sign up for Access with",
    "start": "1592880",
    "end": "1597980"
  },
  {
    "text": "meta and then you need to pass in your homophones authentication token um but yeah we load that model we load",
    "start": "1597980",
    "end": "1605720"
  },
  {
    "text": "the tokenizer for that model and then we create a texture Innovation pipeline",
    "start": "1605720",
    "end": "1612200"
  },
  {
    "text": "okay and with that we can see we can generate our text but right now everything we",
    "start": "1612200",
    "end": "1620360"
  },
  {
    "text": "just said is within the hug and face Library which doesn't have all of like the the processing pipelines or the like",
    "start": "1620360",
    "end": "1629480"
  },
  {
    "text": "agents or chains that line chain has so what we now need to do is take that and",
    "start": "1629480",
    "end": "1635659"
  },
  {
    "text": "basically just insert it into the line chain Library which uh obviously like Jade has an integration for that so we",
    "start": "1635659",
    "end": "1643039"
  },
  {
    "text": "have the quick and face pipeline um and we just feed in our tattoo and",
    "start": "1643039",
    "end": "1648440"
  },
  {
    "text": "ocean pipeline there and now when we run it we're actually running it through line chain",
    "start": "1648440",
    "end": "1654500"
  },
  {
    "text": "which is great because now we can access all the other components um of line chain including the",
    "start": "1654500",
    "end": "1660380"
  },
  {
    "text": "retrievable QA chain which is basically like a super easy way for us to do",
    "start": "1660380",
    "end": "1665720"
  },
  {
    "text": "retrieve augmented generation um so there's two methods that we could do this with we have to retrieval QA or",
    "start": "1665720",
    "end": "1672620"
  },
  {
    "text": "retriever QA with sources chain the with sources chain would just allow you to",
    "start": "1672620",
    "end": "1679340"
  },
  {
    "text": "um basically return where you're getting the information from which can be pretty useful in in a lot of cases",
    "start": "1679340",
    "end": "1685940"
  },
  {
    "text": "um so when we're saying that up we would load our python index again through line",
    "start": "1685940",
    "end": "1692600"
  },
  {
    "text": "chain and we can just confirm it works so if I say I'll mix alarm is too special we're",
    "start": "1692600",
    "end": "1698960"
  },
  {
    "text": "going to get a load of like documents returned from like various points in the",
    "start": "1698960",
    "end": "1706700"
  },
  {
    "text": "in the database now looking at these are kind of hard to read uh but fortunately the model actually manages a lot better",
    "start": "1706700",
    "end": "1713480"
  },
  {
    "text": "than I do with that so we just move on to the uh creating",
    "start": "1713480",
    "end": "1719299"
  },
  {
    "text": "that retrieve augmented generation pipeline we include our LM we include the retriever which is just the vector",
    "start": "1719299",
    "end": "1726200"
  },
  {
    "text": "cell um and yeah we move on to actually asking",
    "start": "1726200",
    "end": "1731779"
  },
  {
    "text": "some questions so the first thing I want to do here is just try and ask",
    "start": "1731779",
    "end": "1737539"
  },
  {
    "text": "um the lens I'm going to ask longitude about itself without any retrieval augmentation",
    "start": "1737539",
    "end": "1745640"
  },
  {
    "text": "and here we go okay long two is unique and special animal for several reasons",
    "start": "1745640",
    "end": "1751279"
  },
  {
    "text": "um yeah alarms are known for the size and other things",
    "start": "1751279",
    "end": "1756380"
  },
  {
    "text": "um yeah they're silky to the touch you know it's talking about their online uh",
    "start": "1756380",
    "end": "1761480"
  },
  {
    "text": "which is is fine but basically llama 2 doesn't know about itself because it's",
    "start": "1761480",
    "end": "1767299"
  },
  {
    "text": "it's too recent so what if we try it with our road pipeline",
    "start": "1767299",
    "end": "1774260"
  },
  {
    "text": "um so we get okay alarm two is a collection of retrain of fine tunes uh large language models so yes it knows",
    "start": "1774260",
    "end": "1780679"
  },
  {
    "text": "what we're talking about this time um optimized for dialogue use cases and",
    "start": "1780679",
    "end": "1786260"
  },
  {
    "text": "out form other open source chat models on the most benchmarks of tester which",
    "start": "1786260",
    "end": "1791539"
  },
  {
    "text": "is you know one of the points I do make it special so that's cool",
    "start": "1791539",
    "end": "1796640"
  },
  {
    "text": "now let's try some more again let's let's just uh torture the alarm 2 model",
    "start": "1796640",
    "end": "1802700"
  },
  {
    "text": "without retrieval augmentation a little more and that's about safety measures",
    "start": "1802700",
    "end": "1808399"
  },
  {
    "text": "um and I don't know what it's talking about here but it's definitely not what we want so let's skip that and try it",
    "start": "1808399",
    "end": "1816200"
  },
  {
    "text": "with retrieval augmentation okay so now we do get relevant",
    "start": "1816200",
    "end": "1821779"
  },
  {
    "text": "information so development and longitude safety measures pre-training fine tuning and model safety approaches and they",
    "start": "1821779",
    "end": "1828200"
  },
  {
    "text": "also delayed the release of the 34 billion parameter model because they didn't have time to Red Team",
    "start": "1828200",
    "end": "1834620"
  },
  {
    "text": "okay so that that's again pretty cool answer um",
    "start": "1834620",
    "end": "1840380"
  },
  {
    "text": "but I want to know a little more so you know what what are these red teaming procedures",
    "start": "1840380",
    "end": "1845779"
  },
  {
    "text": "um that they use belong to and then we get this answer it was just kind of explaining",
    "start": "1845779",
    "end": "1852380"
  },
  {
    "text": "um what those actually are and finally that's one more question how",
    "start": "1852380",
    "end": "1858679"
  },
  {
    "text": "does it perform uh as alarm to other local LMS and we see that the we'll get this so",
    "start": "1858679",
    "end": "1866480"
  },
  {
    "text": "long to platforms other models on serious helpfulness and safety benchmarks they've tested",
    "start": "1866480",
    "end": "1871820"
  },
  {
    "text": "and it appears to be on source with some of the closed Source models and so that's the the Jeep G 3.5 results that",
    "start": "1871820",
    "end": "1879620"
  },
  {
    "text": "we saw earlier so yeah that's our little example we can",
    "start": "1879620",
    "end": "1884840"
  },
  {
    "text": "clearly see that llm performance by itself on like this this information it's not",
    "start": "1884840",
    "end": "1892760"
  },
  {
    "text": "that great as soon as we add in that pipeline we add in retrieve augmentation performance goes up quite a lot and we",
    "start": "1892760",
    "end": "1899480"
  },
  {
    "text": "actually get relevant results and this is just using out of the box",
    "start": "1899480",
    "end": "1904779"
  },
  {
    "text": "like prompts and everything through Lang chain we haven't even you know didn't even need to modify any",
    "start": "1904779",
    "end": "1911720"
  },
  {
    "text": "process so it's pretty cool that it just works um okay great so let me switch back to the",
    "start": "1911720",
    "end": "1919580"
  },
  {
    "text": "presentation and yeah you can",
    "start": "1919580",
    "end": "1925039"
  },
  {
    "text": "I think that that's actually downloads it for the slides awesome one thing that I want to ask and",
    "start": "1925039",
    "end": "1931460"
  },
  {
    "text": "this will probably kick off a larger discussion but like towards the end you said you know it works with the default prompts so I I kind of got like two",
    "start": "1931460",
    "end": "1938720"
  },
  {
    "text": "questions for both both plants and James which is like um",
    "start": "1938720",
    "end": "1943840"
  },
  {
    "text": "one like is this the four first like open source model that kind of just like",
    "start": "1943840",
    "end": "1949640"
  },
  {
    "text": "works with these default prompts have you noticed other ones how does it compare James I know you did great kind",
    "start": "1949640",
    "end": "1955100"
  },
  {
    "text": "of like video on on Falcon as well maybe we can just start with that one yeah like how like you know a lot of the",
    "start": "1955100",
    "end": "1960679"
  },
  {
    "text": "prompts in link chain are optimized for kind of like open AI models um just because that's what people like",
    "start": "1960679",
    "end": "1966440"
  },
  {
    "text": "llama 2 seems to work reasonably well with most of them like is this the first",
    "start": "1966440",
    "end": "1971659"
  },
  {
    "text": "open source model to do that or are there others that do that as well I'd say it's not not the first like at",
    "start": "1971659",
    "end": "1979580"
  },
  {
    "text": "least in this use case where we're doing like just retrievable QA our retriever",
    "start": "1979580",
    "end": "1984740"
  },
  {
    "text": "QA is a lot simpler of a task um than say for example the using agents",
    "start": "1984740",
    "end": "1991460"
  },
  {
    "text": "right if you if you try and get llama like I I try to get along Ascent to be",
    "start": "1991460",
    "end": "1996799"
  },
  {
    "text": "working as a conversation agent and eventually after like a lot of",
    "start": "1996799",
    "end": "2003279"
  },
  {
    "text": "you know like prompt engineering like getting the output passes right like it",
    "start": "2003279",
    "end": "2008799"
  },
  {
    "text": "did work fairly fairly well but that's like a 70 billion parameter model as soon as I try with like the 13",
    "start": "2008799",
    "end": "2015880"
  },
  {
    "text": "billion parameter model like it it does through a lot more um whereas yeah like falcon 40b",
    "start": "2015880",
    "end": "2024580"
  },
  {
    "text": "I I think it also you can get it working just about uh in in that instance",
    "start": "2024580",
    "end": "2031480"
  },
  {
    "text": "so it depends on the complexity of the test with retrieval QA um I think there are other open source",
    "start": "2031480",
    "end": "2037779"
  },
  {
    "text": "models that can work with this as well foreign yeah I can build on that a little bit",
    "start": "2037779",
    "end": "2044380"
  },
  {
    "text": "and in the chat I'm going to share something that I uh kind of tweeted out a while back and James I'd be curious to",
    "start": "2044380",
    "end": "2049898"
  },
  {
    "text": "your thoughts on this as well looking through some of the Facebook code uh I kind of identified that appears llama",
    "start": "2049899",
    "end": "2056440"
  },
  {
    "text": "will recognize a particular tokens for example system versus instruction uh",
    "start": "2056440",
    "end": "2063099"
  },
  {
    "text": "tokens that can be included in your prompt and I have tried that only empirically I",
    "start": "2063099",
    "end": "2069220"
  },
  {
    "text": "think it is a bit better I haven't systematically evaluated I'm curious if you played with that at all or have you observed anything there I think I saw",
    "start": "2069220",
    "end": "2075220"
  },
  {
    "text": "you reference that on Twitter is that something that you've also observed",
    "start": "2075220",
    "end": "2080740"
  },
  {
    "text": "yeah yeah so with those tokens it does work a lot better um I think I think I was I was kind of",
    "start": "2080740",
    "end": "2088300"
  },
  {
    "text": "like going around in circles trying to get it to work for a long time and not realizing that these tokens are right thing",
    "start": "2088300",
    "end": "2094000"
  },
  {
    "text": "uh then I kind of like stumbled upon them yeah like added them in and then that",
    "start": "2094000",
    "end": "2100000"
  },
  {
    "text": "was when it actually started working um and one other thing that was kind of interesting and I I kind of I'm not sure",
    "start": "2100000",
    "end": "2107619"
  },
  {
    "text": "if this is the correct entirely the correct way of using them or not um but they did mention the paper that",
    "start": "2107619",
    "end": "2115839"
  },
  {
    "text": "over multiple interactions um as a like as an agent it seems to forget the original instructions and I",
    "start": "2115839",
    "end": "2122140"
  },
  {
    "text": "found that as well right so as a conversational agent over multiple interactions uh it would forget to Output the Json",
    "start": "2122140",
    "end": "2129420"
  },
  {
    "text": "format and then it would just kind of start chatting instead so uh what I added in and what they",
    "start": "2129420",
    "end": "2137020"
  },
  {
    "text": "mentioned in the paper is that if they insert some the instructions into the",
    "start": "2137020",
    "end": "2142180"
  },
  {
    "text": "user query um then it will continue over more interactions so that's what I started",
    "start": "2142180",
    "end": "2147940"
  },
  {
    "text": "doing and so I used these instruction tokens on either side of the instructions I insert into the user",
    "start": "2147940",
    "end": "2154780"
  },
  {
    "text": "query and after doing that it was just like a brief reminder it wasn't like the",
    "start": "2154780",
    "end": "2160480"
  },
  {
    "text": "full-on instructions that included within the system message it was just something like remember to Output in",
    "start": "2160480",
    "end": "2167320"
  },
  {
    "text": "Json format with action and action input keys and just adding that little sentence",
    "start": "2167320",
    "end": "2174160"
  },
  {
    "text": "like improved the performance a lot then it was like every time almost every time",
    "start": "2174160",
    "end": "2179200"
  },
  {
    "text": "it was like a perfect like response for an agent which is pretty cool got it and in the notebook we just",
    "start": "2179200",
    "end": "2186760"
  },
  {
    "text": "showed I believe you're just using the default retrieval QA prompt which as you showed does basically work but I guess",
    "start": "2186760",
    "end": "2192880"
  },
  {
    "text": "what you're saying is with these Special tokens you have further enhanced uh performance",
    "start": "2192880",
    "end": "2198520"
  },
  {
    "text": "yeah I think the idea is um sorry I use not with retrievable QA I",
    "start": "2198520",
    "end": "2204700"
  },
  {
    "text": "didn't use these special tokens um I should also add that these special tokens are specific to the chats the",
    "start": "2204700",
    "end": "2211000"
  },
  {
    "text": "fine-tuned chat version as far as I understand um at least see the instructions",
    "start": "2211000",
    "end": "2218079"
  },
  {
    "text": "um so there's also that but yeah they within the retrieval QA I haven't used",
    "start": "2218079",
    "end": "2224320"
  },
  {
    "text": "them before so I don't know but I'm not sure how the performance would vary there yep",
    "start": "2224320",
    "end": "2230500"
  },
  {
    "text": "I'm adding something to the chat now as well actually today we just um merge a new a web research Retriever",
    "start": "2230500",
    "end": "2237160"
  },
  {
    "text": "and in the code there we actually toggle between different prompts depending on",
    "start": "2237160",
    "end": "2242200"
  },
  {
    "text": "the model choice so it's a nice reference as to how you can select different prompts within Lang chain using conditional prompt selector",
    "start": "2242200",
    "end": "2249280"
  },
  {
    "text": "and it can kind of automatically detect the model and choose the corresponding prompt so",
    "start": "2249280",
    "end": "2254619"
  },
  {
    "text": "that's like a nice trick that can be used particularly when you're like toggling between llama and for example",
    "start": "2254619",
    "end": "2260140"
  },
  {
    "text": "open AI or other providers um it's just a nice thing to keep in mind to the point around the chat model as",
    "start": "2260140",
    "end": "2267040"
  },
  {
    "text": "well one thing which I actually just tried out and it didn't work as well as I thought but like you know a lot of the",
    "start": "2267040",
    "end": "2272680"
  },
  {
    "text": "chat models are very verbose and like want to respond with chats and stuff and for a lot of the agent stuff kind of as",
    "start": "2272680",
    "end": "2278500"
  },
  {
    "text": "you were saying James you just want a structured response and you want to do like some pattern recognition stuff so I tried out one of the just like base",
    "start": "2278500",
    "end": "2284920"
  },
  {
    "text": "llama models um with some of the old school prompts for for agents that were more about",
    "start": "2284920",
    "end": "2290740"
  },
  {
    "text": "pattern recognition with the hope that it would work well there didn't really work out but I do think there's",
    "start": "2290740",
    "end": "2296680"
  },
  {
    "text": "something kind of like there where like um you know chat models are verbose been",
    "start": "2296680",
    "end": "2302500"
  },
  {
    "text": "kind of not great for agents and maybe the base models can actually be better for some of the instruction following especially for the less powerful models",
    "start": "2302500",
    "end": "2308680"
  },
  {
    "text": "like this but",
    "start": "2308680",
    "end": "2312240"
  },
  {
    "text": "um all right I'm going to jump into some of the questions that we're getting a lot of them are around retrieval which is awesome",
    "start": "2313780",
    "end": "2319660"
  },
  {
    "text": "um the top one is can you speak about how much performance boost you can get with hybrid search retrieval and",
    "start": "2319660",
    "end": "2325839"
  },
  {
    "text": "um yeah maybe we can even Zoom this back out to like you know you you talked a lot about kind of like similarity search",
    "start": "2325839",
    "end": "2331300"
  },
  {
    "text": "what other tips and tricks can we do on top of similarity search whether it be hybrid whether it be other things to",
    "start": "2331300",
    "end": "2337300"
  },
  {
    "text": "improve performance and then how would you recommend people getting started and exploring those yeah so yeah there are there are loads",
    "start": "2337300",
    "end": "2346359"
  },
  {
    "text": "of extra things you can do so kind of what we describe here it's almost like the base version of uh semantic social",
    "start": "2346359",
    "end": "2354520"
  },
  {
    "text": "or perpet search um so hybrid search hybrid search tapes both this what you",
    "start": "2354520",
    "end": "2362320"
  },
  {
    "text": "see here where we have like the what we call dense vectors where you're kind of encoding the semantic meaning into these",
    "start": "2362320",
    "end": "2368980"
  },
  {
    "text": "vectors um but then it also kind of measures it with the more traditional search where",
    "start": "2368980",
    "end": "2375160"
  },
  {
    "text": "you're kind of looking at more like keywords so like uh in this case here",
    "start": "2375160",
    "end": "2381700"
  },
  {
    "text": "with this like the first three where you're looking at bank um and more traditional search a keyword",
    "start": "2381700",
    "end": "2388720"
  },
  {
    "text": "search might actually rate those things as being very closely related whereas a semantic search would not now you know",
    "start": "2388720",
    "end": "2396820"
  },
  {
    "text": "why would you want to go with that traditional search well in some cases that can actually be very useful",
    "start": "2396820",
    "end": "2403480"
  },
  {
    "text": "particularly if you have like more domain specific language like okay if",
    "start": "2403480",
    "end": "2410020"
  },
  {
    "text": "you work in um if you work in like the one that I've",
    "start": "2410020",
    "end": "2415300"
  },
  {
    "text": "seen come up a bit recently is like for crypto uh there's tokens like like",
    "start": "2415300",
    "end": "2421119"
  },
  {
    "text": "ethereum or f um semantic search might kind of relate",
    "start": "2421119",
    "end": "2427180"
  },
  {
    "text": "that very closely with Bitcoin and if someone's asking a question about ethereum they they probably want an",
    "start": "2427180",
    "end": "2432820"
  },
  {
    "text": "answer that's about ethereum so by having that traditional search",
    "start": "2432820",
    "end": "2438579"
  },
  {
    "text": "component you can specify okay I want this you know I want ethereum not Bitcoin right where's my search might",
    "start": "2438579",
    "end": "2445180"
  },
  {
    "text": "struggle with that um but at the same time you probably kind of want not just traditional but",
    "start": "2445180",
    "end": "2451660"
  },
  {
    "text": "almost like a mix of both um so hybrid search is putting those two together so you're you're basically",
    "start": "2451660",
    "end": "2458020"
  },
  {
    "text": "you're doing your semantic search you're also doing your keyword search and then you're merging those result results and",
    "start": "2458020",
    "end": "2463839"
  },
  {
    "text": "kind of like re-ranking based on you know whether you want more traditional whether you are more semantic",
    "start": "2463839",
    "end": "2470740"
  },
  {
    "text": "so if I would say like it do threshold for whether you should",
    "start": "2470740",
    "end": "2476859"
  },
  {
    "text": "consider like hybrid is you know you try dents",
    "start": "2476859",
    "end": "2482200"
  },
  {
    "text": "um and it doesn't really seem to work and particularly if you know it's that kind of keyword thing where like",
    "start": "2482200",
    "end": "2488560"
  },
  {
    "text": "keywords don't seem to have enough importance then that's why you might want to use a hybrid search where you're",
    "start": "2488560",
    "end": "2494920"
  },
  {
    "text": "using both um yeah so hybrid searching help a lot",
    "start": "2494920",
    "end": "2502540"
  },
  {
    "text": "um what are the other other parts of that question Harrison well well this the",
    "start": "2502540",
    "end": "2508540"
  },
  {
    "text": "other big question um or the other top question is also about retrieval and it's basically we've got a lot of documents",
    "start": "2508540",
    "end": "2515440"
  },
  {
    "text": "um we find that queries return um irrelevant documents causing alums",
    "start": "2515440",
    "end": "2520839"
  },
  {
    "text": "hallucinations exactly what you were saying earlier about wanting to pull down the context and then one thing that they're asking for is basically they",
    "start": "2520839",
    "end": "2527320"
  },
  {
    "text": "want to kind of like set a search distance filter to set off at certain similarities and they're asking about",
    "start": "2527320",
    "end": "2532780"
  },
  {
    "text": "whether this is like a pine cone and like and that's works but like more generally if you reframe this as like",
    "start": "2532780",
    "end": "2539140"
  },
  {
    "text": "yeah how like and I guess this very much ties into like things Beyond semantic search it's like it's like maybe like",
    "start": "2539140",
    "end": "2545320"
  },
  {
    "text": "yeah one maybe specifically is there a specific way to put a cut off on the cosine distance within Pinecone or would",
    "start": "2545320",
    "end": "2552280"
  },
  {
    "text": "you recommend doing that outside of Pinecone and then just like second like hacky things or not hacky things but",
    "start": "2552280",
    "end": "2557380"
  },
  {
    "text": "like tips and tricks like this that are that are good to do yeah so I mean if you're returning like",
    "start": "2557380",
    "end": "2563200"
  },
  {
    "text": "a lot of irrelevant information like the first thing I would look at is the embedding process like are you",
    "start": "2563200",
    "end": "2570040"
  },
  {
    "text": "um are you putting too much text into each embedding that would probably be uh like a one of the common issues",
    "start": "2570040",
    "end": "2577480"
  },
  {
    "text": "so you want to you want to try and split your tapes into smaller chunks and then embed uh and then once you've done that",
    "start": "2577480",
    "end": "2584079"
  },
  {
    "text": "if what you can do on the other side so on the actual retrieval side um within Pine counter isn't a feature",
    "start": "2584079",
    "end": "2590980"
  },
  {
    "text": "for this um but the solution is still pretty straightforward uh you you have the top",
    "start": "2590980",
    "end": "2596560"
  },
  {
    "text": "K parameter which is basically how many items or how many documents you're retrieving let's say usually you'd want to retrieve",
    "start": "2596560",
    "end": "2603700"
  },
  {
    "text": "like five documents um what you can do is just retrieve like",
    "start": "2603700",
    "end": "2610960"
  },
  {
    "text": "and then retrieve like 50 maybe 100 documents and then add that cut off and just take like the top five",
    "start": "2610960",
    "end": "2617859"
  },
  {
    "text": "um so you when whenever you retrieve items from from Pine Cone you're going",
    "start": "2617859",
    "end": "2623140"
  },
  {
    "text": "to get a Samaritan score so you can tweak your your threshold for what you want to let through based on that",
    "start": "2623140",
    "end": "2629680"
  },
  {
    "text": "um that would be kind of like the most straightforward solution another",
    "start": "2629680",
    "end": "2636040"
  },
  {
    "text": "solution that you can go for and this is something that I've seen use like fairly effectively is again",
    "start": "2636040",
    "end": "2643420"
  },
  {
    "text": "retrieving more documents from Pinecone and then using a re-ranking model",
    "start": "2643420",
    "end": "2649180"
  },
  {
    "text": "like after you've retrieved your items so like you're here I have a re-ranking",
    "start": "2649180",
    "end": "2654940"
  },
  {
    "text": "model um also through at least through the sentence Transformers model uh you can",
    "start": "2654940",
    "end": "2661780"
  },
  {
    "text": "get re-ranking models as well they're open source so basically what they will do is they'll just look at like these",
    "start": "2661780",
    "end": "2668260"
  },
  {
    "text": "hundred items and they're just going to re-rank them and they are less slower but they're more powerful than a typical",
    "start": "2668260",
    "end": "2676359"
  },
  {
    "text": "embedding model um so that's why we would only use it on like the last 100 items",
    "start": "2676359",
    "end": "2683140"
  },
  {
    "text": "um but they will basically re-rank everything and you'll you'll get usually high quality results from that as well",
    "start": "2683140",
    "end": "2691260"
  },
  {
    "text": "awesome and Lance you've done a lot of stuff with retrieval do you have any kind of like favorite kind of like",
    "start": "2696099",
    "end": "2702160"
  },
  {
    "text": "retrieval tips and tricks I've got one in mind but I'm I'll let you go first and then I'll",
    "start": "2702160",
    "end": "2707319"
  },
  {
    "text": "add my favorite yeah we've actually um I'll share some some Tweets in the in",
    "start": "2707319",
    "end": "2713200"
  },
  {
    "text": "the chat we've we've had a few different recent retrievers",
    "start": "2713200",
    "end": "2718300"
  },
  {
    "text": "um so let's see um",
    "start": "2718300",
    "end": "2724240"
  },
  {
    "text": "I think well actually maybe you go first I'm gonna I'm gonna pull something up so I",
    "start": "2724240",
    "end": "2730420"
  },
  {
    "text": "can share uh all right my favorite one is one of the ones that we call kind of like self query and so it builds on top",
    "start": "2730420",
    "end": "2737380"
  },
  {
    "text": "of the um it builds on top of the vector store and basically a lot of vector stores including Pinecone I think we",
    "start": "2737380",
    "end": "2742540"
  },
  {
    "text": "actually did first for Pinecone support like metadata filtering um and so when you get like a query or a",
    "start": "2742540",
    "end": "2748420"
  },
  {
    "text": "question some of the query might not actually be about the semantic meaning of it",
    "start": "2748420",
    "end": "2753520"
  },
  {
    "text": "um but about like particular filters that you might want to apply so for example like what's a movie about aliens",
    "start": "2753520",
    "end": "2759099"
  },
  {
    "text": "in the Year 1980 or something like that so like the Year 1980 it's not it's not",
    "start": "2759099",
    "end": "2764800"
  },
  {
    "text": "a semantic thing you want to search on it's like the literal year that you want to filter on and so if you have year as kind of like a metadata attribute you",
    "start": "2764800",
    "end": "2770980"
  },
  {
    "text": "can kind of like split that out and so we kind of like use a language model to split out kind of like that filter from",
    "start": "2770980",
    "end": "2777280"
  },
  {
    "text": "and then pass the semantic bit and create a vector and do kind of like the the vector search that way but then also",
    "start": "2777280",
    "end": "2782740"
  },
  {
    "text": "pass in a filter that we extract um and I think this is um yeah this is this is one of my more uh I like this",
    "start": "2782740",
    "end": "2789760"
  },
  {
    "text": "method a lot I think it's pretty good um and and uh yeah that so that that's my probably",
    "start": "2789760",
    "end": "2795520"
  },
  {
    "text": "like favorite hack on top of on top of just straight Vector search yeah I just added something to the chat",
    "start": "2795520",
    "end": "2802960"
  },
  {
    "text": "so one other thing we didn't talk about this too much but basically persisting metadata with every chunk is a very nice trick because metadata gives you kind of",
    "start": "2802960",
    "end": "2809859"
  },
  {
    "text": "a handle when you're doing the retrieval to fill different things and so we have a number of different",
    "start": "2809859",
    "end": "2815380"
  },
  {
    "text": "ways to persist metadata associated with like for if you're working with documents where did each chunk of the",
    "start": "2815380",
    "end": "2821980"
  },
  {
    "text": "document actually come from um like the introduction or what section of the document likewise we have the",
    "start": "2821980",
    "end": "2827680"
  },
  {
    "text": "same thing for code we have something for markdown files and this ability to persist metadata",
    "start": "2827680",
    "end": "2833680"
  },
  {
    "text": "in the retrieval stage is quite nice and actually plays very well into self queer",
    "start": "2833680",
    "end": "2839319"
  },
  {
    "text": "retriever because then in fact I'll share a I'll share some documentation but basically when you have these",
    "start": "2839319",
    "end": "2845140"
  },
  {
    "text": "metadata tags that came from splitting you can use them with a self query retriever very nicely I'll share some",
    "start": "2845140",
    "end": "2851560"
  },
  {
    "text": "documentation there as well um so that is one trick that I that I",
    "start": "2851560",
    "end": "2856780"
  },
  {
    "text": "really like and that's been very popular in the community",
    "start": "2856780",
    "end": "2860819"
  },
  {
    "text": "awesome here's a here's some really good documentation as well about how that",
    "start": "2862180",
    "end": "2868000"
  },
  {
    "text": "plays in with uh sub query retriever and of course Pinecone is very nice",
    "start": "2868000",
    "end": "2874300"
  },
  {
    "text": "support for metadata filtering so it's um",
    "start": "2874300",
    "end": "2879640"
  },
  {
    "text": "it's um you know it really it really works well uh all right maybe the last question to end",
    "start": "2879640",
    "end": "2886780"
  },
  {
    "text": "on kind of like combining um retrieval and uh and the open source",
    "start": "2886780",
    "end": "2892000"
  },
  {
    "text": "nature is basically around like fine-tuning embedding models um like have you guys played around with this have you seen people who are doing",
    "start": "2892000",
    "end": "2898599"
  },
  {
    "text": "that do you have any tips and tricks for people looking to do that I actually haven't done that at all so I'm just looking to learn from you guys at this",
    "start": "2898599",
    "end": "2904359"
  },
  {
    "text": "point yeah um so find something embedding models",
    "start": "2904359",
    "end": "2909460"
  },
  {
    "text": "actually it's actually not that difficult and doesn't require that much compute",
    "start": "2909460",
    "end": "2914740"
  },
  {
    "text": "comparison fine-tuning a lot of other models um so",
    "start": "2914740",
    "end": "2920859"
  },
  {
    "text": "iron and this is this is like kind of going back a little bit now like when I was actually fine-tuning",
    "start": "2920859",
    "end": "2926859"
  },
  {
    "text": "these models I haven't since you know the whole uh open air I think but like before that time",
    "start": "2926859",
    "end": "2933040"
  },
  {
    "text": "um I worked a lot on just like fine-tuning sentence transform models and there's a lot of different methods",
    "start": "2933040",
    "end": "2939160"
  },
  {
    "text": "depending on your like your data set that you can use even even methods where",
    "start": "2939160",
    "end": "2944440"
  },
  {
    "text": "you don't need to like um label your data generally speaking though you",
    "start": "2944440",
    "end": "2950800"
  },
  {
    "text": "should you should label your data basically what you want to do is you want to get pairs of sentences or",
    "start": "2950800",
    "end": "2955839"
  },
  {
    "text": "paragraphs that are similar or dissimilar and you're just going to feed that into the model and tell it which",
    "start": "2955839",
    "end": "2961540"
  },
  {
    "text": "one similar which ones are not similar um and you can usually get pretty good",
    "start": "2961540",
    "end": "2967780"
  },
  {
    "text": "results if you like the sort of rule of thumb again back then things may have",
    "start": "2967780",
    "end": "2973660"
  },
  {
    "text": "changed a bit now the Royal thumb back downwards like if you have 10 000 of these pairs",
    "start": "2973660",
    "end": "2979599"
  },
  {
    "text": "um you can like you don't need any more than that and then a lot of time you can actually deal actually train a model",
    "start": "2979599",
    "end": "2986740"
  },
  {
    "text": "with much less than 10 000 pairs like five thousand was something that I did",
    "start": "2986740",
    "end": "2991839"
  },
  {
    "text": "fairly often um and training times that we like on a",
    "start": "2991839",
    "end": "2997960"
  },
  {
    "text": "consumer grade GPU doing a couple of hours nice",
    "start": "2997960",
    "end": "3003780"
  },
  {
    "text": "one thing I'll throw I just add something to the chat glean's actually a company that's been pretty interesting in kind of Enterprise retrieval they",
    "start": "3003780",
    "end": "3010140"
  },
  {
    "text": "talk a lot about fine tuning embedding models on your data as kind of a significant impact on quality",
    "start": "3010140",
    "end": "3017819"
  },
  {
    "text": "um and it's kind of interesting to recognize um so I I think it's a great area to think",
    "start": "3017819",
    "end": "3023940"
  },
  {
    "text": "about all right well I want to thank you guys",
    "start": "3023940",
    "end": "3029040"
  },
  {
    "text": "for uh for for helping give this webinar I think uh yeah I think retrieval is",
    "start": "3029040",
    "end": "3035220"
  },
  {
    "text": "always one of the most interesting things to talk about and llama 2 kind of took the World by storm last week so glad we were able to combine the two",
    "start": "3035220",
    "end": "3041280"
  },
  {
    "text": "into one awesome um one awesome webinar and I want to thank everyone for tuning in this this will be available on uh YouTube and",
    "start": "3041280",
    "end": "3049440"
  },
  {
    "text": "um yeah looking looking forward to the next one thank you guys cool thanks guys thanks a lot bye",
    "start": "3049440",
    "end": "3058200"
  },
  {
    "text": "foreign",
    "start": "3058200",
    "end": "3060800"
  }
]