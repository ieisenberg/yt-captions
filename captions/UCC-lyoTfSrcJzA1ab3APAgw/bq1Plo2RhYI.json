[
  {
    "text": "hey this is Lance from Lang chain So Meta just released llama 3.2 today which is really exciting I've been looking",
    "start": "880",
    "end": "6359"
  },
  {
    "text": "forward to this for a while and what's really neat is you can use increasingly small models to do pretty interesting",
    "start": "6359",
    "end": "12519"
  },
  {
    "text": "things locally on your devices so today I'm going to show how we can go from scratch build a fairly complex rag agent",
    "start": "12519",
    "end": "20240"
  },
  {
    "text": "using a 3 billion parameter model that will run locally on my laptop and this diagram shows the entire flow of the",
    "start": "20240",
    "end": "26720"
  },
  {
    "text": "agent I'll just zoom in here so we're going to be able to take questions route them using the model to either an index",
    "start": "26720",
    "end": "33719"
  },
  {
    "text": "or web search grade documents from retrieval based on relevance to our",
    "start": "33719",
    "end": "39520"
  },
  {
    "text": "question produce an answer grade the answer for hallucinations grade it for relevance to",
    "start": "39520",
    "end": "46360"
  },
  {
    "text": "the question and finally return an answer to the user we can do all that logic locally with llama 3.2 3 billion",
    "start": "46360",
    "end": "54480"
  },
  {
    "text": "parameter model and I'll show you how to do that shortly so first there's kind of",
    "start": "54480",
    "end": "59680"
  },
  {
    "text": "two two variants of the models here there's Compact and there's Vision models so there's a 1 billion and 3",
    "start": "59680",
    "end": "65720"
  },
  {
    "text": "billion parameter compact model we're going to be working with the 3 billion there's also Vision models which are interesting for other reasons and which",
    "start": "65720",
    "end": "71759"
  },
  {
    "text": "I may talk about in a future video so again these compact models are",
    "start": "71759",
    "end": "77400"
  },
  {
    "text": "me meant for smaller uh on device use cases um and what's kind of cool is I",
    "start": "77400",
    "end": "83439"
  },
  {
    "text": "show some statistics here comparing the prior 8 billion to the current 3.23 billion and it's pretty pretty close 3",
    "start": "83439",
    "end": "91000"
  },
  {
    "text": "versus 8 you can see the 3 billion parameter model is as strong on one of the evals and so it's pretty promising",
    "start": "91000",
    "end": "97920"
  },
  {
    "text": "is telling us that smaller models are getting indeed better and that opens up",
    "start": "97920",
    "end": "104079"
  },
  {
    "text": "access to more folks which is very exciting so I want to talk a little bit about this rag agent we're going to",
    "start": "104079",
    "end": "109119"
  },
  {
    "text": "build and then let's just get into the code so we're going to incorporate ideas from three papers one is routing so",
    "start": "109119",
    "end": "115240"
  },
  {
    "text": "that's the ability to take a question and Route it to either a vector store or web search based model the content of",
    "start": "115240",
    "end": "120719"
  },
  {
    "text": "the question the other is fallback this idea that you can basically retrieve documents and grade them so if any",
    "start": "120719",
    "end": "126759"
  },
  {
    "text": "documents are not relevant you can filter them out a big problem with retrieval is erroneous retrievals and so",
    "start": "126759",
    "end": "132959"
  },
  {
    "text": "we have a fallback for that and self-correction we can fix and catch hallucinations and we can basically",
    "start": "132959",
    "end": "140400"
  },
  {
    "text": "reject answers that don't uh address our question we can build all that from scratch and I'll show you right now so",
    "start": "140400",
    "end": "147519"
  },
  {
    "text": "I'm going to be moving between a notebook I've already set this up and this code will be available to you I've just done a few pip",
    "start": "147519",
    "end": "153280"
  },
  {
    "text": "installs and here's where some interesting decisions can be made I'm going to use local embeddings from nomic",
    "start": "153280",
    "end": "159920"
  },
  {
    "text": "the GPD for all embeddings the PIP installs are are presented up here you can try other local embeddings I tend to",
    "start": "159920",
    "end": "166040"
  },
  {
    "text": "like these but obviously that's up to you and I'm going to access llama 32",
    "start": "166040",
    "end": "171959"
  },
  {
    "text": "instruct uh fp16 from olama you can try other providers to access local models I",
    "start": "171959",
    "end": "177879"
  },
  {
    "text": "happen to like AMA um so if you go to llama's website here they provide a",
    "start": "177879",
    "end": "183519"
  },
  {
    "text": "bunch of different models that you can just download and all you need to do is just a llama pull and then uh whatever",
    "start": "183519",
    "end": "191120"
  },
  {
    "text": "model you want so that's all you need to do very simple just make sure I have a llama installed you can access any of these models for free pretty cool so",
    "start": "191120",
    "end": "199239"
  },
  {
    "text": "I've done that and now there's two other pieces of tooling and then we can get right into it I use Tav for web search",
    "start": "199239",
    "end": "205200"
  },
  {
    "text": "I'm going to set up this kind of web search component of my agent and Tav is a really nice search ented optimized for",
    "start": "205200",
    "end": "211640"
  },
  {
    "text": "Rag and other agent use cases it has a generous free tier so you can sign up and use it uh simply they'll give you an",
    "start": "211640",
    "end": "218920"
  },
  {
    "text": "API key just make sure it's set I'll also use lsmith for tracing which is just a way I'm going to visualize what",
    "start": "218920",
    "end": "224480"
  },
  {
    "text": "I'm doing this is totally optional so those are only things I've set here and then we have a blank notebook we can",
    "start": "224480",
    "end": "230200"
  },
  {
    "text": "just get going first I'm just going to find my llm so again I set the name of the model",
    "start": "230200",
    "end": "236360"
  },
  {
    "text": "that I pulled using this command AMA pull this is now available to me and I'm using Lang change integration with the",
    "start": "236360",
    "end": "242840"
  },
  {
    "text": "Llama so I just simply specify this model name and I load it now you see I",
    "start": "242840",
    "end": "248920"
  },
  {
    "text": "do two interesting things here first I load just the raw model and I also load",
    "start": "248920",
    "end": "254480"
  },
  {
    "text": "this Json mode and you'll see why we're going to use that a little bit later next I'm going to build a vector store",
    "start": "254480",
    "end": "261120"
  },
  {
    "text": "so this is where it's kind of up to you this could be your personal documents this could be really anything you want",
    "start": "261120",
    "end": "266320"
  },
  {
    "text": "to work with for any question answer application locally in my case I'm just take three URLs from some blog post that",
    "start": "266320",
    "end": "272600"
  },
  {
    "text": "I like and you're going to see here this is just some very simple utilities to load the documents using a web loader",
    "start": "272600",
    "end": "279440"
  },
  {
    "text": "split them as I do here you can select your chunk size arbitrarily so what's happening here is we're loading the raw",
    "start": "279440",
    "end": "285600"
  },
  {
    "text": "documents we're splitting them into chunks of size a th000 tokens and I'm",
    "start": "285600",
    "end": "290960"
  },
  {
    "text": "going to add them to a vector store this is free to use Vector store from sklearn I load in my document splits I",
    "start": "290960",
    "end": "297360"
  },
  {
    "text": "load in my embedding now here's where I'm going to be using those nomic local embeddings uh that again are free to use",
    "start": "297360",
    "end": "304960"
  },
  {
    "text": "and I'm going to create a retriever from that let's get that started so retriever is a line chain abstraction that gives",
    "start": "304960",
    "end": "311479"
  },
  {
    "text": "you a few common methods so you can use invoke to very simply agent memory get documents",
    "start": "311479",
    "end": "319720"
  },
  {
    "text": "related to any input here we go so we've just retrieved some documents relevant",
    "start": "319720",
    "end": "325280"
  },
  {
    "text": "to this question or statement agent memory of course what's happening is this taking the statement embedding it",
    "start": "325280",
    "end": "330960"
  },
  {
    "text": "using our model and then doing a similarity search with the embedded documents to find similar matches based",
    "start": "330960",
    "end": "336199"
  },
  {
    "text": "upon semantics of my question relative to the documents that's it so we get documents back so now we have a",
    "start": "336199",
    "end": "343400"
  },
  {
    "text": "retriever the retriever you can see falls into our flow here this is kind of one of the core components of a rag",
    "start": "343400",
    "end": "349120"
  },
  {
    "text": "system you need some document reval mechanism and we've built that so now let's build some this logic around it",
    "start": "349120",
    "end": "354319"
  },
  {
    "text": "first let's start with routing and I'll show you how we can do that so now I'm going to build a router and I'll show",
    "start": "354319",
    "end": "359520"
  },
  {
    "text": "you a really simple trick for doing this with local LMS like this very simply I'm",
    "start": "359520",
    "end": "365240"
  },
  {
    "text": "just going to set up a prompt that says you're an expert routing the vector store contains",
    "start": "365240",
    "end": "371840"
  },
  {
    "text": "documents related to agents prompt engineering and adial tax use the vector store for questions on these topics for",
    "start": "371840",
    "end": "378759"
  },
  {
    "text": "everything else just especially current events just use web search now here's where I just instruct return Json object",
    "start": "378759",
    "end": "385800"
  },
  {
    "text": "with a single key data source with that is either web search or Vector store depending on the question now when I use",
    "start": "385800",
    "end": "392680"
  },
  {
    "text": "this llm Json mode if you go up here right here because I've set format",
    "start": "392680",
    "end": "399440"
  },
  {
    "text": "Json it's going to enforce the output of the LM to be Json so what's nice is I've",
    "start": "399440",
    "end": "406400"
  },
  {
    "text": "built myself a simple little router just using kind of llm and Json mode so all I need to do is I'm going to pass in a",
    "start": "406400",
    "end": "412880"
  },
  {
    "text": "question now one other thing I want to mention this is a chat model so we can",
    "start": "412880",
    "end": "418160"
  },
  {
    "text": "pass a set of chat m messages you can import from Lang chain core messages human message system message this is my",
    "start": "418160",
    "end": "425360"
  },
  {
    "text": "system message with the router instructions and then my human message is just whatever question I have what",
    "start": "425360",
    "end": "431240"
  },
  {
    "text": "are the types of agent memory and those can be passed in as a list that's",
    "start": "431240",
    "end": "436879"
  },
  {
    "text": "it and I can go ahead and run that",
    "start": "436879",
    "end": "442479"
  },
  {
    "text": "I can go ahead and run",
    "start": "455720",
    "end": "458680"
  },
  {
    "text": "that and we can see it looks at this question and reasons that oh the data source for this is Vector store and I",
    "start": "466360",
    "end": "472319"
  },
  {
    "text": "get a little Json back that says data source Vector store so you build a simple router using Json mode and a",
    "start": "472319",
    "end": "478360"
  },
  {
    "text": "local LM and I use this all the time it's extremely convenient way to just do really simple kind of binary routing",
    "start": "478360",
    "end": "484319"
  },
  {
    "text": "using llm to reason about some input and return a structured object like a Json",
    "start": "484319",
    "end": "489400"
  },
  {
    "text": "with for example the two outputs that I want either Vector store or web search",
    "start": "489400",
    "end": "494560"
  },
  {
    "text": "so now we built this red router piece and next I'm going to build this blue greater piece so if I go",
    "start": "494560",
    "end": "501840"
  },
  {
    "text": "down here it's also really simple let's just give some instructions for grading",
    "start": "501840",
    "end": "507039"
  },
  {
    "text": "the main idea here is I want to look at the retriev documents and determine that they're relevant to the question I ask",
    "start": "507039",
    "end": "512279"
  },
  {
    "text": "because sometimes with semantic similarity search you can get kind of erroneous retrievals of documents that are not particularly relevant to the",
    "start": "512279",
    "end": "519440"
  },
  {
    "text": "question uh but happen to be returned based upon the idiosyncrasies of your chunking or your embedding model so this",
    "start": "519440",
    "end": "526839"
  },
  {
    "text": "can happen sometimes so it's nice to have this additional layer of kind of protection or reasoning on your retrievals so your greater assessing the",
    "start": "526839",
    "end": "533959"
  },
  {
    "text": "relevance of retrieved documents if the document contains keywords or semantic meaning related to the question grade it",
    "start": "533959",
    "end": "540440"
  },
  {
    "text": "as relevant okay so here I'm just going to pass in the document the question",
    "start": "540440",
    "end": "546640"
  },
  {
    "text": "think carefully about this and return to Json with a binary score uh yes or no",
    "start": "546640",
    "end": "552560"
  },
  {
    "text": "that's it so here we go I'm just going to ask a question what is chaini thought prompting I'm going to retrieve",
    "start": "552560",
    "end": "558440"
  },
  {
    "text": "documents from my Vector store I'll pick one of them okay you can see that's what I'm doing right here and I'll go ahead",
    "start": "558440",
    "end": "564920"
  },
  {
    "text": "and pass that into my prompt right here I just format the string with those two inputs and I do just what I did before",
    "start": "564920",
    "end": "571440"
  },
  {
    "text": "I'm going to go ahead and pass in those greater instructions here I'm going to pass in my formatted prompt here which",
    "start": "571440",
    "end": "578399"
  },
  {
    "text": "has my question and my document there we go so we get binary score yes which makes sense basically",
    "start": "578399",
    "end": "584920"
  },
  {
    "text": "what is Chain of Thought prompting we can look at the document to confirm that",
    "start": "584920",
    "end": "591880"
  },
  {
    "text": "and there it is so the document contains Chain of Thought prompting here so mentions in a few places so it's",
    "start": "594240",
    "end": "600279"
  },
  {
    "text": "relevant to our question that's good so now we've built the router we've built",
    "start": "600279",
    "end": "606880"
  },
  {
    "text": "this particular document grader you can see we've done this really simply using a chat model and Json mode that's all we",
    "start": "606880",
    "end": "612200"
  },
  {
    "text": "need to build these logical building blocks now let's just show you how to do really simple rag here's just a rag",
    "start": "612200",
    "end": "618480"
  },
  {
    "text": "prompt you're an assistant for question answering here's some context think about it carefully review the user's",
    "start": "618480",
    "end": "624680"
  },
  {
    "text": "question and then provide an answer that's all I do I retrieve some docs",
    "start": "624680",
    "end": "630120"
  },
  {
    "text": "I just format them really simply just to pull out the page content from each document",
    "start": "630120",
    "end": "635800"
  },
  {
    "text": "object and I format my prompt accordingly just here just string",
    "start": "635800",
    "end": "641120"
  },
  {
    "text": "formatting and then I just go ahead and pass that as a message into my chat",
    "start": "641120",
    "end": "646800"
  },
  {
    "text": "model in this case I don't need to use Json mode because I want natural language uh just string output I don't",
    "start": "646800",
    "end": "652600"
  },
  {
    "text": "need a Json object let's try that out there it is so this is like a nice",
    "start": "652600",
    "end": "658000"
  },
  {
    "text": "answer about chain thought prompting so we've actually already done quite a bit we have a router we've shown how to do",
    "start": "658000",
    "end": "664120"
  },
  {
    "text": "document grading now we've shown how to generate an answer pretty nice this web",
    "start": "664120",
    "end": "669360"
  },
  {
    "text": "search thing we actually already have we use Tav for that we have the web search tool defined down here so that's cool I",
    "start": "669360",
    "end": "675160"
  },
  {
    "text": "can show you how we would use that really quickly if I go down here so this",
    "start": "675160",
    "end": "680560"
  },
  {
    "text": "all I I need to do is just invoke it invoke and I can pass in a question",
    "start": "680560",
    "end": "690639"
  },
  {
    "text": "and I get some documents out that are related to llm agents and it gives me URLs and all that so this is pretty nice",
    "start": "699360",
    "end": "706160"
  },
  {
    "text": "that we'll be using this later so now let's just show these final two components hallucination grading and",
    "start": "706160",
    "end": "712600"
  },
  {
    "text": "answer grading so it's going to be really similar we already did we'll copy this over so just some hallucination",
    "start": "712600",
    "end": "718839"
  },
  {
    "text": "grer instructions just kind of logically the same as this but with slightly different prompt but same concept right",
    "start": "718839",
    "end": "725959"
  },
  {
    "text": "you're a grader teaching a quiz uh grading a quiz you'll be given some retrieved documents you'll be given an",
    "start": "725959",
    "end": "732120"
  },
  {
    "text": "answer and you're basically saying hey grade to see if it's if the answer is ground in the documents that's all we're",
    "start": "732120",
    "end": "737959"
  },
  {
    "text": "going to do here um so here is the final prompt this is where I'll put the",
    "start": "737959",
    "end": "743920"
  },
  {
    "text": "documents this is where I'll put the generation and I'm just going to format it using the document that I already",
    "start": "743920",
    "end": "749800"
  },
  {
    "text": "retrieved which is here that's all my doc text and the generate answer from",
    "start": "749800",
    "end": "756839"
  },
  {
    "text": "above that's this and let's just run this grater on and see cool so one of",
    "start": "756839",
    "end": "763360"
  },
  {
    "text": "the nice thing I add uh basically this little explanation in the prompt so I basically say hey give me a grade and an",
    "start": "763360",
    "end": "770160"
  },
  {
    "text": "explanation in this case you can see why it graded so if we step back what's kind of cool is this is all being done on my",
    "start": "770160",
    "end": "776320"
  },
  {
    "text": "laptop locally using uh 3 billion par model and it runs pretty quickly so you",
    "start": "776320",
    "end": "782560"
  },
  {
    "text": "know we've already implemented quite a bit of logic here we're through the grading hallucination bit and there's one more",
    "start": "782560",
    "end": "788120"
  },
  {
    "text": "grer that's just the final answer grader this is going to be similar but it's going to look at the final answer and",
    "start": "788120",
    "end": "795279"
  },
  {
    "text": "it's going to look at the question it just checks hey is does the does the does the answer actually address the",
    "start": "795279",
    "end": "801440"
  },
  {
    "text": "question so that's really it let's go ahead and run that here I have a test case I basically pass in a question I",
    "start": "801440",
    "end": "808519"
  },
  {
    "text": "pass in an example answer um that I got from one of the earlier tests of the",
    "start": "808519",
    "end": "813639"
  },
  {
    "text": "system and I just run that cool so I get a score and I get an explanation that's",
    "start": "813639",
    "end": "821240"
  },
  {
    "text": "great so we've actually already done really most of the hard work here we've built all these components individually",
    "start": "821240",
    "end": "827000"
  },
  {
    "text": "we've shown how to use Json mode to build a router Json mode how to build graders for documents hallucinations and",
    "start": "827000",
    "end": "833519"
  },
  {
    "text": "answer relevance we've just used an llm to produce uh answers using rag we have",
    "start": "833519",
    "end": "840399"
  },
  {
    "text": "a web search tool and we have a document grader tool and we have a retriever so",
    "start": "840399",
    "end": "845759"
  },
  {
    "text": "we have all these interesting components now here's the question how do you actually orchestrate this flow and tie",
    "start": "845759",
    "end": "851079"
  },
  {
    "text": "them all together and that's where I want to introduce this idea of Lang graph so Lang graph is a is an agent orchestration tool that we built use it",
    "start": "851079",
    "end": "857600"
  },
  {
    "text": "quite extensively there's other videos on it and I just want to show again how you can use it to orchestrate this whole",
    "start": "857600",
    "end": "862839"
  },
  {
    "text": "workflow really easily and running locally on my laptop",
    "start": "862839",
    "end": "868800"
  },
  {
    "text": "so the first part of L graph is actually defining some State that's going to live",
    "start": "877680",
    "end": "883399"
  },
  {
    "text": "across this entire agent flow so the state is basically going to be a set of things it's going to be like a schema",
    "start": "883399",
    "end": "890560"
  },
  {
    "text": "that each of these nodes has access to now because we're doing rag it's going to be things that are intuitive to the",
    "start": "890560",
    "end": "895680"
  },
  {
    "text": "rag process it's going to be like the input question the retrieve documents the answer all the things that I input",
    "start": "895680",
    "end": "903120"
  },
  {
    "text": "and output from each of these steps individually I'm going to save to this common state and that's going to persist",
    "start": "903120",
    "end": "909720"
  },
  {
    "text": "throughout my agent's lifetime in Lang graph so what's cool is in Lang graph I",
    "start": "909720",
    "end": "916000"
  },
  {
    "text": "can go ahead and Define this state class and this is very flexible I use a type dick for this",
    "start": "916000",
    "end": "923759"
  },
  {
    "text": "here's the keys these are things going to be relevant to my agent's behavior and here is a type pin for each one one",
    "start": "923759",
    "end": "929480"
  },
  {
    "text": "so that's all I need that's kind of the first component of my",
    "start": "929480",
    "end": "934720"
  },
  {
    "text": "agent now the second component of my agent is basically taking each of these steps that we did individually and",
    "start": "946000",
    "end": "952560"
  },
  {
    "text": "wrapping them those all as individual functions okay those are known as nodes",
    "start": "952560",
    "end": "958040"
  },
  {
    "text": "so those functions really all do one thing they take State as an input and",
    "start": "958040",
    "end": "963440"
  },
  {
    "text": "they modify it in some way so let's look at an example if I go down and I look at um our router for example what did our",
    "start": "963440",
    "end": "970560"
  },
  {
    "text": "router actually do it like took in the question and it output um like where to",
    "start": "970560",
    "end": "976959"
  },
  {
    "text": "go next go to a vector store or go to um web search for example so this function",
    "start": "976959",
    "end": "985000"
  },
  {
    "text": "is for example uh an edge in our Gra graph so it will basically look at the",
    "start": "985000",
    "end": "993519"
  },
  {
    "text": "question and Route it to one of two places so edges Pro provide some",
    "start": "993519",
    "end": "998920"
  },
  {
    "text": "connectivity between the nodes in our graph nodes on the other hand if you look at",
    "start": "998920",
    "end": "1004279"
  },
  {
    "text": "retrieval um for example this this takes in a question and returns documents so",
    "start": "1004279",
    "end": "1010440"
  },
  {
    "text": "for retrieval this is what I call a node because in this case we're actually",
    "start": "1010440",
    "end": "1015759"
  },
  {
    "text": "modifying the state we're adding something new to it like documents and in edges we're just deciding where",
    "start": "1015759",
    "end": "1021839"
  },
  {
    "text": "to go next those are the two concepts they're going to be using here nose and edges edges kind of direct where to go",
    "start": "1021839",
    "end": "1029038"
  },
  {
    "text": "nodes CH modify the state now I'm going to show you that all right now so here's",
    "start": "1029039",
    "end": "1035720"
  },
  {
    "text": "some code that lays out the nodes and edges of the graph using the things we already defined above so we're going to",
    "start": "1035720",
    "end": "1041038"
  },
  {
    "text": "find a node for retrieve that's going to take our retriever invoke it with a question the question we get from State",
    "start": "1041039",
    "end": "1047918"
  },
  {
    "text": "the state we pass into the node and the state is just this thing the state's just this dictionary this dictionary has",
    "start": "1047919",
    "end": "1054880"
  },
  {
    "text": "a key question which we can just reference easily as we see here get the",
    "start": "1054880",
    "end": "1060039"
  },
  {
    "text": "question from State invoke the Retriever with the question get documents now see",
    "start": "1060039",
    "end": "1066160"
  },
  {
    "text": "this we just can write the documents back to State using a dict just like we do here so in this case documents then",
    "start": "1066160",
    "end": "1073600"
  },
  {
    "text": "overwrite the value of documents here in the state that's really it",
    "start": "1073600",
    "end": "1079960"
  },
  {
    "text": "same idea for Generate you saw above that we actually ran same with generate so if you look at this logic here this",
    "start": "1079960",
    "end": "1085640"
  },
  {
    "text": "is exactly what we did above here so this is actually where we ran and tested this in isolation now I'm",
    "start": "1085640",
    "end": "1092600"
  },
  {
    "text": "just wrapping this functionality in a function called generate which is going",
    "start": "1092600",
    "end": "1098400"
  },
  {
    "text": "to now grab from our state question documents and loop step this is going to",
    "start": "1098400",
    "end": "1104080"
  },
  {
    "text": "track the number of generations that I do and it's going to write back the",
    "start": "1104080",
    "end": "1109640"
  },
  {
    "text": "generation and increment the loop step write those back out to the state that's all it's going to happen now we're at",
    "start": "1109640",
    "end": "1116960"
  },
  {
    "text": "grade documents so if you kind of look at our overall flow grade documents is this piece we've done a retrieval that",
    "start": "1116960",
    "end": "1123440"
  },
  {
    "text": "was the first node we defined grade documents is this other node and in grade documents we're going to do this",
    "start": "1123440",
    "end": "1129520"
  },
  {
    "text": "is exactly what we did above but in this case we're just going to grab the documents that we had in state which",
    "start": "1129520",
    "end": "1135280"
  },
  {
    "text": "were written to state by a retriever so if I back up a little bit we've",
    "start": "1135280",
    "end": "1141240"
  },
  {
    "text": "retriever node that takes in a question from State retrieves documents writes",
    "start": "1141240",
    "end": "1146400"
  },
  {
    "text": "those documents back out to State then the grade documents",
    "start": "1146400",
    "end": "1152480"
  },
  {
    "text": "node takes in state and has access to the documents that we wrote and does",
    "start": "1152480",
    "end": "1157640"
  },
  {
    "text": "something else on them in this node it will basically look at each document grade it for relevance to the question",
    "start": "1157640",
    "end": "1164799"
  },
  {
    "text": "this just like we did above and here's is where we can add some logic so",
    "start": "1164799",
    "end": "1170760"
  },
  {
    "text": "basically um we will keep it if it's relevant if it's not relevant we're",
    "start": "1170760",
    "end": "1176280"
  },
  {
    "text": "going to set this flag to do web search and we're going to at the end of this process write the new value of our web",
    "start": "1176280",
    "end": "1183120"
  },
  {
    "text": "search flag and the filter documents back out to state so that's really all we're doing in these nodes we're",
    "start": "1183120",
    "end": "1189120"
  },
  {
    "text": "basically taking the state looking at it reasoning about it writing something back out to State that's all that's",
    "start": "1189120",
    "end": "1196159"
  },
  {
    "text": "happening web search is another node we basically start here we take the question we take any existing documents",
    "start": "1196159",
    "end": "1203120"
  },
  {
    "text": "that we have we do web search using the retriever tool and we write the documents back out to State that's all",
    "start": "1203120",
    "end": "1209480"
  },
  {
    "text": "that's going on now we're almost done with this process here's where we to find our edges so the edges Define the The",
    "start": "1209480",
    "end": "1216280"
  },
  {
    "text": "Logical routing between the nodes so if you go back to the start of our graph we had this",
    "start": "1216280",
    "end": "1223400"
  },
  {
    "text": "router the router is just an edge it's conditional Edge that looks at the question decides either to go true or",
    "start": "1223400",
    "end": "1228480"
  },
  {
    "text": "the retrieval node or go to the web search node and so you can see we invoke",
    "start": "1228480",
    "end": "1233559"
  },
  {
    "text": "our router just like we did above we get the source like where to go",
    "start": "1233559",
    "end": "1238960"
  },
  {
    "text": "if it's web search return web search if it's Vector Store return Vector store",
    "start": "1238960",
    "end": "1244200"
  },
  {
    "text": "and you you'll see a little bit later how we use that but in these edges you can see what's going on is we're",
    "start": "1244200",
    "end": "1249760"
  },
  {
    "text": "returning a node to go to where web searches our web search node Vector",
    "start": "1249760",
    "end": "1255320"
  },
  {
    "text": "stores our Vector store node so the way to think about this is in nodes I take",
    "start": "1255320",
    "end": "1260760"
  },
  {
    "text": "in state and I update state in edges I take in state and I decide what node to",
    "start": "1260760",
    "end": "1266400"
  },
  {
    "text": "visit next that's all it's happening decide to generate this one is",
    "start": "1266400",
    "end": "1271720"
  },
  {
    "text": "going to um basically decide if there is web search flag that's been enabled I go",
    "start": "1271720",
    "end": "1279320"
  },
  {
    "text": "to web search otherwise I go directly to generate and that matches exactly what",
    "start": "1279320",
    "end": "1284840"
  },
  {
    "text": "we show here that's kind of what this Edge is so basically this is indicating um I either go to web search if any",
    "start": "1284840",
    "end": "1291000"
  },
  {
    "text": "documents have been deemed irrelevant or I go to generate now this is the final step in",
    "start": "1291000",
    "end": "1297279"
  },
  {
    "text": "our flow where we do grading for hallucinations and answer relevance or answer usefulness to the question so",
    "start": "1297279",
    "end": "1305400"
  },
  {
    "text": "this is the hallucination check we already showed that above we're just adding it to this Edge and we're going",
    "start": "1305400",
    "end": "1310760"
  },
  {
    "text": "to get a grade so check hallucinations if the grade is yes as in it's it is not",
    "start": "1310760",
    "end": "1316279"
  },
  {
    "text": "there's no hallucination it's grounded the documents then we do the grade",
    "start": "1316279",
    "end": "1321640"
  },
  {
    "text": "generation versus the question so that's the second check we already showed that above that's what we do right here and",
    "start": "1321640",
    "end": "1327799"
  },
  {
    "text": "then if that's yes then we deem the result is useful so that is that the",
    "start": "1327799",
    "end": "1333520"
  },
  {
    "text": "answer is actually useful to kind of finalize and send to the user otherwise",
    "start": "1333520",
    "end": "1339279"
  },
  {
    "text": "the answer is deemed not useful okay now you can also see we do this Max retry",
    "start": "1339279",
    "end": "1345080"
  },
  {
    "text": "check here so if you recall in the generation node we incremented that every time we do generation we want to",
    "start": "1345080",
    "end": "1351640"
  },
  {
    "text": "make sure we don't keep retrying so we basically check this Max retry default to three and if that's exceeded then we",
    "start": "1351640",
    "end": "1359240"
  },
  {
    "text": "just will exit um and that's what you can kind of see here Max retries is reached",
    "start": "1359240",
    "end": "1365799"
  },
  {
    "text": "um so this looks at basically if there was hallucination present then the",
    "start": "1365799",
    "end": "1373960"
  },
  {
    "text": "answer is not supported by the documents and then finally",
    "start": "1373960",
    "end": "1379440"
  },
  {
    "text": "uh Max retries again so that is the final Edge now we're going to add logic to",
    "start": "1379440",
    "end": "1386080"
  },
  {
    "text": "stitch this all together so this is where I'm going to add all the nodes I defined above with",
    "start": "1386080",
    "end": "1392720"
  },
  {
    "text": "names and I'm going to then take all the conditional edges which I defined",
    "start": "1392720",
    "end": "1398840"
  },
  {
    "text": "basically here route question for example this returns either web",
    "start": "1398840",
    "end": "1405240"
  },
  {
    "text": "search or vector store right so basically this is where I can",
    "start": "1405240",
    "end": "1413880"
  },
  {
    "text": "map the outputs of my Edge to the node names in this case I make it simple",
    "start": "1413880",
    "end": "1419320"
  },
  {
    "text": "because I keep them the same but you can imagine those can be different so basically this is the mapping between",
    "start": "1419320",
    "end": "1424679"
  },
  {
    "text": "the node to visit and the output of the conditional Edge but that's really it so",
    "start": "1424679",
    "end": "1430880"
  },
  {
    "text": "I add a conditional entry point I then add a Edge from web search to generate",
    "start": "1430880",
    "end": "1436200"
  },
  {
    "text": "and so I create the connectivity of my graph here I compile it and I can",
    "start": "1436200",
    "end": "1441360"
  },
  {
    "text": "visualize it and there we go that's our entire graph visualize it's pretty cool so think about all we're doing we're",
    "start": "1441360",
    "end": "1447919"
  },
  {
    "text": "starting with that conditional Edge that's going to look at the question and decide to either visit web search or",
    "start": "1447919",
    "end": "1453159"
  },
  {
    "text": "vector store based upon the question that's that router we built first then we're going to do",
    "start": "1453159",
    "end": "1459360"
  },
  {
    "text": "retrieval we'll grade documents if any documents are not relevant we're going",
    "start": "1459360",
    "end": "1464760"
  },
  {
    "text": "to go ahead and do web search and supplement them with web search otherwise we'll go to",
    "start": "1464760",
    "end": "1471080"
  },
  {
    "text": "generate so from web search we'll go to generate now when we've done with",
    "start": "1471080",
    "end": "1478120"
  },
  {
    "text": "generate we did two checks one is that",
    "start": "1478120",
    "end": "1483200"
  },
  {
    "text": "the generation is not supported by the documents so there's hallucinations in",
    "start": "1483200",
    "end": "1489760"
  },
  {
    "text": "that case we just retry that's what this is representing the other is that it's",
    "start": "1489760",
    "end": "1495600"
  },
  {
    "text": "actually not useful so it doesn't actually answer the question if that occurs we kick back and retry web search",
    "start": "1495600",
    "end": "1503000"
  },
  {
    "text": "so that's kind of the current logic in the graph and you can modify this accordingly but that's what's going to go on so those are what the two",
    "start": "1503000",
    "end": "1508760"
  },
  {
    "text": "fallbacks do and then finally the answer is either useful which means that it's",
    "start": "1508760",
    "end": "1515760"
  },
  {
    "text": "both supported as in no hallucinations and it's useful as it answers the question or Max retries that's another",
    "start": "1515760",
    "end": "1522679"
  },
  {
    "text": "way we end but that's the overall connectivity of our graph we've put together in L graph so if we Zoom all",
    "start": "1522679",
    "end": "1527760"
  },
  {
    "text": "the way back out out we've used AMA we've used AMA to load llama",
    "start": "1527760",
    "end": "1535760"
  },
  {
    "text": "3B uh fp16 llm and Json mode I've tested",
    "start": "1535760",
    "end": "1541760"
  },
  {
    "text": "each of these components out individually I build a router build a retrieval grader build a rag prompt",
    "start": "1541760",
    "end": "1549520"
  },
  {
    "text": "build a hallucination grader build an answer grater we tested all them individually now I put them together",
    "start": "1549520",
    "end": "1555840"
  },
  {
    "text": "into a graph using line graph here's the overall graph connectivity and we can test this now so when I kick",
    "start": "1555840",
    "end": "1563640"
  },
  {
    "text": "this off all I need to do is supply some initial State values to work with so",
    "start": "1563640",
    "end": "1570960"
  },
  {
    "text": "basically I can pass in a question and I'll pass in Max retries three these",
    "start": "1570960",
    "end": "1576240"
  },
  {
    "text": "will just populate my state with those two inputs and that's all the graph needs to get started now I just can run",
    "start": "1576240",
    "end": "1583000"
  },
  {
    "text": "graph. stream that's one way to run the graph pass in the inputs and I'm going to stream in the graph State at every",
    "start": "1583000",
    "end": "1590080"
  },
  {
    "text": "step and I'll print it out so we can see this running right now so first we're going Route",
    "start": "1590080",
    "end": "1597039"
  },
  {
    "text": "question is going to decide whether to use the vector store or web search based",
    "start": "1597039",
    "end": "1602120"
  },
  {
    "text": "upon the content of the question so it it desides the vector store that's good it retrieved these documents so it's I'm",
    "start": "1602120",
    "end": "1608880"
  },
  {
    "text": "basically spitting out everything here so it's writing out the whole state every time so this is cool it's already",
    "start": "1608880",
    "end": "1614159"
  },
  {
    "text": "done a bunch of work here it's gone ahead and it has uh graded the documents it deemed um one of them to be",
    "start": "1614159",
    "end": "1621840"
  },
  {
    "text": "non-relevant but the rest are relevant so then it decides we'll go to web search it's going to be kicking off web",
    "start": "1621840",
    "end": "1629279"
  },
  {
    "text": "search it runs that this is the entire State printed out by web search cool and basically it did the",
    "start": "1629279",
    "end": "1637880"
  },
  {
    "text": "hallucination check and it also graded the generation R to the question and",
    "start": "1637880",
    "end": "1644000"
  },
  {
    "text": "that's it so the entire graph ran now I can go to Langs Smith and show you also",
    "start": "1644000",
    "end": "1649520"
  },
  {
    "text": "the entire flow of that graph so here I'm in lsmith I'm looking",
    "start": "1649520",
    "end": "1654640"
  },
  {
    "text": "at the trace so you can see first we do retrieval from the vector store",
    "start": "1654640",
    "end": "1662080"
  },
  {
    "text": "retriever we get a bunch of documents that's cool we graded them for relevance",
    "start": "1662080",
    "end": "1667279"
  },
  {
    "text": "I can open that up and we can see here is cadow Lama grading each document pretty",
    "start": "1667279",
    "end": "1673440"
  },
  {
    "text": "nice and here is what I pass and here's the system prompt I pass",
    "start": "1673440",
    "end": "1678799"
  },
  {
    "text": "here is the um the human message pretty cool which contains all the documents",
    "start": "1678799",
    "end": "1686360"
  },
  {
    "text": "and here's a rendered output it deems that to be relevant it goes through each one of the four so that's",
    "start": "1686360",
    "end": "1692640"
  },
  {
    "text": "cool now here's where it basically hits that conditional ledge decide to generate because one of the documents is",
    "start": "1695960",
    "end": "1703080"
  },
  {
    "text": "deemed to be not relevant it decides I want to do a web search so then it kicks over and goes to the web search search",
    "start": "1703080",
    "end": "1709000"
  },
  {
    "text": "node that runs Tav that returns some web search results for me pretty nice then I go to",
    "start": "1709000",
    "end": "1717200"
  },
  {
    "text": "generate which runs chatow llama again in this case it generates an answer to",
    "start": "1717200",
    "end": "1722360"
  },
  {
    "text": "my question my question was related to agent memory there are two categories of agent memory short-term and longterm and",
    "start": "1722360",
    "end": "1729519"
  },
  {
    "text": "so forth based upon web search and my retrieval it answers the question so",
    "start": "1729519",
    "end": "1734799"
  },
  {
    "text": "then this is the conditional Edge that will both grade the uh",
    "start": "1734799",
    "end": "1740919"
  },
  {
    "text": "hallucinations so in this particular case it deems that the uh answer is",
    "start": "1740919",
    "end": "1746240"
  },
  {
    "text": "grounded so that's cool I can look at the grading here and it finally",
    "start": "1746240",
    "end": "1752080"
  },
  {
    "text": "graded um you can see here yes uh for answer helpfulness so it deems that it's",
    "start": "1752080",
    "end": "1757919"
  },
  {
    "text": "actually helpful and actually addresses the question so that's the entire flow of our agent that does a whole bunch of",
    "start": "1757919",
    "end": "1764480"
  },
  {
    "text": "different things all running locally on my laptop using a 3 billion parameter model so that's pretty neat if I go back",
    "start": "1764480",
    "end": "1772080"
  },
  {
    "text": "and kind of show The Notebook I'll make sure this is all accessible so you can play with this yourself this is all free",
    "start": "1772080",
    "end": "1777559"
  },
  {
    "text": "to use I'm using local free embeddings I'm using locally running three billion parameter model I'm orchestrating this",
    "start": "1777559",
    "end": "1783679"
  },
  {
    "text": "using L graph you can orchestrate things that are much more complicated or much less complicated but this is really",
    "start": "1783679",
    "end": "1789519"
  },
  {
    "text": "showing you that even with a 3 billion parameter model you can build pretty complicated interesting multi-step",
    "start": "1789519",
    "end": "1795640"
  },
  {
    "text": "workflows that actually run reliably and will run locally so again I've actually",
    "start": "1795640",
    "end": "1800799"
  },
  {
    "text": "been really excited about working with local models for a while I find that orchestrating them with langra is a very nice way to kind of build these reliable",
    "start": "1800799",
    "end": "1808000"
  },
  {
    "text": "multi-step workflows that can run locally on my laptop to do arbitrary tasks this is just a toy example i' like",
    "start": "1808000",
    "end": "1815360"
  },
  {
    "text": "to use to showcase the functionality uh in a rag setting but it can handle many",
    "start": "1815360",
    "end": "1820760"
  },
  {
    "text": "other types of tasks you can imagine running locally and also reliably as we just saw here this is a agent that would",
    "start": "1820760",
    "end": "1827360"
  },
  {
    "text": "be fairly difficult to run using a more react style open-ended agent Loop but",
    "start": "1827360",
    "end": "1833039"
  },
  {
    "text": "using Lang graph with each of these nodes and edges very clearly enumerated you're asking the llm to make very",
    "start": "1833039",
    "end": "1839679"
  },
  {
    "text": "specific narrow decisions at specific points as you can see here using Json",
    "start": "1839679",
    "end": "1845600"
  },
  {
    "text": "mode like we did in the router um and if you structure it that way you actually can build pretty complicated workflows",
    "start": "1845600",
    "end": "1851880"
  },
  {
    "text": "that actually run reliably and locally so please leave any questions um in in",
    "start": "1851880",
    "end": "1857080"
  },
  {
    "text": "the comments happy to address them and I'll make sure this is all open source thanks",
    "start": "1857080",
    "end": "1863480"
  }
]