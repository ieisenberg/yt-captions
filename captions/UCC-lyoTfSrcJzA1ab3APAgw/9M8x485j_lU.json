[
  {
    "start": "0",
    "end": "90000"
  },
  {
    "text": "be live um thank you everyone for joining",
    "start": "4259",
    "end": "10620"
  },
  {
    "text": "um today we're gonna be talking about a new kind of like syntax and expression language that uh uh we largely you know",
    "start": "10620",
    "end": "18539"
  },
  {
    "text": "um came up with over the past few weeks um he can talk a lot more about the motivation for it",
    "start": "18539",
    "end": "24359"
  },
  {
    "text": "um the interface for it what you can do with it and all that it's a minor logistic stuff before we uh get going",
    "start": "24359",
    "end": "32578"
  },
  {
    "text": "this is being recorded it will be posted on YouTube afterwards",
    "start": "32579",
    "end": "37620"
  },
  {
    "text": "um we will mostly be talking about this new uh commercial language today so",
    "start": "37620",
    "end": "43860"
  },
  {
    "text": "we're very open to questions but please try to keep them focused on that um and in that vein if you do have",
    "start": "43860",
    "end": "49260"
  },
  {
    "text": "questions please put them on the in the little q a section with the Box on the",
    "start": "49260",
    "end": "54840"
  },
  {
    "text": "right um so we'll we'll be going through those after um and and you can also upload the ones",
    "start": "54840",
    "end": "60600"
  },
  {
    "text": "that you want to hear answer the most and will basically go down in order answer that",
    "start": "60600",
    "end": "66000"
  },
  {
    "text": "um for the for the chat today we'll be talking about the motivation the interface and then running through some",
    "start": "66000",
    "end": "71159"
  },
  {
    "text": "examples it will probably be on the shorter side so if you guys have questions or examples of what you want",
    "start": "71159",
    "end": "77040"
  },
  {
    "text": "to see it would look like or anything like that please just post them in there and we'll probably rely on those pretty liberally or we can end early as well",
    "start": "77040",
    "end": "85320"
  },
  {
    "text": "um I think that's all the logistics stuff uh you know you want to take it away uh",
    "start": "85320",
    "end": "91979"
  },
  {
    "start": "90000",
    "end": "240000"
  },
  {
    "text": "yeah uh so I think just touching briefly on the motivation to start uh",
    "start": "91979",
    "end": "98579"
  },
  {
    "text": "uh I think something that we found as we work with our lives is that they're most useful when we pair them up with some",
    "start": "98579",
    "end": "105119"
  },
  {
    "text": "code that runs either before the llm or after the llm so if it's before it would",
    "start": "105119",
    "end": "110640"
  },
  {
    "text": "be to like assemble the inputs that we pass into the llm if it's after it would be to parse the the outputs of the llm",
    "start": "110640",
    "end": "117840"
  },
  {
    "text": "or to run them use them somehow uh in some way so that",
    "start": "117840",
    "end": "123119"
  },
  {
    "text": "in Essence is actually what all of our",
    "start": "123119",
    "end": "128220"
  },
  {
    "text": "built-in chains are they're just common patterns to an organized code that runs",
    "start": "128220",
    "end": "133860"
  },
  {
    "text": "either before or after another one so that's things like the SQL database",
    "start": "133860",
    "end": "139319"
  },
  {
    "text": "chain the API chain that you know tagging chain extraction chain all those things and that mostly works great at",
    "start": "139319",
    "end": "148020"
  },
  {
    "text": "least we like to think so um but it wasn't easy to create custom",
    "start": "148020",
    "end": "153720"
  },
  {
    "text": "combinations so custom patterns for your specific use cases or to modify the ones",
    "start": "153720",
    "end": "159840"
  },
  {
    "text": "that we have created and made available in in the in the library so that's the",
    "start": "159840",
    "end": "166200"
  },
  {
    "text": "main motivation behind the expression language is to just make it super easy to build any custom combinations in a",
    "start": "166200",
    "end": "173220"
  },
  {
    "text": "way that you can fully inspect what's there and modify it Etc uh but before we could do that uh we",
    "start": "173220",
    "end": "180959"
  },
  {
    "text": "actually have to do something else which was to create a single interface that uh",
    "start": "180959",
    "end": "186540"
  },
  {
    "text": "all of our building blocks Implement so we have lots of different building blocks in in Lang chain we have prompt",
    "start": "186540",
    "end": "192959"
  },
  {
    "text": "templates we have llms chat models output parsers chains tools retrievers",
    "start": "192959",
    "end": "198900"
  },
  {
    "text": "all sorts of things uh and they all implemented different different",
    "start": "198900",
    "end": "205200"
  },
  {
    "text": "interfaces so different methods so you'd call generate on a chat model but you'd",
    "start": "205200",
    "end": "210239"
  },
  {
    "text": "call run on a chain and you'd call a format on a prompt or whatever it may be",
    "start": "210239",
    "end": "217019"
  },
  {
    "text": "um and that turns out to not work very well if you want to compose them because we'd have to keep guessing what kind of thing",
    "start": "217019",
    "end": "223500"
  },
  {
    "text": "is this what method do we call on it Etc so the first step that we took was to",
    "start": "223500",
    "end": "228799"
  },
  {
    "text": "just create a single interface that all of these things Implement uh which I'm",
    "start": "228799",
    "end": "234720"
  },
  {
    "text": "going to show to you now and I think adding adding on to that even like we we had like a lot of chains",
    "start": "234720",
    "end": "242879"
  },
  {
    "start": "240000",
    "end": "320000"
  },
  {
    "text": "didn't support kind of like that properly or they didn't or or a lot of different components didn't even have",
    "start": "242879",
    "end": "248459"
  },
  {
    "text": "batch they didn't even have streaming so it wasn't even like so there's one method there's there's one there's one",
    "start": "248459",
    "end": "254400"
  },
  {
    "text": "uh there's one motivation where like there's different ways of like call and run kind of do the same thing and",
    "start": "254400",
    "end": "260880"
  },
  {
    "text": "there's a bunch of confusion around that there's another component where you know streamings just wasn't supported on a",
    "start": "260880",
    "end": "266820"
  },
  {
    "text": "lot of the chains or you had to do it through basically uh callbacks which which still exist and you can still",
    "start": "266820",
    "end": "272280"
  },
  {
    "text": "absolutely use and then I think there's pros and cons to that but now there's a more standard way to kind of do all",
    "start": "272280",
    "end": "277860"
  },
  {
    "text": "these things and async as well yeah exactly uh so yeah something that",
    "start": "277860",
    "end": "283259"
  },
  {
    "text": "you'll notice as well uh in as you update blank chain to the latest version is the async support throughout",
    "start": "283259",
    "end": "290580"
  },
  {
    "text": "the library is way more complete than it was and now there should be very little that doesn't have async support",
    "start": "290580",
    "end": "297419"
  },
  {
    "text": "um and that's uh true of the new methods that I'm gonna explain now as well uh so",
    "start": "297419",
    "end": "304199"
  },
  {
    "text": "the most basic method is is just you know you have a single uh input that you",
    "start": "304199",
    "end": "309600"
  },
  {
    "text": "want to pass to a prompt or a model or a retriever and you want to get back a single output so that's what we call",
    "start": "309600",
    "end": "314759"
  },
  {
    "text": "invoke um so that's uh that's the most basic",
    "start": "314759",
    "end": "322220"
  },
  {
    "text": "ah there we go uh I wasn't finding that uh so you just pass a single input and",
    "start": "322220",
    "end": "327840"
  },
  {
    "text": "you get back a single output then the next step up is you have",
    "start": "327840",
    "end": "335280"
  },
  {
    "text": "a list of inputs um so in this case we're calling the same chain with you know two two",
    "start": "335280",
    "end": "342060"
  },
  {
    "text": "dictionaries of uh inputs one is topic Bears and the other is topic cats and we",
    "start": "342060",
    "end": "347580"
  },
  {
    "text": "get back a list of two different outputs uh which basically is the result of running this chain over these uh two",
    "start": "347580",
    "end": "355380"
  },
  {
    "text": "inputs um the reason this is useful is that where possible we actually take advantage of",
    "start": "355380",
    "end": "364199"
  },
  {
    "text": "the batch capabilities of the underlying um components of the chain so if you",
    "start": "364199",
    "end": "370320"
  },
  {
    "text": "built a chain that is a prompt passed into say an open iilm we actually uh",
    "start": "370320",
    "end": "377400"
  },
  {
    "text": "when you call batch with a list of three things we actually call open AI only once because they offer that capability",
    "start": "377400",
    "end": "383520"
  },
  {
    "text": "capability in their API and we take advantage of it and you don't have to think about that or do anything special",
    "start": "383520",
    "end": "389220"
  },
  {
    "text": "for that and then finally something that a lot of people have asked for is a stream method",
    "start": "389220",
    "end": "398220"
  },
  {
    "text": "that returns an iterator or an async iterator in the async version um so this you can just consume this",
    "start": "398220",
    "end": "404759"
  },
  {
    "text": "with a standard for Loop in Python so it becomes a lot easier to consume",
    "start": "404759",
    "end": "411180"
  },
  {
    "text": "streaming out streaming output from Adella lab or from a sequence of prompt",
    "start": "411180",
    "end": "417000"
  },
  {
    "text": "to LM Etc and then you have the async versions uh",
    "start": "417000",
    "end": "424500"
  },
  {
    "start": "420000",
    "end": "460000"
  },
  {
    "text": "so a invoke is the async version of of the invoke a batch",
    "start": "424500",
    "end": "430620"
  },
  {
    "text": "is the async version of batch and a stream is the Asic version of the stream",
    "start": "430620",
    "end": "436500"
  },
  {
    "text": "which returns an async directory so now that we have this interface uh we",
    "start": "436500",
    "end": "447120"
  },
  {
    "text": "we can combine these things because they all implement the same interface we can combine them into sequences",
    "start": "447120",
    "end": "454080"
  },
  {
    "text": "um and let's see what that looks like so",
    "start": "454080",
    "end": "459960"
  },
  {
    "text": "so first of all if you're used to using Lang chain from before notice that I'm importing exactly the same things as",
    "start": "459960",
    "end": "466620"
  },
  {
    "text": "you're already using so chat prompt template chat open AI you've already seen these things before you're already",
    "start": "466620",
    "end": "472080"
  },
  {
    "text": "using the menu code we haven't changed any of that all the arguments are the same uh you you create these things in",
    "start": "472080",
    "end": "479280"
  },
  {
    "text": "exactly the same way so we create a model we create a prompt and then if we",
    "start": "479280",
    "end": "484919"
  },
  {
    "text": "want a chain which is just you know pass the inputs into the prompt format",
    "start": "484919",
    "end": "490259"
  },
  {
    "text": "The Prompt pass the formatted prompt into the model and get me the result of the model that's expressed by this so we",
    "start": "490259",
    "end": "497520"
  },
  {
    "text": "pipe The Prompt into the model and that becomes a sequence where we first call",
    "start": "497520",
    "end": "502680"
  },
  {
    "text": "The Prompt and then the model so if I call this",
    "start": "502680",
    "end": "508039"
  },
  {
    "text": "I get back an AI message because this is a chat model and if we go into langsmith",
    "start": "508620",
    "end": "515940"
  },
  {
    "text": "I mean make sure I'm getting the right run so yes 5 PM for me",
    "start": "515940",
    "end": "522839"
  },
  {
    "text": "um so you can see that overall we call this thing that we call a runnable sequence",
    "start": "522839",
    "end": "529500"
  },
  {
    "text": "and we pass the input which is Foo Bears and the final output was a message which",
    "start": "529500",
    "end": "536399"
  },
  {
    "text": "you know contains why don't bears like fast food because they can't catch it and you can see inside this sequence",
    "start": "536399",
    "end": "542820"
  },
  {
    "text": "actually two things Ran So one is the prompt where we pass the inputs and we",
    "start": "542820",
    "end": "547860"
  },
  {
    "text": "got back a formatted prompt in the form of a prompt value and then the second",
    "start": "547860",
    "end": "554279"
  },
  {
    "text": "thing is we called the chat open AI model with",
    "start": "554279",
    "end": "559800"
  },
  {
    "text": "that formatted prompt and we got back this AI message as a response and you",
    "start": "559800",
    "end": "565200"
  },
  {
    "text": "can see you can inspect exactly what happened here and you see this is going to become even more useful for more",
    "start": "565200",
    "end": "571620"
  },
  {
    "text": "complex change that I'm going to show you in a second so sometimes you're going to want to",
    "start": "571620",
    "end": "579320"
  },
  {
    "text": "customize some arguments that you'd pass to like modeled or generate uh before or",
    "start": "579320",
    "end": "586260"
  },
  {
    "text": "predict messages or all the other methods that we had so for instance the stop words is a is an example of that in",
    "start": "586260",
    "end": "593220"
  },
  {
    "text": "order to do that you can still do that here all you need to do is call model.bind with the arguments that you",
    "start": "593220",
    "end": "599459"
  },
  {
    "text": "want to specify so in this case I'm building it a different chain now which is the same",
    "start": "599459",
    "end": "605459"
  },
  {
    "text": "prompt but uh making sure that the model only produces a single line because I'm stopping on",
    "start": "605459",
    "end": "612000"
  },
  {
    "text": "the First new line character and if I call that I get back an incomplete joke which",
    "start": "612000",
    "end": "618959"
  },
  {
    "text": "isn't terribly nice but that is what I asked for um so you can see",
    "start": "618959",
    "end": "625860"
  },
  {
    "text": "now we get back a single line because the stop word was applied and we can see",
    "start": "625860",
    "end": "633000"
  },
  {
    "text": "the stop word here in the invocation params that were sent to the model",
    "start": "633000",
    "end": "638160"
  },
  {
    "text": "so that's not the only thing we can we can attach to models so",
    "start": "638160",
    "end": "645200"
  },
  {
    "start": "640000",
    "end": "715000"
  },
  {
    "text": "that's we use the same bind the same bind method to attach functions if we",
    "start": "645200",
    "end": "652140"
  },
  {
    "text": "want to use the open AI function So This Is Us describing a function here so this",
    "start": "652140",
    "end": "657899"
  },
  {
    "text": "is a function called joke uh which uh open AI is supposed to then",
    "start": "657899",
    "end": "663839"
  },
  {
    "text": "call so to speak with a setup and a punchline for the joke so kind of",
    "start": "663839",
    "end": "669000"
  },
  {
    "text": "splitting up the joke into two uh components and so what we're doing is",
    "start": "669000",
    "end": "674700"
  },
  {
    "text": "we're using the exact same prompt as before but we're saying now hey I want the model to be bound to these functions",
    "start": "674700",
    "end": "681180"
  },
  {
    "text": "and I want to actually Force the model to call the joke function",
    "start": "681180",
    "end": "687240"
  },
  {
    "text": "um so if we now call this oh that would be because I didn't run",
    "start": "687240",
    "end": "693000"
  },
  {
    "text": "this uh if we now call this then we get the",
    "start": "693000",
    "end": "700019"
  },
  {
    "text": "next one so you can see now the output we got was a function call for the",
    "start": "700019",
    "end": "705600"
  },
  {
    "text": "function called joke and the arguments uh were this the Json object with a",
    "start": "705600",
    "end": "711240"
  },
  {
    "text": "setup and the punch line but maybe we can just keep changing",
    "start": "711240",
    "end": "717779"
  },
  {
    "start": "715000",
    "end": "815000"
  },
  {
    "text": "things and see what happens so now um something that you probably are going to",
    "start": "717779",
    "end": "724019"
  },
  {
    "text": "find useful is because this model is a chat model it outputs chat messages but",
    "start": "724019",
    "end": "730019"
  },
  {
    "text": "sometimes you're just going to want the the string output like the content of the message so to do that all you have",
    "start": "730019",
    "end": "735959"
  },
  {
    "text": "to do is use this string output parser and create a chain where you pipe The Prompt into the model and then you pipe",
    "start": "735959",
    "end": "742680"
  },
  {
    "text": "it to the string output parser and all this does is extract the content out of the message so if we run that",
    "start": "742680",
    "end": "750260"
  },
  {
    "text": "you see we get back the same joke or similar I assume uh but now only the",
    "start": "751800",
    "end": "757260"
  },
  {
    "text": "text so if I go and check it out",
    "start": "757260",
    "end": "762660"
  },
  {
    "text": "there we go so now you see this sequence now actually has three steps because we asked for three steps so we still have",
    "start": "762660",
    "end": "768600"
  },
  {
    "text": "the same chat prompt template exactly the same prompt and we have the model which produced an",
    "start": "768600",
    "end": "776519"
  },
  {
    "text": "AI message but then the string output parser is",
    "start": "776519",
    "end": "782160"
  },
  {
    "text": "actually extracting only the text out of the message so it gets the full message with content",
    "start": "782160",
    "end": "788820"
  },
  {
    "text": "Etc and then it extracts the the string up so",
    "start": "788820",
    "end": "794519"
  },
  {
    "text": "if we go back to those open AI functions um remember we look here",
    "start": "794519",
    "end": "802560"
  },
  {
    "text": "this is what we got back last time it was the raw output from openai with a",
    "start": "802560",
    "end": "807899"
  },
  {
    "text": "function call but we actually probably don't want to use it exactly like that we want to",
    "start": "807899",
    "end": "813839"
  },
  {
    "text": "parse it and extract the information we actually asked for so we have a few parsers we already had these before now",
    "start": "813839",
    "end": "820620"
  },
  {
    "start": "815000",
    "end": "860000"
  },
  {
    "text": "they just implement this same interface so we have a few parses designed for open AI functions so let's try this Json",
    "start": "820620",
    "end": "828060"
  },
  {
    "text": "output functions parser so if we run that so you see the same",
    "start": "828060",
    "end": "833519"
  },
  {
    "text": "prompt the same model bound to the functions and now we are adding the Json output functions parser at the end",
    "start": "833519",
    "end": "841860"
  },
  {
    "text": "can I run that already no uh and now what this does is it extracts the",
    "start": "841860",
    "end": "849180"
  },
  {
    "text": "arguments uh for the function so setup and punch line if we go and",
    "start": "849180",
    "end": "854700"
  },
  {
    "text": "check out what that looks like you can see we have a sequence of three",
    "start": "854700",
    "end": "859920"
  },
  {
    "text": "again chat open AI produce the full function call with the name of the function and",
    "start": "859920",
    "end": "866700"
  },
  {
    "text": "the arguments but the output functions parser turn that big thing into just the",
    "start": "866700",
    "end": "872220"
  },
  {
    "text": "arguments so the setup and the punchline all this took was the chain one more",
    "start": "872220",
    "end": "878279"
  },
  {
    "start": "875000",
    "end": "970000"
  },
  {
    "text": "thing at the end so it's really easy for us to you know evolve and iterate on the software",
    "start": "878279",
    "end": "884639"
  },
  {
    "text": "designing because we just chain one other thing and and so this is basically the the tagging and extraction chains",
    "start": "884639",
    "end": "891660"
  },
  {
    "text": "that we've got in link chain right now where you kind of pass in unstructured data and then you get back structured",
    "start": "891660",
    "end": "896820"
  },
  {
    "text": "stuff and actually what and and if you've kind of like used those in link chain there's a function called like",
    "start": "896820",
    "end": "901980"
  },
  {
    "text": "create tagging chain or something like that and basically what that does under the hood is it kind of creates a",
    "start": "901980",
    "end": "908579"
  },
  {
    "text": "specific prompt it creates a specific kind of like model quarks and it creates a specific output Power Circle which are",
    "start": "908579",
    "end": "914639"
  },
  {
    "text": "exactly the same here and then it passes them into llm chain um and so it's it this is the equivalent",
    "start": "914639",
    "end": "920519"
  },
  {
    "text": "of that it's just kind of like expressed um more kind of like declaratively and",
    "start": "920519",
    "end": "925620"
  },
  {
    "text": "and and um is kind of like shown to the user and it's a bit more clear kind of like",
    "start": "925620",
    "end": "930959"
  },
  {
    "text": "what's going on but this is exactly that kind of like functionality there yeah exactly and now",
    "start": "930959",
    "end": "939300"
  },
  {
    "text": "um I guess a big advantage of this one is that we can just switch out something at the end and see what happens so there",
    "start": "939300",
    "end": "945540"
  },
  {
    "text": "we have Json output functions parser which produced the whole kwrx object now if we switch out for",
    "start": "945540",
    "end": "953399"
  },
  {
    "text": "Json key output functions parser and we select the key called setup I'm guessing",
    "start": "953399",
    "end": "959639"
  },
  {
    "text": "we're just going to get the value of the key the setup key but let's find out foreign",
    "start": "959639",
    "end": "966800"
  },
  {
    "text": "if we go and check that out",
    "start": "967459",
    "end": "971540"
  },
  {
    "text": "so we can see open AI produced the full function call name of the function setup punchline and",
    "start": "973019",
    "end": "981600"
  },
  {
    "text": "then the new parser that we chose out of all this stuff it extracted uh just why",
    "start": "981600",
    "end": "987480"
  },
  {
    "text": "don't Bears wear shoes which was this value of the setup argument here",
    "start": "987480",
    "end": "993500"
  },
  {
    "text": "and yeah so that's uh just showing you how to iterate on the stuff you're",
    "start": "994579",
    "end": "1000440"
  },
  {
    "start": "995000",
    "end": "1035000"
  },
  {
    "text": "designing and uh you know switching out parts of the sequence and seeing what happens and inspecting in maximum so",
    "start": "1000440",
    "end": "1010040"
  },
  {
    "text": "something we can do after now is",
    "start": "1010040",
    "end": "1015320"
  },
  {
    "text": "uh we have a few helpers which you can use to take inputs from",
    "start": "1015320",
    "end": "1022399"
  },
  {
    "text": "say the beginning of of the sequence and use them further down later in the",
    "start": "1022399",
    "end": "1028339"
  },
  {
    "text": "sequence but I think it's easier to just see them in action so here we are going to",
    "start": "1028339",
    "end": "1036220"
  },
  {
    "text": "basically recreate what we have as the llm chain in the library but in a way",
    "start": "1036220",
    "end": "1042678"
  },
  {
    "text": "that you know is designed exactly for uh the use case that you have so uh we're",
    "start": "1042679",
    "end": "1049280"
  },
  {
    "text": "importing a few things we're importing the chroma Vector store the open AI embeddings model and this runnable pass",
    "start": "1049280",
    "end": "1055460"
  },
  {
    "text": "through so runnable passthrough is just a function that receives uh some",
    "start": "1055460",
    "end": "1061760"
  },
  {
    "text": "argument and just Returns the same argument just the identity function and and I was a little confused by this at",
    "start": "1061760",
    "end": "1068480"
  },
  {
    "text": "the start but I think like the way to think about it is like you know the the you you basically want to maybe like",
    "start": "1068480",
    "end": "1074900"
  },
  {
    "text": "pipe some arguments through to the next step Without Really changing them in any way because you don't you because you",
    "start": "1074900",
    "end": "1081140"
  },
  {
    "text": "need them not only at the first step but also at the second step and then maybe you do that for the third step as well and so this is basically just a way of",
    "start": "1081140",
    "end": "1086960"
  },
  {
    "text": "doing that of like tearing Arguments for yeah exactly um so the first thing we do is we create",
    "start": "1086960",
    "end": "1094340"
  },
  {
    "text": "the retriever so we create the vector store in this case we have a single document which is just this very short",
    "start": "1094340",
    "end": "1100960"
  },
  {
    "text": "sentence and the open air embeddings we create a retriever from that and then",
    "start": "1100960",
    "end": "1108620"
  },
  {
    "text": "the main Temple the main prompt that we're going to be using is this prompt that expects two inputs so uh answer the",
    "start": "1108620",
    "end": "1115880"
  },
  {
    "text": "question based only on the following context and it expects us to inject something there that we're calling context and then it also expects us to",
    "start": "1115880",
    "end": "1123620"
  },
  {
    "text": "inject the actual question that the user passed in so from reading this uh I",
    "start": "1123620",
    "end": "1129200"
  },
  {
    "text": "would guess that the question is something that we're going to get when we invoke the chain but the context sounds like something that we're going",
    "start": "1129200",
    "end": "1135440"
  },
  {
    "text": "to need to produce ourselves inside this chain so let's see how we do that so and",
    "start": "1135440",
    "end": "1141320"
  },
  {
    "text": "if we skip ahead we can we can see that that is actually exactly what's happening so uh we're invoking the chain",
    "start": "1141320",
    "end": "1149539"
  },
  {
    "text": "with the question and now somehow we need to create this context so creating",
    "start": "1149539",
    "end": "1154940"
  },
  {
    "text": "this context is what the retriever is get very good for because we pass the question into the Retriever and we get",
    "start": "1154940",
    "end": "1161240"
  },
  {
    "text": "back the set of relevant documents uh so so but let's say that all we did was",
    "start": "1161240",
    "end": "1170840"
  },
  {
    "text": "this all we did is call the Retriever with a question what would happen in this case",
    "start": "1170840",
    "end": "1176840"
  },
  {
    "text": "is we'd get back the list of documents but the list of documents is not enough",
    "start": "1176840",
    "end": "1182240"
  },
  {
    "text": "to inject into the prompt template because the prompt template also needs the original question because that's uh",
    "start": "1182240",
    "end": "1188419"
  },
  {
    "text": "that's kind of what this use case of retrieval augmented generation is is you know we still need the original question",
    "start": "1188419",
    "end": "1194240"
  },
  {
    "text": "we just enrich it with some documents that we got from somewhere so that's kind of the motivation for",
    "start": "1194240",
    "end": "1201020"
  },
  {
    "text": "this runnable pass-through where we keep the question but we're so we're passing",
    "start": "1201020",
    "end": "1207200"
  },
  {
    "text": "the question through to the next step but we're enriching it with an additional key which is the the",
    "start": "1207200",
    "end": "1214400"
  },
  {
    "text": "documents we get from the Retriever and we're calling that key context then we pipe that into the prompt which",
    "start": "1214400",
    "end": "1220880"
  },
  {
    "text": "remember takes two keys context and question which match these Keys here",
    "start": "1220880",
    "end": "1226760"
  },
  {
    "text": "and then we pipe The Prompt into the model and then finally we pipe the model into the string output parser just to",
    "start": "1226760",
    "end": "1233600"
  },
  {
    "text": "get the the content of the message so if we run that",
    "start": "1233600",
    "end": "1238700"
  },
  {
    "text": "well it should actually run them all and we should get",
    "start": "1238700",
    "end": "1245500"
  },
  {
    "text": "the answer and now uh if we go and check that out here",
    "start": "1245539",
    "end": "1251240"
  },
  {
    "start": "1250000",
    "end": "1365000"
  },
  {
    "text": "we have a larger tree so you can see this is a sequence that actually has a",
    "start": "1251240",
    "end": "1258620"
  },
  {
    "text": "few steps right so we start with this thing called the runnable map so the runnable map",
    "start": "1258620",
    "end": "1265100"
  },
  {
    "text": "um kind of combines the output of the two things that are inside it so it passes the question through that's just",
    "start": "1265100",
    "end": "1272000"
  },
  {
    "text": "you know get the question produce the question does it change it at any way but it also kind of enriches",
    "start": "1272000",
    "end": "1278120"
  },
  {
    "text": "that with the document so in this case because this is a toy example we have the same document",
    "start": "1278120",
    "end": "1283820"
  },
  {
    "text": "repeated twice which isn't terribly useful but um so what yeah what we're doing here is",
    "start": "1283820",
    "end": "1291620"
  },
  {
    "text": "we're passing query into a retriever and the retriever is producing these documents as relevant information that",
    "start": "1291620",
    "end": "1297919"
  },
  {
    "text": "we then pass into the prompt template which means what we expect now is if we click here into the prompt",
    "start": "1297919",
    "end": "1303799"
  },
  {
    "text": "oops we should get now two things being",
    "start": "1303799",
    "end": "1309260"
  },
  {
    "text": "injected as input to The Prompt I think this is easy and Json uh so we have the",
    "start": "1309260",
    "end": "1314840"
  },
  {
    "text": "question itself which is the original question that was passed in and we have the the list of documents uh which is",
    "start": "1314840",
    "end": "1321140"
  },
  {
    "text": "the context and then we format that into this big prompt that's going to be passed",
    "start": "1321140",
    "end": "1326480"
  },
  {
    "text": "into chat open AI and you can see that's what we're passing the chat open AI is answer the",
    "start": "1326480",
    "end": "1332299"
  },
  {
    "text": "question this is what was written into the prompt this is the space where we had uh the the",
    "start": "1332299",
    "end": "1338540"
  },
  {
    "text": "[Music] word I'm looking for the placeholder for the context which",
    "start": "1338540",
    "end": "1343640"
  },
  {
    "text": "has now been replaced with the list of documents and this is this is where the placeholder for the question was which",
    "start": "1343640",
    "end": "1348980"
  },
  {
    "text": "has now been replaced with the question and the chat model produced the the",
    "start": "1348980",
    "end": "1354080"
  },
  {
    "text": "answer as an AI message and then the string output parser took that AA message and turned it into just the",
    "start": "1354080",
    "end": "1360140"
  },
  {
    "text": "string",
    "start": "1360140",
    "end": "1362440"
  },
  {
    "text": "foreign and now we have a small variation of that which",
    "start": "1365659",
    "end": "1372260"
  },
  {
    "text": "is now we want to have a more complicated prompt uh where we still",
    "start": "1372260",
    "end": "1378320"
  },
  {
    "text": "have the original question we still have the context which is the documents from the retriever but now we also want to",
    "start": "1378320",
    "end": "1385340"
  },
  {
    "text": "pass in the language that we want the answer in uh so you can see now we're invoking",
    "start": "1385340",
    "end": "1392659"
  },
  {
    "text": "this chain with this dictionary of two keys one key is the question where did",
    "start": "1392659",
    "end": "1398120"
  },
  {
    "text": "Harrison work the other key is the language Italian so uh this looks",
    "start": "1398120",
    "end": "1404480"
  },
  {
    "text": "similar to the above the difference is now instead of passing through we want",
    "start": "1404480",
    "end": "1409580"
  },
  {
    "text": "to out of the input we want to get the key language and we want to keep that with the name language and we want to",
    "start": "1409580",
    "end": "1416240"
  },
  {
    "text": "get the key question and keep that with the key with called question and then we want to get the context out of the",
    "start": "1416240",
    "end": "1423740"
  },
  {
    "text": "retriever passing the question into it and then we pipe that into the prompt and we pipe that into the model and we",
    "start": "1423740",
    "end": "1430700"
  },
  {
    "text": "pipe that into the string output parser and if we run that and and just highlighting something",
    "start": "1430700",
    "end": "1437120"
  },
  {
    "text": "quick here as well because this was a little confusing to me as well initially like the the reason we're using like",
    "start": "1437120",
    "end": "1443600"
  },
  {
    "text": "item getter or you'll see like some lambdas down down below as opposed to like normal pass-through",
    "start": "1443600",
    "end": "1450260"
  },
  {
    "text": "um is because of the difference in the input types so in the in the first one it was just a string and that was",
    "start": "1450260",
    "end": "1455360"
  },
  {
    "text": "because the first step was uh I mean there was only one input and so we I think we could have made it a dictionary",
    "start": "1455360",
    "end": "1461480"
  },
  {
    "text": "right you know and it would have been yeah we could and in that case we would have used the the item getter about",
    "start": "1461480",
    "end": "1467419"
  },
  {
    "text": "there as well yeah so I think like that that difference in the in the input string versus dictionary is just the the",
    "start": "1467419",
    "end": "1474200"
  },
  {
    "text": "cause for the difference between the roundable pass through and the item getter and the other the other thing that I'll say is",
    "start": "1474200",
    "end": "1479840"
  },
  {
    "text": "um having it written like this also I think makes it a lot more clear how to change the prompt and then how to change",
    "start": "1479840",
    "end": "1484940"
  },
  {
    "text": "kind of like the input variables as well um because it's you can kind of see here the whole flow it's not hidden within",
    "start": "1484940",
    "end": "1491600"
  },
  {
    "text": "kind of like uh the call method of a chain or anything like that so I think it makes it a lot more clear to see",
    "start": "1491600",
    "end": "1496880"
  },
  {
    "text": "what's going on yeah I'm going to skip this example which is",
    "start": "1496880",
    "end": "1503480"
  },
  {
    "text": "a little more a little longer and you guys can check it out in the documentation uh I think this one is interesting to",
    "start": "1503480",
    "end": "1511760"
  },
  {
    "text": "chat about quickly but maybe I need to format this differently",
    "start": "1511760",
    "end": "1518140"
  },
  {
    "text": "but",
    "start": "1518840",
    "end": "1521320"
  },
  {
    "text": "[Music] uh so in this one uh so another thing that sometimes we need to do with with",
    "start": "1525720",
    "end": "1532760"
  },
  {
    "text": "llms is to actually call an llm multiple times to achieve some goal that we have",
    "start": "1532760",
    "end": "1538220"
  },
  {
    "text": "in mind um so this would be the case for instance of the SQL database chain where we you know call the llm to turn the",
    "start": "1538220",
    "end": "1545720"
  },
  {
    "text": "natural language question into a uh in into a SQL query then we run the",
    "start": "1545720",
    "end": "1552679"
  },
  {
    "text": "SQL query in the database and then we call the llm again to uh to turn like the table of",
    "start": "1552679",
    "end": "1559460"
  },
  {
    "text": "results into some kind of summary answer that we give to the user uh so that's",
    "start": "1559460",
    "end": "1565640"
  },
  {
    "text": "actually turns out to be something you know that's a useful pattern so uh this",
    "start": "1565640",
    "end": "1571100"
  },
  {
    "text": "is what I'm gonna show you here uh or something similar uh so the idea",
    "start": "1571100",
    "end": "1577580"
  },
  {
    "text": "here is if we let me actually do the following",
    "start": "1577580",
    "end": "1584080"
  },
  {
    "text": "this out",
    "start": "1589840",
    "end": "1593140"
  },
  {
    "text": "so if we check here we have this prompt one which is just what is the city person is from",
    "start": "1597200",
    "end": "1603860"
  },
  {
    "text": "and if we this is a very simple prop right so this prompt we can CH we can",
    "start": "1603860",
    "end": "1610159"
  },
  {
    "text": "pipe it into a model and pipe it into the string output parser and we should get back some kind of response so if we",
    "start": "1610159",
    "end": "1616640"
  },
  {
    "text": "try and run that uh it expects a person um key so we need to pass it a",
    "start": "1616640",
    "end": "1623120"
  },
  {
    "text": "dictionary with a person key in this case the person is Obama so if we run that",
    "start": "1623120",
    "end": "1629240"
  },
  {
    "text": "there we go so what was the question what was the city person is from so what is the city Obama is from Barack Obama",
    "start": "1629240",
    "end": "1635659"
  },
  {
    "text": "was born in Honolulu Hawaii so we go and check it out in Lang Smith",
    "start": "1635659",
    "end": "1641840"
  },
  {
    "start": "1640000",
    "end": "2585000"
  },
  {
    "text": "it's very verbose by the way it's not just giving you the city it's giving you a horse yeah everything yeah exactly so",
    "start": "1641840",
    "end": "1648799"
  },
  {
    "text": "uh we could chain it into some other model to cut the verbosity down uh so",
    "start": "1648799",
    "end": "1654799"
  },
  {
    "text": "you can see three things were run so we have the prompt uh which took as input this single key this is always easy for",
    "start": "1654799",
    "end": "1662000"
  },
  {
    "text": "C and Jason and took as input this single key Obama person and it formatted",
    "start": "1662000",
    "end": "1668360"
  },
  {
    "text": "it that into this prompt which is what is the city Obama is from and then we ran that into a chat open AI",
    "start": "1668360",
    "end": "1676460"
  },
  {
    "text": "what is the city Obama is from and it gave us this verbose answer as an AI",
    "start": "1676460",
    "end": "1681620"
  },
  {
    "text": "message and then we extracted the content out of the AI message just into the text",
    "start": "1681620",
    "end": "1687320"
  },
  {
    "text": "so that's you know easy enough but now let's say that we actually want",
    "start": "1687320",
    "end": "1694279"
  },
  {
    "text": "to find out something more complicated which is what country is the city in",
    "start": "1694279",
    "end": "1700279"
  },
  {
    "text": "respond in a particular language um so we could use that by itself as",
    "start": "1700279",
    "end": "1705799"
  },
  {
    "text": "well so let's build that something dangerous I'm doing writing code live",
    "start": "1705799",
    "end": "1713539"
  },
  {
    "text": "I Believe In You Nino",
    "start": "1713539",
    "end": "1716799"
  },
  {
    "text": "so now let's comment that out and we call chain three the chain three",
    "start": "1723980",
    "end": "1731320"
  },
  {
    "text": "expects two things which is a city and a language so let's call this with the",
    "start": "1731320",
    "end": "1738080"
  },
  {
    "text": "city and let's try the city that we know we want so Honolulu",
    "start": "1738080",
    "end": "1744380"
  },
  {
    "text": "and the language Spanish like we have there",
    "start": "1744380",
    "end": "1751419"
  },
  {
    "text": "[Music] run that we got the answer uh so the city of",
    "start": "1752640",
    "end": "1760039"
  },
  {
    "text": "Honolulu is in the United States great so this is also a simple enough",
    "start": "1760039",
    "end": "1765080"
  },
  {
    "text": "sequence so again so I think sequence of three things like the other one we're passing",
    "start": "1765080",
    "end": "1771320"
  },
  {
    "text": "two input variables Honolulu and Spanish and we're getting back the final output which is the city of Honolulu is in the",
    "start": "1771320",
    "end": "1777860"
  },
  {
    "text": "United States so um we can see the prompt template",
    "start": "1777860",
    "end": "1782960"
  },
  {
    "text": "formatted this into this prompt and that prompt what country is the city Honolulu",
    "start": "1782960",
    "end": "1788000"
  },
  {
    "text": "and respond in Spanish was what was passed into the llm as received the answer as an AI message because this is",
    "start": "1788000",
    "end": "1794659"
  },
  {
    "text": "a chat model and then got the text output out of that but now let's say",
    "start": "1794659",
    "end": "1800720"
  },
  {
    "text": "that we actually want something more complicated which is we want to run the",
    "start": "1800720",
    "end": "1805880"
  },
  {
    "text": "whole chain at once and basically we want the answer of what country is the",
    "start": "1805880",
    "end": "1811340"
  },
  {
    "text": "city that Obama was born in uh respond in Spanish something like that but we",
    "start": "1811340",
    "end": "1817159"
  },
  {
    "text": "want to build that out of these building blocks so the way we do that now let me comment",
    "start": "1817159",
    "end": "1822679"
  },
  {
    "text": "out the other stuff that I'm messing about so the way we do that is by building you",
    "start": "1822679",
    "end": "1829460"
  },
  {
    "text": "know this chain two out of the other two things basically so first the city which",
    "start": "1829460",
    "end": "1835580"
  },
  {
    "text": "is remember city is this output is this input that we want to pass into prompt to right so the value of city is",
    "start": "1835580",
    "end": "1842899"
  },
  {
    "text": "actually going to be the value of running chain one so I could just copy this and put it",
    "start": "1842899",
    "end": "1849500"
  },
  {
    "text": "here so this basically means hey to get the value of this key City please run this",
    "start": "1849500",
    "end": "1855980"
  },
  {
    "text": "whole thing um but let's here we go and the language is just",
    "start": "1855980",
    "end": "1862220"
  },
  {
    "text": "going to be the language that we pass in and then that way we can now run prompt2",
    "start": "1862220",
    "end": "1868580"
  },
  {
    "text": "because now we have a city and we have the language we can pass that to the model and we can",
    "start": "1868580",
    "end": "1874159"
  },
  {
    "text": "run the string output parser so let's see what that looks like so now",
    "start": "1874159",
    "end": "1879520"
  },
  {
    "text": "uh the the inputs that we actually pass in is person and language",
    "start": "1879520",
    "end": "1884539"
  },
  {
    "text": "and if we run that and that prompt to model stir output",
    "start": "1884539",
    "end": "1890659"
  },
  {
    "text": "parser is actually the same as chain three right so we could have actually just replaced that all with chains yes we could yeah and I can actually do that",
    "start": "1890659",
    "end": "1898580"
  },
  {
    "text": "let me actually do that maybe I can even give these nice names",
    "start": "1898580",
    "end": "1905350"
  },
  {
    "text": "[Music]",
    "start": "1905350",
    "end": "1908500"
  },
  {
    "text": "there we go N3 would be",
    "start": "1913600",
    "end": "1918740"
  },
  {
    "text": "to get entry for City",
    "start": "1918740",
    "end": "1925898"
  },
  {
    "text": "and I watch so now oops would help so",
    "start": "1926860",
    "end": "1935840"
  },
  {
    "text": "now we see we can actually build you know more complicated thing I'll just do",
    "start": "1935840",
    "end": "1941539"
  },
  {
    "text": "simple things",
    "start": "1941539",
    "end": "1944259"
  },
  {
    "text": "so chain two which I guess is",
    "start": "1952659",
    "end": "1958399"
  },
  {
    "text": "get country four person um",
    "start": "1958399",
    "end": "1963559"
  },
  {
    "text": "so this basically is you know we get use the other get City for person chain",
    "start": "1963559",
    "end": "1969799"
  },
  {
    "text": "and we get the language as an input and then we pass this into this get country for City chain and that put together",
    "start": "1969799",
    "end": "1976940"
  },
  {
    "text": "becomes a third chain uh which is get country for person and if I'm really",
    "start": "1976940",
    "end": "1983240"
  },
  {
    "text": "lucky this may actually I'll run",
    "start": "1983240",
    "end": "1987100"
  },
  {
    "text": "yes there we go uh so now we can see what this looks like here",
    "start": "1988460",
    "end": "1995380"
  },
  {
    "text": "so you see the the total yes I type very loudly",
    "start": "1995659",
    "end": "2002460"
  },
  {
    "text": "so the total is uh not the total the final odd the",
    "start": "2002760",
    "end": "2008559"
  },
  {
    "text": "the the the inputs at the beginning were the person and the language and the final output was",
    "start": "2008559",
    "end": "2015480"
  },
  {
    "text": "uh the sentence in Spanish that describes what country the city that",
    "start": "2015480",
    "end": "2021880"
  },
  {
    "text": "Obama was in uh yeah so and we can see",
    "start": "2021880",
    "end": "2027100"
  },
  {
    "text": "that now looks like a more complicated chain because this is actually those two chains composed so this runnable",
    "start": "2027100",
    "end": "2033460"
  },
  {
    "text": "sequence here um these four things this is actually the first chain that we ran a few",
    "start": "2033460",
    "end": "2040480"
  },
  {
    "text": "minutes ago and then this this other Ultra sequence is the",
    "start": "2040480",
    "end": "2046480"
  },
  {
    "text": "other one which uh which combines the two so we",
    "start": "2046480",
    "end": "2051580"
  },
  {
    "text": "can see runnable map gets as input the two um the two input variables and the",
    "start": "2051580",
    "end": "2058480"
  },
  {
    "text": "output of runnable map is basically converting an a dictionary of person and",
    "start": "2058480",
    "end": "2064118"
  },
  {
    "text": "language into a dictionary of city and language so person Obama language Spanish becomes this dictionary of City",
    "start": "2064119",
    "end": "2070658"
  },
  {
    "text": "something something Honolulu and language Spanish and then we just pass that into",
    "start": "2070659",
    "end": "2078580"
  },
  {
    "text": "The Prompt template which formats what country is the city Barack Obama the",
    "start": "2078580",
    "end": "2083800"
  },
  {
    "text": "good thing llms are smart because I wouldn't be able to figure this one out uh but this is caused by just the first",
    "start": "2083800",
    "end": "2091000"
  },
  {
    "text": "llm responding very robustly and so I think if you know if we were doing this for real would probably use maybe like",
    "start": "2091000",
    "end": "2097780"
  },
  {
    "text": "open AI functions or something to get a more like structured output out yeah exactly uh in this case we were",
    "start": "2097780",
    "end": "2105700"
  },
  {
    "text": "lucky and it still figured things out but um and then we just call openai with",
    "start": "2105700",
    "end": "2111339"
  },
  {
    "text": "this uh yeah with this prompt and we get back the answer we extract the text and",
    "start": "2111339",
    "end": "2116440"
  },
  {
    "text": "there we go that's like uh composed a chain that was composed out of two",
    "start": "2116440",
    "end": "2121540"
  },
  {
    "text": "smaller chains and that is yeah that's kind of a big reason why we think this",
    "start": "2121540",
    "end": "2128079"
  },
  {
    "text": "may be useful is to just you know you can build these building blocks and then just compose them into larger things",
    "start": "2128079",
    "end": "2135539"
  },
  {
    "text": "um and just a reminder that yeah you know I think maybe we'll do one if if that more",
    "start": "2137320",
    "end": "2144280"
  },
  {
    "text": "chains but if people have questions please put them in the chat um and we'll try to answer them",
    "start": "2144280",
    "end": "2150460"
  },
  {
    "text": "um I I see one in there but we're happy to answer others as well and by the time I mean the box with the question mark",
    "start": "2150460",
    "end": "2157660"
  },
  {
    "text": "uh so the the only other one that I want to show just to show you how powerful it is that everything now implements the",
    "start": "2157660",
    "end": "2162880"
  },
  {
    "text": "same interface is we can actually now just use any of the I don't know 100",
    "start": "2162880",
    "end": "2168579"
  },
  {
    "text": "tools we have in line chain or whatever it is and use them in these sequences as well so here I am importing the",
    "start": "2168579",
    "end": "2175599"
  },
  {
    "text": "DuckDuckGo search run tool um",
    "start": "2175599",
    "end": "2180660"
  },
  {
    "text": "and what I what we have is uh",
    "start": "2180660",
    "end": "2187000"
  },
  {
    "text": "a chain that basically uses an llm to convert some kind of freeform input into",
    "start": "2187000",
    "end": "2192760"
  },
  {
    "text": "something that is more resembles more a search query so turn the following user",
    "start": "2192760",
    "end": "2199180"
  },
  {
    "text": "input into a search query for a search engine and then we have the input as a placeholder and we make a prompt",
    "start": "2199180",
    "end": "2206200"
  },
  {
    "text": "template out of that we pass it into the model and we use the string output parser to get the text and then we just",
    "start": "2206200",
    "end": "2212260"
  },
  {
    "text": "pipe that into the Search tool and so if I run out I get the chain and I invoke",
    "start": "2212260",
    "end": "2221099"
  },
  {
    "text": "and that's it that uh you know hard to read string is the output of the",
    "start": "2221560",
    "end": "2227079"
  },
  {
    "text": "DuckDuckGo search too uh so this is really powerful and you can use any of the tools any of the",
    "start": "2227079",
    "end": "2233200"
  },
  {
    "text": "retrievers that we have in the library in these uh in these chain in these pipelines and finally let me just show",
    "start": "2233200",
    "end": "2240460"
  },
  {
    "text": "you that also that shows up here so you can see we have a prompt template open",
    "start": "2240460",
    "end": "2245980"
  },
  {
    "text": "AI so you can see we turned I'd like to figure out what games are on tonight into just games tonight which is",
    "start": "2245980",
    "end": "2252460"
  },
  {
    "text": "something that a search engine might be best happier with then be extracted just",
    "start": "2252460",
    "end": "2258579"
  },
  {
    "text": "that as a string and then we called uh DuckDuckGo without simple string and we got back this",
    "start": "2258579",
    "end": "2265119"
  },
  {
    "text": "unreadable thing but you know that's what that girl produces um and I think important importantly",
    "start": "2265119",
    "end": "2271119"
  },
  {
    "text": "there like um it everything will depend on the input type right so DuckDuckGo as a tool",
    "start": "2271119",
    "end": "2277240"
  },
  {
    "text": "accepts just a string there are also tools that accept structured things and for that you'd want it to be a",
    "start": "2277240",
    "end": "2283060"
  },
  {
    "text": "dictionary retrievers accept a string so for that it's fine prompt templates except the dictionary because they're",
    "start": "2283060",
    "end": "2289540"
  },
  {
    "text": "they're keys so there's a little bit of um making sure that you have the right the right inputs lining up yeah exactly",
    "start": "2289540",
    "end": "2296800"
  },
  {
    "text": "and so for instance if you have a tool that accepts a dictionary uh then what would what is probably a",
    "start": "2296800",
    "end": "2303160"
  },
  {
    "text": "nice thing for you to do is to use the open AI functions to build the dictionary of inputs to that tool and",
    "start": "2303160",
    "end": "2309579"
  },
  {
    "text": "then pipe that those arguments that open AI produced directly into the Tool uh so",
    "start": "2309579",
    "end": "2314920"
  },
  {
    "text": "you get the the arguments and you just pipe them into the tool and it executes the tool",
    "start": "2314920",
    "end": "2320640"
  },
  {
    "text": "awesome maybe let's answer a few questions in the chat um and and if there are um yeah people",
    "start": "2321640",
    "end": "2328480"
  },
  {
    "text": "should feel free to add more and and we'll get to them but going through the first one",
    "start": "2328480",
    "end": "2334180"
  },
  {
    "text": "um why another language how is it different than similar to python JavaScript and other current languages I",
    "start": "2334180",
    "end": "2340359"
  },
  {
    "text": "mean this is implemented in both Python and JavaScript it's it's not another language we kind of took inspiration",
    "start": "2340359",
    "end": "2347680"
  },
  {
    "text": "from the SQL Alchemy kind of like they have a SQL Alchemy expression language which is just a way to write SQL Alchemy",
    "start": "2347680",
    "end": "2354040"
  },
  {
    "text": "in a nice composable chainable way um so not not a different programming",
    "start": "2354040",
    "end": "2359320"
  },
  {
    "text": "language is just a just a different syntax anything you'd add to that uh yeah exactly so and we even try uh you",
    "start": "2359320",
    "end": "2366640"
  },
  {
    "text": "know we tried as much as possible to use all the things that are built into python so for instance this item getter",
    "start": "2366640",
    "end": "2371800"
  },
  {
    "text": "that you have here this is actually a built-in python function um so the idea is you know we're just",
    "start": "2371800",
    "end": "2377859"
  },
  {
    "text": "using all the tools that are existing python to make it easier for you to build things in python or in JavaScript",
    "start": "2377859",
    "end": "2383680"
  },
  {
    "text": "whatever it is you want to use foreign",
    "start": "2383680",
    "end": "2388619"
  },
  {
    "text": "I see uh I see two questions that are in the same vein and is basically uh is",
    "start": "2388920",
    "end": "2395260"
  },
  {
    "text": "this similar to lmql um which is I forget what it stands for but it's another kind of like framework",
    "start": "2395260",
    "end": "2401079"
  },
  {
    "text": "slash language mostly around I believe prompting strategies",
    "start": "2401079",
    "end": "2406300"
  },
  {
    "text": "um it's not super similar I would say lmql is more similar to guidance and that is extracting kind of like",
    "start": "2406300",
    "end": "2413460"
  },
  {
    "text": "I mean maybe it's slightly different I don't think it's super simpler I I don't sorry",
    "start": "2413460",
    "end": "2419619"
  },
  {
    "text": "I don't think it's super similar um I think this uh I actually need to brush up on lmql in the direction that",
    "start": "2419619",
    "end": "2427000"
  },
  {
    "text": "they've gone um",
    "start": "2427000",
    "end": "2431099"
  },
  {
    "text": "um I think uh sorry so just the only thing I was going to say is lmql if I'm not mistaken",
    "start": "2436180",
    "end": "2443460"
  },
  {
    "text": "is something that requires support from the from the language model that you're",
    "start": "2443460",
    "end": "2449260"
  },
  {
    "text": "trying to run it against um whereas you know uh our expression",
    "start": "2449260",
    "end": "2455680"
  },
  {
    "text": "language here is just about you know everything that exists in Bank chain today uh can you know be used and",
    "start": "2455680",
    "end": "2462339"
  },
  {
    "text": "composed in this way foreign yeah I think yeah I think lmql is much",
    "start": "2462339",
    "end": "2467680"
  },
  {
    "text": "more I think they have what like they have better support for more low-level things like beam search over various",
    "start": "2467680",
    "end": "2473440"
  },
  {
    "text": "things and decoding um and it's more around interacting with like the raw language model this is much",
    "start": "2473440",
    "end": "2478540"
  },
  {
    "text": "more about composing the language model output with other things and making it easy to do that and change things",
    "start": "2478540",
    "end": "2484240"
  },
  {
    "text": "um so I'd say almql and guidance are are different um another another question with this",
    "start": "2484240",
    "end": "2490359"
  },
  {
    "text": "introduction the normal syntax code which exists will that still be supported um yeah do you want to chat a little bit",
    "start": "2490359",
    "end": "2496060"
  },
  {
    "text": "about how we're thinking about this which by the way is early on but you know we're we're figuring it out",
    "start": "2496060",
    "end": "2503020"
  },
  {
    "text": "uh yeah so uh we we still want to support uh the existing way of you know",
    "start": "2503020",
    "end": "2508660"
  },
  {
    "text": "using uh all the built-in chains that we have uh using uh all the building blocks",
    "start": "2508660",
    "end": "2514780"
  },
  {
    "text": "by themselves so everything that was there before we still want to support uh I'd say one thing that we might do over",
    "start": "2514780",
    "end": "2521619"
  },
  {
    "text": "time is now that we have the single interface we may at some point I don't",
    "start": "2521619",
    "end": "2526839"
  },
  {
    "text": "know a few months from now whatever it may be start to deprecate some of the previous methods",
    "start": "2526839",
    "end": "2533140"
  },
  {
    "text": "um just to standardize on the new kind of invoke stream batch methods uh but uh",
    "start": "2533140",
    "end": "2539260"
  },
  {
    "text": "yeah this expression language is you know completely optional it's just meant to make it easier for you to build custom things but if you don't want to",
    "start": "2539260",
    "end": "2545320"
  },
  {
    "text": "use it all the stuff you've been used to is going to stay around yeah I think we definitely see benefits",
    "start": "2545320",
    "end": "2552040"
  },
  {
    "text": "to this namely kind of like making it like the standard interface and and kind",
    "start": "2552040",
    "end": "2557079"
  },
  {
    "text": "of like making it easier to swap in pieces and one big piece of feedback that we've gotten that we're working on is making it easier to customize blank",
    "start": "2557079",
    "end": "2564160"
  },
  {
    "text": "chain and so I think this is a step in that direction and so I think we'll probably start to move more and more of our examples towards this but we'll",
    "start": "2564160",
    "end": "2570579"
  },
  {
    "text": "still absolutely keep around the the existing um uh a few questions about basically",
    "start": "2570579",
    "end": "2578020"
  },
  {
    "text": "can I assume that agents can also be executed with this syntax",
    "start": "2578020",
    "end": "2583240"
  },
  {
    "text": "uh so agents uh we don't support them in",
    "start": "2583240",
    "end": "2588780"
  },
  {
    "start": "2585000",
    "end": "2690000"
  },
  {
    "text": "so you can execute an agent with the with the new interface because an agent executor is just a very complicated",
    "start": "2590440",
    "end": "2597700"
  },
  {
    "text": "chain so it also implements the invoke batch methods uh and the async ones as",
    "start": "2597700",
    "end": "2603280"
  },
  {
    "text": "well uh what you can do is you cannot build a custom agent using the the pipeline",
    "start": "2603280",
    "end": "2609880"
  },
  {
    "text": "Syntax for now because that comes with some additional complexities that were you know we want to take small steps and",
    "start": "2609880",
    "end": "2616960"
  },
  {
    "text": "see what people think of this and then introduce new capabilities maybe we're gonna you know make that expand out",
    "start": "2616960",
    "end": "2623920"
  },
  {
    "text": "capability in the future and maybe you'll be able to use this to build custom agents but one thing you can do",
    "start": "2623920",
    "end": "2629140"
  },
  {
    "text": "for now is definitely run all the existing agents that we already have with the new interface",
    "start": "2629140",
    "end": "2636220"
  },
  {
    "text": "and also use them in in sequential and yeah exactly you can yeah use like uh a",
    "start": "2636220",
    "end": "2643720"
  },
  {
    "text": "retriever to build some input that you then pipe into an agent that's something you could definitely do which is really powerful because I think",
    "start": "2643720",
    "end": "2649960"
  },
  {
    "text": "a lot of the uh you know the a lot of what we see being done to get reliable",
    "start": "2649960",
    "end": "2655540"
  },
  {
    "text": "things is basically using agents in a more modular and small way so being able to pipe them in even if you can't create",
    "start": "2655540",
    "end": "2661060"
  },
  {
    "text": "them with this language yet um is is still beneficial is is it ready",
    "start": "2661060",
    "end": "2666579"
  },
  {
    "text": "to use or still a work in progress yeah it's definitely ready to use uh we have uh I think",
    "start": "2666579",
    "end": "2673119"
  },
  {
    "text": "pretty much close to complete code coverage for all the code related to",
    "start": "2673119",
    "end": "2678640"
  },
  {
    "text": "this so yeah this is very much tested obviously it's a new feature you know any feedback is very welcome but this is",
    "start": "2678640",
    "end": "2685000"
  },
  {
    "text": "completely ready to use um is it possible to turn on verbose mode",
    "start": "2685000",
    "end": "2691720"
  },
  {
    "start": "2690000",
    "end": "2760000"
  },
  {
    "text": "for specific chain like in classic chains we have reverse equals true using new syntax not only debugging using",
    "start": "2691720",
    "end": "2698140"
  },
  {
    "text": "langtrain.debug uh so I um if I'm not mistaken the uh",
    "start": "2698140",
    "end": "2706660"
  },
  {
    "text": "what we what you get out of doing Lang chain dot Rebels equals true is something that is designed specifically",
    "start": "2706660",
    "end": "2713680"
  },
  {
    "text": "for two things only one is agents and the other one is the llm chain uh",
    "start": "2713680",
    "end": "2720579"
  },
  {
    "text": "uh we don't currently have um output design specifically for this in",
    "start": "2720579",
    "end": "2727480"
  },
  {
    "text": "the verbose mode so for now I'd suggest just doing length chain dot debug equals true uh but if if anyone has you know",
    "start": "2727480",
    "end": "2734619"
  },
  {
    "text": "specific output that they want to see uh just let us know and uh yeah we'll uh",
    "start": "2734619",
    "end": "2740800"
  },
  {
    "text": "that's something we can yeah I think verbose was a lot of the nice stuff of verbose was",
    "start": "2740800",
    "end": "2748480"
  },
  {
    "text": "done within specific chains and the whole point of this is that it's a generic thing so it's a bit of an anti-pattern a bit tough to do it there",
    "start": "2748480",
    "end": "2757359"
  },
  {
    "text": "um is the expression language going to be maintained by an offshoot team or by the core link chain team",
    "start": "2757359",
    "end": "2764319"
  },
  {
    "start": "2760000",
    "end": "2835000"
  },
  {
    "text": "uh no it's uh the idea is that yeah we the core like chain team will maintain the expression language for sure and we",
    "start": "2764319",
    "end": "2771040"
  },
  {
    "text": "do plan on you know building new examples on top of the expression language and uh maybe even you know",
    "start": "2771040",
    "end": "2777339"
  },
  {
    "text": "forwarding some of the existing changes to the expression language so yeah definitely something we plan to maintain",
    "start": "2777339",
    "end": "2783520"
  },
  {
    "text": "a question about streaming how is the underlying HTTP stream a handled when it's when during an",
    "start": "2783520",
    "end": "2789940"
  },
  {
    "text": "interruption like if you interrupt a stream does would that prevent further token usage on their end",
    "start": "2789940",
    "end": "2796480"
  },
  {
    "text": "um there yes the answer is yeah if and by the way interrupting a stream in in",
    "start": "2796480",
    "end": "2801640"
  },
  {
    "text": "this new model is super easy because you're consuming you're doing like four token in model.stream and then all you",
    "start": "2801640",
    "end": "2809079"
  },
  {
    "text": "need to do to interrupt that stream is break inside that Loop and if you do break uh that gets you know propagated",
    "start": "2809079",
    "end": "2815980"
  },
  {
    "text": "all the way back to the open AI uh uh the open Ai call and we break that call",
    "start": "2815980",
    "end": "2823180"
  },
  {
    "text": "with what type open AI does with that information that is up to them but you know we send them the information that it's broken that it's interrupted",
    "start": "2823180",
    "end": "2831579"
  },
  {
    "text": "did you use the new syntax for the link chain teacher application yes we did so I'll share that link in the chat we",
    "start": "2831579",
    "end": "2838359"
  },
  {
    "start": "2835000",
    "end": "2950000"
  },
  {
    "text": "created a little teacher bot to help walk you through getting started with this and we will open source that",
    "start": "2838359",
    "end": "2844799"
  },
  {
    "text": "um does this introduce any performance gains over previous methods or is it just for a better Dev experience",
    "start": "2845500",
    "end": "2852160"
  },
  {
    "text": "so that's an interesting question so um there are performance gains in specific",
    "start": "2852160",
    "end": "2858160"
  },
  {
    "text": "methods so for instance in the new stream methods what we do is we actually",
    "start": "2858160",
    "end": "2863200"
  },
  {
    "text": "um uh basically use the the iterator that we get back from uh from the",
    "start": "2863200",
    "end": "2868599"
  },
  {
    "text": "underlying providers so the iterator that we get back from the open aisdk or the anthropic SDK that's what we pass",
    "start": "2868599",
    "end": "2874119"
  },
  {
    "text": "directly to you and that's more performant than just using consuming it",
    "start": "2874119",
    "end": "2879400"
  },
  {
    "text": "via callbacks um so that's a performance gain if you're doing streaming and another",
    "start": "2879400",
    "end": "2885540"
  },
  {
    "text": "performance Improvement is to do with the batch method so when you",
    "start": "2885540",
    "end": "2893800"
  },
  {
    "text": "use the batch method one if the underlying provider supports batch calls we actually do that so you know we don't",
    "start": "2893800",
    "end": "2900940"
  },
  {
    "text": "do 10 open AI calls we do a single one even if the model the open AI model is",
    "start": "2900940",
    "end": "2906339"
  },
  {
    "text": "in the middle of a sequence of other things and second whenever there's things we",
    "start": "2906339",
    "end": "2912339"
  },
  {
    "text": "can do in parallel we do them in parallel um using your threadpool executor so if you",
    "start": "2912339",
    "end": "2919180"
  },
  {
    "text": "know there were some examples that I showed where we were getting two keys out of the dictionary",
    "start": "2919180",
    "end": "2925180"
  },
  {
    "text": "um because those two keys don't depend on each other we we actually get those two in parallel so the overall sequence runs",
    "start": "2925180",
    "end": "2931420"
  },
  {
    "text": "faster good question on that one do we need to",
    "start": "2931420",
    "end": "2936940"
  },
  {
    "text": "update the link chain package module to use this new interface yes it's in the most recent one",
    "start": "2936940",
    "end": "2942460"
  },
  {
    "text": "um I forget which number um but to to 249 or something like that",
    "start": "2942460",
    "end": "2950440"
  },
  {
    "start": "2950000",
    "end": "2975000"
  },
  {
    "text": "anyway we can see the metrics which we see in langsmith locally in our code for this syntax I believe you should see all",
    "start": "2950440",
    "end": "2957220"
  },
  {
    "text": "the same stuff with debug equals true if you set it there exactly so if you do Lang train.debug the output that you get",
    "start": "2957220",
    "end": "2964060"
  },
  {
    "text": "is actually very similar to the output you'd get in langsmith just uh you know a little uglier because it's in your",
    "start": "2964060",
    "end": "2970420"
  },
  {
    "text": "terminal this is this is a good question do you foresee that there will be a need for",
    "start": "2970420",
    "end": "2977079"
  },
  {
    "start": "2975000",
    "end": "3080000"
  },
  {
    "text": "some of the old calls in some use cases or do you think the Expressions will work for all use cases",
    "start": "2977079",
    "end": "2982800"
  },
  {
    "text": "uh that's an interesting question so I think we have part of the answer in what I said earlier where you can't Implement",
    "start": "2982800",
    "end": "2989200"
  },
  {
    "text": "a custom agent today in the new expression language right so that kind of implies that at least as it stands",
    "start": "2989200",
    "end": "2995319"
  },
  {
    "text": "today there are some things that require you writing custom python code [Music]",
    "start": "2995319",
    "end": "3000660"
  },
  {
    "text": "um that might be true of other things um",
    "start": "3000660",
    "end": "3005420"
  },
  {
    "text": "so yeah I think uh the most likely scenarios where that's going to be true is where you say some kind of for Loop",
    "start": "3005819",
    "end": "3013140"
  },
  {
    "text": "or a while loop or some other kind of kind of repeated application where because the sequence as we as we have it",
    "start": "3013140",
    "end": "3020520"
  },
  {
    "text": "today doesn't mean that you know it won't evolve in the future but as we have it today the sequence is you just",
    "start": "3020520",
    "end": "3026040"
  },
  {
    "text": "run it through it once um so yeah not sure if that answers the",
    "start": "3026040",
    "end": "3031440"
  },
  {
    "text": "question yeah no I think I think we've um look at the cookbook we've put together",
    "start": "3031440",
    "end": "3037200"
  },
  {
    "text": "has a lot of the most popular chains and and um there's there's also a Discord",
    "start": "3037200",
    "end": "3043140"
  },
  {
    "text": "Channel specifically for this and we're like we we want to try to implement as many change as possible using this so if",
    "start": "3043140",
    "end": "3049800"
  },
  {
    "text": "you have kind of like questions about particular chains please let us know we will do the work we will we will",
    "start": "3049800",
    "end": "3055020"
  },
  {
    "text": "Implement them this is something that we we've been doing internally for the past like week or so and have already added a",
    "start": "3055020",
    "end": "3060780"
  },
  {
    "text": "few new kind of like um runnable things specifically to patch up some blind spots and stuff it's",
    "start": "3060780",
    "end": "3066480"
  },
  {
    "text": "something that constantly involving will do it um I mean yeah in that vein as well",
    "start": "3066480",
    "end": "3073260"
  },
  {
    "text": "what's the rough roadmap for this what do you see being added to this in the in the short and medium term",
    "start": "3073260",
    "end": "3080099"
  },
  {
    "start": "3080000",
    "end": "3145000"
  },
  {
    "text": "so I think um if we think about what we have in the library today",
    "start": "3080099",
    "end": "3085140"
  },
  {
    "text": "um I'd say the two biggest uh gaps in the expression language are one is the use",
    "start": "3085140",
    "end": "3091859"
  },
  {
    "text": "of memory uh which right now if you look at the cookbook we actually have an example for that but you'll see that",
    "start": "3091859",
    "end": "3097020"
  },
  {
    "text": "it's a little verbose um because memory is um I think we just need to think a",
    "start": "3097020",
    "end": "3102540"
  },
  {
    "text": "little bit about our memory plays into this um and the other one is is Agents so I'd",
    "start": "3102540",
    "end": "3108720"
  },
  {
    "text": "say two things on the roadmap here one is you know memory memory and agents another thing that might be those are",
    "start": "3108720",
    "end": "3115740"
  },
  {
    "text": "the two most exciting things you know those two things yeah exactly I should have started there",
    "start": "3115740",
    "end": "3123920"
  },
  {
    "text": "another thing that may be useful in the future that we're thinking about is",
    "start": "3123920",
    "end": "3129000"
  },
  {
    "text": "as you build these sequences I think it's quite natural that you're going to start to build longer kind of long long",
    "start": "3129000",
    "end": "3135240"
  },
  {
    "text": "running sequences uh and uh so that's something that we may think about in the",
    "start": "3135240",
    "end": "3140579"
  },
  {
    "text": "future like how how can we make that easier awesome and then and then the last",
    "start": "3140579",
    "end": "3145920"
  },
  {
    "start": "3145000",
    "end": "3240000"
  },
  {
    "text": "question is a very easy one and so I can take this is there a difficult channel for link chain users and is there an",
    "start": "3145920",
    "end": "3151440"
  },
  {
    "text": "invite yeah link yes there is I'll post that in the chat afterwards we actually announced this um on either Friday or",
    "start": "3151440",
    "end": "3158339"
  },
  {
    "text": "Saturday so so people on the Discord Channel got a few days of of heads up and actually a bunch of them helped test",
    "start": "3158339",
    "end": "3164940"
  },
  {
    "text": "it out and there were some YouTube videos that were created so I'd highly recommend joining and for all the stuff around memory and agents that Nina",
    "start": "3164940",
    "end": "3171119"
  },
  {
    "text": "talked about we'll probably release that in the Discord Channel a few a few days ahead of time as well um",
    "start": "3171119",
    "end": "3176760"
  },
  {
    "text": "I think that's basically it for us today I want to thank everyone for joining I I also want to thank everyone for the",
    "start": "3176760",
    "end": "3183000"
  },
  {
    "text": "awesome questions that we got in like there was a lot of them that were all really good and I think they helped us think through some things live as well",
    "start": "3183000",
    "end": "3189660"
  },
  {
    "text": "I'd be curious for people's feedback on this whether they liked like kind of like just the lane chain team and like",
    "start": "3189660",
    "end": "3195059"
  },
  {
    "text": "20 minutes of just like answering random questions or whether you guys want to see more kind of like bring in",
    "start": "3195059",
    "end": "3200400"
  },
  {
    "text": "interesting people who are building their own things into deep Dives on that so if you guys have kind of like thoughts on that please let us know in",
    "start": "3200400",
    "end": "3206579"
  },
  {
    "text": "the Discord um on Twitter or in the chat here afterwards um or in the comments on YouTube once",
    "start": "3206579",
    "end": "3212819"
  },
  {
    "text": "this gets posted on YouTube which it will be uh sometime before the end of the week um with that thank you guys for joining",
    "start": "3212819",
    "end": "3219599"
  },
  {
    "text": "thank you Nuno for being here I'm excited to see what you get to building with us yeah excited to see it",
    "start": "3219599",
    "end": "3227900"
  },
  {
    "text": "foreign",
    "start": "3228300",
    "end": "3230839"
  }
]