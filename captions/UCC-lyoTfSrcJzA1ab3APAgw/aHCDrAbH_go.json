[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "hi this is Lance from Lang chain and thropic recently put up this blog post called building effective agents where",
    "start": "40",
    "end": "5440"
  },
  {
    "text": "they Define what an agent is and also explain what a workflow is so I'm going to build every workflow and agent that",
    "start": "5440",
    "end": "12320"
  },
  {
    "text": "they talk about in this blog post from scratch and explain what they all are how to build them and why each one can",
    "start": "12320",
    "end": "18199"
  },
  {
    "text": "be very useful so here's a simple way to think about their definition of a workflow versus an agent think about a",
    "start": "18199",
    "end": "25119"
  },
  {
    "text": "workflow as some kind of scaffolding of predefined code paths around LM calls",
    "start": "25119",
    "end": "31119"
  },
  {
    "text": "now this has been popular for years and it makes a lot of sense in many cases to take llm calls and embed them in some",
    "start": "31119",
    "end": "37520"
  },
  {
    "text": "fixed set of code paths now sometimes you can actually have the LM decide what",
    "start": "37520",
    "end": "43320"
  },
  {
    "text": "po what paths to take in a workflow and that's kind of this middle category that I drew here and we'll talk about all",
    "start": "43320",
    "end": "49960"
  },
  {
    "text": "these in detail but that's kind of the intuition workflows are kind of like some scaffolding of predefined code",
    "start": "49960",
    "end": "55800"
  },
  {
    "text": "paths and you can embed LM calls within it now agents remove the scaffolding so",
    "start": "55800",
    "end": "62280"
  },
  {
    "start": "60000",
    "end": "120000"
  },
  {
    "text": "you're basically let an llm direct its own actions now in this particular case",
    "start": "62280",
    "end": "67320"
  },
  {
    "text": "actions are typically tool calls and an llm will receive the feedback from those",
    "start": "67320",
    "end": "72439"
  },
  {
    "text": "tool calls and decide what to do next I do want to call out that you can certainly have llms that perform tool",
    "start": "72439",
    "end": "78520"
  },
  {
    "text": "calls within workflows but the difference is that workflows have some scaffolding around",
    "start": "78520",
    "end": "84520"
  },
  {
    "text": "them whereas an agent is unbounded so an agent just directly re receives the",
    "start": "84520",
    "end": "90439"
  },
  {
    "text": "output of environmental feedback from Tool calls decides what to do next",
    "start": "90439",
    "end": "95479"
  },
  {
    "text": "without the kind of reasoning scaffolding around it so that's really the differentiation now just before I get started here it's important to call",
    "start": "95479",
    "end": "101600"
  },
  {
    "text": "out why Frameworks okay so implementing these patterns doesn't require framework",
    "start": "101600",
    "end": "107040"
  },
  {
    "text": "right sometimes this can be done in a few lines of code I completely understand that so you might say okay why are you talking about this why are",
    "start": "107040",
    "end": "112880"
  },
  {
    "text": "you trying to promote Frameworks here's kind of my take on it Lang graph really means aims to minimize the overhead of",
    "start": "112880",
    "end": "119079"
  },
  {
    "text": "implemen these patterns okay it doesn't abstract prompts it doesn't abstract",
    "start": "119079",
    "end": "125119"
  },
  {
    "start": "120000",
    "end": "240000"
  },
  {
    "text": "architecture what's really happening with langra is it's supporting infrastructure underneath any workflow",
    "start": "125119",
    "end": "131520"
  },
  {
    "text": "or agent that's the key point and really it's three things one is persistence",
    "start": "131520",
    "end": "136599"
  },
  {
    "text": "this gives you memory this gives you human the loops the ability to pause for example while an agent is processing and",
    "start": "136599",
    "end": "142879"
  },
  {
    "text": "approve a tool call pause and riew what the agent is doing this is extremely",
    "start": "142879",
    "end": "148200"
  },
  {
    "text": "useful in many cases streaming so of course LM apis stream tokens we all know",
    "start": "148200",
    "end": "154239"
  },
  {
    "text": "that but when you build these worklow rents sometimes you want to stream for example what certain steps output you",
    "start": "154239",
    "end": "161120"
  },
  {
    "text": "might want to stream the outputs of tool calls you often want more flexibility over streaming than just what the llm",
    "start": "161120",
    "end": "167200"
  },
  {
    "text": "calls produce and so Lang graph give you a lot of control over what you're outputting from your workflow or agent",
    "start": "167200",
    "end": "173159"
  },
  {
    "text": "this is very useful when actually building these and putting them in production and the third is deployment",
    "start": "173159",
    "end": "178680"
  },
  {
    "text": "so testing debugging deploying that is a big benefit of using Lang graph it is",
    "start": "178680",
    "end": "184360"
  },
  {
    "text": "extremely easy to go from any workflow or h&u Implement to a deployment lra",
    "start": "184360",
    "end": "190080"
  },
  {
    "text": "doesn't abstract prompts and it does not abstract architecture it's really giving",
    "start": "190080",
    "end": "195120"
  },
  {
    "text": "you low-level infrastructure that sits underneath any of these workflows or agents so now we've laid the foundations",
    "start": "195120",
    "end": "200480"
  },
  {
    "text": "let's talk through the various patterns laid out in the blog post first starting with the augmented L LMS can be",
    "start": "200480",
    "end": "206080"
  },
  {
    "text": "augmented with many different things one is for example M we talked a little bit about that that's one of the things that",
    "start": "206080",
    "end": "212159"
  },
  {
    "text": "a framework such as landcraft can provide also LMS can interact with tools and this is the foundation for building",
    "start": "212159",
    "end": "217799"
  },
  {
    "text": "many workflows and agents so let's build this from scratch here's my notebook I've just pip installed a few packages",
    "start": "217799",
    "end": "223720"
  },
  {
    "text": "set my in API key create my llm now let's show the augmentation for structured output I can take a schema in",
    "start": "223720",
    "end": "231080"
  },
  {
    "text": "this case a pantic model I can bind it to my llm in this particular case I'm",
    "start": "231080",
    "end": "236720"
  },
  {
    "text": "using Lang chain and I'm going to use the wi structured output method Lang chain to do this but again Lang graph",
    "start": "236720",
    "end": "242480"
  },
  {
    "start": "240000",
    "end": "300000"
  },
  {
    "text": "does not require you to use Lang chain you can use the raw model apis it's completely fine I run that and the",
    "start": "242480",
    "end": "248799"
  },
  {
    "text": "output adheres to my schema which I passed here so that's great now let's do the same with tool calling I'm going to",
    "start": "248799",
    "end": "255200"
  },
  {
    "text": "take a tool I'm going basically Define this function multiply as a tool that the LM has access to I can use the bind",
    "start": "255200",
    "end": "262639"
  },
  {
    "text": "tools method in line chain to bind it to my llm now I have an llm with tools I",
    "start": "262639",
    "end": "268600"
  },
  {
    "text": "invoke it with an input that's likely to elicit the tool call and I get the",
    "start": "268600",
    "end": "274960"
  },
  {
    "text": "output great you can see a tool call is produced it takes this input and creates",
    "start": "274960",
    "end": "281199"
  },
  {
    "text": "the arguments necessary to actually run this function that's it remember when",
    "start": "281199",
    "end": "286320"
  },
  {
    "text": "llms are creating tool calls they're really just giving you payloads to actually run that tool you could then",
    "start": "286320",
    "end": "291479"
  },
  {
    "text": "run the tool pass the output of the tool back to llm and really doing that Loop gives you an agent like we saw before",
    "start": "291479",
    "end": "297960"
  },
  {
    "text": "we've talked about augmented llm as a building block now let's talk about some of the workflow patterns covered in the",
    "start": "297960",
    "end": "303080"
  },
  {
    "start": "300000",
    "end": "540000"
  },
  {
    "text": "blog post starting with prompt chaining here's the intuition each LM call process the output of the previous one",
    "start": "303080",
    "end": "310360"
  },
  {
    "text": "when do you want to do this when you ATT tested you can decompose into a few different llm calls so in this little",
    "start": "310360",
    "end": "316880"
  },
  {
    "text": "example they show there's an input call one you can have some gating on the output of call one output goes to call",
    "start": "316880",
    "end": "324000"
  },
  {
    "text": "two that output goes to call three so let's basically build this from scratch and let's do an example we build a chain",
    "start": "324000",
    "end": "330960"
  },
  {
    "text": "that takes a topic from a user the LM makes a joke we check to make sure the",
    "start": "330960",
    "end": "336280"
  },
  {
    "text": "joke has a punchline and we improve it twice with two subsequent llm call now when I'm working L graph all I need to",
    "start": "336280",
    "end": "342919"
  },
  {
    "text": "do is basically Define a container for everything that I want to modify over my",
    "start": "342919",
    "end": "347960"
  },
  {
    "text": "workflow so in this particular case I'm just going to create a dict and it's",
    "start": "347960",
    "end": "353199"
  },
  {
    "text": "going to contain topic that's we'll get from the user the joke will be the output of that first call improved joke",
    "start": "353199",
    "end": "358880"
  },
  {
    "text": "will the output of the second call final joke would the output of the final call now when laying out workflows I actually",
    "start": "358880",
    "end": "364199"
  },
  {
    "text": "like to draw them out anyway so this is kind of cool that the blog post has all these laid out and when you draw these",
    "start": "364199",
    "end": "369440"
  },
  {
    "text": "out think about for example each of these calls or steps is just a different",
    "start": "369440",
    "end": "375120"
  },
  {
    "text": "function so for this particular workflow I'm going to create a function generate joke improve joke Polish joke and those",
    "start": "375120",
    "end": "382680"
  },
  {
    "text": "are all going to be just simple llm calls now what's interesting is with Lang graph this container or state that",
    "start": "382680",
    "end": "390120"
  },
  {
    "text": "I Define is pass into every one of these steps and I can extract whatever I want",
    "start": "390120",
    "end": "395160"
  },
  {
    "text": "from it in this case I go State topic to get the topic that written to state by the user and I can write things back to",
    "start": "395160",
    "end": "402599"
  },
  {
    "text": "State just by basically returning this joke key the output of my",
    "start": "402599",
    "end": "409960"
  },
  {
    "text": "LM call improve joke I populate improv joke final joke I populate final joke so",
    "start": "409960",
    "end": "418960"
  },
  {
    "text": "what's happening is at each see these steps I'm making LM calls and I'm populating my state or this container in",
    "start": "418960",
    "end": "427280"
  },
  {
    "text": "the workflow with the output of each llm call that's really all it's going on now",
    "start": "427280",
    "end": "432400"
  },
  {
    "text": "notice in this workflow there's a gate here so in this case I'm just going to create some gating logic so what's interesting is I can take in the state",
    "start": "432400",
    "end": "439280"
  },
  {
    "text": "and I can just check hey does a joke have a question mark or an exclamation point that's just some arbitrary",
    "start": "439280",
    "end": "445560"
  },
  {
    "text": "criteria and what I can do is I can pass anything I want here just two strings",
    "start": "445560",
    "end": "452400"
  },
  {
    "text": "pass or fail now this can be used as conditional Edge or gate in Lang graph",
    "start": "452400",
    "end": "457960"
  },
  {
    "text": "so now I have a container that has everything I want to modify in my",
    "start": "457960",
    "end": "463080"
  },
  {
    "text": "workflow I've defined each step of my workflow as an independent function I've defined a gate that's going to serve as",
    "start": "463080",
    "end": "470039"
  },
  {
    "text": "that check on the output of the joke and now I lay this out in Lang graph as a",
    "start": "470039",
    "end": "475520"
  },
  {
    "text": "simple workflow so all I need to do here is just take in my state initi ize the workflow add those three steps to it we",
    "start": "475520",
    "end": "482840"
  },
  {
    "text": "added generate joke improve joke Polish joke and this is where I Define the connectivity of my graph or workflow so",
    "start": "482840",
    "end": "491120"
  },
  {
    "text": "I'm going to start I'm going to go to generate joke first and you can see then I want to check the output of my first",
    "start": "491120",
    "end": "497560"
  },
  {
    "text": "call to see whether or not it has a punch line so I add a conditional Edge",
    "start": "497560",
    "end": "503520"
  },
  {
    "text": "that connects generate joke and based upon the output of this function",
    "start": "503520",
    "end": "510400"
  },
  {
    "text": "if it's pass I go to improve joke that's my other node if it's fail I just end",
    "start": "510400",
    "end": "517320"
  },
  {
    "text": "then I create the edges from improve to Polish Polish to end compile that as a workflow and there we go so that's the",
    "start": "517320",
    "end": "524360"
  },
  {
    "text": "exact same workflow that they drew out in the blog post applied to joke",
    "start": "524360",
    "end": "530600"
  },
  {
    "text": "generation now once I have this workflow I can very simply just run chain. invoke",
    "start": "530600",
    "end": "535839"
  },
  {
    "text": "that's a very simple way to invoke any chain workflow or whatever whatever you have in Lang graph I pass in a topic",
    "start": "535839",
    "end": "542519"
  },
  {
    "start": "540000",
    "end": "660000"
  },
  {
    "text": "from the user and we're can see the llms create an initial joke they improved it",
    "start": "542519",
    "end": "548079"
  },
  {
    "text": "and Claude creates final joke so this is a very simple example of a chain great so we've covered the augment to LM we've",
    "start": "548079",
    "end": "555680"
  },
  {
    "text": "covered basic prom chaining now let's get into some more interesting and complex workflows like parallelization",
    "start": "555680",
    "end": "561640"
  },
  {
    "text": "now let's talk about parallelization do this when you for example have multiple perspectives that",
    "start": "561640",
    "end": "567680"
  },
  {
    "text": "you want for single task so I've this quite a bit with things like multi-query rag if I have a question I want to Fan",
    "start": "567680",
    "end": "573600"
  },
  {
    "text": "it out into like three or four different sub questions or when independent task can be performed with different prompts",
    "start": "573600",
    "end": "578760"
  },
  {
    "text": "or different llms so lots of cases where you want to just paralyze tasks and workflows so let's just build this let's",
    "start": "578760",
    "end": "584640"
  },
  {
    "text": "take a topic create a joke story and poem all in parallel so in the same way",
    "start": "584640",
    "end": "590079"
  },
  {
    "text": "I did before I create my state again this is just a container for everything I'm going to modify in my workflow in",
    "start": "590079",
    "end": "596079"
  },
  {
    "text": "this case I'll have a topic from a user and I'll have the joke the story the poem and all just combine them at the",
    "start": "596079",
    "end": "601399"
  },
  {
    "text": "end now this workflow is going to have three LM calls they'll run in parallel and then one aggregation that'll pull",
    "start": "601399",
    "end": "607360"
  },
  {
    "text": "them all together so all I'm going to do is I'm going to create a function for each of those different steps in my",
    "start": "607360",
    "end": "613360"
  },
  {
    "text": "workflow LM call one is going to write a joke about the topic a story a poem and",
    "start": "613360",
    "end": "620959"
  },
  {
    "text": "I'll just aggregate them all into a string super simple now just like I showed before we can build this in L graph pass in that state the container",
    "start": "620959",
    "end": "628440"
  },
  {
    "text": "add the nodes and just connect them so in this case I go from start to LM call 1 2 3 and they all connect to the",
    "start": "628440",
    "end": "636440"
  },
  {
    "text": "aggregator on the end and I can show it there you go so you have a nice visualization of your workflow and I can",
    "start": "636440",
    "end": "641600"
  },
  {
    "text": "run it there we go so we have a story now a joke and a poem all created in",
    "start": "641600",
    "end": "647480"
  },
  {
    "text": "parallel so we've covered the augmented elements our building block we talked about prom chaining we talked about parallelization now let's get into",
    "start": "647480",
    "end": "653720"
  },
  {
    "text": "routing I've used routing a lot and it's extremely useful in a bunch of use cases when you want to decide",
    "start": "653720",
    "end": "659760"
  },
  {
    "text": "for example if you want to only go to one of those steps which one to go to you can use LM to make that decision so",
    "start": "659760",
    "end": "666480"
  },
  {
    "start": "660000",
    "end": "840000"
  },
  {
    "text": "a specific example this is I've used routing a lot in working with retrieval in taking a question and routing it to",
    "start": "666480",
    "end": "671959"
  },
  {
    "text": "different retrieval systems so let's do an example where we take an input and we route it to either a joke a story or a",
    "start": "671959",
    "end": "678600"
  },
  {
    "text": "poem generation based on what the user asks for now I'm going to show a very nice trick here for routing I often like",
    "start": "678600",
    "end": "684720"
  },
  {
    "text": "to do this I basically take an llm and I give it a structured output so",
    "start": "684720",
    "end": "691279"
  },
  {
    "text": "that guarantees the LM is going to produce in this particular case the strings poem story or joke as a",
    "start": "691279",
    "end": "698519"
  },
  {
    "text": "structured object and I'm going to give it in this particular case a pedantic model just like before I'm going to",
    "start": "698519",
    "end": "704720"
  },
  {
    "text": "Define my state again this is a container we'll take an input from the user we'll get the decision from our",
    "start": "704720",
    "end": "711480"
  },
  {
    "text": "router and we'll get an output we'll save that as the final output now in",
    "start": "711480",
    "end": "717399"
  },
  {
    "text": "this case for my nodes just like before four I'm going to have three different LM calls that will either write a story",
    "start": "717399",
    "end": "723839"
  },
  {
    "text": "a joke or a poem and I'm going to have my router which is going to take the user input and basically decide whether",
    "start": "723839",
    "end": "730240"
  },
  {
    "text": "or not to Route it to story joke or poem based on the content of the input and it's just going to return that as a",
    "start": "730240",
    "end": "736800"
  },
  {
    "text": "structured object and I can extract from the data model the step decision write",
    "start": "736800",
    "end": "744279"
  },
  {
    "text": "that to State as decision so remember if you look at the model here it's just a pantic model which is going to Output a",
    "start": "744279",
    "end": "752560"
  },
  {
    "text": "single key step which is either going to be poem story or jokes I extract that from the pedantic model that is returned",
    "start": "752560",
    "end": "759320"
  },
  {
    "text": "by the router and I write that to state so then I have my decision in state and now this is just going to be a",
    "start": "759320",
    "end": "764519"
  },
  {
    "text": "conditional Edge that'll look at the decision and determine what node to go",
    "start": "764519",
    "end": "769600"
  },
  {
    "text": "to super simple so basically if the decision is story go to node one joke 2",
    "start": "769600",
    "end": "776959"
  },
  {
    "text": "poem 3 that's it now you'll see in this toy example each of these steps are",
    "start": "776959",
    "end": "782320"
  },
  {
    "text": "doing the same thing but in real world examples the router will send to",
    "start": "782320",
    "end": "788480"
  },
  {
    "text": "different steps that will have different logic different LM calls this is just a toy example showing you how to hook up",
    "start": "788480",
    "end": "793880"
  },
  {
    "text": "that logic between for example a router step a structured output and a decision",
    "start": "793880",
    "end": "800600"
  },
  {
    "text": "about where to go next cool so this is showing you visualization of that you'll see what's kind of nice in Lang graph",
    "start": "800600",
    "end": "806440"
  },
  {
    "text": "when we visualize this this dotted line means a condition Edge so it's going to go to only one of those three paths",
    "start": "806440",
    "end": "812560"
  },
  {
    "text": "whenever this runs and of course that's going to be based upon the decision of the router now we can look at these",
    "start": "812560",
    "end": "818639"
  },
  {
    "text": "nodes and just create a print statement that'll just tell us which node was visited to confirm we can run this cool",
    "start": "818639",
    "end": "825720"
  },
  {
    "text": "so we know we go to the node that creates a joke and we get the joke output we've covered the augment elementer building block we talked about",
    "start": "825720",
    "end": "832959"
  },
  {
    "text": "promp chaining we talked about parallelization we talked about routing and I'll talk about another workflow or",
    "start": "832959",
    "end": "839240"
  },
  {
    "text": "rator worker now this is a really interesting one and I've used this quite a bit as well so this is a case where you want an llm to break down a task",
    "start": "839240",
    "end": "846079"
  },
  {
    "start": "840000",
    "end": "1200000"
  },
  {
    "text": "into a set of subtasks delegate each subtask to an independent worker and then synthesize the results so it's kind",
    "start": "846079",
    "end": "852600"
  },
  {
    "text": "of like parallelization except the key difference is this worker assignment you don't know ahead of time so you're",
    "start": "852600",
    "end": "858399"
  },
  {
    "text": "having an llm reason about something and then create a bunch of workers based upon its reasoning so again in this case",
    "start": "858399",
    "end": "865279"
  },
  {
    "text": "the LM is kind of gating or creating the control flow just like in the case of routing so here's an example that I've",
    "start": "865279",
    "end": "871120"
  },
  {
    "text": "used quite a bit report writing maybe you've played with deep research and llm",
    "start": "871120",
    "end": "877079"
  },
  {
    "text": "reasons about the plan for the report and dynamically generates a bunch of report sections and then goes and does",
    "start": "877079",
    "end": "882600"
  },
  {
    "text": "research on all them classic example of an orchestrator worker type workflow so let's actually do that right now so for",
    "start": "882600",
    "end": "889040"
  },
  {
    "text": "this the trick is I'm also going to use structured outputs so I'm going to create a data model for a report section",
    "start": "889040",
    "end": "895000"
  },
  {
    "text": "it's going to have a name and a description and I'm going to have a list of sections here so what I'm going to do",
    "start": "895000",
    "end": "900279"
  },
  {
    "text": "is I'm going to bind that section list to my llm and that's going to be my planner so what's cool here is the",
    "start": "900279",
    "end": "906839"
  },
  {
    "text": "planner is going to take an input reflect on it and produce a list of sections based upon Its Reflection so",
    "start": "906839",
    "end": "913480"
  },
  {
    "text": "this is dynamic I don't know how many sections will create a priori that's why this is a very good orchestrator worker",
    "start": "913480",
    "end": "918680"
  },
  {
    "text": "use case now in line graph look like before I create my state now in this case what I'm going to do is I'm going",
    "start": "918680",
    "end": "924639"
  },
  {
    "text": "to create a state for my orchestrator graph which is going to have like a topic from a user a list of sections a",
    "start": "924639",
    "end": "932040"
  },
  {
    "text": "list of completed sections which the workers will all write to and a final report okay so that's State one now this",
    "start": "932040",
    "end": "939639"
  },
  {
    "text": "is where things get interesting with orchestrated worker workflows in langra the way we often like to do it is for",
    "start": "939639",
    "end": "946240"
  },
  {
    "text": "the workers give them their own state so why is this because each of those workers you want to handle independent",
    "start": "946240",
    "end": "953199"
  },
  {
    "text": "inputs and they're kind of all self-contained objects think about as their own little buckets in which work's",
    "start": "953199",
    "end": "960480"
  },
  {
    "text": "being done and different Works being done in each one but they're all writing out to the same output and this is why I",
    "start": "960480",
    "end": "968880"
  },
  {
    "text": "include this completed sections key in the worker State and in the graph state",
    "start": "968880",
    "end": "973920"
  },
  {
    "text": "so what's interesting in line graph is when you have overlapping keys and you write to for example this completed",
    "start": "973920",
    "end": "980880"
  },
  {
    "text": "sections key in each worker the outer state will also have that update",
    "start": "980880",
    "end": "986720"
  },
  {
    "text": "reflected so what's going to happen is all the worker is going to write to this completed sections key in parallel and",
    "start": "986720",
    "end": "993279"
  },
  {
    "text": "we structure this key with an annotation that allows for the addition of new",
    "start": "993279",
    "end": "999920"
  },
  {
    "text": "elements so that's really all we need to do so here I'm going to create a work an orchestrator and this is basically going",
    "start": "999920",
    "end": "1006120"
  },
  {
    "text": "to be my planner I'm going to invoke it with the input topic from the user and",
    "start": "1006120",
    "end": "1011519"
  },
  {
    "text": "I'm going to tell it to create a plan then I'm going to have this LM call which is my worker it takes in the",
    "start": "1011519",
    "end": "1017319"
  },
  {
    "text": "worker state and it basically says write a report section and that's all I need",
    "start": "1017319",
    "end": "1023279"
  },
  {
    "text": "and I just pass it the section name and section description and this completed sections is a state key that all the",
    "start": "1023279",
    "end": "1030798"
  },
  {
    "text": "workers can write to in parallel that's the key point and all those sections are",
    "start": "1030799",
    "end": "1036000"
  },
  {
    "text": "going to be accumulated in that completed sections key then I'm going to have a synthesizer that's going to read",
    "start": "1036000",
    "end": "1041520"
  },
  {
    "text": "out the completed sections and just write them all out as a string that's it write that to the final report key in my",
    "start": "1041520",
    "end": "1048240"
  },
  {
    "text": "state now this is the only thing that's new and a little bit special in orchestrator worker style",
    "start": "1048240",
    "end": "1054120"
  },
  {
    "text": "workflows because this is so common we have a special API called send and Lang",
    "start": "1054120",
    "end": "1059640"
  },
  {
    "text": "graph that allows you to basically spawn these workers dynamically and what's happening is remember my planner wrote",
    "start": "1059640",
    "end": "1067160"
  },
  {
    "text": "the sections of the port to the state and I can iterate through those sections",
    "start": "1067160",
    "end": "1072240"
  },
  {
    "text": "and I can basically assign each section to an independent worker just like this for each section in section s send to",
    "start": "1072240",
    "end": "1081559"
  },
  {
    "text": "llm call that's my worker and basically initialize the state as section when I",
    "start": "1081559",
    "end": "1088679"
  },
  {
    "text": "do that so then that LM call you can see receives worker State and it's receiving",
    "start": "1088679",
    "end": "1095000"
  },
  {
    "text": "from State the section name and description and then it goes and writes",
    "start": "1095000",
    "end": "1100120"
  },
  {
    "text": "that section it writes it output to completed sections which my orchestrator has access to and then the synthesizer",
    "start": "1100120",
    "end": "1107480"
  },
  {
    "text": "basically just grabs completed SE and combines them that's it you're done that's all you need to do now I can",
    "start": "1107480",
    "end": "1113159"
  },
  {
    "text": "build this workflow and there we go so this dotted line just shows you that you use the send API to spawn a whole bunch",
    "start": "1113159",
    "end": "1119840"
  },
  {
    "text": "of llm call workers you don't know how many a HEB ton that's why it doesn't draw out each one specifically it's",
    "start": "1119840",
    "end": "1125360"
  },
  {
    "text": "going to be dynamically determined based upon the orchestrator plan that's the key characteristic of these orchestrator",
    "start": "1125360",
    "end": "1131120"
  },
  {
    "text": "worker workflows you don't know a priori how many workers you need the llm will determine that on the fly now let's Show",
    "start": "1131120",
    "end": "1138320"
  },
  {
    "text": "an example this I want a report on L scaling logs so what's kind of cool is",
    "start": "1138320",
    "end": "1144400"
  },
  {
    "text": "this runs fairly quickly you can see the planner generate all these report sections great and I can look at the",
    "start": "1144400",
    "end": "1151120"
  },
  {
    "text": "final report and it concatenates them all together and you can see I get this Rich introduction and then I get you",
    "start": "1151120",
    "end": "1156400"
  },
  {
    "text": "know fundamentals scaling relationships following the plan now in reality when I do this I have much more detailed",
    "start": "1156400",
    "end": "1163200"
  },
  {
    "text": "prompts I showed here this is really showing you the workflow rather than the particulars of How I build a high",
    "start": "1163200",
    "end": "1168320"
  },
  {
    "text": "quality report writer in fact I've have separate videos on report writing you could check out but this is just showing",
    "start": "1168320",
    "end": "1173360"
  },
  {
    "text": "you how to set up an orchestrator worker style of workflow if we just talk about",
    "start": "1173360",
    "end": "1179559"
  },
  {
    "text": "the orchestrator worker workflow let's talk about something that's a bit related the evaluator Optimizer workflow",
    "start": "1179559",
    "end": "1185679"
  },
  {
    "text": "in both these cases you're going to have llms directing the control flow through",
    "start": "1185679",
    "end": "1191120"
  },
  {
    "text": "predefined code paths we have one llm gener response and another kind of grade",
    "start": "1191120",
    "end": "1196440"
  },
  {
    "text": "it and give feedback in a loop now I've used this quite a bit for example grading responses from a rag system for",
    "start": "1196440",
    "end": "1202520"
  },
  {
    "start": "1200000",
    "end": "1440000"
  },
  {
    "text": "hallucinations or for factual accuracy I've used this kind of like kind of evaluator Gates and if for example",
    "start": "1202520",
    "end": "1209360"
  },
  {
    "text": "there's hallucination in the output that isn't grounded by the documents I send it back and have a regenerate response",
    "start": "1209360",
    "end": "1215360"
  },
  {
    "text": "and here I'm going to use structured outputs again you see there's kind of a trend here structured outputs is like",
    "start": "1215360",
    "end": "1221280"
  },
  {
    "text": "kind of all you need and quotes it's extremely convenient because you can really build all these workflows just of",
    "start": "1221280",
    "end": "1226640"
  },
  {
    "text": "structured outputs you don't have to use tool calling for example you can use routers to basically conditionally",
    "start": "1226640",
    "end": "1232120"
  },
  {
    "text": "determine where to go next you can have then nodes that for example just call tools depending on the result of the",
    "start": "1232120",
    "end": "1237720"
  },
  {
    "text": "router itself so really just structured outputs is a nice way you can build a",
    "start": "1237720",
    "end": "1242880"
  },
  {
    "text": "lot of complex workflows in this case I'm going to create a structured output that's basically my my grader model",
    "start": "1242880",
    "end": "1248640"
  },
  {
    "text": "that's going to be grade and feedback decide if the joke is funny or not in my case and if it is not funny give some",
    "start": "1248640",
    "end": "1256760"
  },
  {
    "text": "feedback okay so that's going to be my evaluator and again I'm going to find the graph state in this case I'm going to take a joke based upon a topic from a",
    "start": "1256760",
    "end": "1263320"
  },
  {
    "text": "user I'll generate the joke and then I'll grade it is give it feedback determine if it's funny or not based on",
    "start": "1263320",
    "end": "1269640"
  },
  {
    "text": "the feedback I'll go back regenerate a new joke that's it nice and simple so",
    "start": "1269640",
    "end": "1274840"
  },
  {
    "text": "this is going to be my generator it's going to generate a joke now you'll see some do something kind of interesting",
    "start": "1274840",
    "end": "1280320"
  },
  {
    "text": "here I'm going to check if there's feedback in the state okay now there might be because I've basically looped",
    "start": "1280320",
    "end": "1287159"
  },
  {
    "text": "back to this node if I determine that the joke is not good so there may be feedback in the state if there's",
    "start": "1287159",
    "end": "1292919"
  },
  {
    "text": "feedback included in my prompt otherwise I just say write a joke about the topic so nice and easy and then I have an",
    "start": "1292919",
    "end": "1300159"
  },
  {
    "text": "evaluator that basically takes in the joke from State and grades it again this",
    "start": "1300159",
    "end": "1305840"
  },
  {
    "text": "evaluator has structured output so it'll basically produce a grade and some feedback and then I have additional Edge",
    "start": "1305840",
    "end": "1311960"
  },
  {
    "text": "that'll look at State funny or not so again that's like kind of my grade and if it's funny route to accepted if it's",
    "start": "1311960",
    "end": "1319960"
  },
  {
    "text": "not funny route to rejected and feedback so again this is the initial ledge I use to determine where to go next just like",
    "start": "1319960",
    "end": "1326440"
  },
  {
    "text": "we saw it's routing build My Graph and you can see has everything we want here here's the generator here is the",
    "start": "1326440",
    "end": "1332120"
  },
  {
    "text": "evaluator based on the evaluator we either go back and again you can see we set that conditional ledge up right here",
    "start": "1332120",
    "end": "1338799"
  },
  {
    "text": "so that route joke conditional ledge we just talked about it if funny or not if it's funny we go to accepted so if",
    "start": "1338799",
    "end": "1345919"
  },
  {
    "text": "accepted we go to end if it is rejected in feedback we go back to LM call",
    "start": "1345919",
    "end": "1351919"
  },
  {
    "text": "generat so you can see when you st the initial Edge this is how you can basically route from the output of your",
    "start": "1351919",
    "end": "1358919"
  },
  {
    "text": "logic your Edge logic to the next node to go to that's it it's extremely simple",
    "start": "1358919",
    "end": "1365799"
  },
  {
    "text": "and you can see you get a nice visualization of it so let's give this a shot run it with an input of cats so we",
    "start": "1365799",
    "end": "1372200"
  },
  {
    "text": "get the out of the joke and we actually can look at the state to see what the feedback was so State feedback in this",
    "start": "1372200",
    "end": "1378120"
  },
  {
    "text": "case it seems to like it and let's check funny or not and it determines it's",
    "start": "1378120",
    "end": "1384640"
  },
  {
    "text": "funny okay so greater like the joke we went ahead and returned it then to the user we ended and so we can see that the",
    "start": "1384640",
    "end": "1390320"
  },
  {
    "text": "greater was initiated and decided to like the joke so it passes and we finish",
    "start": "1390320",
    "end": "1395679"
  },
  {
    "text": "so we talked about a number of different workflows that use llms within some kind of reasoning scaffolding and in the case",
    "start": "1395679",
    "end": "1402080"
  },
  {
    "text": "of orchestrator worker evaluator Optimizer routing you actually do let",
    "start": "1402080",
    "end": "1408039"
  },
  {
    "text": "the L M make decisions to Route the control flow through that scaffolding now let's remove the scaffolding and",
    "start": "1408039",
    "end": "1414960"
  },
  {
    "text": "let's talk about agents with agents You're simply allowing an llm to form actions in form of tool calls and",
    "start": "1414960",
    "end": "1421520"
  },
  {
    "text": "directly receive the output or feedback from those actions and so in the",
    "start": "1421520",
    "end": "1426559"
  },
  {
    "text": "workflow case we talked about there are always kind of these predefined code paths that we had an llm kind of",
    "start": "1426559",
    "end": "1432679"
  },
  {
    "text": "follow and kind of route through in the case of an agent we've removed those",
    "start": "1432679",
    "end": "1439440"
  },
  {
    "text": "now when do you actually need an agent this is kind of the big question you see agents being used in cases where you",
    "start": "1439440",
    "end": "1445600"
  },
  {
    "start": "1440000",
    "end": "1620000"
  },
  {
    "text": "really have open ended problems that you cannot easily capture in a workflow for",
    "start": "1445600",
    "end": "1451440"
  },
  {
    "text": "example you want llm to utilize different Tools in a pattern that you",
    "start": "1451440",
    "end": "1458159"
  },
  {
    "text": "just cannot predict out priority so it's not easy to lay it in the workflow so it's kind of open-ended task we some",
    "start": "1458159",
    "end": "1463559"
  },
  {
    "text": "some really interesting examples of challenges like s bench so a benchmark for software engineering in which",
    "start": "1463559",
    "end": "1470039"
  },
  {
    "text": "anthropic actually used a agents architecture just shown like this and achieves very strong performance so we",
    "start": "1470039",
    "end": "1476120"
  },
  {
    "text": "know for certain open-ended tasks agents are appropriate I do want to caveat in the event that LMS get extremely",
    "start": "1476120",
    "end": "1484399"
  },
  {
    "text": "proficient at tool calling it's also possible that a lot of the scaffolding that we talked about with various",
    "start": "1484399",
    "end": "1489679"
  },
  {
    "text": "workflows is unnecessary today if you know roughly",
    "start": "1489679",
    "end": "1495520"
  },
  {
    "text": "the sequence tools need to be initiated it's often better just capture it in a workflow in terms of reliability than",
    "start": "1495520",
    "end": "1501640"
  },
  {
    "text": "just give it to an agent let the agent hopefully call that correct sequence now let's just set up an LM with tools I'm",
    "start": "1501640",
    "end": "1507799"
  },
  {
    "text": "going to give it multiply add and divide nice and simple now I'm going to Define three nodes so I'm going to call my LM",
    "start": "1507799",
    "end": "1514600"
  },
  {
    "text": "and I'm going to basically allow the llm to call a tool okay so I'm using my LM",
    "start": "1514600",
    "end": "1521200"
  },
  {
    "text": "with tools and in this particular case I'm saying you're a helpful assistant task with performing arithmetic so the",
    "start": "1521200",
    "end": "1526799"
  },
  {
    "text": "output of that tool call is going to be saved to this messages key in our state so what's happening is my state has a",
    "start": "1526799",
    "end": "1533039"
  },
  {
    "text": "single key in this particular place messages which is going to accumulate what I pass as the user what the LM",
    "start": "1533039",
    "end": "1539440"
  },
  {
    "text": "produces and so forth so what's going to happen is I have another node called tool node it's going to look at the",
    "start": "1539440",
    "end": "1545559"
  },
  {
    "text": "state look at the last message determine if it's a tool call if it is it'll go",
    "start": "1545559",
    "end": "1551919"
  },
  {
    "text": "ahead and just call that tool that's it nice and simple and it's going to return",
    "start": "1551919",
    "end": "1557320"
  },
  {
    "text": "that to state so what's interesting here is I'm G to have a sequence of human",
    "start": "1557320",
    "end": "1563440"
  },
  {
    "text": "input model agent in this case the size to call a tool this tool node looks and sees oh",
    "start": "1563440",
    "end": "1571080"
  },
  {
    "text": "the LM decided to call a tool it actually runs that tool call that then is written to State messages as a tool",
    "start": "1571080",
    "end": "1579240"
  },
  {
    "text": "message this is that environmental feedback thing you hear about so you talk about agents they can perform",
    "start": "1579240",
    "end": "1584559"
  },
  {
    "text": "actions that's the tool call done up here fine they also can receive feedback from the environment and act on it that",
    "start": "1584559",
    "end": "1591480"
  },
  {
    "text": "is the output of this tool noes this tool Noe is basically the environmental feedback saying here's the output of the tool call the llm then we'll get that",
    "start": "1591480",
    "end": "1599080"
  },
  {
    "text": "decide what to do next that's it that's an agent now the only other thing I need",
    "start": "1599080",
    "end": "1604240"
  },
  {
    "text": "is this conditional ledge that basically says was the last message uh a tool call",
    "start": "1604240",
    "end": "1611360"
  },
  {
    "text": "if so I'm going to go ahead and route to the tool node and if not I will end so",
    "start": "1611360",
    "end": "1618240"
  },
  {
    "text": "you know you can modify this in different ways but a lot of times people basically just say allow the agent continue making tool calls until it",
    "start": "1618240",
    "end": "1624120"
  },
  {
    "start": "1620000",
    "end": "1800000"
  },
  {
    "text": "decides it doesn't need one anymore and then you don't and then you're done so there we are I mean that's our agent Loop that you know and that's kind of",
    "start": "1624120",
    "end": "1630279"
  },
  {
    "text": "why agents are elegant they're extremely simple in this kind of formulation all is it's basically an llm initially with",
    "start": "1630279",
    "end": "1637120"
  },
  {
    "text": "a bunch of tools and it's like a tool node that will run the tool for you and return",
    "start": "1637120",
    "end": "1643640"
  },
  {
    "text": "that environmental feedback to the llm and let the llm keep spinning until it decides you don't need a tool call",
    "start": "1643640",
    "end": "1649640"
  },
  {
    "text": "anymore then you're done that's really it now again this tool call thing think about that is actions so this LM can perform actions the actions are",
    "start": "1649640",
    "end": "1656600"
  },
  {
    "text": "determined by the user so you can pass in any set of tools to this LM it decides to perform those actions now you",
    "start": "1656600",
    "end": "1662600"
  },
  {
    "text": "need some system that actually does those actions so that's like that's what we create with this tool node and this",
    "start": "1662600",
    "end": "1668200"
  },
  {
    "text": "just Loops until the LM says I don't need a tool call anymore and then that conditional Edge says okay I can just",
    "start": "1668200",
    "end": "1674519"
  },
  {
    "text": "end that's really it so let's see an example of that I'm going to basic tell this agent add three and four then take",
    "start": "1674519",
    "end": "1681840"
  },
  {
    "text": "the output and multiply by four okay now look this is extremely simple and LM can",
    "start": "1681840",
    "end": "1687279"
  },
  {
    "text": "just do this I totally get it this is more showing you the principle setting up an agent with these tools for Edition",
    "start": "1687279",
    "end": "1694039"
  },
  {
    "text": "and multiplication and testing whether or not it can correctly perform these tool calls in sequence and looking at",
    "start": "1694039",
    "end": "1701360"
  },
  {
    "text": "the flow of messages so here we go this is pretty cool now again this is like a toy",
    "start": "1701360",
    "end": "1707799"
  },
  {
    "text": "example Le but that's showing you the flow and that's what matters you have an input from the human my instructions the",
    "start": "1707799",
    "end": "1713080"
  },
  {
    "text": "llm looks at that and says okay I need to make a tool call makes a tool call the tool node executes the tool call",
    "start": "1713080",
    "end": "1721080"
  },
  {
    "text": "returns environmental feedback to my llm as the result of the tool call which is",
    "start": "1721080",
    "end": "1726559"
  },
  {
    "text": "seven my agent thinks about it makes another tool call responds with 28 agent thinks about it final result is 28",
    "start": "1726559",
    "end": "1733000"
  },
  {
    "text": "here's how we got there no more tool calls needed done for all the hyp agents that's all it is extremely simple is",
    "start": "1733000",
    "end": "1739279"
  },
  {
    "text": "tool calling in a loop now again why don't we just do this because look a lot",
    "start": "1739279",
    "end": "1744440"
  },
  {
    "text": "of problems can be solved with workflows which are bit simpler agents haven't been particularly reliable to date",
    "start": "1744440",
    "end": "1749600"
  },
  {
    "text": "particularly with large numbers of tools or complex trajectories of tool calls and so blot people actually in",
    "start": "1749600",
    "end": "1754880"
  },
  {
    "text": "production prefer workflows I've done a lot more workflows than agents to be honest over the last year or two but I",
    "start": "1754880",
    "end": "1760519"
  },
  {
    "text": "completely acknowledge in the event that you have ver capacity reasoning models that can perform low latency high",
    "start": "1760519",
    "end": "1766559"
  },
  {
    "text": "quality tool calling and we have more kind of confidence that they can perform reliably in production",
    "start": "1766559",
    "end": "1772880"
  },
  {
    "text": "I think you will see the movement to this classic style of very simple tool",
    "start": "1772880",
    "end": "1778640"
  },
  {
    "text": "calling agent in production but the game in the field today is a little bit more like people are saying well I want to",
    "start": "1778640",
    "end": "1784559"
  },
  {
    "text": "put workflows into production because they're a little bit more trustworthy again I have some scaffolding around the",
    "start": "1784559",
    "end": "1790240"
  },
  {
    "text": "core kind of L calls now I do also want to show this is looking at the",
    "start": "1790240",
    "end": "1796279"
  },
  {
    "text": "documentation that we do have AE pre-built method called create react agent that basically wraps what we just",
    "start": "1796279",
    "end": "1801640"
  },
  {
    "start": "1800000",
    "end": "1910000"
  },
  {
    "text": "built from scratch right here for convenience now this depends if you want",
    "start": "1801640",
    "end": "1807720"
  },
  {
    "text": "to build it from scratch yourself just as we did that is completely fine if you want to use the preo method that's fine",
    "start": "1807720",
    "end": "1813120"
  },
  {
    "text": "as well but it's available to you and I will be sharing the link to this tutorial page which has everything we",
    "start": "1813120",
    "end": "1820000"
  },
  {
    "text": "just went through so all the different uh workflows we talk through are all in this tutorial which I will be sharing",
    "start": "1820000",
    "end": "1827120"
  },
  {
    "text": "but just from this little tutorial you've seen here's how you can lay out all these different workflows and an agent all in Lang graph",
    "start": "1827120",
    "end": "1834799"
  },
  {
    "text": "if I go back to the Y langra story when you get with langra is when you compile",
    "start": "1834799",
    "end": "1840440"
  },
  {
    "text": "those workflows or agent in langra you're getting a persistence layer for free which gives you short and long-term",
    "start": "1840440",
    "end": "1845640"
  },
  {
    "text": "memory and that also gives you the ability to stop perform interruptions review and continue IE human in the loop",
    "start": "1845640",
    "end": "1853960"
  },
  {
    "text": "it gives you a whole bunch of streaming capacities you can stream independent values from your State at any point in time you can stream of course tokens out",
    "start": "1853960",
    "end": "1861080"
  },
  {
    "text": "of LM calls and you also get deployment so we have a very nice and easy onramp",
    "start": "1861080",
    "end": "1866639"
  },
  {
    "text": "for testing debugging and deploying any of these so you could take any of those workflows R we just built and deploy",
    "start": "1866639",
    "end": "1872799"
  },
  {
    "text": "them in like five minutes so very very quickly that's really what you're getting with the framework and you can",
    "start": "1872799",
    "end": "1879320"
  },
  {
    "text": "also see with L graph it's pretty easy to lay them out and actually working on making it even easier so we're trying to reduce the overhead so you can lay it",
    "start": "1879320",
    "end": "1885919"
  },
  {
    "text": "out almost as if you're writing python you're not even thinking about the framework but you're getting these benefits kind of for free when you do",
    "start": "1885919",
    "end": "1892720"
  },
  {
    "text": "use a framework that's kind of the big idea here so hopefully this was a helpful overview to present uh how to",
    "start": "1892720",
    "end": "1899120"
  },
  {
    "text": "lay these various workflows or agents out using Lang graph and what benefits you may get from L graph as a",
    "start": "1899120",
    "end": "1905240"
  },
  {
    "text": "consideration so thanks very much feel free to leave any comments below",
    "start": "1905240",
    "end": "1911000"
  }
]