[
  {
    "text": "what's up everyone it's brace and this is the third video in our gener of UI series on building gener UI applications",
    "start": "359",
    "end": "6279"
  },
  {
    "text": "with Lang chain in this video we are going to walk through how to build a generative UI chatbot with a python back",
    "start": "6279",
    "end": "12599"
  },
  {
    "text": "end and then an xjs front end um if you've not seen the first video you should go back and watch that because",
    "start": "12599",
    "end": "18160"
  },
  {
    "text": "that's where we cover some high level Concepts like what is generative UI uh some different use cases why it's better",
    "start": "18160",
    "end": "24400"
  },
  {
    "text": "than previous methods and then we go into a little bit of detail into the apps we're going to build today um if",
    "start": "24400",
    "end": "29599"
  },
  {
    "text": "you're looking for the JavaScript version that's going to be linked in the description that in that video we build the same chatbot that we built here um",
    "start": "29599",
    "end": "37399"
  },
  {
    "text": "but we built it with a full JavaScript typescript stack uh this video is going to have a python backend uh but we're",
    "start": "37399",
    "end": "43520"
  },
  {
    "text": "still going to be using some JavaScript for the nextjs front end so for a quick",
    "start": "43520",
    "end": "48879"
  },
  {
    "text": "refresher if you watch the first video this is the architecture diagram of the",
    "start": "48879",
    "end": "53960"
  },
  {
    "text": "chat bot we're going to be building today and we can see we have two distinct sections the server which is where our python code will live and then",
    "start": "53960",
    "end": "60239"
  },
  {
    "text": "the client which is where our nextjs code will live uh so the server takes in some inputs some user input any images",
    "start": "60239",
    "end": "66360"
  },
  {
    "text": "chat history those then get passed to an LM and the LM has a few tools bound to",
    "start": "66360",
    "end": "71600"
  },
  {
    "text": "it these tools all correspond to UI components which we have on the",
    "start": "71600",
    "end": "76680"
  },
  {
    "text": "client this LM is then invoked with these tools um it can either select a tool to call if the user's input",
    "start": "76680",
    "end": "83400"
  },
  {
    "text": "requires it and if not then the LM will just return plain text um we're going using using Lang",
    "start": "83400",
    "end": "90400"
  },
  {
    "text": "graph for our python back end and that's where this conditional Edge goes to um",
    "start": "90400",
    "end": "95640"
  },
  {
    "text": "if you have not if you're not familiar with L graph I'm going to add a link somewhere on the screen to our Lang graph playlist where we go into detail",
    "start": "95640",
    "end": "102479"
  },
  {
    "text": "on Lang graph and all of its apis um but as a quick refresher we can take a look at this simple diagram L graph is",
    "start": "102479",
    "end": "109920"
  },
  {
    "text": "essentially um one of our libraries which you can use to construct graphs um",
    "start": "109920",
    "end": "116320"
  },
  {
    "text": "or we like to use them for anything we would have used an for in the past so",
    "start": "116320",
    "end": "121479"
  },
  {
    "text": "this simple diagram shows you um what kind of what a l graph application consists of so you take an input each of",
    "start": "121479",
    "end": "128720"
  },
  {
    "text": "these circles are a node in L graph a node is just a function that gets invoked and some state is passed to it",
    "start": "128720",
    "end": "134480"
  },
  {
    "text": "so the question gets passed to the retrieve node um and then at the end of each node so in the beginning all the",
    "start": "134480",
    "end": "140440"
  },
  {
    "text": "state or your current state gets passed into the node that could be a list of messages it could be a dictionary with",
    "start": "140440",
    "end": "147239"
  },
  {
    "text": "you know five Keys um or whatever you want your State can be really whatever you want so that your state always gets",
    "start": "147239",
    "end": "153120"
  },
  {
    "text": "passed into the node and then uh when you return that node you can return in an individual item or the entire State",
    "start": "153120",
    "end": "160239"
  },
  {
    "text": "and L graph will just combine um what you returned with the state so if you just returned one item in your",
    "start": "160239",
    "end": "166200"
  },
  {
    "text": "dictionary it's just going to replace that field or there's some more complexities you can go into where you",
    "start": "166200",
    "end": "172040"
  },
  {
    "text": "can make them like combine or add or you know have a custom function deal with you combining State um but for now we'll",
    "start": "172040",
    "end": "178879"
  },
  {
    "text": "just think about it it gets all the state to the input and whatever you return just replaces that field in the",
    "start": "178879",
    "end": "184200"
  },
  {
    "text": "state so we have a retrieve node the results of that get then get passed to our grading node uh the results of our",
    "start": "184200",
    "end": "190440"
  },
  {
    "text": "grading node get passed to this conditional Edge we also have a conditional edge here um and this",
    "start": "190440",
    "end": "196440"
  },
  {
    "text": "conditional Edge essentially says are the documents relevant if they're not relevant or sorry are any docs",
    "start": "196440",
    "end": "203080"
  },
  {
    "text": "irrelevant if they're all relevant then it goes right to the generate node and then the generate node returns an answer",
    "start": "203080",
    "end": "209799"
  },
  {
    "text": "if they're irrelevant then it gets routed to the rewrite query node the results of the rewrite query node go to",
    "start": "209799",
    "end": "215760"
  },
  {
    "text": "the web search and then finally we go back to the generate node and then to the answer so L graph essentially as we",
    "start": "215760",
    "end": "221439"
  },
  {
    "text": "can see here allows you to have a series of nodes and then route your application flow between these nodes um without",
    "start": "221439",
    "end": "228640"
  },
  {
    "text": "having it be say an agent which could pick any node and it's not very predictable and it could you know go",
    "start": "228640",
    "end": "234680"
  },
  {
    "text": "right from retrieve to generate or something um or an llm chain which will always do the same flow so with L graph",
    "start": "234680",
    "end": "241519"
  },
  {
    "text": "you're able to construct your graph in a way which it can be somewhat smart and make decisions on its own but it's still",
    "start": "241519",
    "end": "248239"
  },
  {
    "text": "somewhat fenced in um so it can't just do whatever it",
    "start": "248239",
    "end": "253760"
  },
  {
    "text": "wants so if we go back here we see our llm is our first node that gets invoked",
    "start": "253760",
    "end": "258880"
  },
  {
    "text": "and the results of that get passed to our conditional Edge um if no tool was called then we just stream that text",
    "start": "258880",
    "end": "264120"
  },
  {
    "text": "right back to the to the UI and as these chunks are coming in then they get rendered on the UI if a tool is used it",
    "start": "264120",
    "end": "270840"
  },
  {
    "text": "gets passed to our invoked tool node here you see we stream back the name of the tool that was used we then execute",
    "start": "270840",
    "end": "276560"
  },
  {
    "text": "some tool function this is any arbitrary python function in our case it's typically be hitting an API um and then",
    "start": "276560",
    "end": "283520"
  },
  {
    "text": "after that we uh invoke our or we return our function results which then get",
    "start": "283520",
    "end": "288800"
  },
  {
    "text": "streamed back to the client we're going to be using the stream events endpoint from Lang chain which essentially allows",
    "start": "288800",
    "end": "294639"
  },
  {
    "text": "you to stream back every event which is yielded inside of a function in your Lang chain in our case our lane graph",
    "start": "294639",
    "end": "301120"
  },
  {
    "text": "graph so one of these events that'll yielded back is the name of the tool we then send that back to the client as",
    "start": "301120",
    "end": "307000"
  },
  {
    "text": "soon as it get selected so we can map that to a loading component or some sort of component to let the user know that",
    "start": "307000",
    "end": "313240"
  },
  {
    "text": "we're processing the request we've selected this tool um and that gets rendered on the UI right away so instead of having to wait until the entire Lane",
    "start": "313240",
    "end": "319400"
  },
  {
    "text": "graph graph uh finishes and we have the results we can select the tool that usually happens pretty quickly and then",
    "start": "319400",
    "end": "326479"
  },
  {
    "text": "instantly renders something on the page so the user knows we're working on their requ Quest um and has a much quicker time to First interaction then while",
    "start": "326479",
    "end": "333680"
  },
  {
    "text": "their loading component is being shown to the user we're executing our tool function in the background and then once",
    "start": "333680",
    "end": "338880"
  },
  {
    "text": "the results come in we then stream those back to the client and map our tool to our component and this will then be our",
    "start": "338880",
    "end": "345039"
  },
  {
    "text": "final component on our loading component we'll then populate that component with whatever fields are returned from our",
    "start": "345039",
    "end": "350520"
  },
  {
    "text": "function and then we update the UI um and this updating and appending the UI process can happen or sorry we can",
    "start": "350520",
    "end": "357360"
  },
  {
    "text": "update it or append the UI as many times as we would would like in our case we're only going to update it once and then",
    "start": "357360",
    "end": "363440"
  },
  {
    "text": "finish it with a final component uh but you could update and append your UI as many times as you would like let's you",
    "start": "363440",
    "end": "369800"
  },
  {
    "text": "you could have some much more complex L graph graph like this where the retrieve node updates the UI and then you let",
    "start": "369800",
    "end": "376479"
  },
  {
    "text": "them know you're grading it and then you let them know the result of the conditional Edge um so since we're using stream events we're able to get all",
    "start": "376479",
    "end": "381720"
  },
  {
    "text": "those events and render them on the UI as they happen on our server so for our",
    "start": "381720",
    "end": "387000"
  },
  {
    "text": "python backend you're going to want to go into the backend folder and then gen UI backend",
    "start": "387000",
    "end": "392520"
  },
  {
    "text": "and find the chain. Pui file this is the file where we will be implementing our laying graph chain um and the first",
    "start": "392520",
    "end": "400199"
  },
  {
    "text": "thing you want to do here is Define the state of the chain which can be passed through to each of the nodes so we're going to name our state",
    "start": "400199",
    "end": "407199"
  },
  {
    "text": "generative UI State at our Imports uh we will use this AI message later but for now we just need the human message our",
    "start": "407199",
    "end": "413599"
  },
  {
    "text": "state contains the input which will be a human message um and that's going to be the user's input",
    "start": "413599",
    "end": "420879"
  },
  {
    "text": "it will also contain the result which is optional because this will only be set if the llm calls a string or calls does",
    "start": "420879",
    "end": "428479"
  },
  {
    "text": "not call tool and only responds with a string so it's the plain text response of no tool was was used we also have an",
    "start": "428479",
    "end": "434599"
  },
  {
    "text": "optional tool calls um list of objects so a list of parse tool calls if the LM",
    "start": "434599",
    "end": "441000"
  },
  {
    "text": "does call a tool or tools we're going to parse it and set that value before we invoke the tool and then the result of a",
    "start": "441000",
    "end": "447280"
  },
  {
    "text": "tool call if the LM does call A tool we'll call invoke tools and then this will return this tool res result value",
    "start": "447280",
    "end": "454120"
  },
  {
    "text": "which will then use on the client to update the chat history so the lmc's are user input and then the result of a tool",
    "start": "454120",
    "end": "459240"
  },
  {
    "text": "so it knows it properly processed that tool now we can Implement our create",
    "start": "459240",
    "end": "464520"
  },
  {
    "text": "graph function we have not implemented our nodes yet but this will give us an idea about the different nodes and the",
    "start": "464520",
    "end": "470159"
  },
  {
    "text": "flow our graph is going to take uh we're going to want to implement or import our state graph and compile graph um this is",
    "start": "470159",
    "end": "479000"
  },
  {
    "text": "we're going to use as a type or type hint and this is going to be the state graph we're going to use for l l graph",
    "start": "479000",
    "end": "484479"
  },
  {
    "text": "uh as you can see it's pretty simple there's two nodes invoke model which will be this model or this node and then",
    "start": "484479",
    "end": "490720"
  },
  {
    "text": "invoke tools which will be here you see we don't have a node for plain text response because this conditional Edge",
    "start": "490720",
    "end": "496919"
  },
  {
    "text": "which is this part will essentially say if the model use a tool then call the",
    "start": "496919",
    "end": "502199"
  },
  {
    "text": "invoke tools node and if it didn't use a tool it's just going to end and end the graph and send the response back or",
    "start": "502199",
    "end": "507879"
  },
  {
    "text": "sorry the result back to the Cent our entry point is going to be invoke model and our finish point is going to",
    "start": "507879",
    "end": "514039"
  },
  {
    "text": "be invoke tools or the end variable which this conditional Edge will return",
    "start": "514039",
    "end": "519560"
  },
  {
    "text": "if um no tools were called then we're going to compile the graph and return it and then inside of our Lang serve server",
    "start": "519560",
    "end": "526480"
  },
  {
    "text": "file when we import this um this is going to be the runnable which Lang serve can call now that we've defined",
    "start": "526480",
    "end": "533120"
  },
  {
    "text": "our graph structure we can Define our first model so that or sorry our first node which is going to be invoked model",
    "start": "533120",
    "end": "540079"
  },
  {
    "text": "is going to take in two inputs one for state which is going to be the full generi state that we' defined since this",
    "start": "540079",
    "end": "546519"
  },
  {
    "text": "will be the first node that's called it will only have the input um and then pre",
    "start": "546519",
    "end": "551959"
  },
  {
    "text": "or nodes that are called after this will have these different state values populated if the model called a tool or",
    "start": "551959",
    "end": "559519"
  },
  {
    "text": "return a string or you know whichever one the model uses then we have a config object which will pass to um the llm",
    "start": "559519",
    "end": "566240"
  },
  {
    "text": "when we invoke it and then finally it's going to return an instance of generate",
    "start": "566240",
    "end": "571440"
  },
  {
    "text": "State and as we see we have total false and that's so we don't have to return all of the different values in this in",
    "start": "571440",
    "end": "578360"
  },
  {
    "text": "this uh class now that we defined the structure we can go ahead and Define the first part of our invoke model node",
    "start": "578360",
    "end": "585040"
  },
  {
    "text": "we're going to have a tool parser which is a Json output tools parser from the open AI tools output parsers and then a",
    "start": "585040",
    "end": "590839"
  },
  {
    "text": "prompt this prompt is going to be pretty simple your helpful assistant you got some tools you need to determine whether",
    "start": "590839",
    "end": "596079"
  },
  {
    "text": "or not the tool can hander the US user's input or return plain text and then we have a messages placeholder for the",
    "start": "596079",
    "end": "602160"
  },
  {
    "text": "input where the input in chat history will go after defining our tools parser in",
    "start": "602160",
    "end": "608920"
  },
  {
    "text": "our prompt we can go and Define our model and all the other tools we will assign to it so we can paste that in as",
    "start": "608920",
    "end": "616959"
  },
  {
    "text": "you can see we imported our gab Rebo tool our invoice tool and our weather data tool um we will imp Implement these",
    "start": "616959",
    "end": "623720"
  },
  {
    "text": "in a second uh and we've also imported our chat open AI class so we Define our model chat open AI gbt 40 uh temperature",
    "start": "623720",
    "end": "631959"
  },
  {
    "text": "zero and streaming is true we then Define our list of tools which is the get a Revo tool invoice parer tool and",
    "start": "631959",
    "end": "638279"
  },
  {
    "text": "weather data tool next we're going to bind the tools to the model so we Define a new variable model with tools and then",
    "start": "638279",
    "end": "645040"
  },
  {
    "text": "we're binding these tools to the model and finally we use our Lang train we use the Lang chain expression language to",
    "start": "645040",
    "end": "652079"
  },
  {
    "text": "pipe the initial prompt all the way to the model with tools and then invoke it",
    "start": "652079",
    "end": "657360"
  },
  {
    "text": "passing in our input and our config and we get this result which will either contain the tool calls or it will",
    "start": "657360",
    "end": "663480"
  },
  {
    "text": "contain just a plain text response now we can Implement our parsing logic so first we make sure that",
    "start": "663480",
    "end": "670880"
  },
  {
    "text": "the result is an instance of AI message it should always do that but we have this checked here just so we get this",
    "start": "670880",
    "end": "677079"
  },
  {
    "text": "typed down here um this should in theory never throw then we check to see if",
    "start": "677079",
    "end": "682480"
  },
  {
    "text": "result. tool calls is a list and if there are more than zero or if there is a tool call there if a tool call does",
    "start": "682480",
    "end": "689480"
  },
  {
    "text": "exist then we're going to parse this tool call passing in our result from the",
    "start": "689480",
    "end": "694680"
  },
  {
    "text": "chain. invoke and the config and then we're going to return tool calls with parse tools which will populate this",
    "start": "694680",
    "end": "701760"
  },
  {
    "text": "field um if tool calls were not called then we're just going to return the content as a string in the result field",
    "start": "701760",
    "end": "708360"
  },
  {
    "text": "which will populate this um and then now we can Implement our add conditional our",
    "start": "708360",
    "end": "713720"
  },
  {
    "text": "our conditional Edge which will say if result is defined and and if tool calls are defined then and uh call our invoke",
    "start": "713720",
    "end": "722240"
  },
  {
    "text": "tools node which we'll Implement after our conditional Edge so for our invoke tools or",
    "start": "722240",
    "end": "728560"
  },
  {
    "text": "return method it takes in the state and Returns the string so if result is in",
    "start": "728560",
    "end": "734440"
  },
  {
    "text": "the state and in it is an instance of string which means it would have been defined because we returned it then",
    "start": "734440",
    "end": "739760"
  },
  {
    "text": "return end and this end variable is a special variable from from L graph which indicates to L graph to finish and not",
    "start": "739760",
    "end": "746839"
  },
  {
    "text": "call any more um nodes it's essentially like setting like calling set finish point but you can dynamically call it",
    "start": "746839",
    "end": "753600"
  },
  {
    "text": "because if Ling graph CES returned to n from conditional Edge it's just going to end uh if result is not defined but tool",
    "start": "753600",
    "end": "760519"
  },
  {
    "text": "calls are defined and they are in instance of list then return tool calls Lan graph will read this and then it",
    "start": "760519",
    "end": "767079"
  },
  {
    "text": "will call the tool tool calls tool invoke tools node in theory this will never happen",
    "start": "767079",
    "end": "773720"
  },
  {
    "text": "because we should always either return a string via result or tool calls but we",
    "start": "773720",
    "end": "778880"
  },
  {
    "text": "add the this just to make it happy in case there is somehow a weird Edge case where that happens now that we've",
    "start": "778880",
    "end": "784880"
  },
  {
    "text": "implemented our conditional Edge we can implement the invoke tools function which will then process or handle",
    "start": "784880",
    "end": "791360"
  },
  {
    "text": "invoking these tools and sending the data back to the client where we can process it and send the UI components",
    "start": "791360",
    "end": "797000"
  },
  {
    "text": "over to the UI so for the invoked tools function this is somewhat similar to",
    "start": "797000",
    "end": "802839"
  },
  {
    "text": "what we saw in the server. TSX file",
    "start": "802839",
    "end": "809199"
  },
  {
    "text": "where we're mapping or adding the map tool map here",
    "start": "809279",
    "end": "817320"
  },
  {
    "text": "um it basically has a tool map with the same names of the tools and then those tools and we're going to use the state",
    "start": "817320",
    "end": "823560"
  },
  {
    "text": "to find the tool that was requested and then we can we can invoke it so what we do after this is we say if",
    "start": "823560",
    "end": "831399"
  },
  {
    "text": "tool calls is not none which means that tool calls have been returned here and",
    "start": "831399",
    "end": "836639"
  },
  {
    "text": "our conditional Edge called tool calls which which they should never be none um",
    "start": "836639",
    "end": "841759"
  },
  {
    "text": "but once again linting issue got to make it happy uh because invoke tools should in theory never be called unless there",
    "start": "841759",
    "end": "846920"
  },
  {
    "text": "already an instance of a list uh but yeah we need to make it happy by confirming that they are defined we will",
    "start": "846920",
    "end": "852959"
  },
  {
    "text": "then extract the tool from State tool calls and then just the zero with item you could update this to process",
    "start": "852959",
    "end": "859120"
  },
  {
    "text": "multiple tools that your language model returns for this demo we're only going to handle a single tool that the",
    "start": "859120",
    "end": "865000"
  },
  {
    "text": "language model selects then via our tools map tool. type type is always going to be the name of the tool um we",
    "start": "865000",
    "end": "871480"
  },
  {
    "text": "can use our tools map to find the proper tool so now we have our selected tool and then we return tool result with the",
    "start": "871480",
    "end": "877720"
  },
  {
    "text": "select tool. invoke with the RX language model supplied and that's going to populate this field and then since tool",
    "start": "877720",
    "end": "886639"
  },
  {
    "text": "invoke tools is our finish point the lane graph graph will end now we can Implement our g a repo tool and then",
    "start": "886639",
    "end": "892680"
  },
  {
    "text": "I'll just walk you through how the invoice and weather data tool are implemented they're pretty similar to get a Breo um but we'll only implement",
    "start": "892680",
    "end": "899160"
  },
  {
    "text": "the gith REO tool so in your backend you should navigate to tools",
    "start": "899160",
    "end": "905680"
  },
  {
    "text": "github.io input with two Fields owner and repo the owner will be the name of",
    "start": "922279",
    "end": "927959"
  },
  {
    "text": "the repository owner and repo is the name of the repository like Lan chain AI Lan graph and these are the fields that",
    "start": "927959",
    "end": "935000"
  },
  {
    "text": "the GI of API requires in order to fetch data about a given repo next we're going",
    "start": "935000",
    "end": "940120"
  },
  {
    "text": "to want to define the actual tool for a GitHub tool so we can we're going to",
    "start": "940120",
    "end": "945440"
  },
  {
    "text": "import tool from Lang chain core. tools so from Lang chain core. tools import",
    "start": "945440",
    "end": "954519"
  },
  {
    "text": "tool we're going to add this decorator on top of our GitHub repo um method",
    "start": "954519",
    "end": "959720"
  },
  {
    "text": "we're setting the name to get a repo which we also have here obviously so we",
    "start": "959720",
    "end": "964839"
  },
  {
    "text": "can map it properly and then the schema for this tool and return direct tool true and then our GI a repo tool takes",
    "start": "964839",
    "end": "972920"
  },
  {
    "text": "in the same inputs as here owner and repo and it returns let's add these",
    "start": "972920",
    "end": "978639"
  },
  {
    "text": "Imports object and string so now we can implement the core logic here which is going to uh hit the GI of API if it",
    "start": "978639",
    "end": "986160"
  },
  {
    "text": "returns an error then we'll return a string and if it does not return return eror we're going to return the data that the API gave us so first things first",
    "start": "986160",
    "end": "993720"
  },
  {
    "text": "we'll add our um documentation string and then implement or import OS to get",
    "start": "993720",
    "end": "1000120"
  },
  {
    "text": "the GI of token from your environment I have a read me in this repo if you want to use the tools that we provided or",
    "start": "1000120",
    "end": "1005360"
  },
  {
    "text": "that we've yeah we've provided in this repo pre-built um you're going to need a GitHub token and then for the weather",
    "start": "1005360",
    "end": "1010399"
  },
  {
    "text": "tool you're going to want this geoc code API key they're all free to get and I've added instructions in the repo on how",
    "start": "1010399",
    "end": "1016279"
  },
  {
    "text": "how to get them but then you should set them in your environment and inside this tool we're going to want to confirm that",
    "start": "1016279",
    "end": "1021440"
  },
  {
    "text": "this token is set before calling the get up API then we will Define our headers with",
    "start": "1021440",
    "end": "1027918"
  },
  {
    "text": "our environment token and the API version and the URL for the GI up API passing in the owner and repo because",
    "start": "1027919",
    "end": "1035438"
  },
  {
    "text": "this is an FST string um and now we can use requests to actually hit this URL and hopefully get back the data from our",
    "start": "1035439",
    "end": "1042120"
  },
  {
    "text": "repo if the user and the LM provided the proper owner and repo for a given",
    "start": "1042120",
    "end": "1047600"
  },
  {
    "text": "repository so what we'll do is we will wrap our request in a try and accept so",
    "start": "1047600",
    "end": "1055240"
  },
  {
    "text": "if an error is thrown we can return a string and just log the error instead of killing the whole thing what this is",
    "start": "1055240",
    "end": "1060720"
  },
  {
    "text": "going to do is it's going to try to make a get request to this URL with these headers raise for status get the data",
    "start": "1060720",
    "end": "1068160"
  },
  {
    "text": "back and then return the owner repo description stars and language this is going to be the owner of the repo the",
    "start": "1068160",
    "end": "1074120"
  },
  {
    "text": "name of the repo description if the description is set how many stars uh are",
    "start": "1074120",
    "end": "1079320"
  },
  {
    "text": "on that repo and then the primary language like python this is the end of the get a repo tool and now we can",
    "start": "1079320",
    "end": "1085559"
  },
  {
    "text": "quickly go and look at the invoice and weather tool as we can see they're pretty much the same the invoice tool",
    "start": "1085559",
    "end": "1091320"
  },
  {
    "text": "has a bit or is much more complex with the schema and that's because um it's",
    "start": "1091320",
    "end": "1097120"
  },
  {
    "text": "going to extract these fields from any image you could upload uh and then it's going to use our pre-built invoice",
    "start": "1097120",
    "end": "1104080"
  },
  {
    "text": "component on the front end to fill out any Fields like you know the line items",
    "start": "1104080",
    "end": "1109200"
  },
  {
    "text": "or the total price um shipping address from an invoice image that you update and then it just returns these",
    "start": "1109200",
    "end": "1115960"
  },
  {
    "text": "fields for the weather tool just going to hit three",
    "start": "1115960",
    "end": "1121039"
  },
  {
    "text": "apis um in order to get the city the weather for your city state country and",
    "start": "1121039",
    "end": "1126400"
  },
  {
    "text": "then today's forecast which is the temperature and then the schema is also simple city state optional countries",
    "start": "1126400",
    "end": "1133440"
  },
  {
    "text": "defaults to USA now that we've Define our tools we can Define our laying serve",
    "start": "1133440",
    "end": "1138559"
  },
  {
    "text": "and end point which we'll use as the backend server endpoint that our front end will actually connect",
    "start": "1138559",
    "end": "1143720"
  },
  {
    "text": "to for the L serve server you're going want to go to your geni backend and then the server.py file and then the first",
    "start": "1143720",
    "end": "1150400"
  },
  {
    "text": "thing we're going to want to do here is load any environment variables using thein um dependency and this will load",
    "start": "1150400",
    "end": "1158159"
  },
  {
    "text": "any enironment variables from your INF file like your open API key or open AI API key your GI up token yada y y now to",
    "start": "1158159",
    "end": "1166720"
  },
  {
    "text": "implement our um fast API for a l serve endpoint if you've ever worked with Lang serve this should be pretty familiar U",
    "start": "1166720",
    "end": "1173280"
  },
  {
    "text": "but we're going to have this start this should be named start start cly does not",
    "start": "1173280",
    "end": "1178799"
  },
  {
    "text": "make much sense um and then we're going to Define new instance of fast API which is going to return this app we're going",
    "start": "1178799",
    "end": "1184559"
  },
  {
    "text": "to give it a title of genui backend and then this is you know just the default for um Lang",
    "start": "1184559",
    "end": "1191080"
  },
  {
    "text": "serve since our backend API is going to be hosted on locally Local Host 8000 and",
    "start": "1191080",
    "end": "1197520"
  },
  {
    "text": "then our front end is Local Host 3,000 we need to add some code for Cores so that it can accept our requests um we're",
    "start": "1197520",
    "end": "1204039"
  },
  {
    "text": "going to add this import as well once we've added cores we can go",
    "start": "1204039",
    "end": "1211039"
  },
  {
    "text": "and add our route which is going to contain our runnable which we defined inside of our chain. piy file this",
    "start": "1211039",
    "end": "1218240"
  },
  {
    "text": "create graph function so we will create a new",
    "start": "1218240",
    "end": "1225200"
  },
  {
    "text": "graph add in types so L serve knows what the input and output types are we're",
    "start": "1226000",
    "end": "1231520"
  },
  {
    "text": "going to add a route SL chat it's going to be a chat type and then passing in",
    "start": "1231520",
    "end": "1237280"
  },
  {
    "text": "our runnable in our app this runnable is going to be what's called when you hit the endo and then finally start the",
    "start": "1237280",
    "end": "1244880"
  },
  {
    "text": "server here at Port 8000 as you can see we have this chat",
    "start": "1244880",
    "end": "1250280"
  },
  {
    "text": "input type here which is going to define the input type for our chat um so we're going to want to go to back end/ types",
    "start": "1250280",
    "end": "1256000"
  },
  {
    "text": "and Define this type this type is fairly simple it's our chat input type which",
    "start": "1256000",
    "end": "1261240"
  },
  {
    "text": "contains a single input which is a list of human message AI message or system messages and these are going to be our",
    "start": "1261240",
    "end": "1267840"
  },
  {
    "text": "input and chat history um that we are compiling on the client and sending over",
    "start": "1267840",
    "end": "1273520"
  },
  {
    "text": "the API to the back end once this is done your server is finished and you can",
    "start": "1273520",
    "end": "1278960"
  },
  {
    "text": "go to your console and",
    "start": "1278960",
    "end": "1285559"
  },
  {
    "text": "run or poetry Run start and this should",
    "start": "1285559",
    "end": "1291080"
  },
  {
    "text": "start your a that's right we updated that name so we need to update this file as our",
    "start": "1291080",
    "end": "1298880"
  },
  {
    "text": "poetry or Pi Project sorry to instead of trying to call",
    "start": "1298880",
    "end": "1304640"
  },
  {
    "text": "the start cly it should just call start so now but if we go back here and we run",
    "start": "1304640",
    "end": "1310640"
  },
  {
    "text": "po Run start our length serve server has started um and then we can go to our browser and go to locost 8000 docs and",
    "start": "1310640",
    "end": "1319559"
  },
  {
    "text": "we can see all the a automatically generated Swagger docs for API endpoint",
    "start": "1319559",
    "end": "1325200"
  },
  {
    "text": "and this is the stream events endpoint which we are going to be using now that we've done this we have one thing left to do or which is add the remote",
    "start": "1325200",
    "end": "1332799"
  },
  {
    "text": "runnable to our client so we can connect to this and then using our uh UI chat",
    "start": "1332799",
    "end": "1338400"
  },
  {
    "text": "box which this repo already pre-built out you just clone the repo and you can use that then we can actually start",
    "start": "1338400",
    "end": "1343760"
  },
  {
    "text": "making API requests and check out the demo so for our remote runnable you're want to go back to to the front end",
    "start": "1343760",
    "end": "1348960"
  },
  {
    "text": "directory app and agent. TSX we're then going to import server only because this is should only run in the server and",
    "start": "1348960",
    "end": "1355760"
  },
  {
    "text": "then add our API URL obviously if you're into production this should not be Local Host 8000 but for us in this demo it is",
    "start": "1355760",
    "end": "1362720"
  },
  {
    "text": "and SL chat which is this chat end point we defined here once",
    "start": "1362720",
    "end": "1368960"
  },
  {
    "text": "we've done that we can Define our agent function which takes in some inputs your input your chat history and any images",
    "start": "1368960",
    "end": "1374360"
  },
  {
    "text": "are uploaded and designate this as a server function this is similar to the or this is the inputs we saw here and",
    "start": "1374360",
    "end": "1381480"
  },
  {
    "text": "then we're going to want to create a remote runnable so we'll say const remote runnable equals new remote remote",
    "start": "1381480",
    "end": "1390600"
  },
  {
    "text": "runnable from Lan chain core runnable remote passing in the URL as the API URL",
    "start": "1390600",
    "end": "1396520"
  },
  {
    "text": "here and this is how we will have a runnable that can then connect to our Lang serve API in the back end um but",
    "start": "1396520",
    "end": "1402880"
  },
  {
    "text": "since it's a runable we can use all the nice Lan chain types and invoke and stream events that we implemented in our",
    "start": "1402880",
    "end": "1409480"
  },
  {
    "text": "stream runnable UI function here so this remote runnable is what we'll pass to this function and then we'll call stream",
    "start": "1409480",
    "end": "1415159"
  },
  {
    "text": "events on so now we can import stream runnable UI import stream runnable UI from u/s",
    "start": "1415159",
    "end": "1424279"
  },
  {
    "text": "server and then we can return stream runnable UI with the remote runnable inputs but then we need to also update",
    "start": "1424279",
    "end": "1431559"
  },
  {
    "text": "these inputs to match the proper type that the backend is",
    "start": "1431559",
    "end": "1436600"
  },
  {
    "text": "expecting so we iterate over our chat history creating a new object with a",
    "start": "1436600",
    "end": "1441960"
  },
  {
    "text": "type rooll and content of the content and then finally the input from the user",
    "start": "1441960",
    "end": "1447000"
  },
  {
    "text": "should be type human and content is inputs. input once this is done we'll be",
    "start": "1447000",
    "end": "1453080"
  },
  {
    "text": "able to use this agent function on the client um but first we need to add our or export our context so this is be",
    "start": "1453080",
    "end": "1460720"
  },
  {
    "text": "going to be able to be used so export const ends Point endpoints context",
    "start": "1460720",
    "end": "1467600"
  },
  {
    "text": "equals Expos end points passing our agent and this is using that same function we defined in our server. TSX",
    "start": "1467600",
    "end": "1473960"
  },
  {
    "text": "file which is going to add this agent function to the react context so now in our chat. TSX file which you should use",
    "start": "1473960",
    "end": "1482520"
  },
  {
    "text": "um from the repo and not really updated at all we have our use actions hook",
    "start": "1482520",
    "end": "1487919"
  },
  {
    "text": "passing our end points context which we defined here and then since we're using reacts",
    "start": "1487919",
    "end": "1495159"
  },
  {
    "text": "create context it knows it can call an agent it's then going to push these elements",
    "start": "1495159",
    "end": "1500520"
  },
  {
    "text": "to a new array with the UI that was returned from the uh stream and then",
    "start": "1500520",
    "end": "1505960"
  },
  {
    "text": "finally parse out our invoke model or invoke tools um into the chat history so",
    "start": "1505960",
    "end": "1514600"
  },
  {
    "text": "the LM has the proper chat history this is obviously implementation specific so if you're updating this for your own app",
    "start": "1514600",
    "end": "1520679"
  },
  {
    "text": "with your own um Lang graph back end you should update these to match your nodes",
    "start": "1520679",
    "end": "1525720"
  },
  {
    "text": "and kind of how you want to update your chat history finally we clean up the",
    "start": "1525720",
    "end": "1530919"
  },
  {
    "text": "inputs uh resetting our input text box and any files that were uploaded um and",
    "start": "1530919",
    "end": "1536559"
  },
  {
    "text": "then this is just the jsx which we'll render in our chat poot go to the frontend utils server. TSX file and this",
    "start": "1536559",
    "end": "1543240"
  },
  {
    "text": "is where we will Implement all the code around uh streaming UI components that we get back from the server to the",
    "start": "1543240",
    "end": "1549559"
  },
  {
    "text": "component and calling the servers um runnable via stream",
    "start": "1549559",
    "end": "1555480"
  },
  {
    "text": "events so first thing to do in this file is",
    "start": "1555480",
    "end": "1560240"
  },
  {
    "text": "import import server only and that's going to tell let's say you're using forell forell that this file should only",
    "start": "1561559",
    "end": "1567320"
  },
  {
    "text": "be ran on the server next we are going to implement this with resolvers function um essentially this has a",
    "start": "1567320",
    "end": "1574000"
  },
  {
    "text": "resolve reject function those are then assigned to a resolve and a reject function in a new promise and then it's",
    "start": "1574000",
    "end": "1579480"
  },
  {
    "text": "all returned and we have to TS ignore this because typescript thinks that resolve is being",
    "start": "1579480",
    "end": "1585480"
  },
  {
    "text": "used before it's assigned um and technically in the context of just this function that's correct however we know",
    "start": "1585480",
    "end": "1593320"
  },
  {
    "text": "that we will not use this resolve reject function before we use this promise so",
    "start": "1593320",
    "end": "1599919"
  },
  {
    "text": "in practice this is not the case next we're going to implement this expose endpoints function this is going",
    "start": "1599919",
    "end": "1606080"
  },
  {
    "text": "to take in a generic type which will then be assigned to actions this action in practice will be our lane graph agent",
    "start": "1606080",
    "end": "1612200"
  },
  {
    "text": "which we will then invoke or the remote remote runnable which will call this L graph agent on the server and then it",
    "start": "1612200",
    "end": "1618360"
  },
  {
    "text": "returns um a jsx element this jsx element is going to be",
    "start": "1618360",
    "end": "1625399"
  },
  {
    "text": "a function called AI which takes in children um of type react node so any",
    "start": "1625399",
    "end": "1630919"
  },
  {
    "text": "react node children and then it passes the actions variable here as a prop to",
    "start": "1630919",
    "end": "1636320"
  },
  {
    "text": "the AI provider which we'll look at in a second and then any children and this AI provider is essentially going to use",
    "start": "1636320",
    "end": "1642559"
  },
  {
    "text": "react create context to give context to our children which will be the elements",
    "start": "1642559",
    "end": "1647600"
  },
  {
    "text": "that we are pass passing back to the client and any actions that we want to use on the client which will be our",
    "start": "1647600",
    "end": "1654679"
  },
  {
    "text": "agent action which will then call the server um and it uses reacts create context to give context to these files",
    "start": "1654679",
    "end": "1661360"
  },
  {
    "text": "um if we look inside of our app SL layout. TSX file we see we",
    "start": "1661360",
    "end": "1668360"
  },
  {
    "text": "are also wrapping the page in this end endpoint context variable which we will Implement in just a minute uh now that",
    "start": "1668360",
    "end": "1675440"
  },
  {
    "text": "these two are implemented we can go and implement the the function which will handle actually calling the server",
    "start": "1675440",
    "end": "1680720"
  },
  {
    "text": "calling stream events on that and then processing each of the events so this function is going to be",
    "start": "1680720",
    "end": "1686880"
  },
  {
    "text": "called stream runnable UI we will add our Imports",
    "start": "1686880",
    "end": "1693279"
  },
  {
    "text": "import runnable [Music]",
    "start": "1693279",
    "end": "1698919"
  },
  {
    "text": "from score runnables and then also",
    "start": "1698919",
    "end": "1704679"
  },
  {
    "text": "import it's not getting it import compiled State graph from Lang chain",
    "start": "1704679",
    "end": "1710640"
  },
  {
    "text": "SL Lang graph so our runnable will be our remote runnable which we'll use to",
    "start": "1710640",
    "end": "1716480"
  },
  {
    "text": "hit our server endpoint uh this remote runnable we're going to call stream events on so we get each of the events",
    "start": "1716480",
    "end": "1722640"
  },
  {
    "text": "or all the events that our server streams back and then we're going to have a set of inputs these inputs are going to be things like the user input",
    "start": "1722640",
    "end": "1729320"
  },
  {
    "text": "and chat history which will then pass to a runable when we invoke it the first thing we want to do in this function is",
    "start": "1729320",
    "end": "1735399"
  },
  {
    "text": "create a new streamable UI which we can import this fun function from the aisk this create streamable UI function is",
    "start": "1735399",
    "end": "1741760"
  },
  {
    "text": "what we will use to actually stream back these components from a react server component to the client and then we're",
    "start": "1741760",
    "end": "1748919"
  },
  {
    "text": "going to use our with resolvers function we defined to get our last event and resolve which we will resolve and await",
    "start": "1748919",
    "end": "1755760"
  },
  {
    "text": "a little bit later next we're going to implement this ASN function which we're calling let's add our Imports this has a",
    "start": "1755760",
    "end": "1764039"
  },
  {
    "text": "last event value which we will assign at the end of each stream event we it over so that this will always contain the",
    "start": "1764039",
    "end": "1770039"
  },
  {
    "text": "last event we're then going to use this a little bit later on um after we resolve our promise on the client so we",
    "start": "1770039",
    "end": "1775919"
  },
  {
    "text": "know when the last event is resolved because this function will resolve before add this import this function or",
    "start": "1775919",
    "end": "1783279"
  },
  {
    "text": "this asnc function that is returned will resolve um before the actual API call is",
    "start": "1783279",
    "end": "1789919"
  },
  {
    "text": "finished so we need to assign each of the events to that so that the last event will be in this variable and then",
    "start": "1789919",
    "end": "1796600"
  },
  {
    "text": "when we await our last event will be to access our last event on the client even though the async function would have",
    "start": "1796600",
    "end": "1801960"
  },
  {
    "text": "already resolved we also have this callbacks object which is an object containing a",
    "start": "1801960",
    "end": "1807399"
  },
  {
    "text": "string and then either create runnable UI or sorry create streamable UI or create streamable value this is going to",
    "start": "1807399",
    "end": "1814159"
  },
  {
    "text": "be an object which tracks which streamed events we've um processed already the",
    "start": "1814159",
    "end": "1819799"
  },
  {
    "text": "string will be the ID of that stream event and the return type will be the UI stream which is getting sent back to the",
    "start": "1819799",
    "end": "1825640"
  },
  {
    "text": "client which corresponds to that event so could be a tool call or it could be",
    "start": "1825640",
    "end": "1830760"
  },
  {
    "text": "um just a plain text llm response after this we need to go up above this function and Define two types",
    "start": "1830760",
    "end": "1837559"
  },
  {
    "text": "and then one object map let's add our Imports",
    "start": "1837559",
    "end": "1842600"
  },
  {
    "text": "first why is that deprecated that's because we import it from the wrong place um we need to add",
    "start": "1842600",
    "end": "1850080"
  },
  {
    "text": "this here as well so these are some pre-built components rendering like a GitHub repo card we have a loading",
    "start": "1850080",
    "end": "1857159"
  },
  {
    "text": "component for that as well and then the actual component which takes them props um these are all just",
    "start": "1857159",
    "end": "1862639"
  },
  {
    "text": "normal react components even though we're using them on the react Ser components on the server they're normal",
    "start": "1862639",
    "end": "1868000"
  },
  {
    "text": "react components that'll get streamed back to the client so you can essentially stream back any component that you would build in react and they",
    "start": "1868000",
    "end": "1874399"
  },
  {
    "text": "can have state they can connect apis um and that's kind of what makes this so powerful is you can use actual react",
    "start": "1874399",
    "end": "1880200"
  },
  {
    "text": "components that can have their own life inside of them so you can stream this back to the client you get a new UI",
    "start": "1880200",
    "end": "1886600"
  },
  {
    "text": "component on your client that user or CES and that UI component can be very Dynamic and stateful and",
    "start": "1886600",
    "end": "1892720"
  },
  {
    "text": "whatnot um but those are pre-built and we have this map here tool component map we will use this as our tool component",
    "start": "1892720",
    "end": "1899159"
  },
  {
    "text": "map here so when we get an event back which matches the name of our tool we can then map it to the Loading component",
    "start": "1899159",
    "end": "1905039"
  },
  {
    "text": "and the final component um there will be a different event which we'll Implement in a second which checks if it's a if it",
    "start": "1905039",
    "end": "1910880"
  },
  {
    "text": "should be the loading component get stream back or the final component gets stream back and then you can pass any props to these components",
    "start": "1910880",
    "end": "1919039"
  },
  {
    "text": "now we're going to Define two variables selected tool component and selected tool UI these are going to keep track of",
    "start": "1919039",
    "end": "1925480"
  },
  {
    "text": "the individual component and the UI stream which we've implemented to stream the components back to the client that's",
    "start": "1925480",
    "end": "1932320"
  },
  {
    "text": "because after this we're going to be iterating over stream events and we need these variables to be outside of each event so we have access to them in all",
    "start": "1932320",
    "end": "1939320"
  },
  {
    "text": "the subsequent events after they've already been assigned um but now we can implement the stream",
    "start": "1939320",
    "end": "1944639"
  },
  {
    "text": "events that's just going to call runnable Dost stream events with the V1 version passing any inputs this runnable",
    "start": "1944639",
    "end": "1951000"
  },
  {
    "text": "is the same runable that gets passed in here which will be well we'll implement we will implement this in a second but",
    "start": "1951000",
    "end": "1956600"
  },
  {
    "text": "it's essentially going to be a remote runnable function which calls our Lang serve python server um and now we can",
    "start": "1956600",
    "end": "1962760"
  },
  {
    "text": "iterate over all of the stream events and extract the different events that we want to then either update our UI or",
    "start": "1962760",
    "end": "1970720"
  },
  {
    "text": "update these variables or callbacks and whatnot so really quick we're going to",
    "start": "1970720",
    "end": "1976159"
  },
  {
    "text": "extract the output and and the Chunk from our stream event. data and then the type of event which we will use a little",
    "start": "1976159",
    "end": "1982399"
  },
  {
    "text": "bit later on now we're going to implement our handle invoke model event this handles",
    "start": "1982399",
    "end": "1989000"
  },
  {
    "text": "the invoke model event by checking for the tool calls in the output if a tool call is found and no tool component is selected yet it selects the selected",
    "start": "1989000",
    "end": "1995639"
  },
  {
    "text": "tool component based on the tool type and Depends the loading state to the UI so what this is going to do is we will",
    "start": "1995639",
    "end": "2001240"
  },
  {
    "text": "call this if the streamed event is the invoke model um node when we do",
    "start": "2001240",
    "end": "2007480"
  },
  {
    "text": "implement our python backend one of the nodes in our lane graph graph is going to be invoke model and this is the",
    "start": "2007480",
    "end": "2013840"
  },
  {
    "text": "function which is going to process any events streamed after um that invoke model is",
    "start": "2013840",
    "end": "2020840"
  },
  {
    "text": "called now for the body of this function we first check to see if tool calls is in the output um and if output. tool",
    "start": "2021120",
    "end": "2029480"
  },
  {
    "text": "calls length is greater than zero so if there are more than if there are one if there is one tool call then we're going",
    "start": "2029480",
    "end": "2035120"
  },
  {
    "text": "to extract that tool call this is the invoke model so it's going to be this first step um and this conditional node",
    "start": "2035120",
    "end": "2041000"
  },
  {
    "text": "will either return a tool or string if returns a tool then this should get caught we extract that tool and then if",
    "start": "2041000",
    "end": "2048679"
  },
  {
    "text": "these two variables have not been assigned yet then we're going to find the component in the component map",
    "start": "2048679",
    "end": "2054158"
  },
  {
    "text": "create a new stream whe UI passing in the initial value as the loading",
    "start": "2054159",
    "end": "2059358"
  },
  {
    "text": "component for that component and this is going to then update we're then then going to pass the stream streamable ui.",
    "start": "2059359",
    "end": "2066599"
  },
  {
    "text": "value to our our create streamable UI which is getting which is going to get sent back",
    "start": "2066599",
    "end": "2072320"
  },
  {
    "text": "to the client um with the value of our new great streamable",
    "start": "2072320",
    "end": "2077800"
  },
  {
    "text": "UI which will be our loading component for the first event the next function we want to",
    "start": "2077800",
    "end": "2083358"
  },
  {
    "text": "process or sorry the event we want to process is the invoke tools event um we're going to update the selected tools",
    "start": "2083359",
    "end": "2089040"
  },
  {
    "text": "UI with the final State sorry with the final State and Tool result data that",
    "start": "2089040",
    "end": "2094839"
  },
  {
    "text": "will be from this node um and it takes an input handle invoke tools event so",
    "start": "2094839",
    "end": "2100680"
  },
  {
    "text": "now it's going to be pretty similar to this where we're going to take the event of this tool node and update the UI but",
    "start": "2100680",
    "end": "2106079"
  },
  {
    "text": "using these already defined uh variables so if selected tool UI is true",
    "start": "2106079",
    "end": "2112960"
  },
  {
    "text": "and selected tool component are true which they should always be because the invoke tool node should never be called",
    "start": "2112960",
    "end": "2119040"
  },
  {
    "text": "until the invoke model tool is called which we'll see when we pl our python server then we're going to want to get",
    "start": "2119040",
    "end": "2125079"
  },
  {
    "text": "the data from the output here via the tool result and then tool ui. done with",
    "start": "2125079",
    "end": "2131240"
  },
  {
    "text": "the selected component which we assigned here and then the final version of that",
    "start": "2131240",
    "end": "2136280"
  },
  {
    "text": "component passing in any props so for example let's say we have our weather tool it's then going to use the uh UI",
    "start": "2136280",
    "end": "2144040"
  },
  {
    "text": "stream for the weather tool find the final version of that component which is",
    "start": "2144040",
    "end": "2149359"
  },
  {
    "text": "the current weather pass in any props to it and then update that stream and call",
    "start": "2149359",
    "end": "2154400"
  },
  {
    "text": "done to end the stream um updating the weather component that is already being rendered on the",
    "start": "2154400",
    "end": "2159680"
  },
  {
    "text": "UI now the last function we want to implement is going to be handle chat",
    "start": "2159680",
    "end": "2164720"
  },
  {
    "text": "model stream event and that's going to be if the language model just um does",
    "start": "2164720",
    "end": "2169760"
  },
  {
    "text": "not pick a tool and is only stream back text it's going to stream back all of those text Chunk chunks and we're going",
    "start": "2169760",
    "end": "2175079"
  },
  {
    "text": "to want to extract those to then stream them again to our UI so handles the on chat mod stream",
    "start": "2175079",
    "end": "2181920"
  },
  {
    "text": "event by creating a new text stream from the for the AI message if one does not already exist and for the current ID",
    "start": "2181920",
    "end": "2188599"
  },
  {
    "text": "then it pends the chunk to the cont content um and then app pends the chunk content to the corresponding text Stream",
    "start": "2188599",
    "end": "2195760"
  },
  {
    "text": "So the value of this function is going to be this we're going to use our callbacks object here after we add our",
    "start": "2195760",
    "end": "2204760"
  },
  {
    "text": "import and we're going to say if callbacks um if the Run ID for the stream event does not exist in our",
    "start": "2204760",
    "end": "2210960"
  },
  {
    "text": "callback object then create a new text stream we want to create a text stream because this bypasses some um back in",
    "start": "2210960",
    "end": "2218040"
  },
  {
    "text": "that the create runnable UI does uh because we're only stream back text so we create our text stream use our stream",
    "start": "2218040",
    "end": "2225720"
  },
  {
    "text": "or sorry create streamable UI and add our AI message which will look like our",
    "start": "2225720",
    "end": "2232839"
  },
  {
    "text": "you know AI message text bubble and the value of that is going to be the text stream and then we are going to set this",
    "start": "2232839",
    "end": "2238359"
  },
  {
    "text": "callback object with the Run ID to this value of the text stream then if we set",
    "start": "2238359",
    "end": "2243960"
  },
  {
    "text": "that or if it was already set then we're going to check make sure it's it exists and then append any of the content from",
    "start": "2243960",
    "end": "2251240"
  },
  {
    "text": "the Stream So each chunk of the LM streams will be chunk. content and we will append that to our text stream",
    "start": "2251240",
    "end": "2258319"
  },
  {
    "text": "value which will then stream each text and update the UI message as those",
    "start": "2258319",
    "end": "2263560"
  },
  {
    "text": "chunks come in now we've implemented these functions we're going to want to implement our if else statements um on",
    "start": "2263560",
    "end": "2270040"
  },
  {
    "text": "the different stream events so we can get the proper events and up call the",
    "start": "2270040",
    "end": "2276000"
  },
  {
    "text": "the functions which are required for those events so the first one we want to implement is if the type is end so that",
    "start": "2276000",
    "end": "2283359"
  },
  {
    "text": "means if the chain has ended and the type of output as an object we first check to see if the stream event. name",
    "start": "2283359",
    "end": "2288839"
  },
  {
    "text": "is invoke model if it was invoke model then we want to handle the invoke model event passing in the output and if this",
    "start": "2288839",
    "end": "2295800"
  },
  {
    "text": "or if the stream event was invoked tools then we call the invoked tools event makes sense passing in the object the",
    "start": "2295800",
    "end": "2303040"
  },
  {
    "text": "last function we need we need to add an if statement for is the chat model stream so those are not going to be tool",
    "start": "2303040",
    "end": "2308640"
  },
  {
    "text": "nodes instead they're going to be on chunk model streams so we're going to say if the event is on chat model stream",
    "start": "2308640",
    "end": "2315720"
  },
  {
    "text": "the chunk is true and the type of Chunk is an object then handle the chat model stream and then finally at the end of",
    "start": "2315720",
    "end": "2322280"
  },
  {
    "text": "our let me collapse these once we're at the end of our stream event iteration we",
    "start": "2322280",
    "end": "2328960"
  },
  {
    "text": "assign the last of value to the stream event and this is so this value is always going to be the last stream once",
    "start": "2328960",
    "end": "2335119"
  },
  {
    "text": "the stream exits finally we're going to clean all this up so using our resolve function return",
    "start": "2335119",
    "end": "2341960"
  },
  {
    "text": "from our with resolvers we're going to pass in the data. output from the last event so this is going to be the last",
    "start": "2341960",
    "end": "2348079"
  },
  {
    "text": "value from our stream um if it was text it's going to be text if it was the result of a tool it's going to be a tool",
    "start": "2348079",
    "end": "2353520"
  },
  {
    "text": "that data we will set when we Implement our python backend we're then going to iterate over all of our",
    "start": "2353520",
    "end": "2360640"
  },
  {
    "text": "callbacks and call done on each of them which is going to call this stream do",
    "start": "2360640",
    "end": "2367760"
  },
  {
    "text": "sorry stream. even though we're calling UI and that's just so this um create",
    "start": "2367760",
    "end": "2373680"
  },
  {
    "text": "streamable value stream finishes and then call UI Doone and that's for this",
    "start": "2373680",
    "end": "2380760"
  },
  {
    "text": "create streamable UI and it's going to end the stream streaming UI components back to the",
    "start": "2380760",
    "end": "2386400"
  },
  {
    "text": "client finally outside of this async function we're going to want to return the value of our UI stream this is going",
    "start": "2386400",
    "end": "2392400"
  },
  {
    "text": "to be the jsx element which we'll render on the client and then the last event",
    "start": "2392400",
    "end": "2397440"
  },
  {
    "text": "right here which is that promise that we can resolve once our stream events have finished resolving and then get the",
    "start": "2397440",
    "end": "2403960"
  },
  {
    "text": "value of the last event now everything is finished we can go back to our terminal and we can run yarn",
    "start": "2403960",
    "end": "2411520"
  },
  {
    "text": "Dev this will start up a server at locost 3000 we can go to our UI reload",
    "start": "2411520",
    "end": "2417560"
  },
  {
    "text": "this page and we should see our generative UI application that we just built and we say something like what's",
    "start": "2417560",
    "end": "2424599"
  },
  {
    "text": "the weather in SF send that over boom we get back our",
    "start": "2424599",
    "end": "2430280"
  },
  {
    "text": "loading component it recognized that it was in San Francisco California um as we saw it selected the tool sent that back",
    "start": "2430280",
    "end": "2436119"
  },
  {
    "text": "to the client that was a map to our loading component that was rendered here and then once the weather API was or had",
    "start": "2436119",
    "end": "2443440"
  },
  {
    "text": "resolved it then sent that data back again and it updated this component with the proper data so we can also say",
    "start": "2443440",
    "end": "2449839"
  },
  {
    "text": "something like what's the info on linkchain AI SL",
    "start": "2449839",
    "end": "2457480"
  },
  {
    "text": "graph we send that over it should select our GitHub tool we saw it was loading for a second and now we have our GitHub",
    "start": "2457480",
    "end": "2463520"
  },
  {
    "text": "um repo component here which has the um description and the language and all the",
    "start": "2463520",
    "end": "2469880"
  },
  {
    "text": "Stars this is you know react component so it's interactable we can click on the star button and it takes us to the L",
    "start": "2469880",
    "end": "2476119"
  },
  {
    "text": "graph repo and we see that the um description and stars all",
    "start": "2476119",
    "end": "2482800"
  },
  {
    "text": "matches so before we finish the last thing I want to do is show you the Lang Smith Trace as we see this is a link",
    "start": "2482800",
    "end": "2488960"
  },
  {
    "text": "serve endpoint / chat it passes in the input the tool calls and then the most",
    "start": "2488960",
    "end": "2494079"
  },
  {
    "text": "recent input as we can see the output contains tool calls and Tool result which we use to update our um chat",
    "start": "2494079",
    "end": "2501400"
  },
  {
    "text": "message history but it calls invoke model as the first node in Lang graph as we can see obviously there's no inputs",
    "start": "2501400",
    "end": "2507760"
  },
  {
    "text": "for these because they have not been called yet um but it does contain the messages input field that then calls our",
    "start": "2507760",
    "end": "2514920"
  },
  {
    "text": "chat model our chat model is Prov provided with some tools it's selected to get a repo tool which is what we want",
    "start": "2514920",
    "end": "2520560"
  },
  {
    "text": "because we asked about to get a repo return the values for that that then got par passed to our output parser and then",
    "start": "2520560",
    "end": "2528319"
  },
  {
    "text": "our invoke tools or return uh conditional Edge which obviously we invoke tools so it's then going to call",
    "start": "2528319",
    "end": "2534760"
  },
  {
    "text": "the invoke tools node which invoked our tool was while it was invoking our tool",
    "start": "2534760",
    "end": "2540040"
  },
  {
    "text": "it was stringing back the name of the tool which we used to send the loading component to the client then after it hit the Gib API it streamed back the fin",
    "start": "2540040",
    "end": "2547400"
  },
  {
    "text": "result of our tool as we can see here and then that on our client was used to",
    "start": "2547400",
    "end": "2552839"
  },
  {
    "text": "um update the component with the final data and then since invoke tools was the last node it finished and that is it for",
    "start": "2552839",
    "end": "2560079"
  },
  {
    "text": "this demo on building Lang graph or sorry generative UI with python and react front end um if you are interested",
    "start": "2560079",
    "end": "2567640"
  },
  {
    "text": "in the types video which is just the same demo as this but with a full typescript app that'll Link in the",
    "start": "2567640",
    "end": "2573040"
  },
  {
    "text": "description and I hope you all have a better understanding of how to build gener applications with linkchain now",
    "start": "2573040",
    "end": "2581799"
  }
]