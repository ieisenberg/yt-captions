[
  {
    "start": "0",
    "end": "317000"
  },
  {
    "text": "I always enjoy doing these uh deep dive webinars and this one's gonna be a really fun one um we've got",
    "start": "0",
    "end": "6060"
  },
  {
    "text": "um Andrew from shortwave um do you want to do quick intro of yourself Andrew",
    "start": "6060",
    "end": "11280"
  },
  {
    "text": "sure first of all thank you for having me uh here I'm excited to talk to everyone and show you what we've been",
    "start": "11280",
    "end": "16619"
  },
  {
    "text": "doing um I won't give you my whole life story but the thing I'm probably best known for was before this I founded a company",
    "start": "16619",
    "end": "22199"
  },
  {
    "text": "called Firebase uh that was eventually bought by Google that does uh a lot of",
    "start": "22199",
    "end": "27300"
  },
  {
    "text": "infrastructure for mobile and web developers so this this is not a developer product it's a little different for me uh it's more of a kind",
    "start": "27300",
    "end": "33960"
  },
  {
    "text": "of a prosumer product um but it has some really cool infrastructure behind it as well and I'm",
    "start": "33960",
    "end": "40140"
  },
  {
    "text": "excited to talk about it awesome so yeah we're gonna we're just gonna deep dive on this for basically",
    "start": "40140",
    "end": "46440"
  },
  {
    "text": "the the whole webinar um I'll be asking questions throughout but if you guys have questions that you",
    "start": "46440",
    "end": "51600"
  },
  {
    "text": "want answered drop them in either the chat if they're if they're relevant to kind of like uh things that are going on",
    "start": "51600",
    "end": "58199"
  },
  {
    "text": "on the screen or there's a little box on the right the Q a box with the question mark drop them in there if they're just",
    "start": "58199",
    "end": "64140"
  },
  {
    "text": "kind of like General higher level questions um and we'll we'll get to those at the end and you can also upload ones that",
    "start": "64140",
    "end": "69840"
  },
  {
    "text": "you want to hear answered so um yeah you know this will be pretty",
    "start": "69840",
    "end": "75000"
  },
  {
    "text": "interactive I'll be I'll be interrupting uh Andrew a bunch hopefully you guys will be jumping in with questions as",
    "start": "75000",
    "end": "80759"
  },
  {
    "text": "well um and yeah I don't know I'm always really excited by these so um with that",
    "start": "80759",
    "end": "86100"
  },
  {
    "text": "I think and this is being recorded this is being recorded by the way um and it will be available at the link and then we'll put it up on YouTube",
    "start": "86100",
    "end": "92100"
  },
  {
    "text": "afterwards um I think those are the only Logistics so let's just let's just dive into it",
    "start": "92100",
    "end": "97259"
  },
  {
    "text": "Andrew sounds good so I wanna I wanna say first that this is my actual inbox right here",
    "start": "97259",
    "end": "103079"
  },
  {
    "text": "this isn't like a demo I prepared or something like these are my actual emails so if you email me right now",
    "start": "103079",
    "end": "108380"
  },
  {
    "text": "android.com it will show up here um maybe I shouldn't say that out loud um but this is this is my really unbox",
    "start": "108380",
    "end": "114540"
  },
  {
    "text": "this is a live demo here so uh the thing that we launched last week that everyone would be so excited about is this button",
    "start": "114540",
    "end": "120960"
  },
  {
    "text": "up here in the corner um that you can click and it gives you this little panel where you can talk to",
    "start": "120960",
    "end": "127320"
  },
  {
    "text": "a conversational agent um and I think the the reason Harrison",
    "start": "127320",
    "end": "133200"
  },
  {
    "text": "invited me on here is because I talked a bit on Twitter about how this thing works because it's uh it's a bit more I",
    "start": "133200",
    "end": "138480"
  },
  {
    "text": "think than many of the examples of this you might have seen in the past where we've I think we've done two things differently one is we've like really",
    "start": "138480",
    "end": "144720"
  },
  {
    "text": "deeply integrated into the actual ux of the product so it you know for example it knows if I open an email like it",
    "start": "144720",
    "end": "152160"
  },
  {
    "text": "knows what email is open and they can see it um and it knows if I started typing a draft it knows about the draft and give",
    "start": "152160",
    "end": "158340"
  },
  {
    "text": "me a suggestions so at a ux level it's like very deeply integrated but the other thing we've done is we've used AI",
    "start": "158340",
    "end": "164300"
  },
  {
    "text": "uh to not just answer the questions but also to go look up the right data to",
    "start": "164300",
    "end": "169860"
  },
  {
    "text": "include the prompt answer those questions so we it gives the agent the appearance of having a full",
    "start": "169860",
    "end": "175019"
  },
  {
    "text": "understanding of all the emails that you've ever sent or received and your calendar and the the settings that you",
    "start": "175019",
    "end": "181920"
  },
  {
    "text": "have and the shortwave documentation and stuff like that so it ends up feeling much smarter than other agents like this",
    "start": "181920",
    "end": "188580"
  },
  {
    "text": "you might have tried so so what does that mean like what like what does it mean for the agent to go look up the",
    "start": "188580",
    "end": "193980"
  },
  {
    "text": "information and like how is that done behind the scenes well let me let me start with a little",
    "start": "193980",
    "end": "199860"
  },
  {
    "text": "demo here so uh let's let's start with a simple question so I'll say like who is Harrison by the way I have not tried",
    "start": "199860",
    "end": "207180"
  },
  {
    "text": "these discussions before so let's just we'll see what happens um and you're gonna see it's gonna",
    "start": "207180",
    "end": "212519"
  },
  {
    "text": "actually tell us a little bit about like what's going on so right now it's off searching for Relevant emails signing some is giving me an answer and this",
    "start": "212519",
    "end": "219360"
  },
  {
    "text": "answer is based on my actual email history so it doesn't know necessarily",
    "start": "219360",
    "end": "225659"
  },
  {
    "text": "because my email is not super clear but it's able to figure some things out based on what I found so it knows we've",
    "start": "225659",
    "end": "230819"
  },
  {
    "text": "talked on Twitter and email it knows uh it's you know here's some threads that that were involved",
    "start": "230819",
    "end": "237480"
  },
  {
    "text": "um and if I click on these it'll tell me so I can click here and it'll it'll show me like this is the thing that it actually",
    "start": "237480",
    "end": "243299"
  },
  {
    "text": "found that gave me some information and like here's some other ones as well um the uh the way this works at a high",
    "start": "243299",
    "end": "252239"
  },
  {
    "text": "level here um is then I'll dive into a diagram a little",
    "start": "252239",
    "end": "257820"
  },
  {
    "text": "bit here but at a high level What's Happening Here is we're taking an llm or looking at the question that you're answering you're asking we're saying hey",
    "start": "257820",
    "end": "264960"
  },
  {
    "text": "what what's the intent of the user here what are they trying to do and if it appears that they're trying to ask a question that would be answered by their",
    "start": "264960",
    "end": "271740"
  },
  {
    "text": "email history um it will then go off and it will call into our back end where we have a whole",
    "start": "271740",
    "end": "278040"
  },
  {
    "text": "pipeline that uses a combination of a bunch of different things some AI things",
    "start": "278040",
    "end": "283259"
  },
  {
    "text": "some non-ai things to pull up all the emails we could find that might be related to that bring them back to client stuff in the prompt uh for the",
    "start": "283259",
    "end": "290220"
  },
  {
    "text": "for the response that you're getting",
    "start": "290220",
    "end": "293600"
  },
  {
    "text": "so there's some routing going on to choose kind of like the right retriever to use and then you guys have like an",
    "start": "295380",
    "end": "301199"
  },
  {
    "text": "email retriever because you guys own you guys own the whole email kind of the whole email system and you've basically",
    "start": "301199",
    "end": "306900"
  },
  {
    "text": "now hooked into that um do some retrieval get those emails back",
    "start": "306900",
    "end": "312419"
  },
  {
    "text": "and and then kind of like do some do some rag with those emails precisely so yeah why don't I why don't",
    "start": "312419",
    "end": "319020"
  },
  {
    "start": "317000",
    "end": "615000"
  },
  {
    "text": "I dive into a little dog animate here and this is this is a very simplified diagram but the the basic system",
    "start": "319020",
    "end": "325560"
  },
  {
    "text": "works like this um so let's say you get a new query that comes",
    "start": "325560",
    "end": "330720"
  },
  {
    "text": "in um we take that query and we send it off to an llm with a prompt that we've",
    "start": "330720",
    "end": "336539"
  },
  {
    "text": "written and in we call this tool selection this is where we say hey find",
    "start": "336539",
    "end": "341820"
  },
  {
    "text": "me the things that matter and in that prompt we we don't just include the question we also include a bunch of",
    "start": "341820",
    "end": "347759"
  },
  {
    "text": "contextual information about what's going on with the user's email client right now to help disambiguate what the",
    "start": "347759",
    "end": "355560"
  },
  {
    "text": "person is talking about so for example if I have a thread open at the time and it's about playing chain and I say hey",
    "start": "355560",
    "end": "362759"
  },
  {
    "text": "find me similar threads um if someone was sitting next to me they would know I'm probably thinking",
    "start": "362759",
    "end": "368880"
  },
  {
    "text": "about Lang chain I probably want to look up some Lang chain stuff but uh if they weren't sitting next to me they wouldn't",
    "start": "368880",
    "end": "373979"
  },
  {
    "text": "necessarily know that and we want to give that appearance and so in the tool selection we have we have the history of",
    "start": "373979",
    "end": "379080"
  },
  {
    "text": "the conversation you've had we have a prop that we've written but then we also have like other information about the state of the world you know your time",
    "start": "379080",
    "end": "385560"
  },
  {
    "text": "zone and and some settings and the threads that are open and the drafts that you've written and stuff like that so that it's able to figure out what",
    "start": "385560",
    "end": "393060"
  },
  {
    "text": "you're talking about like if you say this it knows what this is we then go off and from that tool",
    "start": "393060",
    "end": "399060"
  },
  {
    "text": "section we go and we we load a bunch of things in parallel we have we have more than three tools there's a bunch of tools but each one of those is is kind",
    "start": "399060",
    "end": "406380"
  },
  {
    "text": "of a separate code path which is great from a modularity perspective I think one of the things we really struggled with is like how do you make an AI",
    "start": "406380",
    "end": "412979"
  },
  {
    "text": "system not just a total black box like how do you how do you put like normal computer science into it",
    "start": "412979",
    "end": "419460"
  },
  {
    "text": "um and break things apart and this is one of the things that lets us do this so each one of these pipelines has a very simple responsibility which is",
    "start": "419460",
    "end": "425160"
  },
  {
    "text": "find the right data to stuff in the main prompt at the end related to this particular type of question",
    "start": "425160",
    "end": "431819"
  },
  {
    "text": "um and we have we have a bunch of those and I'll go into the AI search form which is the most interesting one I think later",
    "start": "431819",
    "end": "436919"
  },
  {
    "text": "and then the output of those gets fed into a prompt at the end um that it you know includes you know",
    "start": "436919",
    "end": "444479"
  },
  {
    "text": "some wording that we wrote to make it sound appropriate for an email assistant um uh but then uh all the stuff that we",
    "start": "444479",
    "end": "450900"
  },
  {
    "text": "pulled from from that information and some information also will be extracted at tool selection time puts it all together calls off to an llm",
    "start": "450900",
    "end": "458280"
  },
  {
    "text": "um in this case we're using gpd4 and we get back you know the streaming output from that we do some prose processing on",
    "start": "458280",
    "end": "464819"
  },
  {
    "text": "it we like linkify things and we do and we do like uh markdown formatting and a few other steps there and you get your",
    "start": "464819",
    "end": "471599"
  },
  {
    "text": "response so this is kind of the system the system end to end here pull a few questions you should dive in so you",
    "start": "471599",
    "end": "477419"
  },
  {
    "text": "mentioned using gpt4 for the response do you also use that for the tool selection as well",
    "start": "477419",
    "end": "482940"
  },
  {
    "text": "we we do right now we tried to use we tried a bunch of different things here",
    "start": "482940",
    "end": "488759"
  },
  {
    "text": "um the the downside of gpd4 is twofold one it's kind of slow um and so you'll notice when you in our",
    "start": "488759",
    "end": "495599"
  },
  {
    "text": "app if you press enter there's going to be a little dot dot dot there for a second for like 600 milliseconds and then it switches over to say like you",
    "start": "495599",
    "end": "502020"
  },
  {
    "text": "know answer your question or looking up emails or something and that's that's our tool selection running and one is",
    "start": "502020",
    "end": "507060"
  },
  {
    "text": "console the other it's really expensive to do this because we're not just you know every time you ask a question we're",
    "start": "507060",
    "end": "513599"
  },
  {
    "text": "not making one llm call we're actually making like 10. um because a bunch of the tools also do llm calls as well so",
    "start": "513599",
    "end": "519959"
  },
  {
    "text": "there's a lot of there's a lot of pressing going on every time you do this to make this thing work but uh we we",
    "start": "519959",
    "end": "526560"
  },
  {
    "text": "tried you know Palm we tried DP 3.5 we tried Claude instant",
    "start": "526560",
    "end": "532980"
  },
  {
    "text": "um we tried some open source stuff like we tried all kinds of things and what we discovered was if you really wanted to",
    "start": "532980",
    "end": "539700"
  },
  {
    "text": "be able to disambiguate uh like if you want to mix",
    "start": "539700",
    "end": "545100"
  },
  {
    "text": "with the user doesn't have to like very carefully phrase everything they say to this and figure what they're talking about if you want to really feel smart",
    "start": "545100",
    "end": "551399"
  },
  {
    "text": "um gpd4 was really the only one that we could get to work uh across like a wide range of tools across a wide different",
    "start": "551399",
    "end": "557279"
  },
  {
    "text": "type of set of inputs and so that's what we're using that makes sense and then I'm curious about the tool selection bit",
    "start": "557279",
    "end": "563100"
  },
  {
    "text": "as well because I think that's something people are building on top of LinkedIn are really interested in like do you",
    "start": "563100",
    "end": "568920"
  },
  {
    "text": "select multiple of them and then what are the what are the inputs to the tools like do you have the in do you have the",
    "start": "568920",
    "end": "574980"
  },
  {
    "text": "llm generate an input to the tool or is the input to the tool always kind of like the previous contextual information",
    "start": "574980",
    "end": "580740"
  },
  {
    "text": "that you that you use for the tool selection anyways yeah so",
    "start": "580740",
    "end": "586440"
  },
  {
    "text": "um the let's start let's start with uh talking about what the what the inputs are to the tools are here so",
    "start": "586440",
    "end": "592320"
  },
  {
    "text": "um it depends on the Tool uh for some of them are very simple so for example for for documentation right now we're uh we",
    "start": "592320",
    "end": "599040"
  },
  {
    "text": "do the very simple thing which is anytime we think you're talking about the shortwave docs we just embed like",
    "start": "599040",
    "end": "604560"
  },
  {
    "text": "a single version of that which is probably more expensive than it needs to be but like there's no arguments at all it's just like oh you're talking about",
    "start": "604560",
    "end": "609660"
  },
  {
    "text": "the docs here's all of the docs we're gonna stop the whole thing in the prompt um uh other ones are a bit more complex",
    "start": "609660",
    "end": "616440"
  },
  {
    "start": "615000",
    "end": "720000"
  },
  {
    "text": "the most interesting one and I have a diagram here for this as well is our AI search so this is where we shine so",
    "start": "616440",
    "end": "623040"
  },
  {
    "text": "there's there's a bunch of folks out there that have like talk to your individual email uh features there's a",
    "start": "623040",
    "end": "628860"
  },
  {
    "text": "bunch of them that can do like run basic like gmail search like Bard put out a",
    "start": "628860",
    "end": "634019"
  },
  {
    "text": "thing where like you can like look up emails um and those you know maybe doing some",
    "start": "634019",
    "end": "639540"
  },
  {
    "text": "aspects of this but no one that we know of has this piece which is let's actually use a bunch of AI Tech to do",
    "start": "639540",
    "end": "645660"
  },
  {
    "text": "lookup and the way this works is the the arguments to this are uh",
    "start": "645660",
    "end": "653420"
  },
  {
    "text": "well it starts with a thing called query formulation where the arguments for this are is the same inputs that we have for",
    "start": "653420",
    "end": "658980"
  },
  {
    "text": "the tools it's like all the contents the state of the world of like what threads are open what draft you have in your settings in the time zone and all of",
    "start": "658980",
    "end": "664800"
  },
  {
    "text": "that um and this time rather than us asking the LOM what you know what tools for you",
    "start": "664800",
    "end": "671640"
  },
  {
    "text": "to answer this question we are uh asking the llm hey take this whole giant prompt and and",
    "start": "671640",
    "end": "680459"
  },
  {
    "text": "this user's long conversation there might be like multiple messages sent and I want you to get an instance down to the single question",
    "start": "680459",
    "end": "686279"
  },
  {
    "text": "right that includes all of the relevant information that you need to answer this um and the reason for it is that for the",
    "start": "686279",
    "end": "693420"
  },
  {
    "text": "things in the next stage if you actually gave them like a whole chat history or you gave them a whole chat history and",
    "start": "693420",
    "end": "699360"
  },
  {
    "text": "the thread and the like all of this information they would struggle partly because they're using um like faster cheaper llms uh partly",
    "start": "699360",
    "end": "706019"
  },
  {
    "text": "because for like embedding like you don't want to embed you know all of that context you want to embedge the relevant stuff so the query formulation take has",
    "start": "706019",
    "end": "712680"
  },
  {
    "text": "distill everything that I the whole chat history and all this context is still down to like the single thing that the",
    "start": "712680",
    "end": "718140"
  },
  {
    "text": "user is trying to ask right now um and this this was I think a big win for us when we figured out to do this we",
    "start": "718140",
    "end": "724560"
  },
  {
    "start": "720000",
    "end": "1109000"
  },
  {
    "text": "tried a bunch of other stuff uh before this but when we figured out okay first step is actually to distill this down it",
    "start": "724560",
    "end": "730019"
  },
  {
    "text": "made the things a lot easier I think like once we've just sorry just to maybe put this kind of like in context for",
    "start": "730019",
    "end": "735839"
  },
  {
    "text": "some of the link chain users as well I think this is really similar to some of the stuff we have around reformulating",
    "start": "735839",
    "end": "740880"
  },
  {
    "text": "like the chat history um which I think is like uh relevant for other reasons like if if you mention if",
    "start": "740880",
    "end": "746220"
  },
  {
    "text": "you use kind of like uh uh pronouns or something to say like what was what was he talking about like who he is like is",
    "start": "746220",
    "end": "752820"
  },
  {
    "text": "really impactful for what you want to search for right and so I think it's like a similar idea here where you take the context and then you generate some",
    "start": "752820",
    "end": "759000"
  },
  {
    "text": "some query that that has everything one question here is like have you considered splitting into like",
    "start": "759000",
    "end": "765360"
  },
  {
    "text": "multiple queries like we've seen that sometimes be helpful when people are like this is for like more like research",
    "start": "765360",
    "end": "770399"
  },
  {
    "text": "style agents and there's less agents uh less latency concerns but like basically if if someone asks like you know",
    "start": "770399",
    "end": "776880"
  },
  {
    "text": "uh what what like how do the features of like shortwave compare to the features",
    "start": "776880",
    "end": "782160"
  },
  {
    "text": "of of Gmail or something like that you maybe want two queries one that looks up shortwave one that looks up Gmail or",
    "start": "782160",
    "end": "787680"
  },
  {
    "text": "something like that rather than one that combines the two did you experiment with this at all or is that ever a",
    "start": "787680",
    "end": "792899"
  },
  {
    "text": "consideration yeah we so we we did play with this",
    "start": "792899",
    "end": "799200"
  },
  {
    "text": "um and our experience was at least at the time that anytime we had sort of",
    "start": "799200",
    "end": "805440"
  },
  {
    "text": "like multiple stages where the output of one stage would feed into the the input of the next stage it was really hard for",
    "start": "805440",
    "end": "813240"
  },
  {
    "text": "us to avoid like error propagation issues um like if it if at each stage you were",
    "start": "813240",
    "end": "818820"
  },
  {
    "text": "counting on it like outputting the exact right set of information for the next stage to do its thing like and you've",
    "start": "818820",
    "end": "825480"
  },
  {
    "text": "changed a bunch of these together it would start to break down and so we said hey let's just throw some money at the",
    "start": "825480",
    "end": "830760"
  },
  {
    "text": "problem and rather than chaining the stages together let's get all the",
    "start": "830760",
    "end": "836220"
  },
  {
    "text": "information let's stuff all of it into the prompt and then make gpd4 at the end with all the context make the call",
    "start": "836220",
    "end": "843240"
  },
  {
    "text": "um I think that the downsides of this is is you know it's it's more expensive",
    "start": "843240",
    "end": "848279"
  },
  {
    "text": "um it uh and it probably is like less flexible in what it can do um we can't we can't do things that",
    "start": "848279",
    "end": "854579"
  },
  {
    "text": "require like a lot of different things to play again um but for the set of things that we",
    "start": "854579",
    "end": "859620"
  },
  {
    "text": "wanted to launch with we felt like it could do a reasonably wide set of things in a Fairly reliable manner",
    "start": "859620",
    "end": "865740"
  },
  {
    "text": "um so this is where we landed this is something I do want to play with later because there's definitely some downsides of this this approach but at",
    "start": "865740",
    "end": "871920"
  },
  {
    "text": "least the the albums of the time we didn't figure feel like we're delivering the you know the the low error rates we",
    "start": "871920",
    "end": "878100"
  },
  {
    "text": "needed to make that work well makes sense yeah I'd be curious your your thoughts",
    "start": "878100",
    "end": "884699"
  },
  {
    "text": "here if if uh you know you've you've seen more success there than I have or",
    "start": "884699",
    "end": "890339"
  },
  {
    "text": "or if you're bullish on this getting easier over time um I mean I think uh",
    "start": "890339",
    "end": "898260"
  },
  {
    "text": "specifically with like regards to like their propagation of of steps feeding into other steps Downstream",
    "start": "898260",
    "end": "904920"
  },
  {
    "text": "yeah um I think uh",
    "start": "904920",
    "end": "910519"
  },
  {
    "text": "I'm I'm bought on this getting easier over time yeah for sure um I think I think like another thing is",
    "start": "911220",
    "end": "916860"
  },
  {
    "text": "just like generally as you do more and more steps the context kind of like builds up and with more context this oh",
    "start": "916860",
    "end": "924000"
  },
  {
    "text": "I'm at the final answer if it's like doing multiple steps it almost like needs to track what it's",
    "start": "924000",
    "end": "930000"
  },
  {
    "text": "doing at each step and it gets a little bit confused we've sometimes seen like I think this sometimes happens with like",
    "start": "930000",
    "end": "935339"
  },
  {
    "text": "long-running agents where it's kind of like thinking through its Prof thought process and doing one step at a time and then one step at a time and so I think",
    "start": "935339",
    "end": "941880"
  },
  {
    "text": "like the way like if you just kind of like circumvent that and even if it's the",
    "start": "941880",
    "end": "947339"
  },
  {
    "text": "same amount of context but it's just like a single step rather than like multiple steps that it has to like reason through",
    "start": "947339",
    "end": "952860"
  },
  {
    "text": "um I think we've seen that to be a little bit better um",
    "start": "952860",
    "end": "958139"
  },
  {
    "text": "and then yeah I mean for the problem of like query reformulation in general like I think yeah we see people doing this a",
    "start": "958139",
    "end": "964079"
  },
  {
    "text": "bunch like oftentimes the the conversation or the question that people are asking is not what you want to be",
    "start": "964079",
    "end": "969540"
  },
  {
    "text": "passing to your retrieval engine whatever that might be um and you know we've even seen people write like",
    "start": "969540",
    "end": "974820"
  },
  {
    "text": "prompts that are specific for like Amazon Pandora retriever because the camera Amazon Kindle retriever has like",
    "start": "974820",
    "end": "980160"
  },
  {
    "text": "a specific way that you want to pass that you want to query it right and so I think query formulation is is a big",
    "start": "980160",
    "end": "986220"
  },
  {
    "text": "um is a big thing one of the things I'm excited about trying going forward is at the time of",
    "start": "986220",
    "end": "992579"
  },
  {
    "text": "reformulation there are still like for us there's still a lot of like",
    "start": "992579",
    "end": "997740"
  },
  {
    "text": "relative things in the reformative query so for example dates and time so one of these things where you know the",
    "start": "997740",
    "end": "1003079"
  },
  {
    "text": "reformative query might still say like next Tuesday and we probably go farther",
    "start": "1003079",
    "end": "1008480"
  },
  {
    "text": "and try to start replacing all of the sort of relative phrasing in there with",
    "start": "1008480",
    "end": "1014899"
  },
  {
    "text": "specifics so like maybe our in the future free reformator always encodes like an actual date it or an ax name or",
    "start": "1014899",
    "end": "1022399"
  },
  {
    "text": "or something like that um we haven't tried that yet but how would you think about doing that like I would imagine",
    "start": "1022399",
    "end": "1027438"
  },
  {
    "text": "that would mostly be prompt engineering and kind of like saying like today's date is blah and you should always reference dates like you know by their",
    "start": "1027439",
    "end": "1034220"
  },
  {
    "text": "exact kind of like day rather than rather than relative things is that how you would think about it or is there some other approach",
    "start": "1034220",
    "end": "1040900"
  },
  {
    "text": "maybe we've our experience is that even even gpp4 really struggles with date",
    "start": "1040900",
    "end": "1046579"
  },
  {
    "text": "stuff yeah um like it it it's not so good at the math and like I was talking to uh the clockwise folks that they're",
    "start": "1046579",
    "end": "1054919"
  },
  {
    "text": "building an AI agent to do scheduling and like the way they've set it up is all the actual data arithmetic is done",
    "start": "1054919",
    "end": "1060860"
  },
  {
    "text": "in their code like they have a little framework for doing it but and the llm sort of like finds the dates and then",
    "start": "1060860",
    "end": "1066860"
  },
  {
    "text": "hands it off so maybe there's some you know maybe there's some hybrid approach there where we can do the data",
    "start": "1066860",
    "end": "1073100"
  },
  {
    "text": "arithmetic in you know date code and do the rest of the llm or like that or maybe the LMS will get get better to the",
    "start": "1073100",
    "end": "1080120"
  },
  {
    "text": "point where that doesn't become a problem anymore that's what I was going to say yeah hopefully they'll just get better",
    "start": "1080120",
    "end": "1086179"
  },
  {
    "text": "yeah yeah the difference between gt3.5 and 4 is is dramatic for this type of thing I imagine you know in a year or",
    "start": "1086179",
    "end": "1092299"
  },
  {
    "text": "two this will be a different story yeah on on this slide just because I don't think you've mentioned it yet what's the",
    "start": "1092299",
    "end": "1098480"
  },
  {
    "text": "feature extraction bit like I get the embedding model Vector DB and I'm glad you put pine cone there because I saw",
    "start": "1098480",
    "end": "1104000"
  },
  {
    "text": "some questions about your Tech stack and so maybe we can get into that later but like what's the feature extraction bit",
    "start": "1104000",
    "end": "1109580"
  },
  {
    "start": "1109000",
    "end": "1225000"
  },
  {
    "text": "yeah I'm happy to talk about the the workings of all this stuff so you know to be to be thorough the formulation we",
    "start": "1109580",
    "end": "1115520"
  },
  {
    "text": "also deal with gpd4 um once we've done the query formulation we run with each of these sort of",
    "start": "1115520",
    "end": "1121220"
  },
  {
    "text": "vertical slices we call a fetcher and it's basically what we're trying to do is build up a set of candidate documents",
    "start": "1121220",
    "end": "1127280"
  },
  {
    "text": "in memory that might contain relevant information so if you look down here like this all fans together here down at",
    "start": "1127280",
    "end": "1133220"
  },
  {
    "text": "the bottom and each one of these uh uses sort of a different technique to find",
    "start": "1133220",
    "end": "1138500"
  },
  {
    "text": "relevant documents and obviously I'm not listing them all but the the one on the far left I think is the probably the",
    "start": "1138500",
    "end": "1144620"
  },
  {
    "text": "most interesting which is the kind of standard uh embedding plus Vector database retrieval model where we take",
    "start": "1144620",
    "end": "1150679"
  },
  {
    "text": "the reformative query we uh we we have a model called instructor XL it's an open",
    "start": "1150679",
    "end": "1156380"
  },
  {
    "text": "source model you can just get it on hugging face um but we embed that we'll finally a query we look at our Vector database",
    "start": "1156380",
    "end": "1162440"
  },
  {
    "text": "because we've already embedded all of your existing emails and put them in there we find things that are you know similar cosine similarity and we pull",
    "start": "1162440",
    "end": "1168620"
  },
  {
    "text": "those into memory but the the thing is that's really good for answering questions where the answer is contained",
    "start": "1168620",
    "end": "1175760"
  },
  {
    "text": "in a document that like like looks similar to the embedding of the question which is often true",
    "start": "1175760",
    "end": "1182240"
  },
  {
    "text": "um but is often not true as well and uh so the other pipelines we have the other",
    "start": "1182240",
    "end": "1187820"
  },
  {
    "text": "fetches we have here are basically going after other avenues of attack that we've",
    "start": "1187820",
    "end": "1193220"
  },
  {
    "text": "seen work so as an example we have a we have an extractor around email addresses so if you if you actually use an email",
    "start": "1193220",
    "end": "1199160"
  },
  {
    "text": "address in your query we look at that and say oh let's just pull a whole bunch of emails that were sent from or to that",
    "start": "1199160",
    "end": "1206539"
  },
  {
    "text": "email address and let's pull those into memory too we have one that looks at labels so you can reference your labels",
    "start": "1206539",
    "end": "1212240"
  },
  {
    "text": "and pull those in we look for keywords we look for dates um there's there's a bunch of things so",
    "start": "1212240",
    "end": "1218240"
  },
  {
    "text": "we say okay the thing in the far left this is like the normal kind of vector DB fit thing but all the rest of these",
    "start": "1218240",
    "end": "1224480"
  },
  {
    "text": "are using an llm to extract search criteria and then looking for those particular search criteria",
    "start": "1224480",
    "end": "1231500"
  },
  {
    "text": "um at the end of this whole thing we have a big pile is right we have like thousands of emails at this point",
    "start": "1231500",
    "end": "1238340"
  },
  {
    "text": "um and uh we we used to try to just stop here and see if it would work and the problem here is like if you uh if you",
    "start": "1238340",
    "end": "1244700"
  },
  {
    "text": "have thousands of emails like you can't recently stuff that all into uh a prompt even if you're using Claude right like",
    "start": "1244700",
    "end": "1250580"
  },
  {
    "text": "it's it's you know it's gonna take forever so you need to cut this down some somehow so that's I think where",
    "start": "1250580",
    "end": "1257179"
  },
  {
    "text": "things get get more interesting where we have a couple steps to this so first is we have just a bunch of like hand rolled",
    "start": "1257179",
    "end": "1263120"
  },
  {
    "text": "rules um that we that we use to call this down so we look at things like you know",
    "start": "1263120",
    "end": "1268760"
  },
  {
    "text": "things are labeled as like marketing promotions they're we're gonna we're gonna wait against them they're usually less relevant to people's queries",
    "start": "1268760",
    "end": "1275900"
  },
  {
    "text": "um we do uh time based biasing depending on the question so actually we use",
    "start": "1275900",
    "end": "1281559"
  },
  {
    "text": "uh this is actually shown in the diagram here but we use some of the features that we extracted earlier to feed into",
    "start": "1281559",
    "end": "1287480"
  },
  {
    "text": "the heuristics so we say for example like is this a question for which the",
    "start": "1287480",
    "end": "1292640"
  },
  {
    "text": "user is probably asking about something recent so for example if you say like when was my last flight",
    "start": "1292640",
    "end": "1298280"
  },
  {
    "text": "right the you know the llm can tell you they're probably asking about something that didn't happen that long ago and so",
    "start": "1298280",
    "end": "1303559"
  },
  {
    "text": "then well in the custom heuristic phase we're like okay we have flight data for the last 20 years but we're going to",
    "start": "1303559",
    "end": "1309320"
  },
  {
    "text": "weight the things that happen more recently we're gonna wait more highlights so we have a whole bunch of custom heuristics that sort of address and this has just been you know things",
    "start": "1309320",
    "end": "1315440"
  },
  {
    "text": "that we've learned through trial and error over time so addressing the sort of biases that we need to like work on an email system and then we take the",
    "start": "1315440",
    "end": "1322400"
  },
  {
    "start": "1321000",
    "end": "1499000"
  },
  {
    "text": "output of that and we feed that into a cross encoder that's uh just like an",
    "start": "1322400",
    "end": "1327620"
  },
  {
    "text": "open source model running on our gpus who whose purpose is to say for a given",
    "start": "1327620",
    "end": "1332960"
  },
  {
    "text": "document and a given question How likely is this document to actually help so once we've called this set from",
    "start": "1332960",
    "end": "1340539"
  },
  {
    "text": "thousands down to hundreds the cross encoder then runs on those hundreds and",
    "start": "1340539",
    "end": "1346220"
  },
  {
    "text": "calls this down to like 30 or 40. and that actually does fit into the prompt",
    "start": "1346220",
    "end": "1351260"
  },
  {
    "text": "and then we send those back to this diagram over here um and we plug that in to the main",
    "start": "1351260",
    "end": "1357799"
  },
  {
    "text": "prompt answer your question that makes sense that makes sense",
    "start": "1357799",
    "end": "1365659"
  },
  {
    "text": "and that is that cross encoding mileage or something kind of like off of hugging face or did you guys fine-tune that in",
    "start": "1365659",
    "end": "1370820"
  },
  {
    "text": "any way it's it's right it's just a thing I  face right now we use a model called Ms Marco",
    "start": "1370820",
    "end": "1376340"
  },
  {
    "text": "um that is trained on Bing data that Microsoft open source so it's a bunch of q a data",
    "start": "1376340",
    "end": "1383059"
  },
  {
    "text": "um this is also an area that we are planning to invest in the this the model works pretty well for finding",
    "start": "1383059",
    "end": "1391580"
  },
  {
    "text": "you know text that answers the question um but often the question you have are things for which the metadata is is",
    "start": "1391580",
    "end": "1398360"
  },
  {
    "text": "really more interesting for answering the question and because the model wasn't trained on email metadata it",
    "start": "1398360",
    "end": "1403880"
  },
  {
    "text": "can't really help you with that so much so one of the areas that we're looking for is like how do we make this cross encoding model smarter for things that",
    "start": "1403880",
    "end": "1410840"
  },
  {
    "text": "where the metadata matters where you know the dates and the senders and the labels and stuff like that are really",
    "start": "1410840",
    "end": "1415880"
  },
  {
    "text": "important yeah interesting have you tried just putting the metadata in with the text and and cross encoding on that",
    "start": "1415880",
    "end": "1424700"
  },
  {
    "text": "we've we've played around with a bunch of stuff like that we haven't we haven't found success yet uh but we haven't",
    "start": "1424700",
    "end": "1430940"
  },
  {
    "text": "given up cool okay there's a there's a few questions in the chat so maybe we can cycle",
    "start": "1430940",
    "end": "1436280"
  },
  {
    "text": "through them now I don't know if you have uh other things as well but I figured it would it's good good stopping",
    "start": "1436280",
    "end": "1441740"
  },
  {
    "text": "point if I can answer some of these um is this using structured data behind",
    "start": "1441740",
    "end": "1447380"
  },
  {
    "text": "the scene for data retrieval I guess it kind of is because you're extracting some of the attributes and",
    "start": "1447380",
    "end": "1452960"
  },
  {
    "text": "then if I'm understanding the question correctly I think it's asking but I think you are because you're extracting",
    "start": "1452960",
    "end": "1459200"
  },
  {
    "text": "features and then doing that in some way yeah",
    "start": "1459200",
    "end": "1464299"
  },
  {
    "text": "um yeah if I I think I understand the question the same way you do yeah we're we are taking an LOM or a bunch of LM",
    "start": "1464299",
    "end": "1470299"
  },
  {
    "text": "calls looking at the query trying to figure out certain types of structure and then from that structure we are",
    "start": "1470299",
    "end": "1476539"
  },
  {
    "text": "running normal searches of various types",
    "start": "1476539",
    "end": "1481059"
  },
  {
    "text": "um what llm are using for Tool selection it's impressively fast uh gpd4",
    "start": "1481640",
    "end": "1488120"
  },
  {
    "text": "um and the key here is keep your output tokens really short um it's you know if how do you how do",
    "start": "1488120",
    "end": "1495140"
  },
  {
    "text": "you do that uh we're we're we are only spitting out",
    "start": "1495140",
    "end": "1500900"
  },
  {
    "text": "like one the output is simply one token basically per tool and it's just like a",
    "start": "1500900",
    "end": "1505940"
  },
  {
    "text": "it's just like a comma separated list or maybe just like a space separated list um got it we used to have had more",
    "start": "1505940",
    "end": "1512179"
  },
  {
    "text": "structure and like we were doing like XML tags and stuff and we cut it down to be like it's like you know three tokens",
    "start": "1512179",
    "end": "1518000"
  },
  {
    "text": "on average or something got it and and so then like do you have like tool names that are like I don't",
    "start": "1518000",
    "end": "1524419"
  },
  {
    "text": "like the one token's very short so they they can't be like that human readable right",
    "start": "1524419",
    "end": "1529520"
  },
  {
    "text": "it might be two tokens yeah they're not particularly uh yeah verbose um we so we have yeah we we have a you",
    "start": "1529520",
    "end": "1536779"
  },
  {
    "text": "know a bunch of scripts that we used to like test these and we tried a bunch of different stuff we tried",
    "start": "1536779",
    "end": "1542000"
  },
  {
    "text": "um like spitting out like numerical values we tried having it like just explain its reasoning and then split out",
    "start": "1542000",
    "end": "1548840"
  },
  {
    "text": "numerical values um and settled on this approach because it works almost as well like the best",
    "start": "1548840",
    "end": "1555440"
  },
  {
    "text": "one is explain your reasoning and then spit out numerical values and then add some weights applied after that but like",
    "start": "1555440",
    "end": "1561440"
  },
  {
    "text": "if you wait for it to explain its reasoning it takes a long time um so if you just spit out the tokens",
    "start": "1561440",
    "end": "1567380"
  },
  {
    "text": "it's almost as good and it's way faster and and just to um uh dive into something you said",
    "start": "1567380",
    "end": "1573679"
  },
  {
    "text": "around testing um like I think a lot of people are wondering how to test these things I imagine the tool selections",
    "start": "1573679",
    "end": "1578900"
  },
  {
    "text": "basically kind of like just classification where you can maybe select multiple things so I'm like",
    "start": "1578900",
    "end": "1584500"
  },
  {
    "text": "evaluation for you or testing for you probably isn't like terribly difficult",
    "start": "1584500",
    "end": "1589820"
  },
  {
    "text": "or because you're just like you know did it get the two tools that it was supposed to get and then some kind of like Precision recall kind of like",
    "start": "1589820",
    "end": "1596480"
  },
  {
    "text": "trade-off calculation is that correct or are there some nuances that I'm that I'm not thinking of um testing is I think",
    "start": "1596480",
    "end": "1602900"
  },
  {
    "text": "like probably everyone you you talked to uh we're bad at testing um everyone's bad at testing this stuff",
    "start": "1602900",
    "end": "1608600"
  },
  {
    "text": "uh we have It's a combination of we have a bunch of Jupiter notebooks that we use to like just test specific prompts",
    "start": "1608600",
    "end": "1615500"
  },
  {
    "text": "um where we say okay here like here's the thing to evaluate the tool selector and here's the thing to evaluate like this particular feature extractor yeah",
    "start": "1615500",
    "end": "1621860"
  },
  {
    "text": "and for that we just have you know hand rolled like a whole bunch of specific",
    "start": "1621860",
    "end": "1627740"
  },
  {
    "text": "test cases based on Alpha user feedback that we've gotten how many test cases do you have like is it like order order 10",
    "start": "1627740",
    "end": "1634279"
  },
  {
    "text": "order 100 order a thousand order order a hundred yeah not it's not",
    "start": "1634279",
    "end": "1639919"
  },
  {
    "text": "a number um and then uh the other test that we do is like end-to-end test and that's just",
    "start": "1639919",
    "end": "1645500"
  },
  {
    "text": "manual as well like we have a set of gold and like hey we know these things should work roughly and we go through",
    "start": "1645500",
    "end": "1650600"
  },
  {
    "text": "them periodically um this is actually a major Focus for us right now is figuring out how do we automate this better",
    "start": "1650600",
    "end": "1658039"
  },
  {
    "text": "um especially automate this in a way that isn't like you know completely a waste of time as soon as we swap a",
    "start": "1658039",
    "end": "1663140"
  },
  {
    "text": "component out because the system is evolving very quickly I think you know one of the reasons I'm being so open here is like in three months it's probably going to be a totally different",
    "start": "1663140",
    "end": "1668900"
  },
  {
    "text": "system um uh and so yeah we're trying to figure out how do we how do we how do we measure this stuff and test this stuff",
    "start": "1668900",
    "end": "1675080"
  },
  {
    "text": "in a way that like uh will you know the data will accrue and be valuable over time rather than be obsolete right away",
    "start": "1675080",
    "end": "1680419"
  },
  {
    "text": "and and for the end-to-end tests like do you do anything automated or is it just",
    "start": "1680419",
    "end": "1686960"
  },
  {
    "text": "like look at the it like or I guess do you even know what like the expected answer should be because if you're",
    "start": "1686960",
    "end": "1692720"
  },
  {
    "text": "running it on like your email your emails change kind of like you know every week so the answers could change",
    "start": "1692720",
    "end": "1698659"
  },
  {
    "text": "every week right right yeah so I did uh I'll give you",
    "start": "1698659",
    "end": "1703820"
  },
  {
    "text": "I'll give you a fun example of this so actually I'll show you uh so the blog post that we wrote for our announcement",
    "start": "1703820",
    "end": "1709600"
  },
  {
    "text": "actually I used our AI system to help ride the blog post which was which was",
    "start": "1709600",
    "end": "1715039"
  },
  {
    "text": "super fun and before I started I said like you know look up uh user feedback",
    "start": "1715039",
    "end": "1722299"
  },
  {
    "text": "or our AI assistant and use that to suggest an outline sorry",
    "start": "1722299",
    "end": "1731360"
  },
  {
    "text": "blog post announcing it's not um and I ran this thing before uh the",
    "start": "1731360",
    "end": "1740059"
  },
  {
    "text": "you know the launch uh like a ways before and it gave me one set of answers but then I I started sending a whole",
    "start": "1740059",
    "end": "1746419"
  },
  {
    "text": "bunch of emails related to the launch to press and various partners and stuff and it the you know my email history got",
    "start": "1746419",
    "end": "1752840"
  },
  {
    "text": "polluted with lots of descriptions of me talking about what our blog post was about and then it started just",
    "start": "1752840",
    "end": "1758720"
  },
  {
    "text": "regurgitating my own talking points became like much less useful for me um so I thought this was kind of a fun",
    "start": "1758720",
    "end": "1764299"
  },
  {
    "text": "example um yeah social",
    "start": "1764299",
    "end": "1771860"
  },
  {
    "text": "so how do you think about testing that and then are you just like do you like",
    "start": "1771860",
    "end": "1777260"
  },
  {
    "text": "do you even have ground truth answers or do you just have like queries that you run and then look at the results and see",
    "start": "1777260",
    "end": "1783140"
  },
  {
    "text": "if they seem reasonable right now it's the latter we have we have sort of golden test cases we expect",
    "start": "1783140",
    "end": "1789080"
  },
  {
    "text": "to work and we we update them over time we're we're working on that more kind of automated uh bit uh we also like we have",
    "start": "1789080",
    "end": "1797120"
  },
  {
    "text": "in our test environments like we can load specific sets of email history too right so I don't have to run this on my",
    "start": "1797120",
    "end": "1803539"
  },
  {
    "text": "personal inbox we can also say Okay given this you know Frozen in Time set of email history and these questions",
    "start": "1803539",
    "end": "1808880"
  },
  {
    "text": "like we should build a good specific answers um but yeah it's just it's just like aesthetical and test cases right now and",
    "start": "1808880",
    "end": "1815899"
  },
  {
    "text": "my experience has been I've you know when I when I go and have lunch with like other AI founder groups and stuff and sit down and ask them like seems",
    "start": "1815899",
    "end": "1822620"
  },
  {
    "text": "like this is what most people are doing right now uh the automated testing is real hard yeah yeah I think I think uh",
    "start": "1822620",
    "end": "1828919"
  },
  {
    "text": "we've seen kind of people do it at the individual prompt or individual kind of like you know that level",
    "start": "1828919",
    "end": "1835880"
  },
  {
    "text": "um but end to end um yeah and and end to end I think it's a lot of looking at stuff",
    "start": "1835880",
    "end": "1843980"
  },
  {
    "text": "um what what is your Tech stack that you've talked about open AI you've talked about",
    "start": "1843980",
    "end": "1849799"
  },
  {
    "text": "um Ms Marco you've talked about Pinecone um any other kind of like big components",
    "start": "1849799",
    "end": "1855679"
  },
  {
    "text": "yeah so uh the um",
    "start": "1855679",
    "end": "1861159"
  },
  {
    "text": "the left portion of this like the the tool selector and the main problem this most of the code here is actually",
    "start": "1861159",
    "end": "1867020"
  },
  {
    "text": "running in our client so this is a bunch of typescript code um we do proxy it through our back end",
    "start": "1867020",
    "end": "1873919"
  },
  {
    "text": "um to you know before we actually make the the calls over open AI but this is actually just like typescript code on on",
    "start": "1873919",
    "end": "1880940"
  },
  {
    "text": "our client the reason for that is we need easy access to like all the context um and that stuff's available in the",
    "start": "1880940",
    "end": "1887120"
  },
  {
    "text": "client um on the back end here uh we're running on Google Cloud",
    "start": "1887120",
    "end": "1892820"
  },
  {
    "text": "um we you know we gotta we got a bunch of gpus and a bunch of servers there and a bunch of big databases",
    "start": "1892820",
    "end": "1899659"
  },
  {
    "text": "um we're using for our models we're using instructor XL for embeddings um which is pretty awesome and pretty",
    "start": "1899659",
    "end": "1906500"
  },
  {
    "text": "fast um and much much much cheaper than using say open ai's embedding model",
    "start": "1906500",
    "end": "1912140"
  },
  {
    "text": "um we're using python for our Vector database we're using Ms Marco was the is the encoding model that we're using here",
    "start": "1912140",
    "end": "1918140"
  },
  {
    "text": "um what did I miss Paul kotlin on the back end uh which I",
    "start": "1918140",
    "end": "1923899"
  },
  {
    "text": "know is kind of weird um but we like jvm languages and kotlin so does Pinecone even have a",
    "start": "1923899",
    "end": "1930740"
  },
  {
    "text": "kotlin client or are you just using apis directly they",
    "start": "1930740",
    "end": "1937880"
  },
  {
    "text": "I don't remember the answer to that all right we I remember us we were using some client",
    "start": "1938840",
    "end": "1944480"
  },
  {
    "text": "that was really slow and then we swapped it up for something much faster and I don't remember what we did there but",
    "start": "1944480",
    "end": "1949820"
  },
  {
    "text": "uh we made it work somehow the next one's a fun more open-ended one",
    "start": "1949820",
    "end": "1955520"
  },
  {
    "text": "if if you had a million token context window what would you do would you do anything different also it",
    "start": "1955520",
    "end": "1962179"
  },
  {
    "text": "was another way of that because then you've talked a lot about speed and so that's also a consideration that goes into this",
    "start": "1962179",
    "end": "1968120"
  },
  {
    "text": "so I I think my answer is it depends on I think two things one uh what does it",
    "start": "1968120",
    "end": "1974419"
  },
  {
    "text": "cost and two does it does its reasoning get worse with size",
    "start": "1974419",
    "end": "1981559"
  },
  {
    "text": "um and right now for us actually the limiting factors for us are the latter two",
    "start": "1981559",
    "end": "1987860"
  },
  {
    "text": "um like we you know if we stuff every prompt full of of",
    "start": "1987860",
    "end": "1994580"
  },
  {
    "text": "stuff I think uh in many cases like there's extraneous information that makes the reasoning worse and it's really expensive so we're trying hard",
    "start": "1994580",
    "end": "2000880"
  },
  {
    "text": "not to use more tokens than we could um I think if the reasoning didn't degrade and if it was really cheap so",
    "start": "2000880",
    "end": "2008740"
  },
  {
    "text": "that it was just very practical I think I would do a fair amount differently I think a lot of the",
    "start": "2008740",
    "end": "2013779"
  },
  {
    "text": "um the tool selection would just go away where I'd say oh okay you know many of",
    "start": "2013779",
    "end": "2019240"
  },
  {
    "text": "these don't need that many token so a good example would be like the docs right like we've our artist our docs",
    "start": "2019240",
    "end": "2024519"
  },
  {
    "text": "description is like six thousand tokens or something right like we'll just shove that in every time why not um so I think a lot of that will",
    "start": "2024519",
    "end": "2030460"
  },
  {
    "text": "disappear um we probably couldn't fit your entire email history like your email history is a lot more than a million tokens probably this at least for for me it is",
    "start": "2030460",
    "end": "2037659"
  },
  {
    "text": "um uh so we probably still need the the back end retrieval but we could be uh probably much more generous in what we",
    "start": "2037659",
    "end": "2044140"
  },
  {
    "text": "include um rather than trying to cut down like right now a typical question like you know if we ask you know",
    "start": "2044140",
    "end": "2051099"
  },
  {
    "text": "what are some fun things we shipped recently by the way it shows you what what it",
    "start": "2051099",
    "end": "2057099"
  },
  {
    "text": "looked up if you if you're wondering like you know what emails did they consider in the answer",
    "start": "2057099",
    "end": "2062200"
  },
  {
    "text": "it'll tell you the right at the top here you see it says where it says based on 16 emails if",
    "start": "2062200",
    "end": "2067540"
  },
  {
    "text": "I click that we'll let it finish",
    "start": "2067540",
    "end": "2072480"
  },
  {
    "text": "yeah if I click this it actually tells me here the emails that were considered and here are the emails that we end up",
    "start": "2076119",
    "end": "2081158"
  },
  {
    "text": "putting in the prompt and so it's actually like this email thread here at the top is the one that ended up it's",
    "start": "2081159",
    "end": "2086200"
  },
  {
    "text": "220 on email that inserted so um and it only shows 16 out of those so at like a typical response it's going to",
    "start": "2086200",
    "end": "2093280"
  },
  {
    "text": "be you know 10 to 50 emails or so that we actually include nice",
    "start": "2093280",
    "end": "2098800"
  },
  {
    "text": "okay um next question up does this whole pipeline run on every chat reply versus",
    "start": "2098800",
    "end": "2105880"
  },
  {
    "text": "our replies processed differently I guess I think this means if you're having like a conversation is there any",
    "start": "2105880",
    "end": "2111460"
  },
  {
    "text": "difference in handling kind of like follow-ups or is it the same we run the whole pipeline every time we",
    "start": "2111460",
    "end": "2117520"
  },
  {
    "text": "we used to not we tried a various different things under the belief that hey like if you're continuing the",
    "start": "2117520",
    "end": "2122859"
  },
  {
    "text": "conversation maybe you don't need to like retrieve context again or maybe we can like be faster in some in some way",
    "start": "2122859",
    "end": "2128619"
  },
  {
    "text": "but like I think that what we discovered is people people like people's topics their",
    "start": "2128619",
    "end": "2134980"
  },
  {
    "text": "conversations evolve over time they ask follow-up questions that need additional data and the",
    "start": "2134980",
    "end": "2140200"
  },
  {
    "text": "the best way to like reliably give good answers was just to build a pipeline that works the same for every question",
    "start": "2140200",
    "end": "2147220"
  },
  {
    "text": "and just deal with the cost and latency of that and try to make it fast",
    "start": "2147220",
    "end": "2153520"
  },
  {
    "text": "um and so that's yeah every question is the same right now that makes sense the next question is",
    "start": "2153520",
    "end": "2159099"
  },
  {
    "text": "about kind of like costs um so obviously functionality is most important but curious how you think",
    "start": "2159099",
    "end": "2164980"
  },
  {
    "text": "about modeling costs of embedding everything and using gpd4 slash other models I know you've touched on this a",
    "start": "2164980",
    "end": "2170920"
  },
  {
    "text": "little bit throughout I don't know so yeah I think right now the biggest",
    "start": "2170920",
    "end": "2176920"
  },
  {
    "start": "2173000",
    "end": "2318000"
  },
  {
    "text": "cost for us is actually the gbd4 use at serving um it's only slightly more expensive",
    "start": "2176920",
    "end": "2184180"
  },
  {
    "text": "though than the rest of the pipeline we um we spend a fair amount on gpus to do the",
    "start": "2184180",
    "end": "2190720"
  },
  {
    "text": "embeddings we spend a fair amount of gpus to run the like the live query with the cross encoder and stuff",
    "start": "2190720",
    "end": "2196420"
  },
  {
    "text": "um we have actually two different pools of gpus we have the pool of gpus that does the embeddings where we're not so latency sensitive we're trying to be",
    "start": "2196420",
    "end": "2202480"
  },
  {
    "text": "cost efficient then we have a much uh higher uh performing set of gpus that we use for the live serving because we're",
    "start": "2202480",
    "end": "2208839"
  },
  {
    "text": "much more latency sensitive there um uh pine cones also a pretty big cost",
    "start": "2208839",
    "end": "2214599"
  },
  {
    "text": "driver for us um uh and they one of the reasons we chose them is they have in theory a new",
    "start": "2214599",
    "end": "2221619"
  },
  {
    "text": "platform coming up very soon where they're gonna start separating compute storage that should drive costs down significantly so actually if I can if I",
    "start": "2221619",
    "end": "2228940"
  },
  {
    "text": "can briefly plug Pinecone there's another there's a couple cool things that pineco has one is they should have",
    "start": "2228940",
    "end": "2234040"
  },
  {
    "text": "a like a serverless platform to shoot soon that if you have a lot of vectors",
    "start": "2234040",
    "end": "2239079"
  },
  {
    "text": "should provide much better off performance for you um for someone like us who like does you",
    "start": "2239079",
    "end": "2245020"
  },
  {
    "text": "know few few searches and has lots of vectors um the other great thing about Pinecone is they do uh namespacing",
    "start": "2245020",
    "end": "2251500"
  },
  {
    "text": "um so for someone like us where we have you know many many users that you know",
    "start": "2251500",
    "end": "2256599"
  },
  {
    "text": "you don't want to query across them um they give you the ability to separate",
    "start": "2256599",
    "end": "2261700"
  },
  {
    "text": "those in a way that doesn't affect performance like there are other solutions that let you do something like this but the actual way it happens is",
    "start": "2261700",
    "end": "2267820"
  },
  {
    "text": "like a post filter like they'll do they'll go do the cosine similar to search to retrieve all the documents and then they'll throw out the ones that",
    "start": "2267820",
    "end": "2274119"
  },
  {
    "text": "like don't fit your filter um and that ends up being really slow so like we looked at using um PD Vector",
    "start": "2274119",
    "end": "2280300"
  },
  {
    "text": "with postgres for this we looked at uh forget all several other ones and we",
    "start": "2280300",
    "end": "2285940"
  },
  {
    "text": "looked at using like elastics new stuff and like none of them could provide the the performance profile with the the",
    "start": "2285940",
    "end": "2292359"
  },
  {
    "text": "level of sharding that we have uh it's a Franco so that's why I've used them",
    "start": "2292359",
    "end": "2298000"
  },
  {
    "text": "I know a lot of people are always asking about like you know Vector databases so I think that was a very I appreciate",
    "start": "2298000",
    "end": "2303940"
  },
  {
    "text": "that insightful kind of like thought on it because it's I think it's always top of top of people's minds",
    "start": "2303940",
    "end": "2309760"
  },
  {
    "text": "um the next question is a good one as well is there any self-learning for example if I ask the agent to draft an email then I correct the tone will the",
    "start": "2309760",
    "end": "2316180"
  },
  {
    "text": "tone be different next time uh not not in that sense this is",
    "start": "2316180",
    "end": "2321460"
  },
  {
    "start": "2318000",
    "end": "2550000"
  },
  {
    "text": "something I would love to do right like you can have a conversation you can teach it how you want to want to work we do have a feature that can help you with",
    "start": "2321460",
    "end": "2329200"
  },
  {
    "text": "this particular case though so if you go to your settings and you go down you go down to compose",
    "start": "2329200",
    "end": "2336940"
  },
  {
    "text": "um we have this this AI assistant writing style here and this is kind of fun though and the way it works is if",
    "start": "2336940",
    "end": "2342339"
  },
  {
    "text": "you I'll I'll reset it so you can all watch if I click personalized writing style it's going to take a whole bunch",
    "start": "2342339",
    "end": "2348520"
  },
  {
    "text": "of emails that I've sent in the past it's going to send them off to an llm and it's going to have describe how I",
    "start": "2348520",
    "end": "2354880"
  },
  {
    "text": "write um and one of the fun things about this feature is it's really Direct in its",
    "start": "2354880",
    "end": "2361180"
  },
  {
    "text": "descriptions sometimes so like for one of my co-founders this description came back where he had said you know he is for verbose to the point of being",
    "start": "2361180",
    "end": "2367480"
  },
  {
    "text": "annoying which was was really funny and like then we were like okay do we want",
    "start": "2367480",
    "end": "2372700"
  },
  {
    "text": "to maybe have it so it doesn't insult people like when you know writing these descriptions but anyways this actually works pretty well and I think this has",
    "start": "2372700",
    "end": "2378820"
  },
  {
    "text": "two benefits here one is it's a very simple implementation if you want to like do writing style stuff yourself like this sort of approach actually",
    "start": "2378820",
    "end": "2385420"
  },
  {
    "text": "works pretty well um the other though is is I think the more important one which is a product benefit of you actually",
    "start": "2385420",
    "end": "2391060"
  },
  {
    "text": "maybe don't want the AI to write in your style what you probably want is to write in your ideal style and so you can do",
    "start": "2391060",
    "end": "2397599"
  },
  {
    "text": "this have it generate the thing and then be like well you know maybe I don't want to say you know this one's so I'm just I",
    "start": "2397599",
    "end": "2404020"
  },
  {
    "text": "say I'd love too much I'm just gonna remove that and I won't do that anymore um and you know then you know I have my",
    "start": "2404020",
    "end": "2410500"
  },
  {
    "text": "new writing style and this yeah so this is and this gets inserted into the prompt is like a",
    "start": "2410500",
    "end": "2416560"
  },
  {
    "text": "system message or something like that yeah so this is one of our tools actually so when we when we look at the",
    "start": "2416560",
    "end": "2424780"
  },
  {
    "text": "thing and say oh they're trying to get the AI to write a thing we then pull it and your writing style let me insert",
    "start": "2424780",
    "end": "2431320"
  },
  {
    "text": "that got it okay okay that makes sense I like that yeah we've seen um I I think",
    "start": "2431320",
    "end": "2437440"
  },
  {
    "text": "the idea like we did a webinar with um Greg good comrad uh a few weeks or a",
    "start": "2437440",
    "end": "2443440"
  },
  {
    "text": "month or two ago around basically this idea of like tone and imitating kind of like people's tone and I think it's",
    "start": "2443440",
    "end": "2449079"
  },
  {
    "text": "really I think this is super interesting um I think I like the point that you made that sometimes you don't want it to",
    "start": "2449079",
    "end": "2455140"
  },
  {
    "text": "copy your exact tone you maybe want it to be a you know an idealized version of your tone",
    "start": "2455140",
    "end": "2460780"
  },
  {
    "text": "um I think it's um yeah the uh the uh",
    "start": "2460780",
    "end": "2466900"
  },
  {
    "text": "the the idea of automatically extracting it is also good because if I was asked to like write down my tone of how I",
    "start": "2466900",
    "end": "2473440"
  },
  {
    "text": "wrote in emails I don't even know where I'd begin so I think having a language model kind of like look at it and start",
    "start": "2473440",
    "end": "2479800"
  },
  {
    "text": "um one thing that we we both the the Genesis of that webinar was actually fine-tuning um because basically I think",
    "start": "2479800",
    "end": "2485619"
  },
  {
    "text": "I I do think that if I want to like learn a person's tone fine-tuning arguably seems to me that it would work",
    "start": "2485619",
    "end": "2492820"
  },
  {
    "text": "better than kind of like some system prompts or even few shot examples and then have you played around with",
    "start": "2492820",
    "end": "2498280"
  },
  {
    "text": "that or thought about that at all I'm guessing that's not super feasible at your scale but we have",
    "start": "2498280",
    "end": "2504579"
  },
  {
    "text": "um yeah there's some tech we've been looking at that uh that I'm not gonna not gonna share here actually but uh",
    "start": "2504579",
    "end": "2509920"
  },
  {
    "text": "that lets you do kind of scalable per user fine-tuning um and uh we may we may release",
    "start": "2509920",
    "end": "2518740"
  },
  {
    "text": "something along those lines I think the the trick is how do you make it uh you",
    "start": "2518740",
    "end": "2523780"
  },
  {
    "text": "know cost effective and performant if you have a whole bunch of little fine-tune models like that and uh I",
    "start": "2523780",
    "end": "2529540"
  },
  {
    "text": "think also the the product experience of you know maybe this benefit to having it be more of a contextual description so",
    "start": "2529540",
    "end": "2535240"
  },
  {
    "text": "yeah yeah that's an area of research for us right now yeah",
    "start": "2535240",
    "end": "2540339"
  },
  {
    "text": "all right some next question can it work with llama two models instead of gpd4 I",
    "start": "2540339",
    "end": "2545440"
  },
  {
    "text": "think you've already answered this gpd4 works the best",
    "start": "2545440",
    "end": "2550260"
  },
  {
    "start": "2550000",
    "end": "2646000"
  },
  {
    "text": "yeah I'm sure you could build a system like this with with llama I I don't think it would work as well",
    "start": "2550480",
    "end": "2556359"
  },
  {
    "text": "um the next question I actually think leads to an interesting place so are there any plans to support email",
    "start": "2556359",
    "end": "2562300"
  },
  {
    "text": "providers other than Gmail so this isn't Gmail right this is shortwave uh yeah we so we we only work with Gmail",
    "start": "2562300",
    "end": "2570099"
  },
  {
    "text": "right now so we don't you don't get like a stand-up email address we we plug into your Gmail account",
    "start": "2570099",
    "end": "2575440"
  },
  {
    "text": "um our our primary focus is really is like Google workspace users we're we're going after startups using Google",
    "start": "2575440",
    "end": "2581200"
  },
  {
    "text": "workspace for their email and trying to give them like their experience um I would love and long-term support",
    "start": "2581200",
    "end": "2586900"
  },
  {
    "text": "other folks like we get a lot of uh you know Microsoft ecosystem people being like hey when are you gonna help us out",
    "start": "2586900",
    "end": "2592420"
  },
  {
    "text": "and uh a bunch of people who are like hey I rather like not have my data in Gmail like you know self-hosted version",
    "start": "2592420",
    "end": "2598300"
  },
  {
    "text": "or do you have a Cloud solution that you guys provide um we want to do all of that um but I think it's one thing at a",
    "start": "2598300",
    "end": "2603819"
  },
  {
    "text": "time I think our our Focus right now is you know we want to have the world's",
    "start": "2603819",
    "end": "2609339"
  },
  {
    "text": "best most capable email AI assistant and that is uh that is a tall order and that",
    "start": "2609339",
    "end": "2614500"
  },
  {
    "text": "is where we are focusing right now so and and so so on this vein like how do you think about",
    "start": "2614500",
    "end": "2622119"
  },
  {
    "text": "um like Gmail potentially building something like this in there and having",
    "start": "2622119",
    "end": "2627160"
  },
  {
    "text": "it kind of like be natively integrated with Gmail and I think you know with with Spotify announcing kind of like",
    "start": "2627160",
    "end": "2632319"
  },
  {
    "text": "their translation service yesterday there there's a lot of startups doing kind of like translation for for kind of",
    "start": "2632319",
    "end": "2637660"
  },
  {
    "text": "like podcasts and stuff and now the platform kind of like builds it in and so how do you as like a you know product",
    "start": "2637660",
    "end": "2642880"
  },
  {
    "text": "and an entrepreneur think about that kind of like trade-off there yeah so I think it's helpful back I",
    "start": "2642880",
    "end": "2648819"
  },
  {
    "text": "understand that I I was a Google employee uh not that long ago company sold and uh when I was there a",
    "start": "2648819",
    "end": "2658000"
  },
  {
    "text": "bunch of the people that I recruited on my team were X Google inbox folks um because they built this thing that a",
    "start": "2658000",
    "end": "2663579"
  },
  {
    "text": "lot of people loved and they innovated on the UI of email a whole bunch and and it you know for various big company",
    "start": "2663579",
    "end": "2668980"
  },
  {
    "text": "reasons did not go well uh ended up getting shut down and you know while we were there there were some refugees from that team that came over and joined our",
    "start": "2668980",
    "end": "2675520"
  },
  {
    "text": "team and I got I think a good appreciation for why it is so hard for a",
    "start": "2675520",
    "end": "2683140"
  },
  {
    "text": "big company to take a big old product like this with bajillion stakeholders and like actually do meaningful",
    "start": "2683140",
    "end": "2689740"
  },
  {
    "text": "Innovation and I I think you see a lot of noise from Google around this type of",
    "start": "2689740",
    "end": "2695920"
  },
  {
    "text": "thing but I don't think the rubber is going to hit the road like like some people think it will and a great example",
    "start": "2695920",
    "end": "2701619"
  },
  {
    "text": "of this is uh go try duet AI um which is like the Enterprise AI thing",
    "start": "2701619",
    "end": "2706780"
  },
  {
    "text": "that they they launched recently it's 30 a month it's only available to Enterprise users we signed up",
    "start": "2706780",
    "end": "2712900"
  },
  {
    "text": "um it does almost none of this and the only thing it really does is it does some of the writing stuff but the writing stuff is not particularly good I",
    "start": "2712900",
    "end": "2719619"
  },
  {
    "text": "was not impressed um and they claim there's a bunch more stuff coming um",
    "start": "2719619",
    "end": "2725200"
  },
  {
    "text": "we'll see um I think we have a like we are ahead of them right now I think we have a very",
    "start": "2725200",
    "end": "2731680"
  },
  {
    "text": "good chance of staying ahead we're going to keep moving super super fast and I think even",
    "start": "2731680",
    "end": "2737380"
  },
  {
    "text": "if they somehow managed to keep up just on like the technical Innovation side which I really doubt there's ux",
    "start": "2737380",
    "end": "2743200"
  },
  {
    "text": "considerations where like the the actual UI a Gmail has not changed in a meaningful way in a very very long time",
    "start": "2743200",
    "end": "2749619"
  },
  {
    "text": "I there's good reason why it is very hard to change so like I'll show you",
    "start": "2749619",
    "end": "2755020"
  },
  {
    "text": "actually one of the one of the good arguments here so if you go into Gmail and you click on the settings like there are",
    "start": "2755020",
    "end": "2761200"
  },
  {
    "text": "10 billion settings here and the way to think about these settings is every one",
    "start": "2761200",
    "end": "2767079"
  },
  {
    "text": "of these settings is a stakeholder somewhere at Google that wants it to work a certain way and they and so if you're a PM there and you're like we",
    "start": "2767079",
    "end": "2773200"
  },
  {
    "text": "need to like totally reimagine the UI around AI good luck",
    "start": "2773200",
    "end": "2779079"
  },
  {
    "text": "um and we aren't constrained by that so if we want to say you know what we're gonna you know we're gonna say there's a little thing",
    "start": "2779079",
    "end": "2784599"
  },
  {
    "text": "that pops up from the side and everything is built around this and everything plugs into this and every and this AI has knowledge of everything and",
    "start": "2784599",
    "end": "2790660"
  },
  {
    "text": "can do everything like that is something we can you know the whole company can rally around that we can focus on that in a way that it's really tough for",
    "start": "2790660",
    "end": "2796720"
  },
  {
    "text": "Gmail so our plan is just to it's just to run ahead of them um and we've been doing it so far we're",
    "start": "2796720",
    "end": "2802180"
  },
  {
    "text": "gonna keep doing defense um what was your inspiration about query",
    "start": "2802180",
    "end": "2808359"
  },
  {
    "text": "formulation um",
    "start": "2808359",
    "end": "2814000"
  },
  {
    "text": "we tried a lot of stuff uh I don't I don't know that there was any uh major inspiration we tried a lot of bunch of",
    "start": "2814000",
    "end": "2820240"
  },
  {
    "text": "stuff we talked to a bunch of people we we looked at what other people were doing um yeah I don't remember the particular spark but it worked so",
    "start": "2820240",
    "end": "2828040"
  },
  {
    "start": "2828000",
    "end": "2941000"
  },
  {
    "text": "cool um all right one more question and then one softball to end it because I think",
    "start": "2828040",
    "end": "2833079"
  },
  {
    "text": "we're running a bit short on time what are you doing are you doing something I'm providing the privacy on",
    "start": "2833079",
    "end": "2839020"
  },
  {
    "text": "the content of emails this is maybe like the number one question we get from everyone around gen AI so I'm curious to",
    "start": "2839020",
    "end": "2844359"
  },
  {
    "text": "hear your take on it yeah this is this is absolutely top priority for us um obviously like your business email is",
    "start": "2844359",
    "end": "2850960"
  },
  {
    "text": "super super sensitive um we're doing uh we're doing a bunch of",
    "start": "2850960",
    "end": "2856060"
  },
  {
    "text": "stuff so the the first thing I want to note is if you don't use the AI assistant nothing leaves our system right so so we",
    "start": "2856060",
    "end": "2863619"
  },
  {
    "text": "we don't embed things unless you ask us to even if we we do embed things we",
    "start": "2863619",
    "end": "2868720"
  },
  {
    "text": "embed stuff on our own gpus um we're using an open source model like we're not sending that stuff off to open",
    "start": "2868720",
    "end": "2873819"
  },
  {
    "text": "AI um we do have one external service provider which is open AI which for the",
    "start": "2873819",
    "end": "2879280"
  },
  {
    "text": "questions you ask Google send just the relevant emails and the question off to them and so uh but the",
    "start": "2879280",
    "end": "2887560"
  },
  {
    "text": "I think one of the big reasons we decided to move forward with them is they changed their policies around the way they do training and stuff so that",
    "start": "2887560",
    "end": "2894040"
  },
  {
    "text": "um they are not training on the emails that we send them so we've we've gained some confidence that they are a",
    "start": "2894040",
    "end": "2899200"
  },
  {
    "text": "trustworthy Provider from privacy standpoint um but even then we only send them the the questions and the relevant stuff and",
    "start": "2899200",
    "end": "2905140"
  },
  {
    "text": "if you don't use it it doesn't doesn't go anywhere um we are not currently training any",
    "start": "2905140",
    "end": "2910240"
  },
  {
    "text": "models um we may train models but um we definitely won't do so without",
    "start": "2910240",
    "end": "2916540"
  },
  {
    "text": "like very clear communication around what we're doing and what kind of data we're using we probably won't start with",
    "start": "2916540",
    "end": "2921760"
  },
  {
    "text": "any generative models so like the models we train will probably be like feature extraction models and cross encoders and things like that for which like the you",
    "start": "2921760",
    "end": "2929500"
  },
  {
    "text": "know the risk of like something you type getting generated out to someone else is is not there um so yeah we're being very careful",
    "start": "2929500",
    "end": "2936460"
  },
  {
    "text": "about this very thoughtful about this um very much top of mind",
    "start": "2936460",
    "end": "2941619"
  },
  {
    "start": "2941000",
    "end": "3168000"
  },
  {
    "text": "awesome and then and then final question and then take this whatever Direction you want what are you personally most",
    "start": "2941619",
    "end": "2947800"
  },
  {
    "text": "excited about in the future of shortwave like what can you yeah what can you tell people how can people try shortwave yeah",
    "start": "2947800",
    "end": "2955660"
  },
  {
    "text": "yeah um oh there's a lot there uh so one of the",
    "start": "2955660",
    "end": "2961540"
  },
  {
    "text": "things I want to mention before we before I go talk about the AI stuff because obviously I'm going to talk about AI stuff but uh there's a lot of",
    "start": "2961540",
    "end": "2966880"
  },
  {
    "text": "cool stuff in the intro that has nothing to do with AI right like that we we have some strong opinionation opinions about",
    "start": "2966880",
    "end": "2972339"
  },
  {
    "text": "like how to get inbox zero and how to manage your inbox and give you like to-do list type features so for example like I can take my emails and be like",
    "start": "2972339",
    "end": "2979240"
  },
  {
    "text": "hey these two things seem related let me like stack these up and like you know add a note",
    "start": "2979240",
    "end": "2984760"
  },
  {
    "text": "um and then I can take that and I can dig drag it around my inbox to be like okay that's top grade I put it up here um so we we put a lot of thought into",
    "start": "2984760",
    "end": "2991060"
  },
  {
    "text": "like how do we help you effectively manage your to-do's how do we help you you know uh automate things so we have",
    "start": "2991060",
    "end": "2997599"
  },
  {
    "text": "this thing called delivery schedules where you can like say I only want these emails at certain times of day or hey I don't want these emails my inbox at all",
    "start": "2997599",
    "end": "3003660"
  },
  {
    "text": "I just want them like Auto archive and my search history so there's a lot of like Automation and like taskbands and stuff that we have built in there's",
    "start": "3003660",
    "end": "3009240"
  },
  {
    "text": "nothing to do with AI um but the stuff I'm personally most excited about is definitely Ai and I think the uh",
    "start": "3009240",
    "end": "3016859"
  },
  {
    "text": "some of it is just improving the capabilities of this assistant so like it if you try it today it does a decent",
    "start": "3016859",
    "end": "3024060"
  },
  {
    "text": "job it'll answer your questions MO isters it'll graph emails that are",
    "start": "3024060",
    "end": "3029099"
  },
  {
    "text": "pretty good but it isn't quite at the level of like an actual human assistant um but I think that's where it's going like I think the all of the trends are",
    "start": "3029099",
    "end": "3036540"
  },
  {
    "text": "with us we're going to have you know llms that get smarter and smarter we're going to figure out the algorithms better and better the gpus are going to",
    "start": "3036540",
    "end": "3043020"
  },
  {
    "text": "get cheaper so we can be considering much more and more data over time um so all of those Trends are with us I do think we can level with this sidebar",
    "start": "3043020",
    "end": "3049740"
  },
  {
    "text": "really truly is a reasonable replacement for for a huge chunk of like what a human assistant might do for you",
    "start": "3049740",
    "end": "3056339"
  },
  {
    "text": "um the other set of things I'm excited about are the things that maybe on the AI side that don't necessarily fit neatly into the sidebar here so here's",
    "start": "3056339",
    "end": "3062400"
  },
  {
    "text": "an example like long-running jobs um today everything is like Mission response right what if I could say hey I",
    "start": "3062400",
    "end": "3068400"
  },
  {
    "text": "want you to go look at you know don't just do the vector database thing I want you to like run my entire email history",
    "start": "3068400",
    "end": "3073920"
  },
  {
    "text": "through an llm and I want you to create a report that like analyzes in great detail like everything that users ever",
    "start": "3073920",
    "end": "3080520"
  },
  {
    "text": "said about the AI system um it'd be really cool if we could be like cool we're gonna spend three hours",
    "start": "3080520",
    "end": "3085980"
  },
  {
    "text": "and it's gonna cost you twenty dollars is and we are going to generate that report for you and we have the data we have the technology to do it it's just a",
    "start": "3085980",
    "end": "3092760"
  },
  {
    "text": "matter of of building it so I think long-range jobs is one I think AI based",
    "start": "3092760",
    "end": "3097800"
  },
  {
    "text": "automations so right now our AI system can't actually do anything all it does",
    "start": "3097800",
    "end": "3103619"
  },
  {
    "text": "is like suggest actions you can take and that's there's there's security reasons for this right we don't we don't want",
    "start": "3103619",
    "end": "3108780"
  },
  {
    "text": "people to be like be able to like prompt inject you and stuff but I think there is potential for us to be able to do like things like set up a filter where",
    "start": "3108780",
    "end": "3115319"
  },
  {
    "text": "you can describe you know to the AI hey emails that look like this sort of thing I want you",
    "start": "3115319",
    "end": "3121200"
  },
  {
    "text": "to do this but you know label them or like have them Skip by inbox or you know set up an AI autoresponder right so like",
    "start": "3121200",
    "end": "3127680"
  },
  {
    "text": "you go on vacation you're like hey for people on my team tell them this message for people that are external send them this message for if you get questions of",
    "start": "3127680",
    "end": "3133619"
  },
  {
    "text": "this type like respond you know using the notes in this document link to the document",
    "start": "3133619",
    "end": "3139200"
  },
  {
    "text": "um so anyways lots of fun stuff [Music] sounds like there's a lot appreciate you",
    "start": "3139200",
    "end": "3145680"
  },
  {
    "text": "taking the time to come on and and talk about it I really enjoyed this this was great I think the level of detail that",
    "start": "3145680",
    "end": "3150900"
  },
  {
    "text": "we went into was awesome um yeah thank thanks for joining yeah thank you for having it it's a lot",
    "start": "3150900",
    "end": "3157319"
  },
  {
    "text": "of fun appreciate it all right and with that we're done thank you everyone for tuning in see ya see",
    "start": "3157319",
    "end": "3162839"
  },
  {
    "text": "you at the next one cheers all thank you",
    "start": "3162839",
    "end": "3167720"
  }
]