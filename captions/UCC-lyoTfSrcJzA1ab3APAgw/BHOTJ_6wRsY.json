[
  {
    "text": "what's up everyone it's brace and this is the second video in a three-part series on building generative UI applications with laying chain if you",
    "start": "880",
    "end": "7680"
  },
  {
    "text": "haven't seen the first video already you should go back and watch that in that video we cover some high Lev Concepts",
    "start": "7680",
    "end": "12920"
  },
  {
    "text": "such as what is generative UI we cover use cases and then also we go over the",
    "start": "12920",
    "end": "18000"
  },
  {
    "text": "highle architecture of how we're going to build this chatbot and then also in the next video which releases tomorrow",
    "start": "18000",
    "end": "23880"
  },
  {
    "text": "the python chatbot uh which will be a python powered back end and nextjs front end that being said if you are not a",
    "start": "23880",
    "end": "30519"
  },
  {
    "text": "typescript developer you should probably hold out for tomorrow's video or you can still watch today's video because some",
    "start": "30519",
    "end": "36680"
  },
  {
    "text": "of the topics will overlapped with the nextjs front end um and the aisk which we will use to power some of the uh code",
    "start": "36680",
    "end": "44600"
  },
  {
    "text": "which sends the UI component from the server or in this video's case the react server component back to the UI so as a",
    "start": "44600",
    "end": "51879"
  },
  {
    "text": "quick quick refresher this is the highle architecture of what the chap bot we're",
    "start": "51879",
    "end": "57559"
  },
  {
    "text": "going to be building today will look like on the inside so it takes some inputs user input any images some chat",
    "start": "57559",
    "end": "63680"
  },
  {
    "text": "history sends that to an LM the LM has some tools bound to it then using that",
    "start": "63680",
    "end": "69240"
  },
  {
    "text": "response uh we have this conditional Edge if you're familiar with L graph you should know the what a conditional Edge",
    "start": "69240",
    "end": "74439"
  },
  {
    "text": "is if not I'm going to link a uh video going over L graph which you should watch because that is what we're going",
    "start": "74439",
    "end": "80200"
  },
  {
    "text": "to be using to build our L graph agent in this video so the conditional Edge says if there's only a plain text",
    "start": "80200",
    "end": "86600"
  },
  {
    "text": "response then stream those chunks back to the client and then as this those chunks come in from open AI or whatever",
    "start": "86600",
    "end": "92520"
  },
  {
    "text": "model provider we use we render them on the client if a tool is used that gets sent to our invoke tool section where we",
    "start": "92520",
    "end": "99479"
  },
  {
    "text": "first stream the initial component back to the UI which could be some sort of loading component or UI element that",
    "start": "99479",
    "end": "107000"
  },
  {
    "text": "tells the user hey we have process your request we're using this tool um and we're going to get back to you in a",
    "start": "107000",
    "end": "112719"
  },
  {
    "text": "second so it just allows for a quicker time to First interaction we then can execute some arbitrary tool function",
    "start": "112719",
    "end": "119159"
  },
  {
    "text": "which is just a generic typescript JavaScript function so in our case it'll typically be hitting an a an external",
    "start": "119159",
    "end": "124799"
  },
  {
    "text": "API and then once we get the response back from that we update our UI component stream with the final component and close it uh you can call",
    "start": "124799",
    "end": "132040"
  },
  {
    "text": "update or append as many times you would like to update or append UI components",
    "start": "132040",
    "end": "137360"
  },
  {
    "text": "um to the UI your user see sees the way we're able to stream all",
    "start": "137360",
    "end": "142680"
  },
  {
    "text": "these events back to the client is via the stream events endpoint in Lang chain stream events essentially is a way to",
    "start": "142680",
    "end": "148760"
  },
  {
    "text": "stream intermediate steps from your Langan chain application back to the",
    "start": "148760",
    "end": "154319"
  },
  {
    "text": "client or just send them in a in a response object um since this is all using Lane chain and Lane graph stream",
    "start": "154319",
    "end": "160480"
  },
  {
    "text": "events is able to access every single um yield that a function might yield so in",
    "start": "160480",
    "end": "166400"
  },
  {
    "text": "our case we're going to be yielding UI components we're going to be yielding data and then once again yielding UI components again or yielding text um in",
    "start": "166400",
    "end": "173879"
  },
  {
    "text": "stream events is able to just capture all of those streams and then forward them back to the client okay so the",
    "start": "173879",
    "end": "180200"
  },
  {
    "text": "first file we're going to want to implement is our utils server. TSX oh and if you want to follow along the link",
    "start": "180200",
    "end": "186239"
  },
  {
    "text": "to this GitHub um repo will be linked in the description you can clone it and then you can go through and read each of",
    "start": "186239",
    "end": "191680"
  },
  {
    "text": "the files as we as we code them um the code will all be there in that refo but",
    "start": "191680",
    "end": "196920"
  },
  {
    "text": "this first server. TSX file this file is what's going to contain all of our logic around streaming UI components and",
    "start": "196920",
    "end": "203080"
  },
  {
    "text": "invoking our runnable so let's add our Imports the first line you can see it's",
    "start": "203080",
    "end": "209799"
  },
  {
    "text": "server only the all of the streaming UI components and invoking the runnable will have will happen inside of a react",
    "start": "209799",
    "end": "216239"
  },
  {
    "text": "server component so we want to make sure this file is only used on the server next we're going to import react node",
    "start": "216239",
    "end": "222200"
  },
  {
    "text": "and is valid element from react we're going to use uh the this for typing as this is a type and then is valid element",
    "start": "222200",
    "end": "229040"
  },
  {
    "text": "we're going to use to make sure the elements inside of our stream from our runnable are UI elements before sending",
    "start": "229040",
    "end": "235799"
  },
  {
    "text": "them back via the aisk which leads us into this next import the AIS SDK Imports um we use the AI SDK under the",
    "start": "235799",
    "end": "242959"
  },
  {
    "text": "hood to handle streaming back UI components because they do a lot of the heavy lifting with react server",
    "start": "242959",
    "end": "248400"
  },
  {
    "text": "components next we have some imports from runnables or sorry from Lang chain core runnables uh this will we're going",
    "start": "248400",
    "end": "255920"
  },
  {
    "text": "to be using as a type and then also runnable Lambda is what we're going to use to wrap our one of our streaming UI",
    "start": "255920",
    "end": "261799"
  },
  {
    "text": "components back so we can um essentially upsert the UI components into the stream",
    "start": "261799",
    "end": "267600"
  },
  {
    "text": "event next these are all going to be for Ty types we have our stream event type and then we have our AI provider which",
    "start": "267600",
    "end": "273720"
  },
  {
    "text": "is going to provide context we're going to use this to wrap um our children so that all of the UI components have the",
    "start": "273720",
    "end": "280240"
  },
  {
    "text": "proper context and then this we're also going to use as a type so the first function we're going",
    "start": "280240",
    "end": "285960"
  },
  {
    "text": "to implement is the with resolvers function this function is going to return um a promise and then a resolve",
    "start": "285960",
    "end": "292680"
  },
  {
    "text": "and a reject function we're going to use these so that when you are streaming",
    "start": "292680",
    "end": "297960"
  },
  {
    "text": "back from the UI component or sorry from when you're streaming the UI component and any other values from our chain",
    "start": "297960",
    "end": "303759"
  },
  {
    "text": "we're able to properly resolve the final promise and know when our UI is done um",
    "start": "303759",
    "end": "310720"
  },
  {
    "text": "streaming inside of this component we have a resolve reject function a new",
    "start": "310720",
    "end": "315880"
  },
  {
    "text": "promise which then assigns the resolve and the reject function to the resolve and reject function from the promise and",
    "start": "315880",
    "end": "322160"
  },
  {
    "text": "then we return it we need to expect the error here because typescript does not think that these have been assigned",
    "start": "322160",
    "end": "329080"
  },
  {
    "text": "technically that true however the way we're going to use this um we will always use this promise",
    "start": "329080",
    "end": "335680"
  },
  {
    "text": "before calling these resolve and reject functions so although typescript thinks they're not used yet in practice we will",
    "start": "335680",
    "end": "343560"
  },
  {
    "text": "use them after calling our promise next we're going to implement",
    "start": "343560",
    "end": "349080"
  },
  {
    "text": "our expose endpoints function this function is what we're going to use to return the proper actions and provide",
    "start": "349080",
    "end": "355639"
  },
  {
    "text": "context to these actions so they can call our agent and then also so our UI components have the proper context we're",
    "start": "355639",
    "end": "362479"
  },
  {
    "text": "using this AI provider react context that we imported",
    "start": "362479",
    "end": "367759"
  },
  {
    "text": "from client. TSX which is is in which is in the same UTS file if you're following",
    "start": "367759",
    "end": "373280"
  },
  {
    "text": "along and then as we see here we also have this use context hook which uses the use context from react and it's",
    "start": "373280",
    "end": "380280"
  },
  {
    "text": "going to provide context to our client side uh files so they can properly invoke the",
    "start": "380280",
    "end": "386120"
  },
  {
    "text": "agent here it's pretty simple returns a new function AI which contains an AI provider um react contacts or jsx",
    "start": "386120",
    "end": "394280"
  },
  {
    "text": "component which you saw in the other file the next function we're going to want to implement is the one which will",
    "start": "394280",
    "end": "400160"
  },
  {
    "text": "handle streaming back all the UI components so handling this stream handling this stream and then it's what",
    "start": "400160",
    "end": "406919"
  },
  {
    "text": "passes the UI components from our tool call functions up into our stream events",
    "start": "406919",
    "end": "412919"
  },
  {
    "text": "call so we'll implement the stream events function after this one but this function we're going to implement will",
    "start": "412919",
    "end": "418280"
  },
  {
    "text": "handle we will is essentially wrapping the aisk and a runnable Lambda and is uh streaming and",
    "start": "418280",
    "end": "425240"
  },
  {
    "text": "yielding these UI components so that we can access them inside of our stream",
    "start": "425240",
    "end": "430479"
  },
  {
    "text": "events we'll Implement that here it's going to be called create runnable UI takes in two args one is required the",
    "start": "430479",
    "end": "436680"
  },
  {
    "text": "second is optional the config argument this is going to be used to we're going to pass in our config config values here",
    "start": "436680",
    "end": "443680"
  },
  {
    "text": "and this is so these stream events um when we invoke stream events it's going to have access to this runable Lambda",
    "start": "443680",
    "end": "450960"
  },
  {
    "text": "function next we take in a an optional initial value this gets passed to the",
    "start": "450960",
    "end": "456840"
  },
  {
    "text": "create streamable UI function so you see right down here we have our runnable Lambda this Lambda takes in a single",
    "start": "456840",
    "end": "463280"
  },
  {
    "text": "input of initial value which should actually be",
    "start": "463280",
    "end": "469440"
  },
  {
    "text": "optional um this initial value is going to get passed to the create streamable UI function from the asdk and then we're",
    "start": "469440",
    "end": "475280"
  },
  {
    "text": "going to return this UI value this UI uh function is going to have a value which",
    "start": "475280",
    "end": "480440"
  },
  {
    "text": "is the jsx element this is what we're going to use to actually send back to our client and it's going to contain the",
    "start": "480440",
    "end": "485639"
  },
  {
    "text": "stream which can update uh anytime we call update aen error done um and this",
    "start": "485639",
    "end": "491159"
  },
  {
    "text": "is what we're going to render on the client we then attach a config so that",
    "start": "491159",
    "end": "496440"
  },
  {
    "text": "this runnable Lambda always has the same name stream UI Lambda and this is what we'll use later on to identify this",
    "start": "496440",
    "end": "502680"
  },
  {
    "text": "runnable Lambda and extract the UI value from it and then we're going to return us invoking that function passing in the",
    "start": "502680",
    "end": "509520"
  },
  {
    "text": "proper config so that using the we pass in the config so that stream events is able to find this runnable Lambda inside",
    "start": "509520",
    "end": "516959"
  },
  {
    "text": "the St stream event the next function we're going to want to implement is the function which",
    "start": "516959",
    "end": "522479"
  },
  {
    "text": "we'll we'll use to invoke our agent this is also going to call stream events and extract any UI values we return from",
    "start": "522479",
    "end": "529040"
  },
  {
    "text": "here so when we invoke our agent it's going to be using calling stream events inside of this function so let's paste",
    "start": "529040",
    "end": "536360"
  },
  {
    "text": "that in and then let's walk through exactly what this does so it's called stream runnable",
    "start": "536360",
    "end": "542200"
  },
  {
    "text": "UI the first line we're creating a new streamable UI from thek and then using",
    "start": "542200",
    "end": "547480"
  },
  {
    "text": "that with resolvers function we implemented below uh the last event which is our promise and then our",
    "start": "547480",
    "end": "553680"
  },
  {
    "text": "resolve function we then have an async function which executes in here uh we",
    "start": "553680",
    "end": "558880"
  },
  {
    "text": "have some callbacks which contain a string and then either the return type of create streamable UI or create",
    "start": "558880",
    "end": "563959"
  },
  {
    "text": "streamable value we'll use those in a second and then as you can see our first input is a runnable",
    "start": "563959",
    "end": "569920"
  },
  {
    "text": "we're going to call stream events on this runnable to stream back all the events this runnable will be our",
    "start": "569920",
    "end": "575720"
  },
  {
    "text": "agent we iterate over each event and the first thing we do is we check to see if it's a UI value which was returned from",
    "start": "575720",
    "end": "582480"
  },
  {
    "text": "this Lambda so as we can see our run name is here we're checking to see if the stream event. name is that run name",
    "start": "582480",
    "end": "589120"
  },
  {
    "text": "and it's after that Lambda has finished so on chain end should be the event if",
    "start": "589120",
    "end": "595000"
  },
  {
    "text": "it is that event and and runnable Lambda we're then going to check and make sure that the value of the output is a valid",
    "start": "595000",
    "end": "602600"
  },
  {
    "text": "UI element if it is then we're going to append that UI element to our UI which",
    "start": "602600",
    "end": "609160"
  },
  {
    "text": "we created via the great streamable UI next we're going to check to make",
    "start": "609160",
    "end": "614240"
  },
  {
    "text": "sure that it's a stream and it's not a chain um this is what we're going to use to extract any text values from the uh",
    "start": "614240",
    "end": "622240"
  },
  {
    "text": "from the our agent or our Ling graph graph that will be this part of our diagram where the LM returns just text",
    "start": "622240",
    "end": "629519"
  },
  {
    "text": "I'm going to send it right back so if it is text and type of chunk. text is",
    "start": "629519",
    "end": "638240"
  },
  {
    "text": "string we want to make sure we have not already processed this this run run event because your llm could in theory",
    "start": "638240",
    "end": "645880"
  },
  {
    "text": "invoke a language model twice and get two sets of text streams back so we make",
    "start": "645880",
    "end": "650920"
  },
  {
    "text": "sure that we've not already processed it if we haven't then as this comment says the create streamable value sluse",
    "start": "650920",
    "end": "656480"
  },
  {
    "text": "streamable value is preferred as the stream events are updated immedi in the UI rather than being batched by react",
    "start": "656480",
    "end": "662320"
  },
  {
    "text": "via create streamable UI so if we're just updating text we want to use this create streamable value and not create",
    "start": "662320",
    "end": "668519"
  },
  {
    "text": "streamable UI then we're going to append our create streamable UI with a generic",
    "start": "668519",
    "end": "676560"
  },
  {
    "text": "react or jsx function this could be you know this is customizable for what you want we have ai message text which is",
    "start": "676560",
    "end": "683079"
  },
  {
    "text": "going to be our text bubbles but you should probably replace this with whatever UI you want and here you say",
    "start": "683079",
    "end": "688760"
  },
  {
    "text": "you see using Create streamable value which we mentioned there and then the value of that is the value of our text",
    "start": "688760",
    "end": "694600"
  },
  {
    "text": "stream and we're using text stream so we can bypass any sort of batching that react does and instantly update our UI",
    "start": "694600",
    "end": "700959"
  },
  {
    "text": "with the stream as it comes in next if the Run idea is true which it",
    "start": "700959",
    "end": "706560"
  },
  {
    "text": "is because we just said it we're going to append the text from the stream event chunk this will be the text language",
    "start": "706560",
    "end": "712519"
  },
  {
    "text": "while stream back and then we're also going to be updating our last event value of the stream event this happens",
    "start": "712519",
    "end": "718279"
  },
  {
    "text": "at the end of each stream event so we know that the last event value will always be the last stream event finally",
    "start": "718279",
    "end": "724680"
  },
  {
    "text": "when our stream events has resolved we're going to resolve our promise which",
    "start": "724680",
    "end": "729800"
  },
  {
    "text": "we implemented in the with resolvers function with the output of our stream event this will typically be some sort of string but it could also be um an",
    "start": "729800",
    "end": "737000"
  },
  {
    "text": "object with say our tool call we're then going to iterate over",
    "start": "737000",
    "end": "742160"
  },
  {
    "text": "all of our callbacks and call done on them to finish our callbacks and then call UI Doone which is going to close",
    "start": "742160",
    "end": "749000"
  },
  {
    "text": "the UI stream between the server and the client via the the aisk finally we're",
    "start": "749000",
    "end": "754320"
  },
  {
    "text": "going to return these values in the last event which we will use in our client when we Implement",
    "start": "754320",
    "end": "759880"
  },
  {
    "text": "that now that we've implemented our stream runnable UI and create runnable UI we can go ahead and Implement our",
    "start": "759880",
    "end": "766279"
  },
  {
    "text": "graph this is going to be this language model graph with the Edge invoking tools",
    "start": "766279",
    "end": "771320"
  },
  {
    "text": "or sending back the response we're going to go into the AI graph file and first things first we're going to add our",
    "start": "771320",
    "end": "777560"
  },
  {
    "text": "Imports as you can see we're not adding any serveron um text because since this",
    "start": "777560",
    "end": "783240"
  },
  {
    "text": "is only going to be used inside of our serveron code here it'll already be",
    "start": "783240",
    "end": "788600"
  },
  {
    "text": "server only we're importing some prompt templates um start and end uh variables",
    "start": "788600",
    "end": "794959"
  },
  {
    "text": "from L graph State graph which is what we're going to use to create our lane graph chat open AI you can obviously",
    "start": "794959",
    "end": "800279"
  },
  {
    "text": "replace this with any language model which supports tool calling from the the lane chain Library um our GitHub tools",
    "start": "800279",
    "end": "806880"
  },
  {
    "text": "which we'll Implement later or after after this I guess we're going to find our base message which we use for types",
    "start": "806880",
    "end": "813279"
  },
  {
    "text": "and then runable con config which we will also use for types once we've added our Imports we're going to want to add",
    "start": "813279",
    "end": "819000"
  },
  {
    "text": "our type for our L graph agent so we're going to name it agent executor State",
    "start": "819000",
    "end": "824480"
  },
  {
    "text": "the first value is a is the input this is going to be the input that the User submitted right here next is chat",
    "start": "824480",
    "end": "831560"
  },
  {
    "text": "history once again the chat history from their previous conversations and then we have some optional values these are",
    "start": "831560",
    "end": "837639"
  },
  {
    "text": "optional because they're only going to be popular ated later on in our graph so result the plain text result in LM if no",
    "start": "837639",
    "end": "843240"
  },
  {
    "text": "tools used that's going to be this part so if the LM does not invoke a tool and only returns some text that's going to",
    "start": "843240",
    "end": "849959"
  },
  {
    "text": "populate this value next is the parse tool result that was called if the LM does call a tool we're going to parse",
    "start": "849959",
    "end": "856600"
  },
  {
    "text": "that and return it that will be pointing to this conditional Edge and then finally the result of a tool that will",
    "start": "856600",
    "end": "862120"
  },
  {
    "text": "be anything that this arbitrary function returns um and we're going to uh actually include the result of that",
    "start": "862120",
    "end": "868399"
  },
  {
    "text": "because we want to update that in our chat history later on so the language model knows that it did in fact um",
    "start": "868399",
    "end": "874839"
  },
  {
    "text": "invoke and complete any tool requests that the user ask for after that we can skip adding our",
    "start": "874839",
    "end": "881279"
  },
  {
    "text": "nodes for now because we're want to con we're going to want to construct the uh graph",
    "start": "881279",
    "end": "886320"
  },
  {
    "text": "first so we're going to create our agent executor function this is going to create a new state graph passing in our",
    "start": "886320",
    "end": "892639"
  },
  {
    "text": "state and then also creating these channels um here we're going to add just two nodes one for invoking the model and",
    "start": "892639",
    "end": "898639"
  },
  {
    "text": "one for for invoking the tools invoke model will obviously be this part make",
    "start": "898639",
    "end": "903920"
  },
  {
    "text": "that a little bigger and then invoke tools will be what happens when we pick a tool and we want to invoke that tool",
    "start": "903920",
    "end": "910320"
  },
  {
    "text": "invoke model will then always call invoke tools or return which is this conditional Edge this conditional Edge",
    "start": "910320",
    "end": "916320"
  },
  {
    "text": "will just check to see if the tool is used if it is used then this function will then return invoke tools so invoke",
    "start": "916320",
    "end": "922399"
  },
  {
    "text": "tools is called if it's not used then it's going to return end which is this end variable and that indic Ates to Lane",
    "start": "922399",
    "end": "929800"
  },
  {
    "text": "graph that it should always finish and that's what this is where it finishes and sends the response back invoke tools",
    "start": "929800",
    "end": "935319"
  },
  {
    "text": "will also then always end so that once the tool is done invoking it returns and",
    "start": "935319",
    "end": "940959"
  },
  {
    "text": "finishes the L graph graph and responds back to the UI and obviously start is",
    "start": "940959",
    "end": "946959"
  },
  {
    "text": "always going to call invoke model because that's the first thing we want to do in all of our graphs we're then",
    "start": "946959",
    "end": "952800"
  },
  {
    "text": "going to compile it and return our graph and then we're going to use this a little bit later on when we're passing it to our stream runnable UI",
    "start": "952800",
    "end": "959920"
  },
  {
    "text": "and this graph right here will be the runnable that is in is invoked via",
    "start": "959920",
    "end": "965079"
  },
  {
    "text": "stream events so if we go back to our graph file the first node we're going to want to implement is the invoke model as",
    "start": "965079",
    "end": "971639"
  },
  {
    "text": "that's going to be the first node which is always called so if we paste that in let make this a little bit bigger we can",
    "start": "971639",
    "end": "976959"
  },
  {
    "text": "see it takes in two values state which is our agent executor State this is kind of the magic behind Lang graph where",
    "start": "976959",
    "end": "983360"
  },
  {
    "text": "it'll always pass the state to every single node even though we're not returning the full State here so Lane",
    "start": "983360",
    "end": "990639"
  },
  {
    "text": "graph can recognize what values in your state that you returned it just appends that to the total State and then passes",
    "start": "990639",
    "end": "997160"
  },
  {
    "text": "the complete State through to each node so we take in our state and then also our config value this is what we're",
    "start": "997160",
    "end": "1003440"
  },
  {
    "text": "going to pass to invoke so that out our Lang Smith traces and the stream events all have the same all contain the same",
    "start": "1003440",
    "end": "1010519"
  },
  {
    "text": "runs in the single trace so stream events can get back all the values first thing inside our functions",
    "start": "1010519",
    "end": "1015639"
  },
  {
    "text": "we Define our prompt you're a helpful assistant you given a list of tools need to determine which tool is best to handle the user input or respond with",
    "start": "1015639",
    "end": "1021920"
  },
  {
    "text": "plain text we then have a message placeholder for the chat history this is where our chat history rle go so the",
    "start": "1021920",
    "end": "1027760"
  },
  {
    "text": "language model has access to all of our history it's obviously going to be optional because there will be no",
    "start": "1027760",
    "end": "1033280"
  },
  {
    "text": "history on the very first invocation and then we have our human message with just a plain input uh",
    "start": "1033280",
    "end": "1040360"
  },
  {
    "text": "argument next we Define our tools we'll Define those after this function but we're going to provide three Tools in",
    "start": "1040360",
    "end": "1046319"
  },
  {
    "text": "language model uh GitHub tool invoice to web weather tool when we Define these tools we'll talk about what each of them",
    "start": "1046319",
    "end": "1052960"
  },
  {
    "text": "do next we're going to Define our llm we're not going to give it we're going to give it a temperature of zero so it's",
    "start": "1052960",
    "end": "1058360"
  },
  {
    "text": "more predictable and not as um creative I guess we're going to use gbt 40 because that's their newest",
    "start": "1058360",
    "end": "1065080"
  },
  {
    "text": "fastest model which can give us text back super quickly and also process our images um and then we're going to bind",
    "start": "1065080",
    "end": "1071280"
  },
  {
    "text": "tools to this model so all of the the model has access to all the tools we've defined next we're going to use the line",
    "start": "1071280",
    "end": "1078039"
  },
  {
    "text": "chain expression language anguage to pipe our prompt to our language model and create a chain and then we're going to invoke our chain passing in our input",
    "start": "1078039",
    "end": "1085080"
  },
  {
    "text": "from the user input and the chat history and then also our config object once",
    "start": "1085080",
    "end": "1090280"
  },
  {
    "text": "this is finished invoking we're going to check to see if any tool calls were on the model or if the model uses any tool",
    "start": "1090280",
    "end": "1096960"
  },
  {
    "text": "calls and if they are we're going to return this tool call Value which will populate this",
    "start": "1096960",
    "end": "1102039"
  },
  {
    "text": "field if the model did not use any tool calls we're just going to return the content now that we've defined our first",
    "start": "1102039",
    "end": "1109159"
  },
  {
    "text": "node we're going to want to Define our conditional Edge which will always be called after this",
    "start": "1109159",
    "end": "1114840"
  },
  {
    "text": "node invoke tools or return this takes in the state and it essentially says if",
    "start": "1114840",
    "end": "1119919"
  },
  {
    "text": "tool calls are defined then you want to call the invoke tools node next and if it's not defined but the result field is",
    "start": "1119919",
    "end": "1127440"
  },
  {
    "text": "defined then we're going to end because that's just the string that was returned and then this should never happen but if",
    "start": "1127440",
    "end": "1133720"
  },
  {
    "text": "for some reason neither of these are defined um then it's going to throw an error but we're never going to get this",
    "start": "1133720",
    "end": "1139360"
  },
  {
    "text": "so that should not cause an issue for us finally we're going to Define our last node invoke",
    "start": "1139360",
    "end": "1145280"
  },
  {
    "text": "tools this takes in the same input arguments as the invoke model our state and our config um and it's going to",
    "start": "1145280",
    "end": "1152679"
  },
  {
    "text": "first check to make sure there's a tool call once again this should never happen because it should only call invoke tools",
    "start": "1152679",
    "end": "1158400"
  },
  {
    "text": "if tool calls are defined but because of typescript we need to add this here but we should never see this error next",
    "start": "1158400",
    "end": "1164280"
  },
  {
    "text": "we're going to Define our tools map which is going to be a map containing key value pairs each each key is going",
    "start": "1164280",
    "end": "1169720"
  },
  {
    "text": "to be the name of the tool and then the value is going to be the actual tool using this map and our tool input we're",
    "start": "1169720",
    "end": "1177080"
  },
  {
    "text": "going to try and find the tool in there once again this should never happen because our language model especially if",
    "start": "1177080",
    "end": "1183080"
  },
  {
    "text": "you're using a um state-of-the-art language model it should never pick a tool which doesn't exist you know that",
    "start": "1183080",
    "end": "1189000"
  },
  {
    "text": "you didn't provide to it um but we have this here once again for typescript once we have our selected tool we're going to",
    "start": "1189000",
    "end": "1194640"
  },
  {
    "text": "then invoke that tool passing the parameters that the tool called for that the language will provided to us in our",
    "start": "1194640",
    "end": "1200640"
  },
  {
    "text": "config and then finally once we get a result back this result will always be a string but usually we're returning an",
    "start": "1200640",
    "end": "1206320"
  },
  {
    "text": "objects we're going to parse that result and then return it in our tool result value which will populate this",
    "start": "1206320",
    "end": "1213679"
  },
  {
    "text": "field now that we've done this we've implemented our entire agent and we can go and Implement our tools which we will",
    "start": "1213679",
    "end": "1219200"
  },
  {
    "text": "then provide to the agent these tools are going to contain all the logic around streaming back UI components and",
    "start": "1219200",
    "end": "1224880"
  },
  {
    "text": "hitting any external apis so for our tools we have this tools folder which contains some files uh get",
    "start": "1224880",
    "end": "1231520"
  },
  {
    "text": "a repo weather and invoice we're just going to implement the get a repo tool and then I'll quickly walk through the other tools because they're all pretty",
    "start": "1231520",
    "end": "1237919"
  },
  {
    "text": "redundant and contain kind of the same logic uh but for for our GitHub repo tool we're going to want to add our",
    "start": "1237919",
    "end": "1243360"
  },
  {
    "text": "Imports first we're going to be using Zod Zod is what we're going to use to define the schema so language model knows what parameters to pass or extract",
    "start": "1243360",
    "end": "1250640"
  },
  {
    "text": "from the input and then pass to our tool um ocit which is the GitHub API wrapper",
    "start": "1250640",
    "end": "1257280"
  },
  {
    "text": "SDK which is is what we're going to use to actually call the giab API create runnable UI which we defined in our",
    "start": "1257280",
    "end": "1262919"
  },
  {
    "text": "server. TSX file which is going to wrap that Lambda create a new streamable UI",
    "start": "1262919",
    "end": "1268000"
  },
  {
    "text": "and that's what we're going to use to actually stream back these UI components our tool from Lane chain and then our",
    "start": "1268000",
    "end": "1274640"
  },
  {
    "text": "pre-built components which we can quickly look at we have a loading component which just contains some",
    "start": "1274640",
    "end": "1279720"
  },
  {
    "text": "skeletons to show that we're loading and then the actual component which contains a card um this card is going to have a",
    "start": "1279720",
    "end": "1285559"
  },
  {
    "text": "link to the GitHub repo it's going to show how many stars they have the repo description and other things like that",
    "start": "1285559",
    "end": "1291640"
  },
  {
    "text": "which we'll get back from the GitHub API the first thing we're going to want to do is Define our schema our schema is",
    "start": "1291640",
    "end": "1297480"
  },
  {
    "text": "can to be the owner in the repo which is the which are the fields that the giab API requires in order to fetch details",
    "start": "1297480",
    "end": "1302600"
  },
  {
    "text": "about a repository um if the language model sees that a user is submitted an input with an owner and repo fields that",
    "start": "1302600",
    "end": "1309679"
  },
  {
    "text": "look like a get of repo it'll likely call this tool and provide us the name of the repository and the repository",
    "start": "1309679",
    "end": "1316320"
  },
  {
    "text": "owner next we're going to Define our fun which we'll actually call the giab API we're going to call it the giab revo",
    "start": "1316320",
    "end": "1322520"
  },
  {
    "text": "tool the input is going to be the type of our Zod schema so z. infer type of",
    "start": "1322520",
    "end": "1327840"
  },
  {
    "text": "our schema and that will infer the type here next we're going to make sure you",
    "start": "1327840",
    "end": "1333760"
  },
  {
    "text": "you have your GitHub API token in your environment if it's not we're going to throw an airror obviously if you're",
    "start": "1333760",
    "end": "1339840"
  },
  {
    "text": "going to use this tool you should set that in the read me of this repo I add instructions on how to get all the API Keys you need for free for the different",
    "start": "1339840",
    "end": "1347720"
  },
  {
    "text": "tools um except for your language model API key which will obviously cost money uh when you invoke the language model",
    "start": "1347720",
    "end": "1354440"
  },
  {
    "text": "we're then going to instantiate our octo kit SDK passing in our GitHub token and that's going to return an instance of",
    "start": "1354440",
    "end": "1360480"
  },
  {
    "text": "the GitHub client then we can just call our GitHub client calling the repos um with a get request and that's going to",
    "start": "1360480",
    "end": "1366720"
  },
  {
    "text": "get us the information on this repo and then we return the data from the um from",
    "start": "1366720",
    "end": "1373640"
  },
  {
    "text": "the API response which is the input so we have the owner and the repo and then the get",
    "start": "1373640",
    "end": "1379000"
  },
  {
    "text": "repo description how many stars they have and the primary programming language if there's an error we're just",
    "start": "1379000",
    "end": "1384480"
  },
  {
    "text": "going to return a string and then we'll process this inside of our tool to return either the GitHub final component",
    "start": "1384480",
    "end": "1391120"
  },
  {
    "text": "or an error component if an error was occurred now we can Implement our tool",
    "start": "1391120",
    "end": "1396679"
  },
  {
    "text": "it's going to be a dynamic structured tool with a name GitHub repo a description tool to fetch details of",
    "start": "1396679",
    "end": "1402000"
  },
  {
    "text": "GitHub repository um we're going to pass in our schema so language model knows what fields to provide and then we have",
    "start": "1402000",
    "end": "1408320"
  },
  {
    "text": "our function inside this function we see it takes two arguments input which should be the schema we defined and then",
    "start": "1408320",
    "end": "1415000"
  },
  {
    "text": "config we're going to first create a new runnable UI stream passing in",
    "start": "1415000",
    "end": "1420520"
  },
  {
    "text": "our initial value which is going to be this loading component and this is going to tell the user that as soon as the",
    "start": "1420520",
    "end": "1425600"
  },
  {
    "text": "language model picks this tool and this function is invoked it's going to the user is instantly going to get back their",
    "start": "1425600",
    "end": "1431279"
  },
  {
    "text": "first loading component so they see that we're working on something that's this step here next we're going to hit our GI",
    "start": "1431279",
    "end": "1438000"
  },
  {
    "text": "of API with this function we defined above passing in our input then if they get of API return to string there's an",
    "start": "1438000",
    "end": "1443960"
  },
  {
    "text": "error and we're just going to return um a P tag with the error message calling",
    "start": "1443960",
    "end": "1449760"
  },
  {
    "text": "stream DOD if the type was not a string so this object here then we're going to",
    "start": "1449760",
    "end": "1455880"
  },
  {
    "text": "return our GitHub component which is that jsx component which will actually show all the data passing our data and",
    "start": "1455880",
    "end": "1461400"
  },
  {
    "text": "finally we're going to return the result of all result of our tool you can also call stream. update or append as as many",
    "start": "1461400",
    "end": "1468640"
  },
  {
    "text": "times you would like if you want to say hit an API update your tool hit another",
    "start": "1468640",
    "end": "1474559"
  },
  {
    "text": "API add some more values to that UI element you can really call update um and append as many times you would like",
    "start": "1474559",
    "end": "1480640"
  },
  {
    "text": "to keep interacting or updating that interactable UI component with the user the nice thing about this as well is",
    "start": "1480640",
    "end": "1486679"
  },
  {
    "text": "since it takes in a react node our GitHub component these don't have state however they could be stateful they",
    "start": "1486679",
    "end": "1492480"
  },
  {
    "text": "could contain some button which hits an API or makes another language model call which then updates the UI again and",
    "start": "1492480",
    "end": "1498360"
  },
  {
    "text": "they're really just generic react components so everything that you could do before with react components and make",
    "start": "1498360",
    "end": "1504480"
  },
  {
    "text": "them super Dynamic and interactable you can do that here as well because it's any sort of react component you want and",
    "start": "1504480",
    "end": "1509679"
  },
  {
    "text": "you can pass props to it so they can take in these different inputs and be dynamic and customizable for the user so",
    "start": "1509679",
    "end": "1515960"
  },
  {
    "text": "now that we've imped these tools we can quickly look at our invoice and weather tool invoice is just a schema this is",
    "start": "1515960",
    "end": "1522600"
  },
  {
    "text": "because we're going to want the language model to extract these fields from any uploaded image and then it just creates",
    "start": "1522600",
    "end": "1528000"
  },
  {
    "text": "a own UI with this initial loading which is kind of redundant because then it instantly turns around and um updates it",
    "start": "1528000",
    "end": "1535320"
  },
  {
    "text": "with the final component but just to show the same thing and then it Returns the input and then for the weather",
    "start": "1535320",
    "end": "1540760"
  },
  {
    "text": "component or for the weather tool same thing our weather schema is a city and state and then an optional country which",
    "start": "1540760",
    "end": "1547080"
  },
  {
    "text": "which it defaults to USA and then it hits a couple apis if you want this API key I've added some instructions in the",
    "start": "1547080",
    "end": "1553559"
  },
  {
    "text": "REM on how to get it it's totally free um it's going to get the uh longitude",
    "start": "1553559",
    "end": "1559120"
  },
  {
    "text": "and latitude from this API and then it's going to use the weather.gov API which is free passing in these values and then",
    "start": "1559120",
    "end": "1566600"
  },
  {
    "text": "it's going to extract the current weather for your location the tool also",
    "start": "1566600",
    "end": "1572120"
  },
  {
    "text": "the same thing creates a stream passes back that loading weather component then it invokes our weather data function to",
    "start": "1572120",
    "end": "1579320"
  },
  {
    "text": "actually get the weather data from these apis and finally it updates the weather component with our um with the data that",
    "start": "1579320",
    "end": "1587559"
  },
  {
    "text": "the API returns finally we're going to want to implement our chat jsx component which is going to",
    "start": "1587559",
    "end": "1593880"
  },
  {
    "text": "be the chat you can interact with if you want to follow along you should go to pre component prebuilt chat this going",
    "start": "1593880",
    "end": "1599880"
  },
  {
    "text": "to be a client component because it doesn't is not actually uh using anything in the server instead it's",
    "start": "1599880",
    "end": "1605039"
  },
  {
    "text": "going to call our server component we're going to have a state input which are just some sh Shad CN components that",
    "start": "1605039",
    "end": "1612960"
  },
  {
    "text": "they built uh endpoints endpoints context which we already defined",
    "start": "1612960",
    "end": "1619600"
  },
  {
    "text": "here or sorry we need to Define this after this um our use actions which we",
    "start": "1619600",
    "end": "1624640"
  },
  {
    "text": "saw how that was defined that's going to provide the agent act action for us and then our context which is going to",
    "start": "1624640",
    "end": "1630799"
  },
  {
    "text": "provide context to our action and also um render our UI",
    "start": "1630799",
    "end": "1636600"
  },
  {
    "text": "elements these are just some util functions on converting files to base 64",
    "start": "1636600",
    "end": "1641880"
  },
  {
    "text": "we need to convert them to a Bas 64 string on the client because react server components don't allow for any",
    "start": "1641880",
    "end": "1647440"
  },
  {
    "text": "arbitrary function fun to be passed over the or sorry any arbitrary object to be passed over the wire only specific",
    "start": "1647440",
    "end": "1653919"
  },
  {
    "text": "objects and obviously uh key value pairs where they're both strings and we would need to convert it to Bas 64 in the",
    "start": "1653919",
    "end": "1659799"
  },
  {
    "text": "server anyway so we just do it on the client U because they don't allow for file objects to get passed over the wire",
    "start": "1659799",
    "end": "1665120"
  },
  {
    "text": "so it's now string and we send it over and then we use it to invoke our language model for our chat function we're going",
    "start": "1665120",
    "end": "1672399"
  },
  {
    "text": "to want to add our state variables first our use actions hook providing our",
    "start": "1672399",
    "end": "1677480"
  },
  {
    "text": "endpoint cont text which is our agent takes in our inputs which we also see here this is how we're going to then",
    "start": "1677480",
    "end": "1683880"
  },
  {
    "text": "invoke our agent we then have a few State variables Elements which is a list of jsx elements these are going to be",
    "start": "1683880",
    "end": "1689760"
  },
  {
    "text": "the elements that the UI returns to be rendered uh your chat history chat",
    "start": "1689760",
    "end": "1695080"
  },
  {
    "text": "history input and then a selected file you've selected here we see we're wrapping our elements with this local",
    "start": "1695080",
    "end": "1700919"
  },
  {
    "text": "context. provider um context jsx uh element and that's going to provide",
    "start": "1700919",
    "end": "1706279"
  },
  {
    "text": "context to our react elements and then we have a simple form as we would see right here just for submitting",
    "start": "1706279",
    "end": "1713200"
  },
  {
    "text": "your inputs and any uh images you might upload next we're going to want to implement our on submit function which",
    "start": "1713200",
    "end": "1719559"
  },
  {
    "text": "is going to actually call our agent so this on submit function does a few things we're going to paste it in",
    "start": "1719559",
    "end": "1725679"
  },
  {
    "text": "and then walk through it so first it makes a copy of our elements array so in case this somehow gets updated later on",
    "start": "1725679",
    "end": "1732200"
  },
  {
    "text": "it's not going to mix and match them so it's always going to be the same at the beginning of the function it's then",
    "start": "1732200",
    "end": "1737559"
  },
  {
    "text": "going to convert your file to base 64 string format if you did upload one and",
    "start": "1737559",
    "end": "1742679"
  },
  {
    "text": "then it's going to use our action our agent action to invoke our agent this is going to return an element which is UI",
    "start": "1742679",
    "end": "1749640"
  },
  {
    "text": "and last event we saw that here it's essentially calling this function which",
    "start": "1749640",
    "end": "1755679"
  },
  {
    "text": "returns our UI and last event passes any necessary inputs and",
    "start": "1755679",
    "end": "1761320"
  },
  {
    "text": "then updates our element array with the UI value from our return value from our",
    "start": "1761320",
    "end": "1766799"
  },
  {
    "text": "agent and then also so the human message that the User submitted and then if they uploaded a file the",
    "start": "1766799",
    "end": "1773120"
  },
  {
    "text": "file finally we saw in our server function the first function we implemented was was this with resolvers",
    "start": "1773120",
    "end": "1779799"
  },
  {
    "text": "we're going to use that here so element. last event which once again is",
    "start": "1779799",
    "end": "1786960"
  },
  {
    "text": "the last event here from with from our with resolvers",
    "start": "1786960",
    "end": "1792559"
  },
  {
    "text": "function it's then going to check and see if it's an object this right here is specific to this chatbot we've",
    "start": "1792559",
    "end": "1799440"
  },
  {
    "text": "implemented so you're obviously going to want to update these to reflect your L graph node but we have or nodes we have",
    "start": "1799440",
    "end": "1807519"
  },
  {
    "text": "invoke model and invoke tools as we saw we defined in our graph so if it's an",
    "start": "1807519",
    "end": "1812799"
  },
  {
    "text": "object then we're going to want to see if last event. invoke model. result is",
    "start": "1812799",
    "end": "1817960"
  },
  {
    "text": "true that would be this plain text string if it is then we update the history with that plain text and if it's",
    "start": "1817960",
    "end": "1824480"
  },
  {
    "text": "not true that means that tool was used so we see invoke tool and then we update our history um but",
    "start": "1824480",
    "end": "1830480"
  },
  {
    "text": "make the assistant message be this tool result and then the result of our tools and that's so the the assistant knows",
    "start": "1830480",
    "end": "1836720"
  },
  {
    "text": "it's it's successfully completed the request we made in the past using the",
    "start": "1836720",
    "end": "1842240"
  },
  {
    "text": "result of our tool API um and that's so you can say something like uh you know what's the details on this GI up repo it",
    "start": "1842240",
    "end": "1849320"
  },
  {
    "text": "sends the response it didn't actually give you any text there wouldn't be any text in the chat history but since we",
    "start": "1849320",
    "end": "1854640"
  },
  {
    "text": "are creating this assistant message when you send a followup it's able to see your question and see that it was",
    "start": "1854640",
    "end": "1860200"
  },
  {
    "text": "submitted or resolved and in those it should ignore any questions in your chat history because it's already resolved",
    "start": "1860200",
    "end": "1866480"
  },
  {
    "text": "them here finally we clean up by setting our elements State value and then",
    "start": "1866480",
    "end": "1872039"
  },
  {
    "text": "clearing any inputs finally we can go and Implement our agent wrapper which is going to uh",
    "start": "1872039",
    "end": "1879440"
  },
  {
    "text": "use this stream runnable UI function and call our graph agent executor function",
    "start": "1879440",
    "end": "1887440"
  },
  {
    "text": "um and and that's what we're going to use to invoke our agents let's go Implement that now so if you're following along you should go to app SL",
    "start": "1887440",
    "end": "1894080"
  },
  {
    "text": "agent and you're going to want to paste in our Imports once again server only because we're going from this client",
    "start": "1894080",
    "end": "1900200"
  },
  {
    "text": "component to our server so we want to make sure it's only invoked on the server we're then going to import our",
    "start": "1900200",
    "end": "1905480"
  },
  {
    "text": "agent executor from our graph which we built a little while ago and our expose endpoints and stream runal UI from our",
    "start": "1905480",
    "end": "1911559"
  },
  {
    "text": "server file which we invoked in the first part finally we're going to want to import our AI message and human",
    "start": "1911559",
    "end": "1917399"
  },
  {
    "text": "message uh Lane chain message types we're going to use this when we're constructing our chat history to give",
    "start": "1917399",
    "end": "1922639"
  },
  {
    "text": "the proper message types to our um chat history and this will also when we look",
    "start": "1922639",
    "end": "1928440"
  },
  {
    "text": "at the Lang graph Trace you'll be able to see human message AI message human message AI message um in the proper",
    "start": "1928440",
    "end": "1935440"
  },
  {
    "text": "types uh so the language W knows what or who said what now we can implement this little",
    "start": "1935440",
    "end": "1942519"
  },
  {
    "text": "util function for converting our chat history type see from chat roll and",
    "start": "1942519",
    "end": "1948000"
  },
  {
    "text": "content into a proper list of human message and AI message so iterate over",
    "start": "1948000",
    "end": "1954559"
  },
  {
    "text": "if it was a human then return a human message or is thiser AI return an AI message and then by default we're",
    "start": "1954559",
    "end": "1960760"
  },
  {
    "text": "returning a human message this could also be say chat message from linkchain",
    "start": "1960760",
    "end": "1965799"
  },
  {
    "text": "core and this would just be a generic",
    "start": "1965799",
    "end": "1970398"
  },
  {
    "text": "um we also need a roll so you could say you know roll um and this would just be a",
    "start": "1971320",
    "end": "1979039"
  },
  {
    "text": "generic chat message and it wouldn't be specifically human or AI but in our case this will probably never happen because",
    "start": "1979039",
    "end": "1984960"
  },
  {
    "text": "we know we're always adding this role and assistant but we're adding it just to make our switch case happy next since we're doing image um",
    "start": "1984960",
    "end": "1993000"
  },
  {
    "text": "inputs we need this process file function this is essentially going to process any files you upload and convert",
    "start": "1993000",
    "end": "1998720"
  },
  {
    "text": "them to the proper message type for passing multimodal inputs to our language model so if file is defined",
    "start": "1998720",
    "end": "2005760"
  },
  {
    "text": "it's going to create a new human message um passing into content which is a list of Type image URL and then image URL",
    "start": "2005760",
    "end": "2013519"
  },
  {
    "text": "with our base 64 uh conversion of our file and this is the right or this is the proper uh",
    "start": "2013519",
    "end": "2020360"
  },
  {
    "text": "template format for uploading multimodal types to language models I'll add a link",
    "start": "2020360",
    "end": "2025600"
  },
  {
    "text": "in the description to our multimodel how-to guide in the JS documentation if you're interested in that and then",
    "start": "2025600",
    "end": "2031760"
  },
  {
    "text": "finally we return our input and our chat history which matches as we saw the",
    "start": "2031760",
    "end": "2038679"
  },
  {
    "text": "input in chat history from our Lan graph agent if you didn't upload a file then",
    "start": "2038679",
    "end": "2043880"
  },
  {
    "text": "we just need input in chat history and don't need to do anything with image prompt templates now we need to implement our",
    "start": "2043880",
    "end": "2050919"
  },
  {
    "text": "agent function this is going to be the function that when you call where is it actions. agent um this",
    "start": "2050919",
    "end": "2058638"
  },
  {
    "text": "is going to be the function that's going to that it's going to invoke so it's going to take in as we see the same inputs we were passing in here input",
    "start": "2058639",
    "end": "2065720"
  },
  {
    "text": "chat history and file um use server so we make sure that this is always executed on the server as in a",
    "start": "2065720",
    "end": "2072560"
  },
  {
    "text": "react server component we're then going to this does not need to be a",
    "start": "2072560",
    "end": "2078560"
  },
  {
    "text": "sync um we're then going to process the file to get the process inputs which is",
    "start": "2078560",
    "end": "2083679"
  },
  {
    "text": "input in chat history that's going to do what we saw up here and then finally it's going to return a new stream",
    "start": "2083679",
    "end": "2088760"
  },
  {
    "text": "runnable UI passing in our Asian executor which is our Lang graph agent and any inputs um we saw this already",
    "start": "2088760",
    "end": "2095079"
  },
  {
    "text": "but this is just going to call stream events on our runable which we pass into",
    "start": "2095079",
    "end": "2100160"
  },
  {
    "text": "it which is our agent executor function and the last thing we need to do is",
    "start": "2100160",
    "end": "2105440"
  },
  {
    "text": "actually create uh expose the context for this agent so that our actions.",
    "start": "2105440",
    "end": "2110480"
  },
  {
    "text": "agent um and with our use actions hook has the proper context available to invoke",
    "start": "2110480",
    "end": "2117160"
  },
  {
    "text": "this so as we see here there's an error because exposed context does not exist yet but once we expose our context",
    "start": "2117160",
    "end": "2125040"
  },
  {
    "text": "passing in our agent function and Export this the goes away and as we",
    "start": "2125040",
    "end": "2130160"
  },
  {
    "text": "see with our actions. aent it has access to our agent function with the same",
    "start": "2130160",
    "end": "2136000"
  },
  {
    "text": "inputs we just defined here we see we're passing expose uh endpoints this agent",
    "start": "2136000",
    "end": "2142079"
  },
  {
    "text": "function with the same inputs and this just gives the proper context to um our",
    "start": "2142079",
    "end": "2147240"
  },
  {
    "text": "CL C client component so it knows that it can invoke this agent now that we've",
    "start": "2147240",
    "end": "2152599"
  },
  {
    "text": "implemented all this we can go and actually demo our application",
    "start": "2152599",
    "end": "2158160"
  },
  {
    "text": "restart",
    "start": "2159720",
    "end": "2162720"
  },
  {
    "text": "go now that we've implemented all this we can go and start our Dev server and actually check out our demo so we're",
    "start": "2198440",
    "end": "2204800"
  },
  {
    "text": "going to want to navigate to your terminal go into the proper directory and run yarn Dev or you can build it run",
    "start": "2204800",
    "end": "2210319"
  },
  {
    "text": "yarn start kind of the same once it's running we're going to go to Local Host",
    "start": "2210319",
    "end": "2215920"
  },
  {
    "text": "3000 and we will see our website so gener VII with L chain we see our chat bot and our inputs um we can say",
    "start": "2215920",
    "end": "2222720"
  },
  {
    "text": "something like what's the weather in SF it then streamed back our text if we",
    "start": "2222720",
    "end": "2230960"
  },
  {
    "text": "go back here we can see that the language model decided did not have enough um inputs from the user to select",
    "start": "2230960",
    "end": "2237400"
  },
  {
    "text": "the weather tool or any other any of the other tools so it just sent back some text but since we've implemented chat",
    "start": "2237400",
    "end": "2243560"
  },
  {
    "text": "history can it it said can you please specify the state so we can just say California and it's going to use our",
    "start": "2243560",
    "end": "2250040"
  },
  {
    "text": "chat history and our current message to then invoke the weather tool and we see right there there's a loading component",
    "start": "2250040",
    "end": "2256240"
  },
  {
    "text": "because it picked the weather tool sent us back our loading component and then went and actually hit the weather apis",
    "start": "2256240",
    "end": "2261599"
  },
  {
    "text": "and then updated this component to show the actual weather we can also say",
    "start": "2261599",
    "end": "2266720"
  },
  {
    "text": "something like what's the deal with my invoice and we can upload an image let's",
    "start": "2266720",
    "end": "2274440"
  },
  {
    "text": "say we upload our receipt we submit that and this is going to use GPD 40's multimodal capability to read our image",
    "start": "2274440",
    "end": "2281640"
  },
  {
    "text": "and then send us back this nice fully interactable if you implemented this component it wasn't just a demo um uh",
    "start": "2281640",
    "end": "2288720"
  },
  {
    "text": "receipt component or invoice component um and this is all populated with the receipt image I uploaded so it extracted",
    "start": "2288720",
    "end": "2295240"
  },
  {
    "text": "the fields and then passed in the proper properties to this component so it could render and then finally we can check out",
    "start": "2295240",
    "end": "2302000"
  },
  {
    "text": "our GitHub component which we implemented so what the info on",
    "start": "2302000",
    "end": "2308079"
  },
  {
    "text": "Lang chain AI SL Lang graph and our language model is going to be able to",
    "start": "2308079",
    "end": "2313560"
  },
  {
    "text": "recognize that this is a in a get up repo so we submit that and then error",
    "start": "2313560",
    "end": "2320319"
  },
  {
    "text": "doing that so I wonder if I spelled something wrong yes I spelled Lang graph wrong try Lang chain AI",
    "start": "2320319",
    "end": "2328520"
  },
  {
    "text": "SL Lang graph so as we saw there it gave us our",
    "start": "2328520",
    "end": "2334440"
  },
  {
    "text": "loading component but then the get of API returned an error so it just responded with this string when I did it",
    "start": "2334440",
    "end": "2339680"
  },
  {
    "text": "properly it gave us the proper component and then obviously it's fully interactable so you can click on this and it'll bring you to the L graph",
    "start": "2339680",
    "end": "2346839"
  },
  {
    "text": "repository that's it for this video um if you're interested in the python video it's going to come out tomorrow or if",
    "start": "2346839",
    "end": "2352119"
  },
  {
    "text": "it's already released then we will link in the description where we will implement this exact same uh chatbot",
    "start": "2352119",
    "end": "2357920"
  },
  {
    "text": "demo website but with a full python backend I will see you all in the next video",
    "start": "2357920",
    "end": "2365319"
  }
]