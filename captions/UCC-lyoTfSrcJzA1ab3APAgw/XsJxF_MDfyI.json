[
  {
    "text": "hey this is Lance Lang chain Google just",
    "start": "40",
    "end": "2080"
  },
  {
    "text": "released Gemma 3 smaller compact but",
    "start": "2080",
    "end": "4440"
  },
  {
    "text": "high performance models that presumably",
    "start": "4440",
    "end": "7040"
  },
  {
    "text": "are distilled from one of the larger",
    "start": "7040",
    "end": "8960"
  },
  {
    "text": "Gemini models so you can kind of think",
    "start": "8960",
    "end": "10360"
  },
  {
    "text": "about this as Gemini mini now this comes",
    "start": "10360",
    "end": "13519"
  },
  {
    "text": "in a few different sizes 1B 4B 12b and",
    "start": "13519",
    "end": "17439"
  },
  {
    "text": "27b they are open source which is nice",
    "start": "17439",
    "end": "19880"
  },
  {
    "text": "as you can run them locally for example",
    "start": "19880",
    "end": "21600"
  },
  {
    "text": "on your device and we'll be showing that",
    "start": "21600",
    "end": "23720"
  },
  {
    "text": "shortly they're multimodal they're up to",
    "start": "23720",
    "end": "25680"
  },
  {
    "text": "120k tokens 140 languages structured",
    "start": "25680",
    "end": "29039"
  },
  {
    "text": "outputs and function col so these look",
    "start": "29039",
    "end": "30759"
  },
  {
    "text": "like very nice compact high performance",
    "start": "30759",
    "end": "33160"
  },
  {
    "text": "models that can be run locally and",
    "start": "33160",
    "end": "35320"
  },
  {
    "text": "privately so again here's the point",
    "start": "35320",
    "end": "36680"
  },
  {
    "text": "about distillation they are distilled",
    "start": "36680",
    "end": "38719"
  },
  {
    "text": "from larger instruct models probably one",
    "start": "38719",
    "end": "40800"
  },
  {
    "text": "of the Gemini models and they employ rhf",
    "start": "40800",
    "end": "44239"
  },
  {
    "text": "as well as RMF and RF for Math and",
    "start": "44239",
    "end": "47800"
  },
  {
    "text": "coding now here's some interesting",
    "start": "47800",
    "end": "49160"
  },
  {
    "text": "evaluation scores so Gemma 3 27b score",
    "start": "49160",
    "end": "53320"
  },
  {
    "text": "similar to Gemini 1.5 Pro an earlier",
    "start": "53320",
    "end": "57039"
  },
  {
    "text": "version of the Gemini model series so",
    "start": "57039",
    "end": "60320"
  },
  {
    "text": "you're seeing some clear performance",
    "start": "60320",
    "end": "61879"
  },
  {
    "text": "benefits from distillation and what's",
    "start": "61879",
    "end": "63680"
  },
  {
    "text": "really interesting is that the 27",
    "start": "63680",
    "end": "65600"
  },
  {
    "text": "billion parameter Gemma 3 model has an",
    "start": "65600",
    "end": "68920"
  },
  {
    "text": "ELO score of 1339 if you look at chapot",
    "start": "68920",
    "end": "71080"
  },
  {
    "text": "Arina that actually ranks it quite high",
    "start": "71080",
    "end": "72880"
  },
  {
    "text": "it's tied for ninth place here in the",
    "start": "72880",
    "end": "75040"
  },
  {
    "text": "company of some very strong and much",
    "start": "75040",
    "end": "77280"
  },
  {
    "text": "larger models and this was a nice way to",
    "start": "77280",
    "end": "79759"
  },
  {
    "text": "represent this so if you look at ELO",
    "start": "79759",
    "end": "81600"
  },
  {
    "text": "score versus model size 27 billion",
    "start": "81600",
    "end": "84640"
  },
  {
    "text": "parameter Gemma 3 does quite well for",
    "start": "84640",
    "end": "88040"
  },
  {
    "text": "its size you can see it occupies this",
    "start": "88040",
    "end": "89520"
  },
  {
    "text": "pretty interesting regime up in this",
    "start": "89520",
    "end": "91479"
  },
  {
    "text": "corner where it's a very small model",
    "start": "91479",
    "end": "93560"
  },
  {
    "text": "comparatively speaking comparing to like",
    "start": "93560",
    "end": "95759"
  },
  {
    "text": "deep seek R1 or deep seek V3 but it's",
    "start": "95759",
    "end": "98840"
  },
  {
    "text": "quite strong in terms of Elo score now",
    "start": "98840",
    "end": "100640"
  },
  {
    "text": "what's really neat is these models are",
    "start": "100640",
    "end": "101840"
  },
  {
    "text": "small enough you can run them locally so",
    "start": "101840",
    "end": "103960"
  },
  {
    "text": "if I check out AMA for example I can see",
    "start": "103960",
    "end": "106320"
  },
  {
    "text": "that they have the 1B 4B 12b and 27",
    "start": "106320",
    "end": "109840"
  },
  {
    "text": "billion parameter models all provided",
    "start": "109840",
    "end": "112799"
  },
  {
    "text": "and you can have a look here you can see",
    "start": "112799",
    "end": "114320"
  },
  {
    "text": "how large they are now 27 billion",
    "start": "114320",
    "end": "116759"
  },
  {
    "text": "parameter model is a bit much for my",
    "start": "116759",
    "end": "118680"
  },
  {
    "text": "device I'm on an M2 MacBook Pro 32 gigs",
    "start": "118680",
    "end": "121560"
  },
  {
    "text": "but I can very comfortably run the 4",
    "start": "121560",
    "end": "123360"
  },
  {
    "text": "billion parameter model and probably the",
    "start": "123360",
    "end": "125320"
  },
  {
    "text": "12 billion as well and let's try testing",
    "start": "125320",
    "end": "127240"
  },
  {
    "text": "out the 4 billion parameter model so all",
    "start": "127240",
    "end": "128920"
  },
  {
    "text": "I'm going to do is a llama pull Jama",
    "start": "128920",
    "end": "131400"
  },
  {
    "text": "34b and if I want to run this all I",
    "start": "131400",
    "end": "133800"
  },
  {
    "text": "would need to do is just pip install L",
    "start": "133800",
    "end": "135480"
  },
  {
    "text": "CH llama and then initialize my chat",
    "start": "135480",
    "end": "138840"
  },
  {
    "text": "model as seen here and specify the model",
    "start": "138840",
    "end": "141000"
  },
  {
    "text": "name now let's actually run this with a",
    "start": "141000",
    "end": "143040"
  },
  {
    "text": "library that I've been using for testing",
    "start": "143040",
    "end": "145080"
  },
  {
    "text": "many different local models this is a",
    "start": "145080",
    "end": "146519"
  },
  {
    "text": "llama deep researcher so this is a repo",
    "start": "146519",
    "end": "148800"
  },
  {
    "text": "that I have that it's a little deep",
    "start": "148800",
    "end": "150400"
  },
  {
    "text": "research assistant that runs locally",
    "start": "150400",
    "end": "152560"
  },
  {
    "text": "it's configured to run with lots of",
    "start": "152560",
    "end": "153760"
  },
  {
    "text": "different search engines and what it's",
    "start": "153760",
    "end": "156400"
  },
  {
    "text": "going to do is simply an iterative",
    "start": "156400",
    "end": "158040"
  },
  {
    "text": "search and summarization Loop for as",
    "start": "158040",
    "end": "160840"
  },
  {
    "text": "long as you want what's cool is it uses",
    "start": "160840",
    "end": "163400"
  },
  {
    "text": "local models so it's free so basically",
    "start": "163400",
    "end": "165760"
  },
  {
    "text": "it'll take a topic generate a search",
    "start": "165760",
    "end": "167239"
  },
  {
    "text": "query now that search query generation",
    "start": "167239",
    "end": "169519"
  },
  {
    "text": "we use structured outputs to basically",
    "start": "169519",
    "end": "171760"
  },
  {
    "text": "format the query and we'll then kick off",
    "start": "171760",
    "end": "174440"
  },
  {
    "text": "a web search from that structured query",
    "start": "174440",
    "end": "176720"
  },
  {
    "text": "take those search results and distill or",
    "start": "176720",
    "end": "178599"
  },
  {
    "text": "summarize them reflect on the summary to",
    "start": "178599",
    "end": "181000"
  },
  {
    "text": "generate a new query and repeat so this",
    "start": "181000",
    "end": "183360"
  },
  {
    "text": "process will repeat as long as you want",
    "start": "183360",
    "end": "185280"
  },
  {
    "text": "so to run this all you need to do is",
    "start": "185280",
    "end": "186879"
  },
  {
    "text": "just you can look at the readme here and",
    "start": "186879",
    "end": "189239"
  },
  {
    "text": "you can just run this command to spin up",
    "start": "189239",
    "end": "191760"
  },
  {
    "text": "langra Studio which will run the Deep",
    "start": "191760",
    "end": "193519"
  },
  {
    "text": "researcher locally on your machine so",
    "start": "193519",
    "end": "195400"
  },
  {
    "text": "I'm in curser now I just want to show",
    "start": "195400",
    "end": "196760"
  },
  {
    "text": "really quickly all I've done so",
    "start": "196760",
    "end": "198599"
  },
  {
    "text": "basically AMA pull and you specify the",
    "start": "198599",
    "end": "202080"
  },
  {
    "text": "model you want in this case I'm going to",
    "start": "202080",
    "end": "203680"
  },
  {
    "text": "grab the 4 billion parameter model and",
    "start": "203680",
    "end": "205400"
  },
  {
    "text": "I've already pulled it so that's really",
    "start": "205400",
    "end": "206599"
  },
  {
    "text": "quick and then all I need to do is as",
    "start": "206599",
    "end": "209799"
  },
  {
    "text": "mentioned the read me just kick off my",
    "start": "209799",
    "end": "212200"
  },
  {
    "text": "local deep researcher and you'll see",
    "start": "212200",
    "end": "213680"
  },
  {
    "text": "studio just spin up in your browser so",
    "start": "213680",
    "end": "215480"
  },
  {
    "text": "this is the local deep researcher and",
    "start": "215480",
    "end": "217519"
  },
  {
    "text": "what's really nice is just go to manage",
    "start": "217519",
    "end": "219159"
  },
  {
    "text": "assistants and I've already created an",
    "start": "219159",
    "end": "220840"
  },
  {
    "text": "assistant with this model all I need to",
    "start": "220840",
    "end": "222720"
  },
  {
    "text": "do is specify the name of the local LM I",
    "start": "222720",
    "end": "225040"
  },
  {
    "text": "pulled and everything else is can be",
    "start": "225040",
    "end": "227680"
  },
  {
    "text": "configured as you want in this case I'll",
    "start": "227680",
    "end": "229480"
  },
  {
    "text": "use tavali for search but again you can",
    "start": "229480",
    "end": "231360"
  },
  {
    "text": "use duck Dogo or perplexity and you can",
    "start": "231360",
    "end": "233879"
  },
  {
    "text": "set the number of research Loops you",
    "start": "233879",
    "end": "235079"
  },
  {
    "text": "want to do maximum I'll say three cool",
    "start": "235079",
    "end": "237680"
  },
  {
    "text": "and I just have to hit activate to",
    "start": "237680",
    "end": "240280"
  },
  {
    "text": "initiate this particular assistant with",
    "start": "240280",
    "end": "243120"
  },
  {
    "text": "these configurations and again this is",
    "start": "243120",
    "end": "245200"
  },
  {
    "text": "going to use aama under the hood so all",
    "start": "245200",
    "end": "247159"
  },
  {
    "text": "I to to do is specify the new model name",
    "start": "247159",
    "end": "249840"
  },
  {
    "text": "and I can give it a research topic and",
    "start": "249840",
    "end": "251599"
  },
  {
    "text": "I'd be really interested in model",
    "start": "251599",
    "end": "252959"
  },
  {
    "text": "context protocol recently or mCP so I",
    "start": "252959",
    "end": "255400"
  },
  {
    "text": "basically ask give me an overview of",
    "start": "255400",
    "end": "257239"
  },
  {
    "text": "model context protocol and usage with",
    "start": "257239",
    "end": "259079"
  },
  {
    "text": "cursor as an example and again you can",
    "start": "259079",
    "end": "261600"
  },
  {
    "text": "see my configuration here I'll be using",
    "start": "261600",
    "end": "263880"
  },
  {
    "text": "my new local model and I kick that off",
    "start": "263880",
    "end": "266360"
  },
  {
    "text": "cool so generated some search queries",
    "start": "266360",
    "end": "268240"
  },
  {
    "text": "web search is done now it's doing a",
    "start": "268240",
    "end": "270400"
  },
  {
    "text": "summarization and we can see the",
    "start": "270400",
    "end": "271759"
  },
  {
    "text": "summarize summarization streaming as we",
    "start": "271759",
    "end": "274080"
  },
  {
    "text": "go now it will reflect and determine any",
    "start": "274080",
    "end": "276759"
  },
  {
    "text": "knowledge gaps it runs nice and fast",
    "start": "276759",
    "end": "278840"
  },
  {
    "text": "it's a 4 billion parameter models which",
    "start": "278840",
    "end": "280320"
  },
  {
    "text": "is really nice it's going to kick off",
    "start": "280320",
    "end": "282520"
  },
  {
    "text": "another round of summarization so it's",
    "start": "282520",
    "end": "284280"
  },
  {
    "text": "actually moving quite fast here we can",
    "start": "284280",
    "end": "285720"
  },
  {
    "text": "see it stream reflects and produces",
    "start": "285720",
    "end": "287880"
  },
  {
    "text": "followup queries for us web search will",
    "start": "287880",
    "end": "290400"
  },
  {
    "text": "use the followup queries and again we",
    "start": "290400",
    "end": "292199"
  },
  {
    "text": "continue to iteratively improve our",
    "start": "292199",
    "end": "293759"
  },
  {
    "text": "summary based upon each search iteration",
    "start": "293759",
    "end": "296479"
  },
  {
    "text": "and we get kind of a nice overview here",
    "start": "296479",
    "end": "298039"
  },
  {
    "text": "along with the sources it collected in",
    "start": "298039",
    "end": "300360"
  },
  {
    "text": "its iterative search so we can see that",
    "start": "300360",
    "end": "302840"
  },
  {
    "text": "Gemma runs pretty quickly we can go",
    "start": "302840",
    "end": "304479"
  },
  {
    "text": "ahead and open up the Langs Smith trace",
    "start": "304479",
    "end": "306120"
  },
  {
    "text": "and actually inspect it in a little bit",
    "start": "306120",
    "end": "307360"
  },
  {
    "text": "more detail and this is pretty cool so",
    "start": "307360",
    "end": "308960"
  },
  {
    "text": "we can see each chadow llama call to",
    "start": "308960",
    "end": "312199"
  },
  {
    "text": "Gemma how fast 1 and 1/2 seconds 5",
    "start": "312199",
    "end": "314680"
  },
  {
    "text": "seconds for summarization 4 and 1/2",
    "start": "314680",
    "end": "317080"
  },
  {
    "text": "seconds for reflection so it's pretty",
    "start": "317080",
    "end": "319160"
  },
  {
    "text": "quick and again this was all zero cost",
    "start": "319160",
    "end": "321319"
  },
  {
    "text": "us because this ran locally and I'm",
    "start": "321319",
    "end": "323520"
  },
  {
    "text": "using a free tier of tavali web search",
    "start": "323520",
    "end": "326199"
  },
  {
    "text": "so that's also free for me and it is",
    "start": "326199",
    "end": "327960"
  },
  {
    "text": "worth noting that in this test it uses",
    "start": "327960",
    "end": "329960"
  },
  {
    "text": "structured outputs and so it appears",
    "start": "329960",
    "end": "331919"
  },
  {
    "text": "that Gemma is producing valid Json",
    "start": "331919",
    "end": "335120"
  },
  {
    "text": "effectively as we want so overall this",
    "start": "335120",
    "end": "337800"
  },
  {
    "text": "looks like a really nice new local model",
    "start": "337800",
    "end": "341280"
  },
  {
    "text": "with a good variation in size the 1B 12b",
    "start": "341280",
    "end": "346120"
  },
  {
    "text": "are all fairly feasible on higher-end",
    "start": "346120",
    "end": "349600"
  },
  {
    "text": "consumer laptops the 27b which is quite",
    "start": "349600",
    "end": "352960"
  },
  {
    "text": "performant based upon evaluation as",
    "start": "352960",
    "end": "355160"
  },
  {
    "text": "mentioned previously probably requires a",
    "start": "355160",
    "end": "357400"
  },
  {
    "text": "GPU but it's really nice to see that",
    "start": "357400",
    "end": "359720"
  },
  {
    "text": "this new release from Google and I'm",
    "start": "359720",
    "end": "362720"
  },
  {
    "text": "excited to work with these models a bit",
    "start": "362720",
    "end": "363880"
  },
  {
    "text": "more so have a look and test them out",
    "start": "363880",
    "end": "366280"
  },
  {
    "text": "for yourself feel free to leave any",
    "start": "366280",
    "end": "367680"
  },
  {
    "text": "comments below",
    "start": "367680",
    "end": "370400"
  }
]