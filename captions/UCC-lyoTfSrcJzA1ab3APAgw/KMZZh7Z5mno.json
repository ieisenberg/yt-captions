[
  {
    "text": "hey this is Lance from Lang chain uh I",
    "start": "4400",
    "end": "7000"
  },
  {
    "text": "want to share some results that we've uh",
    "start": "7000",
    "end": "8719"
  },
  {
    "text": "recently working on related to uh rag",
    "start": "8719",
    "end": "11400"
  },
  {
    "text": "over documents that contain a kind of a",
    "start": "11400",
    "end": "13280"
  },
  {
    "text": "mixture of texts and",
    "start": "13280",
    "end": "15039"
  },
  {
    "text": "tables um the motivation here is that",
    "start": "15039",
    "end": "17640"
  },
  {
    "text": "tables are present in a wide range of",
    "start": "17640",
    "end": "19400"
  },
  {
    "text": "documents um Financial reports earnings",
    "start": "19400",
    "end": "22920"
  },
  {
    "text": "reports things along those lines",
    "start": "22920",
    "end": "24359"
  },
  {
    "text": "academic papers uh white papers um",
    "start": "24359",
    "end": "28000"
  },
  {
    "text": "technical manuals so tables kind of",
    "start": "28000",
    "end": "30640"
  },
  {
    "text": "present all over and the question is if",
    "start": "30640",
    "end": "34160"
  },
  {
    "text": "I want to do retrieval or reasoning over",
    "start": "34160",
    "end": "36760"
  },
  {
    "text": "tables how best can actually send these",
    "start": "36760",
    "end": "39040"
  },
  {
    "text": "into the context window of an",
    "start": "39040",
    "end": "41800"
  },
  {
    "text": "llm and there's a few approaches that",
    "start": "41800",
    "end": "44920"
  },
  {
    "text": "you can imagine that have been also",
    "start": "44920",
    "end": "46640"
  },
  {
    "text": "reported so one option is well we have",
    "start": "46640",
    "end": "49640"
  },
  {
    "text": "some uh llms that have very large",
    "start": "49640",
    "end": "51800"
  },
  {
    "text": "context Windows um up to 100,000 tokens",
    "start": "51800",
    "end": "55520"
  },
  {
    "text": "or up to hundreds of pages we can",
    "start": "55520",
    "end": "57680"
  },
  {
    "text": "imagine just taking documents that",
    "start": "57680",
    "end": "58920"
  },
  {
    "text": "contain tables and St stuffing them in",
    "start": "58920",
    "end": "61039"
  },
  {
    "text": "together so the text uh as well as",
    "start": "61039",
    "end": "64239"
  },
  {
    "text": "tables kind of just just are fed in",
    "start": "64239",
    "end": "67439"
  },
  {
    "text": "directly um there's also a set of",
    "start": "67439",
    "end": "70920"
  },
  {
    "text": "methods that try to perform some kind of",
    "start": "70920",
    "end": "72920"
  },
  {
    "text": "targeted table extraction we'll talk a",
    "start": "72920",
    "end": "74960"
  },
  {
    "text": "little bit more about that later on but",
    "start": "74960",
    "end": "77119"
  },
  {
    "text": "the idea here is you actually look at",
    "start": "77119",
    "end": "78280"
  },
  {
    "text": "the document you try to identify table",
    "start": "78280",
    "end": "79840"
  },
  {
    "text": "elements you extract them you pass those",
    "start": "79840",
    "end": "81600"
  },
  {
    "text": "into the llm in kind of a more targeted",
    "start": "81600",
    "end": "84799"
  },
  {
    "text": "way and the third which is very common",
    "start": "84799",
    "end": "87400"
  },
  {
    "text": "for rag in general is you take documents",
    "start": "87400",
    "end": "89360"
  },
  {
    "text": "you split them based upon some kind of",
    "start": "89360",
    "end": "91920"
  },
  {
    "text": "uh chunk size and you pass those chunks",
    "start": "91920",
    "end": "95200"
  },
  {
    "text": "into first the vector store with",
    "start": "95200",
    "end": "98280"
  },
  {
    "text": "embedding in order to basically make",
    "start": "98280",
    "end": "100560"
  },
  {
    "text": "them easily retrievable and you build an",
    "start": "100560",
    "end": "102920"
  },
  {
    "text": "application that can very easily",
    "start": "102920",
    "end": "104920"
  },
  {
    "text": "retrieve based on semetic similarity to",
    "start": "104920",
    "end": "106920"
  },
  {
    "text": "your question documents and then you",
    "start": "106920",
    "end": "109600"
  },
  {
    "text": "document chunks and you send those into",
    "start": "109600",
    "end": "111159"
  },
  {
    "text": "the llm context window so option one",
    "start": "111159",
    "end": "115119"
  },
  {
    "text": "just take full documents with tables",
    "start": "115119",
    "end": "117799"
  },
  {
    "text": "stuff them into the context window of a",
    "start": "117799",
    "end": "120159"
  },
  {
    "text": "large llm option two try to extract the",
    "start": "120159",
    "end": "123320"
  },
  {
    "text": "table specifically option three try to",
    "start": "123320",
    "end": "126320"
  },
  {
    "text": "take documents split them up and then",
    "start": "126320",
    "end": "128840"
  },
  {
    "text": "kind of do chunk based retrieval on",
    "start": "128840",
    "end": "131640"
  },
  {
    "text": "chunks that also contain",
    "start": "131640",
    "end": "133280"
  },
  {
    "text": "tables",
    "start": "133280",
    "end": "135160"
  },
  {
    "text": "um so to evaluate these different",
    "start": "135160",
    "end": "138319"
  },
  {
    "text": "options we built a public Langs Smith",
    "start": "138319",
    "end": "141040"
  },
  {
    "text": "Benchmark so Langs Smith is our platform",
    "start": "141040",
    "end": "144400"
  },
  {
    "text": "uh that allows for uh tracing evaluation",
    "start": "144400",
    "end": "148879"
  },
  {
    "text": "and other kinds of like observability",
    "start": "148879",
    "end": "150800"
  },
  {
    "text": "monitoring and",
    "start": "150800",
    "end": "152040"
  },
  {
    "text": "evaluation and what he did here",
    "start": "152040",
    "end": "154560"
  },
  {
    "text": "is um we took four earnings",
    "start": "154560",
    "end": "158319"
  },
  {
    "text": "reports um from data dog Microsoft uh",
    "start": "158319",
    "end": "161840"
  },
  {
    "text": "Google Amazon from Q3 2023 so they're",
    "start": "161840",
    "end": "165200"
  },
  {
    "text": "kind of recent they're kind of of of",
    "start": "165200",
    "end": "166800"
  },
  {
    "text": "some",
    "start": "166800",
    "end": "167560"
  },
  {
    "text": "interest um and two random white papers",
    "start": "167560",
    "end": "170519"
  },
  {
    "text": "the white papers just contained a",
    "start": "170519",
    "end": "171720"
  },
  {
    "text": "mixture of tables and text and we can",
    "start": "171720",
    "end": "174200"
  },
  {
    "text": "see that in particular the earnings",
    "start": "174200",
    "end": "175840"
  },
  {
    "text": "reports contain lots of inting tables",
    "start": "175840",
    "end": "178319"
  },
  {
    "text": "and they're typically pretty dense",
    "start": "178319",
    "end": "180400"
  },
  {
    "text": "you can see some examples here of like",
    "start": "180400",
    "end": "182680"
  },
  {
    "text": "kind of segment information as well as",
    "start": "182680",
    "end": "185480"
  },
  {
    "text": "um Revenue operating income from Google",
    "start": "185480",
    "end": "187959"
  },
  {
    "text": "on the left",
    "start": "187959",
    "end": "189159"
  },
  {
    "text": "here um we built 30 questions from these",
    "start": "189159",
    "end": "193879"
  },
  {
    "text": "documents and lsmith allows us to",
    "start": "193879",
    "end": "197640"
  },
  {
    "text": "basically automate",
    "start": "197640",
    "end": "199040"
  },
  {
    "text": "evaluation so we have all these question",
    "start": "199040",
    "end": "201159"
  },
  {
    "text": "answer pairs that we've built 30 of",
    "start": "201159",
    "end": "204280"
  },
  {
    "text": "them and we have our different",
    "start": "204280",
    "end": "206760"
  },
  {
    "text": "approaches for reasoning over these do",
    "start": "206760",
    "end": "209760"
  },
  {
    "text": "documents be it chunk based rag long",
    "start": "209760",
    "end": "212680"
  },
  {
    "text": "context",
    "start": "212680",
    "end": "213920"
  },
  {
    "text": "LMS um and what we can do is for every",
    "start": "213920",
    "end": "216959"
  },
  {
    "text": "question we can get the kind of chain",
    "start": "216959",
    "end": "218840"
  },
  {
    "text": "generated answer we can compare that to",
    "start": "218840",
    "end": "220400"
  },
  {
    "text": "the ground truth answer you can kind of",
    "start": "220400",
    "end": "222400"
  },
  {
    "text": "see this happens at stage three um and",
    "start": "222400",
    "end": "226239"
  },
  {
    "text": "then lmth is very nice cap abilities to",
    "start": "226239",
    "end": "228239"
  },
  {
    "text": "compare those results across many runs",
    "start": "228239",
    "end": "230560"
  },
  {
    "text": "debug what's going on and kind of root",
    "start": "230560",
    "end": "232879"
  },
  {
    "text": "cause um so that's kind of the approach",
    "start": "232879",
    "end": "235799"
  },
  {
    "text": "build a small Benchmark 30",
    "start": "235799",
    "end": "238439"
  },
  {
    "text": "questions um and we're going to take our",
    "start": "238439",
    "end": "240439"
  },
  {
    "text": "different approaches for rag across",
    "start": "240439",
    "end": "242920"
  },
  {
    "text": "tables and run an evaluation on this",
    "start": "242920",
    "end": "245400"
  },
  {
    "text": "data",
    "start": "245400",
    "end": "246319"
  },
  {
    "text": "set um so let's look at stuffing",
    "start": "246319",
    "end": "249799"
  },
  {
    "text": "documents into long context llms",
    "start": "249799",
    "end": "252159"
  },
  {
    "text": "first um what we see here is",
    "start": "252159",
    "end": "256320"
  },
  {
    "text": "the uh y- AIS shows you the percentage",
    "start": "256320",
    "end": "259560"
  },
  {
    "text": "of questions that are",
    "start": "259560",
    "end": "261199"
  },
  {
    "text": "correct and the x-axis shows you",
    "start": "261199",
    "end": "263320"
  },
  {
    "text": "different approaches so we tested Claud",
    "start": "263320",
    "end": "266600"
  },
  {
    "text": "2.1 and gp4 128k",
    "start": "266600",
    "end": "271320"
  },
  {
    "text": "now these documents combined were not",
    "start": "271320",
    "end": "273880"
  },
  {
    "text": "that big they're around 36,000 tokens so",
    "start": "273880",
    "end": "277800"
  },
  {
    "text": "they don't go to the full capacity of",
    "start": "277800",
    "end": "279759"
  },
  {
    "text": "these models which are around 100,000",
    "start": "279759",
    "end": "282440"
  },
  {
    "text": "128,000 tokens but it's still reasonably",
    "start": "282440",
    "end": "285600"
  },
  {
    "text": "large it's not something you could just",
    "start": "285600",
    "end": "287160"
  },
  {
    "text": "stuff into like for example Baseline GPD",
    "start": "287160",
    "end": "289639"
  },
  {
    "text": "4 um depending on the model you're using",
    "start": "289639",
    "end": "292360"
  },
  {
    "text": "8K 32k kind of exceeds those typical",
    "start": "292360",
    "end": "295919"
  },
  {
    "text": "bounds a bit so you do need these longer",
    "start": "295919",
    "end": "298160"
  },
  {
    "text": "context LMS for the for this",
    "start": "298160",
    "end": "300199"
  },
  {
    "text": "concatenated set of documents that",
    "start": "300199",
    "end": "301840"
  },
  {
    "text": "contain all of our",
    "start": "301840",
    "end": "303199"
  },
  {
    "text": "tables and we can see is you can get",
    "start": "303199",
    "end": "306080"
  },
  {
    "text": "decent performance now the nice thing is",
    "start": "306080",
    "end": "308160"
  },
  {
    "text": "this is very simple you do almost no",
    "start": "308160",
    "end": "310160"
  },
  {
    "text": "work and you can get for example with",
    "start": "310160",
    "end": "311919"
  },
  {
    "text": "GPT",
    "start": "311919",
    "end": "313000"
  },
  {
    "text": "gp4 72% correct it's not bad especially",
    "start": "313000",
    "end": "316720"
  },
  {
    "text": "given these are kind of very dense",
    "start": "316720",
    "end": "318840"
  },
  {
    "text": "Financial",
    "start": "318840",
    "end": "319919"
  },
  {
    "text": "tables now there's been some interesting",
    "start": "319919",
    "end": "323400"
  },
  {
    "text": "work this is from Greg camand",
    "start": "323400",
    "end": "327000"
  },
  {
    "text": "um that has shown",
    "start": "327000",
    "end": "331000"
  },
  {
    "text": "that the ability to factually retrieve",
    "start": "331000",
    "end": "333960"
  },
  {
    "text": "information from context of these",
    "start": "333960",
    "end": "337319"
  },
  {
    "text": "models depends upon the context",
    "start": "337319",
    "end": "341680"
  },
  {
    "text": "length as well as the fact placement",
    "start": "341680",
    "end": "345400"
  },
  {
    "text": "depth um and what Greg showed is that",
    "start": "345400",
    "end": "348600"
  },
  {
    "text": "for example as this is with gp4",
    "start": "348600",
    "end": "352360"
  },
  {
    "text": "128k as the the context length gets",
    "start": "352360",
    "end": "356600"
  },
  {
    "text": "greater so like as your your passing a",
    "start": "356600",
    "end": "360240"
  },
  {
    "text": "much longer context into these",
    "start": "360240",
    "end": "362680"
  },
  {
    "text": "llms um there are errors um and he did",
    "start": "362680",
    "end": "367759"
  },
  {
    "text": "this by placing a single fact in the sea",
    "start": "367759",
    "end": "370840"
  },
  {
    "text": "of context at different",
    "start": "370840",
    "end": "372520"
  },
  {
    "text": "locations um and then asking can you",
    "start": "372520",
    "end": "375800"
  },
  {
    "text": "retrieve that so you can see that there",
    "start": "375800",
    "end": "377280"
  },
  {
    "text": "are these issues with retrieval quality",
    "start": "377280",
    "end": "380360"
  },
  {
    "text": "with respect to both the length and the",
    "start": "380360",
    "end": "382080"
  },
  {
    "text": "placement depth in documents so that's",
    "start": "382080",
    "end": "384720"
  },
  {
    "text": "kind of a thing to flag with these long",
    "start": "384720",
    "end": "386280"
  },
  {
    "text": "context approaches you can see",
    "start": "386280",
    "end": "388479"
  },
  {
    "text": "degradation particularly as your context",
    "start": "388479",
    "end": "390599"
  },
  {
    "text": "window gets longer um as",
    "start": "390599",
    "end": "395120"
  },
  {
    "text": "your as the the fact placement uh is",
    "start": "395120",
    "end": "399440"
  },
  {
    "text": "more Central in the document so here's",
    "start": "399440",
    "end": "403520"
  },
  {
    "text": "our results looking at our documents so",
    "start": "403520",
    "end": "408560"
  },
  {
    "text": "again our context is around 36,000",
    "start": "408560",
    "end": "411199"
  },
  {
    "text": "tokens so not huge um but again not",
    "start": "411199",
    "end": "416520"
  },
  {
    "text": "particularly small not easily fit into",
    "start": "416520",
    "end": "418599"
  },
  {
    "text": "like into more some of the common llms",
    "start": "418599",
    "end": "421479"
  },
  {
    "text": "that folks",
    "start": "421479",
    "end": "422800"
  },
  {
    "text": "use and we do see kind of a cluster of",
    "start": "422800",
    "end": "426240"
  },
  {
    "text": "failures kind of in the middle uh of the",
    "start": "426240",
    "end": "429479"
  },
  {
    "text": "document and then this this is a heat",
    "start": "429479",
    "end": "431800"
  },
  {
    "text": "map I don't show the color bar here but",
    "start": "431800",
    "end": "433360"
  },
  {
    "text": "this dark indicates there's three",
    "start": "433360",
    "end": "434639"
  },
  {
    "text": "failures kind of in this region uh this",
    "start": "434639",
    "end": "436919"
  },
  {
    "text": "is one particular table pertaining to",
    "start": "436919",
    "end": "439759"
  },
  {
    "text": "Amazon uh where we continually get a",
    "start": "439759",
    "end": "441800"
  },
  {
    "text": "number of questions wrong a few others",
    "start": "441800",
    "end": "444199"
  },
  {
    "text": "here so it it would be hard to make",
    "start": "444199",
    "end": "446800"
  },
  {
    "text": "strong claims about the impact of",
    "start": "446800",
    "end": "450080"
  },
  {
    "text": "placement depth on these errors more",
    "start": "450080",
    "end": "452960"
  },
  {
    "text": "would have to be kind of studied there",
    "start": "452960",
    "end": "454160"
  },
  {
    "text": "to really be certain about that but it",
    "start": "454160",
    "end": "456840"
  },
  {
    "text": "is worth noting that if you take",
    "start": "456840",
    "end": "460240"
  },
  {
    "text": "documents along these lines which have",
    "start": "460240",
    "end": "462440"
  },
  {
    "text": "very dense tables you send them to a",
    "start": "462440",
    "end": "464039"
  },
  {
    "text": "long context L you can get these",
    "start": "464039",
    "end": "466159"
  },
  {
    "text": "retrieval failures that may or may not",
    "start": "466159",
    "end": "468000"
  },
  {
    "text": "be easy to understand they may relate to",
    "start": "468000",
    "end": "470440"
  },
  {
    "text": "what Greg showed kind of in depth here",
    "start": "470440",
    "end": "472879"
  },
  {
    "text": "it may depend on the context length that",
    "start": "472879",
    "end": "474720"
  },
  {
    "text": "you're passing in as well as like kind",
    "start": "474720",
    "end": "476319"
  },
  {
    "text": "of where they are in the document we saw",
    "start": "476319",
    "end": "478560"
  },
  {
    "text": "that there's a bunch of saor kind of in",
    "start": "478560",
    "end": "479879"
  },
  {
    "text": "the middle here and maybe that's kind of",
    "start": "479879",
    "end": "482319"
  },
  {
    "text": "a result that is consistent with some",
    "start": "482319",
    "end": "484120"
  },
  {
    "text": "other reports showing you can get these",
    "start": "484120",
    "end": "486560"
  },
  {
    "text": "kind of lost in the middle type effects",
    "start": "486560",
    "end": "489440"
  },
  {
    "text": "where retrieval kind of gets worse as it",
    "start": "489440",
    "end": "491240"
  },
  {
    "text": "enters the middle of the document but in",
    "start": "491240",
    "end": "493479"
  },
  {
    "text": "any case I think more would have to be",
    "start": "493479",
    "end": "495039"
  },
  {
    "text": "really kind of dug into here but it does",
    "start": "495039",
    "end": "496919"
  },
  {
    "text": "show that you can't necessarily expect",
    "start": "496919",
    "end": "499759"
  },
  {
    "text": "very high performance by just passing in",
    "start": "499759",
    "end": "503120"
  },
  {
    "text": "very large context and hoping to get",
    "start": "503120",
    "end": "504879"
  },
  {
    "text": "really really high quality question",
    "start": "504879",
    "end": "506319"
  },
  {
    "text": "answering over tables so that's kind of",
    "start": "506319",
    "end": "508319"
  },
  {
    "text": "observation one",
    "start": "508319",
    "end": "510639"
  },
  {
    "text": "um now observation two is you can try",
    "start": "510639",
    "end": "513760"
  },
  {
    "text": "targeted table extraction methods and",
    "start": "513760",
    "end": "515800"
  },
  {
    "text": "there's a number of them um the main",
    "start": "515800",
    "end": "518320"
  },
  {
    "text": "intuition is this is table Transformer",
    "start": "518320",
    "end": "520080"
  },
  {
    "text": "from Microsoft um you try to do some",
    "start": "520080",
    "end": "523479"
  },
  {
    "text": "kind of table detection looking at the",
    "start": "523479",
    "end": "525240"
  },
  {
    "text": "document seeing where the tables are",
    "start": "525240",
    "end": "527480"
  },
  {
    "text": "trying to extract the different columns",
    "start": "527480",
    "end": "530720"
  },
  {
    "text": "rows and kind of pull these tables out",
    "start": "530720",
    "end": "533600"
  },
  {
    "text": "of documents in some kind of structured",
    "start": "533600",
    "end": "535800"
  },
  {
    "text": "way um a number of different apis doc",
    "start": "535800",
    "end": "539440"
  },
  {
    "text": "offers some things for this unstructured",
    "start": "539440",
    "end": "541320"
  },
  {
    "text": "offers things for this they are it's an",
    "start": "541320",
    "end": "544279"
  },
  {
    "text": "interesting class of methods we are",
    "start": "544279",
    "end": "546760"
  },
  {
    "text": "actually going to follow up in more",
    "start": "546760",
    "end": "548200"
  },
  {
    "text": "detail on evaluations here we have done",
    "start": "548200",
    "end": "551120"
  },
  {
    "text": "some initial rounds of evaluation and",
    "start": "551120",
    "end": "553519"
  },
  {
    "text": "one of the things I can flag",
    "start": "553519",
    "end": "555959"
  },
  {
    "text": "currently is that these methods have a",
    "start": "555959",
    "end": "559600"
  },
  {
    "text": "high ceiling most likely it's very",
    "start": "559600",
    "end": "561519"
  },
  {
    "text": "promising to think about the ability to",
    "start": "561519",
    "end": "563200"
  },
  {
    "text": "Tech tables pull them out particularly",
    "start": "563200",
    "end": "564760"
  },
  {
    "text": "for really complex tables in practice",
    "start": "564760",
    "end": "567720"
  },
  {
    "text": "what we did see is that",
    "start": "567720",
    "end": "570200"
  },
  {
    "text": "uh because tables represented in many",
    "start": "570200",
    "end": "572240"
  },
  {
    "text": "different ways in different documents",
    "start": "572240",
    "end": "574640"
  },
  {
    "text": "consistency of extraction is a challenge",
    "start": "574640",
    "end": "577040"
  },
  {
    "text": "so for example one thing we saw in",
    "start": "577040",
    "end": "579240"
  },
  {
    "text": "particular is that um this is one of the",
    "start": "579240",
    "end": "583959"
  },
  {
    "text": "example tables from the Amazon document",
    "start": "583959",
    "end": "585600"
  },
  {
    "text": "we saw that some of these methods",
    "start": "585600",
    "end": "587640"
  },
  {
    "text": "actually failed to grab the uh the the",
    "start": "587640",
    "end": "591720"
  },
  {
    "text": "header columns and so none of the",
    "start": "591720",
    "end": "593920"
  },
  {
    "text": "questions could be answered correctly",
    "start": "593920",
    "end": "595760"
  },
  {
    "text": "because the llm did not receive uh um",
    "start": "595760",
    "end": "599720"
  },
  {
    "text": "these headers so didn't know how to",
    "start": "599720",
    "end": "601079"
  },
  {
    "text": "reason about this information so in any",
    "start": "601079",
    "end": "603640"
  },
  {
    "text": "case I think the intuition here is that",
    "start": "603640",
    "end": "605680"
  },
  {
    "text": "table extraction methods are absolutely",
    "start": "605680",
    "end": "607839"
  },
  {
    "text": "interesting to consider they require",
    "start": "607839",
    "end": "610519"
  },
  {
    "text": "like kind of more sophisticated methods",
    "start": "610519",
    "end": "611920"
  },
  {
    "text": "they may be trickier and they may suffer",
    "start": "611920",
    "end": "614079"
  },
  {
    "text": "from failure modes due to the diversity",
    "start": "614079",
    "end": "618160"
  },
  {
    "text": "of um table representations that we",
    "start": "618160",
    "end": "620920"
  },
  {
    "text": "actually encounter in real world",
    "start": "620920",
    "end": "622240"
  },
  {
    "text": "documents so we're going to have more",
    "start": "622240",
    "end": "624040"
  },
  {
    "text": "evaluations on this we're doing some",
    "start": "624040",
    "end": "625399"
  },
  {
    "text": "work with unstructured uh as well as",
    "start": "625399",
    "end": "628040"
  },
  {
    "text": "dyami um on these topics and so uh we",
    "start": "628040",
    "end": "632360"
  },
  {
    "text": "will provide some more evaluations here",
    "start": "632360",
    "end": "634160"
  },
  {
    "text": "in the future but I want to just kind of",
    "start": "634160",
    "end": "636000"
  },
  {
    "text": "give a flavor that these methods have a",
    "start": "636000",
    "end": "637800"
  },
  {
    "text": "pretty high ceiling but there's also",
    "start": "637800",
    "end": "640399"
  },
  {
    "text": "some challenges related to the kind of",
    "start": "640399",
    "end": "642240"
  },
  {
    "text": "the diversity of of kind of table",
    "start": "642240",
    "end": "645120"
  },
  {
    "text": "representation that you might see in",
    "start": "645120",
    "end": "647240"
  },
  {
    "text": "real world",
    "start": "647240",
    "end": "648399"
  },
  {
    "text": "documents so we kind of wanted to move",
    "start": "648399",
    "end": "650360"
  },
  {
    "text": "ahead to",
    "start": "650360",
    "end": "652160"
  },
  {
    "text": "chunking now chunking something a lot of",
    "start": "652160",
    "end": "654440"
  },
  {
    "text": "people are pretty familiar with um it's",
    "start": "654440",
    "end": "657120"
  },
  {
    "text": "been very common in rag to take",
    "start": "657120",
    "end": "658639"
  },
  {
    "text": "documents to chunk them up because",
    "start": "658639",
    "end": "660320"
  },
  {
    "text": "context windows are limited you can't",
    "start": "660320",
    "end": "662240"
  },
  {
    "text": "just send all your documents into an LM",
    "start": "662240",
    "end": "664320"
  },
  {
    "text": "at once um in for many cases and so this",
    "start": "664320",
    "end": "667440"
  },
  {
    "text": "idea of chunking and then retrieving",
    "start": "667440",
    "end": "669480"
  },
  {
    "text": "chunks based on centic similarity to a",
    "start": "669480",
    "end": "671800"
  },
  {
    "text": "question is like a very popular",
    "start": "671800",
    "end": "673200"
  },
  {
    "text": "appealing",
    "start": "673200",
    "end": "674279"
  },
  {
    "text": "approach now here's the results and I'll",
    "start": "674279",
    "end": "678720"
  },
  {
    "text": "kind of walk through this in a little",
    "start": "678720",
    "end": "679880"
  },
  {
    "text": "bit more detail but we see an",
    "start": "679880",
    "end": "681120"
  },
  {
    "text": "interesting Trend here so we start with",
    "start": "681120",
    "end": "684720"
  },
  {
    "text": "a pretty small chunk size 50 tokens so",
    "start": "684720",
    "end": "687920"
  },
  {
    "text": "you know four characters token maybe 200",
    "start": "687920",
    "end": "689959"
  },
  {
    "text": "characters and roughly here and you can",
    "start": "689959",
    "end": "692519"
  },
  {
    "text": "see performance is really bad and as you",
    "start": "692519",
    "end": "695800"
  },
  {
    "text": "bump up the chunk size you can see",
    "start": "695800",
    "end": "697519"
  },
  {
    "text": "performance gets better and better and",
    "start": "697519",
    "end": "698839"
  },
  {
    "text": "better until we get to this page splits",
    "start": "698839",
    "end": "700800"
  },
  {
    "text": "which I'll explain later and this",
    "start": "700800",
    "end": "702560"
  },
  {
    "text": "Ensemble thing which I'll also explain",
    "start": "702560",
    "end": "704200"
  },
  {
    "text": "later but let's kind of zoom in on the",
    "start": "704200",
    "end": "706639"
  },
  {
    "text": "this 50 token result and kind of get",
    "start": "706639",
    "end": "708360"
  },
  {
    "text": "some intuition here so Greg Cameron also",
    "start": "708360",
    "end": "711360"
  },
  {
    "text": "recently put out chunk fiz it's a really",
    "start": "711360",
    "end": "714240"
  },
  {
    "text": "nice kind of UI just for visualization",
    "start": "714240",
    "end": "716639"
  },
  {
    "text": "of of of chunking strategies on",
    "start": "716639",
    "end": "719160"
  },
  {
    "text": "different um kind of input",
    "start": "719160",
    "end": "722120"
  },
  {
    "text": "documents so I took one of our tables I",
    "start": "722120",
    "end": "725320"
  },
  {
    "text": "input it here and then I set the chunk",
    "start": "725320",
    "end": "727680"
  },
  {
    "text": "size to 200 characters which corresponds",
    "start": "727680",
    "end": "729920"
  },
  {
    "text": "to 50 tokens which is our first you can",
    "start": "729920",
    "end": "732120"
  },
  {
    "text": "see this table gets split up entirely by",
    "start": "732120",
    "end": "735839"
  },
  {
    "text": "the chunk by the chunking approach which",
    "start": "735839",
    "end": "739160"
  },
  {
    "text": "very much explains why you have such",
    "start": "739160",
    "end": "742040"
  },
  {
    "text": "poor retrieval",
    "start": "742040",
    "end": "743399"
  },
  {
    "text": "results because if you take a table and",
    "start": "743399",
    "end": "746000"
  },
  {
    "text": "you break it up in different chunks",
    "start": "746000",
    "end": "747360"
  },
  {
    "text": "you're unlikely to retrieve all those",
    "start": "747360",
    "end": "748920"
  },
  {
    "text": "chunks and reconstitute them such that",
    "start": "748920",
    "end": "751399"
  },
  {
    "text": "the LM can reason about that whole table",
    "start": "751399",
    "end": "753560"
  },
  {
    "text": "kind of together and this is kind of the",
    "start": "753560",
    "end": "756760"
  },
  {
    "text": "a main intuition for the chunking",
    "start": "756760",
    "end": "758399"
  },
  {
    "text": "strategy is we want to try to design",
    "start": "758399",
    "end": "761279"
  },
  {
    "text": "chunks such that they retain full tables",
    "start": "761279",
    "end": "764560"
  },
  {
    "text": "now of course this is what we were also",
    "start": "764560",
    "end": "766199"
  },
  {
    "text": "trying to do previously with our",
    "start": "766199",
    "end": "768480"
  },
  {
    "text": "discussion about table extraction table",
    "start": "768480",
    "end": "771199"
  },
  {
    "text": "extraction is kind of the the probably",
    "start": "771199",
    "end": "773600"
  },
  {
    "text": "the most exact highest performance in",
    "start": "773600",
    "end": "776720"
  },
  {
    "text": "way of doing this re actually should try",
    "start": "776720",
    "end": "777720"
  },
  {
    "text": "to detect tables and isolate them",
    "start": "777720",
    "end": "779440"
  },
  {
    "text": "extract them individually chunking",
    "start": "779440",
    "end": "782120"
  },
  {
    "text": "you're trying to kind of guess based on",
    "start": "782120",
    "end": "784199"
  },
  {
    "text": "chunk size what is going to capture",
    "start": "784199",
    "end": "786199"
  },
  {
    "text": "tables and which which is not and that's",
    "start": "786199",
    "end": "788360"
  },
  {
    "text": "obviously hard to do we can see in this",
    "start": "788360",
    "end": "790839"
  },
  {
    "text": "extreme case small chunks are very",
    "start": "790839",
    "end": "792639"
  },
  {
    "text": "likely to break up nearly all their",
    "start": "792639",
    "end": "794199"
  },
  {
    "text": "tables and that's a bad thing of course",
    "start": "794199",
    "end": "797160"
  },
  {
    "text": "um but one thing we found which is kind",
    "start": "797160",
    "end": "800399"
  },
  {
    "text": "of very obvious in retrospect is that in",
    "start": "800399",
    "end": "803760"
  },
  {
    "text": "many cases tables honor the semantic",
    "start": "803760",
    "end": "806519"
  },
  {
    "text": "boundaries of pages which people kind of",
    "start": "806519",
    "end": "809000"
  },
  {
    "text": "have have used to you know improve",
    "start": "809000",
    "end": "812720"
  },
  {
    "text": "readability for documents so for example",
    "start": "812720",
    "end": "814639"
  },
  {
    "text": "there absolutely are many cases of of",
    "start": "814639",
    "end": "816360"
  },
  {
    "text": "tables that span many pages but of",
    "start": "816360",
    "end": "818560"
  },
  {
    "text": "course this does hurt readability for",
    "start": "818560",
    "end": "820480"
  },
  {
    "text": "the table and a lot of documents at",
    "start": "820480",
    "end": "822639"
  },
  {
    "text": "least we are focused on for this noal",
    "start": "822639",
    "end": "824360"
  },
  {
    "text": "study did not contain those kind of",
    "start": "824360",
    "end": "826440"
  },
  {
    "text": "multiple page spanning tables and so the",
    "start": "826440",
    "end": "829639"
  },
  {
    "text": "approach here is very simple if you just",
    "start": "829639",
    "end": "831839"
  },
  {
    "text": "build your splits based upon page",
    "start": "831839",
    "end": "834199"
  },
  {
    "text": "boundaries it actually retains your",
    "start": "834199",
    "end": "836399"
  },
  {
    "text": "tables pretty well and so then all",
    "start": "836399",
    "end": "840040"
  },
  {
    "text": "you're doing is you're taking a page",
    "start": "840040",
    "end": "842000"
  },
  {
    "text": "embedding that page of text and then",
    "start": "842000",
    "end": "844959"
  },
  {
    "text": "retrieving based upon that embedding um",
    "start": "844959",
    "end": "847959"
  },
  {
    "text": "and then sending that that particular",
    "start": "847959",
    "end": "849560"
  },
  {
    "text": "page to the LM in practice this got us",
    "start": "849560",
    "end": "851839"
  },
  {
    "text": "93% accuracy so quite strong extremely",
    "start": "851839",
    "end": "855279"
  },
  {
    "text": "simple approach uh and I I completely",
    "start": "855279",
    "end": "858199"
  },
  {
    "text": "acknowledge that this will break down in",
    "start": "858199",
    "end": "859560"
  },
  {
    "text": "cases of complex tables that span many",
    "start": "859560",
    "end": "861480"
  },
  {
    "text": "pages and it may also break down for",
    "start": "861480",
    "end": "864399"
  },
  {
    "text": "other types of complex tables um that",
    "start": "864399",
    "end": "868800"
  },
  {
    "text": "were not captured in this evaluation set",
    "start": "868800",
    "end": "871560"
  },
  {
    "text": "but I do want to flag that this is an",
    "start": "871560",
    "end": "873199"
  },
  {
    "text": "extremely simple approach we found it to",
    "start": "873199",
    "end": "875519"
  },
  {
    "text": "be actually quite",
    "start": "875519",
    "end": "876759"
  },
  {
    "text": "performant um and um it requires very",
    "start": "876759",
    "end": "881959"
  },
  {
    "text": "little specialized engineering you know",
    "start": "881959",
    "end": "883880"
  },
  {
    "text": "unlike for example using some method to",
    "start": "883880",
    "end": "885920"
  },
  {
    "text": "extract the tables",
    "start": "885920",
    "end": "887320"
  },
  {
    "text": "individually um or even like a longer",
    "start": "887320",
    "end": "890000"
  },
  {
    "text": "context llm which is kind of not",
    "start": "890000",
    "end": "891639"
  },
  {
    "text": "accessible to many people more expensive",
    "start": "891639",
    "end": "893399"
  },
  {
    "text": "this is very simple you take your",
    "start": "893399",
    "end": "895040"
  },
  {
    "text": "document you basically embed each page",
    "start": "895040",
    "end": "898880"
  },
  {
    "text": "and then you retrieve Pages pass them to",
    "start": "898880",
    "end": "901399"
  },
  {
    "text": "the llm for final answer summarization",
    "start": "901399",
    "end": "904160"
  },
  {
    "text": "synthesis those pages typically en",
    "start": "904160",
    "end": "906560"
  },
  {
    "text": "capsulate tables pretty effectively for",
    "start": "906560",
    "end": "908759"
  },
  {
    "text": "many documents in particular the ones",
    "start": "908759",
    "end": "910800"
  },
  {
    "text": "that we we covered here um so using",
    "start": "910800",
    "end": "913680"
  },
  {
    "text": "these natural page boundaries actually",
    "start": "913680",
    "end": "915320"
  },
  {
    "text": "did work really well in practice just",
    "start": "915320",
    "end": "916880"
  },
  {
    "text": "for these kind of simple documents that",
    "start": "916880",
    "end": "918880"
  },
  {
    "text": "we are working with um now there was a",
    "start": "918880",
    "end": "923440"
  },
  {
    "text": "problem though we looked at some of the",
    "start": "923440",
    "end": "924880"
  },
  {
    "text": "failures for the you know the the proty",
    "start": "924880",
    "end": "927800"
  },
  {
    "text": "showed it got up to",
    "start": "927800",
    "end": "929120"
  },
  {
    "text": "93% but of course there's still some",
    "start": "929120",
    "end": "930720"
  },
  {
    "text": "headro for improvement there and we saw",
    "start": "930720",
    "end": "933839"
  },
  {
    "text": "this so here was one of the questions",
    "start": "933839",
    "end": "935680"
  },
  {
    "text": "that failed and it failed because it did",
    "start": "935680",
    "end": "939040"
  },
  {
    "text": "not",
    "start": "939040",
    "end": "940079"
  },
  {
    "text": "retrieve uh the page with the table it",
    "start": "940079",
    "end": "943440"
  },
  {
    "text": "retrieved a number of other pages that",
    "start": "943440",
    "end": "945079"
  },
  {
    "text": "discussed what we were interested in so",
    "start": "945079",
    "end": "946920"
  },
  {
    "text": "the question pertained to the",
    "start": "946920",
    "end": "948240"
  },
  {
    "text": "capitalized software development cost",
    "start": "948240",
    "end": "949880"
  },
  {
    "text": "for data dog and what we found is that",
    "start": "949880",
    "end": "953040"
  },
  {
    "text": "particular statement was present lots of",
    "start": "953040",
    "end": "956120"
  },
  {
    "text": "the text chunks outside of the page that",
    "start": "956120",
    "end": "958600"
  },
  {
    "text": "contained our table and so what happened",
    "start": "958600",
    "end": "960680"
  },
  {
    "text": "is when we did retrieval based on",
    "start": "960680",
    "end": "962519"
  },
  {
    "text": "semantic similarity a bunch of those",
    "start": "962519",
    "end": "964079"
  },
  {
    "text": "text chunks got retrieved and we did not",
    "start": "964079",
    "end": "966319"
  },
  {
    "text": "retrieve the table and so therefore we",
    "start": "966319",
    "end": "968440"
  },
  {
    "text": "couldn't answer the question so a very",
    "start": "968440",
    "end": "972079"
  },
  {
    "text": "simple trick that we kind of find in",
    "start": "972079",
    "end": "975040"
  },
  {
    "text": "practice worked really well for this is",
    "start": "975040",
    "end": "978360"
  },
  {
    "text": "you basically kind of build two",
    "start": "978360",
    "end": "979959"
  },
  {
    "text": "different retrievers and you can",
    "start": "979959",
    "end": "981480"
  },
  {
    "text": "Ensemble the results and so what I did",
    "start": "981480",
    "end": "984240"
  },
  {
    "text": "here was really simple so I iterate",
    "start": "984240",
    "end": "986800"
  },
  {
    "text": "through each page and I built what I",
    "start": "986800",
    "end": "988319"
  },
  {
    "text": "call kind of this this it's it's using L",
    "start": "988319",
    "end": "991199"
  },
  {
    "text": "change multiv Vector retriever but you",
    "start": "991199",
    "end": "992639"
  },
  {
    "text": "can think of this as like a a table",
    "start": "992639",
    "end": "995040"
  },
  {
    "text": "retriever so what I do is I iterate",
    "start": "995040",
    "end": "999319"
  },
  {
    "text": "through each page for every page of the",
    "start": "999319",
    "end": "1001040"
  },
  {
    "text": "document I ask an LM to tell me does",
    "start": "1001040",
    "end": "1004079"
  },
  {
    "text": "this page contain a table if so",
    "start": "1004079",
    "end": "1006319"
  },
  {
    "text": "summarize it and if not just kind of say",
    "start": "1006319",
    "end": "1009880"
  },
  {
    "text": "there's no table",
    "start": "1009880",
    "end": "1011240"
  },
  {
    "text": "present and what this multiv Vector triv",
    "start": "1011240",
    "end": "1013959"
  },
  {
    "text": "lets you do is let you index those",
    "start": "1013959",
    "end": "1016440"
  },
  {
    "text": "summaries in a vector store so so you",
    "start": "1016440",
    "end": "1018639"
  },
  {
    "text": "can imagine what we have is a bunch of",
    "start": "1018639",
    "end": "1021279"
  },
  {
    "text": "table",
    "start": "1021279",
    "end": "1023000"
  },
  {
    "text": "summaries um that an llm has",
    "start": "1023000",
    "end": "1026199"
  },
  {
    "text": "generated that link to the corresponding",
    "start": "1026199",
    "end": "1029839"
  },
  {
    "text": "page in the documents so again it's like",
    "start": "1029839",
    "end": "1033640"
  },
  {
    "text": "you have a whole bunch of summaries that",
    "start": "1033640",
    "end": "1034918"
  },
  {
    "text": "you've distilled for every every page",
    "start": "1034919",
    "end": "1036959"
  },
  {
    "text": "that contains a table you have all those",
    "start": "1036959",
    "end": "1038959"
  },
  {
    "text": "summaries index that are retrievable",
    "start": "1038959",
    "end": "1040760"
  },
  {
    "text": "based on symthic similarity and then",
    "start": "1040760",
    "end": "1042918"
  },
  {
    "text": "that raw that full raw page you then can",
    "start": "1042919",
    "end": "1045798"
  },
  {
    "text": "pass the LM for answer synthesis um and",
    "start": "1045799",
    "end": "1049360"
  },
  {
    "text": "then of course I combine this with the",
    "start": "1049360",
    "end": "1051039"
  },
  {
    "text": "original just page split Vector store",
    "start": "1051039",
    "end": "1053760"
  },
  {
    "text": "retriever which we had previously and",
    "start": "1053760",
    "end": "1055880"
  },
  {
    "text": "with this assemble retriever you can",
    "start": "1055880",
    "end": "1058320"
  },
  {
    "text": "actually do something that's kind of",
    "start": "1058320",
    "end": "1059400"
  },
  {
    "text": "nice you can effectively tell it to",
    "start": "1059400",
    "end": "1062960"
  },
  {
    "text": "prioritize any chunks from one of the",
    "start": "1062960",
    "end": "1065799"
  },
  {
    "text": "two retrievers so in short what I ended",
    "start": "1065799",
    "end": "1068280"
  },
  {
    "text": "up doing is I built this table retriever",
    "start": "1068280",
    "end": "1071280"
  },
  {
    "text": "at top that indexed all the different",
    "start": "1071280",
    "end": "1073919"
  },
  {
    "text": "pages that had tables with some table",
    "start": "1073919",
    "end": "1076159"
  },
  {
    "text": "summary and I combined that with a",
    "start": "1076159",
    "end": "1078159"
  },
  {
    "text": "result result of just the general page",
    "start": "1078159",
    "end": "1081360"
  },
  {
    "text": "retrieval and if there ever is um a",
    "start": "1081360",
    "end": "1085280"
  },
  {
    "text": "retrieve document from my table",
    "start": "1085280",
    "end": "1087880"
  },
  {
    "text": "retriever I put at the top of the queue",
    "start": "1087880",
    "end": "1090120"
  },
  {
    "text": "in this ensembling and so what this kind",
    "start": "1090120",
    "end": "1092559"
  },
  {
    "text": "of does is if there's ever a table",
    "start": "1092559",
    "end": "1094760"
  },
  {
    "text": "relevant to my question I try to enforce",
    "start": "1094760",
    "end": "1097640"
  },
  {
    "text": "or ensure that we return that because I",
    "start": "1097640",
    "end": "1099480"
  },
  {
    "text": "want to make sure that information is",
    "start": "1099480",
    "end": "1100760"
  },
  {
    "text": "included in the answer synthesis and",
    "start": "1100760",
    "end": "1102880"
  },
  {
    "text": "when you do this that's how I got up to",
    "start": "1102880",
    "end": "1105440"
  },
  {
    "text": "97% uh and there still is believe this",
    "start": "1105440",
    "end": "1107919"
  },
  {
    "text": "one question that it's incorrect and uh",
    "start": "1107919",
    "end": "1110799"
  },
  {
    "text": "that actually had to do with the need to",
    "start": "1110799",
    "end": "1113120"
  },
  {
    "text": "retrieve documents across a wide range",
    "start": "1113120",
    "end": "1116200"
  },
  {
    "text": "uh of the different documents and",
    "start": "1116200",
    "end": "1117440"
  },
  {
    "text": "synthesize them um and I'll be following",
    "start": "1117440",
    "end": "1121240"
  },
  {
    "text": "up on that in kind of separate probably",
    "start": "1121240",
    "end": "1123200"
  },
  {
    "text": "separate video or notebook that kind of",
    "start": "1123200",
    "end": "1125000"
  },
  {
    "text": "just addresses how to how to deal with",
    "start": "1125000",
    "end": "1127600"
  },
  {
    "text": "that case um but this was kind of the",
    "start": "1127600",
    "end": "1131960"
  },
  {
    "text": "original set of results that we wanted",
    "start": "1131960",
    "end": "1133520"
  },
  {
    "text": "to share um and kind of in summary",
    "start": "1133520",
    "end": "1139000"
  },
  {
    "text": "you long comex llms are really nice for",
    "start": "1139000",
    "end": "1142320"
  },
  {
    "text": "getting",
    "start": "1142320",
    "end": "1143480"
  },
  {
    "text": "started they do fairly well to be honest",
    "start": "1143480",
    "end": "1147960"
  },
  {
    "text": "um you know 72% is actually pretty good",
    "start": "1147960",
    "end": "1150640"
  },
  {
    "text": "for effectively you know very little",
    "start": "1150640",
    "end": "1152320"
  },
  {
    "text": "effort just taking these six documents",
    "start": "1152320",
    "end": "1154400"
  },
  {
    "text": "and stuffing them in if you have smaller",
    "start": "1154400",
    "end": "1157240"
  },
  {
    "text": "context like a single document with",
    "start": "1157240",
    "end": "1158799"
  },
  {
    "text": "tables that probably works very well um",
    "start": "1158799",
    "end": "1162840"
  },
  {
    "text": "the caveat is there can be these failure",
    "start": "1162840",
    "end": "1165120"
  },
  {
    "text": "modes with respect to both like the the",
    "start": "1165120",
    "end": "1168360"
  },
  {
    "text": "text length so if I try to send in more",
    "start": "1168360",
    "end": "1170919"
  },
  {
    "text": "tokens like our case was 36,000 if you I",
    "start": "1170919",
    "end": "1174360"
  },
  {
    "text": "think as Greg shows in these tables if",
    "start": "1174360",
    "end": "1176400"
  },
  {
    "text": "you try to push up to over 100,000 you",
    "start": "1176400",
    "end": "1178400"
  },
  {
    "text": "can get these this kind of degradation",
    "start": "1178400",
    "end": "1180679"
  },
  {
    "text": "performance and also depending on where",
    "start": "1180679",
    "end": "1183240"
  },
  {
    "text": "the table is in your context you could",
    "start": "1183240",
    "end": "1185080"
  },
  {
    "text": "see degradation there's been reports",
    "start": "1185080",
    "end": "1187280"
  },
  {
    "text": "that in the middle of documents does",
    "start": "1187280",
    "end": "1188600"
  },
  {
    "text": "worse Greg shows actually there seems to",
    "start": "1188600",
    "end": "1190480"
  },
  {
    "text": "be kind of a a lot of failure modes",
    "start": "1190480",
    "end": "1193240"
  },
  {
    "text": "towards the the top of the document so",
    "start": "1193240",
    "end": "1196679"
  },
  {
    "text": "uh in any case this it's worth more",
    "start": "1196679",
    "end": "1198440"
  },
  {
    "text": "investigation but the point is that long",
    "start": "1198440",
    "end": "1201080"
  },
  {
    "text": "long condex llms very easy to get",
    "start": "1201080",
    "end": "1203640"
  },
  {
    "text": "started with have some interesting",
    "start": "1203640",
    "end": "1205440"
  },
  {
    "text": "failure modes with respect to kind of",
    "start": "1205440",
    "end": "1206919"
  },
  {
    "text": "where the table is present in the",
    "start": "1206919",
    "end": "1208360"
  },
  {
    "text": "document so that's kind of point",
    "start": "1208360",
    "end": "1210039"
  },
  {
    "text": "one now point two is that chunking is",
    "start": "1210039",
    "end": "1215159"
  },
  {
    "text": "also quite easy very commonly done the",
    "start": "1215159",
    "end": "1217799"
  },
  {
    "text": "only challenge is you have to ensure",
    "start": "1217799",
    "end": "1219280"
  },
  {
    "text": "that your chunking strategy is not split",
    "start": "1219280",
    "end": "1220840"
  },
  {
    "text": "up tables just kind of very obvious you",
    "start": "1220840",
    "end": "1222760"
  },
  {
    "text": "don't want to break them up because then",
    "start": "1222760",
    "end": "1224080"
  },
  {
    "text": "you can't reason about the information",
    "start": "1224080",
    "end": "1225720"
  },
  {
    "text": "present in the table and you can see it",
    "start": "1225720",
    "end": "1227880"
  },
  {
    "text": "these chunk sizes you do certainly break",
    "start": "1227880",
    "end": "1231240"
  },
  {
    "text": "up many of your tables and you and you",
    "start": "1231240",
    "end": "1232919"
  },
  {
    "text": "reduce your answer quality quite",
    "start": "1232919",
    "end": "1235000"
  },
  {
    "text": "significantly in short we found is just",
    "start": "1235000",
    "end": "1237799"
  },
  {
    "text": "simply splitting on page breaks uh at",
    "start": "1237799",
    "end": "1240880"
  },
  {
    "text": "least retains for ourl set u many of the",
    "start": "1240880",
    "end": "1244240"
  },
  {
    "text": "tables and therefore lets you uh get",
    "start": "1244240",
    "end": "1246880"
  },
  {
    "text": "pretty good performance up to",
    "start": "1246880",
    "end": "1248760"
  },
  {
    "text": "93% and a small trick of additional",
    "start": "1248760",
    "end": "1252080"
  },
  {
    "text": "embling just to make sure that you",
    "start": "1252080",
    "end": "1254480"
  },
  {
    "text": "always retrieve relevant table chunks if",
    "start": "1254480",
    "end": "1257400"
  },
  {
    "text": "they pertain to the question relative to",
    "start": "1257400",
    "end": "1259960"
  },
  {
    "text": "document chunks um is a nice way to kind",
    "start": "1259960",
    "end": "1263120"
  },
  {
    "text": "of boost the performance slightly",
    "start": "1263120",
    "end": "1266240"
  },
  {
    "text": "further um and ensure that information",
    "start": "1266240",
    "end": "1269159"
  },
  {
    "text": "from tables does get prioritized if it's",
    "start": "1269159",
    "end": "1271240"
  },
  {
    "text": "relevant to your question um and that's",
    "start": "1271240",
    "end": "1273919"
  },
  {
    "text": "really it I think um that was all I had",
    "start": "1273919",
    "end": "1277760"
  },
  {
    "text": "for slides um and we'll be following up",
    "start": "1277760",
    "end": "1280720"
  },
  {
    "text": "a bit more on approaches that involve um",
    "start": "1280720",
    "end": "1284120"
  },
  {
    "text": "targeted table extraction in the future",
    "start": "1284120",
    "end": "1287080"
  },
  {
    "text": "thanks",
    "start": "1287080",
    "end": "1290080"
  }
]