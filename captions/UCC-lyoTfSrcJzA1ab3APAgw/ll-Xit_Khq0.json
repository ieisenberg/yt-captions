[
  {
    "text": "all right good morning everyone uh pull this up thank you all for",
    "start": "2320",
    "end": "8000"
  },
  {
    "text": "waiting uh for this beginning of the webinar um I'm really excited that you're all tuning in to today's webinar on Lessons Learned deploying LMS into",
    "start": "8000",
    "end": "14839"
  },
  {
    "text": "production with L Smith um we've got a lot of material here to get through since um all these people bring a wealth",
    "start": "14839",
    "end": "19840"
  },
  {
    "text": "of experience so I'll keep the intros short and then I'll go over a couple of ground rules L what we're going to go",
    "start": "19840",
    "end": "25039"
  },
  {
    "text": "through today um I'm excited to invite some amazing guests to the webinar today we've got Takami got rich so from dyami",
    "start": "25039",
    "end": "33559"
  },
  {
    "text": "we've got um Jean PA and Tucky Jeffrey Jean is the Jean is the um CEO of dyami",
    "start": "33559",
    "end": "41000"
  },
  {
    "text": "and co-creator of the XML standard um so he brings a wealth of experience and delivering invaluable products across a",
    "start": "41000",
    "end": "46760"
  },
  {
    "text": "number of different areas and then Tucky is a co-founder and head of product and is leading a lot of the efforts to bring",
    "start": "46760",
    "end": "51879"
  },
  {
    "text": "a number of llm powered features to Taki going from document authoring to extraction and more um Amil is a lead",
    "start": "51879",
    "end": "58480"
  },
  {
    "text": "engineer at reat the company that puts all the tools real estate agents need within one super app um Emil is",
    "start": "58480",
    "end": "64920"
  },
  {
    "text": "overseeing the development of Lucy a conversational entry point to all the functionality of reach using an llm",
    "start": "64920",
    "end": "70240"
  },
  {
    "text": "powered a um before we dive in I want to say a few points of Order each presenter",
    "start": "70240",
    "end": "75439"
  },
  {
    "text": "will have roughly 25 minutes to provide um the the content that they want to share and then go through their slides",
    "start": "75439",
    "end": "82040"
  },
  {
    "text": "we'll do our best to reserve about 10 minutes at the end for Q&A um for everyone in the audience I would",
    "start": "82040",
    "end": "87600"
  },
  {
    "text": "encourage you to be posting your questions to the chat and I'll be monitoring them if there's things that I feel like are best addressed in real",
    "start": "87600",
    "end": "93720"
  },
  {
    "text": "time I'll try to erase them and interrupt a little bit there um otherwise if you don't have um your",
    "start": "93720",
    "end": "98840"
  },
  {
    "text": "answer your question answer today we'll be recording this session and sharing it on YouTube and other places so hopefully",
    "start": "98840",
    "end": "104399"
  },
  {
    "text": "we'll be able to follow up with a little bit more information for you if we don't have time and with that I would like to",
    "start": "104399",
    "end": "111360"
  },
  {
    "text": "turn it over first to dyami wonderful thank you will for that",
    "start": "111360",
    "end": "116640"
  },
  {
    "text": "intro Let me just share my screen hopefully that",
    "start": "116640",
    "end": "121840"
  },
  {
    "text": "works and please let me know when my screen is",
    "start": "122039",
    "end": "129920"
  },
  {
    "text": "live I see here okay you folks see my screen yep",
    "start": "129920",
    "end": "137080"
  },
  {
    "text": "okay so thank you everybody for joining us today we are docy gami and uh we will",
    "start": "137080",
    "end": "142800"
  },
  {
    "text": "be sharing some lessons that we have learned uh deploying llms into production and we're super proud uh to",
    "start": "142800",
    "end": "149840"
  },
  {
    "text": "sh um you know all these lessons we've been learning working together with Lang chain and and the Lang Smith team since",
    "start": "149840",
    "end": "156680"
  },
  {
    "text": "the beginning of the the lsmith beta uh so uh we only have around 20",
    "start": "156680",
    "end": "162000"
  },
  {
    "text": "minutes so we're going to go pretty fast and um docy gami is actually quite",
    "start": "162000",
    "end": "167519"
  },
  {
    "text": "different from other systems so we'll spend a little bit of time explaining as we go uh we have a lot of code to share",
    "start": "167519",
    "end": "173840"
  },
  {
    "text": "as well uh so we'll sort of start with the explanations and then we'll jump into code uh we we're going to talk",
    "start": "173840",
    "end": "180280"
  },
  {
    "text": "about uh these four real life challenges here we'll go into more detail as we go",
    "start": "180280",
    "end": "186480"
  },
  {
    "text": "uh we have some code uh that even if we can't get into the code in the 20 minutes today we will share links so you",
    "start": "186480",
    "end": "193200"
  },
  {
    "text": "folks can follow the code along afterwards um and then we'll try to summarize at the end How We Do endtoend",
    "start": "193200",
    "end": "200840"
  },
  {
    "text": "llm Ops uh with our language models in production um over the last few years",
    "start": "200840",
    "end": "207760"
  },
  {
    "text": "okay so with that um so I think will you already did um fantastic intros thank",
    "start": "207760",
    "end": "212959"
  },
  {
    "text": "you but maybe you folks would to say hello uh Jean Mike uh Zubin go ahead",
    "start": "212959",
    "end": "219000"
  },
  {
    "text": "please yeah know this jei thank you really will and L chain uh it's an",
    "start": "219000",
    "end": "225040"
  },
  {
    "text": "amazing framework so thank you for inviting us yeah and you know Jan built this",
    "start": "225040",
    "end": "230599"
  },
  {
    "text": "little thing called XML and if you used docx and xlsx um you know you've you've",
    "start": "230599",
    "end": "236640"
  },
  {
    "text": "seen a little bit of his work uh Mike would you like to go ahead yeah hey uh",
    "start": "236640",
    "end": "242400"
  },
  {
    "text": "you know like Jean said I want to Echo you know thank you for inviting us to speak today um yeah I have a background",
    "start": "242400",
    "end": "249760"
  },
  {
    "text": "at Microsoft uh you know been at dyami as a co-founder for a little over five years now so yeah just excited uh to",
    "start": "249760",
    "end": "257600"
  },
  {
    "text": "speak with everyone today thanks yeah and Mike can answer a lot of questions that people might have over you know running these models U on gpus sort of",
    "start": "257600",
    "end": "265720"
  },
  {
    "text": "at scale inference Etc Mike's Mike's Mike's the guy uh zoom in please hey L",
    "start": "265720",
    "end": "272120"
  },
  {
    "text": "chain Community uh I'm on the product team here at dyami been here about four years uh been uh making documents more",
    "start": "272120",
    "end": "278360"
  },
  {
    "text": "useful and understandable for the last 20 plus years and uh looking forward to",
    "start": "278360",
    "end": "283560"
  },
  {
    "text": "sharing our knowledge with you all awesome so",
    "start": "283560",
    "end": "289360"
  },
  {
    "text": "uh as I said we're going to be talking about a series of challenges uh that we",
    "start": "289360",
    "end": "294880"
  },
  {
    "text": "ran into uh at toyami deploying llms at scale and of course at dyami we uh we're",
    "start": "294880",
    "end": "303160"
  },
  {
    "text": "a document engineering company uh we build a pipeline to understand and create real world documents and very",
    "start": "303160",
    "end": "310919"
  },
  {
    "text": "quickly what we realized is real world documents are not just flat text right",
    "start": "310919",
    "end": "317560"
  },
  {
    "text": "so there's a lot of great uh tools out there uh but what you'll realize is it",
    "start": "317560",
    "end": "322880"
  },
  {
    "text": "is it's sometimes challenging to detect things like headings and you know sort of these structural elements like here",
    "start": "322880",
    "end": "329560"
  },
  {
    "text": "you see an inline heading here you see actually in this document it's a two column layout so the text actually flows",
    "start": "329560",
    "end": "336919"
  },
  {
    "text": "from one column to the other um it's even harder in this case because there's",
    "start": "336919",
    "end": "341960"
  },
  {
    "text": "a table that actually flows across the two columns and you know you have these sort of key value pairs so in docy gami",
    "start": "341960",
    "end": "349360"
  },
  {
    "text": "this is just a screenshot of the docy gami system you folks can try it later if you want uh what we do here as we",
    "start": "349360",
    "end": "355759"
  },
  {
    "text": "know with llms I mean the first thing you need to do is extract the text um and then you know then off you go to",
    "start": "355759",
    "end": "362039"
  },
  {
    "text": "the races uh we have a few mitigations that we've learned for example we actually structurally chunk documents",
    "start": "362039",
    "end": "369120"
  },
  {
    "text": "around these sort of structural elements that you're seeing we stitch together the reading order using language models",
    "start": "369120",
    "end": "376759"
  },
  {
    "text": "and then uh for retrieval augmented generation U you know rather than just naive tiling or text splitting we might",
    "start": "376759",
    "end": "384440"
  },
  {
    "text": "for example just use actual paragraphs or actual table cells or rows so here",
    "start": "384440",
    "end": "389639"
  },
  {
    "text": "Jean since you've been working with documents for a while would you like to add no I just want to say you know um if",
    "start": "389639",
    "end": "395680"
  },
  {
    "text": "you look humans create documents in general and so they convey their intent",
    "start": "395680",
    "end": "401960"
  },
  {
    "text": "both in term of layout and text you know it's they go together that's why everybody talk about",
    "start": "401960",
    "end": "407919"
  },
  {
    "text": "multimodal um this is like all of this like if you think for example also the",
    "start": "407919",
    "end": "412960"
  },
  {
    "text": "way we think about you know documents it's hierarchical so if you look at this B initial statement in actually we look",
    "start": "412960",
    "end": "420400"
  },
  {
    "text": "at it in dyami as all the stuff basically that's go around it here in this page is inside of the the kind of",
    "start": "420400",
    "end": "426680"
  },
  {
    "text": "hierarchy that is represented by this B section let's start with that and I'm sure Taki will",
    "start": "426680",
    "end": "432919"
  },
  {
    "text": "continue right and I think uh I have I have code coming right after challenge",
    "start": "432919",
    "end": "438639"
  },
  {
    "text": "one and challenge two to sort of tie it all together but if if this first one challenge one is about essentially",
    "start": "438639",
    "end": "445199"
  },
  {
    "text": "structurally chunking the document challenge two is actually the semantics",
    "start": "445199",
    "end": "450360"
  },
  {
    "text": "of the document in terms of the concepts are also nested so here again you'll",
    "start": "450360",
    "end": "457160"
  },
  {
    "text": "notice that here's a landlord uh here's a tenant Etc and you",
    "start": "457160",
    "end": "462400"
  },
  {
    "text": "know a text based system I'm sure could could extract something like this but here you see the hierarchy sort of get",
    "start": "462400",
    "end": "468440"
  },
  {
    "text": "more complex right so you have all the basic lease terms here and then you have the commencement date over here um and",
    "start": "468440",
    "end": "475800"
  },
  {
    "text": "so maybe here Zubin you want to jump in because you work with a lot of Advanced long form document types how",
    "start": "475800",
    "end": "482560"
  },
  {
    "text": "you've seen documents be knowledge graphs in in in real life yeah I mean this structural and",
    "start": "482560",
    "end": "488680"
  },
  {
    "text": "semantic chunking that we do with this hierarchical awareness is enormously valuable in the real world if you go",
    "start": "488680",
    "end": "495800"
  },
  {
    "text": "over to the next uh slide Taki where we talk about the XML representation which is sort of the way we represent this in",
    "start": "495800",
    "end": "502520"
  },
  {
    "text": "XML um if you think about Commercial Insurance in the in the world of uh say",
    "start": "502520",
    "end": "507599"
  },
  {
    "text": "employee benefits claims and employee benefits quotes you'll see situations",
    "start": "507599",
    "end": "512800"
  },
  {
    "text": "where you're dealing with a document that has essentially potentially a dozen different plans and each of these",
    "start": "512800",
    "end": "518839"
  },
  {
    "text": "different plans have a different design they have different classes they have different networks and all of this",
    "start": "518839",
    "end": "525320"
  },
  {
    "text": "disambiguation is only possible if you have that hierarchical awareness to be",
    "start": "525320",
    "end": "530920"
  },
  {
    "text": "able to know that the maximum po outof pocket maximum over here pertains to a",
    "start": "530920",
    "end": "536440"
  },
  {
    "text": "particular plan a particular Class A particular Network and so on and so this",
    "start": "536440",
    "end": "542160"
  },
  {
    "text": "High signal retrieval that you can attain because of this hierarchical awareness is really essential to sort of",
    "start": "542160",
    "end": "550240"
  },
  {
    "text": "Drive Great outcomes so I just wanted to mention that as a sort of the implicit benefit here that comes through this",
    "start": "550240",
    "end": "557600"
  },
  {
    "text": "awesome so I think I actually if I can add one thing toi which is you know a",
    "start": "557600",
    "end": "563560"
  },
  {
    "text": "lot of people talk about trunking and just what we call the XML data model historically you know it you can",
    "start": "563560",
    "end": "570399"
  },
  {
    "text": "represent it in Json in whatever markup language you want it's basically think about it as chunks inside of chunks very",
    "start": "570399",
    "end": "577760"
  },
  {
    "text": "simple chunks contain other chunks and every chunk has a label which represent",
    "start": "577760",
    "end": "584000"
  },
  {
    "text": "represents semantically what this chunk means you know if it's a start date or is it an insurance number or is it an",
    "start": "584000",
    "end": "591240"
  },
  {
    "text": "entire you know section for uh for negotiation just think about it that's a very simple model and then in addition",
    "start": "591240",
    "end": "598360"
  },
  {
    "text": "we group do documents that looks that like talks about similar things those",
    "start": "598360",
    "end": "603399"
  },
  {
    "text": "documents should share similar labels the same labels on their charts that's literally the nutshell of this model",
    "start": "603399",
    "end": "610000"
  },
  {
    "text": "that uh that that scales where well right so this was this is of course the you know the explanation uh let's jump",
    "start": "610000",
    "end": "617079"
  },
  {
    "text": "into a little bit of code so you might have all seen uh something like this",
    "start": "617079",
    "end": "623200"
  },
  {
    "text": "from you know if if you're attending a lang chain seminar uh or a webinar so with retrieval augment generation you",
    "start": "623200",
    "end": "630120"
  },
  {
    "text": "know pretty standard we have a data loader for dyami you can use any other data loader uh but our data loader of",
    "start": "630120",
    "end": "636600"
  },
  {
    "text": "course chunks the way we just described and it actually introd puts metadata on",
    "start": "636600",
    "end": "641839"
  },
  {
    "text": "all the chunks uh using the XML Knowledge Graph that Jean was just talking about and that actually allows",
    "start": "641839",
    "end": "648399"
  },
  {
    "text": "you to do things that that are not possible otherwise so um anybody",
    "start": "648399",
    "end": "654519"
  },
  {
    "text": "following along can click this link uh to to see the code I'm going to show you",
    "start": "654519",
    "end": "659680"
  },
  {
    "text": "in the interest of time I'm not going to be able to go in too much detail uh but this is the code um hopefully you folks",
    "start": "659680",
    "end": "666360"
  },
  {
    "text": "can see uh that you know you can you can download and run yourself and and here",
    "start": "666360",
    "end": "672519"
  },
  {
    "text": "uh essentially uh you'll notice that in docy gami once you've processed a bunch of documents you get a doite ID you can",
    "start": "672519",
    "end": "679880"
  },
  {
    "text": "specify doite ID to the docy gami loader and the chunks that it returns have",
    "start": "679880",
    "end": "686040"
  },
  {
    "text": "additional metadata on them so this metadata right here is coming from that XML U Knowledge Graph that we were just",
    "start": "686040",
    "end": "693680"
  },
  {
    "text": "talking about so every chunk is actually richer than just the text embedding or",
    "start": "693680",
    "end": "700279"
  },
  {
    "text": "the text contents of the chunk and uh I know I'm going pretty fast here uh just",
    "start": "700279",
    "end": "705680"
  },
  {
    "text": "in the interest of time but this allows us to do some pretty cool things with uh for example the self querying retriever",
    "start": "705680",
    "end": "711959"
  },
  {
    "text": "if people in the audience don't uh know about this you can just click this link here uh this is a lang chain concept and",
    "start": "711959",
    "end": "720240"
  },
  {
    "text": "uh actually in this codebase you'll notice that uh I actually asked a question without the dyami metadata uh",
    "start": "720240",
    "end": "727920"
  },
  {
    "text": "using the self quing Retriever and uh because you know this is a lang Smith webinar we can actually",
    "start": "727920",
    "end": "734680"
  },
  {
    "text": "click on the trace and see what that looks like and uh so you know you ask a",
    "start": "734680",
    "end": "740399"
  },
  {
    "text": "question like what is the rentable area for the property owned by DHA group and immediately you'll notice that the",
    "start": "740399",
    "end": "745959"
  },
  {
    "text": "retriever did not return any documents so that's a problem I mean so no wonder",
    "start": "745959",
    "end": "751760"
  },
  {
    "text": "if the retriever did not return any documents of course the answer to the question is I don't know but if you dig",
    "start": "751760",
    "end": "757279"
  },
  {
    "text": "in a little bit why did the retriever not return any documents well it's because the squaring retriever depends",
    "start": "757279",
    "end": "763120"
  },
  {
    "text": "on um you know the information that's on all the chunks and here it's trying to",
    "start": "763120",
    "end": "768720"
  },
  {
    "text": "filter by name which essentially is the file name uh by default for standard um",
    "start": "768720",
    "end": "775040"
  },
  {
    "text": "retrievers and you know there's just no file named DHA group like this piece of",
    "start": "775040",
    "end": "780320"
  },
  {
    "text": "information DHA group is actually uh elsewhere in the document it's not in the file name uh so you know without",
    "start": "780320",
    "end": "786639"
  },
  {
    "text": "doam this question literally fails but if all you do is you actually use the docy gami chunks instead with the",
    "start": "786639",
    "end": "793760"
  },
  {
    "text": "additional information uh from the XML Knowledge Graph uh well it's now able to",
    "start": "793760",
    "end": "798959"
  },
  {
    "text": "answer the question because now it's actually filtering not on the file name name is just a file name uh but it's",
    "start": "798959",
    "end": "805279"
  },
  {
    "text": "actually filtering on landlord and that that that metadata landlord actually came from the XML Knowledge Graph so if",
    "start": "805279",
    "end": "812639"
  },
  {
    "text": "you can uh look over here you know you have the the same question but now it's actually found a bunch of documents in",
    "start": "812639",
    "end": "819880"
  },
  {
    "text": "the in the vector database uh using the self quaring Retriever and now of course it's it's able to come up with the",
    "start": "819880",
    "end": "825800"
  },
  {
    "text": "answer so I invite you all to uh to play around with this it's actually pretty",
    "start": "825800",
    "end": "830920"
  },
  {
    "text": "powerful um and uh of course if you folks have any questions uh we'll address them um in the in the Q&A",
    "start": "830920",
    "end": "838040"
  },
  {
    "text": "section so I'm going to go back now uh these were essentially our first two",
    "start": "838040",
    "end": "843759"
  },
  {
    "text": "challenges you know with with semantic uh retrieval with especially long form",
    "start": "843759",
    "end": "850040"
  },
  {
    "text": "documents either there's texting text extraction and chunking problems which",
    "start": "850040",
    "end": "855639"
  },
  {
    "text": "we address uh using our sort of structural chunking and then sometimes there's you know these knowledge",
    "start": "855639",
    "end": "861160"
  },
  {
    "text": "representation problems and the knowledge graph is not rich enough uh so docy gami helps with that as well okay",
    "start": "861160",
    "end": "868320"
  },
  {
    "text": "so the third thing I'd like to talk about is actually has something will you",
    "start": "868320",
    "end": "873519"
  },
  {
    "text": "helped us with so uh we really appreciate that uh is you know in",
    "start": "873519",
    "end": "879639"
  },
  {
    "text": "practice sometimes chains get very very complex so if uh people in the audience",
    "start": "879639",
    "end": "885160"
  },
  {
    "text": "have been playing around with the Lang chain expression language um this this will resonate with them um I mean it",
    "start": "885160",
    "end": "892920"
  },
  {
    "text": "starts off with a very naive scenario right like you have a database that you're trying to query with uh natural",
    "start": "892920",
    "end": "898360"
  },
  {
    "text": "language now at Doc ugami we do um we do SQL as well as things like xquery Etc over over",
    "start": "898360",
    "end": "904399"
  },
  {
    "text": "XML but just to simplify the scenario imagine you have a question like this",
    "start": "904399",
    "end": "910000"
  },
  {
    "text": "and then of course you want to create the SQL query using a language model by passing in the you know the table schema",
    "start": "910000",
    "end": "917440"
  },
  {
    "text": "uh but what if the SQL is invalid right or has a you know has a small problem",
    "start": "917440",
    "end": "923320"
  },
  {
    "text": "like in this case the table name is wrong you run the SQL query but you get some sort of an exception and then you",
    "start": "923320",
    "end": "929040"
  },
  {
    "text": "know what if you want to actually attempt a fix up sort of like what agents do um and then once you know the",
    "start": "929040",
    "end": "936720"
  },
  {
    "text": "SQL has been fixed up and you have your result what if you want to do",
    "start": "936720",
    "end": "942079"
  },
  {
    "text": "explanations of the result in sort of human natural language because this is not a great answer to give back to a",
    "start": "942079",
    "end": "947519"
  },
  {
    "text": "user right you want to say something like you know the average sales were you know four you know 51,000 Etc so the",
    "start": "947519",
    "end": "956319"
  },
  {
    "text": "sample code is here in the interest of time I'm not going to click through the sample code in this case uh but uh you",
    "start": "956319",
    "end": "962440"
  },
  {
    "text": "can all click here and sort of it it walks through how you can do stuff like",
    "start": "962440",
    "end": "968079"
  },
  {
    "text": "this a more complicated chain without actually using an agent because in in production what we found is Agents you",
    "start": "968079",
    "end": "974360"
  },
  {
    "text": "know use lot of tokens and some some of the smaller models we use we have our own custom model uh are not really tuned",
    "start": "974360",
    "end": "981920"
  },
  {
    "text": "to the the agent syntax uh so we ended up doing things this way uh for efficiency and performance uh you guys",
    "start": "981920",
    "end": "989120"
  },
  {
    "text": "can click the link here to find out but what I will do is um I will click through the Lang chain uh the Lang Smith",
    "start": "989120",
    "end": "995920"
  },
  {
    "text": "Trace because you know that's that's what this webinar is about and that's",
    "start": "995920",
    "end": "1001120"
  },
  {
    "text": "actually pretty cool because there's a lot of richness here that allows you to",
    "start": "1001120",
    "end": "1006240"
  },
  {
    "text": "understand in Lang Smith what's going on so I'm going to expand this and you know",
    "start": "1006240",
    "end": "1012240"
  },
  {
    "text": "uh turn on the you know the time the Run stats as well and here you'll notice you",
    "start": "1012240",
    "end": "1018639"
  },
  {
    "text": "can step through the entire thing that we were talking about so for example here we have a runnable sequence which",
    "start": "1018639",
    "end": "1025280"
  },
  {
    "text": "is taking the question and generating the SQL itself you can see how it's getting the table info Etc we're using",
    "start": "1025280",
    "end": "1031918"
  },
  {
    "text": "fuse shot prompt seating uh so for in context learning so you can uh literally",
    "start": "1031919",
    "end": "1036959"
  },
  {
    "text": "go in and see which examples the system found uh you know to generate the query",
    "start": "1036959",
    "end": "1043000"
  },
  {
    "text": "uh we have a bunch of output parsers uh that you can look into as well uh Etc",
    "start": "1043000",
    "end": "1048480"
  },
  {
    "text": "and then the the last thing that we were talking about which is um you know uh taking the output and running it in",
    "start": "1048480",
    "end": "1054919"
  },
  {
    "text": "parallel so rather than serly doing the explanations doing it in parallel well that's a runable map over here as you",
    "start": "1054919",
    "end": "1060440"
  },
  {
    "text": "can see these these two maps in parallel and and U you know it's doing the",
    "start": "1060440",
    "end": "1065559"
  },
  {
    "text": "explained SQL result as well as the explained SQL query so the meta Point here is these things get very very",
    "start": "1065559",
    "end": "1071720"
  },
  {
    "text": "complicated in real life when you deploy them especially when you start doing things in parallel and have uh retri",
    "start": "1071720",
    "end": "1078600"
  },
  {
    "text": "logic built in ETC uh but you know L Smith is a great way to debug that so",
    "start": "1078600",
    "end": "1083840"
  },
  {
    "text": "we're happy to take questions about this as well I'm just looking at time and uh you know this part um you",
    "start": "1083840",
    "end": "1091880"
  },
  {
    "text": "know we'll we'll follow up with a blog post with a lot of these links but there's actually an art in the science",
    "start": "1091880",
    "end": "1098240"
  },
  {
    "text": "to getting these traces to look good and be actionable so will I think you wrote",
    "start": "1098240",
    "end": "1104000"
  },
  {
    "text": "this cookbook I linked it here which has a lot of interesting tips around uh naming your lambdas and passing config",
    "start": "1104000",
    "end": "1111320"
  },
  {
    "text": "in uh to conditionally invoke runnables uh just to make sure you know the nesting is correct um and then I'm I",
    "start": "1111320",
    "end": "1119240"
  },
  {
    "text": "also linked some common failures that we've seen so again for example this one was interesting I just ran into this",
    "start": "1119240",
    "end": "1125000"
  },
  {
    "text": "this morning these all public you folks can browse around them where if if something like this failed in production",
    "start": "1125000",
    "end": "1131960"
  },
  {
    "text": "how do we found out what actually happened well in this case it's you know the there's this is a context overflow",
    "start": "1131960",
    "end": "1138159"
  },
  {
    "text": "right it's literally telling you what it is it's a model eror in this particular model whatever it's running Etc and so",
    "start": "1138159",
    "end": "1145400"
  },
  {
    "text": "you can click around uh this this particular one for example is is a case where the fix up did not work you know",
    "start": "1145400",
    "end": "1152000"
  },
  {
    "text": "the fix up logic that I'm talking about uh so it was like the SQL was so broken that that it wasn't even able to fix",
    "start": "1152000",
    "end": "1157919"
  },
  {
    "text": "that it was a complicated schema uh so this would be a case for we'll talk about it at the end of like actually",
    "start": "1157919",
    "end": "1163679"
  },
  {
    "text": "building a data set and actually fine-tuning your model uh to operate better",
    "start": "1163679",
    "end": "1169080"
  },
  {
    "text": "okay and then I think uh just to close it out um just only have a few more minutes but I know we have a Q&A at the",
    "start": "1169080",
    "end": "1176720"
  },
  {
    "text": "end is how do we tie all this together into sort of an endtoend",
    "start": "1176720",
    "end": "1181840"
  },
  {
    "text": "llm Ops flow so so this is what we do and we'd love to hear from others in the",
    "start": "1181840",
    "end": "1188640"
  },
  {
    "text": "audience what what you folks do um so we as I said have a you know a custom llm",
    "start": "1188640",
    "end": "1195039"
  },
  {
    "text": "deployed um for efficiency sake it's you know a smaller model fine-tuned uh for smaller tasks and a",
    "start": "1195039",
    "end": "1202520"
  },
  {
    "text": "bigger model for sort of more complicated chat interactions and then we regularly look",
    "start": "1202520",
    "end": "1207679"
  },
  {
    "text": "at failed runs as well as enduser disliked runs in in the cookbook there's",
    "start": "1207679",
    "end": "1212799"
  },
  {
    "text": "a there's a way to you know wire up user feedback thumbs up thumbs down and then whatever these problematic runs are we",
    "start": "1212799",
    "end": "1219799"
  },
  {
    "text": "actually add them to data sets in Lang Smith and then we actually fix the uh",
    "start": "1219799",
    "end": "1226960"
  },
  {
    "text": "fix these problematic runs and here's a couple of tips here that we found are useful um we actually use a larger llm",
    "start": "1226960",
    "end": "1234760"
  },
  {
    "text": "to propose the fixes to mistakes made by smaller llms we do this offline right now U so we might use like a 70 billion",
    "start": "1234760",
    "end": "1242000"
  },
  {
    "text": "parameter model to to fit to fix up the mistakes of a smaller model and then we",
    "start": "1242000",
    "end": "1247600"
  },
  {
    "text": "also do a lot of post-processing so whatever the fixed runs are we make sure",
    "start": "1247600",
    "end": "1252960"
  },
  {
    "text": "they're syntactically correct for example the xquery is syntactically correct or the SQL is correct and then",
    "start": "1252960",
    "end": "1258200"
  },
  {
    "text": "then you know find you in the model and deploy it again so here uh I'd like to just you know this is our last slide um",
    "start": "1258200",
    "end": "1264679"
  },
  {
    "text": "just like to call on Mike if you had any uh nuggets to share because you do a lot of this work for us in terms of yeah",
    "start": "1264679",
    "end": "1272039"
  },
  {
    "text": "thanks takam and I just just quickly you know to pull back the curtain a little bit more about what our document processing pipeline looks like um you",
    "start": "1272039",
    "end": "1279039"
  },
  {
    "text": "know we use Apache spark as kind of like the I guess I'd say the execution engine for all of this um and then right now",
    "start": "1279039",
    "end": "1287240"
  },
  {
    "text": "we're in the process we we host our llms locally and what I mean by that is like",
    "start": "1287240",
    "end": "1292440"
  },
  {
    "text": "you know in cluster in the same kubernetes cluster that we r on the rest of uh dami's code um you know right now",
    "start": "1292440",
    "end": "1299559"
  },
  {
    "text": "we're in the process of rolling out Nvidia Triton you know as kind of the platform to to host these models on um",
    "start": "1299559",
    "end": "1307440"
  },
  {
    "text": "and you know for things like uh you know for kind of like I guess I'd say for Vector database and caching and other",
    "start": "1307440",
    "end": "1313799"
  },
  {
    "text": "things we heavily use redus um uh and so that that's kind of a high level view of",
    "start": "1313799",
    "end": "1320520"
  },
  {
    "text": "like what our technology stack looks like underneath uh all of this um yeah",
    "start": "1320520",
    "end": "1325640"
  },
  {
    "text": "happy to you know take more questions about that in the in the Q&A right so so will we're right at 9:55",
    "start": "1325640",
    "end": "1332640"
  },
  {
    "text": "when you asked us uh to be so are there any questions that you'd like us to address now or should we wait till the",
    "start": "1332640",
    "end": "1338760"
  },
  {
    "text": "end after this is super super cool um I wanted to I guess follow up with a",
    "start": "1338760",
    "end": "1345600"
  },
  {
    "text": "question and say or first of all commend the fact that I like this tactic of of using a larger model offline in order to",
    "start": "1345600",
    "end": "1352000"
  },
  {
    "text": "be fixing and helping reduce the amount of human labor to improve this this pipeline you've really set up a nice endtoend flow to continuously improve",
    "start": "1352000",
    "end": "1359919"
  },
  {
    "text": "everything um and including some manual checks including some of these like llm things so I'm sure a lot of people would",
    "start": "1359919",
    "end": "1365799"
  },
  {
    "text": "have um questions about this at the end I guess leading up to this would love to",
    "start": "1365799",
    "end": "1371559"
  },
  {
    "text": "hear some of the like questions and other things um that you've had around",
    "start": "1371559",
    "end": "1376760"
  },
  {
    "text": "like improving the few shot experience there or or like are there any techniques that you've been able to",
    "start": "1376760",
    "end": "1382480"
  },
  {
    "text": "apply that have improved the average accuracy before fine tuning right so yeah no what what we do",
    "start": "1382480",
    "end": "1389080"
  },
  {
    "text": "right now is uh every 500 examples we just take 50 of them so",
    "start": "1389080",
    "end": "1395799"
  },
  {
    "text": "10% um um so every every 500 fixed runs we take 10% of them and just add them to",
    "start": "1395799",
    "end": "1402880"
  },
  {
    "text": "our few shot learning set and so if I go back here when we're running this sort",
    "start": "1402880",
    "end": "1409000"
  },
  {
    "text": "of stuff in in in production uh every time our service comes back up we redownload the few shots set and as as",
    "start": "1409000",
    "end": "1417080"
  },
  {
    "text": "you know of course will but you know for the audience what happens is the user asks a question and because we've",
    "start": "1417080",
    "end": "1424400"
  },
  {
    "text": "redownloaded let's say every week a new set of a few shot examples which are",
    "start": "1424400",
    "end": "1430000"
  },
  {
    "text": "correct and they've been human corrected um what we're just trying to find are you know some examples that are",
    "start": "1430000",
    "end": "1437159"
  },
  {
    "text": "similar uh to the particular question and the particular schema uh in you know",
    "start": "1437159",
    "end": "1442960"
  },
  {
    "text": "that the user is asking about uh so we are we have been measuring and we we",
    "start": "1442960",
    "end": "1448000"
  },
  {
    "text": "measure a few things um we were we were debating with you I think you you've seen our document on this so we measure",
    "start": "1448000",
    "end": "1455080"
  },
  {
    "text": "uh like essentially the user satisfaction with these results and what we found is yeah absolutely like better",
    "start": "1455080",
    "end": "1460720"
  },
  {
    "text": "few shot examples better in context learning like for that particular user",
    "start": "1460720",
    "end": "1466279"
  },
  {
    "text": "over time like their experience will get get very much better it's not great for generalization to be honest uh but for a",
    "start": "1466279",
    "end": "1473120"
  },
  {
    "text": "particular user I know for a particular type of of of question and answer the",
    "start": "1473120",
    "end": "1478919"
  },
  {
    "text": "the in context learning works great for generalization well at least what we",
    "start": "1478919",
    "end": "1484279"
  },
  {
    "text": "found there's no substitute to having either larger models or actually fine-tuning the model I don't know that",
    "start": "1484279",
    "end": "1491720"
  },
  {
    "text": "that's a really good response um I've got a bunch more to follow up later I'll ask one more question um from the",
    "start": "1491720",
    "end": "1497120"
  },
  {
    "text": "audience before passing it over to ml um and this is that how does hierarchical",
    "start": "1497120",
    "end": "1502960"
  },
  {
    "text": "structure work when you're extracting from PDF documents where maybe it's difficult to extract the right the right",
    "start": "1502960",
    "end": "1508480"
  },
  {
    "text": "type of structure and everything from there I think this is a really common use case where people are taking in PDFs",
    "start": "1508480",
    "end": "1513919"
  },
  {
    "text": "arbitrary from the web and then trying to do Q&A over it this is a tailor made",
    "start": "1513919",
    "end": "1519240"
  },
  {
    "text": "question for Jean maybe well this is what we been working on since four years",
    "start": "1519240",
    "end": "1524480"
  },
  {
    "text": "will that's my answer it's a lot of work um we have we are chaining a lot of llms",
    "start": "1524480",
    "end": "1530760"
  },
  {
    "text": "and layout models it's multimodel um I mean it's a lot of lot of work the answer that's why we",
    "start": "1530760",
    "end": "1537200"
  },
  {
    "text": "actually uh contributed to L chain our document loader so people can use it",
    "start": "1537200",
    "end": "1543360"
  },
  {
    "text": "with the apis and all of this so yeah so I think the quick answer would be in nowh out of time is when a PDF goes in",
    "start": "1543360",
    "end": "1552200"
  },
  {
    "text": "and something like this comes out U so it's XML it's got labels",
    "start": "1552200",
    "end": "1558320"
  },
  {
    "text": "as well as it's got structure and the text right so this is what the hierarchical structure effectively looks",
    "start": "1558320",
    "end": "1564919"
  },
  {
    "text": "like now you can run this yourself and then we have some code uh it's open source it's in in in our docy gami",
    "start": "1564919",
    "end": "1572240"
  },
  {
    "text": "loader in Lang chain which essentially takes this uh this XML format and comes",
    "start": "1572240",
    "end": "1578640"
  },
  {
    "text": "up with chunks so you know unlike other chunkers uh who just might go by you",
    "start": "1578640",
    "end": "1584000"
  },
  {
    "text": "know text or white space or something like that uh we in our case our chunks are hierarchical so you literally might",
    "start": "1584000",
    "end": "1590080"
  },
  {
    "text": "have a bigger chunk and then you might have like a little chunk inside it and we've been doing this at this point you know for for a while but I think you",
    "start": "1590080",
    "end": "1596960"
  },
  {
    "text": "folks added something recently right uh in Lang chain I think it's uh I I forget",
    "start": "1596960",
    "end": "1602600"
  },
  {
    "text": "the name but you might know there's a concept like this in Lang chain too now yeah the recursive chunker yeah",
    "start": "1602600",
    "end": "1609720"
  },
  {
    "text": "something like that yeah there's recursive CH chunker and some of the parent document parent document one exactly the parent document chunker is",
    "start": "1609720",
    "end": "1615640"
  },
  {
    "text": "the one I was thinking about so it's conceptually very similar except that we do it um using you know the structure of",
    "start": "1615640",
    "end": "1622559"
  },
  {
    "text": "the document uh as well as the semantics of the document if that makes any sense awesome yeah that's a great response I'm",
    "start": "1622559",
    "end": "1629039"
  },
  {
    "text": "gonna in interest of time turn over the mic the metaphorical mic to Emil and we've got a lot of other questions at",
    "start": "1629039",
    "end": "1634360"
  },
  {
    "text": "the end um so yeah really excited to hear what you've um prepared for us",
    "start": "1634360",
    "end": "1640720"
  },
  {
    "text": "Emil I think the the mic isn't working right now I might have to reset a little",
    "start": "1642799",
    "end": "1649520"
  },
  {
    "text": "that uh can you guys hear me now okay perfect thank you um Taki and uh the",
    "start": "1650480",
    "end": "1658440"
  },
  {
    "text": "rest of the doy gami team this was this was very helpful and I didn't know about the doy gami loader I'm gonna have to",
    "start": "1658440",
    "end": "1663679"
  },
  {
    "text": "look into it we have a use case uh so looking forward to that uh my name is Emil and uh I am read",
    "start": "1663679",
    "end": "1672799"
  },
  {
    "text": "the rehat hopefully you guys are able to see my screen right now",
    "start": "1672799",
    "end": "1679519"
  },
  {
    "text": "so uh essentially reat uh is a software as a service solution uh for real estate",
    "start": "1681000",
    "end": "1687279"
  },
  {
    "text": "agents and brokerages as of right now they're using 10 to 20 different uh",
    "start": "1687279",
    "end": "1693080"
  },
  {
    "text": "different tools uh to get their work done which is a super app that allows them to do all of that in one app uh the",
    "start": "1693080",
    "end": "1701360"
  },
  {
    "text": "reason we are relevant to this conversation is that we are creating Lucy which is a real estate uh agents",
    "start": "1701360",
    "end": "1707080"
  },
  {
    "text": "co-pilot uh the idea behind Luc is that it's a co-pilot that can do anything that resad",
    "start": "1707080",
    "end": "1713440"
  },
  {
    "text": "can do and there's a lot resad can do so Lucy should be able to do a lot of things for the agents including managing",
    "start": "1713440",
    "end": "1719679"
  },
  {
    "text": "the CRM marketing blah blah blah whatever the functionality is uh so",
    "start": "1719679",
    "end": "1725480"
  },
  {
    "text": "essentially this fact that Lucy should be able to do many things it means that it needs to have some reasoning",
    "start": "1725480",
    "end": "1732799"
  },
  {
    "text": "capability uh and it needs to have a structured output uh calling and for",
    "start": "1732799",
    "end": "1738279"
  },
  {
    "text": "this essentially the uh main uh agent that was created was L Chain's",
    "start": "1738279",
    "end": "1744039"
  },
  {
    "text": "structured chat agent this was this was one of the first ideas behind Lang chain and one of the first",
    "start": "1744039",
    "end": "1749360"
  },
  {
    "text": "agents that got a lot of traction uh the structur chat agent",
    "start": "1749360",
    "end": "1754519"
  },
  {
    "text": "allowed us to start building the features the infrastructure blah blah blah everything that we needed to get",
    "start": "1754519",
    "end": "1761120"
  },
  {
    "text": "the Prototype and MVPs working but the problem was it was way too slow to be usable because we had a lot of tools and",
    "start": "1761120",
    "end": "1769000"
  },
  {
    "text": "the prompt would get so big and that also would contribute to very bogus structured outputs so we were scratching",
    "start": "1769000",
    "end": "1775919"
  },
  {
    "text": "our heads like is fine tuning going to be able to improve things so much or should we just have a different product",
    "start": "1775919",
    "end": "1781840"
  },
  {
    "text": "Direction because we may not be able to pull this off that's when one really good thing happened for us and that's",
    "start": "1781840",
    "end": "1788159"
  },
  {
    "text": "when open AI announced the function calling capability essentially function calling",
    "start": "1788159",
    "end": "1793360"
  },
  {
    "text": "to us it seems like that um open AI people took everything that react",
    "start": "1793360",
    "end": "1799039"
  },
  {
    "text": "Frameworks offer that is reasoning and structured outputs and embedded all of that into their language model so with",
    "start": "1799039",
    "end": "1807000"
  },
  {
    "text": "function calling we had three to four times improved latency and a much more",
    "start": "1807000",
    "end": "1812120"
  },
  {
    "text": "reliable structured output this is still not perfect but uh but it's still much",
    "start": "1812120",
    "end": "1817559"
  },
  {
    "text": "more usable uh the fact that open AI actually released this at the same time that we",
    "start": "1817559",
    "end": "1823279"
  },
  {
    "text": "were looking at it gave us the confidence that hey the problems that you're having apparently or everybody",
    "start": "1823279",
    "end": "1828600"
  },
  {
    "text": "else is everybody else that is trying to build a similar thing so at least they are on the right track so that gives us",
    "start": "1828600",
    "end": "1833919"
  },
  {
    "text": "the confidence boost that what whatever we are doing seems to be we are tackling the right problems at",
    "start": "1833919",
    "end": "1840279"
  },
  {
    "text": "least so at this point using the uh the structured uh agent within Lang chain we",
    "start": "1840279",
    "end": "1847440"
  },
  {
    "text": "had created a lot of features on our client a lot a lot of the software components of it the web iOS back end",
    "start": "1847440",
    "end": "1854039"
  },
  {
    "text": "blah blah blah everything was built and now due to open functions we uh realize",
    "start": "1854039",
    "end": "1859639"
  },
  {
    "text": "that the idea that we working on is not feasible however it is not a polished experience yet it is still uh with",
    "start": "1859639",
    "end": "1867000"
  },
  {
    "text": "misfire a lot and with hallucinate and blah blah all the issues that people have experienced and this is where the real",
    "start": "1867000",
    "end": "1873919"
  },
  {
    "text": "challenges uh begun for us uh the challenge of how to improve the performance of uh of your language model",
    "start": "1873919",
    "end": "1881080"
  },
  {
    "text": "even even your use case now generally speaking there's two different uh",
    "start": "1881080",
    "end": "1886360"
  },
  {
    "text": "methods of proving your language models performance one is prompt engineering one is fine tuning for both of them",
    "start": "1886360",
    "end": "1892559"
  },
  {
    "text": "there is a big prerequisite and that is for you to be able to measure uh the",
    "start": "1892559",
    "end": "1898120"
  },
  {
    "text": "performance you're extracting because if you can measure it you obviously would would not have an idea if you're",
    "start": "1898120",
    "end": "1903320"
  },
  {
    "text": "actually improving things or not you would be changing things without knowing if they are getting",
    "start": "1903320",
    "end": "1908960"
  },
  {
    "text": "better uh that's when we started to look into uh to the options of how do we",
    "start": "1908960",
    "end": "1914120"
  },
  {
    "text": "measure uh our llm uh there there are some different methods for example you",
    "start": "1914120",
    "end": "1919279"
  },
  {
    "text": "hear a lot these days about evaluation Frameworks uh that turns out is not the",
    "start": "1919279",
    "end": "1925440"
  },
  {
    "text": "exact thing we looking for because they're dedicated to evaluating language models not the applications that are",
    "start": "1925440",
    "end": "1932039"
  },
  {
    "text": "built using language models and if we are not building a language model we are building the application that uses the",
    "start": "1932039",
    "end": "1937399"
  },
  {
    "text": "language model so I think evaluation Frameworks there a wrong level of um",
    "start": "1937399",
    "end": "1943360"
  },
  {
    "text": "evaluation for us uh we could have also looked at our user feedback but at this point they didn't have any users to get",
    "start": "1943360",
    "end": "1949799"
  },
  {
    "text": "feedback from and there was a chicken and egg problem because if we don't if we couldn't make it much smaller we",
    "start": "1949799",
    "end": "1955480"
  },
  {
    "text": "couldn't release it to users and if we couldn't release it to users we couldn't get enough feedback uh that's when we started",
    "start": "1955480",
    "end": "1963120"
  },
  {
    "text": "looking at another option and that is oldfashioned testing using cicd components that we all have used for",
    "start": "1963120",
    "end": "1969600"
  },
  {
    "text": "years and years essentially our idea was that we would create a pipeline that",
    "start": "1969600",
    "end": "1975279"
  },
  {
    "text": "would test the different use cases for uh for Lucy U the problem as you all",
    "start": "1975279",
    "end": "1981720"
  },
  {
    "text": "know is that language models are not deterministic even when you set the temperature to zero uh so we would uh",
    "start": "1981720",
    "end": "1988399"
  },
  {
    "text": "create a test case to for Lucy to do this it may work sometimes it may not work some other times it would be",
    "start": "1988399",
    "end": "1995240"
  },
  {
    "text": "somewhat random uh but the idea we had was that",
    "start": "1995240",
    "end": "2000399"
  },
  {
    "text": "if we run a lot of tests then we would get an idea an statistical idea rather",
    "start": "2000399",
    "end": "2006360"
  },
  {
    "text": "than a yes and no of if it's working or not we get a statistical idea of if things are improving or not uh so that",
    "start": "2006360",
    "end": "2013760"
  },
  {
    "text": "would be in the in the right direction but having that data of of creating so",
    "start": "2013760",
    "end": "2019760"
  },
  {
    "text": "many different cases would also require a sta users which we didn't have so we started looking into synthetically",
    "start": "2019760",
    "end": "2026799"
  },
  {
    "text": "generated uh test cases that's one problem we had uh about creating a",
    "start": "2026799",
    "end": "2032679"
  },
  {
    "text": "pipeline of tests and the second problem was that the existing test Frameworks were not that suitable for this use case",
    "start": "2032679",
    "end": "2039039"
  },
  {
    "text": "for example jist is really focused on front end testing mocha is not able to",
    "start": "2039039",
    "end": "2044760"
  },
  {
    "text": "handle many concurrent tests and we really needed to run hundreds of concurrent tests otherwise each pipeline",
    "start": "2044760",
    "end": "2050800"
  },
  {
    "text": "run would take like an hour or something which wasn't uh which wasn't good and",
    "start": "2050800",
    "end": "2056118"
  },
  {
    "text": "there's a lot of new tools and services being uh introduced every day in this space but we couldn't find any that was",
    "start": "2056119",
    "end": "2063240"
  },
  {
    "text": "very exact for our use case so at that point we about creating our own test",
    "start": "2063240",
    "end": "2068560"
  },
  {
    "text": "Runners and obviously how hard this can be right so uh I'm going to skip all the",
    "start": "2068560",
    "end": "2077760"
  },
  {
    "text": "all the way that we ended up creating this but and show you what we ended up what we ended up with is a platform that",
    "start": "2077760",
    "end": "2084440"
  },
  {
    "text": "allows us to generate a lot of test cases for use case like the one that you can see in this screenshot we ask chat",
    "start": "2084440",
    "end": "2091960"
  },
  {
    "text": "GPT in a very manual way that hey create um create user requests for this use",
    "start": "2091960",
    "end": "2099400"
  },
  {
    "text": "case for me create 50 of them for me and give them to me in a Json file format uh",
    "start": "2099400",
    "end": "2105599"
  },
  {
    "text": "we import them manually into our cicd um and then we run that cicd and we get uh",
    "start": "2105599",
    "end": "2112960"
  },
  {
    "text": "and they get results of how each use case is running this this is an actual screenshot from our cicd it shows that",
    "start": "2112960",
    "end": "2119640"
  },
  {
    "text": "hey for example for this this case we've had these many tests this many are uh passing through these many are failing",
    "start": "2119640",
    "end": "2126640"
  },
  {
    "text": "and we are continuously adding new assertions to find the new ways that our",
    "start": "2126640",
    "end": "2132720"
  },
  {
    "text": "language models can fail and adding them so that you're actually testing better and",
    "start": "2132720",
    "end": "2139359"
  },
  {
    "text": "better uh so in Virtual specs the measuring platform that we created makes it very very easy for us to create new",
    "start": "2139480",
    "end": "2148200"
  },
  {
    "text": "uh test cases for each use case that we have to luy and it's very easy for us to run those test cases I think we have",
    "start": "2148200",
    "end": "2154800"
  },
  {
    "text": "hundreds of cases that run under like 3 four minutes every time you run the test so it's very easy for us to make some",
    "start": "2154800",
    "end": "2160319"
  },
  {
    "text": "changes run them see if it's getting better or worse there were some mistakes for example we are still not sure if",
    "start": "2160319",
    "end": "2167000"
  },
  {
    "text": "writing our on test Runner was the right choice it wasn't that difficult to create but it would have been much",
    "start": "2167000",
    "end": "2173640"
  },
  {
    "text": "better if we um if we leveraged an existing platform no JS test Runners may",
    "start": "2173640",
    "end": "2179240"
  },
  {
    "text": "have been a very good choice for us but for some reason we didn't use that we may have to revert that back and we",
    "start": "2179240",
    "end": "2186040"
  },
  {
    "text": "should have probably lever Smith for this um this predates A Smith so that's",
    "start": "2186040",
    "end": "2191839"
  },
  {
    "text": "why we didn't but we in the future we should probably be switching to Smith and I'll be explaining how Smith helps",
    "start": "2191839",
    "end": "2197280"
  },
  {
    "text": "us in this context anyways going forward now with the",
    "start": "2197280",
    "end": "2202839"
  },
  {
    "text": "measurement piece in place now we had the capability to actually do some prompt engineering and improve the",
    "start": "2202839",
    "end": "2209160"
  },
  {
    "text": "performance of our El but before we could actually do some prompt engineering it was very important for us",
    "start": "2209160",
    "end": "2215480"
  },
  {
    "text": "to be able to do this fast and have a developer process and experience that makes it very easy for",
    "start": "2215480",
    "end": "2221920"
  },
  {
    "text": "us to iterate through different problems uh find them find what why has something",
    "start": "2221920",
    "end": "2227920"
  },
  {
    "text": "failed reproduce it change it manipulate it and repeat the process uh so the",
    "start": "2227920",
    "end": "2233839"
  },
  {
    "text": "development experience for this was very important for us imagine like not having part reload and react and how how long",
    "start": "2233839",
    "end": "2240839"
  },
  {
    "text": "would take for you to develop something without that that's that's where Smith came to our help uh",
    "start": "2240839",
    "end": "2247640"
  },
  {
    "text": "essentially we connected our app to Smith under five minutes it's super easy",
    "start": "2247640",
    "end": "2253160"
  },
  {
    "text": "to do that and once we did that we started integrating more and more for example we started sending tags like",
    "start": "2253160",
    "end": "2260240"
  },
  {
    "text": "commit hash Branch name test street name blah blah everything to A Smith so it becomes easier and easier for us to take",
    "start": "2260240",
    "end": "2266319"
  },
  {
    "text": "a look at a Smith and get an understanding of uh of what it is",
    "start": "2266319",
    "end": "2271440"
  },
  {
    "text": "exactly that we are pushing to Smith uh and then we added some",
    "start": "2271440",
    "end": "2276599"
  },
  {
    "text": "Integrations from Smith to our um to our CI pipeline for example now anytime we",
    "start": "2276599",
    "end": "2283079"
  },
  {
    "text": "have a a test that fails it puts a tiny link uh under Smith under the failure uh",
    "start": "2283079",
    "end": "2290119"
  },
  {
    "text": "that links directly to the Smith uh case so we can click on it and get an",
    "start": "2290119",
    "end": "2295240"
  },
  {
    "text": "understanding of what has failed what exactly has failed and and get into the details so it's very easy for us to take",
    "start": "2295240",
    "end": "2302440"
  },
  {
    "text": "a look at the logs of a pipeline see which tests are failed click on them get the details of the exact failure and",
    "start": "2302440",
    "end": "2309400"
  },
  {
    "text": "then there's a tiny little play ground button uh up on every Smith chain that",
    "start": "2309400",
    "end": "2314920"
  },
  {
    "text": "you can click on and it would load everything ready all the prompts and functions and everything that you put",
    "start": "2314920",
    "end": "2320359"
  },
  {
    "text": "together in that prompt for you to be able to reproduce it this this gives us a fantastic and beautiful developer",
    "start": "2320359",
    "end": "2327000"
  },
  {
    "text": "experience because we can just click on a link of a tail test case reproduce it",
    "start": "2327000",
    "end": "2333240"
  },
  {
    "text": "using uh using Smith playground make sure sure that uh everything uh make",
    "start": "2333240",
    "end": "2339359"
  },
  {
    "text": "sure it's super producible and then start manipulating it changing it getting to a better prompt and once we",
    "start": "2339359",
    "end": "2344839"
  },
  {
    "text": "have a prompt that is working in this case we can commit that prompt to our cicd and have that uh run against all of",
    "start": "2344839",
    "end": "2352359"
  },
  {
    "text": "our test cases and get uh the Fantastic results back hopefully uh but this this",
    "start": "2352359",
    "end": "2358119"
  },
  {
    "text": "whole process would take only a few minutes for us to push something go read",
    "start": "2358119",
    "end": "2363520"
  },
  {
    "text": "the logs reproduce them fix them blah blah all of that becomes a becomes a",
    "start": "2363520",
    "end": "2369280"
  },
  {
    "text": "process that happens in a matter of minutes uh using Smith so this is where Smith actually helped us get the",
    "start": "2369280",
    "end": "2375640"
  },
  {
    "text": "Fantastic um developer experience for the prompt engineering that we wanted to",
    "start": "2375640",
    "end": "2380839"
  },
  {
    "text": "do but after that we had the actual challenges of prompt",
    "start": "2380839",
    "end": "2386040"
  },
  {
    "text": "engineering uh because as mentioned Lucy is is an agent that is supposed to do",
    "start": "2386040",
    "end": "2391240"
  },
  {
    "text": "many things it relies on open AI function calling uh and by definition uh",
    "start": "2391240",
    "end": "2397040"
  },
  {
    "text": "that's that puts some constraints uh on the things that we can do first of all",
    "start": "2397040",
    "end": "2402440"
  },
  {
    "text": "uh this open area function calling is a process that requires your agent to make many many different calls uh to the to",
    "start": "2402440",
    "end": "2409920"
  },
  {
    "text": "the language model before it can extract results it's kind of an iterative process and that means it's by Nature",
    "start": "2409920",
    "end": "2415440"
  },
  {
    "text": "somewhat slow so techniques like breaking down a problem and running it in different proms would make it even",
    "start": "2415440",
    "end": "2422280"
  },
  {
    "text": "more slower so that's usually a no go for us because no body wants to send the",
    "start": "2422280",
    "end": "2427520"
  },
  {
    "text": "message to their uh co-pilot and wait for two minutes to get an answer so this needs to be a faster",
    "start": "2427520",
    "end": "2434280"
  },
  {
    "text": "process uh another challenge is that in open AI functions uh we can provide a",
    "start": "2434280",
    "end": "2441119"
  },
  {
    "text": "lot of functions and those functions can have descriptions and Json schemas provided to them uh the language model",
    "start": "2441119",
    "end": "2448560"
  },
  {
    "text": "does a very good job of following the schemas that is provided uh however if there is a",
    "start": "2448560",
    "end": "2455000"
  },
  {
    "text": "mistake you can you it doesn't really pick up the instructions that you give to it in terms of how to use those uh",
    "start": "2455000",
    "end": "2463640"
  },
  {
    "text": "schemas so for example we have a use case that um the users asked for Lucy to",
    "start": "2463640",
    "end": "2471040"
  },
  {
    "text": "write an email for them language model always puts a placeholder at the bottom of the email that says uh regards and",
    "start": "2471040",
    "end": "2478640"
  },
  {
    "text": "the placeholders your name and no matter how how hard we try we cannot get the language model not to do that so the",
    "start": "2478640",
    "end": "2485800"
  },
  {
    "text": "next step would be fine-tuning or software patching that because uh using",
    "start": "2485800",
    "end": "2490880"
  },
  {
    "text": "prompt engineering techniques is somewhat difficult for us to communicate with the language model what to do and",
    "start": "2490880",
    "end": "2496480"
  },
  {
    "text": "what not to do um in functions for example it completely ignores few shot",
    "start": "2496480",
    "end": "2501680"
  },
  {
    "text": "prompting in the tools tool descriptions and if you provide a lot of examples in the Json schemas that can lead to",
    "start": "2501680",
    "end": "2508800"
  },
  {
    "text": "hallucinations more than it helps with actually doing the right thing another thing we found that uh is that open AI",
    "start": "2508800",
    "end": "2516359"
  },
  {
    "text": "is not great with enumerations uh in the functions that you pass to it especially if those enams",
    "start": "2516359",
    "end": "2522800"
  },
  {
    "text": "are somewhat optional but Lang chains handle parsing errors can help this",
    "start": "2522800",
    "end": "2528160"
  },
  {
    "text": "massively we basically manage to reduce the number of issues we had in this to zero once we just turn the handle parts",
    "start": "2528160",
    "end": "2534400"
  },
  {
    "text": "and errs flag on on L now up until now we covered what we've",
    "start": "2534400",
    "end": "2542160"
  },
  {
    "text": "learned uh in terms of prompt engineering and the developer experience that we managed to create for our prompt",
    "start": "2542160",
    "end": "2548319"
  },
  {
    "text": "Engineers but as mentioned there are some challenges that are not easy to resolve with prompt engineering alone so",
    "start": "2548319",
    "end": "2554400"
  },
  {
    "text": "the next step is for us to do fine-tuning uh because we rely very",
    "start": "2554400",
    "end": "2559720"
  },
  {
    "text": "heavily on function calling and reasoning capabilities and GPT models",
    "start": "2559720",
    "end": "2565240"
  },
  {
    "text": "are known uh to be best for those we have never bothered with any other language model so we double down on GPT",
    "start": "2565240",
    "end": "2571680"
  },
  {
    "text": "models as of right now also gp4 is extremely slow for us again because open",
    "start": "2571680",
    "end": "2577559"
  },
  {
    "text": "function calling is an iterative process with gp4 you have to call the language model many many times that makes it slow",
    "start": "2577559",
    "end": "2583960"
  },
  {
    "text": "so as of right now we are limited to GPT 3.5 open AI announced fine tuning for uh",
    "start": "2583960",
    "end": "2590640"
  },
  {
    "text": "GPT 3.5 last month uh so we were very excited about that because that finally",
    "start": "2590640",
    "end": "2597839"
  },
  {
    "text": "allowed us to start fine-tuning our model and make it much more smarter but as of right now the GPT 3.5 fine tune",
    "start": "2597839",
    "end": "2605359"
  },
  {
    "text": "models would lose their function calling uh capability uh which is a huge part of",
    "start": "2605359",
    "end": "2611559"
  },
  {
    "text": "um what Lucy does uh but they announced that in later in Fall they're going to",
    "start": "2611559",
    "end": "2617720"
  },
  {
    "text": "introduce uh fine tuned U function calling models so where we are is that",
    "start": "2617720",
    "end": "2623680"
  },
  {
    "text": "we are right now trying to create the pipelines the processing and everything uh processes necessary uh to build a",
    "start": "2623680",
    "end": "2630960"
  },
  {
    "text": "pipeline so we have the data that we need so when open AI uh",
    "start": "2630960",
    "end": "2637480"
  },
  {
    "text": "uh announces fine tune fine tuning capabilities for function calling models",
    "start": "2637480",
    "end": "2643319"
  },
  {
    "text": "we have everything ready to um to essentially roll out the smarter version",
    "start": "2643319",
    "end": "2648640"
  },
  {
    "text": "of our app uh this is the process that we are creating right now uh we are leveraging um a Smiths for this again so",
    "start": "2648640",
    "end": "2656760"
  },
  {
    "text": "the idea is that you would have conversations with Lucy which obviously uses the language model uh when those",
    "start": "2656760",
    "end": "2664160"
  },
  {
    "text": "conversations uh go well we put them in a smoth database called good samples",
    "start": "2664160",
    "end": "2669680"
  },
  {
    "text": "when they don't go that well we put them manually in a database called bad samples uh for the good samples they are",
    "start": "2669680",
    "end": "2676480"
  },
  {
    "text": "good so we can we can essentially leverage them for fine tuning but for bad samples we would have our engineering team uh start a curation",
    "start": "2676480",
    "end": "2683720"
  },
  {
    "text": "process of looking into them getting an understanding what has failed uh and then annotating and labeling uh that",
    "start": "2683720",
    "end": "2691880"
  },
  {
    "text": "data so we can turn them into good samples uh so that we can later find",
    "start": "2691880",
    "end": "2697480"
  },
  {
    "text": "feed them back into the find tuning model one problem we have as of right now is that the tooling around this",
    "start": "2697480",
    "end": "2703559"
  },
  {
    "text": "manual curations that our Engineers would have to do is not there yet so we",
    "start": "2703559",
    "end": "2708640"
  },
  {
    "text": "are considering building some but a lot of that already exist in Smith so it's a",
    "start": "2708640",
    "end": "2713720"
  },
  {
    "text": "it's always a challenge of finding what exactly you have to build in housee and what are the things that you can leverage from outside tooling we hoping",
    "start": "2713720",
    "end": "2720680"
  },
  {
    "text": "that Tooling in this space would uh would continue to uh to improve uh some",
    "start": "2720680",
    "end": "2726599"
  },
  {
    "text": "final thoughts around this is that what we find out is that the best practices are not really formed yet um nobody is",
    "start": "2726599",
    "end": "2734400"
  },
  {
    "text": "100% sure what are the best practices in terms of creating a uh capable agent",
    "start": "2734400",
    "end": "2740400"
  },
  {
    "text": "like this the tooling is not there yet U as well but it's moving extremely fast",
    "start": "2740400",
    "end": "2746839"
  },
  {
    "text": "um we are seeing improvements every day we uh we had a miraculous moment where",
    "start": "2746839",
    "end": "2752800"
  },
  {
    "text": "split showed up otherwise we would have had to create a lot more manual Ian in house be we're glad it did uh but we",
    "start": "2752800",
    "end": "2759720"
  },
  {
    "text": "expect a lot of more and better tooling to uh to show off for data annotation and manual curation and whatnot uh also",
    "start": "2759720",
    "end": "2767119"
  },
  {
    "text": "one other thing thing we know is that a lot of pieces that we have created so far in this whole pipelines that we've",
    "start": "2767119",
    "end": "2773880"
  },
  {
    "text": "created uh are probably going to be deprecated pretty soon um so sooner or",
    "start": "2773880",
    "end": "2780680"
  },
  {
    "text": "later we're going to have a better pipeline better test running better this and better that",
    "start": "2780680",
    "end": "2786319"
  },
  {
    "text": "uh so to us everything as of right now is temporary until a better solution comes in and we would be glad to change",
    "start": "2786319",
    "end": "2792079"
  },
  {
    "text": "everything but we are going in this with with an understanding of that the fact that everything is moving so fast in",
    "start": "2792079",
    "end": "2798240"
  },
  {
    "text": "this area uh one other thing we we learned is that not everything needs to be 100% automated and scalable uh for",
    "start": "2798240",
    "end": "2805280"
  },
  {
    "text": "example a part of our process is talking to chat GPD and generating synthetic cases and importing that uh those cases",
    "start": "2805280",
    "end": "2812359"
  },
  {
    "text": "into our code base that is not a super uh automated process but it works really",
    "start": "2812359",
    "end": "2818480"
  },
  {
    "text": "well as of right now and it's scalable enough for us uh and that's pretty much that's",
    "start": "2818480",
    "end": "2826000"
  },
  {
    "text": "pretty much it uh that's pretty much everything that we had learned in our journey to create a very capable",
    "start": "2826000",
    "end": "2834520"
  },
  {
    "text": "co-pilot awesome thank you Emil that was really exciting I I love the takeaways around the different prompting",
    "start": "2834520",
    "end": "2840200"
  },
  {
    "text": "techniques what didn't work um and also the the shared thread with also the docu comy team on um like thinking about fine",
    "start": "2840200",
    "end": "2847240"
  },
  {
    "text": "tuning and trying to use it tactfully within your overall application in a way that makes sense and that is devoted to",
    "start": "2847240",
    "end": "2853400"
  },
  {
    "text": "improving the service of your um users one question that I've seen a few people asking um is around this whole process",
    "start": "2853400",
    "end": "2860680"
  },
  {
    "text": "of trying to get the right training set or prepare it for fine tuning um I know that you both sort of touched on some",
    "start": "2860680",
    "end": "2866400"
  },
  {
    "text": "things about how you're thinking about organizing data sets and thinking about fine tuning I wonder if you could each",
    "start": "2866400",
    "end": "2872040"
  },
  {
    "text": "um I guess mention some of the the the key decisions that you've made or the",
    "start": "2872040",
    "end": "2877680"
  },
  {
    "text": "key thoughts or open questions you have there uh yeah so the biggest part is",
    "start": "2877680",
    "end": "2884280"
  },
  {
    "text": "that we have to be really open-minded in terms of what we decide to do because what we find out is that whatever we",
    "start": "2884280",
    "end": "2891280"
  },
  {
    "text": "decide to do in two months time may be deprecated and there may be better ideas so I think that the biggest one is Let's",
    "start": "2891280",
    "end": "2896760"
  },
  {
    "text": "Stay open-minded and be ready to change uh however I think the biggest decisions",
    "start": "2896760",
    "end": "2901800"
  },
  {
    "text": "we've made so far was that hey as of right now and again able enough is good",
    "start": "2901800",
    "end": "2906839"
  },
  {
    "text": "enough uh so as of right now for example I see people asking how do we differentiate between uh between good",
    "start": "2906839",
    "end": "2913920"
  },
  {
    "text": "and bad examples and whatnot what we do is that we leverage our internal QA team so we are having conversations with our",
    "start": "2913920",
    "end": "2920200"
  },
  {
    "text": "with our co-pilots hundreds on hundreds of times a day and as of right now there's a human making that decision",
    "start": "2920200",
    "end": "2926880"
  },
  {
    "text": "that hey this was a good case like Lucy answered me B enough so I'm going to uh",
    "start": "2926880",
    "end": "2932920"
  },
  {
    "text": "label this as a good example and put it in a good box bucket but if the response was not good enough um I'm going to put",
    "start": "2932920",
    "end": "2939440"
  },
  {
    "text": "it in a bad examples uh bucket uh Lang chain evaluators could be the next",
    "start": "2939440",
    "end": "2945680"
  },
  {
    "text": "scalable options for us that we rely on language models to do this but as of right now we want to have that human",
    "start": "2945680",
    "end": "2952440"
  },
  {
    "text": "mentorship of the of the copilot so uh so it's on the right track yeah so for",
    "start": "2952440",
    "end": "2958960"
  },
  {
    "text": "dyami we sort of benefit from the fact that we've been doing this for five and a half years you know uh so so there's",
    "start": "2958960",
    "end": "2965960"
  },
  {
    "text": "some uh you know some techniques that predate the let's say the modern llm era",
    "start": "2965960",
    "end": "2973480"
  },
  {
    "text": "uh so in the computer vision world for example uh because we we look at documents as you know images and",
    "start": "2973480",
    "end": "2980000"
  },
  {
    "text": "structures Etc so in the computer vision world there's actually pretty robust Tooling in this area uh so down to uh",
    "start": "2980000",
    "end": "2990280"
  },
  {
    "text": "you know identifying things like this like bad examples good examples usually using other models sort ofer student",
    "start": "2990280",
    "end": "2997280"
  },
  {
    "text": "models right like the idea that I gave uh down to versioning your data sets we found that's a critical thing like last",
    "start": "2997280",
    "end": "3004319"
  },
  {
    "text": "week's data set versus you know last month's data set Etc sometimes data sets go wild and you have to revert back to",
    "start": "3004319",
    "end": "3011280"
  },
  {
    "text": "an older data set so I think we benefited uh we use this tool I love you",
    "start": "3011280",
    "end": "3016319"
  },
  {
    "text": "know I love them they call V7 Labs you know they're pretty good uh you know for our image uh data set management for for",
    "start": "3016319",
    "end": "3023160"
  },
  {
    "text": "a text based sort of uh data sets we're using lsmith and and will you know I've",
    "start": "3023160",
    "end": "3028920"
  },
  {
    "text": "been sort of asking you for a few features but roughly the way I think about it is a a you need robust data set",
    "start": "3028920",
    "end": "3036200"
  },
  {
    "text": "management like I said versioning and what's a published data set what's a draft data set you also need robust",
    "start": "3036200",
    "end": "3043319"
  },
  {
    "text": "workflows so uh imagine there's a there's a run that comes in that needs to be fixed up like we said sometimes",
    "start": "3043319",
    "end": "3050160"
  },
  {
    "text": "you can have a model help the annotator fix the Run uh right like sort of a syst",
    "start": "3050160",
    "end": "3055680"
  },
  {
    "text": "models then you need some sort of checks like did somebody fat finger something you also want a pipeline what we found",
    "start": "3055680",
    "end": "3061799"
  },
  {
    "text": "super useful is having multiple eyes on it so having an annotator than having a reviewer right sort of like a two eyes",
    "start": "3061799",
    "end": "3068799"
  },
  {
    "text": "protocol even if you use humans one of those eyes can be machine right like a machine and a human so I think there",
    "start": "3068799",
    "end": "3074440"
  },
  {
    "text": "it's a very rich topic but uh personally yeah I'm excited to have Lang Smith uh",
    "start": "3074440",
    "end": "3081280"
  },
  {
    "text": "have a lot of these capabilities built in like the the llm evaluators are awesome as a start but let's just say a",
    "start": "3081280",
    "end": "3087640"
  },
  {
    "text": "few more things needed and then we'd love to get get used get you know be all over",
    "start": "3087640",
    "end": "3092880"
  },
  {
    "text": "that totally yeah those are great points um I think versioning is really key being able to put the um put in some",
    "start": "3092880",
    "end": "3099520"
  },
  {
    "text": "little El grease into labeling and making sure you get some some human supervision there and using whatever",
    "start": "3099520",
    "end": "3105000"
  },
  {
    "text": "tooling is really good for that in order to be reviewing uh in that domain so and thanks for the um the the recommendation",
    "start": "3105000",
    "end": "3111240"
  },
  {
    "text": "there set seven Labs you said for the any it's V7 they're gonna get a bunch of",
    "start": "3111240",
    "end": "3116400"
  },
  {
    "text": "inbound now because of this mention but yeah they're they're they're pretty good you know I love calling out good products they're a small company uh yeah",
    "start": "3116400",
    "end": "3123200"
  },
  {
    "text": "they're but they're for image annotation now for NLP annotation in the past we've also used Prodigy quite a bit uh i' I'd",
    "start": "3123200",
    "end": "3129839"
  },
  {
    "text": "love to recommend them there by you know the explosion AI folks uh space the people who do Spacey that's fantastic uh",
    "start": "3129839",
    "end": "3136440"
  },
  {
    "text": "so Prodigy is absolutely amazing and frankly I would love to see some sort of a link up between you know a tool like",
    "start": "3136440",
    "end": "3143599"
  },
  {
    "text": "Prodigy and a tool like Langs Smith uh there's a lot of great tools out there",
    "start": "3143599",
    "end": "3148839"
  },
  {
    "text": "essentially totally um sort of related to the image part of it um someone asked",
    "start": "3148839",
    "end": "3154319"
  },
  {
    "text": "how you're handling multimedia types of documents in dyami how are you integrating this information with the LM",
    "start": "3154319",
    "end": "3161599"
  },
  {
    "text": "pip yes so I answered it partially in the chat but let me just summarize and then maybe others want to jump in uh so",
    "start": "3161599",
    "end": "3169079"
  },
  {
    "text": "look it's like we said right it there's the document but it's a complex Beast",
    "start": "3169079",
    "end": "3174599"
  },
  {
    "text": "it's not just a simple as just a stream of text sometimes there's nonlinear layouts right like there's no linear",
    "start": "3174599",
    "end": "3181440"
  },
  {
    "text": "representation of the document tables are a great example you could have a floating box in the middle of the",
    "start": "3181440",
    "end": "3186760"
  },
  {
    "text": "document Etc right so yeah essentially it's a multipass approach as that that's what I described we first find these",
    "start": "3186760",
    "end": "3193359"
  },
  {
    "text": "regions like here's a tabular region here's a figure here's a chart and then we take the text flow the reading order",
    "start": "3193359",
    "end": "3199960"
  },
  {
    "text": "you know around it so the llm can operate on it and then within a particular Island like a particular you",
    "start": "3199960",
    "end": "3205760"
  },
  {
    "text": "know table or whatever we do further sub processing and it's it's complicated right like I said there's multicolumn",
    "start": "3205760",
    "end": "3211200"
  },
  {
    "text": "flow stitching things together but then there's things like page headers and Footers that you need to jump over right",
    "start": "3211200",
    "end": "3217520"
  },
  {
    "text": "uh so you know it's it's the joke right like documents not text there is an additional even you know complexity",
    "start": "3217520",
    "end": "3224280"
  },
  {
    "text": "which is what we call the Z order which is when we group documents that talks about the same thing that's very",
    "start": "3224280",
    "end": "3230960"
  },
  {
    "text": "important right even if they look different like if you take a different I don't know real estate documents or they",
    "start": "3230960",
    "end": "3238240"
  },
  {
    "text": "just look different but they speak about the same thing so our approach and that's the XML data model is to make",
    "start": "3238240",
    "end": "3245040"
  },
  {
    "text": "sure those trees are labeled in the same using the same label for every chunk and",
    "start": "3245040",
    "end": "3251160"
  },
  {
    "text": "sub Chun that's added added an additional you know additional thing so it's not only document by document but",
    "start": "3251160",
    "end": "3257839"
  },
  {
    "text": "it's just doc set around the same documents in a category just a funny thing around that I mean the St that",
    "start": "3257839",
    "end": "3264240"
  },
  {
    "text": "Jean mentioned I mean it's in our logo too right so documents are two dimensional objects like particular",
    "start": "3264240",
    "end": "3269960"
  },
  {
    "text": "document but there's a third dimension which is all the other similar documents",
    "start": "3269960",
    "end": "3275000"
  },
  {
    "text": "so uh that's that's something I'd encourage the audience to to look into as well yeah it seems like a lot of what",
    "start": "3275000",
    "end": "3281680"
  },
  {
    "text": "you're pointing at is taking advantage of whatever your domain is so it may not be a general for an llm thing but like",
    "start": "3281680",
    "end": "3287599"
  },
  {
    "text": "whatever your domain is and you leverage that domain knowledge in order to provide some sort of supervision to provide additional structure to your",
    "start": "3287599",
    "end": "3294160"
  },
  {
    "text": "application even production and use that in different steps of the pipeline in order to improve the overall quality for the end users so really don't don't",
    "start": "3294160",
    "end": "3300760"
  },
  {
    "text": "don't like forget all the stuff that is unique to your domain whenever you're building a product that's how",
    "start": "3300760",
    "end": "3307440"
  },
  {
    "text": "you exactly um I one final question from the audience uh and it's pretty General",
    "start": "3307440",
    "end": "3313079"
  },
  {
    "text": "so I'll give it up to both of you um and then we'll we'll finish up and I'm sure there'll be more questions after since",
    "start": "3313079",
    "end": "3318680"
  },
  {
    "text": "there's a lot more content to share the what was like the single most uh like",
    "start": "3318680",
    "end": "3325119"
  },
  {
    "text": "largest obstacle that you had to overcome in getting from the prototype to production what was the single thing",
    "start": "3325119",
    "end": "3330359"
  },
  {
    "text": "that you really had to do you want to",
    "start": "3330359",
    "end": "3335599"
  },
  {
    "text": "start uh I mean that's a that's a very abstract one and difficult to answer but",
    "start": "3335599",
    "end": "3340640"
  },
  {
    "text": "I think uh getting the language model uh to work is uh is busy enough but getting",
    "start": "3340640",
    "end": "3348319"
  },
  {
    "text": "it to work reliably for for users to the level that they they they expect that's a little bit difficult so getting the",
    "start": "3348319",
    "end": "3355839"
  },
  {
    "text": "level that that last two three% of quality that they expect pushing it through that that's the most difficult",
    "start": "3355839",
    "end": "3361960"
  },
  {
    "text": "part everything else is easier I mean for us uh because we you",
    "start": "3361960",
    "end": "3367839"
  },
  {
    "text": "know very early on made the decision to have our own Foundation model um so I",
    "start": "3367839",
    "end": "3373160"
  },
  {
    "text": "know a lot of demos I gave and the links you know are using open AI Etc just so others can reproduce but in in in",
    "start": "3373160",
    "end": "3379440"
  },
  {
    "text": "production we have our own uh Foundation model so I think performance and reliability running it at scale across",
    "start": "3379440",
    "end": "3387119"
  },
  {
    "text": "distributed nodes that fail um Mike you want to say a few words about that I think that's been for us the biggest",
    "start": "3387119",
    "end": "3393440"
  },
  {
    "text": "challenge the cogs you know the cost of it yeah know I mean I mean you you hit",
    "start": "3393440",
    "end": "3399440"
  },
  {
    "text": "all the the high level points but yeah I mean running these large M you know models at scale and and keeping cost in",
    "start": "3399440",
    "end": "3406680"
  },
  {
    "text": "mind you know is something that I'll say is a continual area of investment for us",
    "start": "3406680",
    "end": "3412400"
  },
  {
    "text": "you know and and especially you you know when you throw in things like dynamically having to scale things up",
    "start": "3412400",
    "end": "3418680"
  },
  {
    "text": "and down based on usage um uh it it yeah it just it's it's non-trivial you know",
    "start": "3418680",
    "end": "3425920"
  },
  {
    "text": "certainly um you know and like like you know I know there are there are thirdparty services that offer things",
    "start": "3425920",
    "end": "3432599"
  },
  {
    "text": "like this but uh for doyi you know we've chosen for various reasons to host",
    "start": "3432599",
    "end": "3438400"
  },
  {
    "text": "things locally and and and so hence the investment in that space for us yeah",
    "start": "3438400",
    "end": "3446200"
  },
  {
    "text": "custo customer data privacy is super critical yeah that makes sense that",
    "start": "3446200",
    "end": "3451760"
  },
  {
    "text": "makes sense a lot of times you can't really be shoving this information over to external or third party API um well",
    "start": "3451760",
    "end": "3458400"
  },
  {
    "text": "it looks like we're running out of time here I just wanted to thank again all of our presenters here today I think that we've got a lot of valuable insights",
    "start": "3458400",
    "end": "3464640"
  },
  {
    "text": "from this and um looking forward to following up with other things to share afterwards so thank you thank you thank",
    "start": "3464640",
    "end": "3471359"
  },
  {
    "text": "you so much everybody thank you L chain thank you yeah thank you well thank",
    "start": "3471359",
    "end": "3477079"
  },
  {
    "text": "you",
    "start": "3477079",
    "end": "3480079"
  }
]