[
  {
    "text": "he's going to walk over how to build a chat GPT like clone in in JavaScript",
    "start": "0",
    "end": "8340"
  },
  {
    "text": "um we're doing one that next week on SQL and how to how to query SQL with",
    "start": "8340",
    "end": "13440"
  },
  {
    "text": "language models um and then where uh and then we're going to do one the week after that with",
    "start": "13440",
    "end": "19199"
  },
  {
    "text": "uh yet to be announced guests but we'll announce that on Friday um my uh was here I think he's coming",
    "start": "19199",
    "end": "26580"
  },
  {
    "text": "back shortly hopefully because he'll be doing most of the interesting talking here um",
    "start": "26580",
    "end": "32279"
  },
  {
    "text": "and so yeah so for the the focus of today's session basically what we're going to walk through is how to create a",
    "start": "32279",
    "end": "38219"
  },
  {
    "text": "chat gbt like clone um in JavaScript um and so why why these two things",
    "start": "38219",
    "end": "45719"
  },
  {
    "text": "um the first uh the first the chat GPT like stuff I think that's been the first major kind of um application that we've",
    "start": "45719",
    "end": "53039"
  },
  {
    "text": "seen come out of these English models everyone has proprietary data um",
    "start": "53039",
    "end": "58620"
  },
  {
    "text": "and there's uh there's there's uh different ways that people want to",
    "start": "58620",
    "end": "63960"
  },
  {
    "text": "interact with it but the main one is uh is chatting over it and so chat GPT over",
    "start": "63960",
    "end": "70080"
  },
  {
    "text": "your data has been one of the main uh applications that we've seen and then why JavaScript",
    "start": "70080",
    "end": "75720"
  },
  {
    "text": "um well so we recently released a typescript and a JavaScript package",
    "start": "75720",
    "end": "81240"
  },
  {
    "text": "um and uh this has been really exciting because we've seen a whole new set of users come in",
    "start": "81240",
    "end": "87180"
  },
  {
    "text": "um and uh Maya's been doing a bunch of great uh work there highlighting a lot",
    "start": "87180",
    "end": "92220"
  },
  {
    "text": "of questions and it's I mean if we're being honest the package is a bit newer than the python one so there's there's more rough edges and has been helping",
    "start": "92220",
    "end": "99479"
  },
  {
    "text": "out a bunch um with with that so really excited to have him here today really excited to be",
    "start": "99479",
    "end": "105540"
  },
  {
    "text": "going over all of this um this will be recorded um we will do uh 20 to 30 minutes ish of",
    "start": "105540",
    "end": "112740"
  },
  {
    "text": "uh demo and then we'll just take questions from you guys for the remaining time um so if you have questions there's",
    "start": "112740",
    "end": "119159"
  },
  {
    "text": "actually a dedicated q a um Channel um and would prefer that you use that",
    "start": "119159",
    "end": "125340"
  },
  {
    "text": "rather than the chat Channel just uh just to separate um the the two",
    "start": "125340",
    "end": "131160"
  },
  {
    "text": "um but yeah really excited to uh to do this and uh yeah my are you ready do you",
    "start": "131160",
    "end": "139080"
  },
  {
    "text": "want to give a brief introduction of yourself or just jump right into it",
    "start": "139080",
    "end": "144379"
  },
  {
    "text": "yeah can you hear me yep oh",
    "start": "148319",
    "end": "153900"
  },
  {
    "text": "um yeah I mean we've spoken about this while I was probably one of the very earliest users of snack chain uh I had a",
    "start": "153900",
    "end": "161640"
  },
  {
    "text": "major issue where I was building an application where the uh the person I was building for",
    "start": "161640",
    "end": "167580"
  },
  {
    "text": "wanted to summarize YouTube videos there were you know long three hours four hours five hours and I just remember",
    "start": "167580",
    "end": "173519"
  },
  {
    "text": "spending just sleepless nights trying to figure out how do I reduce how do I chunk this",
    "start": "173519",
    "end": "179160"
  },
  {
    "text": "thing right because the contacts window issues and um it was just very expensive",
    "start": "179160",
    "end": "184800"
  },
  {
    "text": "as well at that time um to basically send all these API calls",
    "start": "184800",
    "end": "190379"
  },
  {
    "text": "and then each of them was just changed the cost and a bomb and that's when I just stumble across the repo and uh",
    "start": "190379",
    "end": "197459"
  },
  {
    "text": "haven't looked back since so here we are today and um yeah so hopefully",
    "start": "197459",
    "end": "202860"
  },
  {
    "text": "you know we can cover um you know mostly answer most of the",
    "start": "202860",
    "end": "208140"
  },
  {
    "text": "questions people have I'll try and show a demo as well and um you know you can just jump in and talk we'll talk through",
    "start": "208140",
    "end": "215700"
  },
  {
    "text": "it but I think one of the first things I'm just going to start with just an overview of the architecture in terms of",
    "start": "215700",
    "end": "222120"
  },
  {
    "text": "just showing the visuals and then you can just chime in as I I go through",
    "start": "222120",
    "end": "228060"
  },
  {
    "text": "sounds great cool so let me just get that going",
    "start": "228060",
    "end": "235400"
  },
  {
    "text": "just let me know if you see my screen I can see it",
    "start": "235680",
    "end": "243060"
  },
  {
    "text": "yeah hopefully everyone else can see it and I think this video is also recorded",
    "start": "243060",
    "end": "248400"
  },
  {
    "text": "right yep cool so",
    "start": "248400",
    "end": "253799"
  },
  {
    "text": "in a nutshell the first thing we want to kind of talk about is is you know what",
    "start": "253799",
    "end": "259199"
  },
  {
    "text": "is Lang chain and you know what problems is like chain solving so I just gave an",
    "start": "259199",
    "end": "265320"
  },
  {
    "text": "example of myself personally where I was trying to build an application and it involved dealing with text that",
    "start": "265320",
    "end": "272880"
  },
  {
    "text": "was very large right this is Texas beyond the context window which is uh about 3 000 words and if you know most",
    "start": "272880",
    "end": "281460"
  },
  {
    "text": "documents we're trying to deal with most data we're trying to deal with is much bigger than 3000 words and so",
    "start": "281460",
    "end": "288120"
  },
  {
    "text": "one of the things that Lang chain helps with are one of the problems that it tries to solve is this splitting of text",
    "start": "288120",
    "end": "295139"
  },
  {
    "text": "into different chunks that we can then send to open AI to get a response right",
    "start": "295139",
    "end": "301680"
  },
  {
    "text": "so let's say you have a 50 page PDF or you have a ton of CSV files or you could",
    "start": "301680",
    "end": "307560"
  },
  {
    "text": "just have a directory of different files what lag chain has is a different a",
    "start": "307560",
    "end": "313080"
  },
  {
    "text": "bunch of different loaders that would be assisting it might be good to zoom in a",
    "start": "313080",
    "end": "318840"
  },
  {
    "text": "little bit I think uh some people including myself are having a bit of trouble reading some of this text box a",
    "start": "318840",
    "end": "324960"
  },
  {
    "text": "lot better yeah that looks great cool so how do we",
    "start": "324960",
    "end": "332280"
  },
  {
    "text": "um uh just give me a second let me switch from my wind of my tab to my window so",
    "start": "332280",
    "end": "338699"
  },
  {
    "text": "that I can just jump over to another example real quick just give it a second",
    "start": "338699",
    "end": "345440"
  },
  {
    "text": "let's go",
    "start": "345539",
    "end": "348380"
  },
  {
    "text": "all right I will just um yeah my window",
    "start": "362759",
    "end": "369800"
  },
  {
    "text": "all right is he clear now",
    "start": "372539",
    "end": "376400"
  },
  {
    "text": "yeah cool so what line chain has affected me",
    "start": "379020",
    "end": "384419"
  },
  {
    "text": "these are the documents here for Js is a bunch of these things called",
    "start": "384419",
    "end": "389819"
  },
  {
    "text": "loaders what loaders are effectively is they take different",
    "start": "389819",
    "end": "395160"
  },
  {
    "text": "formats of text or different formats or documents you have and convert that into",
    "start": "395160",
    "end": "400319"
  },
  {
    "text": "text which you can then doing a vector store retrieve and use",
    "start": "400319",
    "end": "406080"
  },
  {
    "text": "the response you want so let me just show you a quick demo of what this looks like in real time so lag chain already",
    "start": "406080",
    "end": "411900"
  },
  {
    "text": "has a you know chat bot at the top top right",
    "start": "411900",
    "end": "417060"
  },
  {
    "text": "here so let's say you come in here and you say okay how do I",
    "start": "417060",
    "end": "422340"
  },
  {
    "text": "build a chatbot for um a 50 page PDF right",
    "start": "422340",
    "end": "430979"
  },
  {
    "text": "you ask the question and so what happens is",
    "start": "430979",
    "end": "436740"
  },
  {
    "text": "the chatbot will basically say if they cannot find what it's looking for it'll give a best",
    "start": "436740",
    "end": "442620"
  },
  {
    "text": "estimation then let's say you you follow up and say something that it knows about so you can say okay how do I uh you how",
    "start": "442620",
    "end": "453120"
  },
  {
    "text": "do I uh split the text of a large document",
    "start": "453120",
    "end": "461160"
  },
  {
    "text": "okay so if we press enter we should see at this point",
    "start": "461160",
    "end": "466500"
  },
  {
    "text": "that it then gives you a bunch of functions uh it also gives you reference",
    "start": "466500",
    "end": "472979"
  },
  {
    "text": "documents as well so you can see looks like this is what you're looking for can be found here",
    "start": "472979",
    "end": "478380"
  },
  {
    "text": "and you can see it's got verified sources as well right so not only do you get a response but you also get",
    "start": "478380",
    "end": "486780"
  },
  {
    "text": "um uh the sources as well from what you're looking for so this is just a brief demo",
    "start": "486780",
    "end": "492120"
  },
  {
    "text": "of what's possible and if you can imagine this for your documents or document or application you're trying to",
    "start": "492120",
    "end": "498539"
  },
  {
    "text": "build for someone else but what's actually going on under the hood is what",
    "start": "498539",
    "end": "503699"
  },
  {
    "text": "we want to cover today so if I jump back to diagram what Lang chain does effectively is it",
    "start": "503699",
    "end": "511139"
  },
  {
    "text": "takes your documents it loads these documents so these are the document loaders that we spoke about",
    "start": "511139",
    "end": "516839"
  },
  {
    "text": "so you've got loaders for CSV files Json PDF text files so let's say you had a",
    "start": "516839",
    "end": "523500"
  },
  {
    "text": "PDF file you would just run this function and this function will effectively split",
    "start": "523500",
    "end": "529740"
  },
  {
    "text": "the document into chunks so you would have chunks of text in each chunk you",
    "start": "529740",
    "end": "537779"
  },
  {
    "text": "can Define so what blank chain lets you do for example is you have these things called uh text",
    "start": "537779",
    "end": "545880"
  },
  {
    "text": "splitters which should be in the they're in the indices tab",
    "start": "545880",
    "end": "553459"
  },
  {
    "text": "indices tab right here we go so when you split a text you basically",
    "start": "553459",
    "end": "559320"
  },
  {
    "text": "split by character lines but what's pretty cool is you can specify you can",
    "start": "559320",
    "end": "564720"
  },
  {
    "text": "say I want to split by 2000 characters and overlaps or overlap",
    "start": "564720",
    "end": "570959"
  },
  {
    "text": "means that it basically controls how the text how much of the text overlap",
    "start": "570959",
    "end": "577320"
  },
  {
    "text": "between each chunk right so you can split say a 50 page PDF or a bunch of",
    "start": "577320",
    "end": "584580"
  },
  {
    "text": "markdown files into chunks of 2000 words and those chunks",
    "start": "584580",
    "end": "591240"
  },
  {
    "text": "are going to constitute these different chunks in here right now once we have these chunks what we need to do",
    "start": "591240",
    "end": "597540"
  },
  {
    "text": "effectively create these things for the beddings so if I zoom out a little bit",
    "start": "597540",
    "end": "603240"
  },
  {
    "text": "water embedded so embedding is basically just number representations of text",
    "start": "603240",
    "end": "608339"
  },
  {
    "text": "right so you can kind of think of them as basically floating Point numbers the",
    "start": "608339",
    "end": "614279"
  },
  {
    "text": "decimal numbers in each decimal number is a representation of a dimension right",
    "start": "614279",
    "end": "621060"
  },
  {
    "text": "so the best way to explain it very very simplified is Imagine you've got three",
    "start": "621060",
    "end": "626700"
  },
  {
    "text": "text right one is a line one is a pet and one is a dog right and we can represent these in these vectors or",
    "start": "626700",
    "end": "633540"
  },
  {
    "text": "these embeddings right now let's say we're working with the two or three-dimensional view of all of this",
    "start": "633540",
    "end": "640740"
  },
  {
    "text": "well we can see obviously that when we plot this",
    "start": "640740",
    "end": "646320"
  },
  {
    "text": "uh these vectors that the line is far off right because what this is saying is",
    "start": "646320",
    "end": "652620"
  },
  {
    "text": "okay the pet and the dog have more similarity to each other than the line",
    "start": "652620",
    "end": "658200"
  },
  {
    "text": "right so this is effectively what happens when you're doing a uh when you",
    "start": "658200",
    "end": "664140"
  },
  {
    "text": "when you when you do these things called embeddings all we're doing is turning this text into these",
    "start": "664140",
    "end": "670860"
  },
  {
    "text": "numbers which we can then compare uh from one to the other right so",
    "start": "670860",
    "end": "676680"
  },
  {
    "text": "these embeddings are then stored so first of all we we will then create",
    "start": "676680",
    "end": "682019"
  },
  {
    "text": "these embeddings from uh Lang chains open AI embeddies",
    "start": "682019",
    "end": "689040"
  },
  {
    "text": "so we have the embeddings here the open AI has an",
    "start": "689040",
    "end": "694380"
  },
  {
    "text": "embeddings function which effectively takes chunks of text",
    "start": "694380",
    "end": "700019"
  },
  {
    "text": "like this and it returns a bunch of numbers that look like that it has one",
    "start": "700019",
    "end": "706860"
  },
  {
    "text": "instead of just four dimensions here it has 1536 Dimensions right so you have those",
    "start": "706860",
    "end": "713279"
  },
  {
    "text": "thousand Thirty uh 536 dimensions for that particular",
    "start": "713279",
    "end": "719160"
  },
  {
    "text": "text that you store as a chunk right so you have the chunks",
    "start": "719160",
    "end": "725279"
  },
  {
    "text": "then",
    "start": "725279",
    "end": "727940"
  },
  {
    "text": "oh looks like there's Maybe a freeze on uh on Mayo's end",
    "start": "735839",
    "end": "743880"
  },
  {
    "text": "um while uh we're resolving that um I've seen a few questions in the chat",
    "start": "743880",
    "end": "749279"
  },
  {
    "text": "about some stuff around document loading and splitting text which I think are really good questions and there's a lot",
    "start": "749279",
    "end": "754920"
  },
  {
    "text": "of nuance here so I'll try to speak a little bit about that um first uh the the document loading so",
    "start": "754920",
    "end": "761820"
  },
  {
    "text": "these are all um so these are all contributed by the community so okay",
    "start": "761820",
    "end": "767760"
  },
  {
    "text": "there's there's a lot of documents that you could possibly want to load things from or a lot of sources that you'd want to load documents from so you could",
    "start": "767760",
    "end": "775320"
  },
  {
    "text": "um you know you could load things from the internet you could load things from local files you could pull things from PDFs",
    "start": "775320",
    "end": "781500"
  },
  {
    "text": "um and so a lot of the the loaders in um in in LinkedIn are commit or contributed",
    "start": "781500",
    "end": "788040"
  },
  {
    "text": "from the community because oftentimes you know there's such long tale of places where you could possibly want to",
    "start": "788040",
    "end": "794760"
  },
  {
    "text": "load text from um that that it's uh it's it's it's tough for us to add them all so we",
    "start": "794760",
    "end": "801540"
  },
  {
    "text": "really rely on a lot of people to add and share their own document loaders with other folks",
    "start": "801540",
    "end": "807540"
  },
  {
    "text": "um there's also a bunch of really cool companies that are doing um this as a more focused effort like",
    "start": "807540",
    "end": "812639"
  },
  {
    "text": "obvious like this is this is just one part of Lang chain um and so we want to integrate and play really nicely with",
    "start": "812639",
    "end": "817800"
  },
  {
    "text": "those companies as well so one in particular is unstructured um and so unstructured is mostly",
    "start": "817800",
    "end": "823440"
  },
  {
    "text": "python-based but they're releasing um uh a kind of like hosted version of",
    "start": "823440",
    "end": "829560"
  },
  {
    "text": "their platform which will let um or or they're releasing uh like a non",
    "start": "829560",
    "end": "834959"
  },
  {
    "text": "kind of like um in in-memory version of their platform which will let people uh use it from JavaScript as well and so",
    "start": "834959",
    "end": "841800"
  },
  {
    "text": "we're really excited about that integration that should unlock a lot of different document parsing type so they have support for",
    "start": "841800",
    "end": "847639"
  },
  {
    "text": "HTML word docs PowerPoints everything like that",
    "start": "847639",
    "end": "853800"
  },
  {
    "text": "um I also saw a lot of questions around text splitting um and and so I think",
    "start": "853800",
    "end": "859200"
  },
  {
    "text": "um yeah this this is another kind of like nuanced Point um and so first of all like why why do",
    "start": "859200",
    "end": "865920"
  },
  {
    "text": "we even need to split um uh text in in the first place",
    "start": "865920",
    "end": "871019"
  },
  {
    "text": "um we need to split text um that because we need the the overall documents by themselves are too big to",
    "start": "871019",
    "end": "877440"
  },
  {
    "text": "feed um into the the the model by itself",
    "start": "877440",
    "end": "883440"
  },
  {
    "text": "um I'm going to take a second to put myself back on the screen um",
    "start": "883440",
    "end": "889819"
  },
  {
    "text": "all right um okay so we need to split these documents um because there's uh",
    "start": "890339",
    "end": "897420"
  },
  {
    "text": "there's this um that we need to split them into chunks that we can pass to the language model",
    "start": "897420",
    "end": "902880"
  },
  {
    "text": "so how how do we split them into chunks the the naive thing to do is basically",
    "start": "902880",
    "end": "908880"
  },
  {
    "text": "just go like you know every thousand characters or thousand tokens we put in this chunk the next thousand tokens we",
    "start": "908880",
    "end": "914699"
  },
  {
    "text": "put in another chance the next thousand jump into another chunk what are what are the downsides of that so basically",
    "start": "914699",
    "end": "920600"
  },
  {
    "text": "that means that when you're passing these chunks to the language model it may not have kind of like the full",
    "start": "920600",
    "end": "926279"
  },
  {
    "text": "amount of information that it needs or it may not have the semantically kind of like meaningful Parts together",
    "start": "926279",
    "end": "931920"
  },
  {
    "text": "um so I saw a question about splitting a PDF into chunks based on page",
    "start": "931920",
    "end": "938480"
  },
  {
    "text": "I don't know if you I just kept talking I didn't realize the thing went off yeah we lost we lost you for a bit but maybe",
    "start": "939420",
    "end": "946680"
  },
  {
    "text": "you can just carry on um I was actually just going through the diagrams to explain how things work under the hood",
    "start": "946680",
    "end": "952019"
  },
  {
    "text": "but you can just carry on if you if you want to yeah I'm just I'm just finishing up some stuff on splitting on the text",
    "start": "952019",
    "end": "958860"
  },
  {
    "text": "splitting stuff um okay yeah but then but then just just yeah but but then after that after you",
    "start": "958860",
    "end": "965160"
  },
  {
    "text": "go back so basically the um yeah the the idea of spleen taxes you want to create",
    "start": "965160",
    "end": "970560"
  },
  {
    "text": "like meaningful chunks and so splitting it by Page or splitting it by paragraph is a pretty good way to do that because",
    "start": "970560",
    "end": "976800"
  },
  {
    "text": "that's generally what like we as humans kind of find semantically meaningful um there's also obviously some",
    "start": "976800",
    "end": "982199"
  },
  {
    "text": "information that's translated between paragraphs um and between pages and so you may want",
    "start": "982199",
    "end": "987240"
  },
  {
    "text": "to do something with like the chunk overlap there um and so yeah I think there's a lot of",
    "start": "987240",
    "end": "993480"
  },
  {
    "text": "nuance in the defining these chunks so by General so by default we use something that kind of like first tries",
    "start": "993480",
    "end": "999480"
  },
  {
    "text": "to split by paragraphs and then split by sentences and then split by words and then if it needs to split by characters",
    "start": "999480",
    "end": "1005779"
  },
  {
    "text": "um but we also have more fine uh fine-grained ones um so for markdown we have a markdown",
    "start": "1005779",
    "end": "1011180"
  },
  {
    "text": "specific text splitter that splits based on like the different headers um and things like that and then for",
    "start": "1011180",
    "end": "1016820"
  },
  {
    "text": "python we have a python specific um text splitter if you're if you're like reading code files",
    "start": "1016820",
    "end": "1023360"
  },
  {
    "text": "um and and that splits by um basically classes and then methods and and then Lines within that",
    "start": "1023360",
    "end": "1031280"
  },
  {
    "text": "um so yeah there's we don't have as many um variations of text Splitters as",
    "start": "1031280",
    "end": "1037280"
  },
  {
    "text": "opposed to like document loaders for example but I think that's uh I think that's an under kind of like invested in",
    "start": "1037280",
    "end": "1042740"
  },
  {
    "text": "area so if people have ideas for like different types of text Splitters we'd love to would love to get those contributed and get those shared with",
    "start": "1042740",
    "end": "1049520"
  },
  {
    "text": "the community I think there's a bunch of interesting stuff to do there um that's pretty much all I wanted to",
    "start": "1049520",
    "end": "1055940"
  },
  {
    "text": "say so uh Mario if you want to take back over",
    "start": "1055940",
    "end": "1060340"
  },
  {
    "text": "cool yeah um maybe just uh you know make some ad-libs as I'm",
    "start": "1061700",
    "end": "1067520"
  },
  {
    "text": "talking just so I make sure that you're here and everyone can still hear me so what was the last thing you heard me say",
    "start": "1067520",
    "end": "1073600"
  },
  {
    "text": "before I dropped off um I think you were talking about putting them in a vector store",
    "start": "1073600",
    "end": "1080919"
  },
  {
    "text": "okay cool so let me just go back to share my",
    "start": "1081320",
    "end": "1086900"
  },
  {
    "text": "screen",
    "start": "1086900",
    "end": "1089080"
  },
  {
    "text": "okay you see the screen yep",
    "start": "1098000",
    "end": "1103820"
  },
  {
    "text": "okay uh so this is where the last part you heard me say about just throwing in",
    "start": "1103820",
    "end": "1109220"
  },
  {
    "text": "the vector store did I go did you see this section about the embeddings yeah we we yeah we did the embeddings",
    "start": "1109220",
    "end": "1115520"
  },
  {
    "text": "part and then you jumped back up to talk about putting the embeddings in the vector store yeah so yeah I was just saying that once",
    "start": "1115520",
    "end": "1122480"
  },
  {
    "text": "it's in the vector store these things called the vector stores just a place where you store the uh I'm basically",
    "start": "1122480",
    "end": "1130160"
  },
  {
    "text": "these number representations of your text right so you have different choices this pros and cons of each one which we",
    "start": "1130160",
    "end": "1136160"
  },
  {
    "text": "can talk about later on once you've chosen what Vector store you want to work with",
    "start": "1136160",
    "end": "1141679"
  },
  {
    "text": "you're pretty much good to go so this phase is known as ingestion right and it's just a phase of",
    "start": "1141679",
    "end": "1147860"
  },
  {
    "text": "effectively transforming your documents into numbers that you the computers cannot understand",
    "start": "1147860",
    "end": "1154640"
  },
  {
    "text": "so what happens when a search is made right so this is a very uh brief uh just",
    "start": "1154640",
    "end": "1161000"
  },
  {
    "text": "quick overview so let's say with this example here we have the lion the pet and the dog right now before I",
    "start": "1161000",
    "end": "1168679"
  },
  {
    "text": "go into this intuitively if someone is going to use the word cat right uh it",
    "start": "1168679",
    "end": "1174620"
  },
  {
    "text": "should have similarity with pet and dog right without even seeing the numbers so what",
    "start": "1174620",
    "end": "1181700"
  },
  {
    "text": "happens is we take the query so this is cat we transform this into an embedding which",
    "start": "1181700",
    "end": "1188780"
  },
  {
    "text": "you understand now in this case it would be Open the Eyes embeddings and then we create these vectors",
    "start": "1188780",
    "end": "1195260"
  },
  {
    "text": "then we now we go to your vector store wherever you store it for and we basically say hey",
    "start": "1195260",
    "end": "1201679"
  },
  {
    "text": "we have this new thing called cats and these are the numbers that represent cat can you please respond back with",
    "start": "1201679",
    "end": "1209960"
  },
  {
    "text": "anything you have that's similar to it it goes in it checks it runs some",
    "start": "1209960",
    "end": "1215780"
  },
  {
    "text": "calculations I'm not going to get into that but effectively returns back",
    "start": "1215780",
    "end": "1221240"
  },
  {
    "text": "X that is similar uh Chase can you still hear me yep",
    "start": "1221240",
    "end": "1226760"
  },
  {
    "text": "cool so now we're going back to if this all",
    "start": "1226760",
    "end": "1233480"
  },
  {
    "text": "makes sense then let's go back to a typical typical case right so",
    "start": "1233480",
    "end": "1240020"
  },
  {
    "text": "what Lang chain does effectively is this entire process you can imagine I'll just scroll down",
    "start": "1240020",
    "end": "1246860"
  },
  {
    "text": "here a little bit which will come and revisit this this entire process",
    "start": "1246860",
    "end": "1252020"
  },
  {
    "text": "which is also up here blank chain basically makes it much easier much faster for you to do to do it for any",
    "start": "1252020",
    "end": "1260360"
  },
  {
    "text": "documents you have or the vast majority of file formats it makes it much easier to do so it solves this problem of the",
    "start": "1260360",
    "end": "1267559"
  },
  {
    "text": "context window if you've ever tried to copy paste into you know uh opening",
    "start": "1267559",
    "end": "1273200"
  },
  {
    "text": "hours chat GPT and you'll you know you've got a large document and then you just try and copy paste the whole thing",
    "start": "1273200",
    "end": "1278960"
  },
  {
    "text": "well obviously it's going to say it's too big right uh also if you try and ask chat gbt questions about your document",
    "start": "1278960",
    "end": "1285799"
  },
  {
    "text": "say you say okay tell me you know um",
    "start": "1285799",
    "end": "1291020"
  },
  {
    "text": "what's what what is this employee handbook have to say about you know the policy for XYZ company well the GPT",
    "start": "1291020",
    "end": "1299000"
  },
  {
    "text": "doesn't know anything about your company uh because it's trained on a data set",
    "start": "1299000",
    "end": "1304100"
  },
  {
    "text": "the first of all it's it's pre-2022 and also",
    "start": "1304100",
    "end": "1311059"
  },
  {
    "text": "um it doesn't have that data so it doesn't have knowledge of your data it has character limits fine-tuning is",
    "start": "1311059",
    "end": "1317059"
  },
  {
    "text": "expensive there you have the issue of memory how do you keep track of previous conversations as well so land chain",
    "start": "1317059",
    "end": "1324140"
  },
  {
    "text": "helps to resolve all these issues uh through the different tools you can see here so",
    "start": "1324140",
    "end": "1330559"
  },
  {
    "text": "we went over the the document loaders the indexes which",
    "start": "1330559",
    "end": "1337340"
  },
  {
    "text": "is the stores you've got a chain which is just effectively um",
    "start": "1337340",
    "end": "1343640"
  },
  {
    "text": "what I showed in dot which I'm going to go into in a second um you've got agents which um Chase can",
    "start": "1343640",
    "end": "1350419"
  },
  {
    "text": "also talk about as well but effectively it's you can think if it's your personal assistant that goes out in the world and",
    "start": "1350419",
    "end": "1356240"
  },
  {
    "text": "interacts with other tools maybe other apis and then you have a chat which is",
    "start": "1356240",
    "end": "1362480"
  },
  {
    "text": "basically you know using the new GPT chat models uh for your application as well and all",
    "start": "1362480",
    "end": "1370940"
  },
  {
    "text": "of this which you would have had to figure out which I had to figure out on my own the painful way you know um last",
    "start": "1370940",
    "end": "1377840"
  },
  {
    "text": "year it it's just so much easier with Lang change so if I jump in to",
    "start": "1377840",
    "end": "1383360"
  },
  {
    "text": "this drive this diagram this is effectively what",
    "start": "1383360",
    "end": "1388700"
  },
  {
    "text": "the vast majority of chat Bots are going to end up using this is the architecture of a chatbot",
    "start": "1388700",
    "end": "1394880"
  },
  {
    "text": "so you basically have a question so how do I purchase a Premium plan this is a",
    "start": "1394880",
    "end": "1399980"
  },
  {
    "text": "question to your document right and this question is effectively",
    "start": "1399980",
    "end": "1405559"
  },
  {
    "text": "combined with the history so if you've ever used chat gbt you notice that you",
    "start": "1405559",
    "end": "1410960"
  },
  {
    "text": "can ask questions get a response and then ask another question based on the",
    "start": "1410960",
    "end": "1416360"
  },
  {
    "text": "previous question and then you have a response from that context well how is it able to do that is because it takes",
    "start": "1416360",
    "end": "1423260"
  },
  {
    "text": "the chat history and you're able well lime chain does is it can take the child",
    "start": "1423260",
    "end": "1428840"
  },
  {
    "text": "history combine it requestion create a standalone question so",
    "start": "1428840",
    "end": "1434120"
  },
  {
    "text": "basically the problems would be something along the lines of given this question and the chat history create a",
    "start": "1434120",
    "end": "1440240"
  },
  {
    "text": "standalone question based on this topic or context then you take that standard question we",
    "start": "1440240",
    "end": "1447020"
  },
  {
    "text": "go and run through this process here right where we embed that question",
    "start": "1447020",
    "end": "1452059"
  },
  {
    "text": "remember like we took the cat we embedded the cats we got the vectors went to the store retrieved similar",
    "start": "1452059",
    "end": "1458659"
  },
  {
    "text": "similar things and then got a similar result so that's literally What's Happening Here",
    "start": "1458659",
    "end": "1464720"
  },
  {
    "text": "the users made a question about your document is to create a standard question",
    "start": "1464720",
    "end": "1471380"
  },
  {
    "text": "check for Relevant docs they're embedded in your store wherever you want to store",
    "start": "1471380",
    "end": "1476480"
  },
  {
    "text": "it you can even store it locally if you want to you don't have to use um hosted solution",
    "start": "1476480",
    "end": "1483760"
  },
  {
    "text": "you get the record box we combine the Standalone question with the relevant Docs",
    "start": "1485840",
    "end": "1492260"
  },
  {
    "text": "you get the GPT to basic General response based on",
    "start": "1492260",
    "end": "1497600"
  },
  {
    "text": "the standard question and run for docs because you're providing these relevant documents as context that context is",
    "start": "1497600",
    "end": "1504919"
  },
  {
    "text": "going to result in an answer so the answer is going to be something like to upgrade to a Premium plan please go to",
    "start": "1504919",
    "end": "1511100"
  },
  {
    "text": "this URL so we saw that example we saw something similar when we came here we said you know how do I split a document",
    "start": "1511100",
    "end": "1520340"
  },
  {
    "text": "into uh chunks for but I split a large document",
    "start": "1520340",
    "end": "1527539"
  },
  {
    "text": "to chance I can embed right and so",
    "start": "1527539",
    "end": "1534080"
  },
  {
    "text": "what this is basically what's going on beneath the hood you made this request right this request",
    "start": "1534080",
    "end": "1541580"
  },
  {
    "text": "effectively went through this process and if we jump back in",
    "start": "1541580",
    "end": "1547940"
  },
  {
    "text": "and say okay maybe I don't understand something so I say okay what do you mean what do you mean",
    "start": "1547940",
    "end": "1557240"
  },
  {
    "text": "by chunk slice press enter we're going to see",
    "start": "1557240",
    "end": "1563179"
  },
  {
    "text": "that it's going to remember the history of what we said",
    "start": "1563179",
    "end": "1568340"
  },
  {
    "text": "and I can go back and say Okay um",
    "start": "1568340",
    "end": "1573440"
  },
  {
    "text": "can you explain what",
    "start": "1573440",
    "end": "1578740"
  },
  {
    "text": "a text splitter is is based on the first response",
    "start": "1579080",
    "end": "1587960"
  },
  {
    "text": "so we press enter again now we're going to see a response back",
    "start": "1587960",
    "end": "1593059"
  },
  {
    "text": "the reason it's able to do this is because like I said earlier it it's going back in history",
    "start": "1593059",
    "end": "1601100"
  },
  {
    "text": "and each title question is made it's picking up from the previous history so you just get a stock of chat history",
    "start": "1601100",
    "end": "1607659"
  },
  {
    "text": "with the new question create the sound known question then you",
    "start": "1607659",
    "end": "1612860"
  },
  {
    "text": "get this uh response that comes back um chase you can still hear me",
    "start": "1612860",
    "end": "1618799"
  },
  {
    "text": "yep you're good and and so yeah so I might jump in here with a few uh comments if you want to zoom back out to",
    "start": "1618799",
    "end": "1625100"
  },
  {
    "text": "the luxury architecture and then um I've seen a bunch of questions as well so yeah so maybe we can transition",
    "start": "1625100",
    "end": "1631940"
  },
  {
    "text": "to some of those after um but I think the first thing that I wanted to highlight is this um is the",
    "start": "1631940",
    "end": "1637520"
  },
  {
    "text": "kind of like creation of the Standalone question and and why we do that",
    "start": "1637520",
    "end": "1643039"
  },
  {
    "text": "um so so that's kind of like done or uh that's done for two purposes the main",
    "start": "1643039",
    "end": "1648799"
  },
  {
    "text": "purpose is looking up relevant documents in the vector store basically the the issue with um could you stay on the",
    "start": "1648799",
    "end": "1655700"
  },
  {
    "text": "diagram by any chance you want me to zoom in now actually",
    "start": "1655700",
    "end": "1661340"
  },
  {
    "text": "zooming out to maybe even better just to get a whole sense of it okay but luckily the yes so um so you've got the chat",
    "start": "1661340",
    "end": "1668419"
  },
  {
    "text": "history so we want to look up like relevant documents and so we need some query to or we need some embeddings to",
    "start": "1668419",
    "end": "1673700"
  },
  {
    "text": "look up against the the document store and so one option would be to embed the",
    "start": "1673700",
    "end": "1679760"
  },
  {
    "text": "most recent question but if that references previous things then you're getting the embedding you're getting the numerical representation for something",
    "start": "1679760",
    "end": "1686120"
  },
  {
    "text": "that doesn't have the full context of what it needs to look up um likewise if you embed all of the chat",
    "start": "1686120",
    "end": "1692900"
  },
  {
    "text": "history and the new question if the new question is completely Standalone and doesn't use anything in the chat history",
    "start": "1692900",
    "end": "1698360"
  },
  {
    "text": "then you're getting some kind of like no longer relevant Parts um that are that are now in the",
    "start": "1698360",
    "end": "1704059"
  },
  {
    "text": "embeddings um this isn't the only way to do this by the way there are there are definitely other techniques um that you could use",
    "start": "1704059",
    "end": "1711440"
  },
  {
    "text": "um so rather than rephrase the question you could uh you know you could look up",
    "start": "1711440",
    "end": "1716480"
  },
  {
    "text": "um documents based on the question and then documents based on the chat history and combine those that was an approach",
    "start": "1716480",
    "end": "1722120"
  },
  {
    "text": "that I tried as well um you could also look up you could also do some sort of like",
    "start": "1722120",
    "end": "1728720"
  },
  {
    "text": "um uh logic to determine which of the previous responses are relevant maybe you could",
    "start": "1728720",
    "end": "1735020"
  },
  {
    "text": "even use embeddings for those um so you know take take the new question create an embedding for it",
    "start": "1735020",
    "end": "1741380"
  },
  {
    "text": "um look at previous things to see if they're relevant and then pass all those as the final embedding that I haven't",
    "start": "1741380",
    "end": "1747260"
  },
  {
    "text": "tried that although a would worry a bit again there about the like pronoun and the referencing stuff",
    "start": "1747260",
    "end": "1753380"
  },
  {
    "text": "um so yeah I think the I just want to highlight that that is just one",
    "start": "1753380",
    "end": "1758840"
  },
  {
    "text": "particular way of um of doing that part um and I think uh there are a lot of",
    "start": "1758840",
    "end": "1764659"
  },
  {
    "text": "other ways hopefully link chain provides a bunch of the modular components that can be used to do those other ways but",
    "start": "1764659",
    "end": "1769880"
  },
  {
    "text": "we also provide this kind of like end-to-end way of constructing and inquiring the vector store for related",
    "start": "1769880",
    "end": "1775700"
  },
  {
    "text": "things um and then another question that I've gotten a lot is basically around like",
    "start": "1775700",
    "end": "1781700"
  },
  {
    "text": "the idea of like sources in the answer um and so there's like two things that",
    "start": "1781700",
    "end": "1787460"
  },
  {
    "text": "you want to keep in mind when I'm trying to get the language model to uh",
    "start": "1787460",
    "end": "1792799"
  },
  {
    "text": "reference sources one is it has to have access to the sources itself so if you don't tell the language model what the",
    "start": "1792799",
    "end": "1798740"
  },
  {
    "text": "sources are there's no way that it's going to know um what what the sources are and so this means that when you're passing the",
    "start": "1798740",
    "end": "1805100"
  },
  {
    "text": "documents in to the prompt you need to one have sources associated with those",
    "start": "1805100",
    "end": "1810740"
  },
  {
    "text": "documents if you have and those sources need to be what you want to be cited as sources so if you're trying to sort of",
    "start": "1810740",
    "end": "1816620"
  },
  {
    "text": "cite a specific uh web page um you you want to have that as the source if you're instead doing the",
    "start": "1816620",
    "end": "1822559"
  },
  {
    "text": "question answering over a YouTube video and you want to sort of cite the like minute Mark or something like that you",
    "start": "1822559",
    "end": "1828500"
  },
  {
    "text": "need to have that somewhere in like the metadata of the document um and then once it's in the document",
    "start": "1828500",
    "end": "1833600"
  },
  {
    "text": "that needs to be like put into the prompt and so a lot of a lot of the prompts that we have do this by default",
    "start": "1833600",
    "end": "1839419"
  },
  {
    "text": "they'll look for a source and they'll put it in there um but you can also change these prompts and by Source I",
    "start": "1839419",
    "end": "1844760"
  },
  {
    "text": "generally mean like a specific key in the metadata called Source but you can edit those prompts to pull in like more",
    "start": "1844760",
    "end": "1851120"
  },
  {
    "text": "things so besides just a single Source key you could also pull in other things so for example",
    "start": "1851120",
    "end": "1856820"
  },
  {
    "text": "um if you're uh and I'm going to zoom back in to uh",
    "start": "1856820",
    "end": "1864320"
  },
  {
    "text": "yeah uh oh I'm going to zoom back in off",
    "start": "1864320",
    "end": "1869419"
  },
  {
    "text": "the off the screen because I don't think I'm gonna share anymore so so um yeah if you if you're doing like question answering over a book and you",
    "start": "1869419",
    "end": "1876080"
  },
  {
    "text": "want to cite like the chapter and the page number and everything like that",
    "start": "1876080",
    "end": "1881720"
  },
  {
    "text": "um then you want to have those different components somewhere in the metadata and be pulling those into the prompt",
    "start": "1881720",
    "end": "1887720"
  },
  {
    "text": "um so I think those are the two things that I and and then also in in the prompt you want to tell uh you want to",
    "start": "1887720",
    "end": "1895039"
  },
  {
    "text": "tell the language model that it should cite its sources um so if you don't tell it it probably",
    "start": "1895039",
    "end": "1900380"
  },
  {
    "text": "won't do it or it will do it erratically or something like that um so it's pretty important to tell the",
    "start": "1900380",
    "end": "1905419"
  },
  {
    "text": "language model hey make sure to set your sources and stuff like um Maya is there anything you wanted to",
    "start": "1905419",
    "end": "1911600"
  },
  {
    "text": "add before we transitioned to questions uh well I mean we just look at time but",
    "start": "1911600",
    "end": "1918559"
  },
  {
    "text": "uh I guess it depends if they want it going through the court or they just want to go straight and so you just walk",
    "start": "1918559",
    "end": "1924559"
  },
  {
    "text": "you in it um so I guess let's just scroll through the",
    "start": "1924559",
    "end": "1929600"
  },
  {
    "text": "questions I think this is just seems to be more interesting um",
    "start": "1929600",
    "end": "1934940"
  },
  {
    "text": "getting answers from you so uh we can go either way maybe I could just do a brief code overview and then",
    "start": "1934940",
    "end": "1941240"
  },
  {
    "text": "you can just go back to q a sure let's do a code overview for like",
    "start": "1941240",
    "end": "1947179"
  },
  {
    "text": "uh yeah like three or four minutes or something like that and we'll we'll include a link as well because I imagine it might be hard to look at it live and",
    "start": "1947179",
    "end": "1954679"
  },
  {
    "text": "then while I was doing that if people could go to the questions um panel and just upload ones that they'd like to get answered",
    "start": "1954679",
    "end": "1961340"
  },
  {
    "text": "um we'll probably go through them in in that order um and so yeah and so there's the Q a",
    "start": "1961340",
    "end": "1966980"
  },
  {
    "text": "tab which is below the chat box um on the right of your screen and so please yeah upload questions that you want to",
    "start": "1966980",
    "end": "1973100"
  },
  {
    "text": "get answered and we'll prioritize those oh all right let me share my screen",
    "start": "1973100",
    "end": "1979460"
  },
  {
    "text": "again so we can try to do that",
    "start": "1979460",
    "end": "1985720"
  },
  {
    "text": "okay sure",
    "start": "1990080",
    "end": "1995080"
  },
  {
    "text": "okay she is great",
    "start": "2007659",
    "end": "2011220"
  },
  {
    "text": "yes and let me Spotlight that",
    "start": "2013179",
    "end": "2017460"
  },
  {
    "text": "is that is that fine yep cool so yeah this is just more",
    "start": "2019600",
    "end": "2027039"
  },
  {
    "text": "um code representation or examples of very basic um opponents if you want to say our blank",
    "start": "2027039",
    "end": "2033940"
  },
  {
    "text": "chain which I covered in the diagram which effectively thought you chains your agents your document loaders",
    "start": "2033940",
    "end": "2042100"
  },
  {
    "text": "your memory problems uh indexes and then uh you also have the",
    "start": "2042100",
    "end": "2050618"
  },
  {
    "text": "the splitters and the other things that we we discussed earlier so",
    "start": "2050619",
    "end": "2057520"
  },
  {
    "text": "basically What's Happening Here is um I'm just going to run through different examples so just to show you how how",
    "start": "2057520",
    "end": "2064060"
  },
  {
    "text": "much easier this is what Lang chain has is basically the models that you typically",
    "start": "2064060",
    "end": "2070240"
  },
  {
    "text": "have to install so let's say you wanted to use open AI you would just simply",
    "start": "2070240",
    "end": "2075700"
  },
  {
    "text": "get open AI from line chain llms which is basically a uh you can you can go in",
    "start": "2075700",
    "end": "2082898"
  },
  {
    "text": "here you can choose from go away you can choose open AI so it does give you",
    "start": "2082899",
    "end": "2088839"
  },
  {
    "text": "different choices so you're not you can swap in and out you know and that's a really good thing too is if you do",
    "start": "2088839",
    "end": "2095378"
  },
  {
    "text": "change your mind about any external Services you're using you can just swap in and out so in this case we just have",
    "start": "2095379",
    "end": "2102339"
  },
  {
    "text": "a template which is just what capital was the city of the country then a new prompt template so a prompt",
    "start": "2102339",
    "end": "2108700"
  },
  {
    "text": "template effectively is a way that helps you to structure your prompts now this is a a bit of",
    "start": "2108700",
    "end": "2115119"
  },
  {
    "text": "Overkill here because I don't need is the prompt is so basic where you pass",
    "start": "2115119",
    "end": "2121359"
  },
  {
    "text": "the template and then you say you want this thing here to be the input variable",
    "start": "2121359",
    "end": "2126579"
  },
  {
    "text": "this will typically come from a user so in this case um the country which will be passed here",
    "start": "2126579",
    "end": "2133540"
  },
  {
    "text": "but imagine a much more complicated situation where you're dealing with large documents and you want to have",
    "start": "2133540",
    "end": "2139900"
  },
  {
    "text": "specific problems with different users or you know you just want to have a lot more complexity of variation and problem",
    "start": "2139900",
    "end": "2146800"
  },
  {
    "text": "templates are useful because what they do is effectively allow you to almost create these inputs these variables for",
    "start": "2146800",
    "end": "2154420"
  },
  {
    "text": "text right and so if I was to run",
    "start": "2154420",
    "end": "2160420"
  },
  {
    "text": "what the France looks like",
    "start": "2160420",
    "end": "2164760"
  },
  {
    "text": "so let's see if",
    "start": "2166900",
    "end": "2171359"
  },
  {
    "text": "so let him run and we are in",
    "start": "2180839",
    "end": "2186520"
  },
  {
    "text": "games yes",
    "start": "2186520",
    "end": "2190119"
  },
  {
    "text": "I think I'm the wrong folder run",
    "start": "2196900",
    "end": "2203099"
  },
  {
    "text": "technology",
    "start": "2206200",
    "end": "2209200"
  },
  {
    "text": "yeah Chase I think I'm targeting the wrong directory um",
    "start": "2219220",
    "end": "2225119"
  },
  {
    "text": "okay um I think um I think what may be interesting is if you want to show the um if you want to",
    "start": "2227500",
    "end": "2234460"
  },
  {
    "text": "show the prompts that we use for the question answering actually they're they're",
    "start": "2234460",
    "end": "2239619"
  },
  {
    "text": "um they're they're a little bit simple but I think we'll be instructive to see how they're put in so if you want to go to",
    "start": "2239619",
    "end": "2246700"
  },
  {
    "text": "like or uh oh this would be different Repository right okay I'll jump into the repo",
    "start": "2246700",
    "end": "2255339"
  },
  {
    "text": "right now just give me a second",
    "start": "2255339",
    "end": "2259380"
  },
  {
    "text": "um I'm also happy to share my screen and jump through them as well if we're just",
    "start": "2262660",
    "end": "2268300"
  },
  {
    "text": "looking at like the prompts yeah you can go ahead that's fine",
    "start": "2268300",
    "end": "2273579"
  },
  {
    "text": "yeah all right so um",
    "start": "2273579",
    "end": "2278880"
  },
  {
    "text": "so can you guys see this",
    "start": "2284079",
    "end": "2287280"
  },
  {
    "text": "I'm going to assume that's a yes",
    "start": "2291040",
    "end": "2294780"
  },
  {
    "text": "all right um cool so yeah so this is the the",
    "start": "2298540",
    "end": "2303880"
  },
  {
    "text": "entire prompt that we use for uh for for the questions and so you can see that",
    "start": "2303880",
    "end": "2309640"
  },
  {
    "text": "like here is where we tell the the language model to use the following pieces of context",
    "start": "2309640",
    "end": "2316000"
  },
  {
    "text": "um and so this this is where we put all the pieces of context into one prompt so we put it all into there",
    "start": "2316000",
    "end": "2322060"
  },
  {
    "text": "um and we tell the language models to use it um this is where we tell it I saw a question about how do you uh how do you",
    "start": "2322060",
    "end": "2328060"
  },
  {
    "text": "stop it from hallucinating um there's a yeah there's a",
    "start": "2328060",
    "end": "2334060"
  },
  {
    "text": "that there's this work is generally pretty well where you basically just tell it to only restrict it to this to",
    "start": "2334060",
    "end": "2341020"
  },
  {
    "text": "this uh information and if you don't know just say that you don't know don't try to make up an answer",
    "start": "2341020",
    "end": "2346060"
  },
  {
    "text": "um and so yeah it sounds a bit silly but when uh whenever there's questions about you know how do you get a language model",
    "start": "2346060",
    "end": "2351640"
  },
  {
    "text": "to do certain things um it's generally you kind of ask it you tell it to",
    "start": "2351640",
    "end": "2358119"
  },
  {
    "text": "um this is the context and so this is where the docs will come in and so this is as a put as a variable in between uh",
    "start": "2358119",
    "end": "2364720"
  },
  {
    "text": "curly brackets because basically this is going to be different for every question and this is where like the idea of like retrieving relevant passages and putting",
    "start": "2364720",
    "end": "2371020"
  },
  {
    "text": "them in comes into play so this is something that's not hard-coded this will this will this will vary on runtime",
    "start": "2371020",
    "end": "2376060"
  },
  {
    "text": "same with the question um so that comes in during run time and then helpful answer and so this is just prompting the",
    "start": "2376060",
    "end": "2382300"
  },
  {
    "text": "language model to return a helpful answer there's some really interesting work here um that maybe it's probably less",
    "start": "2382300",
    "end": "2388780"
  },
  {
    "text": "relevant with some of the chat models but with a lot of the old language models they basically",
    "start": "2388780",
    "end": "2394480"
  },
  {
    "text": "um or even with some of the existing chat models if you can put words into the language model's mouth and make it think that it's already started talking",
    "start": "2394480",
    "end": "2401320"
  },
  {
    "text": "a certain way it will kind of like continue talking that way so by putting helpful answer rather than",
    "start": "2401320",
    "end": "2406660"
  },
  {
    "text": "just like answer we're kind of like we're kind of having the language model pattern match that it's giving like a",
    "start": "2406660",
    "end": "2412960"
  },
  {
    "text": "like an answer that's actually helpful and this is actually um produces kind of like better results than um just having",
    "start": "2412960",
    "end": "2419200"
  },
  {
    "text": "like answer um this is maybe a little bit uh less relevant again with some of the chat",
    "start": "2419200",
    "end": "2424540"
  },
  {
    "text": "models but um this this prompt was uh uh optimized for",
    "start": "2424540",
    "end": "2429880"
  },
  {
    "text": "um just generic um uh uh models we can see below",
    "start": "2429880",
    "end": "2435220"
  },
  {
    "text": "um and this is so this is a prompt that's optimized for chat models and so I'm happy to chat more about the the",
    "start": "2435220",
    "end": "2440619"
  },
  {
    "text": "specifics of differences between like chat models versus regular models but here where we have the exact same prompt",
    "start": "2440619",
    "end": "2448180"
  },
  {
    "text": "um and and so this is used as a system template and this is just saying uh this is uh like the instructions for the",
    "start": "2448180",
    "end": "2454720"
  },
  {
    "text": "language model and then we can see that we have a human message prompt template and this is just with the question",
    "start": "2454720",
    "end": "2460660"
  },
  {
    "text": "um and then we no longer have the helpful answer and that's because with the um existing uh chat GPT API you can",
    "start": "2460660",
    "end": "2468400"
  },
  {
    "text": "actually ask the link you all new messages are new messages so you can't actually have the assistant start",
    "start": "2468400",
    "end": "2474160"
  },
  {
    "text": "talking um or you can't put words in the assistant Mouse so to speak and so I just wanted to show these as kind of",
    "start": "2474160",
    "end": "2480760"
  },
  {
    "text": "like pieces of of code for for what the prompt templates look like and then if",
    "start": "2480760",
    "end": "2485800"
  },
  {
    "text": "we look at some examples um so I think there should be some examples and and this is so this is all",
    "start": "2485800",
    "end": "2491920"
  },
  {
    "text": "in the link chain JS repo um and and so I would I would come over here and look for some if you want",
    "start": "2491920",
    "end": "2498820"
  },
  {
    "text": "um if we look in Chains at something like question answering",
    "start": "2498820",
    "end": "2504400"
  },
  {
    "text": "um and actually this is just over regular documents so let's look at like vector database question answering so",
    "start": "2504400",
    "end": "2509859"
  },
  {
    "text": "this uses the vector database we can see here that it's like 20 25 lines of code",
    "start": "2509859",
    "end": "2514960"
  },
  {
    "text": "to set this up and run this um and so specifically first we're initializing the language model that we",
    "start": "2514960",
    "end": "2521020"
  },
  {
    "text": "want to use then we're using um that then we're reading in um a document and so we're not actually",
    "start": "2521020",
    "end": "2527440"
  },
  {
    "text": "using the document loader here um just just for uh kind of ease because it's a simple text file",
    "start": "2527440",
    "end": "2534700"
  },
  {
    "text": "um then this is splitting it up into chunks and so we're using the recursive character text splitter and so this will",
    "start": "2534700",
    "end": "2539740"
  },
  {
    "text": "split it this this is the text splitter I was mentioning before that splits it up into paragraphs and then sentences and then words",
    "start": "2539740",
    "end": "2546280"
  },
  {
    "text": "um we're creating documents and so documents um are a pretty simple concept they're",
    "start": "2546280",
    "end": "2551680"
  },
  {
    "text": "just text and then Associated metadata so here because uh and by metadata what",
    "start": "2551680",
    "end": "2557740"
  },
  {
    "text": "usually goes in metadata is like source and so here because everything's from the same file the source will be the same",
    "start": "2557740",
    "end": "2562960"
  },
  {
    "text": "um but but that's generally where that would go then we create the vector Shore and we also pass in embeddings so this",
    "start": "2562960",
    "end": "2568660"
  },
  {
    "text": "is Talent this is saying hey use open AI embeddings create this type of vector store from from some documents and then",
    "start": "2568660",
    "end": "2574960"
  },
  {
    "text": "we initialize the chain and so we give the chain the model and the vector store because those are the two lovers that",
    "start": "2574960",
    "end": "2580480"
  },
  {
    "text": "people most often want to control you can also pass in custom prompts so if you want to instruct the language model",
    "start": "2580480",
    "end": "2586660"
  },
  {
    "text": "to respond in a certain way you can pass this in um in in this method as well and now we",
    "start": "2586660",
    "end": "2592540"
  },
  {
    "text": "can um and now we can call uh kind of like the the the the chain over these documents",
    "start": "2592540",
    "end": "2598599"
  },
  {
    "text": "um I know we're a little bit short on time so I'm going to stop sharing there and I think now is probably",
    "start": "2598599",
    "end": "2604119"
  },
  {
    "text": "um a good uh segue into the questions",
    "start": "2604119",
    "end": "2610960"
  },
  {
    "text": "um okay so I can show um a live demo of a finished product if you want and then",
    "start": "2610960",
    "end": "2616780"
  },
  {
    "text": "you can jump if you if you want to do that or if you're pressing time",
    "start": "2616780",
    "end": "2622480"
  },
  {
    "text": "um if it's going to take a minute let's do that but we're we're down to 14 minutes so let's uh yeah yeah literally",
    "start": "2622480",
    "end": "2627940"
  },
  {
    "text": "it's just gonna be a minute so uh let me just show you that real quick",
    "start": "2627940",
    "end": "2634680"
  },
  {
    "text": "you see the screen yep cool so um if we add more time but we don't but",
    "start": "2644980",
    "end": "2652060"
  },
  {
    "text": "effectively what happened in this case is we're able to do with live trainers imagine you've got uh loads of documents",
    "start": "2652060",
    "end": "2660040"
  },
  {
    "text": "somewhere so in this case it's in lotion and each document in this case we're using cron so cron is a calendar app",
    "start": "2660040",
    "end": "2667119"
  },
  {
    "text": "they've got support documents in here so each section has text in each page has",
    "start": "2667119",
    "end": "2673900"
  },
  {
    "text": "text inside as well and so what we're able to do is",
    "start": "2673900",
    "end": "2679119"
  },
  {
    "text": "use Lang chain for example if you can imagine a document you have loads of pages and loads of words or you could",
    "start": "2679119",
    "end": "2685359"
  },
  {
    "text": "just say something like you know how does cron work and we basically run through everything",
    "start": "2685359",
    "end": "2691780"
  },
  {
    "text": "that chase just said now where it goes it does the Q a uh combines into a",
    "start": "2691780",
    "end": "2697660"
  },
  {
    "text": "standard question goes into the vector store and then just gives you a response so obviously we're showing time I can't",
    "start": "2697660",
    "end": "2704560"
  },
  {
    "text": "show more of this but again just show what's possible alongside the uh chatbot",
    "start": "2704560",
    "end": "2710800"
  },
  {
    "text": "you saw in the live chain dots that that's it awesome all right and then so I'm going",
    "start": "2710800",
    "end": "2717819"
  },
  {
    "text": "to bring uh I'm gonna bring and and we're joined by uh Nuno as well oops I can't seem to",
    "start": "2717819",
    "end": "2724960"
  },
  {
    "text": "figure out how to add all three of us onto the big screen so I'll put Nuno on the big screen so Nuno has been leading",
    "start": "2724960",
    "end": "2730359"
  },
  {
    "text": "a lot of the development for the the JavaScript package",
    "start": "2730359",
    "end": "2735400"
  },
  {
    "text": "um and so has uh yeah probably knows more about the JavaScript package than anyone else so we're uh we're honored to",
    "start": "2735400",
    "end": "2742900"
  },
  {
    "text": "have him here um and uh",
    "start": "2742900",
    "end": "2748119"
  },
  {
    "text": "yeah let's go to the question and answering so I'm just going to go down by the the order of what's uploaded so",
    "start": "2748119",
    "end": "2754000"
  },
  {
    "text": "if if you want specific questions to be answered please upload them and I'll probably defer to you for for a bunch of",
    "start": "2754000",
    "end": "2761260"
  },
  {
    "text": "them you know I think the first one deals with um oh awesome all right we're all on the big screen",
    "start": "2761260",
    "end": "2767200"
  },
  {
    "text": "um the first one deals with uh yeah three ways to filter data in Pinecone index namespace and metadata and the",
    "start": "2767200",
    "end": "2773020"
  },
  {
    "text": "Pinecone store from index um yeah so um I'm not so",
    "start": "2773020",
    "end": "2780839"
  },
  {
    "text": "we don't intend to kind of like fully replicate all of the vector stores kind of like functionality",
    "start": "2780839",
    "end": "2787119"
  },
  {
    "text": "um in in Lane chain like we kind of wrap it um and so um I'm not 100 sure what the status of",
    "start": "2787119",
    "end": "2794680"
  },
  {
    "text": "the current um uh implementation is I know in the python one you can use metadata to",
    "start": "2794680",
    "end": "2800680"
  },
  {
    "text": "filter um I know there's been some updates to the client Nuno recently uh yeah I I",
    "start": "2800680",
    "end": "2806140"
  },
  {
    "text": "imagine this is maybe something that's just on the backlog but hasn't made it in yet yeah so this is a feature we can",
    "start": "2806140",
    "end": "2811560"
  },
  {
    "text": "basically easily add it's not there yet but it's uh definitely something we can",
    "start": "2811560",
    "end": "2817900"
  },
  {
    "text": "cool um the second one is about reducing hallucinations yeah so I think just",
    "start": "2817900",
    "end": "2823540"
  },
  {
    "text": "instructing it to pay extra close attention to um the the",
    "start": "2823540",
    "end": "2829140"
  },
  {
    "text": "documents and not make stuff up is is generally what I do um have you guys seen any other kind of like tricks for",
    "start": "2829140",
    "end": "2836500"
  },
  {
    "text": "reducing hallucinations",
    "start": "2836500",
    "end": "2839460"
  },
  {
    "text": "someone said they lost your case oh can you hear me",
    "start": "2842980",
    "end": "2849819"
  },
  {
    "text": "I can't hear you because someone said they can't hear you oh yeah I think we're good",
    "start": "2849819",
    "end": "2856119"
  },
  {
    "text": "um yeah do you guys have any other tricks for kind of like reducing hallucinations besides telling the model",
    "start": "2856119",
    "end": "2862300"
  },
  {
    "text": "not to make stuff up well I guess depending on the use case uh sometimes reducing the temperature",
    "start": "2862300",
    "end": "2868300"
  },
  {
    "text": "can also help um yeah or some of the other more exotic",
    "start": "2868300",
    "end": "2873339"
  },
  {
    "text": "parameters uh but uh yeah I think just what you're saying about putting it in",
    "start": "2873339",
    "end": "2879700"
  },
  {
    "text": "the prompt is usually the best solution yeah",
    "start": "2879700",
    "end": "2884819"
  },
  {
    "text": "cool um is there a way to log out all API calls in their entirety from an llm",
    "start": "2884980",
    "end": "2890619"
  },
  {
    "text": "model for JavaScript typescript you know do you want to tackle this uh so we added the Callback manager",
    "start": "2890619",
    "end": "2898420"
  },
  {
    "text": "functionality one or two days ago so that's going to be your best bet for",
    "start": "2898420",
    "end": "2904000"
  },
  {
    "text": "achieving that so so you can now uh subscribe to a bunch of different",
    "start": "2904000",
    "end": "2910000"
  },
  {
    "text": "events including llm start llm new token llm error llmnt which should give you",
    "start": "2910000",
    "end": "2917560"
  },
  {
    "text": "all the detail about all the calls and that go in in your models as well as a",
    "start": "2917560",
    "end": "2923680"
  },
  {
    "text": "bunch of other events throughout the whole Library so just really check that out so do we have a link for that that",
    "start": "2923680",
    "end": "2929920"
  },
  {
    "text": "we can maybe drop one question that I get asked a bunch and and I think you've",
    "start": "2929920",
    "end": "2935440"
  },
  {
    "text": "think it's in the documents right yeah let me find that yeah let's drop",
    "start": "2935440",
    "end": "2941079"
  },
  {
    "text": "that link in here um so so I get this question a bunch because yeah you've got these like complicated chains and these complicated",
    "start": "2941079",
    "end": "2947380"
  },
  {
    "text": "prompts and it's kind of tricky to know like what's going into where and how they're all constructed",
    "start": "2947380",
    "end": "2952839"
  },
  {
    "text": "um so yeah the the main way that we had to deal with this is so we have this idea of callbacks which basically get",
    "start": "2952839",
    "end": "2958300"
  },
  {
    "text": "run on the start of an alarm call on the end of an alarm call as well as the start of a chain call end of a chain",
    "start": "2958300",
    "end": "2963520"
  },
  {
    "text": "call and stuff like that um and so for logging there's generally like two ways that we've seen people",
    "start": "2963520",
    "end": "2969460"
  },
  {
    "text": "want to log things one is to like the console and we call this I actually don't know what's called in JavaScript",
    "start": "2969460",
    "end": "2975040"
  },
  {
    "text": "but in Python we call this the standard out um uh kind of like callback Handler is that what's called in JavaScript as well",
    "start": "2975040",
    "end": "2980800"
  },
  {
    "text": "you know or uh travel script it's usually called just console.logs but yeah I think",
    "start": "2980800",
    "end": "2987579"
  },
  {
    "text": "yeah so we have a so we have a callback Handler that logs things to to the",
    "start": "2987579",
    "end": "2993280"
  },
  {
    "text": "console and then we also have a concept of like tracing um and so this is um this uses uh so",
    "start": "2993280",
    "end": "2999099"
  },
  {
    "text": "this is a separate like platform that basically logs all these things um and displays them in a UI um for you",
    "start": "2999099",
    "end": "3005819"
  },
  {
    "text": "to kind of like click through and look at um and so both of these were recently added as of like a few days ago",
    "start": "3005819",
    "end": "3012540"
  },
  {
    "text": "um and so that's why that's why it's so exciting and I think yeah if if you know can share awesome yeah and so you know",
    "start": "3012540",
    "end": "3019800"
  },
  {
    "text": "you just shared a link and I would definitely check that out this is like one of the biggest questions that we get and this is a new feature that we're",
    "start": "3019800",
    "end": "3025500"
  },
  {
    "text": "really excited about so check that out um",
    "start": "3025500",
    "end": "3030599"
  },
  {
    "text": "how to split a large document per chapter in order to do embeddings using JavaScript",
    "start": "3030599",
    "end": "3037079"
  },
  {
    "text": "um yeah so so this is uh so right now there's nothing that does this by default this is one that you'd probably",
    "start": "3037079",
    "end": "3043500"
  },
  {
    "text": "want to write some custom code to do um and if but this is this is one that",
    "start": "3043500",
    "end": "3050280"
  },
  {
    "text": "so this is this is something where there's like a long tale of possible ways that",
    "start": "3050280",
    "end": "3055440"
  },
  {
    "text": "you'd want to split documents and so we we want uh and so like how do we think",
    "start": "3055440",
    "end": "3061079"
  },
  {
    "text": "about this as a library um and so we think about this in two ways one we want to make things modular so that if you need to write custom code",
    "start": "3061079",
    "end": "3067920"
  },
  {
    "text": "for some part you can easily plug it in because we recognize that there's a really long tail of use cases we're probably not gonna be able to cover them",
    "start": "3067920",
    "end": "3073440"
  },
  {
    "text": "all um but we want you to be able to learn how like write custom code but still utilize all the other parts so we try to",
    "start": "3073440",
    "end": "3078480"
  },
  {
    "text": "design the library and make it like modular as possible and then the other thing that it says that uh you know I",
    "start": "3078480",
    "end": "3083520"
  },
  {
    "text": "hope someone contributes this to the library this would be awesome to get in I think like um you know there's probably different ways to do this",
    "start": "3083520",
    "end": "3090420"
  },
  {
    "text": "depending on the type of book or things like that um and so yeah I mean",
    "start": "3090420",
    "end": "3097020"
  },
  {
    "text": "um you know I I'd love for some example of this to get in but unfortunately at the moment it's it's not",
    "start": "3097020",
    "end": "3103680"
  },
  {
    "text": "um another question about kind of like loading documents is do we support do we plan to support docx formats in the",
    "start": "3103680",
    "end": "3108900"
  },
  {
    "text": "future this will this will get for free when we do the unstructured integration um you know I don't know if you yeah I",
    "start": "3108900",
    "end": "3115559"
  },
  {
    "text": "think we're still waiting for them on that right yeah let me confirm the status of that",
    "start": "3115559",
    "end": "3122280"
  },
  {
    "text": "um all right Chase before we wrap up I think one uh frequently Asked question",
    "start": "3122280",
    "end": "3128640"
  },
  {
    "text": "is about production deployment and you know the options for that so maybe we",
    "start": "3128640",
    "end": "3134579"
  },
  {
    "text": "can cover that before we head off um what in particular",
    "start": "3134579",
    "end": "3141839"
  },
  {
    "text": "I think people are just asking about you know deployment is it how does it do basically does memory",
    "start": "3141839",
    "end": "3148440"
  },
  {
    "text": "work and deployment to split in work the Surplus functions people talk about for sale",
    "start": "3148440",
    "end": "3154020"
  },
  {
    "text": "um I know Nuno is speaking to you that edge functions and not you know the streaming of effect",
    "start": "3154020",
    "end": "3162079"
  },
  {
    "text": "yeah it doesn't work with the resale at the moment so I can quickly touch on that let me",
    "start": "3163740",
    "end": "3171780"
  },
  {
    "text": "also add a link to the docs here we go uh so",
    "start": "3171780",
    "end": "3178260"
  },
  {
    "text": "um at the moment we support node but we're actively working on so you can",
    "start": "3178260",
    "end": "3183359"
  },
  {
    "text": "other environments like uh Edge and Deno and even browser so yeah just uh keep an",
    "start": "3183359",
    "end": "3192300"
  },
  {
    "text": "eye on it it will come soon yeah and and for so the memory part I",
    "start": "3192300",
    "end": "3199740"
  },
  {
    "text": "saw a few questions about memory um and and that with production um so right now we rely on that kind of",
    "start": "3199740",
    "end": "3206280"
  },
  {
    "text": "like being managed outside of of Lane chain um I think uh yeah there are",
    "start": "3206280",
    "end": "3213780"
  },
  {
    "text": "um we're we're considering kind of like options to make that more baked in but I think like um",
    "start": "3213780",
    "end": "3219059"
  },
  {
    "text": "right now that's that's done kind of like outside of link chain we're gonna be working on ways to like uh help we're",
    "start": "3219059",
    "end": "3225720"
  },
  {
    "text": "gonna be working on at least like instructions for how to kind of like manage that and then hydrate the memory",
    "start": "3225720",
    "end": "3232020"
  },
  {
    "text": "um with previous things so that's definitely on the roadmap I would expect that in the next few weeks",
    "start": "3232020",
    "end": "3237960"
  },
  {
    "text": "um but for fully kind of like managed memory I think that will be a bit longer that was literally something that takes",
    "start": "3237960",
    "end": "3243119"
  },
  {
    "text": "a bit longer to um uh to go through",
    "start": "3243119",
    "end": "3248160"
  },
  {
    "text": "um all right I'm gonna just go through some rapid fire questions",
    "start": "3248160",
    "end": "3253800"
  },
  {
    "text": "um thoughts on llm based Splitters so I'm assuming this means using language models to split text themselves uh I'm I",
    "start": "3253800",
    "end": "3261660"
  },
  {
    "text": "think language models are getting pretty good at a lot of things so I think it's entirely feasible I haven't tried it myself but but I like it in theory",
    "start": "3261660",
    "end": "3269819"
  },
  {
    "text": "um how to deal if we have sensitive data and don't want to share info with third-party service um so there are um",
    "start": "3269819",
    "end": "3277260"
  },
  {
    "text": "so one when using link chain like everything's run locally so so you're not sharing any data with us the way",
    "start": "3277260",
    "end": "3282960"
  },
  {
    "text": "that you'd be sharing it is if you're using like these models like open Ai and you're sending and you're sending data to them",
    "start": "3282960",
    "end": "3288960"
  },
  {
    "text": "um I know they're working on ways to kind of like uh do private deployments of stuff there's also like local models",
    "start": "3288960",
    "end": "3295020"
  },
  {
    "text": "like hugging face that you could run locally um I think most of these have better",
    "start": "3295020",
    "end": "3300240"
  },
  {
    "text": "support in Python but you can run these in a separate python server to the side and then kind of use these in in link",
    "start": "3300240",
    "end": "3305700"
  },
  {
    "text": "chain um so I would recommend that um what is mendable mendable is a great",
    "start": "3305700",
    "end": "3310740"
  },
  {
    "text": "company they um I interacted with the those guys um a long time ago and there",
    "start": "3310740",
    "end": "3318119"
  },
  {
    "text": "yeah they power the search bar and they're um uh they're uh they're they're",
    "start": "3318119",
    "end": "3324420"
  },
  {
    "text": "doing this like search natively for all kinds of like Dockers I think it's docosaurus kind of like builds um so any",
    "start": "3324420",
    "end": "3331380"
  },
  {
    "text": "site that's using docky source for their documentation they can jump in there",
    "start": "3331380",
    "end": "3336480"
  },
  {
    "text": "um under what circumstance can we use index alongside Lane chain that's more of a",
    "start": "3336480",
    "end": "3342359"
  },
  {
    "text": "python question so we'll probably cover that in the python part um I'm interested in using an agent that",
    "start": "3342359",
    "end": "3348000"
  },
  {
    "text": "interacts with the vector store specifically would like to answer technical questions with cited sources augmented by different specific",
    "start": "3348000",
    "end": "3353579"
  },
  {
    "text": "knowledge provided by other tools it appears the current agent implementation does not yet provide detailed sources",
    "start": "3353579",
    "end": "3359460"
  },
  {
    "text": "for the information or retrieves in the vector stores this feature music with our map yeah so this is um I",
    "start": "3359460",
    "end": "3364680"
  },
  {
    "text": "think this is something that there'd be two ways to do this one you could change",
    "start": "3364680",
    "end": "3371099"
  },
  {
    "text": "the you could use like a question answering tool that cited its sources as",
    "start": "3371099",
    "end": "3376140"
  },
  {
    "text": "a tool itself and then rely on that um but then you'd also probably want to change the default prompt of the agent",
    "start": "3376140",
    "end": "3383099"
  },
  {
    "text": "to tell it to cite its sources um so by default you know this is a generic agent um it won't know decided",
    "start": "3383099",
    "end": "3389220"
  },
  {
    "text": "sources it won't know that it should be doing that so I would definitely definitely definitely change the prompt that instructs the agent what to do and",
    "start": "3389220",
    "end": "3395579"
  },
  {
    "text": "then you may also need to change the vector store implementation however you're using that to to return the",
    "start": "3395579",
    "end": "3401339"
  },
  {
    "text": "sources um yeah I think this is a bit more unfortunately I don't have a great kind",
    "start": "3401339",
    "end": "3406559"
  },
  {
    "text": "of like surface level answer for this this is a bit more um in in detail one but I would definitely start by one making sure the",
    "start": "3406559",
    "end": "3413760"
  },
  {
    "text": "agent has all the information and so like yeah making sure that the executive chair is returning the sources and stuff",
    "start": "3413760",
    "end": "3418920"
  },
  {
    "text": "like that and then two making sure that it knows to like cited stuff so I'd probably like make a custom agent in",
    "start": "3418920",
    "end": "3424079"
  },
  {
    "text": "some regard um we are out of time there's a lot of questions that we",
    "start": "3424079",
    "end": "3429960"
  },
  {
    "text": "didn't answer we'll we'll yeah we have a Discord um we'll try to answer them in there there's also other folks that can help",
    "start": "3429960",
    "end": "3436380"
  },
  {
    "text": "answer them um this was a lot of fun we'll do it more so so keep an eye out for more",
    "start": "3436380",
    "end": "3441480"
  },
  {
    "text": "we're doing one with SQL next week and then and then we're gonna announce a pretty cool one for the week after that",
    "start": "3441480",
    "end": "3447359"
  },
  {
    "text": "um yeah any any final thoughts you know or or Maya",
    "start": "3447359",
    "end": "3452819"
  },
  {
    "text": "yeah you know first time mistakes but it's all good I mean the main thing",
    "start": "3452819",
    "end": "3457980"
  },
  {
    "text": "is you know if people get an introduction and you know they're also always can be follow-ups and I think the",
    "start": "3457980",
    "end": "3463140"
  },
  {
    "text": "main thing is people wanted to talk to you and and you know get you to answer questions because you're you're not the",
    "start": "3463140",
    "end": "3468540"
  },
  {
    "text": "easiest person to reach so uh hopefully everyone enjoyed this um and um yeah I look forward to the the",
    "start": "3468540",
    "end": "3475680"
  },
  {
    "text": "next one coming up soon that's next week Thursday right",
    "start": "3475680",
    "end": "3480680"
  },
  {
    "text": "uh next Wednesday so we'll do we'll try to do these uh pretty consistently on Wednesdays mornings",
    "start": "3481260",
    "end": "3487800"
  },
  {
    "text": "yeah join the Discord and um uh before you jump off let me just post",
    "start": "3487800",
    "end": "3492900"
  },
  {
    "text": "the Discord link if anyone doesn't have",
    "start": "3492900",
    "end": "3498440"
  },
  {
    "text": "all right and yeah you know any final words from you",
    "start": "3499619",
    "end": "3504500"
  },
  {
    "text": "uh no yeah just uh you know where you release new stuff every day so just keep",
    "start": "3506220",
    "end": "3511380"
  },
  {
    "text": "your eyes out for new features yes because they're they're released coming today uh there is a release coming today",
    "start": "3511380",
    "end": "3517920"
  },
  {
    "text": "with the whole video Carlos I love that suggestion of the channel for events we'll definitely add",
    "start": "3517920",
    "end": "3523319"
  },
  {
    "text": "that in the Discord so it's more easy to keep track of these that's a great idea all right we're over",
    "start": "3523319",
    "end": "3528420"
  },
  {
    "text": "time thank you guys",
    "start": "3528420",
    "end": "3531859"
  }
]