[
  {
    "text": "hi everyone viim here from Lang chain today I'd like to show you how to build and deploy a self-corrective rag",
    "start": "480",
    "end": "7399"
  },
  {
    "text": "application using Lang graph for those of you who are not familiar Lang graph is a library developed by linkchain that",
    "start": "7399",
    "end": "14280"
  },
  {
    "text": "allows you to build complex AI applications represented as graphs and a",
    "start": "14280",
    "end": "19560"
  },
  {
    "text": "node in the graph do some unit of work passes his outputs to other nodes it's",
    "start": "19560",
    "end": "24760"
  },
  {
    "text": "connected to and a node can be an llm a retriever or some other C function that",
    "start": "24760",
    "end": "31240"
  },
  {
    "text": "you can Define and this way langra allows you to build more complex flows",
    "start": "31240",
    "end": "36879"
  },
  {
    "text": "using conditionals and Loops which is very important for building reliable AI applications and that's also very",
    "start": "36879",
    "end": "43840"
  },
  {
    "text": "important in regag applications too so for our documents in the reg application",
    "start": "43840",
    "end": "50079"
  },
  {
    "text": "we're going to be using documentation from pandas that's a very popular library for data analysis and structured",
    "start": "50079",
    "end": "56960"
  },
  {
    "text": "data and what one of my favorite python libraries as well so um first off let's talk about what",
    "start": "56960",
    "end": "64960"
  },
  {
    "text": "self corrective means in this case consider a very simple rag pipeline very",
    "start": "64960",
    "end": "70640"
  },
  {
    "text": "vanilla we just have a vector store and llm we take user question we retrieve",
    "start": "70640",
    "end": "76560"
  },
  {
    "text": "relevant documents from the vector store we pass those to llm put that in the",
    "start": "76560",
    "end": "81960"
  },
  {
    "text": "context window and then try to generate the answer based on that there are at least several problems with this vanula",
    "start": "81960",
    "end": "88119"
  },
  {
    "text": "approach first off with we can have issues at the document retrieval step with the document relevance with",
    "start": "88119",
    "end": "94799"
  },
  {
    "text": "irrelevant documents we will not be able to generate a good answer and then even if we did retrieve good documents at the",
    "start": "94799",
    "end": "100920"
  },
  {
    "text": "generation phase there's a couple of other issues that we can encounter one is hallucination so I'm can just",
    "start": "100920",
    "end": "107759"
  },
  {
    "text": "hallucinate an answer and ignore retrieve documents or there could just be a completely irrelevant answer even",
    "start": "107759",
    "end": "113600"
  },
  {
    "text": "if it is grounded in the document consider another approach to sort of",
    "start": "113600",
    "end": "120520"
  },
  {
    "text": "defining these flows where it's not entirely defined by the developer ahead of time like in that simple vanilla rag",
    "start": "120520",
    "end": "127640"
  },
  {
    "text": "but rather we have llm as part of that control flow where LM actually makes",
    "start": "127640",
    "end": "133400"
  },
  {
    "text": "decisions on where to go next so for example I can inspect the outputs of the previous step in the execution and",
    "start": "133400",
    "end": "140560"
  },
  {
    "text": "decide should we redo it or maybe everything is good and we continue our execution and that's particularly",
    "start": "140560",
    "end": "147519"
  },
  {
    "text": "important for rag as well so over the last couple of months uh",
    "start": "147519",
    "end": "153519"
  },
  {
    "text": "there there's been a lot of different papers addressing that so couple of them",
    "start": "153519",
    "end": "158920"
  },
  {
    "text": "were self Rag and corrective rag that dealt with different types of issues that I just described we actually did",
    "start": "158920",
    "end": "165360"
  },
  {
    "text": "videos on that already in terms of how to implement those papers using Lang graph so definitely make sure to check",
    "start": "165360",
    "end": "171519"
  },
  {
    "text": "those out and here what we're doing is taking ideas from both of those and",
    "start": "171519",
    "end": "178200"
  },
  {
    "text": "particularly focusing on the sort of answer hallucination answer quality",
    "start": "178200",
    "end": "184959"
  },
  {
    "text": "phase of this rather than necessarily the um document retrieval relevance so",
    "start": "184959",
    "end": "191200"
  },
  {
    "text": "again what we're doing here is we take user's question we retrieve a set of",
    "start": "191200",
    "end": "196599"
  },
  {
    "text": "documents from the vector store and we try and generate uh the candidate answer",
    "start": "196599",
    "end": "201760"
  },
  {
    "text": "based on that from here things get interesting first off we're trying to check the answer for hallucinations so",
    "start": "201760",
    "end": "209519"
  },
  {
    "text": "if if there are any hallucinations we go back and try to regenerate the answer again if there are no hallucinations we",
    "start": "209519",
    "end": "217319"
  },
  {
    "text": "do another check which is does the answer actually address Regional question is it good quality answer and",
    "start": "217319",
    "end": "224040"
  },
  {
    "text": "if it's no uh not relevant we'll try and rewrite the question and do another",
    "start": "224040",
    "end": "230640"
  },
  {
    "text": "document search then we'll redo all of those checks again and if we paste those that's great great and then we'll return",
    "start": "230640",
    "end": "237560"
  },
  {
    "text": "the answer to the user we try that for several times and you know if we're still failing at the end we're going to",
    "start": "237560",
    "end": "244120"
  },
  {
    "text": "f back in a very simple web search where we take the user question we retrieve",
    "start": "244120",
    "end": "249439"
  },
  {
    "text": "set of web service results generate the answer based on that and then just return that answer to the user so before",
    "start": "249439",
    "end": "257000"
  },
  {
    "text": "we proceed I'd like to show you how this looks like in a final deployed state so",
    "start": "257000",
    "end": "263400"
  },
  {
    "text": "what we're going to be using for deployment is langra cloud that's our oneclick solution to deploy and host",
    "start": "263400",
    "end": "269199"
  },
  {
    "text": "langra applications and uh let me show you how that's going to look like so here you'll",
    "start": "269199",
    "end": "275919"
  },
  {
    "text": "see lra studio for our application deployment and lra Studio allows you to",
    "start": "275919",
    "end": "281840"
  },
  {
    "text": "visualize and interact with your graph as well as debug its execution for",
    "start": "281840",
    "end": "287560"
  },
  {
    "text": "different types of inputs and I'm going to show you two queries here first off I'm going to show you a query where",
    "start": "287560",
    "end": "294039"
  },
  {
    "text": "we're going to get good quality answer with no hallucinations based on the documents in the vectors store and the",
    "start": "294039",
    "end": "300840"
  },
  {
    "text": "second query we're going to try and ask the question about documents St not even",
    "start": "300840",
    "end": "306199"
  },
  {
    "text": "in the vector store so we know for fact the model will try to hallucinate we're then going to regenerate the answer for",
    "start": "306199",
    "end": "313720"
  },
  {
    "text": "one additional retry and then if we fail we're going to just fall back on the web",
    "start": "313720",
    "end": "319800"
  },
  {
    "text": "search so let's try the successful query first so because we're working with pendis uh let's try a very simple query",
    "start": "319800",
    "end": "326919"
  },
  {
    "text": "such as how can I calculate some by",
    "start": "326919",
    "end": "332479"
  },
  {
    "text": "groups so you'll see what we're doing here we're retrieving documents generating candidate",
    "start": "332479",
    "end": "339080"
  },
  {
    "text": "answer and then since we passed all of the checks we finalized the answer and",
    "start": "339080",
    "end": "344120"
  },
  {
    "text": "return to the user so things look pretty good now contrast this with the query",
    "start": "344120",
    "end": "349800"
  },
  {
    "text": "where we're going to have a hallucination the reason for that is that there's no document in the vector",
    "start": "349800",
    "end": "355680"
  },
  {
    "text": "store to answer this question so the model will try and come up with the answer that's not based on documents so",
    "start": "355680",
    "end": "362720"
  },
  {
    "text": "let's ask how can I convert a categorical column into",
    "start": "362720",
    "end": "369440"
  },
  {
    "text": "Dumis and you'll know the first steps are pretty much the same we retrieve the documents and we generate CED answer but",
    "start": "369440",
    "end": "377120"
  },
  {
    "text": "then we actually fail at the hallucination grading and we need to regenerate the answer again so you see",
    "start": "377120",
    "end": "383680"
  },
  {
    "text": "it over here and then because we did only one retry we actually go to web",
    "start": "383680",
    "end": "389120"
  },
  {
    "text": "search and then generate the answer again and now we succeeded because we did get a relevant document from web",
    "start": "389120",
    "end": "395759"
  },
  {
    "text": "search so the answer that we're returning to the user is actually grounded in those search results now",
    "start": "395759",
    "end": "402440"
  },
  {
    "text": "let's head back to the notebook and I'll actually show you how to build this step by step our um lolm is going to be",
    "start": "402440",
    "end": "410000"
  },
  {
    "text": "clawed by entropic we're going to also use the chroma database for Vector store",
    "start": "410000",
    "end": "415080"
  },
  {
    "text": "retrieval I'm going to use open AI uh embeddings also for storing the data in",
    "start": "415080",
    "end": "421080"
  },
  {
    "text": "the vector database we are also going to use T search as our web search tool for the",
    "start": "421080",
    "end": "429280"
  },
  {
    "text": "fallbacks and we're also going to Define lsmith uh tracing for uh login data to",
    "start": "429280",
    "end": "436560"
  },
  {
    "text": "our production system for developers that allow us to monitor our Lang graph application and see different steps in",
    "start": "436560",
    "end": "444039"
  },
  {
    "text": "more detail so let's set up our environment we'll install several depend",
    "start": "444039",
    "end": "449520"
  },
  {
    "text": "dependencies and then we'll set our API Keys which I've already done here we're",
    "start": "449520",
    "end": "455000"
  },
  {
    "text": "also going to import a few things from various L chain libraries ranging from the Integrations to uh just key",
    "start": "455000",
    "end": "462199"
  },
  {
    "text": "Primitives for Vector store retrieval for um prompting Etc and then we'll also",
    "start": "462199",
    "end": "468240"
  },
  {
    "text": "import line graph which is the most important part for building the line graph application now we're going to set up",
    "start": "468240",
    "end": "475199"
  },
  {
    "text": "several key components such as model Retriever and tools for the retriever",
    "start": "475199",
    "end": "480919"
  },
  {
    "text": "we'll have to do several additional steps to format the data in the right way as I mentioned we're going to be",
    "start": "480919",
    "end": "486759"
  },
  {
    "text": "working with the pendas documentation but you'll know that we're looking at a very small subset of it specifically",
    "start": "486759",
    "end": "493680"
  },
  {
    "text": "just three pages from user guides the reason why it's so small is actually um",
    "start": "493680",
    "end": "500080"
  },
  {
    "text": "it it's done on purpose because the library itself is very popular and",
    "start": "500080",
    "end": "506400"
  },
  {
    "text": "there's a lot of information on the web already about how to use it and all of",
    "start": "506400",
    "end": "511960"
  },
  {
    "text": "this data was actually part of training data for llms so the model already has a lot of Bendis knowledge encoded in the",
    "start": "511960",
    "end": "518880"
  },
  {
    "text": "weights so what we could end up with is if the model does not see the",
    "start": "518880",
    "end": "524720"
  },
  {
    "text": "information in the retrieve documents it might still try and generate the answer now in some cases is actually",
    "start": "524720",
    "end": "531160"
  },
  {
    "text": "nice and could be a useful fullback but in other cases it might not be desirable and what we're going to show here is how",
    "start": "531160",
    "end": "537519"
  },
  {
    "text": "to actually deal with those situations where the model still hallucinates the answer or like tries to come up with the",
    "start": "537519",
    "end": "543600"
  },
  {
    "text": "answer even though that answer is not grounded in the retrieve documents so",
    "start": "543600",
    "end": "549880"
  },
  {
    "text": "again for retriever what we're going to do is we're going to try and fetch data from those three Source web pages then",
    "start": "549880",
    "end": "556120"
  },
  {
    "text": "what we're going to do is prepare those documents namely we'll chunk them up into smaller pieces for retrieval and",
    "start": "556120",
    "end": "563680"
  },
  {
    "text": "then we're going to index them into this chroma in memory database using the open",
    "start": "563680",
    "end": "569360"
  },
  {
    "text": "settings for embedding the text and then we'll create that retriever we'll also initialize the llm",
    "start": "569360",
    "end": "576600"
  },
  {
    "text": "which is an our case anthropic most recent quote 3.5 Sonet model and we're",
    "start": "576600",
    "end": "582120"
  },
  {
    "text": "going to define t Search tool for doing the web search now we're getting to the key part in setting up any L graph",
    "start": "582120",
    "end": "589160"
  },
  {
    "text": "application we're going to define the graph state so state is the core object",
    "start": "589160",
    "end": "594279"
  },
  {
    "text": "for any L graph application because that's going to be",
    "start": "594279",
    "end": "599399"
  },
  {
    "text": "representing the data that each node has access to and the object that each node",
    "start": "599399",
    "end": "604839"
  },
  {
    "text": "is going to update as it's executing so what we're going to keep track of here",
    "start": "604839",
    "end": "610120"
  },
  {
    "text": "are a list of messages because we want our app to be uh chat based so user",
    "start": "610120",
    "end": "615560"
  },
  {
    "text": "inputs a human message and gets AI message with the uh final answer as the",
    "start": "615560",
    "end": "620800"
  },
  {
    "text": "response then we're also going to keep track of the original question that the user asks the documents that we retrieve",
    "start": "620800",
    "end": "628680"
  },
  {
    "text": "at any Ser of retrial Step the uh candidate answer that we're going to be",
    "start": "628680",
    "end": "633880"
  },
  {
    "text": "grading number of retries so that's basically just allows us to control you",
    "start": "633880",
    "end": "639320"
  },
  {
    "text": "know how many retries we've already done so that we can go to the web search fallback and then the actual indicator",
    "start": "639320",
    "end": "646600"
  },
  {
    "text": "whether we're still allowed to do web search fullback or not at the end and then we're also going to have the",
    "start": "646600",
    "end": "652440"
  },
  {
    "text": "configuration for the graph that allows us to pass the maximum number of free tries parameter at run time which will",
    "start": "652440",
    "end": "659800"
  },
  {
    "text": "be quite handy now let's set up the graph nodes the first one quite obviously is going",
    "start": "659800",
    "end": "665639"
  },
  {
    "text": "to be the actual document search node uh you'll know that each node uh",
    "start": "665639",
    "end": "671240"
  },
  {
    "text": "implementation is going to be a function that takes the state object we just defined because it needs to read the",
    "start": "671240",
    "end": "677279"
  },
  {
    "text": "information from State and then it's going to return the update to the state the update is usually going to look like",
    "start": "677279",
    "end": "684200"
  },
  {
    "text": "a dictionary where the key is going to correspond the key in the state and the value is going going to be the value of",
    "start": "684200",
    "end": "690360"
  },
  {
    "text": "the actual update that needs to conform to the same surf type and in the",
    "start": "690360",
    "end": "696279"
  },
  {
    "text": "document search it's going to be exactly what you would expect we look at the list of messages and grab the most",
    "start": "696279",
    "end": "701959"
  },
  {
    "text": "recent message from the user which is the human message get the content of it pass it to the Retriever and get the",
    "start": "701959",
    "end": "708959"
  },
  {
    "text": "list of documents then we also update the question and the web fallback is set",
    "start": "708959",
    "end": "714959"
  },
  {
    "text": "to True which means that so far we're allowed to do web search fallback if we exhaust the number of retries that'll be",
    "start": "714959",
    "end": "721920"
  },
  {
    "text": "relevant in just a little bit then we Define our answer generation node here",
    "start": "721920",
    "end": "727680"
  },
  {
    "text": "what we're seeing is we'll need the question and documents from the state we're also going to look at how many",
    "start": "727680",
    "end": "735040"
  },
  {
    "text": "retries we've done so far because we would want to increment that we're going to be incrementing this at the",
    "start": "735040",
    "end": "740160"
  },
  {
    "text": "generation step and then we're going to define the actual chain or sequence that's going to",
    "start": "740160",
    "end": "746600"
  },
  {
    "text": "be responsible for answer generation what we're doing here we're basically saying we're going to take the llm and",
    "start": "746600",
    "end": "752760"
  },
  {
    "text": "we're going to call it with a specific prompt that we've pulled from L chain Hub which is a Prett find set of prompts",
    "start": "752760",
    "end": "758920"
  },
  {
    "text": "in this case so for reg we already have that there and then we are also going to",
    "start": "758920",
    "end": "765279"
  },
  {
    "text": "grab the content of the last message from the LM and just return it as a string that's what the output parer does",
    "start": "765279",
    "end": "771920"
  },
  {
    "text": "here so we pass our context in question get the generation and return that to",
    "start": "771920",
    "end": "777839"
  },
  {
    "text": "the state as a candidate answer note that all we need to return from the node is just the update so we don't need to",
    "start": "777839",
    "end": "784560"
  },
  {
    "text": "still return all of the keys at each step we only returning the updates that are relevant at this node now we're",
    "start": "784560",
    "end": "791880"
  },
  {
    "text": "going to define the node that rewrites the original user question this is useful if we see the answer being",
    "start": "791880",
    "end": "798120"
  },
  {
    "text": "irrelevant we go and rewrite the original question and try the whole loop again that's where it's going to be",
    "start": "798120",
    "end": "803959"
  },
  {
    "text": "relevant so we grab the question here and then we are going to do a similar",
    "start": "803959",
    "end": "810480"
  },
  {
    "text": "definition of a query writer chain where we take a l with the prompt and then parse the output into a string and",
    "start": "810480",
    "end": "817320"
  },
  {
    "text": "simply invoke it with the question to write now we're going to define the web search node uh that's going to take the",
    "start": "817320",
    "end": "824440"
  },
  {
    "text": "original user question and then invoke the Search tool with that question now we're going to stitch the content from",
    "start": "824440",
    "end": "831279"
  },
  {
    "text": "all of those search results into a single document and we'll just mark it as a source web search for convenience",
    "start": "831279",
    "end": "839120"
  },
  {
    "text": "we're just going to add this document to the set of existing documents for Simplicity and we're going to say that",
    "start": "839120",
    "end": "845440"
  },
  {
    "text": "once we've done web search we no longer are allowed to do web search fullback anymore so we're updating it here and",
    "start": "845440",
    "end": "852720"
  },
  {
    "text": "then finally we're just going to take the uh graph State once we're ready to return to the user and just format the",
    "start": "852720",
    "end": "859000"
  },
  {
    "text": "candidate answer into an AI message because again we want this to be um structured as a set of chat messages for",
    "start": "859000",
    "end": "866040"
  },
  {
    "text": "the chat application here we're getting to a quite interesting part where we are doing those checks on the answer",
    "start": "866040",
    "end": "873079"
  },
  {
    "text": "hallucination and answer relevance so we're defining the system prompts where we're asking the model to",
    "start": "873079",
    "end": "880079"
  },
  {
    "text": "check whether the answer is grounded in the source documents as well as whether the answer actually addresses the",
    "start": "880079",
    "end": "885440"
  },
  {
    "text": "original question and what we're also doing is we're defining these pantic",
    "start": "885440",
    "end": "890759"
  },
  {
    "text": "models which will actually be used just uh a little bit lower in this notebook",
    "start": "890759",
    "end": "897000"
  },
  {
    "text": "where we're going to ask the llm to return the output in the structured form so these are just going to be used to",
    "start": "897000",
    "end": "903800"
  },
  {
    "text": "parameterize the llm co to always return a identic model instead of just a",
    "start": "903800",
    "end": "909440"
  },
  {
    "text": "message or a string and this is the key piece of our control flow logic it's going to be a",
    "start": "909440",
    "end": "916480"
  },
  {
    "text": "conditional Edge that I'll cover in just a little bit but effectively this is a function that says given the state where",
    "start": "916480",
    "end": "924399"
  },
  {
    "text": "should I go next based on the set of conditions so unlike the node implementation where each function takes",
    "start": "924399",
    "end": "931120"
  },
  {
    "text": "the state and returns update to the state the conditional edge here will take the state and return the name of",
    "start": "931120",
    "end": "937839"
  },
  {
    "text": "the node where you should go next to based on satisfying certain conditions",
    "start": "937839",
    "end": "943480"
  },
  {
    "text": "so we're going to look at the question document and candidate answer generation so far we'll check if we're still",
    "start": "943480",
    "end": "949759"
  },
  {
    "text": "allowed to do web search fallback keep track of fre tries and also we're going to grab the",
    "start": "949759",
    "end": "956600"
  },
  {
    "text": "maximum number of retries from the config so you see we're also passing config in addition to the state in this",
    "start": "956600",
    "end": "963920"
  },
  {
    "text": "function you can actually do that in node implementation as well so let's look at the key pieces of",
    "start": "963920",
    "end": "970800"
  },
  {
    "text": "logic here we're going to be grading the answer for hallucination you'll see that",
    "start": "970800",
    "end": "976480"
  },
  {
    "text": "the LM configuration takes the um identic model",
    "start": "976480",
    "end": "982360"
  },
  {
    "text": "that we Define earlier and we just need to call the LM with structured output this is going to ensure that we're going",
    "start": "982360",
    "end": "988680"
  },
  {
    "text": "to get the output as identic model and then we're going to invoke invoke that",
    "start": "988680",
    "end": "994519"
  },
  {
    "text": "with the set of documents we retrieved as well as the candidate answer generation that's what the prompt here",
    "start": "994519",
    "end": "1000839"
  },
  {
    "text": "is expecting then we're going to say if the grade is no meaning the answer is",
    "start": "1000839",
    "end": "1006199"
  },
  {
    "text": "not grounded in the documents and there is a hallucination we'll either try to",
    "start": "1006199",
    "end": "1011639"
  },
  {
    "text": "generate the answer again if we're still allowed to do that based on the number of freet tries or we'll just go to web",
    "start": "1011639",
    "end": "1018040"
  },
  {
    "text": "search fullback if we're not if we don't have any hallucinations",
    "start": "1018040",
    "end": "1023399"
  },
  {
    "text": "we actually get to the next stage which is the greater of answer relevance so",
    "start": "1023399",
    "end": "1029360"
  },
  {
    "text": "we're going to similarly invoke the llm with the structured output ensuring that",
    "start": "1029360",
    "end": "1035199"
  },
  {
    "text": "we return identic model and we'll say if the answer is re addressing the question",
    "start": "1035199",
    "end": "1041400"
  },
  {
    "text": "and is relevant we're going to just finalize and return to the user and if it's not we're going to try and rewrite",
    "start": "1041400",
    "end": "1047678"
  },
  {
    "text": "the query again and ensuring that we're still allowed to do that based on the",
    "start": "1047679",
    "end": "1053080"
  },
  {
    "text": "retries and if not we're going to go and full back and web search lastly there was this piece of logic here all it's",
    "start": "1053080",
    "end": "1060120"
  },
  {
    "text": "doing is just saying if we are no longer allowed to do web search fallback which would happen after we do the web search",
    "start": "1060120",
    "end": "1066960"
  },
  {
    "text": "fallback we just go to the finalized answer and return to the user now let's",
    "start": "1066960",
    "end": "1072160"
  },
  {
    "text": "assemble the graph so we've done all of our node definitions and we can add them here first off we Define the the uh",
    "start": "1072160",
    "end": "1079960"
  },
  {
    "text": "State graph object that takes the schema of the state which is the graph State as well as the schema of the config which",
    "start": "1079960",
    "end": "1086400"
  },
  {
    "text": "has the max number of R tries is the only parameter We'll add all of our nodes uh which is going to add the name",
    "start": "1086400",
    "end": "1094280"
  },
  {
    "text": "and then the function that corresponds to the node then we're going to set the entry point that's where our graph",
    "start": "1094280",
    "end": "1099799"
  },
  {
    "text": "execution is going to start in this case is going to be document search as the",
    "start": "1099799",
    "end": "1105200"
  },
  {
    "text": "first node so take the question and do the search First and then we'll Define a",
    "start": "1105200",
    "end": "1110480"
  },
  {
    "text": "bunch of edges we're going to Define both regular edges and conditional edges so regular Edge is basically just saying",
    "start": "1110480",
    "end": "1117200"
  },
  {
    "text": "you have to go from A to B so once you're done executing node a you just go",
    "start": "1117200",
    "end": "1122679"
  },
  {
    "text": "sequentially to node B conditional Edge is uh saying from",
    "start": "1122679",
    "end": "1128240"
  },
  {
    "text": "node a you can go to a bunch of other nodes depending on satisfying certain conditions and this function is what's",
    "start": "1128240",
    "end": "1135000"
  },
  {
    "text": "going to determine how you're supposed to do that so that's exactly what we",
    "start": "1135000",
    "end": "1140200"
  },
  {
    "text": "just looked at in this grade generation function and this is why we're returning the names of the nodes here it's because",
    "start": "1140200",
    "end": "1147480"
  },
  {
    "text": "it's telling the graph that that's where you should go next lastly we have to compile the graph so that we can",
    "start": "1147480",
    "end": "1153320"
  },
  {
    "text": "actually use that as a runnable you can also visualize the graph and this visual corresponds very",
    "start": "1153320",
    "end": "1159840"
  },
  {
    "text": "closely to the original flow we Define it just looks slightly different because it's uh",
    "start": "1159840",
    "end": "1170480"
  },
  {
    "text": "autogenerate and we can actually run the graph one thing that L graph is particularly well suited for and has",
    "start": "1170480",
    "end": "1176720"
  },
  {
    "text": "first class support for is streaming so in particular uh we can stream the full",
    "start": "1176720",
    "end": "1183159"
  },
  {
    "text": "values of the state after each step in execution of the graph or we can also stream the updates uh instead of the",
    "start": "1183159",
    "end": "1190360"
  },
  {
    "text": "full State and we can even stream the individual tokens from llms inside the graph nodes if we wish to do so but",
    "start": "1190360",
    "end": "1198159"
  },
  {
    "text": "first let's try and stream the outputs from the graph here I'm just going to",
    "start": "1198159",
    "end": "1203280"
  },
  {
    "text": "print out the print outs from each node saying what's happening so you can see it step by step going to ask a very",
    "start": "1203280",
    "end": "1210360"
  },
  {
    "text": "simple question how do I calculate some by groups it's very van append operation it's actually covered in our underlying",
    "start": "1210360",
    "end": "1217120"
  },
  {
    "text": "documents that we indexed so let's run this and you'll see that we're retrieving document we're generating the",
    "start": "1217120",
    "end": "1223159"
  },
  {
    "text": "answer checking hallucinations and we pass that filter we also pass the check for the answer",
    "start": "1223159",
    "end": "1231120"
  },
  {
    "text": "relevant and we're ready to finalize response here I just printed out the outputs from the notes but um you'll see",
    "start": "1231120",
    "end": "1238159"
  },
  {
    "text": "that the full sort of State update outputs will look like this get the",
    "start": "1238159",
    "end": "1243440"
  },
  {
    "text": "output from the document search node with the question documents and some other",
    "start": "1243440",
    "end": "1249320"
  },
  {
    "text": "information then we'll get the output from the generate node with the candidate",
    "start": "1249320",
    "end": "1254360"
  },
  {
    "text": "answer and because the answer actually pass all the checks we'll get to the finalized response and we get AI message",
    "start": "1254360",
    "end": "1261080"
  },
  {
    "text": "with the content here and if we' like to stream the underlying tokens from the LM we'll see",
    "start": "1261080",
    "end": "1268080"
  },
  {
    "text": "that these are the individual tokens from candidate answer generation as it's",
    "start": "1268080",
    "end": "1273480"
  },
  {
    "text": "coming out of the LM we're also getting some partial tool call information the",
    "start": "1273480",
    "end": "1278559"
  },
  {
    "text": "reason you see tool calls is because that's what's powering our identic model structured output parsing under the hood",
    "start": "1278559",
    "end": "1286400"
  },
  {
    "text": "and that's basically it for a simple streaming demo now let's try a query",
    "start": "1286400",
    "end": "1292679"
  },
  {
    "text": "that would require web search fallback this query is uh for the information",
    "start": "1292679",
    "end": "1299480"
  },
  {
    "text": "that's not contained in the index documents that's where we actually going to trigger that hallucination",
    "start": "1299480",
    "end": "1305720"
  },
  {
    "text": "Behavior so you'll see we're retrieving the document we're generating the answer",
    "start": "1305720",
    "end": "1311679"
  },
  {
    "text": "and we're not passing the first check which is the hallucination check we do",
    "start": "1311679",
    "end": "1316880"
  },
  {
    "text": "one more try because we've set Max rri with the configuration here and after",
    "start": "1316880",
    "end": "1322919"
  },
  {
    "text": "that we're doing the web search fback and once again we're generating the answer and now we're just finalizing the",
    "start": "1322919",
    "end": "1329039"
  },
  {
    "text": "response to the user so that's how you can also implement the check and do a",
    "start": "1329039",
    "end": "1334440"
  },
  {
    "text": "web search fullback the other piece of langra functionality that's incredibly useful",
    "start": "1334440",
    "end": "1340840"
  },
  {
    "text": "for these kinds of workflows is persistence and that's what enables you to do things like human in the loop so",
    "start": "1340840",
    "end": "1347400"
  },
  {
    "text": "persistence basically just gives your graph memory and you can create different",
    "start": "1347400",
    "end": "1353760"
  },
  {
    "text": "types of memory here the simplest one is just in memory that's what we're going to use and you just compile the graph",
    "start": "1353760",
    "end": "1359960"
  },
  {
    "text": "differently you know with passing this checkpoint or memory object we're going to create a thread uh configuration here",
    "start": "1359960",
    "end": "1366880"
  },
  {
    "text": "that's just saying that like this is going to be our current graphic execution history if you want to start a",
    "start": "1366880",
    "end": "1371960"
  },
  {
    "text": "different one you'll just create a new thread and let's try a question again where we will we had the hallucination",
    "start": "1371960",
    "end": "1380480"
  },
  {
    "text": "before so you'll see we run the same steps we get some documents here but we",
    "start": "1380480",
    "end": "1386320"
  },
  {
    "text": "actually stop before Generation step so that's because we specified this",
    "start": "1386320",
    "end": "1391559"
  },
  {
    "text": "interrupt before and the name of the node where we want to stop so we stop before the generate node what we can do",
    "start": "1391559",
    "end": "1399000"
  },
  {
    "text": "next is actually help the model a little bit because we know for a fact none of those documents that we indexed contain",
    "start": "1399000",
    "end": "1405320"
  },
  {
    "text": "the right information to answer this we can actually inject this information on the fly so we can take the list of",
    "start": "1405320",
    "end": "1411840"
  },
  {
    "text": "documents that we retrieved and we can just overwrite the content of the first document to contain the right answer",
    "start": "1411840",
    "end": "1418279"
  },
  {
    "text": "saying to convert a column into damis use this method so we're going to do",
    "start": "1418279",
    "end": "1424200"
  },
  {
    "text": "that and we're going to call update state which is for that specific thread",
    "start": "1424200",
    "end": "1430039"
  },
  {
    "text": "in memory we're going to update the documents value and after that we can",
    "start": "1430039",
    "end": "1437960"
  },
  {
    "text": "continue is streaming from the interrupt point and see what happens",
    "start": "1437960",
    "end": "1443840"
  },
  {
    "text": "now so note that when we're doing this we're also passing empty",
    "start": "1445520",
    "end": "1450720"
  },
  {
    "text": "inputs and you see that here we got the generation step we produce the candidate answer and we passed the checks we",
    "start": "1450720",
    "end": "1457000"
  },
  {
    "text": "immediately got to finalize response so we didn't have to do any retries we didn't need to do web search fallback",
    "start": "1457000",
    "end": "1463880"
  },
  {
    "text": "and now I'd like to show you the same thing but in a deployed version on on langra cloud langra cloud is our",
    "start": "1463880",
    "end": "1470919"
  },
  {
    "text": "oneclick solution to deploy and host langra applications first off I like to show",
    "start": "1470919",
    "end": "1476919"
  },
  {
    "text": "you how implementation of the same app would look like in sort of lra cloud",
    "start": "1476919",
    "end": "1482840"
  },
  {
    "text": "friendly specification so let's head over to visual studio and you'll see",
    "start": "1482840",
    "end": "1488559"
  },
  {
    "text": "that agent file will contain exactly the same logic that I just described you",
    "start": "1488559",
    "end": "1494240"
  },
  {
    "text": "know it defines all of the necessary components it has all of the ne prompt and Grading logic we have our graph",
    "start": "1494240",
    "end": "1501279"
  },
  {
    "text": "State we have our graph configuration we have all of our nodes and edges and we",
    "start": "1501279",
    "end": "1507640"
  },
  {
    "text": "Define the graph workflow exactly the same way and the key piece here is that you need to have the variable with the",
    "start": "1507640",
    "end": "1514399"
  },
  {
    "text": "compiled graph that's what we're going to be looking for when trying to deploy the Lang graph application on L graph",
    "start": "1514399",
    "end": "1520520"
  },
  {
    "text": "cloud and the second piece you'll need is the langra Json configuration you can specify multiple",
    "start": "1520520",
    "end": "1527720"
  },
  {
    "text": "things here the the key one is pointing um the line graph deployment to the",
    "start": "1527720",
    "end": "1534279"
  },
  {
    "text": "right list of graphs to work with you can specify multiple ones here but we're just going to do one we'll call it agent",
    "start": "1534279",
    "end": "1540919"
  },
  {
    "text": "and we're going to point it to the right python file and to the variable that contains the compiled graph that's very",
    "start": "1540919",
    "end": "1547799"
  },
  {
    "text": "important then you can also specify dependencies to install I'm just saying I want to install dependencies from the",
    "start": "1547799",
    "end": "1553720"
  },
  {
    "text": "current working directory and it's going to look for p project or requirements file in this case we have a bunch of",
    "start": "1553720",
    "end": "1560919"
  },
  {
    "text": "requirements specified in poetry by project file so we're going to do that",
    "start": "1560919",
    "end": "1566000"
  },
  {
    "text": "and then uh you can also specify additional things like environment variables or files containing",
    "start": "1566000",
    "end": "1572880"
  },
  {
    "text": "environment variables Etc this is all you need and then you just need to also put all of this into a GitHub repository",
    "start": "1572880",
    "end": "1580640"
  },
  {
    "text": "that contains exactly the same structure so lra Json file and the uh",
    "start": "1580640",
    "end": "1587240"
  },
  {
    "text": "agent implementation or multiple implementations if you need and then",
    "start": "1587240",
    "end": "1592320"
  },
  {
    "text": "you'll head over to lsmith specifically we'll go to deployments page and if you",
    "start": "1592320",
    "end": "1599480"
  },
  {
    "text": "wish to create a new deployment all you're doing is specifying the reaper you'd like the name for your deployment",
    "start": "1599480",
    "end": "1606039"
  },
  {
    "text": "the configuration file that you need and you can even Define your own environment variables here and then you'll be good",
    "start": "1606039",
    "end": "1612279"
  },
  {
    "text": "to go but I'm going to show you the deployment that already exists for exactly the same application I just",
    "start": "1612279",
    "end": "1618320"
  },
  {
    "text": "showed you here so when you open that you'll see that you have a list of revisions so every time you deploy",
    "start": "1618320",
    "end": "1624840"
  },
  {
    "text": "you'll see a bunch of uh revisions reflected here you'll also get traces uh",
    "start": "1624840",
    "end": "1630399"
  },
  {
    "text": "simar to how you would normally see traces when you're working with lsmith so that allows you to actually go ahead",
    "start": "1630399",
    "end": "1635799"
  },
  {
    "text": "and inspect a particular uh L graph execution you can see all of the steps",
    "start": "1635799",
    "end": "1641000"
  },
  {
    "text": "here again like document search Generation all of our gradings in between and so on",
    "start": "1641000",
    "end": "1648399"
  },
  {
    "text": "and you can actually interact with the graph here through the deployment using Lang graph Studio this is our",
    "start": "1648399",
    "end": "1654240"
  },
  {
    "text": "interactive UI where you can um do the same runs that I just demonstrated in a",
    "start": "1654240",
    "end": "1659600"
  },
  {
    "text": "notebook but in sort of more user friendly way so you see exactly the same graph structure graph architecture that",
    "start": "1659600",
    "end": "1665799"
  },
  {
    "text": "outlined earlier and you can sort of start a new graph run by um first",
    "start": "1665799",
    "end": "1672720"
  },
  {
    "text": "specifying some inputs and then submitting them so let's try the same question which is how do I calculate",
    "start": "1672720",
    "end": "1680760"
  },
  {
    "text": "some by groups so we'll run that and you'll see that we're getting the",
    "start": "1680760",
    "end": "1686559"
  },
  {
    "text": "outputs live as the graph is streaming so we um get document search results we",
    "start": "1686559",
    "end": "1692720"
  },
  {
    "text": "get a bunch of documents retrieved here and we can inspect further if we want to um we see the candidate answer generated",
    "start": "1692720",
    "end": "1700760"
  },
  {
    "text": "we didn't need any retries so we got immediately the finalizing answer and we got our AI message here at the end so",
    "start": "1700760",
    "end": "1707799"
  },
  {
    "text": "that's a pretty simple workflow now let's try something else uh",
    "start": "1707799",
    "end": "1713760"
  },
  {
    "text": "let's try the query that was failing namely how do I convert a categorical",
    "start": "1713760",
    "end": "1719919"
  },
  {
    "text": "column into dummies what we're also going to do is to mirror that interruption in the notebook we're going",
    "start": "1719919",
    "end": "1726919"
  },
  {
    "text": "to add an interrupt here now when I run this you'll note that I stopped here and",
    "start": "1726919",
    "end": "1734919"
  },
  {
    "text": "the craft knows the next note to go to is going to be generation node but before we do that we also want to try",
    "start": "1734919",
    "end": "1741880"
  },
  {
    "text": "and edit it here similarly how we did this in a notebook so I'm just going to",
    "start": "1741880",
    "end": "1747039"
  },
  {
    "text": "copy this answer from here for convenience and I'm going to just",
    "start": "1747039",
    "end": "1752440"
  },
  {
    "text": "completely replace this document content over here now what we're going to do in the UI is we're going to create a new",
    "start": "1752440",
    "end": "1759159"
  },
  {
    "text": "Fork so that's going to create an alternative execution from that point on from the interrupt point and you see",
    "start": "1759159",
    "end": "1765760"
  },
  {
    "text": "that what we're doing here is we're generating the end answer which is the candidate answer we're doing some",
    "start": "1765760",
    "end": "1771760"
  },
  {
    "text": "grading for hallucinations and you see that instead of going to generate again",
    "start": "1771760",
    "end": "1777080"
  },
  {
    "text": "which would happen because of the hallucinations we didn't have any therefore we're going to straight to",
    "start": "1777080",
    "end": "1783480"
  },
  {
    "text": "finalize answer and return to the user so you see that we helped by injecting",
    "start": "1783480",
    "end": "1788720"
  },
  {
    "text": "this information over here um so that's a quick demo also of how you can deploy",
    "start": "1788720",
    "end": "1795480"
  },
  {
    "text": "and reproduce the same behavior that we showed in The Notebook for self-corrective rag but using L graph",
    "start": "1795480",
    "end": "1802000"
  },
  {
    "text": "cloud and that's all for today thank you so much for watching hope you've enjoyed that and let us know if you have any",
    "start": "1802000",
    "end": "1808559"
  },
  {
    "text": "questions or suggestions Below in the comments and hope to see you in the next one bye",
    "start": "1808559",
    "end": "1816200"
  }
]