[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "Hi all, Will here. Let's build a \nchatbot together with long term  ",
    "start": "80",
    "end": "3480"
  },
  {
    "text": "memory powered by semantic search. \nThis can let it remember facts and  ",
    "start": "3480",
    "end": "7040"
  },
  {
    "text": "important details from and thousands \nof conversations we might have with it.",
    "start": "7040",
    "end": "10200"
  },
  {
    "text": "By the end of this video, you \nwill have built a chatbot that  ",
    "start": "10200",
    "end": "12519"
  },
  {
    "text": "can use personalized information \nto provide great recommendations.",
    "start": "12520",
    "end": "16840"
  },
  {
    "text": "As you can see here, it remembers that I \nlike spicy food and live in San Francisco.",
    "start": "16840",
    "end": "21000"
  },
  {
    "start": "20000",
    "end": "65000"
  },
  {
    "text": "This memory is enabled by a new \nfeature we've added to LangGraph,  ",
    "start": "21000",
    "end": "24119"
  },
  {
    "text": "semantic search in the BaseStore API. Let's see \nhow it works. The two key ingredients are a store,  ",
    "start": "24120",
    "end": "30520"
  },
  {
    "text": "such as this example inMemoryStore, which runs \nephemerally in a notebook, and embeddings,  ",
    "start": "30520",
    "end": "34760"
  },
  {
    "text": "to be encoding the information that we're \ngoing to be saving. You initialize the store  ",
    "start": "34760",
    "end": "38519"
  },
  {
    "text": "here with an index configuration that specifies \nhow we're going to be indexing the documents as  ",
    "start": "38520",
    "end": "43880"
  },
  {
    "text": "well as the dimensionality next we'll put some \ninformation in. This process puts the documents  ",
    "start": "43880",
    "end": "49320"
  },
  {
    "text": "into the base store and indexes them for later \nsearch. You can then search for them using a  ",
    "start": "49320",
    "end": "54320"
  },
  {
    "text": "natural language query. As you can see here, it's \nfetched information sorted by similarity. Here,  ",
    "start": "54320",
    "end": "60080"
  },
  {
    "text": "Italian food is the most similar to food, pizza \nis also similar, plumber is a little less so.",
    "start": "60080",
    "end": "65239"
  },
  {
    "start": "65000",
    "end": "91000"
  },
  {
    "text": "To use this in your LangGraph agent,  ",
    "start": "65240",
    "end": "67479"
  },
  {
    "text": "simply add this store argument to any node \nin your graph. The store that you provide  ",
    "start": "67480",
    "end": "72320"
  },
  {
    "text": "when you compile your graph will then be \naccessible from any node in the agent.",
    "start": "72320",
    "end": "77160"
  },
  {
    "text": "Here we see the agent is able \nto search for the information  ",
    "start": "77160",
    "end": "79680"
  },
  {
    "text": "and give a recommendation based \non the documents we saved above.",
    "start": "79680",
    "end": "83680"
  },
  {
    "text": "If you're curious, you can also use this in \nCreate React Agent. You can add it in tools,  ",
    "start": "83680",
    "end": "88680"
  },
  {
    "text": "or in any given preparation there, \nand other nodes in your graph.",
    "start": "88680",
    "end": "91640"
  },
  {
    "start": "91000",
    "end": "108000"
  },
  {
    "text": "So to review, there's three ingredients \nneeded in order to enable semantic search  ",
    "start": "91640",
    "end": "95040"
  },
  {
    "text": "in your LangGraph instance. One, a \nstore, which is available by default  ",
    "start": "95040",
    "end": "99400"
  },
  {
    "text": "in any LangGraph platform deployment. Two, \nan index configuration with embeddings to  ",
    "start": "99400",
    "end": "104480"
  },
  {
    "text": "specify that you would like to index all \nthe information that you're putting in.",
    "start": "104480",
    "end": "108000"
  },
  {
    "start": "108000",
    "end": "115000"
  },
  {
    "text": "And three, modify your nodes so that \nthey'll accept the base store so you  ",
    "start": "108000",
    "end": "111080"
  },
  {
    "text": "can use it to put. And search for \ninformation whenever you'd like.",
    "start": "111080",
    "end": "115520"
  },
  {
    "start": "115000",
    "end": "151000"
  },
  {
    "text": "Let's apply this to an end to end application.",
    "start": "115520",
    "end": "118399"
  },
  {
    "text": "First, we're going to open our terminal  ",
    "start": "118400",
    "end": "120080"
  },
  {
    "text": "and use the LangGraph CLI to \nclone the memory agent template.",
    "start": "120080",
    "end": "123280"
  },
  {
    "text": "We'll step into the repository,",
    "start": "123280",
    "end": "125760"
  },
  {
    "text": "Install dependencies and then start the server",
    "start": "125760",
    "end": "131000"
  },
  {
    "text": "this will launch the application we saw \nat the beginning of this tutorial. If  ",
    "start": "131000",
    "end": "134760"
  },
  {
    "text": "you've set up everything correctly, \nyou'll have an agent that's able to  ",
    "start": "134760",
    "end": "137760"
  },
  {
    "text": "use tools in order to save memories and \nthen recall them in later conversations.",
    "start": "137760",
    "end": "142360"
  },
  {
    "text": "Let's check out the code to see how we set it \nup in the LangGraph Platform. If you recall,  ",
    "start": "142360",
    "end": "146240"
  },
  {
    "text": "there's three ingredients. Store configuration \nwith embeddings and accepting it in my graph.",
    "start": "146240",
    "end": "151280"
  },
  {
    "start": "151000",
    "end": "206000"
  },
  {
    "text": "First, store configuration. When deploying \nthings to the LangGraph platform,  ",
    "start": "151280",
    "end": "155280"
  },
  {
    "text": "we define most of our configuration in \nthis LangGraph. json file. This is where  ",
    "start": "155280",
    "end": "159520"
  },
  {
    "text": "you'll see the pointers to our actual graph \nimplementation, our agent implementation,  ",
    "start": "159520",
    "end": "163920"
  },
  {
    "text": "environment, as well as other \ntypes of dependency information.",
    "start": "163920",
    "end": "167520"
  },
  {
    "text": "Finally, we have this store configuration. If \nI provide an index with an embedding instance,  ",
    "start": "167520",
    "end": "173440"
  },
  {
    "text": "this will let us actually instruct \nthe line graph platform to prepare  ",
    "start": "173440",
    "end": "177240"
  },
  {
    "text": "the database for vector search, so that we \ncan add semantic search to our store. Here,  ",
    "start": "177240",
    "end": "182240"
  },
  {
    "text": "I'm using this concise syntax to specify that I'll \nbe using OpenAI's text embedding 3 small model.",
    "start": "182240",
    "end": "188760"
  },
  {
    "text": "This requires LangChain to use. Alternatively,  ",
    "start": "188760",
    "end": "191640"
  },
  {
    "text": "I can point it to a custom file so that I can \ndefine my embeddings as a custom function.",
    "start": "191640",
    "end": "195880"
  },
  {
    "text": "So we've covered the first two ingredients. \nWe've set up the configuration for the store  ",
    "start": "195880",
    "end": "199000"
  },
  {
    "text": "and added embeddings. Now we just \nneed to use it in our graph. If  ",
    "start": "199000",
    "end": "202360"
  },
  {
    "text": "you recall from the visualization of \nthe app, our graph has two main nodes.",
    "start": "202360",
    "end": "206320"
  },
  {
    "start": "206000",
    "end": "294000"
  },
  {
    "text": "The call model node prompts the LLM with \nthe information of the conversation.  ",
    "start": "206320",
    "end": "210120"
  },
  {
    "text": "Thus far, we've added a store dot \nsearch method in order to search  ",
    "start": "210120",
    "end": "214080"
  },
  {
    "text": "for relevant content and template that into \nthe prompt that we send to the LLM. Second,  ",
    "start": "214080",
    "end": "219360"
  },
  {
    "text": "we provide the LLM with a single tool. This lets \nit optionally call, the tool to save information  ",
    "start": "219360",
    "end": "225560"
  },
  {
    "text": "if the conversation brings up important \nfacts that it wants to save for later.",
    "start": "225560",
    "end": "229959"
  },
  {
    "text": "This is going to be executed in the \nStoreMemories node. Let's see what  ",
    "start": "229960",
    "end": "233280"
  },
  {
    "text": "that looks like in code. Here we have this \ngraph file that defines the main nodes,  ",
    "start": "233280",
    "end": "237800"
  },
  {
    "text": "as well as the, actual graph. We'll \nfirst look at the call model node.",
    "start": "237800",
    "end": "242200"
  },
  {
    "text": "You'll see here, we take the most recent \nmessages from the context and format that  ",
    "start": "242200",
    "end": "246080"
  },
  {
    "text": "as the query. This is a simple, fast \nway in order to be looking at things  ",
    "start": "246080",
    "end": "249920"
  },
  {
    "text": "that are semantically similar to whatever \nis happening in the conversation right now.  ",
    "start": "249920",
    "end": "254200"
  },
  {
    "text": "These memories are fetched, and if there \nare anything that matches, these will be  ",
    "start": "254200",
    "end": "258440"
  },
  {
    "text": "formatted as a string and then passed into \nthe model that we're going to call here.",
    "start": "258440",
    "end": "263440"
  },
  {
    "text": "After the call model node, we \nhave a conditional edge. The  ",
    "start": "263440",
    "end": "266720"
  },
  {
    "text": "route messages decides if we have any tool calls,  ",
    "start": "266720",
    "end": "269440"
  },
  {
    "text": "this is going to go to the store memory node. \nOtherwise, we'll return directly to the user.",
    "start": "269440",
    "end": "275560"
  },
  {
    "text": "The store memory node calls this tool that \nwe've provided to the LLM. This tool also  ",
    "start": "275560",
    "end": "281560"
  },
  {
    "text": "uses the store in a different way. The LLM \npopulates arguments for content and context,  ",
    "start": "281560",
    "end": "286440"
  },
  {
    "text": "which are both going to be stored as the document.",
    "start": "286440",
    "end": "289360"
  },
  {
    "text": "And LangGraph provides information about \nthe store as well as the configuration.",
    "start": "289360",
    "end": "294199"
  },
  {
    "start": "294000",
    "end": "369000"
  },
  {
    "text": "We separate memories by user ID so that the LLM \ndoesn't mix up information that it's stored from  ",
    "start": "294200",
    "end": "300360"
  },
  {
    "text": "different users who are interacting with it. \nAnd we save them all with a unique identifier.  ",
    "start": "300360",
    "end": "304280"
  },
  {
    "text": "This identifier is used so that the LLM can \nactually update existing memories if it sees fit.",
    "start": "304280",
    "end": "310200"
  },
  {
    "text": "By default, these memories will all be indexed.  ",
    "start": "310200",
    "end": "312880"
  },
  {
    "text": "According to whatever configuration I've \ndefined in the LangGraph dot json file,",
    "start": "312880",
    "end": "316720"
  },
  {
    "text": "Which by default is the whole object itself \nsymbolized by this dollar sign. Alternatively,  ",
    "start": "316720",
    "end": "322480"
  },
  {
    "text": "I can add specific fields to the \nembedding for multivector search.",
    "start": "322480",
    "end": "326800"
  },
  {
    "text": "I can also dynamically specify these at put \ntime whenever I'm inserting new memories.",
    "start": "326800",
    "end": "332159"
  },
  {
    "text": "This tells LangGraph, only generate an embedding \nfor the content field within this object.",
    "start": "332160",
    "end": "336960"
  },
  {
    "text": "If I want to specify that I don't want \nan object to be indexed for search,  ",
    "start": "336960",
    "end": "340880"
  },
  {
    "text": "perhaps I just don't want it showing up \nhigh on the list whenever I'm searching  ",
    "start": "340880",
    "end": "343800"
  },
  {
    "text": "for values by semantic similarity, I \ncan specify index equals false. Note  ",
    "start": "343800",
    "end": "349680"
  },
  {
    "text": "that if we haven't satisfied the first two \ningredients for setting up semantic search,  ",
    "start": "349680",
    "end": "353440"
  },
  {
    "text": "namely the index configuration with embeddings, \nall of these arguments will be ignored.",
    "start": "353440",
    "end": "358280"
  },
  {
    "text": "Every object will be provided in the \nstore, but it won't be retrievable by,  ",
    "start": "358280",
    "end": "361960"
  },
  {
    "text": "query all of the memories will \nstill be inserted to the store,  ",
    "start": "361960",
    "end": "364680"
  },
  {
    "text": "but no embeddings will be provided, so we can't \nbe searching by natural language query. Thank you.",
    "start": "364680",
    "end": "369240"
  },
  {
    "start": "369000",
    "end": "417000"
  },
  {
    "text": "And that's all you need.",
    "start": "369240",
    "end": "370120"
  },
  {
    "text": "If you want to dive deeper, I'd suggest \nchecking out the documentation. We have  ",
    "start": "370120",
    "end": "373680"
  },
  {
    "text": "documents on how to add semantic search for \nlong term memory that shows how to implement  ",
    "start": "373680",
    "end": "377320"
  },
  {
    "text": "it in LangGraph OSS. We also have an example \nof how to configure it in LangGraph Platform,  ",
    "start": "377320",
    "end": "381640"
  },
  {
    "text": "which walks you through similar steps to what \nwe just did in this video. This also shows you  ",
    "start": "381640",
    "end": "385160"
  },
  {
    "text": "how to specify custom embeddings if you want \nto define embeddings as a custom function.",
    "start": "385160",
    "end": "389960"
  },
  {
    "text": "Finally, the base store reference documents also \nprovide a lot more information if you want more  ",
    "start": "389960",
    "end": "394880"
  },
  {
    "text": "details about what the index argument \nmeans and what the behavior is there.",
    "start": "394880",
    "end": "400040"
  },
  {
    "text": "Currently, Semantic Search is supported in the \nInMemory store as well as the Postgres stores,  ",
    "start": "400040",
    "end": "404280"
  },
  {
    "text": "which is provided with every LangGraph deployment.",
    "start": "404280",
    "end": "407280"
  },
  {
    "text": "And that's all we have for today. If you have any \nquestions or comments, please leave them below,  ",
    "start": "407280",
    "end": "411560"
  },
  {
    "text": "or open an issue or discussion in the LangGraph \nrepository. Thanks again, and see you next time.",
    "start": "411560",
    "end": "416320"
  }
]