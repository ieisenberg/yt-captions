[
  {
    "text": "hello and welcome so today is a big day at Ling chain today we have G linki so",
    "start": "1480",
    "end": "7839"
  },
  {
    "text": "thank you all for coming to join uh my name is Julia shottenstein I lead go to market here and I'm joined by anush",
    "start": "7839",
    "end": "14519"
  },
  {
    "text": "who's co-founder of linkchain and and today anush is going to be demoing and showing off all the new",
    "start": "14519",
    "end": "21359"
  },
  {
    "text": "capabilities of Link Smith and we really thought about this product to be helpful",
    "start": "21359",
    "end": "27599"
  },
  {
    "text": "in All Phases of your development life cycle so specifically we'll spend some time on what it's like to prototype your",
    "start": "27599",
    "end": "33760"
  },
  {
    "text": "app how link Smith can help how get into beta phase uh link Smith will also be",
    "start": "33760",
    "end": "39160"
  },
  {
    "text": "useful and then hopefully when you hit the promis land of GA link Smith will get you there too even faster than you",
    "start": "39160",
    "end": "45559"
  },
  {
    "text": "could without it so just some Rules of Engagement feel free to ask questions in",
    "start": "45559",
    "end": "51920"
  },
  {
    "text": "the chat and I will read them out loud and interrupt en Kush at appropriate",
    "start": "51920",
    "end": "57039"
  },
  {
    "text": "times but we want to make this interactive so so thank you all for joining and with that I'll turn it over",
    "start": "57039",
    "end": "63000"
  },
  {
    "text": "to anos to say hello hey everyone I'm anos co-founder of Lang chain um really excited to be",
    "start": "63000",
    "end": "69280"
  },
  {
    "text": "here and demo lsmith for you all um and yeah if uh no other intros needed I'm",
    "start": "69280",
    "end": "77640"
  },
  {
    "text": "happy to get started and dive right right into",
    "start": "77640",
    "end": "81720"
  },
  {
    "text": "it okay awesome yeah so uh today we're",
    "start": "88840",
    "end": "94360"
  },
  {
    "text": "super excited to launch the general availability of lsmith um as most of you",
    "start": "94360",
    "end": "100560"
  },
  {
    "text": "of you are aware lsmith is our platform for L application development monitoring",
    "start": "100560",
    "end": "105680"
  },
  {
    "text": "and testing uh here I'll give a quick overview of lsmith and dive into all the",
    "start": "105680",
    "end": "113119"
  },
  {
    "text": "features that lsmith supports um and how they relate to each phase of the LM",
    "start": "113119",
    "end": "118560"
  },
  {
    "text": "application development life cycle specifically prototyping beta testing and even production so uh with that",
    "start": "118560",
    "end": "125799"
  },
  {
    "text": "being said here I've logged into Lang Smith um I am a part of the Lang chain",
    "start": "125799",
    "end": "131400"
  },
  {
    "text": "organization um and I can see a few different sections on the homepage I see projects I see data sets and testing",
    "start": "131400",
    "end": "137840"
  },
  {
    "text": "annotation cues in the hub I'll first dive in projects uh I'll preface this by",
    "start": "137840",
    "end": "143360"
  },
  {
    "text": "saying uh projects are nothing more than collections of traces that you're sending to Langs Smith and uh usually uh",
    "start": "143360",
    "end": "151319"
  },
  {
    "text": "what we see people do is map a single application to a single project so you",
    "start": "151319",
    "end": "156879"
  },
  {
    "text": "can have each project represent or uh be a collection of traces for specific",
    "start": "156879",
    "end": "164280"
  },
  {
    "text": "applications so here I can see all the all the projects that I have um and the",
    "start": "164280",
    "end": "169840"
  },
  {
    "text": "statistics that are uh associated with each project in each row I'll dive into the chat Lang train project um some of",
    "start": "169840",
    "end": "177120"
  },
  {
    "text": "you might be aware that we have an an application um that's deployed on chat.",
    "start": "177120",
    "end": "184400"
  },
  {
    "text": "chain.com it's an llm powered application built with built with Lang chain um that is designed to answer",
    "start": "184400",
    "end": "192519"
  },
  {
    "text": "questions about uh uh Lang Chain's python documentation and so you can ask",
    "start": "192519",
    "end": "198879"
  },
  {
    "text": "it questions you can submit feedback um and uh you know uh get your questions",
    "start": "198879",
    "end": "204720"
  },
  {
    "text": "answered about about linkedin's pth documentation and we have all the tra is",
    "start": "204720",
    "end": "210680"
  },
  {
    "text": "uh set up to uh log to Langs Smith with under the chat Lang chain project so",
    "start": "210680",
    "end": "217000"
  },
  {
    "text": "here I'll just highlight a quick uh a few quick um attributes that you can see",
    "start": "217000",
    "end": "222159"
  },
  {
    "text": "as part of the project so um you know you can see the total run count for the past seven days which is the default",
    "start": "222159",
    "end": "227760"
  },
  {
    "text": "time period you can see uh total tokens um the cost associated with the tokens",
    "start": "227760",
    "end": "233400"
  },
  {
    "text": "so we have a token based cost tracking currently this is only available for open AI models but uh support for",
    "start": "233400",
    "end": "240280"
  },
  {
    "text": "uh all of the other models is on the road map for the near future uh we have median tokens uh some",
    "start": "240280",
    "end": "248239"
  },
  {
    "text": "statistics about error rates and streaming latency uh time to First token",
    "start": "248239",
    "end": "253840"
  },
  {
    "text": "and then we have some filter shortcuts that allow you to filter by a number of different attributes which I can dive",
    "start": "253840",
    "end": "259320"
  },
  {
    "text": "into later so um what we can do is uh expand a Trace by uh clicking on a",
    "start": "259320",
    "end": "268199"
  },
  {
    "text": "single Row in the uh in the project Details page so a trace is an endtoend",
    "start": "268199",
    "end": "275360"
  },
  {
    "text": "invocation of your application so uh in",
    "start": "275360",
    "end": "280400"
  },
  {
    "text": "this case we're looking at a trace for the llm pipeline that backs chat link chain and you can see all the different",
    "start": "280400",
    "end": "286880"
  },
  {
    "text": "steps that the system takes to arrive at the final answer so at the top level you'll see the uh highle input and the",
    "start": "286880",
    "end": "294680"
  },
  {
    "text": "final output but if you dive into um any of the uh intermediate steps you can see",
    "start": "294680",
    "end": "301360"
  },
  {
    "text": "exactly what happens at that level I'll dive into the most interesting ones so I'll dive into the retriever step here",
    "start": "301360",
    "end": "308360"
  },
  {
    "text": "in this case um we are doing a retrieval step we're going into an index and",
    "start": "308360",
    "end": "313600"
  },
  {
    "text": "retrieving documents based on a query how do we use a recursive URL loader to load content from a page and then we",
    "start": "313600",
    "end": "320360"
  },
  {
    "text": "have a generation step that uses um an llm uh so here uh if you look at",
    "start": "320360",
    "end": "326800"
  },
  {
    "text": "metadata you can look you can see all of the uh different um invocation prams and",
    "start": "326800",
    "end": "334360"
  },
  {
    "text": "like the model name um and other helpful information like your runtime",
    "start": "334360",
    "end": "339639"
  },
  {
    "text": "information and you can um inspect the raw prompt so here we see like the",
    "start": "339639",
    "end": "346919"
  },
  {
    "text": "system message um we can look at the human message and the final AI",
    "start": "346919",
    "end": "353759"
  },
  {
    "text": "output if we did have feedback for this run uh we would see it here um but we",
    "start": "354800",
    "end": "360240"
  },
  {
    "text": "don't um I'll actually dive into feedback right now um",
    "start": "360240",
    "end": "366440"
  },
  {
    "text": "so when you uh when you submit a thumbs up or thumbs down score with chat Lang",
    "start": "366440",
    "end": "372599"
  },
  {
    "text": "chain we actually have that wired up to a a specific feedback key called user score that shows up in Langs Smith so",
    "start": "372599",
    "end": "380800"
  },
  {
    "text": "when we filter by uh feedback key we see all of the traces or all of the",
    "start": "380800",
    "end": "387080"
  },
  {
    "text": "application indications that result in a positive user score um and we could you",
    "start": "387080",
    "end": "393599"
  },
  {
    "text": "know obviously like use the same principle to filter for traces that have a negative feed uh user",
    "start": "393599",
    "end": "400039"
  },
  {
    "text": "score this is really useful uh when it comes time to understand how your",
    "start": "400039",
    "end": "405199"
  },
  {
    "text": "application is doing in real world scenarios and it can also help you",
    "start": "405199",
    "end": "410400"
  },
  {
    "text": "construct data sets that allow you to test uh future versions of your",
    "start": "410400",
    "end": "416479"
  },
  {
    "text": "application so um before I go into data sets and testing I'll just highlight a",
    "start": "416479",
    "end": "422599"
  },
  {
    "text": "few more things in the project Details page so we had um talked about the trace",
    "start": "422599",
    "end": "430400"
  },
  {
    "text": "um we talked about feedback we talked about metadata one thing that I'll highlight",
    "start": "430400",
    "end": "435919"
  },
  {
    "text": "is we also have a tab that allows you to see only uh llm calls isolated so if",
    "start": "435919",
    "end": "442479"
  },
  {
    "text": "you're only interested in the Raw prompts and the generations you can find all of these in the llm calls tab we",
    "start": "442479",
    "end": "450000"
  },
  {
    "text": "also have a monitoring section and this allows you to see different metrics for your project over time so you can see uh",
    "start": "450000",
    "end": "457720"
  },
  {
    "text": "you know the trace count uh over time you can see the llm call count uh a",
    "start": "457720",
    "end": "462960"
  },
  {
    "text": "bunch of other helpful uh metrics llm latency tokens per second uh you'll see",
    "start": "462960",
    "end": "468639"
  },
  {
    "text": "feedback metrics here um and Yep this is",
    "start": "468639",
    "end": "474840"
  },
  {
    "text": "um you know this is what you'll find in the monitoring section you can also Group by",
    "start": "474840",
    "end": "480080"
  },
  {
    "text": "metadata and this allows you to look at different versions of your application",
    "start": "480080",
    "end": "487199"
  },
  {
    "text": "and how they're performing side by side one other one other thing I'll",
    "start": "487199",
    "end": "492560"
  },
  {
    "text": "highlight is it's super easy to get ones the visibility was was really clean what",
    "start": "492560",
    "end": "499360"
  },
  {
    "text": "if I'm not using linkchain as the underlying Logic for my application how my traces get",
    "start": "499360",
    "end": "506120"
  },
  {
    "text": "displayed yeah that's that's a good question so they get displayed exactly the same way um you can use um uh either",
    "start": "506120",
    "end": "514719"
  },
  {
    "text": "Langs Smith's python SDK Lang Smith's typescript SDK or even our API to log",
    "start": "514719",
    "end": "521680"
  },
  {
    "text": "traces to lsmith and um you can have the traces show up the exact same way",
    "start": "521680",
    "end": "528320"
  },
  {
    "text": "whether or not you're using L chain so you have a lot of control over um you know how how you can send traces up to",
    "start": "528320",
    "end": "535080"
  },
  {
    "text": "the system and how they display as well cool um one thing I'll mention I guess",
    "start": "535080",
    "end": "543000"
  },
  {
    "text": "like one thing that's important to talk about is is why is this important like why why is it even useful to have",
    "start": "543000",
    "end": "548640"
  },
  {
    "text": "tracing and and and how how does it help well when you're starting off um",
    "start": "548640",
    "end": "554160"
  },
  {
    "text": "prototyping an llm application you can run into a lot of like really frustrating issues you can run into um",
    "start": "554160",
    "end": "561760"
  },
  {
    "text": "you know infinite Loops if you're using an gentic workflow you can run into issues where um your LM pipeline is used",
    "start": "561760",
    "end": "569160"
  },
  {
    "text": "using more tokens than expected um and then you can also just in general be",
    "start": "569160",
    "end": "575240"
  },
  {
    "text": "seeing uh very uh wrong or concerning output at the end of your pipeline and",
    "start": "575240",
    "end": "581320"
  },
  {
    "text": "you can even isolate these outputs by doing what we did before by by filtering",
    "start": "581320",
    "end": "586959"
  },
  {
    "text": "for uh traces that have bad feedback associated with them and so often times",
    "start": "586959",
    "end": "594240"
  },
  {
    "text": "when you have these issues it's really hard to root cause them if you can't see what's happen happening in your lmm",
    "start": "594240",
    "end": "600480"
  },
  {
    "text": "pipeline step by step so by doing so you can really get a high like a like a",
    "start": "600480",
    "end": "607200"
  },
  {
    "text": "close understanding of the inputs and outputs at each step and isolate any",
    "start": "607200",
    "end": "613480"
  },
  {
    "text": "issues that you see with the final",
    "start": "613480",
    "end": "617279"
  },
  {
    "text": "response so one thing that I alluded to earlier is",
    "start": "618519",
    "end": "623760"
  },
  {
    "text": "um uh I guess before I dive into that I think it would be helpful to show case",
    "start": "623760",
    "end": "630040"
  },
  {
    "text": "uh the the the prompt playground so one other thing that's useful when you're",
    "start": "630040",
    "end": "635680"
  },
  {
    "text": "prototyping your application is rapid iteration and Rapid",
    "start": "635680",
    "end": "641240"
  },
  {
    "text": "experimentation so let's say you know you get this you're at this llm call and then you see like the system message",
    "start": "641240",
    "end": "647560"
  },
  {
    "text": "human message the output and um you know let's say you want to",
    "start": "647560",
    "end": "654040"
  },
  {
    "text": "um you know mess around with um The Prompt or other",
    "start": "654040",
    "end": "659639"
  },
  {
    "text": "parameters associated with the application um okay I guess in this case",
    "start": "659639",
    "end": "664800"
  },
  {
    "text": "we hit a token limit um so let me try to open up another",
    "start": "664800",
    "end": "670560"
  },
  {
    "text": "trace a good",
    "start": "675200",
    "end": "678519"
  },
  {
    "text": "one yeah so you can um actually edit your prompt and edit any parameters",
    "start": "683200",
    "end": "689560"
  },
  {
    "text": "uh that are associated with uh the model and then you can see how they affect the generation so this is really this",
    "start": "689560",
    "end": "695320"
  },
  {
    "text": "playground environment that we have here is super useful for Rapid iteration and rapid",
    "start": "695320",
    "end": "700959"
  },
  {
    "text": "prototyping we have a number of different models available in the playground and actually two of them are free so we offer uh Google pal and",
    "start": "700959",
    "end": "708000"
  },
  {
    "text": "fireworks for free um and we have uh a number of different model types within",
    "start": "708000",
    "end": "714240"
  },
  {
    "text": "each provider if you're using uh Azure open AI uh we support that as",
    "start": "714240",
    "end": "723240"
  },
  {
    "text": "well hey Anish I'm seeing some tools in the playground do you want to talk a little bit about how tools can be",
    "start": "723399",
    "end": "730959"
  },
  {
    "text": "used yeah definitely so um when okay so I can let me back up a",
    "start": "730959",
    "end": "738760"
  },
  {
    "text": "bit here when you're using um uh I guess if you're building like an",
    "start": "738760",
    "end": "745760"
  },
  {
    "text": "agentic system that relies on your model uh uh using",
    "start": "745760",
    "end": "753079"
  },
  {
    "text": "like function calls uh returning function calls you can also play around with that in the playground so basically",
    "start": "753079",
    "end": "760639"
  },
  {
    "text": "if you have an agenting system and you supply the model with uh some functions",
    "start": "760639",
    "end": "767720"
  },
  {
    "text": "so a number of uh models now like support function calling you can um uh",
    "start": "767720",
    "end": "773839"
  },
  {
    "text": "you know the model will output the function and the arguments that it wants to invoke and and uh this is also",
    "start": "773839",
    "end": "779959"
  },
  {
    "text": "supported in the playground",
    "start": "779959",
    "end": "783199"
  },
  {
    "text": "environment cool um should I move on to data sets and testing now any other",
    "start": "788240",
    "end": "796240"
  },
  {
    "text": "questions yeah we had one question in the chat about whether we support Azure open AI in the playground and the answer",
    "start": "796240",
    "end": "803279"
  },
  {
    "text": "is yes we do you just have to supply your own proxy and token but we have a lot of varability there",
    "start": "803279",
    "end": "810760"
  },
  {
    "text": "cool awesome okay uh so let me dive into the data sets and testing",
    "start": "815959",
    "end": "822480"
  },
  {
    "text": "section so uh let me preface this and ground this a bit uh let me first",
    "start": "822480",
    "end": "830120"
  },
  {
    "text": "find examples so often times when you're building an llm application you'll want",
    "start": "831320",
    "end": "838519"
  },
  {
    "text": "to to um not only rely on Vibe tracks before",
    "start": "838519",
    "end": "844399"
  },
  {
    "text": "you deploy it to an initial set of test users you'll want to create a",
    "start": "844399",
    "end": "849440"
  },
  {
    "text": "benchmarking set and then run some tests against uh the benchmarking set",
    "start": "849440",
    "end": "854959"
  },
  {
    "text": "that you've created so we've followed like a similar idea here with this example so we have a data set here",
    "start": "854959",
    "end": "862120"
  },
  {
    "text": "called chat Lang chain complex questions and then this data set has a list of",
    "start": "862120",
    "end": "867600"
  },
  {
    "text": "examples associated with it each example is nothing more than an input and a reference",
    "start": "867600",
    "end": "873440"
  },
  {
    "text": "output uh that you can use to uh ground the results of your llm",
    "start": "873440",
    "end": "881040"
  },
  {
    "text": "pipeline so once you've assembled a list of data sets sorry a list of examples in",
    "start": "881160",
    "end": "886519"
  },
  {
    "text": "a data set you can uh run tests against the",
    "start": "886519",
    "end": "891959"
  },
  {
    "text": "data set and so we offer the capability to do this with the Python and the types",
    "start": "891959",
    "end": "899240"
  },
  {
    "text": "SDK uh there's also a way to do that uh with the API and once you've run a series of",
    "start": "899240",
    "end": "906199"
  },
  {
    "text": "tests over a data set you'll see all the test results in the test section of that",
    "start": "906199",
    "end": "912720"
  },
  {
    "text": "data set we offer a uh chart that shows",
    "start": "912720",
    "end": "918440"
  },
  {
    "text": "different metrics for all of your test results you can easily see how your test results are performing relative to each",
    "start": "918440",
    "end": "924759"
  },
  {
    "text": "other and we made this pretty flexible so each test can be um you know you",
    "start": "924759",
    "end": "930319"
  },
  {
    "text": "could label it according to something that's useful for you to understand like at a high level what",
    "start": "930319",
    "end": "936440"
  },
  {
    "text": "that what that um uh I guess like what's specific to that test result these can",
    "start": "936440",
    "end": "942279"
  },
  {
    "text": "be GitHub revisions uh for an llm pipeline you can also tag it with commit",
    "start": "942279",
    "end": "948639"
  },
  {
    "text": "commit messages and things like that and you can also see a bunch of different",
    "start": "948639",
    "end": "954120"
  },
  {
    "text": "metrics in this case uh we're looking at correctness here and so when you run a",
    "start": "954120",
    "end": "961079"
  },
  {
    "text": "uh when you run a test what you can do with the Python and the typescript SDK",
    "start": "961079",
    "end": "966759"
  },
  {
    "text": "is configure a set of evaluators to score the results of your tests these uh",
    "start": "966759",
    "end": "972759"
  },
  {
    "text": "can be a combination of llm orisic based evaluators we offer a number of",
    "start": "972759",
    "end": "979480"
  },
  {
    "text": "evaluators in uh the Lang chain library that you can use off the shelf these are",
    "start": "979480",
    "end": "985120"
  },
  {
    "text": "evaluators that can score the results of your tests on a number of CR criteria such as correctness um I think we have a",
    "start": "985120",
    "end": "992360"
  },
  {
    "text": "vagueness criteria uh evaluator helpfulness criteria evaluator",
    "start": "992360",
    "end": "997959"
  },
  {
    "text": "things like that um and obviously as I mentioned earlier this is completely configurable so um all you need to do is",
    "start": "997959",
    "end": "1005079"
  },
  {
    "text": "write a function that takes in an input uh an output and a reference output and",
    "start": "1005079",
    "end": "1010600"
  },
  {
    "text": "then submits a score based on those inputs and then the results will show up",
    "start": "1010600",
    "end": "1016399"
  },
  {
    "text": "in the data sets and testing section uh for that for that test",
    "start": "1016399",
    "end": "1023079"
  },
  {
    "text": "result you can dive in and look at an individual test result um that this",
    "start": "1024000",
    "end": "1030319"
  },
  {
    "text": "gives you a good uh overview of the input the reference output and the actual output of your",
    "start": "1030319",
    "end": "1037400"
  },
  {
    "text": "application you can also open up a test run comparison view that allows you to see the results",
    "start": "1037400",
    "end": "1044760"
  },
  {
    "text": "of multiple tests side by side so this is really useful if you're uh if you",
    "start": "1044760",
    "end": "1051799"
  },
  {
    "text": "want to track regressions across multiple revisions of your application we've also added some charts to get a",
    "start": "1051799",
    "end": "1059039"
  },
  {
    "text": "quick overview about how your tests are performing um this is completely configurable in this case we just have",
    "start": "1059039",
    "end": "1065120"
  },
  {
    "text": "one feedback tag associated with um you know with this chart uh but if you have",
    "start": "1065120",
    "end": "1071840"
  },
  {
    "text": "multiple feedback tags that are associated with different evaluators you'll see the results of those here we",
    "start": "1071840",
    "end": "1078760"
  },
  {
    "text": "can also look at the latency um the token usage and then uh we can also we have these distribution charts as well",
    "start": "1078760",
    "end": "1085640"
  },
  {
    "text": "that allow you to see um uh how the different test results",
    "start": "1085640",
    "end": "1091480"
  },
  {
    "text": "are stacking alongside each other hey anos how would you make",
    "start": "1091480",
    "end": "1098919"
  },
  {
    "text": "recommendation for folks on reading eals using off the shelf evaluators that we",
    "start": "1098919",
    "end": "1103960"
  },
  {
    "text": "have in link Smith versus custom evaluators that they can also craft any guidance you want to",
    "start": "1103960",
    "end": "1110720"
  },
  {
    "text": "share yeah that's a good question so I think one thing that I'll point out is",
    "start": "1110720",
    "end": "1115760"
  },
  {
    "text": "that evaluation is very use case specific and",
    "start": "1115760",
    "end": "1121640"
  },
  {
    "text": "so often times off the shelf evaluators are a good starting point but they",
    "start": "1121640",
    "end": "1127760"
  },
  {
    "text": "likely won't be enough for you to get an indepth understanding about",
    "start": "1127760",
    "end": "1134919"
  },
  {
    "text": "how the application is performing relative to criteria that's important to you and your organization so one of the",
    "start": "1134919",
    "end": "1141840"
  },
  {
    "text": "benefits uh one of the key benefits of having um your tracing system and your",
    "start": "1141840",
    "end": "1148720"
  },
  {
    "text": "evaluation system in the same place which is what length Smith offers is is is it allows you to grow your data sets",
    "start": "1148720",
    "end": "1156240"
  },
  {
    "text": "your benchmarking data sets over time with real data so when you initially",
    "start": "1156240",
    "end": "1162520"
  },
  {
    "text": "deploy your application to a set of test users you might only be testing your",
    "start": "1162520",
    "end": "1169440"
  },
  {
    "text": "application on a few examples that you've come up with your head or a few",
    "start": "1169440",
    "end": "1175240"
  },
  {
    "text": "examples that you've seen while running your application however when you submit your",
    "start": "1175240",
    "end": "1180400"
  },
  {
    "text": "application to initial set of beta testers you're you're getting a really",
    "start": "1180400",
    "end": "1186840"
  },
  {
    "text": "um uh good idea of how your application is performing in real world scenarios so",
    "start": "1186840",
    "end": "1193000"
  },
  {
    "text": "a common workflow we see is for users to go in and drill down into traces that",
    "start": "1193000",
    "end": "1199600"
  },
  {
    "text": "receive either very good feedback uh from the from the from users or really",
    "start": "1199600",
    "end": "1204720"
  },
  {
    "text": "poor feedback from the users and this can be something that is wired up to a",
    "start": "1204720",
    "end": "1209960"
  },
  {
    "text": "thumbs up thumbs down button or a comment box or something similar and",
    "start": "1209960",
    "end": "1216600"
  },
  {
    "text": "then it helps the user draw attention to the most interesting data points uh from",
    "start": "1216600",
    "end": "1224000"
  },
  {
    "text": "what they've collected um by rolling out to an initial set of test users and then you start to develop a an intuition for",
    "start": "1224000",
    "end": "1232120"
  },
  {
    "text": "how your application is breaking down on certain inputs and uh how it's breaking",
    "start": "1232120",
    "end": "1238200"
  },
  {
    "text": "down and this allows you to as I as I mentioned earlier not only craft",
    "start": "1238200",
    "end": "1245480"
  },
  {
    "text": "examples that you can use to test further versions of your application but also gain an intuition for what heris",
    "start": "1245480",
    "end": "1251320"
  },
  {
    "text": "sixs or what LM based evals you want to run in the future to defend against uh",
    "start": "1251320",
    "end": "1257000"
  },
  {
    "text": "the problems that you're seeing in that specific version um I don't know if that's helpful or if that",
    "start": "1257000",
    "end": "1262960"
  },
  {
    "text": "specifically the question but that's kind of how I think about evaluation with link",
    "start": "1262960",
    "end": "1269120"
  },
  {
    "text": "Smith great yeah that helps it and how are people using how do you see people doing evaluation if they're not using",
    "start": "1269120",
    "end": "1275480"
  },
  {
    "text": "link Smith like what's maybe the before State yeah we've seen a lot of",
    "start": "1275480",
    "end": "1281480"
  },
  {
    "text": "teams run uh you know collect data in spreadsheets and uh",
    "start": "1281480",
    "end": "1290039"
  },
  {
    "text": "um it's not like very collaborative in that sense and it's very it's very um I",
    "start": "1290360",
    "end": "1297080"
  },
  {
    "text": "guess like rudimentary in a in a sense and uh I think one of the key benefits",
    "start": "1297080",
    "end": "1302400"
  },
  {
    "text": "and then the other struggle that we've seen is um you know once you have like an initial",
    "start": "1302400",
    "end": "1308000"
  },
  {
    "text": "spreadsheet how do you get how does it get updated with data that you're seeing in the wild right like how how like how",
    "start": "1308000",
    "end": "1314840"
  },
  {
    "text": "are those benchmarking sets evolving over time and and lsmith I think it it",
    "start": "1314840",
    "end": "1320400"
  },
  {
    "text": "solves both of those problems because it gives you like a central place where all of your data sets live and grow over",
    "start": "1320400",
    "end": "1325720"
  },
  {
    "text": "time and then since it's also connected to your tracing logging system you can",
    "start": "1325720",
    "end": "1331159"
  },
  {
    "text": "very easily Port data from your real production traces to your benchmarking",
    "start": "1331159",
    "end": "1336240"
  },
  {
    "text": "data sets um another thing I'll point out is data sets don't only have to be used for evaluation they can also be",
    "start": "1336240",
    "end": "1343880"
  },
  {
    "text": "used to improve your application so we can use data sets for things like fuse",
    "start": "1343880",
    "end": "1350720"
  },
  {
    "text": "shot prompting and we've also seen a couple of teams use lsmith data sets for",
    "start": "1350720",
    "end": "1356080"
  },
  {
    "text": "fine tuning so this is something that will probably",
    "start": "1356080",
    "end": "1362279"
  },
  {
    "text": "um uh invest some more resources in like building better workflows for f shot",
    "start": "1362279",
    "end": "1367360"
  },
  {
    "text": "prompting and fine tuning in the in the near future and maybe one final question",
    "start": "1367360",
    "end": "1372720"
  },
  {
    "text": "before we move on from from testing when are people testing in link Smith looks",
    "start": "1372720",
    "end": "1378159"
  },
  {
    "text": "like this is offline evaluation when do you recommend in your workflow that you should you know build your data set run",
    "start": "1378159",
    "end": "1385039"
  },
  {
    "text": "your evaluation tests is it pre-prod postr and how do I prevent regressions",
    "start": "1385039",
    "end": "1390919"
  },
  {
    "text": "from making it into my production application yeah that's a good question",
    "start": "1390919",
    "end": "1396600"
  },
  {
    "text": "I think um I would like to see a future where people",
    "start": "1396600",
    "end": "1402120"
  },
  {
    "text": "run tests uh for their LM pipelines in cicd obviously there are some uh I guess",
    "start": "1402120",
    "end": "1411520"
  },
  {
    "text": "there are some blockers right now from for for people to be doing that one is",
    "start": "1411520",
    "end": "1416919"
  },
  {
    "text": "the if you're using llm based evals uh they can be like pretty expensive to run the other thing is",
    "start": "1416919",
    "end": "1425240"
  },
  {
    "text": "oftentimes we've seen that llm based evals to score your results aren't",
    "start": "1425240",
    "end": "1432320"
  },
  {
    "text": "always accurate in a sense so it takes a good bit of human review to look",
    "start": "1432320",
    "end": "1439720"
  },
  {
    "text": "at the results and uh really understand you know",
    "start": "1439720",
    "end": "1445400"
  },
  {
    "text": "whether or not the result of the evaluator is actually what you want so right now um I you know the",
    "start": "1445400",
    "end": "1453320"
  },
  {
    "text": "recommendation is to test as much as possible but with these two uh things in",
    "start": "1453320",
    "end": "1458679"
  },
  {
    "text": "mind that if you run evaluations if you run tests that use LM based evaluations",
    "start": "1458679",
    "end": "1464720"
  },
  {
    "text": "too often it can be expensive and then the other thing is",
    "start": "1464720",
    "end": "1469840"
  },
  {
    "text": "oftentimes uh you'll need human review on the llm as a judge results uh that",
    "start": "1469840",
    "end": "1476240"
  },
  {
    "text": "run against your that run against your test",
    "start": "1476240",
    "end": "1480200"
  },
  {
    "text": "results cool and I guess one thing I'll also mention is the concept of online",
    "start": "1484960",
    "end": "1491159"
  },
  {
    "text": "evaluation so here I've talked about offline evaluation which is running your",
    "start": "1491159",
    "end": "1497840"
  },
  {
    "text": "LM pipeline against a data set and then um running uh combination of heris and LM",
    "start": "1497840",
    "end": "1505919"
  },
  {
    "text": "based evals to to score the results of your tests but oftentimes it's also",
    "start": "1505919",
    "end": "1512039"
  },
  {
    "text": "really useful to run automatic evaluations on your production",
    "start": "1512039",
    "end": "1517799"
  },
  {
    "text": "data so lsmith provides uh let me just dive into the projects view",
    "start": "1517799",
    "end": "1524760"
  },
  {
    "text": "again a me a mechanism for you to to annotate your run so you can submit",
    "start": "1524760",
    "end": "1531559"
  },
  {
    "text": "feedback um on on a particular trace this can be done at any level of the trace so for example if the overall uh",
    "start": "1531559",
    "end": "1540559"
  },
  {
    "text": "output of your LM pipeline looks good you might want to give it like a correctness score of of one um you can",
    "start": "1540559",
    "end": "1546600"
  },
  {
    "text": "also add a number of tags that that you can use to to score your your your",
    "start": "1546600",
    "end": "1555720"
  },
  {
    "text": "pipeline these are these are tags that are like associated with your tenant and you can add new tags as you see",
    "start": "1555919",
    "end": "1562279"
  },
  {
    "text": "fit um but sometimes it's it's really useful to actually look into an",
    "start": "1562279",
    "end": "1570360"
  },
  {
    "text": "individual run so in this case I'm in the retrieval step and then you can annotate the retrieval step separately",
    "start": "1570360",
    "end": "1577279"
  },
  {
    "text": "so basically if your overall output was bad and you traced it down to your",
    "start": "1577279",
    "end": "1583480"
  },
  {
    "text": "retrieval step like it's not returning the right documents based on the query you actually go in and annotate the",
    "start": "1583480",
    "end": "1589679"
  },
  {
    "text": "retrieval step separately um and then uh you can you know query",
    "start": "1589679",
    "end": "1596559"
  },
  {
    "text": "for you can query for traces that have a poor that have poor retrieval feedback",
    "start": "1596559",
    "end": "1601640"
  },
  {
    "text": "and see how that um um see how that impacted the overall result uh so",
    "start": "1601640",
    "end": "1609640"
  },
  {
    "text": "getting back to online evaluations in addition to running",
    "start": "1609640",
    "end": "1615880"
  },
  {
    "text": "automatic evals offline um on your test results what we're",
    "start": "1615880",
    "end": "1621360"
  },
  {
    "text": "working on right now is a way to configure eval evaluators to run on a sample of your traffic so the interface",
    "start": "1621360",
    "end": "1629000"
  },
  {
    "text": "for this will look something like run a vness evaluator on 20% of my downvoted",
    "start": "1629000",
    "end": "1636720"
  },
  {
    "text": "traces and so what that will allow you to do is get a really good sense of how uh your",
    "start": "1636720",
    "end": "1645600"
  },
  {
    "text": "application is doing in production on real data with respect to the criteria",
    "start": "1645600",
    "end": "1651880"
  },
  {
    "text": "that's important to you and your organization so the evaluation step doesn't just stop at you know testing or",
    "start": "1651880",
    "end": "1659080"
  },
  {
    "text": "the CI step it's something that carries into uh deploying your application to",
    "start": "1659080",
    "end": "1665799"
  },
  {
    "text": "production awesome one quick thing I'll point out is we also offer a workflow",
    "start": "1669559",
    "end": "1677120"
  },
  {
    "text": "called the annotation queue so you",
    "start": "1677120",
    "end": "1682279"
  },
  {
    "text": "can add any Trace to an annotation que and you can then go into The annotation",
    "start": "1682279",
    "end": "1689640"
  },
  {
    "text": "queue and look at each Trace that you've submitted to The annotation queue one by",
    "start": "1689640",
    "end": "1696279"
  },
  {
    "text": "one and review them in a cyclical manner so this is really helpful when you want",
    "start": "1696279",
    "end": "1702960"
  },
  {
    "text": "a really focused view for evaluation and annotation um um and we're actually",
    "start": "1702960",
    "end": "1709200"
  },
  {
    "text": "working on a permissioning system that allows you to um you know Mark certain users as",
    "start": "1709200",
    "end": "1717360"
  },
  {
    "text": "annotators and so they can go in and access The annotation queue and uh annotate the results that you're getting",
    "start": "1717360",
    "end": "1724240"
  },
  {
    "text": "in in uh uh in your uh projects in your tracing",
    "start": "1724240",
    "end": "1730240"
  },
  {
    "text": "projects cool we had one more question on testing I want to hit so the question",
    "start": "1731559",
    "end": "1737120"
  },
  {
    "text": "was how how many tests uh are needed generally speaking yeah that's that's a um",
    "start": "1737120",
    "end": "1745840"
  },
  {
    "text": "question that's kind of hard to answer I think what we've seen from teams that we",
    "start": "1745840",
    "end": "1752679"
  },
  {
    "text": "work with is when they're initially launching a version of their application it's around 20 examples to get um uh to",
    "start": "1752679",
    "end": "1761039"
  },
  {
    "text": "gain some confidence in in their in their application uh but we've seen",
    "start": "1761039",
    "end": "1766799"
  },
  {
    "text": "we've also seen teams that run their LM pipelines against multiple data sets",
    "start": "1766799",
    "end": "1772720"
  },
  {
    "text": "each data set having you know 20 to 50 examples sometimes more so it can really",
    "start": "1772720",
    "end": "1779159"
  },
  {
    "text": "vary uh based on the team um how much time you're willing to spend on",
    "start": "1779159",
    "end": "1784519"
  },
  {
    "text": "evaluation or testing and um I guess like how expensive it is to run your",
    "start": "1784519",
    "end": "1790960"
  },
  {
    "text": "evaluators and um score score your results of of your tests so it really",
    "start": "1790960",
    "end": "1798159"
  },
  {
    "text": "yeah just to add I think I think what we've seen people do in link Smith is dramatically grow their testing coverage",
    "start": "1798159",
    "end": "1804480"
  },
  {
    "text": "and you know Enos stated the before world where it was a lot of manual tests and spreadsheets most teams have some",
    "start": "1804480",
    "end": "1811240"
  },
  {
    "text": "kind of internal tooling maybe that they've built or it's imperfect um what we've heard is that Lang Smith allows",
    "start": "1811240",
    "end": "1817600"
  },
  {
    "text": "you to add the incremental test more easily with less effort which which",
    "start": "1817600",
    "end": "1822760"
  },
  {
    "text": "results in higher quality production applications",
    "start": "1822760",
    "end": "1829240"
  },
  {
    "text": "yep awesome um one thing that's helpful for",
    "start": "1831399",
    "end": "1838679"
  },
  {
    "text": "production is U monitoring that's something that I demoed in the uh when I",
    "start": "1838679",
    "end": "1845919"
  },
  {
    "text": "was when I was uh demoing the tracing projects but every project has this monitoring Tab and um I don't know if I",
    "start": "1845919",
    "end": "1854399"
  },
  {
    "text": "mentioned this before but it's useful sometimes to deploy",
    "start": "1854399",
    "end": "1860480"
  },
  {
    "text": "multiple versions of your application to production um and there are multiple",
    "start": "1860480",
    "end": "1866639"
  },
  {
    "text": "ways to do this you can Route traffic to a specific version of your application",
    "start": "1866639",
    "end": "1872039"
  },
  {
    "text": "or you can have a single version of your application that everyone hits and you",
    "start": "1872039",
    "end": "1877559"
  },
  {
    "text": "can have a shadow pipeline running so such that the uh input gets sent to your",
    "start": "1877559",
    "end": "1884440"
  },
  {
    "text": "main application but also uh Your Shadow pipeline Your Shadow pipeline might have some changes relative to the retrieval",
    "start": "1884440",
    "end": "1890760"
  },
  {
    "text": "strategy or the prompt or the model um but anyways at the end of the day you could have multiple versions of your",
    "start": "1890760",
    "end": "1896639"
  },
  {
    "text": "application running and accepting production data and we've done that with",
    "start": "1896639",
    "end": "1902320"
  },
  {
    "text": "chatlink chain um and uh we have like multiple models serving",
    "start": "1902320",
    "end": "1907919"
  },
  {
    "text": "traffic and generating responses and this is really uh I guess the monitoring",
    "start": "1907919",
    "end": "1914240"
  },
  {
    "text": "section and being able to group my metadata is really useful for being able to understand how different versions of",
    "start": "1914240",
    "end": "1920960"
  },
  {
    "text": "your application are doing side by side so it allows you to AB test different configurations of your",
    "start": "1920960",
    "end": "1926480"
  },
  {
    "text": "application um so yeah this could I right now we're we're looking at uh different models but you know these can",
    "start": "1926480",
    "end": "1933960"
  },
  {
    "text": "be different prompts uh different retrieval strategies can be like look back window length things like that so",
    "start": "1933960",
    "end": "1941559"
  },
  {
    "text": "it really depends there's so many different knobs that you can that you can dial um change in your LM Pipeline",
    "start": "1941559",
    "end": "1949120"
  },
  {
    "text": "and uh being able to AB test all of them at at the same time can can be useful",
    "start": "1949120",
    "end": "1956919"
  },
  {
    "text": "so at the end of the day all of these all of these uh features that we've",
    "start": "1956919",
    "end": "1962120"
  },
  {
    "text": "supplied in lsmith allow you to not only gain observability in your data but",
    "start": "1962120",
    "end": "1969240"
  },
  {
    "text": "answer the question am I getting better over time or am I doing worse over time",
    "start": "1969240",
    "end": "1974480"
  },
  {
    "text": "so that's that's really what we're building towards here hey on we had a couple questions on",
    "start": "1974480",
    "end": "1981120"
  },
  {
    "text": "open source models do we support it in link Smith and can you see the full capabilities like token tracking uh as",
    "start": "1981120",
    "end": "1988240"
  },
  {
    "text": "well as uh multimodal or multilingual applications how is that supported in",
    "start": "1988240",
    "end": "1994240"
  },
  {
    "text": "link Smith yes so we support uh basically all you can use any model",
    "start": "1994240",
    "end": "2001840"
  },
  {
    "text": "you'd like and get that to trace uh get that to render correctly in link Smith",
    "start": "2001840",
    "end": "2007919"
  },
  {
    "text": "this is uh regardless of whether or not you're using Lang chain um Lang chain has a bunch of wrappers around open",
    "start": "2007919",
    "end": "2014840"
  },
  {
    "text": "source models um but if you're not using Lang chain you can still use our SDK and we",
    "start": "2014840",
    "end": "2021200"
  },
  {
    "text": "have a few examples in our docs about how to send up llm runs that uh allow",
    "start": "2021200",
    "end": "2027559"
  },
  {
    "text": "your run to we actually do token counting um in the back end",
    "start": "2027559",
    "end": "2032880"
  },
  {
    "text": "so you don't have to send the tokens up to L Smith we'll do our will do a best",
    "start": "2032880",
    "end": "2038000"
  },
  {
    "text": "effort token count um so you can still get that information uh um saved in the",
    "start": "2038000",
    "end": "2043840"
  },
  {
    "text": "back end and you can view it in the UI for multimodal we do support uh",
    "start": "2043840",
    "end": "2049638"
  },
  {
    "text": "multimodal I don't think I have a multimodal trace hold up here but if you're using the gbd4 vision model and",
    "start": "2049639",
    "end": "2057158"
  },
  {
    "text": "you're sending a b 64 encoding encoded image or an image URL um as your as your",
    "start": "2057159",
    "end": "2063358"
  },
  {
    "text": "input and you're asking a question about about the image that will show up uh uh",
    "start": "2063359",
    "end": "2069158"
  },
  {
    "text": "in line with your Trace in the llm",
    "start": "2069159",
    "end": "2073919"
  },
  {
    "text": "run and I just dropped a link to a Blog as well for multimodal rag examples and",
    "start": "2076000",
    "end": "2082158"
  },
  {
    "text": "if you click in there there should be some public links to how Lang Smith handles multimodal",
    "start": "2082159",
    "end": "2090118"
  },
  {
    "text": "models cool awesome all right um I'll just quick",
    "start": "2090119",
    "end": "2096878"
  },
  {
    "text": "quickly dive into the the promp tuub which we launched a while ago but I think it's worth mentioning again and",
    "start": "2096879",
    "end": "2103720"
  },
  {
    "text": "then I'm happy to answer any more questions about uh about LMI so the",
    "start": "2103720",
    "end": "2110960"
  },
  {
    "text": "prompt tub is a solution that we offer is part of",
    "start": "2110960",
    "end": "2116440"
  },
  {
    "text": "lsmith that allows you to manage your prompts and view uh prompts that other",
    "start": "2116440",
    "end": "2122200"
  },
  {
    "text": "people submit to the system um so here you can see a combination",
    "start": "2122200",
    "end": "2127400"
  },
  {
    "text": "of public prompts um and then you can filter for prompts um by different tags",
    "start": "2127400",
    "end": "2134200"
  },
  {
    "text": "and use cases you can see prompts for agents chatbots so on and so forth you can also see prompts that are specific",
    "start": "2134200",
    "end": "2140720"
  },
  {
    "text": "your to your tenant um you know some of these are public um and you can try out",
    "start": "2140720",
    "end": "2146119"
  },
  {
    "text": "any prompt in a playground environment um we we were able to open",
    "start": "2146119",
    "end": "2151520"
  },
  {
    "text": "up the playground environment from an llm run uh by clicking in on a trace and",
    "start": "2151520",
    "end": "2156839"
  },
  {
    "text": "then going down to the llm run and then opening that llm run in the playground but you could also get to the playground",
    "start": "2156839",
    "end": "2163839"
  },
  {
    "text": "by opening up a prompt from The Prompt tuub in the playground and uh this",
    "start": "2163839",
    "end": "2169240"
  },
  {
    "text": "prompt has different variables you can input the variables and then you can run a completion just like you did in the",
    "start": "2169240",
    "end": "2175200"
  },
  {
    "text": "playground environment from the llm call we're actually doing some work to consolidate the playground environment",
    "start": "2175200",
    "end": "2180760"
  },
  {
    "text": "so that it's accessible as soon as you enter Langs Smith but right now these are the two ways to get to the",
    "start": "2180760",
    "end": "2187280"
  },
  {
    "text": "uh the the experimental or or or the playground the iterative which gives you an um an iterative kind of um",
    "start": "2187280",
    "end": "2195920"
  },
  {
    "text": "environment to test your your prompts and your model",
    "start": "2195920",
    "end": "2201319"
  },
  {
    "text": "configurations how do you expect people to work in the playground do you expect developers to only be crafting prompts",
    "start": "2202000",
    "end": "2208319"
  },
  {
    "text": "or how does this help you collaborate with other team",
    "start": "2208319",
    "end": "2212920"
  },
  {
    "text": "members yeah so there isn't really a good solution right",
    "start": "2214400",
    "end": "2222000"
  },
  {
    "text": "now for um uh Team prompt management a lot",
    "start": "2222000",
    "end": "2229280"
  },
  {
    "text": "of people use a number of different templating libraries and they uh also",
    "start": "2229280",
    "end": "2237680"
  },
  {
    "text": "store their prompts in GitHub and other Version Control",
    "start": "2237680",
    "end": "2242760"
  },
  {
    "text": "Systems I think while Works um we've seen a lot of companies",
    "start": "2242760",
    "end": "2251160"
  },
  {
    "text": "want to offer a solution that makes it easy for people with non-technical",
    "start": "2251160",
    "end": "2258079"
  },
  {
    "text": "backgrounds to go in and edit and deploy prompts um and and you know play around",
    "start": "2258079",
    "end": "2263720"
  },
  {
    "text": "with prompts in a playground environment and uh you know iterate on versions of prompts",
    "start": "2263720",
    "end": "2271200"
  },
  {
    "text": "and that's kind of the solution that we're moving towards with the with the prompt playground we still we still have",
    "start": "2272079",
    "end": "2277839"
  },
  {
    "text": "a lot of work to do but we want to make it as easy as possible for people with non-technical backgrounds to use um to",
    "start": "2277839",
    "end": "2286319"
  },
  {
    "text": "use the uh to use the promp tub um so",
    "start": "2286319",
    "end": "2291760"
  },
  {
    "text": "yeah if does that kind of answer the question anything you wanted to add there Julia yeah yeah I think what we've",
    "start": "2291760",
    "end": "2298280"
  },
  {
    "text": "been seeing a lot is that developers are the ones that are building the full orchestration systems making sure the",
    "start": "2298280",
    "end": "2305079"
  },
  {
    "text": "testing and evaluation and monitor ing is all running smoothly uh but a lot of times you need to collaborate with",
    "start": "2305079",
    "end": "2311720"
  },
  {
    "text": "subject matter experts maybe you're writing a prompt or legal industry and your engineering team work closely with",
    "start": "2311720",
    "end": "2318960"
  },
  {
    "text": "someone in legal profession to get a Vibe check or make sure that the quality",
    "start": "2318960",
    "end": "2324000"
  },
  {
    "text": "is uh as expected and so what we've seen a lot of teams do is use the prompt Hub",
    "start": "2324000",
    "end": "2331720"
  },
  {
    "text": "as a way for non-developers subject matter experts uh to contribute to the LM app development",
    "start": "2331720",
    "end": "2339640"
  },
  {
    "text": "process and I think you know with all things that are being used in the real world it takes collaboration between you",
    "start": "2339640",
    "end": "2346640"
  },
  {
    "text": "know Builders and and and users and so this is a really fantastic way that you can bring more people along along for",
    "start": "2346640",
    "end": "2353440"
  },
  {
    "text": "the ride in the prompt tub yep it's well said and then we have some other",
    "start": "2353440",
    "end": "2360000"
  },
  {
    "text": "questions on public prompts prompts versus private",
    "start": "2360000",
    "end": "2365440"
  },
  {
    "text": "prompts uh can I just have prompt for my team can I integrate with other tools",
    "start": "2365440",
    "end": "2371520"
  },
  {
    "text": "such as GitHub how do you suggest teams to work on",
    "start": "2371520",
    "end": "2377480"
  },
  {
    "text": "prompts yeah um so yeah you can uh",
    "start": "2379200",
    "end": "2384720"
  },
  {
    "text": "submit prompts that are private to your organizational tenant or your personal tenant um any prompt that you submit can",
    "start": "2384720",
    "end": "2392800"
  },
  {
    "text": "either be private or public so if it's private only members of your tenant can see it if it's public anyone can see",
    "start": "2392800",
    "end": "2399839"
  },
  {
    "text": "it how does this integrate with GitHub right now it doesn't um we have a",
    "start": "2399839",
    "end": "2406839"
  },
  {
    "text": "um we have like an SDK that allows you to pull and push prompts to the hub",
    "start": "2406839",
    "end": "2412599"
  },
  {
    "text": "right now um so we don't have an integration with GitHub although that that could be something interesting that",
    "start": "2412599",
    "end": "2418319"
  },
  {
    "text": "we that we work on uh we're also working to support a lot of different types of",
    "start": "2418319",
    "end": "2423760"
  },
  {
    "text": "prompt templates so we've seen teams use handlebars and mustache um alongside you know other",
    "start": "2423760",
    "end": "2431640"
  },
  {
    "text": "other templating libraries for prompt so we want to support all of those and as I",
    "start": "2431640",
    "end": "2437560"
  },
  {
    "text": "guess as Julia mentioned we want the prompt Hub to evolve into a system",
    "start": "2437560",
    "end": "2443839"
  },
  {
    "text": "that um makes it really easy to manage and experiment with prompts regardless",
    "start": "2443839",
    "end": "2449560"
  },
  {
    "text": "of whether or not you're a developer or a PM or someone on a marketing or sales",
    "start": "2449560",
    "end": "2455880"
  },
  {
    "text": "team um so we want to like lower the barrier entry and make working with llms and",
    "start": "2455880",
    "end": "2462240"
  },
  {
    "text": "prompts less scary and we also want to actually offer native integration with our",
    "start": "2462240",
    "end": "2468400"
  },
  {
    "text": "testing uh and benchmarking workflow um so right now in the prompt playground",
    "start": "2468400",
    "end": "2474480"
  },
  {
    "text": "you can only run a uh I guess like a single prompt um but we would like to build a",
    "start": "2474480",
    "end": "2483760"
  },
  {
    "text": "we would like to build up the capability for you to run um a data set in the playground and this",
    "start": "2483760",
    "end": "2491960"
  },
  {
    "text": "allows you to I guess this lowers a barrier of entry for testing as",
    "start": "2491960",
    "end": "2500079"
  },
  {
    "text": "well we had a really good question around I'm going to combine two questions we had in the chat one is how",
    "start": "2500800",
    "end": "2508119"
  },
  {
    "text": "is Ling your your application code whether it's written in L chain or not synced up with the traces in lsmith like",
    "start": "2508119",
    "end": "2515280"
  },
  {
    "text": "how do you wire it up and does link Smith or Hub add any additional latency that that we should",
    "start": "2515280",
    "end": "2522319"
  },
  {
    "text": "be aware of yes so let me answer the first question um we",
    "start": "2522319",
    "end": "2529520"
  },
  {
    "text": "actually updated our docs a bit to make this more clear but we have a quick start guide that you can follow to",
    "start": "2529520",
    "end": "2536280"
  },
  {
    "text": "understand how you can use the python SDK the typescript SDK or the API to",
    "start": "2536280",
    "end": "2541599"
  },
  {
    "text": "submit traces to Lang Smith so if you're using Lang chain it's very easy uh you just export a couple of",
    "start": "2541599",
    "end": "2549720"
  },
  {
    "text": "environment variables and then you run your application as you normally would if you're using the python SDK um let me",
    "start": "2549720",
    "end": "2556640"
  },
  {
    "text": "actually go into a more indepth guide here if you're using the python SDK we",
    "start": "2556640",
    "end": "2563760"
  },
  {
    "text": "offer a number of different ways for you to log things to lsmith you can use our",
    "start": "2563760",
    "end": "2568839"
  },
  {
    "text": "run tree uh API which allows you to build up traces and spans uh in a",
    "start": "2568839",
    "end": "2576720"
  },
  {
    "text": "pre like format and then send them up to the Langs Smith back end we also offer a",
    "start": "2576720",
    "end": "2582319"
  },
  {
    "text": "decorator that you can use to decorate your functions and this would turn them",
    "start": "2582319",
    "end": "2587400"
  },
  {
    "text": "into into individual spans within a trace and it allows you to submit traces",
    "start": "2587400",
    "end": "2593040"
  },
  {
    "text": "to the system we also have an open AI wrapper um that you can import and then",
    "start": "2593040",
    "end": "2599240"
  },
  {
    "text": "wrap the open a client and then this will this client that you wrapped will",
    "start": "2599240",
    "end": "2605760"
  },
  {
    "text": "now have all the same functionality as the openi client but the added benefit of having",
    "start": "2605760",
    "end": "2611400"
  },
  {
    "text": "all of your uh uh inputs and outputs loged to Links Smith if you're using",
    "start": "2611400",
    "end": "2619000"
  },
  {
    "text": "typescript we offer a couple of ways to log traces there is this um uh run tree",
    "start": "2619000",
    "end": "2627359"
  },
  {
    "text": "uh run Tre API that we that we that I just showed that works",
    "start": "2627359",
    "end": "2633160"
  },
  {
    "text": "with python as well but we also recently launched a just today actually um let me",
    "start": "2633160",
    "end": "2643160"
  },
  {
    "text": "see yeah this traceable um this traceable function that allows you to wrap any arbitrary",
    "start": "2643160",
    "end": "2649440"
  },
  {
    "text": "function and have it logged to lsmith so I'll update the docs to include this in",
    "start": "2649440",
    "end": "2654680"
  },
  {
    "text": "the core functionality how to guide as well but this is something that we just released today so we're constantly working on lowering the barrier to entry",
    "start": "2654680",
    "end": "2662839"
  },
  {
    "text": "for using Langs Smith and um it's completely independent system for Lang",
    "start": "2662839",
    "end": "2668079"
  },
  {
    "text": "chain so you can get all the benefits of lsmith regardless of whether or not you're using Lang chain obviously we",
    "start": "2668079",
    "end": "2673760"
  },
  {
    "text": "strive to have seamless support with Lang chain but we also want everyone to benefit from lsmith regardless of",
    "start": "2673760",
    "end": "2680079"
  },
  {
    "text": "whether or not they're they're using linkchain um and then if you're using the API",
    "start": "2680079",
    "end": "2687359"
  },
  {
    "text": "um we have the API docs here actually and all of the functionality",
    "start": "2687359",
    "end": "2693400"
  },
  {
    "text": "that you see uh in the UI in the DK with lsmith is available in the API so this",
    "start": "2693400",
    "end": "2700839"
  },
  {
    "text": "includes logging traces um quering for statistics querying for",
    "start": "2700839",
    "end": "2706280"
  },
  {
    "text": "runs uh uh creating creating projects things like",
    "start": "2706280",
    "end": "2712559"
  },
  {
    "text": "that thanks and then how about latency how is it wired up such that blank Smith",
    "start": "2712559",
    "end": "2719280"
  },
  {
    "text": "doesn't have doesn't add additional latency to your application uh and and",
    "start": "2719280",
    "end": "2724440"
  },
  {
    "text": "then maybe speak about hub specifically where if you decide to to pull in",
    "start": "2724440",
    "end": "2731160"
  },
  {
    "text": "prompts yeah so we again answer the latency question first uh everything is logged in the background so if you're if",
    "start": "2731160",
    "end": "2738880"
  },
  {
    "text": "you're using the SDK and if you're using uh Lang",
    "start": "2738880",
    "end": "2744760"
  },
  {
    "text": "chain setting up tracing to lsmith will not impact the latency of your",
    "start": "2744760",
    "end": "2750400"
  },
  {
    "text": "application uh additionally if Lang Smith is down for whatever reason it",
    "start": "2750400",
    "end": "2755680"
  },
  {
    "text": "will will also not affect your application if you want to use the prompt Hub you can pull in prompts using",
    "start": "2755680",
    "end": "2765640"
  },
  {
    "text": "um so we have some documentation here on quick start and how how to use the Hub",
    "start": "2765640",
    "end": "2772079"
  },
  {
    "text": "but it's really easy to uh create a handle start using the Hub uh commit and",
    "start": "2772079",
    "end": "2779440"
  },
  {
    "text": "then um all you need to do is install uh linkchain and linkchain hub configure some environment variables and then you",
    "start": "2779440",
    "end": "2786240"
  },
  {
    "text": "can pull objects from The Hub and start using prompts that are available in the",
    "start": "2786240",
    "end": "2791400"
  },
  {
    "text": "Hub in your in your application",
    "start": "2791400",
    "end": "2795240"
  },
  {
    "text": "code yeah and just to call out I think people like prompt tuub for a number of reasons one it is a way to collaborate",
    "start": "2796960",
    "end": "2805040"
  },
  {
    "text": "with non-engineers which you mentioned and two uh is get maxed says in the chat",
    "start": "2805040",
    "end": "2810920"
  },
  {
    "text": "it makes code a lot cleaner what we see a lot of teams doing today is littering",
    "start": "2810920",
    "end": "2816079"
  },
  {
    "text": "their code with a lot of prompt text this gives you a nice UI to store and",
    "start": "2816079",
    "end": "2821200"
  },
  {
    "text": "version it it's not backed by GitHub but we've created our own versioning system",
    "start": "2821200",
    "end": "2827599"
  },
  {
    "text": "there's a unique commit Shaw for every uh prompt so you can pull in an exact",
    "start": "2827599",
    "end": "2833640"
  },
  {
    "text": "version and and uh pin it to not change and it really helps people test and",
    "start": "2833640",
    "end": "2840760"
  },
  {
    "text": "iterate over their prompt construction because that often is a a big driver for",
    "start": "2840760",
    "end": "2845839"
  },
  {
    "text": "code quality and an application",
    "start": "2845839",
    "end": "2850319"
  },
  {
    "text": "performance because I know we're getting to the end here maybe what's next for for Ling Smith or how do you think about",
    "start": "2857920",
    "end": "2865400"
  },
  {
    "text": "the workflows that we want to enable what are some of the things the teams thinking about and should that should",
    "start": "2865400",
    "end": "2872440"
  },
  {
    "text": "keep an eye out for in the future yeah it's a great question so we",
    "start": "2872440",
    "end": "2879400"
  },
  {
    "text": "actually uh wrote up a blog post about our uh the general availability of",
    "start": "2879400",
    "end": "2885359"
  },
  {
    "text": "lsmith and our recent fund raise and here you can get a good idea of the",
    "start": "2885359",
    "end": "2892040"
  },
  {
    "text": "different workflows that link Smith supports at each step of the LM application development life cycle and",
    "start": "2892040",
    "end": "2897839"
  },
  {
    "text": "how they fit in with each other and at the end uh we have the road ahead so",
    "start": "2897839",
    "end": "2904240"
  },
  {
    "text": "what's next for Lang Smith well we want more native support for regression testing um so we want to be able to",
    "start": "2904240",
    "end": "2911119"
  },
  {
    "text": "integrate Langs Smith with CCD pipelines such that you can run lsmith tests in",
    "start": "2911119",
    "end": "2917319"
  },
  {
    "text": "GitHub actions or in gitlab or or whichever Sky tool you're using and we",
    "start": "2917319",
    "end": "2923319"
  },
  {
    "text": "also want to make it very simple for people to go in and make Corrections for",
    "start": "2923319",
    "end": "2929480"
  },
  {
    "text": "um uh scores that were submitted by llm evaluators as I mentioned earlier uh one",
    "start": "2929480",
    "end": "2936200"
  },
  {
    "text": "pain point that you seen uh when running tests for llm pipelines and using llm based evals is that they're not always",
    "start": "2936200",
    "end": "2942599"
  },
  {
    "text": "correct so you want to streamline this process by making it very easy to hook up lsmith tests in cicd and then also",
    "start": "2942599",
    "end": "2950079"
  },
  {
    "text": "streamline the process and making Corrections um that uh that um result um",
    "start": "2950079",
    "end": "2957440"
  },
  {
    "text": "I guess make make corrections to L based evaluator",
    "start": "2957440",
    "end": "2962599"
  },
  {
    "text": "scores the second thing is uh I mentioned this earlier but the ability to run online evaluators on a sample of",
    "start": "2962599",
    "end": "2970400"
  },
  {
    "text": "your production data so we are actually working on this right now and you'll be",
    "start": "2970400",
    "end": "2975559"
  },
  {
    "text": "able to configure an online evaluator uh in a very simple manner um such that you",
    "start": "2975559",
    "end": "2981880"
  },
  {
    "text": "can do things like run a vagueness or repetitiveness llm based evaluator on",
    "start": "2981880",
    "end": "2988599"
  },
  {
    "text": "let's say 20% of your downloaded production",
    "start": "2988599",
    "end": "2994520"
  },
  {
    "text": "traces the next thing that I'll point out is better filtering and conversation support so this is",
    "start": "2994520",
    "end": "3003160"
  },
  {
    "text": "a new interface that we want to provide in Langs",
    "start": "3003160",
    "end": "3008359"
  },
  {
    "text": "Smith when you have a chat based application and you're uh and you've",
    "start": "3008359",
    "end": "3016000"
  },
  {
    "text": "hooked it up to lsmith each invocation of the chatbot will actually log a new",
    "start": "3016000",
    "end": "3021440"
  },
  {
    "text": "Trace to Links Smith and currently you can group all of the these traces",
    "start": "3021440",
    "end": "3027119"
  },
  {
    "text": "together by sending in a particular metadata key value and",
    "start": "3027119",
    "end": "3032640"
  },
  {
    "text": "then uh filtering on that metadata key value so you can send up in the metadata",
    "start": "3032640",
    "end": "3038160"
  },
  {
    "text": "payload something like conversation ID and then have the value be the ID of the",
    "start": "3038160",
    "end": "3044440"
  },
  {
    "text": "conversation that's going on with the chatbot and then if you filter on that you can look at all the traces that",
    "start": "3044440",
    "end": "3050559"
  },
  {
    "text": "result um uh that are part of that conversation however this is a bit of a",
    "start": "3050559",
    "end": "3057760"
  },
  {
    "text": "um cumbersome workload and we don't want people to have to think about uh you know conversation ideas and filtering",
    "start": "3057760",
    "end": "3064280"
  },
  {
    "text": "for them um so on and so forth so what we're working on is a more",
    "start": "3064280",
    "end": "3072000"
  },
  {
    "text": "native um a more native experience for people to view their chatbot",
    "start": "3072079",
    "end": "3079839"
  },
  {
    "text": "history without having to perform an additional filtering step so this is",
    "start": "3079839",
    "end": "3085119"
  },
  {
    "text": "something that's in the works the other thing that we're working",
    "start": "3085119",
    "end": "3091000"
  },
  {
    "text": "on is the ability to deploy LM based applications that you build with",
    "start": "3091000",
    "end": "3096880"
  },
  {
    "text": "linkchain in uh hosted L serve uh so more to come",
    "start": "3096880",
    "end": "3102960"
  },
  {
    "text": "on this soon but we want to provide a really easy experience for people to deploy their applications in the first place and obviously with L serve there's",
    "start": "3102960",
    "end": "3110319"
  },
  {
    "text": "there's uh going to be seamless integration with lsmith so you can get everything that lsmith has to offer out",
    "start": "3110319",
    "end": "3117000"
  },
  {
    "text": "of the box uh with uh this hosted Linker product the last thing that I'll point",
    "start": "3117000",
    "end": "3123559"
  },
  {
    "text": "out is we're working on Enterprise features to support um",
    "start": "3123559",
    "end": "3129720"
  },
  {
    "text": "admin security needs of of large teams um one thing that I alluded to earlier",
    "start": "3129720",
    "end": "3135000"
  },
  {
    "text": "was permissioning so for example if you only want people to go in and access annotation cues to annotate your traces",
    "start": "3135000",
    "end": "3142160"
  },
  {
    "text": "that you send to The annotation queue this is something that would be possible with the um rback model that we're that",
    "start": "3142160",
    "end": "3148319"
  },
  {
    "text": "we're working on right now maybe a final question from the",
    "start": "3148319",
    "end": "3155760"
  },
  {
    "text": "group unless others write something into the chat in the last last minute or two",
    "start": "3155760",
    "end": "3160920"
  },
  {
    "text": "a lot of questions on linkchain expression language seems a bit newer for folks how can link Smith help with",
    "start": "3160920",
    "end": "3167400"
  },
  {
    "text": "chain construction and visibility and linkchain expression language and anything more you can comment on around",
    "start": "3167400",
    "end": "3175240"
  },
  {
    "text": "playing serve yeah so if you build up a chain",
    "start": "3175240",
    "end": "3184200"
  },
  {
    "text": "with lank chain expression language and you have the um I think I",
    "start": "3184200",
    "end": "3189720"
  },
  {
    "text": "have an example in the docs that uses Lang chain expression",
    "start": "3189720",
    "end": "3196200"
  },
  {
    "text": "language yeah here we go Lang chain so this is this is Lang chain expression",
    "start": "3197920",
    "end": "3203079"
  },
  {
    "text": "language um it uses this pipe syntax that allows you to stitch different",
    "start": "3203079",
    "end": "3208160"
  },
  {
    "text": "components together so in this case we have a prompt model EP parser this is like one of the simplest chains that you",
    "start": "3208160",
    "end": "3213359"
  },
  {
    "text": "could build and by by default all of this is logged",
    "start": "3213359",
    "end": "3218760"
  },
  {
    "text": "to uh to lsmith and actually chat Lang chain is also built with uh Lang chain",
    "start": "3218760",
    "end": "3224319"
  },
  {
    "text": "expression language so any of these uh traces that you see",
    "start": "3224319",
    "end": "3230880"
  },
  {
    "text": "is they show you the different steps of the uh linkchain expression language uh",
    "start": "3230880",
    "end": "3237200"
  },
  {
    "text": "pipeline so you'll get immediate visibility if you're using LC with link",
    "start": "3237200",
    "end": "3243720"
  },
  {
    "text": "with lsmith and this allows you to get a good sense for what's happening at each",
    "start": "3243720",
    "end": "3249920"
  },
  {
    "text": "step of the pipeline and make changes as as needed not sure if that completely",
    "start": "3249920",
    "end": "3255280"
  },
  {
    "text": "answered the question but uh yeah we have native support with LCL out of the box yeah to add on the because we have",
    "start": "3255280",
    "end": "3264640"
  },
  {
    "text": "uh every llm call can open up a playground we'll keep track of all the previous state for where the chain has",
    "start": "3264640",
    "end": "3272319"
  },
  {
    "text": "gotten to or where the agent has gotten to so when you open up a playground call for a multi",
    "start": "3272319",
    "end": "3279160"
  },
  {
    "text": "llm uh a chain or agent that calls multiple llms you'll be able to see",
    "start": "3279160",
    "end": "3284640"
  },
  {
    "text": "what's been called previously and and will'll keep track of state so certainly helps helps as well and then um langing",
    "start": "3284640",
    "end": "3291480"
  },
  {
    "text": "serve uh it's that's that's Source available we do have a posted links serve",
    "start": "3291480",
    "end": "3297319"
  },
  {
    "text": "beta that you can reach out to me if you want to get access to at Julia linkchain",
    "start": "3297319",
    "end": "3302359"
  },
  {
    "text": "dodev um but that's not in GA",
    "start": "3302359",
    "end": "3307039"
  },
  {
    "text": "yet and I think we'll wrap there thank you all for joining we're super excited for you to try out link Smith if you've",
    "start": "3307839",
    "end": "3314240"
  },
  {
    "text": "been using link Smith uh we really appreciate you and feel free to give us",
    "start": "3314240",
    "end": "3319720"
  },
  {
    "text": "feedback uh Team ships very very quickly and we're always trying to evolve and get better um but it's a big day for us",
    "start": "3319720",
    "end": "3326119"
  },
  {
    "text": "and and we appreciate all of your support and helping helping us get here so I will hopefully see you all soon",
    "start": "3326119",
    "end": "3333200"
  },
  {
    "text": "thanks so much thanks Julia thanks everyone bye",
    "start": "3333200",
    "end": "3339880"
  },
  {
    "text": "right",
    "start": "3341200",
    "end": "3344200"
  }
]