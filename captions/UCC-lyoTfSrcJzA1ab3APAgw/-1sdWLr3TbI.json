[
  {
    "text": "hello everyone and good morning good",
    "start": "1120",
    "end": "3159"
  },
  {
    "text": "afternoon or good evening depending on",
    "start": "3159",
    "end": "4920"
  },
  {
    "text": "where you're tuning in",
    "start": "4920",
    "end": "6799"
  },
  {
    "text": "from my name is Jacob Lee and I'm a",
    "start": "6799",
    "end": "9240"
  },
  {
    "text": "software engineer at Lang chain I'm",
    "start": "9240",
    "end": "11559"
  },
  {
    "text": "thrilled to be here presenting at the",
    "start": "11559",
    "end": "12960"
  },
  {
    "text": "2023 Google web ml Summit and today I'll",
    "start": "12960",
    "end": "16039"
  },
  {
    "text": "be talking a bit about how to",
    "start": "16039",
    "end": "17160"
  },
  {
    "text": "effectively build with large language",
    "start": "17160",
    "end": "18760"
  },
  {
    "text": "models in the",
    "start": "18760",
    "end": "21640"
  },
  {
    "text": "browser and a quick aside before I get",
    "start": "22519",
    "end": "25080"
  },
  {
    "text": "started if you're not familiar with Lang",
    "start": "25080",
    "end": "27199"
  },
  {
    "text": "chain we're an open source framework for",
    "start": "27199",
    "end": "28840"
  },
  {
    "text": "building context aware reasoning",
    "start": "28840",
    "end": "32560"
  },
  {
    "text": "applications more concretely this",
    "start": "33480",
    "end": "35719"
  },
  {
    "text": "includes orchestrating pipelines that",
    "start": "35719",
    "end": "37360"
  },
  {
    "text": "parse format and retrieve data from",
    "start": "37360",
    "end": "39440"
  },
  {
    "text": "various sources to present as context to",
    "start": "39440",
    "end": "41879"
  },
  {
    "text": "an llm as well as various useful ways of",
    "start": "41879",
    "end": "44399"
  },
  {
    "text": "prompting and chaining different model",
    "start": "44399",
    "end": "47160"
  },
  {
    "text": "calls together to build sophisticated",
    "start": "47160",
    "end": "49160"
  },
  {
    "text": "and Powerful",
    "start": "49160",
    "end": "51960"
  },
  {
    "text": "applications to start things off a bit",
    "start": "53480",
    "end": "55879"
  },
  {
    "text": "about the current state of webml",
    "start": "55879",
    "end": "58199"
  },
  {
    "text": "models it's no secret that that for a",
    "start": "58199",
    "end": "60199"
  },
  {
    "text": "long time machine learning was mostly a",
    "start": "60199",
    "end": "61879"
  },
  {
    "text": "python",
    "start": "61879",
    "end": "63119"
  },
  {
    "text": "game however all the recent investments",
    "start": "63119",
    "end": "65518"
  },
  {
    "text": "in tooling and developer experience in",
    "start": "65519",
    "end": "67320"
  },
  {
    "text": "the space have made many formerly Arcane",
    "start": "67320",
    "end": "70000"
  },
  {
    "text": "machine learning Concepts accessible to",
    "start": "70000",
    "end": "71600"
  },
  {
    "text": "a much wider slice of",
    "start": "71600",
    "end": "73840"
  },
  {
    "text": "developers and with JavaScript being",
    "start": "73840",
    "end": "75799"
  },
  {
    "text": "most widely used programmed language in",
    "start": "75799",
    "end": "77200"
  },
  {
    "text": "the world it's no surprise that this has",
    "start": "77200",
    "end": "79159"
  },
  {
    "text": "included many web developers who",
    "start": "79159",
    "end": "81200"
  },
  {
    "text": "naturally sought to use these tools in",
    "start": "81200",
    "end": "83320"
  },
  {
    "text": "web",
    "start": "83320",
    "end": "85680"
  },
  {
    "text": "environments as a result there's been a",
    "start": "85840",
    "end": "87680"
  },
  {
    "text": "lot of awesome development in webml",
    "start": "87680",
    "end": "89159"
  },
  {
    "text": "lately",
    "start": "89159",
    "end": "90439"
  },
  {
    "text": "a few to call out here are zenova who's",
    "start": "90439",
    "end": "92640"
  },
  {
    "text": "the X in the middle of your screen there",
    "start": "92640",
    "end": "94799"
  },
  {
    "text": "who's done some really cool work on",
    "start": "94799",
    "end": "96200"
  },
  {
    "text": "quantizing hugging face models to make",
    "start": "96200",
    "end": "97880"
  },
  {
    "text": "them smaller and more suited for the",
    "start": "97880",
    "end": "99680"
  },
  {
    "text": "browser while still retaining",
    "start": "99680",
    "end": "101880"
  },
  {
    "text": "Effectiveness there is a machine",
    "start": "101880",
    "end": "103439"
  },
  {
    "text": "learning compilation for their amazing",
    "start": "103439",
    "end": "105079"
  },
  {
    "text": "work on web llm running some powerful",
    "start": "105079",
    "end": "108320"
  },
  {
    "text": "large models with GPU",
    "start": "108320",
    "end": "110240"
  },
  {
    "text": "acceleration and of course tensorflow.js",
    "start": "110240",
    "end": "112840"
  },
  {
    "text": "who I'm I'm sure you've heard a lot",
    "start": "112840",
    "end": "114799"
  },
  {
    "text": "about by now and have long been leaders",
    "start": "114799",
    "end": "117320"
  },
  {
    "text": "and Pioneers in web ml",
    "start": "117320",
    "end": "121840"
  },
  {
    "text": "one thing all the previous models and",
    "start": "123439",
    "end": "124960"
  },
  {
    "text": "Frameworks have in common is that",
    "start": "124960",
    "end": "126920"
  },
  {
    "text": "they're all open source which is a",
    "start": "126920",
    "end": "128800"
  },
  {
    "text": "prerequisite to running models in the",
    "start": "128800",
    "end": "130360"
  },
  {
    "text": "browser because they run locally on a",
    "start": "130360",
    "end": "132640"
  },
  {
    "text": "user's",
    "start": "132640",
    "end": "135040"
  },
  {
    "text": "machine llms on the other hand have been",
    "start": "136000",
    "end": "138760"
  },
  {
    "text": "to date pretty dominated by private",
    "start": "138760",
    "end": "140400"
  },
  {
    "text": "companies with pockets deep enough to",
    "start": "140400",
    "end": "142360"
  },
  {
    "text": "train powerful models like open AI",
    "start": "142360",
    "end": "145080"
  },
  {
    "text": "anthropic and",
    "start": "145080",
    "end": "146720"
  },
  {
    "text": "Google an open question is whether the",
    "start": "146720",
    "end": "149160"
  },
  {
    "text": "best open s llms can compete with the",
    "start": "149160",
    "end": "151120"
  },
  {
    "text": "likes of gp4 Claude 2 and palm or at",
    "start": "151120",
    "end": "155959"
  },
  {
    "text": "least approximate enough functionality",
    "start": "155959",
    "end": "157440"
  },
  {
    "text": "for a wide enough variety of use",
    "start": "157440",
    "end": "161159"
  },
  {
    "text": "cases and recent benchmarks suggest that",
    "start": "161840",
    "end": "164159"
  },
  {
    "text": "this is becoming the case the largest",
    "start": "164159",
    "end": "166640"
  },
  {
    "text": "variant of excuse me the largest variant",
    "start": "166640",
    "end": "169239"
  },
  {
    "text": "of a recent state-of-the-art open source",
    "start": "169239",
    "end": "170959"
  },
  {
    "text": "model llama 2 for meta Stacks up quite",
    "start": "170959",
    "end": "174319"
  },
  {
    "text": "well with gbt 3.5 turbo on language",
    "start": "174319",
    "end": "178120"
  },
  {
    "text": "benchmarks though it lags behind on",
    "start": "178120",
    "end": "180000"
  },
  {
    "text": "coding benchmarks it of course has the",
    "start": "180000",
    "end": "182080"
  },
  {
    "text": "added benefit of being free and",
    "start": "182080",
    "end": "184560"
  },
  {
    "text": "completely",
    "start": "184560",
    "end": "186959"
  },
  {
    "text": "private so to test this for myself I",
    "start": "188720",
    "end": "191200"
  },
  {
    "text": "decided to try recreating one of the",
    "start": "191200",
    "end": "192720"
  },
  {
    "text": "most popular use cases of L chain with",
    "start": "192720",
    "end": "194959"
  },
  {
    "text": "open source locally running software a",
    "start": "194959",
    "end": "197959"
  },
  {
    "text": "chain that performs retrieval augmented",
    "start": "197959",
    "end": "199720"
  },
  {
    "text": "generation or rag for short and allows",
    "start": "199720",
    "end": "202720"
  },
  {
    "text": "you to chat with your",
    "start": "202720",
    "end": "205080"
  },
  {
    "text": "documents the general idea is to",
    "start": "205080",
    "end": "207239"
  },
  {
    "text": "customize llm generated output us using",
    "start": "207239",
    "end": "209959"
  },
  {
    "text": "data from an external source as",
    "start": "209959",
    "end": "212319"
  },
  {
    "text": "context though this can be as simple as",
    "start": "212319",
    "end": "214640"
  },
  {
    "text": "stuffing the entire Source document into",
    "start": "214640",
    "end": "216319"
  },
  {
    "text": "the prompt we often have to work around",
    "start": "216319",
    "end": "218040"
  },
  {
    "text": "limitations like the maximum number of",
    "start": "218040",
    "end": "219720"
  },
  {
    "text": "tokens the model can accept and",
    "start": "219720",
    "end": "221760"
  },
  {
    "text": "distracting the llm with unnecessary",
    "start": "221760",
    "end": "226280"
  },
  {
    "text": "information and here's one way this can",
    "start": "227720",
    "end": "230120"
  },
  {
    "text": "work we first split the document in",
    "start": "230120",
    "end": "232599"
  },
  {
    "text": "semantic chunks with a text splitter",
    "start": "232599",
    "end": "234640"
  },
  {
    "text": "then use an embeddings model to convert",
    "start": "234640",
    "end": "236239"
  },
  {
    "text": "each chunk into a vector that captures",
    "start": "236239",
    "end": "238680"
  },
  {
    "text": "the Chunk meaning",
    "start": "238680",
    "end": "241040"
  },
  {
    "text": "converting text into a representation",
    "start": "241040",
    "end": "243079"
  },
  {
    "text": "like this is called embedding the",
    "start": "243079",
    "end": "246959"
  },
  {
    "text": "text we then load these vectors into a",
    "start": "246959",
    "end": "249599"
  },
  {
    "text": "specialized database called a vector",
    "start": "249599",
    "end": "251319"
  },
  {
    "text": "store that allows us to search for a",
    "start": "251319",
    "end": "253120"
  },
  {
    "text": "similar Vector to an",
    "start": "253120",
    "end": "256280"
  },
  {
    "text": "input our strategy will be later to",
    "start": "257680",
    "end": "260400"
  },
  {
    "text": "search for the most meaningfully similar",
    "start": "260400",
    "end": "262240"
  },
  {
    "text": "chunks given a query and feed the text",
    "start": "262240",
    "end": "264560"
  },
  {
    "text": "from the original document corresponding",
    "start": "264560",
    "end": "266199"
  },
  {
    "text": "to that chunk back into the llm to",
    "start": "266199",
    "end": "268280"
  },
  {
    "text": "generate a final output",
    "start": "268280",
    "end": "271600"
  },
  {
    "text": "and there's one additional thing we need",
    "start": "272600",
    "end": "273639"
  },
  {
    "text": "to account for during conversations it's",
    "start": "273639",
    "end": "276240"
  },
  {
    "text": "common to reference past interactions",
    "start": "276240",
    "end": "277840"
  },
  {
    "text": "with pronouns or other references for",
    "start": "277840",
    "end": "280120"
  },
  {
    "text": "example you might ask who's this",
    "start": "280120",
    "end": "282160"
  },
  {
    "text": "document",
    "start": "282160",
    "end": "283400"
  },
  {
    "text": "about and then follow up with do they",
    "start": "283400",
    "end": "286120"
  },
  {
    "text": "know",
    "start": "286120",
    "end": "287160"
  },
  {
    "text": "JavaScript they in this case would be a",
    "start": "287160",
    "end": "289199"
  },
  {
    "text": "reference to the answer to the first",
    "start": "289199",
    "end": "290759"
  },
  {
    "text": "question this can throw off the vector",
    "start": "290759",
    "end": "292720"
  },
  {
    "text": "store",
    "start": "292720",
    "end": "295039"
  },
  {
    "text": "search so to get around this issue we",
    "start": "295520",
    "end": "297960"
  },
  {
    "text": "add an additional step that passes the",
    "start": "297960",
    "end": "300160"
  },
  {
    "text": "chat history and user query into an llm",
    "start": "300160",
    "end": "302759"
  },
  {
    "text": "and asks it to rephrase the query as a",
    "start": "302759",
    "end": "304400"
  },
  {
    "text": "standalone question that's a question",
    "start": "304400",
    "end": "306520"
  },
  {
    "text": "free of references to past chat history",
    "start": "306520",
    "end": "309360"
  },
  {
    "text": "and use that to search our Vector",
    "start": "309360",
    "end": "311360"
  },
  {
    "text": "store we then take the resulting chunk",
    "start": "311360",
    "end": "314120"
  },
  {
    "text": "original question and chat history and",
    "start": "314120",
    "end": "316919"
  },
  {
    "text": "pass it to a final llm call to",
    "start": "316919",
    "end": "318960"
  },
  {
    "text": "synthesize and",
    "start": "318960",
    "end": "321800"
  },
  {
    "text": "answer here's a quick outline of the",
    "start": "324520",
    "end": "326560"
  },
  {
    "text": "specific OSS pieces I used for my fully",
    "start": "326560",
    "end": "329000"
  },
  {
    "text": "local chat over documents",
    "start": "329000",
    "end": "330800"
  },
  {
    "text": "chain a new state-of-the-art small llm",
    "start": "330800",
    "end": "334240"
  },
  {
    "text": "called mistol run using a handy tool",
    "start": "334240",
    "end": "336560"
  },
  {
    "text": "called a llama Zen's Transformers JS to",
    "start": "336560",
    "end": "339919"
  },
  {
    "text": "load a hugging face embeddings model and",
    "start": "339919",
    "end": "342960"
  },
  {
    "text": "a neat open- Source web assembly Vector",
    "start": "342960",
    "end": "345160"
  },
  {
    "text": "store called",
    "start": "345160",
    "end": "348080"
  },
  {
    "text": "voy and I actually shipped a live",
    "start": "349479",
    "end": "351639"
  },
  {
    "text": "version of this on versell as a next.js",
    "start": "351639",
    "end": "354319"
  },
  {
    "text": "app you can try it for yourself at webml",
    "start": "354319",
    "end": "357440"
  },
  {
    "text": "dodo. R.A apppp that'll quickly show off",
    "start": "357440",
    "end": "360400"
  },
  {
    "text": "how it looks now",
    "start": "360400",
    "end": "363919"
  },
  {
    "text": "live",
    "start": "366440",
    "end": "368080"
  },
  {
    "text": "cool so I'm going to navigate to my next",
    "start": "368080",
    "end": "371199"
  },
  {
    "text": "JS app here move myself up here so I'm",
    "start": "371199",
    "end": "374080"
  },
  {
    "text": "not in the way and I'm going to start by",
    "start": "374080",
    "end": "375720"
  },
  {
    "text": "uploading a old resume of",
    "start": "375720",
    "end": "379800"
  },
  {
    "text": "mine and what's going to happen behind",
    "start": "380199",
    "end": "382280"
  },
  {
    "text": "the scenes here is exactly what I showed",
    "start": "382280",
    "end": "383800"
  },
  {
    "text": "you on the previous slide it's going to",
    "start": "383800",
    "end": "386720"
  },
  {
    "text": "read the document and",
    "start": "386720",
    "end": "388400"
  },
  {
    "text": "embed split it into chunks and embed",
    "start": "388400",
    "end": "390440"
  },
  {
    "text": "those chunks into a vector store and now",
    "start": "390440",
    "end": "393080"
  },
  {
    "text": "if I try asking who is this document",
    "start": "393080",
    "end": "398720"
  },
  {
    "text": "about it's going through the steps and",
    "start": "398720",
    "end": "401160"
  },
  {
    "text": "it's going to query the vector store",
    "start": "401160",
    "end": "402599"
  },
  {
    "text": "with my query",
    "start": "402599",
    "end": "405440"
  },
  {
    "text": "here and it tells me this document",
    "start": "408479",
    "end": "410560"
  },
  {
    "text": "appears to be about Jacob J Lee which is",
    "start": "410560",
    "end": "412479"
  },
  {
    "text": "correct and I can ask a followup",
    "start": "412479",
    "end": "414360"
  },
  {
    "text": "question such as do they know",
    "start": "414360",
    "end": "418400"
  },
  {
    "text": "JavaScript",
    "start": "418400",
    "end": "420879"
  },
  {
    "text": "and it'll be re it'll rephrase that",
    "start": "420879",
    "end": "422520"
  },
  {
    "text": "question free of St uh free of",
    "start": "422520",
    "end": "424520"
  },
  {
    "text": "references into a standalone question",
    "start": "424520",
    "end": "426840"
  },
  {
    "text": "which should successfully get good",
    "start": "426840",
    "end": "428560"
  },
  {
    "text": "results from the vector store yes based",
    "start": "428560",
    "end": "430599"
  },
  {
    "text": "on the information provided in the",
    "start": "430599",
    "end": "431759"
  },
  {
    "text": "search results it appears that Jacob J",
    "start": "431759",
    "end": "433240"
  },
  {
    "text": "Lee has experienced with JavaScript and",
    "start": "433240",
    "end": "434919"
  },
  {
    "text": "it goes into bit about my experience",
    "start": "434919",
    "end": "436919"
  },
  {
    "text": "there pretty cool so effectively quering",
    "start": "436919",
    "end": "440520"
  },
  {
    "text": "over chatting over documents with all",
    "start": "440520",
    "end": "442440"
  },
  {
    "text": "local",
    "start": "442440",
    "end": "443400"
  },
  {
    "text": "models back to the",
    "start": "443400",
    "end": "446840"
  },
  {
    "text": "talk",
    "start": "448319",
    "end": "450280"
  },
  {
    "text": "so to better help un uh to help you",
    "start": "450280",
    "end": "452120"
  },
  {
    "text": "better understand the individual steps",
    "start": "452120",
    "end": "453319"
  },
  {
    "text": "in this chain I prepared traces using",
    "start": "453319",
    "end": "455080"
  },
  {
    "text": "our external llm observability and",
    "start": "455080",
    "end": "457360"
  },
  {
    "text": "tracing platform lsmith that you can",
    "start": "457360",
    "end": "459800"
  },
  {
    "text": "interact with after the talk at the",
    "start": "459800",
    "end": "461720"
  },
  {
    "text": "links in the",
    "start": "461720",
    "end": "465000"
  },
  {
    "text": "slide so I I do have a slight confession",
    "start": "466879",
    "end": "469240"
  },
  {
    "text": "to make however and it's one that",
    "start": "469240",
    "end": "470560"
  },
  {
    "text": "underscores some of the current",
    "start": "470560",
    "end": "471680"
  },
  {
    "text": "difficulties with building with large",
    "start": "471680",
    "end": "473479"
  },
  {
    "text": "language models in the browser AMA isn't",
    "start": "473479",
    "end": "476280"
  },
  {
    "text": "running in the browser it's actually a",
    "start": "476280",
    "end": "478199"
  },
  {
    "text": "desktop app that runs on my",
    "start": "478199",
    "end": "482120"
  },
  {
    "text": "MacBook so why do you ask why did I do",
    "start": "483840",
    "end": "486680"
  },
  {
    "text": "this I previously mentioned some of the",
    "start": "486680",
    "end": "488639"
  },
  {
    "text": "cool efforts from the machine learning",
    "start": "488639",
    "end": "489879"
  },
  {
    "text": "compilation to run llms directly in the",
    "start": "489879",
    "end": "492039"
  },
  {
    "text": "browser but the fact is large language",
    "start": "492039",
    "end": "494599"
  },
  {
    "text": "models are large and techniques and new",
    "start": "494599",
    "end": "498120"
  },
  {
    "text": "developments are coming out almost",
    "start": "498120",
    "end": "500520"
  },
  {
    "text": "monthly at this",
    "start": "500520",
    "end": "501960"
  },
  {
    "text": "point my initial version of this talk",
    "start": "501960",
    "end": "504039"
  },
  {
    "text": "used the smallest variant of llama 2 the",
    "start": "504039",
    "end": "506479"
  },
  {
    "text": "7 billion parameter variant which still",
    "start": "506479",
    "end": "508960"
  },
  {
    "text": "took up 7 GB of dis space on my",
    "start": "508960",
    "end": "511399"
  },
  {
    "text": "computer and even then a week while",
    "start": "511399",
    "end": "514719"
  },
  {
    "text": "recording this talk a week later a new",
    "start": "514719",
    "end": "516560"
  },
  {
    "text": "hyped model came out mistro which I",
    "start": "516560",
    "end": "518518"
  },
  {
    "text": "ended up using for the",
    "start": "518519",
    "end": "521120"
  },
  {
    "text": "demo the more powerful variant of llama",
    "start": "521120",
    "end": "523360"
  },
  {
    "text": "that performed well against GPT 3.5",
    "start": "523360",
    "end": "525000"
  },
  {
    "text": "turbo requires 129 GB of d g space and",
    "start": "525000",
    "end": "528600"
  },
  {
    "text": "far more RAM and bigger ones are coming",
    "start": "528600",
    "end": "531279"
  },
  {
    "text": "out all the time the bottom screen shot",
    "start": "531279",
    "end": "533480"
  },
  {
    "text": "is from the Yama team announcing support",
    "start": "533480",
    "end": "535040"
  },
  {
    "text": "for another 180 billion parameter model",
    "start": "535040",
    "end": "538000"
  },
  {
    "text": "that requires a requires a whopping 192",
    "start": "538000",
    "end": "540640"
  },
  {
    "text": "GB of RAM not disc space since I don't",
    "start": "540640",
    "end": "544160"
  },
  {
    "text": "have a supercomputer I wasn't able to",
    "start": "544160",
    "end": "545640"
  },
  {
    "text": "run it for the",
    "start": "545640",
    "end": "547800"
  },
  {
    "text": "demo and so even with a gigabit internet",
    "start": "547800",
    "end": "550279"
  },
  {
    "text": "connection downloading such a large",
    "start": "550279",
    "end": "551760"
  },
  {
    "text": "model on page load adds a ton of latency",
    "start": "551760",
    "end": "554000"
  },
  {
    "text": "in startup time caching protocols aren't",
    "start": "554000",
    "end": "556800"
  },
  {
    "text": "currently standardized which can result",
    "start": "556800",
    "end": "558279"
  },
  {
    "text": "in the browser cache filling up quite",
    "start": "558279",
    "end": "561640"
  },
  {
    "text": "quickly so while someday folks like",
    "start": "562320",
    "end": "564360"
  },
  {
    "text": "zenova may be able to quantize effective",
    "start": "564360",
    "end": "566320"
  },
  {
    "text": "llms and make them feasible to run and",
    "start": "566320",
    "end": "568040"
  },
  {
    "text": "cache in the browser individual sites",
    "start": "568040",
    "end": "569839"
  },
  {
    "text": "you visit because of the current size of",
    "start": "569839",
    "end": "572399"
  },
  {
    "text": "useful llms it made more sense for me to",
    "start": "572399",
    "end": "574800"
  },
  {
    "text": "ask the user to download a powerful",
    "start": "574800",
    "end": "577440"
  },
  {
    "text": "general purpose llm that they could",
    "start": "577440",
    "end": "579000"
  },
  {
    "text": "update on their own and expose to a web",
    "start": "579000",
    "end": "581120"
  },
  {
    "text": "app using a shell",
    "start": "581120",
    "end": "583760"
  },
  {
    "text": "command but since your average",
    "start": "585839",
    "end": "588240"
  },
  {
    "text": "non-technical web app user wouldn't be",
    "start": "588240",
    "end": "590000"
  },
  {
    "text": "comfortable running such command there's",
    "start": "590000",
    "end": "591880"
  },
  {
    "text": "a missing piece here and that piece is a",
    "start": "591880",
    "end": "594040"
  },
  {
    "text": "new browser API that would allow web",
    "start": "594040",
    "end": "596360"
  },
  {
    "text": "apps to perform effective text inference",
    "start": "596360",
    "end": "598399"
  },
  {
    "text": "with large language",
    "start": "598399",
    "end": "600480"
  },
  {
    "text": "models it would essentially recreate the",
    "start": "600480",
    "end": "602519"
  },
  {
    "text": "shell command using a popup asking the",
    "start": "602519",
    "end": "605000"
  },
  {
    "text": "user for permission to run inference via",
    "start": "605000",
    "end": "607600"
  },
  {
    "text": "on a locally running powerful general",
    "start": "607600",
    "end": "609240"
  },
  {
    "text": "purpose Foundation model such as mistol",
    "start": "609240",
    "end": "611040"
  },
  {
    "text": "or llama",
    "start": "611040",
    "end": "612120"
  },
  {
    "text": "2 the web app could then take advantage",
    "start": "612120",
    "end": "614320"
  },
  {
    "text": "of some of the reasoning capabilities",
    "start": "614320",
    "end": "615480"
  },
  {
    "text": "that a larger model provides while",
    "start": "615480",
    "end": "617200"
  },
  {
    "text": "dedicating smaller domain specific tasks",
    "start": "617200",
    "end": "619600"
  },
  {
    "text": "like text embeddings to in browser",
    "start": "619600",
    "end": "621720"
  },
  {
    "text": "models much like in the",
    "start": "621720",
    "end": "625079"
  },
  {
    "text": "demo and while locally running models",
    "start": "625200",
    "end": "627560"
  },
  {
    "text": "aren't iniquitous at the moment both the",
    "start": "627560",
    "end": "629640"
  },
  {
    "text": "software and Hardware required to run",
    "start": "629640",
    "end": "631399"
  },
  {
    "text": "such models is improving",
    "start": "631399",
    "end": "633320"
  },
  {
    "text": "rapidly Apple silicon gpus are now",
    "start": "633320",
    "end": "635800"
  },
  {
    "text": "standard in MacBooks and iPhones and in",
    "start": "635800",
    "end": "638240"
  },
  {
    "text": "the future I believe it'll be possible",
    "start": "638240",
    "end": "640360"
  },
  {
    "text": "for just about every device to run an",
    "start": "640360",
    "end": "641880"
  },
  {
    "text": "effective llm",
    "start": "641880",
    "end": "645440"
  },
  {
    "text": "locally and that's it thank you for",
    "start": "645800",
    "end": "648120"
  },
  {
    "text": "listening and I hope you enjoyed my talk",
    "start": "648120",
    "end": "650760"
  },
  {
    "text": "if you'd like to keep in touch you can",
    "start": "650760",
    "end": "652160"
  },
  {
    "text": "follow me at hubu on X formerly",
    "start": "652160",
    "end": "655360"
  },
  {
    "text": "Twitter and Lang chain linkchain AI",
    "start": "655360",
    "end": "659880"
  },
  {
    "text": "I've also included links for both the",
    "start": "659880",
    "end": "661279"
  },
  {
    "text": "live demo app the source code for the",
    "start": "661279",
    "end": "663079"
  },
  {
    "text": "app and all the Technologies usable in",
    "start": "663079",
    "end": "664880"
  },
  {
    "text": "in it",
    "start": "664880",
    "end": "666079"
  },
  {
    "text": "below thanks again for listening and I",
    "start": "666079",
    "end": "668279"
  },
  {
    "text": "hope you enjoy the rest of the",
    "start": "668279",
    "end": "671519"
  },
  {
    "text": "summit",
    "start": "672040",
    "end": "675040"
  }
]