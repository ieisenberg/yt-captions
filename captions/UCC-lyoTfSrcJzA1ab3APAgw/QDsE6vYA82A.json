[
  {
    "text": "thank you hello everyone and hello emmer who joined right as we're going live we're",
    "start": "0",
    "end": "6180"
  },
  {
    "text": "now live so thank you everyone for for joining and and tuning in to the uh",
    "start": "6180",
    "end": "12120"
  },
  {
    "text": "hallucinations and document question answering webinar so it should be a really fun one we've heard this as a big pain point from a lot of folks",
    "start": "12120",
    "end": "19619"
  },
  {
    "text": "um they try to put partially answering systems in production um and so yeah really excited to be here",
    "start": "19619",
    "end": "27359"
  },
  {
    "text": "we've got four awesome presentations um Daniel should hopefully be joining he was on in the Green Room earlier um and",
    "start": "27359",
    "end": "33899"
  },
  {
    "text": "then he went offline but we hope to see him back here soon um so what we're going to do uh is maybe",
    "start": "33899",
    "end": "39360"
  },
  {
    "text": "we can do quick introductions from everyone then we'll jump into presentations from each each group for",
    "start": "39360",
    "end": "45360"
  },
  {
    "text": "like eightish minutes um and then we'll do question answering at the end minor logistical stuff before we get started this is being recorded",
    "start": "45360",
    "end": "52860"
  },
  {
    "text": "um so so it will be available at the link after the fact um the for for questions",
    "start": "52860",
    "end": "60120"
  },
  {
    "text": "if you have questions for the presenters please put them in the question answering box on the right so you can",
    "start": "60120",
    "end": "66600"
  },
  {
    "text": "see there's a little chat icon and then below that that there's a little box with a question mark please put questions in there and then upload the",
    "start": "66600",
    "end": "73200"
  },
  {
    "text": "ones that you want to hear answered as well and those will be the ones that will prioritize if there's immediate things that pop up",
    "start": "73200",
    "end": "79260"
  },
  {
    "text": "um like zooming in or audio gets bad during the presentation please put those in the chat um although for video quality I'd",
    "start": "79260",
    "end": "85560"
  },
  {
    "text": "recommend kind of like refreshing the page we've seen that a few times um yeah I think that's all the logistical",
    "start": "85560",
    "end": "92159"
  },
  {
    "text": "stuff so maybe we can jump in quick to Quick introductions from everyone and then get into the first presentation",
    "start": "92159",
    "end": "97380"
  },
  {
    "text": "Nick do you want to start yeah for sure so hi everyone my name is Nick I'm the co-founder of mendible",
    "start": "97380",
    "end": "104820"
  },
  {
    "text": "um and yeah amendable I mean if you if you haven't seen us in the Lang chain docs yet we basically provide app art",
    "start": "104820",
    "end": "110759"
  },
  {
    "text": "search for developer documentation awesome Mathis do you want to go next",
    "start": "110759",
    "end": "118140"
  },
  {
    "text": "yeah um hi uh nice uh talking to you today and thanks for the invitation",
    "start": "118140",
    "end": "123720"
  },
  {
    "text": "definitely um I'm matters um I'm head of product at Deep set and",
    "start": "123720",
    "end": "130080"
  },
  {
    "text": "um we have an open source framework called a stack basically to build NLP applications and then commercial product",
    "start": "130080",
    "end": "138060"
  },
  {
    "text": "called Deep set Cloud which is more focused on Enterprises building NLP",
    "start": "138060",
    "end": "143580"
  },
  {
    "text": "applications great and Daniel do you want to go",
    "start": "143580",
    "end": "151040"
  },
  {
    "text": "oh we seem to be having some internet connections with Dan also maybe offer",
    "start": "157379",
    "end": "163500"
  },
  {
    "text": "and amarf and Victoria do you guys want to do a quick intro sure I see Daniel moving again now he",
    "start": "163500",
    "end": "170340"
  },
  {
    "text": "was disconnected for a second hey Daniel we were asking you to introduce yourself try again",
    "start": "170340",
    "end": "176519"
  },
  {
    "text": "yeah guys uh check check sound check can you hear me all good yeah",
    "start": "176519",
    "end": "182540"
  },
  {
    "text": "yeah and now we work uh they're having some troubles um hey guys I'm Daniel I'm a foreign",
    "start": "182540",
    "end": "189300"
  },
  {
    "text": "engineer at Shopper uh learning initial attack in a commercial construction space and on the nights and weekends",
    "start": "189300",
    "end": "194879"
  },
  {
    "text": "I've been hacking on this branch called Auto evaluator which is an evaluation framework for testing Qi systems we're",
    "start": "194879",
    "end": "202560"
  },
  {
    "text": "excited to give you all a demo and here are some learnings building that",
    "start": "202560",
    "end": "207980"
  },
  {
    "text": "okay I'll go next uh so hi hi folks I'm Amar Abdullah I previously was the",
    "start": "209159",
    "end": "214920"
  },
  {
    "text": "founder and CTO of Cloudera obviously some of the Hadoop or mapreduce or spark or some of the great things we have in",
    "start": "214920",
    "end": "221340"
  },
  {
    "text": "our distribution uh currently founder and co-founder and CEO for Victora and",
    "start": "221340",
    "end": "228180"
  },
  {
    "text": "our mission at Victoria is to allow you to easily have a conversation with your data so I have an API that make it very",
    "start": "228180",
    "end": "234480"
  },
  {
    "text": "straightforward to upload whatever data you have and then make another call to have a conversation with that data and",
    "start": "234480",
    "end": "240060"
  },
  {
    "text": "then today I also have offer with me offer you want to briefly introduce yourself yes hi everyone can you can you hear me",
    "start": "240060",
    "end": "247140"
  },
  {
    "text": "yeah uh so nice nice to meet everyone thanks all for coming uh I'm also at",
    "start": "247140",
    "end": "252720"
  },
  {
    "text": "Victoria working with Amber on developer advocacy uh and have been working on uh llms uh",
    "start": "252720",
    "end": "259919"
  },
  {
    "text": "for a while for about three or four years so I like to think of we're talking about hallucinations today I",
    "start": "259919",
    "end": "265139"
  },
  {
    "text": "think when it was gpt2 which is called it nonsense but it was a different thing because it was just in Korean stuff but",
    "start": "265139",
    "end": "271620"
  },
  {
    "text": "anyway glad to be part of this conversation and looking forward to what's next and next",
    "start": "271620",
    "end": "277620"
  },
  {
    "text": "all right so so we've got a packed packed webinar awesome guest really excited to hear from everyone let's",
    "start": "277620",
    "end": "283979"
  },
  {
    "text": "start with you Nick awesome yeah so let me share my screen",
    "start": "283979",
    "end": "289440"
  },
  {
    "text": "real quick",
    "start": "289440",
    "end": "292160"
  },
  {
    "text": "all right let me know when y'all can see I can see it awesome",
    "start": "299280",
    "end": "307199"
  },
  {
    "text": "great so let me skip through this uh so before we start I just want to give a quick overview of mandible again you",
    "start": "307199",
    "end": "313020"
  },
  {
    "text": "know so we're basically we provide chatbart search for documentation uh we're in the link chain docs element",
    "start": "313020",
    "end": "318540"
  },
  {
    "text": "index uh right now we're currently focused on developer uh Focus companies and Enterprises",
    "start": "318540",
    "end": "325259"
  },
  {
    "text": "um and we also we provide like embeddable components uh that's on react JavaScript in our API so you can",
    "start": "325259",
    "end": "331680"
  },
  {
    "text": "Implement chat uh in your own products so um going to next year I think it's",
    "start": "331680",
    "end": "338400"
  },
  {
    "text": "pretty important for us to Define what we're talking about and hallucination and I think like",
    "start": "338400",
    "end": "343860"
  },
  {
    "text": "um everyone has kind of like a different persp uh different definition of what hallucination means uh so I'm gonna go",
    "start": "343860",
    "end": "351300"
  },
  {
    "text": "I'm gonna say like I'm gonna go with what mendable things uh and what mandible defines as a hallucination in",
    "start": "351300",
    "end": "357960"
  },
  {
    "text": "our evaluation data set but I would love to hear from everyone what um what and hallucination really means",
    "start": "357960",
    "end": "364800"
  },
  {
    "text": "so for us um is when the AI model produces a result that was not present or formed on",
    "start": "364800",
    "end": "370979"
  },
  {
    "text": "the documents that we fed through the prompt so that's when we call it a hallucination and I mean in our use case",
    "start": "370979",
    "end": "377220"
  },
  {
    "text": "this often leads to incorrect result but not always right because it definitely",
    "start": "377220",
    "end": "382860"
  },
  {
    "text": "leads uh to creative answers sometimes and I I mean the incorrect results part",
    "start": "382860",
    "end": "388500"
  },
  {
    "text": "as um because the model probably wasn't trained on our customers data for",
    "start": "388500",
    "end": "394139"
  },
  {
    "text": "example right so the model has no idea what it's saying so if we feed something wrong",
    "start": "394139",
    "end": "400319"
  },
  {
    "text": "um the model will most likely produce something incorrect so that's how we Define what I would",
    "start": "400319",
    "end": "406080"
  },
  {
    "text": "love to hear from our speakers here um what they think uh hallucination really means",
    "start": "406080",
    "end": "413900"
  },
  {
    "text": "well so one common knife here and I'm curious for other tastes of all Jedi but so just to clarify like there's kind of",
    "start": "414539",
    "end": "421380"
  },
  {
    "text": "like getting a question wrong or hallucinating because so you're doing like retrieval augmented generation",
    "start": "421380",
    "end": "426539"
  },
  {
    "text": "where you have a question then you retrieve some documents and then you basically ask the model to come up with some answer based on those documents and",
    "start": "426539",
    "end": "432720"
  },
  {
    "text": "you're saying Hallucination is when like uh could you go back to the definition it's like uh it's it produces a result",
    "start": "432720",
    "end": "439380"
  },
  {
    "text": "that was not present or formed based on the document shred through the pump so I guess like how do you think about or in",
    "start": "439380",
    "end": "444720"
  },
  {
    "text": "like do you distinguish between cases when like the because the big step in there is the retrieval step right and",
    "start": "444720",
    "end": "450900"
  },
  {
    "text": "putting the documents into the prompt and so yeah how how do you think about that and distinguish between that in hallucination like is there is there a",
    "start": "450900",
    "end": "457740"
  },
  {
    "text": "hallucination where it like retrieves the right results but the model just doesn't pick them out and is that like",
    "start": "457740",
    "end": "464340"
  },
  {
    "text": "and how do you think about that differently from when the model doesn't retrieve the right results or maybe",
    "start": "464340",
    "end": "469860"
  },
  {
    "text": "retrieves like half of the results and start and hallucinates half of it or something like how do you yeah totally",
    "start": "469860",
    "end": "476220"
  },
  {
    "text": "totally can I give an example actually uh that that it's a very funny example of somebody a very famous reporter from",
    "start": "476220",
    "end": "483120"
  },
  {
    "text": "The New York Times they're not going to say his name but his name starts with c and he was at some other uh famous",
    "start": "483120",
    "end": "489120"
  },
  {
    "text": "machine Learning Company their name also starts with C which is very interesting and they had indexed their",
    "start": "489120",
    "end": "494759"
  },
  {
    "text": "documentations and then they asked the question what is temperature and temperature above large language models",
    "start": "494759",
    "end": "499800"
  },
  {
    "text": "here right which is the the randomness in the output words being produced and that the response came back explaining",
    "start": "499800",
    "end": "505919"
  },
  {
    "text": "temperature well like in in terms of the context of a large language model but then the last sentence was and",
    "start": "505919",
    "end": "511500"
  },
  {
    "text": "temperature is measured in Kelvin now that was hallucination because this",
    "start": "511500",
    "end": "517200"
  },
  {
    "text": "is not the same temperature that we use for physics this is the temperature that we use for Life language models so that's what we mean when you mean",
    "start": "517200",
    "end": "522779"
  },
  {
    "text": "hallucination like there's additional outputs that the model contributes that has nothing to do with the facts that we",
    "start": "522779",
    "end": "528240"
  },
  {
    "text": "give to it exactly",
    "start": "528240",
    "end": "533600"
  },
  {
    "text": "yeah I think this is really it's really interesting and I think the for yeah exactly the information retrieval piece",
    "start": "533600",
    "end": "539880"
  },
  {
    "text": "for us is very critical um but there are cases where of course",
    "start": "539880",
    "end": "545220"
  },
  {
    "text": "there's like not the majority of cases at all but there are cases where the document did we did pass the right",
    "start": "545220",
    "end": "550680"
  },
  {
    "text": "documents and it actually didn't like complete and those are honestly like the hard ones to really like uh like Auto",
    "start": "550680",
    "end": "558120"
  },
  {
    "text": "evaluate um normally we use some we need to like uh like one of the biggest things that",
    "start": "558120",
    "end": "565920"
  },
  {
    "text": "we do is we go manually through it uh and see it and check it um",
    "start": "565920",
    "end": "572160"
  },
  {
    "text": "but moving forward here I think like some ways that we use uh to to reduce",
    "start": "572160",
    "end": "578160"
  },
  {
    "text": "General hallucinations I think this is really important uh and the first one is like pretty classic and pretty um",
    "start": "578160",
    "end": "583860"
  },
  {
    "text": "obvious and it would be like a true prompt engineering so I think here like some four techniques they listed here",
    "start": "583860",
    "end": "589980"
  },
  {
    "text": "that we use and um that might help you and your use case um so like the first one is like a",
    "start": "589980",
    "end": "595260"
  },
  {
    "text": "request for evidence right so like always making sure like hey according to the provided documents refer to the",
    "start": "595260",
    "end": "600600"
  },
  {
    "text": "documents when answering quote the original document uh the second one is like setting boundaries so this is what",
    "start": "600600",
    "end": "607140"
  },
  {
    "text": "um we basically set a boundary in the prompt like hey here's where the the verified documents are going to be",
    "start": "607140",
    "end": "613320"
  },
  {
    "text": "here's where it's going to end answer solely based on that chunk",
    "start": "613320",
    "end": "618899"
  },
  {
    "text": "um and then another one is the step-by-step reasoning you've probably seen in a million websites and",
    "start": "618899",
    "end": "625620"
  },
  {
    "text": "but you know let's think step by step you know um how can we answer this based on the",
    "start": "625620",
    "end": "631800"
  },
  {
    "text": "provided documents um and I think my favorite is the described first in detail and it's",
    "start": "631800",
    "end": "637860"
  },
  {
    "text": "probably very it's very similar to what chapter PT does but basically like describe the",
    "start": "637860",
    "end": "643440"
  },
  {
    "text": "question in detail before attempting to answer you know really provide that",
    "start": "643440",
    "end": "648480"
  },
  {
    "text": "description really like um teaching them having the model say uh",
    "start": "648480",
    "end": "654360"
  },
  {
    "text": "exactly what it's thinking so it can basically be more creative when completing uh I think it's really",
    "start": "654360",
    "end": "660600"
  },
  {
    "text": "important Nick can I ask one quick question on the request for evidence but",
    "start": "660600",
    "end": "665880"
  },
  {
    "text": "um do you do you verify that the documents that they quote or cite are",
    "start": "665880",
    "end": "670920"
  },
  {
    "text": "actually valid documents that were kind of like passed in based on the retrieval step",
    "start": "670920",
    "end": "676940"
  },
  {
    "text": "yes yes we do um we have um we do verify if the documents is",
    "start": "677060",
    "end": "684180"
  },
  {
    "text": "actually correct we do have a we also like and then I'm gonna get into later but we do",
    "start": "684180",
    "end": "689880"
  },
  {
    "text": "um something that helps us a lot is like evaluating each step of our pipeline separately so we have like an evaluation",
    "start": "689880",
    "end": "696720"
  },
  {
    "text": "system just for the retrieval mechanism we have evaluation system just for the completion mechanism but we also have an",
    "start": "696720",
    "end": "702899"
  },
  {
    "text": "evaluation system that takes into account like both the uh retrieval and the completion and we need to make we we",
    "start": "702899",
    "end": "710040"
  },
  {
    "text": "try to make sure that those documents match the right ones um",
    "start": "710040",
    "end": "715320"
  },
  {
    "text": "yeah so I mean going going on the in retrieval system I think another General way to",
    "start": "715320",
    "end": "721920"
  },
  {
    "text": "like reduce hallucinations is just improving your retrieval system at least for us right because we are doing all",
    "start": "721920",
    "end": "728100"
  },
  {
    "text": "this um document injection in the prompt so for us having a good retrieval system means that our answers are going to be",
    "start": "728100",
    "end": "735120"
  },
  {
    "text": "really good or are going to be better than um so basically like for those of you",
    "start": "735120",
    "end": "741240"
  },
  {
    "text": "that are not aware like if you're like retriever for example for us if it grabs irrelevant documents documents that are",
    "start": "741240",
    "end": "748019"
  },
  {
    "text": "not split accordingly you know the completion will probably hallucinate most of the time um at least in our use case so investing",
    "start": "748019",
    "end": "754860"
  },
  {
    "text": "in a good IR evaluation system is really important um also like how the way you split your",
    "start": "754860",
    "end": "760320"
  },
  {
    "text": "tax like do you want to have a larger tax splitting to maintain contacts or do",
    "start": "760320",
    "end": "765420"
  },
  {
    "text": "you want to have smaller ones for like efficiency and more accurate cases",
    "start": "765420",
    "end": "770519"
  },
  {
    "text": "um and another thing we use and you guys can hear about on the we on the previous webinars we we use hybrid search which",
    "start": "770519",
    "end": "777540"
  },
  {
    "text": "is a mixture between like the semantics um search and keyword search to retrieve the documents which",
    "start": "777540",
    "end": "783959"
  },
  {
    "text": "um for us it does improve quite a bit the results um and you guys can feel free to listen",
    "start": "783959",
    "end": "789420"
  },
  {
    "text": "on YouTube I think on the other webinar we talk a lot more about that um",
    "start": "789420",
    "end": "794940"
  },
  {
    "text": "but um just moving forward here I think um yeah go for it",
    "start": "794940",
    "end": "802699"
  },
  {
    "text": "yeah I think I'm curious um would you still consider this a health Nation if the an improper context improper",
    "start": "804060",
    "end": "811019"
  },
  {
    "text": "documents were provided prompt",
    "start": "811019",
    "end": "814279"
  },
  {
    "text": "anyone feel free to jump in that's a good question maybe so how we",
    "start": "816560",
    "end": "821579"
  },
  {
    "text": "think about it is that sometimes we also pass the metadata of the document and then we look at okay from that document",
    "start": "821579",
    "end": "830220"
  },
  {
    "text": "enter metadata could the language model have known that this is not the correct document so if I don't know I'm asking",
    "start": "830220",
    "end": "837120"
  },
  {
    "text": "about the revenue of Google but we pass in a document about Microsoft and it actually has",
    "start": "837120",
    "end": "843120"
  },
  {
    "text": "company Microsoft in in the header kind of um then the large language model can",
    "start": "843120",
    "end": "849779"
  },
  {
    "text": "actually know that's not the right document and what we then do is providing escape hatch so that it says",
    "start": "849779",
    "end": "855959"
  },
  {
    "text": "if if the answer is not there also I don't know um yeah",
    "start": "855959",
    "end": "863100"
  },
  {
    "text": "yeah but clearly the the relevance is super important for this right if you get the",
    "start": "863100",
    "end": "868620"
  },
  {
    "text": "wrong documents you give something bad for the summarizer to",
    "start": "868620",
    "end": "873839"
  },
  {
    "text": "work with and so it'll it'll fail and do a lot more hallucinations right right yeah I think yeah we would",
    "start": "873839",
    "end": "881160"
  },
  {
    "text": "consider as like I think it would be like hallucination as like the complete like workflow but we would identify that",
    "start": "881160",
    "end": "887339"
  },
  {
    "text": "was because the IR Retreat the wrong documents and not necessarily the motto being wrong and not responding",
    "start": "887339",
    "end": "895440"
  },
  {
    "text": "but yeah that's really interesting um and so some ways that we use to prevent",
    "start": "895440",
    "end": "901800"
  },
  {
    "text": "incorrect results now so going from the hallucination to like actually being factually incorrect",
    "start": "901800",
    "end": "907380"
  },
  {
    "text": "so I mean on the IR retrieval system we detect when the information returns zero",
    "start": "907380",
    "end": "912959"
  },
  {
    "text": "documents for example right it can retrieve any documents or the documents are like too low score so that raises",
    "start": "912959",
    "end": "918240"
  },
  {
    "text": "like a big flag for us in the backhand and we're like okay like we're not feeding anything to the model it's just",
    "start": "918240",
    "end": "923699"
  },
  {
    "text": "going to be creative it's going to do what it does um so what we do is we decrease the",
    "start": "923699",
    "end": "929040"
  },
  {
    "text": "confidence level to the user and we let the user know like hey like sorry um we can find the information and",
    "start": "929040",
    "end": "935639"
  },
  {
    "text": "documentation provided so expect your answer to be less accurate you know so like hey like I like you may not know",
    "start": "935639",
    "end": "942480"
  },
  {
    "text": "what it's talking about um and that's something that we leave a lot to our customers to decide what they",
    "start": "942480",
    "end": "947760"
  },
  {
    "text": "want to do so some of the customers that we work on like Enterprises they really don't want the model to be like anything",
    "start": "947760",
    "end": "954779"
  },
  {
    "text": "creative it's not confidence if our R fails so then we just say like oh I don't know like you know documents are",
    "start": "954779",
    "end": "961019"
  },
  {
    "text": "not provided and then some of them want the model to be a little bit more creative because maybe the training data",
    "start": "961019",
    "end": "966720"
  },
  {
    "text": "does include the documentation uh because they're a big company or something so like we kind of let our",
    "start": "966720",
    "end": "972060"
  },
  {
    "text": "customers decide what they want to do that um and then once when they are creative",
    "start": "972060",
    "end": "977100"
  },
  {
    "text": "you know the model can just it can tries to answer so it will say like oh I'm",
    "start": "977100",
    "end": "982320"
  },
  {
    "text": "sorry I couldn't find anything exactly on our documents but here's an example",
    "start": "982320",
    "end": "987600"
  },
  {
    "text": "of how um and then another example I'm very",
    "start": "987600",
    "end": "993540"
  },
  {
    "text": "curious in this setting do you dynamically set the temperature like if you have no documents",
    "start": "993540",
    "end": "1000860"
  },
  {
    "text": "um returned from their triple tab with your updated temperature to be more creative or would you still have it I",
    "start": "1000860",
    "end": "1007579"
  },
  {
    "text": "assume like me or zero if you have at least sector documents are tripped right",
    "start": "1007579",
    "end": "1013759"
  },
  {
    "text": "um so we so right now I mean we actually just",
    "start": "1013759",
    "end": "1019339"
  },
  {
    "text": "we're testing with actually dynamically changing but right now we kind of leave up to the customer I mean we don't",
    "start": "1019339",
    "end": "1025880"
  },
  {
    "text": "uh we still let it zero most cases um but I think that's an interesting take because then",
    "start": "1025880",
    "end": "1032660"
  },
  {
    "text": "um you could potentially of course with a higher temperature get better results",
    "start": "1032660",
    "end": "1037880"
  },
  {
    "text": "but because it's a setting it's a customer setting we haven't done this yet but",
    "start": "1037880",
    "end": "1044839"
  },
  {
    "text": "um we're definitely starting to try it out",
    "start": "1044839",
    "end": "1048339"
  },
  {
    "text": "um it's a great question so another thing that we do too um it's the teach to model functionality",
    "start": "1050120",
    "end": "1057559"
  },
  {
    "text": "um you know so we let users revise and correct results in our and so we basically like we can we provide some",
    "start": "1057559",
    "end": "1063860"
  },
  {
    "text": "insights to the user based on like how um users write the questions and also we",
    "start": "1063860",
    "end": "1069260"
  },
  {
    "text": "show all the conversations and if the if our customer like says sees an incorrect",
    "start": "1069260",
    "end": "1074720"
  },
  {
    "text": "answer they can go and revise it uh and then the revision is pretty it's pretty basic right now basically like the input",
    "start": "1074720",
    "end": "1081380"
  },
  {
    "text": "the correct response the input The Source link we just re-index into the vector database with some metadata and",
    "start": "1081380",
    "end": "1087200"
  },
  {
    "text": "some higher priority uh so that someone asks the similar questions it will query",
    "start": "1087200",
    "end": "1092419"
  },
  {
    "text": "that document as well um and you say higher priority because",
    "start": "1092419",
    "end": "1097460"
  },
  {
    "text": "basically your retrieval step not only takes into account relevance but also some notion of priority and so yes",
    "start": "1097460",
    "end": "1103520"
  },
  {
    "text": "exactly so we have um yeah we set up in our system to account for like higher priority like",
    "start": "1103520",
    "end": "1111380"
  },
  {
    "text": "sources versus like low priority so there's a lot of people that want to ingest like their documentation but they",
    "start": "1111380",
    "end": "1117860"
  },
  {
    "text": "also want to ingest YouTube videos they also want to ingest zendesk's help support center and some of those data",
    "start": "1117860",
    "end": "1124039"
  },
  {
    "text": "sources might not be as relevant as the docs for example are as not as updated as the docs so we want to be able to set",
    "start": "1124039",
    "end": "1130100"
  },
  {
    "text": "that priority um so the model knows like oh this is the most up-to-date data source I should",
    "start": "1130100",
    "end": "1136400"
  },
  {
    "text": "probably rely on that one makes sense also we're running a little bit short on time so maybe let's speed",
    "start": "1136400",
    "end": "1142039"
  },
  {
    "text": "through the the rest of the slides awesome I mean yeah I think it's pretty close to the end now here is just some",
    "start": "1142039",
    "end": "1147200"
  },
  {
    "text": "some general evaluation uh things and I would love to hear from all you how you all evaluate but just this is mostly for",
    "start": "1147200",
    "end": "1154400"
  },
  {
    "text": "the audience you know I think something that helped us a lot is just having an evaluation system for every step of the",
    "start": "1154400",
    "end": "1159860"
  },
  {
    "text": "process you know so having a IR evaluation having a completion evaluation having a pre-processing one",
    "start": "1159860",
    "end": "1166160"
  },
  {
    "text": "you know and then having one all together uh second one of course is just invest",
    "start": "1166160",
    "end": "1171260"
  },
  {
    "text": "time in creating great evaluation data set you know you're your results are going to be as good as your evaluations",
    "start": "1171260",
    "end": "1176780"
  },
  {
    "text": "so make sure that you have you went and do the work even if that takes like",
    "start": "1176780",
    "end": "1181940"
  },
  {
    "text": "manual like manual work for like days you know having good evaluation data set makes a big difference uh the third one",
    "start": "1181940",
    "end": "1188960"
  },
  {
    "text": "is now is just using open AI evals it's great and also the link chain out evaluator uh use the tools around you to",
    "start": "1188960",
    "end": "1196400"
  },
  {
    "text": "help improve the model and improve your responses and reduce hallucination",
    "start": "1196400",
    "end": "1201460"
  },
  {
    "text": "and I just put something here for adversary testing you know like having your data sets",
    "start": "1201460",
    "end": "1208280"
  },
  {
    "text": "um questions that will um that will basically prompt the model to hallucinate or basically that you you",
    "start": "1208280",
    "end": "1214820"
  },
  {
    "text": "know that there are no documents or they're like partial documents about that answer but you wanted to try to",
    "start": "1214820",
    "end": "1220580"
  },
  {
    "text": "um either say I don't know or you want to try to let it be more creative",
    "start": "1220580",
    "end": "1226100"
  },
  {
    "text": "um so I think these are just some general things General takeaways that we use and I think it's useful for everyone",
    "start": "1226100",
    "end": "1233559"
  },
  {
    "text": "all right all right we'll we'll surely get into a lot more around evaluations later with Daniel but for now let's",
    "start": "1237320",
    "end": "1242660"
  },
  {
    "text": "transition over to Victoria so you guys are out okay let me share my",
    "start": "1242660",
    "end": "1248539"
  },
  {
    "text": "screen so first I I wanna also agree with the importance of evaluation and how that's very critical uh to do this",
    "start": "1248539",
    "end": "1255740"
  },
  {
    "text": "uh correctly I want to highlight a key paper that my co-founder and our CTO at",
    "start": "1255740",
    "end": "1263080"
  },
  {
    "text": "Victoria had published and worked on and led the research for a while at Google",
    "start": "1263080",
    "end": "1268460"
  },
  {
    "text": "actually it's not this one is this one the eqa framework which is one of the key methods for evaluation for",
    "start": "1268460",
    "end": "1275179"
  },
  {
    "text": "evaluating retrieval models for Q a systems and so I don't know uh",
    "start": "1275179",
    "end": "1281240"
  },
  {
    "text": "if you folks are using it or not already but you should definitely take take a look at that so with that said let me jump into my",
    "start": "1281240",
    "end": "1288620"
  },
  {
    "text": "presentation and I'll try to make it very quick here so this is what the Victoria system",
    "start": "1288620",
    "end": "1294380"
  },
  {
    "text": "looks like our goal is to simplify all of these steps these are the same steps I think in all of our systems we all",
    "start": "1294380",
    "end": "1300200"
  },
  {
    "text": "have a very similar approach you extract the input data from different sources you encode that data into an embedding",
    "start": "1300200",
    "end": "1306980"
  },
  {
    "text": "space that unifies the information based on meaning and though sometimes you can augment that with keywords to support",
    "start": "1306980",
    "end": "1313520"
  },
  {
    "text": "the hybrid approach and then you have a vector database that does the retrieval to find the best meanings that match the",
    "start": "1313520",
    "end": "1319940"
  },
  {
    "text": "query that the user is inputting you use a cross-attentionally re-ranker model to re-rank the outputs that's not necessary",
    "start": "1319940",
    "end": "1326539"
  },
  {
    "text": "but sometimes that can improve the results significantly we do actually have a cross-attentional re-ranker in",
    "start": "1326539",
    "end": "1331880"
  },
  {
    "text": "our Pipeline and then finally calibrate the results and feed them into a summarizer model to provide the final",
    "start": "1331880",
    "end": "1337880"
  },
  {
    "text": "answer that the user sees so that's what the pipeline looks like at Victoria our goal is to simplify this entire process",
    "start": "1337880",
    "end": "1344299"
  },
  {
    "text": "so for our users they only see this kind of view where they have one API to upload the data and then another API to",
    "start": "1344299",
    "end": "1351860"
  },
  {
    "text": "issue the problem and get the answers back from from that data what we focus on is the aspects of",
    "start": "1351860",
    "end": "1358280"
  },
  {
    "text": "making this easier for developers meaning how do you balance the performance for on disk versus in memory",
    "start": "1358280",
    "end": "1364520"
  },
  {
    "text": "and to balance the cost and to balance the retrieval times and the latency of the responses and keep that at the",
    "start": "1364520",
    "end": "1370220"
  },
  {
    "text": "minimum how to reduce the hallucination effect that is the key topic of this discussion today and then how to do it",
    "start": "1370220",
    "end": "1376520"
  },
  {
    "text": "in a way that is very real time meaning the updates can be coming in real time and the documents could be inserted",
    "start": "1376520",
    "end": "1382340"
  },
  {
    "text": "deleted changed etc etc and then the effects of that represent themselves on",
    "start": "1382340",
    "end": "1388100"
  },
  {
    "text": "the output right away one of the key benefits of our system which my my colleagues here might also have in their",
    "start": "1388100",
    "end": "1394280"
  },
  {
    "text": "systems is that we are cross-lingual meaning that the input documents can be in many many languages French English",
    "start": "1394280",
    "end": "1400520"
  },
  {
    "text": "Arabic German Chinese Korean and the questions the prompts can be in any language and that the best answer is",
    "start": "1400520",
    "end": "1407419"
  },
  {
    "text": "extracted and provided back to the prompt in the language of the prompt that the user is asking it and the",
    "start": "1407419",
    "end": "1413360"
  },
  {
    "text": "question ends so this kind of removes language as a barrier for finding information this is some longer term",
    "start": "1413360",
    "end": "1419600"
  },
  {
    "text": "stuff that we want to be working on which is to add images audio and video not just extract the text out of them",
    "start": "1419600",
    "end": "1425900"
  },
  {
    "text": "but to add an embedding recognition to what's in them for example if this is an image of a kubernetes cluster but the",
    "start": "1425900",
    "end": "1432140"
  },
  {
    "text": "word of kubernetes is not there anywhere in the image with the proper embedding you can still retain that meaning and",
    "start": "1432140",
    "end": "1438140"
  },
  {
    "text": "that's not a feature that we have yet but we plan to add that I'm curious if some of the other folks the Systems",
    "start": "1438140",
    "end": "1443900"
  },
  {
    "text": "Support or not we can hear about later on in the in the conversation there's a longer term Vision here for all of us I",
    "start": "1443900",
    "end": "1450679"
  },
  {
    "text": "think is to move from a Legacy Legacy is about search results right or search engines where you have a query string",
    "start": "1450679",
    "end": "1457159"
  },
  {
    "text": "and then you get a back a list of results that I call that Legacy modern meaning current present time is",
    "start": "1457159",
    "end": "1464659"
  },
  {
    "text": "about answer engines instead where you issue a question or a prompt and get back the answer for your question right",
    "start": "1464659",
    "end": "1470659"
  },
  {
    "text": "away you don't have to click on the results to try and read and find out what the answer is the answer is is provided to you and if we do that",
    "start": "1470659",
    "end": "1476960"
  },
  {
    "text": "correctly in some cases we'll be able to move to what's called action engines where that the answer to that problem I'm trying to solve for example my",
    "start": "1476960",
    "end": "1483440"
  },
  {
    "text": "snowflake system is too slow the query is not running at the speed it needs to be and then it tells you the answer and",
    "start": "1483440",
    "end": "1489380"
  },
  {
    "text": "then it tells you at the end would you like me to do that for you and that's what I refer to as action engine and",
    "start": "1489380",
    "end": "1495140"
  },
  {
    "text": "that's I think the longer term mission that many of us are on uh we truly",
    "start": "1495140",
    "end": "1500240"
  },
  {
    "text": "believe that in five years every single application whether that be SAS mobile e-commerce you name it would be powered",
    "start": "1500240",
    "end": "1507260"
  },
  {
    "text": "by something like this I think all of us in this room not just Victor we all believe that that that's that's the mission that we're after is to enable",
    "start": "1507260",
    "end": "1513799"
  },
  {
    "text": "applications to evolve from the Legacy Way for how we interact with them to a",
    "start": "1513799",
    "end": "1518840"
  },
  {
    "text": "more futuristic way more verbally we can just Express what we want and what we need and it just happens and that's what",
    "start": "1518840",
    "end": "1524960"
  },
  {
    "text": "we're working towards these are some of the use cases that we can predict and see today I think that's what many of us are working on here how to make customer",
    "start": "1524960",
    "end": "1531559"
  },
  {
    "text": "support better knowledge Discovery and you can see in some of the other ones listed here here but I think this these",
    "start": "1531559",
    "end": "1536900"
  },
  {
    "text": "are only the use cases that our feeble Minds can predict for what is happening right now I think there will be new use",
    "start": "1536900",
    "end": "1542779"
  },
  {
    "text": "cases unlocked in the future that we can't even predict yet in the same way that when the iPhone came out nobody",
    "start": "1542779",
    "end": "1547940"
  },
  {
    "text": "could have predicted Snapchat but Snapchat still happens right and I think there will be some things like that that",
    "start": "1547940",
    "end": "1553100"
  },
  {
    "text": "will happen over time now uh this was already covered earlier but briefly there's two ways to have large language",
    "start": "1553100",
    "end": "1559580"
  },
  {
    "text": "models speak about your data and provide answers to your data the first one is to",
    "start": "1559580",
    "end": "1565520"
  },
  {
    "text": "fine-tune the model to take your customer data feed feed that customer data into a fine tuning engine to",
    "start": "1565520",
    "end": "1572539"
  },
  {
    "text": "fine-tune the large language model on your own data and then get a new model that's now trained with your own data",
    "start": "1572539",
    "end": "1578299"
  },
  {
    "text": "and then you issue the question to that model and you get back an answer this is actually not ideal to be doing it has",
    "start": "1578299",
    "end": "1584779"
  },
  {
    "text": "many disadvantages one of the key ones is it will lead to a significant hallucinations where sometimes you can",
    "start": "1584779",
    "end": "1590659"
  },
  {
    "text": "get outputs that have nothing to do with your data that were in the existing model itself depending on the temperature and how you set the",
    "start": "1590659",
    "end": "1596659"
  },
  {
    "text": "temperature obviously another disadvantage is training training these models can take a very long time so that",
    "start": "1596659",
    "end": "1602480"
  },
  {
    "text": "means new data coming in might not materialize on the output until hours if not Days Later until the fine tuning",
    "start": "1602480",
    "end": "1608539"
  },
  {
    "text": "process is is finished and then last but not least that's very costly actually to do this kind of retraining over and over",
    "start": "1608539",
    "end": "1614779"
  },
  {
    "text": "again compared to some of the other techniques that we are discussing today so the technique that many of us employ",
    "start": "1614779",
    "end": "1620740"
  },
  {
    "text": "in this conversation today is what we refer to as grounded generation or",
    "start": "1620740",
    "end": "1626720"
  },
  {
    "text": "sometimes you read the term retrieval augment generation I prefer the term grounded generation because I'm a gamer",
    "start": "1626720",
    "end": "1633080"
  },
  {
    "text": "as you can see from my t-shirt and the ground regeneration is GG so what's a better acronym to go with than that and",
    "start": "1633080",
    "end": "1639559"
  },
  {
    "text": "ground degeneration in a nutshell is about we're gonna still",
    "start": "1639559",
    "end": "1645140"
  },
  {
    "text": "provide an alternative outputs but we're going to ground it in the facts so we're going to retrieve the facts first we're going to use a high-end information",
    "start": "1645140",
    "end": "1651020"
  },
  {
    "text": "retrieval semantic model that's still using neural networks to find the best facts for that prompt and then we're",
    "start": "1651020",
    "end": "1656779"
  },
  {
    "text": "going to summarize these facts with the large language models that's kind of a in a nutshell what another generation is about so I have a couple of pictures I",
    "start": "1656779",
    "end": "1663200"
  },
  {
    "text": "want to show you here this one is just doing the aspect of training a model to",
    "start": "1663200",
    "end": "1669860"
  },
  {
    "text": "be very good at finding facts meaning the information retrieval models so you have training data that's not the",
    "start": "1669860",
    "end": "1675020"
  },
  {
    "text": "customer data that's training data that trains this retrieval model on how can it be very good at finding a fact the",
    "start": "1675020",
    "end": "1681559"
  },
  {
    "text": "facts relevant to the prompt or the question that the user has and then once",
    "start": "1681559",
    "end": "1686900"
  },
  {
    "text": "that model finishes training in training time we now use the customer data to index it using or encode it you can hear",
    "start": "1686900",
    "end": "1693679"
  },
  {
    "text": "the team encoding or indexing or embeddings or vectors this means the process of running that customer data",
    "start": "1693679",
    "end": "1699559"
  },
  {
    "text": "through the model to produce these embeddings that we then store in the vector database the customer question comes in to the convector database and",
    "start": "1699559",
    "end": "1705860"
  },
  {
    "text": "then we produce a list of results that is Step number one step number two looks like this",
    "start": "1705860",
    "end": "1711200"
  },
  {
    "text": "this next slide here and I'm sorry for the animations uh let me just forward to them so you get the final picture so",
    "start": "1711200",
    "end": "1717320"
  },
  {
    "text": "essentially this is what what happens here is the question or the prompt from the user goes to the information retrieval model the customer data have",
    "start": "1717320",
    "end": "1724220"
  },
  {
    "text": "already been encoded with that model into the vector database the vector database finds the closest proximity",
    "start": "1724220",
    "end": "1729320"
  },
  {
    "text": "vectors using dot product or cosine product or other techniques you get back a list of results these are the facts",
    "start": "1729320",
    "end": "1735140"
  },
  {
    "text": "the facts that addressing your your question the same question goes to the generative model and we tell it these",
    "start": "1735140",
    "end": "1740840"
  },
  {
    "text": "are the facts for this question give us the best response that summarizes these facts into an answer for the end user",
    "start": "1740840",
    "end": "1748340"
  },
  {
    "text": "and try to not go beyond the facts and that's the hard part is restricting the model to not go beyond the facts and",
    "start": "1748340",
    "end": "1753919"
  },
  {
    "text": "that's kind of a key theme that we're all discussing today so that's very roughly it my last slide I want to show",
    "start": "1753919",
    "end": "1759620"
  },
  {
    "text": "you is that I asked chargpt about some ways to reduce hallucinations and I give this very very long list of ways to",
    "start": "1759620",
    "end": "1766399"
  },
  {
    "text": "reduce hallucinations the key technique that we're all employing here I mean we have some of these other aspects in here",
    "start": "1766399",
    "end": "1771799"
  },
  {
    "text": "but the key one we have is this number five technique right is that we are grounding it in the external knowledge",
    "start": "1771799",
    "end": "1777200"
  },
  {
    "text": "about the question and underlying data set and trying to limit the response back to that but some of these other",
    "start": "1777200",
    "end": "1783559"
  },
  {
    "text": "techniques are also very good and we're investigating some of them from a research perspective to try and deploy in the product going forward so I'm",
    "start": "1783559",
    "end": "1789919"
  },
  {
    "text": "sorry if I went over my time but that's uh the quick uh Victoria story no that's that's great I I have one",
    "start": "1789919",
    "end": "1796880"
  },
  {
    "text": "quick question for you before we go to the other folks which is so when you're breaking it down in this like I mean",
    "start": "1796880",
    "end": "1802760"
  },
  {
    "text": "we're talking we've talked a bunch about kind of like retrieval versus generation as well and so when you guys are thinking about it at Vector like where",
    "start": "1802760",
    "end": "1809840"
  },
  {
    "text": "do you focus like how do you think about those different steps where do you think where do you focus most of your time what do you think is the harder problem",
    "start": "1809840",
    "end": "1815659"
  },
  {
    "text": "where do you see most of the errors coming from excellent question excellent question so",
    "start": "1815659",
    "end": "1821419"
  },
  {
    "text": "everybody in the industry right now is uh open source and lots of the activity",
    "start": "1821419",
    "end": "1827059"
  },
  {
    "text": "happening of course from major companies like stable stability Labs or stable LM uh of course the the more",
    "start": "1827059",
    "end": "1834460"
  },
  {
    "text": "proprietary Technologies from open AI etc etc they're all focused on this on this problem right how do we make a very",
    "start": "1834460",
    "end": "1840500"
  },
  {
    "text": "very good generative model so that's not what we're focusing our efforts and I think everybody in this room might be",
    "start": "1840500",
    "end": "1846260"
  },
  {
    "text": "following that approach we're focused on this if we can get this to be really really good if you can get this information retrieval ml model that",
    "start": "1846260",
    "end": "1853159"
  },
  {
    "text": "retrieves the facts that then we're grounding the generative Model N if you get this to be superior in its",
    "start": "1853159",
    "end": "1859159"
  },
  {
    "text": "performance and do it in a hybrid way properly because sometimes when it's semantic long question the performance",
    "start": "1859159",
    "end": "1864799"
  },
  {
    "text": "might not be as good when it's a keyword lookup or a Boolean kind of expression question you really want to mix The Best of Both Worlds if you do this really",
    "start": "1864799",
    "end": "1871460"
  },
  {
    "text": "really well then you win so that's what we're very very focused on is how to make this the best it can be and then on",
    "start": "1871460",
    "end": "1878000"
  },
  {
    "text": "this part we'll Leverage whatever we can get our hands on out there that does this a good job at that and there's many many it's become a commodity now where",
    "start": "1878000",
    "end": "1884360"
  },
  {
    "text": "there's many good options for that I hope hopefully that addresses your your passion yeah it does Super insightful here how",
    "start": "1884360",
    "end": "1891020"
  },
  {
    "text": "do you think about it all right um I'm sure I see a lot more",
    "start": "1891020",
    "end": "1896419"
  },
  {
    "text": "questions but we'll answer those in the QA part so uh Daniel do you want to take over",
    "start": "1896419",
    "end": "1902200"
  },
  {
    "text": "thanks Harrison uh guys I will give you a a nice brief intro uh around the",
    "start": "1903500",
    "end": "1909620"
  },
  {
    "text": "project I've been working on alongside uh last tomorrow in Bongo bungalberg the past um I'm on the half two months it's",
    "start": "1909620",
    "end": "1915320"
  },
  {
    "text": "called Auto Violator and we've built a hosted um framework it's an open source project",
    "start": "1915320",
    "end": "1920899"
  },
  {
    "text": "but it's also hosted online chain thanks Patterson for uh sponsoring it and we",
    "start": "1920899",
    "end": "1926960"
  },
  {
    "text": "enable developers to go from a demo Opera type into a more production",
    "start": "1926960",
    "end": "1932120"
  },
  {
    "text": "environment with confidence because one thing I have an audience is that we all have awesome cute demos on Twitter",
    "start": "1932120",
    "end": "1938179"
  },
  {
    "text": "around our QA you know 3gpt for documents but I've seen a lack of tools",
    "start": "1938179",
    "end": "1944419"
  },
  {
    "text": "uh how can we get into a production environment",
    "start": "1944419",
    "end": "1949460"
  },
  {
    "text": "um with uh data behind it how can we come up to our leaders within our organizations and give them data that",
    "start": "1949460",
    "end": "1956779"
  },
  {
    "text": "our QA childbot is going to be accurate um around maybe 85 percent of the time",
    "start": "1956779",
    "end": "1962840"
  },
  {
    "text": "and if that's good enough we are good through the rigorous production because in a production use case you know think",
    "start": "1962840",
    "end": "1968120"
  },
  {
    "text": "legal Insurance health care we don't want Health stations we don't want false positives we don't want to damage the",
    "start": "1968120",
    "end": "1975140"
  },
  {
    "text": "brand and that is something that we care a lot about at Shepherds which is an insured attack and a commercial",
    "start": "1975140",
    "end": "1980600"
  },
  {
    "text": "construction space and you can obviously assume like as an insurance company we cannot afford to tell our research that",
    "start": "1980600",
    "end": "1987860"
  },
  {
    "text": "oh we do like nuclear Insurance in our childbot so we have to build guard rails and use data to enable us to go to",
    "start": "1987860",
    "end": "1994519"
  },
  {
    "text": "production I will give a quick um demo of the tool it is open source contributions are welcomed and only guys",
    "start": "1994519",
    "end": "2002559"
  },
  {
    "text": "jump in and walk you through",
    "start": "2002559",
    "end": "2005820"
  },
  {
    "text": "okay are you able guys wait to see my tab The Honorable later",
    "start": "2007960",
    "end": "2013720"
  },
  {
    "text": "yes you see it now awesome awesome awesome so I'll walk you",
    "start": "2013720",
    "end": "2019179"
  },
  {
    "text": "through a um demo environment over here so a high level what this tool does is it has",
    "start": "2019179",
    "end": "2026440"
  },
  {
    "text": "three stages uh stage one is uh we will you will provide a document or a set of",
    "start": "2026440",
    "end": "2033640"
  },
  {
    "text": "documents that you want to build your um q a system with let's say here we have added a car party",
    "start": "2033640",
    "end": "2040600"
  },
  {
    "text": "podcast from electron and stage one is we're going to dynamically automatically uh generate a",
    "start": "2040600",
    "end": "2048820"
  },
  {
    "text": "synthetic task that's left for you so here um we can see that there's generated by",
    "start": "2048820",
    "end": "2054700"
  },
  {
    "text": "the tool in stage one and we use an L lamp to do that so let's say we will put",
    "start": "2054700",
    "end": "2061960"
  },
  {
    "text": "this document in a in an index right it will randomly pick the index we'll randomly pick um a vector and then we'll",
    "start": "2061960",
    "end": "2070179"
  },
  {
    "text": "try to come up with a question and ask repair out of that letter so here we have dynamically generated that as data",
    "start": "2070179",
    "end": "2077560"
  },
  {
    "text": "set from this document and the goal here is to enable a developer to automatically",
    "start": "2077560",
    "end": "2084099"
  },
  {
    "text": "generate their tested settings out of hiring a team or doing it themselves",
    "start": "2084099",
    "end": "2089138"
  },
  {
    "text": "and we have seen this to be a quite accurate and quite effective it's not",
    "start": "2089139",
    "end": "2094419"
  },
  {
    "text": "perfect it's not as amazing as you know a manual testing a manual uh human labor",
    "start": "2094419",
    "end": "2100480"
  },
  {
    "text": "around this task but it's free uh it's automatic it takes 10 seconds to do it",
    "start": "2100480",
    "end": "2106599"
  },
  {
    "text": "so uh that's stage one so stage two is given the parameters that you you will",
    "start": "2106599",
    "end": "2111820"
  },
  {
    "text": "provide here in the left hand side let's say a model you can use my Tropic or LGBT 3.5",
    "start": "2111820",
    "end": "2117339"
  },
  {
    "text": "uh and uh chunk size chunk overlap all these different parameters that you uh",
    "start": "2117339",
    "end": "2122800"
  },
  {
    "text": "we were all used to when building a QA chain uh using a LinkedIn Library uh we will build in stage to a QA change",
    "start": "2122800",
    "end": "2130240"
  },
  {
    "text": "a question answer system for you on the Fly and then step three we are",
    "start": "2130240",
    "end": "2136800"
  },
  {
    "text": "grit grades that uh stage two will grade the QA change against the test data that",
    "start": "2136800",
    "end": "2143320"
  },
  {
    "text": "we have generated for you in stage one and then we'll run next from it so let's take a look what happens here so here we",
    "start": "2143320",
    "end": "2150820"
  },
  {
    "text": "have um greatest this is the output of stage three so we have graded each entry",
    "start": "2150820",
    "end": "2157599"
  },
  {
    "text": "here and again we use an hour to do that so kind of cool right we use AI to create AI uh so we use some clever-ish",
    "start": "2157599",
    "end": "2167020"
  },
  {
    "text": "problem engineering uh to do that so we provide uh in the prompt we provide the",
    "start": "2167020",
    "end": "2172060"
  },
  {
    "text": "question and answer here and then we provide the observed answer now we ask an LM to provide us um with a relevancy",
    "start": "2172060",
    "end": "2179920"
  },
  {
    "text": "and how accurate is that in its own terms and then all the way at the end here you can see that we will um give",
    "start": "2179920",
    "end": "2187480"
  },
  {
    "text": "you a summary of experiments so for this demo run we have uh three experiments",
    "start": "2187480",
    "end": "2192940"
  },
  {
    "text": "that has a different retriever methods and different chunk sizes different overlap parameters as well and here we",
    "start": "2192940",
    "end": "2201339"
  },
  {
    "text": "can see how those experiments have scored against each other so on the",
    "start": "2201339",
    "end": "2207160"
  },
  {
    "text": "x-axis we have the uh kind of correctness score like how accurate",
    "start": "2207160",
    "end": "2213339"
  },
  {
    "text": "um uh those Brands had been and then on the y-axis we have the latency I mean that",
    "start": "2213339",
    "end": "2219280"
  },
  {
    "text": "was already two big things we care about right now latency and correctness so here we can see that experiment",
    "start": "2219280",
    "end": "2226599"
  },
  {
    "text": "uh one this one had the highest latency but",
    "start": "2226599",
    "end": "2232780"
  },
  {
    "text": "also had pretty good accuracy right but we want to see that Experiment three had",
    "start": "2232780",
    "end": "2238180"
  },
  {
    "text": "the lowest latency and also had a high accuracy so here we can deduce that uh",
    "start": "2238180",
    "end": "2244420"
  },
  {
    "text": "based off data and accumulation uh we can say that for that set of documents",
    "start": "2244420",
    "end": "2249640"
  },
  {
    "text": "that you have provided Us in this Tool uh these are the parameters the chunk",
    "start": "2249640",
    "end": "2255640"
  },
  {
    "text": "size 500 overlap of zero this flips method and this is terrible",
    "start": "2255640",
    "end": "2261040"
  },
  {
    "text": "are the best ones to use for that chain and of course you can keep going and",
    "start": "2261040",
    "end": "2266619"
  },
  {
    "text": "maybe take a look at anthropic models so um Martin he released the anthropic",
    "start": "2266619",
    "end": "2272740"
  },
  {
    "text": "model of adobe 100K in Auto elevator a day ago and he did awesome testing",
    "start": "2272740",
    "end": "2278440"
  },
  {
    "text": "around that we can see that the accuracy is around the same uh however um politics is quite higher there",
    "start": "2278440",
    "end": "2286540"
  },
  {
    "text": "I will end around here and just give you one little thing here so a playground",
    "start": "2286540",
    "end": "2292320"
  },
  {
    "text": "environment here you can upload your own text and if you follow the GitHub link you",
    "start": "2292320",
    "end": "2297520"
  },
  {
    "text": "will see that um I don't know if you guys can see the GitHub link but yeah if you can just click on the GitHub",
    "start": "2297520",
    "end": "2303460"
  },
  {
    "text": "um icon here use the repository you can also find the repository by going to linking um yeah organization",
    "start": "2303460",
    "end": "2311200"
  },
  {
    "text": "thank you guys know if you guys uh have any questions about that awesome thank you thank you you know I I put two links",
    "start": "2311200",
    "end": "2317920"
  },
  {
    "text": "for those in the chat as well and um reminder that passenger the the final",
    "start": "2317920",
    "end": "2323020"
  },
  {
    "text": "presentation by Mathis will be doing question and answers for the whole group I see a few that are that are really",
    "start": "2323020",
    "end": "2328660"
  },
  {
    "text": "high outputted but reminder to everyone to go over and upload their favorites or add new ones if you have some new ones",
    "start": "2328660",
    "end": "2334839"
  },
  {
    "text": "um so with that being said over to the last and and uh saving the best for last I'm sure so math is syrup",
    "start": "2334839",
    "end": "2342280"
  },
  {
    "text": "um thanks Harrison can you can you see my screen yes yes all right great uh and really neat",
    "start": "2342280",
    "end": "2349180"
  },
  {
    "text": "Tool uh Daniel that look great from a tech background and also Sleek UI I I",
    "start": "2349180",
    "end": "2354820"
  },
  {
    "text": "love that um all right uh so I want to talk about",
    "start": "2354820",
    "end": "2359980"
  },
  {
    "text": "um some strategies that we've been working on on detecting hallucinations using smaller fine-tuned models",
    "start": "2359980",
    "end": "2368320"
  },
  {
    "text": "um so the setup is the same it's about hallucination and retrieval augmentation",
    "start": "2368320",
    "end": "2373900"
  },
  {
    "text": "or document question answering and um we've tried we've been trying out a",
    "start": "2373900",
    "end": "2380079"
  },
  {
    "text": "few things um generally here David said I said it before we work on an open source",
    "start": "2380079",
    "end": "2386079"
  },
  {
    "text": "framework and a commercial product um you can check that out and what I'm going to do now is I'm",
    "start": "2386079",
    "end": "2393099"
  },
  {
    "text": "going to skip the basics because we already owed them um and then I'm gonna talk about two",
    "start": "2393099",
    "end": "2399760"
  },
  {
    "text": "hallucination patterns that we see um I would talk about verifiability and",
    "start": "2399760",
    "end": "2407380"
  },
  {
    "text": "attribution in recent research there's a few interesting papers that we've been looking at and then I'm going to share",
    "start": "2407380",
    "end": "2415839"
  },
  {
    "text": "some of the results that we had training smaller Transformer models but also",
    "start": "2415839",
    "end": "2421260"
  },
  {
    "text": "evaluate evaluating statistical methods of evaluation and also some other kind",
    "start": "2421260",
    "end": "2428560"
  },
  {
    "text": "of fact chat checking models that we tried um so yeah let's I'm gonna Skip and I'm",
    "start": "2428560",
    "end": "2436060"
  },
  {
    "text": "gonna go right to the patterns and hallucination patterns in retrieval",
    "start": "2436060",
    "end": "2442480"
  },
  {
    "text": "augmentation with large language models the first one we call it not paying",
    "start": "2442480",
    "end": "2448480"
  },
  {
    "text": "attention pun intended so this one here you have a question uh on the left side",
    "start": "2448480",
    "end": "2455260"
  },
  {
    "text": "uh what types of aircrafts operate between Guernsey and Southampton to",
    "start": "2455260",
    "end": "2460680"
  },
  {
    "text": "cities in England and then you have a gpt4 generated answer below that and",
    "start": "2460680",
    "end": "2468220"
  },
  {
    "text": "then you have some retrieved documents and if you look at that at first sight it kind of looks alright so there's two",
    "start": "2468220",
    "end": "2474520"
  },
  {
    "text": "types of aircraft and these are flybase Emperor jet aircraft and an ATR 72. but if you take",
    "start": "2474520",
    "end": "2483579"
  },
  {
    "text": "a closer look then you can see that actually in the retrieved documents um the flyby jet operates between",
    "start": "2483579",
    "end": "2490660"
  },
  {
    "text": "Guernsey and London Gatwick which is not Southampton so the model just didn't",
    "start": "2490660",
    "end": "2496060"
  },
  {
    "text": "pick up on that and we see that pretty often even with a strong model like gpt4",
    "start": "2496060",
    "end": "2502420"
  },
  {
    "text": "that it doesn't pick up on small contextual things in the data it doesn't",
    "start": "2502420",
    "end": "2507460"
  },
  {
    "text": "make up something completely but it's pretty frequent and then the other thing I think many of you of",
    "start": "2507460",
    "end": "2515500"
  },
  {
    "text": "you know that numbers uh so here I'm asking or the question from from a data",
    "start": "2515500",
    "end": "2520960"
  },
  {
    "text": "set actually is how long is Krishna River and here we have a generated answer from you.com so the generative",
    "start": "2520960",
    "end": "2528160"
  },
  {
    "text": "search engine and it says that the river is",
    "start": "2528160",
    "end": "2534180"
  },
  {
    "text": "1288 kilometers long and actually if you look at the supporting document that was",
    "start": "2534180",
    "end": "2540460"
  },
  {
    "text": "cited as a source even um you can see okay that's incorrect and",
    "start": "2540460",
    "end": "2545800"
  },
  {
    "text": "we've seen that pretty often like that it just besides the wrong numbers for whatever",
    "start": "2545800",
    "end": "2551500"
  },
  {
    "text": "reason I think um in general they've seen way less numbers than text so pretty",
    "start": "2551500",
    "end": "2558820"
  },
  {
    "text": "understandable but can be tricky and then we thought about okay how can",
    "start": "2558820",
    "end": "2567760"
  },
  {
    "text": "we now detect these hallucinations reliably because our customers kind of",
    "start": "2567760",
    "end": "2573460"
  },
  {
    "text": "want that the models rely on their data and that they don't make things up",
    "start": "2573460",
    "end": "2579280"
  },
  {
    "text": "um so we looked at a little bit of research one paper I want to share here",
    "start": "2579280",
    "end": "2584460"
  },
  {
    "text": "comes from researchers at Stanford evaluating verifiability and generative",
    "start": "2584460",
    "end": "2590619"
  },
  {
    "text": "search engines pretty interesting they looked at four search engines Bing chat",
    "start": "2590619",
    "end": "2596260"
  },
  {
    "text": "perplexity you and neither and they checked if a statement is supported and",
    "start": "2596260",
    "end": "2602560"
  },
  {
    "text": "then if that cited reference actually with a citation and then if that cited reference actually supports the",
    "start": "2602560",
    "end": "2609040"
  },
  {
    "text": "statement and they found that only like half of the statements had a citation and then out of those only about 75",
    "start": "2609040",
    "end": "2617980"
  },
  {
    "text": "percent actually supported the reference actually supported the statement so you",
    "start": "2617980",
    "end": "2623380"
  },
  {
    "text": "get an error rate of about 25 for the state statements that are supported",
    "start": "2623380",
    "end": "2629260"
  },
  {
    "text": "which isn't great in some contexts might be enough for others but some context is",
    "start": "2629260",
    "end": "2636099"
  },
  {
    "text": "it isn't great and then this yeah the question",
    "start": "2636099",
    "end": "2644099"
  },
  {
    "text": "all right um so and then the other one automatic",
    "start": "2644099",
    "end": "2649119"
  },
  {
    "text": "evaluation of attribution by llms um comes from researchers at Ohio State",
    "start": "2649119",
    "end": "2655540"
  },
  {
    "text": "and they've basically developed an attribution score where they judged uh",
    "start": "2655540",
    "end": "2662560"
  },
  {
    "text": "cited references as uh attributable uh",
    "start": "2662560",
    "end": "2667839"
  },
  {
    "text": "kind of expertory so that it extrapolates it says something that is not in the source",
    "start": "2667839",
    "end": "2674619"
  },
  {
    "text": "document and then contradictory so that it even contradicts the document and then they",
    "start": "2674619",
    "end": "2681160"
  },
  {
    "text": "fine-tune the range of models on the task they had some data sets that they repurposed and had a small evaluation",
    "start": "2681160",
    "end": "2687460"
  },
  {
    "text": "set and they found land T5 to outperform both the find hue and alpaca and also a",
    "start": "2687460",
    "end": "2695800"
  },
  {
    "text": "few short chat GPT so even that was surprising a little bit because even the much bigger Parker model which was fine",
    "start": "2695800",
    "end": "2702640"
  },
  {
    "text": "human on the data underperformed here and of course chat gbt not being fine-tuned on a task that",
    "start": "2702640",
    "end": "2709720"
  },
  {
    "text": "kind of seems reasonable and then",
    "start": "2709720",
    "end": "2714940"
  },
  {
    "text": "um we've been going down the same route so we looked okay",
    "start": "2714940",
    "end": "2720460"
  },
  {
    "text": "what existing methods are there to evaluate this maybe and can we train a model and we took that data set from the",
    "start": "2720460",
    "end": "2727359"
  },
  {
    "text": "first research paper and evalued created sort of like an evaluation set in the training set from",
    "start": "2727359",
    "end": "2734380"
  },
  {
    "text": "that and the mode that we um yeah the task for the model was",
    "start": "2734380",
    "end": "2739780"
  },
  {
    "text": "basically they get a statement the Pittsburgh Steelers when did they win",
    "start": "2739780",
    "end": "2744819"
  },
  {
    "text": "their first championship and then they get the the evidence uh which comes from",
    "start": "2744819",
    "end": "2749920"
  },
  {
    "text": "the source document and then you feed it to the model as statement separator",
    "start": "2749920",
    "end": "2755020"
  },
  {
    "text": "token evidence and then it produces a score and as you can see here uh score",
    "start": "2755020",
    "end": "2761140"
  },
  {
    "text": "is 0.74 that's quite good we want to work a little bit on that but it indicates that",
    "start": "2761140",
    "end": "2768700"
  },
  {
    "text": "this is not hallucination and we also tested this of course on our",
    "start": "2768700",
    "end": "2776680"
  },
  {
    "text": "test set and what we found is that statistical methods that come from a",
    "start": "2776680",
    "end": "2783520"
  },
  {
    "text": "more translation evaluation background like blue or Rouge they underperform",
    "start": "2783520",
    "end": "2790000"
  },
  {
    "text": "here a little bit so um yeah they have a low correlation with",
    "start": "2790000",
    "end": "2795160"
  },
  {
    "text": "with actual scores basically and we also looked at Birth score which is more like",
    "start": "2795160",
    "end": "2800560"
  },
  {
    "text": "semantic similarity also not that great um we compared uni evil fact a T5 large",
    "start": "2800560",
    "end": "2808480"
  },
  {
    "text": "model trained I think last last year by a research group also not great and then the deborder",
    "start": "2808480",
    "end": "2817000"
  },
  {
    "text": "models are very small the builder-based model that we find tuned has quite good correlation but we also",
    "start": "2817000",
    "end": "2824319"
  },
  {
    "text": "looked at the results manually and there's there's a few things to to find here indefinitely still",
    "start": "2824319",
    "end": "2829839"
  },
  {
    "text": "and then remaining challenges is that the data",
    "start": "2829839",
    "end": "2835900"
  },
  {
    "text": "set itself the search verifiability data set has quality issues so we found using",
    "start": "2835900",
    "end": "2841900"
  },
  {
    "text": "our model that oftentimes when the model was pretty sure that this is a hallucination it actually was a",
    "start": "2841900",
    "end": "2849520"
  },
  {
    "text": "hallucination the annotator made a mistake um so we kind of want to relabel a",
    "start": "2849520",
    "end": "2856180"
  },
  {
    "text": "little bit there and then we still want to get better performance we want to kind of tweak the scores to reflect",
    "start": "2856180",
    "end": "2864280"
  },
  {
    "text": "um a bit better how a human would maybe perceive that hallucination and then the",
    "start": "2864280",
    "end": "2870099"
  },
  {
    "text": "question is as always how well with this approach generalize across data sets and",
    "start": "2870099",
    "end": "2875680"
  },
  {
    "text": "domains and maybe also how would we deal with very large context sizes",
    "start": "2875680",
    "end": "2882520"
  },
  {
    "text": "um yeah and that's it awesome",
    "start": "2882520",
    "end": "2888579"
  },
  {
    "text": "thank you for the the presentation and we've got we've got 10 minutes for some awesome questions and uh continuing off",
    "start": "2888579",
    "end": "2896380"
  },
  {
    "text": "of the last points you were making Mathis about um uh uh evaluation the number one kind",
    "start": "2896380",
    "end": "2901900"
  },
  {
    "text": "of like question is basically can you provide examples of good IR evaluation models and practices",
    "start": "2901900",
    "end": "2909760"
  },
  {
    "text": "um and so I guess that's kind of like talking about there's two components here there's the information retrieval",
    "start": "2909760",
    "end": "2914920"
  },
  {
    "text": "step and then there's also like the generation step and so I think the math is if I'm understanding correctly you're kind of like after the in from after the",
    "start": "2914920",
    "end": "2922480"
  },
  {
    "text": "the stuff's been generated like you know is it is it hallucinating or not based on that this and I think that's also",
    "start": "2922480",
    "end": "2928720"
  },
  {
    "text": "what we've talked a little bit most about before in the general context so maybe like switching to the other side",
    "start": "2928720",
    "end": "2934599"
  },
  {
    "text": "for the IR step in particular like you know we've heard retrieval is really important you guys are spending a lot of",
    "start": "2934599",
    "end": "2940599"
  },
  {
    "text": "time there Victoria you're doing some high research stuff Nick like how do you evaluate the IR step",
    "start": "2940599",
    "end": "2946060"
  },
  {
    "text": "by itself do one of you guys maybe want to take that I think yeah so go ahead",
    "start": "2946060",
    "end": "2954779"
  },
  {
    "text": "um I think for us basically we also evaluated separately um and for that generous part we put",
    "start": "2955300",
    "end": "2963700"
  },
  {
    "text": "more at metrics like mean rest broker rank or mean average Precision because",
    "start": "2963700",
    "end": "2968920"
  },
  {
    "text": "you don't want to have recall because you've probably only gonna feed like three four five documents maybe into GPT",
    "start": "2968920",
    "end": "2976440"
  },
  {
    "text": "so you really want to have that Precision at the top and you don't care that much about the rest of the results",
    "start": "2976440",
    "end": "2982900"
  },
  {
    "text": "so this is what we do usually yeah I was just going to say that uh we",
    "start": "2982900",
    "end": "2990460"
  },
  {
    "text": "we focus a lot as Omar mentioned earlier about the IR component that's really kind of core to what we do and I mean",
    "start": "2990460",
    "end": "2997660"
  },
  {
    "text": "one of the co-founders you know has spent a lot of his career on that so there are standard metrics to just",
    "start": "2997660",
    "end": "3003240"
  },
  {
    "text": "optimize the hell out of this engine to make it the best it can be so we use a",
    "start": "3003240",
    "end": "3008640"
  },
  {
    "text": "lot of the standard metrics you see in in sort of academics to optimize this",
    "start": "3008640",
    "end": "3015300"
  },
  {
    "text": "engine in general not just particularly for a particular if we use three facts or five facts or whatever just to make",
    "start": "3015300",
    "end": "3021060"
  },
  {
    "text": "it the best it could be",
    "start": "3021060",
    "end": "3023960"
  },
  {
    "text": "Nick Dental anything anything to add there um yeah I think for some people uh I",
    "start": "3026520",
    "end": "3031680"
  },
  {
    "text": "know some people are asking like how do you structure like the data set Etc like something that we do for the IR eval",
    "start": "3031680",
    "end": "3038579"
  },
  {
    "text": "Parts basically we have like the question uh we also have like the correct answer and then what we do is we",
    "start": "3038579",
    "end": "3044579"
  },
  {
    "text": "have basically like um a list of IDs that correspond to the",
    "start": "3044579",
    "end": "3050280"
  },
  {
    "text": "documents in our database like that list of IDs is basically like oh the answer",
    "start": "3050280",
    "end": "3055500"
  },
  {
    "text": "is contained on this IDs and we also have like a list of like references so",
    "start": "3055500",
    "end": "3061680"
  },
  {
    "text": "like if you have like um like basically inside that document ID if you have like a quote that would",
    "start": "3061680",
    "end": "3068819"
  },
  {
    "text": "answer the question you'd list them as a reference and you can have that um in your data set",
    "start": "3068819",
    "end": "3075420"
  },
  {
    "text": "um and then that way you can know exactly like oh the answer is contained on these documents",
    "start": "3075420",
    "end": "3081540"
  },
  {
    "text": "um it can take as a reference this parts of the text and then you can like correlate and you know like",
    "start": "3081540",
    "end": "3087599"
  },
  {
    "text": "um evaluate like using F1 and um Etc",
    "start": "3087599",
    "end": "3092720"
  },
  {
    "text": "yeah I want to stress F1 I mean F1 is definitely one of the key metrics that even in a lot of our customer",
    "start": "3092760",
    "end": "3098880"
  },
  {
    "text": "engagements when we're doing evaluation compared to Legacy systems they might have the F1 score is one of the key",
    "start": "3098880",
    "end": "3104880"
  },
  {
    "text": "metrics that we measure to try and show how much better these new techniques are and as many of you know the F1 score",
    "start": "3104880",
    "end": "3111359"
  },
  {
    "text": "Blends both the Precision and recall of the information refusal model and that's really what our our customers want to",
    "start": "3111359",
    "end": "3117720"
  },
  {
    "text": "see they want a significant they want to see a significant lift in the F1 score compared to whatever they have before",
    "start": "3117720",
    "end": "3124700"
  },
  {
    "text": "I think Harrison now we just add one point over there is simply like when you when you want to test either in the",
    "start": "3125220",
    "end": "3131400"
  },
  {
    "text": "output of level Lam on health Nations or uh the original quality uh think think",
    "start": "3131400",
    "end": "3136740"
  },
  {
    "text": "deeply about the work on the process like make sure you are storing",
    "start": "3136740",
    "end": "3142140"
  },
  {
    "text": "um either in a vector DB or maybe an imposter sequel you're storing those intermediate results so that you're now",
    "start": "3142140",
    "end": "3147240"
  },
  {
    "text": "you're later enabled to perform the testing either manual by yourself or maybe exporting it into like you know",
    "start": "3147240",
    "end": "3153780"
  },
  {
    "text": "scale AI or you know mechanical torque and hiring someone else to do it for you yeah but like again without adding data",
    "start": "3153780",
    "end": "3160200"
  },
  {
    "text": "you can test it that makes sense and actually that so there's another question a bit further",
    "start": "3160200",
    "end": "3165780"
  },
  {
    "text": "down which is like what what do these kind of like IR data sets look like like is it as simple as there's a question",
    "start": "3165780",
    "end": "3172200"
  },
  {
    "text": "and one document that has the right answer what if what if there's multiple documents that have the same answer what",
    "start": "3172200",
    "end": "3177780"
  },
  {
    "text": "is the labeling for that look like what if there's questions that maybe need things from multiple different documents how do you kind of like judge the IR",
    "start": "3177780",
    "end": "3184140"
  },
  {
    "text": "retrieval step of those I guess like you know yeah what do these actual data sets",
    "start": "3184140",
    "end": "3189960"
  },
  {
    "text": "look like in practice I think again double down my earlier",
    "start": "3189960",
    "end": "3195900"
  },
  {
    "text": "points around recording the data that you're producing and our devil down here is like record everything",
    "start": "3195900",
    "end": "3202260"
  },
  {
    "text": "um you know with pycon for example or any other you know letter to be uh you have awesome metadata and you have other",
    "start": "3202260",
    "end": "3208980"
  },
  {
    "text": "like you know for example in the car you know or manual you guys are using a priority like the ranking uh the",
    "start": "3208980",
    "end": "3216839"
  },
  {
    "text": "original step um I can record that too I like to really enable yourself to record all",
    "start": "3216839",
    "end": "3223619"
  },
  {
    "text": "um data points better Yeah by high level yeah it's the top care results that you were",
    "start": "3223619",
    "end": "3229680"
  },
  {
    "text": "that you're querying against again also I record a k maybe uh you're sending the dynamic layer yeah",
    "start": "3229680",
    "end": "3236339"
  },
  {
    "text": "and and Harrelson yes I mean that's why the data sets uh there is a human that helps calibrate labeling the data sets",
    "start": "3236339",
    "end": "3242880"
  },
  {
    "text": "and what's the good answer what's another good answer that's how we calibrate against that because it's very complex as we correctly identified like",
    "start": "3242880",
    "end": "3248819"
  },
  {
    "text": "sometimes the same answer can exist in multiple documents what's the best one uh and that's where we for for the",
    "start": "3248819",
    "end": "3254760"
  },
  {
    "text": "purpose of doing the measurements and calibration that is what we rely on so that's why labeling labeling is still",
    "start": "3254760",
    "end": "3260819"
  },
  {
    "text": "very very important task for that but the good news is there's a number of very good data sets out there from Reddit from Amazon from the squad data",
    "start": "3260819",
    "end": "3268559"
  },
  {
    "text": "set from Stanford there's a Wiki uh based data set there's a bunch of really really good ones out there that people",
    "start": "3268559",
    "end": "3274260"
  },
  {
    "text": "can use but if you want to do it for your own data internally at your company then one of the first exercises we would",
    "start": "3274260",
    "end": "3279720"
  },
  {
    "text": "do with you is to create a labeled data set that a human editor helped Define so",
    "start": "3279720",
    "end": "3285480"
  },
  {
    "text": "we can do the calibration against yeah exactly I think like like if you",
    "start": "3285480",
    "end": "3292140"
  },
  {
    "text": "and also like I I would reinforce that you probably want to create a data set that you know like you are an expert on",
    "start": "3292140",
    "end": "3298920"
  },
  {
    "text": "so you can really like evaluate and like manually and you know if the right answer is there so you can save some",
    "start": "3298920",
    "end": "3304440"
  },
  {
    "text": "time because yeah like there's so many data sets out there but like it's it's a bit hard to know like oh is this the",
    "start": "3304440",
    "end": "3309660"
  },
  {
    "text": "actual correct answer you know especially for like even the completion of vows um so yeah just having something",
    "start": "3309660",
    "end": "3315960"
  },
  {
    "text": "that you know it's important",
    "start": "3315960",
    "end": "3319880"
  },
  {
    "text": "awesome um there's uh there's one question down",
    "start": "3321300",
    "end": "3326339"
  },
  {
    "text": "at the end which I think is a little bit in the weeds but I think it's kind of interesting do you and this gets around",
    "start": "3326339",
    "end": "3331859"
  },
  {
    "text": "kind of like uh uh prioritization and importance for documents do you dynamically prioritize the like get do",
    "start": "3331859",
    "end": "3339839"
  },
  {
    "text": "you dynamically prioritize different sources based on the input query so like is the and I'm guessing I'm guessing the",
    "start": "3339839",
    "end": "3345960"
  },
  {
    "text": "probably the prioritization is probably like fixed but I guess like and expanding this to why I think it's",
    "start": "3345960",
    "end": "3351599"
  },
  {
    "text": "interesting like I think a lot of times when people think of a query they think of like semantic search or like hybrid search where you're looking up things",
    "start": "3351599",
    "end": "3357660"
  },
  {
    "text": "but there's also like other metadata attributes that it could be asking about right it could be asking about like from",
    "start": "3357660",
    "end": "3362940"
  },
  {
    "text": "an author and so I guess like this kind of gets like parsing the query out into different things whether it be a",
    "start": "3362940",
    "end": "3368640"
  },
  {
    "text": "dynamically prioritized source as this is kind of pointing out or just like some type of metadata filter more so",
    "start": "3368640",
    "end": "3374640"
  },
  {
    "text": "even than some sort of things how how do you guys think about very open-ended and",
    "start": "3374640",
    "end": "3379800"
  },
  {
    "text": "I know we have one minute left so this is a bad question to ask any quick any quick thoughts on that my my thought is",
    "start": "3379800",
    "end": "3386339"
  },
  {
    "text": "like how does how do humans do it right so again we're trying to rely on the large language model to be smart enough",
    "start": "3386339",
    "end": "3391619"
  },
  {
    "text": "that it's gonna both around the large language model and the retrieval model together and the re-ranking model in the",
    "start": "3391619",
    "end": "3397680"
  },
  {
    "text": "middle to be smart enough to figure out that even without the com without the additional metadata like that we want it",
    "start": "3397680",
    "end": "3402839"
  },
  {
    "text": "to be that good like it it looks at the question it looks at the response like this is really a good response just but",
    "start": "3402839",
    "end": "3408180"
  },
  {
    "text": "because I understand the domain and I understand the language and I know it's a good response that said you were right like if we can help it if you can nudge",
    "start": "3408180",
    "end": "3414119"
  },
  {
    "text": "it here or there that that would help a lot so we do have custom attributes that you can add in the metadata aspects that",
    "start": "3414119",
    "end": "3420180"
  },
  {
    "text": "can be further used in the ranking functions and for doing the cross attention on the re-ranking to further",
    "start": "3420180",
    "end": "3425760"
  },
  {
    "text": "boost certain sources to boost recency if this is a more recent document than not to boost maybe the social graph",
    "start": "3425760",
    "end": "3432300"
  },
  {
    "text": "connections meaning this this is the CEO if they posted this then maybe it's more accurate than if it's uh somebody else",
    "start": "3432300",
    "end": "3439440"
  },
  {
    "text": "so you can you can add these additional elements to try and improve the the scoring but at the end of the day",
    "start": "3439440",
    "end": "3444720"
  },
  {
    "text": "Nirvana is to have the system figure it out based on the information contents but this is the best response",
    "start": "3444720",
    "end": "3451160"
  },
  {
    "text": "awesome and since the last question the very last question what's what's the Husky's name from here",
    "start": "3452099",
    "end": "3459260"
  },
  {
    "text": "thank you for the excellent question she's a Pomsky actually which is a Miss between Pomeranian so that keeps her",
    "start": "3459260",
    "end": "3465240"
  },
  {
    "text": "small in size and her name is Luna which means the moon okay and embedding an",
    "start": "3465240",
    "end": "3470579"
  },
  {
    "text": "embedding Vector mapping would tell you that right away Luna l-u-n-a yeah right we're gonna",
    "start": "3470579",
    "end": "3477720"
  },
  {
    "text": "we're gonna end it there um thank you guys for for for joining and sharing all your wisdom I definitely",
    "start": "3477720",
    "end": "3483720"
  },
  {
    "text": "learned a lot and I think a lot of really interesting papers to read and things to look into and and thank you",
    "start": "3483720",
    "end": "3488940"
  },
  {
    "text": "everyone for for tuning in and all the excellent questions hope you guys enjoyed it um yeah thank you for all you do you're",
    "start": "3488940",
    "end": "3495599"
  },
  {
    "text": "like amazing in terms of how you're really building out this community and doing it in a very collaborative way",
    "start": "3495599",
    "end": "3501420"
  },
  {
    "text": "like you just bring people together we're all trying to make the world better with this technology and we really love what you're doing so please",
    "start": "3501420",
    "end": "3507119"
  },
  {
    "text": "keep doing that we're all trying to figure it out",
    "start": "3507119",
    "end": "3511880"
  }
]