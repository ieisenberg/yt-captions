[
  {
    "text": "hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which",
    "start": "2200",
    "end": "7480"
  },
  {
    "text": "launched today um let's get started uh to begin let's",
    "start": "7480",
    "end": "12960"
  },
  {
    "text": "go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um",
    "start": "12960",
    "end": "19640"
  },
  {
    "text": "Lang chain is a framework for developing llm powered applications um it's the company I work",
    "start": "19640",
    "end": "26359"
  },
  {
    "text": "at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for",
    "start": "26359",
    "end": "31920"
  },
  {
    "text": "folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine",
    "start": "31920",
    "end": "40920"
  },
  {
    "text": "which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um",
    "start": "40920",
    "end": "48440"
  },
  {
    "text": "before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it",
    "start": "48440",
    "end": "55399"
  },
  {
    "text": "is the first party um observability tool built by the us at Lang chain um we're",
    "start": "55399",
    "end": "62640"
  },
  {
    "text": "going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which",
    "start": "62640",
    "end": "68159"
  },
  {
    "text": "is part of Lang Smith um in order to host our application in the end um and",
    "start": "68159",
    "end": "74000"
  },
  {
    "text": "uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um",
    "start": "74000",
    "end": "80280"
  },
  {
    "text": "that allows you to host your chains as rest endpoints um and uh we're going to",
    "start": "80280",
    "end": "85320"
  },
  {
    "text": "be using the hosted one as well within Lang Smith uh let's dive it so first",
    "start": "85320",
    "end": "90439"
  },
  {
    "text": "we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a",
    "start": "90439",
    "end": "96200"
  },
  {
    "text": "rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in",
    "start": "96200",
    "end": "102920"
  },
  {
    "text": "Lang serve in order to um get it accessible from anyone on the internet",
    "start": "102920",
    "end": "108920"
  },
  {
    "text": "um so let's dive in so for our notebook um I'm just going to be using the",
    "start": "108920",
    "end": "114640"
  },
  {
    "text": "Jupiter notebook in my vs code instance um first we're going to want to install",
    "start": "114640",
    "end": "120159"
  },
  {
    "text": "some of our dependencies um as some of you may have noticed we're starting to split out the package into um multiple",
    "start": "120159",
    "end": "126439"
  },
  {
    "text": "smaller partner packages as well so today we're going to be um pip",
    "start": "126439",
    "end": "131720"
  },
  {
    "text": "installing uh Lang chain core which offers um we're going to be mostly using",
    "start": "131720",
    "end": "137400"
  },
  {
    "text": "the runable utilities as well as prompts from there um we're going to use Lang chain open AI um in order to um use the",
    "start": "137400",
    "end": "147040"
  },
  {
    "text": "GPT 3.5 model for Our Generation you can obviously install any uh other llm into",
    "start": "147040",
    "end": "152560"
  },
  {
    "text": "this application as well um and last but not least we're going to be installing the brand new L chain exop package um",
    "start": "152560",
    "end": "158840"
  },
  {
    "text": "which is going to give us our retriever which allows us to kind of search uh the web the Lang chain EXA package also",
    "start": "158840",
    "end": "165120"
  },
  {
    "text": "offers some tools which enable agents to um have search capabilities as well um",
    "start": "165120",
    "end": "170360"
  },
  {
    "text": "but for today's generation use case we're going to be using uh the retriever",
    "start": "170360",
    "end": "175440"
  },
  {
    "text": "um once you have that installed um we are going to set some environment",
    "start": "175440",
    "end": "182280"
  },
  {
    "text": "variables um so to do that we'll have OS and we'll set um our open AI API",
    "start": "182280",
    "end": "192760"
  },
  {
    "text": "key and our X API key",
    "start": "193159",
    "end": "198760"
  },
  {
    "text": "um um I already have these set in my environment so I'm not going to fill this out for yall today um but you can",
    "start": "201519",
    "end": "208400"
  },
  {
    "text": "provision your own open AI API key at uh platform. open.com and uh I'll link the",
    "start": "208400",
    "end": "215200"
  },
  {
    "text": "exod docs um on how to get your own xit API key with some free um search credits",
    "start": "215200",
    "end": "222480"
  },
  {
    "text": "um on it as well to this video um the other thing that we're",
    "start": "222480",
    "end": "228120"
  },
  {
    "text": "going to do is we're going to set some environment variables that um enable us to use Langs Smith um Langs Smith is in",
    "start": "228120",
    "end": "235079"
  },
  {
    "text": "private beta at the moment feel free to DM me in order to get access to it um",
    "start": "235079",
    "end": "241159"
  },
  {
    "text": "so in order to do that um we are going to set the Lang chain",
    "start": "241159",
    "end": "248959"
  },
  {
    "text": "facing uh V2",
    "start": "248959",
    "end": "255400"
  },
  {
    "text": "uh to do that we're going to set the L chain tracing V2 environment variable",
    "start": "258079",
    "end": "263280"
  },
  {
    "text": "we're going to set our um L change API key and we're going to",
    "start": "263280",
    "end": "272160"
  },
  {
    "text": "set our Lang chain project as well um the last step is optional um",
    "start": "272160",
    "end": "281280"
  },
  {
    "text": "and it just allows us to uh find um the traces for what we're doing",
    "start": "285919",
    "end": "292479"
  },
  {
    "text": "today a little bit easier um and most importantly we're going to",
    "start": "292479",
    "end": "300919"
  },
  {
    "text": "build our chain um so the rough architecture of today's chain is we're going to first uh take some sort of user",
    "start": "300919",
    "end": "307800"
  },
  {
    "text": "query um we're going to search EXA to get some documents related to that um",
    "start": "307800",
    "end": "313320"
  },
  {
    "text": "EXA has this great feature we're going to use called highlights which kind of summarizes the highlights from any of the sources it retrieves um and so we're",
    "start": "313320",
    "end": "320759"
  },
  {
    "text": "going to be able to PL those into an llm uh we're going to kind of wrestle with um formatting that prompt in a way that",
    "start": "320759",
    "end": "326560"
  },
  {
    "text": "the llm understands uh well for a little bit and then we're going to pass that into the llm for Generation Um so to",
    "start": "326560",
    "end": "333759"
  },
  {
    "text": "start uh let's actually just play with Exel a little bit and see what that looks like um so we can import our",
    "start": "333759",
    "end": "341680"
  },
  {
    "text": "retriever uh from the L chain EXA package we're going to be using the retriever today um let's just use the",
    "start": "341680",
    "end": "350919"
  },
  {
    "text": "default settings and see what documents we get from that so we",
    "start": "350919",
    "end": "357000"
  },
  {
    "text": "can just run retriever do invoke of uh best time to visit",
    "start": "357000",
    "end": "364800"
  },
  {
    "text": "Japan um and if we inspect those um or inspect the first",
    "start": "365240",
    "end": "372840"
  },
  {
    "text": "one uh we can see that we have some content which is",
    "start": "372840",
    "end": "380800"
  },
  {
    "text": "actually going to be the entire page content um of that website so it's going to be rather large um and we'll also",
    "start": "380800",
    "end": "389199"
  },
  {
    "text": "have some metadata on that document um and so here we'll have um some like kind",
    "start": "389199",
    "end": "397280"
  },
  {
    "text": "of page titles URLs um as well as um if we pass the",
    "start": "397280",
    "end": "404599"
  },
  {
    "text": "highlights feature in we'll actually get some highlights as well so let's pass",
    "start": "404599",
    "end": "410720"
  },
  {
    "text": "in um highlights as true and then let's actually only get the three most",
    "start": "410720",
    "end": "418160"
  },
  {
    "text": "relevant results just so we can save some of our EXA credits um so here we'll",
    "start": "418160",
    "end": "424080"
  },
  {
    "text": "now see that we get um this highlights array which is going to summarize kind of the main points um of this page um so",
    "start": "424080",
    "end": "434000"
  },
  {
    "text": "we're mostly going to be using that uh highlights field as well",
    "start": "434000",
    "end": "441680"
  },
  {
    "text": "as um the URL uh since we want our generation bot to to site its",
    "start": "441680",
    "end": "448879"
  },
  {
    "text": "sources in the generations it do does um so with that let's try to create",
    "start": "448879",
    "end": "457360"
  },
  {
    "text": "some sort of um retrieval chain which is going to format all of those outputs in",
    "start": "457360",
    "end": "463639"
  },
  {
    "text": "a way that we want to pass it to the Ln um so the kind of first step of that is we're going to run our retriever um and",
    "start": "463639",
    "end": "471479"
  },
  {
    "text": "just to keep it clear we can Define our retriever down here as",
    "start": "471479",
    "end": "477199"
  },
  {
    "text": "well um keeping the kind of final output in a single cell makes it little easier",
    "start": "477199",
    "end": "482479"
  },
  {
    "text": "uh for us to convert this to a l serve application at the end um we'll also want to format this with um a prompt",
    "start": "482479",
    "end": "490360"
  },
  {
    "text": "template so let's import that as well from Lang chain core.",
    "start": "490360",
    "end": "497039"
  },
  {
    "text": "prompt and uh we can import other things as we go on so first we're going to",
    "start": "497759",
    "end": "503120"
  },
  {
    "text": "start with our retriever um then we're going to want to uh format that list of",
    "start": "503120",
    "end": "508759"
  },
  {
    "text": "documents in some sort of way so let's pass that into some sort of documents",
    "start": "508759",
    "end": "514159"
  },
  {
    "text": "chain um or single document chain uh and we'll have it operate on each element of",
    "start": "514159",
    "end": "521080"
  },
  {
    "text": "that list so we can kind of call map on that um and let's define that up here",
    "start": "521080",
    "end": "527440"
  },
  {
    "text": "and so the Assumption here is that input is going to be that single document um here we'll have this be a runnable",
    "start": "527440",
    "end": "534440"
  },
  {
    "text": "Lambda where we'll take uh that document and we will pass",
    "start": "534440",
    "end": "542240"
  },
  {
    "text": "back the highlights as well as the uh URL of that document so here we'll have",
    "start": "542240",
    "end": "550120"
  },
  {
    "text": "document. metadata highlights and",
    "start": "550120",
    "end": "556000"
  },
  {
    "text": "document metadata URL um and we'll need to",
    "start": "556000",
    "end": "565800"
  },
  {
    "text": "import um our rable Lambda as well which is going to be from L chain core",
    "start": "567399",
    "end": "574360"
  },
  {
    "text": "runnables P runable Lambda um and then the next",
    "start": "574360",
    "end": "580680"
  },
  {
    "text": "step is we're going to want to format this with some sort of prompt so let's pass this into um our document prompt which we can",
    "start": "580680",
    "end": "589360"
  },
  {
    "text": "Define up here as prompt template uh from",
    "start": "589360",
    "end": "597680"
  },
  {
    "text": "template and here uh let's actually just wrap the whole thing in XML tags just",
    "start": "598120",
    "end": "604279"
  },
  {
    "text": "because it'll allow the llm to kind of isolate information from um the inputed",
    "start": "604279",
    "end": "610480"
  },
  {
    "text": "documents a little bit better um and then we can include the",
    "start": "610480",
    "end": "617439"
  },
  {
    "text": "URL and the",
    "start": "618040",
    "end": "624399"
  },
  {
    "text": "highlights um and in this format the highlights are going to get past in as a list of strings that's probably okay um",
    "start": "625760",
    "end": "633279"
  },
  {
    "text": "if we wanted to make this um a little more intense and we can do this later we can actually uh split those out into",
    "start": "633279",
    "end": "639079"
  },
  {
    "text": "different sections um but because all the highlights come from the same URL uh this is probably okay for the uh point",
    "start": "639079",
    "end": "646200"
  },
  {
    "text": "of our application today um so that kind of forms our document chain um where at",
    "start": "646200",
    "end": "654399"
  },
  {
    "text": "the end of this we're going to have a list of um prompt values um which we can",
    "start": "654399",
    "end": "661240"
  },
  {
    "text": "use Downstream um and then let's then",
    "start": "661240",
    "end": "666399"
  },
  {
    "text": "combine all of those kind of source information contexts um with another",
    "start": "666399",
    "end": "675160"
  },
  {
    "text": "Lambda um which is going to take all the docs and we're going to want to",
    "start": "675160",
    "end": "681760"
  },
  {
    "text": "join some sort of list of these so we can do the text of each of those prompt",
    "start": "681760",
    "end": "688040"
  },
  {
    "text": "values um for each of those documents so let's see what we actually get from that",
    "start": "688040",
    "end": "698279"
  },
  {
    "text": "um it looks like we are missing a comma",
    "start": "698279",
    "end": "706480"
  },
  {
    "text": "somewhere um because a string object is not",
    "start": "706800",
    "end": "713360"
  },
  {
    "text": "callable it looks like here I should have called backend. join at this",
    "start": "713360",
    "end": "720320"
  },
  {
    "text": "um and we can see that we're getting kind of a single string back with our three sources uh as well as the",
    "start": "720320",
    "end": "725800"
  },
  {
    "text": "highlights of uh each of these sources uh where it's recommending public holidays uh the most romantic time of",
    "start": "725800",
    "end": "732399"
  },
  {
    "text": "the year um and the rainy season um which is quite interesting um but now we",
    "start": "732399",
    "end": "739199"
  },
  {
    "text": "kind of have our retrieval chain which we can use in our broader chain down here",
    "start": "739199",
    "end": "745040"
  },
  {
    "text": "um for further processing so um our overall chain is actually going to take",
    "start": "745040",
    "end": "750360"
  },
  {
    "text": "a very similar format to a general rag chain um where we're going to have some",
    "start": "750360",
    "end": "756040"
  },
  {
    "text": "sort of um runnable parallel to start we're going to want to",
    "start": "756040",
    "end": "763360"
  },
  {
    "text": "um start with um plumbing through the query which is just going to be a",
    "start": "763360",
    "end": "768760"
  },
  {
    "text": "runnable pass through um this is going to just pass the users's query onto the",
    "start": "768760",
    "end": "774079"
  },
  {
    "text": "next step through this query uh key and then we're going to want to pass some context Tex um which will be our",
    "start": "774079",
    "end": "781880"
  },
  {
    "text": "retrieval chain then we'll want to pass that into some sort of generation prompt which is",
    "start": "781880",
    "end": "787600"
  },
  {
    "text": "going to take the query and context um and format it for the llm and then we'll",
    "start": "787600",
    "end": "792839"
  },
  {
    "text": "pass that into our llm um so let's fill this out a little bit so first We'll add",
    "start": "792839",
    "end": "800040"
  },
  {
    "text": "some imports um we'll import our runnable pass through and our runnable parallel from Leng chain core um we'll",
    "start": "800040",
    "end": "807959"
  },
  {
    "text": "Define a generation prompt as uh and let's do this one as a",
    "start": "807959",
    "end": "814639"
  },
  {
    "text": "chat prompt so we can use an a chat model for the llm um so we'll use a chat",
    "start": "814639",
    "end": "820560"
  },
  {
    "text": "prompt template for messages um and we'll have some sort of",
    "start": "820560",
    "end": "825839"
  },
  {
    "text": "system prompt as well as the content which we",
    "start": "825839",
    "end": "830880"
  },
  {
    "text": "can fill out in a second um we'll need chat PR template for that for",
    "start": "830880",
    "end": "836920"
  },
  {
    "text": "that uh which co-pilot is quite nicely supporting for us um and then we'll just",
    "start": "836920",
    "end": "843600"
  },
  {
    "text": "use the default chat open AI uh which uses GPT 3.5 turbo um for",
    "start": "843600",
    "end": "850880"
  },
  {
    "text": "our llm so we can import that from Lang chain open aai um import chat open",
    "start": "850880",
    "end": "857519"
  },
  {
    "text": "AI um this is kind of the new format of importing from some of those partner",
    "start": "857519",
    "end": "864279"
  },
  {
    "text": "packages um cool so let's fill out our prompt um so let's tell our llm that you",
    "start": "864279",
    "end": "872320"
  },
  {
    "text": "are an expert uh research assistant um you use um XML formatted",
    "start": "872320",
    "end": "884360"
  },
  {
    "text": "contexts to um to",
    "start": "884360",
    "end": "890519"
  },
  {
    "text": "research people's questions uh and then we'll actually",
    "start": "890519",
    "end": "896880"
  },
  {
    "text": "format this uh query that we're going to ask down here where we're going to say please",
    "start": "896880",
    "end": "903839"
  },
  {
    "text": "answer the following query based on the provided",
    "start": "903839",
    "end": "909720"
  },
  {
    "text": "context and we'll pass in our query um add a little separator and",
    "start": "909720",
    "end": "918199"
  },
  {
    "text": "then continuing with some of the XML tags we can pass in our context",
    "start": "918199",
    "end": "926440"
  },
  {
    "text": "here which is just going to be this some form of the string up here which is going to have multiple",
    "start": "926440",
    "end": "931680"
  },
  {
    "text": "sources um let's also add an instruction to please",
    "start": "931680",
    "end": "938199"
  },
  {
    "text": "site your sources at the end of your response um cool so now we have our",
    "start": "938199",
    "end": "946560"
  },
  {
    "text": "chain defined let's see if we're able to",
    "start": "946560",
    "end": "952079"
  },
  {
    "text": "invoke it and we'll continue with our same",
    "start": "952079",
    "end": "957560"
  },
  {
    "text": "question",
    "start": "957600",
    "end": "960600"
  },
  {
    "text": "and it looks like it's giving us a nice and long response um detailing some things and",
    "start": "964759",
    "end": "971319"
  },
  {
    "text": "hopefully at the end it'll have some sources of what it used to generate that",
    "start": "971319",
    "end": "977079"
  },
  {
    "text": "um so now that we have that constructed and tested um let's actually go back to Lang",
    "start": "977079",
    "end": "985399"
  },
  {
    "text": "Smith and see what it generated for us um so here",
    "start": "985399",
    "end": "992079"
  },
  {
    "text": "in Lang Smith we can see um the default view is going to show us the most uh",
    "start": "992079",
    "end": "998279"
  },
  {
    "text": "kind of relevant pieces of the trace so in this case because it's a retrieval augmented generation bot um it's going",
    "start": "998279",
    "end": "1003839"
  },
  {
    "text": "to be our retrieval step such that we can inspect which documents it's actually retrieving here um as well as",
    "start": "1003839",
    "end": "1010160"
  },
  {
    "text": "the llm call so we can see uh what prompt we're actually passing in so we can see that we're formatting our",
    "start": "1010160",
    "end": "1015680"
  },
  {
    "text": "contexts and sources um properly early here which is good as well as the output from the llm at the end um if you want",
    "start": "1015680",
    "end": "1023079"
  },
  {
    "text": "to inspect kind of what's happening in each of those runnable Lambda steps uh you can inspect further and see um that",
    "start": "1023079",
    "end": "1030438"
  },
  {
    "text": "the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um",
    "start": "1030439",
    "end": "1036079"
  },
  {
    "text": "like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were",
    "start": "1036079",
    "end": "1042120"
  },
  {
    "text": "constructing it um so now we have kind of our uh fully constructed chain uh for",
    "start": "1042120",
    "end": "1047760"
  },
  {
    "text": "our uh search enable chatbot with XF um and now let's convert that to Lang serve",
    "start": "1047760",
    "end": "1055080"
  },
  {
    "text": "um so to do that we'll go back to vs code um and here we're going",
    "start": "1055080",
    "end": "1062280"
  },
  {
    "text": "to um start with uh installing the um Lang chain CLI um we'll just do this in",
    "start": "1062280",
    "end": "1070400"
  },
  {
    "text": "a terminal down here um you can just install um recommend running it with",
    "start": "1070400",
    "end": "1075760"
  },
  {
    "text": "upgrade in case you already haveen it installed from uh either using Lang chain templates or something else",
    "start": "1075760",
    "end": "1082640"
  },
  {
    "text": "um you can just run install upgrade Lang chain clly um then you can uh run the line",
    "start": "1082640",
    "end": "1091559"
  },
  {
    "text": "chain commands so first we're going to initialize our app with Lang chain app new um I'm just going to create it in",
    "start": "1091559",
    "end": "1097679"
  },
  {
    "text": "the current perplexity directory um if you give it a name it'll just create a directory uh within that um we will skip",
    "start": "1097679",
    "end": "1107600"
  },
  {
    "text": "adding packages just because we're going to paste in a chain that we have defined up here um the packages are just L chain",
    "start": "1107600",
    "end": "1114000"
  },
  {
    "text": "templates um and then we can see that it has bootstrapped an application for us um with a Docker file P project toml a",
    "start": "1114000",
    "end": "1121480"
  },
  {
    "text": "readme um as well as an app directory and a packages directory um we're actually not going to be using the",
    "start": "1121480",
    "end": "1127280"
  },
  {
    "text": "packages directory um as mentioned that's for L chain templates and we're just going to be playing in this uh",
    "start": "1127280",
    "end": "1132640"
  },
  {
    "text": "server.py file in the app um so first",
    "start": "1132640",
    "end": "1138559"
  },
  {
    "text": "we're going to um add some dependencies um I use poetry for my dependency",
    "start": "1138559",
    "end": "1143919"
  },
  {
    "text": "management um it allows um it to automatically update this P project toml",
    "start": "1143919",
    "end": "1150039"
  },
  {
    "text": "file which is quite nice um we're going to install the same set of dependencies um that we had in our uh",
    "start": "1150039",
    "end": "1158280"
  },
  {
    "text": "jupyter notebook so we're going to poetry add um linkchain",
    "start": "1158280",
    "end": "1163799"
  },
  {
    "text": "core um and we'll see that show up in the Pi Project tomel and then we're also going to add L chain open Ai and Lang",
    "start": "1163919",
    "end": "1171080"
  },
  {
    "text": "chain XF um I'll add those and then be back in a",
    "start": "1171080",
    "end": "1179600"
  },
  {
    "text": "second and now that we have our dependencies installed um also added the command to the notion doc over here just",
    "start": "1179600",
    "end": "1185720"
  },
  {
    "text": "so it's easier to follow along um now we will copy our chain over into our",
    "start": "1185720",
    "end": "1192679"
  },
  {
    "text": "application um I like keeping the server.py file a little bit cleaner in order to just keep the",
    "start": "1192679",
    "end": "1198880"
  },
  {
    "text": "um fast API kind of app logic separate from the Lang chain chains um so let's",
    "start": "1198880",
    "end": "1204760"
  },
  {
    "text": "actually just Define a chain. piy file next to this um where we",
    "start": "1204760",
    "end": "1212880"
  },
  {
    "text": "can um paste in our um chain from this other",
    "start": "1212880",
    "end": "1220840"
  },
  {
    "text": "document um so I'm pretty sure I can just paste in these",
    "start": "1220840",
    "end": "1226400"
  },
  {
    "text": "two um cells and we'll remove the invoke in the middle um so first we'll Define a",
    "start": "1226400",
    "end": "1233039"
  },
  {
    "text": "retrieval chain and then down here we will uh Define our regular chain um and we can",
    "start": "1233039",
    "end": "1240320"
  },
  {
    "text": "even put all of our Imports together up here to keep uh vs code a little bit",
    "start": "1240320",
    "end": "1247320"
  },
  {
    "text": "happier um and we'll save that file and now our chain should be accessible from",
    "start": "1247320",
    "end": "1252480"
  },
  {
    "text": "our server file um and then by default it doesn't have any rout defined but it",
    "start": "1252480",
    "end": "1259200"
  },
  {
    "text": "gives you this nice little stub and so we'll actually just add uh that chain",
    "start": "1259200",
    "end": "1264559"
  },
  {
    "text": "which we'll import um from app. chain import chain um because we have it",
    "start": "1264559",
    "end": "1272679"
  },
  {
    "text": "in chain. piy as well is chain is the name of our variable um and then that",
    "start": "1272679",
    "end": "1277799"
  },
  {
    "text": "will just host uh this endpoint at the kind of default SL endpoint um if we",
    "start": "1277799",
    "end": "1284080"
  },
  {
    "text": "want we can define a path um as kind of",
    "start": "1284080",
    "end": "1289640"
  },
  {
    "text": "Slash search or something like that um which could be kind of nice so we've",
    "start": "1289640",
    "end": "1295080"
  },
  {
    "text": "copied the chain over we've added the routes um and now uh we can serve it um",
    "start": "1295080",
    "end": "1300760"
  },
  {
    "text": "so let's poetry run L chain",
    "start": "1300760",
    "end": "1306400"
  },
  {
    "text": "serve um and see if that works hopefully I remembered to add all",
    "start": "1306400",
    "end": "1312000"
  },
  {
    "text": "the dependencies um and if you get this message you're probably in business",
    "start": "1312000",
    "end": "1318960"
  },
  {
    "text": "um so from here uh we can see that we're hting it on Port 8000 so we can click",
    "start": "1318960",
    "end": "1324679"
  },
  {
    "text": "onto that um by default it'll show us the stocks page um but we want to go to",
    "start": "1324679",
    "end": "1330400"
  },
  {
    "text": "the search playground um right now it's telling me that there is no applicable",
    "start": "1330400",
    "end": "1336120"
  },
  {
    "text": "renderer found which is too bad um let's go back to our chain and see if we can",
    "start": "1336120",
    "end": "1346039"
  },
  {
    "text": "add a uh type definition for the input um where here we have our",
    "start": "1346039",
    "end": "1353880"
  },
  {
    "text": "input type uh set just to a",
    "start": "1353880",
    "end": "1360279"
  },
  {
    "text": "string um and see if that does anything for",
    "start": "1360279",
    "end": "1366159"
  },
  {
    "text": "us and perfect so now we can see that we get our input as a string um generally",
    "start": "1366159",
    "end": "1372880"
  },
  {
    "text": "Lang serve will try to detect uh the defined input type for our input over",
    "start": "1372880",
    "end": "1378760"
  },
  {
    "text": "here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a",
    "start": "1378760",
    "end": "1385919"
  },
  {
    "text": "string um or some sort of formatted dictionary and so uh we had to Define",
    "start": "1385919",
    "end": "1392279"
  },
  {
    "text": "our input type as a string if you want to pass in uh multiple um pieces of the input you are",
    "start": "1392279",
    "end": "1398840"
  },
  {
    "text": "welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so",
    "start": "1398840",
    "end": "1405600"
  },
  {
    "text": "here let's try um when is the best time to visit",
    "start": "1405600",
    "end": "1412360"
  },
  {
    "text": "Japan um and we can see uh in the link of playground it actually will show us",
    "start": "1412400",
    "end": "1417679"
  },
  {
    "text": "kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all",
    "start": "1417679",
    "end": "1424279"
  },
  {
    "text": "through to the llm um it's taking a little bit of time so we can check our stack Trace right",
    "start": "1424279",
    "end": "1431520"
  },
  {
    "text": "now it is telling me that I am using a wrong API key um so let's try",
    "start": "1431520",
    "end": "1439880"
  },
  {
    "text": "adding my API keys to my environment again and running Lang chain serve",
    "start": "1440200",
    "end": "1449480"
  },
  {
    "text": "again um and then we can try this",
    "start": "1451080",
    "end": "1456200"
  },
  {
    "text": "again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and",
    "start": "1456200",
    "end": "1462400"
  },
  {
    "text": "then in the playground we actually get streaming output as well um which is",
    "start": "1462400",
    "end": "1468360"
  },
  {
    "text": "pretty good uh so here we get kind of a guide for uh why it might depend on",
    "start": "1468360",
    "end": "1475240"
  },
  {
    "text": "several factors let's ask specifically for a ski",
    "start": "1475240",
    "end": "1481919"
  },
  {
    "text": "trip um and see if it gives us a more concrete answer and here it does and it",
    "start": "1481919",
    "end": "1488480"
  },
  {
    "text": "starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about",
    "start": "1488480",
    "end": "1495679"
  },
  {
    "text": "Lan serve is even when using it hosted uh locally like this um we will actually",
    "start": "1495679",
    "end": "1502200"
  },
  {
    "text": "see these traces show up in lsmith as well um so here because the playground",
    "start": "1502200",
    "end": "1508559"
  },
  {
    "text": "is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked",
    "start": "1508559",
    "end": "1516000"
  },
  {
    "text": "output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace",
    "start": "1516000",
    "end": "1523320"
  },
  {
    "text": "actually looks pretty similar um where we can see kind of the that's actually",
    "start": "1523320",
    "end": "1528440"
  },
  {
    "text": "formatted in the um chat open AI step and we can even see which documents are",
    "start": "1528440",
    "end": "1534360"
  },
  {
    "text": "actually plumed through um one note is this app that we build is just uh using",
    "start": "1534360",
    "end": "1540399"
  },
  {
    "text": "the highlights that are given to us by EXA um if you want you can also operate",
    "start": "1540399",
    "end": "1546520"
  },
  {
    "text": "on the entire page content um but the EXA highlights work pretty well and they",
    "start": "1546520",
    "end": "1552039"
  },
  {
    "text": "save you a lot of tokens um as mentioned earlier um if you want access to Lang",
    "start": "1552039",
    "end": "1558279"
  },
  {
    "text": "Smith uh feel free to DM me um and I can get you access to that okay and then last but not least uh",
    "start": "1558279",
    "end": "1565760"
  },
  {
    "text": "we're going to um host our chain in hosted Lang serve um so hosted Lang",
    "start": "1565760",
    "end": "1573880"
  },
  {
    "text": "serve is this deployments tab in Lang Smith um right now I don't have any deployments in my",
    "start": "1573880",
    "end": "1580840"
  },
  {
    "text": "um uh in this account um so we can go over to uh new deployment and see kind",
    "start": "1580840",
    "end": "1588919"
  },
  {
    "text": "of what steps it wants us to do we'll name our deployment so here we'll have this as",
    "start": "1588919",
    "end": "1595080"
  },
  {
    "text": "perplexa T um emphasis on EXA and then",
    "start": "1595080",
    "end": "1600240"
  },
  {
    "text": "we'll want to import our repository from GitHub um so in this case I'll actually just",
    "start": "1600240",
    "end": "1605760"
  },
  {
    "text": "deploy um this uh project to just a",
    "start": "1605760",
    "end": "1611200"
  },
  {
    "text": "public repository um you can also deploy uh private repos and that all works well",
    "start": "1611200",
    "end": "1618320"
  },
  {
    "text": "too um so here we'll want to initialize our repo um we'll see what it is going",
    "start": "1618320",
    "end": "1625919"
  },
  {
    "text": "to try to commit um just in case we've created any",
    "start": "1625919",
    "end": "1631880"
  },
  {
    "text": "of these py cach folders let's actually create a g ignore where we'll",
    "start": "1631880",
    "end": "1637600"
  },
  {
    "text": "oh we already have py cach in there so let's try adding everything and see what",
    "start": "1637600",
    "end": "1642640"
  },
  {
    "text": "files it's going to add that looks pretty good um",
    "start": "1642640",
    "end": "1649120"
  },
  {
    "text": "and uh we can commit um that project there um I'm",
    "start": "1649120",
    "end": "1656279"
  },
  {
    "text": "realizing I'm using a a local development version of the EXA package because it's not published yet um so I'm",
    "start": "1656279",
    "end": "1661799"
  },
  {
    "text": "going to fix that and be back in a second okay I fixed that",
    "start": "1661799",
    "end": "1669000"
  },
  {
    "text": "dependency um that will not be a problem for you because you'll be using piie dependencies if you do ever use um kind",
    "start": "1669000",
    "end": "1675600"
  },
  {
    "text": "of local development dependencies in your your application you will have to either make those git dependencies or uh",
    "start": "1675600",
    "end": "1682200"
  },
  {
    "text": "publish them to piie in order for hosted L serve to be able to download them um unless it's kind of within the same",
    "start": "1682200",
    "end": "1688279"
  },
  {
    "text": "directory um so let's uh commit our",
    "start": "1688279",
    "end": "1694200"
  },
  {
    "text": "project um and then we",
    "start": "1694200",
    "end": "1699240"
  },
  {
    "text": "can create a repo",
    "start": "1699240",
    "end": "1704519"
  },
  {
    "text": "um we'll push our existing local deposit repository we'll call it FX",
    "start": "1704519",
    "end": "1714320"
  },
  {
    "text": "T um lank chain perplexity uh we'll push it to just my",
    "start": "1714320",
    "end": "1720080"
  },
  {
    "text": "personal ER don't need a description and we'll make it public um we can add the",
    "start": "1720080",
    "end": "1726840"
  },
  {
    "text": "remote As origin and we will push our local commits to it",
    "start": "1726840",
    "end": "1735159"
  },
  {
    "text": "um cool so now we can and go over to GitHub and see if that",
    "start": "1735159",
    "end": "1741399"
  },
  {
    "text": "is um properly showing up as L chain perplexity um and looks",
    "start": "1741399",
    "end": "1749200"
  },
  {
    "text": "like we are in business um so if we go back to our deployments we can import that repository from",
    "start": "1749200",
    "end": "1754960"
  },
  {
    "text": "GitHub um we'll give it access to Lang chain",
    "start": "1754960",
    "end": "1761960"
  },
  {
    "text": "perplexity and we will authorize",
    "start": "1761960",
    "end": "1769279"
  },
  {
    "text": "and we can close that back here we can see that we have access to that",
    "start": "1769279",
    "end": "1774440"
  },
  {
    "text": "repository if you give access to all your repos um you'll just be able to select that here um it's stored in the",
    "start": "1774440",
    "end": "1781000"
  },
  {
    "text": "root of the directory we're using the main branch um and we'll need to set some environment variables so here we're",
    "start": "1781000",
    "end": "1787519"
  },
  {
    "text": "going to want our open AI API key we're going to want uh that to be a secret and",
    "start": "1787519",
    "end": "1794320"
  },
  {
    "text": "we're going to want our EXA API key as well um I'm going to populate those two",
    "start": "1794320",
    "end": "1801799"
  },
  {
    "text": "and be back in a second okay now that we have those environment variables Set uh we can",
    "start": "1801799",
    "end": "1807640"
  },
  {
    "text": "submit our deployment and uh hosted Lang serve will kind of take",
    "start": "1807640",
    "end": "1813720"
  },
  {
    "text": "over from here um it'll provision a machine for us um hosted Lang serve is",
    "start": "1813720",
    "end": "1819640"
  },
  {
    "text": "very well integrated with Lang Smith um such that all of our traces will actually populate in its own project",
    "start": "1819640",
    "end": "1825679"
  },
  {
    "text": "over there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for",
    "start": "1825679",
    "end": "1835559"
  },
  {
    "text": "hosted Lang serve um this is also in private beta um and if you would like",
    "start": "1835559",
    "end": "1840840"
  },
  {
    "text": "access to it uh feel free to DM me about that as well um so we set our",
    "start": "1840840",
    "end": "1846080"
  },
  {
    "text": "environment variables and um I'll kind of cut out the section where this is",
    "start": "1846080",
    "end": "1851120"
  },
  {
    "text": "deploying um and then we can test it in the hosted",
    "start": "1851120",
    "end": "1856240"
  },
  {
    "text": "playground okay and now our uh posted Lang",
    "start": "1856240",
    "end": "1861320"
  },
  {
    "text": "application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it",
    "start": "1861320",
    "end": "1867760"
  },
  {
    "text": "are showing up and we can even go to our same",
    "start": "1867760",
    "end": "1873240"
  },
  {
    "text": "search playground endpoint um and see if we can ask a question um such",
    "start": "1873240",
    "end": "1881399"
  },
  {
    "text": "as when is the best time to visit Japan for the cherry blossoms",
    "start": "1881399",
    "end": "1889158"
  },
  {
    "text": "and we get a result in recap uh today we uh went",
    "start": "1892679",
    "end": "1900600"
  },
  {
    "text": "through uh building kind of a search enabled chatbot with EXA um using Lang",
    "start": "1900600",
    "end": "1906760"
  },
  {
    "text": "chain EXA Lang Smith and Lang serve uh we started by developing our chain in a",
    "start": "1906760",
    "end": "1913240"
  },
  {
    "text": "jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported",
    "start": "1913240",
    "end": "1921440"
  },
  {
    "text": "that over to a lang serve application using the Lang chain CLI um and kind of",
    "start": "1921440",
    "end": "1927399"
  },
  {
    "text": "monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve",
    "start": "1927399",
    "end": "1933679"
  },
  {
    "text": "application in hosted Lang serve uh also a beta product um as part of Lang Smith",
    "start": "1933679",
    "end": "1939840"
  },
  {
    "text": "um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and",
    "start": "1939840",
    "end": "1947240"
  },
  {
    "text": "and thanks for listening bye-bye",
    "start": "1947240",
    "end": "1952080"
  }
]