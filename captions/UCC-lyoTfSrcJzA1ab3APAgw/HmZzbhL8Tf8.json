[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "text": "foreign for doing this uh morning your time uh",
    "start": "120",
    "end": "6480"
  },
  {
    "text": "you know this is a very exciting webinar we had the most number of registrations for this webinar over 200 odd people",
    "start": "6480",
    "end": "13740"
  },
  {
    "text": "have registered for the webinar uh and uh uh you know unfortunately for all the",
    "start": "13740",
    "end": "19560"
  },
  {
    "text": "four parts with zoom it is what it is but here we are so very excited uh and",
    "start": "19560",
    "end": "27960"
  },
  {
    "text": "uh you know so for all of you who are there who are listening to this webinar and will join in the future this webinar",
    "start": "27960",
    "end": "34620"
  },
  {
    "text": "is a part of the series of seed to scale uh decoding AI uh at Excel in India we",
    "start": "34620",
    "end": "40980"
  },
  {
    "text": "have started the series helping entrepreneurs Founders AI Engineers uh",
    "start": "40980",
    "end": "46140"
  },
  {
    "text": "and the AI enthusiasts to discover artificial intelligence applications and",
    "start": "46140",
    "end": "52200"
  },
  {
    "text": "specifically very targeted on how to build applications in Ai and Lang chain needs no introduction I think anybody",
    "start": "52200",
    "end": "58379"
  },
  {
    "text": "who has ever been interested in AI or has been building apps for the last you",
    "start": "58379",
    "end": "64500"
  },
  {
    "text": "know one year or so has has come across language and I think it's a go-to framework for people to build uh their",
    "start": "64500",
    "end": "71460"
  },
  {
    "text": "AI apps um I am very excited to have lands on the webinar this webinar has taken three",
    "start": "71460",
    "end": "78240"
  },
  {
    "text": "to four months in the making uh first time I heard Lance was on the ml Ops community",
    "start": "78240",
    "end": "83900"
  },
  {
    "text": "and it was quite exciting to hear him I hope today's webinar is as exciting for",
    "start": "83900",
    "end": "89939"
  },
  {
    "text": "all of you he has agreed to do uh almost like almost uh I mean Langston is so",
    "start": "89939",
    "end": "96060"
  },
  {
    "start": "95000",
    "end": "151000"
  },
  {
    "text": "deep that uh you know I can't call it a master class it's very difficult to cover everything in short but he has",
    "start": "96060",
    "end": "102780"
  },
  {
    "text": "agreed to give it a go so here we are uh thank you Lance for doing this absolutely well it's great to be here",
    "start": "102780",
    "end": "109500"
  },
  {
    "text": "and actually um I spent a summer in India during college I was very fond of my time there",
    "start": "109500",
    "end": "114600"
  },
  {
    "text": "in fact Hyderabad as well as Kerala so um yeah I always enjoy speaking with Indian Community it's fantastic",
    "start": "114600",
    "end": "121740"
  },
  {
    "text": "can you see my screen yes we can see your screen okay maybe I should kick it off",
    "start": "121740",
    "end": "127259"
  },
  {
    "text": "so on land I'm a software engineer at line chain I've been at lion chain I've been working with langshane since March",
    "start": "127259",
    "end": "133440"
  },
  {
    "text": "or February uh but eventually exchange since about May and I spent many years in applied AI I",
    "start": "133440",
    "end": "140640"
  },
  {
    "text": "actually worked on self-driving cars for many years the PHD at Stanford um and more recently have been obviously",
    "start": "140640",
    "end": "147060"
  },
  {
    "text": "working very closely with language models which has been a lot of fun um so maybe first or some context what is",
    "start": "147060",
    "end": "154080"
  },
  {
    "text": "Lang Chan Lang Chang is really an application development framework that makes it easy to build llm-powered",
    "start": "154080",
    "end": "159599"
  },
  {
    "text": "applications but it has a few different pieces so at the bottom we'll talk about this",
    "start": "159599",
    "end": "165000"
  },
  {
    "text": "in detail is some of the newer pieces we've introduced uh on platform this is Lang Smith",
    "start": "165000",
    "end": "171120"
  },
  {
    "text": "which is a set of tools for debugging testing and monitoring applications in the middle here you can see building",
    "start": "171120",
    "end": "177420"
  },
  {
    "text": "blocks and these are kind of the core pieces of our open source Library which people are probably more familiar with and have used",
    "start": "177420",
    "end": "183599"
  },
  {
    "text": "and the top I show kind of some common use cases and we'll talk through all this",
    "start": "183599",
    "end": "189239"
  },
  {
    "text": "um and I'll give you some kind of context as to how to use Lang chain with these various applications",
    "start": "189239",
    "end": "195360"
  },
  {
    "text": "um let's talk about building blocks quickly first um so loaders this connects kind of the",
    "start": "195360",
    "end": "202200"
  },
  {
    "text": "Lang chain world to your data or to different kinds of data and we have a lot of them is the point there's around",
    "start": "202200",
    "end": "208080"
  },
  {
    "text": "140 Integrations um it covers both public data you know",
    "start": "208080",
    "end": "213300"
  },
  {
    "text": "like YouTube or Twitter um in proprietary data like your documents your internal company",
    "start": "213300",
    "end": "219780"
  },
  {
    "text": "documents unstructured data just like text files as well as structured spreadsheets apis",
    "start": "219780",
    "end": "226620"
  },
  {
    "text": "and so forth so this kind of gives you an overview of how to get data into line chain",
    "start": "226620",
    "end": "232739"
  },
  {
    "text": "um Now text Splitters are kind of second piece here we actually have a playground",
    "start": "232739",
    "end": "238140"
  },
  {
    "text": "um Tech players are commonly used to basically break documents up and to put",
    "start": "238140",
    "end": "243659"
  },
  {
    "text": "them and embed them in Vector databases so this is kind of a nice playground you can play with we have many different",
    "start": "243659",
    "end": "249480"
  },
  {
    "text": "text Splitters um but beyond that there's actually a few other things you can do with we call",
    "start": "249480",
    "end": "255299"
  },
  {
    "text": "document Transformers is kind of General category um we have a number of tools for what we",
    "start": "255299",
    "end": "261600"
  },
  {
    "text": "call context aware splitting so you split the document and it could be code it could be markdown it could be",
    "start": "261600",
    "end": "267600"
  },
  {
    "text": "PDF but every chunk actually has a kind of a tag as to where",
    "start": "267600",
    "end": "274800"
  },
  {
    "text": "that came from in the original document so it can be like you know the abstract or the introduction within the original",
    "start": "274800",
    "end": "280440"
  },
  {
    "text": "document that's preserved in the chunks which is useful context which we we can talk about a little bit later",
    "start": "280440",
    "end": "286560"
  },
  {
    "text": "uh we have other things like doctran that you can use for translation of documents so I think the main point here",
    "start": "286560",
    "end": "293220"
  },
  {
    "text": "is that you have loaders to pull data in U.S Transformers like text Splitters to",
    "start": "293220",
    "end": "298919"
  },
  {
    "text": "break text up or to do other Transformations on it and then you have storage",
    "start": "298919",
    "end": "305759"
  },
  {
    "text": "so we have over 40 Vector stores we have over 30 embedding models",
    "start": "305759",
    "end": "310919"
  },
  {
    "text": "um and this is very commonly used as an intermediate to store information prior",
    "start": "310919",
    "end": "316199"
  },
  {
    "text": "to retrieval and various types of llm applications um one interesting note here is we have",
    "start": "316199",
    "end": "322620"
  },
  {
    "text": "options for both hosted and private so for example with hosted everyone's",
    "start": "322620",
    "end": "327660"
  },
  {
    "text": "familiar with using open AI for example opening is very popular as well as hosted Vector stores like",
    "start": "327660",
    "end": "333180"
  },
  {
    "text": "Pinecone but also if you want to do something only on your device we have embeddings like Nomex gbt for",
    "start": "333180",
    "end": "339660"
  },
  {
    "text": "all that will run locally we have Vector stores like chroma that of course will run locally as well",
    "start": "339660",
    "end": "346620"
  },
  {
    "text": "um and for llms we have over 60 Integrations",
    "start": "346620",
    "end": "352979"
  },
  {
    "text": "um and maybe I'll highlight some interesting tidbits of the landscape",
    "start": "352979",
    "end": "358860"
  },
  {
    "text": "so everyone's probably familiar with openai models which vary in context window from 4K tokens up to 32k",
    "start": "358860",
    "end": "367080"
  },
  {
    "text": "gpd4 currently is state of the art um kind of overall best performance",
    "start": "367080",
    "end": "373080"
  },
  {
    "text": "um and the cost is around 0.06 cents per 1K token but it's worth",
    "start": "373080",
    "end": "378720"
  },
  {
    "text": "noting other models like anthropic have emerged a larger context window very",
    "start": "378720",
    "end": "384240"
  },
  {
    "text": "close to gpt4 for cloud 2. four to five x cheaper and then the open source ecosystem with",
    "start": "384240",
    "end": "391620"
  },
  {
    "text": "walls like llama2 is becoming very popular smaller context window",
    "start": "391620",
    "end": "397199"
  },
  {
    "text": "the performance is on par with gbt35 I show some details down here you can",
    "start": "397199",
    "end": "403620"
  },
  {
    "text": "see on language it's pretty close here's three five here's llama two this is llama 270b",
    "start": "403620",
    "end": "412039"
  },
  {
    "text": "um but it does lag quite a bit on coding which is an interesting to talk about we might talk about that a little bit later",
    "start": "412259",
    "end": "418440"
  },
  {
    "text": "um and of course they're free now one other cool thing about",
    "start": "418440",
    "end": "423780"
  },
  {
    "text": "um these models is actually I'll go in slideshow mode here sorry I realized that that might help a",
    "start": "423780",
    "end": "429720"
  },
  {
    "text": "little bit they run locally so this is llama 2 for example running at 50 tokens per second",
    "start": "429720",
    "end": "436979"
  },
  {
    "text": "um on on a Mac N2 32 gigs um so that's kind of the benefit open",
    "start": "436979",
    "end": "443220"
  },
  {
    "text": "source models of course are smaller a little bit less performant but they're free they can run on device",
    "start": "443220",
    "end": "450419"
  },
  {
    "text": "um and so they're they're very interesting to kind of track um in terms of the progress",
    "start": "450419",
    "end": "456840"
  },
  {
    "text": "um with this Integrations Hub which gives you kind of a nice overview of all",
    "start": "456840",
    "end": "462360"
  },
  {
    "text": "the various Integrations we have in Langston you can see kind of on the left here over 140 well 140 exactly loaders",
    "start": "462360",
    "end": "470160"
  },
  {
    "text": "over 40 Vector stores and betting models and so forth you can kind of browse here I'll share these slides of course and",
    "start": "470160",
    "end": "475560"
  },
  {
    "text": "the links but you can kind of browse and see what Integrations you're interested in what you want to test",
    "start": "475560",
    "end": "481940"
  },
  {
    "text": "um now let's talk about use cases so this is kind of a nice overview of all the different building blocks but like what",
    "start": "482880",
    "end": "489240"
  },
  {
    "start": "483000",
    "end": "866000"
  },
  {
    "text": "do you want to use them for what can they be used for in kind of practical applications",
    "start": "489240",
    "end": "495240"
  },
  {
    "text": "um probably the most popular which a lot of people are interested in is is rag retrieval augment to generation and the",
    "start": "495240",
    "end": "501240"
  },
  {
    "text": "idea here is we all know these large language malls are trained on large corpses of data",
    "start": "501240",
    "end": "506699"
  },
  {
    "text": "typically like large amount of internet data as well as code but of course that doesn't mean it does not include your",
    "start": "506699",
    "end": "513180"
  },
  {
    "text": "data doesn't include your personal document it doesn't include your company documents that's proprietary so a very",
    "start": "513180",
    "end": "518339"
  },
  {
    "text": "popular Paradigm here is simply take documents that are relevant to a question from a local storage I.E a",
    "start": "518339",
    "end": "524940"
  },
  {
    "text": "vector store and load those into the context Windows model",
    "start": "524940",
    "end": "530100"
  },
  {
    "text": "and then the model kind of has this information available to it to answer the question this is like a",
    "start": "530100",
    "end": "536100"
  },
  {
    "text": "very general concept so we talk about loaders we talk about Splitters we talk about storage we introduced this new idea of a",
    "start": "536100",
    "end": "541800"
  },
  {
    "text": "retriever system often up after store which given a question you fetch rolled",
    "start": "541800",
    "end": "547500"
  },
  {
    "text": "in splits using some methodology it could be similarity search or otherwise",
    "start": "547500",
    "end": "553740"
  },
  {
    "text": "and you load that in the prompt and we'll actually show exactly how that works here in a minute it's worth noting that Landing chain has a few different",
    "start": "553740",
    "end": "559380"
  },
  {
    "text": "ways to do this a few different levels of abstraction so if you want kind of a",
    "start": "559380",
    "end": "564720"
  },
  {
    "text": "a pipeline that will just work you pass in a loader it does everything else we",
    "start": "564720",
    "end": "570360"
  },
  {
    "text": "have Dr story next Creator if you want to specify the splitter the vector store independently retrieval QA if you just",
    "start": "570360",
    "end": "576959"
  },
  {
    "text": "want a chain to handle that kind of prompt and llm load QA chain so it's",
    "start": "576959",
    "end": "583080"
  },
  {
    "text": "important to recognize a line chain you can use it at many different levels of attraction and this is just kind of one example of",
    "start": "583080",
    "end": "589800"
  },
  {
    "text": "that now this is where I want to walk through this a little bit slowly because this is",
    "start": "589800",
    "end": "595620"
  },
  {
    "text": "introducing one of our new tools called langsmith so lagsmith is very easy to use you set",
    "start": "595620",
    "end": "602580"
  },
  {
    "text": "some environment variables and every time you use LINE chain all your Generations are logged or",
    "start": "602580",
    "end": "608700"
  },
  {
    "text": "dashboard and you can go and explore everything you did later and in particular you can see here's one",
    "start": "608700",
    "end": "615240"
  },
  {
    "text": "example retrieval chain I ran you can see here's the trace so here's exactly what's going on right so I ran retrieval",
    "start": "615240",
    "end": "622500"
  },
  {
    "text": "QA on some Vector store with some question and we can see we actually here's where",
    "start": "622500",
    "end": "629459"
  },
  {
    "text": "we actually retrieve some documents and this is actually the llm call and",
    "start": "629459",
    "end": "634980"
  },
  {
    "text": "this is actually the prompt so this is showing you this is the system message totally fine use the",
    "start": "634980",
    "end": "641399"
  },
  {
    "text": "following pieces of content to answer the question if you don't know just so you don't know and this is literally the chunks that we",
    "start": "641399",
    "end": "648899"
  },
  {
    "text": "retrieved from our system pertaining to the question here's the question llm processes is",
    "start": "648899",
    "end": "655079"
  },
  {
    "text": "exactly what the llmcs llm produces the response so this is the landsmith trace is",
    "start": "655079",
    "end": "661200"
  },
  {
    "text": "actually linked here you can explore that but this gives you kind of a nice window into what exactly is going on",
    "start": "661200",
    "end": "668040"
  },
  {
    "text": "under the hood when you're running these chains and when you're running all that it's often kind of confusing to know",
    "start": "668040",
    "end": "673620"
  },
  {
    "text": "well what exactly was the prompt that was pressed passed in and this is one way to see it",
    "start": "673620",
    "end": "679500"
  },
  {
    "text": "and with retrieval you can see really all you're doing is you're grabbing documents from your",
    "start": "679500",
    "end": "684660"
  },
  {
    "text": "vector store a similar system using some kind of search like similarity search based on your question",
    "start": "684660",
    "end": "690360"
  },
  {
    "text": "so maybe even unpacking that a little bit more you take your question you're embedding the question you have all your",
    "start": "690360",
    "end": "695399"
  },
  {
    "text": "documents they're also embedded you do some embedding lookup for similarity you fish out documents some of your question",
    "start": "695399",
    "end": "702240"
  },
  {
    "text": "and that's what you see right here these are those documents past the model model processes them and",
    "start": "702240",
    "end": "707640"
  },
  {
    "text": "produces the response um now one extension of this is chat",
    "start": "707640",
    "end": "716040"
  },
  {
    "text": "and really chat is kind of just augmented what we just talked about with the idea of memory so it can remember",
    "start": "716040",
    "end": "721140"
  },
  {
    "text": "the whole history of your conversation and of course you can have chat plus retrieval they too those two can I work",
    "start": "721140",
    "end": "727860"
  },
  {
    "text": "together where you can inject documents into your chat flow",
    "start": "727860",
    "end": "733920"
  },
  {
    "text": "so this is just a trace for chat you can see all that's happening is",
    "start": "733920",
    "end": "739140"
  },
  {
    "text": "you're persisting the history of the conversation over time and you pass it into the context of LM it has this whole",
    "start": "739140",
    "end": "745079"
  },
  {
    "text": "history once when it's producing responses that's all that's going on",
    "start": "745079",
    "end": "750240"
  },
  {
    "text": "um there's different types of memory I link to the documents here you can kind of explore the different you know",
    "start": "750240",
    "end": "756480"
  },
  {
    "text": "there's different types of buffers um some will preserve just a summary of the conversation to save tokens",
    "start": "756480",
    "end": "763200"
  },
  {
    "text": "others will pass the full conversation verbatim to the model so there's some interesting options to consider",
    "start": "763200",
    "end": "770420"
  },
  {
    "text": "um summarization is another big use case and we can kind of walk through this one a little bit slowly we have some",
    "start": "771300",
    "end": "777060"
  },
  {
    "text": "interesting uh very specific examples as well here so",
    "start": "777060",
    "end": "782100"
  },
  {
    "text": "the idea here is you want to take a document and summarize it and often it's not just one document it often is like a",
    "start": "782100",
    "end": "788519"
  },
  {
    "text": "large Corpus of documents so the kind of key decisions are",
    "start": "788519",
    "end": "793920"
  },
  {
    "text": "does it fit into the llm context window if yes you can just pass the documents",
    "start": "793920",
    "end": "799920"
  },
  {
    "text": "in exactly to the prompt and tell them to summarize it",
    "start": "799920",
    "end": "804959"
  },
  {
    "text": "if no you have some options you can",
    "start": "804959",
    "end": "810480"
  },
  {
    "text": "basically cluster the embeddings associated with the documents you can break the document up cluster them and",
    "start": "810480",
    "end": "818220"
  },
  {
    "text": "sample from clusters so here all you're saying is hey these",
    "start": "818220",
    "end": "823680"
  },
  {
    "text": "chunks of the document are similar pick one per cluster take those samples pass them into lln",
    "start": "823680",
    "end": "831660"
  },
  {
    "text": "that's it you can also take each cluster and summarize it with an llm call as well",
    "start": "831660",
    "end": "838740"
  },
  {
    "text": "so that's like imagine all these similar documents that are read just pass them all to an llm say what's",
    "start": "838740",
    "end": "844680"
  },
  {
    "text": "happening in this cluster llm answers you and the third option is this idea of",
    "start": "844680",
    "end": "850139"
  },
  {
    "text": "mapreduce where you take every chunk of your Corpus",
    "start": "850139",
    "end": "855420"
  },
  {
    "text": "and you Summarize each chunk with an llm you pass those summaries to a second llm",
    "start": "855420",
    "end": "861060"
  },
  {
    "text": "to say distill the summaries into a final summary that's this mapreduce idea",
    "start": "861060",
    "end": "867000"
  },
  {
    "start": "866000",
    "end": "940000"
  },
  {
    "text": "so we've actually used this and we have a repo here obviously we'll share the slides we've tested clustering and",
    "start": "867000",
    "end": "873420"
  },
  {
    "text": "mapreduce on a very you know typical application launching gets thousands of questions",
    "start": "873420",
    "end": "879300"
  },
  {
    "text": "about our documents every day and we actually use these summarization",
    "start": "879300",
    "end": "884399"
  },
  {
    "text": "chains to summarize what are people asking about what are people confused about and we actually did this with mendable",
    "start": "884399",
    "end": "890459"
  },
  {
    "text": "which is one of our partners they had a lot of the QA in our documents and you can see this is like a breakdown",
    "start": "890459",
    "end": "896639"
  },
  {
    "text": "of the various question categories um using different methods they're",
    "start": "896639",
    "end": "901740"
  },
  {
    "text": "pretty consistent which is a good sign um this is using different models we use",
    "start": "901740",
    "end": "906779"
  },
  {
    "text": "anthropic we use different methods mapreduce and clustering we use again you can see the different models here",
    "start": "906779",
    "end": "913260"
  },
  {
    "text": "and the nice thing is I share the blog post below I share the notebook you can you can look through all that's all open",
    "start": "913260",
    "end": "918420"
  },
  {
    "text": "source but what's nice is you can with Lion King very easily you do different models to do this kind of summarization",
    "start": "918420",
    "end": "924959"
  },
  {
    "text": "and it's very easy to Plug and Play you can try clog two you can try gbp4 gpd3 516k it's easy to Plug and Play",
    "start": "924959",
    "end": "934019"
  },
  {
    "text": "um so this is kind of one very practical case where we use summarization to benefit our documentation",
    "start": "934019",
    "end": "940040"
  },
  {
    "start": "940000",
    "end": "1062000"
  },
  {
    "text": "another use case is extraction so this is a classic problem llm's",
    "start": "940040",
    "end": "945899"
  },
  {
    "text": "output raw text if anyone's ever tried to take that raw text output and stuff it into something Downstream an API call",
    "start": "945899",
    "end": "952920"
  },
  {
    "text": "say you have like some some kind of database you want structured data it's very tedious you have to do lots of",
    "start": "952920",
    "end": "958260"
  },
  {
    "text": "prompt engineering to get them all to reduce Json for example and functions really opened this up it's",
    "start": "958260",
    "end": "965339"
  },
  {
    "text": "very helpful and important to look at so here's a good example",
    "start": "965339",
    "end": "970440"
  },
  {
    "text": "you have some input string tell the llm the scheme of what you want",
    "start": "970440",
    "end": "976440"
  },
  {
    "text": "I want you know these objects with properties name height hair color",
    "start": "976440",
    "end": "981959"
  },
  {
    "text": "and you describe the function call you want the model to execute the llm will actually take your input execute the",
    "start": "981959",
    "end": "988139"
  },
  {
    "text": "function produce output in the schema you specify very very helpful",
    "start": "988139",
    "end": "994620"
  },
  {
    "text": "um and here we can see exactly what's going on under the hood there's a langsmith Trace",
    "start": "994620",
    "end": "999899"
  },
  {
    "text": "as we can see the prompt is telling them extract and save relevant entities mentioned in the following texts",
    "start": "999899",
    "end": "1007660"
  },
  {
    "text": "use this information extraction function to do that",
    "start": "1007660",
    "end": "1013399"
  },
  {
    "text": "here's the passage and what you don't see is we've passed",
    "start": "1013399",
    "end": "1018980"
  },
  {
    "text": "this function in with this schema to the open AI API so",
    "start": "1018980",
    "end": "1024918"
  },
  {
    "text": "open AI has the knowledge of this function it has an alternative scheme we want out and the prompt just says hey",
    "start": "1024919",
    "end": "1031459"
  },
  {
    "text": "use this function to process the output here's the passage and there you go you",
    "start": "1031459",
    "end": "1037819"
  },
  {
    "text": "get the output as desired now what's kind of nice is this chain also runs a parser",
    "start": "1037819",
    "end": "1043280"
  },
  {
    "text": "takes this Json parses it for you and you get exactly just this object out very nice so that's how function calling",
    "start": "1043280",
    "end": "1050720"
  },
  {
    "text": "kind of works with extraction it's very very useful so I highly encourage you to check out we have a lot of documentation",
    "start": "1050720",
    "end": "1056240"
  },
  {
    "text": "on that here as well um but this is highly useful and I'll mail",
    "start": "1056240",
    "end": "1063440"
  },
  {
    "text": "talk about one or two more we can just get into freeform questions then um Texas Equal",
    "start": "1063440",
    "end": "1069919"
  },
  {
    "text": "a lot of people kind of intuitively know what they want to access from SQL databases in natural language writing",
    "start": "1069919",
    "end": "1076940"
  },
  {
    "text": "SQL queries is tedious I think many of us particularly software Engineers don't necessarily love running writing SQL queries",
    "start": "1076940",
    "end": "1083419"
  },
  {
    "text": "um llms are very good at doing that so we have a number of different chains",
    "start": "1083419",
    "end": "1088940"
  },
  {
    "text": "that allow you to take a natural language question map into a SQL query SQL database",
    "start": "1088940",
    "end": "1095179"
  },
  {
    "text": "and we also have llms that will kind of process that answer for you here's what's going on again let's look",
    "start": "1095179",
    "end": "1101299"
  },
  {
    "text": "under the hood a little bit there's some interesting papers that have shown Texas SQL is quite effective",
    "start": "1101299",
    "end": "1108020"
  },
  {
    "text": "if you pass the llm a create table description for the tables in your database",
    "start": "1108020",
    "end": "1114200"
  },
  {
    "text": "with us uh some examples in a select statement so if you give the llm this context it's",
    "start": "1114200",
    "end": "1121100"
  },
  {
    "text": "able to write SQL queries it's very interesting so this is exactly what's going on here",
    "start": "1121100",
    "end": "1127520"
  },
  {
    "text": "um and you can see that given a question it's able to kind of produce valid SQL given",
    "start": "1127520",
    "end": "1135799"
  },
  {
    "text": "context about how the table structured L1 is very very effective at doing this",
    "start": "1135799",
    "end": "1141760"
  },
  {
    "text": "um and I mentioned that agents can be used here",
    "start": "1142460",
    "end": "1148220"
  },
  {
    "start": "1144000",
    "end": "1349000"
  },
  {
    "text": "so this may be an interesting segue to like kind of what agents are and why they can be useful",
    "start": "1148220",
    "end": "1153620"
  },
  {
    "text": "one simple way to think about agents are agents are things or objects that have",
    "start": "1153620",
    "end": "1159919"
  },
  {
    "text": "access to tools and have access to memory so we can kind of walk through it like a basic llm",
    "start": "1159919",
    "end": "1165200"
  },
  {
    "text": "has no tools no memory now",
    "start": "1165200",
    "end": "1171100"
  },
  {
    "text": "LMS with function calling can have access to tools but not",
    "start": "1171100",
    "end": "1176480"
  },
  {
    "text": "necessary memory so like we've we've just shown that you can have function calls I'll access you",
    "start": "1176480",
    "end": "1182360"
  },
  {
    "text": "know like that can you can have llms that will run function calls like API call um that's kind of a tool",
    "start": "1182360",
    "end": "1189500"
  },
  {
    "text": "you also have chat chains which have memory like we talked about previously now agents kind of combined these two",
    "start": "1189500",
    "end": "1195140"
  },
  {
    "text": "ideas it's basically these these objects that have both memory and Tool use together",
    "start": "1195140",
    "end": "1200360"
  },
  {
    "text": "and we can we'll walk through exactly how this works with SQL now just to give you a quick overview",
    "start": "1200360",
    "end": "1207980"
  },
  {
    "text": "agents are a big topic language is a pretty big agent ecosystem",
    "start": "1207980",
    "end": "1213380"
  },
  {
    "text": "but let's kind of walk through the key pieces we talked about tools so there's lots of tools and toolkits",
    "start": "1213380",
    "end": "1219200"
  },
  {
    "text": "available in lines you can give to agents search access to email",
    "start": "1219200",
    "end": "1224720"
  },
  {
    "text": "access to data frames um various apis so that's like the tool",
    "start": "1224720",
    "end": "1230120"
  },
  {
    "text": "part of it then there's memory we talked about chat short-term memory",
    "start": "1230120",
    "end": "1236059"
  },
  {
    "text": "that's available also longer term memory is available like vector stores we also talked about that with retrieval",
    "start": "1236059",
    "end": "1243320"
  },
  {
    "text": "and then there's like the question of planning how do agents actually plan and there's a bunch of different flavors",
    "start": "1243320",
    "end": "1249440"
  },
  {
    "text": "so you've probably seen some of the more popular like autonomous agents like Auto GPT baby AGI these are like long kind of",
    "start": "1249440",
    "end": "1257660"
  },
  {
    "text": "long running agents that have different mechanisms for long-term planning of tasks",
    "start": "1257660",
    "end": "1262700"
  },
  {
    "text": "they're simulation agents uh degenerative agents paper is very interesting and there's also what we call Action",
    "start": "1262700",
    "end": "1269000"
  },
  {
    "text": "Agents which are ages that just decide kind of vary myopically the next action to take and react was kind of one of the more",
    "start": "1269000",
    "end": "1276020"
  },
  {
    "text": "famous Frameworks for action agents and became very popular and maybe I'll give you a little bit of",
    "start": "1276020",
    "end": "1281360"
  },
  {
    "text": "flavor on react so really all that's happening here is we talked about",
    "start": "1281360",
    "end": "1286940"
  },
  {
    "text": "standard prompting um now Chain of Thought was an extension",
    "start": "1286940",
    "end": "1292400"
  },
  {
    "text": "standard prompting which tells the llm to really show its work or think step by",
    "start": "1292400",
    "end": "1297919"
  },
  {
    "text": "step in this conditions that llm to work more slowly you do less computation per token it tends to produce better results",
    "start": "1297919",
    "end": "1304720"
  },
  {
    "text": "encourages the llm to kind of more carefully walk through a chain of logic",
    "start": "1304720",
    "end": "1310400"
  },
  {
    "text": "um there also is notion of tool use so things like say can this is some",
    "start": "1310400",
    "end": "1315919"
  },
  {
    "text": "robotics paper this is a robotics paper out of Google um kind of show how you can use tool use",
    "start": "1315919",
    "end": "1321740"
  },
  {
    "text": "to produce action observation pairs so tool you know llm as a tool acts on the",
    "start": "1321740",
    "end": "1327620"
  },
  {
    "text": "world gets observation back acts observation react was to say hey let's",
    "start": "1327620",
    "end": "1333320"
  },
  {
    "text": "combine these two we have an llm it has a tool it can form an action it gets an observation but",
    "start": "1333320",
    "end": "1340820"
  },
  {
    "text": "then it reflects on it it thinks and so it uses this kind of Chain of Thought or multi-step reasoning that's the main",
    "start": "1340820",
    "end": "1346640"
  },
  {
    "text": "idea behind react that's really all that's going on and let's",
    "start": "1346640",
    "end": "1351919"
  },
  {
    "start": "1349000",
    "end": "1457000"
  },
  {
    "text": "um let's actually see how this works in practice here's a SQL agent now you can see the trace here this is",
    "start": "1351919",
    "end": "1358760"
  },
  {
    "text": "all the steps is taking which is helpful I mean I think agents have been a very confusing area because it's hard to actually teach what's under the hood",
    "start": "1358760",
    "end": "1365900"
  },
  {
    "text": "but this really shows you what's going on so we're kind of Midway through the",
    "start": "1365900",
    "end": "1371419"
  },
  {
    "text": "trace here and basically um the input is list the total sales per",
    "start": "1371419",
    "end": "1378440"
  },
  {
    "text": "country which countries which countries customers spent the most and",
    "start": "1378440",
    "end": "1384440"
  },
  {
    "text": "the action was you can see here list the tables",
    "start": "1384440",
    "end": "1390200"
  },
  {
    "text": "there it is there's the observation so again tool use action observation here's all the tables that",
    "start": "1390200",
    "end": "1397280"
  },
  {
    "text": "use this tool here got the tables now I'm thinking it's using some chain a Chain of Thought",
    "start": "1397280",
    "end": "1403280"
  },
  {
    "text": "reasoning and here's the generation from that thinking",
    "start": "1403280",
    "end": "1409159"
  },
  {
    "text": "step I should query the schema from the invoice and customer tables and this is the action it takes you can",
    "start": "1409159",
    "end": "1415820"
  },
  {
    "text": "see as you go down the flow it then applies the tool to take this action if that makes sense",
    "start": "1415820",
    "end": "1421640"
  },
  {
    "text": "so again it's prior action get the observation for the action get",
    "start": "1421640",
    "end": "1427280"
  },
  {
    "text": "the tables think about it okay I have these table names now I want to",
    "start": "1427280",
    "end": "1433760"
  },
  {
    "text": "um get the schema for these tables and then I'll apply that tool and it'll just go down the",
    "start": "1433760",
    "end": "1439940"
  },
  {
    "text": "chain like that and this is how it comes up the answer this exactly that's all that's going on it's not nothing crazy",
    "start": "1439940",
    "end": "1446720"
  },
  {
    "text": "but I think agents have been a very confusing topic so anyway we I want to get to q a so I won't take this too much",
    "start": "1446720",
    "end": "1451820"
  },
  {
    "text": "longer but this gives you like a deep deep kind of insight hopefully into kind of how react agents actually work",
    "start": "1451820",
    "end": "1457880"
  },
  {
    "start": "1457000",
    "end": "1533000"
  },
  {
    "text": "um one thing I'll note agents are the most reliable and people probably seen this from their own for",
    "start": "1457880",
    "end": "1462919"
  },
  {
    "text": "their own work we recently been working on web retrieval we started as an agent we",
    "start": "1462919",
    "end": "1468500"
  },
  {
    "text": "actually found a retriever was better for this particular task so I think this is a caveat that you don't always need",
    "start": "1468500",
    "end": "1473960"
  },
  {
    "text": "to use an agent for this particular task basically we want a system to automate web research",
    "start": "1473960",
    "end": "1480799"
  },
  {
    "text": "we thought about an agent so you imagine you can ask a question agent can go do a web query get the result look at the",
    "start": "1480799",
    "end": "1487039"
  },
  {
    "text": "result make a decision query again instead all we do is take a question",
    "start": "1487039",
    "end": "1493580"
  },
  {
    "text": "use an llm to break it up into a number of stuff questions or related questions query them all in parallel bunch of HTML",
    "start": "1493580",
    "end": "1500419"
  },
  {
    "text": "pages load them all stored in vectors for do similarity search basically works",
    "start": "1500419",
    "end": "1507740"
  },
  {
    "text": "really well don't mean so anyway that's just a thing to highlight",
    "start": "1507740",
    "end": "1513260"
  },
  {
    "text": "um and yeah this is this is it's all open source repo is here",
    "start": "1513260",
    "end": "1519140"
  },
  {
    "text": "this is kind of showing it in action it's a fun little tool but again start with an agent eventually didn't end up",
    "start": "1519140",
    "end": "1525919"
  },
  {
    "text": "using one so it's it's good to be realistic about what is what ages can and can't do particularly effectively",
    "start": "1525919",
    "end": "1532640"
  },
  {
    "text": "um and the final thing I'll note I know we want to get to questions so I don't want",
    "start": "1532640",
    "end": "1537980"
  },
  {
    "start": "1533000",
    "end": "1661000"
  },
  {
    "text": "to take too much of your time on just me blabbering on but basically",
    "start": "1537980",
    "end": "1543140"
  },
  {
    "text": "um I highlighted this a few times but langsmith is quite useful as a tooling debugging and observability",
    "start": "1543140",
    "end": "1550820"
  },
  {
    "text": "kind of framework so here's an example of how you can use it in practice",
    "start": "1550820",
    "end": "1556940"
  },
  {
    "text": "you have an app you're collecting feedback and you have the way to collect bad feedback like",
    "start": "1556940",
    "end": "1562640"
  },
  {
    "text": "when the model is doing wrong things right imagine some way of doing that you can collect all those in line Smith",
    "start": "1562640",
    "end": "1569840"
  },
  {
    "text": "pretty easily you can filter on kind of good versus bad if everything is bad you can inspect it like what's going on",
    "start": "1569840",
    "end": "1575600"
  },
  {
    "text": "actually look at the trades but even more interesting you can look at the input and output of",
    "start": "1575600",
    "end": "1583100"
  },
  {
    "text": "that chain and you can actually correct it so you can say here's the output that the model",
    "start": "1583100",
    "end": "1588140"
  },
  {
    "text": "gave and it's it's wrong in some particular way and I can fix it so you can actually",
    "start": "1588140",
    "end": "1593299"
  },
  {
    "text": "edit it within langsmith produce the correct kind of create your own ground truth say that to a data set",
    "start": "1593299",
    "end": "1600320"
  },
  {
    "text": "okay and so kind of the overall flow then is that you can be collecting app",
    "start": "1600320",
    "end": "1606260"
  },
  {
    "text": "feedback collect that feedback put in an email set",
    "start": "1606260",
    "end": "1611600"
  },
  {
    "text": "fix your app and then you have this email set in langsmith we have a bunch of evaluators already in langsmith once you fixed your",
    "start": "1611600",
    "end": "1618440"
  },
  {
    "text": "app you can rerun on that Cindy doll set you've already collected and grade yourself again that's it",
    "start": "1618440",
    "end": "1625460"
  },
  {
    "text": "very simple kind of very obvious eval flow but the point is tools like this for llms have been a little bit lacking",
    "start": "1625460",
    "end": "1631640"
  },
  {
    "text": "obviously there's a lot of new tools coming out now but language is one of them and it integrates very nicely with Lang chain of course",
    "start": "1631640",
    "end": "1637580"
  },
  {
    "text": "it's a very nice place to have look at your generations and spec like we showed throughout this whole talk and also to",
    "start": "1637580",
    "end": "1642679"
  },
  {
    "text": "produce evaluation sets um and reevaluate your models so that's it and again this would show you know",
    "start": "1642679",
    "end": "1649340"
  },
  {
    "text": "you fix your app re-grade it redeploy collect more feedback new eval set fix",
    "start": "1649340",
    "end": "1654860"
  },
  {
    "text": "reevaluate and so forth go around the circle hopefully your app gets better over time",
    "start": "1654860",
    "end": "1660820"
  },
  {
    "start": "1661000",
    "end": "1791000"
  },
  {
    "text": "um I think that's all I have I think we have plenty of time for questions and probably better just to do it more",
    "start": "1661340",
    "end": "1666500"
  },
  {
    "text": "interactive anyway so um yeah I mean I'm happy to go through anything else but uh I'm also happy to take it to questions",
    "start": "1666500",
    "end": "1672559"
  },
  {
    "text": "and thanks for your time and very nice to talk to you all you guys thank you for that uh Lance uh maybe you",
    "start": "1672559",
    "end": "1678500"
  },
  {
    "text": "can stop sharing and we can start taking all the questions uh which people are interested in there uh q a I think uh",
    "start": "1678500",
    "end": "1686720"
  },
  {
    "text": "langsmith looks very powerful obviously the most popular question is going to be you know how do we get access to",
    "start": "1686720",
    "end": "1691760"
  },
  {
    "text": "langsmith since it's in close beta right now so um yeah I can address that yeah so actually",
    "start": "1691760",
    "end": "1699500"
  },
  {
    "text": "um everyone I believe as of now has been removed from the waitlist so actually if you sign up you should get access",
    "start": "1699500",
    "end": "1708260"
  },
  {
    "text": "um and we have some I can also can share some access codes if there are further problems but uh from talking to",
    "start": "1708260",
    "end": "1716960"
  },
  {
    "text": "folks on our side I believe everyone has been removed from the waitlist you should be able to get on immediately now",
    "start": "1716960",
    "end": "1723820"
  },
  {
    "text": "so if anybody has a problem you know we might bug you yeah yeah no problem",
    "start": "1724059",
    "end": "1729260"
  },
  {
    "text": "attendees on this on this uh webinar hopefully we'll get everybody access to the expensive wants so we I'll just go",
    "start": "1729260",
    "end": "1736039"
  },
  {
    "text": "in serial order as to a bunch of questions which have been posted here so uh will Anki is asking",
    "start": "1736039",
    "end": "1742340"
  },
  {
    "text": "um and you can look and look them up on your end on the Q a side as well yes yeah yeah so the first one is are there",
    "start": "1742340",
    "end": "1748940"
  },
  {
    "text": "uh uh there was a way of leveraging Langston agents or linking multiple llms",
    "start": "1748940",
    "end": "1754820"
  },
  {
    "text": "plus trigger automated workflows uh to do forecasting clustering or network",
    "start": "1754820",
    "end": "1759919"
  },
  {
    "text": "optimization is there a way I think the question is there a way for Lang changes to link",
    "start": "1759919",
    "end": "1765020"
  },
  {
    "text": "multiple level apps yep yes it should be possible to do that",
    "start": "1765020",
    "end": "1771200"
  },
  {
    "text": "um I will share some I'll find a way to share this with the community",
    "start": "1771200",
    "end": "1776720"
  },
  {
    "text": "um I will share some agent demos that utilize multiple LMS I don't think we",
    "start": "1776720",
    "end": "1781880"
  },
  {
    "text": "have any cookbooks out of the box but actually I've been working on refactoring all of our docs right now",
    "start": "1781880",
    "end": "1787039"
  },
  {
    "text": "so I will include some examples of that um we can go to the agent piece",
    "start": "1787039",
    "end": "1792140"
  },
  {
    "start": "1791000",
    "end": "1943000"
  },
  {
    "text": "typically with agents um",
    "start": "1792140",
    "end": "1797539"
  },
  {
    "text": "so maybe just to give a bit of context agents can be used with",
    "start": "1797539",
    "end": "1804500"
  },
  {
    "text": "many different elements within Lang chain so that's kind of the first point which is good",
    "start": "1804500",
    "end": "1809600"
  },
  {
    "text": "one thing we've observed is that llms that have poor coding capacity this goes",
    "start": "1809600",
    "end": "1814940"
  },
  {
    "text": "back to the Llama stuff tend to do worse as agents and it may be because agents require but kind of very structured",
    "start": "1814940",
    "end": "1820340"
  },
  {
    "text": "reasoning it's a little bit unclear but for example we've seen llama 2 agents perform worse than gbd35 or",
    "start": "1820340",
    "end": "1828020"
  },
  {
    "text": "um or four agents so that's something that's kind of a caveat but I think the point is that they were curious about",
    "start": "1828020",
    "end": "1835640"
  },
  {
    "text": "um whether or not you can use agents with multiple llms and you can do that and whether or not you can actually chain agents together you can also do",
    "start": "1835640",
    "end": "1842659"
  },
  {
    "text": "that so for example look at baby AGI there's a language and implementation of",
    "start": "1842659",
    "end": "1848840"
  },
  {
    "text": "that and it has a number of different agents which interact with one another and each of those agents could be",
    "start": "1848840",
    "end": "1855080"
  },
  {
    "text": "instantiated with a different element so I think that that hopefully answers the question you can certainly use",
    "start": "1855080",
    "end": "1860720"
  },
  {
    "text": "different llms with blank chain engines and you can certainly interact to kind of connect them together look at maybe",
    "start": "1860720",
    "end": "1866120"
  },
  {
    "text": "AGI as a good example of how to do that but he lands interestingly like as an investor one of the questions I have is",
    "start": "1866120",
    "end": "1872539"
  },
  {
    "text": "uh are agents ready for full Prime Time use like to be put in front of customers",
    "start": "1872539",
    "end": "1878779"
  },
  {
    "text": "or not like yeah it's a good point to be honest in many cases no",
    "start": "1878779",
    "end": "1886460"
  },
  {
    "text": "um there are some very interesting companies that are pushing state-of-art agents multi-on being one",
    "start": "1886460",
    "end": "1892340"
  },
  {
    "text": "div guards company so there obviously are I think kind of",
    "start": "1892340",
    "end": "1897919"
  },
  {
    "text": "there's a small set of companies at the state of the art agents that are probably quite close to production but",
    "start": "1897919",
    "end": "1903140"
  },
  {
    "text": "for example the agents you'll see in open source libraries including language Lane chain it's not entirely clear that",
    "start": "1903140",
    "end": "1908659"
  },
  {
    "text": "anyone is usually used in production right now there's still some experimental things like Auto GPD baby",
    "start": "1908659",
    "end": "1914000"
  },
  {
    "text": "AGI certainly experimental all promising but I would look at the small set of",
    "start": "1914000",
    "end": "1920659"
  },
  {
    "text": "companies that have really hard production agents like multi-on my senses they are reasonably closed for",
    "start": "1920659",
    "end": "1926179"
  },
  {
    "text": "narrow use cases thanks thanks for that uh next question",
    "start": "1926179",
    "end": "1931640"
  },
  {
    "text": "is from Ashwin uh EOS is there any way to increase the uh context length of",
    "start": "1931640",
    "end": "1937700"
  },
  {
    "text": "input tokens above maximum without using that's a good point so that's typically",
    "start": "1937700",
    "end": "1944179"
  },
  {
    "text": "a limitation of the llm so we kind of talk through different llms different context lines so of",
    "start": "1944179",
    "end": "1950000"
  },
  {
    "text": "course you can opt for different models like anthropic gives you very long Concepts like 100k tokens is like 75",
    "start": "1950000",
    "end": "1955520"
  },
  {
    "text": "pages of text that's a lot um now if you still hit these limits then I",
    "start": "1955520",
    "end": "1964340"
  },
  {
    "text": "would suggest doing things that we talk about in summarization like the mapreduce chain so again think about",
    "start": "1964340",
    "end": "1970399"
  },
  {
    "text": "what's happening here you're taking some massive set of documents you break them up into chumps",
    "start": "1970399",
    "end": "1975740"
  },
  {
    "text": "each chunk you send that to um do something on that chunk that's the map step summarize it or something else",
    "start": "1975740",
    "end": "1983360"
  },
  {
    "text": "and then you have all these mapped summaries you then further reduce them into a final summary so this is a way of",
    "start": "1983360",
    "end": "1990380"
  },
  {
    "text": "getting around context limitation models you can do this on very very arbitrarily large sets of documents you know",
    "start": "1990380",
    "end": "1996559"
  },
  {
    "text": "depending on your budget um but that's the point if you really hit a context window limitation you can",
    "start": "1996559",
    "end": "2002860"
  },
  {
    "text": "always do this kind of mapreduce style flow where you kind of break documents up and process them one by one or to be",
    "start": "2002860",
    "end": "2009519"
  },
  {
    "text": "honest smarter is probably something like clustering embed your documents",
    "start": "2009519",
    "end": "2014580"
  },
  {
    "text": "cluster them and only pass kind of unique sampled subsets to the LM for",
    "start": "2014580",
    "end": "2021220"
  },
  {
    "text": "whatever you want to do otherwise you're wasting tokens on things that are very very similar",
    "start": "2021220",
    "end": "2026740"
  },
  {
    "text": "so that's what I would say about that actually the next question is connected to Tech Splitters and the question is uh",
    "start": "2026740",
    "end": "2033700"
  },
  {
    "text": "text spirit is a very inefficient and they have a lot of overlap window so",
    "start": "2033700",
    "end": "2041980"
  },
  {
    "text": "because of the splitting right so how do you I think it's the same question as context Windows",
    "start": "2042539",
    "end": "2049480"
  },
  {
    "text": "um oh I see right yeah exactly Okay so okay it is true that text splitting is",
    "start": "2049480",
    "end": "2055960"
  },
  {
    "text": "kind of frustrating it would be nice to be able to skip that entire stage and just be able to pass context spread in",
    "start": "2055960",
    "end": "2061720"
  },
  {
    "text": "llms I think my answer there would be well certainly there are larger context llms coming out",
    "start": "2061720",
    "end": "2068020"
  },
  {
    "text": "100K tokens for Claude 2 is pretty good it's 75 Pages again 32k for uh I believe",
    "start": "2068020",
    "end": "2074919"
  },
  {
    "text": "there's uh gbd432k definitely 3532k again pretty good so that's happening on",
    "start": "2074919",
    "end": "2081220"
  },
  {
    "text": "the Alum side one limitation is open source models are still quite small contacts so that's a real limitation for",
    "start": "2081220",
    "end": "2087580"
  },
  {
    "text": "that text splitting embedding storage is probably the is going to remain uh you",
    "start": "2087580",
    "end": "2093638"
  },
  {
    "text": "know really the you know the preferred way of Duty and by the way even if you have a 100K context model if you're",
    "start": "2093639",
    "end": "2100780"
  },
  {
    "text": "working with Enterprise data which is massive you still can never fit all of it into context so you need some kind of",
    "start": "2100780",
    "end": "2106000"
  },
  {
    "text": "strategy to split and store embedded chunks for retrieval in a lot of practical and Enterprise settings",
    "start": "2106000",
    "end": "2113380"
  },
  {
    "text": "so I don't really see any way to get around that to a degree although contact windows are definitely getting bigger",
    "start": "2113380",
    "end": "2118900"
  },
  {
    "text": "yeah yeah next question is",
    "start": "2118900",
    "end": "2124500"
  },
  {
    "text": "does conversation buffer memory using chat application use case cases uses",
    "start": "2124500",
    "end": "2130300"
  },
  {
    "text": "contextual compression yeah that's a good one so actually uh yeah the use",
    "start": "2130300",
    "end": "2135460"
  },
  {
    "start": "2133000",
    "end": "2288000"
  },
  {
    "text": "case stock covers this it can there's an option to specify that where is it so",
    "start": "2135460",
    "end": "2140500"
  },
  {
    "text": "this use case documentation you I'll share the slides of course you can see but we do use contextual compression",
    "start": "2140500",
    "end": "2146260"
  },
  {
    "text": "some of the chains um I can find the chain name here if you can uh well I don't need to do this",
    "start": "2146260",
    "end": "2152740"
  },
  {
    "text": "right now but basically um outside right now check out um",
    "start": "2152740",
    "end": "2159040"
  },
  {
    "text": "conversation summary memory will keep a summary of the conversation to save",
    "start": "2159040",
    "end": "2166000"
  },
  {
    "text": "tokens um that is a very useful trick and I'd recommend it",
    "start": "2166000",
    "end": "2173680"
  },
  {
    "text": "yeah um",
    "start": "2173680",
    "end": "2177780"
  },
  {
    "text": "using line chain summarizing and storing the responses do we lose the context okay this is a good point there's a",
    "start": "2179920",
    "end": "2187000"
  },
  {
    "text": "trade-off always so if you use a summarization use summarization memory",
    "start": "2187000",
    "end": "2192160"
  },
  {
    "text": "or contextual compression you will lose some granular details of the conversation",
    "start": "2192160",
    "end": "2198220"
  },
  {
    "text": "it's a trade-off so if you have a very very long conversation you don't have the tokens back to the part question",
    "start": "2198220",
    "end": "2204400"
  },
  {
    "text": "about context length if you don't have the context window to support a very long conversation you need to do some",
    "start": "2204400",
    "end": "2210760"
  },
  {
    "text": "kind of compression but you will lose detail so it's kind of no free lunch one option",
    "start": "2210760",
    "end": "2216640"
  },
  {
    "text": "could be keep the whole conversation history and use a very large context model like Claude to 100K tokens",
    "start": "2216640",
    "end": "2223839"
  },
  {
    "text": "or um or you are just there's a few other",
    "start": "2223839",
    "end": "2230440"
  },
  {
    "text": "options there's also options to keep the last K interactions in the conversation",
    "start": "2230440",
    "end": "2236619"
  },
  {
    "text": "we also show that in the docs so that's like if you just want to keep a buffer of the last like 10 interactions that's",
    "start": "2236619",
    "end": "2242440"
  },
  {
    "text": "the way you have all the detail but um you of course will still save tokens",
    "start": "2242440",
    "end": "2249520"
  },
  {
    "text": "because you're not passing the entire history that's another option",
    "start": "2249520",
    "end": "2254520"
  },
  {
    "text": "absolutely um yeah let's go to the I want to pick",
    "start": "2254740",
    "end": "2260079"
  },
  {
    "text": "up one of the more interesting ones uh maybe the SQL uh there's a question on SQL and LM models yeah sure",
    "start": "2260079",
    "end": "2267040"
  },
  {
    "text": "so um the question is uh",
    "start": "2267040",
    "end": "2273640"
  },
  {
    "text": "text to SQL and llms my own experience is that llms cannot convert questions to SQL when the questions get from simple",
    "start": "2273640",
    "end": "2280780"
  },
  {
    "text": "to intermediate so you know more complicated SQL how does that work uh so",
    "start": "2280780",
    "end": "2287980"
  },
  {
    "text": "right well okay this is a this is a good point one I think you do hit up against just",
    "start": "2287980",
    "end": "2294640"
  },
  {
    "start": "2288000",
    "end": "2463000"
  },
  {
    "text": "the limitation of the Ln at some point there's no question about that um I would obviously consider using",
    "start": "2294640",
    "end": "2301119"
  },
  {
    "text": "state-of-the-art model like dvd4 for example to get the best quality text to SQL conversion or give you the best",
    "start": "2301119",
    "end": "2307900"
  },
  {
    "text": "chance for complex queries um but it is true that uh there's gonna",
    "start": "2307900",
    "end": "2315160"
  },
  {
    "text": "be limitations one workaround to this is consider the SQL agent",
    "start": "2315160",
    "end": "2320680"
  },
  {
    "text": "now agents open up another can of worms I agree so I'm not saying agents are kind of the the",
    "start": "2320680",
    "end": "2328119"
  },
  {
    "text": "um the the default go to in the situation like this but agents have some",
    "start": "2328119",
    "end": "2333220"
  },
  {
    "text": "benefits they can do they can do um kind of restart so they can try",
    "start": "2333220",
    "end": "2339160"
  },
  {
    "text": "multiple times they can reformulate queries they can sometimes attempt to",
    "start": "2339160",
    "end": "2344619"
  },
  {
    "text": "break down the question into multiple parts and queries component also check the use case documentation",
    "start": "2344619",
    "end": "2350980"
  },
  {
    "text": "there are some tricks you can use for basically trying to to check the queries for for",
    "start": "2350980",
    "end": "2356260"
  },
  {
    "text": "um for uh accuracy or for",
    "start": "2356260",
    "end": "2361780"
  },
  {
    "text": "um yeah for compatibility so basically I think there's really no great solution",
    "start": "2361780",
    "end": "2367720"
  },
  {
    "text": "to this if you have very complex questions it is definitely possible to hit the limitations of the lm's capacity",
    "start": "2367720",
    "end": "2372880"
  },
  {
    "text": "in those cases maybe try an agent that can kind of retry it multiple times",
    "start": "2372880",
    "end": "2378040"
  },
  {
    "text": "maybe just break your question down into simpler pieces um frankly I'd be curious to see some of",
    "start": "2378040",
    "end": "2385480"
  },
  {
    "text": "those examples um so maybe there's a way you can get in touch with me am I see my Twitter is is",
    "start": "2385480",
    "end": "2390820"
  },
  {
    "text": "linked so um I would actually be curious to see that awesome",
    "start": "2390820",
    "end": "2397180"
  },
  {
    "text": "um one more simpler question has like chance is election supporting tree of thought uh yeah that's a good point",
    "start": "2397180",
    "end": "2406359"
  },
  {
    "text": "it is supported but I don't cover it in the documentation but I should",
    "start": "2406359",
    "end": "2412000"
  },
  {
    "text": "um uh I will make a note to add that actually because I'm overhauling all our docs right now",
    "start": "2412000",
    "end": "2417760"
  },
  {
    "text": "uh I do think that'd be very interesting to include um so the answer is yes but",
    "start": "2417760",
    "end": "2426220"
  },
  {
    "text": "I should add it to our use case documentation I'll think about the right place to do that I'll make a note of that right now",
    "start": "2426220",
    "end": "2433200"
  },
  {
    "text": "yeah that's a good point um okay uh",
    "start": "2433240",
    "end": "2441420"
  },
  {
    "text": "yeah I'm just trying to choose a good question",
    "start": "2442480",
    "end": "2446280"
  },
  {
    "text": "uh yeah um is there some use case of like chain where we can mask confidential data",
    "start": "2448320",
    "end": "2454480"
  },
  {
    "text": "while trading in LM so that other well I think this is more of a LM question rather than Lang chain",
    "start": "2454480",
    "end": "2460660"
  },
  {
    "text": "questions this is actually a good point though",
    "start": "2460660",
    "end": "2466300"
  },
  {
    "start": "2463000",
    "end": "2632000"
  },
  {
    "text": "I will share um okay I'll share an interesting blog post",
    "start": "2466300",
    "end": "2472480"
  },
  {
    "text": "from a company called glean that does a lot of work on Enterprise search",
    "start": "2472480",
    "end": "2477640"
  },
  {
    "text": "they talk a lot about uh where is it I'm finding so this retrieval augmented",
    "start": "2477640",
    "end": "2484180"
  },
  {
    "text": "generation flow glean for Enterprise church has this problem it's like hey I have all these",
    "start": "2484180",
    "end": "2491800"
  },
  {
    "text": "users they're searching documents and how do I ensure that you know security",
    "start": "2491800",
    "end": "2497980"
  },
  {
    "text": "permissions are are honored and that's more of a retrieval architecture question nlm question",
    "start": "2497980",
    "end": "2504339"
  },
  {
    "text": "and actually this blog post from glean I'll insert it here so that when I share",
    "start": "2504339",
    "end": "2510880"
  },
  {
    "text": "the slides you'll see it is very interesting because it talks a lot this is really what Enterprise Church Solutions will focus on",
    "start": "2510880",
    "end": "2519760"
  },
  {
    "text": "um so it's a little bit more of like a retrieval architecture question than an LM question it'd be something like in",
    "start": "2519760",
    "end": "2524920"
  },
  {
    "text": "this retrieval system it has some awareness of the user it knows user permissions and like it will retrieve",
    "start": "2524920",
    "end": "2530680"
  },
  {
    "text": "documents per those permissions it's more of that kind of thing yep awesome",
    "start": "2530680",
    "end": "2537520"
  },
  {
    "text": "um uh one question on agents so how do callbacks get applied in custom change or agents which we construct for our own",
    "start": "2537520",
    "end": "2545200"
  },
  {
    "text": "use case does it take into account all completion and prompt tokens that goes in the process",
    "start": "2545200",
    "end": "2550300"
  },
  {
    "text": "yeah it should and I will when I share Asian documentation update",
    "start": "2550300",
    "end": "2556060"
  },
  {
    "text": "later this week I will include that a specific example of how to do that um you note that I don't share the agent",
    "start": "2556060",
    "end": "2562420"
  },
  {
    "text": "docs because they're not quite ready yet but I'll make a note which where's that question I'm going to make sure this is",
    "start": "2562420",
    "end": "2568599"
  },
  {
    "text": "the first question top one yeah I see it I'm gonna add that right here",
    "start": "2568599",
    "end": "2574260"
  },
  {
    "text": "yes thank you for doing this yeah this is good well I'm updating all our docs",
    "start": "2577140",
    "end": "2582220"
  },
  {
    "text": "now anyway so actually it's good timing all right uh okay I'll just see if you",
    "start": "2582220",
    "end": "2589300"
  },
  {
    "text": "answered this live uh yeah the next one um I think people are throwing in a lot",
    "start": "2589300",
    "end": "2595839"
  },
  {
    "text": "of questions it's great I think which I don't want this to become a very support uh kind of webinar uh yeah well it's",
    "start": "2595839",
    "end": "2603400"
  },
  {
    "text": "actually helpful to get all the questions because we're up you know we're up to any other docs so it's good",
    "start": "2603400",
    "end": "2610920"
  },
  {
    "text": "um I think this question is uh is a more simplistic question uh there's",
    "start": "2611440",
    "end": "2617859"
  },
  {
    "text": "somebody's creating a conversational agent for business stakeholders and they need multiple databases llms I think",
    "start": "2617859",
    "end": "2624760"
  },
  {
    "text": "Langston already does this so they're trying to use this for any",
    "start": "2624760",
    "end": "2630400"
  },
  {
    "text": "resolution to be honest I would say that",
    "start": "2630400",
    "end": "2635500"
  },
  {
    "start": "2632000",
    "end": "2765000"
  },
  {
    "text": "taking a query decomposing it into subparts querying",
    "start": "2635500",
    "end": "2640839"
  },
  {
    "text": "different databases depending on different parts is actually non-trivial well it's it's obviously non-trivial",
    "start": "2640839",
    "end": "2647859"
  },
  {
    "text": "um so we're actually working on some good documentation now on on query decomposition",
    "start": "2647859",
    "end": "2655119"
  },
  {
    "text": "um I still think to be honest with you that in a business setting an Empire",
    "start": "2655119",
    "end": "2660460"
  },
  {
    "text": "setting realistically you will probably want a real Enterprise solution something like lean that has",
    "start": "2660460",
    "end": "2666280"
  },
  {
    "text": "been doing this for a very long period of time um it is non-trivial and I will we will definitely be sharing",
    "start": "2666280",
    "end": "2673540"
  },
  {
    "text": "some documentation on query decomposition um very soon I'm going to make a note of",
    "start": "2673540",
    "end": "2680440"
  },
  {
    "text": "that um but in an Enterprise setting I actually think you probably want a",
    "start": "2680440",
    "end": "2686920"
  },
  {
    "text": "very hard Enterprise product for retrieval glean being one for example",
    "start": "2686920",
    "end": "2693700"
  },
  {
    "text": "um if you have a use case where you have to very very reliably interact across a large number of different data Stores",
    "start": "2693700",
    "end": "2701140"
  },
  {
    "text": "um and combine that information that is like a hard Enterprise retrieval problem",
    "start": "2701140",
    "end": "2708180"
  },
  {
    "text": "yeah so we got that one yeah next one is interesting uh this is",
    "start": "2710440",
    "end": "2717099"
  },
  {
    "text": "like streaming data support uh in Black chain yeah there's lots of support for streaming",
    "start": "2717099",
    "end": "2724300"
  },
  {
    "text": "um we show that in a lot of places um so",
    "start": "2724300",
    "end": "2729880"
  },
  {
    "text": "um yeah like so for example in this little web app we showed this is obviously streaming have a look at the",
    "start": "2729880",
    "end": "2736000"
  },
  {
    "text": "code here it shows some nice demos as to how to set up streaming you can set many different ways by providing an llm",
    "start": "2736000",
    "end": "2741280"
  },
  {
    "text": "callback that works with many different alarms I think this is more around uh it seems",
    "start": "2741280",
    "end": "2747700"
  },
  {
    "text": "like this question is for e-commerce data where there's a lot of live streaming transaction data coming in and",
    "start": "2747700",
    "end": "2754839"
  },
  {
    "text": "uh is there any Vector database that you might know of which supports high volume",
    "start": "2754839",
    "end": "2761560"
  },
  {
    "text": "input data okay that's interesting yeah so for that I'm going to make a note of",
    "start": "2761560",
    "end": "2767800"
  },
  {
    "text": "that as well you should check out our Integrations page there's 40 Vector stores and there's certainly at least a",
    "start": "2767800",
    "end": "2775900"
  },
  {
    "text": "few that can handle you know streaming data without a question interesting I don't know yeah this",
    "start": "2775900",
    "end": "2783040"
  },
  {
    "text": "brings up an interesting point though uh data freshness and retrieval",
    "start": "2783040",
    "end": "2788619"
  },
  {
    "text": "applications is very tricky so for example if you have a vector DB how often are you re-annexing all your",
    "start": "2788619",
    "end": "2794380"
  },
  {
    "text": "data and like updating that database and so actually I'm going to make a note of this this is good motivation for",
    "start": "2794380",
    "end": "2800500"
  },
  {
    "text": "something that we are putting out somewhat soon um so I'm going to make a kind of a",
    "start": "2800500",
    "end": "2805660"
  },
  {
    "text": "question there's a few different things here so one is we have a blog post yesterday with airbite which is is one",
    "start": "2805660",
    "end": "2813460"
  },
  {
    "text": "interesting option for data connection which has a lot of this kind of like data per kind of data versioning like",
    "start": "2813460",
    "end": "2820660"
  },
  {
    "text": "what's in your vector DB and what's in your like kind of your say you have a buffer that's capturing your stream keeping those in sync is like is a",
    "start": "2820660",
    "end": "2827560"
  },
  {
    "text": "tricky thing so Airbus one option for that we also have uh some new tooling on our end",
    "start": "2827560",
    "end": "2833859"
  },
  {
    "text": "coming out to support that um so I would check out tools like everybody that NL ETL and",
    "start": "2833859",
    "end": "2840099"
  },
  {
    "text": "what support is there for streaming new Lang chain tool coming out soon any existing Vector DBS I mean streaming is",
    "start": "2840099",
    "end": "2846640"
  },
  {
    "text": "always going to be triggered with Vector DBS right you're going to buffer them embed those chunks and then and load them up",
    "start": "2846640",
    "end": "2851859"
  },
  {
    "text": "so yeah I think realistically you're gonna have some latency you're gonna have some",
    "start": "2851859",
    "end": "2856960"
  },
  {
    "text": "kind of like ETL schedule that you need to maintain um with streaming data it's a great application though I I'm actually uh",
    "start": "2856960",
    "end": "2863560"
  },
  {
    "text": "I'll dig into this a little bit more as well on my end but I would first start with like looking at things like everybody integration which came out",
    "start": "2863560",
    "end": "2869260"
  },
  {
    "text": "yesterday look at Neutron we're putting out soon um and check out all existing Vector DB",
    "start": "2869260",
    "end": "2876579"
  },
  {
    "text": "Integrations to see if streaming supported with some of them yeah I think one generic question which",
    "start": "2876579",
    "end": "2883060"
  },
  {
    "text": "is very interesting um one is uh have you seen any very very interesting vertical use cases in the",
    "start": "2883060",
    "end": "2889060"
  },
  {
    "text": "industry where Lang chain has been applied um and along with it there's another question which is Lankan makes it so",
    "start": "2889060",
    "end": "2895660"
  },
  {
    "text": "easy that it's like leveling the playing field for most startups so how do you startups build any mode around uh what",
    "start": "2895660",
    "end": "2902079"
  },
  {
    "text": "is already like how do they build their mod uh yeah yeah that's a good point that's a good point so the first one on",
    "start": "2902079",
    "end": "2908079"
  },
  {
    "start": "2903000",
    "end": "3104000"
  },
  {
    "text": "applications actually kind of maps for use cases rag is a huge application there's a very good talk actually from",
    "start": "2908079",
    "end": "2915640"
  },
  {
    "text": "um from Frank slootman of snowflake and Jensen Wong of Nvidia I can share the",
    "start": "2915640",
    "end": "2921700"
  },
  {
    "text": "link uh it was at their latest Summit where Jensen Wong they're partnering and",
    "start": "2921700",
    "end": "2926980"
  },
  {
    "text": "the question was why and Jensen says every piece of data will become an application",
    "start": "2926980",
    "end": "2932500"
  },
  {
    "text": "you know you can build llms to kind of make every piece of data Interactive all your customer data all your",
    "start": "2932500",
    "end": "2938440"
  },
  {
    "text": "organizational data your documentation all becomes Interactive and so there's a large surface area for applications",
    "start": "2938440",
    "end": "2944140"
  },
  {
    "text": "there a lot of that's going to be enabled by something like rag style pipeline",
    "start": "2944140",
    "end": "2949420"
  },
  {
    "text": "um with for example what one of the prior questions mentioned",
    "start": "2949420",
    "end": "2955119"
  },
  {
    "text": "high quality retrieval tooling will certainly be very important there",
    "start": "2955119",
    "end": "2960520"
  },
  {
    "text": "um so Rags a major kind of Enterprise use case that we're certainly seeing quite a lot of",
    "start": "2960520",
    "end": "2966099"
  },
  {
    "text": "we've actually been focused a little bit more on some of the platform layer on observability and tooling which is kind",
    "start": "2966099",
    "end": "2971740"
  },
  {
    "text": "of central across all the different use cases and we're seeing a lot of interest in this",
    "start": "2971740",
    "end": "2976780"
  },
  {
    "text": "um to support things like rag summarizations another big one that we see a lot of chat is another big one so",
    "start": "2976780",
    "end": "2982240"
  },
  {
    "text": "I would say rag chat summarization are significant Enterprise applications",
    "start": "2982240",
    "end": "2987280"
  },
  {
    "text": "um obviously extraction is kind of a piece of the puzzle there um in a lot of rag applications SQL to",
    "start": "2987280",
    "end": "2994119"
  },
  {
    "text": "be honest I'm I mean I think it's certainly a very useful devtool I don't know if it's like a major Enterprise application at this point quite yet but",
    "start": "2994119",
    "end": "3001200"
  },
  {
    "text": "it may be in the future agents not yet agents are coming but not yet that's kind of my",
    "start": "3001200",
    "end": "3007740"
  },
  {
    "text": "sense in the landscape with respect to competition I think this has been true of ml for a long time it's always about",
    "start": "3007740",
    "end": "3013200"
  },
  {
    "text": "your data that was actually true in self-driving as well self-driving is a very tightly",
    "start": "3013200",
    "end": "3018599"
  },
  {
    "text": "guarded space of ml models for many years but frankly the models over time commoditized the data was",
    "start": "3018599",
    "end": "3023940"
  },
  {
    "text": "differentiating and that's true here too so if you're a company it's really your data that's",
    "start": "3023940",
    "end": "3029280"
  },
  {
    "text": "your differentiated Advantage it's not the it's not the models in fact what's cool about this whole industry is it's",
    "start": "3029280",
    "end": "3035339"
  },
  {
    "text": "it's flipping self-driving in other parts of ml we're always highly differentiated models",
    "start": "3035339",
    "end": "3042240"
  },
  {
    "text": "um this is kind of flipping that everyone has the best model it's all about what data you have",
    "start": "3042240",
    "end": "3047700"
  },
  {
    "text": "because everyone has the same model effectively but it's entirely different which is exciting and yeah it does level",
    "start": "3047700",
    "end": "3054119"
  },
  {
    "text": "the playing field but also opens up the surface area for applications usually yep",
    "start": "3054119",
    "end": "3061220"
  },
  {
    "text": "I want to pick up a lot uh the question which is in the right in the end which is more around hallucination so the",
    "start": "3061559",
    "end": "3067500"
  },
  {
    "text": "question is from Aditya he's talking about they're doing something with medical data patient data and creating",
    "start": "3067500",
    "end": "3072780"
  },
  {
    "text": "uh examinations for doctors and how do they make sure they're trying",
    "start": "3072780",
    "end": "3078660"
  },
  {
    "text": "to use Vector emitted drag but they want to make sure that there",
    "start": "3078660",
    "end": "3084059"
  },
  {
    "text": "are no hallucinations so should they fine-tune the model or is this some what is the better way what is a better",
    "start": "3084059",
    "end": "3089460"
  },
  {
    "text": "approach uh to improve accuracy and have you yeah this is interesting yeah",
    "start": "3089460",
    "end": "3096540"
  },
  {
    "text": "um I'm just making it I'm just reading the question again a little bit to be careful yeah",
    "start": "3096540",
    "end": "3103440"
  },
  {
    "text": "um Okay so let's go back to retrieval uh QA",
    "start": "3103440",
    "end": "3109020"
  },
  {
    "text": "chain so one of the common things to reduce hallucinations is the notion of",
    "start": "3109020",
    "end": "3115740"
  },
  {
    "text": "retrieval because really what you're telling the model to do is use only this context to",
    "start": "3115740",
    "end": "3123000"
  },
  {
    "text": "answer the question if you don't know the answer just say you don't know and you're giving it the",
    "start": "3123000",
    "end": "3128700"
  },
  {
    "text": "contract itself so if retrieval is one tool this is quite effective obviously with temperature zero this is already",
    "start": "3128700",
    "end": "3135540"
  },
  {
    "text": "pretty good now when you get into a safety critical setting like health I",
    "start": "3135540",
    "end": "3145079"
  },
  {
    "text": "think that's probably where you start to think about overlays there's a package from Shreya called guardrails AI which",
    "start": "3145079",
    "end": "3151680"
  },
  {
    "text": "is interesting I think you kind of get into the world of legitimate llm safety",
    "start": "3151680",
    "end": "3156839"
  },
  {
    "text": "and need for additional checks on top of LM like guardrails or",
    "start": "3156839",
    "end": "3162720"
  },
  {
    "text": "otherwise so that's probably what I would say there I think at the LM level things like retrieval low temperature",
    "start": "3162720",
    "end": "3168980"
  },
  {
    "text": "obviously help a lot with hallucinations but then on the output side you probably need additional things like guardrails",
    "start": "3168980",
    "end": "3177318"
  },
  {
    "text": "yeah thank you for that uh folks we are coming towards the end of the webinar",
    "start": "3178460",
    "end": "3184980"
  },
  {
    "text": "um and you know I'm just from the interest of time uh I just wanted to thank Lance for doing this I we probably",
    "start": "3184980",
    "end": "3191160"
  },
  {
    "text": "could not pick up all the questions uh uh and but I also want to plug in",
    "start": "3191160",
    "end": "3196859"
  },
  {
    "text": "about our own AI accelerator so um Axel in India has launched the AI",
    "start": "3196859",
    "end": "3203339"
  },
  {
    "text": "accelerator uh where we are investing in AI startups and you know would be great",
    "start": "3203339",
    "end": "3209099"
  },
  {
    "text": "if you folks here are interested to apply for it the applications are open we'll be starting off the batch on the",
    "start": "3209099",
    "end": "3215940"
  },
  {
    "text": "first week in the on 20th September uh that's the high level and a lot of you",
    "start": "3215940",
    "end": "3221760"
  },
  {
    "text": "have asked questions on whether the slides and the recording will be available or not we'll be making this uh",
    "start": "3221760",
    "end": "3227220"
  },
  {
    "text": "the recording available in the next few weeks on our website and YouTube channels and Lan says gracefully uh said",
    "start": "3227220",
    "end": "3234839"
  },
  {
    "text": "he will share his slides as well uh really really thankful for that",
    "start": "3234839",
    "end": "3240420"
  },
  {
    "text": "um and that's really really thankful for taking out your time I know you know Lang chain is must be very busy at lunch",
    "start": "3240420",
    "end": "3246359"
  },
  {
    "text": "and you're a small team with a very active project so uh you know thanks for",
    "start": "3246359",
    "end": "3251880"
  },
  {
    "text": "that uh and uh you know thanks for doing this for us so",
    "start": "3251880",
    "end": "3257460"
  },
  {
    "text": "yeah definitely there's a lot of fun great questions feel free to DM me on uh LinkedIn or on um Twitter to follow up",
    "start": "3257460",
    "end": "3267240"
  },
  {
    "text": "and um I will I made some notes here to address the questions that came up that",
    "start": "3267240",
    "end": "3272760"
  },
  {
    "text": "I we didn't have immediate documentation answers to so",
    "start": "3272760",
    "end": "3277859"
  },
  {
    "text": "all right thank you thank you thank you Lance thanks for doing this hopefully we'll see more often uh and",
    "start": "3277859",
    "end": "3285240"
  },
  {
    "text": "hopefully we'll get you to India as well one more time very nice of course thank you very much okay bye everyone",
    "start": "3285240",
    "end": "3292500"
  },
  {
    "text": "thank you",
    "start": "3292500",
    "end": "3295160"
  }
]