[
  {
    "text": "[Music]",
    "start": "0",
    "end": "6720"
  },
  {
    "text": "welcome to practical AI if you work in artificial intelligence aspire to or are",
    "start": "6720",
    "end": "13920"
  },
  {
    "text": "curious how AI related Tech is changing the world this is the show for you thank",
    "start": "13920",
    "end": "20519"
  },
  {
    "text": "you to our partners at fly.io the home of",
    "start": "20519",
    "end": "25759"
  },
  {
    "text": "changelog.md 30 plus regions on six continents so you can launch your app",
    "start": "28960",
    "end": "34440"
  },
  {
    "text": "near your users learn more at",
    "start": "34440",
    "end": "38718"
  },
  {
    "text": "fly.io welcome to another fully connected episode of the Practical AI",
    "start": "42280",
    "end": "48280"
  },
  {
    "text": "podcast in these fully connected episodes Chris and I keep you fully connected with everything that's",
    "start": "48280",
    "end": "54840"
  },
  {
    "text": "happening in the AI and machine learning World we'll take some time to dig into",
    "start": "54840",
    "end": "61399"
  },
  {
    "text": "the latest uh news articles and releases from the AI community and hopefully",
    "start": "61399",
    "end": "67960"
  },
  {
    "text": "share some learning resources that will help you level up your machine learning game my name is Daniel whack I am the",
    "start": "67960",
    "end": "75320"
  },
  {
    "text": "founder and CEO at prediction guard and I'm joined as always by my co-host Chris Benson who is a tech strategist at",
    "start": "75320",
    "end": "82320"
  },
  {
    "text": "locked Martin how you doing Chris doing great Daniel how's it going it's going great I'm spending a few weeks in the UK",
    "start": "82320",
    "end": "90079"
  },
  {
    "text": "um which is a lot of fun and uh have got enough sleep to not be jet lagged quite",
    "start": "90079",
    "end": "96759"
  },
  {
    "text": "as much so that's encouraging okay so we have a transatlantic podcast going here today",
    "start": "96759",
    "end": "103079"
  },
  {
    "text": "exactly worldwide that's right across the pond practical AI worldwide 21st",
    "start": "103079",
    "end": "109159"
  },
  {
    "text": "century Incorporated I don't know we need rebranding exactly yeah yeah well",
    "start": "109159",
    "end": "115040"
  },
  {
    "text": "Chris one of the things I was going through I don't know how often people are flying these days but one of the",
    "start": "115040",
    "end": "122680"
  },
  {
    "text": "things that stood out to me as I took my flight across the pond was now when you",
    "start": "122680",
    "end": "130119"
  },
  {
    "text": "board at least some flights you don't even give them your ticket right you just go up and there's a little I guess",
    "start": "130119",
    "end": "137280"
  },
  {
    "text": "you would call it a kiosk a little Edge device that takes a picture of your face",
    "start": "137280",
    "end": "142959"
  },
  {
    "text": "and matches it I assume with what was your scanned passport which you scanned",
    "start": "142959",
    "end": "148440"
  },
  {
    "text": "at the time of check-in and you board your plane of course and it was really really fast as well and the same thing",
    "start": "148440",
    "end": "155800"
  },
  {
    "text": "happened you know Crossing into the border into the UK as long as you have a",
    "start": "155800",
    "end": "162120"
  },
  {
    "text": "certain passport you just go up to the little machine and scan your passport and then it takes your picture and I'm",
    "start": "162120",
    "end": "168879"
  },
  {
    "text": "assuming I I could do a little bit of research I'm assuming what's happening under the hood is that it's matching",
    "start": "168879",
    "end": "174800"
  },
  {
    "text": "your actual facial features up with the image on your passport and Computing",
    "start": "174800",
    "end": "181239"
  },
  {
    "text": "some score of shadiness or something like that or risk associated with you",
    "start": "181239",
    "end": "188200"
  },
  {
    "text": "not being the person in the but I was amazed at how fast it was and I'm",
    "start": "188200",
    "end": "193599"
  },
  {
    "text": "assuming I could be wrong I'm assuming maybe some of that's running at the edge",
    "start": "193599",
    "end": "198760"
  },
  {
    "text": "not ryant on an internet connection to do that facial recognition I'm not sure if you if you know or if you've had also",
    "start": "198760",
    "end": "206319"
  },
  {
    "text": "this experience Chris I don't know what they're using algorithmically uh but I definitely partake of the technology",
    "start": "206319",
    "end": "212040"
  },
  {
    "text": "it's an area that I forgo privacy and and always buy my way into expeditious",
    "start": "212040",
    "end": "217640"
  },
  {
    "text": "processing so yes I'm curious well I don't know in that case if you have a choice maybe there is an opt out",
    "start": "217640",
    "end": "224200"
  },
  {
    "text": "situation or something I'm not sure but it's pretty cool that some of this technology is being applied at the edge",
    "start": "224200",
    "end": "232360"
  },
  {
    "text": "and in a very seemingly efficient way such that you could use it on a math",
    "start": "232360",
    "end": "238360"
  },
  {
    "text": "scale like that or I don't know if You' consider that a math scale but it's definitely in use for many you know",
    "start": "238360",
    "end": "244280"
  },
  {
    "text": "there's a huge flood of people going through those stalls and the computation",
    "start": "244280",
    "end": "249400"
  },
  {
    "text": "happens very quickly and reliably enough to make a judgment in the midst of all",
    "start": "249400",
    "end": "255519"
  },
  {
    "text": "the hype around generative AI one of the things that stood out to me over this",
    "start": "255519",
    "end": "261239"
  },
  {
    "text": "last news cycle Chris was the release of YOLO v9 so we're on the the ninth",
    "start": "261239",
    "end": "268840"
  },
  {
    "text": "iteration of this YOLO model did you happen to see any of the videos of YOLO",
    "start": "268840",
    "end": "275759"
  },
  {
    "text": "9 in action Chris I haven't seen the YOLO 9 one but I'm I'm kind of stunned you know when you think about it yolo",
    "start": "275759",
    "end": "281520"
  },
  {
    "text": "has been around a long time I was occurring to me because we actually had some conversations about YOLO back in",
    "start": "281520",
    "end": "287960"
  },
  {
    "text": "the very first days of this podcast which has been you know closing in on six years now so it's uh v9 is a long",
    "start": "287960",
    "end": "295080"
  },
  {
    "text": "time coming uh and we haven't really gone back and touched uh such models in quite a while we're long overdue yeah",
    "start": "295080",
    "end": "301759"
  },
  {
    "text": "yeah so as everyone is freaking out and enjoying the hype over large language",
    "start": "301759",
    "end": "308080"
  },
  {
    "text": "models and other generative types of models uh Sora and all the things coming",
    "start": "308080",
    "end": "313120"
  },
  {
    "text": "out in the background somewhere there's these amazing computer vision people that are just really cranking and",
    "start": "313120",
    "end": "320000"
  },
  {
    "text": "innovating on actually at the architecture level of neural networks um in really interesting ways so it might",
    "start": "320000",
    "end": "327080"
  },
  {
    "text": "be good to set a little bit of background for this Chris you mentioned um we've been kind of talking about YOLO",
    "start": "327080",
    "end": "334039"
  },
  {
    "text": "for some time so if people just search for yolo y l o object detection you'll",
    "start": "334039",
    "end": "340759"
  },
  {
    "text": "see a you know a huge set of Articles and GitHub and everything about YOLO",
    "start": "340759",
    "end": "347639"
  },
  {
    "text": "YOLO actually kind of made a splash because it processed entire images in a",
    "start": "347639",
    "end": "354360"
  },
  {
    "text": "single pass for object detection and bounding box detection so if you think about if you've ever seen one of those",
    "start": "354360",
    "end": "361199"
  },
  {
    "text": "videos of like a street with a bunch of people walking around and cars and dogs",
    "start": "361199",
    "end": "367000"
  },
  {
    "text": "and shops and scooters and whatever with their boxes around them yeah and they",
    "start": "367000",
    "end": "372120"
  },
  {
    "text": "have their boxes around them and they they're labeled person or or whatever that's likely YOLO so what happens is",
    "start": "372120",
    "end": "380319"
  },
  {
    "text": "that single image in a YOLO model goes into the model and then out comes the bounding boxes and the actual",
    "start": "380319",
    "end": "387639"
  },
  {
    "text": "classification of those bounding boxes which is interesting because previous models previous to YOLO I'm still sure",
    "start": "387639",
    "end": "395160"
  },
  {
    "text": "some models do this in a multi stage way which is more computationally expensive",
    "start": "395160",
    "end": "400919"
  },
  {
    "text": "so they actually take multiple passes through a model or multiple models to compute both the bounding boxes and the",
    "start": "400919",
    "end": "408039"
  },
  {
    "text": "classes yeah I remember way back when we were first starting and I was act at a",
    "start": "408039",
    "end": "414000"
  },
  {
    "text": "different employer I was at Honeywell leading AI there at the time I remember just as 2 came out we were using that",
    "start": "414000",
    "end": "421639"
  },
  {
    "text": "for a couple of projects that we were working on way back in the day but that seems I mean that's like before",
    "start": "421639",
    "end": "426960"
  },
  {
    "text": "dinosaurs were in the Earth by AI standards but yeah way back yeah and I think even we had a podcast episode",
    "start": "426960",
    "end": "434840"
  },
  {
    "text": "maybe about fast R CNN or whatever it's called the fast version of R CNN good",
    "start": "434840",
    "end": "442080"
  },
  {
    "text": "memory good memory um that one's cool I mean that one I think how that one",
    "start": "442080",
    "end": "447440"
  },
  {
    "text": "worked was you pass your image in and then it detects the bounding boxes of",
    "start": "447440",
    "end": "453680"
  },
  {
    "text": "objects and then in a second pass it then classifies each kind of sub section of",
    "start": "453680",
    "end": "461000"
  },
  {
    "text": "the image as its class which also is very effective but it's less efficient",
    "start": "461000",
    "end": "466280"
  },
  {
    "text": "computationally than the YOLO kind of single pass thing and as you mentioned",
    "start": "466280",
    "end": "471759"
  },
  {
    "text": "there have been multiple versions of this so between YOLO and now version two",
    "start": "471759",
    "end": "477240"
  },
  {
    "text": "version three all the way up to version nine kind of each version of these in",
    "start": "477240",
    "end": "483080"
  },
  {
    "text": "some ways has and not just in a kind of more train with more data way they've",
    "start": "483080",
    "end": "488919"
  },
  {
    "text": "actually made kind of very significant discoveries and improvements in neural network architecture training",
    "start": "488919",
    "end": "496520"
  },
  {
    "text": "methodologies this sort of thing that has led it to be kind of the go-to",
    "start": "496520",
    "end": "501680"
  },
  {
    "text": "solution for at least realtime object detection in images which is why you see",
    "start": "501680",
    "end": "506879"
  },
  {
    "text": "all these videos of the bounding boxes around around people and such they've at",
    "start": "506879",
    "end": "512200"
  },
  {
    "text": "least gotten the visual bit a little bit nicer than they used to where you had the big clunky boxes overlaying",
    "start": "512200",
    "end": "517719"
  },
  {
    "text": "everything it was correct yes yeah well the v9 version of the project uh which",
    "start": "517719",
    "end": "525560"
  },
  {
    "text": "dropped at least if the date on the archive article link is right that would",
    "start": "525560",
    "end": "531160"
  },
  {
    "text": "have been the 21st of February of 2024 as we're recording this so not that long",
    "start": "531160",
    "end": "537839"
  },
  {
    "text": "ago but it was um developed by an open- Source team and kind of built on on top",
    "start": "537839",
    "end": "544000"
  },
  {
    "text": "of a code base from ultral litics YOLO version 5 and it's released I believe",
    "start": "544000",
    "end": "550920"
  },
  {
    "text": "under the GPL 3 license is the code that they released but the it seems like what",
    "start": "550920",
    "end": "557160"
  },
  {
    "text": "they focused on with yolo v9 was continued focus on efficiency to",
    "start": "557160",
    "end": "564279"
  },
  {
    "text": "where like you can do real time object detection meaning like as the frames of a video are coming in you can process",
    "start": "564279",
    "end": "571120"
  },
  {
    "text": "those in in real time with the model so efficiency is is really key in these",
    "start": "571120",
    "end": "576560"
  },
  {
    "text": "types of applications and then they focused on one of the fundamental challenges of of deep learning models of",
    "start": "576560",
    "end": "584560"
  },
  {
    "text": "these deep neural network models which is called the information bottleneck",
    "start": "584560",
    "end": "590279"
  },
  {
    "text": "principle which happens because especially as you kind of propagate if",
    "start": "590279",
    "end": "596680"
  },
  {
    "text": "you think about a neural network what it is is a big data transformation right you take a bunch of Matrix data in the",
    "start": "596680",
    "end": "604160"
  },
  {
    "text": "front end maybe representative of an image and that gets processed through successive layers of processing and then",
    "start": "604160",
    "end": "612079"
  },
  {
    "text": "out the other end comes maybe these indication of classes or other things and the information bottleneck principle",
    "start": "612079",
    "end": "619560"
  },
  {
    "text": "talks about the errors or the lack of information or the loss of information",
    "start": "619560",
    "end": "626399"
  },
  {
    "text": "that you lose as you process an input through the successive layers of the",
    "start": "626399",
    "end": "632240"
  },
  {
    "text": "feed forward process of that of that neural network which in some ways can be",
    "start": "632240",
    "end": "637519"
  },
  {
    "text": "addressed by having bigger networks and more data maybe you're less prone to",
    "start": "637519",
    "end": "643040"
  },
  {
    "text": "these informational problems but it's more of a problem when you're dealing",
    "start": "643040",
    "end": "648440"
  },
  {
    "text": "with these very efficient lightweight networks like the YOLO networks because",
    "start": "648440",
    "end": "653639"
  },
  {
    "text": "you have less layers to deal with and you don't want to lose any information that might be relevant to the",
    "start": "653639",
    "end": "659839"
  },
  {
    "text": "classification of the outputs I notice within Yolo's 9 docs they talk about",
    "start": "659839",
    "end": "666120"
  },
  {
    "text": "also reversible functions as well does that feed into no pun intended does that",
    "start": "666120",
    "end": "671720"
  },
  {
    "text": "feed into the ability to not lose data by reversing that feed forward through a function backward how does how do you",
    "start": "671720",
    "end": "678440"
  },
  {
    "text": "see that's utility yeah so the interesting way that they dealt with this or or kind of address this at least",
    "start": "678440",
    "end": "685760"
  },
  {
    "text": "in this version of the model is something that they're calling programmable gradient information or",
    "start": "685760",
    "end": "693680"
  },
  {
    "text": "PGI and the PGI portion of their research and and advancement relies on a",
    "start": "693680",
    "end": "701320"
  },
  {
    "text": "couple of things but one of the main things is this focus on again improving",
    "start": "701320",
    "end": "708920"
  },
  {
    "text": "the informational efficiency of the network and one of the ways that they've",
    "start": "708920",
    "end": "713959"
  },
  {
    "text": "done this is with what they call an auxiliary reversible branch and this",
    "start": "713959",
    "end": "719279"
  },
  {
    "text": "gets to these reversible functions that you mention so the concept of a reversible function for those that maybe",
    "start": "719279",
    "end": "727279"
  },
  {
    "text": "that's new to them means that the function and the inverse of the function can transform data without the loss of",
    "start": "727279",
    "end": "734800"
  },
  {
    "text": "information and so again there's that loss of information piece there and so it's a little bit hard to describe this",
    "start": "734800",
    "end": "741720"
  },
  {
    "text": "on the podcast without having a whiteboard or a visual but if you think about this PGI functionality that",
    "start": "741720",
    "end": "748800"
  },
  {
    "text": "they've add added into the network it's kind of like they're bolting on this auxiliary reversible branch which helps",
    "start": "748800",
    "end": "756600"
  },
  {
    "text": "deal with this information loss as gradients are calculated during the the training process and so during the",
    "start": "756600",
    "end": "764079"
  },
  {
    "text": "training process this reversible Branch helps not lose that gradient information",
    "start": "764079",
    "end": "770959"
  },
  {
    "text": "as during the forward pass and during the calculation of the updates of the weights of the model and that helps it",
    "start": "770959",
    "end": "778079"
  },
  {
    "text": "be very efficient during the training process but it's called auxiliary which is key because you can actually unbolt",
    "start": "778079",
    "end": "785600"
  },
  {
    "text": "it and take it off for inference which means I think part of the problem in the past with these reversible branches and",
    "start": "785600",
    "end": "793279"
  },
  {
    "text": "efforts at this were helped with the information loss but it also decreases",
    "start": "793279",
    "end": "798320"
  },
  {
    "text": "the efficiency in terms of computational efficiency of the model during inference",
    "start": "798320",
    "end": "804160"
  },
  {
    "text": "I'm going to throw a question at you and I realize this is not your your thing but just in case is you're using a",
    "start": "804160",
    "end": "810639"
  },
  {
    "text": "reversible function in that programmable gradient information process that you're talking about and in a normal feed",
    "start": "810639",
    "end": "817279"
  },
  {
    "text": "forward Network you know you're maintaining the weights as they're going through and change those are are changed and are you reversing functions to",
    "start": "817279",
    "end": "824920"
  },
  {
    "text": "maintain that back in the same space to where you're actually maintaining a new weight and you're keeping that that",
    "start": "824920",
    "end": "831600"
  },
  {
    "text": "gradient information for maybe future feed forward passes or do you have any sense of what the purpose of that is",
    "start": "831600",
    "end": "837360"
  },
  {
    "text": "yeah I think that uh so definitely will link some of the papers and the explanations in the show notes so feel",
    "start": "837360",
    "end": "844800"
  },
  {
    "text": "free to to look at that for um accurate information and let us know if we get it",
    "start": "844800",
    "end": "850079"
  },
  {
    "text": "wrong but yeah I think that the idea is that and the reason why this is",
    "start": "850079",
    "end": "855360"
  },
  {
    "text": "especially useful in the training side of what they're trying to do and it it's",
    "start": "855360",
    "end": "861880"
  },
  {
    "text": "kind of unbolted during the inference side is that during the training time it's really crucial that as your",
    "start": "861880",
    "end": "869160"
  },
  {
    "text": "calculating the updates to your weights you can do that in a very informationally accurate precise manner",
    "start": "869160",
    "end": "877720"
  },
  {
    "text": "especially for these lightweight uh networks which have fewer parameters to",
    "start": "877720",
    "end": "883639"
  },
  {
    "text": "train and so maintaining that information especially as you're calculating updates based on the",
    "start": "883639",
    "end": "889160"
  },
  {
    "text": "gradients is really important [Music]",
    "start": "889160",
    "end": "897599"
  },
  {
    "text": "gotcha [Music]",
    "start": "898040",
    "end": "908959"
  },
  {
    "text": "this is a chang log news break shipping quality software in hostile environments",
    "start": "908959",
    "end": "915160"
  },
  {
    "text": "Luca cladic writes quote I once had the opportunity to work for a startup that",
    "start": "915160",
    "end": "920360"
  },
  {
    "text": "had fallen from Tech debt into Tech bankruptcy bankruptcy Michael is Nature's doover it's a fresh start it's",
    "start": "920360",
    "end": "928079"
  },
  {
    "text": "a clean slate like the witness protection program exactly not at all although we managed to get it back on",
    "start": "928079",
    "end": "933959"
  },
  {
    "text": "the right track it made me rethink the concept of tech debt and how we ship software especially in hostile",
    "start": "933959",
    "end": "940199"
  },
  {
    "text": "environments end quote he goes on to tell this true story in great detail which is horrifying yet Echoes so many",
    "start": "940199",
    "end": "947440"
  },
  {
    "text": "of our experiences here's just one of the many horror scenes Luca describes quote there is also a handcrafted build",
    "start": "947440",
    "end": "955120"
  },
  {
    "text": "server a Jenkins box hosted in the office but no record of how it's provisioned or configured if something",
    "start": "955120",
    "end": "961519"
  },
  {
    "text": "were to happen to it the way you build software would just be lost each job on it is subtly different even for the same",
    "start": "961519",
    "end": "968959"
  },
  {
    "text": "Tech you have an Android source code that you build three instances out of but each of them builds in a different",
    "start": "968959",
    "end": "975519"
  },
  {
    "text": "way end quote this is a solid essay replete with warnings and a plea at the",
    "start": "975519",
    "end": "981120"
  },
  {
    "text": "end to ditch the tech Deb concept alog together you just heard one of our five",
    "start": "981120",
    "end": "987360"
  },
  {
    "text": "top stories from Monday's changelog news subscribe to the podcast to get all of the week's top stories and pop your",
    "start": "987360",
    "end": "994199"
  },
  {
    "text": "email address in at changel log.com newws to also receive our free companion email with even more developer news",
    "start": "994199",
    "end": "1001920"
  },
  {
    "text": "worth your attention once again that's Chang log.com [Music]",
    "start": "1001920",
    "end": "1010959"
  },
  {
    "text": "newws we talked a little bit about YOLO version 99's",
    "start": "1010959",
    "end": "1016680"
  },
  {
    "text": "programmable gradient information I I had to remind myself PGI programmable",
    "start": "1016680",
    "end": "1022480"
  },
  {
    "text": "gradient information the other piece of the architecture and I think this is just really interesting you've sort of",
    "start": "1022480",
    "end": "1029038"
  },
  {
    "text": "got all of this going on on the llm side where things are getting very",
    "start": "1029039",
    "end": "1034120"
  },
  {
    "text": "interesting ways to fine tune and preference tune and all all these families of models on the computer",
    "start": "1034120",
    "end": "1040079"
  },
  {
    "text": "vision side man they're really really thinking deeply about the architectures going into these models which have made",
    "start": "1040079",
    "end": "1048160"
  },
  {
    "text": "them so so efficient the other thing that kind of is a combination of things",
    "start": "1048160",
    "end": "1053360"
  },
  {
    "text": "that have come in the past that they're utilizing and this YOLO v9 is a",
    "start": "1053360",
    "end": "1058480"
  },
  {
    "text": "generalized Elon architecture so this is kind of a progression of a couple things",
    "start": "1058480",
    "end": "1064880"
  },
  {
    "text": "that have been in Yolo models in previous generations but they've",
    "start": "1064880",
    "end": "1070000"
  },
  {
    "text": "combined them in kind of a unique way this stands for generalized efficient layer aggregation Network or G Elon and",
    "start": "1070000",
    "end": "1078960"
  },
  {
    "text": "and this combines a couple of things from previous generations of YOLO and",
    "start": "1078960",
    "end": "1085080"
  },
  {
    "text": "from things like CSP net this has to do with how features are aggregated and",
    "start": "1085080",
    "end": "1092559"
  },
  {
    "text": "gradients are aggregated through the model in a very efficient way again leading to a very parameter efficient",
    "start": "1092559",
    "end": "1099640"
  },
  {
    "text": "model meaning a smaller set of parameters in Yolo v9 will have similar",
    "start": "1099640",
    "end": "1107760"
  },
  {
    "text": "performance to maybe models with many more parameters so this leads to the",
    "start": "1107760",
    "end": "1113039"
  },
  {
    "text": "efficiency overall it's pretty interesting they talk about being able to uh adapt to a much wider range of",
    "start": "1113039",
    "end": "1119520"
  },
  {
    "text": "applications without sacrificing speed or accuracy is that a form of fine",
    "start": "1119520",
    "end": "1124679"
  },
  {
    "text": "tuning the model uh or something that they're doing ahead of time that you're then fine-tuning on top of that at least",
    "start": "1124679",
    "end": "1130520"
  },
  {
    "text": "how I read some of that flexibility in was yes uh there's kind of a parameter",
    "start": "1130520",
    "end": "1136760"
  },
  {
    "text": "efficient this is a parameter efficient setup for fine-tuning maybe to a variety",
    "start": "1136760",
    "end": "1143760"
  },
  {
    "text": "of types of scenarios or even training a new model from scratch in an entirely",
    "start": "1143760",
    "end": "1150480"
  },
  {
    "text": "new domain and and doing that very efficient and some of the things that I've seen you know people have already",
    "start": "1150480",
    "end": "1157520"
  },
  {
    "text": "quantized this model using things like open Veno which is is very popular for",
    "start": "1157520",
    "end": "1163919"
  },
  {
    "text": "these kind of edge Vision cases and running this very efficient so real time",
    "start": "1163919",
    "end": "1169760"
  },
  {
    "text": "object detection on even desktop or laptop CPUs so the new architecture",
    "start": "1169760",
    "end": "1176840"
  },
  {
    "text": "developments are both geared towards yeah that efficiency but also squeezing",
    "start": "1176840",
    "end": "1182440"
  },
  {
    "text": "every ounce of performance out of parameter efficient models both in terms of training and flexibility across",
    "start": "1182440",
    "end": "1188520"
  },
  {
    "text": "different use cases yeah I think there's great applications for this on the edge",
    "start": "1188520",
    "end": "1193559"
  },
  {
    "text": "where you're not in a one of the giant clouds with essentially if you're willing to pay for it infinite compute",
    "start": "1193559",
    "end": "1199280"
  },
  {
    "text": "available to you whether it be training or for inference either way so the fact that this can run on just about anything",
    "start": "1199280",
    "end": "1205320"
  },
  {
    "text": "I mean back in the early days we could do YOLO V2 on smaller equipment but it",
    "start": "1205320",
    "end": "1211039"
  },
  {
    "text": "was uh it didn't run smoothly you'd have points where the it would overwhelm the computational cycle and so it's nice",
    "start": "1211039",
    "end": "1217679"
  },
  {
    "text": "seeing something like this has come this far uh it's quite an open source Library yeah and there's a a link that we'll add",
    "start": "1217679",
    "end": "1224799"
  },
  {
    "text": "into the show notes which includes a notebook for running YOLO v9 in a collab",
    "start": "1224799",
    "end": "1231760"
  },
  {
    "text": "notebook even like I say on CPUs so in terms of the efficiency one of the",
    "start": "1231760",
    "end": "1237760"
  },
  {
    "text": "things that I saw was YOLO v9 operates with 42% fewer parameters and 21% less",
    "start": "1237760",
    "end": "1247000"
  },
  {
    "text": "computational demand than YOLO V7 yet it achieves comparable accuracy",
    "start": "1247000",
    "end": "1253559"
  },
  {
    "text": "so you know it was already fairly accurate right and kind of an industry",
    "start": "1253559",
    "end": "1258640"
  },
  {
    "text": "standard but now with much fewer parameters and I think that that is",
    "start": "1258640",
    "end": "1264120"
  },
  {
    "text": "definitely a trend that we've been seeing not only in computer vision but in other cases where you see things like",
    "start": "1264120",
    "end": "1271520"
  },
  {
    "text": "ol llama or other things that llama CPP that are allowing you to run large",
    "start": "1271520",
    "end": "1278159"
  },
  {
    "text": "language models on a variety of Hardware including just on your on your local laptop and you know quantization type of",
    "start": "1278159",
    "end": "1286600"
  },
  {
    "text": "libraries like bits and bytes and Optimum and big DL and these libraries",
    "start": "1286600",
    "end": "1294080"
  },
  {
    "text": "that allow you to run maybe 7 billion parameter large language models or other",
    "start": "1294080",
    "end": "1300000"
  },
  {
    "text": "generative AI models but in lower precisions so that you can run them on a",
    "start": "1300000",
    "end": "1305919"
  },
  {
    "text": "variety of Hardware or optimize them for a variety of Hardware we also had neural",
    "start": "1305919",
    "end": "1311240"
  },
  {
    "text": "Magic on the show a little while back now who has a set of libraries for",
    "start": "1311240",
    "end": "1317200"
  },
  {
    "text": "optimizing models to run on CPUs and yeah so there's a lot of kind of",
    "start": "1317200",
    "end": "1322480"
  },
  {
    "text": "precision and quantization that can happen even on top of the use of these",
    "start": "1322480",
    "end": "1327559"
  },
  {
    "text": "parameter efficient models one of the interesting things also that I saw this",
    "start": "1327559",
    "end": "1333360"
  },
  {
    "text": "last new cycle which at least in the circles that I run in with large language models people were talking",
    "start": "1333360",
    "end": "1339679"
  },
  {
    "text": "about a lot which is this release from Microsoft or a paper from",
    "start": "1339679",
    "end": "1344840"
  },
  {
    "text": "Microsoft that I think is titled something like the era of one bit llms",
    "start": "1344840",
    "end": "1350480"
  },
  {
    "text": "which is interesting because um you know a lot of people have talked about going from maybe float 32 to float 16 and 8",
    "start": "1350480",
    "end": "1358200"
  },
  {
    "text": "and 4 bit Precision that sort of thing and this kind of uh brings in this idea",
    "start": "1358200",
    "end": "1363799"
  },
  {
    "text": "of onebit llms with this architecture bitet and so",
    "start": "1363799",
    "end": "1369559"
  },
  {
    "text": "I found it interesting that we got both YOLO v9 but now comes on the llm side",
    "start": "1369559",
    "end": "1375880"
  },
  {
    "text": "this onebit architecture and it seems like a similar thing is happening I",
    "start": "1375880",
    "end": "1381600"
  },
  {
    "text": "don't know if you remember back when we were talking about R CNN and some of the",
    "start": "1381600",
    "end": "1386760"
  },
  {
    "text": "larger computer vision models we've seen the progression to more and more parameter efficiency and flexibility",
    "start": "1386760",
    "end": "1393960"
  },
  {
    "text": "across deployment scenarios and now we're seeing that maybe in a more rapid",
    "start": "1393960",
    "end": "1400600"
  },
  {
    "text": "way with llms and this you know one bit llm but also all the other quantization",
    "start": "1400600",
    "end": "1406240"
  },
  {
    "text": "and that sort of stuff that we've seen on generative side do you have any sense from an application standpoint like",
    "start": "1406240",
    "end": "1412840"
  },
  {
    "text": "where you might go with these one bit llms like what are some of the use cases that come to mind for you yeah I think",
    "start": "1412840",
    "end": "1418120"
  },
  {
    "text": "it's interesting so um this one bit llm that was released they talk about it",
    "start": "1418120",
    "end": "1424559"
  },
  {
    "text": "having similar performance to a model of the same parameter size but more",
    "start": "1424559",
    "end": "1431120"
  },
  {
    "text": "computational efficiency because of course these parameters are are bits they're actually not just zero and one",
    "start": "1431120",
    "end": "1438240"
  },
  {
    "text": "we we can talk about that here in a second but more computational efficiency so I think that this is really",
    "start": "1438240",
    "end": "1444679"
  },
  {
    "text": "interesting for cases where you do want to run maybe an llm on an edge device in",
    "start": "1444679",
    "end": "1451039"
  },
  {
    "text": "a scenario like think about disaster relief and you have a device out in the",
    "start": "1451039",
    "end": "1456200"
  },
  {
    "text": "field that's giving help to First Responders or something giving them",
    "start": "1456200",
    "end": "1461720"
  },
  {
    "text": "information or processing information from training documents or something and you're using an llm to provide ERS is",
    "start": "1461720",
    "end": "1469440"
  },
  {
    "text": "likely very spotty internet connection in that case and so having something that could run on device in a variety of",
    "start": "1469440",
    "end": "1477840"
  },
  {
    "text": "scenarios would be quite relevant or maybe so it one scenario would be lack",
    "start": "1477840",
    "end": "1482919"
  },
  {
    "text": "of connectivity I think another scenario would be very latency sensitive",
    "start": "1482919",
    "end": "1489559"
  },
  {
    "text": "scenarios where you want a response very quickly you don't want to have to rely on network overhead or things going out",
    "start": "1489559",
    "end": "1497720"
  },
  {
    "text": "of a network that you're operating in for security reasons that sort of thing might be a good use of these yep that",
    "start": "1497720",
    "end": "1504279"
  },
  {
    "text": "sounds interesting they have a term in here that I'm curious about uh referring",
    "start": "1504279",
    "end": "1509360"
  },
  {
    "text": "to bitnet uh they talk about it being a 1.58 bit llm uh and hugging face in",
    "start": "1509360",
    "end": "1516240"
  },
  {
    "text": "their paper notes that all large language models are 1.58 do you have any comment about that what that means the",
    "start": "1516240",
    "end": "1523159"
  },
  {
    "text": "reality is if you I think they talk about this in the paper if you go down to a truly one bit llm each weight of",
    "start": "1523159",
    "end": "1531480"
  },
  {
    "text": "your model is either zero or one right then yeah you would expect to lose a lot",
    "start": "1531480",
    "end": "1538720"
  },
  {
    "text": "of information that might be important and so they make a a slight compromise in here maybe it's unfair to call it a",
    "start": "1538720",
    "end": "1545960"
  },
  {
    "text": "compromise they they make an astute um conversion from bytes in other words",
    "start": "1545960",
    "end": "1552279"
  },
  {
    "text": "zero to one or bits to what they call T uh Turner so these are basically",
    "start": "1552279",
    "end": "1560440"
  },
  {
    "text": "Triplets of or or three bits together so you have for example weight could be",
    "start": "1560440",
    "end": "1567000"
  },
  {
    "text": "minus one 01 or something like that so you've got three numbers that represent",
    "start": "1567000",
    "end": "1573520"
  },
  {
    "text": "certain information and that's where they kind of get this 1.58 bit gotcha so this is also why it's",
    "start": "1573520",
    "end": "1581880"
  },
  {
    "text": "kind of they release this new type of architecture that processes these uh",
    "start": "1581880",
    "end": "1588520"
  },
  {
    "text": "turnery bits or Turner um these combinations of three bits and that's",
    "start": "1588520",
    "end": "1595600"
  },
  {
    "text": "presented in the Microsoft paper but yeah I think this is only the kind of latest I I think we'll see my prediction",
    "start": "1595600",
    "end": "1603159"
  },
  {
    "text": "would be that we'll see many more things like this where people are trying to be",
    "start": "1603159",
    "end": "1608520"
  },
  {
    "text": "parameter and compute efficient with large language models we've seen models",
    "start": "1608520",
    "end": "1614279"
  },
  {
    "text": "getting U more more efficient and more you know compact over time time and uh",
    "start": "1614279",
    "end": "1620039"
  },
  {
    "text": "as we're looking at so many smaller very capable models being used out on edge",
    "start": "1620039",
    "end": "1625799"
  },
  {
    "text": "devices do you envision something like this where they're really targeting you know efficiency in terms of being able",
    "start": "1625799",
    "end": "1631799"
  },
  {
    "text": "to do that in something like small electronics or is that a little bit overly ambitious for where this might",
    "start": "1631799",
    "end": "1637760"
  },
  {
    "text": "take us in a reasonably foreseeable future yeah it's actually a good question because one of the things that",
    "start": "1637760",
    "end": "1644159"
  },
  {
    "text": "we saw also um I don't know if it was this week but recently at least was",
    "start": "1644159",
    "end": "1650360"
  },
  {
    "text": "qualcomm's announcement and release of a huge number of I forget how many a whole",
    "start": "1650360",
    "end": "1656399"
  },
  {
    "text": "bunch of models on what they're calling the Qualcomm AI hub for models that run",
    "start": "1656399",
    "end": "1662240"
  },
  {
    "text": "on device on their Snapdragon processors and other things",
    "start": "1662240",
    "end": "1667279"
  },
  {
    "text": "at the edge on small devices so these wouldn't be like the small devices of",
    "start": "1667279",
    "end": "1674000"
  },
  {
    "text": "like a microcontroller or something like that there's still a good bit of power",
    "start": "1674000",
    "end": "1679399"
  },
  {
    "text": "in these processors but it is super interesting that Qualcomm has made the",
    "start": "1679399",
    "end": "1685240"
  },
  {
    "text": "effort to make these types of models whether they be object detection or large language models or other things",
    "start": "1685240",
    "end": "1692600"
  },
  {
    "text": "available in optimized forms to run on very small devices and I think it's a",
    "start": "1692600",
    "end": "1698600"
  },
  {
    "text": "trend that we'll we'll keep [Music]",
    "start": "1698600",
    "end": "1707120"
  },
  {
    "text": "seeing [Music]",
    "start": "1707120",
    "end": "1714519"
  },
  {
    "text": "it seems somehow like in computer vision it took maybe what five years we've been",
    "start": "1714519",
    "end": "1721360"
  },
  {
    "text": "doing uh five or six years we've been doing this podcast and over that time we've seen computer vision models shrink",
    "start": "1721360",
    "end": "1728559"
  },
  {
    "text": "down and down and become faster and more parameter efficient it almost seems like that's happening much faster on the",
    "start": "1728559",
    "end": "1735559"
  },
  {
    "text": "large language model side and generative model size um it's like Shunk from five",
    "start": "1735559",
    "end": "1741120"
  },
  {
    "text": "years to one year where a lot of that's coming out for on device usage when we",
    "start": "1741120",
    "end": "1748080"
  },
  {
    "text": "and the rest of the change log team are looking at what content to bring onto the show uh there various guests and",
    "start": "1748080",
    "end": "1754600"
  },
  {
    "text": "there are all sorts of topics and advancements going out uh it's become quite challenging to narrow it down to",
    "start": "1754600",
    "end": "1761279"
  },
  {
    "text": "just uh what we can cover in these shows and largely that's because of what Daniel was just saying that tremendous",
    "start": "1761279",
    "end": "1767399"
  },
  {
    "text": "acceleration and the advancement of this technology is very hard to keep up with and report",
    "start": "1767399",
    "end": "1772960"
  },
  {
    "text": "on especially trying to figure out what folks are most in need of hearing are",
    "start": "1772960",
    "end": "1778440"
  },
  {
    "text": "being pointed to so on any given week how which of the dozens of things that that are happening do you want to do so",
    "start": "1778440",
    "end": "1785559"
  },
  {
    "text": "and I would say for those out there listening like in this episode we've talked a lot about parameter efficient",
    "start": "1785559",
    "end": "1792399"
  },
  {
    "text": "models and whether it be the Qualcomm AI models or the onebit llm or YOLO and",
    "start": "1792399",
    "end": "1799960"
  },
  {
    "text": "running these on device and at the edge might be natural to think oh the new cycle is totally switched to local",
    "start": "1799960",
    "end": "1806080"
  },
  {
    "text": "models running all the models locally and that'll solve all the problems and I think the reality is in the future it's",
    "start": "1806080",
    "end": "1813600"
  },
  {
    "text": "going to be kind of both and right you're not going to serve let's say that",
    "start": "1813600",
    "end": "1820120"
  },
  {
    "text": "you integrate a model into some social media application or whatever mobile",
    "start": "1820120",
    "end": "1826799"
  },
  {
    "text": "application or or you're serving a web app and it's got some AI integration or",
    "start": "1826799",
    "end": "1833000"
  },
  {
    "text": "something like that it's very unlikely I think that you are going to want to serve up millions and millions of",
    "start": "1833000",
    "end": "1840000"
  },
  {
    "text": "requests using only local models and in the same way if you've got an Enterprise",
    "start": "1840000",
    "end": "1846240"
  },
  {
    "text": "batch use case right and you want to process 1.5 million documents through a",
    "start": "1846240",
    "end": "1852039"
  },
  {
    "text": "large language model you likely don't want that running on your Mac M2 or",
    "start": "1852039",
    "end": "1857440"
  },
  {
    "text": "something like like that's not the deployment strategy for that scenario but yet you will see a lot of models",
    "start": "1857440",
    "end": "1863399"
  },
  {
    "text": "running at the edge or or locally and I think the reality is that we'll go into",
    "start": "1863399",
    "end": "1869399"
  },
  {
    "text": "kind of a both and sort of scenario where yes a lot of things you'll be able",
    "start": "1869399",
    "end": "1874639"
  },
  {
    "text": "to run locally but the same as like I mean you can run a lot of software locally but it doesn't mean that you're",
    "start": "1874639",
    "end": "1880840"
  },
  {
    "text": "also not running software in the cloud you know AI is just a new layer in your",
    "start": "1880840",
    "end": "1886039"
  },
  {
    "text": "kind of software stack so we're going to run it locally yeah and we're going to run it in the cloud that's exactly right",
    "start": "1886039",
    "end": "1893279"
  },
  {
    "text": "that was where I was going to go uh anyway you just hit it and and that was it's following the maturity trend of",
    "start": "1893279",
    "end": "1899279"
  },
  {
    "text": "software and just as we have huge software systems that you can only run in the cloud and in a massive scale and",
    "start": "1899279",
    "end": "1906480"
  },
  {
    "text": "you have apps on your phone and you have also very small micro Electronics which have even smaller software functions on",
    "start": "1906480",
    "end": "1913240"
  },
  {
    "text": "them uh integrated in maybe in the Bios all these different areas and we're see models doing the same thing so one of",
    "start": "1913240",
    "end": "1920799"
  },
  {
    "text": "the things that we're often asked to address and we have done repeatedly over the years is what's the current way to",
    "start": "1920799",
    "end": "1927679"
  },
  {
    "text": "do training and deployment and I think to your point Daniel there are now now",
    "start": "1927679",
    "end": "1933200"
  },
  {
    "text": "that we're maturing uh rapidly in this industry there are many ways and there's not one right way to do it anymore",
    "start": "1933200",
    "end": "1940399"
  },
  {
    "text": "that's kind of figuring out your use case figuring out what mixture of different model types need to contribute",
    "start": "1940399",
    "end": "1947360"
  },
  {
    "text": "into that and what the architecture for all those models and how they communicate through the software and",
    "start": "1947360",
    "end": "1953159"
  },
  {
    "text": "what Hardware is available to them so it's become quite complicated there's no longer the way uh you know to borrow the",
    "start": "1953159",
    "end": "1960440"
  },
  {
    "text": "Mandalorian saying it's it's now it's now manyways do you have any thoughts on",
    "start": "1960440",
    "end": "1966039"
  },
  {
    "text": "how people might approach that how do you think about it when you're doing things in prediction guard and trying to help your customers move forward",
    "start": "1966039",
    "end": "1973120"
  },
  {
    "text": "basically you kind of have to split things up a little bit by stage of your",
    "start": "1973120",
    "end": "1979840"
  },
  {
    "text": "project and also the use case that you're considering so what I mean by",
    "start": "1979840",
    "end": "1986000"
  },
  {
    "text": "stage of your project is I really encourage people like especially if they",
    "start": "1986000",
    "end": "1991120"
  },
  {
    "text": "have a generative AI use case the best thing you can do to get a sense of like",
    "start": "1991120",
    "end": "1996919"
  },
  {
    "text": "let's say that I want to summarize news articles related to",
    "start": "1996919",
    "end": "2002000"
  },
  {
    "text": "stocks that I want to trade on you know or something like that the very best thing you can do is not jump right to",
    "start": "2002000",
    "end": "2008720"
  },
  {
    "text": "okay I'm going to fine-tune a model for that or spin up some crazy GPU",
    "start": "2008720",
    "end": "2013919"
  },
  {
    "text": "infrastructure or something like that the best thing you can do is just get some off-the-shelf models and if you",
    "start": "2013919",
    "end": "2020720"
  },
  {
    "text": "want to either run them the easiest Cloud way to run those would be to run them you know if they're small enough in",
    "start": "2020720",
    "end": "2027679"
  },
  {
    "text": "just a collab notebook or a hosted notebook environment like that that's more than enough to figure out if if",
    "start": "2027679",
    "end": "2034320"
  },
  {
    "text": "they're going to work for your use case right or if you want to go the more local deployment route there's things",
    "start": "2034320",
    "end": "2042240"
  },
  {
    "text": "like I I already mentioned you know of course if you want to run YOLO that's that's easier now than ever and there's",
    "start": "2042240",
    "end": "2048878"
  },
  {
    "text": "quantized versions of that that you can run on on a CPU even you don't need even a special type of Hardware but then for",
    "start": "2048879",
    "end": "2056240"
  },
  {
    "text": "the generative side of things there's things like AMA and LM studio and llama",
    "start": "2056240",
    "end": "2062919"
  },
  {
    "text": "CPP and these things that will allow you to prompt models and figure out if they'll work for your use case locally",
    "start": "2062919",
    "end": "2069320"
  },
  {
    "text": "so that's kind of exploration stage then you have to decide okay well if this",
    "start": "2069320",
    "end": "2075200"
  },
  {
    "text": "project is a work project right I figured out maybe that I can prototype this and figure out it might work then",
    "start": "2075200",
    "end": "2083040"
  },
  {
    "text": "you kind of have to play through the scenarios in your mind well oh if this is a mobile app and I'm processing",
    "start": "2083040",
    "end": "2089679"
  },
  {
    "text": "customers private data Maybe it makes sense to try to run a model at the edge in my mobile app on their device you",
    "start": "2089679",
    "end": "2096878"
  },
  {
    "text": "know a qualcom AI model from their AI Hub on their mobile device and that would be really",
    "start": "2096879",
    "end": "2102800"
  },
  {
    "text": "good but if it's a web app application and there's not as aggressive of a",
    "start": "2102800",
    "end": "2108599"
  },
  {
    "text": "security posture probably you want to figure out how you're going to run and host that model in a way that makes",
    "start": "2108599",
    "end": "2115040"
  },
  {
    "text": "sense to you even from a public endpoint that's just a product like you know together AI or mistol or or something",
    "start": "2115040",
    "end": "2123720"
  },
  {
    "text": "like that or you're going to figure out how to run it in a secure local environment with either a product",
    "start": "2123720",
    "end": "2131839"
  },
  {
    "text": "that can host that model in a secure environment in your own cloud or in your own network or your own kind of self-",
    "start": "2131839",
    "end": "2139680"
  },
  {
    "text": "deployment of that model using things in your Cloud infrastructure like Sage",
    "start": "2139680",
    "end": "2145160"
  },
  {
    "text": "maker and AWS or or other things like that yeah it's increasingly it it's",
    "start": "2145160",
    "end": "2150440"
  },
  {
    "text": "becoming part of the software and your larger architecture as you you know we've seen in you know the recent couple",
    "start": "2150440",
    "end": "2157240"
  },
  {
    "text": "of years especially the strong rise of mlops you know which kind of corresponds",
    "start": "2157240",
    "end": "2162720"
  },
  {
    "text": "to devops in terms of deployment and all those things do you tend to think of it",
    "start": "2162720",
    "end": "2167839"
  },
  {
    "text": "in more of an integrated way or do you still at this point in time as we're in 2024 think of it as separate approaches",
    "start": "2167839",
    "end": "2175240"
  },
  {
    "text": "you know from the software what how do you parse those two sides of that coin it's interesting I think I at least in",
    "start": "2175240",
    "end": "2181319"
  },
  {
    "text": "my own mind I tend to separate them out maybe depending on some of what's",
    "start": "2181319",
    "end": "2188280"
  },
  {
    "text": "involved in a project so if it's the use of a pre-trained model I think the",
    "start": "2188280",
    "end": "2195000"
  },
  {
    "text": "burden is a lot more in kind of the traditional devops monitoring testing",
    "start": "2195000",
    "end": "2201040"
  },
  {
    "text": "uptime automation deployment that sort of thing because likely you're just",
    "start": "2201040",
    "end": "2206119"
  },
  {
    "text": "interacting with the model via an API like you would integrate any other API",
    "start": "2206119",
    "end": "2211560"
  },
  {
    "text": "now there's certain things that can help you like versioning prompts and testing",
    "start": "2211560",
    "end": "2217680"
  },
  {
    "text": "for model drift and or data drift and that sort of thing but it's not so",
    "start": "2217680",
    "end": "2222920"
  },
  {
    "text": "dissimilar I would say to traditional software development whereas if you are",
    "start": "2222920",
    "end": "2228839"
  },
  {
    "text": "really have a unique scenario and you're fine-tuning a model for a unique scenario you're likely going multiple",
    "start": "2228839",
    "end": "2236400"
  },
  {
    "text": "iterations on curating your data set on training your model on evaluating your",
    "start": "2236400",
    "end": "2241720"
  },
  {
    "text": "model on versioning your model releasing it in your model servers up updating it",
    "start": "2241720",
    "end": "2248160"
  },
  {
    "text": "with new data that comes in and I think some of that specific mlops type of",
    "start": "2248160",
    "end": "2254480"
  },
  {
    "text": "software will likely appeal to the people that are doing that process which are usually data scientists and not not",
    "start": "2254480",
    "end": "2262599"
  },
  {
    "text": "software engineers and versioning your model versioning your data evaluating",
    "start": "2262599",
    "end": "2268400"
  },
  {
    "text": "your model and the way that those systems are set up like weights and biases or clear ML and these types of",
    "start": "2268400",
    "end": "2274280"
  },
  {
    "text": "things are quite useful in terms of versioning your model out when you're",
    "start": "2274280",
    "end": "2279599"
  },
  {
    "text": "training it like that so I think mlops is alive and well but I also think that",
    "start": "2279599",
    "end": "2284880"
  },
  {
    "text": "with the rise of this kind of apid driven AI Development A lot of that does",
    "start": "2284880",
    "end": "2292319"
  },
  {
    "text": "or can fit into more of the devops side",
    "start": "2292319",
    "end": "2297359"
  },
  {
    "text": "of things yeah when you're using an API that somebody else is hosting maintaining has fine-tuned all that",
    "start": "2297359",
    "end": "2304200"
  },
  {
    "text": "you're basically using it as a service like any other service that would not be Ai and so you just treat it as an API",
    "start": "2304200",
    "end": "2310560"
  },
  {
    "text": "along the way yeah yeah and and where that's maybe slightly different is you",
    "start": "2310560",
    "end": "2315680"
  },
  {
    "text": "are getting kind of some variability out of that API both in terms of performance and latency which are maybe common",
    "start": "2315680",
    "end": "2322480"
  },
  {
    "text": "across software projects but also in terms of the performance output of the model especially if you're using like a",
    "start": "2322480",
    "end": "2329160"
  },
  {
    "text": "closed model product like an open AI or anthropic or something like that they're",
    "start": "2329160",
    "end": "2335319"
  },
  {
    "text": "making improvements to their underlying model under the hood all the time and it is really more of a product it's not",
    "start": "2335319",
    "end": "2342480"
  },
  {
    "text": "just you're hitting the model there's layers around the model which are product layers that can influence the",
    "start": "2342480",
    "end": "2349119"
  },
  {
    "text": "behavior of that model I mean you just kind of look at what's happened with gini over the past three or four weeks",
    "start": "2349119",
    "end": "2356400"
  },
  {
    "text": "we don't need to you know get into all of the details of that if people want to look look it up they can but I think a",
    "start": "2356400",
    "end": "2362880"
  },
  {
    "text": "lot of those issues that that product had were actual product issues that we're at the product layer surrounding",
    "start": "2362880",
    "end": "2369440"
  },
  {
    "text": "the model not performance necessarily or biases in the actual model but in the",
    "start": "2369440",
    "end": "2376000"
  },
  {
    "text": "filters around the model and how things are modified in and out of the model and so that actual product that you're",
    "start": "2376000",
    "end": "2383400"
  },
  {
    "text": "interacting with can really cause small changes in how things go into the model",
    "start": "2383400",
    "end": "2389760"
  },
  {
    "text": "on the product level can make huge changes in the quality of the outputs of",
    "start": "2389760",
    "end": "2395880"
  },
  {
    "text": "the model that sounds like some pretty good practical AI advice right there I",
    "start": "2395880",
    "end": "2401520"
  },
  {
    "text": "think for me at least that very much helps me to kind of contextualize the different things that we may be doing at",
    "start": "2401520",
    "end": "2408440"
  },
  {
    "text": "work for myself and as we're making choices and decisions and how we're going to tackle different problems so I",
    "start": "2408440",
    "end": "2414040"
  },
  {
    "text": "appreciate uh you sharing that guidance there yeah and I guess we're talking",
    "start": "2414040",
    "end": "2419599"
  },
  {
    "text": "about the ml off side of things and we've talked about practicalities of deployment schemes and quantization and",
    "start": "2419599",
    "end": "2426760"
  },
  {
    "text": "all of that this episode and in terms of a Learning Resource for people if they",
    "start": "2426760",
    "end": "2432680"
  },
  {
    "text": "want to dive into some of this there's a lot of great ones out there one one is to follow the mlops community podcast",
    "start": "2432680",
    "end": "2441040"
  },
  {
    "text": "which is a podcast that Chris and I love and have collaborated with over time",
    "start": "2441040",
    "end": "2446480"
  },
  {
    "text": "Demetrios shout out to the great things you're doing funniest guy in AI yeah check out everything that",
    "start": "2446480",
    "end": "2452960"
  },
  {
    "text": "they're doing over there I also ran across this Intel Ops professional",
    "start": "2452960",
    "end": "2459400"
  },
  {
    "text": "certification from Intel there's if you just search for Intel ml Ops",
    "start": "2459400",
    "end": "2465000"
  },
  {
    "text": "certification um this is totally free as as far as I can tell there's seven",
    "start": "2465000",
    "end": "2470480"
  },
  {
    "text": "modules and eight Hands-On labs and I'm talking about software solution",
    "start": "2470480",
    "end": "2477200"
  },
  {
    "text": "architectures for machine learning and AI API and endpoint design principles of",
    "start": "2477200",
    "end": "2483839"
  },
  {
    "text": "mlops optimizing the full stack so really seems to be a good a good set of",
    "start": "2483839",
    "end": "2490240"
  },
  {
    "text": "things to look at if you're wanting to think more about the practicalities of these deployments and other things all",
    "start": "2490240",
    "end": "2497800"
  },
  {
    "text": "right sounds good well thanks for sharing your wisdom again today really good episode I'm going to uh I guess",
    "start": "2497800",
    "end": "2504480"
  },
  {
    "text": "I'll see you uh in the UK for the next few weeks to come sounds good yeah thanks Chris we'll see you soon see you",
    "start": "2504480",
    "end": "2513280"
  },
  {
    "text": "later all right that is practical AI for this",
    "start": "2516240",
    "end": "2522319"
  },
  {
    "text": "week subscribe now if you haven't already head to practical a.m for all",
    "start": "2522319",
    "end": "2528920"
  },
  {
    "text": "the ways and join our free slack team where you can hang out with Daniel Chris",
    "start": "2528920",
    "end": "2534000"
  },
  {
    "text": "and the entire change log Community sign up today at practical ai. fm/ Community",
    "start": "2534000",
    "end": "2541960"
  },
  {
    "text": "thanks again to our partners at fly.io to our beat freaking residence breakmaster cylinder and to you for",
    "start": "2541960",
    "end": "2548440"
  },
  {
    "text": "listening we appreciate you spending time with us that's all for now we'll talk to you again next",
    "start": "2548440",
    "end": "2555160"
  },
  {
    "text": "[Music]",
    "start": "2555160",
    "end": "2563679"
  },
  {
    "text": "time",
    "start": "2564400",
    "end": "2567400"
  }
]