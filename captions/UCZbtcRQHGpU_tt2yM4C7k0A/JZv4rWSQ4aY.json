[
  {
    "text": "I'm not terribly worried about whether or not a significant portion of code starts coming from AI sources in the",
    "start": "120",
    "end": "6879"
  },
  {
    "text": "years ahead because if there's one thing I have figured out is that things are constantly evolving and we should stop",
    "start": "6879",
    "end": "13599"
  },
  {
    "text": "trying to keep things exactly as they are today you know our comfort level right now so whatever the future is I'm",
    "start": "13599",
    "end": "20320"
  },
  {
    "text": "going to dive into it I'm going to enjoy it I'm going to take advantage of that I'm all for",
    "start": "20320",
    "end": "26760"
  },
  {
    "text": "it big thanks to our partners Leno fle and launch dark we love Leno they keep it fast and",
    "start": "26840",
    "end": "32279"
  },
  {
    "text": "simple check them out at lin.com changelog our bandwidth is provided by fastly learn more at fastly.com and get",
    "start": "32279",
    "end": "39520"
  },
  {
    "text": "your feature Flags powered by launch Darkly get a demo at launchd dark.com",
    "start": "39520",
    "end": "45079"
  },
  {
    "text": "with advancements in Ai and deep learning evolving at lightning Pace it's",
    "start": "45559",
    "end": "50879"
  },
  {
    "text": "more important now than ever to research the best options suited to your unique needs this is particularly true when",
    "start": "50879",
    "end": "57559"
  },
  {
    "text": "building custom systems and those systems that are GPU heavy not only do the applications running on the system",
    "start": "57559",
    "end": "63480"
  },
  {
    "text": "matter but your AI infrastructure and budget constraints need to be front of mind as well pssc Labs which is an HPC",
    "start": "63480",
    "end": "71560"
  },
  {
    "text": "and AI custom Solutions provider based in California has been creating high performance Computing systems to meet",
    "start": "71560",
    "end": "77880"
  },
  {
    "text": "their clients unique Enterprise Computing challenges for more than 25 years and with cloud computing costs",
    "start": "77880",
    "end": "84320"
  },
  {
    "text": "growing at astronomical rates plus companies increasingly losing control of their data security it is is no wonder",
    "start": "84320",
    "end": "90560"
  },
  {
    "text": "that enterprises and government agencies need to continually look for ways to Take Back Control of their Data",
    "start": "90560",
    "end": "96360"
  },
  {
    "text": "Solutions from pssc Labs provide a coste effective highly secure and performance",
    "start": "96360",
    "end": "101920"
  },
  {
    "text": "guarantee that organizations need to reach their Ai and machine learning goals for more information and a free",
    "start": "101920",
    "end": "108799"
  },
  {
    "text": "consultation please visit PSS cbs.com practical aai once again that's PSS",
    "start": "108799",
    "end": "115240"
  },
  {
    "text": "cbs.com practical aai",
    "start": "115240",
    "end": "121840"
  },
  {
    "text": "[Music] welcome to practical AI a weekly podcast",
    "start": "121920",
    "end": "129200"
  },
  {
    "text": "that makes artificial intelligence practical productive and accessible to everyone this is where conversations",
    "start": "129200",
    "end": "134959"
  },
  {
    "text": "around AI machine learning and data science happen join the community and slack with us around various topics of",
    "start": "134959",
    "end": "140800"
  },
  {
    "text": "the show at cha.com community and follow us on Twitter we at practical",
    "start": "140800",
    "end": "147159"
  },
  {
    "text": "aifm welcome to another fully connected",
    "start": "148840",
    "end": "155040"
  },
  {
    "text": "episode of practical AI where Chris and I keep you fully connected with",
    "start": "155040",
    "end": "160440"
  },
  {
    "text": "everything that's happening in the AI Community we'll take some time to discuss the latest AI news and we'll dig",
    "start": "160440",
    "end": "166280"
  },
  {
    "text": "a little bit into learning resources to help you level up your machine learning game I'm Daniel whack I'm a data",
    "start": "166280",
    "end": "173440"
  },
  {
    "text": "scientist with s International and I'm joined as always by my co-host Chris",
    "start": "173440",
    "end": "178640"
  },
  {
    "text": "Benson who is a strategist at loed Martin how you doing Chris I am doing good today it's good to be recording and",
    "start": "178640",
    "end": "186280"
  },
  {
    "text": "I think we have something pretty interesting to dive into today with everyone yeah for sure well I mean in",
    "start": "186280",
    "end": "192760"
  },
  {
    "text": "these episodes where it's just you and I uh we kind of get the privilege to just talk about whatever seemed interesting",
    "start": "192760",
    "end": "198760"
  },
  {
    "text": "to us and there's been a lot I know I've had a little bit of vacation and we've been in and out so we've had some time",
    "start": "198760",
    "end": "204599"
  },
  {
    "text": "to sort of scroll through uh scroll through Twitter and the blog post world",
    "start": "204599",
    "end": "209799"
  },
  {
    "text": "and all sorts of things to to find a few things the first thing that I wanted to",
    "start": "209799",
    "end": "215439"
  },
  {
    "text": "talk about and mention was GitHub co-pilot MH which was uh trained by open",
    "start": "215439",
    "end": "224319"
  },
  {
    "text": "AI so it's a collaboration between open Ai and GitHub but essentially the idea",
    "start": "224319",
    "end": "232200"
  },
  {
    "text": "with GitHub co-pilot as I understand it I haven't used it yet and I'm not on the team but the idea being that it's sort",
    "start": "232200",
    "end": "240200"
  },
  {
    "text": "of like an AI pair programmer where you have while you're writing code all the",
    "start": "240200",
    "end": "247640"
  },
  {
    "text": "sudden you can sort of have the AI pair programmer step in and you know complete",
    "start": "247640",
    "end": "253400"
  },
  {
    "text": "things for you or help structure your code put in some things that it's expecting that you're going to need and",
    "start": "253400",
    "end": "259560"
  },
  {
    "text": "that sort of thing thank goodness my God my God I've needed that for 30 years man",
    "start": "259560",
    "end": "266320"
  },
  {
    "text": "yeah yeah it's pretty clutch when you get that tab complete but this is a whole another level so like I say I",
    "start": "266320",
    "end": "274080"
  },
  {
    "text": "unfortunately haven't tried it yet I'm on the weight list if you go to Coop pilot. github.com you can sign up to be",
    "start": "274080",
    "end": "281800"
  },
  {
    "text": "on the wait list for it so it's not everybody yet and at least in my understanding it plugs into vs code",
    "start": "281800",
    "end": "289880"
  },
  {
    "text": "which I also don't use I do this is perfect oh I'm signing up right now maybe you could pass the weit list Chris",
    "start": "289880",
    "end": "296560"
  },
  {
    "text": "because one of the questions in the weit list thing was like how often do use vs code and I chose the option that was",
    "start": "296560",
    "end": "303840"
  },
  {
    "text": "like you know hardly ever or whatever rarely I think it was so if there's any",
    "start": "303840",
    "end": "310039"
  },
  {
    "text": "GitHub co-pilot people out there um I would like to try it out we'd also like to have you on the podcast but I would",
    "start": "310039",
    "end": "316600"
  },
  {
    "text": "love to try out the uh the system but yeah what are your general thoughts about this sort of thing in terms of you",
    "start": "316600",
    "end": "325120"
  },
  {
    "text": "know how it's different from other developer tools in the past and what it",
    "start": "325120",
    "end": "330319"
  },
  {
    "text": "means for us I think I will say you know both of us have been programming for many years and the thing about it is I",
    "start": "330319",
    "end": "338560"
  },
  {
    "text": "will dip in and out of programming depending on what I'm doing and what projects I have going and as I'm ramping",
    "start": "338560",
    "end": "345199"
  },
  {
    "text": "back in at a given moment I'm always trying to go I know what I'm trying to do but I can't remember the specific and",
    "start": "345199",
    "end": "350880"
  },
  {
    "text": "that kind of thing and that's where you know we both like go for instance I know we met in the go community and uh and",
    "start": "350880",
    "end": "356880"
  },
  {
    "text": "I'll stop doing go for a few months and then I want to do something I'll swing back in and write some and I'll be like",
    "start": "356880",
    "end": "362199"
  },
  {
    "text": "oh crap I can't remember how to do this one thing yeah it's like you're in Python world and then you want to put",
    "start": "362199",
    "end": "368000"
  },
  {
    "text": "that colon yeah for after your four I and something colon and then you forget",
    "start": "368000",
    "end": "374360"
  },
  {
    "text": "you're in go without the braces totally this is going to help me ramp back in",
    "start": "374360",
    "end": "379400"
  },
  {
    "text": "and I'm sure it'll help me you know after that as well but I've been waiting on this for my entire career",
    "start": "379400",
    "end": "384680"
  },
  {
    "text": "man I mean on the website they talk about it so it's powered by what they're",
    "start": "384680",
    "end": "390039"
  },
  {
    "text": "calling codex which is a a new they say AI system I assume that I I don't know",
    "start": "390039",
    "end": "396120"
  },
  {
    "text": "if it's one or more models but it seems to be consistent and to be honest this",
    "start": "396120",
    "end": "402720"
  },
  {
    "text": "is all things I don't know because I'm not part of the team I haven't interviewed any of them yet hopefully we",
    "start": "402720",
    "end": "408360"
  },
  {
    "text": "will be soon but exactly I'm assuming it's similar to a um you know these sort of large scale language models of recent",
    "start": "408360",
    "end": "416160"
  },
  {
    "text": "times that can like take in a lot of context and generate good language out of it",
    "start": "416160",
    "end": "422759"
  },
  {
    "text": "although they say you know it's more than autocomplete in the sense that it's understanding both you know it's not",
    "start": "422759",
    "end": "428840"
  },
  {
    "text": "just looking for an autocomplete for like the method on your object or something like that it's actually",
    "start": "428840",
    "end": "434879"
  },
  {
    "text": "looking at both your like your comments and your actual code structure the",
    "start": "434879",
    "end": "441240"
  },
  {
    "text": "function names all those sorts of things it understands more of that structure within your code yep more of the syntax",
    "start": "441240",
    "end": "448759"
  },
  {
    "text": "and so it can actually help you help you and and uh actually create some really",
    "start": "448759",
    "end": "454000"
  },
  {
    "text": "interesting things so all I've seen is other people using it and you know screencasts of people using it and it",
    "start": "454000",
    "end": "460199"
  },
  {
    "text": "seems like it does generate some really really interesting stuff and some",
    "start": "460199",
    "end": "465479"
  },
  {
    "text": "helpful things so I hope I get past the weight list me too but I know that uh",
    "start": "465479",
    "end": "472080"
  },
  {
    "text": "the there were some people that were saying some things about oh is this sort",
    "start": "472080",
    "end": "477240"
  },
  {
    "text": "of signal the end you know now our AI is creating code and which could create AI",
    "start": "477240",
    "end": "482560"
  },
  {
    "text": "which could create code which you know so that that's one thing scenario the",
    "start": "482560",
    "end": "487759"
  },
  {
    "text": "other thing is you know does this sort of spell the end of software engineering as we as we know it what are your",
    "start": "487759",
    "end": "494479"
  },
  {
    "text": "thoughts I have my own thoughts on those two things which you might guess from knowing me for a while but yeah my",
    "start": "494479",
    "end": "499720"
  },
  {
    "text": "thoughts well my thoughts and this has changed over time my thoughts have evolved even over the last few years as",
    "start": "499720",
    "end": "505080"
  },
  {
    "text": "we've been doing this show my thoughts are I am totally totally looking forward",
    "start": "505080",
    "end": "510680"
  },
  {
    "text": "to taking advantage of this I'm not terribly worried about whether or not a significant portion of code starts",
    "start": "510680",
    "end": "517599"
  },
  {
    "text": "coming from AI sources in the years ahead because if there's one thing I have figured out is that things are",
    "start": "517599",
    "end": "523839"
  },
  {
    "text": "constantly evolving and we should stop trying to keep things exactly as they are today you know our comfort level",
    "start": "523839",
    "end": "530760"
  },
  {
    "text": "right now so whatever the future is I'm going to dive into it I'm going to enjoy it I'm going to take advantage of that",
    "start": "530760",
    "end": "537600"
  },
  {
    "text": "you know there's the Paradigm I've recently learned not everyone's aware of this but there's kind of the idea the",
    "start": "537600",
    "end": "543399"
  },
  {
    "text": "meme of the lazy programmer and that is you know you create great tools because",
    "start": "543399",
    "end": "548519"
  },
  {
    "text": "in theory because you're a lazy programmer and you don't want to keep doing the same stuff over and over again so in the meme of the lazy programmer I",
    "start": "548519",
    "end": "555480"
  },
  {
    "text": "want this AI to help me so that I can get through my task and go relax and",
    "start": "555480",
    "end": "560519"
  },
  {
    "text": "spend time with my family and chill and produce amazing code along the way I'm all for it yeah what do you think are",
    "start": "560519",
    "end": "567680"
  },
  {
    "text": "the dangers of something like I mean they obviously released it in the sort of weight list type thing y probably",
    "start": "567680",
    "end": "573959"
  },
  {
    "text": "similar to like you know you remember when gpt3 was released for those that don't know that's a very large language model",
    "start": "573959",
    "end": "580920"
  },
  {
    "text": "it's very good at generating like very realistic looking language and there was",
    "start": "580920",
    "end": "585959"
  },
  {
    "text": "a lot of talk about dangers around that and they released it in a restrictive way what are your thoughts there I think",
    "start": "585959",
    "end": "593640"
  },
  {
    "text": "that a tool like this is not a strategic threat to us I think that this is a",
    "start": "593640",
    "end": "600680"
  },
  {
    "text": "tactical goodie that we get to use to make our lives better I'm going to worry",
    "start": "600680",
    "end": "606839"
  },
  {
    "text": "a whole lot more when someday in the future we're getting to AI capabilities",
    "start": "606839",
    "end": "613279"
  },
  {
    "text": "that are able to think divergently of all the different things and figure out",
    "start": "613279",
    "end": "618760"
  },
  {
    "text": "which one makes sense and what the long-term strategies are for those things because you know that's what human brains are still really quite good",
    "start": "618760",
    "end": "625640"
  },
  {
    "text": "at and we don't have any AI That's remotely approaching that",
    "start": "625640",
    "end": "630880"
  },
  {
    "text": "anywhere yeah it's very narrow in its capability right now so I'm not worried at all about this it you know someday I",
    "start": "630880",
    "end": "637560"
  },
  {
    "text": "may worry about a future thing but not this the danger if there is one probably",
    "start": "637560",
    "end": "642839"
  },
  {
    "text": "is in the fact in expectations of this right if you expect too much of this you",
    "start": "642839",
    "end": "648519"
  },
  {
    "text": "could just start having this fill in a bunch of things of course we know that the model is not going to perform 100%",
    "start": "648519",
    "end": "654760"
  },
  {
    "text": "right sure and so that could introduce very bad breaking ch into your code base",
    "start": "654760",
    "end": "660680"
  },
  {
    "text": "which are also very difficult to debug right because you didn't actually write every single line of code so I think",
    "start": "660680",
    "end": "668120"
  },
  {
    "text": "that it's not so much a danger of like job security or or something like that or um you know sentience or Singularity",
    "start": "668120",
    "end": "676760"
  },
  {
    "text": "it's it's more in the human computer interaction that happens when you",
    "start": "676760",
    "end": "682519"
  },
  {
    "text": "introduce something like this into your development cycle get Hub folks get a",
    "start": "682519",
    "end": "687880"
  },
  {
    "text": "right please get a right yeah yeah this thing work the other thing cool that I saw is it actually does so it lets you",
    "start": "687880",
    "end": "694360"
  },
  {
    "text": "write out comments and then it will sort of convert those comments into code so",
    "start": "694360",
    "end": "701040"
  },
  {
    "text": "you can sort of write comments like I'm going to do this and then I'm going to do this and then I'm going to do that and then it sort of generates the code",
    "start": "701040",
    "end": "707880"
  },
  {
    "text": "for that which is very different from autocomplete right it's more of like a",
    "start": "707880",
    "end": "713279"
  },
  {
    "text": "what's the right way to put it's more of like a machine translation type problem I love the idea I'm going to create a",
    "start": "713279",
    "end": "718320"
  },
  {
    "text": "development branch to go try an idea and we can try that out and if it turns out to not work then I'll trash the branch",
    "start": "718320",
    "end": "725279"
  },
  {
    "text": "and move back you know and try again or something but I'm all about using it I want them to take the pain of debugging",
    "start": "725279",
    "end": "732200"
  },
  {
    "text": "away from me I want them to fix that that would be good yeah the other interesting uh thing I forget when I",
    "start": "732200",
    "end": "739720"
  },
  {
    "text": "listened to a talk once and I really wish I could remember who gave the talk if some of",
    "start": "739720",
    "end": "745519"
  },
  {
    "text": "our audience remembers and they want to let me know where this talk happen in our slack Channel or LinkedIn or Twitter",
    "start": "745519",
    "end": "751600"
  },
  {
    "text": "or somewhere but there was a talk about like GitHub open source and like where",
    "start": "751600",
    "end": "757320"
  },
  {
    "text": "code lives and basically the thought with that was people underestimate how",
    "start": "757320",
    "end": "765440"
  },
  {
    "text": "much code is living in private repositories yeah that people don't have",
    "start": "765440",
    "end": "770800"
  },
  {
    "text": "access to do these sorts of projects right I'm sure that like GitHub I'm",
    "start": "770800",
    "end": "776000"
  },
  {
    "text": "assuming you know maybe this can be answered somewhere on the internet but I'm assuming that GitHub didn't give",
    "start": "776000",
    "end": "781519"
  },
  {
    "text": "open AI all private repositories on GitHub to train their model it seems",
    "start": "781519",
    "end": "787399"
  },
  {
    "text": "like that would be a sort of breach of some type of agreement so this was",
    "start": "787399",
    "end": "793079"
  },
  {
    "text": "probably built on top of like open- Source repositories I'm assuming of code",
    "start": "793079",
    "end": "800800"
  },
  {
    "text": "I'm pretty sure that they would Rec if nothing else their attorneys would be telling them don't go down that other",
    "start": "800800",
    "end": "807040"
  },
  {
    "text": "path I'm sure there was a lawyer involved some there must have been a lawyer somewhere but the result of that",
    "start": "807040",
    "end": "812440"
  },
  {
    "text": "is basically that if you're building some type of expansive system like this on top of quote all code which is really",
    "start": "812440",
    "end": "819560"
  },
  {
    "text": "not all code it's the code you can get access to right which is public code right it actually has a severe bias",
    "start": "819560",
    "end": "827480"
  },
  {
    "text": "because there's so much code living in private repositories because companies still primarily use private repositories",
    "start": "827480",
    "end": "836320"
  },
  {
    "text": "that actually you're sort of missing a bit of the goodness of like code that is",
    "start": "836320",
    "end": "842800"
  },
  {
    "text": "actually robust and supported and you're biased a little bit towards like",
    "start": "842800",
    "end": "848000"
  },
  {
    "text": "people's side projects that they post on GitHub and you know maybe don't actually",
    "start": "848000",
    "end": "853800"
  },
  {
    "text": "work that well they talked a little bit about bias and like how the coverage of languages like does this work for Python",
    "start": "853800",
    "end": "860759"
  },
  {
    "text": "and go and JavaScript and whatever so I totally get that but I think also there's another relevant side of the",
    "start": "860759",
    "end": "866320"
  },
  {
    "text": "bias here which is you know what code is driving these suggestions in my",
    "start": "866320",
    "end": "871519"
  },
  {
    "text": "experience you know I would love to have that code be dri or those suggestions and that generated stuff be driven by",
    "start": "871519",
    "end": "879240"
  },
  {
    "text": "good quality code and bias towards that rather than sort of the bulk of code",
    "start": "879240",
    "end": "885560"
  },
  {
    "text": "that's out there that's maybe not as good of quality so I'm wondering if there's some type of filtering that went",
    "start": "885560",
    "end": "891040"
  },
  {
    "text": "on to figure out hey what is the quality code to train this model on there are a lot of static tools for code quality",
    "start": "891040",
    "end": "897839"
  },
  {
    "text": "maybe I'm hoping maybe they biased it by only selecting those that did very well",
    "start": "897839",
    "end": "903079"
  },
  {
    "text": "in static code analysis and then use those as Source yeah yeah I don't know I",
    "start": "903079",
    "end": "908120"
  },
  {
    "text": "hope that we can get someone from there on the show and probably some of these questions are answered in their",
    "start": "908120",
    "end": "914199"
  },
  {
    "text": "frequently answered questions on their website the co-pilot website which we'll point you to in our show notes so there",
    "start": "914199",
    "end": "920000"
  },
  {
    "text": "you go go ahead and check that out and let us know what you [Music]",
    "start": "920000",
    "end": "927819"
  },
  {
    "text": "think [Music]",
    "start": "927959",
    "end": "933319"
  },
  {
    "text": "this episode is brought to you by snowplow analytics snowplow is the behavioral data management platform for",
    "start": "933319",
    "end": "939880"
  },
  {
    "text": "data teams maximize the value of your behavioral data using snowplow insights",
    "start": "939880",
    "end": "945319"
  },
  {
    "text": "a managed data platform that's built on leading open source Tech leveraged by tens of thousands of users capture and",
    "start": "945319",
    "end": "952959"
  },
  {
    "text": "process high quality behavioral data from all your platforms and your products and deliver that data to your Cloud destination of chice when",
    "start": "952959",
    "end": "959720"
  },
  {
    "text": "marketing needs to make data informed decisions when product needs Next Level understanding and when analytics needs",
    "start": "959720",
    "end": "965639"
  },
  {
    "text": "rich and accurate data snowplow is a solution for data teams who want to manage the collection processing and",
    "start": "965639",
    "end": "971639"
  },
  {
    "text": "warehousing of data across all their platforms and products get started and experience snowplow data for yourself at",
    "start": "971639",
    "end": "977440"
  },
  {
    "text": "snowplow analytics.com again snowplow analytics.com",
    "start": "977440",
    "end": "983060"
  },
  {
    "text": "[Music]",
    "start": "983060",
    "end": "1002490"
  },
  {
    "text": "okay Chris you pointed me to an article this must have been one of the ones that you saw in past couple weeks I think",
    "start": "1003720",
    "end": "1009600"
  },
  {
    "text": "it's a very recent article it is that you wanted to talk through and I I thought it had an interesting Concept in",
    "start": "1009600",
    "end": "1015079"
  },
  {
    "text": "that it talks through a lot of the you know looking back back on the progression of AI systems and how those",
    "start": "1015079",
    "end": "1022880"
  },
  {
    "text": "have changed over time leading up to deep learning and maybe what's beyond and that's from Yan laon Jeffrey Hinton",
    "start": "1022880",
    "end": "1030079"
  },
  {
    "text": "and uh Yoshua benio and a recent ACM article in the communications of the ACM",
    "start": "1030079",
    "end": "1037360"
  },
  {
    "text": "from July from this month of 2021 yeah um which we'll link that in the show notes for people to find it but what",
    "start": "1037360",
    "end": "1043839"
  },
  {
    "text": "what struck you about this article that that stood out to you well before we even dive into the article self it's",
    "start": "1043839",
    "end": "1049919"
  },
  {
    "text": "notable that those three Pioneers I mean they're often referred to as the Godfathers of deep learning or the",
    "start": "1049919",
    "end": "1056039"
  },
  {
    "text": "Godfathers of AI and the the three of them together received the touring award",
    "start": "1056039",
    "end": "1061400"
  },
  {
    "text": "in 2018 you know and these days just benio is at the Montreal Institute for",
    "start": "1061400",
    "end": "1067440"
  },
  {
    "text": "learning algorithms Jeffrey Hinton is at the University of Toronto as and Google",
    "start": "1067440",
    "end": "1073000"
  },
  {
    "text": "and Yan laon is the chief AI scientist at Facebook you can't become a bigger luminary than these three in the AI",
    "start": "1073000",
    "end": "1079280"
  },
  {
    "text": "world yeah and the for those that don't know the touring award it's sort of like a Nobel Prize type award yes but in the",
    "start": "1079280",
    "end": "1086840"
  },
  {
    "text": "sort of computing world it is huge and it is given out to these three and this is being done by the Association for",
    "start": "1086840",
    "end": "1094600"
  },
  {
    "text": "computing Machinery ACM and if you're not familiar with that it is one of the big giant original Computing",
    "start": "1094600",
    "end": "1102360"
  },
  {
    "text": "associations you know maybe you know certainly among the most prestigious in the world if not the most prestigious",
    "start": "1102360",
    "end": "1107880"
  },
  {
    "text": "yeah you'll see a lot of papers published in journals and you know events and other things and the title is",
    "start": "1107880",
    "end": "1115360"
  },
  {
    "text": "just deep learning for AI it's a very generic title but yeah when you sent it to me I'm like I don't I don't know if",
    "start": "1115360",
    "end": "1122520"
  },
  {
    "text": "this seems interesting from the title um you know but then I saw the authors and I like you know so you got to read it it",
    "start": "1122520",
    "end": "1129600"
  },
  {
    "text": "was interesting because of where they're going so there essentially there's a lot of",
    "start": "1129600",
    "end": "1135760"
  },
  {
    "text": "conversation as we record this first of all this is a very new article this is from this month as we record this we are",
    "start": "1135760",
    "end": "1141799"
  },
  {
    "text": "in early July 2021 as we record this today and this is a July 2021 article by",
    "start": "1141799",
    "end": "1149080"
  },
  {
    "text": "these three luminaries alog together for the communication in one of the biggest you know most important Computing",
    "start": "1149080",
    "end": "1155280"
  },
  {
    "text": "associations in the world and so the fact that they chose to publish together means they're trying to make a point and",
    "start": "1155280",
    "end": "1161679"
  },
  {
    "text": "they're standing together to do that and if we want to foreshadow a little bit of",
    "start": "1161679",
    "end": "1167120"
  },
  {
    "text": "where we're going on this but maybe not not go into the detail yet they're talking about the future of deep",
    "start": "1167120",
    "end": "1172200"
  },
  {
    "text": "learning and it finishes that way and they're talking about why they're still",
    "start": "1172200",
    "end": "1177320"
  },
  {
    "text": "very bullish about this is where the title comes in about deep learning as AI",
    "start": "1177320",
    "end": "1183720"
  },
  {
    "text": "or in the case of the title deep learning for AI because there's a lot of conversation going on right now in this",
    "start": "1183720",
    "end": "1190440"
  },
  {
    "text": "industry about whether deep learning is the right Paradigm for the future of AI",
    "start": "1190440",
    "end": "1197120"
  },
  {
    "text": "or should there be other approaches that do not fall within the realm of deep learning to carry us forward that might",
    "start": "1197120",
    "end": "1204440"
  },
  {
    "text": "be more closely associated with how a human brain operates because there are",
    "start": "1204440",
    "end": "1211159"
  },
  {
    "text": "very substantial differences in most deep learning models and how a human brain is believed to operate and that",
    "start": "1211159",
    "end": "1218200"
  },
  {
    "text": "research in Neuroscience has been has been developing and it's something that I'm very interested in I've been",
    "start": "1218200",
    "end": "1223919"
  },
  {
    "text": "following so I want to read this partly because I will admit from the very beginning I'm a huge skeptic I'm a giant",
    "start": "1223919",
    "end": "1232080"
  },
  {
    "text": "skeptic and I am certainly do not have the qualifications that these three gentlemen have I'm just your typical AI",
    "start": "1232080",
    "end": "1240200"
  },
  {
    "text": "career professional out there and we do this podcast but I'm struggling a little bit with that and that's one of the",
    "start": "1240200",
    "end": "1245880"
  },
  {
    "text": "reasons I wanted to bring it up and so I thought they start the article kind of doing a little bit of history of and we",
    "start": "1245880",
    "end": "1251919"
  },
  {
    "text": "don't want to take everyone deeply but they kind of Hit the high points about why why they're so bullish on deep",
    "start": "1251919",
    "end": "1258799"
  },
  {
    "text": "learning for the future of AI and so they start off going all the way back to",
    "start": "1258799",
    "end": "1264080"
  },
  {
    "text": "a section that's from hand-coded symbolic Expressions to learn distributed representations and you know",
    "start": "1264080",
    "end": "1269679"
  },
  {
    "text": "Daniel do you want to take a stab at you know kind of most people probably in",
    "start": "1269679",
    "end": "1274720"
  },
  {
    "text": "this industry don't really know what symbolic Expressions even are yeah I mean to be honest it's not really an",
    "start": "1274720",
    "end": "1281400"
  },
  {
    "text": "area that I even have a lot of experience with when I think so my sort",
    "start": "1281400",
    "end": "1286640"
  },
  {
    "text": "of introduction to or when I first started hearing those words was when I was using the some people recognize a",
    "start": "1286640",
    "end": "1293880"
  },
  {
    "text": "computer program called Mathematica yeah which is like a I think at some point they they were talking about you know",
    "start": "1293880",
    "end": "1300840"
  },
  {
    "text": "symbolic computations and that sort of thing that are done on uh symbolic",
    "start": "1300840",
    "end": "1306000"
  },
  {
    "text": "equations but uh yeah I don't think we should actually spend much time on it because it's mostly thought by most",
    "start": "1306000",
    "end": "1312039"
  },
  {
    "text": "people there are exceptions as something from the past that we should not return to and there going to be people that are",
    "start": "1312039",
    "end": "1318200"
  },
  {
    "text": "just screaming at me right now that I just said that out loud but I'm talking about if you survey you know a 100",
    "start": "1318200",
    "end": "1324720"
  },
  {
    "text": "people in the room there might be two or three that still believe in that but the majority of them aren't and to give you",
    "start": "1324720",
    "end": "1330919"
  },
  {
    "text": "a sense of this since I'm very open about my age I'm I'm 50 and my mother",
    "start": "1330919",
    "end": "1336440"
  },
  {
    "text": "and father when I was a little kid they were engaged in symbolic AI back in the",
    "start": "1336440",
    "end": "1342720"
  },
  {
    "text": "1980s at the Georgia Institute of Technology Georgia Tech and that is a",
    "start": "1342720",
    "end": "1347960"
  },
  {
    "text": "pathway that has largely Fallen to the Wayside and so they kind of talk about",
    "start": "1347960",
    "end": "1353159"
  },
  {
    "text": "you know going through that they talk about some of the benefits of symbolic approach in the article they also talk",
    "start": "1353159",
    "end": "1360159"
  },
  {
    "text": "about why why it moved on to deep learning and and it's important to notice that you know people associate",
    "start": "1360159",
    "end": "1366640"
  },
  {
    "text": "neural networks with deep learning but neural networks predate the concept of deep learning by decades right they are",
    "start": "1366640",
    "end": "1373720"
  },
  {
    "text": "older than I am even and so there was already some advancement in limited",
    "start": "1373720",
    "end": "1378840"
  },
  {
    "text": "capacity in neural networks in the ' 80s and 90s 1980s and 1990s then we had yet",
    "start": "1378840",
    "end": "1384120"
  },
  {
    "text": "another nuclear winter and finally came about with the rise of of deep learning and you know or AI winter yeah and I'm",
    "start": "1384120",
    "end": "1390880"
  },
  {
    "text": "just curious maybe nuclear winter too exactly for AI it was a bit of that for",
    "start": "1390880",
    "end": "1397760"
  },
  {
    "text": "a while I mean that's really where the term came from then the rise of deep learning came about yeah I think I've",
    "start": "1397760",
    "end": "1403039"
  },
  {
    "text": "talked about this on the show before but in my PhD the part of my uh original",
    "start": "1403039",
    "end": "1409360"
  },
  {
    "text": "work was in what's called abono modeling in physics that means you're just using",
    "start": "1409360",
    "end": "1414559"
  },
  {
    "text": "the expressions and symbols to model a physical system even if you're doing",
    "start": "1414559",
    "end": "1420720"
  },
  {
    "text": "sort of like computational integration or something like that numerical",
    "start": "1420720",
    "end": "1425880"
  },
  {
    "text": "integration you're still you don't have fitted parameters it was probably around the same time people started doing this",
    "start": "1425880",
    "end": "1433159"
  },
  {
    "text": "very expansive fitting of parameters in our space for doing modeling of atoms and mo molecules and I sort of view it",
    "start": "1433159",
    "end": "1441240"
  },
  {
    "text": "as a similar process to that where part of the reason why that became useful is",
    "start": "1441240",
    "end": "1448480"
  },
  {
    "text": "also because of the emergence of certain Computing right and they do talk in the article about the emergence of",
    "start": "1448480",
    "end": "1455919"
  },
  {
    "text": "gpus and whereas in the past you could have a very shallow neural network",
    "start": "1455919",
    "end": "1461159"
  },
  {
    "text": "meaning that and for those that are maybe newer into this when when we're talking about shallow neural network we",
    "start": "1461159",
    "end": "1467120"
  },
  {
    "text": "mean that the neural network is composed of nodes each of those nodes takes in a",
    "start": "1467120",
    "end": "1472360"
  },
  {
    "text": "series of inputs they're added together with coefficients called weights and",
    "start": "1472360",
    "end": "1477760"
  },
  {
    "text": "maybe another parameter called a bias those are added together and maybe another function is applied called an",
    "start": "1477760",
    "end": "1484559"
  },
  {
    "text": "activation function and that itself is it's kind of just like input output it's like a function but then you can start",
    "start": "1484559",
    "end": "1490559"
  },
  {
    "text": "layering these functions together right and have like sub functions of functions",
    "start": "1490559",
    "end": "1495640"
  },
  {
    "text": "and so shallow networks don't have that many of these layers and so when they're talking about they usually have like one",
    "start": "1495640",
    "end": "1502279"
  },
  {
    "text": "hidden layer in them yeah exactly so maybe an input and then a a hidden layer",
    "start": "1502279",
    "end": "1507640"
  },
  {
    "text": "of these subf functions and then the output an internal layer a hidden layer is not the input layer and not the",
    "start": "1507640",
    "end": "1515000"
  },
  {
    "text": "output layer it would be any other layer and there used to be only one really because it was limited by computation",
    "start": "1515000",
    "end": "1522200"
  },
  {
    "text": "you can already imagine like even if I have 10 of those functions and there's you know three or four parameters",
    "start": "1522200",
    "end": "1528919"
  },
  {
    "text": "each in a very small type of scenario that starts actually to become a l you",
    "start": "1528919",
    "end": "1534320"
  },
  {
    "text": "know a reasonable number of parameters to especially if I have to fit those so I'm doing some model fitting and I fit",
    "start": "1534320",
    "end": "1541039"
  },
  {
    "text": "those on data you know depending on what computers you had available that could take a while but then with the emergence",
    "start": "1541039",
    "end": "1549640"
  },
  {
    "text": "of gpus now people were saying well we're not limited by that compute anymore let's start making it deeper so",
    "start": "1549640",
    "end": "1556880"
  },
  {
    "text": "adding more and more layers one of the things that I thought was interesting was that they talk about hey",
    "start": "1556880",
    "end": "1562120"
  },
  {
    "text": "it's partly that the Deep learning had success because it was deep there was",
    "start": "1562120",
    "end": "1567279"
  },
  {
    "text": "the depth of these layers it's really more than that let's see if I can find it here the way they phrase it is that",
    "start": "1567279",
    "end": "1572840"
  },
  {
    "text": "they exploit a particular form of composability in which features in one layer are combined in many different",
    "start": "1572840",
    "end": "1579600"
  },
  {
    "text": "ways to create more abstract features in a next layer and so you get more benefit",
    "start": "1579600",
    "end": "1586039"
  },
  {
    "text": "than just like piling things together you actually get a lot of sort of layered complexity that can be model it",
    "start": "1586039",
    "end": "1592520"
  },
  {
    "text": "creates more more capability for abstraction so you get abstractions built on abstractions built on",
    "start": "1592520",
    "end": "1597960"
  },
  {
    "text": "abstractions so really the deeper it goes the more capable the network is the",
    "start": "1597960",
    "end": "1605399"
  },
  {
    "text": "more accurate it becomes and they talk about that all the way to the end of the article is they talk about the fact that",
    "start": "1605399",
    "end": "1611360"
  },
  {
    "text": "you can literally scale it in terms of depth and width and get more and more",
    "start": "1611360",
    "end": "1617799"
  },
  {
    "text": "Improv Improvement in what you're trying to get and that's what we've been doing for years with gpus really advancing a",
    "start": "1617799",
    "end": "1624559"
  },
  {
    "text": "lot in recent years we have seen the fact that we have gotten these giant",
    "start": "1624559",
    "end": "1629799"
  },
  {
    "text": "performance increases which has fueled this whole AI you know movement the modern movement in the 2000s and Beyond",
    "start": "1629799",
    "end": "1636919"
  },
  {
    "text": "and so I think that's important is the question is is that scalability alone",
    "start": "1636919",
    "end": "1642320"
  },
  {
    "text": "and there's other aspects which we'll talk about in a second but that scalability has driven the whole",
    "start": "1642320",
    "end": "1647440"
  },
  {
    "text": "movement as going back to my original point that doesn't mean it's exactly like what a human brain looks like is as uh you know",
    "start": "1647440",
    "end": "1655080"
  },
  {
    "text": "does it matter I guess is what they would be suggesting yeah yeah I I don't know the whole human brain thing I guess",
    "start": "1655080",
    "end": "1661600"
  },
  {
    "text": "I don't think about too much I think more about the practicalities of does this solve a task or a problem maybe",
    "start": "1661600",
    "end": "1668760"
  },
  {
    "text": "it's because I don't know that much about human brains myself that I'm not really that concerned of how well this",
    "start": "1668760",
    "end": "1674960"
  },
  {
    "text": "models a human brain I'm more concerned with using my human brain to apply these tools to solve tasks yeah I don't know",
    "start": "1674960",
    "end": "1683039"
  },
  {
    "text": "if you have any thoughts in terms of is it even useful to wonder how closely",
    "start": "1683039",
    "end": "1689320"
  },
  {
    "text": "these things model the human brain so I think that's where we're going with the argument is I think that's the question",
    "start": "1689320",
    "end": "1696240"
  },
  {
    "text": "it's not how close it is it's does that matter in the long run as we explore this because no one who's in this",
    "start": "1696240",
    "end": "1702799"
  },
  {
    "text": "industry can deny the incredible utility of neural networks of deep neural",
    "start": "1702799",
    "end": "1707880"
  },
  {
    "text": "network ORS and all of these architectures which we can talk about in a moment along the way but it's still",
    "start": "1707880",
    "end": "1714399"
  },
  {
    "text": "not the same thing and we don't yet have human brain level capabilities and most",
    "start": "1714399",
    "end": "1719559"
  },
  {
    "text": "people would say we're not close to that yet so the question is to get close to it do we can we do it along this path or",
    "start": "1719559",
    "end": "1727279"
  },
  {
    "text": "do we need a completely separate path so as we as we look at the question",
    "start": "1727279",
    "end": "1734480"
  },
  {
    "text": "going forward you know let's take a brief moment and just talk about some of the high levels of of where things have",
    "start": "1734480",
    "end": "1740880"
  },
  {
    "text": "gone we've gotten different architectures that have been evolved out of the the vanilla neural network such",
    "start": "1740880",
    "end": "1746000"
  },
  {
    "text": "as generative adversarial networks and we have natural language processing is using all sorts of deep learning",
    "start": "1746000",
    "end": "1752039"
  },
  {
    "text": "capability we have computer vision using convolutional neural networks and capsule networks and all of these other",
    "start": "1752039",
    "end": "1759159"
  },
  {
    "text": "capabilities in the NLP side we've had a whole bunch of episodes on Transformers which maybe could you take a moment I",
    "start": "1759159",
    "end": "1765559"
  },
  {
    "text": "know you work with them a lot and kind of explain what Transformers are CU they've had a huge impact on on where",
    "start": "1765559",
    "end": "1771080"
  },
  {
    "text": "NLP has gone in recent years yeah I think maybe it's um it's twofold part of",
    "start": "1771080",
    "end": "1776279"
  },
  {
    "text": "the utility of these large Transformer models is in the fact that they utilize",
    "start": "1776279",
    "end": "1782000"
  },
  {
    "text": "this unsupervised pre-training technique which they talk about in the article and",
    "start": "1782000",
    "end": "1787480"
  },
  {
    "text": "part of it is the actual you know architecture that's used and they also talk about convolutional architectures",
    "start": "1787480",
    "end": "1793000"
  },
  {
    "text": "and where those are useful for other things but the unsupervised pre-training bit I think",
    "start": "1793000",
    "end": "1799039"
  },
  {
    "text": "that idea goes beyond Transformers in that the great thing that was discovered",
    "start": "1799039",
    "end": "1806200"
  },
  {
    "text": "was you could create these reusable almost modules or components",
    "start": "1806200",
    "end": "1812240"
  },
  {
    "text": "that took your input data and extracted out features that were relevant for",
    "start": "1812240",
    "end": "1818080"
  },
  {
    "text": "particular kinds of problems so let's say that we're doing an object you know some type of image-based modeling like",
    "start": "1818080",
    "end": "1825320"
  },
  {
    "text": "object detection or classification or something like that if you train one",
    "start": "1825320",
    "end": "1832760"
  },
  {
    "text": "model with a whole bunch of layers which are typically composed of convolutional layers and maybe a very small number of",
    "start": "1832760",
    "end": "1840039"
  },
  {
    "text": "layers at the end that do the actual task related thing like the classification then what you end up is",
    "start": "1840039",
    "end": "1845880"
  },
  {
    "text": "with a network or a model where the bulk of that model the work of that model is",
    "start": "1845880",
    "end": "1851000"
  },
  {
    "text": "taking the input data and creating this set of features or extracting this set",
    "start": "1851000",
    "end": "1856480"
  },
  {
    "text": "of features from the input data dat it's only that last little bit that's used for the classification and so what this",
    "start": "1856480",
    "end": "1863799"
  },
  {
    "text": "ends up creating is if you chop off that last bit of the model the part that does",
    "start": "1863799",
    "end": "1869399"
  },
  {
    "text": "the task that you want in the case of image based models maybe that's classification or recognition in the",
    "start": "1869399",
    "end": "1875600"
  },
  {
    "text": "case of text based models maybe that's sentiment analysis or named entity recognition or something what you end up",
    "start": "1875600",
    "end": "1882080"
  },
  {
    "text": "with is this pre-trained model that you can actually transfer to a whole VAR",
    "start": "1882080",
    "end": "1887919"
  },
  {
    "text": "variety of tasks and I think that that's the big you know one of the big boosts in recent times is you know someone like",
    "start": "1887919",
    "end": "1895279"
  },
  {
    "text": "Google or a large player in the space will release this large pre-trained",
    "start": "1895279",
    "end": "1900519"
  },
  {
    "text": "model that is generalizable to a wide variety of problems in a certain space maybe it's",
    "start": "1900519",
    "end": "1907039"
  },
  {
    "text": "text problems or maybe it's image-based problems or maybe it's speech based problems and this allows you to be much",
    "start": "1907039",
    "end": "1914240"
  },
  {
    "text": "more efficient in terms of the actual usage of AI techniques because you sort of stand on",
    "start": "1914240",
    "end": "1920559"
  },
  {
    "text": "the shoulders of giants in that way and so they do specifically call out this method of unsupervised pre-training as",
    "start": "1920559",
    "end": "1927880"
  },
  {
    "text": "something that's driven a lot of success and we've seen more and more of that in recent years I mean when you and I",
    "start": "1927880",
    "end": "1934000"
  },
  {
    "text": "started this podcast about three years ago supervised training was still by far",
    "start": "1934000",
    "end": "1939480"
  },
  {
    "text": "the dominant mechanism and we have watched this over this time period we have watched the steady progression into",
    "start": "1939480",
    "end": "1946279"
  },
  {
    "text": "unsupervised training approaches because a it scales much better than supervised",
    "start": "1946279",
    "end": "1952159"
  },
  {
    "text": "training does if you have to have supervised training meaning that your training data has labels associated with",
    "start": "1952159",
    "end": "1958919"
  },
  {
    "text": "them about what is right and wrong and as you train your network you are going off those labels that means you got to",
    "start": "1958919",
    "end": "1964320"
  },
  {
    "text": "label everything we used to have episodes on that early on about the problems and the challenges how do you",
    "start": "1964320",
    "end": "1970240"
  },
  {
    "text": "do that and the science of this has evolved over time to recognize the the value of unsupervised plus when you",
    "start": "1970240",
    "end": "1977480"
  },
  {
    "text": "think about about it a lot of you know just to kind of keep our eye on this kind of comparison with human brain we",
    "start": "1977480",
    "end": "1983880"
  },
  {
    "text": "do a lot of unsupervised training of different types in our own brain and so that is and often do it without many",
    "start": "1983880",
    "end": "1991399"
  },
  {
    "text": "iterations as well so as we are learning more about deep learning and applying these new techniques we are to some",
    "start": "1991399",
    "end": "1998519"
  },
  {
    "text": "degree taking even where they're not the same we're taking inspiration from the human brain and recognizing that",
    "start": "1998519",
    "end": "2004840"
  },
  {
    "text": "unsupervised training is where most of the future is going to be yeah and come coming back to the sort of Transformer",
    "start": "2004840",
    "end": "2011679"
  },
  {
    "text": "thing which people might most commonly associate with NLP or language-based",
    "start": "2011679",
    "end": "2017480"
  },
  {
    "text": "modeling has actually turned out to be a very significant Advance because it's",
    "start": "2017480",
    "end": "2024120"
  },
  {
    "text": "applicable beyond that as well because in some ways it's like moving from a",
    "start": "2024120",
    "end": "2031000"
  },
  {
    "text": "sequential or you know very uh strictly connected type of network into something",
    "start": "2031000",
    "end": "2037360"
  },
  {
    "text": "that's Dynamic and so how that happens is like this in sort of traditional ways",
    "start": "2037360",
    "end": "2044120"
  },
  {
    "text": "of processing text you see text as a sequence right so there's the first word",
    "start": "2044120",
    "end": "2049960"
  },
  {
    "text": "and the second word and the third word and the fourth word and so that's when people started thinking about recurrent",
    "start": "2049960",
    "end": "2055878"
  },
  {
    "text": "layers which process things in a sequence now you could have bir directional layers so there's like",
    "start": "2055879",
    "end": "2061878"
  },
  {
    "text": "bidirectional lstm layers which are very popular and that looks at your sequence",
    "start": "2061879",
    "end": "2066919"
  },
  {
    "text": "of inputs from left to right and right to left and you can sort of do really you know creative things like that in",
    "start": "2066919",
    "end": "2074599"
  },
  {
    "text": "terms of how your model is actually processing the data that goes through it however like as your sequence gets",
    "start": "2074599",
    "end": "2081839"
  },
  {
    "text": "longer and longer and as your space of possible inputs like your vocabulary",
    "start": "2081839",
    "end": "2088079"
  },
  {
    "text": "gets larger and larger which is what was happening with these larger and larger language models right we had tons of",
    "start": "2088079",
    "end": "2094118"
  },
  {
    "text": "data they're training on very large vocabularies they're looking at larger",
    "start": "2094119",
    "end": "2099240"
  },
  {
    "text": "chunks of text to process um what they realized is this sort of sequential",
    "start": "2099240",
    "end": "2105320"
  },
  {
    "text": "processing I mean one thing is it's just not that computationally favorable if",
    "start": "2105320",
    "end": "2110760"
  },
  {
    "text": "you start looking at all of the sort of spaghetti of connections that happens as you kind of add in by lsdm or recurrent",
    "start": "2110760",
    "end": "2119640"
  },
  {
    "text": "layers it gets very complicated and so what happened was partly out of",
    "start": "2119640",
    "end": "2125480"
  },
  {
    "text": "computational reasons people started looking at these layers called uh attention or self attention or soft",
    "start": "2125480",
    "end": "2132480"
  },
  {
    "text": "attention layers where actually you take your set of inputs and you shove them",
    "start": "2132480",
    "end": "2138200"
  },
  {
    "text": "all at once into a layer and let the model decide which ones of the inputs",
    "start": "2138200",
    "end": "2144160"
  },
  {
    "text": "are important to pay attention to as it processes them through you still have",
    "start": "2144160",
    "end": "2149440"
  },
  {
    "text": "this sort of sequence of things and in some cases you want to also put in something indicative of the sequence",
    "start": "2149440",
    "end": "2155839"
  },
  {
    "text": "order but in general you can sort of the model can decide on the Fly which of the",
    "start": "2155839",
    "end": "2161200"
  },
  {
    "text": "inputs are important to pay attention to and if you start stacking these into",
    "start": "2161200",
    "end": "2166319"
  },
  {
    "text": "what they call multi-headed attention then you sort of have a whole bunch of combinations of different inputs and",
    "start": "2166319",
    "end": "2173720"
  },
  {
    "text": "secondary inputs that the model can decide to pay attention to even if let's say it's paying attention to a word that",
    "start": "2173720",
    "end": "2180800"
  },
  {
    "text": "happened much earlier in the sequence but it knows that it's really important",
    "start": "2180800",
    "end": "2186240"
  },
  {
    "text": "to process that with words that occurred much later in the sequence and so that",
    "start": "2186240",
    "end": "2192119"
  },
  {
    "text": "sort of had a boom for NLP processing both computationally and because it had this",
    "start": "2192119",
    "end": "2199920"
  },
  {
    "text": "attention mechanism but after that this sort of attention mechanism and Transformer architecture has been",
    "start": "2199920",
    "end": "2206319"
  },
  {
    "text": "applied in a whole bunch of different places with with images and with speech",
    "start": "2206319",
    "end": "2211359"
  },
  {
    "text": "and all sorts of things because it applies this way of looking at inputs in",
    "start": "2211359",
    "end": "2218119"
  },
  {
    "text": "a very Dynamic way which is actually significantly different from some of the",
    "start": "2218119",
    "end": "2223680"
  },
  {
    "text": "layers that have been used for a long time like convolutional and recurrent yeah and that and by the way that was a",
    "start": "2223680",
    "end": "2229640"
  },
  {
    "text": "great explanation of Transformers that was well it's not a full explanation but there there are really good uh",
    "start": "2229640",
    "end": "2235760"
  },
  {
    "text": "explanations out there that I can put in the show notes but I think it's that attention mechanism is really one of the",
    "start": "2235760",
    "end": "2241560"
  },
  {
    "text": "key things and they call out that attention mechanism in the article and then they get into and they cover a few",
    "start": "2241560",
    "end": "2246920"
  },
  {
    "text": "other things some of the architectures but with the time left you know then they get into the future of deep learning and kind of talk about you know",
    "start": "2246920",
    "end": "2254240"
  },
  {
    "text": "where to go and they start that conversation kind of going back to that scalability thing that we talked about",
    "start": "2254240",
    "end": "2260000"
  },
  {
    "text": "at the beginning of this and that was they specifically call out the language model gpt3 which we have talked quite a",
    "start": "2260000",
    "end": "2266319"
  },
  {
    "text": "bit about over time it has 175 billion parameters and they point",
    "start": "2266319",
    "end": "2271760"
  },
  {
    "text": "out that that's still tiny compared to the snaps is in the in a human brain but that it generates notably better text",
    "start": "2271760",
    "end": "2278520"
  },
  {
    "text": "than gpt2 that only has only 1.5 billion parameters and they also talk about some",
    "start": "2278520",
    "end": "2285200"
  },
  {
    "text": "chatbots but their point is that you're getting better performance simply by",
    "start": "2285200",
    "end": "2290520"
  },
  {
    "text": "scaling up in these areas and then they go on to really talk about areas for",
    "start": "2290520",
    "end": "2296119"
  },
  {
    "text": "improvement and the three things that I'd like to throw out for us to talk about that they mention is they say supervise learning requires too much",
    "start": "2296119",
    "end": "2302920"
  },
  {
    "text": "label data and model free reinforcement learning requires too many trials",
    "start": "2302920",
    "end": "2308200"
  },
  {
    "text": "and that we which I think is a really good point here humans seem to be able to generalize well with far less",
    "start": "2308200",
    "end": "2314440"
  },
  {
    "text": "experience which we talked about a few minutes ago is that you and I can go decide to learn something new depending",
    "start": "2314440",
    "end": "2320599"
  },
  {
    "text": "on the complexity of the thing and we might pick it up almost immediately you know without having to go through thousands of iterations they also",
    "start": "2320599",
    "end": "2327520"
  },
  {
    "text": "mentioned that current systems are not as robust to changes as humans are because we we humans adapt so quickly to",
    "start": "2327520",
    "end": "2334920"
  },
  {
    "text": "changes with very few examples which is more of the same yeah I think that that second one is definitely a big one in my mind in terms",
    "start": "2334920",
    "end": "2342119"
  },
  {
    "text": "of robustness because that both helps protect yourself in terms of your model",
    "start": "2342119",
    "end": "2349040"
  },
  {
    "text": "behaving well even with unexpected data but it also helps in terms of you know",
    "start": "2349040",
    "end": "2356079"
  },
  {
    "text": "you being able to utilize other people's models that might be adaptable with very few examples to new scenarios and",
    "start": "2356079",
    "end": "2362920"
  },
  {
    "text": "there's been a lot of work with this I know we had an episode once where we talked through the open AI effort where",
    "start": "2362920",
    "end": "2367960"
  },
  {
    "text": "they had the robot hand that was moving around the Rubik's Cube yeah and they sort of perturbed that system with a",
    "start": "2367960",
    "end": "2374319"
  },
  {
    "text": "variety of things like a stuffed giraffe and they hit the hand and other things and that was in the context of",
    "start": "2374319",
    "end": "2381040"
  },
  {
    "text": "reinforcement learning and also some sort of multimodal inputs and they had",
    "start": "2381040",
    "end": "2387200"
  },
  {
    "text": "various ways there where they specifically called out their efforts to improve the robustness of the system and",
    "start": "2387200",
    "end": "2394839"
  },
  {
    "text": "I think that that's one area that I probably I think about how well my",
    "start": "2394839",
    "end": "2401000"
  },
  {
    "text": "models are doing on the test sets that I've organized and I also you know I try",
    "start": "2401000",
    "end": "2407000"
  },
  {
    "text": "to look at hey how is my model doing over time is it drifting you know in production those are all good things to",
    "start": "2407000",
    "end": "2413160"
  },
  {
    "text": "think about but this idea of like how to educate people on how to make their",
    "start": "2413160",
    "end": "2419400"
  },
  {
    "text": "models more robust and maybe use adversarial examples and other things that's one area that practitioners could",
    "start": "2419400",
    "end": "2426599"
  },
  {
    "text": "in particular really grow in that's true and by the way the third item they talked about even if we move on is they",
    "start": "2426599",
    "end": "2434000"
  },
  {
    "text": "have this idea of system one task versus system two tasks and basically kind of the simpler things that we were starting",
    "start": "2434000",
    "end": "2440079"
  },
  {
    "text": "to do with deep learning are called system one things like perception and you know these kind of basic level",
    "start": "2440079",
    "end": "2445839"
  },
  {
    "text": "building blocks for putting together things but there is the concept of system two tasks which are more complex",
    "start": "2445839",
    "end": "2452880"
  },
  {
    "text": "and things that you know in short we might often associate still with human brain and less with deep learning it's",
    "start": "2452880",
    "end": "2459520"
  },
  {
    "text": "more aspirational on the Deep learning side and they acknowledge that we're in the infancy on that and that's just wanted to point out what the third one",
    "start": "2459520",
    "end": "2465640"
  },
  {
    "text": "was they kind of go into what needs to be improved and the robustness is one of those areas that you just talked about",
    "start": "2465640",
    "end": "2472319"
  },
  {
    "text": "that they really address upfront and they talk about kind of how to do that do you have any any more comments based",
    "start": "2472319",
    "end": "2478920"
  },
  {
    "text": "on that or I don't think so I I mean I'm drawn back to the conversation we had um",
    "start": "2478920",
    "end": "2485480"
  },
  {
    "text": "with the developer of uh Tex attack which is was a package where he was",
    "start": "2485480",
    "end": "2491560"
  },
  {
    "text": "trying to bring this idea of robustness and adversarial examples to text based",
    "start": "2491560",
    "end": "2497560"
  },
  {
    "text": "problems where people don't really think about that as much they might think about it more in robotics and yeah",
    "start": "2497560",
    "end": "2503119"
  },
  {
    "text": "perception type problems that was a good conversation that was a good conversation we'll we'll link it in the",
    "start": "2503119",
    "end": "2508960"
  },
  {
    "text": "show notes but I think that my thought was like how do I start integrating my",
    "start": "2508960",
    "end": "2514400"
  },
  {
    "text": "thought process around robustness and adversarial examples into my own workflow I think that's what I'm trying",
    "start": "2514400",
    "end": "2520480"
  },
  {
    "text": "to cuz often I'm just so focused on kind of getting through the problem getting",
    "start": "2520480",
    "end": "2525720"
  },
  {
    "text": "you know reaching a certain level of performance and yes I do want to go back and do updates to the model and see how",
    "start": "2525720",
    "end": "2532680"
  },
  {
    "text": "it's performing but it's not really like before I release the model I've always",
    "start": "2532680",
    "end": "2538720"
  },
  {
    "text": "done a ton of work around you know robustness and probing the behavior of",
    "start": "2538720",
    "end": "2544079"
  },
  {
    "text": "my model um doing a lot of sort of minimal viability tests and perturbations to understand how it",
    "start": "2544079",
    "end": "2550440"
  },
  {
    "text": "behaves and maybe that's partly just the time that's allotted to me to solve a problem that has to be balanced I know",
    "start": "2550440",
    "end": "2557240"
  },
  {
    "text": "but we're almost to the end here of their article the next thing they have is one that I'm keenly interested in and",
    "start": "2557240",
    "end": "2563599"
  },
  {
    "text": "that is kind of alternative architectures so you know we took you know you described what a basic neural",
    "start": "2563599",
    "end": "2570760"
  },
  {
    "text": "network kind of looks like and how the neuron operates earlier in this conversation and I think it's key to",
    "start": "2570760",
    "end": "2577240"
  },
  {
    "text": "recognize that those hidden layers that you that you were discussing are kind of homogeneous they create these layers of",
    "start": "2577240",
    "end": "2584200"
  },
  {
    "text": "abstraction by having many many layers in there th thus the word deep but they are homogeneous in the way that they",
    "start": "2584200",
    "end": "2591440"
  },
  {
    "text": "operate and then we have the other all these other architectures that have evolved out of that or where people have",
    "start": "2591440",
    "end": "2597720"
  },
  {
    "text": "something in mind and they make a change to the architecture they may introduce you know the recurrent Pathways or",
    "start": "2597720",
    "end": "2604880"
  },
  {
    "text": "there's there's a whole bunch of different of tech techniques that they have introduced but it doesn't",
    "start": "2604880",
    "end": "2610000"
  },
  {
    "text": "fundamentally change that kind of Baseline architecture there and one of",
    "start": "2610000",
    "end": "2615319"
  },
  {
    "text": "the things in brain research these days is the idea of having groups of neurons that represent entities so they're not",
    "start": "2615319",
    "end": "2622240"
  },
  {
    "text": "homogeneous you have regions of neurons and synapses that they start as",
    "start": "2622240",
    "end": "2628480"
  },
  {
    "text": "homogeneous they start as a blank slate but as the brain develops they take on certain roles and they represent",
    "start": "2628480",
    "end": "2634960"
  },
  {
    "text": "entities and the relationships between entities and those and there's now a fair amount of work that's being done to",
    "start": "2634960",
    "end": "2641920"
  },
  {
    "text": "kind of connect artificial intelligence you know and I'm almost separating those",
    "start": "2641920",
    "end": "2647200"
  },
  {
    "text": "meaning that the neural network approaches back to these other possible architectures and so there is a lot of",
    "start": "2647200",
    "end": "2654160"
  },
  {
    "text": "discussion I'm quite fascinated with the way some of those are going and without foreshadowing we may have some guesss on",
    "start": "2654160",
    "end": "2660559"
  },
  {
    "text": "before long that will'll address some of those because it may be a turn along the pathway and I frankly think that that",
    "start": "2660559",
    "end": "2667680"
  },
  {
    "text": "specific issue is one of the reasons that these three gentlemen wrote this article at this point in time I think",
    "start": "2667680",
    "end": "2674119"
  },
  {
    "text": "cuz I really think that they are saying no no this this pathway that we're on is",
    "start": "2674119",
    "end": "2679200"
  },
  {
    "text": "the right path as opposed to making a a turn or taking an alternate path along the way so what do you think about that",
    "start": "2679200",
    "end": "2685640"
  },
  {
    "text": "what do you think about this homogeneous you know generating these networks as we have been doing out of this homogeneous",
    "start": "2685640",
    "end": "2691800"
  },
  {
    "text": "hidden layer collection versus considering other options for for neurons being grouped together to",
    "start": "2691800",
    "end": "2698160"
  },
  {
    "text": "represent entities and relationships I think it's consistent with other conversations we had I remember a brief",
    "start": "2698160",
    "end": "2705079"
  },
  {
    "text": "comment from William Falcon who was the creator of pie torch lightning on on his",
    "start": "2705079",
    "end": "2710160"
  },
  {
    "text": "episode with us he talked a little bit about how their pie torch lightning is structured in such a way s such you can",
    "start": "2710160",
    "end": "2716960"
  },
  {
    "text": "do research into these sort of composable pieces and modules yeah I'm excited to see what happens there I",
    "start": "2716960",
    "end": "2723040"
  },
  {
    "text": "think there will be additionally really interesting advances in in new AR or",
    "start": "2723040",
    "end": "2729400"
  },
  {
    "text": "architectures that process data in interesting patterns or structures like",
    "start": "2729400",
    "end": "2735359"
  },
  {
    "text": "graph neural networks that actually operate natively on different structures",
    "start": "2735359",
    "end": "2740559"
  },
  {
    "text": "of data rather than let's say just you know matrices or tabular data I think",
    "start": "2740559",
    "end": "2747440"
  },
  {
    "text": "that's another interesting area in addition to this sort of groupings of neurons and all of that but yeah I'm",
    "start": "2747440",
    "end": "2754319"
  },
  {
    "text": "excited to see where things go I definitely think that for one we have some exciting things to look forward to",
    "start": "2754319",
    "end": "2760079"
  },
  {
    "text": "and for two GitHub co-pilot isn't going to write all our codes so we'll get to code some of it",
    "start": "2760079",
    "end": "2766760"
  },
  {
    "text": "anyway so I'm excited about that I know in these episodes we normally like to share a u a Learning Resource I did want",
    "start": "2766760",
    "end": "2773920"
  },
  {
    "text": "to just share something real quick before we end I noticed this new book that came out in June called human in",
    "start": "2773920",
    "end": "2780880"
  },
  {
    "text": "the loop machine learning by Robert Monarch it seems to be getting quite good reception and I know I would like",
    "start": "2780880",
    "end": "2787520"
  },
  {
    "text": "to go through it it is from Manning which is great because it means that we",
    "start": "2787520",
    "end": "2794160"
  },
  {
    "text": "have a a discount code from Manning for our listeners so if you want 40% off",
    "start": "2794160",
    "end": "2800520"
  },
  {
    "text": "that's crazy 40% off that's pretty good that's a good discount it's pretty it's pretty good you can use the the code pod",
    "start": "2800520",
    "end": "2807160"
  },
  {
    "text": "practical ai19 that's uh that book and we'll put that information in the show notes as",
    "start": "2807160",
    "end": "2813280"
  },
  {
    "text": "well that sounds like a good one I'll offer one as well which I don't think you'll be surprised at when I mention",
    "start": "2813280",
    "end": "2819280"
  },
  {
    "text": "this there's a book that came out just a few months ago it's on this topic that we're just finishing up with uh it's",
    "start": "2819280",
    "end": "2825800"
  },
  {
    "text": "called a thousand brains a new theory of intelligence and it's by Jeff Hawkins",
    "start": "2825800",
    "end": "2832520"
  },
  {
    "text": "and I have that book it's a very interesting book and I definitely recommend if you're wanting to explore",
    "start": "2832520",
    "end": "2838240"
  },
  {
    "text": "some new ideas about where AI could go it's definitely a book that will stimulate some thought so that's my",
    "start": "2838240",
    "end": "2844920"
  },
  {
    "text": "recommendation a thousand brains by Jeff Hawkins awesome well I'll let I'll let you get to your reading Chris absolutely",
    "start": "2844920",
    "end": "2852520"
  },
  {
    "text": "thanks for the discussion it was a good one today thanks [Music]",
    "start": "2852520",
    "end": "2857559"
  },
  {
    "text": "Daniel thank you for listening to practical AI we appreciate your time and your attention if you enjoyed this",
    "start": "2857559",
    "end": "2864079"
  },
  {
    "text": "episode help us out by spreading the word think of a friend think of a colleague somebody who would benefit",
    "start": "2864079",
    "end": "2870160"
  },
  {
    "text": "from listening to it and send them a link we'd really appreciate it practical AI is hosted by Chris Benson and Daniel",
    "start": "2870160",
    "end": "2876599"
  },
  {
    "text": "White neck it's produced by Jared Santo with music by breakmaster cylinder thanks again to our sponsors fastly Leno",
    "start": "2876599",
    "end": "2883599"
  },
  {
    "text": "and launch darkle that's our show we hope you enjoyed it and we'll talk to you again next",
    "start": "2883599",
    "end": "2888830"
  },
  {
    "text": "[Music]",
    "start": "2888830",
    "end": "2904059"
  },
  {
    "text": "week [Music]",
    "start": "2905760",
    "end": "2915949"
  },
  {
    "text": "k",
    "start": "2916880",
    "end": "2919880"
  }
]