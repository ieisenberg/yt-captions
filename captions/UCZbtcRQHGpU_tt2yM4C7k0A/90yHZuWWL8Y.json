[
  {
    "text": "B with for change log is provided by fastly learn more at fastly.com we move",
    "start": "80",
    "end": "6040"
  },
  {
    "text": "fast and fix things here at Chang log because of rbar check them out at rar.com and we're hosted on Leno servers",
    "start": "6040",
    "end": "12799"
  },
  {
    "text": "head to lin.com changelog this episode is brought to you by digital ocean they now have CPU",
    "start": "12799",
    "end": "19560"
  },
  {
    "text": "optimized droplets with dedicated hyper threads from best in-class Intel CPUs",
    "start": "19560",
    "end": "25199"
  },
  {
    "text": "for all your machine learning and batch processing needs you can easily spin up their oneclick machine learning and AI",
    "start": "25199",
    "end": "31039"
  },
  {
    "text": "application image this gives you immediate access to Python 3 R jupyter",
    "start": "31039",
    "end": "36200"
  },
  {
    "text": "notebook tensor flow s kit and pytorch use our special link to get a $100",
    "start": "36200",
    "end": "42239"
  },
  {
    "text": "credit for digital ocean and try today for free head to do. changelog once",
    "start": "42239",
    "end": "48160"
  },
  {
    "text": "again do. [Music]",
    "start": "48160",
    "end": "58920"
  },
  {
    "text": "changelog welcome to practical AI a weekly podcast",
    "start": "58920",
    "end": "66479"
  },
  {
    "text": "about making artificial intelligence practical productive and accessible to everyone this is where conversations",
    "start": "66479",
    "end": "72880"
  },
  {
    "text": "around AI machine learning and data science happen join the community and select with us around various topics of",
    "start": "72880",
    "end": "78240"
  },
  {
    "text": "the show at change.com Community follow us on Twitter we're at practical aifm",
    "start": "78240",
    "end": "83560"
  },
  {
    "text": "and now on to the [Music] show hey there welcome welcome to",
    "start": "83560",
    "end": "90240"
  },
  {
    "text": "another episode of the Practical AI podcast uh this is Chris Benson I'm an AI strategist and my co-host is Daniel",
    "start": "90240",
    "end": "97200"
  },
  {
    "text": "whack a data scientist we have a real treat in store for you today um we have",
    "start": "97200",
    "end": "103600"
  },
  {
    "text": "a uh a special guest uh that we uh have have looked forward to having on the show for a long time now and I am super",
    "start": "103600",
    "end": "111200"
  },
  {
    "text": "excited about this episode so that guest uh is Bill D who is the chief scientist",
    "start": "111200",
    "end": "117560"
  },
  {
    "text": "and Senior vice president of research for viia uh he is also a professor at Stanford University Welcome very much",
    "start": "117560",
    "end": "124520"
  },
  {
    "text": "Bill oh it's great to be here and how's it going today Daniel it's going uh it's going great I'm excited to to talk to",
    "start": "124520",
    "end": "130479"
  },
  {
    "text": "Bill I I'm uh of course a huge fan as everyone is of of everything Nvidia is doing um in in this space so I'm excited",
    "start": "130479",
    "end": "137760"
  },
  {
    "text": "to hear more Yep so the Genesis for this episode uh came back earlier this year",
    "start": "137760",
    "end": "143760"
  },
  {
    "text": "in March uh I was at the uh Nvidia GTC conference in Silicon Valley and I got",
    "start": "143760",
    "end": "149920"
  },
  {
    "text": "to attend a a small group session called AI for business uh cxo Summit and in",
    "start": "149920",
    "end": "156000"
  },
  {
    "text": "that the the Nvidia CEO Jenson Wong was a kind of in a small group environment and it was just an amazing amount of",
    "start": "156000",
    "end": "163480"
  },
  {
    "text": "wisdom that I got and I was thinking and as I sat there um that was very very business oriented in a lot of ways but",
    "start": "163480",
    "end": "169920"
  },
  {
    "text": "um I kept thinking if we had nvidia's Chief scientists come on board to talk us through kind of what Nvidia does but",
    "start": "169920",
    "end": "176640"
  },
  {
    "text": "but give it to us as practitioners of uh of Neal Network Technology and other AI",
    "start": "176640",
    "end": "181760"
  },
  {
    "text": "technology that would just be amazing so uh Bill thank you so much for coming on board uh really appreciate it hey you're",
    "start": "181760",
    "end": "187280"
  },
  {
    "text": "very welcome so I wanted to to real quick ask uh if you could just give us a little bit of background I I mentioned",
    "start": "187280",
    "end": "192840"
  },
  {
    "text": "that you were the chief scientist in Nvidia and a professor at Stanford could you tell us just a little bit about yourself before we launch into questions",
    "start": "192840",
    "end": "199040"
  },
  {
    "text": "sure so I'm uh you know sort of a uh a hardware engineer who's been um working on both hardware and software for AI in",
    "start": "199040",
    "end": "206239"
  },
  {
    "text": "recent years my first experience with neural networks was in the 1980s when I took a course from John hopfield at",
    "start": "206239",
    "end": "211680"
  },
  {
    "text": "Caltech and was building hopfield uh Nets and and things like that I was on the faculty at MIT for 11 years where I",
    "start": "211680",
    "end": "219159"
  },
  {
    "text": "uh built a research group that built a number of pioneering um supercomputers collaborated with cray on the design of",
    "start": "219159",
    "end": "225080"
  },
  {
    "text": "their cray t3d and t3e and then moved to Stanford in in 1997 where I continued to",
    "start": "225080",
    "end": "231720"
  },
  {
    "text": "lead research on on high performance Computing and and uh special purpose processors for numerous tasks including",
    "start": "231720",
    "end": "238560"
  },
  {
    "text": "Graphics I first got involved with Nvidia in 2003 when I was hired as a consultant to help with the what was",
    "start": "238560",
    "end": "244799"
  },
  {
    "text": "called internally the nv50 became the the G80 when it was announced and in particular to help on the extensions to",
    "start": "244799",
    "end": "250920"
  },
  {
    "text": "the G80 that enabled Cuda the ability to run general purpose Computing programs on on gpus and uh I really got to like",
    "start": "250920",
    "end": "258120"
  },
  {
    "text": "the folks at Nvidia particularly Jensen and uh he convinced me to join full-time in 2009 so since 2009 I've been building",
    "start": "258120",
    "end": "265720"
  },
  {
    "text": "Nvidia research the research organization at Nvidia and myself doing the research on on numerous topics most",
    "start": "265720",
    "end": "271800"
  },
  {
    "text": "recently on some of the path planning algorithms for our self-driving cars and on uh very efficient AI inference that's",
    "start": "271800",
    "end": "278680"
  },
  {
    "text": "awesome yeah that's uh an amazing background and uh I'm sure yeah I mean",
    "start": "278680",
    "end": "283720"
  },
  {
    "text": "it sounds like you joined Nvidia at a at a really exciting time of course things have have really kind of exploded in in",
    "start": "283720",
    "end": "289960"
  },
  {
    "text": "a good way for them and I'm sure it's it's a lot of a lot of excitement and and thrills uh being at the at the",
    "start": "289960",
    "end": "295720"
  },
  {
    "text": "center of that yeah it's a really fun place to be awesome yeah so I I was wondering from my perspective kind of",
    "start": "295720",
    "end": "301960"
  },
  {
    "text": "growing up uh the context in which I heard about Nvidia was kind of nid uh",
    "start": "301960",
    "end": "308360"
  },
  {
    "text": "processing and and gaming which kind of led to the to the rise of of the GPU uh",
    "start": "308360",
    "end": "313880"
  },
  {
    "text": "I was wondering if you could speak a little bit to how how and why that transition into this very AI oriented",
    "start": "313880",
    "end": "321960"
  },
  {
    "text": "approach that that Nvidia is taking now and kind of comment on how that Evolution occurred and and how you see",
    "start": "321960",
    "end": "327800"
  },
  {
    "text": "it from your perspective sure so in St roots are really in graphics gaming is one aspect of that but we've also always",
    "start": "327800",
    "end": "333639"
  },
  {
    "text": "done professional graphics and if you think about what the graphics problem is it's basically simulating you know how",
    "start": "333639",
    "end": "339880"
  },
  {
    "text": "light bounces off of a scene and and is you know appears at your eye or at a camera and and doing that simulation",
    "start": "339880",
    "end": "346479"
  },
  {
    "text": "basically rendering the scene shading each pixel is a very computationally intensive task and it's a very parallel",
    "start": "346479",
    "end": "352919"
  },
  {
    "text": "task so gpus evolve to be very efficient parallel computers with very high",
    "start": "352919",
    "end": "358440"
  },
  {
    "text": "computational intensity and it turns out a lot of other problems have this nature of of having a lot of computational",
    "start": "358440",
    "end": "364199"
  },
  {
    "text": "intensity and being very parallel and so early on you probably in the you know",
    "start": "364199",
    "end": "369880"
  },
  {
    "text": "early 2000s people started trying to use gpus for tasks other than Graphics is it",
    "start": "369880",
    "end": "375000"
  },
  {
    "text": "it's sort of a movement called gpgpu general purpose gpus and around that same time I was leading a project at",
    "start": "375000",
    "end": "380400"
  },
  {
    "text": "Stamford and what we called stream processors which actually wound up developing the right set of programming",
    "start": "380400",
    "end": "386319"
  },
  {
    "text": "tools to program GP gpus we were developing anguage called Brooke the lead student on that project Ian Buck",
    "start": "386319",
    "end": "393039"
  },
  {
    "text": "graduated got his PhD came to Nvidia and evolved you know along with the people inidia including John Nichols who was",
    "start": "393039",
    "end": "399360"
  },
  {
    "text": "heading the computer architecture group at the time evolved Brook into Cuda and that basically made it very easy for",
    "start": "399360",
    "end": "404880"
  },
  {
    "text": "people to take the you know huge number of arithmetic units who were in gpus and their ability to to execute parallel",
    "start": "404880",
    "end": "411840"
  },
  {
    "text": "programs very efficiently and apply them to other problems and so at first they were applied to high performance",
    "start": "411840",
    "end": "417919"
  },
  {
    "text": "Computing problems and gpus have have continued to be very good at that uh we currently provide the arithmetic for the",
    "start": "417919",
    "end": "424680"
  },
  {
    "text": "number one supercomputer in the world Summit at Oakridge um National Laboratories and you know they've been",
    "start": "424680",
    "end": "430000"
  },
  {
    "text": "applied to things from you know you know oil and gas Reservoir modeling to simulating more efficient you know",
    "start": "430000",
    "end": "436960"
  },
  {
    "text": "combustion engines to um simulating how galaxies Collide all sorts of high performance Computing problems",
    "start": "436960",
    "end": "442960"
  },
  {
    "text": "predicting whether climate change stuff like that are now done on gpus so it was very natural since we basically now had",
    "start": "442960",
    "end": "450000"
  },
  {
    "text": "the platforms this is you know we announced Cuda in 2006 you know you know a few years later substantial fraction",
    "start": "450000",
    "end": "457039"
  },
  {
    "text": "of all the large um supercomputers being built were based on on gpus it was very",
    "start": "457039",
    "end": "462080"
  },
  {
    "text": "natural that when other very demanding problems came along that people would apply gpus to them and so if you look at",
    "start": "462080",
    "end": "468280"
  },
  {
    "text": "at Deep learning and particularly the training for deep learning it's a very comp computationally intensive problem",
    "start": "468280",
    "end": "473800"
  },
  {
    "text": "you know it takes you know you know when this was first started to be done it was taking weeks on the fastest G pus we had",
    "start": "473800",
    "end": "480560"
  },
  {
    "text": "and it's very parallel so it was a perfect match for gpus and so you know early on you we we saw this match and",
    "start": "480560",
    "end": "488280"
  },
  {
    "text": "and um applied gpus to that for me and for NVIDIA the start really came when I",
    "start": "488280",
    "end": "493479"
  },
  {
    "text": "had a breakfast with my Stanford colleague Andrew ining and I think it was about probably in 2010 or early 2011",
    "start": "493479",
    "end": "499919"
  },
  {
    "text": "and at the time he was at Google brain and was Finding cats on the Internet by building very large neural networks",
    "start": "499919",
    "end": "505840"
  },
  {
    "text": "running on 16,000 CPUs and when he described what he was doing to me I said you you ought to be doing that on gpus",
    "start": "505840",
    "end": "512640"
  },
  {
    "text": "and so um I found somebody in Nvidia research um a guy named Brian katenzaro who now runs our applied deep learning",
    "start": "512640",
    "end": "519200"
  },
  {
    "text": "research group at the time he was actually a programming language researcher but he was interested in in",
    "start": "519200",
    "end": "524480"
  },
  {
    "text": "deep learning and had the the right background knowledge and his assignment was to work with Andrew and move",
    "start": "524480",
    "end": "529760"
  },
  {
    "text": "Andrew's neural network for finding cats to run on cheep us we were able to take",
    "start": "529760",
    "end": "534920"
  },
  {
    "text": "what took 16,000 CPUs and run it on I think it was you know 40 8 gpus in you",
    "start": "534920",
    "end": "541079"
  },
  {
    "text": "know I think even higher performance than he was getting and the software that came out of that turned into Cuda",
    "start": "541079",
    "end": "546200"
  },
  {
    "text": "DNN on top of which we basically ported just about every framework there is now",
    "start": "546200",
    "end": "551519"
  },
  {
    "text": "the other thing that happened is the first you know the gpus we had at that time which were you know our it was",
    "start": "551519",
    "end": "556959"
  },
  {
    "text": "right around our firy to Kepler transition weren't originally designed to do deep warning they were designed to do U graphics and high performance",
    "start": "556959",
    "end": "563399"
  },
  {
    "text": "Computing and so they had you know good 32-bit footing Point performance good 64-bit footing Point performance but",
    "start": "563399",
    "end": "569399"
  },
  {
    "text": "turns out what you want for deporting training is fp16 and what you want for deporting inference is int 8 they",
    "start": "569399",
    "end": "574839"
  },
  {
    "text": "weren't actually particularly good at either of those so as we learn more about what deep learning needed our subsequent generations of gpus have been",
    "start": "574839",
    "end": "582320"
  },
  {
    "text": "specialized for deep learning we've added support for fp16 for training we've added support for INT 8 and int4",
    "start": "582320",
    "end": "588360"
  },
  {
    "text": "and int1 for inference and we built tensor cores which are special purpose",
    "start": "588360",
    "end": "593560"
  },
  {
    "text": "units that basically give us the efficiency of hardwired deep warning processors like the Google t GPU but",
    "start": "593560",
    "end": "600160"
  },
  {
    "text": "without giving up the programmability of a GPU so while the original gpus were good at Deep learning now that we've",
    "start": "600160",
    "end": "606279"
  },
  {
    "text": "gotten more experience with deep warning learned what it really needs and have specialized and optimized gpus for that",
    "start": "606279",
    "end": "611600"
  },
  {
    "text": "the gpus today especially you know Volta and Turing are really great at Deep warning yeah that's awesome I I know I",
    "start": "611600",
    "end": "618680"
  },
  {
    "text": "was just kind of trying to soak all that up there's so much context and and great information that I know I didn't I",
    "start": "618680",
    "end": "624680"
  },
  {
    "text": "wasn't aware of before for example you know the the evolution of Cuda and how it came from this Brook language that",
    "start": "624680",
    "end": "630959"
  },
  {
    "text": "that you mention and you know how the uh how the uh classifying of cats fit in",
    "start": "630959",
    "end": "636639"
  },
  {
    "text": "and and all of that I don't know were you aware of uh of a lot of that Chris a lot of that's that's great new context",
    "start": "636639",
    "end": "642440"
  },
  {
    "text": "that I I wasn't aware of yeah I mean he took uh topics that I that that I now would consider a shallow understanding",
    "start": "642440",
    "end": "648519"
  },
  {
    "text": "of up until at this point and uh and went deep which is fantastic so be careful bill because we have a whole",
    "start": "648519",
    "end": "654360"
  },
  {
    "text": "bunch more questions for you uh we're going to dive deep into some of these things you're telling us about okay yeah so in particular I know um you mentioned",
    "start": "654360",
    "end": "661279"
  },
  {
    "text": "a lot of things that I I would love just a little bit of clarification on for for those in our audience that maybe maybe",
    "start": "661279",
    "end": "667320"
  },
  {
    "text": "new to them so you mentioned kind of the evolution of of Cuda you also mentioned you know how gpus were um were kind of",
    "start": "667320",
    "end": "675600"
  },
  {
    "text": "integral to this this scaling of the of the deep learning training and all of that I was wondering if we could just",
    "start": "675600",
    "end": "681760"
  },
  {
    "text": "kind of take a step back and from your perspective kind of get get your explanation of um you know what a uh",
    "start": "681760",
    "end": "690200"
  },
  {
    "text": "what a GPU is generally why why it's uh useful for for deep learning in",
    "start": "690200",
    "end": "696200"
  },
  {
    "text": "particular and how Cuda fits fits into that what what it is like what what that interface looks like um today yeah so so",
    "start": "696200",
    "end": "703680"
  },
  {
    "text": "a GPU generally is just a very efficient parallel computer you know the you know Volta has 5120 what we call cacor which",
    "start": "703680",
    "end": "711680"
  },
  {
    "text": "really means 5120 separate arithmetic units can be operating in parallel and",
    "start": "711680",
    "end": "717760"
  },
  {
    "text": "coupled to that is a very efficient system for supplying data to those units and and accessing memory and so you know",
    "start": "717760",
    "end": "725320"
  },
  {
    "text": "for any problem that's very parallel they are orders of magnitude more efficient than CPUs CPUs in contrast are",
    "start": "725320",
    "end": "731839"
  },
  {
    "text": "optimized for single thread performance and for very low latency but to do that they wind up spending enormous amounts",
    "start": "731839",
    "end": "738160"
  },
  {
    "text": "of energy reorganizing your program on the fly to schedule instructions around",
    "start": "738160",
    "end": "744040"
  },
  {
    "text": "Long latency cache misses right so if you try to access memory and you're lucky you get a number in three clocks",
    "start": "744040",
    "end": "749240"
  },
  {
    "text": "Cycles if you're not so lucky it might be 200 clock cycles and so they've got to you know do a lot of bookkeeping to",
    "start": "749240",
    "end": "755480"
  },
  {
    "text": "to work around that uncertainty the result of that is a huge amount of energy that's spent and therefore",
    "start": "755480",
    "end": "760920"
  },
  {
    "text": "performance and Energy Efficiency that's orders of magnitude less than a than a GPU a GPU takes advantage of the fact",
    "start": "760920",
    "end": "767240"
  },
  {
    "text": "that if you have a very parallel program you can hide any memory latency with more parallelism you work on something",
    "start": "767240",
    "end": "772519"
  },
  {
    "text": "else while you wait for the data to come back so they wind up being extremely efficient platforms for tasks like deep",
    "start": "772519",
    "end": "778639"
  },
  {
    "text": "learning where you have many parallel operations that can be done simultaneously before you get the results of one of them back and that's",
    "start": "778639",
    "end": "786120"
  },
  {
    "text": "like for the uh the The Matrix type operations that you're talking about and and also the kind of um iterative",
    "start": "786120",
    "end": "793199"
  },
  {
    "text": "training processes is that is that right right so so you know at the core of deep learning are convolutions and and Matrix",
    "start": "793199",
    "end": "801000"
  },
  {
    "text": "multiplies and in fact you can turn the convolutions into Matrix multiplies through a process called lowering so",
    "start": "801000",
    "end": "807399"
  },
  {
    "text": "fundamentally if you can do a very efficient Matrix multiply you can do really well at at Deep learning and gpus",
    "start": "807399",
    "end": "813000"
  },
  {
    "text": "are very good at doing those Matrix multiplies both because they have an enormous number of arithmetic units",
    "start": "813000",
    "end": "818480"
  },
  {
    "text": "because they have a very highly optimized memory and on chip communication system for keeping those",
    "start": "818480",
    "end": "824519"
  },
  {
    "text": "um arithmetic units busy and and occupied so that that is really a great",
    "start": "824519",
    "end": "830279"
  },
  {
    "text": "explanation and that's helping me a lot uh I I would like to understand Beyond",
    "start": "830279",
    "end": "835480"
  },
  {
    "text": "just nvidia's gpus those of us you know that are out here kind of consuming information in the space are always",
    "start": "835480",
    "end": "841920"
  },
  {
    "text": "hearing tons of other acronyms and if you and you know CPUs tpus as6 if if you",
    "start": "841920",
    "end": "847680"
  },
  {
    "text": "could explain to us a little bit what's different about a GPU from those other architectures that are out there and",
    "start": "847680",
    "end": "853199"
  },
  {
    "text": "what are some of the advantages and disadvantages um you know why is it that Nvidia is able to lead the way with this",
    "start": "853199",
    "end": "858959"
  },
  {
    "text": "GPU technology that you've been bringing us for these last few years sure so I I already mentioned some of that by comparing CPUs and gpus a CPU a central",
    "start": "858959",
    "end": "867160"
  },
  {
    "text": "processing unit like you know an Intel um you know Zeon or um you know amd's",
    "start": "867160",
    "end": "872759"
  },
  {
    "text": "latest Parts is optimized for very fast execution of a single computational thread and as a result of that it spends",
    "start": "872759",
    "end": "879519"
  },
  {
    "text": "an enormous amount of energy rescheduling instructions around cach misses and as a result winds up burning",
    "start": "879519",
    "end": "886800"
  },
  {
    "text": "something on the order of you know a u you know a nan per instruction where the actual work of that instruction maybe",
    "start": "886800",
    "end": "894079"
  },
  {
    "text": "only takes 1% of that energy so that you can think of them as being 1% efficient CPUs actually spend more than half of",
    "start": "894079",
    "end": "901040"
  },
  {
    "text": "their energy doing the payload arithmetic on on computational intensive problems so they are many times more",
    "start": "901040",
    "end": "907600"
  },
  {
    "text": "efficient than CPUs at that now CPUs have Vector extensions that try to get",
    "start": "907600",
    "end": "912680"
  },
  {
    "text": "some of the efficiency of gpus but if you look at the core CPU they're extremely inefficient but very good at",
    "start": "912680",
    "end": "919519"
  },
  {
    "text": "at you know doing a single thread if you don't have any parallelism you need the answer quickly a CPU is what you want if",
    "start": "919519",
    "end": "925480"
  },
  {
    "text": "you've got plenty of parallelism and you can hide your memory latency by working on something else while you're waiting",
    "start": "925480",
    "end": "930720"
  },
  {
    "text": "for that result to come back from memory then a GPU is what you want now you mentioned also tpus and as6 well the TPU",
    "start": "930720",
    "end": "937959"
  },
  {
    "text": "is a type of Asic right it's an application specific integrated circuit in this case it's the application it's",
    "start": "937959",
    "end": "944240"
  },
  {
    "text": "specific for is doing Matrix multiplies the Google TPU especially the tpu1 which",
    "start": "944240",
    "end": "949680"
  },
  {
    "text": "they just had an article in in cacm about is a big unit that basically has a systolic array to multiply two matrices",
    "start": "949680",
    "end": "957560"
  },
  {
    "text": "together and it's extremely efficient at that and so all you need to do is multiply matrices it's very hard to beat",
    "start": "957560",
    "end": "963560"
  },
  {
    "text": "a TPU now the approach we've taken with our latest gpus is to put tensor cores",
    "start": "963560",
    "end": "968720"
  },
  {
    "text": "in them and what tensor cores are are little um Matrix multiply units they're very specialized to multiply matrices",
    "start": "968720",
    "end": "975920"
  },
  {
    "text": "together the difference is by specializing by adding a unit to a general purpose processor we get the",
    "start": "975920",
    "end": "981680"
  },
  {
    "text": "efficiency of that specialization without giving up the programmability of the GPU so if you need to write a custom",
    "start": "981680",
    "end": "988000"
  },
  {
    "text": "layer to do you know a mask because you're doing you know a a pruning and have a sparse set of weights or if you",
    "start": "988000",
    "end": "994920"
  },
  {
    "text": "need a custom layer to do a new type of nonlinear function that you're experimenting with or you want to do",
    "start": "994920",
    "end": "1000079"
  },
  {
    "text": "some type of concatenation between layers that is a little bit different it's really easy to write that in Cuda",
    "start": "1000079",
    "end": "1005920"
  },
  {
    "text": "program it on the GPU and and it will execute it extremely well with all the efficiency of the hardwired Matrix",
    "start": "1005920",
    "end": "1011959"
  },
  {
    "text": "multiply units coming from the the tensor cores whereas on the TPU you have that efficiency but you don't have the",
    "start": "1011959",
    "end": "1017319"
  },
  {
    "text": "flexibility you can only do what that you know one unit has been has been you know designed and hardwired um to do now",
    "start": "1017319",
    "end": "1024760"
  },
  {
    "text": "the advantage of that is it's it's about the same Energy Efficiency right so when you're not using the other features of",
    "start": "1024760",
    "end": "1030839"
  },
  {
    "text": "the GPU you're not paying for them they don't burn any energy but they are sitting there using up die area so the TPU costs a little bit less to",
    "start": "1030839",
    "end": "1037438"
  },
  {
    "text": "manufacture because you don't have all of that general purpose processor um sitting around it but what you give up",
    "start": "1037439",
    "end": "1043839"
  },
  {
    "text": "for that is the flexibility of being able to support new deep warning algorithms as they come out because if",
    "start": "1043839",
    "end": "1049760"
  },
  {
    "text": "it if it those algorithms don't match what the TPU is hardwired for it can't do it yeah and as we've as we've seen uh",
    "start": "1049760",
    "end": "1056720"
  },
  {
    "text": "the the industry isn't moving very fast at new new neural network architectures right they're coming up every day I mean",
    "start": "1056720",
    "end": "1063520"
  },
  {
    "text": "it's hard to keep up with with all the papers on archive yeah it is definitely uh we we try a little bit on this show",
    "start": "1063520",
    "end": "1069679"
  },
  {
    "text": "but uh we're all constantly falling behind so quick followup um in that case",
    "start": "1069679",
    "end": "1075080"
  },
  {
    "text": "based on the fact that you have the tensor cores in the gpus it's unlikely that Nvidia then would likely go to a",
    "start": "1075080",
    "end": "1081440"
  },
  {
    "text": "you know some sort of Asic architecture or something else like that since you've you essentially have already accounted",
    "start": "1081440",
    "end": "1087080"
  },
  {
    "text": "for that value in your GPU architectur is that a fair so actually not we we actually have our own um Asic like",
    "start": "1087080",
    "end": "1093520"
  },
  {
    "text": "architecture as well in that we have um something called the um Nvidia deep warning accelerator the mvla um which",
    "start": "1093520",
    "end": "1101080"
  },
  {
    "text": "we've actually open sourced if you go to mvd.org um you'll see our web page where you can download the RTL and the",
    "start": "1101080",
    "end": "1107720"
  },
  {
    "text": "programming tools and everything else for what is actually a very efficient hardwired neural network accelerator and",
    "start": "1107720",
    "end": "1113159"
  },
  {
    "text": "we use the nvda ourselves in our Xavier chip which is the the system on a chip that we have for our self-driving cars",
    "start": "1113159",
    "end": "1120240"
  },
  {
    "text": "the Xavier has a number of arm cores of our own design it has basically a tenth of a Volta GPU it's you know 512 cacor",
    "start": "1120240",
    "end": "1128679"
  },
  {
    "text": "rather than 5120 and then it has um the mvda as well as a computer vision accelerator because in in embedded",
    "start": "1128679",
    "end": "1135760"
  },
  {
    "text": "processors on the edge the that area efficiency is important we don't want to give up the die area for doing um you",
    "start": "1135760",
    "end": "1142440"
  },
  {
    "text": "know deep warning entirely on on the GPU now there's still an awful lot of GPU performance on on Xavier it's um you",
    "start": "1142440",
    "end": "1150559"
  },
  {
    "text": "know over 10 ter Ops on the on the Cuda cor um but there's also um another 20",
    "start": "1150559",
    "end": "1157120"
  },
  {
    "text": "ter Ops on the Deep learning accelerators so you wind up being able to support very um efficiently you know",
    "start": "1157120",
    "end": "1162880"
  },
  {
    "text": "large numbers of inference tasks on on that so we're we're actually doing it both ways for the embed edit",
    "start": "1162880",
    "end": "1169000"
  },
  {
    "text": "applications we have a hardware deep learning accelerator for both um inference and training in the data",
    "start": "1169000",
    "end": "1175000"
  },
  {
    "text": "center you after considering all options we have decided it's just much better to put the efficient tensor cores onto a",
    "start": "1175000",
    "end": "1182120"
  },
  {
    "text": "programmable engine rather than building a hardware accelerator so you've mentioned uh and this is a a great lead",
    "start": "1182120",
    "end": "1188559"
  },
  {
    "text": "in um you you mentioned kind of a variety of fronts on which Nvidia is",
    "start": "1188559",
    "end": "1193640"
  },
  {
    "text": "working and you've also mentioned a desire that you guys have to keep things programmable and and uh easy to to",
    "start": "1193640",
    "end": "1200760"
  },
  {
    "text": "interface with and and customize um one of the things that that I've definitely seen is that um Nvidia is is definitely",
    "start": "1200760",
    "end": "1209080"
  },
  {
    "text": "um making contributions not only on on the hardware side but on kind of uh on",
    "start": "1209080",
    "end": "1214760"
  },
  {
    "text": "the front of helping users be able to interface with all sorts of these these new types of hardware for example I see",
    "start": "1214760",
    "end": "1222240"
  },
  {
    "text": "um you know like Nvidia Docker and I see things related to kubernetes and uh and",
    "start": "1222240",
    "end": "1228919"
  },
  {
    "text": "working to kind of help people both uh both program their their Hardware but",
    "start": "1228919",
    "end": "1234799"
  },
  {
    "text": "also access and manage and orchestrate things I was wondering if you could if if there's anything you want to",
    "start": "1234799",
    "end": "1240000"
  },
  {
    "text": "highlight on on that side and mention you know where where the different areas that you see uh See Nvidia working on",
    "start": "1240000",
    "end": "1246720"
  },
  {
    "text": "that are really exciting maybe not on the the hardware side but maybe on the orchestration or software side yeah no",
    "start": "1246720",
    "end": "1252640"
  },
  {
    "text": "we um we actually do research on on deep warning that spans the gamut from you",
    "start": "1252640",
    "end": "1257960"
  },
  {
    "text": "know fundamental you know deep warning algorithms and models training methods to tools that make it easier for people",
    "start": "1257960",
    "end": "1264760"
  },
  {
    "text": "to use deep warning all the way up up to the hardware the stuff that I'm actually most excited about is is some of the",
    "start": "1264760",
    "end": "1269880"
  },
  {
    "text": "work on on fundamental models and and um and algorithms we for example right now",
    "start": "1269880",
    "end": "1275120"
  },
  {
    "text": "have the world's best neural network for doing optical flow which is a really nice hybrid of classical computer vision",
    "start": "1275120",
    "end": "1281320"
  },
  {
    "text": "and deep learning because we've applied a lot of what's been learned over 30 years of doing optical flow the old way",
    "start": "1281320",
    "end": "1287279"
  },
  {
    "text": "uh but then have built built that around a deep learning approach and we get the best of both worlds we also have done",
    "start": "1287279",
    "end": "1293080"
  },
  {
    "text": "enormous amount of research on generative adversarial networks we developed um a method of We're the first",
    "start": "1293080",
    "end": "1298559"
  },
  {
    "text": "people to train high resolution generative networks um in the past you know basically you just had too many",
    "start": "1298559",
    "end": "1304640"
  },
  {
    "text": "free variables if you tried to train a gan to build a high resolution image it would just get confused and never",
    "start": "1304640",
    "end": "1310120"
  },
  {
    "text": "converge um we applied curricular learning where we train the Gan first to do low resolution images once it's",
    "start": "1310120",
    "end": "1316120"
  },
  {
    "text": "mastered that we then increase the resolution progressively we call it Progressive Gan and we're very",
    "start": "1316120",
    "end": "1321520"
  },
  {
    "text": "successfully able to generate high resolution images this um has been applied to uh to numerous um tasks we've",
    "start": "1321520",
    "end": "1328760"
  },
  {
    "text": "also um been able to build um coupled Gans where we we can use them to",
    "start": "1328760",
    "end": "1333840"
  },
  {
    "text": "transfer style so for example um if we have a bunch of images um in daylight",
    "start": "1333840",
    "end": "1339640"
  },
  {
    "text": "good weather we can change those to images at night or images in the rain or images in the snow and this lets us",
    "start": "1339640",
    "end": "1345640"
  },
  {
    "text": "augment data sets for self-driving cars we can also use these Gans to um",
    "start": "1345640",
    "end": "1350799"
  },
  {
    "text": "generate medical data sets being able to you know take you know for example brain images and tumor images and combine them",
    "start": "1350799",
    "end": "1356919"
  },
  {
    "text": "in various ways to build larger training sets than you could get by just using the raw data and then a combination of",
    "start": "1356919",
    "end": "1363360"
  },
  {
    "text": "the real data and these synthetic images winds up giving you better accuracy than than one alone so so that works very",
    "start": "1363360",
    "end": "1369520"
  },
  {
    "text": "exciting we also have a number of tools you mentioned our our Docker platforms we also have um a tool called tensor RT",
    "start": "1369520",
    "end": "1376720"
  },
  {
    "text": "which optimizes neural networks um for inference so we get much more efficient execution on our gpus and if you simply",
    "start": "1376720",
    "end": "1383520"
  },
  {
    "text": "naively mapped um the networks on there um and so across the board we've been",
    "start": "1383520",
    "end": "1388799"
  },
  {
    "text": "trying to build the whole ecosystem so that somebody who has a problem can and draw from our collection of algorithms",
    "start": "1388799",
    "end": "1394679"
  },
  {
    "text": "they can draw from our tools and then ultimately run it on our hardware and get a a complete solution for their",
    "start": "1394679",
    "end": "1400120"
  },
  {
    "text": "problem how do you uh how do you keep all of those wheels turning as the uh as",
    "start": "1400120",
    "end": "1405240"
  },
  {
    "text": "as the VP of research there a lot of different areas span all the way from Hardware to to software to to tooling to",
    "start": "1405240",
    "end": "1412440"
  },
  {
    "text": "AI research I I'm sure it's exciting but a lot going on yeah so I I fortunately",
    "start": "1412440",
    "end": "1417679"
  },
  {
    "text": "don't have to keep the M turning myself I'm I'm responsible for NVIDIA research which is an organization of about 200",
    "start": "1417679",
    "end": "1423320"
  },
  {
    "text": "people and we do research on topics ranging from circuit design to AI algorithms and um you know basically",
    "start": "1423320",
    "end": "1430840"
  },
  {
    "text": "what we do is we hire really smart people and then we try to enable them to take all the obstacles out of their way",
    "start": "1430840",
    "end": "1436360"
  },
  {
    "text": "get them excited about the important problems and and the you know the objective of Nvidia research is um is to",
    "start": "1436360",
    "end": "1442880"
  },
  {
    "text": "do two things one is to do research there are a lot of corporate research Labs that actually don't do research they wind up really doing development",
    "start": "1442880",
    "end": "1449279"
  },
  {
    "text": "because they get pulled in too close to you various product groups you know they the product groups always wind up having",
    "start": "1449279",
    "end": "1455120"
  },
  {
    "text": "some fire to put out and so they'll pull the researchers onto the short-term development work put the lest fire out and they wind up not not really doing",
    "start": "1455120",
    "end": "1461400"
  },
  {
    "text": "fundamental research so our goal is to do that fundamental research and and we succeed in that as evidence by",
    "start": "1461400",
    "end": "1466720"
  },
  {
    "text": "publishing you know lots of papers at leading conferences like you know nips and iclr and icml and cvpr um then the",
    "start": "1466720",
    "end": "1474440"
  },
  {
    "text": "the other goal is to make sure that that research is um beneficial to Nvidia that it makes a difference for the company",
    "start": "1474440",
    "end": "1479799"
  },
  {
    "text": "and again that's another failure mode of Industrial Research Labs many of them publish lots of great papers do lots of great research and it has absolutely no",
    "start": "1479799",
    "end": "1486880"
  },
  {
    "text": "impact on their parent company um I think I'd have trouble convincing Jensen to continue uh running the research lab",
    "start": "1486880",
    "end": "1492440"
  },
  {
    "text": "if we didn't um have many successes but we do so for example the the rate tracing cores and we're originally an",
    "start": "1492440",
    "end": "1499360"
  },
  {
    "text": "Nvidia research project qnn as I mentioned um came out of research we are applying deep warning to Graphics we we",
    "start": "1499360",
    "end": "1506200"
  },
  {
    "text": "demonstrated with touring something called Deep learning um super sampling which basically basically anti-alias is",
    "start": "1506200",
    "end": "1511760"
  },
  {
    "text": "and up samples an image um using neural networks and does it in a temporarily stable way our djx2 which includes NV",
    "start": "1511760",
    "end": "1519640"
  },
  {
    "text": "switch NV switch started as a project in Nvidia research as did Envy link on which um the switch is based so um we",
    "start": "1519640",
    "end": "1526360"
  },
  {
    "text": "have a long track record of taking kind of crazy ideas you know maturing them within Nvidia research and then getting",
    "start": "1526360",
    "end": "1532919"
  },
  {
    "text": "the product groups to you know embrace them and ultimately put them into future gpus and um software and and systems",
    "start": "1532919",
    "end": "1539320"
  },
  {
    "text": "products that we produce so Bill as we come back out of break uh I wanted to ask you uh kind of back out just a",
    "start": "1539320",
    "end": "1545200"
  },
  {
    "text": "little bit because we've gone down some some amazing paths I know Daniel and I have learned so much already on the show",
    "start": "1545200",
    "end": "1550919"
  },
  {
    "text": "from you but I wanted to to put a little context around some of that and kind of get a sense as you've told us about all",
    "start": "1550919",
    "end": "1557399"
  },
  {
    "text": "of these amazing Technologies what is nvidia's Vision kind of for the for the future of AI and and as you've talked",
    "start": "1557399",
    "end": "1564880"
  },
  {
    "text": "about some of the parts of your AI platform you know H how are you utilizing that platform strategically to",
    "start": "1564880",
    "end": "1570480"
  },
  {
    "text": "realize that and and what kind of Investments are you expecting Nvidia to make going forward that's a really good",
    "start": "1570480",
    "end": "1575520"
  },
  {
    "text": "question so you know the short answer for the future of AI is continued rapid Innovation I expect to continue to have",
    "start": "1575520",
    "end": "1581720"
  },
  {
    "text": "to stay up late every night reading papers on archive and even then not be able to keep up with what's going on but",
    "start": "1581720",
    "end": "1586919"
  },
  {
    "text": "if you look at at how that rapid Innovation is happening I think it's along several different axes the first",
    "start": "1586919",
    "end": "1592320"
  },
  {
    "text": "axis I think is breadth of applications I think we've only begun to scratch the surface of how AI is affecting you know",
    "start": "1592320",
    "end": "1598760"
  },
  {
    "text": "our daily lives how we do business how we entertain ourselves how we you know you know practice our professions and",
    "start": "1598760",
    "end": "1605200"
  },
  {
    "text": "and I expect more applications of AI to be occurring every day and and those applications to present unique demands",
    "start": "1605200",
    "end": "1612440"
  },
  {
    "text": "the type of models we need how we curate training data um how we train the networks with that data and so on the",
    "start": "1612440",
    "end": "1619559"
  },
  {
    "text": "next axis I would say is one of scale scale of both model size and data sets",
    "start": "1619559",
    "end": "1625760"
  },
  {
    "text": "um we've seen this in areas like computer vision in speech recognition in in machine translation where over time",
    "start": "1625760",
    "end": "1633080"
  },
  {
    "text": "people collect larger data sets to have the capacity to learn those data sets they build larger models um that really",
    "start": "1633080",
    "end": "1639640"
  },
  {
    "text": "raises the bar for the performance you need to train those models on those large data sets in a reasonable amount",
    "start": "1639640",
    "end": "1645399"
  },
  {
    "text": "of time and then finally the access is probably most exciting to me is coming",
    "start": "1645399",
    "end": "1650640"
  },
  {
    "text": "up with new models and and new methods U that basically increase the capability",
    "start": "1650640",
    "end": "1655760"
  },
  {
    "text": "of of deep learning to be more than just perception to basically give it more cognitive ability to have it be able to",
    "start": "1655760",
    "end": "1663360"
  },
  {
    "text": "um reason about things to have longer term memories you know to operate and interact with environments a lot of the",
    "start": "1663360",
    "end": "1669039"
  },
  {
    "text": "work in reinforcement learning we find very exciting along along that axis so seeing you know AI know this constant",
    "start": "1669039",
    "end": "1675720"
  },
  {
    "text": "Innovation along all three of these axes our goal with our platform is to evolve to meet these needs to meet the needs of",
    "start": "1675720",
    "end": "1682399"
  },
  {
    "text": "newer applications to meet the needs of larger scale and you know more capable you know models and and methods and",
    "start": "1682399",
    "end": "1689159"
  },
  {
    "text": "there's a couple ways we need to do that one is to continue to raise the bar on performance you know to train larger",
    "start": "1689159",
    "end": "1694600"
  },
  {
    "text": "models and larger data sets requires more performance and um Mo's law is dead",
    "start": "1694600",
    "end": "1700120"
  },
  {
    "text": "we're not getting any more performance out of process technology so it requires us to innovate with our architecture",
    "start": "1700120",
    "end": "1705240"
  },
  {
    "text": "with our circuit designs to do that and we've done that generation to generation if you look at the performance from you",
    "start": "1705240",
    "end": "1711159"
  },
  {
    "text": "know Kepler where we started working on deep learning to uh Maxwell and Pascal",
    "start": "1711159",
    "end": "1716360"
  },
  {
    "text": "volten now Turing we've been able to really increase by large multiples deep learning performance on each subsequent",
    "start": "1716360",
    "end": "1723880"
  },
  {
    "text": "Generation Um in the absence of really any help from process technology and we expect to to continue doing that the",
    "start": "1723880",
    "end": "1730320"
  },
  {
    "text": "next thing we need to do is we need to make it easier to program so that you know people who are not you know experts",
    "start": "1730320",
    "end": "1735960"
  },
  {
    "text": "in in AI but are rather experts in their domain can easily cultivate a data set you know",
    "start": "1735960",
    "end": "1741120"
  },
  {
    "text": "acquire the right models and and train them and we do that you know through our tools we support every framework we have",
    "start": "1741120",
    "end": "1747080"
  },
  {
    "text": "tensor RT to make it easy to map your applications onto um inference platforms",
    "start": "1747080",
    "end": "1752760"
  },
  {
    "text": "um and then we also have training programs we have a deep warning Institute where we basically take people who are application experts and train",
    "start": "1752760",
    "end": "1759120"
  },
  {
    "text": "them so that they can apply deep learning to their application and then the final way we want our platforms to evolve is to remain flexible the the",
    "start": "1759120",
    "end": "1766679"
  },
  {
    "text": "Deep learning world is changing every day and so we don't want to hardwire too much in and not be able to support the",
    "start": "1766679",
    "end": "1772360"
  },
  {
    "text": "latest idea in fact we think it would inhibit people coming up with the latest idea if you know the platform that",
    "start": "1772360",
    "end": "1777480"
  },
  {
    "text": "everybody is using was too rigid we want to make it a very flexible platform so that people can continue to experiment",
    "start": "1777480",
    "end": "1782799"
  },
  {
    "text": "and develop new methods yeah so in light of that I'd be really interested to hear",
    "start": "1782799",
    "end": "1788559"
  },
  {
    "text": "from your perspective how ideas at Nvidia actually advance from research to reality in particularly in light of what",
    "start": "1788559",
    "end": "1795559"
  },
  {
    "text": "you just said in light of that you want to make things easier for easier for people to program easier for people to",
    "start": "1795559",
    "end": "1802000"
  },
  {
    "text": "interface with application people to to interface with while at the same time you know uh pushing performance forward",
    "start": "1802000",
    "end": "1809720"
  },
  {
    "text": "and and keeping flexible it definitely seems like it it might be hard to to balance those things but uh as you've",
    "start": "1809720",
    "end": "1816320"
  },
  {
    "text": "already mentioned there there's been a lot of great things that that you guys have come out with that do balance that",
    "start": "1816320",
    "end": "1821480"
  },
  {
    "text": "really well so it's wondering if from that perspective how you see things advancing from from research to reality",
    "start": "1821480",
    "end": "1828240"
  },
  {
    "text": "Nvidia yeah that's a good question and one that I'm I'm very excited about because it's kind of my job to make sure those things advance so not not all",
    "start": "1828240",
    "end": "1835320"
  },
  {
    "text": "ideas start in in Nvidia research many ideas start in the product groups many ideas start you know with uh application",
    "start": "1835320",
    "end": "1840880"
  },
  {
    "text": "Engineers who work with the customers and see the need but for the ideas that do start in Nvidia research um which is",
    "start": "1840880",
    "end": "1846240"
  },
  {
    "text": "an organization of about 200 people individual researchers generally just start experimenting with things come up",
    "start": "1846240",
    "end": "1851320"
  },
  {
    "text": "with a good idea and then the goal is to um find a way for that idea to have impact on the company and so we try to",
    "start": "1851320",
    "end": "1858679"
  },
  {
    "text": "make sure everybody when they come up with an idea identifies both a champion and a consumer who are often the same",
    "start": "1858679",
    "end": "1864440"
  },
  {
    "text": "person in the product groups for that technology and you know as they develop",
    "start": "1864440",
    "end": "1869639"
  },
  {
    "text": "the technology further they get some indication about gee does does the champion care about this technology can",
    "start": "1869639",
    "end": "1874919"
  },
  {
    "text": "they will it make their product better and if it doesn't it's often an indication they should drop the idea in fact to me one of the keys of good",
    "start": "1874919",
    "end": "1881519"
  },
  {
    "text": "research is to kill things quickly most research projects actually don't go anywhere and there's there's nothing",
    "start": "1881519",
    "end": "1887600"
  },
  {
    "text": "wrong with coming up with research ideas that don't work what's wrong is spending a lot of resources on them before you",
    "start": "1887600",
    "end": "1892880"
  },
  {
    "text": "give up on on the ones that don't work and so we try to kill the ideas that either aren't going to work or aren't going to have impact on the company",
    "start": "1892880",
    "end": "1898880"
  },
  {
    "text": "pretty quickly but the ones that are going to have impact on the company one thing that's really great about about Nvidia is it's a company where it's like",
    "start": "1898880",
    "end": "1905760"
  },
  {
    "text": "a big startup there's there's no politics there's no not invented here so if there's a good idea the product groups don't care that it came out of",
    "start": "1905760",
    "end": "1911480"
  },
  {
    "text": "research they say that's a great idea we want that and very often they'll grab things out of our hands before we even think we're done with them NV switch was",
    "start": "1911480",
    "end": "1917960"
  },
  {
    "text": "great example of that we wanted to actually complete a prototype in research we didn't get the chance they they grabbed it and made it a product",
    "start": "1917960",
    "end": "1924159"
  },
  {
    "text": "before we had the chance to do that and it's really about people um the people come up with the concept are",
    "start": "1924159",
    "end": "1929559"
  },
  {
    "text": "communicating with the the people who will turn it into reality and then once it it sort of jumps over to that side it",
    "start": "1929559",
    "end": "1934919"
  },
  {
    "text": "becomes more of an engineering Endeavor less of a research Endeavor where people have to you know hit goals things have",
    "start": "1934919",
    "end": "1940440"
  },
  {
    "text": "to work they have to be verified uh but the whole process works and ultimately we're able to very quickly go from",
    "start": "1940440",
    "end": "1945960"
  },
  {
    "text": "concept to delivering very you know very reliable products to our end customers",
    "start": "1945960",
    "end": "1951679"
  },
  {
    "text": "so I I would like to to take you into particular use case I know when I was at GTC in March uh Jensen was on stage",
    "start": "1951679",
    "end": "1958799"
  },
  {
    "text": "doing his keynote and we had all walked in um looking at you know the the amazing autonomous vehicles that you",
    "start": "1958799",
    "end": "1965360"
  },
  {
    "text": "guys had in the lobby and stuff and he made a comment that really struck me and I was just wanting to get uh to to get",
    "start": "1965360",
    "end": "1971159"
  },
  {
    "text": "your thoughts on it he said everything that moves will be autonomous and and in that presentation he went Way Beyond",
    "start": "1971159",
    "end": "1977279"
  },
  {
    "text": "just cars he was talking about literally everything whether it be on the you know the the land sea or air and and so",
    "start": "1977279",
    "end": "1984039"
  },
  {
    "text": "obviously that would include gpus uh and maybe other specialized processors that",
    "start": "1984039",
    "end": "1989399"
  },
  {
    "text": "you guys put into those vehicles but what other the things are you doing to realize that Vision considering how how cool it is to the rest of us that's a",
    "start": "1989399",
    "end": "1995679"
  },
  {
    "text": "great question so um one thing we're we're doing in a video research is we're actively pursuing both autonomous",
    "start": "1995679",
    "end": "2001360"
  },
  {
    "text": "vehicles and Robotics and in fact autonomous vehicles are are a special case and in many ways an easy case of of",
    "start": "2001360",
    "end": "2007039"
  },
  {
    "text": "Robotics and that all they really have to do is navigate around and not hit anything robots actually have a much",
    "start": "2007039",
    "end": "2012320"
  },
  {
    "text": "harder task in that they have to manipulate they have to pick things up and insert you know you know bolts into",
    "start": "2012320",
    "end": "2017720"
  },
  {
    "text": "nuts they have to hit things but hit things in a controlled way so that they can actually you know manipulate the",
    "start": "2017720",
    "end": "2022760"
  },
  {
    "text": "world in a way that that that they desire and so I've recently started a robotics uh research lab at at Nvidia",
    "start": "2022760",
    "end": "2029320"
  },
  {
    "text": "it's in Seattle we hired deer Fox from the University Washington to lead that lab and and robots are just a great",
    "start": "2029320",
    "end": "2035440"
  },
  {
    "text": "example of how deep warning is changing the world because historically robots have been very accurate positioning",
    "start": "2035440",
    "end": "2041799"
  },
  {
    "text": "machines if you look at how they've actually been applied in the world autom manufacturers use them on their lines to",
    "start": "2041799",
    "end": "2047000"
  },
  {
    "text": "do spot welding and to spray paint but they're not responding to the environment they simply have been",
    "start": "2047000",
    "end": "2052358"
  },
  {
    "text": "programmed to very accurately move an actuator to a position repeatedly over and over again do exactly the same thing",
    "start": "2052359",
    "end": "2058919"
  },
  {
    "text": "with deep learning we're able to actually give robots perception and the ability to interact with the environment",
    "start": "2058919",
    "end": "2065679"
  },
  {
    "text": "so that they can respond to a part not being being in the right place adjust manipulate pick that part up move it",
    "start": "2065679",
    "end": "2071079"
  },
  {
    "text": "around they can perhaps even work with people working as a team where you know the robot and the person are interacting",
    "start": "2071079",
    "end": "2077280"
  },
  {
    "text": "together by using deep learning to provide them with both sensory abilities and also through reinforcement learning",
    "start": "2077280",
    "end": "2082839"
  },
  {
    "text": "the ability to reason and and choose actions for a given state that they find themselves in and so our goal from this",
    "start": "2082839",
    "end": "2089158"
  },
  {
    "text": "is by doing this fundamental research in in robotics is to basically learn how to build future platforms that will be the",
    "start": "2089159",
    "end": "2096440"
  },
  {
    "text": "brains for all of the world World robots just like we want to build the platform that's going to be the brains for all",
    "start": "2096440",
    "end": "2101720"
  },
  {
    "text": "the world's autonomous vehicles hopefully this research will ultimately lead to that platform not just the",
    "start": "2101720",
    "end": "2107040"
  },
  {
    "text": "hardware but the various layers of software and ultimately the the fundamental um methods that those future",
    "start": "2107040",
    "end": "2113280"
  },
  {
    "text": "robots and autonomous vehicles will be using so Bill we we've kind of transitioned into talking about you know",
    "start": "2113280",
    "end": "2118720"
  },
  {
    "text": "use cases and you've mentioned a lot about uh about robots and and other things kind of at quote unquote the The",
    "start": "2118720",
    "end": "2125800"
  },
  {
    "text": "Edge I was wondering if you could give us a little bit of a of a perspective on",
    "start": "2125800",
    "end": "2131320"
  },
  {
    "text": "you know what you at moving forward what you see as the as the edge and how um",
    "start": "2131320",
    "end": "2137680"
  },
  {
    "text": "how neural network both training and inference will be kind of spread across you know centralized compute in the",
    "start": "2137680",
    "end": "2143240"
  },
  {
    "text": "cloud or on premise and on on edge devices and what those Edge devices might might look like that's that's a",
    "start": "2143240",
    "end": "2150000"
  },
  {
    "text": "good question so I see deep boarding is happening in in sort of three ways so the first is training which by large",
    "start": "2150000",
    "end": "2157040"
  },
  {
    "text": "takes place in in the cloud and and the reason why you want it to take place in the cloud is that first of all you need to have a large data set so you need to",
    "start": "2157040",
    "end": "2163119"
  },
  {
    "text": "have someplace where you can store terabytes of data maybe even even um you know more than that and you know you",
    "start": "2163119",
    "end": "2170240"
  },
  {
    "text": "really want to do that in a centralized location you also if you're Gathering training data say from a fleet of",
    "start": "2170240",
    "end": "2175800"
  },
  {
    "text": "autonomous vehicles you you want them all to learn from each other's experiences right so you want to gather",
    "start": "2175800",
    "end": "2181000"
  },
  {
    "text": "all that data coate it one place curate the data to basically discard the stuff that's not very interesting keep the",
    "start": "2181000",
    "end": "2186520"
  },
  {
    "text": "stuff that is and then train One Network on on all of the data so so training really wants to happen in the cloud it",
    "start": "2186520",
    "end": "2193319"
  },
  {
    "text": "requires a large data set it has a large memory footprint has unique requirements requires fp16 and then there's inference",
    "start": "2193319",
    "end": "2200319"
  },
  {
    "text": "and inference happens in in both the edge and the cloud I think most people if you can do inference in the cloud",
    "start": "2200319",
    "end": "2206240"
  },
  {
    "text": "would prefer to do it there there's an economy of scale um you can also share resources you know if you have a task",
    "start": "2206240",
    "end": "2212760"
  },
  {
    "text": "where you're not doing inference constantly but on demand then you don't need to have a resource tied up all the",
    "start": "2212760",
    "end": "2219920"
  },
  {
    "text": "time you can only you know you can share it use it when you need it somebody else can use it when you don't need it so",
    "start": "2219920",
    "end": "2225119"
  },
  {
    "text": "it's just more efficient to do inference in the cloud but there are cases where you can't do inference in the cloud and you know an autonomous vehicle is a",
    "start": "2225119",
    "end": "2231040"
  },
  {
    "text": "great example first of all you may have latency requirements right if your camera sees you know the the you know",
    "start": "2231040",
    "end": "2237040"
  },
  {
    "text": "kid running into the street you can't afford the latency to send that image to the cloud do the inference there and send the braking command back you need",
    "start": "2237040",
    "end": "2243680"
  },
  {
    "text": "to have a very tight Loop that that commands the car to stop you also May not be connected or you may have",
    "start": "2243680",
    "end": "2249079"
  },
  {
    "text": "bandwidth limits so for example people who have networks of surveillance cameras are producing just too much data to send all of it to the cloud they need",
    "start": "2249079",
    "end": "2255960"
  },
  {
    "text": "to do some data reduction at least locally have some local inference that filters the data and then send only the",
    "start": "2255960",
    "end": "2261599"
  },
  {
    "text": "interesting data to the cloud for for further processing and then finally there may be privacy um constraints that",
    "start": "2261599",
    "end": "2267720"
  },
  {
    "text": "that limit your ability to send stuff up to the cloud you may want to handle things locally to avoid sharing data",
    "start": "2267720",
    "end": "2272839"
  },
  {
    "text": "that you don't want to share so I think there are a lot of reasons why you want to do inference in these embedded devices almost no reason why I think you",
    "start": "2272839",
    "end": "2278920"
  },
  {
    "text": "would want to do training there and in in the case where um where you are doing inference in the embedded devices that",
    "start": "2278920",
    "end": "2285000"
  },
  {
    "text": "often has very strong Energy Efficiency constraints they may be battery operated they may you know need to run for a long",
    "start": "2285000",
    "end": "2291440"
  },
  {
    "text": "period of time without being being recharged and so the efficiency demands are even higher than than for inference",
    "start": "2291440",
    "end": "2297000"
  },
  {
    "text": "in the cloud yeah I've actually run into that myself in terms of those uh the the battery constraints doing inferencing uh",
    "start": "2297000",
    "end": "2304200"
  },
  {
    "text": "on mobile devices you know we we've covered so much ground if you are a software developer or maybe a data",
    "start": "2304200",
    "end": "2311000"
  },
  {
    "text": "scientist who's doing software development and in the engineering and and you're looking at all of these things that we have been talking about",
    "start": "2311000",
    "end": "2317400"
  },
  {
    "text": "kind of from an appd perspective you know from training and the hardware working on the edge the different tools",
    "start": "2317400",
    "end": "2323480"
  },
  {
    "text": "Cuda you name it what are the things that the the necessary skills that",
    "start": "2323480",
    "end": "2328599"
  },
  {
    "text": "people should be thinking about so many people are are kind of self-training themselves into this and there is there",
    "start": "2328599",
    "end": "2334680"
  },
  {
    "text": "is so much uh for a person who's just trying trying to get into AI to learn how would you structure that if somebody",
    "start": "2334680",
    "end": "2341560"
  },
  {
    "text": "is trying to to self-train them into this field well I think actually what you need to know to be successful in AI falls into two categories one is basic",
    "start": "2341560",
    "end": "2349000"
  },
  {
    "text": "knowledge and the other is very practical how howto information for the basic knowledge I think what's most",
    "start": "2349000",
    "end": "2354160"
  },
  {
    "text": "important is having a really strong back background in mathematics and particularly in statistics and probability Theory because that's what",
    "start": "2354160",
    "end": "2360119"
  },
  {
    "text": "all of AI is based on it's it's you know you're basically you know doing statistical estimation of of a number of",
    "start": "2360119",
    "end": "2366720"
  },
  {
    "text": "things and then the Practical side of it is is knowing how to use the tools that are available whatever your favorite",
    "start": "2366720",
    "end": "2372720"
  },
  {
    "text": "framework is whether it's pytorch or whether it's tensorflow having the Practical knowledge to you know get a",
    "start": "2372720",
    "end": "2378200"
  },
  {
    "text": "model get a data set and and run the tools to train it so uh since you mentioned that I'm just just curious uh",
    "start": "2378200",
    "end": "2384839"
  },
  {
    "text": "because Daniel and I have used different tools do do you have any personal favorites that you like to use not",
    "start": "2384839",
    "end": "2390200"
  },
  {
    "text": "suggesting anything with that you say is the right thing that everybody should do but we always like to find out what",
    "start": "2390200",
    "end": "2395560"
  },
  {
    "text": "people's preferences are I I don't really have any strong preferences I have to confess that I actually don't do that much coding myself anymore and the",
    "start": "2395560",
    "end": "2402280"
  },
  {
    "text": "people I work with often you know migrate to one or another for different reasons a lot of people use pytorch",
    "start": "2402280",
    "end": "2407720"
  },
  {
    "text": "because they like to sort of work from the from the the python base many people use tensorflow I think it is probably",
    "start": "2407720",
    "end": "2413160"
  },
  {
    "text": "the most popular framework overall these days yeah and I'm sure a lot of the the Frameworks that your team uses and also",
    "start": "2413160",
    "end": "2420560"
  },
  {
    "text": "the the tools that they generate and and the research that they generate I'm sure a lot of that uses open source tools",
    "start": "2420560",
    "end": "2426640"
  },
  {
    "text": "like you've already mentioned mention are there things that you'd like to highlight that you know Nvidia is uh kind of doing on the the open source",
    "start": "2426640",
    "end": "2432960"
  },
  {
    "text": "front that that maybe our listeners could go and and check out and potentially uh start start playing",
    "start": "2432960",
    "end": "2438680"
  },
  {
    "text": "around with so uh one thing I'll highlight actually is our deep warning accelerator if they you listeners go to mvd.org if they actually want to play",
    "start": "2438680",
    "end": "2445560"
  },
  {
    "text": "with hardware for deep learning they can download the RTL for that accelerator customize it to their needs include it",
    "start": "2445560",
    "end": "2451000"
  },
  {
    "text": "into either an fpj or an Asic of their own design we also open source a lot of um software that comes out of out of our",
    "start": "2451000",
    "end": "2457560"
  },
  {
    "text": "research so you know for example our work on Progressive generative adversarial networks Progressive Gans",
    "start": "2457560",
    "end": "2464119"
  },
  {
    "text": "our work on networks that we use for optical flow our work on D noising all of those networks have been open-",
    "start": "2464119",
    "end": "2470319"
  },
  {
    "text": "sourced um so people can very easily replicate our results and and apply those new methods that we've developed",
    "start": "2470319",
    "end": "2476119"
  },
  {
    "text": "to their own problems awesome yeah that's that's uh super uh helpful and",
    "start": "2476119",
    "end": "2481240"
  },
  {
    "text": "and we'll make sure and includes some some links in our show notes to that as we wrap up here and and get to the end",
    "start": "2481240",
    "end": "2486920"
  },
  {
    "text": "of our conversation once again I really appreciate all of the perspective on these different um different things it",
    "start": "2486920",
    "end": "2492920"
  },
  {
    "text": "was really helpful for me I know um I was wondering if you have any parting thoughts or kind of inspiring thoughts",
    "start": "2492920",
    "end": "2499480"
  },
  {
    "text": "for the listeners um assuming that our listeners are kind of either already in or getting into the the AI field and",
    "start": "2499480",
    "end": "2506160"
  },
  {
    "text": "kind of trying to find their place and find you know what people are working on do you have any parting thoughts for",
    "start": "2506160",
    "end": "2512240"
  },
  {
    "text": "them or or encouragements yeah I think it's just a very exciting time to be working in AI because there are so many",
    "start": "2512240",
    "end": "2518079"
  },
  {
    "text": "new developments happening every day it's uh it's never a dull place um it's in fact so much stuff happening that's",
    "start": "2518079",
    "end": "2523240"
  },
  {
    "text": "hard to keep up as a hardware engineer I think it's also very rewarding to know that this whole revolution in deep learning has been enabled by Hardware",
    "start": "2523240",
    "end": "2530280"
  },
  {
    "text": "all of the algorithms you know convolutional Nets uh multi-layer perceptrons training them using uh",
    "start": "2530280",
    "end": "2536880"
  },
  {
    "text": "stochastic gradient descent and back propagation all of that has been around since the 1980s since I first started",
    "start": "2536880",
    "end": "2542240"
  },
  {
    "text": "playing with with neural networks but it wasn't until we had gpus that it was really practical gpus basically were the",
    "start": "2542240",
    "end": "2548480"
  },
  {
    "text": "spark that ignited the the revolution they know the three ingredients were the algorithms the large data sets those",
    "start": "2548480",
    "end": "2554400"
  },
  {
    "text": "were both there but then you needed the the gpus to make it work it wasn't you know for computer vision it wasn't until",
    "start": "2554400",
    "end": "2560760"
  },
  {
    "text": "alexnet in 2012 where using you know gpus he was able to train a network to",
    "start": "2560760",
    "end": "2566480"
  },
  {
    "text": "win the image net competition that deep warning really took off so I think gpus are what ignited this and I think gpus",
    "start": "2566480",
    "end": "2572760"
  },
  {
    "text": "are still really the platform of choice because with the tensor cores they provide the efficiency of special",
    "start": "2572760",
    "end": "2578760"
  },
  {
    "text": "purpose units but without the inflexibility of a hardwired Asic like a TPU so you get the best of both worlds",
    "start": "2578760",
    "end": "2585000"
  },
  {
    "text": "um you can program in Cuda but get the efficiency of a tensor core well thank you very much Bill uh for me I have",
    "start": "2585000",
    "end": "2590960"
  },
  {
    "text": "learned so much on this episode that I'm probably going have to go back and uh listen to it a couple of times to take",
    "start": "2590960",
    "end": "2596559"
  },
  {
    "text": "in everything that you've taught us today it's been really packed with Incredible information so thank you very",
    "start": "2596559",
    "end": "2601800"
  },
  {
    "text": "very much for coming on oh it's my pleasure thank you and uh with that we'll look forward to our next episode",
    "start": "2601800",
    "end": "2607119"
  },
  {
    "text": "episode I hope our listeners got as much out of it as Daniel and I did Daniel you doing good your head going to explode",
    "start": "2607119",
    "end": "2613160"
  },
  {
    "text": "yet I've got I've got a bunch of websites pulled up that I am going to going to start reading afterwards so uh",
    "start": "2613160",
    "end": "2619400"
  },
  {
    "text": "so it it was a great time and we'll talk to you uh again next week great thank you very much",
    "start": "2619400",
    "end": "2625960"
  },
  {
    "text": "Bill all right thank you for tuning in to this episode of practical AI if you enjoyed this show do us a favor go on",
    "start": "2626119",
    "end": "2632119"
  },
  {
    "text": "iTunes give us a rating go in your podcast app and favorite it if you are on Twitter or social net share a link",
    "start": "2632119",
    "end": "2637720"
  },
  {
    "text": "with a friend whatever you got to do share the show with a friend if you enjoyed it and band withd for change log is provided by fastly learn more at",
    "start": "2637720",
    "end": "2644280"
  },
  {
    "text": "fastly.com and we catch our errors before our users do here at changelog because of robar check them out at",
    "start": "2644280",
    "end": "2649559"
  },
  {
    "text": "roar.com changelog and we're hosted on Leno Cloud servers head to l.com",
    "start": "2649559",
    "end": "2655480"
  },
  {
    "text": "changelog check them out support this show this episode is hosted by Daniel whitenack and Chris Benson editing is",
    "start": "2655480",
    "end": "2661839"
  },
  {
    "text": "done by Tim Smith the music is by break master cylinder and you can find more shows just like like this at Chang",
    "start": "2661839",
    "end": "2668319"
  },
  {
    "text": "law.com when you go there pop in your email address get our weekly email keeping you up to date with the news and",
    "start": "2668319",
    "end": "2674359"
  },
  {
    "text": "podcast for developers in your inbox every single week thanks for tuning in we'll see you next",
    "start": "2674359",
    "end": "2679930"
  },
  {
    "text": "[Music] week",
    "start": "2679930",
    "end": "2687000"
  }
]