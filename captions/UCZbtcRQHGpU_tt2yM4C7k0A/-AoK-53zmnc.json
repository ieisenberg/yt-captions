[
  {
    "text": "[Music]",
    "start": "330",
    "end": "4090"
  },
  {
    "text": "welcome to practical AI if you work in artificial intelligence aspire to or are",
    "start": "7160",
    "end": "13759"
  },
  {
    "text": "curious how AI related Technologies are changing the world this is the show for you thank you to our partners at fastly",
    "start": "13759",
    "end": "21119"
  },
  {
    "text": "for shipping all of our pods super fast to wherever you listen check them out at",
    "start": "21119",
    "end": "26240"
  },
  {
    "text": "fast.com and to our friends at fly deploy your app and database close to",
    "start": "26240",
    "end": "31359"
  },
  {
    "text": "your users no Ops required learn more at",
    "start": "31359",
    "end": "37000"
  },
  {
    "text": "[Music] fly.io welcome to another episode of",
    "start": "37140",
    "end": "44920"
  },
  {
    "text": "practical AI this is Daniel whack I'm a data scientist with s International and",
    "start": "44920",
    "end": "51120"
  },
  {
    "text": "I'm joined as always by my co-host Chris Benson who is a tech strategist at locked Martin how you doing Chris I'm",
    "start": "51120",
    "end": "57920"
  },
  {
    "text": "doing fine it's been interesting time times uh though it's not what we're going to be talking about today been",
    "start": "57920",
    "end": "64080"
  },
  {
    "text": "watching The Showdown uh between you know Google and Microsoft over chat gbt",
    "start": "64080",
    "end": "70320"
  },
  {
    "text": "and Bard and you know things are happening as we're recording this so I",
    "start": "70320",
    "end": "75799"
  },
  {
    "text": "thought maybe you would have been too distracted with the new Harry Potter game I well there is that that's uh you",
    "start": "75799",
    "end": "81960"
  },
  {
    "text": "know but yes we all have our secret little things that we do to keep entertained yeah yeah well I I forget",
    "start": "81960",
    "end": "89720"
  },
  {
    "text": "one of our um recent guests brought up that quote of like you don't need to do",
    "start": "89720",
    "end": "95079"
  },
  {
    "text": "machine learning like Google and you're talking about like Google and Bard and all of these things and when you think",
    "start": "95079",
    "end": "101079"
  },
  {
    "text": "about those things you think about like oh these like data centers full of gpus",
    "start": "101079",
    "end": "106360"
  },
  {
    "text": "and these huge supercomputers that they've got at their disposal to do things which isn't the type of GPU",
    "start": "106360",
    "end": "113240"
  },
  {
    "text": "infrastructure that most practitioners have access to and uh that happens to be",
    "start": "113240",
    "end": "119039"
  },
  {
    "text": "uh maybe the the topic of what we'll get into today a little bit excellent with",
    "start": "119039",
    "end": "124479"
  },
  {
    "text": "Eric dunan founder of banana serverless gpus welcome Eric thank you that was a",
    "start": "124479",
    "end": "130160"
  },
  {
    "text": "beautiful leading um definitely want to help people get the Google level infrastructure without that level of",
    "start": "130160",
    "end": "136200"
  },
  {
    "text": "effort so glad to be here awesome yeah well we're really excited to have you I",
    "start": "136200",
    "end": "141319"
  },
  {
    "text": "have to say I did spin up a model in banana um leading up to this conversation so I'm I'm pretty excited",
    "start": "141319",
    "end": "147920"
  },
  {
    "text": "to to talk about it but before we get into the specifics of all the cool things that you're doing I know that our",
    "start": "147920",
    "end": "155280"
  },
  {
    "text": "listeners like I say they're probably very familiar with gpus and like why they're important to AI machine learning",
    "start": "155280",
    "end": "163440"
  },
  {
    "text": "uh modeling but maybe they've just heard of serverless as like this Cloud thing",
    "start": "163440",
    "end": "169080"
  },
  {
    "text": "that is like a thing that people do in the cloud they do serverless things and they've never thought about like",
    "start": "169080",
    "end": "175159"
  },
  {
    "text": "serverless gpus could you just like step back for a second and describe like",
    "start": "175159",
    "end": "180800"
  },
  {
    "text": "first off like for people that might need just a very brief intro what do you mean when you say serverless and then",
    "start": "180800",
    "end": "188280"
  },
  {
    "text": "kind of take us into like serverless gpus is that a new thing has that existed before um curious to hear your",
    "start": "188280",
    "end": "195640"
  },
  {
    "text": "uh perspective um so I love your specific phrasing what do you mean when you say serverless because serverless is",
    "start": "195640",
    "end": "202640"
  },
  {
    "text": "one of those terms that nobody has really pinned down exactly what it defines um our working definition is",
    "start": "202640",
    "end": "210480"
  },
  {
    "text": "this idea that when you need capacity when you need servers to handle your",
    "start": "210480",
    "end": "216840"
  },
  {
    "text": "requests when you're in periods of spikes in surges of use uh you have more servers when you have less use you have",
    "start": "216840",
    "end": "223799"
  },
  {
    "text": "fewer servers and when you have no use you have zero servers and the idea of this is to make it so that uh you as an",
    "start": "223799",
    "end": "230400"
  },
  {
    "text": "engineering team and as a product don't need to think about your compute as a fixed cost it allows you to essentially",
    "start": "230400",
    "end": "236159"
  },
  {
    "text": "view it as pretty much per request as you go funny enough serverless really does mean servers running under the hood",
    "start": "236159",
    "end": "243680"
  },
  {
    "text": "but the less is that you just don't need to think about it you think about it less happy to dive into what the details",
    "start": "243680",
    "end": "249720"
  },
  {
    "text": "of that mean in regards to gpus but uh serverless has been around for about 10",
    "start": "249720",
    "end": "256160"
  },
  {
    "text": "15 years um I don't know my exact timelines but it's been uh a concept within CPU based compute serving things",
    "start": "256160",
    "end": "263600"
  },
  {
    "text": "like websites backends and people have been wanting this to exist for gpus for",
    "start": "263600",
    "end": "269759"
  },
  {
    "text": "a long time and nobody's really cracked it uh and that's the challenge we've been working on I know that uh like you",
    "start": "269759",
    "end": "276680"
  },
  {
    "text": "talked about websites about like backends that sort of thing just in general when we're talking about",
    "start": "276680",
    "end": "283039"
  },
  {
    "text": "serverless gpus in your mind is the use case that you have mostly on like the",
    "start": "283039",
    "end": "289080"
  },
  {
    "text": "inference side or on the like training side of what um practitioners are doing",
    "start": "289080",
    "end": "295840"
  },
  {
    "text": "or is there a little bit of both the vast majority at least from what we've seen is on inference and I think",
    "start": "295840",
    "end": "301639"
  },
  {
    "text": "inference is where the value of serverless comes in the most there's other tools for training where it's not",
    "start": "301639",
    "end": "307520"
  },
  {
    "text": "as latency constraint where you could use other infrastructure orchestration tools um but for specifically inference",
    "start": "307520",
    "end": "315160"
  },
  {
    "text": "serverless is one of the the keys to the kingdom if you could really do serverless well uh so we just as a team",
    "start": "315160",
    "end": "322120"
  },
  {
    "text": "have chosen to focus mainly on inference realtime inference so if there's a user on at the other end waiting for a",
    "start": "322120",
    "end": "328680"
  },
  {
    "text": "response uh we're the ones responsible for making that response happen quickly gotcha and why has it taken so long to",
    "start": "328680",
    "end": "335600"
  },
  {
    "text": "get to serverless gpus versus serverless CPUs one of the biggest problems in",
    "start": "335600",
    "end": "341919"
  },
  {
    "text": "serverless is What's called the cold boot time cold boot as in you don't have",
    "start": "341919",
    "end": "347680"
  },
  {
    "text": "servers running a request comes in that request coming in triggers a server",
    "start": "347680",
    "end": "352880"
  },
  {
    "text": "scale up going from zero to one and then one to many and the time it takes in",
    "start": "352880",
    "end": "358120"
  },
  {
    "text": "order to get resources provision visioned and ready to handle requests in CPUs can take you know couple seconds on",
    "start": "358120",
    "end": "366919"
  },
  {
    "text": "a platform like AWS Lambda could take multiple seconds maybe 10 seconds for a cold boot um and that's just simply",
    "start": "366919",
    "end": "373440"
  },
  {
    "text": "spinning up the environment spinning up a container or a microv VM whatever they're running and getting an HTTP",
    "start": "373440",
    "end": "380440"
  },
  {
    "text": "server ready to handle that particular call or set of calls for the user before",
    "start": "380440",
    "end": "385720"
  },
  {
    "text": "then shutting down so cold boot has been a big blocker and it's primarily the initialization time of the application",
    "start": "385720",
    "end": "392759"
  },
  {
    "text": "before handling jobs on gpus and machine learning exponentially harder reason",
    "start": "392759",
    "end": "399759"
  },
  {
    "text": "being we're running 20 gab models those models can't be taking up Ram before a",
    "start": "399759",
    "end": "405960"
  },
  {
    "text": "call comes in because that it's not serverless then you're just running and always on replica so the cold boot",
    "start": "405960",
    "end": "412160"
  },
  {
    "text": "problem is deeply exaggerated when you get to gpus because not only do you need to provision the gpus and the envir or",
    "start": "412160",
    "end": "420000"
  },
  {
    "text": "the container you need to load that model from dis onto CPU onto GPU that",
    "start": "420000",
    "end": "425759"
  },
  {
    "text": "process could take 10 minutes for some models and it's just been pretty huge blocker for most GPU use cases so for",
    "start": "425759",
    "end": "433919"
  },
  {
    "text": "that reason this product hasn't existed before definitely not trying to delve into the secret sauce if you will but",
    "start": "433919",
    "end": "440840"
  },
  {
    "text": "can you kind of lay the landscape of how you even start to think about that problem like what are some of the",
    "start": "440840",
    "end": "447000"
  },
  {
    "text": "different ways that you might address um and maybe different orgs you know as you",
    "start": "447000",
    "end": "452680"
  },
  {
    "text": "develop competition over time probably different people will take different approaches like how do you even think about that landscape because that seems",
    "start": "452680",
    "end": "458960"
  },
  {
    "text": "like a daunting task you know when you talk about 10 minutes to get it moved over and stuff that's huge like how do",
    "start": "458960",
    "end": "466120"
  },
  {
    "text": "you even start to approach the problem so this is definitely one of our most",
    "start": "466120",
    "end": "471199"
  },
  {
    "text": "prized pieces of Ip our cold boot Tex so can't dive too deep into the details no",
    "start": "471199",
    "end": "476240"
  },
  {
    "text": "worries whatever works what is publicly known you you got to think about getting",
    "start": "476240",
    "end": "481560"
  },
  {
    "text": "well firstly constraint constraint you cannot take up GPU Ram if you have a 40",
    "start": "481560",
    "end": "486960"
  },
  {
    "text": "GB a100 machine if you put a model into that Ram that portion of the Ram or like",
    "start": "486960",
    "end": "493599"
  },
  {
    "text": "that machine entirely if you're not virtualizing it it's just like it's taken you are paying for it it is dead",
    "start": "493599",
    "end": "500520"
  },
  {
    "text": "space if you're not using it um that's massive GPU burn without any",
    "start": "500520",
    "end": "506000"
  },
  {
    "text": "utilization um so constraint model sit in Ram okay at least GPU Ram um so when",
    "start": "506000",
    "end": "513599"
  },
  {
    "text": "we go about the cold boot problem what we're really thinking about is how do we get the model specifically the weights",
    "start": "513599",
    "end": "519039"
  },
  {
    "text": "as close to Ram as possible without actually occupying resources or you know more precious",
    "start": "519039",
    "end": "525320"
  },
  {
    "text": "compute resources like 40 gigs of limited Ram that's hard but if you have",
    "start": "525320",
    "end": "531040"
  },
  {
    "text": "a terabyte of storage on the machine uh you could at least have local caching the bottle so like you could take that",
    "start": "531040",
    "end": "537320"
  },
  {
    "text": "up passively between calls without incurring you know sacrificing that piece of Hardware because you could fit",
    "start": "537320",
    "end": "543760"
  },
  {
    "text": "so many more models onto the disc got and then you could start thinking about like you know how do you start pre-caching this on the CPU if the CPU",
    "start": "543760",
    "end": "550240"
  },
  {
    "text": "has enough ramp um not saying that's something we do but these are like the framework in which you would start",
    "start": "550240",
    "end": "555720"
  },
  {
    "text": "thinking about it is how do we get that Ram as or that model as close to GPU Ram without actually taking up GPU Ram",
    "start": "555720",
    "end": "561959"
  },
  {
    "text": "because in the end GPU Ram is that's where the cost goes because once you use",
    "start": "561959",
    "end": "567160"
  },
  {
    "text": "that that machine is tied up it's not usable for anything else in your experience I mean I know you've been likely talking to tons of different",
    "start": "567160",
    "end": "575120"
  },
  {
    "text": "clients different use cases like that are really kind of thinking about how",
    "start": "575120",
    "end": "580800"
  },
  {
    "text": "how their workflows could adapt to the serverless workflow I'm just thinking about my own workflows like we're",
    "start": "580800",
    "end": "586079"
  },
  {
    "text": "running a lot of models but none of our models on my team are like receiving",
    "start": "586079",
    "end": "592240"
  },
  {
    "text": "thousands of inferences per second or something like that like it is very much",
    "start": "592240",
    "end": "597680"
  },
  {
    "text": "like in this Zone where we kind of get a burst of activity and then we're kind of",
    "start": "597680",
    "end": "603519"
  },
  {
    "text": "down you know for a bit not getting that much and then maybe another burst uh",
    "start": "603519",
    "end": "608680"
  },
  {
    "text": "that we need to process um so in that case like I would probably be willing in",
    "start": "608680",
    "end": "614800"
  },
  {
    "text": "my own use cases to put up with somewhat like longer of a cold start like",
    "start": "614800",
    "end": "621480"
  },
  {
    "text": "response for the model when it comes up and then subsequent ones during that burst being much faster what have you",
    "start": "621480",
    "end": "628880"
  },
  {
    "text": "noticed with clients like what is the tolerance there like where are you trying to get and where do you think is",
    "start": "628880",
    "end": "635079"
  },
  {
    "text": "like reasonable for most workflows I guess I don't have a perfect answer for",
    "start": "635079",
    "end": "640760"
  },
  {
    "text": "you on this in that ideally cold boots are zero yes that that that's true I",
    "start": "640760",
    "end": "647680"
  },
  {
    "text": "guess um on banana on a serverless platform in general unfortunately you do",
    "start": "647680",
    "end": "652839"
  },
  {
    "text": "have to start thinking about the servers because you want to avoid cold boots when avoidable in the case of Ana if you",
    "start": "652839",
    "end": "660200"
  },
  {
    "text": "have a model uh it's undergone a call Boot it's handled the first call it's ready to go we have it configured to",
    "start": "660200",
    "end": "667639"
  },
  {
    "text": "hang around for 10 seconds just in case more calls come in and that 10 seconds is completely configurable by the user",
    "start": "667639",
    "end": "675040"
  },
  {
    "text": "so if no calls come in we consider it okay we've gone through the surge we could scale down that particular replica",
    "start": "675040",
    "end": "681200"
  },
  {
    "text": "scales itself down if calls start coming in again cold boots are incurred again",
    "start": "681200",
    "end": "686560"
  },
  {
    "text": "um only if the existing replication you have can can't handle that throughput it starts scaling up more so because we",
    "start": "686560",
    "end": "694440"
  },
  {
    "text": "give users the ability to like fine-tune their autoscaler in a sense or fine tune",
    "start": "694440",
    "end": "699680"
  },
  {
    "text": "maybe configure yeah configure you can configure the autoscaler um so we have",
    "start": "699680",
    "end": "705279"
  },
  {
    "text": "some users who choose to run always on replicas with a minimum replica count so at any given time maybe you have a",
    "start": "705279",
    "end": "712079"
  },
  {
    "text": "baseline of two gpus running but you could surge to 20 if you need um so we",
    "start": "712079",
    "end": "717920"
  },
  {
    "text": "have some users doing that we have some some users who have gone away from the default 10 seconds uh idle time to go",
    "start": "717920",
    "end": "723760"
  },
  {
    "text": "longer cuz they know like they would rather pay for those gpus to be up and handle any traffic that may come in than",
    "start": "723760",
    "end": "732000"
  },
  {
    "text": "have more frequent cold boots reason I give that context about banana is I've",
    "start": "732000",
    "end": "737440"
  },
  {
    "text": "been really surprised by how few users increase their Idol time right now or at",
    "start": "737440",
    "end": "744320"
  },
  {
    "text": "least the majority of the customers we're serving are more price sensitive than latency sensitive at least least",
    "start": "744320",
    "end": "749920"
  },
  {
    "text": "given the general tradeoff we give them um in that they could configure the idol timeout and through that tune how much",
    "start": "749920",
    "end": "756040"
  },
  {
    "text": "they pay versus how much they wait but most users would rather have machine shut down and then incur that cold start",
    "start": "756040",
    "end": "762639"
  },
  {
    "text": "time and that's a great thing for us because that allows us to chip away at this cold start problem and give users",
    "start": "762639",
    "end": "769680"
  },
  {
    "text": "an exclusively better experience of the faster your cold starts are the more willing users are to take those cold",
    "start": "769680",
    "end": "775720"
  },
  {
    "text": "starts because it's less impactful on their inferences um and the less idle",
    "start": "775720",
    "end": "781720"
  },
  {
    "text": "time you could run on your gpus following calls before they start shutting down cuz it's not as risky you",
    "start": "781720",
    "end": "787160"
  },
  {
    "text": "[Music]",
    "start": "787160",
    "end": "804000"
  },
  {
    "text": "know Hello friends this is Jared here to tell you about change log Plus Plus+",
    "start": "804000",
    "end": "810360"
  },
  {
    "text": "over the years many of our most DieHard listeners have asked us for ways they",
    "start": "810360",
    "end": "815440"
  },
  {
    "text": "can support our work here at Chang log we didn't have an answer for them for a long time but finally we created Chang",
    "start": "815440",
    "end": "822880"
  },
  {
    "text": "log Plus+ a membership you can join to directly support our work as a thank you",
    "start": "822880",
    "end": "829079"
  },
  {
    "text": "we save you some time with an adree feed sprinkle in bonuses like extended",
    "start": "829079",
    "end": "834440"
  },
  {
    "text": "episodes and give you first access to the new stuff we dream up learn all",
    "start": "834440",
    "end": "839480"
  },
  {
    "text": "about it at changel log.com slpl plus you'll also find the link in your",
    "start": "839480",
    "end": "844519"
  },
  {
    "text": "chapter data and show notes once again that's Chang log.com plusus plus check",
    "start": "844519",
    "end": "850120"
  },
  {
    "text": "it out we'd love to have you with [Music]",
    "start": "850120",
    "end": "860639"
  },
  {
    "text": "us so as you were kind of describing that that was really it's a very interesting mesh of skills it seems to",
    "start": "860639",
    "end": "867600"
  },
  {
    "text": "do what you're doing there because you obviously have to have a pretty good understanding of deep learning in",
    "start": "867600",
    "end": "873639"
  },
  {
    "text": "general and kind of the AI space and the performance characteristics around that",
    "start": "873639",
    "end": "878720"
  },
  {
    "text": "but you also have to go very very deep in terms of uh network engineering and",
    "start": "878720",
    "end": "885440"
  },
  {
    "text": "Architectural considerations and such like that it also kind of brings",
    "start": "885440",
    "end": "890560"
  },
  {
    "text": "different cultures together for instance in terms of like the choices of",
    "start": "890560",
    "end": "896160"
  },
  {
    "text": "languages and stuff distinctly like do you tend to go with one language for",
    "start": "896160",
    "end": "902639"
  },
  {
    "text": "everything for Simplicity sake or do you tend to go with different languages that are catered towards specific use cases",
    "start": "902639",
    "end": "909360"
  },
  {
    "text": "by way of example like python for deep learning specific things and um like",
    "start": "909360",
    "end": "915000"
  },
  {
    "text": "rust you know or something C++ for infrastructure thing how do you or do you stick with one like python for",
    "start": "915000",
    "end": "921600"
  },
  {
    "text": "everything because that way you have a simpler setup to govern what how do you take that strategy wise so the obvious",
    "start": "921600",
    "end": "927399"
  },
  {
    "text": "language for hosting ml model inference is python it's almost a requisite as in",
    "start": "927399",
    "end": "933160"
  },
  {
    "text": "all of our users are running in it um so therefore the framework that we give users to build off of which is",
    "start": "933160",
    "end": "938680"
  },
  {
    "text": "essentially boiler plate for a server uh that's written in Python we don't need",
    "start": "938680",
    "end": "944560"
  },
  {
    "text": "to maintain that too much it's an extremely simple HTTP wrapper and the vast majority of our work on the",
    "start": "944560",
    "end": "950279"
  },
  {
    "text": "pipeline the infrastructure side is all done in go so uh we're probably 95% go",
    "start": "950279",
    "end": "956800"
  },
  {
    "text": "we have some typescript for our web app uh some nextjs that we're running and",
    "start": "956800",
    "end": "962759"
  },
  {
    "text": "then when you get deep into like the run time we work on C++ and Cuda as well but",
    "start": "962759",
    "end": "969680"
  },
  {
    "text": "that's a small subset of our engineering team works at that level the majority of us write pipelines and networks um",
    "start": "969680",
    "end": "976360"
  },
  {
    "text": "within go I got to say it's kind of funny that you bring that up Daniel and I love go uh we actually met in the go",
    "start": "976360",
    "end": "982120"
  },
  {
    "text": "Community because we're both go we were like uh at the time kind of the two AI oriented people in the go community so",
    "start": "982120",
    "end": "988040"
  },
  {
    "text": "uh it's just a little bit ironic to to hear that that's awesome um I've been so",
    "start": "988040",
    "end": "993759"
  },
  {
    "text": "disappointed in Python I mean Python's amazing language it's where I learned my",
    "start": "993759",
    "end": "999079"
  },
  {
    "text": "first bit of serious general purpose programming was python uh but I'm saddened to know that the language you",
    "start": "999079",
    "end": "1006240"
  },
  {
    "text": "chose for GPU programming basically um is a language that like has a global",
    "start": "1006240",
    "end": "1012279"
  },
  {
    "text": "interpreter lock like you uh it does not have great multiprocessing built in I",
    "start": "1012279",
    "end": "1017920"
  },
  {
    "text": "wish go were the choice there uh it doesn't seem like it's going to happen but I'm a huge fan of go I think it's a",
    "start": "1017920",
    "end": "1023199"
  },
  {
    "text": "great language to write in and I could go on for a long time about this in fact one of the reasons I learned about the",
    "start": "1023199",
    "end": "1029240"
  },
  {
    "text": "Chang log network was listening to the go Time Podcast so yeah for sure shout",
    "start": "1029240",
    "end": "1035280"
  },
  {
    "text": "out shout out to that other podcast yeah definitely definitely it's cool to hear",
    "start": "1035280",
    "end": "1040839"
  },
  {
    "text": "about like I guess the setup of how you thought about this problem and how you",
    "start": "1040839",
    "end": "1045880"
  },
  {
    "text": "even structured the team and that sort of thing I'm wondering at this point if you could kind of just give us a sense",
    "start": "1045880",
    "end": "1051000"
  },
  {
    "text": "for like if I'm a data scientist if I'm a or even just a software engineer",
    "start": "1051000",
    "end": "1057080"
  },
  {
    "text": "trying to integrate a model into you know my stack like what does the workflow as of now look like uh for me",
    "start": "1057080",
    "end": "1065240"
  },
  {
    "text": "with banana like what do I do to get a model up and going and maybe just a",
    "start": "1065240",
    "end": "1070600"
  },
  {
    "text": "couple examples of that to give people a sense it's a bit hard on audio podcast but I'm sure you've done similar things",
    "start": "1070600",
    "end": "1076720"
  },
  {
    "text": "in the past so yeah well I I'd love to give a visual demo but going through audio wise generally the process looks",
    "start": "1076720",
    "end": "1083559"
  },
  {
    "text": "like this um a lot of people are building off of standard models so say a",
    "start": "1083559",
    "end": "1089120"
  },
  {
    "text": "stable diffusion or a whisper at least for like this current this hype wave of all these new exciting open source",
    "start": "1089120",
    "end": "1095480"
  },
  {
    "text": "models coming out until next week yeah until next week and then the next one comes out thankfully we have these",
    "start": "1095480",
    "end": "1101919"
  },
  {
    "text": "oneclick templates that you could use on banana so in a single click you could go from an open source model that somebody",
    "start": "1101919",
    "end": "1108120"
  },
  {
    "text": "has published on banana and bring that into your own account and start using it",
    "start": "1108120",
    "end": "1113200"
  },
  {
    "text": "yourself so within a few seconds you could have a functioning endpoint for popular models that have been put up by",
    "start": "1113200",
    "end": "1118840"
  },
  {
    "text": "the community and then we see naturally the Step Beyond that moving from you",
    "start": "1118840",
    "end": "1124440"
  },
  {
    "text": "effectively have an API you don't really know what's running behind the scenes um you can Fork that code you can start",
    "start": "1124440",
    "end": "1130360"
  },
  {
    "text": "working on it yourself and customizing it for your own use case uh so if you're",
    "start": "1130360",
    "end": "1135520"
  },
  {
    "text": "doing some fine-tuning if quite honestly you want to go away from the standard or like the big model templates and roll it",
    "start": "1135520",
    "end": "1143400"
  },
  {
    "text": "yourself just have whatever deep net that you've built um that's where you start getting into sort of the local Dev",
    "start": "1143400",
    "end": "1150360"
  },
  {
    "text": "iteration cycle and this is where I shout out a previous guest uh brev n over there at bre um we recommend users",
    "start": "1150360",
    "end": "1159720"
  },
  {
    "text": "go and have an interactive GPU environment so that you could load your model test it against some inference",
    "start": "1159720",
    "end": "1166480"
  },
  {
    "text": "payload shut it down iterate if you're doing something like a stable diffusion you want to make sure that the",
    "start": "1166480",
    "end": "1172360"
  },
  {
    "text": "image Transformations server side are happening correctly that's where you iterate um you're doing all of this",
    "start": "1172360",
    "end": "1178720"
  },
  {
    "text": "within the banana framework we have an HTTP framework you could find open source online that's generally the",
    "start": "1178720",
    "end": "1185120"
  },
  {
    "text": "building point for most users so you're modifying a function within that that is the inference function takes in some",
    "start": "1185120",
    "end": "1191600"
  },
  {
    "text": "Json runs the model returns some Json do that iteratively until you have your",
    "start": "1191600",
    "end": "1196960"
  },
  {
    "text": "customized model that works to the API you're uh hoping for and then",
    "start": "1196960",
    "end": "1202280"
  },
  {
    "text": "you push that to GitHub then from there you can go into banana you could select that repo and we have a CI P pipeline",
    "start": "1202280",
    "end": "1209080"
  },
  {
    "text": "built in so when you select that uh repo we build the model we deploy it every time you push to main we rebuild and",
    "start": "1209080",
    "end": "1216200"
  },
  {
    "text": "redeploy so um we generally recommend users to if they're shipping uh new fine",
    "start": "1216200",
    "end": "1222200"
  },
  {
    "text": "tune versions it's usually them updating say a link to an S3 uh then in the build",
    "start": "1222200",
    "end": "1227640"
  },
  {
    "text": "pipeline we bundle that model into the Container itself and get that deployed to the gpus so kind of curious and this",
    "start": "1227640",
    "end": "1234480"
  },
  {
    "text": "is sort of a followup largely because of the medium we're in since we're Audio Only and we don't have the ability to to",
    "start": "1234480",
    "end": "1240760"
  },
  {
    "text": "show the process that you're describing just for clarity like your typical",
    "start": "1240760",
    "end": "1246440"
  },
  {
    "text": "customer sluser what skills would they typically have to productively use",
    "start": "1246440",
    "end": "1252200"
  },
  {
    "text": "banana um you know what are those necessary minimum skills for them to be able to really engage productive and",
    "start": "1252200",
    "end": "1259080"
  },
  {
    "text": "move through things a lot of our users are quite surprisingly full stack engineers and not deep experienced data",
    "start": "1259080",
    "end": "1266360"
  },
  {
    "text": "people and ml people uh so as long as you can wrap your head around using",
    "start": "1266360",
    "end": "1271760"
  },
  {
    "text": "Frameworks or abstractions like hugging face for example if you could use a pipeline like that pull it locally",
    "start": "1271760",
    "end": "1278360"
  },
  {
    "text": "that's something you could deploy into banana so some python expertise in order to write the code in the first place",
    "start": "1278360",
    "end": "1284679"
  },
  {
    "text": "it's an HTTP server so you write that you wrap it around say buing face model you don't need to fine-tune it you could",
    "start": "1284679",
    "end": "1290880"
  },
  {
    "text": "use the standard models and then learn fine-tuning later and ideally you do",
    "start": "1290880",
    "end": "1295960"
  },
  {
    "text": "have some knowledge of Docker ultimately what is deployed to Banana is a Docker file if you build within our template",
    "start": "1295960",
    "end": "1301880"
  },
  {
    "text": "generally you don't need to do things that are too custom unless you choose to but a little bit of knowledge of Docker",
    "start": "1301880",
    "end": "1307559"
  },
  {
    "text": "helps so python hugging face Docker that's effectively all you need in order to get something deployed onto banana",
    "start": "1307559",
    "end": "1314159"
  },
  {
    "text": "I'm just on the site now and kind of looking through some of your um community templates which are pretty",
    "start": "1314159",
    "end": "1320200"
  },
  {
    "text": "cool I mean you have all sorts of things um Coden T5 Santa coder all sorts of",
    "start": "1320200",
    "end": "1327400"
  },
  {
    "text": "things with a sort of oneclick deploy button to get them up and going one question I had um just like when I",
    "start": "1327400",
    "end": "1334880"
  },
  {
    "text": "deploy um because it looks like based on your docs I can call it with like the",
    "start": "1334880",
    "end": "1339960"
  },
  {
    "text": "model ID from python for example so I could like integrate this directly in a",
    "start": "1339960",
    "end": "1345279"
  },
  {
    "text": "python app can I also call it sort of like as as a rest endpoint or something",
    "start": "1345279",
    "end": "1350480"
  },
  {
    "text": "like that or is the primary use case a client integration we do have public",
    "start": "1350480",
    "end": "1355840"
  },
  {
    "text": "documentation for the rest endpoint so awesome it's not officially supported we",
    "start": "1355840",
    "end": "1361480"
  },
  {
    "text": "try to encourage people to go through our official sdks which at this point our python uh typescript go in Rust um",
    "start": "1361480",
    "end": "1369400"
  },
  {
    "text": "that said anyone who wants to go directly into the rest endpoint there's documentation to do so we like being",
    "start": "1369400",
    "end": "1375159"
  },
  {
    "text": "able to boil it down to a simple banana. run function where you just give a model key you give whatever Json in you want",
    "start": "1375159",
    "end": "1382120"
  },
  {
    "text": "your server to process and then you receive the Json out from that but our goal is to be able to give people access",
    "start": "1382120",
    "end": "1388600"
  },
  {
    "text": "to the levels of extraction that they choose to run in for example because we have a public rest endpoint people have",
    "start": "1388600",
    "end": "1395200"
  },
  {
    "text": "integrated banana into their Swift applications or into their Ruby applications so it's an hhtp call in the",
    "start": "1395200",
    "end": "1401760"
  },
  {
    "text": "end people uh could unwrap our apis and go at it directly feel free yeah I guess",
    "start": "1401760",
    "end": "1407559"
  },
  {
    "text": "that leads right into my next question which is um does anything stand out in terms of like how people are using this",
    "start": "1407559",
    "end": "1415440"
  },
  {
    "text": "serverless workflow that maybe surprise you um based on what you're seeing I've been amazed at the quantity of fine",
    "start": "1415440",
    "end": "1421960"
  },
  {
    "text": "tunes that are deployed through banana if you look at at the analytics of people deploying from our oneclick",
    "start": "1421960",
    "end": "1428039"
  },
  {
    "text": "templates versus people deploying from Custom repost 80% are custom repos and",
    "start": "1428039",
    "end": "1434360"
  },
  {
    "text": "that means that people are coming to serverless because they have a unique a API that they need to run somewhere and",
    "start": "1434360",
    "end": "1440720"
  },
  {
    "text": "that they can't simply run with a standard API provider or even an API provider with fine tuning features like",
    "start": "1440720",
    "end": "1447480"
  },
  {
    "text": "they want to be able to own the API themselves own the application logic themselves the fine tun themselves and",
    "start": "1447480",
    "end": "1454039"
  },
  {
    "text": "just dockerize that up and send it onto banana so vast majority of our users are doing custom workloads which to me was",
    "start": "1454039",
    "end": "1460400"
  },
  {
    "text": "surprising we little banana lore we previously started as an ml as an API",
    "start": "1460400",
    "end": "1465880"
  },
  {
    "text": "company the idea of showing up click the you want and you get an API for that and",
    "start": "1465880",
    "end": "1472080"
  },
  {
    "text": "there's a lot of pull there especially right now with the hype there's so many people who want to integrate AI into",
    "start": "1472080",
    "end": "1478399"
  },
  {
    "text": "their applications without touching the AI at all so it has been surprising for us seeing how many people are running",
    "start": "1478399",
    "end": "1483840"
  },
  {
    "text": "custom code on us and it's been validating of the idea that the platform approach versus the API approach has",
    "start": "1483840",
    "end": "1489159"
  },
  {
    "text": "been the way to go could you kind of walk us through what a typical one might",
    "start": "1489159",
    "end": "1494279"
  },
  {
    "text": "look like you know where where someone's doing that kind of custom thing just to give us a sense of what it is that you're seeing uh whether it's you know",
    "start": "1494279",
    "end": "1501360"
  },
  {
    "text": "fictional but realistic or a real case example whatever works for you so one thing users are doing just as a very",
    "start": "1501360",
    "end": "1507880"
  },
  {
    "text": "basic example of if latency is an extremely sensitive thing for them and",
    "start": "1507880",
    "end": "1513279"
  },
  {
    "text": "cold boots are particularly painful what they'll do is they'll engineer a conditional like a Boolean in the Json",
    "start": "1513279",
    "end": "1520279"
  },
  {
    "text": "that they send in that's called the warm-up so they'll do like warmup equals true and make it so that server side",
    "start": "1520279",
    "end": "1526279"
  },
  {
    "text": "they actually don't perform any heavy computation it's just intended as a warm-up call so if architecturally they",
    "start": "1526279",
    "end": "1532679"
  },
  {
    "text": "need servers running like fully warmed up by the time the actual inference starts running they engineer this into",
    "start": "1532679",
    "end": "1540039"
  },
  {
    "text": "their Endo um another thing as well if people want to run fine tunes or run",
    "start": "1540039",
    "end": "1545120"
  },
  {
    "text": "multiple models side by side and start doing some model chaining uh we see people building that into banana as well",
    "start": "1545120",
    "end": "1553120"
  },
  {
    "text": "um and then lastly are just basically state-ofthe-art moves so fast",
    "start": "1553120",
    "end": "1558760"
  },
  {
    "text": "right now that the second stable to Fusion launch for example suddenly there's inpainting and inpainting is the",
    "start": "1558760",
    "end": "1565120"
  },
  {
    "text": "next thing that came out a week later and that's some random code people found in a GitHub and they integrated",
    "start": "1565120",
    "end": "1570399"
  },
  {
    "text": "themselves um so customization on that sense allows users to stay as far ahead",
    "start": "1570399",
    "end": "1576840"
  },
  {
    "text": "as they possibly can if it's necessary for their use case could you highlight",
    "start": "1576840",
    "end": "1582320"
  },
  {
    "text": "something you have in your mind as maybe like a workflow that would not be appropriate for the sort of serverless",
    "start": "1582320",
    "end": "1590320"
  },
  {
    "text": "GPU um infrastructure so I think this like like you say fine-tune models",
    "start": "1590320",
    "end": "1596440"
  },
  {
    "text": "inferencing like you know using these state-of-the-art templates is there something where you would say hey like",
    "start": "1596440",
    "end": "1602960"
  },
  {
    "text": "maybe that's not fitting for the serverless uh use case yeah so in",
    "start": "1602960",
    "end": "1608159"
  },
  {
    "text": "inference land if you have completely steady traffic all the time don't use",
    "start": "1608159",
    "end": "1613559"
  },
  {
    "text": "serverless you'll get unnecessary cold boots and it just slows down on your inference and you're paying effectively",
    "start": "1613559",
    "end": "1620360"
  },
  {
    "text": "the same um so that's the inference side training side we like to think that you",
    "start": "1620360",
    "end": "1626720"
  },
  {
    "text": "could currently train on banana though I often find that training is a more interactive experience or at least in",
    "start": "1626720",
    "end": "1633480"
  },
  {
    "text": "like the initial prototyping phase once you have pipelines built in to say",
    "start": "1633480",
    "end": "1638559"
  },
  {
    "text": "automatically collect data and batch train that actually does work on banana because you can just fire that data as",
    "start": "1638559",
    "end": "1643799"
  },
  {
    "text": "the payload train the model server side upload it to S3 return the call and then the replica shuts down but most training",
    "start": "1643799",
    "end": "1652080"
  },
  {
    "text": "jobs or most like exploratory training jobs I would not recommend doing on serverless in part just due to the like",
    "start": "1652080",
    "end": "1659600"
  },
  {
    "text": "the observability that you need to see uh the tracing setting up things like you know this is outdated Tech but",
    "start": "1659600",
    "end": "1665039"
  },
  {
    "text": "tensor board um those more visualization tools also keep in mind I'm not a",
    "start": "1665039",
    "end": "1670080"
  },
  {
    "text": "training expert so um perhaps there's there's space in the training that um people would see value in serverless but",
    "start": "1670080",
    "end": "1677320"
  },
  {
    "text": "generally I'd Rec Rec avoiding serverless um and then lastly if you",
    "start": "1677320",
    "end": "1682720"
  },
  {
    "text": "have any jobs that are batched as in you know exactly when they're going to",
    "start": "1682720",
    "end": "1688159"
  },
  {
    "text": "happen it's a bit easier to automate your own infrastructure and build it yourself to do that ideally we make",
    "start": "1688159",
    "end": "1693919"
  },
  {
    "text": "serverless so good that you don't need to think about that but I think in the current state of serverless a lot of batch processing jobs if you're say",
    "start": "1693919",
    "end": "1700679"
  },
  {
    "text": "running an indexer across um an internal database and you don't need to have it",
    "start": "1700679",
    "end": "1706360"
  },
  {
    "text": "running all the time that's where running on serverless maybe a bit too much lift in order to Port it into",
    "start": "1706360",
    "end": "1712399"
  },
  {
    "text": "serverless versus just doing it yourself I was looking I'm also looking through your website while we're talking uh and",
    "start": "1712399",
    "end": "1718360"
  },
  {
    "text": "I'm I'm in the docks and uh I kind of Hit the SDK area which you kind of talked about a little bit ago uh with",
    "start": "1718360",
    "end": "1724679"
  },
  {
    "text": "the different sdks and python node go rest um did you mention rust earlier or",
    "start": "1724679",
    "end": "1729880"
  },
  {
    "text": "did I mishar that as rust I may have misheard something I did mention rust I",
    "start": "1729880",
    "end": "1735080"
  },
  {
    "text": "actually don't know if we have it documented we launched it two days ago I recall gotcha well so the thing that got",
    "start": "1735080",
    "end": "1740799"
  },
  {
    "text": "me thinking here that's very Leading Edge it's very like out there I'm kind",
    "start": "1740799",
    "end": "1746559"
  },
  {
    "text": "of getting the sense that your your customers are adopting more Le you know",
    "start": "1746559",
    "end": "1751640"
  },
  {
    "text": "forward-leaning languages in general for what they're doing um and that's why",
    "start": "1751640",
    "end": "1756799"
  },
  {
    "text": "they're you know they're leaning forward into this new concept of serverless gpus is that is that consistent with what",
    "start": "1756799",
    "end": "1763880"
  },
  {
    "text": "you're seeing are you really kind of targeting the types of software developers that are uh kind of early",
    "start": "1763880",
    "end": "1770919"
  },
  {
    "text": "adopters uh Paving the way versus somebody that's maybe in some of the older more enterpris languages maybe not",
    "start": "1770919",
    "end": "1779519"
  },
  {
    "text": "quite as uh you know risk-taking and such that's very much in line of what we've been seeing we find that a lot of",
    "start": "1779519",
    "end": "1785760"
  },
  {
    "text": "our users are adamant versell users as an example uh so they're in nextra ass",
    "start": "1785760",
    "end": "1791200"
  },
  {
    "text": "they've chosen a relatively modern framework to build their front end apps in um and they make the same decisions",
    "start": "1791200",
    "end": "1797360"
  },
  {
    "text": "for their back end uh they're often typescript forward if they want to do systems level they'll do rust or go uh",
    "start": "1797360",
    "end": "1803399"
  },
  {
    "text": "for these reasons we've chosen to offer uh these official sdks yeah that's",
    "start": "1803399",
    "end": "1808559"
  },
  {
    "text": "really interesting um I one of my questions in kind of thinking about this is like the different use cases that you",
    "start": "1808559",
    "end": "1814600"
  },
  {
    "text": "could have the different industries that are rapidly adopting AI integrating it in their in their software Stacks like",
    "start": "1814600",
    "end": "1820880"
  },
  {
    "text": "everybody's adopting AI right but um like it's certainly making a lot of strides in certain areas and certain",
    "start": "1820880",
    "end": "1827960"
  },
  {
    "text": "Industries like you know let's say Healthcare or something like that have very unique constraints around even like",
    "start": "1827960",
    "end": "1835360"
  },
  {
    "text": "their own inference data leaving to go like to some hosted model somewhere",
    "start": "1835360",
    "end": "1842200"
  },
  {
    "text": "that's not in their own infrastructure but in other words when I go to Banana I",
    "start": "1842200",
    "end": "1848480"
  },
  {
    "text": "see like all I have to care about is like deploying a model there's my model ID right I can think about like the",
    "start": "1848480",
    "end": "1855679"
  },
  {
    "text": "timeout and all of that it's all very function right and I don't even to like a thought for where that's running I",
    "start": "1855679",
    "end": "1862600"
  },
  {
    "text": "could see like the opposite end of that like certain industries would probably be a little bit uncomfortable with that",
    "start": "1862600",
    "end": "1867960"
  },
  {
    "text": "but there's a whole lot of developers that are just wanting to like you know",
    "start": "1867960",
    "end": "1874159"
  },
  {
    "text": "bootstrap these like amazing AI powered things like very rapidly there's so many",
    "start": "1874159",
    "end": "1879720"
  },
  {
    "text": "things coming to Market like that um so I guess that would be fitting in in that way do you have any plans in the future",
    "start": "1879720",
    "end": "1886600"
  },
  {
    "text": "for like B banana serverless but like connect my AWS infrastructure or",
    "start": "1886600",
    "end": "1892159"
  },
  {
    "text": "something like that to run um in the banana way or or something like that",
    "start": "1892159",
    "end": "1897559"
  },
  {
    "text": "short answer yes long answer it's complicated it's going to be a long time yeah it's very complicated and one of",
    "start": "1897559",
    "end": "1904519"
  },
  {
    "text": "the things that we see with serverless is the fact that we have economies of scale sharing everyone as tenants within",
    "start": "1904519",
    "end": "1910279"
  },
  {
    "text": "our Cloud because that allows us to do more efficient bin packing and make it so that when you're not using a server",
    "start": "1910279",
    "end": "1916760"
  },
  {
    "text": "like when the server contains is shut down you're not charged if you're running on your own cloud you still need to have the underlying resources running",
    "start": "1916760",
    "end": "1924240"
  },
  {
    "text": "we're a venture scill business we want to hit that million dollar annual revenue ideally or sorry not million",
    "start": "1924240",
    "end": "1930639"
  },
  {
    "text": "dollar $100 million annual revenue ideally more um and I think getting into that we're eventually going to have to",
    "start": "1930639",
    "end": "1936159"
  },
  {
    "text": "start thinking about how to more traditional Enterprises integrate this though choosing our Niche right now we",
    "start": "1936159",
    "end": "1941799"
  },
  {
    "text": "see significant poll that could get us to one $10 million annual just from",
    "start": "1941799",
    "end": "1947279"
  },
  {
    "text": "these like new teams who aren't Bound by such constraints of needing to run in their own cloud so long answer restated",
    "start": "1947279",
    "end": "1955039"
  },
  {
    "text": "we'll get to it eventually and I'm sure it's like it'll be a necessary part of the product but it loses out on a lot of",
    "start": "1955039",
    "end": "1960600"
  },
  {
    "text": "the magic that we're currently providing so we'd rather just focus on these new and upcoming startups that are running",
    "start": "1960600",
    "end": "1966639"
  },
  {
    "text": "on us yeah that makes a lot of sense I think um it does make me wonder like",
    "start": "1966639",
    "end": "1973200"
  },
  {
    "text": "because you are creating so much magic for the users and a lot of that like",
    "start": "1973200",
    "end": "1979159"
  },
  {
    "text": "you're saying like thinking about like what gpus are you spinning up like how are you bidding on these like where are",
    "start": "1979159",
    "end": "1985919"
  },
  {
    "text": "you like how are you allocating them have you learned any sort of like",
    "start": "1985919",
    "end": "1991159"
  },
  {
    "text": "General like you can get gpus from a lot of places there's a lot of different kind of scales of pricing um there's a",
    "start": "1991159",
    "end": "1999159"
  },
  {
    "text": "lot of different ways to run gpus in the cloud have you found any just sort of",
    "start": "1999159",
    "end": "2005000"
  },
  {
    "text": "like good practices or things that that you found to be useful just generally in",
    "start": "2005000",
    "end": "2010519"
  },
  {
    "text": "terms of thinking about like using gpus in the cloud yeah that You' love to pass",
    "start": "2010519",
    "end": "2016200"
  },
  {
    "text": "on to listeners so we use this phrase called skate ahead at the puck uh it's",
    "start": "2016200",
    "end": "2022320"
  },
  {
    "text": "phrase from hockey where don't go to where the puck is go to where it's going um so applying that to autoscaling Auto",
    "start": "2022320",
    "end": "2029440"
  },
  {
    "text": "scaling really has two components you're autoscaling the underlying nodes the",
    "start": "2029440",
    "end": "2035559"
  },
  {
    "text": "hardware that's running the gpus like that's you know running the kubernetes cluster whatever your deployment Target",
    "start": "2035559",
    "end": "2041720"
  },
  {
    "text": "is um and then secondly you're autoscaling the deployments themselves going from replication of 0 to one to",
    "start": "2041720",
    "end": "2048638"
  },
  {
    "text": "many within the confines of whatever nodes you have set up uh so you're",
    "start": "2048639",
    "end": "2054040"
  },
  {
    "text": "effectively autoscaling two things kubernetes pots and the nodes themselves",
    "start": "2054040",
    "end": "2059079"
  },
  {
    "text": "so my recommendation are if people are building things like this in house what they should absolutely do is use a",
    "start": "2059079",
    "end": "2064679"
  },
  {
    "text": "platform that has an automation API for the underlying VMS right now GPU cloud",
    "start": "2064679",
    "end": "2071280"
  },
  {
    "text": "is sort of the Wild West there's a lot of new players um traditional hyperscaler uh clouds like Google Cloud",
    "start": "2071280",
    "end": "2078320"
  },
  {
    "text": "AWS Azure they have the automation but the GPU prices are not as competitive as",
    "start": "2078320",
    "end": "2083960"
  },
  {
    "text": "you could get on some of these newer clouds so uh my biggest recommendation for people building mature systems would",
    "start": "2083960",
    "end": "2091320"
  },
  {
    "text": "be to like choose a provider that you get ideally guaranteed access to gpus",
    "start": "2091320",
    "end": "2097480"
  },
  {
    "text": "which which allows you to scale your gpus up ahead of the demand of whatever workloads you're running within your",
    "start": "2097480",
    "end": "2103040"
  },
  {
    "text": "cluster and then doesn't have to be homogeneous the workloads deployed um",
    "start": "2103040",
    "end": "2108240"
  },
  {
    "text": "just as long as you maintain GPU capacity to handle those you should be good but because you're autoscaling like",
    "start": "2108240",
    "end": "2115880"
  },
  {
    "text": "the applications within kubernetes allows you to have a little more lead time for like super slow scale UPS on",
    "start": "2115880",
    "end": "2121560"
  },
  {
    "text": "the gpus this has been a super instructive conversation I'm I'm learning a lot I want",
    "start": "2121560",
    "end": "2128200"
  },
  {
    "text": "extend your analogy one question further because you're talking about skating ahead of the puck not skating to the",
    "start": "2128200",
    "end": "2134440"
  },
  {
    "text": "puck but where it's going to go you are pioneering this field you are out there",
    "start": "2134440",
    "end": "2139599"
  },
  {
    "text": "on the front you are leaning forward and you are supporting other people in other organizations that are trying to lean",
    "start": "2139599",
    "end": "2145920"
  },
  {
    "text": "forward as well so I'm going to ask you where is the puck going you know short-term middle middle longterm how do",
    "start": "2145920",
    "end": "2153200"
  },
  {
    "text": "you see the future for those who are not in your industry but are are going to be",
    "start": "2153200",
    "end": "2158240"
  },
  {
    "text": "supported by you tell us the vision what's it going to fine tunes are going to be huge I think there's two camps for",
    "start": "2158240",
    "end": "2165960"
  },
  {
    "text": "where AI is going to be going there's the the one model rule them all Camp which is there's going to be some Mega",
    "start": "2165960",
    "end": "2172079"
  },
  {
    "text": "model that does everything and then there's the other Camp which is what we're leaning into which is um the best",
    "start": "2172079",
    "end": "2178440"
  },
  {
    "text": "model for you as a user is a model that's trained on data from you specifically you and we see customers",
    "start": "2178440",
    "end": "2185079"
  },
  {
    "text": "deploying fine tunes on us not just for their use case but for their end user",
    "start": "2185079",
    "end": "2190240"
  },
  {
    "text": "imagine you are building a writing assistant app how do you find tune for every single one of your end users and",
    "start": "2190240",
    "end": "2197200"
  },
  {
    "text": "deploy that and make it so that that user has a unique model it's essentially a companion uh almost a clone of them",
    "start": "2197200",
    "end": "2205640"
  },
  {
    "text": "and where the puck is going is where we every human on earth just like they have a phone in their pocket they're going to",
    "start": "2205640",
    "end": "2210920"
  },
  {
    "text": "have a fleet of models fine tune just on them and that's one thing we're excited about with serverless is in order to do",
    "start": "2210920",
    "end": "2217359"
  },
  {
    "text": "that viably you got to have serverless can't have it running all the time so very excited in this sense if you're not",
    "start": "2217359",
    "end": "2222839"
  },
  {
    "text": "looking into user level fine tunes I think it's a very interesting space to be in because it gets you so much",
    "start": "2222839",
    "end": "2228319"
  },
  {
    "text": "further than any application Level stuff you could do to make the experience better that's awesome yeah I think",
    "start": "2228319",
    "end": "2234720"
  },
  {
    "text": "that's a super exciting way to close out the conversation this is a really exciting time to be in the space both in",
    "start": "2234720",
    "end": "2242359"
  },
  {
    "text": "terms of what's possible with fine-tuning and those sorts of Technologies but also like new",
    "start": "2242359",
    "end": "2247560"
  },
  {
    "text": "infrastructure coming up like what you're what you're building so thanks so much for taking time to chat with us",
    "start": "2247560",
    "end": "2253040"
  },
  {
    "text": "Eric it's been a real pleasure this is awesome appreciate it [Music]",
    "start": "2253040",
    "end": "2265040"
  },
  {
    "text": "guys thank you for listening to practical AI your next step is to",
    "start": "2265040",
    "end": "2270319"
  },
  {
    "text": "subscribe now if you haven't already and if you're a longtime listener of the show help us reach more people by",
    "start": "2270319",
    "end": "2276680"
  },
  {
    "text": "sharing practic AI with your friends and colleagues thanks once again to fastly and fly for partnering with us to bring",
    "start": "2276680",
    "end": "2282960"
  },
  {
    "text": "you all Chang doog podcasts check out what they're up to at fastly.com and",
    "start": "2282960",
    "end": "2288160"
  },
  {
    "text": "fly.io and to our beat freaking residents breakmaster cylinder for continuously cranking out the best beats",
    "start": "2288160",
    "end": "2293920"
  },
  {
    "text": "in the biz that's all for now we'll talk to you again next [Music]",
    "start": "2293920",
    "end": "2303619"
  },
  {
    "text": "time k",
    "start": "2306440",
    "end": "2312839"
  }
]