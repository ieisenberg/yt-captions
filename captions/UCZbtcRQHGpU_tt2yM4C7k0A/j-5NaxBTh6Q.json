[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "we are here with David croshaw CTO and co-founder of tail scale tail scale",
    "start": "120",
    "end": "6960"
  },
  {
    "text": "David welcome to the change log yeah um I'm not actually the CTO anymore oh no",
    "start": "6960",
    "end": "12160"
  },
  {
    "text": "your LinkedIn is outdated oh does it still say that I thought I had updated it I'm sorry are you masquerading David",
    "start": "12160",
    "end": "17800"
  },
  {
    "text": "that's real time LinkedIn updates here be doit LinkedIn updates go I read it",
    "start": "17800",
    "end": "23640"
  },
  {
    "text": "somewhere usually snag before the show starts I think my LinkedIn uh it might",
    "start": "23640",
    "end": "28840"
  },
  {
    "text": "be confusing because it still that I was the CTO I stepped back from the CTO last year okay so what are you doing now I am",
    "start": "28840",
    "end": "36239"
  },
  {
    "text": "you know spending my time exploring sort of new product spaces things that can be done so both inside and outside of tail",
    "start": "36239",
    "end": "44039"
  },
  {
    "text": "scale so very cool um most of my work inside tail scale is around helping on",
    "start": "44039",
    "end": "49320"
  },
  {
    "text": "the sort of uh customer side you know talking to users or potential users",
    "start": "49320",
    "end": "54879"
  },
  {
    "text": "about how it can be useful um and then because I have such an interest in sort",
    "start": "54879",
    "end": "60960"
  },
  {
    "text": "of the uh the world of large language models I've been exploring that but that",
    "start": "60960",
    "end": "66680"
  },
  {
    "text": "is not a particularly good fit for the tail scale product you know I spent quite a long time looking for ways to",
    "start": "66680",
    "end": "73200"
  },
  {
    "start": "70000",
    "end": "255000"
  },
  {
    "text": "use this technology inside tail scale and like it doesn't really fit uh and uh I actually think that's a good thing you",
    "start": "73200",
    "end": "79520"
  },
  {
    "text": "know it's really nice to find clear lines like that when you find something where it's not particularly useful and I",
    "start": "79520",
    "end": "84880"
  },
  {
    "text": "wouldn't want to try and you know a lot of companies are attempting to make things work even if they don't quite",
    "start": "84880",
    "end": "91200"
  },
  {
    "text": "make sense and I think it's very sensible of tail scale to not go in that direction do you mean like deploying",
    "start": "91200",
    "end": "97280"
  },
  {
    "text": "llms inside of the tail scale product or how do you mean it wouldn't fit well oh yeah what so what would tail scale do",
    "start": "97280",
    "end": "104680"
  },
  {
    "text": "with llms is the question I was asking from a tail scale perspective I think tail scar is extremely useful for um",
    "start": "104680",
    "end": "112200"
  },
  {
    "text": "running llms yourself uh for a network back uh back plane right uh in",
    "start": "112200",
    "end": "117479"
  },
  {
    "text": "particular because of the sort of surprising nature of uh the network",
    "start": "117479",
    "end": "123039"
  },
  {
    "text": "traffic associated with llms on the inference side so you can kind of think",
    "start": "123039",
    "end": "128959"
  },
  {
    "text": "about working with models from both a training and an inference uh these are",
    "start": "128959",
    "end": "134239"
  },
  {
    "text": "sort of two sides of of the coin here and training is very very data heavy uh",
    "start": "134239",
    "end": "140440"
  },
  {
    "text": "and is usually done on you know extremely extremely high bandwidth low latency networks uh infin band style",
    "start": "140440",
    "end": "148560"
  },
  {
    "text": "setups uh on clusters of machines in a single room uh or you know if they",
    "start": "148560",
    "end": "155160"
  },
  {
    "text": "spread beyond the room you know the next room is literally in the building next door the inference side looks very",
    "start": "155160",
    "end": "162120"
  },
  {
    "text": "different there's very little Network traffic involved in doing inference uh on models and it uh in terms of",
    "start": "162120",
    "end": "169440"
  },
  {
    "text": "bandwidth uh and uh the layout of the network is surprisingly messy um this is because",
    "start": "169440",
    "end": "178080"
  },
  {
    "text": "sort of the nature of finding gpus is tricky even still today despite the fact",
    "start": "178080",
    "end": "183959"
  },
  {
    "text": "that this has been a thing for years now if you have very tricky yeah this is I I",
    "start": "183959",
    "end": "190200"
  },
  {
    "text": "feel I should try and explain it just because it's always worth trying to explain things but I'm sure you all know this which is that uh uh if you are",
    "start": "190200",
    "end": "196720"
  },
  {
    "text": "running a service on a cloud provider that you chose years ago for very good",
    "start": "196720",
    "end": "201879"
  },
  {
    "text": "reasons uh I'm the all the cloud providers are very good at fundamental services but they all have some subset",
    "start": "201879",
    "end": "209560"
  },
  {
    "text": "of gpus and they have them available in some places and not others uh and it's",
    "start": "209560",
    "end": "215400"
  },
  {
    "text": "never quite what you're looking for and if you are deciding to run your own model and do inference on it you might",
    "start": "215400",
    "end": "220640"
  },
  {
    "text": "find your GPU is in a region across the country or it's on a cloud provider that's different than the one you're",
    "start": "220640",
    "end": "225760"
  },
  {
    "text": "using or your cloud provider can do it but it's twice the price of the of another one you can get uh and this",
    "start": "225760",
    "end": "232400"
  },
  {
    "text": "leads to people ending up uh far more in sort of multicloud environments uh than",
    "start": "232400",
    "end": "238120"
  },
  {
    "text": "they do in sort of traditional software uh and so tail scale actually is very useful there so for users I",
    "start": "238120",
    "end": "245319"
  },
  {
    "text": "think it's you know it's a great fit but what does the product actually need as like new features uh to support that and",
    "start": "245319",
    "end": "251879"
  },
  {
    "text": "the answer is you know it actually is really great as it is today for that there's no specific AI angle that you",
    "start": "251879",
    "end": "258320"
  },
  {
    "start": "255000",
    "end": "284000"
  },
  {
    "text": "can add to the product and immediately make it more useful yeah I think that's right I mean there",
    "start": "258320",
    "end": "264199"
  },
  {
    "text": "are we we came up with some proposals but they're not exciting like there they",
    "start": "264199",
    "end": "270400"
  },
  {
    "text": "would be very much we'd be doing it because you know corporate and you know at headquarters told us to find an angle",
    "start": "270400",
    "end": "276880"
  },
  {
    "text": "for AI or something like that like we we as a startup have the option of just not doing that and so we didn't so well we",
    "start": "276880",
    "end": "285160"
  },
  {
    "start": "284000",
    "end": "379000"
  },
  {
    "text": "should probably disclaim that tail scale is a past and of course hopefully a future sponsor of Chang log and that",
    "start": "285160",
    "end": "291960"
  },
  {
    "text": "Adam's a huge we're working on scale user and brings it up often but this is not a sponsored episode in fact well",
    "start": "291960",
    "end": "299080"
  },
  {
    "text": "first of all we don't do sponsored guest appearances but also I had no idea that you were a co-founder of tail scale when",
    "start": "299080",
    "end": "304960"
  },
  {
    "text": "I read your blog post that made me reach out you I found it out afterwards I was like oh cool that's great I didn't know",
    "start": "304960",
    "end": "310800"
  },
  {
    "text": "we'd be talking about tail scale at all when I came here today so that I we're both basically on the same page this",
    "start": "310800",
    "end": "316560"
  },
  {
    "text": "yeah there we go I still work there I'm just kidding real time update I did double",
    "start": "316560",
    "end": "322080"
  },
  {
    "text": "check LinkedIn you are correct it says 2019 to 2024 was CTO but you just see",
    "start": "322080",
    "end": "328120"
  },
  {
    "text": "co-founder tail scale and then next to it and you move on and that's probably what ab and I both did same we real",
    "start": "328120",
    "end": "334680"
  },
  {
    "text": "there end date on that particular role yeah the nomenclature or usage on the metadata usage on LinkedIn is the their",
    "start": "334680",
    "end": "343039"
  },
  {
    "text": "UI is like which date which month did you begin and it says presentence so the Assumption was there I didn't read your",
    "start": "343039",
    "end": "349440"
  },
  {
    "text": "by line on the on the job rooll maybe what I'll do is I'll put something new above it and that'll make it clearer I",
    "start": "349440",
    "end": "356360"
  },
  {
    "text": "don't want to mislead anyone yeah yeah fair enough fair enough I also honestly don't check LinkedIn",
    "start": "356360",
    "end": "362000"
  },
  {
    "text": "very often it's not a big part of my life and so it uh we usually check it right before a show to make sure we get",
    "start": "362000",
    "end": "367599"
  },
  {
    "text": "someone's title right which is why we're both eating crow right now uh for getting it wrong but well to get back",
    "start": "367599",
    "end": "373599"
  },
  {
    "text": "into the into the mood or the groove whichever you want to call it let's get them to both well I'm a tail scale user",
    "start": "373599",
    "end": "380599"
  },
  {
    "start": "379000",
    "end": "432000"
  },
  {
    "text": "as you know um I just trimmed some machines i' I've been doing some uh more home labbing so I use tail scale really",
    "start": "380599",
    "end": "387960"
  },
  {
    "text": "only in my home lab and thank you so much for this free tier because I don't",
    "start": "387960",
    "end": "393039"
  },
  {
    "text": "want to give any money honestly I'm just kidding with you I think you're amazing but like I got to I got to put a 100",
    "start": "393039",
    "end": "398599"
  },
  {
    "text": "machines on my tailet before I have to pay you any money I got 18 there's no way I'm ever going to pay you money",
    "start": "398599",
    "end": "404639"
  },
  {
    "text": "based on your tis not mine that's totally great I'm very happy about that that's by Design that's by Design and I",
    "start": "404639",
    "end": "411520"
  },
  {
    "text": "think you know one thank you because it's let me be more Network curious and more home lab curious so you uh as a",
    "start": "411520",
    "end": "418919"
  },
  {
    "text": "corporation tail scale have allowed me and enabled me and so many others to do just that",
    "start": "418919",
    "end": "424520"
  },
  {
    "text": "and that's so cool and I think that's that I I applaud you all for that choice uh By Design well thank you that's",
    "start": "424520",
    "end": "430479"
  },
  {
    "text": "that's excellent that being said I mean at the same time I got a dig and it's not really a dig it's just really Tails kind of tail scale is kind of boring in",
    "start": "430479",
    "end": "437000"
  },
  {
    "start": "432000",
    "end": "559000"
  },
  {
    "text": "the fact that I don't have to do much to make it work you know I put it in I tail",
    "start": "437000",
    "end": "442400"
  },
  {
    "text": "scale up and I'm done okay you just work I never have to worry about you working",
    "start": "442400",
    "end": "448440"
  },
  {
    "text": "unless you're more what are you looking for I'm just saying like it's pretty boring you know unless I'm doing like serves or I'm sharing a disc across the",
    "start": "448440",
    "end": "455479"
  },
  {
    "text": "network I'm not doing that kind of stuff but you know this whole multicloud share GPU thing is super cool because you can",
    "start": "455479",
    "end": "461879"
  },
  {
    "text": "have a tailet on top of a different network and share that GPU access which I'm assuming is what you meant by that",
    "start": "461879",
    "end": "468479"
  },
  {
    "text": "that that's just so cool honestly it is I mean I love boring software and to me",
    "start": "468479",
    "end": "474120"
  },
  {
    "text": "the fact that you're having a boring experience is perfect no surprises no surprises it's a produ that's designed",
    "start": "474120",
    "end": "479960"
  },
  {
    "text": "to enable you to do more things not uh for you to spend your days having to configure it it is so smooth The devx",
    "start": "479960",
    "end": "486639"
  },
  {
    "text": "Experience on this thing is is Bar None you know I know my machines I know where they're at I know if they're out a date",
    "start": "486639",
    "end": "492919"
  },
  {
    "text": "it's pretty easy to do that kind of stuff and as a Avid user and a lover of tail scale again not sponsored just just",
    "start": "492919",
    "end": "499000"
  },
  {
    "text": "super passionate I can't see how an llm would fit in either I just can't see how you would work in AI to make the",
    "start": "499000",
    "end": "505280"
  },
  {
    "text": "platform better I mean I haven't thought about it deeply besides the this 20-ish minutes",
    "start": "505280",
    "end": "511080"
  },
  {
    "text": "so far in the conversation but I mean give me some time and I might yeah if you come up with anything let me know uh",
    "start": "511080",
    "end": "517839"
  },
  {
    "text": "I'm very excited about the idea of it uh but software has to be in some sense",
    "start": "517839",
    "end": "523200"
  },
  {
    "text": "true to itself you have to think about its purpose when you're when you're working on it and not step too far outside that so I similarly wouldn't",
    "start": "523200",
    "end": "531320"
  },
  {
    "text": "build a computer game in a tail scale I don't think that would be a particularly you know good fit for a product and I",
    "start": "531320",
    "end": "537360"
  },
  {
    "text": "feel an Easter egg as an Easter egg would be great actually like a little quiz game or something built into the",
    "start": "537360",
    "end": "543079"
  },
  {
    "text": "terminal once you have 100 machines in your tailet you get access to you unlock",
    "start": "543079",
    "end": "548440"
  },
  {
    "text": "a secret machine name that's on your tailet by default part of the plan yeah there you go right it ask questions like",
    "start": "548440",
    "end": "554200"
  },
  {
    "text": "what is the oldest machine on your tailet or something like that so yeah that would be that would be a lot of fun actually there are some questions I",
    "start": "554200",
    "end": "560200"
  },
  {
    "start": "559000",
    "end": "737000"
  },
  {
    "text": "would probably ask the tail net like there are actually some things I don't know about my tailet that I could discover via a chat llm interface I mean",
    "start": "560200",
    "end": "568519"
  },
  {
    "text": "there are some things I can some value in but I mean does everybody want that or need that maybe I don't know yeah I",
    "start": "568519",
    "end": "575160"
  },
  {
    "text": "don't know either uh I I very much went looking for something I would use",
    "start": "575160",
    "end": "580959"
  },
  {
    "text": "features like that for and I didn't come up with anything if you do come up with anything again I'd be very happy to hear about honestly now that I'm thinking",
    "start": "580959",
    "end": "586680"
  },
  {
    "text": "about it in real time yeah you know you have a column whenever you're on uh your",
    "start": "586680",
    "end": "592240"
  },
  {
    "text": "admin and you're on your machines dashboard essentially you can see last scene or ones that are out of date and",
    "start": "592240",
    "end": "599720"
  },
  {
    "text": "unless you're Savvy you probably haven't enabled tail scale's ability to auto update maybe you have maybe you haven't",
    "start": "599720",
    "end": "605240"
  },
  {
    "text": "I forget which maain maches I've done it on like everyone I install again once I knew that update was there I do enable",
    "start": "605240",
    "end": "611320"
  },
  {
    "text": "that but sometimes I forget so I might be like okay are there any of my machines that haven't been seen in a",
    "start": "611320",
    "end": "617480"
  },
  {
    "text": "while or are there any versions give me a list of the ones out of date that I should probably concern with around",
    "start": "617480",
    "end": "622680"
  },
  {
    "text": "security cuz you're probably not emailing me about my security concerns but my tailet knows which ones are too",
    "start": "622680",
    "end": "628320"
  },
  {
    "text": "far out of date if I haven't Auto updated that's true I think we did actually email customers once uh about",
    "start": "628320",
    "end": "635200"
  },
  {
    "text": "an out-of-date version MH where we were concerned about the security uh I think that has only come up",
    "start": "635200",
    "end": "640720"
  },
  {
    "text": "once uh mostly uh keeping tail scale up to date is sort of proactive good",
    "start": "640720",
    "end": "646959"
  },
  {
    "text": "security practice uh the uh it is fortunately not been a significant",
    "start": "646959",
    "end": "652800"
  },
  {
    "text": "source of issues in part you know due to careful design you know a lot of Engineers work very hard to make it that",
    "start": "652800",
    "end": "659079"
  },
  {
    "text": "way for sure you got a lot of amazing Engineers there yeah it's a great team I guess now that I'm thinking about it I",
    "start": "659079",
    "end": "665079"
  },
  {
    "text": "do have some ideas nice I mean I think this is the idea I have for notion as",
    "start": "665079",
    "end": "671480"
  },
  {
    "text": "well I use notion a lot uh a lot more anything where you have a platform where you can create your own things on top of",
    "start": "671480",
    "end": "678600"
  },
  {
    "text": "it a tailet you know one tail net is not the same as the next even though they operate the same the way I use mine may",
    "start": "678600",
    "end": "684839"
  },
  {
    "text": "not be the way you use yours it would be nice to have an interface where I can just ask tail scale how to tail scale",
    "start": "684839",
    "end": "692240"
  },
  {
    "text": "basically like I have an idea I want to create a new group or a whatever or I",
    "start": "692240",
    "end": "697959"
  },
  {
    "text": "can be introduced to new features it's Discovery and and you're essentially by not having your own you force people to",
    "start": "697959",
    "end": "704240"
  },
  {
    "text": "go into the you know public llms essentially into the chat gpts into the",
    "start": "704240",
    "end": "709360"
  },
  {
    "text": "clads into the Olas into the deep seeks or whatever you might have out there and",
    "start": "709360",
    "end": "714680"
  },
  {
    "text": "if you can Corner the market and on your own I think you'd be one better served because you know your documentation better two you know it's the",
    "start": "714680",
    "end": "720360"
  },
  {
    "text": "deterministic nature of it is maybe non-deterministic but you can probably fine-tune it a bit more to be more focus",
    "start": "720360",
    "end": "727560"
  },
  {
    "text": "on your customer base I'd probably ask my my uh tail scale llm more questions",
    "start": "727560",
    "end": "732800"
  },
  {
    "text": "if I if I could yeah so that's there's like an interesting sort",
    "start": "732800",
    "end": "738880"
  },
  {
    "start": "737000",
    "end": "923000"
  },
  {
    "text": "of meta question there about llms around",
    "start": "738880",
    "end": "744000"
  },
  {
    "text": "how many models there should be in the world M from a sort of a consumer persp",
    "start": "744000",
    "end": "749160"
  },
  {
    "text": "persective in a sense because that's almost like you know you're just you know consuming it like where and this",
    "start": "749160",
    "end": "755519"
  },
  {
    "text": "sounds very similar to like the question of uh how does search work on websites",
    "start": "755519",
    "end": "762199"
  },
  {
    "text": "which you could have asked 10 years ago or 20 years ago uh you know do I use the",
    "start": "762199",
    "end": "768000"
  },
  {
    "text": "Wikipedia search or do I go to Google and type in my search and maybe put the word Wiki at the end to bring the",
    "start": "768000",
    "end": "774440"
  },
  {
    "text": "Wikipedia links to the top uh both of these are you know valid strategies for",
    "start": "774440",
    "end": "779920"
  },
  {
    "text": "searching Wikipedia and you know I honestly don't use the Wikipedia search and haven't in a while so it may be",
    "start": "779920",
    "end": "785399"
  },
  {
    "text": "amazing but I have as a consumer a general concept that the search systems",
    "start": "785399",
    "end": "790880"
  },
  {
    "text": "on individual websites are not terrific uh and Google is uh Baseline decent uh",
    "start": "790880",
    "end": "797760"
  },
  {
    "text": "and so as long as I'm searching public data uh I would generally prefer the Google search uh I guess in a sense",
    "start": "797760",
    "end": "804040"
  },
  {
    "text": "that's a less and less true statement every year because the large chunks of websites are just not public data",
    "start": "804040",
    "end": "810440"
  },
  {
    "text": "anymore like you can't search Facebook with Google can't search Instagram with it you can't find a Tik Tok with it or",
    "start": "810440",
    "end": "816720"
  },
  {
    "text": "anything like that and so um the existence of those uh I think they",
    "start": "816720",
    "end": "823680"
  },
  {
    "text": "sometimes get called wall Gardens says that we should have more fine-tune tools like",
    "start": "823680",
    "end": "829839"
  },
  {
    "text": "that and you know there's just a lot of similarities there so should uh a",
    "start": "829839",
    "end": "835639"
  },
  {
    "text": "startup the size of tail scale build uh customized models for that for its users",
    "start": "835639",
    "end": "842240"
  },
  {
    "text": "I think is a sort of a big open-ended question uh around how the model space",
    "start": "842240",
    "end": "848680"
  },
  {
    "text": "will evolve and I think you know my last year of working with models fine-tuning them training them uh carefully",
    "start": "848680",
    "end": "855839"
  },
  {
    "text": "prompting them you can do more and more just with carefully structured uh prompts and long contexts uh that you",
    "start": "855839",
    "end": "861959"
  },
  {
    "text": "used to have to use fine tuning to achieve uh but all of this uh my sort of",
    "start": "861959",
    "end": "868000"
  },
  {
    "text": "big takeaway is that they are actually extremely hard to turn into products uh and to to get those details",
    "start": "868000",
    "end": "874920"
  },
  {
    "text": "right in a in a general sense for shipping to users uh they're actually quite easy to get going for yourself and",
    "start": "874920",
    "end": "882680"
  },
  {
    "text": "uh I think if anything more people should you know uh uh explore running models locally and playing with them",
    "start": "882680",
    "end": "889360"
  },
  {
    "text": "because they're a ton of fun and they can be very productive very quickly but in much the same way that it's really easy to write a python script that you",
    "start": "889360",
    "end": "895480"
  },
  {
    "text": "run yourself on your desktop versus a python script you ship in production for users uh llms have this huge sort of",
    "start": "895480",
    "end": "904120"
  },
  {
    "text": "complexity Gap when it comes to trying to build products for others and so I agree that that sort of tooling",
    "start": "904120",
    "end": "910880"
  },
  {
    "text": "would be fun and should exist I also think uh where we are today it's quite hard uh for a team the size of a startup",
    "start": "910880",
    "end": "918079"
  },
  {
    "text": "to to ship that uh as not part of their you know the core product experience what if it uh enabled so much deeper and",
    "start": "918079",
    "end": "926360"
  },
  {
    "start": "923000",
    "end": "1291000"
  },
  {
    "text": "greater usage cuz like the one thing you want to do as a startup or a brand like you are I would imagine at least from",
    "start": "926360",
    "end": "932560"
  },
  {
    "text": "the outside is a deeper customer is better than a shallow customer right if",
    "start": "932560",
    "end": "939120"
  },
  {
    "text": "I've only got a few machines well one my Affinity and my usage is lower so maybe my value is lower but if I'm deeply",
    "start": "939120",
    "end": "945560"
  },
  {
    "text": "entrenched it's and it's it's as a result of great documentation what you have but",
    "start": "945560",
    "end": "951040"
  },
  {
    "text": "docs they are are good when you have a particular Precision thing and you want to read and understand and discover how",
    "start": "951040",
    "end": "959560"
  },
  {
    "text": "a feature works and they only go so far and sometimes they're even out of date just hypothesizing that whether or not",
    "start": "959560",
    "end": "965920"
  },
  {
    "text": "like this what would be required one in terms of a lift one in engineering uh",
    "start": "965920",
    "end": "972759"
  },
  {
    "text": "Power and two potentially Financial power and then two what is that costing You by lack of more deep users and you",
    "start": "972759",
    "end": "981120"
  },
  {
    "text": "know shallow users in comparison I think that is exactly the right way to frame the question for a business and I don't",
    "start": "981120",
    "end": "988639"
  },
  {
    "text": "know the the answer to a lot of those questions I can talk to some of the more technical uh costs involved uh what the",
    "start": "988639",
    "end": "996440"
  },
  {
    "text": "benefits would be to the company is extremely open-ended to me like I I don't actually I can't imagine a way to measure that based on talking to",
    "start": "996440",
    "end": "1003839"
  },
  {
    "text": "customers of tail scale who deploy it you know thinking about the companies where and so to go back to something you",
    "start": "1003839",
    "end": "1010160"
  },
  {
    "text": "said earlier about uh how you use it and you don't pay for it uh uh I think",
    "start": "1010160",
    "end": "1015399"
  },
  {
    "text": "that's great because tail scale has no intention of making money or individual users uh that's not the that's not a",
    "start": "1015399",
    "end": "1021839"
  },
  {
    "text": "major source of revenue for the company uh the company's major source of Revenue is corporate deployments uh and there's",
    "start": "1021839",
    "end": "1027880"
  },
  {
    "text": "a blog post by my co-founder Avery about uh how the free plan stays free uh on",
    "start": "1027880",
    "end": "1033760"
  },
  {
    "text": "our website which sort of explains this that individual users um help bring tail scale to",
    "start": "1033760",
    "end": "1040400"
  },
  {
    "text": "companies who use it for uh business purposes and they they uh fund uh the",
    "start": "1040400",
    "end": "1047160"
  },
  {
    "text": "company's existence so looking at those business deployments uh you do see uh",
    "start": "1047160",
    "end": "1053120"
  },
  {
    "text": "tail scale gets rolled out initially at companies for some tiny subset of the things it could be used for and it often",
    "start": "1053120",
    "end": "1060240"
  },
  {
    "text": "takes quite a while to roll out for more and even even if the company has a really good road map and a really good",
    "start": "1060240",
    "end": "1066160"
  },
  {
    "text": "understanding of all of the ways they could use it uh it can take a very long time to to solve all of their problems",
    "start": "1066160",
    "end": "1071400"
  },
  {
    "text": "with it and that's assuming they have a really good understanding of all the things it can do and the point you're",
    "start": "1071400",
    "end": "1076919"
  },
  {
    "text": "making Adam that uh people often don't even realize all the great things you can do with it is true and I'm sure a",
    "start": "1076919",
    "end": "1082240"
  },
  {
    "text": "tool that helped people explore what they could do would would have some you know effect on Revenue in terms of the",
    "start": "1082240",
    "end": "1089039"
  },
  {
    "text": "sort of the technical side of it and the challenges one of the you know there is there several challenges in the in the",
    "start": "1089039",
    "end": "1094600"
  },
  {
    "text": "very broad sense the biggest challenge with llms is just the enormous amount of what you might call traditional",
    "start": "1094600",
    "end": "1101200"
  },
  {
    "text": "non-model engineering uh has to happen out the front of them to make them work well uh it's surprisingly involved I can",
    "start": "1101200",
    "end": "1108039"
  },
  {
    "text": "talk to some uh things I've been working on over the last year to give you a sense of that",
    "start": "1108039",
    "end": "1113320"
  },
  {
    "text": "beyond that the second sort of Big Technical challenge is one of sort of tail skills Core Design principles is",
    "start": "1113320",
    "end": "1118640"
  },
  {
    "text": "all of the networking is end to end encrypted and the main thing an llm",
    "start": "1118640",
    "end": "1123720"
  },
  {
    "text": "needs to give you Insight is a source of data and the major source of data would",
    "start": "1123720",
    "end": "1129240"
  },
  {
    "text": "be what is happening on your network what talks to what how does it all work and that means that any model telling",
    "start": "1129240",
    "end": "1137559"
  },
  {
    "text": "you how you could change your networking layout or give you insight into what you could do would need access to data that",
    "start": "1137559",
    "end": "1144200"
  },
  {
    "text": "we as a company don't have and don't want um and so we're back to it would have to be a product you run",
    "start": "1144200",
    "end": "1151159"
  },
  {
    "text": "locally and have complete control over which is absolutely you know the sorts my",
    "start": "1151159",
    "end": "1157400"
  },
  {
    "text": "favorite sorts of products are that you know I I like open source software that I can see the source code for compile",
    "start": "1157400",
    "end": "1163320"
  },
  {
    "text": "myself run locally that's how I like all things to be but trying to get there",
    "start": "1163320",
    "end": "1168559"
  },
  {
    "text": "with with llms in the state they are today is actually I think pretty tricky I don't think I've seen an actually",
    "start": "1168559",
    "end": "1174000"
  },
  {
    "text": "shipped product that does that really well for people there's one there's uh there's a developer tool that I hear a",
    "start": "1174000",
    "end": "1181280"
  },
  {
    "text": "lot of good talk about that uh I don't uh I'm just trying to live search for it",
    "start": "1181280",
    "end": "1188559"
  },
  {
    "text": "for you nope that's the wrong one uh that's Magic Shell history which also sounds really cool I should I should use",
    "start": "1188559",
    "end": "1194640"
  },
  {
    "text": "that one uh is that a2n a2n yeah that one's awesome oh you've used it oh great",
    "start": "1194640",
    "end": "1199720"
  },
  {
    "text": "I'm a daily user yeah not no lm's involved on that one yeah I thought that was the L there's another one that uh is",
    "start": "1199720",
    "end": "1206720"
  },
  {
    "text": "uh in the sort of agent space for developers as they're writing programs and it helps you it's it's like a local clae effectively okay uh and it's it's",
    "start": "1206720",
    "end": "1215600"
  },
  {
    "text": "primarily built around helping you construct prompts really carefully for existing open models uh and I've it's",
    "start": "1215600",
    "end": "1222480"
  },
  {
    "text": "come up several times and I'm sorry it's falling out of my head I will look it up later Sor right but it's uh I hear very positive things about it and that",
    "start": "1222480",
    "end": "1229200"
  },
  {
    "text": "that's the closest I've seen to sort of a shipped completely local product uh uh",
    "start": "1229200",
    "end": "1234480"
  },
  {
    "text": "that that does that sort of thing on Which models to use uh I think given the",
    "start": "1234480",
    "end": "1241080"
  },
  {
    "text": "state of models that exist today open models the major shipped open models are",
    "start": "1241080",
    "end": "1247799"
  },
  {
    "text": "so amazing that it always makes sense to start with one of those sort of models",
    "start": "1247799",
    "end": "1253799"
  },
  {
    "text": "as a if nothing else is a pre-trained base for anything that's happening uh building a model from scratch is a is a",
    "start": "1253799",
    "end": "1259919"
  },
  {
    "text": "very significant undertaking and I don't think is necessary for most tasks the",
    "start": "1259919",
    "end": "1266720"
  },
  {
    "text": "the available open models are extremely general purpose and so at worst you",
    "start": "1266720",
    "end": "1271840"
  },
  {
    "text": "would be fine-tuning from one of those uh to build a product you know if you take one of the llamas or um I mean",
    "start": "1271840",
    "end": "1278120"
  },
  {
    "text": "there's a lot of talk about deep seek um which you know produces terrific results it's a very it's a very large model uh",
    "start": "1278120",
    "end": "1283320"
  },
  {
    "text": "it be very hard to uh uh start with it though I understand there's some very good distilled work coming from it using",
    "start": "1283320",
    "end": "1289400"
  },
  {
    "text": "other models so you've been using these in your day-to-day programming work for the last year and back in early January",
    "start": "1289400",
    "end": "1296559"
  },
  {
    "start": "1291000",
    "end": "1766000"
  },
  {
    "text": "you wrote this post how I program with L llms which I found to",
    "start": "1296559",
    "end": "1302880"
  },
  {
    "text": "be refreshingly practical and straightforward your findings you said",
    "start": "1302880",
    "end": "1308080"
  },
  {
    "text": "you've been actively trying these things I feel like I've been passively trying them not really trying to optimize my",
    "start": "1308080",
    "end": "1315039"
  },
  {
    "text": "setup but just like you know like a a andth all kind of poking at a computer",
    "start": "1315039",
    "end": "1320159"
  },
  {
    "text": "box you know like oh does this work no okay for the last couple of years so I do use these things but I don't think as",
    "start": "1320159",
    "end": "1327200"
  },
  {
    "text": "effectively as most or at least some and um I loved your findings and of course",
    "start": "1327200",
    "end": "1333080"
  },
  {
    "text": "you're building something as a result of it but can you take us on that Journey over the last year or so where you started with llms and what you found in",
    "start": "1333080",
    "end": "1340760"
  },
  {
    "text": "your day-to-day programming yeah I don't think your experience is unusual actually I think almost everyone has",
    "start": "1340760",
    "end": "1346640"
  },
  {
    "text": "your experience uh and for most software I am in the same category I try",
    "start": "1346640",
    "end": "1352440"
  },
  {
    "text": "things at a very surface level when they're newish and see if there's any really obvious way they help me and if",
    "start": "1352440",
    "end": "1357799"
  },
  {
    "text": "they don't I put them aside and come back later uh great example of that is",
    "start": "1357799",
    "end": "1362919"
  },
  {
    "text": "uh the Git Version Control System uh it was 10 years before I really sat down",
    "start": "1362919",
    "end": "1368480"
  },
  {
    "text": "and used it I was using other Version Control Systems uh after 10 years I was like okay this thing's probably going to",
    "start": "1368480",
    "end": "1373960"
  },
  {
    "text": "stick around I guess I'll get over its user interface figure it out",
    "start": "1373960",
    "end": "1379360"
  },
  {
    "text": "I I was reluctant but you know I got there in the end llms really struck me as fascinating I decided to you know I",
    "start": "1379360",
    "end": "1385200"
  },
  {
    "text": "made this active decision to not do that with them and like set out on a on a process of trying to actively use them",
    "start": "1385200",
    "end": "1391799"
  },
  {
    "text": "which has involved uh learning just a you know a really appalling amount honestly like",
    "start": "1391799",
    "end": "1397520"
  },
  {
    "text": "it's very reasonable that most Engineers haven't done really significant things with LMS yet because it is it's too much",
    "start": "1397520",
    "end": "1405360"
  },
  {
    "text": "cognitive load like you know if you're if you're writing computer programs you're trying to solve a problem right",
    "start": "1405360",
    "end": "1411760"
  },
  {
    "text": "uh you only have so much of your brain available for the tools you use for solving problems because you have to fit the problem in there as well and the",
    "start": "1411760",
    "end": "1417640"
  },
  {
    "text": "solution you're building and that should be most of what you're thinking about the tools should take up as little space",
    "start": "1417640",
    "end": "1423039"
  },
  {
    "text": "as possible uh and right now to use llms effectively you need to know too much about them and that is my sort of Big T",
    "start": "1423039",
    "end": "1431159"
  },
  {
    "text": "that was my big takeaway you know 11 months ago or so which is why I started you know Working On Tools with some friends like to try and figure this out",
    "start": "1431159",
    "end": "1437840"
  },
  {
    "text": "MH because there there has to be a way to make this easier uh and my my main",
    "start": "1437840",
    "end": "1443120"
  },
  {
    "text": "conclusion from all of that is there's an enormous amount of traditional engineering to do in front of llms to",
    "start": "1443120",
    "end": "1448159"
  },
  {
    "text": "get there so the first really effective thing I saw from llms uh is the same",
    "start": "1448159",
    "end": "1454600"
  },
  {
    "text": "thing I think most Engineers saw which was GitHub co-pilot which is a code completion Al so",
    "start": "1454600",
    "end": "1460720"
  },
  {
    "text": "actually GitHub co-pilot has taken on new meanings it's more than that now right yeah it's it's an umbrella brand",
    "start": "1460720",
    "end": "1466840"
  },
  {
    "text": "that means all sorts of products uh and uh I actually honestly haven't even used most of those products at this point uh",
    "start": "1466840",
    "end": "1472919"
  },
  {
    "text": "the original product uh is a code completion system uh built into Visual",
    "start": "1472919",
    "end": "1478480"
  },
  {
    "text": "Studio code where as you type uh it suggests a completion for the line or a",
    "start": "1478480",
    "end": "1484440"
  },
  {
    "text": "few lines beyond that of where you are which is building on a very uh",
    "start": "1484440",
    "end": "1489640"
  },
  {
    "text": "wellestablished Paradigm for uh uh programming editors M uh you",
    "start": "1489640",
    "end": "1496600"
  },
  {
    "text": "know Visual Studio .0 did this uh 25 years ago uh with intellisense uh for",
    "start": "1496600",
    "end": "1504480"
  },
  {
    "text": "completing methods in C++ this is not a new idea uh and you know around the same time we had e tags uh for emac or or C",
    "start": "1504480",
    "end": "1512320"
  },
  {
    "text": "tags I should say which are which gave us similar things in the Unix World U and so this is sort of extending that",
    "start": "1512320",
    "end": "1518720"
  },
  {
    "text": "idea by bringing out some of the knowledge of a of a large language model",
    "start": "1518720",
    "end": "1523960"
  },
  {
    "text": "in the process of uh uh completing and I I'm really enamored with the entire model like you know co-pilot's original",
    "start": "1523960",
    "end": "1531039"
  },
  {
    "text": "experience uh when it came out was magical like it was just like there was nothing like this before it was really I",
    "start": "1531039",
    "end": "1536799"
  },
  {
    "text": "think jump started a lot of interest in the space uh from people who hadn't been working in it which was almost all of",
    "start": "1536799",
    "end": "1543000"
  },
  {
    "text": "us and from my perspective the the thing that really struck me was wow this works really well uh and wow it makes really",
    "start": "1543000",
    "end": "1549880"
  },
  {
    "text": "obvious silly mistakes uh there both both sides of this it would suggest things that just don't compile in ways",
    "start": "1549880",
    "end": "1556480"
  },
  {
    "text": "that are really obvious uh to anyone who takes a moment to read it uh and it would also make you know really",
    "start": "1556480",
    "end": "1562520"
  },
  {
    "text": "impressive cognitive leaps where it would suggest things that yes that is the direction I was heading and like it would have taken me several minutes to",
    "start": "1562520",
    "end": "1568559"
  },
  {
    "text": "explain it to someone and it got there immediately and so I spent quite a lot of time working on code completion",
    "start": "1568559",
    "end": "1574120"
  },
  {
    "text": "systems uh with the goal of improving them by focusing on a particular programming language um and we've made",
    "start": "1574120",
    "end": "1580279"
  },
  {
    "text": "some good Su you know some good progress there we actually hope to demonstrate some of that publicly soon uh like in",
    "start": "1580279",
    "end": "1586960"
  },
  {
    "text": "the next few weeks probably this sketch dodev thing that we've been building we'll integrate it so that people can see it and give it a try but so those",
    "start": "1586960",
    "end": "1593679"
  },
  {
    "text": "models uh are interesting because they're not the llm experience that most",
    "start": "1593679",
    "end": "1600080"
  },
  {
    "text": "users have like when everyone talks about AI today they talk about chat GPT or Claude well these chatbase systems",
    "start": "1600080",
    "end": "1606240"
  },
  {
    "text": "the thing I really really like about the original co-pilot code completion model is it's not a chat system it's a",
    "start": "1606240",
    "end": "1611880"
  },
  {
    "text": "different user interface experience for the knowledge in an LM and that's you know really a lot of fun and in fact the",
    "start": "1611880",
    "end": "1618840"
  },
  {
    "text": "the technology is uh uh a little bit different too there's a there's a concept in the",
    "start": "1618840",
    "end": "1625159"
  },
  {
    "text": "in the model space called fill inth Middle where a model is taught a few extra tokens that don't exist in the",
    "start": "1625159",
    "end": "1631159"
  },
  {
    "text": "standard chat model with fill in- the middle which is a lot of fun a model is taught a few extra",
    "start": "1631159",
    "end": "1637919"
  },
  {
    "text": "tokens um and the it's taught a prefix token a suffix token and a middle token",
    "start": "1637919",
    "end": "1644159"
  },
  {
    "text": "uh and what you do is you feed in as a prompt to the model the the file you're",
    "start": "1644159",
    "end": "1650159"
  },
  {
    "text": "in uh and everything all the characters before where the cursor is get fed right",
    "start": "1650159",
    "end": "1656399"
  },
  {
    "text": "after a prefix token so you feed in prefix all the characters of the file then you feed in uh suffix and you feed",
    "start": "1656399",
    "end": "1664960"
  },
  {
    "text": "in all the tokens after the cursor and then you feed in the middle token and",
    "start": "1664960",
    "end": "1671679"
  },
  {
    "text": "then you feed in whatever goes into the middle to complete it um and that's that's the prompt structure for of these",
    "start": "1671679",
    "end": "1678600"
  },
  {
    "text": "models uh and then the model keeps completing the thing that you you fed in",
    "start": "1678600",
    "end": "1684080"
  },
  {
    "text": "it writes the next characters and you train the model by taking existing code out there a few",
    "start": "1684080",
    "end": "1690720"
  },
  {
    "text": "there's a few papers on how these models are trained because meta published one of these models uh and Google published",
    "start": "1690720",
    "end": "1695960"
  },
  {
    "text": "one of these models under the Gemma brand uh there's a few others out there there's one from uh quen and uh some",
    "start": "1695960",
    "end": "1703200"
  },
  {
    "text": "other derived ones uh and uh uh you take existing code files you choose some",
    "start": "1703200",
    "end": "1709159"
  },
  {
    "text": "section of code you you mark everything before it as the prefix everything after",
    "start": "1709159",
    "end": "1714399"
  },
  {
    "text": "it as the suffix and you fill in everything after it as the middle and that's your training data you generate a lot of that by taking existing code and",
    "start": "1714399",
    "end": "1720559"
  },
  {
    "text": "breaking it up into these files randomly by randomly inserting a cursor then you uh uh then you've taught",
    "start": "1720559",
    "end": "1728080"
  },
  {
    "text": "a model how to use these extra characters and how to complete them and so it's not a chat model at all it's a",
    "start": "1728080",
    "end": "1733279"
  },
  {
    "text": "sort of a sequence to sequence model it's a it's a it's a ton of fun uh and",
    "start": "1733279",
    "end": "1738600"
  },
  {
    "text": "the advantage of these systems is they're very fast compared to chat models and that's the key to the whole",
    "start": "1738600",
    "end": "1744039"
  },
  {
    "text": "code completion product experience is you want your code completion within a couple hundred milliseconds of you",
    "start": "1744039",
    "end": "1749960"
  },
  {
    "text": "typing a character whereas if you actually time clawed or you time one of",
    "start": "1749960",
    "end": "1755679"
  },
  {
    "text": "the open AI models U they're very they're very slow like they take a good minute to give you a result and there's",
    "start": "1755679",
    "end": "1763000"
  },
  {
    "text": "a lot of UI tricks in hiding that minute they move the text around on the screen they stream it in streaming yeah",
    "start": "1763000",
    "end": "1770840"
  },
  {
    "start": "1766000",
    "end": "1844000"
  },
  {
    "text": "it's very clever it's cuz you're reading it word by word as it comes out but it's like it's basically stalling you're like",
    "start": "1770840",
    "end": "1776120"
  },
  {
    "text": "come on just give me the answer already that's right yeah exactly you can really feel it with the new reasoning models 01",
    "start": "1776120",
    "end": "1783559"
  },
  {
    "text": "uh these things because there's this this pause at the beginning it's uh it hurts the thinking phase I'm like come",
    "start": "1783559",
    "end": "1789039"
  },
  {
    "text": "on I don't and it tells you what it's thinking which is cool but it's like think faster I don't care what you're thinking give me the answer I think it's",
    "start": "1789039",
    "end": "1795960"
  },
  {
    "text": "actually kind of cool when you see that bre come up I mean you get to see like I feel like this is the closest we've",
    "start": "1795960",
    "end": "1801399"
  },
  {
    "text": "glimpsed into the future than we've ever been able to by watching the reasoning in in real time you see you see the the",
    "start": "1801399",
    "end": "1809120"
  },
  {
    "text": "act of reasoning that it's happening it explains the reasoning users asked me this so I'm going to think about that",
    "start": "1809120",
    "end": "1815480"
  },
  {
    "text": "okay I've thought about that which causes this and it's like this step process and it reminds me of how I think",
    "start": "1815480",
    "end": "1821480"
  },
  {
    "text": "too so I'm like that's pretty dang cool but it's also a great trick I agree yeah",
    "start": "1821480",
    "end": "1827039"
  },
  {
    "text": "it's uh it is a ton fun to watch I agree and it is a lot of insight into how the models work too because the the insides",
    "start": "1827039",
    "end": "1833679"
  },
  {
    "text": "of the models are a large number of floating Point numbers holding intermediate State and it's very hard to",
    "start": "1833679",
    "end": "1839559"
  },
  {
    "text": "get insight into those but the words you can read them you can make some sense of them right so code completion is I think",
    "start": "1839559",
    "end": "1845559"
  },
  {
    "start": "1844000",
    "end": "1908000"
  },
  {
    "text": "extremely useful to programmers uh it varies a lot depending on what you're writing and how experienced models are",
    "start": "1845559",
    "end": "1852559"
  },
  {
    "text": "with it and just how sort of out on the edge of",
    "start": "1852559",
    "end": "1858480"
  },
  {
    "text": "programming you are if you're really out in the weeds the models can get less useful uh I used a model for writing a",
    "start": "1858480",
    "end": "1867399"
  },
  {
    "text": "big chunk of AVX assembly a few months ago uh and the model was both very good",
    "start": "1867399",
    "end": "1873720"
  },
  {
    "text": "at it and very bad at it simultaneously and it was very different from the typical asking a model to help with",
    "start": "1873720",
    "end": "1879279"
  },
  {
    "text": "programming experience it would constantly get the order of operations wrong um or over",
    "start": "1879279",
    "end": "1887559"
  },
  {
    "text": "complicate things or misunderstand it was it's a very different experience than uh than typical programming what",
    "start": "1887559",
    "end": "1893080"
  },
  {
    "text": "model was this how did you find it you know I used all of them for that this is",
    "start": "1893080",
    "end": "1899000"
  },
  {
    "text": "what I meant by I'm spending a l time actively exploring the space is I'm I'm putting far too much work into exploring",
    "start": "1899000",
    "end": "1905360"
  },
  {
    "text": "the model space uh as I do work it makes sense that there are specific models that are good for autocomplete versus",
    "start": "1905360",
    "end": "1912279"
  },
  {
    "start": "1908000",
    "end": "1961000"
  },
  {
    "text": "search versus chat but have you found",
    "start": "1912279",
    "end": "1917399"
  },
  {
    "text": "the correct one for each particular subtask or what's your advice there is it like use them all or just stick with",
    "start": "1917399",
    "end": "1924320"
  },
  {
    "text": "this you'll be good I can't advise people to use them all you know that's a bunch of them yeah it's uh and this I",
    "start": "1924320",
    "end": "1931480"
  },
  {
    "text": "think is the big problem and you know you mentioned that most programmers are probably using this as far as we can",
    "start": "1931480",
    "end": "1937360"
  },
  {
    "text": "tell not one fifth of programmers are using these tools today how can you tell",
    "start": "1937360",
    "end": "1942559"
  },
  {
    "text": "that uh through surveys a couple of people have done surveys of programmers and it it seems to come back that most",
    "start": "1942559",
    "end": "1948360"
  },
  {
    "text": "people are not using these tools yet wow which is both shocking to me because",
    "start": "1948360",
    "end": "1954000"
  },
  {
    "text": "they're so useful and also makes a lot of sense because there it's a lot of it's a lot of work figuring out how to",
    "start": "1954000",
    "end": "1959960"
  },
  {
    "text": "use them I have a an analogy that I'd like to share if you're a runner you",
    "start": "1959960",
    "end": "1965720"
  },
  {
    "start": "1961000",
    "end": "2123000"
  },
  {
    "text": "probably wear running shoes right you're probably not going to run Barefoot I think it's like admitting to",
    "start": "1965720",
    "end": "1973440"
  },
  {
    "text": "running Barefoot like you wouldn't do that you would run a marathon with rocks on the Road and debris and things like that",
    "start": "1973440",
    "end": "1980000"
  },
  {
    "text": "Barefoot versus running shoes designed to Aid you in the process of running to",
    "start": "1980000",
    "end": "1986080"
  },
  {
    "text": "make it more Speedy comfortable agile Etc I feel like that's where we're at",
    "start": "1986080",
    "end": "1991320"
  },
  {
    "text": "like I've I've changed my tune let's just say because I feel like it's not going to go away and to to hear that",
    "start": "1991320",
    "end": "1999039"
  },
  {
    "text": "oneth I'm I haven't dug into these surveys but that's surprising oneth is",
    "start": "1999039",
    "end": "2004240"
  },
  {
    "text": "using it and it seems like I guess than the the four of the the five are saying no",
    "start": "2004240",
    "end": "2010080"
  },
  {
    "text": "or for for the time for the moment denying it I mean do either of you disagree with that analogy is it way off",
    "start": "2010080",
    "end": "2016559"
  },
  {
    "text": "or is it jar I saw you kind of like shake your head a little bit what's your thoughts on that now I mean I don't",
    "start": "2016559",
    "end": "2022600"
  },
  {
    "text": "think that these tools have come to the place that the running shoe has I also think there's probably plenty of",
    "start": "2022600",
    "end": "2029039"
  },
  {
    "text": "worldclass Runners who run shoeless and would never run with a shoe on cuz that's for fools but I'm not going to go",
    "start": "2029039",
    "end": "2035120"
  },
  {
    "text": "there um well would you run a New York city marathon with no shoes I wouldn't be I wouldn't be so foolish as to try a",
    "start": "2035120",
    "end": "2041679"
  },
  {
    "text": "new city marathon okay you have to admit though that having the shoes on is probably better",
    "start": "2041679",
    "end": "2047760"
  },
  {
    "text": "for you than worse for you well at what point has the shoe proven itself to be useless because these tools routinely",
    "start": "2047760",
    "end": "2054440"
  },
  {
    "text": "prove themselves to not just be wrong but dramatically wrong in ways",
    "start": "2054440",
    "end": "2060200"
  },
  {
    "text": "that if you follow them you will be like Michael Scott who drives directly into the pond no no no no no look it it means",
    "start": "2060200",
    "end": "2066200"
  },
  {
    "text": "go up to the right Bear right over the bridge and hook up with 307 right maybe it's a shortcut Dwight it said go to the",
    "start": "2066200",
    "end": "2072760"
  },
  {
    "text": "right it can't mean that there's a lake there knows where it is going the machine the stop yelling at me it's",
    "start": "2072760",
    "end": "2080679"
  },
  {
    "text": "there's no room here oh yes because his voice assistant or his GPS tells him to keep going",
    "start": "2080679",
    "end": "2087839"
  },
  {
    "text": "straight and he just I can Cur on that one too I can Cur so I can see why you could get frustrated and throw up your",
    "start": "2087839",
    "end": "2093919"
  },
  {
    "text": "hands and say I'm going to come back to this in a year or two years but I'm going to let all the Frontiers people",
    "start": "2093919",
    "end": "2099720"
  },
  {
    "text": "like David figure all the stuff out in the meantime I got code to write I can see a lot of people saying that I'm not",
    "start": "2099720",
    "end": "2107079"
  },
  {
    "text": "I could see myself saying that I haven't because I am curious and I don't want to fall behind but I'm still don't feel",
    "start": "2107079",
    "end": "2114800"
  },
  {
    "text": "like this is a must have for everybody today but there are moments where I'm like that was amazing so for sure I'm",
    "start": "2114800",
    "end": "2123520"
  },
  {
    "start": "2123000",
    "end": "2172000"
  },
  {
    "text": "actively trying to fit it in into everything I do is I guess my perspective I'm I'm actively if it's",
    "start": "2123520",
    "end": "2128960"
  },
  {
    "text": "home lab it's that if it's contracts agreements proposals if it's thinking if",
    "start": "2128960",
    "end": "2134079"
  },
  {
    "text": "it's exploration if it's coding if it's you pick it you're if it's kind of thing",
    "start": "2134079",
    "end": "2139200"
  },
  {
    "text": "I'm trying to fit it in and I'm just so the I'm sitting down on my bench I got",
    "start": "2139200",
    "end": "2144520"
  },
  {
    "text": "my socks on and I'm trying to put the shoe on let's just say you know if to kind of extend my analogy you're going",
    "start": "2144520",
    "end": "2149599"
  },
  {
    "text": "to wear it yeah I believe that you know I'm going to put this shoe on I'm going to wear it for every scenario that makes sense because I can tell you I move",
    "start": "2149599",
    "end": "2156319"
  },
  {
    "text": "faster I think differently when I'm in those modes are they wrong do I always check it of course but I know that it's",
    "start": "2156319",
    "end": "2163359"
  },
  {
    "text": "coming for almost everything we do every task we do that's productive coding thinking writing whatever it's coming",
    "start": "2163359",
    "end": "2170200"
  },
  {
    "text": "for it in a positive way I mean I I totally agree that it is coming for it I also think it's very early days and a",
    "start": "2170200",
    "end": "2177280"
  },
  {
    "start": "2172000",
    "end": "2350000"
  },
  {
    "text": "great reason to not learn this technology today uh is that it's changing so fast yeah and that you can",
    "start": "2177280",
    "end": "2183440"
  },
  {
    "text": "spend a very long time figuring out how to make it work and then that can all all of that accumulated uh skill can be",
    "start": "2183440",
    "end": "2190520"
  },
  {
    "text": "sort of uh made useless tomorrow right by some new product if you remember",
    "start": "2190520",
    "end": "2196119"
  },
  {
    "text": "stable diffusion first dropped probably two years ago now yeah and we were enamored with prompt engineering and",
    "start": "2196119",
    "end": "2203240"
  },
  {
    "text": "what was that artist name that you would always if you added it to your your stable diffusion prompt it would automatically get awesome and then he",
    "start": "2203240",
    "end": "2209280"
  },
  {
    "text": "got mad because everyone's using him to like make better pictures like that whole technology you know that magical",
    "start": "2209280",
    "end": "2216680"
  },
  {
    "text": "incantation is just completely moot at this point like this probably it's easier now to get better pictures",
    "start": "2216680",
    "end": "2222480"
  },
  {
    "text": "without being such a wizard and whatever name you were invoking in the past is",
    "start": "2222480",
    "end": "2228319"
  },
  {
    "text": "just that name doesn't do what it did on the last version of stable diffusion just as one instance like prompt engineering yeah has changed",
    "start": "2228319",
    "end": "2235200"
  },
  {
    "text": "dramatically and anybody who's ignoring it all and just listening to us talk about on the change log and just like",
    "start": "2235200",
    "end": "2240640"
  },
  {
    "text": "staying with their regular life like they've saved themselves a lot of time than those of us who dove in and decided they were going to memorize all the",
    "start": "2240640",
    "end": "2246640"
  },
  {
    "text": "magic words yeah absolutely like uh a year ago a common technique with open",
    "start": "2246640",
    "end": "2252680"
  },
  {
    "text": "models that existed was to offer them money to solve problems you start every prompt by saying I'll give you $200 if",
    "start": "2252680",
    "end": "2259079"
  },
  {
    "text": "you do this and and it greatly improved out outcomes yeah or let's take this step by step like that phrase was one of",
    "start": "2259079",
    "end": "2264640"
  },
  {
    "text": "those magical things that made it better like all of those techniques are gone now yeah like if you try bribing a model",
    "start": "2264640",
    "end": "2271119"
  },
  {
    "text": "it doesn't help it's a there was a great there was a great uh example I saw of that where someone would kept saying",
    "start": "2271119",
    "end": "2277000"
  },
  {
    "text": "I'll give you $100 if you do this and they did it in a single prompt several times uh and they got to the nth case uh",
    "start": "2277000",
    "end": "2282880"
  },
  {
    "text": "and it said but you haven't paid me for the previous ones yet to listen fella we had a",
    "start": "2282880",
    "end": "2289200"
  },
  {
    "text": "deal no means no that's great that is great no money means now yeah there you",
    "start": "2289200",
    "end": "2294560"
  },
  {
    "text": "go all right they're very funny mod so I spent a long time believing and I actually I still believe this in the long term that chat is our is currently",
    "start": "2294560",
    "end": "2303000"
  },
  {
    "text": "our primary user interface on the models and it's not the best interface for most things",
    "start": "2303000",
    "end": "2308800"
  },
  {
    "text": "the way to get the most value out of models today when you program is to have conversations with models about what",
    "start": "2308800",
    "end": "2314079"
  },
  {
    "text": "you're writing and that's I I think it's it's quite the mode shift to do that",
    "start": "2314079",
    "end": "2321200"
  },
  {
    "text": "it's quite taxing to do that and it feels like uh it feels like a user",
    "start": "2321200",
    "end": "2327040"
  },
  {
    "text": "interface problem that hasn't been solved yet and so uh I've been working a lot with uh Josh bleacher Snider on",
    "start": "2327040",
    "end": "2333240"
  },
  {
    "text": "these things uh and we uh have been sort of we spent a long time looking for how",
    "start": "2333240",
    "end": "2338920"
  },
  {
    "text": "can we avoid the chat Paradigm and make use of models that's why code completion",
    "start": "2338920",
    "end": "2344119"
  },
  {
    "text": "initially was so interesting because it's an example of using models without chat and it's very effective so we spent",
    "start": "2344119",
    "end": "2350400"
  },
  {
    "start": "2350000",
    "end": "2506000"
  },
  {
    "text": "a long time exploring this to give you another example of uh something we built in this space um because we've just been trying to build things to see uh what's",
    "start": "2350400",
    "end": "2357720"
  },
  {
    "text": "actually useful uh we built something called MD md. uh which uh I think we put up a few",
    "start": "2357720",
    "end": "2364680"
  },
  {
    "text": "weeks ago uh and uh it does merge commit for you so if you have a if you try and",
    "start": "2364680",
    "end": "2370319"
  },
  {
    "text": "push a get commit or do a rebase uh and you get a merge conflict uh uh you can",
    "start": "2370319",
    "end": "2376800"
  },
  {
    "text": "actually use llms to generate you know sophisticated merge commits for you it turns out that's a much harder problem",
    "start": "2376800",
    "end": "2382880"
  },
  {
    "text": "than it looks like you would think you just paste in all of the files to the prompt and you ask it to generate uh the",
    "start": "2382880",
    "end": "2389880"
  },
  {
    "text": "the the correct files for you uh even the frontier models are all really bad at this uh you you almost",
    "start": "2389880",
    "end": "2396720"
  },
  {
    "text": "never get a good merge commit out of them uh but with a whole stack of really mundane engineering out the front",
    "start": "2396720",
    "end": "2403480"
  },
  {
    "text": "mundane is not the right work because uh uh a lot of it's actually really very sophisticated but it's not it doesn't",
    "start": "2403480",
    "end": "2408800"
  },
  {
    "text": "involve the llm itself it's about carefully constructing traditional is a much better word yeah it's uh uh you can",
    "start": "2408800",
    "end": "2415720"
  },
  {
    "text": "actually get very good merge commits out of it and that user experience seems much better for programmers to me that",
    "start": "2415720",
    "end": "2421880"
  },
  {
    "text": "you could imagine that being integrated into your workflows to the point where you send a PR there's a conflict it",
    "start": "2421880",
    "end": "2428319"
  },
  {
    "text": "proposes a fix right on the pr for you uh and in fact we we attempted a version",
    "start": "2428319",
    "end": "2433560"
  },
  {
    "text": "of that where there's a little git bot that uh uh you can at mention on a PR and it sort of generates another PR",
    "start": "2433560",
    "end": "2439720"
  },
  {
    "text": "based on it that fixes the merge Conflict for you uh and that sort of experience doesn't require the chat",
    "start": "2439720",
    "end": "2446079"
  },
  {
    "text": "interface to be exposed to the programmer uh to make use of the intelligence in the model and that is",
    "start": "2446079",
    "end": "2454440"
  },
  {
    "text": "where I dream of devel tools getting so that everyone can use them without",
    "start": "2454440",
    "end": "2459960"
  },
  {
    "text": "having to learn a lot about them like you shouldn't you shouldn't have to learn all the tricks for convincing a",
    "start": "2459960",
    "end": "2465119"
  },
  {
    "text": "model to write a merge commit for you it should be a button uh or not even a button it should just do it when GitHub",
    "start": "2465119",
    "end": "2471240"
  },
  {
    "text": "says there's a merge conflict and so uh it's actually it works pretty well uh uh",
    "start": "2471240",
    "end": "2477240"
  },
  {
    "text": "We've uh we've seen it generate some uh some very sophisticated merge uh commits",
    "start": "2477240",
    "end": "2482400"
  },
  {
    "text": "for us love to see more people give it a try and let us know uh what the state of that is but so",
    "start": "2482400",
    "end": "2487839"
  },
  {
    "text": "just because that is such a hard state to get to uh we built sketch which",
    "start": "2487839",
    "end": "2493079"
  },
  {
    "text": "exposes the traditional chat interface uh in the process of writing code uh because we're just not we don't think",
    "start": "2493079",
    "end": "2500160"
  },
  {
    "text": "the models are at a point yet where we can completely get away from chat being part of the developer workflow so at",
    "start": "2500160",
    "end": "2506560"
  },
  {
    "start": "2506000",
    "end": "2893000"
  },
  {
    "text": "what level of granularity is sketch working",
    "start": "2506560",
    "end": "2511680"
  },
  {
    "text": "at and do you imagine it moving up eventually wherever it is is because the",
    "start": "2511680",
    "end": "2519040"
  },
  {
    "text": "Panacea right the Silver Bullet is what some folks are trying to do with Devon",
    "start": "2519040",
    "end": "2524200"
  },
  {
    "text": "for instance where it's like you you describe at a very high",
    "start": "2524200",
    "end": "2530560"
  },
  {
    "text": "level a system and it goes and builds that system vzer uh from versell is",
    "start": "2530560",
    "end": "2535800"
  },
  {
    "text": "another one that's doing these things and they're very much at in my opinion",
    "start": "2535800",
    "end": "2541040"
  },
  {
    "text": "the Prototype SL demo level of quality not the production level of quality in",
    "start": "2541040",
    "end": "2547280"
  },
  {
    "text": "their output and it seems like they're very difficult in my in my limited experience",
    "start": "2547280",
    "end": "2552520"
  },
  {
    "text": "with these things they're very difficult to actually uh mold or what do you do you",
    "start": "2552520",
    "end": "2559200"
  },
  {
    "text": "I'm losing a word here uh sculpt I don't know like a sculpture to actually like sculpt what they come out with and",
    "start": "2559200",
    "end": "2564559"
  },
  {
    "text": "change it into something that you actually would write or like but those are like the very high level of like well it should have a contact form that",
    "start": "2564559",
    "end": "2572400"
  },
  {
    "text": "submits to this thing but maybe you're looking down more where the where I use them currently",
    "start": "2572400",
    "end": "2578680"
  },
  {
    "text": "which is like yo write me a function that does this particular thing you know and at that level it seems a lot easier",
    "start": "2578680",
    "end": "2585040"
  },
  {
    "text": "to even chat to if I have to I would rather not chat to it but spit out code",
    "start": "2585040",
    "end": "2590400"
  },
  {
    "text": "that I could copy paste and modify versus being like I'm G have to throw this away and rewrite it right yes I",
    "start": "2590400",
    "end": "2597160"
  },
  {
    "text": "think that is that is uh that lines up really well with the way Josh and I think about these things where today if",
    "start": "2597160",
    "end": "2604079"
  },
  {
    "text": "you open up a Model A Cloud providers Frontier model or you know a local deep",
    "start": "2604079",
    "end": "2609640"
  },
  {
    "text": "seek or even Al llama 70b you can ask it to write a python script that does",
    "start": "2609640",
    "end": "2614960"
  },
  {
    "text": "something it could be a python script to go to the GitHub API and grab some data and present it neatly for you uh and it",
    "start": "2614960",
    "end": "2621760"
  },
  {
    "text": "will do a great job it you know the these great models can basically do this",
    "start": "2621760",
    "end": "2626960"
  },
  {
    "text": "in a single shot where you write a sentence and out comes a python script that solves a problem and like that's",
    "start": "2626960",
    "end": "2632640"
  },
  {
    "text": "that's an astonishing technical achievement I really it's it's amazing how quickly I've got used to that as a",
    "start": "2632640",
    "end": "2637680"
  },
  {
    "text": "but yeah you're not even impressing me right now you know it can do that we all know it is amazing exactly like like",
    "start": "2637680",
    "end": "2645079"
  },
  {
    "text": "five-year-old five me five years ago if you told me that uh I would struggle to believe it uh and yet now I just take it",
    "start": "2645079",
    "end": "2652440"
  },
  {
    "text": "for granted yes and so that works we've got that we've got a thing that can can write really basic Python scripts for us",
    "start": "2652440",
    "end": "2659359"
  },
  {
    "text": "uh similarly uh these systems at least the frontier models are good at writing a small react component for you uh you",
    "start": "2659359",
    "end": "2667200"
  },
  {
    "text": "give almost any of them like uh you need more than a single sentence you need just a few sentences to structure the react component but outomes some HTML",
    "start": "2667200",
    "end": "2675240"
  },
  {
    "text": "and some JavaScript in the react syntax the jsx syntax or the TSX syntax and",
    "start": "2675240",
    "end": "2681040"
  },
  {
    "text": "it's pretty close you know it might need some tweaking you might have some back and forths to get there but you can get about that out of it uh and clearly",
    "start": "2681040",
    "end": "2690040"
  },
  {
    "text": "models are going to improve I there's no evidence to suggest we're at the the limit here as the models keep improving",
    "start": "2690040",
    "end": "2696760"
  },
  {
    "text": "uh every every month at this rate and part of what we're interested in sketch",
    "start": "2696760",
    "end": "2701839"
  },
  {
    "text": "is getting Beyond helping you write a function which I also use today right I get for Frontier models to write",
    "start": "2701839",
    "end": "2707599"
  },
  {
    "text": "functions for me uh to sort of how can we sort of climb the complexity ladder",
    "start": "2707599",
    "end": "2713319"
  },
  {
    "text": "there and so the the point we chose is a point that you know is comfortable for us and what is helpful for us is the go",
    "start": "2713319",
    "end": "2720359"
  },
  {
    "text": "package how can we get a model to help us build a go package to solve a problem",
    "start": "2720359",
    "end": "2725720"
  },
  {
    "text": "and there's a sort of a there's an implicit assumption here in that the the shape of go packages looks slightly",
    "start": "2725720",
    "end": "2731720"
  },
  {
    "text": "different at the end of this packages are a little bit smaller and you have a few more of them than you would in a",
    "start": "2731720",
    "end": "2738000"
  },
  {
    "text": "sort of traditional go program you wrote by hand but I don't think that is necessarily a bad thing honestly in my",
    "start": "2738000",
    "end": "2745240"
  },
  {
    "text": "own programming I as a as a go programmer I I tend to write larger packages because there's a lot of extra",
    "start": "2745240",
    "end": "2752160"
  },
  {
    "text": "work involved in me breaking it into smaller packages MH and there's often this thought process going on in my mind of like oh in the future this would be",
    "start": "2752160",
    "end": "2758480"
  },
  {
    "text": "more maintainable as more packages but it's more work for me to get there today so I'll just I'll combine it all now and",
    "start": "2758480",
    "end": "2765680"
  },
  {
    "text": "maybe refactor it another day uh and switching to trying to have llms write",
    "start": "2765680",
    "end": "2772200"
  },
  {
    "text": "significant chunks of packages for you makes you do that work upfront that's not necessarily a bad thing it's perhaps",
    "start": "2772200",
    "end": "2779040"
  },
  {
    "text": "more the way we'd like our code to earn up uh and so sketch is about taking an llm and plugging a lot of the tooling",
    "start": "2779040",
    "end": "2785720"
  },
  {
    "text": "for go in into the process of using the llm uh to help it so you know as you you",
    "start": "2785720",
    "end": "2794960"
  },
  {
    "text": "an example as I asked at the other day to write uh some middleware to broadly compress HTTP responses under certain",
    "start": "2794960",
    "end": "2803200"
  },
  {
    "text": "circumstances uh because Chrome can handle broadly encoding and it's very efficient uh it's not in the standard",
    "start": "2803200",
    "end": "2810119"
  },
  {
    "text": "library or at least it wasn't the last time I looked uh and the first thing it did was it included a thirdparty package",
    "start": "2810119",
    "end": "2817040"
  },
  {
    "text": "that Andy had written that uh has a brly encoder in it uh and so sketch go gets",
    "start": "2817040",
    "end": "2822480"
  },
  {
    "text": "that in the background in a little container as you're working and has a little go mod there that modifies uh so",
    "start": "2822480",
    "end": "2829119"
  },
  {
    "text": "that as you're editing the code you get all the code completions from that module just like you would uh in a a",
    "start": "2829119",
    "end": "2834599"
  },
  {
    "text": "programming environment and more importantly we can take that information and feed it into the model as it's",
    "start": "2834599",
    "end": "2839920"
  },
  {
    "text": "working you know if uh uh we run We Run The Go build system as part of it and if",
    "start": "2839920",
    "end": "2845680"
  },
  {
    "text": "a build error appears we can take the build error feed it into the model it's like here's the error and we can let it",
    "start": "2845680",
    "end": "2852319"
  },
  {
    "text": "ask questions about the third party package it included which helps with sort of some",
    "start": "2852319",
    "end": "2857359"
  },
  {
    "text": "of the classic problems you see when you ask Claude to write you some go code where it includes a package and then",
    "start": "2857359",
    "end": "2862640"
  },
  {
    "text": "makes up a method in there that doesn't exist that you really wish existed because it would solve your problem um",
    "start": "2862640",
    "end": "2868240"
  },
  {
    "text": "and so the this sort of tool automated tool feedback is doing a lot of the work I have to do manually when I use a Frontier Model and so I'm trying to cut",
    "start": "2868240",
    "end": "2874760"
  },
  {
    "text": "out some of those intermediate steps where I said that doesn't exist could you do it this way uh any anything like",
    "start": "2874760",
    "end": "2880839"
  },
  {
    "text": "that you can automate saves me time it means I have to chat less and so that's the goal is to slightly climb the",
    "start": "2880839",
    "end": "2886160"
  },
  {
    "text": "complexity ladder in the uh the piece of software we get out of a Frontier Model uh and to chat Less in the process are",
    "start": "2886160",
    "end": "2893480"
  },
  {
    "start": "2893000",
    "end": "3184000"
  },
  {
    "text": "you achieving that by having a system prompt or you actually fine-tuning like how are you as the sketch. deev creators",
    "start": "2893480",
    "end": "2901359"
  },
  {
    "text": "taking a foundation model and doing something to get here today it is almost",
    "start": "2901359",
    "end": "2907119"
  },
  {
    "text": "entirely prompt driven uh there's actually more than one model in use",
    "start": "2907119",
    "end": "2912319"
  },
  {
    "text": "under the hood uh as we try different things for example we use a different model for solving the problem of if we",
    "start": "2912319",
    "end": "2919359"
  },
  {
    "text": "want to go get a package what module do we get to do that uh which sounds like a",
    "start": "2919359",
    "end": "2925280"
  },
  {
    "text": "mechanical process but it it actually isn't there's there's a couple of steps there so model helps out with that",
    "start": "2925280",
    "end": "2931559"
  },
  {
    "text": "there's very different sorts of prompts you use for trying to come up with the name of a sketch than there are for uh",
    "start": "2931559",
    "end": "2938520"
  },
  {
    "text": "answering questions uh but at the moment it's it's entirely uh prompt driven in",
    "start": "2938520",
    "end": "2944000"
  },
  {
    "text": "the sense that a large context window uh and a lot of careful context construction can handle this can improve",
    "start": "2944000",
    "end": "2950720"
  },
  {
    "text": "things and that can that can include a lot of tool use uh tool use is a very",
    "start": "2950720",
    "end": "2956440"
  },
  {
    "text": "fun feature of models where you can instruct uh so to back up and give you a",
    "start": "2956440",
    "end": "2962520"
  },
  {
    "text": "sense of uh uh how the models work an llm generates the next token based on",
    "start": "2962520",
    "end": "2968200"
  },
  {
    "text": "all the tokens that come before it when you're in chat mode and you're chatting with a model you can at any",
    "start": "2968200",
    "end": "2974839"
  },
  {
    "text": "point stop and have the model generate the next token it could be part of the thing you're asking it or its response",
    "start": "2974839",
    "end": "2981200"
  },
  {
    "text": "that meta information about who is talking is uh sort of built into just a",
    "start": "2981200",
    "end": "2986319"
  },
  {
    "text": "stream of tokens so similarly you can you can define a tool that a model",
    "start": "2986319",
    "end": "2992280"
  },
  {
    "text": "can call you can say here's a function that you can call and it will have a result",
    "start": "2992280",
    "end": "2997839"
  },
  {
    "text": "and the model can output the specialized token that says call this function give",
    "start": "2997839",
    "end": "3003000"
  },
  {
    "text": "it a name write some parameters uh and then instead of the model generating the next token you",
    "start": "3003000",
    "end": "3010280"
  },
  {
    "text": "pause the stream you the caller go and run some code you go and run that function call that it",
    "start": "3010280",
    "end": "3016240"
  },
  {
    "text": "defined paste the result of that function call in as the next set of tokens and then ask the model to generate the token after it so that",
    "start": "3016240",
    "end": "3023359"
  },
  {
    "text": "technique uh is a great way to uh have automated feedback into the model so a",
    "start": "3023359",
    "end": "3029000"
  },
  {
    "text": "classic example is a classic example is a weather function and so you define a",
    "start": "3029000",
    "end": "3034079"
  },
  {
    "text": "function which says current weather the model then you can ask the model hey",
    "start": "3034079",
    "end": "3039280"
  },
  {
    "text": "what's the weather and the model can say call function current weather your",
    "start": "3039280",
    "end": "3046000"
  },
  {
    "text": "software that's printing out the tokens pauses calls current weather says sunny",
    "start": "3046000",
    "end": "3051920"
  },
  {
    "text": "you paste sunny in there and then the model generates the next set of tokens which is the chat response saying oh",
    "start": "3051920",
    "end": "3057599"
  },
  {
    "text": "it's currently Sunny uh and that's that's the sort of easy way to plug external systems into a model uh this is",
    "start": "3057599",
    "end": "3065359"
  },
  {
    "text": "going on under the hood of the user interfaces you use onto Frontier models",
    "start": "3065359",
    "end": "3071280"
  },
  {
    "text": "so this is happening in chat GPT and Claw and all these systems uh sometimes they show it to you",
    "start": "3071280",
    "end": "3076640"
  },
  {
    "text": "happening which is how you know it's U you see it less now but uh uh about six",
    "start": "3076640",
    "end": "3082160"
  },
  {
    "text": "months ago you could see in the GPT 4 model you would ask it",
    "start": "3082160",
    "end": "3087440"
  },
  {
    "text": "questions and it would generate python programs and run them and then use the output of the Python program in its",
    "start": "3087440",
    "end": "3092760"
  },
  {
    "text": "answer I had a really fun one where I asked it how many transistors fit on the",
    "start": "3092760",
    "end": "3097880"
  },
  {
    "text": "head of a pen and it started producing an answer and it said like well transistors are about this big pins are about this big",
    "start": "3097880",
    "end": "3105480"
  },
  {
    "text": "and so I guess a magic little uh Emoji appeared uh that this means this many",
    "start": "3105480",
    "end": "3111680"
  },
  {
    "text": "transistors fit on the head of a pin some very large number uh and if you click on the Emoji it shows you the Python program generated to do the",
    "start": "3111680",
    "end": "3117960"
  },
  {
    "text": "arithmetic it executed that as a function called came back with the result and that saved it the trouble of",
    "start": "3117960",
    "end": "3124440"
  },
  {
    "text": "trying to do the arithmetic itself which LMS are notoriously notoriously struggle with",
    "start": "3124440",
    "end": "3129640"
  },
  {
    "text": "doing arithmetic so it's a great thing to Outsource to a program and so it's a funny workaround",
    "start": "3129640",
    "end": "3135200"
  },
  {
    "text": "because you know if you're a calculator for words you're not necessarily necessarily a calculator for numbers",
    "start": "3135200",
    "end": "3141520"
  },
  {
    "text": "yeah much better you can't do those reliably then you could just write a program that does it and Returns the same thing every time yes they're very",
    "start": "3141520",
    "end": "3147480"
  },
  {
    "text": "good at writing programs to do the arithmetic very bad at doing the arithmetic so it's a great uh compromise",
    "start": "3147480",
    "end": "3153119"
  },
  {
    "text": "the thing we do with Sketch is try to give uh the underlying model access to",
    "start": "3153119",
    "end": "3158720"
  },
  {
    "text": "information about uh the environment it's writing code in using function calls so a lot of our work is not",
    "start": "3158720",
    "end": "3165119"
  },
  {
    "text": "fine-tuning the model it's about letting it ask questions about not just the standard library but the other libraries",
    "start": "3165119",
    "end": "3171480"
  },
  {
    "text": "it's trying to use so that it can get better answers it can look up the God do",
    "start": "3171480",
    "end": "3176520"
  },
  {
    "text": "for a method if it thinks it wants to call it use that as part of its decision-making process about the code",
    "start": "3176520",
    "end": "3182559"
  },
  {
    "text": "it generates can you describe let it ask I mean you've said it a couple times and I've been curious about this when you say let it ask what does that mean like",
    "start": "3182559",
    "end": "3191079"
  },
  {
    "start": "3184000",
    "end": "3376000"
  },
  {
    "text": "decompress that compressed definition so at the beginning in your system prompt",
    "start": "3191079",
    "end": "3196839"
  },
  {
    "text": "or something like your system prompt depends on the API on exactly how the model works you say there is a function",
    "start": "3196839",
    "end": "3202520"
  },
  {
    "text": "call which is get method docs um it has a parameter which is name of",
    "start": "3202520",
    "end": "3208480"
  },
  {
    "text": "method and in the middle of then you can ask a uh you can construct a question to",
    "start": "3208480",
    "end": "3214000"
  },
  {
    "text": "an llm that says generate a program that does this uh with the system prompt",
    "start": "3214000",
    "end": "3219400"
  },
  {
    "text": "which explains that there's a tool call there and so as your llm is generating",
    "start": "3219400",
    "end": "3225559"
  },
  {
    "text": "that program it can pause make a system call make a function a tool call that",
    "start": "3225559",
    "end": "3230720"
  },
  {
    "text": "says uh get me the docs for this and so the llm decides that it wants to know",
    "start": "3230720",
    "end": "3236040"
  },
  {
    "text": "something about that method call and then you go and run a program which gets the result gets the uh uh the",
    "start": "3236040",
    "end": "3243799"
  },
  {
    "text": "documentation for that method from the actual source of Truth you paste it into the prompt and then the LM continues",
    "start": "3243799",
    "end": "3250559"
  },
  {
    "text": "writing the program using that documentation as now part of its uh its",
    "start": "3250559",
    "end": "3258079"
  },
  {
    "text": "prompt and so this is the model driving the questions about what it wants to",
    "start": "3258079",
    "end": "3263839"
  },
  {
    "text": "know about and it just blocks and waits for the to come back yes effectively",
    "start": "3263839",
    "end": "3269119"
  },
  {
    "text": "yeah yeah so it's like a it's like an embed if you step back to like uh",
    "start": "3269119",
    "end": "3274960"
  },
  {
    "text": "running llama CPP yourself or something like this can you can sort of",
    "start": "3274960",
    "end": "3280200"
  },
  {
    "text": "oversimplify one of these models as every time you want to generate a token",
    "start": "3280200",
    "end": "3286079"
  },
  {
    "text": "you hand the entire history of the conversation you've had or whatever the text is before",
    "start": "3286079",
    "end": "3291920"
  },
  {
    "text": "it to the GPU to build the state of the model and then it generates the next",
    "start": "3291920",
    "end": "3298440"
  },
  {
    "text": "token it actually generates a probability uh value for every token in its Al in its uh token",
    "start": "3298440",
    "end": "3305480"
  },
  {
    "text": "Set uh and then the CPU picks the next token attaches it to the full set set of tokens and then does that whole process",
    "start": "3305480",
    "end": "3312400"
  },
  {
    "text": "again of sending over the entire conversation uh and then generating the next token and so if you think about",
    "start": "3312400",
    "end": "3318000"
  },
  {
    "text": "that sort of uh that very long big giant for loop around the outside of every",
    "start": "3318000",
    "end": "3324559"
  },
  {
    "text": "time uh there is a there's a new token the token is chosen from the set of probabilities that comes",
    "start": "3324559",
    "end": "3330319"
  },
  {
    "text": "back is added to the set and then a new set of probabilities is generated for the next token you can imagine in the middle of that for Loop having some very",
    "start": "3330319",
    "end": "3338640"
  },
  {
    "text": "traditional code in there that inserts a stack of tokens that wasn't actually decided by the llm but then become part",
    "start": "3338640",
    "end": "3345720"
  },
  {
    "text": "of the history that the llm is generating the next token from and so this is that's how those embeds work you",
    "start": "3345720",
    "end": "3352359"
  },
  {
    "text": "can effectively have the llm communicate with the outside world in the middle there by it driving that or you don't",
    "start": "3352359",
    "end": "3360760"
  },
  {
    "text": "even have to have it drive it you could have software outside the LM that looks at the the tokens set as as it's",
    "start": "3360760",
    "end": "3367240"
  },
  {
    "text": "appeared and then insert more tokens for it so this is all the fun stuff you can",
    "start": "3367240",
    "end": "3372359"
  },
  {
    "text": "do by running these models yourself yeah I know it's so fun it's a is go particularly well suited for this kind",
    "start": "3372359",
    "end": "3378440"
  },
  {
    "start": "3376000",
    "end": "3603000"
  },
  {
    "text": "of tooling because of the nature of the language or is it just your favorite or yeah that's a really good question the",
    "start": "3378440",
    "end": "3384200"
  },
  {
    "text": "best programming language for llms today is Python and I believe that is uh a",
    "start": "3384200",
    "end": "3393200"
  },
  {
    "text": "historical artifact of the fact that all of the researchers working on uh",
    "start": "3393200",
    "end": "3399799"
  },
  {
    "text": "generative models uh work in Python and so they spend the most time",
    "start": "3399799",
    "end": "3406640"
  },
  {
    "text": "testing it with python and judging a model's uh results by python output",
    "start": "3406640",
    "end": "3412000"
  },
  {
    "text": "there was a great example of this in one of the open benchmarks I looked at and I believe this has all been corrected",
    "start": "3412000",
    "end": "3417480"
  },
  {
    "text": "since then this is all about a year old there was a multilanguage benchmark that tested how good a model",
    "start": "3417480",
    "end": "3423960"
  },
  {
    "text": "is across multiple languages and I opened up the source set",
    "start": "3423960",
    "end": "3429319"
  },
  {
    "text": "for it and looked at some of the go code because I'm a go programmer uh and it",
    "start": "3429319",
    "end": "3434799"
  },
  {
    "text": "had been machine translated from python so that all of the variable names in",
    "start": "3434799",
    "end": "3440200"
  },
  {
    "text": "this go code uh used underscores instead of camel case uh and you know the the",
    "start": "3440200",
    "end": "3446599"
  },
  {
    "text": "models were getting a certain percentage success rate generating these results uh so Josh went through actually and uh",
    "start": "3446599",
    "end": "3455240"
  },
  {
    "text": "made these more idiomatic in the go style of using camel case and you know putting everything in the right place",
    "start": "3455240",
    "end": "3460760"
  },
  {
    "text": "and the model gave much better results on this Benchmark H and so that's an",
    "start": "3460760",
    "end": "3466359"
  },
  {
    "text": "example of where languages beyond the basic ones that the developers of the models care about are not being paid as",
    "start": "3466359",
    "end": "3474680"
  },
  {
    "text": "much attention to as what you would like and things are getting a lot better there you know as these you know the",
    "start": "3474680",
    "end": "3479920"
  },
  {
    "text": "models are much more sophisticated the teams building them are much larger they care about a larger set of languages and",
    "start": "3479920",
    "end": "3486520"
  },
  {
    "text": "so I don't think it's all as python Centric as it used to be but that is still very much the first and most",
    "start": "3486520",
    "end": "3491720"
  },
  {
    "text": "important of the languages as for how well go works it seems to work pretty well models are good at it uh by our",
    "start": "3491720",
    "end": "3497920"
  },
  {
    "text": "benchmarks like we said if we if we took the bench marks and made them more go like the models actually got better results they they have a they have a",
    "start": "3497920",
    "end": "3504640"
  },
  {
    "text": "real tendency to uh uh to to understand the language we think it's a pretty good",
    "start": "3504640",
    "end": "3510280"
  },
  {
    "text": "fit you know there are definitely there are definitely times in models struggle but it's a garbage",
    "start": "3510280",
    "end": "3517319"
  },
  {
    "text": "collected language which helps because in just the same way the garbage collection reduces the cognitive load",
    "start": "3517319",
    "end": "3523480"
  },
  {
    "text": "for programmers as they're writing programs uh it reduces the load on the llm in just the same way they don't have",
    "start": "3523480",
    "end": "3529480"
  },
  {
    "text": "to track uh the state of memory and when to free it so they have a bit more",
    "start": "3529480",
    "end": "3534720"
  },
  {
    "text": "thinking time to worry about solving your problem so in that way it's a good language it's not too syntax heavy but",
    "start": "3534720",
    "end": "3541079"
  },
  {
    "text": "it's also it doesn't have ambiguities that humans struggle",
    "start": "3541079",
    "end": "3546640"
  },
  {
    "text": "with yeah it seems to work well yeah there aren't a lot",
    "start": "3546640",
    "end": "3552079"
  },
  {
    "text": "of I don't I haven't seen much research into what is the best language for an llm uh it does seem like an eminently",
    "start": "3552079",
    "end": "3558559"
  },
  {
    "text": "testable thing like there's some interesting in fact it may end up influencing programming language design",
    "start": "3558559",
    "end": "3565319"
  },
  {
    "text": "in a sense imagine you are building a new programming language and you develop",
    "start": "3565319",
    "end": "3570599"
  },
  {
    "text": "a training set that's automatically generated uh based on you know",
    "start": "3570599",
    "end": "3576079"
  },
  {
    "text": "translating some existing programs into your language and you train models for it you could imagine tweaking the syntax",
    "start": "3576079",
    "end": "3582359"
  },
  {
    "text": "of your new language regenerating the training set and then seeing if your benchmarks improve or not so you can",
    "start": "3582359",
    "end": "3588160"
  },
  {
    "text": "imagine yeah you can imagine the driving uh readability of programming languages",
    "start": "3588160",
    "end": "3593680"
  },
  {
    "text": "based on your ability to train an llm to write this language so you there's lots of really fun things that will happen",
    "start": "3593680",
    "end": "3599799"
  },
  {
    "text": "long term that you know I don't think anyone started on work like that yet right so the level that you all are",
    "start": "3599799",
    "end": "3605079"
  },
  {
    "start": "3603000",
    "end": "3727000"
  },
  {
    "text": "working at with Sketch with go in particular is the prompting you're doing and the Contex",
    "start": "3605079",
    "end": "3611240"
  },
  {
    "text": "thing and everything else that you're building is it at a layer of abstraction where you could replace go relatively",
    "start": "3611240",
    "end": "3618440"
  },
  {
    "text": "easily with insert General programming language or is it like well that would be a new product that we would build",
    "start": "3618440",
    "end": "3624960"
  },
  {
    "text": "like how hard is that yeah it's a good question it's all of the techniques we're applying in general uh but they",
    "start": "3624960",
    "end": "3631839"
  },
  {
    "text": "are all very uh each each technique requires a lot of go specific implementation so in",
    "start": "3631839",
    "end": "3638839"
  },
  {
    "text": "much the same way that like a lot of the techniques inside a language server uh for a programming language these are the",
    "start": "3638839",
    "end": "3644839"
  },
  {
    "text": "systems inside vs code for generating information about programming languages those are the techniques are general for",
    "start": "3644839",
    "end": "3651000"
  },
  {
    "text": "like what methods are available on this object are the are very similar in",
    "start": "3651000",
    "end": "3656520"
  },
  {
    "text": "as they would be in Java for example M uh but the specifics of implementing them for both languages are radically",
    "start": "3656520",
    "end": "3662280"
  },
  {
    "text": "different uh and I think it's it's a lot like that for sketch uh the tricks we're using for sketch are very go specific",
    "start": "3662280",
    "end": "3668960"
  },
  {
    "text": "and if we wanted to build one for Ruby we would have to build something very very different okay so yes I consider it",
    "start": "3668960",
    "end": "3674240"
  },
  {
    "text": "very much a go product right now and I I really like that Focus that that gives",
    "start": "3674240",
    "end": "3679559"
  },
  {
    "text": "us sure because go is a big enough problem on its own let alone all of programming yeah yeah I'm just asking",
    "start": "3679559",
    "end": "3686240"
  },
  {
    "text": "that because I wonder how valuable and important tooling like this would be for",
    "start": "3686240",
    "end": "3691960"
  },
  {
    "text": "each language Community to either provide or fund or hope that somebody",
    "start": "3691960",
    "end": "3697400"
  },
  {
    "text": "builds because if the llm related tooling for",
    "start": "3697400",
    "end": "3702559"
  },
  {
    "text": "go because of sketch just hypothetically becomes orders of magnitude more useful than just talking",
    "start": "3702559",
    "end": "3709319"
  },
  {
    "text": "to chat GPT about my Elixir code for instance MH well that's a real Advantage",
    "start": "3709319",
    "end": "3715480"
  },
  {
    "text": "for go and the go Community I mean it's great for productivity for gophers and going",
    "start": "3715480",
    "end": "3720720"
  },
  {
    "text": "back to maybe the original question about you know should tail scale have its own little chat bot built into it",
    "start": "3720720",
    "end": "3726960"
  },
  {
    "text": "like does each Community need to take up this mantle and say we need better tooling or is it like VSS code should",
    "start": "3726960",
    "end": "3734480"
  },
  {
    "start": "3727000",
    "end": "4036000"
  },
  {
    "text": "just do it for everybody I mean that's a really good question uh good job yeah so",
    "start": "3734480",
    "end": "3741359"
  },
  {
    "text": "to to you know I I very I very much admire vs code uh I use it uh and uh",
    "start": "3741359",
    "end": "3748400"
  },
  {
    "text": "which you know I don't I don't actually have to admire a program to use it that's better than admiring is using I think yeah that's right but but I",
    "start": "3748400",
    "end": "3753720"
  },
  {
    "text": "actually I I do both like I both admire but to look at the inside of vs",
    "start": "3753720",
    "end": "3761079"
  },
  {
    "text": "code which I've been doing a bunch of recently uh vs code didn't actually",
    "start": "3761079",
    "end": "3766279"
  },
  {
    "text": "solve language servers for all programming languages they built",
    "start": "3766279",
    "end": "3771480"
  },
  {
    "text": "JavaScript and typescript uh Json and and I think they maintained the",
    "start": "3771480",
    "end": "3777079"
  },
  {
    "text": "C plug-in uh they started the go plugin I think and then it got taken over by",
    "start": "3777079",
    "end": "3783039"
  },
  {
    "text": "the the go team at Google who now maintain the go support in vs code I",
    "start": "3783039",
    "end": "3788319"
  },
  {
    "text": "don't think the Microsoft team built the Ruby support in vs code I don't know who",
    "start": "3788319",
    "end": "3793400"
  },
  {
    "text": "did the python implementation uh but uh a lot of the Machinery in VSS code is",
    "start": "3793400",
    "end": "3798960"
  },
  {
    "text": "actually Community maintained for these various programming languages and so I'm not sure there is another option",
    "start": "3798960",
    "end": "3807000"
  },
  {
    "text": "than imagining a world where each of these communities supports the the Tooling in some form uh I don't know if",
    "start": "3807000",
    "end": "3814160"
  },
  {
    "text": "each programming language needs to go out and build their own sketch maybe there is some generalizable intermediate",
    "start": "3814160",
    "end": "3820480"
  },
  {
    "text": "layer some equivalent of a language server that can be written to feed underlying models um given our we we're",
    "start": "3820480",
    "end": "3828920"
  },
  {
    "text": "just starting to explore this space sketch is very new we basically started it sometime near the end of November so",
    "start": "3828920",
    "end": "3834520"
  },
  {
    "text": "there's not much to it yet uh yeah but uh the uh uh so far what we found is",
    "start": "3834520",
    "end": "3842599"
  },
  {
    "text": "it's far more than the sort of language server environment that you get with vs",
    "start": "3842599",
    "end": "3847880"
  },
  {
    "text": "code more Machinery is needed uh to uh to really give the LM all the tooling it",
    "start": "3847880",
    "end": "3853680"
  },
  {
    "text": "needs uh the language server is very useful we actually use the go language server in sketch go pleas is a big part of our infrastructure it's really",
    "start": "3853680",
    "end": "3860119"
  },
  {
    "text": "wonderful software uh but there's there's far more to it than that um to",
    "start": "3860119",
    "end": "3865319"
  },
  {
    "text": "the point where we need to maintain an entire Linux VM uh to support the tooling behind uh feeding the model uh",
    "start": "3865319",
    "end": "3874520"
  },
  {
    "text": "so what each Community needs to provide I think that's that's a that's the research in progress is figuring that",
    "start": "3874520",
    "end": "3880400"
  },
  {
    "text": "out is it interesting question and one that I think will be open for a while I do not want to see a world where python",
    "start": "3880400",
    "end": "3889240"
  },
  {
    "text": "continues to proliferate merely because of its previous position I I do see with",
    "start": "3889240",
    "end": "3895200"
  },
  {
    "text": "tooling like Devon and bolt and VZ these",
    "start": "3895200",
    "end": "3900720"
  },
  {
    "text": "are very front-end javascrip the companies that are producing these things which is fine but it's like if",
    "start": "3900720",
    "end": "3907319"
  },
  {
    "text": "you are just going to go use that it's going to produce for you a react and nextjs front end with a Prisma vased",
    "start": "3907319",
    "end": "3914839"
  },
  {
    "text": "back and like it's all very much like these are the tools it does and that's all well and good but that's going to",
    "start": "3914839",
    "end": "3921559"
  },
  {
    "text": "proliferate more and more of that one thing where I would love to see a diversity where it's like yeah where's",
    "start": "3921559",
    "end": "3927200"
  },
  {
    "text": "the is there a specific thing for rails people is there one for people who like",
    "start": "3927200",
    "end": "3933599"
  },
  {
    "text": "uh Zig now moving outside of the world of web development but you know what I'm saying like and I think your answer",
    "start": "3933599",
    "end": "3941240"
  },
  {
    "text": "might be right which is like well every Community is going to have to provide some sort of like greasing of the skids",
    "start": "3941240",
    "end": "3947359"
  },
  {
    "text": "for whatever editor is popular or used in order",
    "start": "3947359",
    "end": "3952559"
  },
  {
    "text": "to make their tooling you know work well inside of these llm",
    "start": "3952559",
    "end": "3958599"
  },
  {
    "text": "based helpers Beyond just being like chat gbt knows about you which is kind of like the what people are at right now",
    "start": "3958599",
    "end": "3965160"
  },
  {
    "text": "is like does chat GPT know about me it's the new in my googleable right it's the",
    "start": "3965160",
    "end": "3970880"
  },
  {
    "text": "new SEO at this point I've heard people talk about that uh a startup founder who I would't know him U mentioned that they",
    "start": "3970880",
    "end": "3976960"
  },
  {
    "text": "were busy retooling their product so that uh uh the foundation models under",
    "start": "3976960",
    "end": "3982480"
  },
  {
    "text": "things like vzer and bolt would be more likely to NP include their package to solve a problem it's super smart to do",
    "start": "3982480",
    "end": "3988680"
  },
  {
    "text": "that right now I agree do did they divulge any of the how like what are the the mechanical step that you do that I",
    "start": "3988680",
    "end": "3995520"
  },
  {
    "text": "was actually really happy uh that they said that the their plan was to make it",
    "start": "3995520",
    "end": "4000799"
  },
  {
    "text": "really easy to npm a package and uh not require a separate signup flow to",
    "start": "4000799",
    "end": "4008000"
  },
  {
    "text": "actually get started oh that's nice yeah I I thought it was wonderful like their their solution uh to make their product",
    "start": "4008000",
    "end": "4013920"
  },
  {
    "text": "more chat GP table I guess you might say uh is just make their product better",
    "start": "4013920",
    "end": "4019240"
  },
  {
    "text": "which you know if that's how I Vu guarded them yeah uh I I'm sure one day",
    "start": "4019240",
    "end": "4024680"
  },
  {
    "text": "we'll end up in the search engine optimization world of uh Frontier models but uh uh today me some black magic for",
    "start": "4024680",
    "end": "4031920"
  },
  {
    "text": "sale you know here's how you really do it yeah uh I I don't see why a Frontier",
    "start": "4031920",
    "end": "4038760"
  },
  {
    "start": "4036000",
    "end": "4239000"
  },
  {
    "text": "Model couldn't run an ad auction uh for deciding what fine-tuning set to bring",
    "start": "4038760",
    "end": "4044119"
  },
  {
    "text": "in so I had uh know again to talk about experiences I was using one of the voice",
    "start": "4044119",
    "end": "4050760"
  },
  {
    "text": "models and talking to it as I was walking down the street and I asked it some question about WD40 because I had a",
    "start": "4050760",
    "end": "4056680"
  },
  {
    "text": "squeaky door uh and I think I described in my question WD40 as a lubricant uh",
    "start": "4056680",
    "end": "4064319"
  },
  {
    "text": "and it turns out I just didn't understand that it's not a lubricant it's a solvent uh and the purpose of it",
    "start": "4064319",
    "end": "4070079"
  },
  {
    "text": "is to remove grease uh it took me years to realize that I think someone finally told me cuz I've using it as a Lube all",
    "start": "4070079",
    "end": "4076440"
  },
  {
    "text": "these years yeah oh my gosh why you got to keep reapplying it you know it's not a very good Lube well I I just had your",
    "start": "4076440",
    "end": "4083520"
  },
  {
    "text": "experience but it was an llm that told me hilarious and uh it mentioned in passing it's like yeah you could also",
    "start": "4083520",
    "end": "4089680"
  },
  {
    "text": "you know you could use WD40 and then use a lubricant like and then it listed some brand name the moment I heard the brand",
    "start": "4089680",
    "end": "4095680"
  },
  {
    "text": "name I was like oh I see a Frontier Model could run an ad auction on fine-tuning which brand name to inject",
    "start": "4095680",
    "end": "4101719"
  },
  {
    "text": "there uh and that would be a% yeah it wouldn't require uh wouldn't require",
    "start": "4101719",
    "end": "4107080"
  },
  {
    "text": "doing it into the pre-training months ahead of time you could do that sort of on a on an hour by hour basis so that",
    "start": "4107080",
    "end": "4113520"
  },
  {
    "text": "world is coming and then once there's a world of ads there's a world of SEO and all the rest of it well the more",
    "start": "4113520",
    "end": "4119719"
  },
  {
    "text": "Paramount they become and Adam you can probably speak to this because you're injecting into every aspect of your life like if they if it if the answer",
    "start": "4119719",
    "end": "4126238"
  },
  {
    "text": "includes a product right there like you're just going to be like all right I got to get that sometimes you don't even realize Kleenex is a product you think",
    "start": "4126239",
    "end": "4132480"
  },
  {
    "text": "that that's a category but no that's a product yeah absolutely it's hard to tell honestly Kleenex is an easy one for",
    "start": "4132480",
    "end": "4138758"
  },
  {
    "text": "me because we don't have kleenex in Australia where I'm from so I came here everyone started calling tissues Kleenex",
    "start": "4138759",
    "end": "4144080"
  },
  {
    "text": "and it's it was a bit of a bit of a surprise to me it's like Coke or Coca-Cola something like that you know",
    "start": "4144080",
    "end": "4149440"
  },
  {
    "text": "yeah right exactly yeah uh you know I don't know",
    "start": "4149440",
    "end": "4155080"
  },
  {
    "text": "if I've gotten some hallucinations let's just say on products mhm and even",
    "start": "4155080",
    "end": "4161278"
  },
  {
    "text": "limited information of what's the true good option when it comes to product",
    "start": "4161279",
    "end": "4166838"
  },
  {
    "text": "search I haven't done a ton of it um mainly on like the motherboard search like I want to do something that has the",
    "start": "4166839",
    "end": "4174719"
  },
  {
    "text": "option for either an AMD ryzen or thread Ripper with you know more of a",
    "start": "4174719",
    "end": "4180920"
  },
  {
    "text": "workstation Enterprise class CPU and I want to like maximize some PCI Lanes so",
    "start": "4180920",
    "end": "4186359"
  },
  {
    "text": "I'm just trying to like figure out what's out there I'd prefer the chat interface to find things versus the",
    "start": "4186359",
    "end": "4192159"
  },
  {
    "text": "Google interface which is search by nature to find things but thus far it hasn't been super fruitful I think",
    "start": "4192159",
    "end": "4198560"
  },
  {
    "text": "eventually it'd be cool but it's not there yet I imagine in the YouTube video of this a little Intel Xeon Banner will",
    "start": "4198560",
    "end": "4205280"
  },
  {
    "text": "appear just as you say options on the ad so yeah yeah exactly so I'm a fan of Intel xon too I got the I got the Intel",
    "start": "4205280",
    "end": "4211880"
  },
  {
    "text": "xon uh 4210 oh now it's really popping up there you go bing bing bing uh it's like uh in Silicon Valley when Des was",
    "start": "4211880",
    "end": "4219560"
  },
  {
    "text": "talking out loud and they had the I think they had AI in the VR and the it was yeah it was doing some cool stuff",
    "start": "4219560",
    "end": "4225320"
  },
  {
    "text": "was pulling up ads real time it's cool it was cool but uh yeah Intel's cool ANB",
    "start": "4225320",
    "end": "4231159"
  },
  {
    "text": "is cool but PCI leans are pcie lanes are even cooler you know give me the Max uh",
    "start": "4231159",
    "end": "4238040"
  },
  {
    "text": "buy 16 you know David maybe we close with this for those who aren't Gophers out there of course uh brand new you",
    "start": "4238040",
    "end": "4244480"
  },
  {
    "start": "4239000",
    "end": "4407000"
  },
  {
    "text": "know hot off the press still in development three months old sketch. Dev check it out if you're into to go and",
    "start": "4244480",
    "end": "4250679"
  },
  {
    "text": "those kind of things but let's imagine you're just a Ruby programmer out there and you c a came across your blog post",
    "start": "4250679",
    "end": "4256679"
  },
  {
    "text": "about what you've been doing these three methods of working with AIS you have autocomplete you've got chat You' got",
    "start": "4256679",
    "end": "4264440"
  },
  {
    "text": "search where should folks get started if they haven't yet first of all is today the day like is it worth it now or",
    "start": "4264440",
    "end": "4271360"
  },
  {
    "text": "should I wait and then secondly if I am going to dive in and I just want to use it in my local environment to like just",
    "start": "4271360",
    "end": "4277880"
  },
  {
    "text": "code better today what would you what would you suggest yeah good question especially for non- Gophers uh the I",
    "start": "4277880",
    "end": "4284719"
  },
  {
    "text": "would suggest trying out the codee completion engines because they take a little bit",
    "start": "4284719",
    "end": "4290360"
  },
  {
    "text": "of getting used to but not a lot and depending if you're writing the sorts of",
    "start": "4290360",
    "end": "4295440"
  },
  {
    "text": "programs they're good at they're extremely helpful they save a lot of typing uh and it turns out I was",
    "start": "4295440",
    "end": "4301440"
  },
  {
    "text": "surprised to learn this but what I learned from code completion engines is a lot of my programming is fundamentally typing limited it's only so much my",
    "start": "4301440",
    "end": "4307719"
  },
  {
    "text": "hands can do every day it's uh the uh and that they're extremely helpful there",
    "start": "4307719",
    "end": "4313520"
  },
  {
    "text": "uh the the state of code completion engines is they are uh pretty good at all languages whether you know caveat",
    "start": "4313520",
    "end": "4320880"
  },
  {
    "text": "that they're probably not very good at Cobalt uh or Fortran uh but uh all all",
    "start": "4320880",
    "end": "4326920"
  },
  {
    "text": "the sort of General languages like especially like Ruby uh I'd expect them to be decent at I suspect the world of",
    "start": "4326920",
    "end": "4333360"
  },
  {
    "text": "code completion engines will get better at specific languages as people go deeper on the technology it's a thing I",
    "start": "4333360",
    "end": "4338719"
  },
  {
    "text": "continue to work on and so I I feel confident that it can be improved the other place that I think most",
    "start": "4338719",
    "end": "4345480"
  },
  {
    "text": "programmers could get value today uh if they're not a go programmer is writing",
    "start": "4345480",
    "end": "4351480"
  },
  {
    "text": "small isolated pieces of code in a a chat interface so you could try out a",
    "start": "4351480",
    "end": "4356880"
  },
  {
    "text": "chat GPT or a Claude or if you really want to have some fun run a local model",
    "start": "4356880",
    "end": "4362000"
  },
  {
    "text": "and ask it to solve problems like try try llama CPP try Lama try these various",
    "start": "4362000",
    "end": "4367920"
  },
  {
    "text": "local products um grab one of the really fun models uh it's especially easy to",
    "start": "4367920",
    "end": "4374320"
  },
  {
    "text": "try on a Mac with the unified memory uh if you're on a PC you know you you might",
    "start": "4374320",
    "end": "4379520"
  },
  {
    "text": "have to find a model that fits in your GPU uh but it's a ton of fun and use it to say like write me a ruby function",
    "start": "4379520",
    "end": "4386199"
  },
  {
    "text": "that takes these parameters and produces this result uh and I suspect the model",
    "start": "4386199",
    "end": "4391360"
  },
  {
    "text": "will give you a pretty good result so those are the places I would start because",
    "start": "4391360",
    "end": "4396800"
  },
  {
    "text": "those require the least amount of learning how to hold the model correctly",
    "start": "4396800",
    "end": "4402560"
  },
  {
    "text": "and uh you'll get the most benefit uh quickly love it good answer I'm just",
    "start": "4402560",
    "end": "4408199"
  },
  {
    "start": "4407000",
    "end": "4494000"
  },
  {
    "text": "wondering out loud and feel free not to know but when it comes to prompting I",
    "start": "4408199",
    "end": "4413679"
  },
  {
    "text": "know we're past the age of magical incantations but as you guys have been building out a product which is",
    "start": "4413679",
    "end": "4419760"
  },
  {
    "text": "basically sophisticated prompting are there guides that are useful or they're",
    "start": "4419760",
    "end": "4425280"
  },
  {
    "text": "like I remember finding a site I can't remember right now there's like people are just sharing their system prompts for certain things they do like maybe",
    "start": "4425280",
    "end": "4431600"
  },
  {
    "text": "there's like a ruby prompting guide which makes it a little bit easier to get quality",
    "start": "4431600",
    "end": "4438120"
  },
  {
    "text": "results out faster does either of you guys know yeah i' I've seen people write",
    "start": "4438120",
    "end": "4443159"
  },
  {
    "text": "guides like that I would say the guides I've read are now out of date uh like we were saying earlier they are guides go",
    "start": "4443159",
    "end": "4449239"
  },
  {
    "text": "out of date uh the thing I find most useful is to think",
    "start": "4449239",
    "end": "4456600"
  },
  {
    "text": "of uh the model I'm talking to as someone who just joined the company uh",
    "start": "4456600",
    "end": "4462400"
  },
  {
    "text": "sometimes I think of them as an intern though every now and again the models produce much better code than I can uh",
    "start": "4462400",
    "end": "4469040"
  },
  {
    "text": "but you know interns have done that too that's that happens uh know it's uh uh",
    "start": "4469040",
    "end": "4475000"
  },
  {
    "text": "so and then as you're writing the question for it imagine it like here's a",
    "start": "4475000",
    "end": "4480120"
  },
  {
    "text": "you know I'm talking to a smart person who knows nothing about what I'm doing and they need some background and that",
    "start": "4480120",
    "end": "4486560"
  },
  {
    "text": "gets me really far with the current Frontier models and so that would be my general",
    "start": "4486560",
    "end": "4492199"
  },
  {
    "text": "piece of advice that I think applies to any programming I agree with that too the random just give me X is you'll get",
    "start": "4492199",
    "end": "4499639"
  },
  {
    "start": "4494000",
    "end": "4532000"
  },
  {
    "text": "a result but you'll have to massage it further you'll have to It'll ask you more I will often give it context like",
    "start": "4499639",
    "end": "4506760"
  },
  {
    "text": "you've said this intern the smart person that's new to context they don't have the background awareness that uh that",
    "start": "4506760",
    "end": "4513080"
  },
  {
    "text": "you want somebody to have and I'll often like give it a lot of that have a particular request but then also say is",
    "start": "4513080",
    "end": "4521480"
  },
  {
    "text": "there anything else I can give you or any other information you need to give me a successful just some",
    "start": "4521480",
    "end": "4528080"
  },
  {
    "text": "version of like be successful with our our goal and it's strange even talking like",
    "start": "4528080",
    "end": "4533560"
  },
  {
    "start": "4532000",
    "end": "4831000"
  },
  {
    "text": "that too as I even say it out loud like our goal as if it's you know human and",
    "start": "4533560",
    "end": "4539199"
  },
  {
    "text": "Jared you know where where we're standing on this Jared and I have some history so I've been very kind please and thank you very nice he talks to him",
    "start": "4539199",
    "end": "4544880"
  },
  {
    "text": "like we're on the same team and stuff if it's if it gives me a great result I say fantastic you know I'm like you know",
    "start": "4544880",
    "end": "4550480"
  },
  {
    "text": "high fives why would I yeah high fives why would I be any different you offer it money no you I haven't done that yet",
    "start": "4550480",
    "end": "4556280"
  },
  {
    "text": "I got to try that I got to try that honestly but I will ask it like will do you could you be more successful is",
    "start": "4556280",
    "end": "4562760"
  },
  {
    "text": "there anything else I can give you any more information I can give you to get us to our goal you know and I found that",
    "start": "4562760",
    "end": "4568560"
  },
  {
    "text": "that's it like it's context it's a full circumference of the problem set as much",
    "start": "4568560",
    "end": "4573800"
  },
  {
    "text": "as you can that makes sense and then you will have a more fruitful interaction I",
    "start": "4573800",
    "end": "4579800"
  },
  {
    "text": "will also say that I'm uh more exclusively using chat gbt and so the 01",
    "start": "4579800",
    "end": "4586800"
  },
  {
    "text": "model while it's expensive let's just say I I don't have the expensive plan I",
    "start": "4586800",
    "end": "4592920"
  },
  {
    "text": "I still don't feel like I could be the person that spends 200 bucks a month on this thing uh I would much rather buy a GPU than spend 200 bucks a month somehow",
    "start": "4592920",
    "end": "4600000"
  },
  {
    "text": "that math makes more sense to me um but then like 0 one's been pretty successful",
    "start": "4600000",
    "end": "4606080"
  },
  {
    "text": "with thinking and iterating and you know being more Precision whereas 40 was a",
    "start": "4606080",
    "end": "4612199"
  },
  {
    "text": "bit more brute but that's just my personal take I think you I think you might be on to something with being nice",
    "start": "4612199",
    "end": "4618239"
  },
  {
    "text": "to models uh I caught myself being pretty Curt with models a few months back uh and uh you know discussing this",
    "start": "4618239",
    "end": "4625800"
  },
  {
    "text": "a lot uh with Josh he mentioned uh uh you know the conclusion we came to was",
    "start": "4625800",
    "end": "4631880"
  },
  {
    "text": "you know one of the challenges of not being nice to models is it sort of trains you to not be nice to people yeah you're using all the same tools and so",
    "start": "4631880",
    "end": "4638679"
  },
  {
    "text": "it might just be good for you to be nice to models I mean I just don't if it's humanistic even you know",
    "start": "4638679",
    "end": "4645920"
  },
  {
    "text": "uh similar to a human why not just be kind you know why not I hate to break it",
    "start": "4645920",
    "end": "4651920"
  },
  {
    "text": "to you Adam but this is not similar to a human it's the iteration is it is it",
    "start": "4651920",
    "end": "4658080"
  },
  {
    "text": "certainly is if I were collaborating if that was a human over there giving me the answers back it would be very human",
    "start": "4658080",
    "end": "4664880"
  },
  {
    "text": "volleyball iterative if if it was right",
    "start": "4664880",
    "end": "4669920"
  },
  {
    "text": "I get that it's not but I'm also like like David why not I think I'm I think",
    "start": "4669920",
    "end": "4675480"
  },
  {
    "text": "I'm somewhere between your two uh positions uh because I I do think it's just a machine and it's just a tool I",
    "start": "4675480",
    "end": "4682000"
  },
  {
    "text": "don't think it's human don't don't let me think that well you kind of just said that you think it is did I I mean is",
    "start": "4682000",
    "end": "4687400"
  },
  {
    "text": "that what is that how you interpreted it I just meant that if it maybe what I mean by that to be more",
    "start": "4687400",
    "end": "4694199"
  },
  {
    "text": "clear is is kind of keying off what David said which is like being kind just",
    "start": "4694199",
    "end": "4699840"
  },
  {
    "text": "why not I don't know I'm not I'm like overly like thank you so much you're amazing time",
    "start": "4699840",
    "end": "4705440"
  },
  {
    "text": "I think it's just I'm a kind person to Google when it got you a result in the search engine like you just your life it",
    "start": "4705440",
    "end": "4711639"
  },
  {
    "text": "is not a prompt where there's a an EB and a flow or a back and a forth you know it's just simply return an answer",
    "start": "4711639",
    "end": "4719239"
  },
  {
    "text": "yeah well how are we doing you know ask you at the end how are we doing you know I'm not that being said I'm not like",
    "start": "4719239",
    "end": "4725639"
  },
  {
    "text": "thank you very much I'm just I'm just antagonizing him at this point yeah I know you are you're really digging into me but",
    "start": "4725639",
    "end": "4732280"
  },
  {
    "text": "I I I can catch myself saying like that's awesome or great job or I agree",
    "start": "4732280",
    "end": "4737760"
  },
  {
    "text": "with that things isms like that like you would say to another person if that makes you feel more like a nice human",
    "start": "4737760",
    "end": "4743679"
  },
  {
    "text": "then you should just keep on doing that but I don't think it's doing anything for the computer I don't think it is either it's actually costing resources",
    "start": "4743679",
    "end": "4750280"
  },
  {
    "text": "from the world Listen to you I mean I think you're right it it doesn't help the computer uh I say please and thank",
    "start": "4750280",
    "end": "4756960"
  },
  {
    "text": "you to the models now so that I remember to say please and thank you to humans that's that's it it's it's purely you",
    "start": "4756960",
    "end": "4762480"
  },
  {
    "text": "know I don't want to get into the habit of you're training yourself exactly it's all about training that's fair self trining yeah I don't feel like I have to",
    "start": "4762480",
    "end": "4768840"
  },
  {
    "text": "I feel like it's just a natural atomism how I do things how I operate yourself",
    "start": "4768840",
    "end": "4774000"
  },
  {
    "text": "it's it's who I am in my core I'm a kind person well David thanks so much for joining us man and sharing all your",
    "start": "4774000",
    "end": "4780120"
  },
  {
    "text": "knowledge you've learned a lot and I've learned a lot from you so we appreciate your time this was a ton of fun thanks for having me",
    "start": "4780120",
    "end": "4786560"
  },
  {
    "text": "[Music]",
    "start": "4786560",
    "end": "4828830"
  },
  {
    "text": "love",
    "start": "4829120",
    "end": "4832040"
  }
]