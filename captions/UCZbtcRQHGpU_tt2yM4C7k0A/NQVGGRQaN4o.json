[
  {
    "text": "but then there's this idea of misuse you can create a really flexible extremely",
    "start": "40",
    "end": "5160"
  },
  {
    "text": "powerful model which lets you do a whole variety of Downstream things much easier",
    "start": "5160",
    "end": "11320"
  },
  {
    "text": "so in other words you can do named entity recognition sentiment analysis create fake news or Target marginalized",
    "start": "11320",
    "end": "18960"
  },
  {
    "text": "groups on the one side we're creating these large language models that we would like to be more accessible to",
    "start": "18960",
    "end": "26160"
  },
  {
    "text": "people because we don't want the concentration of power around these Foundation models being with just a few",
    "start": "26160",
    "end": "32840"
  },
  {
    "text": "entities so I don't know I don't know what the right balance is",
    "start": "32840",
    "end": "38110"
  },
  {
    "text": "[Music]",
    "start": "38110",
    "end": "45079"
  },
  {
    "text": "there Hello friends Jared here to tell you about Chang log Plus+ our membership",
    "start": "45079",
    "end": "50320"
  },
  {
    "text": "program for those of you who want to directly support our work your Plus+ membership gets you closer to the metal",
    "start": "50320",
    "end": "56640"
  },
  {
    "text": "with extended episodes makes the ads disappear and and takes our audio to the next level with higher bit rate MP3s you",
    "start": "56640",
    "end": "64119"
  },
  {
    "text": "can join today at changel log.com [Music]",
    "start": "64119",
    "end": "78139"
  },
  {
    "text": "plusplus welcome to practical AI a weekly podcast making artificial intelligence practical productive and",
    "start": "78400",
    "end": "85920"
  },
  {
    "text": "accessible to everyone this is where conversations around AI machine learning and data science happen join us at",
    "start": "85920",
    "end": "93040"
  },
  {
    "text": "practical a. fm/ community and follow the show on Twitter we're at practical",
    "start": "93040",
    "end": "98720"
  },
  {
    "text": "AI FM thank you to our partners at fastly for shipping our pods super fast all around the world check them out at",
    "start": "98720",
    "end": "107010"
  },
  {
    "text": "[Music] fast.com well welcome to another fully",
    "start": "107010",
    "end": "114560"
  },
  {
    "text": "connected episode of the Practical AI podcast in these episodes Chris and I",
    "start": "114560",
    "end": "120280"
  },
  {
    "text": "keep you fully connected with everything that's happening in the AI Community we'll take some time to discuss some of",
    "start": "120280",
    "end": "126640"
  },
  {
    "text": "the latest AI news and then we'll dig into some learning resources to help you level up your machine learning game I'm",
    "start": "126640",
    "end": "133760"
  },
  {
    "text": "Daniel whack I'm a data scientist with s International and I'm joined as always",
    "start": "133760",
    "end": "139200"
  },
  {
    "text": "by my co-host Chris Benson who is a tech strategist at locked Martin how you doing Chris doing great Daniel how you",
    "start": "139200",
    "end": "145879"
  },
  {
    "text": "doing today doing pretty good how's the strategy world the after that our last",
    "start": "145879",
    "end": "151040"
  },
  {
    "text": "fully connected when we were talking about Innovation teams and stuff a lot of those ideas have been swirling around",
    "start": "151040",
    "end": "157280"
  },
  {
    "text": "in my mind and I just figure that's like what the life of a strategist like you're you're thinking about those",
    "start": "157280",
    "end": "162879"
  },
  {
    "text": "things all the time so been thinking about you over the past week as you you've been strategizing we pretend like",
    "start": "162879",
    "end": "169080"
  },
  {
    "text": "we know what's going to happen next it's a great gig if you can get it yeah well that's the thing isn't the saying like",
    "start": "169080",
    "end": "175200"
  },
  {
    "text": "if you try to predict the future that's like the one thing you can be sure you're going to get wrong yeah yeah yeah",
    "start": "175200",
    "end": "181120"
  },
  {
    "text": "I'm pretty okay being wrong too I say Yeah you sort of embraced that we did the strategy one and then I know we've",
    "start": "181120",
    "end": "187400"
  },
  {
    "text": "we've talked about data fabric recently and that's also been another interesting thing I'm still working on that pretty",
    "start": "187400",
    "end": "193239"
  },
  {
    "text": "intensely uh and some stuff that I'm doing and having a good time because you know what this this whole AI thing keeps",
    "start": "193239",
    "end": "199280"
  },
  {
    "text": "rolling forward yeah yeah definitely and there's something I wanted to bring up",
    "start": "199280",
    "end": "204640"
  },
  {
    "text": "in this episode and and sort of talk through with you it's not like a",
    "start": "204640",
    "end": "209879"
  },
  {
    "text": "extremely recent thing it was back in I think August that some of this came out",
    "start": "209879",
    "end": "215760"
  },
  {
    "text": "but we haven't talked about it yet and I realized we haven't talked about it yet and at the time there was a lot of",
    "start": "215760",
    "end": "221280"
  },
  {
    "text": "language floating around that I didn't really know like what people were meaning when they said this stuff you",
    "start": "221280",
    "end": "228400"
  },
  {
    "text": "know I've kind of circled back to some of that and kind of figured out what what we're talking about at least a",
    "start": "228400",
    "end": "234280"
  },
  {
    "text": "little bit and I'd love to talk through that with you and that's around the topic of foundation",
    "start": "234280",
    "end": "240439"
  },
  {
    "text": "models so this is something that was work done at Stanford University",
    "start": "240439",
    "end": "246840"
  },
  {
    "text": "actually in a center for research on Foundation models",
    "start": "246840",
    "end": "252079"
  },
  {
    "text": "crfm at Stanford University and they came out with a fairly lengthy and very",
    "start": "252079",
    "end": "258519"
  },
  {
    "text": "interesting report on this topic which they're calling Foundation models and I",
    "start": "258519",
    "end": "265120"
  },
  {
    "text": "remember at the time people were sort of like responding to I think mostly the",
    "start": "265120",
    "end": "272520"
  },
  {
    "text": "the name Foundation models like oh well what do you like maybe there's some",
    "start": "272520",
    "end": "278039"
  },
  {
    "text": "hubris around like saying hey we're we're working on Foundation models which are the foundation of of AI I think",
    "start": "278039",
    "end": "285520"
  },
  {
    "text": "clearly they've they've also released some articles around this or blog posts and I think that clearly wasn't you know",
    "start": "285520",
    "end": "292000"
  },
  {
    "text": "at least their intention but it's a super interesting article so the the article that they wrote is on the",
    "start": "292000",
    "end": "298800"
  },
  {
    "text": "opportunity and risks of foundation models first off I guess Chris have you come across this",
    "start": "298800",
    "end": "305560"
  },
  {
    "text": "term I have come across it and I too have found it confusing in the same way",
    "start": "305560",
    "end": "311880"
  },
  {
    "text": "and so this is an interesting article that you found here you want to give us a definition well I figure that being in",
    "start": "311880",
    "end": "319440"
  },
  {
    "text": "the good spirit of a programmer I would just rather copy and paste then create",
    "start": "319440",
    "end": "325199"
  },
  {
    "text": "my own so we'll go for the definition from Stanford",
    "start": "325199",
    "end": "330280"
  },
  {
    "text": "and that's I think a good thing to start with just sort of parsing through so they say we Define Foundation models as",
    "start": "330280",
    "end": "340240"
  },
  {
    "text": "models trained on Broad data in parenthesis generally using self",
    "start": "340240",
    "end": "346240"
  },
  {
    "text": "self-supervision at scale that can be adapted to a wide range of Downstream",
    "start": "346240",
    "end": "352759"
  },
  {
    "text": "tasks so let's sort of take a step back and parse through that I think most of our listeners are familiar with",
    "start": "352759",
    "end": "359360"
  },
  {
    "text": "something type of models um that jargon by this probably we're meaning mostly",
    "start": "359360",
    "end": "364560"
  },
  {
    "text": "like neural network type of models a modern model architecture but basically",
    "start": "364560",
    "end": "370160"
  },
  {
    "text": "some sort of parameterized function in code right that is trained based on some",
    "start": "370160",
    "end": "376360"
  },
  {
    "text": "data using training algorithm of some type an iterative training process and",
    "start": "376360",
    "end": "382360"
  },
  {
    "text": "so that's a model and it is they're saying Foundation models are trained on Broad data generally using",
    "start": "382360",
    "end": "391400"
  },
  {
    "text": "self-supervision at scale so some of those terms particularly",
    "start": "391400",
    "end": "396800"
  },
  {
    "text": "self-supervision might not be the most familiar to people have you have you run across this in your own work Chris yes",
    "start": "396800",
    "end": "403759"
  },
  {
    "text": "in terms of different levels of supervised learning and and the the fact that we are now more and more training",
    "start": "403759",
    "end": "410560"
  },
  {
    "text": "models that are either unsupervised or some level between that self-supervised",
    "start": "410560",
    "end": "416599"
  },
  {
    "text": "the thing that I am curious about here is how how should I think about this in the in terms of transfer learning is",
    "start": "416599",
    "end": "423639"
  },
  {
    "text": "this something that I would use as a base for transfer learning to build upon yeah I think that when they're doing",
    "start": "423639",
    "end": "430240"
  },
  {
    "text": "their report two things or the two kind of pillars on which Foundation models",
    "start": "430240",
    "end": "437400"
  },
  {
    "text": "that they're calling them are built if I'm remembering right they talk about",
    "start": "437400",
    "end": "442520"
  },
  {
    "text": "transfer learning and scale so for those that are maybe familiar to transfer",
    "start": "442520",
    "end": "448680"
  },
  {
    "text": "learning that's where you have some sort of pre-training that goes on and that's",
    "start": "448680",
    "end": "454039"
  },
  {
    "text": "on a task that's maybe related to the task that you want to eventually do but then you sort of transfer learn or",
    "start": "454039",
    "end": "462039"
  },
  {
    "text": "fine-tune that model for the actual Downstream task which you're interested in which is different from the one the",
    "start": "462039",
    "end": "468919"
  },
  {
    "text": "the pre-training task and so I think this gets it does get to that crisp because they're talking about a variety",
    "start": "468919",
    "end": "475720"
  },
  {
    "text": "or a wide range of Downstream tasks so this is the scenario where you had some",
    "start": "475720",
    "end": "481400"
  },
  {
    "text": "pre-training that happened on one of these Foundation models a large model",
    "start": "481400",
    "end": "487039"
  },
  {
    "text": "and then your transfer learning or maybe in a zero shot or few shot way that",
    "start": "487039",
    "end": "492960"
  },
  {
    "text": "model is able to perform a variety of Downstream tasks so it fit into that zone of like transfer learning zero shot",
    "start": "492960",
    "end": "500599"
  },
  {
    "text": "few shot type of scenarios so that's one side of it that pillar of transfer learning the other side is scale now the",
    "start": "500599",
    "end": "508240"
  },
  {
    "text": "interesting I think connection between scale and",
    "start": "508240",
    "end": "513360"
  },
  {
    "text": "self-supervision is that in the sort of I guess more traditional M supervised",
    "start": "513360",
    "end": "518959"
  },
  {
    "text": "machine learning world that we kind of started with you always had a data set",
    "start": "518959",
    "end": "526800"
  },
  {
    "text": "that was annotated or labeled for the task that you're interested in right and",
    "start": "526800",
    "end": "531880"
  },
  {
    "text": "then you trained your model on that data set the problem with that right is is",
    "start": "531880",
    "end": "537800"
  },
  {
    "text": "that that's hard to scale right it's hard to scale sort of across a wide variety of tasks because you have to",
    "start": "537800",
    "end": "544279"
  },
  {
    "text": "have a data set for each of those tasks and it's very expensive and timec consuming to create those labeled data",
    "start": "544279",
    "end": "550880"
  },
  {
    "text": "sets but also if you think of the size of even one of those data sets the more",
    "start": "550880",
    "end": "556200"
  },
  {
    "text": "data you need or the more complex complex task it is you might need to label a whole lot of data so for example",
    "start": "556200",
    "end": "562959"
  },
  {
    "text": "the question answering task in natural language processing I've learned because",
    "start": "562959",
    "end": "568200"
  },
  {
    "text": "I'm working with some of that data now it's really timec consuming to build those data sets or like conversational",
    "start": "568200",
    "end": "575600"
  },
  {
    "text": "AI data sets uh sometimes a lot a lot of very hard and difficult work goes into",
    "start": "575600",
    "end": "580839"
  },
  {
    "text": "making those so if you want to scale your data sets for training in that",
    "start": "580839",
    "end": "586399"
  },
  {
    "text": "sense self-supervision is actually really kind of another important feature",
    "start": "586399",
    "end": "592160"
  },
  {
    "text": "here of these models because let's say you had just a bunch of text just like blah bunch of text right well you can",
    "start": "592160",
    "end": "599640"
  },
  {
    "text": "actually in a self-supervised way create your own training data to train this",
    "start": "599640",
    "end": "607079"
  },
  {
    "text": "foundational model or this large language model by let's just take some sentences remove words and say the task",
    "start": "607079",
    "end": "615040"
  },
  {
    "text": "is tell me what word went here and since you are the one that took the word out",
    "start": "615040",
    "end": "620720"
  },
  {
    "text": "right you know what word should go there so you're self-supervised this",
    "start": "620720",
    "end": "625760"
  },
  {
    "text": "pre-training task which is much more scalable in terms of preparing the data",
    "start": "625760",
    "end": "631000"
  },
  {
    "text": "for training so I misused the term earlier I was conflating unsupervised with self-supervised pardon my confusion",
    "start": "631000",
    "end": "637399"
  },
  {
    "text": "there yeah no it's confusing terminology right there's this semi-supervised",
    "start": "637399",
    "end": "642839"
  },
  {
    "text": "self-supervised unsupervised supervis there's a lot of supervision going on of different types so yeah it's worth kind",
    "start": "642839",
    "end": "649839"
  },
  {
    "text": "of diving into those but yeah I think the idea is we're really talking about scale and you can't build large data",
    "start": "649839",
    "end": "656959"
  },
  {
    "text": "sets at least not in a cost efficient time efficient way if you're always",
    "start": "656959",
    "end": "662160"
  },
  {
    "text": "thinking just in terms of supervised learning yeah fair enough so how might you how might you think about",
    "start": "662160",
    "end": "668200"
  },
  {
    "text": "implementing this if you're out there and you're interested in moving into this as a new skill set that you want to",
    "start": "668200",
    "end": "674440"
  },
  {
    "text": "develop for your organization what would a good first step be yeah well I think that that's a really interesting",
    "start": "674440",
    "end": "681959"
  },
  {
    "text": "question in this in this work because actually one of the things that they",
    "start": "681959",
    "end": "687480"
  },
  {
    "text": "talk about cuz they're talking about opportunities and risks with Foundation models is that accessibility and",
    "start": "687480",
    "end": "695600"
  },
  {
    "text": "homogeneity are things that come up with Foundation models meaning that actually",
    "start": "695600",
    "end": "701320"
  },
  {
    "text": "me like my team working I might not be able to create one of these Foundation",
    "start": "701320",
    "end": "706480"
  },
  {
    "text": "models right because I don't have the resources so just to give a a kind of couple examples we're talking about here",
    "start": "706480",
    "end": "713200"
  },
  {
    "text": "things maybe that certain things that people might be familiar with are things",
    "start": "713200",
    "end": "718760"
  },
  {
    "text": "like bir or gpt3 in the language space but then there's other computer vision",
    "start": "718760",
    "end": "725399"
  },
  {
    "text": "models and other sort of modalities speech models uh like thinking of wave",
    "start": "725399",
    "end": "731240"
  },
  {
    "text": "Tove or other things so if you imagine like tasks in computer vision tasks in",
    "start": "731240",
    "end": "738399"
  },
  {
    "text": "natural language processing tasks in speech technology and you look at what people are doing now very often a bulk",
    "start": "738399",
    "end": "746320"
  },
  {
    "text": "of the things that people are doing are sort of homogeneous in the sense that they're",
    "start": "746320",
    "end": "751880"
  },
  {
    "text": "all built on one of these Foundation models whether it's bird or wave to vac or one in the GPT variety or you know",
    "start": "751880",
    "end": "760600"
  },
  {
    "text": "all of these are sort of built on one of these Foundation models like we fine-tune Bert to do this or we you know",
    "start": "760600",
    "end": "766959"
  },
  {
    "text": "transfer learned this Foundation model to do that and so this is one of the things that they bring up in the paper",
    "start": "766959",
    "end": "773120"
  },
  {
    "text": "as a risk is that there's sort of two things that come out from this right one",
    "start": "773120",
    "end": "779560"
  },
  {
    "text": "is not all researchers and practitioners can create their own foundational model",
    "start": "779560",
    "end": "785399"
  },
  {
    "text": "because I don't have like racks of gpus maybe or the computational resources the",
    "start": "785399",
    "end": "791519"
  },
  {
    "text": "manpower to actually create a model on the scale so that's one thing is like hey is it bad is that necessarily a bad",
    "start": "791519",
    "end": "799120"
  },
  {
    "text": "thing I think it is concerning that like this sort of concentration of people",
    "start": "799120",
    "end": "805279"
  },
  {
    "text": "that can actually create this state-of-the-art work but then the other side of that is like if there's an",
    "start": "805279",
    "end": "810800"
  },
  {
    "text": "unexpected Behavior or bias in that Foundation model then that actually",
    "start": "810800",
    "end": "815880"
  },
  {
    "text": "filters through down to a huge number of Downstream tasks and applications",
    "start": "815880",
    "end": "821440"
  },
  {
    "text": "because no one's retraining that Foundation model right it's just there and everybody's using it right and so",
    "start": "821440",
    "end": "827760"
  },
  {
    "text": "that's that's another risk so I think the implications are really what should be on the mind of practitioners in terms",
    "start": "827760",
    "end": "834160"
  },
  {
    "text": "of this trend maybe you know what that reminds me of just as an analogy as you were saying that is if we look at the",
    "start": "834160",
    "end": "841000"
  },
  {
    "text": "software World in parallel there are some software bits out there that you find in open source that are in just",
    "start": "841000",
    "end": "847959"
  },
  {
    "text": "about everything you know and they flow Downstream and if you get a bug you know thinking about the bias in the model or",
    "start": "847959",
    "end": "854279"
  },
  {
    "text": "something but if you get a bug in the software something unintended it can have fairly substantial consequences as",
    "start": "854279",
    "end": "860880"
  },
  {
    "text": "it flows Downstream and I is that the right way of thinking about this is that since you're relying on this so much",
    "start": "860880",
    "end": "866959"
  },
  {
    "text": "that if you do go ay it's going to to multiply itself many times yeah definitely I think that that is exactly",
    "start": "866959",
    "end": "873800"
  },
  {
    "text": "the it's not an exact parallel because we're thinking of like mostly issues",
    "start": "873800",
    "end": "879120"
  },
  {
    "text": "maybe stemming from data and bias and data Maybe or something like that but it is parallel in the sense that like hey",
    "start": "879120",
    "end": "885759"
  },
  {
    "text": "if everybody Imports this one package and it and it breaks Upstream then",
    "start": "885759",
    "end": "891000"
  },
  {
    "text": "there's a huge consequence Downstream right and the parallel here would be everybody's using let's say Bert to",
    "start": "891000",
    "end": "898279"
  },
  {
    "text": "create all of the these different NLP applications well what happens if",
    "start": "898279",
    "end": "903360"
  },
  {
    "text": "there's this bias in Bert and we've seen examples of failur of these large models",
    "start": "903360",
    "end": "909480"
  },
  {
    "text": "right and so yeah I think that is a that is a good parallel so Chris I think maybe one of the things that I remember",
    "start": "909480",
    "end": "916399"
  },
  {
    "text": "when when this came out is that people were kind of thrown off by the",
    "start": "916399",
    "end": "922880"
  },
  {
    "text": "terminology I think even in the blog post that I was reading which I'll link in our show notes they say the name",
    "start": "922880",
    "end": "930160"
  },
  {
    "text": "Foundation model has also drawn significant attention and given that",
    "start": "930160",
    "end": "935680"
  },
  {
    "text": "they want to clarify it so there's a whole clarification in this article I'll link in the show notes they threw me off",
    "start": "935680",
    "end": "941360"
  },
  {
    "text": "at the beginning of the show yeah I mean naming is difficult right it is so like what is that like naming naming V",
    "start": "941360",
    "end": "947720"
  },
  {
    "text": "variables is the most difficult part of software engineering in the software world there's nothing harder than naming",
    "start": "947720",
    "end": "953800"
  },
  {
    "text": "variables well yeah so I do feel for them and I think that they were making a",
    "start": "953800",
    "end": "959319"
  },
  {
    "text": "a good attempt at this and I've I've actually felt this tension maybe in our",
    "start": "959319",
    "end": "965480"
  },
  {
    "text": "previous conversations because you know I come from the NLP world and we have this like term large language model",
    "start": "965480",
    "end": "972839"
  },
  {
    "text": "right which is essentially what they're calling a foundation model for our space",
    "start": "972839",
    "end": "978399"
  },
  {
    "text": "right but the thought process around that type of model and the way it's used Downstream the way it's trained that is",
    "start": "978399",
    "end": "986160"
  },
  {
    "text": "pervasive across other modalities right in computer vision in audio even in",
    "start": "986160",
    "end": "993040"
  },
  {
    "text": "areas where people are doing like biological structures and and other things this idea of having a large base",
    "start": "993040",
    "end": "1002040"
  },
  {
    "text": "model that's trained in a self-supervised way at scale and and used in a variety of Downstream tasks",
    "start": "1002040",
    "end": "1008639"
  },
  {
    "text": "that idea is is kind of pervasive so I I do like the fact that like in the past I",
    "start": "1008639",
    "end": "1014360"
  },
  {
    "text": "felt this tension even when I've taught classes like what do I call this like thing we're doing this this like",
    "start": "1014360",
    "end": "1022079"
  },
  {
    "text": "new trend in how we're operating and I think you could talk about it um under the heading of transfer learning Under",
    "start": "1022079",
    "end": "1028400"
  },
  {
    "text": "The Heading of like few shot zero shot or self-supervision but I I do appreciate",
    "start": "1028400",
    "end": "1034438"
  },
  {
    "text": "the attempt to create a term that sort of encompasses all of these things that",
    "start": "1034439",
    "end": "1040558"
  },
  {
    "text": "are at play because there's definitely this this trend so I don't know if you have any thoughts on that I'm not",
    "start": "1040559",
    "end": "1045798"
  },
  {
    "text": "opposed to the name actually I I kind of like having a name to refer to this as but I don't know I don't know if it's",
    "start": "1045799",
    "end": "1052320"
  },
  {
    "text": "catching on I don't hear like tons of people using it out there so maybe it's not catching on but I I kind of wish",
    "start": "1052320",
    "end": "1059360"
  },
  {
    "text": "that it or something like it would right I had heard the term but just like",
    "start": "1059360",
    "end": "1064400"
  },
  {
    "text": "self-supervised learning I don't think I had really gotten the correct meaning right on it's interesting is in the",
    "start": "1064400",
    "end": "1070320"
  },
  {
    "text": "article here they they kind of talk about this trend as you pointed out and where it's going and whether what the",
    "start": "1070320",
    "end": "1076679"
  },
  {
    "text": "implications are and they they talk to that a little bit lower in the article and they specifically kind of address",
    "start": "1076679",
    "end": "1084039"
  },
  {
    "text": "the pace of technological process the entrenchment of the models themselves because of you know some of the",
    "start": "1084039",
    "end": "1090200"
  },
  {
    "text": "limitations that you pointed out earlier and the demand of kind of this the Human",
    "start": "1090200",
    "end": "1095320"
  },
  {
    "text": "Social side of it and technology and it raises a good point I mean they Empower",
    "start": "1095320",
    "end": "1100799"
  },
  {
    "text": "us to be able to use these things because it gives us a capability we might not otherwise have but there's",
    "start": "1100799",
    "end": "1107039"
  },
  {
    "text": "definitely some pitfalls there you know especially in terms of of you know biasing the data that you pointed",
    "start": "1107039",
    "end": "1113080"
  },
  {
    "text": "out where do you think I mean as someone who is using large models on a regular basis yourself and in the work do you",
    "start": "1113080",
    "end": "1120400"
  },
  {
    "text": "think that this is probably where things will continue going for some time or do you see any alternative I think it's",
    "start": "1120400",
    "end": "1125679"
  },
  {
    "text": "kind of a necessary evil to have foundational models you know by that name or otherwise to build work upon you",
    "start": "1125679",
    "end": "1132919"
  },
  {
    "text": "know shoulders of giants kind of idea any thoughts there or whether that's are the risks too great yeah I think that as",
    "start": "1132919",
    "end": "1140760"
  },
  {
    "text": "you mentioned in my own work like we don't train for the most part we're",
    "start": "1140760",
    "end": "1146039"
  },
  {
    "text": "training a lot of Downstream task models and few maybe what would be considered",
    "start": "1146039",
    "end": "1152200"
  },
  {
    "text": "foundational models or in a this sort of self-supervised pre-training way",
    "start": "1152200",
    "end": "1157600"
  },
  {
    "text": "although we have done it a bit so I've benefited a lot in my own work from from",
    "start": "1157600",
    "end": "1163120"
  },
  {
    "text": "this trend and I think it does come with risks though I think when I was going",
    "start": "1163120",
    "end": "1168760"
  },
  {
    "text": "going through this article and thinking things through a lot of it centered around sort of concentration of power",
    "start": "1168760",
    "end": "1176400"
  },
  {
    "text": "and other kind of Trends in the AI World especially because I work at a nonprofit",
    "start": "1176400",
    "end": "1182480"
  },
  {
    "text": "that works with minority language communities for the most part local languages minority languages are left",
    "start": "1182480",
    "end": "1189400"
  },
  {
    "text": "out of foundational models in the NLP world right so I I already view this as",
    "start": "1189400",
    "end": "1194480"
  },
  {
    "text": "an issue and they bring this up in terms of they talk about you know supporting",
    "start": "1194480",
    "end": "1200480"
  },
  {
    "text": "diverse research and I think regardless of the modality that we're working with",
    "start": "1200480",
    "end": "1207760"
  },
  {
    "text": "it's important that we consider whether it's Geographic diversity language diversity but also diversity of those",
    "start": "1207760",
    "end": "1215120"
  },
  {
    "text": "creating the data sets and training the models and having representation all of",
    "start": "1215120",
    "end": "1220400"
  },
  {
    "text": "those things I think are both are really critical if this trend continues and we don't think about that side of things",
    "start": "1220400",
    "end": "1227919"
  },
  {
    "text": "then those those sort of marginalized groups in terms of how they're able to",
    "start": "1227919",
    "end": "1234159"
  },
  {
    "text": "use these foundational models and the implications of the applications that are produced they're only going to become more marginalized because if",
    "start": "1234159",
    "end": "1241320"
  },
  {
    "text": "foundational models are a key piece or I keep calling them foundational Foundation models are a key piece of",
    "start": "1241320",
    "end": "1249400"
  },
  {
    "text": "this sort of new tech stack that we're building for the digital sphere and those groups are just left out or",
    "start": "1249400",
    "end": "1256679"
  },
  {
    "text": "there's bias in the models against those groups in various ways then that's a real problem in terms of their own sort",
    "start": "1256679",
    "end": "1263240"
  },
  {
    "text": "of flourishing in the digital sphere I think that's a fantastic Insight that you just made there and that is that you",
    "start": "1263240",
    "end": "1270039"
  },
  {
    "text": "know from an economic standpoint people who make these large models these Foundation models are incented to solve",
    "start": "1270039",
    "end": "1277720"
  },
  {
    "text": "the problems which are probably being done by kind of what is current mainstream you know kind of current top",
    "start": "1277720",
    "end": "1284480"
  },
  {
    "text": "of the of the power stack if you will you know in terms of companies and countries even and so there is an",
    "start": "1284480",
    "end": "1291440"
  },
  {
    "text": "incentive to unintentionally perpetuate inequality with these and so it's a real",
    "start": "1291440",
    "end": "1298320"
  },
  {
    "text": "I I think that would be a real dilemma how might you tackle that going forward",
    "start": "1298320",
    "end": "1303440"
  },
  {
    "text": "to where if if we're looking at Foundation models as being somewhat core",
    "start": "1303440",
    "end": "1308880"
  },
  {
    "text": "to most workflows in the AI space because they're available and they accelerate where you're trying to go but",
    "start": "1308880",
    "end": "1316240"
  },
  {
    "text": "that they are built on imperfect data that has bias that that leaves out",
    "start": "1316240",
    "end": "1321679"
  },
  {
    "text": "marginalized groups do you just iterate and add those in and redo that there's a certain investment obviously to be made",
    "start": "1321679",
    "end": "1328799"
  },
  {
    "text": "in that do you have any thoughts on least evil path forward to get the best",
    "start": "1328799",
    "end": "1334200"
  },
  {
    "text": "foundation model with the least amount of unintended consequence over time yeah",
    "start": "1334200",
    "end": "1339760"
  },
  {
    "text": "it's a difficult question my sense is that that is why Stanford created this",
    "start": "1339760",
    "end": "1345799"
  },
  {
    "text": "Center although I wasn't I haven't we would love have someone from the center on the podcast but I haven't talked in",
    "start": "1345799",
    "end": "1351679"
  },
  {
    "text": "detail about the motivation to them so I'm not putting words in their mouth but I think there's multiple sides to this I",
    "start": "1351679",
    "end": "1358840"
  },
  {
    "text": "think one side of it is having you know think tanks or research groups that are",
    "start": "1358840",
    "end": "1364159"
  },
  {
    "text": "really thinking about the problem and how Foundation models large language",
    "start": "1364159",
    "end": "1370039"
  },
  {
    "text": "models you know computer vision models and and pre other pre-trained models are",
    "start": "1370039",
    "end": "1376400"
  },
  {
    "text": "influencing Society more generally and the risks the the opportunities with",
    "start": "1376400",
    "end": "1381600"
  },
  {
    "text": "that because there's clear opportunities but there's also there's also risks so having that research side of things I",
    "start": "1381600",
    "end": "1388360"
  },
  {
    "text": "think is a key piece but also I think as practitioners this needs to be part of",
    "start": "1388360",
    "end": "1395200"
  },
  {
    "text": "our thought process when we're using these models and I mentioned this fairly",
    "start": "1395200",
    "end": "1401279"
  },
  {
    "text": "frequently on the podcast but the idea of kind of probing the mo the behavior",
    "start": "1401279",
    "end": "1406799"
  },
  {
    "text": "of the models that you're releasing in into production I think is a key piece of this so not just trusting that like",
    "start": "1406799",
    "end": "1414080"
  },
  {
    "text": "Hey Google or open AI created this great model I'm going to fine-tune it on this",
    "start": "1414080",
    "end": "1419440"
  },
  {
    "text": "task okay it does great on my test data set so you know push it up to production",
    "start": "1419440",
    "end": "1425559"
  },
  {
    "text": "I think thinking of Behavioral testing where you actually probe how does my",
    "start": "1425559",
    "end": "1430919"
  },
  {
    "text": "model respond to these changes in my data what if I perturb my data in this way what if I switch out this for that",
    "start": "1430919",
    "end": "1438640"
  },
  {
    "text": "what if I make this change which should be invariant in the output is it actually invariant in the output and",
    "start": "1438640",
    "end": "1444960"
  },
  {
    "text": "creating that sort of test Suite of Behavioral tests for your model I think",
    "start": "1444960",
    "end": "1450080"
  },
  {
    "text": "is is a big thing that practitioners can keep in mind in terms of how these foundational models might impact their",
    "start": "1450080",
    "end": "1458440"
  },
  {
    "text": "Downstream customers the report actually does a they talk about the social impact",
    "start": "1458440",
    "end": "1466240"
  },
  {
    "text": "but they have this mindset there's a figure three in the paper their figures are really nice um I wish I could make",
    "start": "1466240",
    "end": "1472919"
  },
  {
    "text": "figures that nice they talk about these sort of almost like a value chain or",
    "start": "1472919",
    "end": "1478600"
  },
  {
    "text": "something of usage of these Foundation models starting from data creation through data curation through training",
    "start": "1478600",
    "end": "1485520"
  },
  {
    "text": "and application and deployment and they think about the impacts of those models",
    "start": "1485520",
    "end": "1492000"
  },
  {
    "text": "along that chain so like there are real people on either side of that chain",
    "start": "1492000",
    "end": "1498159"
  },
  {
    "text": "right right what is the diversity that's represented in those groups and what might be the impact of having biases in",
    "start": "1498159",
    "end": "1507080"
  },
  {
    "text": "this sort of data producers or the consumer side the makeup of the",
    "start": "1507080",
    "end": "1512360"
  },
  {
    "text": "consumers so I think that's a really good way of thinking about it and having that more holistic picture in mind is is",
    "start": "1512360",
    "end": "1519200"
  },
  {
    "text": "helpful yeah you know as as you were going through that it struck me that that in addition to kind of accounting",
    "start": "1519200",
    "end": "1527200"
  },
  {
    "text": "for the unintended consequences behaviorally as you perturb data and stuff that the flip side of that coin",
    "start": "1527200",
    "end": "1533880"
  },
  {
    "text": "becomes vulnerabilities from a security standpoint in terms of of affecting the",
    "start": "1533880",
    "end": "1539080"
  },
  {
    "text": "outcomes of of foundation models or or any model for that matter by doing that",
    "start": "1539080",
    "end": "1544360"
  },
  {
    "text": "and so it do just just has a greater multiplier you know associated with the foundation since you're building so much",
    "start": "1544360",
    "end": "1550600"
  },
  {
    "text": "on top of that so as we look at this in the days ahead it will be interesting to see as well uh what the security",
    "start": "1550600",
    "end": "1558760"
  },
  {
    "text": "implications are of protecting the end users that are impacted by this meaning you know the the model will change the",
    "start": "1558760",
    "end": "1566480"
  },
  {
    "text": "behavior of lots of different you know executions on that where people are are using it for various use cases that",
    "start": "1566480",
    "end": "1573279"
  },
  {
    "text": "affect people I would conclude by noting that in all of this that we're talking about it's starting with human beings",
    "start": "1573279",
    "end": "1578840"
  },
  {
    "text": "and it's ending with human beings and we are a little bit at the mercy of the models as we're trying to make",
    "start": "1578840",
    "end": "1584960"
  },
  {
    "text": "improvements in other ways in our lives to to do things productive in the world it's a curious set of problems that we",
    "start": "1584960",
    "end": "1590399"
  },
  {
    "text": "Face going forward okay Chris so in continuing our way through this material",
    "start": "1590399",
    "end": "1596520"
  },
  {
    "text": "that that Stanford has put out they actually do have I think your point made",
    "start": "1596520",
    "end": "1602679"
  },
  {
    "text": "around like people on either side of this they talk about emphasizing the role of people so I think they're",
    "start": "1602679",
    "end": "1608240"
  },
  {
    "text": "they're very much with you in that they also use this terminology in terms of",
    "start": "1608240",
    "end": "1613799"
  },
  {
    "text": "how to mitigate some of this risk and vulnerability stuff they talk about how",
    "start": "1613799",
    "end": "1619120"
  },
  {
    "text": "Foundation models can and increasingly should be grounded now that was also a",
    "start": "1619120",
    "end": "1627279"
  },
  {
    "text": "term that I wanted to make sure that I sort of dug into a little bit because",
    "start": "1627279",
    "end": "1633320"
  },
  {
    "text": "maybe it could mean a variety of things is that terminology you've you've come across no I I ran across that in the",
    "start": "1633320",
    "end": "1640399"
  },
  {
    "text": "article a moment ago just before you said it and I was wondering and then you went there I was wondering what do they",
    "start": "1640399",
    "end": "1646159"
  },
  {
    "text": "mean by grounded so what do they mean by grounded Daniel so it is interesting",
    "start": "1646159",
    "end": "1651840"
  },
  {
    "text": "that there's sort of a couple of ways that you could come at this and and I found people like talking about this",
    "start": "1651840",
    "end": "1659120"
  },
  {
    "text": "idea of grounded models coming from the vision perspective but also coming from",
    "start": "1659120",
    "end": "1664320"
  },
  {
    "text": "the NLP perspective both I think you you can kind of get to the same idea from",
    "start": "1664320",
    "end": "1670159"
  },
  {
    "text": "from both directions if you're coming from the computer vision side you might want to look up up some uh some things",
    "start": "1670159",
    "end": "1676720"
  },
  {
    "text": "about clip which is is a model that's being talked about or a methodology that's being talked about quite a bit",
    "start": "1676720",
    "end": "1683559"
  },
  {
    "text": "but from the language side the interesting thing about language is that",
    "start": "1683559",
    "end": "1691399"
  },
  {
    "text": "language is sort of a way to encode knowledge about the real world in some",
    "start": "1691399",
    "end": "1698200"
  },
  {
    "text": "sort of concrete way right like even the words that I'm talking about right now I",
    "start": "1698200",
    "end": "1703840"
  },
  {
    "text": "have thoughts and ideas in my mind I have perceptions of the real world and I'm encoding them in a in a concrete set",
    "start": "1703840",
    "end": "1712080"
  },
  {
    "text": "of symbol representations right and so there's a set of symbols that aren't",
    "start": "1712080",
    "end": "1718320"
  },
  {
    "text": "really like like the set of symbols d g",
    "start": "1718320",
    "end": "1723640"
  },
  {
    "text": "you know conjures up ideas in my mind about what that those that set of",
    "start": "1723640",
    "end": "1729799"
  },
  {
    "text": "symbols mean and I know it does for you as well having 10 or more of them at any given time but those symbols are",
    "start": "1729799",
    "end": "1737159"
  },
  {
    "text": "representative of things that are grounded in the real world right the it's just a sort of symbolic",
    "start": "1737159",
    "end": "1742679"
  },
  {
    "text": "representation of things that are grounded in the real world and so this idea of grounding is thinking about more",
    "start": "1742679",
    "end": "1749240"
  },
  {
    "text": "widely like well instead of you know always just having a label data set that",
    "start": "1749240",
    "end": "1756039"
  },
  {
    "text": "says like oh here's dog here's dog in my text and that's an entity I'm going to",
    "start": "1756039",
    "end": "1762960"
  },
  {
    "text": "label it as an entity of some type and you know do named entity recognition or something well thinking about a language",
    "start": "1762960",
    "end": "1770960"
  },
  {
    "text": "model being grounded is a much broader way to look at this where you realize oh",
    "start": "1770960",
    "end": "1777640"
  },
  {
    "text": "well that has other representations that are actually grounded in kind of multiple modalities of how people",
    "start": "1777640",
    "end": "1784240"
  },
  {
    "text": "experience dog and language is just one of those right or or textual language is",
    "start": "1784240",
    "end": "1790919"
  },
  {
    "text": "just one of those there's also a way to say dog right in audio and there's all",
    "start": "1790919",
    "end": "1796799"
  },
  {
    "text": "sorts of of course the internet is filled with images and videos of dogs",
    "start": "1796799",
    "end": "1802399"
  },
  {
    "text": "and there's drawings of dogs and so there's not a single sort of element",
    "start": "1802399",
    "end": "1808320"
  },
  {
    "text": "that's represented of dog in any modality right you could you could do the same from Vision right if you have a",
    "start": "1808320",
    "end": "1813799"
  },
  {
    "text": "single picture of dog in your data set your model is going to assume that's the only way you can represent dog right and",
    "start": "1813799",
    "end": "1820399"
  },
  {
    "text": "so when you see a different type of dog it might cause your model to have a vulnerability because it's not used to",
    "start": "1820399",
    "end": "1828039"
  },
  {
    "text": "classifying a Pug as a dog when it all it's seen is a golden retriever right",
    "start": "1828039",
    "end": "1833960"
  },
  {
    "text": "and so the idea of grounding and you know if anyone wants to correct me our listeners can correct me but it really",
    "start": "1833960",
    "end": "1840000"
  },
  {
    "text": "has to do with pulling together these modalities of data to ground our model",
    "start": "1840000",
    "end": "1846039"
  },
  {
    "text": "in more than just a sort of single symbolic representation of of an idea",
    "start": "1846039",
    "end": "1852440"
  },
  {
    "text": "but building multimodal models that can ground our ideas or our perception of these ideas",
    "start": "1852440",
    "end": "1861559"
  },
  {
    "text": "in these sort of multiple modalities to increase robustness and flexibility",
    "start": "1861559",
    "end": "1866720"
  },
  {
    "text": "especially if we're using these on a variety of Downstream tasks I feel like I said a lot there I don't know if any",
    "start": "1866720",
    "end": "1872480"
  },
  {
    "text": "of that came true if I'm understanding you correctly correct me if I'm not but you're basically saying having that",
    "start": "1872480",
    "end": "1878320"
  },
  {
    "text": "diversity to achieve what the model is representing so the representation is",
    "start": "1878320",
    "end": "1883559"
  },
  {
    "text": "built on diversity gives it that robustness it gives it that ability to to recognize all the things that that",
    "start": "1883559",
    "end": "1890760"
  },
  {
    "text": "representation might take the form of is that a good way of summarizing yeah yeah so I looked up a few things here in",
    "start": "1890760",
    "end": "1898440"
  },
  {
    "text": "terms of this idea of grounded I like the description from actually from DARPA",
    "start": "1898440",
    "end": "1904159"
  },
  {
    "text": "which um you know I know you're you're a fan of but uh they talk about the explosive growth of language models and",
    "start": "1904159",
    "end": "1911399"
  },
  {
    "text": "all this stuff and they say you know ml suffers from a need to train on large",
    "start": "1911399",
    "end": "1916840"
  },
  {
    "text": "amounts of annotated data and then they talk about this sort of project that",
    "start": "1916840",
    "end": "1923519"
  },
  {
    "text": "they're talking about which is related to grounding aims to enable computers to acquire language in a manner similar to",
    "start": "1923519",
    "end": "1930720"
  },
  {
    "text": "how children do because children acquire language based on their perception of oral and visual information not just",
    "start": "1930720",
    "end": "1938399"
  },
  {
    "text": "textual information it's not just like you teach a child to read based on just",
    "start": "1938399",
    "end": "1943639"
  },
  {
    "text": "text apart from audio and visual so it's a sort of broader way of thinking about",
    "start": "1943639",
    "end": "1949880"
  },
  {
    "text": "how you teach a certain learning how we teach our models the information that",
    "start": "1949880",
    "end": "1956320"
  },
  {
    "text": "they need to know which is actually encoded in multiple modalities and it might not always be like audio and video",
    "start": "1956320",
    "end": "1964519"
  },
  {
    "text": "maybe it's related to like physics about how the world works or something like",
    "start": "1964519",
    "end": "1969799"
  },
  {
    "text": "that understood I I just want to note as an aside that I prefer your definition of grounding far more than the one I",
    "start": "1969799",
    "end": "1976880"
  },
  {
    "text": "usually get which is me on the weekends it's my wife telling me no you're not going to go fly that Cessna you're going",
    "start": "1976880",
    "end": "1982919"
  },
  {
    "text": "to do yard work instead you you sir are grounded that's what I usually hear so",
    "start": "1982919",
    "end": "1988480"
  },
  {
    "text": "yours is a much better definition in my book yeah yeah and you can think of how",
    "start": "1988480",
    "end": "1994240"
  },
  {
    "text": "this would create or could create vulnerabilities right like we based on our perception of the world we know that",
    "start": "1994240",
    "end": "2001039"
  },
  {
    "text": "like a bird belongs in a tree right but then if I put like a car in a tree we",
    "start": "2001039",
    "end": "2009039"
  },
  {
    "text": "have the sort of common sense to reason that like based on all our experience and all of our modalities of existence",
    "start": "2009039",
    "end": "2016120"
  },
  {
    "text": "um we know that that's odd and that's that's not quite right but that's that's not what how machine learning and AI",
    "start": "2016120",
    "end": "2023240"
  },
  {
    "text": "systems typically work they would just know hey there's a tree here and there's a car here yeah and you know that can",
    "start": "2023240",
    "end": "2029360"
  },
  {
    "text": "create an issue if it's not a wiing willow it should not have a car in right that is definitely true okay well",
    "start": "2029360",
    "end": "2037559"
  },
  {
    "text": "I really interested in this idea of grounding I think I don't know if you remember back when we talked with Jeff",
    "start": "2037559",
    "end": "2044240"
  },
  {
    "text": "Adams who was the he was on the team or helped lead the team that created a lot",
    "start": "2044240",
    "end": "2050000"
  },
  {
    "text": "of speech technology at Amazon related to Alexa and other things so he talked",
    "start": "2050000",
    "end": "2057240"
  },
  {
    "text": "about what he was really excited by in the future was thinking more",
    "start": "2057240",
    "end": "2062679"
  },
  {
    "text": "holistically about language that that's particularly what he's interested but when he he said that what he kind of",
    "start": "2062679",
    "end": "2069200"
  },
  {
    "text": "expressed was that we experience language in all sorts of modalities so",
    "start": "2069200",
    "end": "2074280"
  },
  {
    "text": "language models and speech technology moving forward is going to be very",
    "start": "2074280",
    "end": "2079960"
  },
  {
    "text": "multimodal was kind of how he was thinking about that and that was something really exciting to him so yeah",
    "start": "2079960",
    "end": "2085919"
  },
  {
    "text": "I think this is something that I'm really excited about and yeah this this idea of grounding is is really",
    "start": "2085919",
    "end": "2092000"
  },
  {
    "text": "interesting it is it'll be interesting to see how it develops I know that in the past as we've hit in interesting",
    "start": "2092000",
    "end": "2097720"
  },
  {
    "text": "ideas we tend to have multiple shows then include them in the discussion so I suspect that grounding is is one of the",
    "start": "2097720",
    "end": "2103920"
  },
  {
    "text": "new things and probably Foundation is another one though I might ask our our larger Community please be better at",
    "start": "2103920",
    "end": "2111119"
  },
  {
    "text": "naming because uh these are we we've spent a good bit of the show defining some confusing terms as you come into it",
    "start": "2111119",
    "end": "2117599"
  },
  {
    "text": "until you get used to it yeah so I would say you and I are only just even touching on this idea of foundation",
    "start": "2117599",
    "end": "2125160"
  },
  {
    "text": "models and the other implications of the just to give listeners a a sense of this",
    "start": "2125160",
    "end": "2131359"
  },
  {
    "text": "there's other things that are discussed in the work from Stanford their report including what I thought was a really",
    "start": "2131359",
    "end": "2138240"
  },
  {
    "text": "interesting one around misuse so we talked about kind of the vulnerability",
    "start": "2138240",
    "end": "2143800"
  },
  {
    "text": "of these Foundation models one being that there's sort of a single point of failure and then we've talked about this",
    "start": "2143800",
    "end": "2151280"
  },
  {
    "text": "idea of grounding where maybe these Foundation models don't always have the common sense that they should or aren't",
    "start": "2151280",
    "end": "2157319"
  },
  {
    "text": "ground founded in multiple modalities of how we experience the world so that's one but then there's this idea of misuse",
    "start": "2157319",
    "end": "2164599"
  },
  {
    "text": "which means that and I thought maybe you would find this one intriguing Chris particularly that the idea that you can",
    "start": "2164599",
    "end": "2171560"
  },
  {
    "text": "create a really flexible extremely powerful model which lets you do",
    "start": "2171560",
    "end": "2177400"
  },
  {
    "text": "Downstream things a whole variety of Downstream things much easier means that",
    "start": "2177400",
    "end": "2183000"
  },
  {
    "text": "you can do a whole variety of Downstream things much easier right so in other words you can do named entity",
    "start": "2183000",
    "end": "2189359"
  },
  {
    "text": "recognition sentiment analysis other things but you can also do other things",
    "start": "2189359",
    "end": "2194400"
  },
  {
    "text": "like create fake news or Target marginalized groups or you know whatever",
    "start": "2194400",
    "end": "2199680"
  },
  {
    "text": "that you can do those things much easier too so you know I think that's a that's",
    "start": "2199680",
    "end": "2204800"
  },
  {
    "text": "a difficult one like on the one side we're creating these large language models that we would like to be more",
    "start": "2204800",
    "end": "2211880"
  },
  {
    "text": "accessible to people because we don't want them sort of the concentration of power around these Foundation models",
    "start": "2211880",
    "end": "2218880"
  },
  {
    "text": "being with just a few kind of entities but the wider you distribute these maybe the more possibility there is for misuse",
    "start": "2218880",
    "end": "2226400"
  },
  {
    "text": "in the community which is I think why we saw open AI kind of restrict usage of",
    "start": "2226400",
    "end": "2231800"
  },
  {
    "text": "gpt3 for for so long so I don't know I don't know what the right balance is",
    "start": "2231800",
    "end": "2237040"
  },
  {
    "text": "there or if there is a right balance it's interesting I think that you you know when we while back you know way",
    "start": "2237040",
    "end": "2242960"
  },
  {
    "text": "back early in the show they were really only data scientists and then as as this industry is Ed and we've gotten many",
    "start": "2242960",
    "end": "2248720"
  },
  {
    "text": "different roles with different names we got deep learning Engineers we got all sorts of different names now I think",
    "start": "2248720",
    "end": "2254520"
  },
  {
    "text": "that you just pointed out that there's yet another one there that has come into being and that is you know like the",
    "start": "2254520",
    "end": "2259760"
  },
  {
    "text": "found Foundation model security analyst you know who is a person charged with",
    "start": "2259760",
    "end": "2265119"
  },
  {
    "text": "evaluating Downstream intended consequences by a by a malicious actor",
    "start": "2265119",
    "end": "2271920"
  },
  {
    "text": "so there's probably a whole cyber security industry that you just coined just now Daniel I'll I'll take credit",
    "start": "2271920",
    "end": "2278599"
  },
  {
    "text": "for it when there you go it's yours it's it's a resumable thing my friend yeah it's like coming up with like voice",
    "start": "2278599",
    "end": "2285200"
  },
  {
    "text": "first or I don't know yeah some of those other things yeah they'll call you Daniel father of dot dot dot father of",
    "start": "2285200",
    "end": "2293280"
  },
  {
    "text": "the foundation model security analyst that's right I'll take that so as we",
    "start": "2293280",
    "end": "2298480"
  },
  {
    "text": "finish up here just to tie in a Learning Resource to this whole discussion of foundation models I don't really know a",
    "start": "2298480",
    "end": "2305960"
  },
  {
    "text": "a better one than this but I have mentioned it before and it keeps you know growing and I think becoming better",
    "start": "2305960",
    "end": "2313160"
  },
  {
    "text": "is the hugging face course so if you just look up hugging face course and of course we'll mention it on our show",
    "start": "2313160",
    "end": "2319240"
  },
  {
    "text": "notes hugging face maybe people think it's just natural language processing but it has grown far beyond that and",
    "start": "2319240",
    "end": "2326400"
  },
  {
    "text": "really like if you want to get a sense of like what is it like to use Foundation models and import them and",
    "start": "2326400",
    "end": "2333200"
  },
  {
    "text": "find tune for a downstream task find tune a pre-trained model or use these",
    "start": "2333200",
    "end": "2338880"
  },
  {
    "text": "big data sets or portions of these data sets that are being used in Foundation models hugging face and the model Hub",
    "start": "2338880",
    "end": "2347040"
  },
  {
    "text": "the data set Hub and their you know training API is really the best way to",
    "start": "2347040",
    "end": "2353119"
  },
  {
    "text": "to start into that topic at least in my opinion and they have their own course that includ videos and you know just",
    "start": "2353119",
    "end": "2359599"
  },
  {
    "text": "looking at the thing here they have already on the leftand side they talk",
    "start": "2359599",
    "end": "2365119"
  },
  {
    "text": "about you know given introduction they talk about Transformers which is their",
    "start": "2365119",
    "end": "2370240"
  },
  {
    "text": "library but you can access all sorts of model architectures and then they talk about fine-tuning a pre-trained model",
    "start": "2370240",
    "end": "2376520"
  },
  {
    "text": "sharing models and tokenizers these are all sort of themes that we've talked about in this in this episode so indeed",
    "start": "2376520",
    "end": "2383520"
  },
  {
    "text": "definitely take take a look I feel the need to tell people that you you're not paid to do that because you've offered",
    "start": "2383520",
    "end": "2388960"
  },
  {
    "text": "them up as a fantastic Learning Resource which they are uh a number of times so if our listeners haven't gone there they",
    "start": "2388960",
    "end": "2395400"
  },
  {
    "text": "really really should they really should yeah and they're uh they continue to just boost our own work at s so yeah I",
    "start": "2395400",
    "end": "2404000"
  },
  {
    "text": "don't feel bad at all about about giving them another plug when you're good you're good you know and they're good yeah I know well uh Chris I appreciate",
    "start": "2404000",
    "end": "2411079"
  },
  {
    "text": "you talking through this subject with me it's been fun thanks foring uh yeah on to the next hope you have a good evening",
    "start": "2411079",
    "end": "2418000"
  },
  {
    "text": "and uh and rest of the week sounds good see you next time",
    "start": "2418000",
    "end": "2423119"
  },
  {
    "text": "bye all right that is practical AI for this week if this is your first time listening",
    "start": "2423839",
    "end": "2429920"
  },
  {
    "text": "subscribe now at practical ai. FM or just search for practical AI in your",
    "start": "2429920",
    "end": "2434960"
  },
  {
    "text": "favorite podcast app we're in there and if you're a longtime listener please do share the show with your friends it is",
    "start": "2434960",
    "end": "2440760"
  },
  {
    "text": "the best way you can help practical AI succeed thanks again to fastly for shipping our shows super fast all around",
    "start": "2440760",
    "end": "2447240"
  },
  {
    "text": "the world to break master cylinder for the beats and to you for listening we appreciate you that's all for this week",
    "start": "2447240",
    "end": "2453240"
  },
  {
    "text": "we'll talk to again next time",
    "start": "2453240",
    "end": "2459240"
  },
  {
    "text": "[Music]",
    "start": "2461880",
    "end": "2465109"
  },
  {
    "text": "[Music]",
    "start": "2469140",
    "end": "2484069"
  },
  {
    "text": "K look",
    "start": "2484880",
    "end": "2488039"
  }
]