[
  {
    "text": "the goal of any technology is to basically support humans right not replace them wherever it can help",
    "start": "40",
    "end": "5120"
  },
  {
    "text": "automate and kind of debias human decision- making we want to support that and not replace that but at the same",
    "start": "5120",
    "end": "10559"
  },
  {
    "text": "time we want to make sure that we're not replacing human judgment I think that the challenge that we have in our world",
    "start": "10559",
    "end": "17520"
  },
  {
    "text": "I don't know that we will have that choice meaning that we don't have a person sitting in every kind of like",
    "start": "17520",
    "end": "24400"
  },
  {
    "text": "critical decision Junction and making that critical human judgment it doesn't scale",
    "start": "24400",
    "end": "31960"
  },
  {
    "text": "big thanks to our partners Leno fastly and launch Darkly we love Leno they keep it fast and simple check them out at",
    "start": "32880",
    "end": "38520"
  },
  {
    "text": "lin.com changelog our bandwidth is provided by fastly learn more at fastly.com and get your feature Flags",
    "start": "38520",
    "end": "45399"
  },
  {
    "text": "Power by launch Darkly get a demo at launchdarkly",
    "start": "45399",
    "end": "49920"
  },
  {
    "text": "decom this episode is brought to you by Me Myself and AI it's a podcast on",
    "start": "50559",
    "end": "56039"
  },
  {
    "text": "artificial intelligence and business and it's produced by our friends at MIT slow management review and Boston Consulting",
    "start": "56039",
    "end": "62039"
  },
  {
    "text": "Group the question is why do only 10% of companies succeed with artificial intelligence that's the question they",
    "start": "62039",
    "end": "68000"
  },
  {
    "text": "aim to answer with this podcast here's Google Cloud's will granis on an unusual AI challenge when I think about what AI",
    "start": "68000",
    "end": "75400"
  },
  {
    "text": "is I find the algorithms mathematically fascinating but I find the use of the algorithms far more fascinating because",
    "start": "75400",
    "end": "81799"
  },
  {
    "text": "from a technical perspective we're finding correlations in extremely high-dimensional nonlinear spaces it's",
    "start": "81799",
    "end": "88759"
  },
  {
    "text": "statistics at scale in some sense right we're finding these correlations between a and b and those algorithms are are",
    "start": "88759",
    "end": "94520"
  },
  {
    "text": "really interesting and I'm still teaching those now and they're fun but what's more interesting to me is what do",
    "start": "94520",
    "end": "100040"
  },
  {
    "text": "those correlations mean for the people all right Me Myself and AI is a collaboration between MIT slow management review and Boston Consulting",
    "start": "100040",
    "end": "106640"
  },
  {
    "text": "Group it's available wherever you get your podcast just search Me Myself and",
    "start": "106640",
    "end": "112880"
  },
  {
    "text": "[Music]",
    "start": "112900",
    "end": "118589"
  },
  {
    "text": "AI [Music]",
    "start": "118840",
    "end": "128670"
  },
  {
    "text": "welcome to practical AI a weekly podcast that makes artificial intelligence practical productive and accessible to",
    "start": "130000",
    "end": "136519"
  },
  {
    "text": "everyone this is where conversations around AI machine learning and data science happen join the community and",
    "start": "136519",
    "end": "142480"
  },
  {
    "text": "slack with us around various topics of the show at cha.com community and follow us on Twitter we at practical aif",
    "start": "142480",
    "end": "150490"
  },
  {
    "text": "[Music]",
    "start": "150490",
    "end": "155959"
  },
  {
    "text": "welcome to another episode of practical AI this is Daniel whack I'm a data",
    "start": "155959",
    "end": "161720"
  },
  {
    "text": "scientist with s International and I'm joined as always by my co-host Chris",
    "start": "161720",
    "end": "166760"
  },
  {
    "text": "Benson who is a tech strategist at locked Martin how you doing Chris doing very well how are you today Daniel I'm",
    "start": "166760",
    "end": "173280"
  },
  {
    "text": "doing well it's I think this episode will release in the new year but there's a lot of sort of year-end stuffff",
    "start": "173280",
    "end": "179400"
  },
  {
    "text": "happening to tie up loose ends and project plan for next year so all good things but yeah trying to do all that",
    "start": "179400",
    "end": "186720"
  },
  {
    "text": "stuff the end of your time warp here even though everyone's hearing it after the fact right exactly I'm pretty",
    "start": "186720",
    "end": "192640"
  },
  {
    "text": "excited I know that you and I have talked many different times on the podcast about where AI models can can",
    "start": "192640",
    "end": "202440"
  },
  {
    "text": "fail or go wrong either in terms of data or in terms of their behavior but maybe",
    "start": "202440",
    "end": "209080"
  },
  {
    "text": "we haven't talked as much about like ways to mitigate that problem",
    "start": "209080",
    "end": "214560"
  },
  {
    "text": "practically and and how people are approaching that and we're really excited today because we have yon singer",
    "start": "214560",
    "end": "221239"
  },
  {
    "text": "with us who is CEO of robust intelligence and uh of course they know",
    "start": "221239",
    "end": "227400"
  },
  {
    "text": "all about how AI models fail and how to do something about it so welcome welcome to the show uh yon yeah great to be here",
    "start": "227400",
    "end": "235239"
  },
  {
    "text": "thanks for having me Daniel and Chris this is you know I'm a big fan I listen to you know I think I've listened to",
    "start": "235239",
    "end": "241400"
  },
  {
    "text": "pretty much all the episodes so it's it's great to be here oh wow yeah that's that's great well it's it's wonderful to",
    "start": "241400",
    "end": "247200"
  },
  {
    "text": "have you on the show could you maybe just start out by getting us up to speed",
    "start": "247200",
    "end": "252840"
  },
  {
    "text": "maybe for those that are out there that aren't really aware of the different ways in which AI models might fail and",
    "start": "252840",
    "end": "261079"
  },
  {
    "text": "the sort of risk associated with that could you give us a little bit of an intro to maybe some of the highlights",
    "start": "261079",
    "end": "267960"
  },
  {
    "text": "from that and uh either things you've seen or things in the community that kind of motivated you to think about",
    "start": "267960",
    "end": "273880"
  },
  {
    "text": "this problem absolutely you know when we say that AI models fail like what do we mean right like failure can come in in",
    "start": "273880",
    "end": "280400"
  },
  {
    "text": "all these sort of like different shapes and forms you know so maybe we you know we kind of like start with some some",
    "start": "280400",
    "end": "285680"
  },
  {
    "text": "examples that you know I think a lot of us in the AI Community are familiar with one example that's been like kind of a",
    "start": "285680",
    "end": "290960"
  },
  {
    "text": "very famous one was um the Microsoft Chad bot example where Microsoft kind of",
    "start": "290960",
    "end": "296800"
  },
  {
    "text": "like developed this uh this chatbot this AI chatbot that was really great technology and you know but what they",
    "start": "296800",
    "end": "302680"
  },
  {
    "text": "did is they basically you know the idea was this for for this Chad bot to like kind of mimic actual human conversation",
    "start": "302680",
    "end": "308479"
  },
  {
    "text": "and the way that they did that is they actually used you know kind of like data from Twitter right kind of um to have",
    "start": "308479",
    "end": "316160"
  },
  {
    "text": "you know trained that bot and when people figured that out what they actually did is they actually you know kind of fed all sort of like kind of",
    "start": "316160",
    "end": "322639"
  },
  {
    "text": "weird different things to the to the Chad bot to the point where they realized that they can actually make this Chad bot racist right so at the end",
    "start": "322639",
    "end": "329240"
  },
  {
    "text": "what you got is like kind of pretty quickly I think within you know a few I think it was like 36 hours after it was",
    "start": "329240",
    "end": "334600"
  },
  {
    "text": "kind of released you know you had this Chad B that was kind of like spinning out all these sort of like awful racial",
    "start": "334600",
    "end": "339680"
  },
  {
    "text": "slurs and and Microsoft end up shutting it down right so you know it's just kind of like one example where you have this",
    "start": "339680",
    "end": "345280"
  },
  {
    "text": "like really the technology right and once it falls into you know the wrong hands right it can obviously you know it",
    "start": "345280",
    "end": "352000"
  },
  {
    "text": "can kind of be led astray and kind of um be manipulated in in all these kind of weird ways that you know we never",
    "start": "352000",
    "end": "357880"
  },
  {
    "text": "expected and intended so there all these sort of ways that you know that that AI models fail and you know even like kind",
    "start": "357880",
    "end": "364000"
  },
  {
    "text": "of as of as of recent like we've all read about or some of us have read about you know zillow's example where if you",
    "start": "364000",
    "end": "369720"
  },
  {
    "text": "went on wired and you read like in June 15th there was like kind of this beautiful headline about how Zillow is going to like improve its uh you know",
    "start": "369720",
    "end": "376840"
  },
  {
    "text": "its predictive pricing like on on on housing using kind of sophisticated",
    "start": "376840",
    "end": "382039"
  },
  {
    "text": "neural networks but then if you kind of redwire again in November it explained",
    "start": "382039",
    "end": "387080"
  },
  {
    "text": "to you why Zillow failed with its AI pricing AI based pricing right and uh",
    "start": "387080",
    "end": "392800"
  },
  {
    "text": "basically they kind of like let go I think of like 25% of the you know of sil's kind of like employees and it had",
    "start": "392800",
    "end": "399479"
  },
  {
    "text": "huge kind of like economic implications for Zillow and basically you know there",
    "start": "399479",
    "end": "404919"
  },
  {
    "text": "the the failure came because of changing conditions and specifically it was because of the pandemic so the models",
    "start": "404919",
    "end": "411360"
  },
  {
    "text": "were trained using old data and then they applied them and then you know on",
    "start": "411360",
    "end": "416479"
  },
  {
    "text": "you know on the world which you know was experiencing a pandemic and that data was you know was very",
    "start": "416479",
    "end": "421680"
  },
  {
    "text": "different so this is what you know we call distributional drift and we saw failure of the AI models there so these",
    "start": "421680",
    "end": "427160"
  },
  {
    "text": "are just kind of like two I think kind of one famous example and one of the more recent examples but of different",
    "start": "427160",
    "end": "432720"
  },
  {
    "text": "you know totally different ways of in which AI systems can you know can just",
    "start": "432720",
    "end": "438000"
  },
  {
    "text": "completely fail for different reasons yeah so so those are kind of like some examples and and the implications are",
    "start": "438000",
    "end": "443440"
  },
  {
    "text": "you know are obvious right you know either you know we have like kind of um very bad reputation very bad risk we're",
    "start": "443440",
    "end": "450160"
  },
  {
    "text": "putting kind of people safety at risk so those are kind of big problems that we're experiencing I definitely see on",
    "start": "450160",
    "end": "456560"
  },
  {
    "text": "the one hand you sort of got these like behavioral problems of models where like",
    "start": "456560",
    "end": "461599"
  },
  {
    "text": "depending on what data you feed them or train them on or update them with you kind of get this nonideal behavior in in",
    "start": "461599",
    "end": "469680"
  },
  {
    "text": "one way or another I'm curious about your perspective Now sort of being in",
    "start": "469680",
    "end": "474759"
  },
  {
    "text": "this this field and seeing how clients are using AI models maybe more and more",
    "start": "474759",
    "end": "480840"
  },
  {
    "text": "over time what is the sort of from your perspective is there an increasing risk",
    "start": "480840",
    "end": "486440"
  },
  {
    "text": "in the way in which people are using models like the the tasks that they're applying to applying models to in",
    "start": "486440",
    "end": "493520"
  },
  {
    "text": "business use cases where like the chatbot one is is interesting and you know it's bad PR for Microsoft but like",
    "start": "493520",
    "end": "501440"
  },
  {
    "text": "are people starting to maybe apply AI models in maybe more Risky Business use",
    "start": "501440",
    "end": "507479"
  },
  {
    "text": "cases versus sort of more toy problems or research type of settings that's a",
    "start": "507479",
    "end": "513680"
  },
  {
    "text": "great question I think in general right regardless of like one organization or another like we are in a world that is",
    "start": "513680",
    "end": "521560"
  },
  {
    "text": "adopting algorithmic decision- making that is completely based on AI and this world is adopting this this sort of aomy",
    "start": "521560",
    "end": "528440"
  },
  {
    "text": "decision-making at an exponential right exponential pace and some examples are examples that you know have been there",
    "start": "528440",
    "end": "534720"
  },
  {
    "text": "out there for a while that we know right but it goes for you know things that are you know as simple as AI models being",
    "start": "534720",
    "end": "541519"
  },
  {
    "text": "used for determining you know basically insurance rates right for home insurance",
    "start": "541519",
    "end": "547519"
  },
  {
    "text": "right for you know home insurance and car insurance but also for health insurance right and when these AI models",
    "start": "547519",
    "end": "553839"
  },
  {
    "text": "can you know can have these sort of different failure points right in Failure modes that that has huge",
    "start": "553839",
    "end": "559279"
  },
  {
    "text": "modifications same with uh with lending so AI is used a lot in lending and deciding like who gets a loan you know",
    "start": "559279",
    "end": "566000"
  },
  {
    "text": "who gets a house loan who doesn't get a house loan who gets a car loan who doesn't and you know and how much they have to pay and we also see this in",
    "start": "566000",
    "end": "572560"
  },
  {
    "text": "things like you know predictive policing right where police departments across the country are using AI models to",
    "start": "572560",
    "end": "579079"
  },
  {
    "text": "basically decide on you know whether you know where they're going to be putting more more forces right and all these",
    "start": "579079",
    "end": "586200"
  },
  {
    "text": "things are I think kind of like the intentions are good right people want to you know want to make good decisions and",
    "start": "586200",
    "end": "592279"
  },
  {
    "text": "they want to do this in a fair way and and and automate you know that process somehow but at the same time with",
    "start": "592279",
    "end": "598000"
  },
  {
    "text": "everything that we know about Ai and disle nature you know the risk is is enormous and I got a question about that",
    "start": "598000",
    "end": "603880"
  },
  {
    "text": "is we've talked about some of these use cases and you kind of think of maybe divide them up a little bit on the on",
    "start": "603880",
    "end": "609680"
  },
  {
    "text": "the Microsoft side there's failure that is uh intentional in nature there's sort",
    "start": "609680",
    "end": "614760"
  },
  {
    "text": "of these for lack of a better order adversaries out there and they're taking advantage of the of the weakness of a",
    "start": "614760",
    "end": "620920"
  },
  {
    "text": "model and seeing what they can do with the chatbot and then you have these kind of environmental influences such as the",
    "start": "620920",
    "end": "627839"
  },
  {
    "text": "Zillow thing where the world changed out from under you it was no one person's intention to do that but nonetheless it",
    "start": "627839",
    "end": "634720"
  },
  {
    "text": "had the same effect does the intentional and the unintentional does that matter in as",
    "start": "634720",
    "end": "641399"
  },
  {
    "text": "you're dealing with these failure situations does it does it change the way that you approach the problem or",
    "start": "641399",
    "end": "648399"
  },
  {
    "text": "does that intention or lack thereof you know just because it happened is that a factor in things that's a very good",
    "start": "648399",
    "end": "653959"
  },
  {
    "text": "question right like doesn't matter whether you know whether the AI failures are due to an adversary that's trying to",
    "start": "653959",
    "end": "660800"
  },
  {
    "text": "to create you know these failure modes or they just or they just sort of like happen because of you know changing",
    "start": "660800",
    "end": "665920"
  },
  {
    "text": "natural conditions or whatnot so I think the answer to that is you know yes and no it like kind of it matters and it",
    "start": "665920",
    "end": "671880"
  },
  {
    "text": "does not matter and in our approach and that you know and that's sort of like kind of the approach that we have in our company right in our company the part",
    "start": "671880",
    "end": "678880"
  },
  {
    "text": "where we think it does not matter right is where basically we put all these",
    "start": "678880",
    "end": "684120"
  },
  {
    "text": "things under the category of risk right where what we look at is we basically we abstract like kind of we abstract the",
    "start": "684120",
    "end": "690639"
  },
  {
    "text": "root cause of the risk right away from you know we sort of say well it doesn't",
    "start": "690639",
    "end": "696360"
  },
  {
    "text": "really matter you know kind of what has caused that model to fail right the important thing is that you know that",
    "start": "696360",
    "end": "701839"
  },
  {
    "text": "the model does not fail for what you know whatever you know whatever reason so in in that sense we're kind of like",
    "start": "701839",
    "end": "707519"
  },
  {
    "text": "we take an approach where we're agnostic right to whether it was an adversary that fooled the model whether it was the",
    "start": "707519",
    "end": "713079"
  },
  {
    "text": "pandemic that changed the conditions right whether it was whether somebody really intended to you know Miss feed to",
    "start": "713079",
    "end": "720120"
  },
  {
    "text": "put in like racial SS or it's just something that you know for whatever reason you know was picked up from the internet so somehow like the the root",
    "start": "720120",
    "end": "727240"
  },
  {
    "text": "cause does not really matter right what's important is that we somehow are able to reduce it right from a technical",
    "start": "727240",
    "end": "734000"
  },
  {
    "text": "perspective right to um to kind of like understand it and and being able to",
    "start": "734000",
    "end": "739160"
  },
  {
    "text": "protect the models from you know from this now yes it does matter right in the sense of you know kind of like the",
    "start": "739160",
    "end": "745839"
  },
  {
    "text": "algorithms you know that you end up using to you know kind of protect models from you know one kind of failure to",
    "start": "745839",
    "end": "752279"
  },
  {
    "text": "like another kind of failure right so you know the algorithms that you would use to protect a model from what we call",
    "start": "752279",
    "end": "759000"
  },
  {
    "text": "like a distributional drift like kind of changing in conditions like due to the pandemic are different from the",
    "start": "759000",
    "end": "764120"
  },
  {
    "text": "algorithms that we use right to protect the model from being kind of handed kind of like adversarial input on that point",
    "start": "764120",
    "end": "771519"
  },
  {
    "text": "are you seeing like as you're engaging with clients and and companies are you seeing I guess two questions one is what",
    "start": "771519",
    "end": "778920"
  },
  {
    "text": "is their perception of the main category of risk in those two cases the adversarial or the sort of data drift",
    "start": "778920",
    "end": "786360"
  },
  {
    "text": "and distributional change types of things and what maybe from your perspective is the sort of reality I",
    "start": "786360",
    "end": "793720"
  },
  {
    "text": "guess I I guess maybe some companies have a natural naturally a higher risk",
    "start": "793720",
    "end": "798760"
  },
  {
    "text": "Vector for adversarial parties coming against them just because of what they do or or whatever but yeah I don't know",
    "start": "798760",
    "end": "805160"
  },
  {
    "text": "if you have any thought on that yeah I think that's a really interesting question I think that it really depends",
    "start": "805160",
    "end": "810680"
  },
  {
    "text": "on you know the company but not only the company you know it actually you know like even you know different teams",
    "start": "810680",
    "end": "816560"
  },
  {
    "text": "within a company can have different concerns right you can imagine that you have a company where they have a team",
    "start": "816560",
    "end": "821680"
  },
  {
    "text": "and that team is responsible for fraud detection in that team right so the team",
    "start": "821680",
    "end": "826880"
  },
  {
    "text": "that's you know dealing with fraud detection in the company right they are constantly dealing with adversarial",
    "start": "826880",
    "end": "832759"
  },
  {
    "text": "input and they very much care about protecting their AI from you know the thread vectors of adversarial input",
    "start": "832759",
    "end": "838720"
  },
  {
    "text": "right the same company can have another team and that team is dealing with forecasting forecasting of different",
    "start": "838720",
    "end": "844720"
  },
  {
    "text": "events right and you know forecasting different events like they don't care about you know they don't think about",
    "start": "844720",
    "end": "850000"
  },
  {
    "text": "adversarial input they worry about you know rainy days changing you know if you're thinking about maybe like a",
    "start": "850000",
    "end": "855440"
  },
  {
    "text": "company that's suing ride shairing or things like that right so they care about like the underlying you know the",
    "start": "855440",
    "end": "861079"
  },
  {
    "text": "the continuously kind of changing conditions and how that affects the you know the models and predictions and if you want to do this well right then you",
    "start": "861079",
    "end": "867560"
  },
  {
    "text": "want to be able to you know kind of build system right that protects models from both these types of cases right the",
    "start": "867560",
    "end": "874600"
  },
  {
    "text": "cases where there's like you know there adversaries that are you know like really just trying to change and manipulate you know Financial",
    "start": "874600",
    "end": "880600"
  },
  {
    "text": "transactions right as well as the thread Vector of just changing conditions right",
    "start": "880600",
    "end": "886079"
  },
  {
    "text": "with no adversary in place but just sort of like uh changing conditions that can change the you know the the predictions",
    "start": "886079",
    "end": "891519"
  },
  {
    "text": "of the [Music]",
    "start": "891519",
    "end": "898040"
  },
  {
    "text": "models [Music]",
    "start": "898040",
    "end": "908680"
  },
  {
    "text": "change log Plus+ is the best way for you to directly support practical AI join",
    "start": "908680",
    "end": "915000"
  },
  {
    "text": "today and unlock access to a private feed that makes the ads disappear gets you closer to the metal and help sustain",
    "start": "915000",
    "end": "922199"
  },
  {
    "text": "our production of practical AI into the future simply follow the Chang log Plus+",
    "start": "922199",
    "end": "928240"
  },
  {
    "text": "Link in your show notes or Point your favorite web browser to",
    "start": "928240",
    "end": "933959"
  },
  {
    "text": "[Music]",
    "start": "938610",
    "end": "941730"
  },
  {
    "text": "[Music]",
    "start": "944210",
    "end": "965759"
  },
  {
    "text": "so as I was looking through some kind of updated best practices out there",
    "start": "965759",
    "end": "970839"
  },
  {
    "text": "actually I think what triggered this was you know Chris and I spoke recently on a show about the open AI API because it's",
    "start": "970839",
    "end": "977839"
  },
  {
    "text": "now sort of generally available and I was reading through a bunch of their documentation in terms of like how they",
    "start": "977839",
    "end": "984240"
  },
  {
    "text": "can what they're thinking about in terms of risk and like best practices in terms terms of using the API to maintain",
    "start": "984240",
    "end": "992279"
  },
  {
    "text": "safety and security and privacy and and all of these things one of the things that they highlighted in there was the",
    "start": "992279",
    "end": "998800"
  },
  {
    "text": "sort of Automation and human in the loop element of this where like if you're",
    "start": "998800",
    "end": "1004480"
  },
  {
    "text": "planning on creating an application for example with the open AI API it's",
    "start": "1004480",
    "end": "1011120"
  },
  {
    "text": "considered much higher risk if it's to automate something right and there's not going to be any sort of human in the",
    "start": "1011120",
    "end": "1016519"
  },
  {
    "text": "loop to review things as someone who kind of works to kind of manage and",
    "start": "1016519",
    "end": "1021759"
  },
  {
    "text": "mitigate the risk in these types of scenarios how do you view I guess as the",
    "start": "1021759",
    "end": "1028438"
  },
  {
    "text": "industry gets more sophisticated at handling the risk are we going to be able to automate things more or maybe is",
    "start": "1028439",
    "end": "1035480"
  },
  {
    "text": "it is it that we deploy our models and we have monitoring infrastructure that helps us know when things are going",
    "start": "1035480",
    "end": "1042038"
  },
  {
    "text": "wrong but it's you know still automated I'm wondering how you kind of view this shifting over time and view this need",
    "start": "1042039",
    "end": "1049520"
  },
  {
    "text": "for humans in the loop in terms of the output of models I think this is really important and really interesting right",
    "start": "1049520",
    "end": "1056280"
  },
  {
    "text": "and I think that what what you're seeing from open AI is just it just Echoes what we're seeing from you know from leading",
    "start": "1056280",
    "end": "1062480"
  },
  {
    "text": "companies and platform in general when you look at the stateof the-art it's all going towards automation right so if",
    "start": "1062480",
    "end": "1068200"
  },
  {
    "text": "you're using data bricks today you can already you're essentially kind of in data bricks notebooks you can you have the option of retraining your models",
    "start": "1068200",
    "end": "1074600"
  },
  {
    "text": "automatically right so and in general that's kind of where the world is going and and I think that like within I think",
    "start": "1074600",
    "end": "1080559"
  },
  {
    "text": "that within I want to be careful with my predictions here but like we're a few years away from basically having most of",
    "start": "1080559",
    "end": "1087320"
  },
  {
    "text": "the retraining tasks being done automatically meaning without a human in the loop right and that's generally",
    "start": "1087320",
    "end": "1093679"
  },
  {
    "text": "where I think AI is going right if you kind of want to think about where I was going to be like kind of you three years",
    "start": "1093679",
    "end": "1099480"
  },
  {
    "text": "from now look at where I AI was you know five or seven years ago if you think about kind of like where we were seven",
    "start": "1099480",
    "end": "1106600"
  },
  {
    "text": "years ago this is like it predates py could learn predates tensorflow predates pie torch right where if you wanted to",
    "start": "1106600",
    "end": "1113520"
  },
  {
    "text": "like you know if you heard about Ai and you thought oh maybe that could have an advantage for my business or my organization you would basically need to",
    "start": "1113520",
    "end": "1120039"
  },
  {
    "text": "hire a PhD from some top school to like code up in see an svm that's kind of",
    "start": "1120039",
    "end": "1126919"
  },
  {
    "text": "what they would do the thought of that today seems like kind of very right we're kind of laughing about that but",
    "start": "1126919",
    "end": "1132200"
  },
  {
    "text": "that's really how it used to be just like in what in 2014 right 2015 there's been a lot of automation since then and",
    "start": "1132200",
    "end": "1138919"
  },
  {
    "text": "I think that where we're going is we're going towards more and more and more of this automation especially when you know",
    "start": "1138919",
    "end": "1144559"
  },
  {
    "text": "if you meet a company now and that company has just a handful of models you know talk to that company again a year",
    "start": "1144559",
    "end": "1149880"
  },
  {
    "text": "from now they'll probably have like hundreds of models right and if you met a company now that has hundreds of models you know a year from now they'll",
    "start": "1149880",
    "end": "1155919"
  },
  {
    "text": "probably have thousands of models so in order to do that in scale it's all automation so that's that's kind of like",
    "start": "1155919",
    "end": "1162320"
  },
  {
    "text": "our our perspective and philosophy in the company that the world of AI is going towards automation",
    "start": "1162320",
    "end": "1169320"
  },
  {
    "text": "and as the world of AI is going towards automation we're trying to really think and understand how do we ensure an",
    "start": "1169320",
    "end": "1176320"
  },
  {
    "text": "organization that is using AI especially kind of in an automated way is really",
    "start": "1176320",
    "end": "1181840"
  },
  {
    "text": "making sure that is not taking on any risk that it really eliminates all the risks that AI has especially with",
    "start": "1181840",
    "end": "1188159"
  },
  {
    "text": "assuming that it does automated retraining of models and all that I think automation definitely presents an",
    "start": "1188159",
    "end": "1193799"
  },
  {
    "text": "additional you know dimension of risk and it's really important to like understand that that risk that we're",
    "start": "1193799",
    "end": "1199400"
  },
  {
    "text": "taking on right and be thoughtful about it I got a follow-up question to that when as Daniel was asking that last one",
    "start": "1199400",
    "end": "1205159"
  },
  {
    "text": "it popped to mind and You' started addressing it already I wanted to explore it a little bit and that is if",
    "start": "1205159",
    "end": "1210400"
  },
  {
    "text": "you're thinking about this journey from kind of where we are now and having",
    "start": "1210400",
    "end": "1215840"
  },
  {
    "text": "human in the loop on lots of critical tasks and lots of Industries especially for retraining and things but if we",
    "start": "1215840",
    "end": "1222679"
  },
  {
    "text": "agree that we're generally moving toward full automation there's risks associated with both of those scenarios",
    "start": "1222679",
    "end": "1229360"
  },
  {
    "text": "and there's also a risk associated with that kind of in between where it's human automation collaboration often said like",
    "start": "1229360",
    "end": "1236720"
  },
  {
    "text": "in the industry I'm in man done man teaming that kind of concept all of those two poles plus the journey in the",
    "start": "1236720",
    "end": "1242760"
  },
  {
    "text": "middle all have a discrete set of risks associate with them how do you see that how do you look at those risk sets and",
    "start": "1242760",
    "end": "1248640"
  },
  {
    "text": "how do you decide what matters you know how do you evaluate that because I think that's a really tough question that I",
    "start": "1248640",
    "end": "1255080"
  },
  {
    "text": "end up talking with folks that are grappling with is is well there's problems with having humans we're not perfect we screw up and we sometimes",
    "start": "1255080",
    "end": "1262280"
  },
  {
    "text": "talk about it in the sense of having a human brings safety but not always and there are other times you know you can",
    "start": "1262280",
    "end": "1268480"
  },
  {
    "text": "go argue the other way what's your perspective on that yeah there's an inevitable trade-off right that's what",
    "start": "1268480",
    "end": "1273960"
  },
  {
    "text": "you're saying I'm of the opinion that the goal of any technology is to basically you know support humans right",
    "start": "1273960",
    "end": "1279400"
  },
  {
    "text": "not replace them on the one hand we want to we want to support human decision- making wherever it can help automate and",
    "start": "1279400",
    "end": "1285440"
  },
  {
    "text": "kind of debias human decision-making we want to support that and not replace that but at the same time we want to",
    "start": "1285440",
    "end": "1292039"
  },
  {
    "text": "make sure that we're not replacing human judgment I think that the challenge that we have in our world I don't know that",
    "start": "1292039",
    "end": "1298320"
  },
  {
    "text": "we will have that choice meaning that we don't have a person sitting in every",
    "start": "1298320",
    "end": "1304159"
  },
  {
    "text": "kind of like critical decision Junction and making that critical human judgment",
    "start": "1304159",
    "end": "1309240"
  },
  {
    "text": "it doesn't scale and that's the biggest challenge that we have so with that in mind I think that like what we need to",
    "start": "1309240",
    "end": "1315279"
  },
  {
    "text": "do is we need to make sure that it's never going to be perfect but we need to we need to know that we're making every",
    "start": "1315279",
    "end": "1320720"
  },
  {
    "text": "possible effort to make sure that it is as safe and risk-free as much as possible that's a very thoughtful answer",
    "start": "1320720",
    "end": "1327520"
  },
  {
    "text": "thank you yeah this whole time I've sort of been thinking and through these sort of wider you know broader issues and",
    "start": "1327520",
    "end": "1335080"
  },
  {
    "text": "thinking through my own use cases and as Chris knows eventually I always get to the point where I'm like okay well I",
    "start": "1335080",
    "end": "1342279"
  },
  {
    "text": "understand the the point here like what can we do practically to address these",
    "start": "1342279",
    "end": "1347600"
  },
  {
    "text": "things and and mitigate them I'm wondering if you could like maybe walk us through like your journey to robust",
    "start": "1347600",
    "end": "1355240"
  },
  {
    "text": "intelligence in terms of how you kind of came to I guess understand what you",
    "start": "1355240",
    "end": "1361480"
  },
  {
    "text": "wanted to focus on in terms of in terms of what you wanted to build and and offer to the community because there are",
    "start": "1361480",
    "end": "1369080"
  },
  {
    "text": "so many problems and so much to address and you know obviously you have to focus",
    "start": "1369080",
    "end": "1374320"
  },
  {
    "text": "on on something to start with so how did how did you get to that point and and how did things kind of get started so uh",
    "start": "1374320",
    "end": "1381320"
  },
  {
    "text": "my journey into this has been through like a lot of AI practitioners I think it started in Academia so I was a PhD",
    "start": "1381320",
    "end": "1388640"
  },
  {
    "text": "student at Berkeley and then I I worked at Google and then I spent seven or",
    "start": "1388640",
    "end": "1393720"
  },
  {
    "text": "eight years at Harvard I've been uh I'm a professor of computer science and applied math at Harvard and basically",
    "start": "1393720",
    "end": "1399840"
  },
  {
    "text": "what what I've been working on at Harvard is exactly this topic the vulnerability the sensitivity and the",
    "start": "1399840",
    "end": "1404960"
  },
  {
    "text": "failure modes of machine learning models right so a little bit before my time I Google and then while I was in Google",
    "start": "1404960",
    "end": "1410200"
  },
  {
    "text": "I've worked on machine learning models and basically algorithmic decision that is based on machine learning models and",
    "start": "1410200",
    "end": "1416159"
  },
  {
    "text": "when you do that and and you start to kind of like do the the theory behind it and you try to prove theorems you start",
    "start": "1416159",
    "end": "1422120"
  },
  {
    "text": "realizing that we have very little foundations for kind of like algorithmic",
    "start": "1422120",
    "end": "1427400"
  },
  {
    "text": "decision- making given machine learning input and there's a good reason for why we have like very little theoretical",
    "start": "1427400",
    "end": "1433559"
  },
  {
    "text": "understanding is because it's pretty terrible to be honest and that's kind of what what I started studying and specifically like kind of I've been",
    "start": "1433559",
    "end": "1439080"
  },
  {
    "text": "looking at you know really the the sort of the whether we can make good algorithmic decisions given input from",
    "start": "1439080",
    "end": "1444520"
  },
  {
    "text": "machine learning models and the answer is you know mathematically no sort of like and this has been my my focus at",
    "start": "1444520",
    "end": "1449679"
  },
  {
    "text": "Harvard and so when you say terrible you sort of mean terrible in the sense that you can't get to a point where you can",
    "start": "1449679",
    "end": "1457080"
  },
  {
    "text": "prove in many cases that algorithmic decision making is a good idea yes",
    "start": "1457080",
    "end": "1462279"
  },
  {
    "text": "exactly you actually prove it mathematically you know we have like kind of mathematical definitions of what it means to learn and what it means to",
    "start": "1462279",
    "end": "1468120"
  },
  {
    "text": "like of make good decisions and and you can you have like kind of very very rigorous kind of uh models and",
    "start": "1468120",
    "end": "1473559"
  },
  {
    "text": "statements and and when you kind of use these rigorous and these are the kind of like the rigorous you know models and",
    "start": "1473559",
    "end": "1478919"
  },
  {
    "text": "statements that kind of make machine learning work when you kind of try to apply a little bit more complex decision- making on top of these results",
    "start": "1478919",
    "end": "1484360"
  },
  {
    "text": "of machine learning models it all starts to break so we've been kind of like proving these theorems about like how",
    "start": "1484360",
    "end": "1489840"
  },
  {
    "text": "much data you would need in order to to make what we call good decisions and turns out that you know the data is like",
    "start": "1489840",
    "end": "1495720"
  },
  {
    "text": "exponential in the dimmension of the input which is really bad and kind of like we studi the sensitivities of you know of models and",
    "start": "1495720",
    "end": "1502559"
  },
  {
    "text": "to like kind of very very small errors and very very kind of like small failures and again like kind of like infinitely small errors in you know in",
    "start": "1502559",
    "end": "1509440"
  },
  {
    "text": "models can lead to errors that are arbitrarily like kind of bad it's quite horrible and quite bad and I've actually",
    "start": "1509440",
    "end": "1515520"
  },
  {
    "text": "spent some time you know in my academ career trying to conly convince people of this right and giving talks about An",
    "start": "1515520",
    "end": "1520880"
  },
  {
    "text": "Inconvenient Truth About algorithms in the era of machine learning if you look at that title up you'll see like a bunch of lectures coming up in you know in in",
    "start": "1520880",
    "end": "1527679"
  },
  {
    "text": "in seminars acoss I'm sure it produces some awkward conversations at conferences absolutely absolutely it",
    "start": "1527679",
    "end": "1534399"
  },
  {
    "text": "does right especially in a time where kind of like machine learning is on the rise and and on the boom and you know and and your department is like you know",
    "start": "1534399",
    "end": "1540520"
  },
  {
    "text": "this gungho about like hiring more people in machine learning and now they have like this professor who's like you",
    "start": "1540520",
    "end": "1545679"
  },
  {
    "text": "know kind of uh proves all these weird theor about why it's not working I'm sitting in the same Department as Les",
    "start": "1545679",
    "end": "1551480"
  },
  {
    "text": "Valiant who created the foundations of machine learning right who's been by the way like the most receptive person to",
    "start": "1551480",
    "end": "1557279"
  },
  {
    "text": "like you know this sort of CR iism and and whatnot and he's been so supportive so it's uh definitely like a lot of some",
    "start": "1557279",
    "end": "1563039"
  },
  {
    "text": "papers took like you know 3 four years to get published but they they got published and then they got kind of like",
    "start": "1563039",
    "end": "1568559"
  },
  {
    "text": "very good recognition but we started from you know the theories about the possibilities and then we moved to",
    "start": "1568559",
    "end": "1574000"
  },
  {
    "text": "algorithms and you know in my group developed a lot of basically focus on algorithms for you know noise robust",
    "start": "1574000",
    "end": "1579440"
  },
  {
    "text": "algorithms for these types of problems and kind of like focusing on what is that we can do as we kind of proved all these impossibility theorems and you",
    "start": "1579440",
    "end": "1585880"
  },
  {
    "text": "know kind of like focus on what what is it the algorithms would be able to do like a very big idea that came up was like decoupling so what we realized that",
    "start": "1585880",
    "end": "1593240"
  },
  {
    "text": "one needs to do is basically decouple the part about model building from model",
    "start": "1593240",
    "end": "1599960"
  },
  {
    "text": "security or model safety meaning that if you try to like build just train a model",
    "start": "1599960",
    "end": "1605520"
  },
  {
    "text": "that is going to be robust to you know let's say adversarial input we talked about that I think kind of the best",
    "start": "1605520",
    "end": "1611399"
  },
  {
    "text": "result known and I think that result is now from like two or three years ago it turns out that if you want to like make",
    "start": "1611399",
    "end": "1616640"
  },
  {
    "text": "your model just by training it to make it more robust to like adversarial input",
    "start": "1616640",
    "end": "1622240"
  },
  {
    "text": "I think in image classification you're going to take the accuracy of the model from 98% to",
    "start": "1622240",
    "end": "1628080"
  },
  {
    "text": "37% right uh in order to get any sort of reasonable robustness which is a",
    "start": "1628080",
    "end": "1633520"
  },
  {
    "text": "trade-off that is just inacceptable right if you're going to come up to like a company and tell them hey you know how your model like has 98% accuracy well in",
    "start": "1633520",
    "end": "1641159"
  },
  {
    "text": "order to make it robust to like the 0.001% of input that is bad I'm going to",
    "start": "1641159",
    "end": "1646760"
  },
  {
    "text": "take that model to accuracy to like 37% would there be an analogy there to kind of in in the software world having a",
    "start": "1646760",
    "end": "1653039"
  },
  {
    "text": "separation of concerns absolutely yeah okay for any listeners that are not software people you want to address the",
    "start": "1653039",
    "end": "1659000"
  },
  {
    "text": "problem you're trying to address but at the same time the ancillary things like security that are very important need to",
    "start": "1659000",
    "end": "1664240"
  },
  {
    "text": "be addressed but you address them separately to maximize both absolutely whenever I talk to people for the first",
    "start": "1664240",
    "end": "1669799"
  },
  {
    "text": "time about this I tell them you know look there are two considerations one is the mathematical consideration and mathematically if you wanted to make",
    "start": "1669799",
    "end": "1677200"
  },
  {
    "text": "your model robust quote unquote by retraining it to like aders input it means you're going to like take accuracy",
    "start": "1677200",
    "end": "1683200"
  },
  {
    "text": "from something like 98% to 37% it's kind of like it's just a mathematical fact now there's another aspect of it which",
    "start": "1683200",
    "end": "1689600"
  },
  {
    "text": "is the product aspect of it right right or the kind of like the engineering aspect of it and the engineering aspect",
    "start": "1689600",
    "end": "1695120"
  },
  {
    "text": "of it is it's exactly what you're talking about Chris right where if you're an engineer that's building a system you probably should not be the",
    "start": "1695120",
    "end": "1701960"
  },
  {
    "text": "one who's also responsible for like protecting that system you know we can just all imagine the nightmare if every",
    "start": "1701960",
    "end": "1707679"
  },
  {
    "text": "time that we wrote software we'd also have to write the antivirus and the firewall right for that software so in",
    "start": "1707679",
    "end": "1714120"
  },
  {
    "text": "software like kind of we've seen such tremendous success exactly because of this decoupling right and kind of like",
    "start": "1714120",
    "end": "1719919"
  },
  {
    "text": "layering of different systems right and components that know how to work together right and almost like this agnostic way and I think that what we're",
    "start": "1719919",
    "end": "1727159"
  },
  {
    "text": "trying to do in the company and robust intelligence is mimic that kind of decoupling so specifically what we do is",
    "start": "1727159",
    "end": "1733240"
  },
  {
    "text": "we build an AI firewall and an AI firewall is a piece of software",
    "start": "1733240",
    "end": "1738799"
  },
  {
    "text": "that wraps around an AI model to protect it from making mistakes so it kind of",
    "start": "1738799",
    "end": "1744799"
  },
  {
    "text": "like it's a one line of code that you add so that that one line of code can basically stands between the data and",
    "start": "1744799",
    "end": "1751399"
  },
  {
    "text": "the model and it can once the data comes in it monitors tests and can even",
    "start": "1751399",
    "end": "1756960"
  },
  {
    "text": "correct it so basically the data point does not fool the model does not cause",
    "start": "1756960",
    "end": "1762279"
  },
  {
    "text": "the model to make the sort of mistake right or a bad prediction and that's sort of like this decoupling process",
    "start": "1762279",
    "end": "1768760"
  },
  {
    "text": "where we're not trying to build a better model all we're trying to do is we're trying to be basically be able to catch",
    "start": "1768760",
    "end": "1775679"
  },
  {
    "text": "bad data so it's reducing it to like a much much simpler task I definitely resonated with your example around the",
    "start": "1775679",
    "end": "1782200"
  },
  {
    "text": "sort of firewall and Antivirus I know in my own sort of building software career",
    "start": "1782200",
    "end": "1788799"
  },
  {
    "text": "anytime I felt like I you know I start poking holes in a firewall to open up ports and like configure things I start",
    "start": "1788799",
    "end": "1796640"
  },
  {
    "text": "feeling like extremely uncomfortable because I have no idea what I'm doing so that definitely resonates with me we",
    "start": "1796640",
    "end": "1803240"
  },
  {
    "text": "talked about that firewall component this AI firewall component kind of wrapping around the model sort of",
    "start": "1803240",
    "end": "1809720"
  },
  {
    "text": "between the data and the model in that firewall are you looking at sort of like",
    "start": "1809720",
    "end": "1815679"
  },
  {
    "text": "outof distribution data that's coming in or maybe particular are there other ways",
    "start": "1815679",
    "end": "1821440"
  },
  {
    "text": "that like some data is more particularly risky than others so I guess my question",
    "start": "1821440",
    "end": "1827360"
  },
  {
    "text": "is like how do you know as data comes in if it's risky data or if it's not risky",
    "start": "1827360",
    "end": "1832760"
  },
  {
    "text": "data yes that's a good question so what we do is basically we test the models we",
    "start": "1832760",
    "end": "1838760"
  },
  {
    "text": "have a process of stress testing the models and we we do that either implicitly if you just install the AI",
    "start": "1838760",
    "end": "1844039"
  },
  {
    "text": "for if we just put the line of code we basically do that in the background we do that implicitly or sometimes you know",
    "start": "1844039",
    "end": "1850600"
  },
  {
    "text": "when there are companies where we start from stress testing and then we kind of graduate to AI firewall and stress",
    "start": "1850600",
    "end": "1856279"
  },
  {
    "text": "testing basically what that means for us is we run a series of tests on the model you know some of the tests can be how",
    "start": "1856279",
    "end": "1862440"
  },
  {
    "text": "does the model respond to distributional drift how does the model respond to unse categoricals how does a model you know",
    "start": "1862440",
    "end": "1869039"
  },
  {
    "text": "just all these sort of different scenarios right and different inputs and then we were measuring the response of",
    "start": "1869039",
    "end": "1875440"
  },
  {
    "text": "the models right to kind of like all these bad things that could happen and as we're measuring them right we're",
    "start": "1875440",
    "end": "1880519"
  },
  {
    "text": "getting a sense basically of we're basically training our own AI firewall so by understanding kind of like how",
    "start": "1880519",
    "end": "1887080"
  },
  {
    "text": "different input can affect the model in different ways when new input comes in we know whether that input is going to",
    "start": "1887080",
    "end": "1894240"
  },
  {
    "text": "lead to like some sort of prediction error some sort of prediction change so let me give you like you know like a",
    "start": "1894240",
    "end": "1899679"
  },
  {
    "text": "silly example right suppose that your model you have like some sort of AI model and maybe that AI model is trying",
    "start": "1899679",
    "end": "1905799"
  },
  {
    "text": "to predict whether you know somebody's going to earn above $100,000 next year",
    "start": "1905799",
    "end": "1911120"
  },
  {
    "text": "so whenever we take that input like we check all the the different features and when we sort of see like how the changes",
    "start": "1911120",
    "end": "1916960"
  },
  {
    "text": "in different features going to affect the prediction of the model right so maybe like you know for example we look at like how does age right affect the",
    "start": "1916960",
    "end": "1924600"
  },
  {
    "text": "prediction of the model and now suppose that like kind of um you know data comes in and somebody has accidentally",
    "start": "1924600",
    "end": "1930880"
  },
  {
    "text": "replaced age with the year of birth right so age is like you know maybe from like I don't know 36 age was changed",
    "start": "1930880",
    "end": "1938200"
  },
  {
    "text": "into like 1985 or 1986 or whatever you know so that's basically some sort of",
    "start": "1938200",
    "end": "1944960"
  },
  {
    "text": "like human error or like kind of error in you know kind of like in feeding in the data and by that point the AI",
    "start": "1944960",
    "end": "1951240"
  },
  {
    "text": "firewall is trained that it knows that you know this feature change in this feature can really affect the model prediction and certainly kind of like it",
    "start": "1951240",
    "end": "1958200"
  },
  {
    "text": "it understands what the right distribution of age is it understands the age needs to be something between you probably it's seeing data from like",
    "start": "1958200",
    "end": "1965120"
  },
  {
    "text": "10 to like 90 or something like that right so it understands whenever it sees like a big number like 1985 right",
    "start": "1965120",
    "end": "1971799"
  },
  {
    "text": "there's something wrong here and then what it can do is basically it can alert and it can prevent that mistake from",
    "start": "1971799",
    "end": "1977279"
  },
  {
    "text": "happening by by kind of even replacing you know 1985 by the mode of the distribution or something like that",
    "start": "1977279",
    "end": "1983200"
  },
  {
    "text": "right to kind of put out output like a better better prediction I'm curious it's an interesting approach that you",
    "start": "1983200",
    "end": "1989039"
  },
  {
    "text": "have here if you are already kind of building your models and you're deploying them out into production you",
    "start": "1989039",
    "end": "1994600"
  },
  {
    "text": "know whatever industry you're in and you have your mlops pipeline in place and on",
    "start": "1994600",
    "end": "2000559"
  },
  {
    "text": "the software side kind of your Dev Ops or Dev SEC Ops as it's kind of evolving into in place how do you integrate this",
    "start": "2000559",
    "end": "2007960"
  },
  {
    "text": "together how do you take what you're talking about and put into or integrate into your existing pipeline so that you",
    "start": "2007960",
    "end": "2015360"
  },
  {
    "text": "gain the benefit of what you're what you're describing and yet not kind of breaking your approach so to speak in",
    "start": "2015360",
    "end": "2021559"
  },
  {
    "text": "terms of of how you're already doing that how does all that work together great so our approach is that the best",
    "start": "2021559",
    "end": "2027519"
  },
  {
    "text": "integration is no integration and that's why we have one way to integrate the product is like kind of we call this",
    "start": "2027519",
    "end": "2032919"
  },
  {
    "text": "like light integration is where basically we're not integrating with the model at all all we do is we just take",
    "start": "2032919",
    "end": "2039120"
  },
  {
    "text": "prediction logs right so if you have like a model that's running and you",
    "start": "2039120",
    "end": "2044200"
  },
  {
    "text": "store prediction log somewhere kind of like meaning that you have like input and the output of the model and that's",
    "start": "2044200",
    "end": "2049919"
  },
  {
    "text": "stored in some sort of csb file then basically our product just basically",
    "start": "2049919",
    "end": "2055118"
  },
  {
    "text": "just just sits there and runs a cicd process and just continuously reads you know that CSV file whenever you dump in",
    "start": "2055119",
    "end": "2062398"
  },
  {
    "text": "you know a new kind of like log file and it just reads it and that's it and it just continuously tests and we call this",
    "start": "2062399",
    "end": "2068398"
  },
  {
    "text": "continuous testing so we continuously test you know the model right without ever being in the critical path with",
    "start": "2068399",
    "end": "2075358"
  },
  {
    "text": "basically zero integration so it literally is like a in production it's like a 2hour integration with kubernetes",
    "start": "2075359",
    "end": "2082720"
  },
  {
    "text": "because again it doesn't stand in the critical path of anything like that when we're doing something like AI firewall",
    "start": "2082720",
    "end": "2088320"
  },
  {
    "text": "this is where we're integrating on the again it's like a single line of code that we're integrating on the on the",
    "start": "2088320",
    "end": "2093480"
  },
  {
    "text": "actual model server and that involves some you know some libraries and things like that but again it uses that same",
    "start": "2093480",
    "end": "2099800"
  },
  {
    "text": "principle where ultimately it throws data in the form of like kind prediction logs in the background so that it",
    "start": "2099800",
    "end": "2104960"
  },
  {
    "text": "doesn't stand in the critical path of you know anything real in in the system so that's something that's that's really",
    "start": "2104960",
    "end": "2110960"
  },
  {
    "text": "important and and a lot of it we do uh we do for customers on premise because having your data leave the organization",
    "start": "2110960",
    "end": "2118079"
  },
  {
    "text": "is a huge pain it's sensitive you know there's like a lot of kind of compliance involved and things like that so on",
    "start": "2118079",
    "end": "2124320"
  },
  {
    "text": "premise is is actually something that is very important I want to follow up a little bit on what you're talking about",
    "start": "2124320",
    "end": "2130960"
  },
  {
    "text": "sort of this continuous testing I think I think is what you called it which is is a really cool idea that that I like",
    "start": "2130960",
    "end": "2137720"
  },
  {
    "text": "and probably I don't know if that's less scary to AI people than a word like monitoring or something you know",
    "start": "2137720",
    "end": "2143480"
  },
  {
    "text": "something but I I like that idea of continuous testing I'm wondering you kind of gave the simple example of like",
    "start": "2143480",
    "end": "2150040"
  },
  {
    "text": "maybe this distribution of age in the model or something like that and I'm thinking there are likely these Maybe",
    "start": "2150040",
    "end": "2157800"
  },
  {
    "text": "these categories sensitive categories like you know personal details of age or",
    "start": "2157800",
    "end": "2164520"
  },
  {
    "text": "you know race or whatever it is that you were giving examples of like the police scenarios and and that sort of thing",
    "start": "2164520",
    "end": "2170880"
  },
  {
    "text": "where these might be categories in which variation in that category should",
    "start": "2170880",
    "end": "2178200"
  },
  {
    "text": "necessarily produce invariance in the in the output at least if if you're",
    "start": "2178200",
    "end": "2183440"
  },
  {
    "text": "monitoring those well what's your perspective on that in terms of how to approach those sensitive",
    "start": "2183440",
    "end": "2190920"
  },
  {
    "text": "categories and is is that something you can kind of program to both in terms of variation and maybe things that should",
    "start": "2190920",
    "end": "2197800"
  },
  {
    "text": "be invariant with change so basically like in our Paradigm one of the kind of",
    "start": "2197800",
    "end": "2204240"
  },
  {
    "text": "um the basic building blocks is a building block of tests basically what you know you discover whatever it is",
    "start": "2204240",
    "end": "2210760"
  },
  {
    "text": "that you test for right if you're not going to test for bias you're not going to discover bias but if this is",
    "start": "2210760",
    "end": "2216920"
  },
  {
    "text": "something that you care about and I think that a lot of practitioners and organizations should very much care about bias right then you test for it",
    "start": "2216920",
    "end": "2224400"
  },
  {
    "text": "and whenever whenever it exists you find it and that's actually a suite of tests that we have embeded in the product that",
    "start": "2224400",
    "end": "2230640"
  },
  {
    "text": "is actually very important right so what we do is we we automatically test the model we just automatically go through",
    "start": "2230640",
    "end": "2237240"
  },
  {
    "text": "like all the different categories and we test like whether there's whether there's bias and prediction whether there's bias in UC see whether there's",
    "start": "2237240",
    "end": "2243520"
  },
  {
    "text": "bias in false positives false negatives like all these different things across different categories right and that's",
    "start": "2243520",
    "end": "2249200"
  },
  {
    "text": "when people discover like all these minuses that they had in the model that they never knew about some of it is protected categories and some of it is",
    "start": "2249200",
    "end": "2255599"
  },
  {
    "text": "just you know just other categories that they you know they they didn't know that they found out that their model is just",
    "start": "2255599",
    "end": "2260760"
  },
  {
    "text": "like they're not training their models on the right data set or they should do kind of different sampling of the data",
    "start": "2260760",
    "end": "2266440"
  },
  {
    "text": "right in order to make sure that the model is not even like kind of performance biased right so I think cool testing for bias is such a critical",
    "start": "2266440",
    "end": "2273079"
  },
  {
    "text": "thing that basically all AI practitioners should do this has been a fascinating exploration and definitely",
    "start": "2273079",
    "end": "2280240"
  },
  {
    "text": "a topic that we have not gotten really into at any point in any previous episode as we wind up where do you see",
    "start": "2280240",
    "end": "2287440"
  },
  {
    "text": "the future going both with robust intelligence as your Enterprise as your",
    "start": "2287440",
    "end": "2292480"
  },
  {
    "text": "company that is doing things but also within the larger world of robust",
    "start": "2292480",
    "end": "2297599"
  },
  {
    "text": "intelligence the larger effort in the industry to drive forward and the evolution what we won't hold you to any",
    "start": "2297599",
    "end": "2303839"
  },
  {
    "text": "predictions but if you were to make some predictions on where where you think it will go or where you would like to see",
    "start": "2303839",
    "end": "2309400"
  },
  {
    "text": "it go I'd love to hear that sure so I think like let's start by the things that are not interesting right what we",
    "start": "2309400",
    "end": "2315560"
  },
  {
    "text": "all know is we all know that the world is kind of like AI is eating the world right like it's we're going to be hardpressed to find an organization that",
    "start": "2315560",
    "end": "2321640"
  },
  {
    "text": "is not adopting AI in a serious way in just a couple of years I think right so okay that's not interesting so let's put",
    "start": "2321640",
    "end": "2327880"
  },
  {
    "text": "that aside but I think when it comes to our little part of the world when it comes to like AI risk we think that",
    "start": "2327880",
    "end": "2334960"
  },
  {
    "text": "within just a few years there are two things I think that any organization that uses AI in a way that can affect",
    "start": "2334960",
    "end": "2341680"
  },
  {
    "text": "people is going to have to like go through some sort of stress testing by third party I think that's going to be",
    "start": "2341680",
    "end": "2347040"
  },
  {
    "text": "mandatory I don't think that it's going to be up to like the director of data science in that company that's just going to be regulation that's number one",
    "start": "2347040",
    "end": "2353880"
  },
  {
    "text": "and number two also on regulation and best practice I think just a few years from now in the same way I think we're",
    "start": "2353880",
    "end": "2361040"
  },
  {
    "text": "going to be hard pressed to find a company that is not protecting its models with an AI firewall not",
    "start": "2361040",
    "end": "2366640"
  },
  {
    "text": "necessarily we're about intelligence right I don't know if any other companies are making a firewalls but but",
    "start": "2366640",
    "end": "2372000"
  },
  {
    "text": "I think that like within a few years when we're going to have this conversation again three years from now we'll go back and remind ourselves like",
    "start": "2372000",
    "end": "2378119"
  },
  {
    "text": "Hey do you remember that you know three years ago you know companies were actually deploying AI models without an AI firewall how crazy was that right I",
    "start": "2378119",
    "end": "2385960"
  },
  {
    "text": "think that's what we're going to find three years from now that's where we're going kind of like in my part of the",
    "start": "2385960",
    "end": "2391079"
  },
  {
    "text": "world and you should consider that an invitation by the way for three years out to come back and we'll have that conversation the data is going to be",
    "start": "2391079",
    "end": "2397079"
  },
  {
    "text": "easy to remember because I think that we're like the last podcast of the year right so yeah as we go into 2025 it'll",
    "start": "2397079",
    "end": "2404280"
  },
  {
    "text": "be good to sort of like uh check you know where where we were with our predictions absolutely yeah for sure and",
    "start": "2404280",
    "end": "2410200"
  },
  {
    "text": "hopefully sooner than that as well it's been an absolute pleasure to talk through these things with you and as for",
    "start": "2410200",
    "end": "2417000"
  },
  {
    "text": "myself and I'm guessing Chris as well we we appreciate your work in this space",
    "start": "2417000",
    "end": "2422319"
  },
  {
    "text": "and pushing these ideas forward and being that voice that has occas awkward",
    "start": "2422319",
    "end": "2428079"
  },
  {
    "text": "conversations at AI conferences it's much needed and appreciate your perspective so thanks for joining us",
    "start": "2428079",
    "end": "2434280"
  },
  {
    "text": "awesome guys thank you all right that's practical AI for",
    "start": "2434280",
    "end": "2440800"
  },
  {
    "text": "this week thanks for listening if this is your first time with us subscribe now at practical ai. FM or simply search for",
    "start": "2440800",
    "end": "2448520"
  },
  {
    "text": "practical AI in your favorite podcast app we're in there and if you're a longtime listener do us a solid by",
    "start": "2448520",
    "end": "2454319"
  },
  {
    "text": "recommending the show to a friend word of mouth is still the number one way people find new podcasts they love",
    "start": "2454319",
    "end": "2460599"
  },
  {
    "text": "special thanks to our partners for supporting our work fastly launch darkley and lenoe we appreciate it and",
    "start": "2460599",
    "end": "2466319"
  },
  {
    "text": "to the mysterious break master cylinder for cranking out new beats for us all the time that's all for now we'll talk",
    "start": "2466319",
    "end": "2472200"
  },
  {
    "text": "to you again next [Music]",
    "start": "2472200",
    "end": "2486070"
  },
  {
    "text": "week [Music]",
    "start": "2486240",
    "end": "2497949"
  },
  {
    "text": "k",
    "start": "2498880",
    "end": "2501880"
  }
]