[
  {
    "text": "[Music]",
    "start": "340",
    "end": "5839"
  },
  {
    "text": "welcome to practical AI if you work in artificial intelligence aspire to or are",
    "start": "7160",
    "end": "13799"
  },
  {
    "text": "curious how air related Technologies are changing the world this is the show for you thank you to our partners at fastly",
    "start": "13799",
    "end": "21160"
  },
  {
    "text": "for shipping all of our pods super fast to wherever you listen check them out at",
    "start": "21160",
    "end": "26240"
  },
  {
    "text": "fast.com and to our friends that fly deploy your app and database close to",
    "start": "26240",
    "end": "31359"
  },
  {
    "text": "your users no Ops required learn more at",
    "start": "31359",
    "end": "37000"
  },
  {
    "text": "[Music]",
    "start": "37160",
    "end": "42680"
  },
  {
    "text": "fly.io welcome to another episode of practical AI this is Daniel whack I'm a",
    "start": "42680",
    "end": "48920"
  },
  {
    "text": "data scientist building a tool called prediction guard and I am not joined",
    "start": "48920",
    "end": "54239"
  },
  {
    "text": "today by uh my co-host Chris but I am joined by an amazing guest who is an",
    "start": "54239",
    "end": "60760"
  },
  {
    "text": "expert in all things model optimization and efficiency and running on CPUs which",
    "start": "60760",
    "end": "66880"
  },
  {
    "text": "is super exciting I've got Mark CTS who's director of machine learning at",
    "start": "66880",
    "end": "72119"
  },
  {
    "text": "neural magic welcome mark thank you Daniel thanks for having me on yeah yeah",
    "start": "72119",
    "end": "77439"
  },
  {
    "text": "of course so um let's maybe just uh start out with a kind of like state of",
    "start": "77439",
    "end": "84799"
  },
  {
    "text": "model optimization right now so first off could you kind of describe like when",
    "start": "84799",
    "end": "90159"
  },
  {
    "text": "you're talking about model optimization or or that set of tooling what do you",
    "start": "90159",
    "end": "96880"
  },
  {
    "text": "mean by that and like how does that fit within maybe the things that a data",
    "start": "96880",
    "end": "102040"
  },
  {
    "text": "scientist or an AI person would want to do so whenever we're looking at model optimization we're usually focused on a",
    "start": "102040",
    "end": "109320"
  },
  {
    "text": "few different techniques but the ultimate goal is to make the overall model smaller and faster right neural",
    "start": "109320",
    "end": "115920"
  },
  {
    "text": "networks known to be very large models especially compared to more traditional machine learning and it turns out the",
    "start": "115920",
    "end": "122880"
  },
  {
    "text": "size of those models is um the important part in terms of exploring a large dimensionality of a space but it",
    "start": "122880",
    "end": "129679"
  },
  {
    "text": "actually doesn't use all those Pathways at inference time so what we specialize in is specifically pruning where we're",
    "start": "129679",
    "end": "136400"
  },
  {
    "text": "going to remove connections within that Network quantization where we're going to reduce Precision of those uh",
    "start": "136400",
    "end": "143440"
  },
  {
    "text": "Connections in the network so going from you know the typical fp32 down to ntate and then additionally distillation where",
    "start": "143440",
    "end": "149959"
  },
  {
    "text": "we're taking larger models and trying to teach a smaller model to mimic the",
    "start": "149959",
    "end": "155959"
  },
  {
    "text": "capability and the functionality of that larger model so it's kind of you know overall a high level and yeah it's h",
    "start": "155959",
    "end": "162560"
  },
  {
    "text": "it's a very exciting space right now it's uh kind of exponential in terms of the number of research papers that are",
    "start": "162560",
    "end": "168120"
  },
  {
    "text": "constantly coming out on the topic everybody's very excited about sparsity specifically mainly because you can turn",
    "start": "168120",
    "end": "174280"
  },
  {
    "text": "these large models and uh get rid of up to 95 even 97% of the weights are",
    "start": "174280",
    "end": "180760"
  },
  {
    "text": "actually useless in these obviously you can use that for a lot of efficiencies around performance and energy and that's",
    "start": "180760",
    "end": "187879"
  },
  {
    "text": "specifically where we've been focusing in at neural magic and what I've been focusing in on my work awesome yeah",
    "start": "187879",
    "end": "193319"
  },
  {
    "text": "that's uh I definitely have felt this problem so I'm sort of asking this",
    "start": "193319",
    "end": "198840"
  },
  {
    "text": "question maybe for others out there that that maybe haven't felt this problem as",
    "start": "198840",
    "end": "203920"
  },
  {
    "text": "much why is it important to like make models smaller or make them more",
    "start": "203920",
    "end": "209239"
  },
  {
    "text": "efficient how does that fit within what Enterprises or like even users running",
    "start": "209239",
    "end": "216159"
  },
  {
    "text": "smaller applications like why is that important for people I guess is the question generally there's going to be",
    "start": "216159",
    "end": "222680"
  },
  {
    "text": "two cases that we're looking at in terms of deployment one would be an embedded space where we're running on the edge",
    "start": "222680",
    "end": "229519"
  },
  {
    "text": "and trying to work there so generally you want real-time latency and optimizing the accuracy as best as",
    "start": "229519",
    "end": "235720"
  },
  {
    "text": "possible so if you're using an object detection model you want to make sure that you know for example you're on a security camera trying to draw object",
    "start": "235720",
    "end": "242040"
  },
  {
    "text": "detection and make sure that you know when a person walks in a frame and whether or not that's alarming or not",
    "start": "242040",
    "end": "248079"
  },
  {
    "text": "versus a dog or something like that so in General on that edge application what you can do is use a larger model remove",
    "start": "248079",
    "end": "256680"
  },
  {
    "text": "a lot of the pieces from that larger model so you can keep the accuracy of the larger model but take up the space",
    "start": "256680",
    "end": "262520"
  },
  {
    "text": "of the smaller model so significant Improvement in terms of accuracy on that edge device while still uh maintaining",
    "start": "262520",
    "end": "269840"
  },
  {
    "text": "you know the constraints that were set for you in terms of memory and latency that you need to request back with and",
    "start": "269840",
    "end": "276479"
  },
  {
    "text": "then the second one would be on the server side and that's generally where we're looking at you know more",
    "start": "276479",
    "end": "282880"
  },
  {
    "text": "throughput based applications and potentially also wait and see if they're shipping the data up to uh some server",
    "start": "282880",
    "end": "289680"
  },
  {
    "text": "to be processed either on NLP or um a computer vision but overall there what",
    "start": "289680",
    "end": "295840"
  },
  {
    "text": "we're looking at is um especially whenever people get into larger deployments on ML and neural networks",
    "start": "295840",
    "end": "303639"
  },
  {
    "text": "the cost significantly shifts not from training but to deployment you're uh for",
    "start": "303639",
    "end": "309400"
  },
  {
    "text": "a lot of larger Enterprises that are actively deploy 80 90% of their costs is",
    "start": "309400",
    "end": "315039"
  },
  {
    "text": "purely in deployment on these machines so what you can do is take the exact same model that you have reduce again",
    "start": "315039",
    "end": "322880"
  },
  {
    "text": "the amount of a compute that you need to run it so that one it'll run faster but two ultimately what that means is that",
    "start": "322880",
    "end": "329160"
  },
  {
    "text": "it's going to run significantly cheaper right and we have cost savings on the order of you know 10x 20x even larger if",
    "start": "329160",
    "end": "336759"
  },
  {
    "text": "you're really trying to specialize and optimize so there can be a significant reduction once you're at that scale I",
    "start": "336759",
    "end": "342759"
  },
  {
    "text": "would say definitely if you know you don't have anything deployed yet don't worry about optimizing the model worry",
    "start": "342759",
    "end": "348240"
  },
  {
    "text": "about getting a use case that works and something that you can prove out as soon as you go into deployment model",
    "start": "348240",
    "end": "353720"
  },
  {
    "text": "optimization is a great thing to start because it's essentially just free performance that's left on the table",
    "start": "353720",
    "end": "360039"
  },
  {
    "text": "that can significantly affect your bottom line we've mostly been talking about kind of like model size and",
    "start": "360039",
    "end": "367680"
  },
  {
    "text": "optimizations and I do want to get sort of down and get into the nerdy stuff around like how some of this works but",
    "start": "367680",
    "end": "374479"
  },
  {
    "text": "before we do that I'm also curious about this element of deployment on gpus",
    "start": "374479",
    "end": "381759"
  },
  {
    "text": "versus CPUs it seems like some of what's indicated at least in like the tooling",
    "start": "381759",
    "end": "387599"
  },
  {
    "text": "that you're building is like the PO potential to take a large model which might require a GPU at inference time",
    "start": "387599",
    "end": "395919"
  },
  {
    "text": "and potentially run that on cheaper commodity Hardware that only has a CPU",
    "start": "395919",
    "end": "401800"
  },
  {
    "text": "maybe doesn't have a GPU like what is the state of that now and like how far",
    "start": "401800",
    "end": "407400"
  },
  {
    "text": "can you push that or maybe also like how could people best think about that in",
    "start": "407400",
    "end": "413639"
  },
  {
    "text": "terms of like when and when that might not be possible I guess as you said uh",
    "start": "413639",
    "end": "419039"
  },
  {
    "text": "we special I almost entirely on CPU performance and in that uh actually our",
    "start": "419039",
    "end": "425000"
  },
  {
    "text": "latest uh ml imprints results on ML perf has uh have come out so in that we show",
    "start": "425000",
    "end": "432879"
  },
  {
    "text": "that we're running faster than t4s and a40s and things like that on just",
    "start": "432879",
    "end": "437960"
  },
  {
    "text": "commodity CPUs so server based CPUs stuff that you have in your laptop desktop things like that and it's very",
    "start": "437960",
    "end": "444560"
  },
  {
    "text": "surprising that you know these what's thought of as these little CPUs can outperform the GPU and we see this",
    "start": "444560",
    "end": "452240"
  },
  {
    "text": "generally across every domain that we've tackled and that's been across image classification object detection excuse",
    "start": "452240",
    "end": "458720"
  },
  {
    "text": "me uh segmentation and now we're working in the NLP and nlg space and um activ",
    "start": "458720",
    "end": "464599"
  },
  {
    "text": "coming out with that but overall we're seeing the same use case where these models are overparameterized we can take",
    "start": "464599",
    "end": "471520"
  },
  {
    "text": "away a lot of that compute and what that means is that you can actually get the CPU and the GPU about equivalent in",
    "start": "471520",
    "end": "477599"
  },
  {
    "text": "terms of compute throughput because with the sparcity and the dynamic setup of CPUs we can run and Skip all those zero",
    "start": "477599",
    "end": "485840"
  },
  {
    "text": "multiplications right so significant reduction compute they're about even but then the CPU has a unique cache",
    "start": "485840",
    "end": "491680"
  },
  {
    "text": "hierarchy which means that we can reuse that cache more often than what you can get on a GPU L1 and L2 being extremely",
    "start": "491680",
    "end": "499240"
  },
  {
    "text": "quick faster than gpu's main memory and L3 being about equivalent so overall what we do on our performance",
    "start": "499240",
    "end": "505479"
  },
  {
    "text": "optimization is Skip all the compute to get even and then use that cash hierarchy as efficiently as possible on",
    "start": "505479",
    "end": "512479"
  },
  {
    "text": "the CPUs so we can get faster memory access than even you can get on a GPU and we pay a little bit more by doing a",
    "start": "512479",
    "end": "518560"
  },
  {
    "text": "little bit more compute by doing that but overall it works out that you can actually beat the gpus in that setup with just pure software I anticipate",
    "start": "518560",
    "end": "525959"
  },
  {
    "text": "this is maybe a question that you get sometimes but it's like you hear so much",
    "start": "525959",
    "end": "531120"
  },
  {
    "text": "about the necessity of gpus for running these large models do you find generally",
    "start": "531120",
    "end": "539120"
  },
  {
    "text": "practi practitioners are just unaware of this like possibility of running these",
    "start": "539120",
    "end": "545040"
  },
  {
    "text": "large models on on CPUs and like how has that been for you and those that you",
    "start": "545040",
    "end": "550920"
  },
  {
    "text": "work with that are actually doing this amazing work and have these awesome tools like how has that been in terms of",
    "start": "550920",
    "end": "557480"
  },
  {
    "text": "overcoming that barrier of perception that people have it was a barrier that",
    "start": "557480",
    "end": "562600"
  },
  {
    "text": "we hit uh especially early on a couple years back where it took a lot of convincing to even get in the door to",
    "start": "562600",
    "end": "569800"
  },
  {
    "text": "talk to anyone because they just didn't believe what we were saying now it's gotten to be quite a bit better",
    "start": "569800",
    "end": "574839"
  },
  {
    "text": "especially with the the newer software that's been pushed out of the newer chipsets on CPUs they're getting a",
    "start": "574839",
    "end": "581079"
  },
  {
    "text": "little bit more even in terms of gpus so it's you know within a stone's throw to try and match uh GPU so people are a",
    "start": "581079",
    "end": "588160"
  },
  {
    "text": "little bit more accepting of it but uh yeah whenever we show them the numbers",
    "start": "588160",
    "end": "593600"
  },
  {
    "text": "generally the first reaction is well let me try that on my Hardware cuz you guys",
    "start": "593600",
    "end": "598920"
  },
  {
    "text": "got to be doing something weird let me let me try to do that let me replicate that and as soon as they do then the",
    "start": "598920",
    "end": "604399"
  },
  {
    "text": "next question is okay well how do I do this to my model and that's usually where the tricky part comes in is model",
    "start": "604399",
    "end": "610880"
  },
  {
    "text": "optimization hasn't always been the easiest thing to do uh it can take a lot of research to enable new architectures",
    "start": "610880",
    "end": "617760"
  },
  {
    "text": "and things like that that's what we've been also specializing on at neural magic is making all the research that",
    "start": "617760",
    "end": "622880"
  },
  {
    "text": "we're doing being able to put that into open source and also building out a SAS platform on top of it so everyone can um",
    "start": "622880",
    "end": "630200"
  },
  {
    "text": "easily play with Hyper parameters and get something that is consumable but I would say that's probably been the",
    "start": "630200",
    "end": "635800"
  },
  {
    "text": "biggest Gap in terms of trying to get people off of gpus onto CPUs is the",
    "start": "635800",
    "end": "641399"
  },
  {
    "text": "model optimization that needs to take place first to be able to run faster than the gpus you talked a little bit",
    "start": "641399",
    "end": "647160"
  },
  {
    "text": "about sparcity which I want to dive into over time and I want to get to those tools and the open source stuff you",
    "start": "647160",
    "end": "653959"
  },
  {
    "text": "mentioned like these large models and people are probably used to hearing these numbers you know 3 billion 7",
    "start": "653959",
    "end": "661600"
  },
  {
    "text": "billion 13 billion like however and up from there like these models have in terms of numbers of parameters could you",
    "start": "661600",
    "end": "668079"
  },
  {
    "text": "describe a little bit what you mean when you say 90 to 95% of these connections",
    "start": "668079",
    "end": "674120"
  },
  {
    "text": "or you know maybe less than that but a high percentage in some models have no",
    "start": "674120",
    "end": "679760"
  },
  {
    "text": "impact on the actual forward pass or inference in the model could you",
    "start": "679760",
    "end": "684959"
  },
  {
    "text": "describe a little bit more by what you mean by that yeah definitely and I'll I'll take two steps to doing that one is",
    "start": "684959",
    "end": "690920"
  },
  {
    "text": "just covering kind of the 90 95% class at least where we've been able to get to on those and the second is looking",
    "start": "690920",
    "end": "697079"
  },
  {
    "text": "specifically at large language models so for the first one whenever we're looking at getting rid of 95% of the weights",
    "start": "697079",
    "end": "703279"
  },
  {
    "text": "let's take res at 50 as an example this is our toy Benchmark model this is essentially what it uh we prove out all",
    "start": "703279",
    "end": "709440"
  },
  {
    "text": "of our technology on because it's a common feature in ml perf and for most performance tests so what we can do",
    "start": "709440",
    "end": "715680"
  },
  {
    "text": "coming in is looking at those uh convolutional layers it has I forget how",
    "start": "715680",
    "end": "720880"
  },
  {
    "text": "many million parameters within it but it's definitely not the 3 billion 7 billion or up uh on top of that but",
    "start": "720880",
    "end": "727440"
  },
  {
    "text": "within that we can actually zero out so what we're doing is taking all imagine taking all those parameters dumping them",
    "start": "727440",
    "end": "733040"
  },
  {
    "text": "into a giant array and we're just going to zero out the ones that are not",
    "start": "733040",
    "end": "738240"
  },
  {
    "text": "important and figuring out the ones that are not important is part of the research the easiest assumption is just",
    "start": "738240",
    "end": "745160"
  },
  {
    "text": "saying that the weights that are the largest are the ones that you want to keep so the ones that are furthest from",
    "start": "745160",
    "end": "750279"
  },
  {
    "text": "zero are the ones that you want to keep generally you can think of this in two ways one is that as the model is training and being regularized the",
    "start": "750279",
    "end": "757279"
  },
  {
    "text": "weights that don't matter are going to move toward zero and then the other thing is during that forwards pass the",
    "start": "757279",
    "end": "762560"
  },
  {
    "text": "weights that are higher magnitude have more an effect on the output right and everything else is going to be noise in",
    "start": "762560",
    "end": "768560"
  },
  {
    "text": "between so we're able to essentially get rid of just our what whenever I say get rid of I mean setting those parameters",
    "start": "768560",
    "end": "775480"
  },
  {
    "text": "to zero within 95% of them so you're left with 5% of your weights that are nonzero that's actually all that you",
    "start": "775480",
    "end": "781760"
  },
  {
    "text": "need to preserve the accuracy um on imet for res 50 for example and some quick",
    "start": "781760",
    "end": "789320"
  },
  {
    "text": "kind of intuition in terms of how I've been able to think about this and why it works and things like that we can see as",
    "start": "789320",
    "end": "795240"
  },
  {
    "text": "we increase the size of our um dimensionality in our optimization space",
    "start": "795240",
    "end": "800760"
  },
  {
    "text": "what we're doing is and there's few research papers out on it they were able to connect more of the local mens right",
    "start": "800760",
    "end": "807160"
  },
  {
    "text": "so the optimization process will slowly converge further and further down because more of the local mens are",
    "start": "807160",
    "end": "813120"
  },
  {
    "text": "connected generally though there's only a few of those Pathways that you actually need to connect those local",
    "start": "813120",
    "end": "818440"
  },
  {
    "text": "mens so all that we're doing is we're following down that most optimized pathway and removing everything else",
    "start": "818440",
    "end": "823800"
  },
  {
    "text": "around us in terms of that dimensionality so it's kind of one of those things that as you're training it's slowly selecting the weights that",
    "start": "823800",
    "end": "830480"
  },
  {
    "text": "matter that gets you down to that local men and there's very few so the important part was that large",
    "start": "830480",
    "end": "835600"
  },
  {
    "text": "dimensionality of the optimization space but not every direction matters right so then we can get rid of it and then",
    "start": "835600",
    "end": "841399"
  },
  {
    "text": "diving in on the llm side and large language models we actually have a uh",
    "start": "841399",
    "end": "847959"
  },
  {
    "text": "recent paper that came out from one of our principal research scientists Dan Alistar uh called Spar GPT and that's",
    "start": "847959",
    "end": "855720"
  },
  {
    "text": "where we're looking at taking opt and blo models all the way up to 175 billion",
    "start": "855720",
    "end": "861600"
  },
  {
    "text": "parameters and being able to optimize those and remove as many weights as possible all in this case in one shot so",
    "start": "861600",
    "end": "869160"
  },
  {
    "text": "just using the model without any retraining we're able to get rid of around 60% of the weights without doing",
    "start": "869160",
    "end": "874959"
  },
  {
    "text": "anything and there's a new paper out of cerebrus actually that was looking at the llm story and they're able now to",
    "start": "874959",
    "end": "881959"
  },
  {
    "text": "get to 80% sparsity on these llms with retraining so that's kind of the",
    "start": "881959",
    "end": "887519"
  },
  {
    "text": "research direction that we're headed down now is proving out how optimized we can make these models because there's",
    "start": "887519",
    "end": "893399"
  },
  {
    "text": "also a lot of interesting stuff that happens with the large language models specifically because it's generating one",
    "start": "893399",
    "end": "899600"
  },
  {
    "text": "token at a time very latency bound and that means that it's a lot of memory access to load those weights so if you",
    "start": "899600",
    "end": "906480"
  },
  {
    "text": "can quantize those and then get rid of half of them you're already at you know anywhere from a 4 to 6X speed up just on",
    "start": "906480",
    "end": "914199"
  },
  {
    "text": "your inference times and that's generally where we're uh where we're focused and looking at currently to try and get those llms to run faster the",
    "start": "914199",
    "end": "920959"
  },
  {
    "text": "other thing to call out for those too is you know 7 billion parameters and 175 billion parameters those don't fit in a",
    "start": "920959",
    "end": "927759"
  },
  {
    "text": "single GPU so now you have you know clusters of gpus to serve one model and",
    "start": "927759",
    "end": "933600"
  },
  {
    "text": "a lot of that compute is just completely wasted because all that it's going to is trying to maximize the memory on the",
    "start": "933600",
    "end": "940440"
  },
  {
    "text": "gpus for CPUs you can throw a few terabytes on there and it works out fine so that's the other thing to call out",
    "start": "940440",
    "end": "947120"
  },
  {
    "text": "with the llms in terms of GPU versus",
    "start": "947120",
    "end": "951519"
  },
  {
    "text": "[Music] CPU",
    "start": "955050",
    "end": "960920"
  },
  {
    "text": "this is really interesting Mark I want to follow up on what you were just talking about which I think is a really",
    "start": "961519",
    "end": "967440"
  },
  {
    "text": "it's sort of a subtle point but it's really interesting in that I think if I understood you right in what you're",
    "start": "967440",
    "end": "973639"
  },
  {
    "text": "saying like let's say that I have one of these large models 175 billion parameters or whatever and even for",
    "start": "973639",
    "end": "980319"
  },
  {
    "text": "inference I have the necessity to have multiple gpus just to load that model",
    "start": "980319",
    "end": "986000"
  },
  {
    "text": "into the memory of the of the cards whereas on a CPU you can have terabytes",
    "start": "986000",
    "end": "993639"
  },
  {
    "text": "of memory what I'm assuming is like you could load that in as long as you're able to execute it quickly which I guess",
    "start": "993639",
    "end": "1000600"
  },
  {
    "text": "is the other piece so am I right you sort of have to have both like the ability to load it into memory and you",
    "start": "1000600",
    "end": "1008839"
  },
  {
    "text": "have a bit more space in that on the CPU side but then you also have to be able to execute it very quickly which I guess",
    "start": "1008839",
    "end": "1015959"
  },
  {
    "text": "is like why you would think about both space and sparity is that an accurate",
    "start": "1015959",
    "end": "1022360"
  },
  {
    "text": "way to put it as you said you know you have a a total space you need to take up and then a minimum latency that you want",
    "start": "1022360",
    "end": "1028360"
  },
  {
    "text": "to respond to the user out right and that's going to set the constraints for your hardware and for CPUs currently at",
    "start": "1028360",
    "end": "1035160"
  },
  {
    "text": "least for the smaller models uh you can get to a usable speed on those if you've seen like llama CPP they're doing in4",
    "start": "1035160",
    "end": "1042280"
  },
  {
    "text": "and things like that on smaller models and they're usable but they're less accurate so we're trying to do and what",
    "start": "1042280",
    "end": "1047438"
  },
  {
    "text": "we're actively working on right now is making sure that we can get that GPU class speed while maintaining the large",
    "start": "1047439",
    "end": "1053480"
  },
  {
    "text": "memory advantage of CPUs so you can deploy this 175 billion parameter model",
    "start": "1053480",
    "end": "1058720"
  },
  {
    "text": "on something local and you don't have to worry about data privacy anything like that it's just there working and",
    "start": "1058720",
    "end": "1064559"
  },
  {
    "text": "available and highly accurate for you I definitely heard people that I've talked to who have like tried various",
    "start": "1064559",
    "end": "1072480"
  },
  {
    "text": "optimization techniques and have maybe been dissatisfied with the performance",
    "start": "1072480",
    "end": "1077840"
  },
  {
    "text": "hit that they're getting not in terms of compute but in terms of like actual model performance or accuracy or",
    "start": "1077840",
    "end": "1084320"
  },
  {
    "text": "whatever does that performance hit often come about because of the quantization",
    "start": "1084320",
    "end": "1091480"
  },
  {
    "text": "that's maybe part of the optimization techniques or are there multiple sources",
    "start": "1091480",
    "end": "1096840"
  },
  {
    "text": "of that like performance hit how how should people think about that I think the biggest thing there is honestly just",
    "start": "1096840",
    "end": "1104080"
  },
  {
    "text": "the amount of choices that people have to apply and not knowing when to apply",
    "start": "1104080",
    "end": "1109559"
  },
  {
    "text": "them because generally for quantization for example you can apply quantization to pretty much anything at 8 for both",
    "start": "1109559",
    "end": "1118120"
  },
  {
    "text": "activations and weights and have it recover but there are definitely cases",
    "start": "1118120",
    "end": "1123720"
  },
  {
    "text": "where for example we've been quantizing efficient that's quite a bit on our image classification side there's one or",
    "start": "1123720",
    "end": "1129080"
  },
  {
    "text": "two layers in some of these that are extremely sensitive for whatever reason to quantization that you can't quantize",
    "start": "1129080",
    "end": "1135200"
  },
  {
    "text": "those so removing those then you get 100% recovery right so a lot of these kind of little things that researchers",
    "start": "1135200",
    "end": "1142039"
  },
  {
    "text": "know intuitively in terms of having use this constantly what will work what won't but that's not really coded into",
    "start": "1142039",
    "end": "1148720"
  },
  {
    "text": "software anywhere right to make it easy for people to use so generally they'll go through try and quantize and there's",
    "start": "1148720",
    "end": "1154880"
  },
  {
    "text": "no feedback loop there's no methodology it's just hey I was able to apply it in one shot but it lost 5% accuracy talking",
    "start": "1154880",
    "end": "1163039"
  },
  {
    "text": "on quantization but if you do you know a quantization we training scheme generally you'll recover all that back",
    "start": "1163039",
    "end": "1169440"
  },
  {
    "text": "and uh It generally works completely for that same thing on pruning pruning you'll definitely see more of a drop and",
    "start": "1169440",
    "end": "1175360"
  },
  {
    "text": "it's much more of requirement to do training aware on the pruning side at least to get to really high sparsities",
    "start": "1175360",
    "end": "1181840"
  },
  {
    "text": "but you definitely will see this kind of the choices that are made in the hyperparameters that are chosen those",
    "start": "1181840",
    "end": "1188240"
  },
  {
    "text": "can significantly affect the recovery and the quality so generally I'd say you know if they were seeing drops in",
    "start": "1188240",
    "end": "1194000"
  },
  {
    "text": "performance it's primarily because of those choices and those issues and just the why breath that's available right",
    "start": "1194000",
    "end": "1200360"
  },
  {
    "text": "now and not knowing how to narrow it down and that's you know what we're actively working on does that make sense",
    "start": "1200360",
    "end": "1205960"
  },
  {
    "text": "yeah yeah that that's good for people to myself I I want to develop a little bit",
    "start": "1205960",
    "end": "1211159"
  },
  {
    "text": "more intuition around these things because like you say sometimes you just like oh here's the command that I run on",
    "start": "1211159",
    "end": "1217840"
  },
  {
    "text": "the command line and like I get this file out that is smaller right but I",
    "start": "1217840",
    "end": "1223640"
  },
  {
    "text": "don't have a great intuition about like similar to hyperparameter tuning right like that takes time to figure out like",
    "start": "1223640",
    "end": "1231240"
  },
  {
    "text": "okay how should I think about changing my learning rate if this happens or if that happens that sort of thing you",
    "start": "1231240",
    "end": "1237760"
  },
  {
    "text": "mentioned one thing which I think would also be good to kind of clarify and help people understand is like training aware",
    "start": "1237760",
    "end": "1245000"
  },
  {
    "text": "optimization versus just non-ra I guess optimization or or I don't know what the",
    "start": "1245000",
    "end": "1251720"
  },
  {
    "text": "counter to that is could you talk about those like how they're differentiated some people might guess what that means",
    "start": "1251720",
    "end": "1258039"
  },
  {
    "text": "but like how are they differentiated and how does that work out in practice in terms of how you would optimize a model",
    "start": "1258039",
    "end": "1264480"
  },
  {
    "text": "technically we have three categories generally available one and the two you're going through is one training",
    "start": "1264480",
    "end": "1270320"
  },
  {
    "text": "aware um and then we have post trining or oneshot which are kind of interchangeable and then We additionally",
    "start": "1270320",
    "end": "1275960"
  },
  {
    "text": "have on sparse transfer which is something that we've been pushing a lot because the research has worked out quite a bit for it so I'll cover all",
    "start": "1275960",
    "end": "1282559"
  },
  {
    "text": "three of those and a little bit more depth so for training aware what we're doing is taking the exact same model",
    "start": "1282559",
    "end": "1289440"
  },
  {
    "text": "that you're wanting to deploy in the exact same data set it was trained on and continuing the training process",
    "start": "1289440",
    "end": "1294640"
  },
  {
    "text": "further so while we're continuing that training process we could be continuing it this is where the hyper parameters",
    "start": "1294640",
    "end": "1300159"
  },
  {
    "text": "come in but generally it'll be about half the time that originally took to train it we'll train it for that much",
    "start": "1300159",
    "end": "1306520"
  },
  {
    "text": "longer and as we're doing that training we're iteratively pruning away or we're",
    "start": "1306520",
    "end": "1311600"
  },
  {
    "text": "applying quantization or both and uh the reason we're continuing that training is because as we're iteratively applying",
    "start": "1311600",
    "end": "1317520"
  },
  {
    "text": "these optimizations we're slowly moving the model away from its local men right",
    "start": "1317520",
    "end": "1322880"
  },
  {
    "text": "and it has to adapt and adjust back so by slowly doing that and training over some time we allow small jumps that the",
    "start": "1322880",
    "end": "1330000"
  },
  {
    "text": "optimizer can recover from and adjust the remaining weights for rather than doing it all at once and the all at once",
    "start": "1330000",
    "end": "1336960"
  },
  {
    "text": "piece is where we get into posttraining in one shot where we're not going to try and retrain the model at all we're going",
    "start": "1336960",
    "end": "1342000"
  },
  {
    "text": "to take a small calibration data set and then uh we're going to use some heris",
    "start": "1342000",
    "end": "1347120"
  },
  {
    "text": "sixs or some her istics or algorithms to uh figure out using that caliberation",
    "start": "1347120",
    "end": "1352360"
  },
  {
    "text": "data set how to optimize that model and remove weights or quantize so the most",
    "start": "1352360",
    "end": "1358760"
  },
  {
    "text": "common case would be static quantization where you're using a calibration data set to figure out the activation ranges",
    "start": "1358760",
    "end": "1366440"
  },
  {
    "text": "for each layer right and once you have the activation ranges for each layer you can set up a simple quantization scheme",
    "start": "1366440",
    "end": "1373480"
  },
  {
    "text": "to say given that it's going from this layer is going from -6 to 6 now I need to fit that range into an intake scale",
    "start": "1373480",
    "end": "1381159"
  },
  {
    "text": "of 0 to 255 right and be able to map that in so that would be a simple post",
    "start": "1381159",
    "end": "1386480"
  },
  {
    "text": "trining or One-Shot application and then the final one is sparse transfer which",
    "start": "1386480",
    "end": "1392400"
  },
  {
    "text": "works exactly the same as um transfer learning or fine tuning it's just we're starting with a sparse model to run",
    "start": "1392400",
    "end": "1399559"
  },
  {
    "text": "through and that's a lot of what we've pushed up in Earl magic into our sparse Zoo are um these open source sparse",
    "start": "1399559",
    "end": "1405679"
  },
  {
    "text": "models that we have you know sparse Birds and reset 50s and YOLO v5s things",
    "start": "1405679",
    "end": "1411720"
  },
  {
    "text": "like that and you can just take those plug in your data set and transfer over to it so the sparity math stays in place",
    "start": "1411720",
    "end": "1418520"
  },
  {
    "text": "and it just adjusts the remaining weights to fit your data set we have a few papers out on that as well that",
    "start": "1418520",
    "end": "1424760"
  },
  {
    "text": "shows that sparse transfer works just as well as regular transfer yeah that's",
    "start": "1424760",
    "end": "1430400"
  },
  {
    "text": "really interesting um I guess this would somewhat depend too like if I'm just",
    "start": "1430400",
    "end": "1435480"
  },
  {
    "text": "thinking of like the average practitioner out there right probably in",
    "start": "1435480",
    "end": "1441760"
  },
  {
    "text": "a lot of the space that I work in in like the larger language Model area then",
    "start": "1441760",
    "end": "1448440"
  },
  {
    "text": "like I'm not going to be able to retrain one of these large models on the original data set right or even half of",
    "start": "1448440",
    "end": "1456120"
  },
  {
    "text": "that or for half of the EPO or whatever um but these other things are certainly things that I do all the time right like",
    "start": "1456120",
    "end": "1462919"
  },
  {
    "text": "fine-tuning transfer learning so it's cool to understand that there's options",
    "start": "1462919",
    "end": "1468880"
  },
  {
    "text": "out there am I correct in assuming that like you mentioned the the zoo um the",
    "start": "1468880",
    "end": "1474240"
  },
  {
    "text": "sparse zoo that people can find on your website and we'll Link in here too um",
    "start": "1474240",
    "end": "1479679"
  },
  {
    "text": "that like researchers your team practitioners who whoever are the people",
    "start": "1479679",
    "end": "1485320"
  },
  {
    "text": "out there are also putting in work to actually release some of these sparse models publicly to the community so that",
    "start": "1485320",
    "end": "1493480"
  },
  {
    "text": "I can take those and then maybe do fine tuning on that or maybe it's just good enough from the what's released uh in",
    "start": "1493480",
    "end": "1499559"
  },
  {
    "text": "the community could you tell us a little bit about that community and like what's being released so on the open source",
    "start": "1499559",
    "end": "1506640"
  },
  {
    "text": "side pretty much everything that we have on The Spar suit currently has either been from our lab we have a few that are",
    "start": "1506640",
    "end": "1513480"
  },
  {
    "text": "from Intel's lab up as well and some hugging face examples and things like that primarily because a lot of the",
    "start": "1513480",
    "end": "1521039"
  },
  {
    "text": "sparsification research is all built around a few models like resent 50 Bert",
    "start": "1521039",
    "end": "1526399"
  },
  {
    "text": "and things like that they don't expand it out pass those models to prove out their algorithms so we have the best of",
    "start": "1526399",
    "end": "1531679"
  },
  {
    "text": "those and then yeah that's exactly what our team is working on and well we get some Community contributions in every",
    "start": "1531679",
    "end": "1537720"
  },
  {
    "text": "once in a while for sparse models that people have generated or transferred right so our goal is to be able to push",
    "start": "1537720",
    "end": "1543039"
  },
  {
    "text": "these up so that as you said anyone can come in from the community and be able to pull those down and get value out of",
    "start": "1543039",
    "end": "1550200"
  },
  {
    "text": "those models so you can think of them as sparse foundational models rather than the dense foundational",
    "start": "1550200",
    "end": "1557039"
  },
  {
    "text": "models [Music]",
    "start": "1557240",
    "end": "1570269"
  },
  {
    "text": "Mark in addition to the sparse Zoo which is really cool I I know that neural",
    "start": "1571000",
    "end": "1576200"
  },
  {
    "text": "magic is producing some pretty uh interesting and useful tooling otherwise",
    "start": "1576200",
    "end": "1581520"
  },
  {
    "text": "as well in terms of actually doing some of this optimization themselves and I",
    "start": "1581520",
    "end": "1586679"
  },
  {
    "text": "see uh certain things sparse sparse andl could you describe a little bit about if",
    "start": "1586679",
    "end": "1592360"
  },
  {
    "text": "I'm a practitioner I have a model and I want to do optimization what does that",
    "start": "1592360",
    "end": "1599360"
  },
  {
    "text": "look like for me right now with the tooling that's available so we have sparse and melt which is our open source",
    "start": "1599360",
    "end": "1606080"
  },
  {
    "text": "model optimization framework it's built primarily on top of a pytorch and then",
    "start": "1606080",
    "end": "1611520"
  },
  {
    "text": "we have Integrations with torch Vision hugging face YOLO ultral YOLO V5 pretty",
    "start": "1611520",
    "end": "1617480"
  },
  {
    "text": "much all the common repos that most people are using we've already integrated with so you can use our Integrations and just plug in your model",
    "start": "1617480",
    "end": "1624399"
  },
  {
    "text": "and go along with that and then the other part is that we have recipes and I'll go through recipes in a second but",
    "start": "1624399",
    "end": "1630200"
  },
  {
    "text": "we have those kind of pre-coded Integrations or you can create your own integration usually it only takes a few lines of code we've done all the hard",
    "start": "1630200",
    "end": "1636480"
  },
  {
    "text": "work in terms of making sure that whenever we want to optimize a model that you just have to wrap the optimizer",
    "start": "1636480",
    "end": "1642480"
  },
  {
    "text": "and P torch essentially wrap the model and the optimizer which is what our code handles and then it's going to go",
    "start": "1642480",
    "end": "1647840"
  },
  {
    "text": "through handle the optimization past that so there's no coding uh really from your side uh or from the practitioner",
    "start": "1647840",
    "end": "1654120"
  },
  {
    "text": "side on implementation and then the other part is then coming up with an optimization recipe that people want to",
    "start": "1654120",
    "end": "1660799"
  },
  {
    "text": "use and what that's going to lay out is saying that I want to prune from this Epoch to this Epoch for example and then",
    "start": "1660799",
    "end": "1666679"
  },
  {
    "text": "apply quantization and Target these layers and at this sparity level things like that we have automed ways to",
    "start": "1666679",
    "end": "1672960"
  },
  {
    "text": "generate those as well as examples on The Spar Su and um in other places that's gener what it would look like is",
    "start": "1672960",
    "end": "1679720"
  },
  {
    "text": "um you get up and running we definitely recommend checking out the sparo first seeing if there's anything that you can do to just transfer the model onto your",
    "start": "1679720",
    "end": "1686679"
  },
  {
    "text": "data set because those are the quickest and fastest ways and then otherwise if you do have a specific model",
    "start": "1686679",
    "end": "1692080"
  },
  {
    "text": "architecture that you're looking at then you can start going down this uh integration pathway and generating your",
    "start": "1692080",
    "end": "1697240"
  },
  {
    "text": "own recipes and that's the current state that we're at the other thing that I wanted to call out is that we're working",
    "start": "1697240",
    "end": "1702840"
  },
  {
    "text": "on a SF platform right now to make all of this more intuitive so you have a UI to be able to predict where that model",
    "start": "1702840",
    "end": "1709159"
  },
  {
    "text": "is going to end up before you start optimizing it and then additionally actively Benchmark it across your",
    "start": "1709159",
    "end": "1715039"
  },
  {
    "text": "different deployment scenarios so this is called sparsify we've had an old kind of alpha state for a while that was uh",
    "start": "1715039",
    "end": "1721640"
  },
  {
    "text": "downloadable and uh we're actually going through Alpha Testing right now so anyone that is interested in trying that",
    "start": "1721640",
    "end": "1728000"
  },
  {
    "text": "out definitely reach out we're going through Alpha currently looking to go to Beta and uh probably next month two",
    "start": "1728000",
    "end": "1733360"
  },
  {
    "text": "months and then uh GA following up after that so definitely check that out as well and yeah yeah that's great um yeah",
    "start": "1733360",
    "end": "1740679"
  },
  {
    "text": "I I love how you're thinking towards uh usability as well around these things because I I do see this as something",
    "start": "1740679",
    "end": "1748519"
  },
  {
    "text": "that kind of blocks people on optimization a lot because they get they get stuck like you say it's like well",
    "start": "1748519",
    "end": "1754720"
  },
  {
    "text": "what what recipe do I use here it seems like there's a million options like where do I start so that's really",
    "start": "1754720",
    "end": "1761360"
  },
  {
    "text": "awesome to hear just so people can understand like there's options available in The Spar Zoo I'm kind of",
    "start": "1761360",
    "end": "1768360"
  },
  {
    "text": "scrolling through there now there's a lot even ones that I know that I use like Dilbert which is fine tun on Squad",
    "start": "1768360",
    "end": "1776399"
  },
  {
    "text": "like I use that all the time for question answer and apparently I should use the sparse one because uh yeah that",
    "start": "1776399",
    "end": "1783320"
  },
  {
    "text": "would help out a lot both in terms of of compute and speed let's say that a model is not there um and in particular like",
    "start": "1783320",
    "end": "1790760"
  },
  {
    "text": "people of course are interested in like all of these things being rapidly released all the time so one of the",
    "start": "1790760",
    "end": "1797440"
  },
  {
    "text": "things I know know that I've seen in optimization platforms over time is like",
    "start": "1797440",
    "end": "1802519"
  },
  {
    "text": "it's hard to maybe support new architectures as they come out so um how",
    "start": "1802519",
    "end": "1808200"
  },
  {
    "text": "are you all approaching that and like what is the state of like being able to",
    "start": "1808200",
    "end": "1813440"
  },
  {
    "text": "be flexible with these optimization schemes for a variety of architectures",
    "start": "1813440",
    "end": "1820440"
  },
  {
    "text": "uh looking at the base framework that we have pushed out in smart smell for example everything's implemented such",
    "start": "1820440",
    "end": "1826440"
  },
  {
    "text": "that it at least it's supposed to be able to run with any model architecture uh so there's no real big assumptions on",
    "start": "1826440",
    "end": "1833200"
  },
  {
    "text": "there other than you have convolutions and you have linear layers inside of your model somewhere that we can Target",
    "start": "1833200",
    "end": "1838720"
  },
  {
    "text": "for optimizations so everything's set up very generically which means that whenever there is a new architecture",
    "start": "1838720",
    "end": "1844880"
  },
  {
    "text": "that comes out or new weights things like that it's going to take some time for you know us to tackle that and",
    "start": "1844880",
    "end": "1850960"
  },
  {
    "text": "that's if it gets onto the list of you know top used models things like that but that is a nice thing about having an",
    "start": "1850960",
    "end": "1857399"
  },
  {
    "text": "open source Community is that people are welcome to come in the tooling and the framework there should work out of the",
    "start": "1857399",
    "end": "1863559"
  },
  {
    "text": "box with everything and they're more than welcome to be able to commit or push up you know whatever they're",
    "start": "1863559",
    "end": "1869760"
  },
  {
    "text": "working on and we have an active uh slack community and GitHub uh Community for people to come in our Engineers are",
    "start": "1869760",
    "end": "1876600"
  },
  {
    "text": "actively on that they can come in look at it and easily get support on any issues they're running into awesome yeah",
    "start": "1876600",
    "end": "1883440"
  },
  {
    "text": "and we'll make sure uh for the listeners who are wanting to get plugged into this we'll make make sure and include the",
    "start": "1883440",
    "end": "1889080"
  },
  {
    "text": "links to the the slot group and the GitHub um in our show notes so make sure",
    "start": "1889080",
    "end": "1894600"
  },
  {
    "text": "you visit there and get plugged in and start optimizing your models as we kind",
    "start": "1894600",
    "end": "1900679"
  },
  {
    "text": "of uh get a little bit closer to the end here I'm wondering about a couple things",
    "start": "1900679",
    "end": "1906000"
  },
  {
    "text": "one is I know that like you're saying you're actively involved in research in this space um what trends are you seeing",
    "start": "1906000",
    "end": "1913960"
  },
  {
    "text": "in research around optimization and in particular like what are the directions that your team is sort of excited to go",
    "start": "1913960",
    "end": "1921080"
  },
  {
    "text": "into in the near future in terms of the research side of this so I would say",
    "start": "1921080",
    "end": "1926399"
  },
  {
    "text": "there's two big trends right now one is around the focus on post training our",
    "start": "1926399",
    "end": "1933360"
  },
  {
    "text": "principal research scientist Dan alart he came uh him along with his lab came up with this algorithm called OBC obq",
    "start": "1933360",
    "end": "1940760"
  },
  {
    "text": "which actually have a webinar which uh will have already aired by the time that this comes out uh it's airing tomorrow",
    "start": "1940760",
    "end": "1947240"
  },
  {
    "text": "but uh that algorithm specifically is where a lot of efforts going around which is requiring as little data as",
    "start": "1947240",
    "end": "1954200"
  },
  {
    "text": "possible and no retraining and trying to increase sparsity as much as possible so",
    "start": "1954200",
    "end": "1959240"
  },
  {
    "text": "that's one of the key things that uh people are trending down is trying to do that the second one I would say is a",
    "start": "1959240",
    "end": "1966880"
  },
  {
    "text": "large push around quantization in terms of getting to lower bits and that's something that's been around for a while",
    "start": "1966880",
    "end": "1973440"
  },
  {
    "text": "but it's something that's becoming more and more prevalent as we're looking at the larger models mainly because their",
    "start": "1973440",
    "end": "1979360"
  },
  {
    "text": "execution time is mainly dominated by trying to pull in these large matrices of Weights so there's been a big push",
    "start": "1979360",
    "end": "1986519"
  },
  {
    "text": "there now around trying to get down to you know pass n a down to N4 N3 N2",
    "start": "1986519",
    "end": "1992799"
  },
  {
    "text": "quantization on these active representations I'd say that's the other Trend the final kind of bonus Trend that",
    "start": "1992799",
    "end": "1999000"
  },
  {
    "text": "I'd throw on there because I said too the third one would be more research around sparse training so specifically",
    "start": "1999000",
    "end": "2006519"
  },
  {
    "text": "trying to figure out how to start with an unoptimized and untrained model and",
    "start": "2006519",
    "end": "2012799"
  },
  {
    "text": "be able to make it sparse from the start and then Heap fary throughout training because generally what we do to",
    "start": "2012799",
    "end": "2019600"
  },
  {
    "text": "guarantee accuracy we'll start from a dense converge model and then edely",
    "start": "2019600",
    "end": "2024799"
  },
  {
    "text": "print on top of that which adds training time right so now there's a lot of research going into trying to figure out",
    "start": "2024799",
    "end": "2031200"
  },
  {
    "text": "how quickly the model can be pruned and then be able to carry that over as a",
    "start": "2031200",
    "end": "2037039"
  },
  {
    "text": "training so so that's where the other big big piece is and all three of these are definitely active areas that we've",
    "start": "2037039",
    "end": "2043080"
  },
  {
    "text": "been investing heavily in especially looking at the generative AI space now going through that yeah yeah I I know it",
    "start": "2043080",
    "end": "2050560"
  },
  {
    "text": "must be a crazy time for you all just like it is a a crazy time for everyone",
    "start": "2050560",
    "end": "2057240"
  },
  {
    "text": "um but yeah I think this is a really important piece of it I know one of the trends we've even talked here on the",
    "start": "2057240",
    "end": "2063560"
  },
  {
    "text": "show about it seems like a lot of people are talking about like serverless deployment ments of of machine learning",
    "start": "2063560",
    "end": "2070480"
  },
  {
    "text": "deep learning models and I know a lot of the issues related to that and things that people are dealing with is cold",
    "start": "2070480",
    "end": "2077040"
  },
  {
    "text": "start time and loading models into memory I don't know if that's impacted you all at all but it seems definitely",
    "start": "2077040",
    "end": "2083720"
  },
  {
    "text": "relevant like if you're going to run your model serverless you probably want it as small as possible I would imagine",
    "start": "2083720",
    "end": "2089800"
  },
  {
    "text": "yeah absolutely absolutely so you mentioned um where people can find out",
    "start": "2089800",
    "end": "2095560"
  },
  {
    "text": "about neural Magic on slack on Hub I would really encourage people to do this um as we close out here what are you",
    "start": "2095560",
    "end": "2103359"
  },
  {
    "text": "kind of personally excited about during this uh I mean it's a like I say it's a",
    "start": "2103359",
    "end": "2108480"
  },
  {
    "text": "crazy time for everyone right now with generative Ai and the way things are trending what's exciting to you right",
    "start": "2108480",
    "end": "2114720"
  },
  {
    "text": "now about the AI community and certain things you're seeing what do you see as kind of positive Trends I guess the part",
    "start": "2114720",
    "end": "2122640"
  },
  {
    "text": "that I'm most excited about is that generative AI space specifically in being able to augment humans obviously",
    "start": "2122640",
    "end": "2130240"
  },
  {
    "text": "there are a lot of privacy concerns and um data concerns and bias issues and",
    "start": "2130240",
    "end": "2136359"
  },
  {
    "text": "things like that in this which I don't want to see you know llms deployed everywhere becoming default response for",
    "start": "2136359",
    "end": "2142320"
  },
  {
    "text": "like Google search or something like that but it is really exciting to see even in my day-to-day starting to use",
    "start": "2142320",
    "end": "2148520"
  },
  {
    "text": "these actively to augment what I'm doing around content generation and Framing and things like that so it's one piece",
    "start": "2148520",
    "end": "2155560"
  },
  {
    "text": "that I'm really excited for and with uh the work that we're doing on neural magic we're especially looking at these",
    "start": "2155560",
    "end": "2161800"
  },
  {
    "text": "because one we want to see that continue to grow to open source and I think that's been the other push that's been",
    "start": "2161800",
    "end": "2167319"
  },
  {
    "text": "really big and really exciting to see is that whenever TBT 4 came out it was",
    "start": "2167319",
    "end": "2172720"
  },
  {
    "text": "completely privatized they put out you know a little white paper on it that had no details about it at all a lot of data",
    "start": "2172720",
    "end": "2178599"
  },
  {
    "text": "concerns and things like that within that but the open source Community has already released I I can name probably",
    "start": "2178599",
    "end": "2184960"
  },
  {
    "text": "10 models so far that have been released since then that are chat GPT like or gp4 like so it's really exciting to see that",
    "start": "2184960",
    "end": "2191760"
  },
  {
    "text": "I think the next stage from those open source models is going to be making them runnable anywhere right so you don't",
    "start": "2191760",
    "end": "2198040"
  },
  {
    "text": "need this big GPU cluster Farm to get something that is usable and that's where we're really looking at going",
    "start": "2198040",
    "end": "2204160"
  },
  {
    "text": "we're actively working on the llm deployment issue right now and hope to have something out in the you know next",
    "start": "2204160",
    "end": "2210880"
  },
  {
    "text": "few weeks next few months that people can start actively using download it they can run it anywhere they want on",
    "start": "2210880",
    "end": "2216280"
  },
  {
    "text": "any CPU and will be just as fast as youp is cool yeah well keep us posted I know",
    "start": "2216280",
    "end": "2222319"
  },
  {
    "text": "I'm personally interested in that one so yeah thank you so much for joining us Mark um this is a a really fun",
    "start": "2222319",
    "end": "2229119"
  },
  {
    "text": "conversation I love getting into the weeds of these practicalities because this is a this is a topic where people",
    "start": "2229119",
    "end": "2236280"
  },
  {
    "text": "get stuck a lot is on the deployment side and the optimization side so yeah",
    "start": "2236280",
    "end": "2242000"
  },
  {
    "text": "thank you for all that you and your team are doing at neurl Magic in this area",
    "start": "2242000",
    "end": "2247160"
  },
  {
    "text": "and um yeah keep up the good work we're excited to see it so thanks for joining",
    "start": "2247160",
    "end": "2252240"
  },
  {
    "text": "thanks Daniel it's great talking with [Music]",
    "start": "2252240",
    "end": "2261580"
  },
  {
    "text": "you thank you for listening to practical AI your next step is to subscribe now if",
    "start": "2261760",
    "end": "2268400"
  },
  {
    "text": "you haven't already and if you're a longtime listener of the show help us reach more people by sharing practical",
    "start": "2268400",
    "end": "2274319"
  },
  {
    "text": "AI with your friends and colleagues thanks once again to fast and fly for partnering with us to bring you all",
    "start": "2274319",
    "end": "2280000"
  },
  {
    "text": "Chang doog podcasts check out what they're up to at fastly.com and fly.io and to our beat freaking",
    "start": "2280000",
    "end": "2286839"
  },
  {
    "text": "residents break master cylinder for continuously cranking out the best beats in the biz that's all for now we'll talk",
    "start": "2286839",
    "end": "2292920"
  },
  {
    "text": "to you again next [Music]",
    "start": "2292920",
    "end": "2305809"
  },
  {
    "text": "time",
    "start": "2306440",
    "end": "2309440"
  }
]