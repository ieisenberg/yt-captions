[
  {
    "text": "[Music]",
    "start": "0",
    "end": "6720"
  },
  {
    "text": "welcome to practical AI if you work in artificial intelligence aspire to or are",
    "start": "6720",
    "end": "13920"
  },
  {
    "text": "curious how AI related Tech is changing the world this is the show for you thank",
    "start": "13920",
    "end": "20519"
  },
  {
    "text": "you to our partners at fly.io the home of",
    "start": "20519",
    "end": "25759"
  },
  {
    "text": "changelog.md 30 plus regions on six continents so you can launch your app",
    "start": "28960",
    "end": "34440"
  },
  {
    "text": "near your users learn more at [Music]",
    "start": "34440",
    "end": "42239"
  },
  {
    "text": "fly.io hello and welcome to another fully connected episode of the Practical",
    "start": "42239",
    "end": "47719"
  },
  {
    "text": "AI podcast in these fully connected episodes Chris and I keep you connected",
    "start": "47719",
    "end": "53440"
  },
  {
    "text": "with everything that's happening in the AI world and hopefully share some resources with you to help you level up",
    "start": "53440",
    "end": "59280"
  },
  {
    "text": "your machine Lear learning game my name is Daniel Whit neack I am CEO and founder at prediction guard where we're",
    "start": "59280",
    "end": "66360"
  },
  {
    "text": "safeguarding private AI models and I'm joined as always by Chris Benson who is a principal AI research engineer at",
    "start": "66360",
    "end": "73520"
  },
  {
    "text": "locked Martin how you doing Chris doing great today Daniel how are you um I'm doing well yeah yeah lots lots going on",
    "start": "73520",
    "end": "80560"
  },
  {
    "text": "lots of fun stuff in the in the AI world but uh Chris I I I have to say uh so one",
    "start": "80560",
    "end": "86680"
  },
  {
    "text": "of our one of our listeners pointed out something which uh I realized afterwards",
    "start": "86680",
    "end": "91759"
  },
  {
    "text": "as well on our last fully connected episode the one about gp40 or Omni we played some clips of The",
    "start": "91759",
    "end": "100640"
  },
  {
    "text": "Voice Assistant and we talked a little bit about the model and the model of course was released and and we were",
    "start": "100640",
    "end": "106840"
  },
  {
    "text": "using it but the voice back and forth wasn't yet plugged into GPT 40 so",
    "start": "106840",
    "end": "114320"
  },
  {
    "text": "they're just I think releasing that shortly so I I think there was a bit of General confusion in the community about",
    "start": "114320",
    "end": "121719"
  },
  {
    "text": "because when you click that button it's like you just take over into the The Voice interface and it's like I was on",
    "start": "121719",
    "end": "128319"
  },
  {
    "text": "GPT 40 and so probably could have been maybe handled a bit better on on our end",
    "start": "128319",
    "end": "135040"
  },
  {
    "text": "and maybe there end too but open AI got us I guess we we were fooled but yeah",
    "start": "135040",
    "end": "141519"
  },
  {
    "text": "still really cool stuff still the the things that we talked about are are still what GPT 40 is but just wanted to",
    "start": "141519",
    "end": "148080"
  },
  {
    "text": "clarify that for our listeners if they had listened to that previous episode while we're while we're doing that I",
    "start": "148080",
    "end": "154200"
  },
  {
    "text": "realized I had made a mistake on that same issue uh after the show that I wanted to uh confess on I had watched",
    "start": "154200",
    "end": "161000"
  },
  {
    "text": "all the videos over and over again about the video portion doing that and then when we were in the show I had I'll",
    "start": "161000",
    "end": "167920"
  },
  {
    "text": "claim is a senior moment where I was thinking just I watched so much the video I was thinking oh yeah I I did",
    "start": "167920",
    "end": "173400"
  },
  {
    "text": "that and so it wasn't afterwards I was like no that was the videos I was watching what was I thinking so anyway I",
    "start": "173400",
    "end": "179239"
  },
  {
    "text": "wanted to conf so we we had a couple of blunders on that one but having fun there you go in our eagerness and",
    "start": "179239",
    "end": "184920"
  },
  {
    "text": "excitement to talk about GPT 40 and record it very quickly afterwards um we",
    "start": "184920",
    "end": "190480"
  },
  {
    "text": "got got so we we got we both got got there so all good we we'll move on well",
    "start": "190480",
    "end": "195799"
  },
  {
    "text": "for for listeners who don't know the the show is fairly spontaneous if you haven't been following us long and we",
    "start": "195799",
    "end": "201799"
  },
  {
    "text": "dive into stuff pretty quick and occasionally occasionally we fumble a little bit with that yeah yeah so thanks",
    "start": "201799",
    "end": "208360"
  },
  {
    "text": "for journeying with us through this this wild and crazy world of of AI and now I",
    "start": "208360",
    "end": "214400"
  },
  {
    "text": "I think something that's been on my mind quite a bit Chris with some recent",
    "start": "214400",
    "end": "219840"
  },
  {
    "text": "announcements but also things that have been kind of developing over the past months is this area of local offline Ai",
    "start": "219840",
    "end": "230799"
  },
  {
    "text": "and AI PCS there's sort of people are using this terminology AIP PCS and",
    "start": "230799",
    "end": "236959"
  },
  {
    "text": "current marketing hype yeah the current the current marketing hype so I thought maybe we could dig into a little bit of",
    "start": "236959",
    "end": "244000"
  },
  {
    "text": "a little bit of that today and talk through kind of what what exactly does that mean how are people using AI models",
    "start": "244000",
    "end": "250040"
  },
  {
    "text": "locally what is an aipc what are the relevant kind of types of models and",
    "start": "250040",
    "end": "255640"
  },
  {
    "text": "optimizations that can run locally all of that seems to be a big well it it's a",
    "start": "255640",
    "end": "261359"
  },
  {
    "text": "little bit hard to parse out some of that maybe if you're new to the new to the space and you know what is a ggf",
    "start": "261359",
    "end": "268840"
  },
  {
    "text": "versus a ol Lama or you know all these these things you know coming out so yeah",
    "start": "268840",
    "end": "275479"
  },
  {
    "text": "I thought that would be good to dig into and I know that you have been interested in kind of AI at the edge for some",
    "start": "275479",
    "end": "282120"
  },
  {
    "text": "period of Time how do you see whether it's a staff laptop or maybe a heavy",
    "start": "282120",
    "end": "289479"
  },
  {
    "text": "compute node in a manufacturing plant or something where you'd want to run AI",
    "start": "289479",
    "end": "296240"
  },
  {
    "text": "quote at the edge or locally let's say generally Al or offline what are the",
    "start": "296240",
    "end": "301680"
  },
  {
    "text": "reasons that people would would sort of want to go that direction well I think you know this is kind of a thread we've",
    "start": "301680",
    "end": "308039"
  },
  {
    "text": "talked about off and on on different episodes uh over time and you know AI",
    "start": "308039",
    "end": "313400"
  },
  {
    "text": "models are hosted in in software and and they're going to always be wrapped in that and as we you know the software",
    "start": "313400",
    "end": "320560"
  },
  {
    "text": "expands from the cloud all the way out into every device that we're already using and so it's only natural that as",
    "start": "320560",
    "end": "326919"
  },
  {
    "text": "AI becomes more accessible and cheaper to deploy that it's going to you're going to start having models you know",
    "start": "326919",
    "end": "333639"
  },
  {
    "text": "that are kind of ramping up existing software out there and there's of course we're going through all the hype that's associated with that but the way I see",
    "start": "333639",
    "end": "340560"
  },
  {
    "text": "it is is simply just a a natural evolution of where software development would go and to your point a moment ago",
    "start": "340560",
    "end": "346960"
  },
  {
    "text": "about the hardware side we don't talk about that a lot we're very software focused in general but the hardware side",
    "start": "346960",
    "end": "352759"
  },
  {
    "text": "is really going through Revolution I know that you I know that in your business you have partnership with Intel",
    "start": "352759",
    "end": "357919"
  },
  {
    "text": "and are are seeing that in that capacity and I certainly see that in my day job and so there are so many more Hardware",
    "start": "357919",
    "end": "365240"
  },
  {
    "text": "capabilities coming out to support these functions many of which will function or",
    "start": "365240",
    "end": "370280"
  },
  {
    "text": "targeting low power disconnected environments and so this is a timely",
    "start": "370280",
    "end": "375639"
  },
  {
    "text": "topic and and if you look back for preent before that we've always seen over the the years in software and Cloud",
    "start": "375639",
    "end": "383000"
  },
  {
    "text": "development kind of a shifting back and forth between local capability and suddenly you get a new generation of",
    "start": "383000",
    "end": "389199"
  },
  {
    "text": "hardware and things will go a little bit more local and you'll have your own equipment and then things will move back into the cloud and that natural give and",
    "start": "389199",
    "end": "396440"
  },
  {
    "text": "take is part of the the flow and I think right now we've been so Cloud focused",
    "start": "396440",
    "end": "401599"
  },
  {
    "text": "the last few years because that was really the only available option that now we're we're seeing a lot of new",
    "start": "401599",
    "end": "407240"
  },
  {
    "text": "capability rolling out on both Hardware software and models that are going to enable Edge functionality to really",
    "start": "407240",
    "end": "413639"
  },
  {
    "text": "explode over the next few years yeah I I was asked I was at a conference last week and I was asked which direction",
    "start": "413639",
    "end": "419759"
  },
  {
    "text": "things would be going either local AI models or hosted in the cloud and I",
    "start": "419759",
    "end": "425560"
  },
  {
    "text": "think the answer is definitely both in the same way that there's a place for if",
    "start": "425560",
    "end": "431560"
  },
  {
    "text": "you just think about databases for example as a technology there's a place for embedded local databases that",
    "start": "431560",
    "end": "439280"
  },
  {
    "text": "operate where an app application operates there's a place for databases",
    "start": "439280",
    "end": "444599"
  },
  {
    "text": "that run kind of at the edge but on a heavier compute node that served maybe",
    "start": "444599",
    "end": "449919"
  },
  {
    "text": "some environment and there's a use case for databases in the cloud and sometimes",
    "start": "449919",
    "end": "456440"
  },
  {
    "text": "those even coexisting for various reasons and in this case we're talking about AI models so I have a bunch of",
    "start": "456440",
    "end": "463199"
  },
  {
    "text": "files on my laptop I may not want those files to leave my laptop so it might be privacy reasons that I want to you know",
    "start": "463199",
    "end": "470840"
  },
  {
    "text": "search those files or ask questions of those files with an AI model so privacy security type of thing or in a",
    "start": "470840",
    "end": "477240"
  },
  {
    "text": "healthcare environment they may have to be air gapped or offline sort of thing or U Public Utilities sort of scenario",
    "start": "477240",
    "end": "484879"
  },
  {
    "text": "where you can't be connected to the public internet but then it it might just be also because of latency or",
    "start": "484879",
    "end": "491680"
  },
  {
    "text": "performance inconsistent networks or uh flaky networks where you have to operate",
    "start": "491680",
    "end": "497479"
  },
  {
    "text": "sort of online offline there's a whole variety of reasons to do this but yeah there there's also a lot of ways that as",
    "start": "497479",
    "end": "504800"
  },
  {
    "text": "you said this is rapidly developing and people are finding all of these various",
    "start": "504800",
    "end": "510639"
  },
  {
    "text": "ways of running models at the edge and we can highlight if you're just into",
    "start": "510639",
    "end": "517080"
  },
  {
    "text": "this now and getting into AI models maybe you've used open ai's endpoint or",
    "start": "517080",
    "end": "522479"
  },
  {
    "text": "you've used an llm API if you wanted to run a large language model or an AM AI",
    "start": "522479",
    "end": "529200"
  },
  {
    "text": "model on your laptop there's a variety of easy ways to do that I know a lot of",
    "start": "529200",
    "end": "535120"
  },
  {
    "text": "people that are using something like LM Studio this is just an application that",
    "start": "535120",
    "end": "540399"
  },
  {
    "text": "you can run and test out different models there's a project called olama",
    "start": "540399",
    "end": "545600"
  },
  {
    "text": "which I think is really nice and really easy to use you kind of just spin it up you can either spin it up as a python",
    "start": "545600",
    "end": "552519"
  },
  {
    "text": "library or as a kind of server that's running on your local machine and",
    "start": "552519",
    "end": "558120"
  },
  {
    "text": "interact with olama as you would kind of an llm API and and then there's things",
    "start": "558120",
    "end": "564399"
  },
  {
    "text": "like uh llama CPP and a bunch of other things these these I I would kind of",
    "start": "564399",
    "end": "569480"
  },
  {
    "text": "categorize as local model applications or systems where there's either a UI or",
    "start": "569480",
    "end": "576279"
  },
  {
    "text": "a server or a python client it's kind of geared specifically towards running",
    "start": "576279",
    "end": "581440"
  },
  {
    "text": "these models locally and then there's a sort of whole set of technologies that",
    "start": "581440",
    "end": "587000"
  },
  {
    "text": "are kind of python libraries or optimization or compilation libraries",
    "start": "587000",
    "end": "592279"
  },
  {
    "text": "that might take a model that's maybe bigger or not suited to run in a local",
    "start": "592279",
    "end": "598399"
  },
  {
    "text": "or lower power environment and run that locally so if you're using the",
    "start": "598399",
    "end": "603800"
  },
  {
    "text": "Transformers library from hugging face you might use something like bits and bites as a library to quantize models",
    "start": "603800",
    "end": "611399"
  },
  {
    "text": "shrink them down there's optimization libraries like Optimum and mlc open Veno",
    "start": "611399",
    "end": "618320"
  },
  {
    "text": "the these all all have some have exist for some period of time actually I think in the past we've had the Apache TVM",
    "start": "618320",
    "end": "625560"
  },
  {
    "text": "project on the show and we talked about octoml so there this is not a New Concept because we've been sort of",
    "start": "625560",
    "end": "632079"
  },
  {
    "text": "optimizing models for various Hardwares for some time but these optimization or compilation libraries are also usually",
    "start": "632079",
    "end": "640320"
  },
  {
    "text": "kind of Hardware specific so you optimize for a specific Hardware whereas",
    "start": "640320",
    "end": "645440"
  },
  {
    "text": "other other of these local model systems are maybe more general purpose less",
    "start": "645440",
    "end": "650839"
  },
  {
    "text": "optimized for Hardware specifically I don't know if you've got a chance to try",
    "start": "650839",
    "end": "655959"
  },
  {
    "text": "out any of these systems Chris uh running some models on on your laptop I",
    "start": "655959",
    "end": "661399"
  },
  {
    "text": "have a little bit I've used a Lama I think that's my go-to and you have a like a M1 or M2 M3 whatever M there is",
    "start": "661399",
    "end": "670320"
  },
  {
    "text": "now MacBook yeah I have an M2 that I may that I have ACC I have a couple of different uh laptops one that's old and",
    "start": "670320",
    "end": "676320"
  },
  {
    "text": "one that's uh well I guess an M2 is old by today's standard so may may have to upgrade that one pretty soon but but",
    "start": "676320",
    "end": "682839"
  },
  {
    "text": "yeah I've used I've used AMA primarily I probably haven't used as many of the tools as you have given given the",
    "start": "682839",
    "end": "688320"
  },
  {
    "text": "business that you're in it's I I think one of the things that I'm really interested as as people are are doing",
    "start": "688320",
    "end": "695600"
  },
  {
    "text": "this now is understanding you know because we're really focusing on this kind of the infrastructure the plumbing",
    "start": "695600",
    "end": "702680"
  },
  {
    "text": "of making all this work locally and and doing the Integrations with the cloud but I think there's another another",
    "start": "702680",
    "end": "708360"
  },
  {
    "text": "thing just to throw into the the mix of the conversation and that is how as people start whether in the cloud or",
    "start": "708360",
    "end": "715279"
  },
  {
    "text": "particularly local uh having multiple models out and you have have the infrastructure now you know as we're",
    "start": "715279",
    "end": "721279"
  },
  {
    "text": "talking about to run it but starting to look at the the API and the middleware to enable inferencing across apis",
    "start": "721279",
    "end": "728639"
  },
  {
    "text": "without direct human intervention and have it make sense where you have different responsibilities much like we",
    "start": "728639",
    "end": "734600"
  },
  {
    "text": "have had for a number of years in the software realm so as we're talking about that I wanted to throw that in as",
    "start": "734600",
    "end": "740279"
  },
  {
    "text": "another topic that I think is going to be really really important and hasn't had nearly as much attention as the",
    "start": "740279",
    "end": "747040"
  },
  {
    "text": "fundamental infrastructure yeah I think that gets probably to a couple things one is that one current major difference",
    "start": "747040",
    "end": "754880"
  },
  {
    "text": "between sort of hosted Cloud models and offerings versus local models is likely",
    "start": "754880",
    "end": "761920"
  },
  {
    "text": "you're not going to run a mix of 10 different llms on your local laptop all",
    "start": "761920",
    "end": "767720"
  },
  {
    "text": "at the same time all loaded into memory um that would be a pretty a pretty significant at least right now a pretty",
    "start": "767720",
    "end": "773880"
  },
  {
    "text": "significant ask to kind of switch between models in that way but there are",
    "start": "773880",
    "end": "779440"
  },
  {
    "text": "certainly cases where you know I think the market is showing that people want",
    "start": "779440",
    "end": "784560"
  },
  {
    "text": "to not be restricted to one model family and they're spreading out their usage against multiple models so that",
    "start": "784560",
    "end": "791199"
  },
  {
    "text": "definitely needs to happen in the cloud and from you know model providers but",
    "start": "791199",
    "end": "797199"
  },
  {
    "text": "that doesn't mean you couldn't throw in the mix a selection of local models as well for specific purposes and I think",
    "start": "797199",
    "end": "804040"
  },
  {
    "text": "that gets to the other thing that you're talking about which is kind of data integration automation pipelining all of",
    "start": "804040",
    "end": "810639"
  },
  {
    "text": "that sort of thing I saw a comment on LinkedIn I think it was even this morning that those that are really",
    "start": "810639",
    "end": "817160"
  },
  {
    "text": "winning in the AI space are those that have taken what they've learned kind of from automation data pipelining data",
    "start": "817160",
    "end": "824519"
  },
  {
    "text": "integration in previous cycles of data science and integrated those with",
    "start": "824519",
    "end": "830399"
  },
  {
    "text": "generative models at various stages because a lot of a lot of the value and I've seen this too A lot of the value",
    "start": "830399",
    "end": "837399"
  },
  {
    "text": "that you get out of these models is not the models themselves but the system that you build around them and you know",
    "start": "837399",
    "end": "844600"
  },
  {
    "text": "that involves a lot of data integration and Automation and maybe even routing between different models in the case",
    "start": "844600",
    "end": "851480"
  },
  {
    "text": "that we're talking about here maybe even routing between local models and Cloud models and so yeah I think that that",
    "start": "851480",
    "end": "858720"
  },
  {
    "text": "that's really stressed especially as you talk about running these models kind of everywhere quote unquote across cloud",
    "start": "858720",
    "end": "865880"
  },
  {
    "text": "and local and on Prem and data center en IRS but yeah it's it's interesting I",
    "start": "865880",
    "end": "871440"
  },
  {
    "text": "don't I don't myself have an aipc yet maybe at some point I will but uh but",
    "start": "871440",
    "end": "878199"
  },
  {
    "text": "yeah I'm I'm excited to see where all this goes uh indeed I think as we push",
    "start": "878199",
    "end": "883480"
  },
  {
    "text": "forward I think one of the things that I'd like to see especially for local whereas you know we have you know we",
    "start": "883480",
    "end": "889040"
  },
  {
    "text": "mentioned a few minutes ago a llama and some of the other tools for infrastructure I was actually as we were",
    "start": "889040",
    "end": "894199"
  },
  {
    "text": "talking here I was looking through um Yan laon's various posts because he was",
    "start": "894199",
    "end": "900880"
  },
  {
    "text": "proposing recently in the last week or two prior to this conversation kind of a way of structuring different model",
    "start": "900880",
    "end": "907839"
  },
  {
    "text": "interactions and and from a responsibility standpoint that they're doing at meta and of course my employer",
    "start": "907839",
    "end": "914160"
  },
  {
    "text": "we have our own version of that on how we structure different things but really interested in seeing if the community",
    "start": "914160",
    "end": "921399"
  },
  {
    "text": "kind of comes around with kind of an open framework you know best practice framework around how to do that you know",
    "start": "921399",
    "end": "927720"
  },
  {
    "text": "be able to do it on a laptop an M2 M3 M4 be able to have those interactions",
    "start": "927720",
    "end": "932880"
  },
  {
    "text": "locally and and a framework that would span between that local and the cloud interaction so that you have something",
    "start": "932880",
    "end": "939680"
  },
  {
    "text": "and I don't think that right now everyone seems to be doing that on their own and I and there there's a lot of",
    "start": "939680",
    "end": "945560"
  },
  {
    "text": "similarities between them but there doesn't seem to be kind of a standard approach to to how that all comes",
    "start": "945560",
    "end": "951360"
  },
  {
    "text": "together",
    "start": "951360",
    "end": "954360"
  },
  {
    "text": "[Music] if you're anything like me you have a",
    "start": "959210",
    "end": "965079"
  },
  {
    "text": "certain tendency to put things off until the very last minute seeing the dentist",
    "start": "965079",
    "end": "970639"
  },
  {
    "text": "going to the doctor Home Improvements that NeverEnding chore list of yours and",
    "start": "970639",
    "end": "976519"
  },
  {
    "text": "while most of the time it works out just fine the one thing in life that you really cannot afford to wait on is",
    "start": "976519",
    "end": "982079"
  },
  {
    "text": "setting up term coverage life insurance you've probably seen life insurance commercials on TV and thought yeah I'll",
    "start": "982079",
    "end": "988920"
  },
  {
    "text": "look into that later no later doesn't come this really isn't something you can wait on choose life insurance through",
    "start": "988920",
    "end": "995480"
  },
  {
    "text": "ladder today here's what we love about ladder and why we allow them as a sponsor they are 100% digital no doctors",
    "start": "995480",
    "end": "1003399"
  },
  {
    "text": "no needles no paperwork when you apply for $3 million in coverage or less just answer a few questions about your health",
    "start": "1003399",
    "end": "1009880"
  },
  {
    "text": "in an application ladder's customers rate them 4.8 out of five stars on trust",
    "start": "1009880",
    "end": "1015120"
  },
  {
    "text": "pilot and they made Forbes best life insurance 2021 list you just need a few minutes and a phone or laptop to apply",
    "start": "1015120",
    "end": "1021959"
  },
  {
    "text": "ladder's smart algorithm Works in real time so you'll find out if you're instantly approved no hidden fees you",
    "start": "1021959",
    "end": "1027880"
  },
  {
    "text": "can can to any time get a full refund if you change your mind in the first 30",
    "start": "1027880",
    "end": "1033000"
  },
  {
    "text": "days latter policies are issued by insurers with long proven histories of",
    "start": "1033000",
    "end": "1038600"
  },
  {
    "text": "paying claims they're rated a and A+ by a am best finally since life insurance",
    "start": "1038600",
    "end": "1045360"
  },
  {
    "text": "costs more as you age now yeah right now Now's the Time to cross it off your list",
    "start": "1045360",
    "end": "1051000"
  },
  {
    "text": "so go to ladderlife decom slprc aai today to see if you're instantly",
    "start": "1051000",
    "end": "1057440"
  },
  {
    "text": "approved again that's ladder.com practical AI l a d d ER",
    "start": "1057440",
    "end": "1066320"
  },
  {
    "text": "life.com practical aai",
    "start": "1066320",
    "end": "1070880"
  },
  {
    "text": "well Chris uh there there's an increasing number of options if if you were to explore this space and kind of",
    "start": "1080600",
    "end": "1087480"
  },
  {
    "text": "that interaction uh of local models with your with your systems there's an",
    "start": "1087480",
    "end": "1092720"
  },
  {
    "text": "increasing number of choices of quote AI PCS which I think is a hyped term now",
    "start": "1092720",
    "end": "1099559"
  },
  {
    "text": "one of them uh you mentioned Intel Intel coming out with aips I think uh Lenovo",
    "start": "1099559",
    "end": "1105799"
  },
  {
    "text": "is is shipping some with the Intel's core Ultra processor I think the name is",
    "start": "1105799",
    "end": "1111799"
  },
  {
    "text": "meteor Lake the code name at least and similar probably to as a response to",
    "start": "1111799",
    "end": "1118720"
  },
  {
    "text": "maybe what's more familiar I already mentioned the M1 M2 M3 Etc line from",
    "start": "1118720",
    "end": "1124240"
  },
  {
    "text": "from Apple but Nvidia is also working hard on like the GeForce RTX AI PCS",
    "start": "1124240",
    "end": "1132440"
  },
  {
    "text": "which there've been kind of gaming PCs with like gpus in them for for sometime",
    "start": "1132440",
    "end": "1139440"
  },
  {
    "text": "but I think most of these quote AIP PCS are more of an integrated type of",
    "start": "1139440",
    "end": "1145880"
  },
  {
    "text": "processor or system where where it's not just like an add-on to the laptop but in",
    "start": "1145880",
    "end": "1152039"
  },
  {
    "text": "the case of like the core Ultra or the M2 there's actual processing in the",
    "start": "1152039",
    "end": "1157600"
  },
  {
    "text": "architecture that is optimized for executing models and so they're sort of",
    "start": "1157600",
    "end": "1163600"
  },
  {
    "text": "AI ready shipping AI ready and that brings up kind of some interesting",
    "start": "1163600",
    "end": "1169440"
  },
  {
    "text": "questions in my mind which are well how do all of these AIP PCS compare if I'm",
    "start": "1169440",
    "end": "1175120"
  },
  {
    "text": "about to get myself an aipc uh where where should I go and in thinking about",
    "start": "1175120",
    "end": "1181159"
  },
  {
    "text": "that and looking at some of these benchmarks I was really encouraged to see that ml Commons which is the",
    "start": "1181159",
    "end": "1188240"
  },
  {
    "text": "organization behind ml perf which is a set of benchmarks that benchmarks and",
    "start": "1188240",
    "end": "1194320"
  },
  {
    "text": "working groups that have been working for some time to Benchmark various",
    "start": "1194320",
    "end": "1199559"
  },
  {
    "text": "systems for performance on running AI workloads machine learning workloads",
    "start": "1199559",
    "end": "1205640"
  },
  {
    "text": "they've just announced this spring an ml perf client working group which is",
    "start": "1205640",
    "end": "1212320"
  },
  {
    "text": "really geared towards essentially an application or a workload that you could",
    "start": "1212320",
    "end": "1217799"
  },
  {
    "text": "run across these various aips or maybe uh AI enabled Edge machines and that",
    "start": "1217799",
    "end": "1227000"
  },
  {
    "text": "that sort of thing um to really do kind of llm based workloads and do some",
    "start": "1227000",
    "end": "1234159"
  },
  {
    "text": "benchmarking for both training and inferencing on these quote clients so",
    "start": "1234159",
    "end": "1239760"
  },
  {
    "text": "they're they're kind of referring to these generally as as clients so they say the new ml Commons effort will build",
    "start": "1239760",
    "end": "1246080"
  },
  {
    "text": "ml benchmarks for desktop laptop and workstations for Microsoft Windows and",
    "start": "1246080",
    "end": "1251640"
  },
  {
    "text": "others other operating systems so um quite interesting I'm glad that this I",
    "start": "1251640",
    "end": "1256799"
  },
  {
    "text": "as far as I can tell some someone in our again you know sometimes we get things wrong so someone in our audience can",
    "start": "1256799",
    "end": "1263760"
  },
  {
    "text": "correct us maybe David uh from ml comments who's been on the show before he can correct us if something's already",
    "start": "1263760",
    "end": "1269840"
  },
  {
    "text": "been published but I couldn't find the sort of set of benchmarks I think it's a work in progress but really excited to",
    "start": "1269840",
    "end": "1276279"
  },
  {
    "text": "see this when it comes out yeah I think it'll be interesting as as these laptops come out I'm I'm it's hard to imagine",
    "start": "1276279",
    "end": "1283880"
  },
  {
    "text": "that the entire industry doesn't have to to go full in on this regardless and thus kind of making the distinction of",
    "start": "1283880",
    "end": "1290520"
  },
  {
    "text": "an AI laptop a little bit of a a redundant thing because there's a point",
    "start": "1290520",
    "end": "1295600"
  },
  {
    "text": "and maybe we're already maybe we've already arrived now where the idea of purchasing a new laptop that is not an",
    "start": "1295600",
    "end": "1302520"
  },
  {
    "text": "AI laptop is a ridiculous thing you know it's it becomes a must-have feature to",
    "start": "1302520",
    "end": "1307640"
  },
  {
    "text": "have going forward and therefore all laptops kind of have to go that direction at some point in here yeah",
    "start": "1307640",
    "end": "1313760"
  },
  {
    "text": "well I I definitely think that that could be one downside of this whole thing thing is there's I I mean I know",
    "start": "1313760",
    "end": "1322039"
  },
  {
    "text": "the prices will go down but these things are really expensive right now so there is going to be a sort of disparity of",
    "start": "1322039",
    "end": "1328640"
  },
  {
    "text": "those uh already for some period of time if if you're a a new developer maybe a",
    "start": "1328640",
    "end": "1336200"
  },
  {
    "text": "an indie developer that purchase of that MacBook is a pretty significant expense",
    "start": "1336200",
    "end": "1342000"
  },
  {
    "text": "for you already and you know myself I just use a refer think pad from like",
    "start": "1342000",
    "end": "1348760"
  },
  {
    "text": "four years ago you know not an AI PC it's a core i5 and you know uh not a",
    "start": "1348760",
    "end": "1356640"
  },
  {
    "text": "terrible laptop but definitely not not anything that anyone uh would",
    "start": "1356640",
    "end": "1362159"
  },
  {
    "text": "necessarily be jealous of now I can run some models on this laptop you know",
    "start": "1362159",
    "end": "1368039"
  },
  {
    "text": "using olama and other other systems and I think that that gets down to maybe",
    "start": "1368039",
    "end": "1373120"
  },
  {
    "text": "another element of this so there's going to be on one side these clients that get increasingly sophisticated and build in",
    "start": "1373120",
    "end": "1380880"
  },
  {
    "text": "more AI enabled or accelerating functionality into their chipset and",
    "start": "1380880",
    "end": "1386919"
  },
  {
    "text": "into their their Hardware but there's also going to be increased sophistication on optimizing models that",
    "start": "1386919",
    "end": "1395279"
  },
  {
    "text": "can't run locally such that they can run locally and this is where people might kind of I think this is also a point of",
    "start": "1395279",
    "end": "1402799"
  },
  {
    "text": "common confusion that I've heard in workshops that I've given at conferences",
    "start": "1402799",
    "end": "1407880"
  },
  {
    "text": "and other plac where people kind of look at let's say it's llama 3 or something like that",
    "start": "1407880",
    "end": "1414480"
  },
  {
    "text": "llama I want to run llama 3 while you go to hugging face the top downloaded llama",
    "start": "1414480",
    "end": "1420440"
  },
  {
    "text": "3 is the base model and then you've got these fine tunes for instruction or chat",
    "start": "1420440",
    "end": "1426159"
  },
  {
    "text": "and then you've got all of these other flavors of llama 3 like you know GG UF",
    "start": "1426159",
    "end": "1432039"
  },
  {
    "text": "gml qat a AQ a WQ uh you know all all",
    "start": "1432039",
    "end": "1438640"
  },
  {
    "text": "sorts of like acronyms that are really difficult to understand so may maybe it",
    "start": "1438640",
    "end": "1444159"
  },
  {
    "text": "would help to just break this down just slightly there's usually when a model is",
    "start": "1444159",
    "end": "1449840"
  },
  {
    "text": "released they release a base model or the pre-trained model or whatever it's",
    "start": "1449840",
    "end": "1455039"
  },
  {
    "text": "called a base model so that's that kind of shortest name usually The Meta you know meta llama 3 that usually is maybe",
    "start": "1455039",
    "end": "1464320"
  },
  {
    "text": "a good model that you might find tune off of but not generally the best model to start with because it's it's a base",
    "start": "1464320",
    "end": "1470600"
  },
  {
    "text": "model it's not fine-tuned for any sort of set of General instruction following or chat and they usually release along",
    "start": "1470600",
    "end": "1477360"
  },
  {
    "text": "with that then a set of fine-tune models for instruction or chat so you've got",
    "start": "1477360",
    "end": "1483080"
  },
  {
    "text": "meta3 or uh llama 3 instruct which is usually the better model and then you've",
    "start": "1483080",
    "end": "1488799"
  },
  {
    "text": "got this whole world of community members out there that build pipelines so we had noose research on they have",
    "start": "1488799",
    "end": "1496000"
  },
  {
    "text": "pipelines built so that when a model is release they can create all these different flavors of it which include",
    "start": "1496000",
    "end": "1502679"
  },
  {
    "text": "flavors for running these optimized in certain ways so these would be these",
    "start": "1502679",
    "end": "1509440"
  },
  {
    "text": "other acronyms that we can dig into a little bit but these are most of the",
    "start": "1509440",
    "end": "1514679"
  },
  {
    "text": "time either additional fine tunes or quantized versions or somehow optimized",
    "start": "1514679",
    "end": "1520720"
  },
  {
    "text": "versions of these models that are meant to be run kind of a diverse set of environments",
    "start": "1520720",
    "end": "1528159"
  },
  {
    "text": "[Music]",
    "start": "1532760",
    "end": "1541480"
  },
  {
    "text": "what's up friends do you remember when chat GPT launched I do it felt like the llm was this magical tool out of the box",
    "start": "1541480",
    "end": "1550159"
  },
  {
    "text": "however the more you use it the more you realize that's just not the case the technology is brilliant don't get me wrong but it's prone to issues like",
    "start": "1550159",
    "end": "1557679"
  },
  {
    "text": "hallucination on its own but there's hope there is still hope feed the llm reliable current",
    "start": "1557679",
    "end": "1563760"
  },
  {
    "text": "data ground it in the right data and context then and only then can it make the right connections and give the right",
    "start": "1563760",
    "end": "1571240"
  },
  {
    "text": "answers the team at neo4j has been exploring how to get results by pairing llms with knowledge graphs and Vector",
    "start": "1571240",
    "end": "1578240"
  },
  {
    "text": "search check out their podcast episode about llms and knowledge graphs throughout 2023 at graph stuff. FM they",
    "start": "1578240",
    "end": "1586120"
  },
  {
    "text": "share tips on retrieval methods prompt engineering and so much more don't miss it find a link in our show notes yes",
    "start": "1586120",
    "end": "1593279"
  },
  {
    "text": "check it out graph stuff. FM episode [Music]",
    "start": "1593279",
    "end": "1615559"
  },
  {
    "text": "23 well Chris I kind of started getting into the alphabet soup a little bit I",
    "start": "1615559",
    "end": "1622240"
  },
  {
    "text": "don't know if if you're sometimes as confused as as I am with all of these",
    "start": "1622240",
    "end": "1628360"
  },
  {
    "text": "all of these model names uh but they're getting increasingly long yes you know",
    "start": "1628360",
    "end": "1633399"
  },
  {
    "text": "one of the uh keep finish your point there and then I have a question for you afterwards yeah yeah sure know I was",
    "start": "1633399",
    "end": "1639279"
  },
  {
    "text": "just gonna highlight a few of these different quantization methods so that",
    "start": "1639279",
    "end": "1644440"
  },
  {
    "text": "people could maybe have them in their mind so there's the flavor that are gml",
    "start": "1644440",
    "end": "1649960"
  },
  {
    "text": "or ggf you might see those letters this is GP generated unified format that's",
    "start": "1649960",
    "end": "1656840"
  },
  {
    "text": "that's what that stands for and this is a this is an optimization of a model that will take that model that maybe",
    "start": "1656840",
    "end": "1663320"
  },
  {
    "text": "requires a GPU to run is larger and creates a C++ replica of the",
    "start": "1663320",
    "end": "1670600"
  },
  {
    "text": "llm and allows you to run it in quantized versions meaning that the parameters of the model are taken from",
    "start": "1670600",
    "end": "1677519"
  },
  {
    "text": "you know number that might have 32 or 16 digits behind the the period and uh get",
    "start": "1677519",
    "end": "1685120"
  },
  {
    "text": "that down to two or four or eight that sort of thing which makes the model smaller and more efficient so these are",
    "start": "1685120",
    "end": "1692399"
  },
  {
    "text": "mostly geared towards CPU sort of CPU or laptop kind of environments there's also",
    "start": "1692399",
    "end": "1700600"
  },
  {
    "text": "gptq which is really a focus kind of quantization method but it's still meant",
    "start": "1700600",
    "end": "1705840"
  },
  {
    "text": "for GPU only so these usually ship in similar kind of formats to previous",
    "start": "1705840",
    "end": "1713159"
  },
  {
    "text": "models but they do kind of some calibration informed quantization to get",
    "start": "1713159",
    "end": "1719600"
  },
  {
    "text": "that model smaller there's qat which is quantization aware training which as it",
    "start": "1719600",
    "end": "1725120"
  },
  {
    "text": "might sound involves some actual training retraining of the model to",
    "start": "1725120",
    "end": "1731440"
  },
  {
    "text": "inform the the quantization there's others like awq and this is another quantization",
    "start": "1731440",
    "end": "1739240"
  },
  {
    "text": "method so all of these sort of letters that you see if you're wondering what those are those are all kind of",
    "start": "1739240",
    "end": "1745159"
  },
  {
    "text": "referring to these different kind of flavors of the model that might be generated for either running the model",
    "start": "1745159",
    "end": "1751799"
  },
  {
    "text": "in an optimized way locally on a CPU on a laptop or optimize still on a GPU but",
    "start": "1751799",
    "end": "1759480"
  },
  {
    "text": "in a smaller format that's more efficient what are your thoughts on the on the CPU derivative what are your",
    "start": "1759480",
    "end": "1766600"
  },
  {
    "text": "thoughts on you know perform performance and capability relative to the its own base model and its GPU siblings yeah I",
    "start": "1766600",
    "end": "1774080"
  },
  {
    "text": "mean I I think that the reality right now and then maybe where it's headed I",
    "start": "1774080",
    "end": "1779640"
  },
  {
    "text": "think the reality right now is the CPU based models even you can run some",
    "start": "1779640",
    "end": "1786039"
  },
  {
    "text": "models that are even you know 7 billion parameters or something in some quantise version on a CPU you're not going to get",
    "start": "1786039",
    "end": "1793679"
  },
  {
    "text": "the same throughput as you would with hosting the model on a GPU in other words the kind of tokens per second um",
    "start": "1793679",
    "end": "1801480"
  },
  {
    "text": "the speed at which you're generating will be lower but it it's possible to run those what you're not going to do is",
    "start": "1801480",
    "end": "1808200"
  },
  {
    "text": "and this is what I also tell people like the same way that like you're not going to host your microservice for your",
    "start": "1808200",
    "end": "1814159"
  },
  {
    "text": "company on your laptop you're not going to host uh some AI functionality for",
    "start": "1814159",
    "end": "1819679"
  },
  {
    "text": "your company on your laptop usually um that's gonna still live in the cloud but",
    "start": "1819679",
    "end": "1825440"
  },
  {
    "text": "if you have some sort of private use case where something can't leave the laptop or maybe it's like your lap",
    "start": "1825440",
    "end": "1832679"
  },
  {
    "text": "you're deploying laptops in a disaster relief scenario and there's going to be not that much connectivity right it's",
    "start": "1832679",
    "end": "1839760"
  },
  {
    "text": "per it's it's still enough throughput to get responses so if you had like chat",
    "start": "1839760",
    "end": "1845519"
  },
  {
    "text": "over your docs disaster relief docks and that on your laptop like it's still",
    "start": "1845519",
    "end": "1850880"
  },
  {
    "text": "enough to get an answer in that scenario and you know people can push it pretty far I think the difference is just again",
    "start": "1850880",
    "end": "1858159"
  },
  {
    "text": "not one or the other it's the it's the use case I guess yeah I think and just to add to that use case slightly I think",
    "start": "1858159",
    "end": "1864720"
  },
  {
    "text": "there's also disconnected or partially connected mobile platforms where you can't necessarily rely on the cloud",
    "start": "1864720",
    "end": "1871440"
  },
  {
    "text": "access and you have devices that are out there as well I would just to lump that",
    "start": "1871440",
    "end": "1876679"
  },
  {
    "text": "in kind of pulling around full circle for a moment back to the laptops these",
    "start": "1876679",
    "end": "1881760"
  },
  {
    "text": "AI laptops coming out kind of thinking back to you know the the natural kind of",
    "start": "1881760",
    "end": "1887159"
  },
  {
    "text": "segregation of responsibilities that we have in software development aside from just the AI world would you imagine that",
    "start": "1887159",
    "end": "1894679"
  },
  {
    "text": "a reasonable level of support in those would be to to be able to actually do training on on like seven billion you",
    "start": "1894679",
    "end": "1902000"
  },
  {
    "text": "know size models the smaller range that are so you know that are there's so much more of those where that maybe in the",
    "start": "1902000",
    "end": "1908880"
  },
  {
    "text": "not so distant future I'm training a model it's in that range my own laptop can handle that not just for inference",
    "start": "1908880",
    "end": "1915480"
  },
  {
    "text": "but for training and then for very large models that's still cloud-based do you think that that is a reasonable level",
    "start": "1915480",
    "end": "1921639"
  },
  {
    "text": "that that we might see AI laptops able to support I mean I think that you can see some people trying some more",
    "start": "1921639",
    "end": "1928200"
  },
  {
    "text": "training or or rather fine-tuning sorts of things on diverse Hardware I think",
    "start": "1928200",
    "end": "1935559"
  },
  {
    "text": "basically what I'm seeing right now is still primarily inference on local",
    "start": "1935559",
    "end": "1941200"
  },
  {
    "text": "machines and utilization of things like in context learning and rag type",
    "start": "1941200",
    "end": "1946240"
  },
  {
    "text": "workflows to integrate data rather than fine-tuning locally so I think that",
    "start": "1946240",
    "end": "1951519"
  },
  {
    "text": "that's that's kind of the reality of where it is I think there's a",
    "start": "1951519",
    "end": "1957000"
  },
  {
    "text": "possibility in the future maybe of you know some type of training type of",
    "start": "1957000",
    "end": "1964080"
  },
  {
    "text": "scenarios that that will happen maybe not on client devices but spread across",
    "start": "1964080",
    "end": "1969559"
  },
  {
    "text": "client devices so you know it's been a while since we've talked about Federated learning but it will be interesting to",
    "start": "1969559",
    "end": "1977039"
  },
  {
    "text": "see if that kind kind of rears its head in this world I know that there's been efforts to kind of train llm adapters in",
    "start": "1977039",
    "end": "1985679"
  },
  {
    "text": "a Federated way and there's some papers about this and share kind of parameter",
    "start": "1985679",
    "end": "1991120"
  },
  {
    "text": "efficient updates to to weights across different client devices that seems",
    "start": "1991120",
    "end": "1996360"
  },
  {
    "text": "really intriguing to me but yeah I don't know we'll we'll see where it goes maybe I'll be proved wrong but I think I'm",
    "start": "1996360",
    "end": "2002760"
  },
  {
    "text": "increasingly more of a proponent of most people don't need to fine-tune you know",
    "start": "2002760",
    "end": "2009480"
  },
  {
    "text": "a lot of it can be done with Rag and chaining and agents and you know",
    "start": "2009480",
    "end": "2014559"
  },
  {
    "text": "selecting the right models so I think that that especially as models get better that will be the case kind of",
    "start": "2014559",
    "end": "2022080"
  },
  {
    "text": "moving forward but yeah I'm I am looking forward to getting an aipc and you know putting it through its Paces eventually",
    "start": "2022080",
    "end": "2028960"
  },
  {
    "text": "yeah right now my I have to my M2 is through my employer so I'm my next one will be I'm hoping to be an uh either an",
    "start": "2028960",
    "end": "2036600"
  },
  {
    "text": "M4 I'm thinking out possibly a non-apple one as well so the m4s are supposed to",
    "start": "2036600",
    "end": "2042120"
  },
  {
    "text": "supposed to be able to do quite a bit more than than the earlier Generations so looking forward to uh to being able",
    "start": "2042120",
    "end": "2048040"
  },
  {
    "text": "to pursue this yeah yeah definitely well I hope uh our audience we we'll include",
    "start": "2048040",
    "end": "2053320"
  },
  {
    "text": "a few links to some blog posts about these quantization methods and some of the systems like AMA and LM studio and",
    "start": "2053320",
    "end": "2061240"
  },
  {
    "text": "others that we talked about i' encourage everyone to to get hands on and and try your own hand at it and you'll get a",
    "start": "2061240",
    "end": "2067158"
  },
  {
    "text": "sense for for the performance of these models locally so definitely give it a",
    "start": "2067159",
    "end": "2072320"
  },
  {
    "text": "try all right well thanks a lot Daniel that great information today uh was good show and thanks for bringing that yep",
    "start": "2072320",
    "end": "2079280"
  },
  {
    "text": "we'll talk to you [Music]",
    "start": "2079280",
    "end": "2087118"
  },
  {
    "text": "soon all right that is practical AI for this week subscribe now if you haven't",
    "start": "2087119",
    "end": "2093960"
  },
  {
    "text": "already head to practical a.m for all the ways and join our free slack team",
    "start": "2093960",
    "end": "2100760"
  },
  {
    "text": "where you can hang out with Daniel Chris and the entire change log Community sign up today at practical AI fm/ Community",
    "start": "2100760",
    "end": "2110800"
  },
  {
    "text": "thanks again to our partners at fly.io to our beat freaking residence breakmaster cylinder and to you for",
    "start": "2110800",
    "end": "2117280"
  },
  {
    "text": "listening we appreciate you spending time with us that's all for now we'll talk to you again next time",
    "start": "2117280",
    "end": "2123990"
  },
  {
    "text": "[Music]",
    "start": "2123990",
    "end": "2131540"
  },
  {
    "text": "k l",
    "start": "2133119",
    "end": "2136240"
  }
]