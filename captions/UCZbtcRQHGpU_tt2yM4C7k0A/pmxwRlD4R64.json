[
  {
    "text": "there's literally billions of ways in which you can compile the same model on the same Hardware Target the question is",
    "start": "320",
    "end": "5920"
  },
  {
    "text": "how do you pick the fastest one you cannot try them all because they will take too long so you have to use",
    "start": "5920",
    "end": "11320"
  },
  {
    "text": "intuition that's what software Engineers do we replace that intuition with machine learning based optimizations",
    "start": "11320",
    "end": "18520"
  },
  {
    "text": "that learn about how the harder behaves in the pr optimization and uses those models of how the harder behaves to tune",
    "start": "18520",
    "end": "24800"
  },
  {
    "text": "and search the right way of optimizing and compiling your model to the harder Target",
    "start": "24800",
    "end": "29990"
  },
  {
    "text": "[Music] big thanks to our partners Leno fastly and launch Darkly we love Leno they keep",
    "start": "29990",
    "end": "35120"
  },
  {
    "text": "it fast and simple check them out at lin.com changelog our bandwidth is provided by fastly learn more at",
    "start": "35120",
    "end": "41920"
  },
  {
    "text": "fastly.com and get your feature Flags Power by lunch Darkly get a demo at launchd dark.com",
    "start": "41920",
    "end": "48800"
  },
  {
    "text": "this episode is brought to you by our friends at O'Reilly many even know O'Reilly for their animal techbooks and",
    "start": "49480",
    "end": "54840"
  },
  {
    "text": "their conferences but you may not know they have an online learning platform as well the platform has all their books",
    "start": "54840",
    "end": "61239"
  },
  {
    "text": "all their videos and all their conference talks plus you can learn by doing with live online training courses",
    "start": "61239",
    "end": "67280"
  },
  {
    "text": "and virtual conferences certification practice exams and interactive sandboxes and scenarios to practice coding",
    "start": "67280",
    "end": "73880"
  },
  {
    "text": "alongside what you're learning they cover a ton of Technology topics machine learning AI programming languages devops",
    "start": "73880",
    "end": "81240"
  },
  {
    "text": "data science Cloud containers security and even soft skills like business",
    "start": "81240",
    "end": "87040"
  },
  {
    "text": "management and presentation skills you name it it is is all in there if you need to keep your team or yourself up to",
    "start": "87040",
    "end": "93240"
  },
  {
    "text": "speed on their Tech skills then check out ay's online learning platform learn more and keep your team skill sharp at",
    "start": "93240",
    "end": "98759"
  },
  {
    "text": "a.com changelog again a.com [Music]",
    "start": "98759",
    "end": "110449"
  },
  {
    "text": "changelog welcome to practical AI a weekly podcast that makes artificial intelligence practical productive and",
    "start": "110880",
    "end": "117840"
  },
  {
    "text": "accessible to everyone this is where conversations around AI machine learning and data science happen join the",
    "start": "117840",
    "end": "123840"
  },
  {
    "text": "community and slack with us around various topics of the show at cha.com community and follow us on Twitter we at",
    "start": "123840",
    "end": "130080"
  },
  {
    "text": "practical [Music]",
    "start": "130080",
    "end": "136840"
  },
  {
    "text": "aifm welcome to another episode of practical AI this is Daniel whack I am a",
    "start": "136840",
    "end": "143599"
  },
  {
    "text": "data scientist with s International and I'm joined as always by my co-host Chris",
    "start": "143599",
    "end": "149319"
  },
  {
    "text": "Benson who is a principal emerging technology strategist at locked Martin how you doing Chris I'm doing very well",
    "start": "149319",
    "end": "156800"
  },
  {
    "text": "a little bit chilly here in Atlanta I don't know what's going on with that miday we're supposed to be sweltering heat yeah it's a little bit chilly here",
    "start": "156800",
    "end": "163599"
  },
  {
    "text": "as well I don't know what's chilly for Atlanta though depends on your reference frame I don't know I'm guessing it's",
    "start": "163599",
    "end": "169159"
  },
  {
    "text": "like 60 out there or something like that you know okay that sounds pretty delightful to me actually",
    "start": "169159",
    "end": "175640"
  },
  {
    "text": "but I mean if it's not 85 by now it's just not normal so yeah well what what",
    "start": "175640",
    "end": "181440"
  },
  {
    "text": "are you keeping busy with these days just lots of technology stuff at work having a good time with it and I'm super",
    "start": "181440",
    "end": "188120"
  },
  {
    "text": "excited to pause in my day job so that we can have a good conversation about AIML yeah yeah and specifically I know",
    "start": "188120",
    "end": "195720"
  },
  {
    "text": "in my job recently we've been working on a few different deployments of the same",
    "start": "195720",
    "end": "201519"
  },
  {
    "text": "model one to like an edge device which is disconnected from the internet One",
    "start": "201519",
    "end": "207680"
  },
  {
    "text": "Cloud deployment and then another one for an on-prem system and that carries",
    "start": "207680",
    "end": "212920"
  },
  {
    "text": "with it of course all sorts of joy in terms of fitting into resource constraints but also optimizing a model",
    "start": "212920",
    "end": "219920"
  },
  {
    "text": "for for different deployments and I think that's definitely one thing that I don't know if anyone ever pitched being",
    "start": "219920",
    "end": "225840"
  },
  {
    "text": "a data scientist to me but I don't know if that was part of the pitch like optimizing for certain hardware and all",
    "start": "225840",
    "end": "231159"
  },
  {
    "text": "that it's cool but it's definitely sometimes laborious and hard I don't know if you've run into this in your",
    "start": "231159",
    "end": "236840"
  },
  {
    "text": "past yeah you have to be a data scientist in a soft engineer to do that right so you know what struck me as you",
    "start": "236840",
    "end": "243599"
  },
  {
    "text": "were saying that Daniel the thing is is I'm kind of doing a similar thing but being in the defense industry we have a",
    "start": "243599",
    "end": "249720"
  },
  {
    "text": "completely UNS softwar likee name for it we call that joint allain operations",
    "start": "249720",
    "end": "254959"
  },
  {
    "text": "nobody outside this world would ever they're like that's software really that's AI really wait what acronym is",
    "start": "254959",
    "end": "260880"
  },
  {
    "text": "that yeah J that's what we call it we call it J yeah J we call it J and stuff",
    "start": "260880",
    "end": "266639"
  },
  {
    "text": "but you never know I I don't think anyone outside my industry would ever associate J or join alate operations",
    "start": "266639",
    "end": "272280"
  },
  {
    "text": "with software and kubernetes and AI deployment I I would have no idea what that meant if you if you gave those",
    "start": "272280",
    "end": "278600"
  },
  {
    "text": "words to me but I think we'll both be enlightened today quite a bit on this subject I'm really excited because we",
    "start": "278600",
    "end": "285479"
  },
  {
    "text": "have uh Luis C with us who is co-founder and CEO of octoml and professor at",
    "start": "285479",
    "end": "292080"
  },
  {
    "text": "University of Washington uh welcome Lis thank you thanks for having me in the show it sounds like you guys are fun to",
    "start": "292080",
    "end": "297680"
  },
  {
    "text": "talk to can't wait for the conversation here so and it's a nice sunny day in Seattle by the way we don't take",
    "start": "297680",
    "end": "303600"
  },
  {
    "text": "ourselves too seriously trust me oh good I love that that makes even even better just to finish the weather theme of the",
    "start": "303600",
    "end": "310039"
  },
  {
    "text": "conversation it's nice sunny day in Seattle in the 70s you know so I'm wearing short t-shirts here what's up",
    "start": "310039",
    "end": "317320"
  },
  {
    "text": "with that he's in Seattle and he's having the great weather and you know Seattle's famous for rain and we're here in these other parts of the country and",
    "start": "317320",
    "end": "323560"
  },
  {
    "text": "it's just kind of isn't it though I mean Louis correct me if I'm wrong I mean summer times in the Pacific Northwest",
    "start": "323560",
    "end": "329080"
  },
  {
    "text": "are quite delightful aren't they it's just like the winter like rain and fog and that does sound nice yeah yeah",
    "start": "329080",
    "end": "335800"
  },
  {
    "text": "that's great yeah Winters are long but the summers are awesome so there's some balance in the universe and nature here",
    "start": "335800",
    "end": "341360"
  },
  {
    "text": "especially for a native Brazilian like me that matters you know so happy a nice summer yeah well thanks",
    "start": "341360",
    "end": "348400"
  },
  {
    "text": "again for having me on the show it sounded like from your adventures with migani model deployments we're going to have a lot to talk about here so yeah",
    "start": "348400",
    "end": "355199"
  },
  {
    "text": "for sure so before we get into all that maybe you could just give us a bit of a back ground on yourself and how you got",
    "start": "355199",
    "end": "361520"
  },
  {
    "text": "interested in this topic and ended up working in the areas that you're working now from your moment of birth right up",
    "start": "361520",
    "end": "367000"
  },
  {
    "text": "until this recording exactly great perfect so as I was mentioning before I grew up in Brazil and the funny thing is",
    "start": "367000",
    "end": "372560"
  },
  {
    "text": "I like to joke that I mean the 20th year of a three-month internship in the US I was in school in Brazil I was recruited",
    "start": "372560",
    "end": "378599"
  },
  {
    "text": "by IBM research this in the early 2000s to work on the blue jean project there was a three-month internship that became",
    "start": "378599",
    "end": "385039"
  },
  {
    "text": "a year-long co-op that do one thing Le an accident I went to grad school in chanana Illinois close to where you are",
    "start": "385039",
    "end": "391000"
  },
  {
    "text": "and then after that came to University of was as a professor and next thing I know 20 years have gone by so anyways at",
    "start": "391000",
    "end": "396720"
  },
  {
    "text": "IPM I worked on hardare software codesign we were part of this team building the blue jean machine the first",
    "start": "396720",
    "end": "402160"
  },
  {
    "text": "blue jean machine and this was like hardware and software designed for you know very high performance molecular",
    "start": "402160",
    "end": "408599"
  },
  {
    "text": "Dynamic simulation and a lot of it like if you look at the kind of workows that we ran there was high performance linear",
    "start": "408599",
    "end": "414520"
  },
  {
    "text": "algebra right and then you have HPC systems with all sorts of internode",
    "start": "414520",
    "end": "419560"
  },
  {
    "text": "Communications reductions pairwise communication and so on so after IBM I went to grad school in grad school I did",
    "start": "419560",
    "end": "425720"
  },
  {
    "text": "work on hardware and compiler support for speculative parallelization essentially making it easier for folks",
    "start": "425720",
    "end": "431280"
  },
  {
    "text": "to write parallel code by not having to prove that the code is actually parallel and then I came to W at w I started in",
    "start": "431280",
    "end": "438319"
  },
  {
    "text": "my faculty career started working on you know harder sofal design for emerging applications so I had done a bunch of",
    "start": "438319",
    "end": "444080"
  },
  {
    "text": "work on approximate Computing essentially the idea is to take advantage of the fact that some applications don't have to have perfect",
    "start": "444080",
    "end": "451000"
  },
  {
    "text": "execution to have a meaningful and useful answer so right simulations are like that there's multiple valid outputs",
    "start": "451000",
    "end": "456800"
  },
  {
    "text": "and of course machine learning is a huge workload in that category that's how it started actually started working on",
    "start": "456800",
    "end": "462000"
  },
  {
    "text": "hardware and software optimized for machine learning for Energy Efficiency better performance and then about six",
    "start": "462000",
    "end": "468440"
  },
  {
    "text": "years or so ago I started collaborating with folks in in the core machine learning group here at w Carlos geston",
    "start": "468440",
    "end": "475159"
  },
  {
    "text": "who's a friend of mine also Brazilian you know Brazilian transplant to the US although he's Brazilian Argentinian",
    "start": "475159",
    "end": "481360"
  },
  {
    "text": "transplanted to the US and you know has a long history machine learning systems and we were chatting 6 years ago so",
    "start": "481360",
    "end": "487759"
  },
  {
    "text": "there was already a growing set of machine learning models that people are interested in and a growing set of Hardware targets you know it's kind of",
    "start": "487759",
    "end": "493960"
  },
  {
    "text": "crazy to think about that you know six years ago gpus were just starting to get popular for machine learning it feels",
    "start": "493960",
    "end": "500039"
  },
  {
    "text": "like that has always been the case but you know six years ago that was just like kind of like picking up right and",
    "start": "500039",
    "end": "505199"
  },
  {
    "text": "people were starting to think about it A lot's happened since then I know yeah so then now there C you know gpus are picking up and people",
    "start": "505199",
    "end": "511639"
  },
  {
    "text": "start talking about fpgs they had for a while but at that time was getting heated again and then people started",
    "start": "511639",
    "end": "517000"
  },
  {
    "text": "talking about building accelerator so we were wondering like oh kind of interesting that there is this growing set of models already starting to",
    "start": "517000",
    "end": "523719"
  },
  {
    "text": "fragment ecosystem with tensor flow pie torch didn't quite exist yet but there was already some fragmentation going on",
    "start": "523719",
    "end": "528839"
  },
  {
    "text": "and a growing at a harder Target so now imagine this cross product of mod those Frameworks and Hardware targets it's",
    "start": "528839",
    "end": "534519"
  },
  {
    "text": "already becoming a pretty complex cross product so we start think about oh",
    "start": "534519",
    "end": "539640"
  },
  {
    "text": "wouldn't it be nice if we had a common intermediate presentation that allows you do high level mod optimization do",
    "start": "539640",
    "end": "546040"
  },
  {
    "text": "and generate special specialized code for that model own specific Hardware targets so and TN Chen who's a an",
    "start": "546040",
    "end": "553760"
  },
  {
    "text": "incredible machine Learning Systems researcher was Carlos's grad student at that time and Carlos and I started co-",
    "start": "553760",
    "end": "559480"
  },
  {
    "text": "advising TNT and that's that was the Genesis of you know let's look at machine learning model optimization and",
    "start": "559480",
    "end": "566040"
  },
  {
    "text": "compilation as a way to actually optimizing theart machine learning models and I had other grad students there Terry Moro who was working on",
    "start": "566040",
    "end": "573320"
  },
  {
    "text": "Specialized architectures for machine learning got interested as well so he quickly became part of the mix and",
    "start": "573320",
    "end": "578640"
  },
  {
    "text": "started working on fpga backends for machine learning models as part of this mix so that's how I got into that in the end everything boils down to high",
    "start": "578640",
    "end": "584760"
  },
  {
    "text": "performance linear algebra would save for machine learning and I had a history with that at IBM and everything kind of came together approximal Computing high",
    "start": "584760",
    "end": "591920"
  },
  {
    "text": "performance linear algebra and machine learning in this in the Genesis of the TVM project so awesome and maybe we",
    "start": "591920",
    "end": "599040"
  },
  {
    "text": "could just just sort of take a step back you talked about like machine learning compilers we talked about intermediate",
    "start": "599040",
    "end": "604959"
  },
  {
    "text": "representation maybe for that person out there that has taken the corsera course",
    "start": "604959",
    "end": "610519"
  },
  {
    "text": "and they went through the corsera course on deep learning something and hey it never talked about machine learning",
    "start": "610519",
    "end": "616920"
  },
  {
    "text": "compilers they learned maybe how to do a training thing and they created an image",
    "start": "616920",
    "end": "622240"
  },
  {
    "text": "classifier and then they learned how to do an NLP thing and do autocomplete or",
    "start": "622240",
    "end": "627600"
  },
  {
    "text": "something and then they end the course and they know how to use tensor flow and do some data pre-processing stuff and",
    "start": "627600",
    "end": "634040"
  },
  {
    "text": "maybe even a little bit about gpus but they never heard this machine learning compiling could you just sort of yeah",
    "start": "634040",
    "end": "639959"
  },
  {
    "text": "absolutely yeah where does that fit in yeah I'm going to translate all of that you're basically saying wow I'm really",
    "start": "639959",
    "end": "645279"
  },
  {
    "text": "missing the audience here like let me translate all of that here so all right when you're write a typical machine learning model by the way I'm really",
    "start": "645279",
    "end": "652160"
  },
  {
    "text": "glad that machine learning tools are becoming more and more accessible more and more people can build meaningful and",
    "start": "652160",
    "end": "657760"
  },
  {
    "text": "useful machine learning models either starting from an existing one or tweaking and creating new ones I'm so glad that the tools are getting much",
    "start": "657760",
    "end": "664000"
  },
  {
    "text": "better for that but now when you create a machine learning model though in the end you need this model to actually run",
    "start": "664000",
    "end": "669200"
  },
  {
    "text": "well on whatever harder device you on them to run right so today this is typically done by having a set of",
    "start": "669200",
    "end": "675959"
  },
  {
    "text": "libraries that Implement uh parts of your model like you know different layers who have different often",
    "start": "675959",
    "end": "682240"
  },
  {
    "text": "hand-coded implementations and then your high level framework like tensor floor pie torch just gets your model and calls",
    "start": "682240",
    "end": "688360"
  },
  {
    "text": "these hand to libraries that either Nvidia provided or AMD or Intel the",
    "start": "688360",
    "end": "693839"
  },
  {
    "text": "harder vendors typically provide those libraries and then the the framework Stitch everything together to produce",
    "start": "693839",
    "end": "699200"
  },
  {
    "text": "the thing that runs on your Hardware right so which is all uh well and good it's nice and easy as being you know",
    "start": "699200",
    "end": "705279"
  },
  {
    "text": "being able to build pretty complex systems that way except that now as we build richer and richer and more complex",
    "start": "705279",
    "end": "712360"
  },
  {
    "text": "machine learning models that really need to make the most out of the harder they are deployed to and which is great like",
    "start": "712360",
    "end": "718519"
  },
  {
    "text": "so it's kind of crazy that you can build gigantic language models or very complex computer vision models that do a ton of",
    "start": "718519",
    "end": "725240"
  },
  {
    "text": "computation that's really pushing at the limit of how fast you could run you know just by writing on M's law right so you",
    "start": "725240",
    "end": "732160"
  },
  {
    "text": "need to be able to squeeze much more performance out of that so that's where a machine learning compiler comes into play right so when you write a piece of",
    "start": "732160",
    "end": "739399"
  },
  {
    "text": "code in language like C for example you run through a compiler and you run it on your heart attack you don't even think",
    "start": "739399",
    "end": "744639"
  },
  {
    "text": "about yeah you run some comp you run it but for machine learning we don't do that right so you just interpret the model what a machine learning compiler",
    "start": "744639",
    "end": "751000"
  },
  {
    "text": "does is essentially treats this process of going from your model to what runs in your harder as a compiler problem what",
    "start": "751000",
    "end": "757600"
  },
  {
    "text": "do you mean by that I mean by translating your model into a representation as we call intermediate",
    "start": "757600",
    "end": "763120"
  },
  {
    "text": "representation that enables optimizations of your model for example you could fuse a layer with an X One say",
    "start": "763120",
    "end": "771079"
  },
  {
    "text": "a fully connected layer which would be matrix multiplication followed by a convolution that you chose in your model",
    "start": "771079",
    "end": "776440"
  },
  {
    "text": "potentially you can fuse them treat them as a unit and generate nice new fresh",
    "start": "776440",
    "end": "781800"
  },
  {
    "text": "codes that specialize to your model to run on your harder Target so and then as a machine your compiler what you get",
    "start": "781800",
    "end": "788760"
  },
  {
    "text": "that as an end user the benefit that you get is your model becomes a highly optimized executable for your Target",
    "start": "788760",
    "end": "794360"
  },
  {
    "text": "hardware and the difference of performance can be huge like two three sometimes 30 50 x better performance",
    "start": "794360",
    "end": "801040"
  },
  {
    "text": "than your stock high level framework execution time and speed right so does",
    "start": "801040",
    "end": "806839"
  },
  {
    "text": "that make sense essentially you treat that basically what take for granted in writing regular software you know it does that for you in machine learning",
    "start": "806839",
    "end": "813760"
  },
  {
    "text": "right so it produces a fresh highly optimized binary for your model yeah and that makes a lot of sense and it sounds",
    "start": "813760",
    "end": "820199"
  },
  {
    "text": "like a lot of what we're talking about here is performance in the sort of maybe",
    "start": "820199",
    "end": "826680"
  },
  {
    "text": "latency and resource consumption type realm when we're thinking about compiling is that mostly what we're",
    "start": "826680",
    "end": "833399"
  },
  {
    "text": "concerned with or do some of these uh compilations like for example does the",
    "start": "833399",
    "end": "839360"
  },
  {
    "text": "actual performance of like the model's predictions does that ever come into play when you're doing this sort of",
    "start": "839360",
    "end": "845079"
  },
  {
    "text": "optimization as well great question so performance here is ambigous for a computer assistance person performance",
    "start": "845079",
    "end": "850800"
  },
  {
    "text": "typically means how fast it runs but for a data scientist performance also means how good the statistical properties of",
    "start": "850800",
    "end": "856360"
  },
  {
    "text": "your model right so by and large machine learning compilers do not change the accuracy of your model there are",
    "start": "856360",
    "end": "861920"
  },
  {
    "text": "optimizations for example you can do automatic quantization you can quantize parameters in your model that can change",
    "start": "861920",
    "end": "867240"
  },
  {
    "text": "the behavior of your model in that you do trade off some accuracy for better",
    "start": "867240",
    "end": "872519"
  },
  {
    "text": "execution time or for better system performance right but for the most part in fact what we focus on in Apache TVM",
    "start": "872519",
    "end": "879360"
  },
  {
    "text": "our transformation optimizations do not change the accuracy of your model at all even though we do support quantization",
    "start": "879360",
    "end": "884519"
  },
  {
    "text": "so on by and large the way it's used is it just compiles your model Faithfully to run without any changes in accuracy",
    "start": "884519",
    "end": "891519"
  },
  {
    "text": "right so and to type back to your comment earlier of of deploying multiple harders say you know deploying on the",
    "start": "891519",
    "end": "897160"
  },
  {
    "text": "edge getting the right right performance and using the right amount of resources that fits in your heart is something",
    "start": "897160",
    "end": "903000"
  },
  {
    "text": "that's an extremely laborious task right so if you have a model that's going to run a Raspberry Pi like device say it's",
    "start": "903000",
    "end": "909880"
  },
  {
    "text": "computer vision and the first version of your model runs at half a frame per second right and uses too much memory",
    "start": "909880",
    "end": "918120"
  },
  {
    "text": "and doesn't leave any other computer resources for the other things you need to run in your device you can't really deploy it right so and our experience",
    "start": "918120",
    "end": "925440"
  },
  {
    "text": "has been with several users and customers is that the process of getting a model that is ready from a data",
    "start": "925440",
    "end": "931000"
  },
  {
    "text": "science point of view from the accuracy point of view to be able to be deployed it could take weeks to months of",
    "start": "931000",
    "end": "936560"
  },
  {
    "text": "Hardcore software engineering work and that's what we want to automate with with Apache TVM and all the machine",
    "start": "936560",
    "end": "942720"
  },
  {
    "text": "learning compilers do similar things but you know Apache TVM is just especially good at doing that because it uses",
    "start": "942720",
    "end": "948279"
  },
  {
    "text": "essentially machine learning for machine learning the process of translating your model to an executable like a Deployable",
    "start": "948279",
    "end": "955000"
  },
  {
    "text": "artifact uh it's a search problem so there's literally billions of ways in which you can compile the same model on",
    "start": "955000",
    "end": "961279"
  },
  {
    "text": "the same Hardware Target the question is how do you pick the fastest one you cannot try them all because they'll take",
    "start": "961279",
    "end": "966519"
  },
  {
    "text": "too long so you have to use intuition that's what software Engineers do we replace that intuition with machine",
    "start": "966519",
    "end": "973680"
  },
  {
    "text": "learning base optimizations that learn about how the hardware behaves in the pr",
    "start": "973680",
    "end": "978880"
  },
  {
    "text": "of optimization and uses those models of how the hardare behaves to tune and search the right way of optimizing and",
    "start": "978880",
    "end": "984199"
  },
  {
    "text": "compiling your model to the harder Target",
    "start": "984199",
    "end": "990920"
  },
  {
    "text": "this episode is brought to you by snowplow analytics snowplow is the behavioral data management platform for",
    "start": "991120",
    "end": "997680"
  },
  {
    "text": "data teams maximize the value of your behavioral data using snowplow insights",
    "start": "997680",
    "end": "1003120"
  },
  {
    "text": "a managed data platform that's built on leading open source Tech leveraged by tens of thousands of users capture and",
    "start": "1003120",
    "end": "1010680"
  },
  {
    "text": "process high quality behavioral data from all your platforms and your products and deliver that data to your",
    "start": "1010680",
    "end": "1015759"
  },
  {
    "text": "Cloud destination of choice when marketing needs to make data informed decisions when product needs Next Level",
    "start": "1015759",
    "end": "1021639"
  },
  {
    "text": "understanding and when analytics needs rich and accurate data snowplow is the solution for data teams who want to",
    "start": "1021639",
    "end": "1027199"
  },
  {
    "text": "manage the collection processing and warehousing of data across all their platforms and products get started and",
    "start": "1027199",
    "end": "1033400"
  },
  {
    "text": "experience no plow data for yourself at snowplow analytics.com again snowplow",
    "start": "1033400",
    "end": "1038678"
  },
  {
    "text": "analytics.com [Music]",
    "start": "1038679",
    "end": "1047109"
  },
  {
    "text": "[Music] so Louise I'm curious there's one of the",
    "start": "1049430",
    "end": "1056880"
  },
  {
    "text": "hardest problems in running this podcast I think is just the wide variety of jargon that is used throughout all sorts",
    "start": "1056880",
    "end": "1063600"
  },
  {
    "text": "of different areas of ML and I know that on this podcast we've talked before about model serialization so we've got",
    "start": "1063600",
    "end": "1070640"
  },
  {
    "text": "something like an Onyx project for example where you know maybe I could",
    "start": "1070640",
    "end": "1075799"
  },
  {
    "text": "quote save a model to Onyx and maybe like loaded into pie torch or a pie",
    "start": "1075799",
    "end": "1082520"
  },
  {
    "text": "torch model into tensorflow it's this sort of overlapping serialization format",
    "start": "1082520",
    "end": "1087799"
  },
  {
    "text": "so as people are thinking about kind of saving and serializing their models where does maybe this compilation fit in",
    "start": "1087799",
    "end": "1095559"
  },
  {
    "text": "in terms of the developer workflow like let's say that I have my model created",
    "start": "1095559",
    "end": "1102080"
  },
  {
    "text": "or I've trained it let's say I'm in tensorflow and i' I've decided on a way to serialize it so I've created a I've",
    "start": "1102080",
    "end": "1108799"
  },
  {
    "text": "output a file that corresponds to my serialized model what happens next what's the workflow look like after that",
    "start": "1108799",
    "end": "1115720"
  },
  {
    "text": "in terms of Apache TVM and the compilation process great question so",
    "start": "1115720",
    "end": "1121200"
  },
  {
    "text": "there's many ways in which Apache TVM ingests your model one is exactly what we talked about so there's a front end",
    "start": "1121200",
    "end": "1127720"
  },
  {
    "text": "to Apache TVM that ingests a model that has been serialized into Onyx right so you just Imports the model and then you",
    "start": "1127720",
    "end": "1134760"
  },
  {
    "text": "specify the harder Target and then you wait for a little bit and then you get your effect ready you're executable for",
    "start": "1134760",
    "end": "1140240"
  },
  {
    "text": "your model and packag it in various ways but there are also ways in which you can call TVM directly from the code that",
    "start": "1140240",
    "end": "1147919"
  },
  {
    "text": "specifies your model so you can do that from tensor flow from pytorch from mxnet or Caris and so on that you do",
    "start": "1147919",
    "end": "1153840"
  },
  {
    "text": "essentially Imports TVM and then you import you load your model graph into the TVM representation and then you",
    "start": "1153840",
    "end": "1159600"
  },
  {
    "text": "choose your harder Target and then you compile right so there's just two ways in which you can interface with TVM",
    "start": "1159600",
    "end": "1165280"
  },
  {
    "text": "either via you know the serialized model or by just EMB a few lines of codes to call TVM so in the flow of from data to",
    "start": "1165280",
    "end": "1172559"
  },
  {
    "text": "your deployed model right you create the data curates the data create a training set specify model architecture sometimes",
    "start": "1172559",
    "end": "1178360"
  },
  {
    "text": "a little bit of architecture search you arrive at your model architecture you train it and then you able to actually",
    "start": "1178360",
    "end": "1183480"
  },
  {
    "text": "test and validate your model then you're happy with it so at that point your model has a right statistical properties",
    "start": "1183480",
    "end": "1188760"
  },
  {
    "text": "you know it does what you're supposed to do and what you want to do you want to make it run fast right so now you have a trained model at that point that's what",
    "start": "1188760",
    "end": "1195000"
  },
  {
    "text": "you hand off to say Apache TVM either via on spherize models or via calling",
    "start": "1195000",
    "end": "1200720"
  },
  {
    "text": "directly the the second then you specify your harder targets and then you click compile right for you call compile",
    "start": "1200720",
    "end": "1207559"
  },
  {
    "text": "that's when TVM does its high level and lowlevel optimization magic and also uses this machine learning for machine",
    "start": "1207559",
    "end": "1212600"
  },
  {
    "text": "learning engine to tune the code for your specific model on your harder Target so I'm curious I got maybe",
    "start": "1212600",
    "end": "1219640"
  },
  {
    "text": "several questions that I'm going to try to combine into one a little bit and you can segment them any way you want if you",
    "start": "1219640",
    "end": "1224840"
  },
  {
    "text": "go through that optimization process a what does that output look like how is that different from the model before you",
    "start": "1224840",
    "end": "1232200"
  },
  {
    "text": "took it through the optimization process in terms of how you approach inference on it and what are the limits in terms",
    "start": "1232200",
    "end": "1240240"
  },
  {
    "text": "of that Target architecture that you're trying to hit you mentioned the Raspberry Pi and there's in this day and age there are tons and tons of kind of",
    "start": "1240240",
    "end": "1247600"
  },
  {
    "text": "low capability or low power targets that you might want to run a model that otherwise would have been impossible can",
    "start": "1247600",
    "end": "1253919"
  },
  {
    "text": "you describe kind of what that looks like after that optimization and what the limits are on it yeah great okay so",
    "start": "1253919",
    "end": "1259799"
  },
  {
    "text": "let me tell first what it looks like so the output is really a executable it's",
    "start": "1259799",
    "end": "1265440"
  },
  {
    "text": "just an executable code for your model that includes you know your model exec for your model plus a runtime like the",
    "start": "1265440",
    "end": "1272720"
  },
  {
    "text": "runtime will be it's like a support for your model that's tuned for that harder Target with TVM you get a custom package",
    "start": "1272720",
    "end": "1279480"
  },
  {
    "text": "binary for your model the way you call it you load that binary and then there's an API to call it could be a shared",
    "start": "1279480",
    "end": "1285000"
  },
  {
    "text": "library for example one way of packaging this whole thing up is is a DOS so library or a dlll in Windows where your",
    "start": "1285000",
    "end": "1291200"
  },
  {
    "text": "model is just fresh executable code with an API that you call to do inference on does that answer your question this this",
    "start": "1291200",
    "end": "1296360"
  },
  {
    "text": "is what it looks like as the output and then there's many ways of making it standard like you can put in a python wheel and have python bindings for it",
    "start": "1296360",
    "end": "1302720"
  },
  {
    "text": "there's many ways to make it easy to call it yeah yeah it does like as a followup and I'm going to say something",
    "start": "1302720",
    "end": "1308559"
  },
  {
    "text": "Extreme as we're building bigger and bigger models and we're taking you know things like gpt3 and things that are",
    "start": "1308559",
    "end": "1313919"
  },
  {
    "text": "that are very large and we have this once upon a time unrealistic expectation",
    "start": "1313919",
    "end": "1319440"
  },
  {
    "text": "of putting him into places that you would be like I think Chris wants to run gpt3 on his Smartwatch you are closer",
    "start": "1319440",
    "end": "1325480"
  },
  {
    "text": "than you realize on that one so so absolutely no and we had people asking",
    "start": "1325480",
    "end": "1330679"
  },
  {
    "text": "about this yeah there's a limit to that right so you can't get around physics right so even if you compress max if it",
    "start": "1330679",
    "end": "1336760"
  },
  {
    "text": "doesn't fit in the memory that you have it just doesn't fit or if he uses you know so much computer it's going to take",
    "start": "1336760",
    "end": "1342360"
  },
  {
    "text": "to long to do the inference you know then it's not going to be an useful output right and of course in the process of searching what Hardware",
    "start": "1342360",
    "end": "1349039"
  },
  {
    "text": "makes sense you can use a tool like the octamizer let me just put a quick plug in here so Optimizer is a soft as a",
    "start": "1349039",
    "end": "1355559"
  },
  {
    "text": "service platform that OCT M built that uses Apache TVM as its engine and it's like a very easy to use you know TVM as",
    "start": "1355559",
    "end": "1362960"
  },
  {
    "text": "as you can tell business conversation is likely to be a more sophisticated stack for you know more General data",
    "start": "1362960",
    "end": "1369320"
  },
  {
    "text": "scientists right so and general data scientists already have enough to worry about making models that do the right",
    "start": "1369320",
    "end": "1375000"
  },
  {
    "text": "thing is what they should focus on with the optimizer we are raising have abstraction to match that so you can",
    "start": "1375000",
    "end": "1380520"
  },
  {
    "text": "upload your model it's a nice graphical user interface where you upload your mod it tells you what the layers are you tell what the input layer is you can",
    "start": "1380520",
    "end": "1386600"
  },
  {
    "text": "click on the harder targets that you want or you can have it choose for you by running across them all and see which",
    "start": "1386600",
    "end": "1392200"
  },
  {
    "text": "ones it runs best at so you can get the highest througho per dollar or it can hit the you know we do support raspberry",
    "start": "1392200",
    "end": "1398200"
  },
  {
    "text": "pies now so you s this you run Raspberry Pi if it doesn't you're just going to say hey I couldn't run this model there so then you know that even doing all",
    "start": "1398200",
    "end": "1404240"
  },
  {
    "text": "this magic it does not run at the harder Target right so anyway so back what you're asking so now on the limits how",
    "start": "1404240",
    "end": "1410400"
  },
  {
    "text": "do we know we can get around physics right so there's only so much compute resources that you can afford in an edge setting but there plenty of new",
    "start": "1410400",
    "end": "1416960"
  },
  {
    "text": "techniques that actually can twist and turn your model to make it fit one for example specifically for language models",
    "start": "1416960",
    "end": "1423000"
  },
  {
    "text": "that you brought up something that we've done quite a bit of work on is support for sparsity right so these language",
    "start": "1423000",
    "end": "1428520"
  },
  {
    "text": "models are giant but there are a lot of zeros in it so as we all know lots of zero is easy to compress right so and if",
    "start": "1428520",
    "end": "1434720"
  },
  {
    "text": "you're going to multiply something by zero you don't even need to multiply it right so and and if it's a bunch of zeros why you going to write a bunch of",
    "start": "1434720",
    "end": "1440760"
  },
  {
    "text": "zeros in memory just say like hey you know only write the nonzero values sure you can do that and that actually goes a",
    "start": "1440760",
    "end": "1447159"
  },
  {
    "text": "long ways in using last memory and using last comput Al don't go as far as saying that we can quite run gpt3 like models",
    "start": "1447159",
    "end": "1454440"
  },
  {
    "text": "on C device but we're getting close to running pretty large models because of good sparse uh Computing supports and",
    "start": "1454440",
    "end": "1460720"
  },
  {
    "text": "also you know other forms of compression and conation that makes big models fit in Edge devices so just to follow up on",
    "start": "1460720",
    "end": "1467360"
  },
  {
    "text": "the the inference side of things so maybe this is part of what you're building with octoml because like you",
    "start": "1467360",
    "end": "1474080"
  },
  {
    "text": "were saying Apache TVM maybe it's a lower level thing for real experts octoml maybe is some more accessible",
    "start": "1474080",
    "end": "1480799"
  },
  {
    "text": "yeah has a bunch of convenience built in yeah so I'm I'm curious on that inference side maybe you could contrast",
    "start": "1480799",
    "end": "1487240"
  },
  {
    "text": "the two like if I compile a model with Apache TVM you mentioned sort of python",
    "start": "1487240",
    "end": "1492720"
  },
  {
    "text": "wrappings around that output model and maybe there's other language wrappings",
    "start": "1492720",
    "end": "1497960"
  },
  {
    "text": "is that as simple as sort of importing a python library and then importing your compiled model and running a an",
    "start": "1497960",
    "end": "1504520"
  },
  {
    "text": "inference or or what other sort of workflow changes might you have to do to run a patchy TVM compiled model in terms",
    "start": "1504520",
    "end": "1512399"
  },
  {
    "text": "of the pythonic ways that people sort of are used to doing things no it's exactly",
    "start": "1512399",
    "end": "1517480"
  },
  {
    "text": "what you said it's two lines of code so the results in Python whe you you import it and you call inference on it and",
    "start": "1517480",
    "end": "1523000"
  },
  {
    "text": "that's it that's that's awesome yeah and that's the experience that we offer with the optimizer and then the the optimizer",
    "start": "1523000",
    "end": "1528919"
  },
  {
    "text": "also has an API call by the way so if you want to embed the model optimization compilation to your workflow you can use",
    "start": "1528919",
    "end": "1536279"
  },
  {
    "text": "the optimizer I don't mean to give a hard cell here I'm just saying it's super easy like TVM itself you have to",
    "start": "1536279",
    "end": "1541919"
  },
  {
    "text": "set up your task framework you have to spin up some instances to run this Auto tuning and this you know optimization",
    "start": "1541919",
    "end": "1548559"
  },
  {
    "text": "tuning component you have to collect the data for the machine learning model based optimizations and so on and it's",
    "start": "1548559",
    "end": "1554640"
  },
  {
    "text": "work it pays off but it's work I have to put up front right so with the o you can replace that with two lines of code in",
    "start": "1554640",
    "end": "1560640"
  },
  {
    "text": "the API you upload the model via the API and then you specify your harder targets",
    "start": "1560640",
    "end": "1566399"
  },
  {
    "text": "and then you start the optimization process and then when it's ready you can download the resulting wi that you can",
    "start": "1566399",
    "end": "1572320"
  },
  {
    "text": "import into your workflow right so you can do all of that either via the web interface or via this API awesome yeah",
    "start": "1572320",
    "end": "1580279"
  },
  {
    "text": "so I know I'm super interested in octoml and I know that you're kind of in in beta now it doesn't have such a long",
    "start": "1580279",
    "end": "1587279"
  },
  {
    "text": "history so maybe you could tell us you know how that came about if it was sort of natural feeling some of these pains",
    "start": "1587279",
    "end": "1593760"
  },
  {
    "text": "of the lower level things you know expertise in in Apache TVM and the desire to have that be more automated",
    "start": "1593760",
    "end": "1600320"
  },
  {
    "text": "and have some convenience around it could you give us a little bit more of the motivation there and your thought process with octoml yeah absolutely yeah",
    "start": "1600320",
    "end": "1607240"
  },
  {
    "text": "so maybe I can do a quick Once Upon a Time on octl as well so we had worked hard on Apache TVM you know started as a",
    "start": "1607240",
    "end": "1612720"
  },
  {
    "text": "research project and got quite a bit of traction you know being deployed in you know Amazon Facebook Microsoft and so on",
    "start": "1612720",
    "end": "1618000"
  },
  {
    "text": "and when running one of our conferences we're like oh wow there's a quite a bit of Interest here but the folks that were attending were really you know machine",
    "start": "1618000",
    "end": "1625080"
  },
  {
    "text": "learning researchers Engineers or you know folks that have data science and machine learning experience as well as",
    "start": "1625080",
    "end": "1630240"
  },
  {
    "text": "software engineering experience we we said oh it's it's cool we want more people to use it and of course we going",
    "start": "1630240",
    "end": "1635720"
  },
  {
    "text": "to continue catering and growing the open source community so that's why we formed o tol to invest in Apache TVM and",
    "start": "1635720",
    "end": "1642440"
  },
  {
    "text": "grow the community grow the ecosystem work closely with Hardware vendors to enable more Hardware Targets on TVM TVM",
    "start": "1642440",
    "end": "1650039"
  },
  {
    "text": "today you know has a pretty broad set of Hardware targets like it has you know server CPUs from Intel and AMD and arm",
    "start": "1650039",
    "end": "1657320"
  },
  {
    "text": "CPUs and mobile and server right so Nvidia gpus arm gpus Intel gpus AMD GPS",
    "start": "1657320",
    "end": "1664840"
  },
  {
    "text": "AMD APS we have fpj support have a broads set and a lot and a lot of these were done with us in partnership with",
    "start": "1664840",
    "end": "1671440"
  },
  {
    "text": "harder vendors and some harder vendors that go and put the work into the open source package and the reason that I keep emphasizing open source here is",
    "start": "1671440",
    "end": "1677760"
  },
  {
    "text": "that I don't think it'll be sustainable to have a project of the diversity that",
    "start": "1677760",
    "end": "1684640"
  },
  {
    "text": "TVM needs or any sort of machine learning compiler needs to thrive because you want diversity of models",
    "start": "1684640",
    "end": "1690240"
  },
  {
    "text": "that he supports Frameworks and hard targets the only way you make the diversity manageable in my opinion is by",
    "start": "1690240",
    "end": "1695399"
  },
  {
    "text": "having an open and welcoming community that even competitors can collaborate kind of like Linux you know Linux is a",
    "start": "1695399",
    "end": "1701360"
  },
  {
    "text": "great story of success there we want to and that's what we want with TVM so and what we wanted to do with octl is enable",
    "start": "1701360",
    "end": "1707600"
  },
  {
    "text": "more harder targets and work with vendors but then also make the power of TVM and machine learning model",
    "start": "1707600",
    "end": "1712679"
  },
  {
    "text": "optimization compilation accessible to the broadest set of users possible right",
    "start": "1712679",
    "end": "1718159"
  },
  {
    "text": "and the way we approach that is by building a high level service that is makes it very very easy and fully you",
    "start": "1718159",
    "end": "1723799"
  },
  {
    "text": "know so natural that You' never choose not to use it right so and that's really the goal of the optimizer and we made it",
    "start": "1723799",
    "end": "1730399"
  },
  {
    "text": "into a SAS offering because machine learning moves fast as you know right so models change fast there's new harder",
    "start": "1730399",
    "end": "1736880"
  },
  {
    "text": "targets almost every other week right so the best way to that is actually package that into a service because then we can",
    "start": "1736880",
    "end": "1742120"
  },
  {
    "text": "take care of all of the you know complicated lower level things that you you don't want your data scientist",
    "start": "1742120",
    "end": "1747760"
  },
  {
    "text": "spending time on so you've Gott me pretty excited about it as we've gone through this and how with different",
    "start": "1747760",
    "end": "1753399"
  },
  {
    "text": "people having their practical workflows that they're using and you know whether it's tensor flow or pytorch and they",
    "start": "1753399",
    "end": "1760159"
  },
  {
    "text": "have a deployment string attached to it and I know that you've mentioned it targets several Frameworks we've talked",
    "start": "1760159",
    "end": "1765679"
  },
  {
    "text": "about you know the the kind of the two biggies and stuff can you talk a little bit about how first on the Apache TVM",
    "start": "1765679",
    "end": "1771480"
  },
  {
    "text": "side and then talk about where we optimally can use octoml but try to give me a sense a practical sense of how do I",
    "start": "1771480",
    "end": "1778960"
  },
  {
    "text": "get that into the my work stream what do I need what are limitations what should I avoid what should I do I'm just trying",
    "start": "1778960",
    "end": "1784399"
  },
  {
    "text": "to get a sense of how do I get started if I'm listening that you've sold us on what to do here and rubber me the road",
    "start": "1784399",
    "end": "1789880"
  },
  {
    "text": "kind of moment yeah absolutely yeah so if you are so inclined and you and you and you want to do some software",
    "start": "1789880",
    "end": "1795159"
  },
  {
    "text": "engineering Adventures you should definitely go to tvm.com.mt right there's plenty of tutorials there on how you can get started uh you know there's",
    "start": "1795159",
    "end": "1801519"
  },
  {
    "text": "a tsor flow example there's a pie torch example there's an nonyx example and you can just go there and that's how you use",
    "start": "1801519",
    "end": "1806600"
  },
  {
    "text": "the open source offering there right so now there's many ways in which you can use open source Apache TVM right so you",
    "start": "1806600",
    "end": "1814360"
  },
  {
    "text": "can use the just as a compiler know what we call autot tuning autotuning is the machine learning based magic that I told",
    "start": "1814360",
    "end": "1820559"
  },
  {
    "text": "you about that searches for better implementations for that you have to do a little bit more work right so you have",
    "start": "1820559",
    "end": "1825760"
  },
  {
    "text": "to set up a benchmark infrastructure rure it could be just your machine but then it takes a little bit longer because it need to run a bunch of",
    "start": "1825760",
    "end": "1831519"
  },
  {
    "text": "experiments it's so there's there's tutorials for that if you have more experience with the lower level parts of",
    "start": "1831519",
    "end": "1838039"
  },
  {
    "text": "your framework and you're ready to use you know a stack like Apache TVM I'll start with that route try and run",
    "start": "1838039",
    "end": "1843760"
  },
  {
    "text": "through the examples there now if you have some work to do and you want to get your model ready quickly and you want to",
    "start": "1843760",
    "end": "1850440"
  },
  {
    "text": "enjoy some automation you can use the octamizer because it's a full size offering then you what you do today the",
    "start": "1850440",
    "end": "1856720"
  },
  {
    "text": "way we support this is seral your model to Onyx you upload the model and then once you upload it there's all sorts of",
    "start": "1856720",
    "end": "1862360"
  },
  {
    "text": "Hardware targets that you can click there's a checkbox for NVIDIA gpus Intel CPUs and Raspberry Pi you can choose the",
    "start": "1862360",
    "end": "1868120"
  },
  {
    "text": "ones that you want and then you click optimize and then you got a notification when your workflow is done you get all",
    "start": "1868120",
    "end": "1873880"
  },
  {
    "text": "the performance comparison across all the hardare targets and across uh different different ways of of compiling your model so so between the the",
    "start": "1873880",
    "end": "1881399"
  },
  {
    "text": "optimizer and the you know having the Baseline open source project you can kind of choose the level of abstraction",
    "start": "1881399",
    "end": "1887600"
  },
  {
    "text": "that you want to get into for what your workflow is so you have you have choice that way right you do have Choice yes",
    "start": "1887600",
    "end": "1893200"
  },
  {
    "text": "you have choice of how you want to take advantage of that you can go through the Apache TVM routes you know then you do it and we'll support that there's a",
    "start": "1893200",
    "end": "1899360"
  },
  {
    "text": "driving community that will help you now if you want really want to get started from day Zero not have to worry about",
    "start": "1899360",
    "end": "1904600"
  },
  {
    "text": "that then you go to the optimizer either using the web interface or the API and then you can use our support and you",
    "start": "1904600",
    "end": "1910840"
  },
  {
    "text": "also you got access to our uh ready to go uh machine learning models for the harder targets that you care",
    "start": "1910840",
    "end": "1916840"
  },
  {
    "text": "about",
    "start": "1916840",
    "end": "1919840"
  },
  {
    "text": "change log Plus+ is the best way for you to directly support practical AI join",
    "start": "1927919",
    "end": "1934200"
  },
  {
    "text": "today and unlock access to a private feed that makes the ads disappear gets you closer to the metal and help sustain",
    "start": "1934200",
    "end": "1941440"
  },
  {
    "text": "our production of practical AI into the future simply follow the Chang log Plus+",
    "start": "1941440",
    "end": "1947440"
  },
  {
    "text": "link in your show notes or Point your favorite web browser to",
    "start": "1947440",
    "end": "1953240"
  },
  {
    "text": "[Music]",
    "start": "1957850",
    "end": "1960970"
  },
  {
    "text": "[Music]",
    "start": "1963770",
    "end": "1967339"
  },
  {
    "text": "[Music]",
    "start": "1969400",
    "end": "1974619"
  },
  {
    "text": "we mentioned a few times is Onyx and I think that sounds like some of what",
    "start": "1977840",
    "end": "1983120"
  },
  {
    "text": "you're sort of centralizing around with octoml is is Onyx maybe as a recommendation since you mentioned it a",
    "start": "1983120",
    "end": "1988840"
  },
  {
    "text": "couple times could you maybe just give those who aren't familiar brief definition of what we're talking about",
    "start": "1988840",
    "end": "1994200"
  },
  {
    "text": "with Onyx and and also maybe from your perspective as someone maybe not working on onyx dayto day but working on",
    "start": "1994200",
    "end": "2001440"
  },
  {
    "text": "something that depends on that um how you see that that project progressing",
    "start": "2001440",
    "end": "2006559"
  },
  {
    "text": "and and the momentum with great great question I want to emphasize that even though you onx several times you know we",
    "start": "2006559",
    "end": "2012559"
  },
  {
    "text": "do support directly you know if you go Str tensor flow or pytorch and so on and there's no I do tend to like what Onyx",
    "start": "2012559",
    "end": "2019519"
  },
  {
    "text": "aims to do because Onyx is just a way of representing your model essentially at the highest level is a description",
    "start": "2019519",
    "end": "2025240"
  },
  {
    "text": "language so you can you have your model you buil it in memory you just specified in whatever framework you you like to",
    "start": "2025240",
    "end": "2030279"
  },
  {
    "text": "use and you can you want to store it right so you need to describe it somehow so Onyx is a agreed upon Way by you know",
    "start": "2030279",
    "end": "2037279"
  },
  {
    "text": "multiple players in the space that this is a good way in which you can describe a machine learning model that includes",
    "start": "2037279",
    "end": "2043519"
  },
  {
    "text": "the computational structure your layers what each you know operator does as well as all of your parameters gets you know",
    "start": "2043519",
    "end": "2049520"
  },
  {
    "text": "calized in one single package right so Onyx is evolving and it has its ups and",
    "start": "2049520",
    "end": "2054919"
  },
  {
    "text": "downs and I think right now you know people are getting more excited you're still excited about it then there's this I think it's an uptick now I'm sure",
    "start": "2054919",
    "end": "2061760"
  },
  {
    "text": "there will be other model description languages and exchange formats right so that would pop up and we we already to",
    "start": "2061760",
    "end": "2067960"
  },
  {
    "text": "support those as well I do think that it's good to have at least one format of storing models that is generally adopted",
    "start": "2067960",
    "end": "2075919"
  },
  {
    "text": "like widely adopted because then you know if you keep your models that way chances are that the software components",
    "start": "2075919",
    "end": "2081320"
  },
  {
    "text": "that you need in your workflow will support your model right so this field is evolving so rapidly right now and you",
    "start": "2081320",
    "end": "2088679"
  },
  {
    "text": "have not only each framework's kind of way of doing things but like I'm looking through the tvm.com.mt",
    "start": "2088679",
    "end": "2096720"
  },
  {
    "text": "TVM and there's so many different options and I can't help but wonder you're covering so many architectures",
    "start": "2097680",
    "end": "2104440"
  },
  {
    "text": "and with all those changes happening and and this is happening at light speed all the time we're constantly getting bombarded with new things and I know",
    "start": "2104440",
    "end": "2110920"
  },
  {
    "text": "that I as a practitioner struggle to keep up at times with all the things how do you do that how do you keep the",
    "start": "2110920",
    "end": "2116200"
  },
  {
    "text": "project going keep it current keep all these new things coming out I'm assuming you don't sleep yeah a great question no",
    "start": "2116200",
    "end": "2122720"
  },
  {
    "text": "I do I do sleep and thank you and one thing that makes me sleep yeah is I I I have to keep my",
    "start": "2122720",
    "end": "2128880"
  },
  {
    "text": "natural neuro Network working the way we do that is by having good sleep and a glass of wine on Fridays you know so",
    "start": "2128880",
    "end": "2134280"
  },
  {
    "text": "glass of wine a very important part very important yeah absolutely um so the",
    "start": "2134280",
    "end": "2139599"
  },
  {
    "text": "answer is how do we keep up is really having a strong Community nurturing it and being player in it encouraging more",
    "start": "2139599",
    "end": "2146119"
  },
  {
    "text": "folks to participate and looking back I mean I'm very grateful to the community and I think we were lucky to have being",
    "start": "2146119",
    "end": "2151839"
  },
  {
    "text": "involved in helping catalyze that Community because somehow luckily TVM was able to capture the interest of",
    "start": "2151839",
    "end": "2158560"
  },
  {
    "text": "folks that build the Frameworks folks that build models because new classes of models say when you there were recurring",
    "start": "2158560",
    "end": "2164319"
  },
  {
    "text": "new networks we had to go support that in TVM once you have Dynamic models with Dynamic shapes all these things that you",
    "start": "2164319",
    "end": "2169880"
  },
  {
    "text": "don't need to know what it is but essentially different aspects of your model that make them more powerful and more General needs to be supported in",
    "start": "2169880",
    "end": "2176280"
  },
  {
    "text": "TVM all of that we're actually contributed by community members and we help make that happen we we put a lot of",
    "start": "2176280",
    "end": "2181599"
  },
  {
    "text": "workers Sal too but then the harder vendors you know so the harder vendors are the ones that actually really feel",
    "start": "2181599",
    "end": "2187599"
  },
  {
    "text": "the pain to be honest of this growing complexity of the ecosystem that you put very well Chris you know so harder",
    "start": "2187599",
    "end": "2194119"
  },
  {
    "text": "vendors today they have to write libraries lowlevel libraries that are tuned for their harder targets for each",
    "start": "2194119",
    "end": "2200119"
  },
  {
    "text": "one of the major operators in these models every time models change have to go in tun it again so they're always",
    "start": "2200119",
    "end": "2205319"
  },
  {
    "text": "like having to update and that that's not sustainable right so that's why TVM automating a lot of that made it very",
    "start": "2205319",
    "end": "2212240"
  },
  {
    "text": "attractive for them to contribute to TVM and they want it to be open source because they also want to enjoy the",
    "start": "2212240",
    "end": "2218040"
  },
  {
    "text": "exemplification effect that the community has so since we incubated Apache TVM into the Apache software",
    "start": "2218040",
    "end": "2225359"
  },
  {
    "text": "Foundation there was even more interest and and Industry became more comfortable in contributing because now there is",
    "start": "2225359",
    "end": "2231640"
  },
  {
    "text": "professional independent governance of the project because before was you know a few grad students and a couple folks",
    "start": "2231640",
    "end": "2237319"
  },
  {
    "text": "in you know some contributors sitting in the room or you know sitting in a virtual room or Folks at the University",
    "start": "2237319",
    "end": "2242680"
  },
  {
    "text": "of Washington too anyway so that was a long answer to your question but basically it is by having a open source",
    "start": "2242680",
    "end": "2249200"
  },
  {
    "text": "community and having the right incent technical incentives for folks to contribute to it that's how we deal with the growing diversity so I'm curious",
    "start": "2249200",
    "end": "2257319"
  },
  {
    "text": "more about that open- source side so could you give maybe there's people out there listening that are working on what",
    "start": "2257319",
    "end": "2264640"
  },
  {
    "text": "they feel like might be the next really cool AI practitioner tooling or data",
    "start": "2264640",
    "end": "2270560"
  },
  {
    "text": "science developer tool or something they want to get this project out there they",
    "start": "2270560",
    "end": "2275960"
  },
  {
    "text": "want to have it be an open source project and get other people involved do",
    "start": "2275960",
    "end": "2281480"
  },
  {
    "text": "you have any tips for those sorts of people out there that are working on tooling working on new things in terms",
    "start": "2281480",
    "end": "2287520"
  },
  {
    "text": "of helping them understand how they might Foster a community around those things and maybe get a little bit of",
    "start": "2287520",
    "end": "2293240"
  },
  {
    "text": "momentum going what are some of maybe the key points with that that's a great question I'll say first uh recruit early",
    "start": "2293240",
    "end": "2299440"
  },
  {
    "text": "users and truly listen to them and make them feel like they're part of your adventure here and then you're helping them succeed and their success is your",
    "start": "2299440",
    "end": "2306040"
  },
  {
    "text": "success right so like we got lucky and we're fortunate that we had early users that were very involved in giving us",
    "start": "2306040",
    "end": "2312359"
  },
  {
    "text": "feedback and we you know by showing that you care about their feedback and Implement quickly and then that catalyzes the process right so you kind",
    "start": "2312359",
    "end": "2318760"
  },
  {
    "text": "of have to treat them as customers paid with love right so they they're giving you feedback and you respond to that by",
    "start": "2318760",
    "end": "2324400"
  },
  {
    "text": "making their lives easier right so that's the first thing you know really treat your community as well as possible",
    "start": "2324400",
    "end": "2330960"
  },
  {
    "text": "and respond fast and then second you know uh whenever picking the general theme try hard and find what are all all",
    "start": "2330960",
    "end": "2337640"
  },
  {
    "text": "the other open- source tools or maybe not open source tools that exist there that are very adjacent to what you do",
    "start": "2337640",
    "end": "2343800"
  },
  {
    "text": "and you know have a lot of clarity on what is your differentiation there you know so what kind of new problem are you",
    "start": "2343800",
    "end": "2350160"
  },
  {
    "text": "solving how do you communicate that and if it's related to research project it might be a little bit easier because you",
    "start": "2350160",
    "end": "2355839"
  },
  {
    "text": "write a paper about it and you know people read the paper it's like oh you're solving a cool problem they come and take a look at your work right so",
    "start": "2355839",
    "end": "2362839"
  },
  {
    "text": "those are two things know clear differentiation and then recruiting users as early as possible so you can",
    "start": "2362839",
    "end": "2368560"
  },
  {
    "text": "iterate fast and if you solve their problem chances are they're going to tell their friends and colleagues and they start using it too and that's how you catalyze it so I'm kind of curious",
    "start": "2368560",
    "end": "2375800"
  },
  {
    "text": "kind of in the same strain of that last question you're affecting so many other",
    "start": "2375800",
    "end": "2381079"
  },
  {
    "text": "communities out there that that you know maybe commercially based with a hardware vendor there's a lot of communities",
    "start": "2381079",
    "end": "2386720"
  },
  {
    "text": "involved in the targets that you're that you're compiling to and um I can't help",
    "start": "2386720",
    "end": "2392119"
  },
  {
    "text": "but wonder how do you see those types of communities getting involved because you are essentially pretty significant",
    "start": "2392119",
    "end": "2398560"
  },
  {
    "text": "influencer in how those targets get used because if the compiling is working if",
    "start": "2398560",
    "end": "2404240"
  },
  {
    "text": "it's really awesome that benefits them in a big way how do they choose to",
    "start": "2404240",
    "end": "2409359"
  },
  {
    "text": "engage you do you need more engagement from those Target communities to do better and what kind of value can they",
    "start": "2409359",
    "end": "2414839"
  },
  {
    "text": "add I just as you're hitting Raspberry Pi I would imagine the Raspberry Pi Community would have to be keenly",
    "start": "2414839",
    "end": "2420599"
  },
  {
    "text": "interested in working with you on this yeah it's a it's a great question Chris so arm is already fully bought into the",
    "start": "2420599",
    "end": "2426240"
  },
  {
    "text": "TVM ecosystem they built their CPU GPU and npu compilers on top of TVM they're",
    "start": "2426240",
    "end": "2431480"
  },
  {
    "text": "very active contributor to the open source community and they work closely with us at octo as well so some of the",
    "start": "2431480",
    "end": "2437640"
  },
  {
    "text": "big players of course like for example let's say Nvidia right Nvidia has a very mature probably the most mature system",
    "start": "2437640",
    "end": "2444920"
  },
  {
    "text": "software stack for machine learning on their hardware and arguably that's a good chunk of their success and even Nvidia is interested in working with t",
    "start": "2444920",
    "end": "2451079"
  },
  {
    "text": "you see some commits from them you know we support Nvidia pretty well and the point I wanted to make here's the following for Hardware that's really",
    "start": "2451079",
    "end": "2457520"
  },
  {
    "text": "popular the community is going to do anyways even if the heart vendors themselves are not involved so we have",
    "start": "2457520",
    "end": "2462680"
  },
  {
    "text": "really good support for NVIDIA because we did a bunch of work the community did a bunch of work because Nvidia Hardware",
    "start": "2462680",
    "end": "2468200"
  },
  {
    "text": "matters and it turns out that we have we have very good performance on Nvidia Hardware because of that and the point",
    "start": "2468200",
    "end": "2473280"
  },
  {
    "text": "here is not to just say compete with CNN and tensor RT of course they know their harder they do well in many cases we do",
    "start": "2473280",
    "end": "2480200"
  },
  {
    "text": "really well some other cases that was not in their radar and we want to offer users a choice of which one to use and",
    "start": "2480200",
    "end": "2486920"
  },
  {
    "text": "sometimes choose it automatically for them so in TVM there's something called Best of Both Worlds when you process a",
    "start": "2486920",
    "end": "2492800"
  },
  {
    "text": "model through TVM you can choose to either use CNN or TVM native code and you picks the best of each each part of your model and you composes for the end",
    "start": "2492800",
    "end": "2499880"
  },
  {
    "text": "user what you car is your model runs fast right so the point you want to make is that for this big Hardware the company support it for merging uh",
    "start": "2499880",
    "end": "2507119"
  },
  {
    "text": "Hardware vendors honestly I don't know if they have an alternative the alternative is to build everything in house so thing for let's say you are a",
    "start": "2507119",
    "end": "2513760"
  },
  {
    "text": "new AI chip company and you have a great idea for a hardware mechanism that's going to make a certain class of models",
    "start": "2513760",
    "end": "2519119"
  },
  {
    "text": "run really well and that's your business or you going to cater to this set of users so it's start in your heart then you go and look at the ecosystem and you",
    "start": "2519119",
    "end": "2524760"
  },
  {
    "text": "say oh now I had to support P torch Tor flow car maxnet I have to support this type of models you started looking at",
    "start": "2524760",
    "end": "2530680"
  },
  {
    "text": "everything that you need to support to connect with the rest of the ecosystem it's a daunting task so that's a huge",
    "start": "2530680",
    "end": "2536560"
  },
  {
    "text": "incentive for them to come and use TVM because if they support a very clean code generation interface that we made",
    "start": "2536560",
    "end": "2543440"
  },
  {
    "text": "very easy for new hard venders to come and be part of the ecosystem I don't think they'll find a more compelling",
    "start": "2543440",
    "end": "2550119"
  },
  {
    "text": "alternative to be honest because you know how are you going to spin up a team of 15 compiler Engineers to go and build",
    "start": "2550119",
    "end": "2555720"
  },
  {
    "text": "your own internal compil that it's just hard to imagine right so basically what I'm trying to say in a in a long way is",
    "start": "2555720",
    "end": "2562880"
  },
  {
    "text": "that the the way we made it easy to add new harder Target and the fact there's a community around it and the fact there's",
    "start": "2562880",
    "end": "2567960"
  },
  {
    "text": "a lot of momentum already sells itself to the new harder vendors and we said that by having harder vendors come",
    "start": "2567960",
    "end": "2573559"
  },
  {
    "text": "wanting to work with us to do some enablement work for them Direct we do some of that for some harder targets but",
    "start": "2573559",
    "end": "2579960"
  },
  {
    "text": "a lot of times we just see harder venders going directly to the new smaller emerging ones going directly to",
    "start": "2579960",
    "end": "2585079"
  },
  {
    "text": "the Apache TVM so does that answer your question Chris or yeah no it does that was a really good answer actually",
    "start": "2585079",
    "end": "2590760"
  },
  {
    "text": "satisfied it thank you yeah definitely I I agree I have sort of a strange question as we get more towards the end",
    "start": "2590760",
    "end": "2597880"
  },
  {
    "text": "here what else out there sort of across the AI industry do you have your eyes on",
    "start": "2597880",
    "end": "2604319"
  },
  {
    "text": "in terms of things that really excite you in ter terms of where the industry is going or maybe particular groups that",
    "start": "2604319",
    "end": "2611800"
  },
  {
    "text": "are innovating or particular technology that you you have your ey on what else do you have your eye on as you're",
    "start": "2611800",
    "end": "2617240"
  },
  {
    "text": "looking kind of towards the future of the AI industry yeah well there's just",
    "start": "2617240",
    "end": "2622359"
  },
  {
    "text": "so many things let me start with the shorter term then it goes the longer term so in the shorter term I think it's really exciting what's going on in doing",
    "start": "2622359",
    "end": "2628920"
  },
  {
    "text": "more and more harder aware Network architecture search there's some companies doing it but essentially as",
    "start": "2628920",
    "end": "2634200"
  },
  {
    "text": "you evolve how your model looks like with network architecture search you do that in a hardware way I think is super",
    "start": "2634200",
    "end": "2639359"
  },
  {
    "text": "important because it's complimentary to everything that we talked about here right so what we talk about here is having a model that is ready to be",
    "start": "2639359",
    "end": "2646200"
  },
  {
    "text": "deployed is going to compile it but now having the model in the first place to actually be better suitable for your",
    "start": "2646200",
    "end": "2651319"
  },
  {
    "text": "Hardware is great so that's that's one of them the other one is more automations in the in the data management side for example I like I",
    "start": "2651319",
    "end": "2657400"
  },
  {
    "text": "love what those Norco folks are doing which essentially I if you've heard of them but they essentially have tools",
    "start": "2657400",
    "end": "2664000"
  },
  {
    "text": "that enables you to construct synthetic data sets in programmatic way that essentially automates a lot of the work",
    "start": "2664000",
    "end": "2670960"
  },
  {
    "text": "that's required for you to start building a data set to train models and then on the hard side what I think is",
    "start": "2670960",
    "end": "2676040"
  },
  {
    "text": "happening right now that's really exciting is just to see more and more reconfigurable architectures coming into",
    "start": "2676040",
    "end": "2681480"
  },
  {
    "text": "the mix right so you I'm sure you've heard of CPUs gpus right and CPUs like accelerators we probably heard of fpgas",
    "start": "2681480",
    "end": "2688000"
  },
  {
    "text": "as well which essentially Hardware fabric that you can program very much the same way that you design Hardware",
    "start": "2688000",
    "end": "2695079"
  },
  {
    "text": "but you know it's not quite as fast as a truly application specific chip but it's pretty General and then you can do a lot",
    "start": "2695079",
    "end": "2701440"
  },
  {
    "text": "of meaningful things with it I find it exciting that those fpgs are getting more and more tuned for machine learning",
    "start": "2701440",
    "end": "2707720"
  },
  {
    "text": "so zyink has offerings that way you know Altera is having you know enriching their fpgs with more blocks that are",
    "start": "2707720",
    "end": "2713720"
  },
  {
    "text": "more uh suitable for new models I find this exciting in the in the short term so now in the medium to long term I just",
    "start": "2713720",
    "end": "2720119"
  },
  {
    "text": "love what's going on in between machine learning and Life Sciences you know so just seeing machine learning enabling",
    "start": "2720119",
    "end": "2726280"
  },
  {
    "text": "you know very large scale genomics uh studies that CES crazy amount of data to",
    "start": "2726280",
    "end": "2731680"
  },
  {
    "text": "go and make sense of data that is incredibly complex because nature is a complex Beast right so and then also",
    "start": "2731680",
    "end": "2737240"
  },
  {
    "text": "using machine learning to Design Systems you know so people using machine learning to design molecular systems",
    "start": "2737240",
    "end": "2742760"
  },
  {
    "text": "people are using machine learning to design aircraft people are using a reverse engineering like what is it",
    "start": "2742760",
    "end": "2747960"
  },
  {
    "text": "again reverse design where you give the properties and you synthesize something that has those properties using machine learning these are all things that I",
    "start": "2747960",
    "end": "2754599"
  },
  {
    "text": "find really really exciting to think about because machine learning itself is also a system so using machine learning",
    "start": "2754599",
    "end": "2760400"
  },
  {
    "text": "for machine learning improvements is pretty interesting we do that to some extent but I feel like we're just scratching the surface right so you can",
    "start": "2760400",
    "end": "2767240"
  },
  {
    "text": "use you can use machine learning to design machine learning chips right you can use machine learning to optimize machine learning training systems and",
    "start": "2767240",
    "end": "2773440"
  },
  {
    "text": "when you close the loop there that's when you should embrace Embrace and let it let it evolve right so I remember a",
    "start": "2773440",
    "end": "2780960"
  },
  {
    "text": "while back we had some guests from Intel and they were talking about just what you just said using machine learning for",
    "start": "2780960",
    "end": "2786319"
  },
  {
    "text": "chip design and yeah it's the sky's the limit you can use it for so many different things at this point yeah absolutely and in maching lhas going",
    "start": "2786319",
    "end": "2793400"
  },
  {
    "text": "back full circle now you know as a workload it's so tolerant to we call noisy execution right so by that I",
    "start": "2793400",
    "end": "2799760"
  },
  {
    "text": "really mean if you have flaky hardware and then there's just no way around it as you go to 2 nanometer Technologies",
    "start": "2799760",
    "end": "2805440"
  },
  {
    "text": "you know which I don't know if you heard IBM just announced they're getting you know 2 nanometer process kind of making",
    "start": "2805440",
    "end": "2810880"
  },
  {
    "text": "a lot of progress with it that's kind of crazy to think about right and it's likely to be very very noisy and have very flaky transistors and the way we",
    "start": "2810880",
    "end": "2817720"
  },
  {
    "text": "make use of that is not just by doing the typical system design of layering eror correcting no we should do that too",
    "start": "2817720",
    "end": "2823520"
  },
  {
    "text": "but with machine learning you can use that directly because it's so tolerant to noisy execution that there's many",
    "start": "2823520",
    "end": "2830040"
  },
  {
    "text": "interesting possibilities there so for better Energy Efficiency and so such that machine Lear wouldn't get as much bad rep by using a whole lot of energy",
    "start": "2830040",
    "end": "2837480"
  },
  {
    "text": "right which I'm sure you've heard that before right so well uh that's the whole topic of the",
    "start": "2837480",
    "end": "2843599"
  },
  {
    "text": "conversation you know so yeah and I hope we can have that conversation some time well Louise I appreciate you uh taking",
    "start": "2843599",
    "end": "2850599"
  },
  {
    "text": "time to help Chris and I fully optimize our uh discussion on the podcast to uh",
    "start": "2850599",
    "end": "2857480"
  },
  {
    "text": "maximize interest it's been uh it's been great I've really enjoyed it and we'll have links to TVM and octoml and all the",
    "start": "2857480",
    "end": "2865640"
  },
  {
    "text": "things in our show notes so definitely encourage our listeners to check those things out I know I'll be playing around",
    "start": "2865640",
    "end": "2872119"
  },
  {
    "text": "a bit after the episode so thanks so much Louise really appreciate you taking time thank you D you guys are really",
    "start": "2872119",
    "end": "2878000"
  },
  {
    "text": "really fun I can't wait to hear your other episodes for other topics as well so you and Chris are really fun thank",
    "start": "2878000",
    "end": "2885078"
  },
  {
    "text": "you thank you for listening to practical AI we appreciate your time and your attention if you enjoyed this episode",
    "start": "2886000",
    "end": "2893079"
  },
  {
    "text": "help us out by spreading the word think of a friend think of a colleague somebody who would benefit from",
    "start": "2893079",
    "end": "2898800"
  },
  {
    "text": "listening to it and send them a link we'd really appreciate it practical AI is hosted by Chris Benson and Daniel wh",
    "start": "2898800",
    "end": "2905319"
  },
  {
    "text": "neack it's produced by Jared Santo with music by break master cylinder thanks again to our sponsors fastly lenoe and",
    "start": "2905319",
    "end": "2912200"
  },
  {
    "text": "launch darkley that's our show we hope you enjoyed it and we'll talk to you again next",
    "start": "2912200",
    "end": "2917470"
  },
  {
    "text": "[Music]",
    "start": "2917470",
    "end": "2925410"
  },
  {
    "text": "[Music]",
    "start": "2929450",
    "end": "2932510"
  },
  {
    "text": "week [Music]",
    "start": "2935720",
    "end": "2945320"
  },
  {
    "text": "King",
    "start": "2945359",
    "end": "2948359"
  }
]