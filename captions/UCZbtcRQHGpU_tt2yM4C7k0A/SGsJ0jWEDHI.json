[
  {
    "text": "[Music]",
    "start": "220",
    "end": "7120"
  },
  {
    "text": "welcome to practical AI if you work in artificial intelligence aspire to or are",
    "start": "7120",
    "end": "13719"
  },
  {
    "text": "curious how AI related Technologies are changing the world this is the show for you thank you to our partners at fastly",
    "start": "13719",
    "end": "21080"
  },
  {
    "text": "for shipping all of our pods super fast to wherever you listen check them out at",
    "start": "21080",
    "end": "26160"
  },
  {
    "text": "fast.com and to our friends that fly deploy your app server and database close to your users no Ops required",
    "start": "26160",
    "end": "33800"
  },
  {
    "text": "learn more at [Music]",
    "start": "33800",
    "end": "42290"
  },
  {
    "text": "fly.io welcome to another fully connected episode of the Practical AI",
    "start": "42320",
    "end": "47399"
  },
  {
    "text": "podcast in these episodes Chris and I keep you fully connected with a bunch of",
    "start": "47399",
    "end": "53359"
  },
  {
    "text": "different things that are happening in the AI and machine learning community and we talk through some things to help",
    "start": "53359",
    "end": "60120"
  },
  {
    "text": "you level up your machine learning game my name is Daniel whack I am a Founder",
    "start": "60120",
    "end": "65799"
  },
  {
    "text": "at prediction guard and I'm joined as always by my co-host Chris Benson who is a tech strategist at locked Martin how",
    "start": "65799",
    "end": "72960"
  },
  {
    "text": "you doing Chris I'm doing great today Daniel how's it going it's going good you know this week I was well it's been",
    "start": "72960",
    "end": "79840"
  },
  {
    "text": "an interesting couple weeks for me in that I was at the Intel Innovation",
    "start": "79840",
    "end": "85640"
  },
  {
    "text": "conference out in San Jose um the week before last last and then this week I",
    "start": "85640",
    "end": "92240"
  },
  {
    "text": "was at uh the go programming language conference called goer con and taught a workshop there and so that was really",
    "start": "92240",
    "end": "99200"
  },
  {
    "text": "enjoyable so two weeks in sunny California or mostly sunny California I",
    "start": "99200",
    "end": "104719"
  },
  {
    "text": "guess that was really cool so maybe even just highlighting a couple of cool",
    "start": "104719",
    "end": "109880"
  },
  {
    "text": "things that are happening in in those communities at Intel there were a couple",
    "start": "109880",
    "end": "114920"
  },
  {
    "text": "of things that were highlighted that might be of interest one is it seems",
    "start": "114920",
    "end": "120159"
  },
  {
    "text": "like their Intel is really diving into the idea of AI enabled applications",
    "start": "120159",
    "end": "128239"
  },
  {
    "text": "on your local machine which I know is something we might talk about a little bit in this show in particular that is",
    "start": "128239",
    "end": "136040"
  },
  {
    "text": "like hey if I want to build a desktop application that people actually run on their laptop and I want that to run",
    "start": "136040",
    "end": "145760"
  },
  {
    "text": "stable diffusion as part of the application and not you know reach out over the network to some API how would I",
    "start": "145760",
    "end": "153120"
  },
  {
    "text": "build that and what would those sort of like AI PCS is I think what what they're",
    "start": "153120",
    "end": "159360"
  },
  {
    "text": "calling them what would those have to look like and they're they're thinking about that with some of their processors",
    "start": "159360",
    "end": "164920"
  },
  {
    "text": "which is interesting and then on the data center side they had a bunch of things including announcing the Intel",
    "start": "164920",
    "end": "172159"
  },
  {
    "text": "developer Cloud which is cool because you can go on there similar to other",
    "start": "172159",
    "end": "178800"
  },
  {
    "text": "Cloud environments spin up either a VM or actually a connect to a bare metal",
    "start": "178800",
    "end": "184319"
  },
  {
    "text": "instance that has their latest generation of processors including these",
    "start": "184319",
    "end": "189440"
  },
  {
    "text": "gouty 2 processors which are from Habana labs they were acquired by Intel I",
    "start": "189440",
    "end": "195840"
  },
  {
    "text": "forget when but they have so they would be sort of on the data center side you're running accelerated workloads on",
    "start": "195840",
    "end": "202840"
  },
  {
    "text": "these and we're actually running some of our prediction guard stuff on these gouty processors and seeing really great",
    "start": "202840",
    "end": "209760"
  },
  {
    "text": "performance so those are a couple things highlighted from there and yeah I don't know have you heard those themes in in",
    "start": "209760",
    "end": "216640"
  },
  {
    "text": "your conversation as well in terms of either new processors advances in data center technology or this kind of local",
    "start": "216640",
    "end": "225680"
  },
  {
    "text": "inference side of things I have quite a bit actually um and I'm certainly not an",
    "start": "225680",
    "end": "231439"
  },
  {
    "text": "expert on micro Electronics by any stretch but I have friends who are and listen to them closely when they talk",
    "start": "231439",
    "end": "237360"
  },
  {
    "text": "there's a bit of a uh an ongoing Revolution on the microprocessor side",
    "start": "237360",
    "end": "242599"
  },
  {
    "text": "and so you know many of us that have been in the AI world for a long time there have been you know for instance",
    "start": "242599",
    "end": "248799"
  },
  {
    "text": "gpus from Nvidia have been kind of a core to that but there's a lot of Chip types uh that have been coming out by a",
    "start": "248799",
    "end": "255519"
  },
  {
    "text": "number of different vendors to compete with that you know famously Google was probably the first one well known with",
    "start": "255519",
    "end": "261680"
  },
  {
    "text": "their tpus uh tensor processing units but there's all sorts of specialized",
    "start": "261680",
    "end": "267080"
  },
  {
    "text": "chips and chiplets that are coming out that uh that are enabling these types of things so I think Intel is definitely",
    "start": "267080",
    "end": "273479"
  },
  {
    "text": "one of the global leaders in that and uh looking forward to having it'll be nice when everyone's laptops uh and phones",
    "start": "273479",
    "end": "280880"
  },
  {
    "text": "and everything are all completely equipped with everything they need yeah yeah it's super interesting especially",
    "start": "280880",
    "end": "286639"
  },
  {
    "text": "for use cases where it's like your personal assistant AI enabled personal",
    "start": "286639",
    "end": "293400"
  },
  {
    "text": "assistant that really is tied to you personally applications like that I",
    "start": "293400",
    "end": "299039"
  },
  {
    "text": "think you'd you'd want to run a lot of those things locally and not be sending a lot of that data all around so that's",
    "start": "299039",
    "end": "305919"
  },
  {
    "text": "kind of interesting they also talked a lot about confidential Computing which",
    "start": "305919",
    "end": "311680"
  },
  {
    "text": "is an interesting topic that I think maybe some of our audience at least",
    "start": "311680",
    "end": "317479"
  },
  {
    "text": "wouldn't be familiar with as much from what we talk on this show about but it is very connected to the AI world in the",
    "start": "317479",
    "end": "325560"
  },
  {
    "text": "sense that if you are running kind of secure workloads through AI models",
    "start": "325560",
    "end": "330840"
  },
  {
    "text": "whether you're doing that on Nvidia chips or other chips like we've talked about there are ways and toolkits to",
    "start": "330840",
    "end": "340280"
  },
  {
    "text": "enable you to actually secure the environments that you are running those",
    "start": "340280",
    "end": "346400"
  },
  {
    "text": "models in and actually provide adastation to know that nothing has been",
    "start": "346400",
    "end": "352479"
  },
  {
    "text": "tampered with inside of those kind of secure environments so I'm going to surprise you I actually know quite a lot",
    "start": "352479",
    "end": "358759"
  },
  {
    "text": "about that those are trusted execution environments let's just say I've touched on those quite a lot I think Intel's",
    "start": "358759",
    "end": "364800"
  },
  {
    "text": "version is like TDX trusted yeah it's something they have a couple of different versions that's the one that's",
    "start": "364800",
    "end": "370599"
  },
  {
    "text": "out in the marketplace right now but yeah it's the idea of ensuring that when you normally if you're running a program",
    "start": "370599",
    "end": "377720"
  },
  {
    "text": "uh for audience if you're running a program and it has to Transit obviously from system to system every system has a",
    "start": "377720",
    "end": "384360"
  },
  {
    "text": "processor it's processing on and even if you're running encryption at the application layer you have to unwrap",
    "start": "384360",
    "end": "389639"
  },
  {
    "text": "that encryption for the processing to happen in the chip an adversary you know",
    "start": "389639",
    "end": "394919"
  },
  {
    "text": "if it's on the order of a a major nation state has the ability to steal",
    "start": "394919",
    "end": "401319"
  },
  {
    "text": "unencrypted information that had been encrypted in transit straight out of the processor memory and uh Intel and other",
    "start": "401319",
    "end": "409280"
  },
  {
    "text": "vendors are starting to push trusted execution environments and products and services around that which protects and",
    "start": "409280",
    "end": "416960"
  },
  {
    "text": "guarantees the safety of that data inside the processor something I've spent some time on actually yeah it's",
    "start": "416960",
    "end": "423280"
  },
  {
    "text": "super interesting and I think even the CTO and his talk had like a t-shirt that",
    "start": "423280",
    "end": "429560"
  },
  {
    "text": "sort of had a vend diagram kind of thing between like security and Ai and at the",
    "start": "429560",
    "end": "435160"
  },
  {
    "text": "intersection of that is a lot of you know what he talked about this sort of",
    "start": "435160",
    "end": "440319"
  },
  {
    "text": "idea that hey whatever Hardware you're running on if you can combine AI",
    "start": "440319",
    "end": "445680"
  },
  {
    "text": "workloads with these sort of trusted or confidential Computing ideas that that can be very powerful and take care of at",
    "start": "445680",
    "end": "452560"
  },
  {
    "text": "least some of the security and privacy concerns that people have with AI",
    "start": "452560",
    "end": "458440"
  },
  {
    "text": "workloads in in general which is cool so yeah the two are converging in a big way",
    "start": "458440",
    "end": "463840"
  },
  {
    "text": "because while trusted execution environments which are referred to as tees have been around for years in",
    "start": "463840",
    "end": "470159"
  },
  {
    "text": "processors now that we are having large Federated workf flows which is really Classic on cloud-based AI jobs where",
    "start": "470159",
    "end": "477680"
  },
  {
    "text": "you're where you're Distributing an AI inference or training across many many systems with very very important data",
    "start": "477680",
    "end": "485360"
  },
  {
    "text": "that you would not want to get into an adversary's hands that Federation is really kind of pushing Ai and you know",
    "start": "485360",
    "end": "493120"
  },
  {
    "text": "chip providers together in that way to guarantee that we we didn't see lots of workloads uh that would be falling in",
    "start": "493120",
    "end": "499520"
  },
  {
    "text": "that category until we hit the AI space and it's chock full of them so I keep remembering things that that happened",
    "start": "499520",
    "end": "505960"
  },
  {
    "text": "over the past couple weeks while I've been traveling and and people have mentioned but one maybe other noteworthy",
    "start": "505960",
    "end": "511199"
  },
  {
    "text": "thing for people to be aware of on the more the infrastructure side which I",
    "start": "511199",
    "end": "518240"
  },
  {
    "text": "think we will talk a little bit more about in this episode is that cloud flare announced their workers Ai and I",
    "start": "518240",
    "end": "525320"
  },
  {
    "text": "think this is the latest in in this sort of series of serverless GPU Solutions so",
    "start": "525320",
    "end": "532040"
  },
  {
    "text": "these worker AIS are Cloud Flare's version of the serverless GPU type",
    "start": "532040",
    "end": "538920"
  },
  {
    "text": "environment that we've talked about with things like modal or base 10 or banana there's a lot",
    "start": "538920",
    "end": "547040"
  },
  {
    "text": "of these coming out but I think it's worth noting that a very large player like Cloud flare is now kind of Dipping",
    "start": "547040",
    "end": "554160"
  },
  {
    "text": "into this serverless GPU space which I think also signals that we'll be kind of",
    "start": "554160",
    "end": "559640"
  },
  {
    "text": "seeing in the cloud side more and more push towards serverless GPU workloads",
    "start": "559640",
    "end": "567040"
  },
  {
    "text": "and environments that that support that interesting very interesting well I",
    "start": "567040",
    "end": "572279"
  },
  {
    "text": "that's a bunch of infrastructure and confidential infrastructure and",
    "start": "572279",
    "end": "577720"
  },
  {
    "text": "Computing and security stuff that has crossed our paths in the past couple weeks but one of the questions that you",
    "start": "577720",
    "end": "584160"
  },
  {
    "text": "asked me leading up to this recording was about things are moving so fast and",
    "start": "584160",
    "end": "592800"
  },
  {
    "text": "I think deploying and managing an AI workload may look different now than it",
    "start": "592800",
    "end": "599880"
  },
  {
    "text": "even looked 6 months ago and it's been a while since we talked through the kind",
    "start": "599880",
    "end": "605560"
  },
  {
    "text": "of developer or technical team perspective on how you might if you want",
    "start": "605560",
    "end": "613360"
  },
  {
    "text": "to use one of these models that's coming out all the time so mistol ai's model",
    "start": "613360",
    "end": "619240"
  },
  {
    "text": "just came out um the ones that received huge amazing amount of funding just earlier in June and now they have their",
    "start": "619240",
    "end": "626160"
  },
  {
    "text": "first model out it's released Apache too so you can download it so the question is let's say you want to use one of",
    "start": "626160",
    "end": "632160"
  },
  {
    "text": "these great models that's coming out these days and you want to host it in your company's infrastructure or even",
    "start": "632160",
    "end": "638440"
  },
  {
    "text": "just play around with it as a developer what does that look like currently",
    "start": "638440",
    "end": "643760"
  },
  {
    "text": "because there's also along with these models that are coming out new tooling that's coming out all the time so what",
    "start": "643760",
    "end": "650279"
  },
  {
    "text": "does that look like these days and what are the various options and things to consider as you're interacting with",
    "start": "650279",
    "end": "657639"
  },
  {
    "text": "these models and considering even hosting them yourself or integrating them in your own infrastructure that's a",
    "start": "657639",
    "end": "664120"
  },
  {
    "text": "fair question because it's been it's been a while since we talked through some of the infrastructure I think Chris",
    "start": "664120",
    "end": "669800"
  },
  {
    "text": "it has and for what it's worth I'm going to brag on you for a second since I know that you would not do that to yourself",
    "start": "669800",
    "end": "676079"
  },
  {
    "text": "with Daniel uh being the founder of prediction guard this is a topic that he",
    "start": "676079",
    "end": "681200"
  },
  {
    "text": "is like Global expert in really really knows what he's doing and as we were talking about I've had so many people",
    "start": "681200",
    "end": "687920"
  },
  {
    "text": "asking me these questions that Daniel was just talking about lately and I was like well you know one of my best",
    "start": "687920",
    "end": "693279"
  },
  {
    "text": "friends is is a real pro at this so thank you uh if you can kind of start walking us through and and this is a",
    "start": "693279",
    "end": "699680"
  },
  {
    "text": "moving topic as you just pointed out it's changed in the last few months and and will continue to evolve over time",
    "start": "699680",
    "end": "705680"
  },
  {
    "text": "but yeah if you can start walking us through what that looks like today you know we're in the beginning of the fall",
    "start": "705680",
    "end": "711839"
  },
  {
    "text": "of 2023 something that might help the rest of us for at least the next few months and maybe one note on this is is",
    "start": "711839",
    "end": "719720"
  },
  {
    "text": "I'm also getting these questions all the time and like you say I'm deploying models all the time with prediction guard I think a lot of people if you're",
    "start": "719720",
    "end": "728079"
  },
  {
    "text": "a developer or a infrastructure person you just have that natural desire even",
    "start": "728079",
    "end": "734279"
  },
  {
    "text": "if you end up using a model that's behind some API that's hosted by someone else it can be useful and instructive in",
    "start": "734279",
    "end": "742480"
  },
  {
    "text": "building your own intuition even to just try deploying one of these models see",
    "start": "742480",
    "end": "748000"
  },
  {
    "text": "see what's involved see how they run that sort of thing it's also kind of",
    "start": "748000",
    "end": "753880"
  },
  {
    "text": "worthwhile from my perspective to experiment with different models before",
    "start": "753880",
    "end": "760000"
  },
  {
    "text": "you say you know lock yourself into a certain model family or something it's",
    "start": "760000",
    "end": "765519"
  },
  {
    "text": "relatively easy now with the tooling to get somewhat of a sense of how these different models perform and build up",
    "start": "765519",
    "end": "771920"
  },
  {
    "text": "that intuition for yourself even if you end up using a model that's behind an API I mentioned I was at goer con this",
    "start": "771920",
    "end": "777880"
  },
  {
    "text": "week and that was some of the questions that came up to t a workshop on generative Ai and that was a good long",
    "start": "777880",
    "end": "785079"
  },
  {
    "text": "discussion in there that people had a lot of questions about was hey let's say I didn't want to use one of these apis",
    "start": "785079",
    "end": "792079"
  },
  {
    "text": "how do I pull down a model and use it so yeah let's jump in let's first maybe",
    "start": "792079",
    "end": "797519"
  },
  {
    "text": "talk about something that I know that we've touched on before but just to",
    "start": "797519",
    "end": "803320"
  },
  {
    "text": "emphasize here where can you get models and let's say that we're putting putting",
    "start": "803320",
    "end": "809360"
  },
  {
    "text": "aside for a second the kind of closed proprietary chunk of models these would",
    "start": "809360",
    "end": "815440"
  },
  {
    "text": "be ones from like open AI anthropic cohere Etc they have their own apis they",
    "start": "815440",
    "end": "821959"
  },
  {
    "text": "host those models let's say that we're interested in either an open an Open",
    "start": "821959",
    "end": "827199"
  },
  {
    "text": "Access model but it could be either an open and somewhat restricted model or an",
    "start": "827199",
    "end": "832759"
  },
  {
    "text": "open and somewhat permissively licensed model and we' we've talked about that on the show too for example there's models",
    "start": "832759",
    "end": "840279"
  },
  {
    "text": "that come out that are licensed for commercial use or non-commercial use or",
    "start": "840279",
    "end": "845639"
  },
  {
    "text": "research purposes only but let's say you want to use one of these Open Access models the first question that might",
    "start": "845639",
    "end": "851920"
  },
  {
    "text": "come up is where do I find these models the best place that you can find these",
    "start": "851920",
    "end": "857320"
  },
  {
    "text": "models is on hugging face so if you go to the hugging face website just",
    "start": "857320",
    "end": "863480"
  },
  {
    "text": "huggingface doco and you click on models you'll see that there's at the time of",
    "start": "863480",
    "end": "868800"
  },
  {
    "text": "this recording around 345,000 models on hugging face a few to",
    "start": "868800",
    "end": "874160"
  },
  {
    "text": "choose from yeah yeah a lot to choose from and and think about this those of you that are familiar with GitHub right",
    "start": "874160",
    "end": "880880"
  },
  {
    "text": "how many GitHub repositories are there there's a lot of GitHub repositories",
    "start": "880880",
    "end": "886160"
  },
  {
    "text": "that are someone like tried something in an afternoon and uploaded something to",
    "start": "886160",
    "end": "891759"
  },
  {
    "text": "their GitHub repo right it doesn't mean that's the most useful thing for you to",
    "start": "891759",
    "end": "897160"
  },
  {
    "text": "use in your your workflows alth Al though you could kind of learn from it maybe it's similar on hugging face",
    "start": "897160",
    "end": "902639"
  },
  {
    "text": "there's a lot of people that might like oh I tried fine-tuning this model and",
    "start": "902639",
    "end": "907759"
  },
  {
    "text": "now I uploaded it to my repo on hugging face and similar to GitHub one of the",
    "start": "907759",
    "end": "914480"
  },
  {
    "text": "things that you want to look at just as a practitioner is look at how many people are downloading the model look at",
    "start": "914480",
    "end": "920839"
  },
  {
    "text": "how many people are Harding the model or or you know liking the model and you can",
    "start": "920839",
    "end": "926440"
  },
  {
    "text": "filter by those things so if if I click on model I can then click on a filter",
    "start": "926440",
    "end": "933360"
  },
  {
    "text": "like the task that I'm interested in a computer vision task or an NLP task or",
    "start": "933360",
    "end": "938600"
  },
  {
    "text": "an audio task and then I can look at both the trending models and how many",
    "start": "938600",
    "end": "944560"
  },
  {
    "text": "models were downloaded filtered by things like licenses and languages so",
    "start": "944560",
    "end": "950319"
  },
  {
    "text": "yeah I think the first thing to be aware of is just the landscape of models and",
    "start": "950319",
    "end": "955639"
  },
  {
    "text": "where you find them and the best place for that currently although there are other repositories is by and far hugging",
    "start": "955639",
    "end": "963279"
  },
  {
    "text": "face and go there and treat it similarly to GitHub and that there's going to be a",
    "start": "963279",
    "end": "968480"
  },
  {
    "text": "lot of there that might not be of interest to you but there's going to be some really great things there as",
    "start": "968480",
    "end": "975680"
  },
  {
    "text": "[Music] well what's up friends there's so much",
    "start": "977700",
    "end": "983600"
  },
  {
    "text": "going on in the data and machine learning space it's just hard to keep up did you know the graph technology ol let",
    "start": "983600",
    "end": "989279"
  },
  {
    "text": "you connect the dots across your data and ground your llm in actual knowledge to learn about this new approach don't",
    "start": "989279",
    "end": "995279"
  },
  {
    "text": "miss nodes on October 26 at this free online conference developers and data",
    "start": "995279",
    "end": "1000360"
  },
  {
    "text": "scientists from around the world will share how they use graph technology for everything from building intelligent",
    "start": "1000360",
    "end": "1005399"
  },
  {
    "text": "apps and apis to enhancing machine learning and improving data visualizations there are 90 inspiring",
    "start": "1005399",
    "end": "1012079"
  },
  {
    "text": "talks over 24 hours so no matter where you're at in the world you can attend live sessions to register for this Free",
    "start": "1012079",
    "end": "1017639"
  },
  {
    "text": "Conference visit Neo 4j.com noodes that's Neo the number",
    "start": "1017639",
    "end": "1024079"
  },
  {
    "text": "4j.com [Music]",
    "start": "1024079",
    "end": "1030400"
  },
  {
    "text": "snodes okay Chris I'm on hugging face and I see a bunch of different models",
    "start": "1030400",
    "end": "1037000"
  },
  {
    "text": "that are potentially available to me and I can click on for example object",
    "start": "1037000",
    "end": "1044120"
  },
  {
    "text": "detection and see that the trending model that I'm looking at is from",
    "start": "1044120",
    "end": "1050520"
  },
  {
    "text": "Facebook D resnet 50 um seems like people have used resnet quite a bit",
    "start": "1050520",
    "end": "1059080"
  },
  {
    "text": "63,000 downloads and so maybe that's a good place I I want to start if I'm looking at object detection if I go to",
    "start": "1059080",
    "end": "1067559"
  },
  {
    "text": "let's say automatic speech recognition up at the top would be open AI whisper",
    "start": "1067559",
    "end": "1073640"
  },
  {
    "text": "model which is a great choice and released openly that you can use for",
    "start": "1073640",
    "end": "1078960"
  },
  {
    "text": "speech transcription if I go to for example text generation which a lot of",
    "start": "1078960",
    "end": "1085440"
  },
  {
    "text": "people care about these days the trending one right now is this new mistol 7 billion model that we mentioned",
    "start": "1085440",
    "end": "1091840"
  },
  {
    "text": "earlier was just released so let's take those as our kind of examples let's say",
    "start": "1091840",
    "end": "1096960"
  },
  {
    "text": "I want to run something like open AI whisper or I want to run text generation",
    "start": "1096960",
    "end": "1102720"
  },
  {
    "text": "with Mistral 7 billion or there's even a range of sizes of models right the 7",
    "start": "1102720",
    "end": "1109520"
  },
  {
    "text": "billion model from mistol Falcon 180 billion was released recently so one",
    "start": "1109520",
    "end": "1115159"
  },
  {
    "text": "question that I think people have is how do I know which model might serve my",
    "start": "1115159",
    "end": "1122360"
  },
  {
    "text": "task well and one thing I'd like to recommend to people is even before you try to download the model yourself and",
    "start": "1122360",
    "end": "1129320"
  },
  {
    "text": "run it you can go in and click on these models like if I click on mistol 7",
    "start": "1129320",
    "end": "1135440"
  },
  {
    "text": "billion version 0.1 if you notice on the right hand side",
    "start": "1135440",
    "end": "1140880"
  },
  {
    "text": "of the hugging face model card for that model A lot of these models already have",
    "start": "1140880",
    "end": "1148640"
  },
  {
    "text": "a hosted interactive interface that you can just click the compute button and",
    "start": "1148640",
    "end": "1154480"
  },
  {
    "text": "see the output of the output of the model so it's kind of like a playground that you can see a bit of the output of",
    "start": "1154480",
    "end": "1161679"
  },
  {
    "text": "you can do the same thing with a lot of you know computer vision models or Audio",
    "start": "1161679",
    "end": "1166760"
  },
  {
    "text": "models and then below that you'll see a little thing called spaces using mistol",
    "start": "1166760",
    "end": "1173880"
  },
  {
    "text": "7 billion or if you're on whisper spaces using whisper these are little demo apps",
    "start": "1173880",
    "end": "1180120"
  },
  {
    "text": "that are actually hosted within hugging faces uh infrastructure where people have",
    "start": "1180120",
    "end": "1187559"
  },
  {
    "text": "actually integrated mistol 7 billion and a lot of these are kind of just a simple",
    "start": "1187559",
    "end": "1192760"
  },
  {
    "text": "input output interface and so even without downloading the model if just",
    "start": "1192760",
    "end": "1198919"
  },
  {
    "text": "trying to get a sense for what these models do you can click through some of these spaces that are using them or just",
    "start": "1198919",
    "end": "1206240"
  },
  {
    "text": "look at that kind of interactive playground feature and just try you know upload some of your own prompts or",
    "start": "1206240",
    "end": "1212120"
  },
  {
    "text": "upload some of your own audio or whatever that is to see how the model operates I think a lot of people might",
    "start": "1212120",
    "end": "1218640"
  },
  {
    "text": "miss this if they're just scrolling through let me ask you a quick question when if you're looking and you're trying",
    "start": "1218640",
    "end": "1224440"
  },
  {
    "text": "to narrow down you know which model you want to pick we've talked on previous episodes about some of the concerns that",
    "start": "1224440",
    "end": "1230760"
  },
  {
    "text": "go with different sizes and such so are there some models that uh unless I have",
    "start": "1230760",
    "end": "1236280"
  },
  {
    "text": "a very large infrastructure available to me many many gpus for instance that I",
    "start": "1236280",
    "end": "1241960"
  },
  {
    "text": "should probably disregard is there like a minimum and maximum practical threshold that let's say that I have",
    "start": "1241960",
    "end": "1249480"
  },
  {
    "text": "some Hardware but not everything that I would dream about that I might want to go for so there's kind of an answer to",
    "start": "1249480",
    "end": "1256880"
  },
  {
    "text": "this and then a followup okay one is for this sort of Transformer language models",
    "start": "1256880",
    "end": "1263440"
  },
  {
    "text": "often times if you go much Beyond 7 billion parameters maybe pushing it up",
    "start": "1263440",
    "end": "1269320"
  },
  {
    "text": "to kind of 13 to 15 billion parameters you're not going to be able to run it very well just by default by downloading",
    "start": "1269320",
    "end": "1277360"
  },
  {
    "text": "it and running it with the kind of standard tooling on anything but a single accelerated processor like a GPU",
    "start": "1277360",
    "end": "1285799"
  },
  {
    "text": "and even then most of the time not on a consumer GPU however the followup to that is that",
    "start": "1285799",
    "end": "1294559"
  },
  {
    "text": "a lot of people have created open-source tooling around model optimization that",
    "start": "1294559",
    "end": "1300799"
  },
  {
    "text": "may allow you to run these models on consumer Hardware or even on CPUs and",
    "start": "1300799",
    "end": "1306640"
  },
  {
    "text": "I'd like to talk about that here in a bit that a lot of times you may want to consider this sort of model optimization",
    "start": "1306640",
    "end": "1314159"
  },
  {
    "text": "piece of your pipeline when you're considering how to run the model because",
    "start": "1314159",
    "end": "1319279"
  },
  {
    "text": "sometimes the sort of default size and default Precision of the model might not be best for you both in terms of your",
    "start": "1319279",
    "end": "1326159"
  },
  {
    "text": "needs in terms of performance or in terms of the hardware that's available to you but I would say in this phase of",
    "start": "1326159",
    "end": "1333960"
  },
  {
    "text": "like what model is going to be good for me go ahead and put that sort of",
    "start": "1333960",
    "end": "1339240"
  },
  {
    "text": "Hardware concern although it's important put it a little bit to the side and focus on which model is giving me the",
    "start": "1339240",
    "end": "1346520"
  },
  {
    "text": "output behavior that I want right because you have a certain task in mind",
    "start": "1346520",
    "end": "1352679"
  },
  {
    "text": "right and if you could figure out hey this model kind of does what I want and",
    "start": "1352679",
    "end": "1358720"
  },
  {
    "text": "it seems like it's giving pretty reasonable output and then you find out oh well I can't run it on the GPU that I",
    "start": "1358720",
    "end": "1365679"
  },
  {
    "text": "have or I need to figure out how to run this on a CPU then then that kind of Narrows down the type of tooling that",
    "start": "1365679",
    "end": "1372240"
  },
  {
    "text": "you're going to have to use for optimization or you might not need to optimize at all so kind of start with",
    "start": "1372240",
    "end": "1378080"
  },
  {
    "text": "the smaller models and build up to something that fulfills the behavior requirements that you have by just using",
    "start": "1378080",
    "end": "1384720"
  },
  {
    "text": "some of these demos using some of these spaces and then think about okay I've now figured out I need Falcon 180",
    "start": "1384720",
    "end": "1392840"
  },
  {
    "text": "billion so what does that look like for me to run that in my own infrastructure then there's kind of a follow-up series",
    "start": "1392840",
    "end": "1399400"
  },
  {
    "text": "of things that we can talk about related to that gotcha thanks so I was uh kind",
    "start": "1399400",
    "end": "1404480"
  },
  {
    "text": "of getting ahead of myself then a little bit in terms of worrying too much about Hardware first yeah yeah I think the question well",
    "start": "1404480",
    "end": "1411559"
  },
  {
    "text": "maybe it's because I come from a data science background right my data science",
    "start": "1411559",
    "end": "1417240"
  },
  {
    "text": "experience always tells me start with the smaller models and work your way up",
    "start": "1417240",
    "end": "1422400"
  },
  {
    "text": "to the bigger ones until you find something that behaves in a way that",
    "start": "1422400",
    "end": "1427720"
  },
  {
    "text": "will work for you and then figure out the kind of infrastructure requirements",
    "start": "1427720",
    "end": "1433600"
  },
  {
    "text": "around that because if you start smaller and work to bigger",
    "start": "1433600",
    "end": "1438960"
  },
  {
    "text": "it's going to be easier to work with that smaller model infrastructure wise and latency wise and all of that but",
    "start": "1438960",
    "end": "1445679"
  },
  {
    "text": "some people do have really complicated sets of problems where they need a really big like let's say you know I",
    "start": "1445679",
    "end": "1453400"
  },
  {
    "text": "want to produce really really really really good synthesized speech or really",
    "start": "1453400",
    "end": "1458600"
  },
  {
    "text": "really good transcriptions from audio I'm going to need maybe a bigger model than the really really small open AI",
    "start": "1458600",
    "end": "1465919"
  },
  {
    "text": "whisper model so it has to do with the requirements of your use case as well I",
    "start": "1465919",
    "end": "1472919"
  },
  {
    "text": "would say Okay so let's say you identify uh a model and you're you've kind of",
    "start": "1472919",
    "end": "1478279"
  },
  {
    "text": "picked what you want to do where do you go from there yeah so let's say that you've picked a model and um let's take",
    "start": "1478279",
    "end": "1484799"
  },
  {
    "text": "the first case where it's a model that could reasonably or you think it could",
    "start": "1484799",
    "end": "1491480"
  },
  {
    "text": "reasonably fit on a single processor a single accelerator or",
    "start": "1491480",
    "end": "1499159"
  },
  {
    "text": "by your own sort of infrastructure constraints you need it to operate on a",
    "start": "1499159",
    "end": "1504200"
  },
  {
    "text": "single accelerator and even if you don't have those infrastructure constraints I",
    "start": "1504200",
    "end": "1510360"
  },
  {
    "text": "think one recommendation I often give is it's just way easier to run something on",
    "start": "1510360",
    "end": "1516720"
  },
  {
    "text": "a single accelerator or a single CPU so I personally recommend a people even if",
    "start": "1516720",
    "end": "1523000"
  },
  {
    "text": "it's a bit larger of a model convince yourself that you can't run it on a single accelerator or a single CPU",
    "start": "1523000",
    "end": "1530320"
  },
  {
    "text": "before you make the jump to spin up a GPU cluster or something something like that it's just a lot harder to deal with",
    "start": "1530320",
    "end": "1537480"
  },
  {
    "text": "even with good tooling some good tooling around that side which we can talk about so yeah let's say that you found a model",
    "start": "1537480",
    "end": "1543640"
  },
  {
    "text": "I don't know let's say it's our mistal 7 billion model you should be able to run that on a single instance with an",
    "start": "1543640",
    "end": "1551320"
  },
  {
    "text": "accelerator or GPU I would then look at that model and depending on the type of",
    "start": "1551320",
    "end": "1559640"
  },
  {
    "text": "the model often times in the model card on hugging face hopefully if it's a",
    "start": "1559640",
    "end": "1566039"
  },
  {
    "text": "nicely maintained model in hugging face then it will likely just like a read me",
    "start": "1566039",
    "end": "1572159"
  },
  {
    "text": "and GitHub it will likely have a little code snippet that says hey here's an example of how to run this what I",
    "start": "1572159",
    "end": "1579080"
  },
  {
    "text": "usually do in that case is I just spin up a Google collab notebook CU I want to",
    "start": "1579080",
    "end": "1585679"
  },
  {
    "text": "see how this thing runs and how many resources it's going to consume so I'll spin up a Google collab notebook if",
    "start": "1585679",
    "end": "1591720"
  },
  {
    "text": "people aren't familiar Google collab is just a hosted version of Jupiter",
    "start": "1591720",
    "end": "1597120"
  },
  {
    "text": "notebooks with a few extra features like you can have certain free access to GPU",
    "start": "1597120",
    "end": "1603720"
  },
  {
    "text": "resources there's similar things from like kaggle and paper space And deep",
    "start": "1603720",
    "end": "1608840"
  },
  {
    "text": "note and a bunch of others so spin up one of these hosted notebooks and just copy paste that example code in that",
    "start": "1608840",
    "end": "1615200"
  },
  {
    "text": "notebook and try a single inference and often times what you can do in these environments is if you look up at the",
    "start": "1615200",
    "end": "1622320"
  },
  {
    "text": "top right corner of Google collab there's a little resources thing and",
    "start": "1622320",
    "end": "1627520"
  },
  {
    "text": "once you load your model in you can actually look at oh how much GPU memory",
    "start": "1627520",
    "end": "1632799"
  },
  {
    "text": "am I taking up right how much CPU memory am I taking up and that gives you a good",
    "start": "1632799",
    "end": "1638000"
  },
  {
    "text": "sense of hey I loaded this model in I performed an inference if I just do nothing else like the the most naive",
    "start": "1638000",
    "end": "1645679"
  },
  {
    "text": "thing I can do then I'm consuming 12 gab of GPU memory or something like",
    "start": "1645679",
    "end": "1652000"
  },
  {
    "text": "that and that kind of tells you if you don't do any optimization then you're going to need a",
    "start": "1652000",
    "end": "1658399"
  },
  {
    "text": "GPU card that at least has 12 gabt of of memory and so maybe you use like a A10 G",
    "start": "1658399",
    "end": "1666760"
  },
  {
    "text": "or you could use an a100 that might be a little bit Overkill in this case but one",
    "start": "1666760",
    "end": "1671840"
  },
  {
    "text": "of these with maybe 24 GB of memory you have a little bit of Headroom there and you can say now you've narrowed down not",
    "start": "1671840",
    "end": "1679200"
  },
  {
    "text": "only the model but potentially the hardware assuming you don't do any optimization potentially the hardware",
    "start": "1679200",
    "end": "1685039"
  },
  {
    "text": "that you could use to to uh deploy it so as of yet I haven't spun up really any",
    "start": "1685039",
    "end": "1690679"
  },
  {
    "text": "infrastructure this is kind of my standard thing where I'm like hey what what's the deal with this model how do I",
    "start": "1690679",
    "end": "1697039"
  },
  {
    "text": "perform a single inference and what kind of resources am I going to need it's a nice little uh cheat code equivalent",
    "start": "1697039",
    "end": "1702880"
  },
  {
    "text": "finding out what you're what you're getting into it sounds like yeah yeah for sure and if you happen to have the",
    "start": "1702880",
    "end": "1709399"
  },
  {
    "text": "the other way I've done this in the past is if you happen to have a VM or maybe",
    "start": "1709399",
    "end": "1714640"
  },
  {
    "text": "it's just your own like personal workstation and you have a consumer GPU card if you have Docker running on that",
    "start": "1714640",
    "end": "1722000"
  },
  {
    "text": "system you could pull down a pre-built you know Transformers hugging face",
    "start": "1722000",
    "end": "1728559"
  },
  {
    "text": "Transformers Docker image and just run it interactively open a bass shell into",
    "start": "1728559",
    "end": "1734799"
  },
  {
    "text": "that Docker container and run an inference just like I said or spin up",
    "start": "1734799",
    "end": "1739960"
  },
  {
    "text": "the model load it into memory in Python and then in another tab or another terminal just run Docker stats and it'll",
    "start": "1739960",
    "end": "1747080"
  },
  {
    "text": "tell you you know how much memory you're consuming and and that sort of thing or run Nvidia SMI or the similar for other",
    "start": "1747080",
    "end": "1756039"
  },
  {
    "text": "systems or other processors that would tell you how much GPU memory you're running so this is kind of a next phase",
    "start": "1756039",
    "end": "1762760"
  },
  {
    "text": "that I do the first is like maybe what kind of model do I want the second is how do I run an inference with this",
    "start": "1762760",
    "end": "1768720"
  },
  {
    "text": "model then kind of is a whole branching series of fness which is either you you",
    "start": "1768720",
    "end": "1776080"
  },
  {
    "text": "go down the path of saying I want to optimize my model in some way to run it either faster or on fewer resources or I",
    "start": "1776080",
    "end": "1784480"
  },
  {
    "text": "want to go down the path of saying nope this is fine I can run it with the",
    "start": "1784480",
    "end": "1790880"
  },
  {
    "text": "resources that I figured out it needs and then you kind of move on to the",
    "start": "1790880",
    "end": "1797440"
  },
  {
    "text": "deployment side of [Music]",
    "start": "1797440",
    "end": "1807169"
  },
  {
    "text": "[Music]",
    "start": "1809900",
    "end": "1818699"
  },
  {
    "text": "things okay Chris let's say that we want to follow the path on our Choose Your",
    "start": "1819039",
    "end": "1825519"
  },
  {
    "text": "Own Adventure that you want to do model optimization on your model Okay the",
    "start": "1825519",
    "end": "1831760"
  },
  {
    "text": "reason you would want to do this is one of two reasons one is hey it turns out I",
    "start": "1831760",
    "end": "1840360"
  },
  {
    "text": "crashed my Google collab trying to run Falcon 180 billion because I ran out of",
    "start": "1840360",
    "end": "1846880"
  },
  {
    "text": "GPU memory and turns out you need more GPU memory for that or multiple gpus and",
    "start": "1846880",
    "end": "1852960"
  },
  {
    "text": "I don't either have access to that or don't want to pay a bunch of money to spin up a GPU cluster and run the model",
    "start": "1852960",
    "end": "1858639"
  },
  {
    "text": "in a distributed way or it's maybe even a smaller model and you want to run it",
    "start": "1858639",
    "end": "1864039"
  },
  {
    "text": "either faster or on standard non-accelerated Hardware like I heard a",
    "start": "1864039",
    "end": "1870639"
  },
  {
    "text": "talk at gophercon about a workflow where people running a model at the edge in a",
    "start": "1870639",
    "end": "1876440"
  },
  {
    "text": "lab to process imagery coming off of a microscope and it was all disconnected",
    "start": "1876440",
    "end": "1882159"
  },
  {
    "text": "from the public internet so in that case that you just have a CPU maybe you need to optimize on the the CPU so there's",
    "start": "1882159",
    "end": "1889880"
  },
  {
    "text": "gradually more and more options that are out there to do this some people might",
    "start": "1889880",
    "end": "1895559"
  },
  {
    "text": "have seen things like llama CPP which is sort of a implementation of the Llama",
    "start": "1895559",
    "end": "1902080"
  },
  {
    "text": "architecture that's very efficient and allows you to run llama language models",
    "start": "1902080",
    "end": "1908200"
  },
  {
    "text": "on like your laptop or on like an I think a lot of people were running them on MacBooks with M1 or M2 processors if",
    "start": "1908200",
    "end": "1916080"
  },
  {
    "text": "you want to kind of scroll through this set of optimization stuff if you go to",
    "start": "1916080",
    "end": "1924120"
  },
  {
    "text": "the Intel analytics big DL repo that's big DL like big deep learning mhm first",
    "start": "1924120",
    "end": "1931840"
  },
  {
    "text": "of all the big DL Library does a lot of this sort of optimization or helps you",
    "start": "1931840",
    "end": "1937559"
  },
  {
    "text": "run these sorts of models an optimized way but they also have this little note at the top which is actually a very I",
    "start": "1937559",
    "end": "1944720"
  },
  {
    "text": "found it to be a very helpful little index as well they say this is built on top of the excellent work of llama CPP",
    "start": "1944720",
    "end": "1952760"
  },
  {
    "text": "gptq gml llama CPP python bits and btes Cur etc etc etc these are all things",
    "start": "1952760",
    "end": "1960880"
  },
  {
    "text": "that people have done to run big models in a smaller way I guess would be the",
    "start": "1960880",
    "end": "1967760"
  },
  {
    "text": "the right way to put it so bits and bites is a good example of this um hugging face has a bunch of blog posts",
    "start": "1967760",
    "end": "1974840"
  },
  {
    "text": "about this where they've run you know the big blo model in a Google collab notebook by loading it not in the full",
    "start": "1974840",
    "end": "1982799"
  },
  {
    "text": "Precision but in a quantized way but there's a lot of different ways to do",
    "start": "1982799",
    "end": "1988080"
  },
  {
    "text": "this and and that's a kind of a good reference to see a bunch of those different ways at some point for a",
    "start": "1988080",
    "end": "1994000"
  },
  {
    "text": "future show we should come back and revisit that that sounds really cool yeah yeah and I think it probably",
    "start": "1994000",
    "end": "2000000"
  },
  {
    "text": "deserves a show in and of itself um people might refer back to a episode that we had with uh neural Magic on the",
    "start": "2000000",
    "end": "2008399"
  },
  {
    "text": "podcast where they talked about the various strategies for optimizing a",
    "start": "2008399",
    "end": "2014240"
  },
  {
    "text": "model to run on commodity Hardware like CPUs but there's a ton of different",
    "start": "2014240",
    "end": "2020639"
  },
  {
    "text": "projects in this space both from companies and open- Source projects like open Vino and Optimum and bits and bites",
    "start": "2020639",
    "end": "2028159"
  },
  {
    "text": "and and all of these so if you are needing to take this big model and make",
    "start": "2028159",
    "end": "2033240"
  },
  {
    "text": "it either make it smaller or run it more optimized on certain Hardware",
    "start": "2033240",
    "end": "2038480"
  },
  {
    "text": "then you might want to go through this model optimization phase assuming you did that",
    "start": "2038480",
    "end": "2044440"
  },
  {
    "text": "or you didn't need to optimize your model then we get to deployment um now Chris what's in your mind when you think",
    "start": "2044440",
    "end": "2051480"
  },
  {
    "text": "of these days where might people want to deploy models yeah I think so it's one",
    "start": "2051480",
    "end": "2059398"
  },
  {
    "text": "of those situations where I a lot of people I'm talking to are trying to",
    "start": "2059399",
    "end": "2064638"
  },
  {
    "text": "decide between Cloud environments and we're seeing some some people that had dived into Cloud pulling back uh and",
    "start": "2064639",
    "end": "2071079"
  },
  {
    "text": "investing in their own and as well as starting to explore some of the other chip offerings so people are kind of",
    "start": "2071079",
    "end": "2078040"
  },
  {
    "text": "reconsidering that go Cloud when it's too big for you now and looking at these open models in their own hardware and",
    "start": "2078040",
    "end": "2084520"
  },
  {
    "text": "trying to figure out okay I don't really know how to do that at this point so that's where I'm really curious is let's",
    "start": "2084520",
    "end": "2090638"
  },
  {
    "text": "say that we go ahead and and buy you know a reasonable GPU capability uh inh",
    "start": "2090639",
    "end": "2096118"
  },
  {
    "text": "housee but it's not too big what can I make of that if I'm willing to do a little bit of investment but not you",
    "start": "2096119",
    "end": "2102119"
  },
  {
    "text": "know we're not talking millions and millions of dollars kind of thing yeah yeah so it might be good for people to",
    "start": "2102119",
    "end": "2109280"
  },
  {
    "text": "kind of categorize the ways that you might want to deploy an AI model for",
    "start": "2109280",
    "end": "2115320"
  },
  {
    "text": "your own application and even before I give those categories I think i' also",
    "start": "2115320",
    "end": "2120720"
  },
  {
    "text": "normally recommend to people that I think still the best way to think about deploying one of these models if you're",
    "start": "2120720",
    "end": "2127720"
  },
  {
    "text": "you're deploying it to support some type of application in your business or for",
    "start": "2127720",
    "end": "2133760"
  },
  {
    "text": "your own personal project or whatever it is any type of scale I think you're going to save yourself a lot of time by",
    "start": "2133760",
    "end": "2141079"
  },
  {
    "text": "thinking about the deployment of the model as a rest API and then your",
    "start": "2141079",
    "end": "2147359"
  },
  {
    "text": "application code connecting to that model or a rest API or a grpc API or",
    "start": "2147359",
    "end": "2152440"
  },
  {
    "text": "whatever type of API you want but the the purpose of the model server is to serve the model and and then you have",
    "start": "2152440",
    "end": "2158240"
  },
  {
    "text": "your application code that connects to that now that could be running on the same machine or the same VM as your",
    "start": "2158240",
    "end": "2165359"
  },
  {
    "text": "application code or it could be running on a different one but as soon as you make that separation a little bit it you",
    "start": "2165359",
    "end": "2171760"
  },
  {
    "text": "know I I don't really promote people you know microservice everything but I think in terms of model serving it's useful",
    "start": "2171760",
    "end": "2177800"
  },
  {
    "text": "because you can take care of the concerns of that model maybe the specialized Hardware it's running on and",
    "start": "2177800",
    "end": "2183560"
  },
  {
    "text": "then take care of the concerns of your application separately and if your application is a front-end web app or is",
    "start": "2183560",
    "end": "2192359"
  },
  {
    "text": "something written an API written in go or you know rust or whatever it is then",
    "start": "2192359",
    "end": "2197839"
  },
  {
    "text": "you don't have to worry about like oh how do I run this in a different language or that sort of thing you just handle that through the API contract so",
    "start": "2197839",
    "end": "2205359"
  },
  {
    "text": "that's maybe one kind of classical separation of concerns you know that any",
    "start": "2205359",
    "end": "2210520"
  },
  {
    "text": "developer would be doing yep yep exactly and then you can test each separately all of that good stuff sure but um if we",
    "start": "2210520",
    "end": "2217119"
  },
  {
    "text": "think about categories of how you might deploy these things there's the case",
    "start": "2217119",
    "end": "2224240"
  },
  {
    "text": "where you would want to run this in a serverless way like we already talked",
    "start": "2224240",
    "end": "2231200"
  },
  {
    "text": "about what cloudflare just released but there's a whole bunch of these options like Cloud flare and banana and base 10",
    "start": "2231200",
    "end": "2238880"
  },
  {
    "text": "and modal and a bunch of different places where you can spin up a GPU when",
    "start": "2238880",
    "end": "2243920"
  },
  {
    "text": "you need it and then it shuts down our scales to zero afterward words and there",
    "start": "2243920",
    "end": "2249560"
  },
  {
    "text": "are so depending on the size of your model and how you implement it the sort of cold start time or the time it takes",
    "start": "2249560",
    "end": "2256720"
  },
  {
    "text": "to spin up that model and have it ready for you to use might be somewhat annoying for you but the advantage is",
    "start": "2256720",
    "end": "2263359"
  },
  {
    "text": "you're not going to pay a lot so you could at least try that first there's kind of this more and more offerings in",
    "start": "2263359",
    "end": "2269319"
  },
  {
    "text": "that space But A lot of them have like you know base 10 the cloud flare thing",
    "start": "2269319",
    "end": "2275800"
  },
  {
    "text": "whatever it is you're going to be running it in someone else's infrastructure so if you have like your",
    "start": "2275800",
    "end": "2281400"
  },
  {
    "text": "own on-prem thing or something like that maybe a little bit harder to deploy that",
    "start": "2281400",
    "end": "2288000"
  },
  {
    "text": "sort of serverless infrastructure because they've optimized those systems for what they are so likely in that",
    "start": "2288000",
    "end": "2294760"
  },
  {
    "text": "scenario you're signing up for an account on one of these platforms and you're deploying your model there and",
    "start": "2294760",
    "end": "2300000"
  },
  {
    "text": "then you can interact with it when you want a second kind of way you could do",
    "start": "2300000",
    "end": "2305040"
  },
  {
    "text": "this is like a containerized model server that's running either on a VM or",
    "start": "2305040",
    "end": "2311520"
  },
  {
    "text": "a bare metal server that has an accelerator on it one or more accelerators on it right and so you",
    "start": "2311520",
    "end": "2319400"
  },
  {
    "text": "could spin up an ec2 instance with a with a GPU or you know you could even",
    "start": "2319400",
    "end": "2324560"
  },
  {
    "text": "run this as part of a auto scaling cluster that's like a kubernetes cluster or something like that but these would",
    "start": "2324560",
    "end": "2330440"
  },
  {
    "text": "be VMS that have a GPU attached or something like that and they would be",
    "start": "2330440",
    "end": "2336160"
  },
  {
    "text": "probably up either all the time or they would have uptime that's different from",
    "start": "2336160",
    "end": "2342160"
  },
  {
    "text": "the serverless offerings sure and so you'd just be paying for that all the time and in in those cases like maybe",
    "start": "2342160",
    "end": "2348720"
  },
  {
    "text": "you could use a model packaging system like base 10's truss is one that I use",
    "start": "2348720",
    "end": "2355400"
  },
  {
    "text": "but there's other ones as well uh Selden and others that will actually create a",
    "start": "2355400",
    "end": "2360680"
  },
  {
    "text": "model package in a dockerized way that allows you to deploy your system is",
    "start": "2360680",
    "end": "2366720"
  },
  {
    "text": "there any standard ization yet in that space or does each vendor have its own",
    "start": "2366720",
    "end": "2371760"
  },
  {
    "text": "approach I think each vendor has its own approach like if you look at um hugging face they have the TGI or text",
    "start": "2371760",
    "end": "2379880"
  },
  {
    "text": "generation inference project which I think is what they use a lot to serve some of their models and that kind of is",
    "start": "2379880",
    "end": "2387200"
  },
  {
    "text": "set up differently than base 10's trust which is set up differently than Selden",
    "start": "2387200",
    "end": "2392440"
  },
  {
    "text": "system um there are some standardization in that like if you have a",
    "start": "2392440",
    "end": "2397960"
  },
  {
    "text": "a general um like onx model or something like that there's various servers that",
    "start": "2397960",
    "end": "2404200"
  },
  {
    "text": "take in that format but the way in which you set up your rest API might be",
    "start": "2404200",
    "end": "2410240"
  },
  {
    "text": "different in different Frameworks so this is a very framework dependent thing I would say gotcha yeah and there's also",
    "start": "2410240",
    "end": "2418200"
  },
  {
    "text": "an additional layer of choice here not only in terms of what framework you use but also in terms of optimizations",
    "start": "2418200",
    "end": "2425280"
  },
  {
    "text": "around that so there's certain optimizations like VM which is an open source project that not so this doesn't",
    "start": "2425280",
    "end": "2433280"
  },
  {
    "text": "modify the model but it modifies the inference code that allows the model to",
    "start": "2433280",
    "end": "2438319"
  },
  {
    "text": "run more efficiently for inference so this is not the sort of model optimization that we talked about",
    "start": "2438319",
    "end": "2444880"
  },
  {
    "text": "earlier which is actually changing the model in terms of precision or in other ways but this is actually a layer of",
    "start": "2444880",
    "end": "2452800"
  },
  {
    "text": "optimization of how the model is called that helps it run faster",
    "start": "2452800",
    "end": "2458119"
  },
  {
    "text": "so yeah there's there's a lot of a lot of choices uh there as well and I think",
    "start": "2458119",
    "end": "2464760"
  },
  {
    "text": "once you get to that point and you've chosen like let's say you're using base",
    "start": "2464760",
    "end": "2470119"
  },
  {
    "text": "10's trust system and you've deployed your model you know either on a VM or in",
    "start": "2470119",
    "end": "2475440"
  },
  {
    "text": "a serverless environment or you're using whatever system you're using I think",
    "start": "2475440",
    "end": "2481520"
  },
  {
    "text": "then kind of gets to these additional operational concerns about like how do I",
    "start": "2481520",
    "end": "2487839"
  },
  {
    "text": "plug all this together in an automated way so if I push my model hugging face",
    "start": "2487839",
    "end": "2493800"
  },
  {
    "text": "or if I update my inference code how does that trigger a rebuild of my server",
    "start": "2493800",
    "end": "2499359"
  },
  {
    "text": "and then redeploy that on my infrastructure and that gets closer than into what is more",
    "start": "2499359",
    "end": "2506040"
  },
  {
    "text": "traditionally Dev opsy infrastructure automation type of things which is its",
    "start": "2506040",
    "end": "2512359"
  },
  {
    "text": "own whole land of Frameworks and options and that sort of thing but it's more of a standardized thing that software",
    "start": "2512359",
    "end": "2518599"
  },
  {
    "text": "Engineers are are familiar with right that's kind of um from my perspective",
    "start": "2518599",
    "end": "2524560"
  },
  {
    "text": "that's if we were to just summarize you kind of go from model selection and",
    "start": "2524560",
    "end": "2531599"
  },
  {
    "text": "experimentation which I would say don't spin up your own infrastructure necessarily for that once you figure out",
    "start": "2531599",
    "end": "2537680"
  },
  {
    "text": "a behavior of a model that works well for you then decide if you need to optimize it to run it in the environment",
    "start": "2537680",
    "end": "2543960"
  },
  {
    "text": "you need to if so optimize it and then once you're ready to deploy it think",
    "start": "2543960",
    "end": "2549720"
  },
  {
    "text": "about a model server which is geared to",
    "start": "2549720",
    "end": "2555720"
  },
  {
    "text": "specifically inferencing of your model and that's the separation of concerns",
    "start": "2555720",
    "end": "2560839"
  },
  {
    "text": "and either you you can use a framework like one of these we've talked about or you could build your own you know fast",
    "start": "2560839",
    "end": "2567000"
  },
  {
    "text": "API service around it or whatever API service you like and deploy it in a way",
    "start": "2567000",
    "end": "2572720"
  },
  {
    "text": "that is ideally automated so that you can uh do all the nice Dev opsy things",
    "start": "2572720",
    "end": "2579640"
  },
  {
    "text": "around it that sounds really good with uh so you've you've done a fantastic job",
    "start": "2579640",
    "end": "2584839"
  },
  {
    "text": "of laying everything out I think I've talked you horse uh at the moment trying",
    "start": "2584839",
    "end": "2590920"
  },
  {
    "text": "to cover everything be careful what you ask for Chris so as we are winding up",
    "start": "2590920",
    "end": "2596040"
  },
  {
    "text": "for this episode what are some of the kind of you open source go-to tools uh",
    "start": "2596040",
    "end": "2602960"
  },
  {
    "text": "that pop top of mind for you that you tend to find yourself going to uh over and over again you know for folks to",
    "start": "2602960",
    "end": "2609040"
  },
  {
    "text": "explore yeah I I think on the pulling a model down and running it for inference",
    "start": "2609040",
    "end": "2616040"
  },
  {
    "text": "just that sort of series of things there's really nothing in my opinion that beats the hugging face uh",
    "start": "2616040",
    "end": "2624200"
  },
  {
    "text": "Transformers library and this is not for people that aren't familiar this is not just for language models and that sort",
    "start": "2624200",
    "end": "2631680"
  },
  {
    "text": "of Transformers but this is general purpose functionality that you can use",
    "start": "2631680",
    "end": "2637119"
  },
  {
    "text": "use also for speech models and computer vision models and all sorts of models",
    "start": "2637119",
    "end": "2642880"
  },
  {
    "text": "both in terms of data sets and pulling down models and con extra convenience on",
    "start": "2642880",
    "end": "2648720"
  },
  {
    "text": "top of that there's not really anything I think that is more comprehensive than that and hugging face has a great",
    "start": "2648720",
    "end": "2655599"
  },
  {
    "text": "hugging face course where you can online if you just search for hugging face course it'll walk you through some of",
    "start": "2655599",
    "end": "2661520"
  },
  {
    "text": "that in terms of the model optimization side of things",
    "start": "2661520",
    "end": "2667559"
  },
  {
    "text": "I would recommend checking on a few different packages one of those is called Optimum it's collaboration",
    "start": "2667559",
    "end": "2675440"
  },
  {
    "text": "between a bunch of different parties but uh it allows you to load models with the",
    "start": "2675440",
    "end": "2682559"
  },
  {
    "text": "hugging face API so similar to like how you would load them with hugging face but then optimize them on the Fly for",
    "start": "2682559",
    "end": "2689760"
  },
  {
    "text": "various architectures like CPUs or gouty processors or special processors in",
    "start": "2689760",
    "end": "2696079"
  },
  {
    "text": "terms of like Quant ization and model optimization of the actual model like the model parameters you could look up",
    "start": "2696079",
    "end": "2702559"
  },
  {
    "text": "bits and bites by hugging face um open Veno by Intel uh this big DL library",
    "start": "2702559",
    "end": "2710319"
  },
  {
    "text": "from Intel which I mentioned that readme in that GitHub also links to other",
    "start": "2710319",
    "end": "2715760"
  },
  {
    "text": "things that people have done so it's nice that you can kind of explore that as well um and there are other projects",
    "start": "2715760",
    "end": "2722640"
  },
  {
    "text": "like Apache TVM and others that um have been around for some time and do model",
    "start": "2722640",
    "end": "2729000"
  },
  {
    "text": "optimization yep and we've talked about that one before yeah and then on the deployment side there's an increasing",
    "start": "2729000",
    "end": "2737280"
  },
  {
    "text": "number the one that I've used quite a bit is called truss from base 10 uh TR",
    "start": "2737280",
    "end": "2742960"
  },
  {
    "text": "SS like a bridge truss and uh that allows kind of packaging and deployment",
    "start": "2742960",
    "end": "2748680"
  },
  {
    "text": "of models you don't have to use their Cloud envir you can deploy to their Cloud environment if you want or you",
    "start": "2748680",
    "end": "2754119"
  },
  {
    "text": "could just run it as a Docker container but it's really this packaging but there's other ones I mentioned too like",
    "start": "2754119",
    "end": "2760079"
  },
  {
    "text": "the TGI uh from hugging face or VM if",
    "start": "2760079",
    "end": "2766200"
  },
  {
    "text": "you're interested in in llms so yeah there's kind of a range there and of",
    "start": "2766200",
    "end": "2771240"
  },
  {
    "text": "course each cloud provider has their option to deploy models as well like",
    "start": "2771240",
    "end": "2776319"
  },
  {
    "text": "Sage maker and AWS uh which a lot of people use also so I think you've given",
    "start": "2776319",
    "end": "2781680"
  },
  {
    "text": "us plenty of homework to go out there and explore a bit yeah yeah there's no shortage of things to try it can be a",
    "start": "2781680",
    "end": "2788720"
  },
  {
    "text": "little bit overwhelming to navigate the landscape but I would just encourage people you know that first step of",
    "start": "2788720",
    "end": "2794480"
  },
  {
    "text": "figuring out what model you need to use doesn't require you toplo a bunch of stuff just try it in a notebook and once",
    "start": "2794480",
    "end": "2800640"
  },
  {
    "text": "you figure that out then find a way even just search for like oh you found out",
    "start": "2800640",
    "end": "2806920"
  },
  {
    "text": "you want to use llama 27 billion just search for the great thing now is you",
    "start": "2806920",
    "end": "2812559"
  },
  {
    "text": "can search and say like running llama 7 billion on a CPU and there'll be a few different blog",
    "start": "2812559",
    "end": "2819520"
  },
  {
    "text": "posts that you can follow to figure out how people have done that and um so just follow that path and kind of follow some",
    "start": "2819520",
    "end": "2826040"
  },
  {
    "text": "of the examples that are out there it's not like any of us that are doing this day-to-day don't do the exact same thing",
    "start": "2826040",
    "end": "2832359"
  },
  {
    "text": "like when we deployed recently on the gudy processors and Intel developer Cloud I just went to the Habana Labs",
    "start": "2832359",
    "end": "2840400"
  },
  {
    "text": "repo where they talk about gouty and they have like you know text generation.",
    "start": "2840400",
    "end": "2846160"
  },
  {
    "text": "py example or whatever it was called and you know there's a lot of copy and pasting that happens so that's okay and",
    "start": "2846160",
    "end": "2852440"
  },
  {
    "text": "that's how development works so fantastic well thank you for letting me pick your brain on this topic for a",
    "start": "2852440",
    "end": "2858160"
  },
  {
    "text": "while sure uh and and like I said I think you're almost horse after this one",
    "start": "2858160",
    "end": "2864040"
  },
  {
    "text": "uh but that was a really really good instructional episode so I'll actually personally be going back over it cool",
    "start": "2864040",
    "end": "2870280"
  },
  {
    "text": "well it's fun Chris uh thanks for letting me ramble on and I'm sure we'll have some follow-ups on similar topics",
    "start": "2870280",
    "end": "2876559"
  },
  {
    "text": "as well well absolutely all right well that'll be it for this episode thank you very much Daniel for uh for filling both",
    "start": "2876559",
    "end": "2883400"
  },
  {
    "text": "the host and the guest seat this week um another fully connected episode I'll talk to you next week all right talk to",
    "start": "2883400",
    "end": "2889119"
  },
  {
    "text": "you [Music]",
    "start": "2889119",
    "end": "2897119"
  },
  {
    "text": "soon thank you for listening to practical AI your next step is to",
    "start": "2897119",
    "end": "2902440"
  },
  {
    "text": "subscribe now if you haven't already and if you're a longtime listener of the show help us reach more people by",
    "start": "2902440",
    "end": "2908760"
  },
  {
    "text": "sharing practical AI with your friends and colleagues thanks once again to fastly and fly for partnering with us to",
    "start": "2908760",
    "end": "2914880"
  },
  {
    "text": "bring you all change doog podcasts check out what they're up to at fastly.com and",
    "start": "2914880",
    "end": "2920200"
  },
  {
    "text": "fly.io and to our beat freaking residence brakemaster cylinder for continuously cranking out the best beats",
    "start": "2920200",
    "end": "2926000"
  },
  {
    "text": "in the biz that's all for now we'll talk to you again next [Music]",
    "start": "2926000",
    "end": "2935720"
  },
  {
    "text": "time [Music]",
    "start": "2935720",
    "end": "2941169"
  }
]