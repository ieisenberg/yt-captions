[
  {
    "text": "what we are doing and specializing is kind of the model design approach where you need to select the right model that",
    "start": "120",
    "end": "7040"
  },
  {
    "text": "is optimized both for the data to reach the accuracy and also for the hardware to reach the latency and by optimizing",
    "start": "7040",
    "end": "14519"
  },
  {
    "text": "the model specifically for the inference Hardware that you're looking to use you can get significant boost in the",
    "start": "14519",
    "end": "20640"
  },
  {
    "text": "performance compared to having kind of an open source model or off-the-shelf model that you just took from an",
    "start": "20640",
    "end": "26800"
  },
  {
    "text": "academic paper or from GitHub or something like that",
    "start": "26800",
    "end": "31960"
  },
  {
    "text": "big thanks to our partners Leno fastly and launch Darkly we love Leno they keep it fast and simple check them out at",
    "start": "32160",
    "end": "37800"
  },
  {
    "text": "lin.com changelog our bandwidth is provided by fastly learn more at fastly.com and get your feature Flags",
    "start": "37800",
    "end": "44719"
  },
  {
    "text": "Power by lunch Darkly get a demo at launchdarkly tocom this episode is brought to you by our friends at Rudder",
    "start": "44719",
    "end": "51199"
  },
  {
    "text": "stack and we're calling all data Engineers to check out ruter stack cloud and start building smart customer data pipelines Rudder stack is Warehouse",
    "start": "51199",
    "end": "58120"
  },
  {
    "text": "first no more silos rstack build your customer data Lake on your data warehouse not theirs enabling all",
    "start": "58120",
    "end": "64400"
  },
  {
    "text": "functionality of a CDP with more security and retaining full ownership of your data it's open source and API first",
    "start": "64400",
    "end": "72040"
  },
  {
    "text": "R stack can be easily integrated into your existing development processes and because they're open source you can see",
    "start": "72040",
    "end": "78119"
  },
  {
    "text": "all their code so you don't have to worry about vendor lockin or black boxes and best of all they have transparent",
    "start": "78119",
    "end": "83439"
  },
  {
    "text": "pricing stop paying your CDP a premium to store your data R stack is free up to 500,000 events and price in skills",
    "start": "83439",
    "end": "90520"
  },
  {
    "text": "transparently from there learn more and get started at rud stack.com again rud stack.com that's Rudd R",
    "start": "90520",
    "end": "100200"
  },
  {
    "text": "[Music]",
    "start": "101490",
    "end": "108938"
  },
  {
    "text": "st.com welcome to practical AI a weekly podcast that makes artificial intelligence practical productive and",
    "start": "109399",
    "end": "116320"
  },
  {
    "text": "accessible to everyone this is where conversations around AI machine learning and data science happen join the",
    "start": "116320",
    "end": "122320"
  },
  {
    "text": "community and slack with us around various topics of the show at kj.com community and follow us on Twitter we at",
    "start": "122320",
    "end": "128560"
  },
  {
    "text": "practical [Music]",
    "start": "128560",
    "end": "135120"
  },
  {
    "text": "aifm welcome to another episode of practical AI this is Daniel whack I am a",
    "start": "135120",
    "end": "141400"
  },
  {
    "text": "data scientist with s International and I'm joined as always by my co-host Chris",
    "start": "141400",
    "end": "146680"
  },
  {
    "text": "Brenson who is a tech strategist at luy Mar how you doing Chris I am doing very",
    "start": "146680",
    "end": "152360"
  },
  {
    "text": "well it's a dog days of summer as we record this and uh just try not to melt",
    "start": "152360",
    "end": "157840"
  },
  {
    "text": "yeah yeah it's hot like you're sitting right on top of a bunch of gpus or",
    "start": "157840",
    "end": "162879"
  },
  {
    "text": "something that's exactly right yeah well speaking of Hardware today's",
    "start": "162879",
    "end": "169200"
  },
  {
    "text": "episode is kind of connected to that really excited to have janathan Gan with",
    "start": "169200",
    "end": "174400"
  },
  {
    "text": "us who is CEO and co-founder of the deep Learning Company desie welcome Jonathan",
    "start": "174400",
    "end": "180360"
  },
  {
    "text": "hi thank you for hosting me how are you yeah doing wonderful so I know a lot of",
    "start": "180360",
    "end": "186760"
  },
  {
    "text": "what your company does is related to productionizing models optimizing models",
    "start": "186760",
    "end": "192720"
  },
  {
    "text": "there's a bunch of things that you do related to optimizing inference and I'd love to dive into all of that as time",
    "start": "192720",
    "end": "199599"
  },
  {
    "text": "goes on but maybe let's just start out by talking about why should we care so",
    "start": "199599",
    "end": "205280"
  },
  {
    "text": "much about inference so a lot of effort and I think screen time if we want to",
    "start": "205280",
    "end": "210360"
  },
  {
    "text": "put it that way in terms of Twitter and other places is is put on the training side of things and scaling up training",
    "start": "210360",
    "end": "217239"
  },
  {
    "text": "but why do deep learning and AI practitioners need to be concerned about",
    "start": "217239",
    "end": "222599"
  },
  {
    "text": "inference so actually the first thing to be concerned with ISD training but after",
    "start": "222599",
    "end": "227720"
  },
  {
    "text": "you finish building a model then you need to deploy it somewhere it could be in the cloud or in the edge and when you",
    "start": "227720",
    "end": "234040"
  },
  {
    "text": "are going to deploy it and do serving or influence at scale you must be concerned",
    "start": "234040",
    "end": "240319"
  },
  {
    "text": "or think about how this model is going to perform in terms of latency or triput",
    "start": "240319",
    "end": "245439"
  },
  {
    "text": "what is the operational cost of this model or are you actually able to deploy it on the edge device that you want to",
    "start": "245439",
    "end": "251640"
  },
  {
    "text": "use in order to serve the inference of that model and it's kind of the second phase of the development cycle after you",
    "start": "251640",
    "end": "258639"
  },
  {
    "text": "reach the accuracy but you must think about it at the beginning when you choose the model in order to know that",
    "start": "258639",
    "end": "264400"
  },
  {
    "text": "you are going to finish the project with a model that meets the SLA of going to production in your environment with your",
    "start": "264400",
    "end": "271160"
  },
  {
    "text": "characteristics of performance that you are looking for in production if you think about it you'll see that a model",
    "start": "271160",
    "end": "278440"
  },
  {
    "text": "is trained once or once in a while but the inference workload is huge because it's kind of linearly scales with the",
    "start": "278440",
    "end": "285960"
  },
  {
    "text": "amount of data in production so if you think about an organization that for",
    "start": "285960",
    "end": "291160"
  },
  {
    "text": "example develops a self-driving car you will think about the number of data scientists that doing the training work",
    "start": "291160",
    "end": "296919"
  },
  {
    "text": "building the models each one of them needs some amount of G Us in order to do his work but when you deploy these",
    "start": "296919",
    "end": "302639"
  },
  {
    "text": "algorithms these are deployed among huge amounts of cars so in order to optimize",
    "start": "302639",
    "end": "309039"
  },
  {
    "text": "the performance you're more concerned about what type of Hardware you'll need to fit into the car compared to how many",
    "start": "309039",
    "end": "315960"
  },
  {
    "text": "gpus you need to have in the cloud for your data scientist to build those models so this is kind of the proportions the training is kind of",
    "start": "315960",
    "end": "322919"
  },
  {
    "text": "linearly scale by the number of data scientists while the inference is scaled by the amount of data the scale of the",
    "start": "322919",
    "end": "329720"
  },
  {
    "text": "deployment that you're going to use and you must think about the performance when you want to go to production at",
    "start": "329720",
    "end": "335319"
  },
  {
    "text": "scale yeah and as a practitioner you mentioned thinking about inference",
    "start": "335319",
    "end": "340720"
  },
  {
    "text": "upfront maybe even before training thinking about what models you might even be able to use in production as",
    "start": "340720",
    "end": "347199"
  },
  {
    "text": "you're working with different companies and different teams of data scientists do you see people running into this",
    "start": "347199",
    "end": "353000"
  },
  {
    "text": "challenge a lot where they're really happy with their model that they spent you know a lot of time and effort training and they just are completely",
    "start": "353000",
    "end": "359600"
  },
  {
    "text": "completely blocked in taking the model to production and how often do you see that and what are the main reasons like",
    "start": "359600",
    "end": "365919"
  },
  {
    "text": "why they're not able to do inference with that model is it a latency thing is",
    "start": "365919",
    "end": "371199"
  },
  {
    "text": "it a memory constraint thing what are some of the challenges you see so it is really divided into several problems but",
    "start": "371199",
    "end": "379000"
  },
  {
    "text": "there is a simple principle that you can always take a larger model and get better accuracy and it's easy to get",
    "start": "379000",
    "end": "385319"
  },
  {
    "text": "better accuracy with larger models but this is only happens if you don't need to think about production when you think",
    "start": "385319",
    "end": "391759"
  },
  {
    "text": "about production you understand that those models are not production ready let's say for example if you want to",
    "start": "391759",
    "end": "396919"
  },
  {
    "text": "deploy them in the edge you have several constraints like the memory and the latency that you want to run probably in",
    "start": "396919",
    "end": "402160"
  },
  {
    "text": "real time or giving some reasonable response time for the model but in the cloud you're more concerned about the",
    "start": "402160",
    "end": "408199"
  },
  {
    "text": "cloud cost of serving this at scale the amount of Hardware that you will need to orchestrate in order to serve your users",
    "start": "408199",
    "end": "415039"
  },
  {
    "text": "or your demand for inference and this is kind of the things that you need to take into account while you develop your",
    "start": "415039",
    "end": "421240"
  },
  {
    "text": "model because as I said at the beginning going larger is easier to get accuracy but it's not easier to get deployed so",
    "start": "421240",
    "end": "428160"
  },
  {
    "text": "we see companies all the time getting some huge models trying to fit them into small devices for Edge inference or",
    "start": "428160",
    "end": "435759"
  },
  {
    "text": "companies that just understand that they took the largest model that they can have and get the right accuracy but then",
    "start": "435759",
    "end": "441720"
  },
  {
    "text": "they're starting to scale and see the costs of surrounding these inference workloads and then trying to think about",
    "start": "441720",
    "end": "448240"
  },
  {
    "text": "okay how can I reduce those gpus costs or the CPU cost in in running production in the cloud and this happens all the",
    "start": "448240",
    "end": "455360"
  },
  {
    "text": "time so our approach is think about these problems at the beginning while you start development choosing the right",
    "start": "455360",
    "end": "461560"
  },
  {
    "text": "architecture at the beginning instead of kind of having a trial and error iterations when you get to the accuracy",
    "start": "461560",
    "end": "467599"
  },
  {
    "text": "but then you understand that you need to change the model in order to get it productized yeah and I actually have a",
    "start": "467599",
    "end": "473000"
  },
  {
    "text": "follow up on that mainly because I only have a certain sphere of experience and",
    "start": "473000",
    "end": "478120"
  },
  {
    "text": "I only know what my organiz ation does and what some other organizations do just for my own curiosity how often are",
    "start": "478120",
    "end": "486240"
  },
  {
    "text": "you seeing people use gpus on the inference side versus just CPUs do you",
    "start": "486240",
    "end": "492280"
  },
  {
    "text": "see that being done more and more or you know what's the majority of inference",
    "start": "492280",
    "end": "498039"
  },
  {
    "text": "use cases that you're seeing is it inference on gpus or inference on CPUs maybe specifically thinking about Cloud",
    "start": "498039",
    "end": "504039"
  },
  {
    "text": "deployments Edge deployments might be somewhat differently but think about Cloud deployments at organizations that",
    "start": "504039",
    "end": "509639"
  },
  {
    "text": "you work with is it often inference on the GPU or is it often inference on a CPU I think that it's really depends on",
    "start": "509639",
    "end": "517120"
  },
  {
    "text": "the task that you're trying to do the inference and what are you trying to achieve I think that if we were talking about video analytics workloads you must",
    "start": "517120",
    "end": "524640"
  },
  {
    "text": "use GPU you have a lot of data you want to process the data the images in higher resolution you need a GPU performance if",
    "start": "524640",
    "end": "532040"
  },
  {
    "text": "you're talking about having some queries of NLP model you usually find those",
    "start": "532040",
    "end": "537080"
  },
  {
    "text": "deployed on CPUs but it does doesn't matter cuz both of them are getting expensive when you get to scale so if",
    "start": "537080",
    "end": "543959"
  },
  {
    "text": "you look about for example prices on the cloud for having a foure CPU and a T4",
    "start": "543959",
    "end": "550120"
  },
  {
    "text": "GPU it's approximately the same so it's not like the problem is only the prices",
    "start": "550120",
    "end": "555800"
  },
  {
    "text": "of GPU also the computer of the CPU is getting expensive when you have to run in large workloads with large clusters",
    "start": "555800",
    "end": "562959"
  },
  {
    "text": "with multiple nodes and course could you talk a little bit as we've kind of touched on edge specifically could you",
    "start": "562959",
    "end": "568360"
  },
  {
    "text": "touch on what some of the challenges you see about deployment for to the edge and the inference that's associated with",
    "start": "568360",
    "end": "574560"
  },
  {
    "text": "that are even Beyond just the GPU CPU consideration that's certainly something",
    "start": "574560",
    "end": "579839"
  },
  {
    "text": "that I'm involved a lot in and more and more people are now having to deploy to Edge and all sorts of different use",
    "start": "579839",
    "end": "586120"
  },
  {
    "text": "cases and I think we're pretty accustomed at this point to thinking about cloud-based deployment because",
    "start": "586120",
    "end": "592560"
  },
  {
    "text": "that's matured a lot faster but as more and more organizations are involved in Edge deployments I think they're trying",
    "start": "592560",
    "end": "598519"
  },
  {
    "text": "to explore their way through that do you have any guidance for that so first of all there's kind of a jungle of edge",
    "start": "598519",
    "end": "604519"
  },
  {
    "text": "Hardware types there are a lot of types of Hardware that you can use at the edge and it really depends on the application",
    "start": "604519",
    "end": "610279"
  },
  {
    "text": "it could be a mobile phone that if you'll deploy a mobile app with deep learning in it you will find out that",
    "start": "610279",
    "end": "616720"
  },
  {
    "text": "your users are spreaded across something like 10 or more types of Hardware from",
    "start": "616720",
    "end": "621920"
  },
  {
    "text": "IOS with the A&E of Apple and Samsung devices with the qualcom Snapdragon and",
    "start": "621920",
    "end": "627959"
  },
  {
    "text": "hard types like that so first of all you need to understand what Hardware your users or you are going to deploy on",
    "start": "627959",
    "end": "634000"
  },
  {
    "text": "that's the first task then you need to understand kind of what is the software stack that is best to use for that type",
    "start": "634000",
    "end": "640079"
  },
  {
    "text": "of Hardware if we're talking about Apple devices and iOS we have the corl but most of the other types of Hardware will",
    "start": "640079",
    "end": "646800"
  },
  {
    "text": "probably be better running with TF light or those Frameworks that are optimized for Edge inference and above that you",
    "start": "646800",
    "end": "652920"
  },
  {
    "text": "need to understand kind of what is the limitations of the hardware that you are going to use for example memory strains",
    "start": "652920",
    "end": "660200"
  },
  {
    "text": "memory bottleneck of loading the weights loading the data and stuff like this and the performance what are you going to",
    "start": "660200",
    "end": "666680"
  },
  {
    "text": "get if you will run for example an object detection model on a Jetson how",
    "start": "666680",
    "end": "671880"
  },
  {
    "text": "many frames per second are you going to get how many video streams can you put on that Jetson and this is kind of",
    "start": "671880",
    "end": "678399"
  },
  {
    "text": "something that if you'll get to the accuracy you'll finish building the model and only then will measure it on",
    "start": "678399",
    "end": "684600"
  },
  {
    "text": "the device that you're looking to deploy on you'll get back to square one redesign the model in order to get the",
    "start": "684600",
    "end": "691800"
  },
  {
    "text": "SLA the latency that you're looking for at the edge and those are things that you need kind of to have analistic",
    "start": "691800",
    "end": "697839"
  },
  {
    "text": "approach and to see how you solve all of them together it's kind of a multi constraint optimization where the",
    "start": "697839",
    "end": "703519"
  },
  {
    "text": "accuracy the latency and the model size are things that you need to consider together when building an edge AI",
    "start": "703519",
    "end": "710040"
  },
  {
    "text": "application and that maybe leads me into another question about that I do want to",
    "start": "710040",
    "end": "715600"
  },
  {
    "text": "get into the specific like methods and technology that you've been involved in",
    "start": "715600",
    "end": "720880"
  },
  {
    "text": "developing but it sounds like you're also sort of suggesting a different kind of workflow that people can have in",
    "start": "720880",
    "end": "726920"
  },
  {
    "text": "their mind where as you contrast it to like oh I'm going to build my model and the environments in which I'm training",
    "start": "726920",
    "end": "733600"
  },
  {
    "text": "my model and testing it are totally different from those where I'm going to deploy it how can people like adjust",
    "start": "733600",
    "end": "740920"
  },
  {
    "text": "their workflow to maybe is it a matter of always making sure that you have a testing cycle where you're testing on",
    "start": "740920",
    "end": "747440"
  },
  {
    "text": "the hardware that you are targeting in the end or how can that be integrated into data scientist workflows in a",
    "start": "747440",
    "end": "754839"
  },
  {
    "text": "better way so we are kind of pushing to Hardware in a loop development approach",
    "start": "754839",
    "end": "760519"
  },
  {
    "text": "when you are taking the inference Hardware in the development stage very early in the model selection stage where",
    "start": "760519",
    "end": "767760"
  },
  {
    "text": "you considering some models usually based on some open source repositories and academic papers and you have to",
    "start": "767760",
    "end": "774760"
  },
  {
    "text": "measure them at the beginning this is kind of the first step in understanding if you're going in the right way after",
    "start": "774760",
    "end": "781240"
  },
  {
    "text": "that you need to understand or think how can you bring that model that meets the SLA to the accuracy that you're looking",
    "start": "781240",
    "end": "787959"
  },
  {
    "text": "for so this is kind of an opposite approach than first reaching the accuracy and then reaching the latency",
    "start": "787959",
    "end": "793639"
  },
  {
    "text": "and this is for Edge applications with constraints about the latency and the model size that those are need to be",
    "start": "793639",
    "end": "800199"
  },
  {
    "text": "considered at the beginning and not at the end makes sense I am wondering maybe",
    "start": "800199",
    "end": "805680"
  },
  {
    "text": "if you can just give like a broad stroke sketch of what people like let's say",
    "start": "805680",
    "end": "813000"
  },
  {
    "text": "that they get to a point where they have the model that they want and it's not quite optimized for the hardware Target",
    "start": "813000",
    "end": "820839"
  },
  {
    "text": "that they have in mind maybe there's too much latency or they need to shrink the model or something what just sort of",
    "start": "820839",
    "end": "827720"
  },
  {
    "text": "generally are people trying out there in terms of methods for optimizing their",
    "start": "827720",
    "end": "833079"
  },
  {
    "text": "models for inference on certain Hardware there's something we call the inference Tech where at the bottom we consider the",
    "start": "833079",
    "end": "839959"
  },
  {
    "text": "hardware itself on top of that we have a layer that we call it the drivers and the graph compilers so for example the",
    "start": "839959",
    "end": "846839"
  },
  {
    "text": "open Veno for indel CPU or the tensor RT for gpus on top of that we have have",
    "start": "846839",
    "end": "852440"
  },
  {
    "text": "like open source methods like pruning and quantization pruning is a method to reduce and eliminate unneeded neurons",
    "start": "852440",
    "end": "859639"
  },
  {
    "text": "and Connections in the network and quantization is representing the weights and the activations of the network in",
    "start": "859639",
    "end": "865320"
  },
  {
    "text": "lower bit representation like 8bit or something like that so those all are",
    "start": "865320",
    "end": "871000"
  },
  {
    "text": "kind of Open Source and public methods and techniques in order to build more",
    "start": "871000",
    "end": "876040"
  },
  {
    "text": "efficient inference on top of that what we are doing and specializing is kind of the model design approach where you need",
    "start": "876040",
    "end": "883399"
  },
  {
    "text": "to select the right model that is optimized both for the data to reach the accuracy and also for the hardware to",
    "start": "883399",
    "end": "890079"
  },
  {
    "text": "reach the latency and what we understand today is that different Hardware type",
    "start": "890079",
    "end": "895199"
  },
  {
    "text": "prefer different types of model I will give a simple example if you think about GPU which has parallelism capabilities",
    "start": "895199",
    "end": "903480"
  },
  {
    "text": "it will prefer large wide layers with fewer layers in the network because it",
    "start": "903480",
    "end": "909399"
  },
  {
    "text": "can parallelize the layer itself but it cannot parallelize between layers in contrast in CPU there's low parallelism",
    "start": "909399",
    "end": "917160"
  },
  {
    "text": "capabilities and you will probably prefer having narrow layers with less neurons but you can have more layers and",
    "start": "917160",
    "end": "924720"
  },
  {
    "text": "this is kind of a general idea how the inference speed could be affected by the",
    "start": "924720",
    "end": "930440"
  },
  {
    "text": "model structure and by optimizing the model specifically for the inference Hardware that you're looking to use you",
    "start": "930440",
    "end": "936959"
  },
  {
    "text": "can get significant boost in the performance compared to having kind of an open source model or off-the-shelf",
    "start": "936959",
    "end": "943639"
  },
  {
    "text": "model that you just took from an academic paper or from GitHub or something like [Music]",
    "start": "943639",
    "end": "955040"
  },
  {
    "text": "that",
    "start": "957920",
    "end": "960920"
  },
  {
    "text": "signal wire is Real Time vide Tech to help you create interactive video",
    "start": "964199",
    "end": "969519"
  },
  {
    "text": "experiences previously not possible it gives you access to broadcast quality ultra low latency video that's proven",
    "start": "969519",
    "end": "977000"
  },
  {
    "text": "and trusted by Amazon ring doorbell zoom and others see why the future of video",
    "start": "977000",
    "end": "983120"
  },
  {
    "text": "communication is being built on Signal wire they have easy to deploy apis sdks",
    "start": "983120",
    "end": "988480"
  },
  {
    "text": "for the most most popular programming languages and expert support from the ogs of software Define Telecom tech try",
    "start": "988480",
    "end": "995639"
  },
  {
    "text": "it today at signal wire.com and use code AI for $25 in developer credit just",
    "start": "995639",
    "end": "1001839"
  },
  {
    "text": "visit signal wire.com that's signal wire.com and use code AI to receive that",
    "start": "1001839",
    "end": "1007440"
  },
  {
    "text": "25 bucks once again that's signal wire.com code",
    "start": "1007440",
    "end": "1013680"
  },
  {
    "text": "AI [Music]",
    "start": "1017839",
    "end": "1023200"
  },
  {
    "text": "so Jonathan I I want to maybe get into some of those design aspects that you were talking of and I find it",
    "start": "1024000",
    "end": "1030720"
  },
  {
    "text": "fascinating that you're sort of diving into this area of thinking about what sorts of layers run best on what sorts",
    "start": "1030720",
    "end": "1038600"
  },
  {
    "text": "of Hardware has that research around that topic has that been going on for",
    "start": "1038600",
    "end": "1043839"
  },
  {
    "text": "some time and you were able to sort of build on that I'm curious cuz Hardware",
    "start": "1043839",
    "end": "1048919"
  },
  {
    "text": "is progressing so rapidly even in the terms of accelerators like gpus and vpus",
    "start": "1048919",
    "end": "1054960"
  },
  {
    "text": "and tpus and all sorts of use all of that's advancing very rapidly so sounds",
    "start": "1054960",
    "end": "1061240"
  },
  {
    "text": "like you have some intuition around what runs best on what Hardware but is that rules that you're encoding into your",
    "start": "1061240",
    "end": "1068039"
  },
  {
    "text": "methods in order to optimize models for certain Hardware or what's going on",
    "start": "1068039",
    "end": "1073559"
  },
  {
    "text": "there I know you've got this sort of architecture search concept yeah there are many types of hardware and we can",
    "start": "1073559",
    "end": "1078960"
  },
  {
    "text": "not have rules for every type of Hardware designing neural architecture is very complicated because it's kind of",
    "start": "1078960",
    "end": "1085039"
  },
  {
    "text": "a composition of multiple layers with multiple types and sizes so searching among neural architecture is a large SE",
    "start": "1085039",
    "end": "1092760"
  },
  {
    "text": "space with a very complicated search so we can't have kind of rule of time for this Hardware use that operation and for",
    "start": "1092760",
    "end": "1099799"
  },
  {
    "text": "this Hardware use this operation what we are doing we are employing new architecture search that is Hardware",
    "start": "1099799",
    "end": "1105960"
  },
  {
    "text": "aware in the sense that it's connected to the hardware and optimizes the structure of the network to both have",
    "start": "1105960",
    "end": "1113440"
  },
  {
    "text": "better accuracy on the data and better latency on the data on the hardware and this is something that we must connect",
    "start": "1113440",
    "end": "1120120"
  },
  {
    "text": "to the specific Hardware in order to get that because you can't really model the hardware and understand how a specific",
    "start": "1120120",
    "end": "1126120"
  },
  {
    "text": "neural network will behave on that Hardware so you can think about it as a Hardware in the loop neural architecture",
    "start": "1126120",
    "end": "1131919"
  },
  {
    "text": "search where we have an automatic search algorithm that search among hundred of",
    "start": "1131919",
    "end": "1137120"
  },
  {
    "text": "thousands of neural architecture and find the best one that kind of sits",
    "start": "1137120",
    "end": "1143240"
  },
  {
    "text": "on The Sweet Spot between the accuracy and the latency on the given Hardware",
    "start": "1143240",
    "end": "1148440"
  },
  {
    "text": "while searching on that space is very complicated because the latency can be measured or estimated by using the",
    "start": "1148440",
    "end": "1155280"
  },
  {
    "text": "hardware but in order to understand the accuracy of a given architecture you must connect the data and train it",
    "start": "1155280",
    "end": "1161080"
  },
  {
    "text": "candidate architectures could be very expensive so the trick here or the secret Source here is how can we do it",
    "start": "1161080",
    "end": "1167880"
  },
  {
    "text": "efficiently how can we scan hundred thousands of neural architectures and find the best one for your operational",
    "start": "1167880",
    "end": "1174320"
  },
  {
    "text": "point in terms of latency accuracy tradeoff and this is kind of what we do",
    "start": "1174320",
    "end": "1179799"
  },
  {
    "text": "is our proprietary in neural architecture search algorithm that is called OAC automatic neural architecture",
    "start": "1179799",
    "end": "1186400"
  },
  {
    "text": "construction could you take us through maybe kind of an example just to make it very tangible for the practitioners",
    "start": "1186400",
    "end": "1192240"
  },
  {
    "text": "they're listening to this in terms of how the practitioner is going to use neural architecture search to do that",
    "start": "1192240",
    "end": "1197919"
  },
  {
    "text": "maybe ideally with an edge use case just to give a sense of how you're approaching it and what that feels like",
    "start": "1197919",
    "end": "1203760"
  },
  {
    "text": "if you're targeting inference on the edge ultimately yeah so we have a collaboration with Intel that we",
    "start": "1203760",
    "end": "1209880"
  },
  {
    "text": "recently published in MLP last September or something like that about a performance boost that we have made to",
    "start": "1209880",
    "end": "1217240"
  },
  {
    "text": "image classification model resident 50 for a Macbook laptop for example we've done that for several types of Hardware",
    "start": "1217240",
    "end": "1224200"
  },
  {
    "text": "but uh let's take for example The Edge use case with the MacBook Pro so we're having the Baseline model we have three",
    "start": "1224200",
    "end": "1231159"
  },
  {
    "text": "inputs to the algorithm of the neural architecture search we're having the Baseline model which is for in this",
    "start": "1231159",
    "end": "1236919"
  },
  {
    "text": "example was resident 50 we're having the hardware itself and the data the data was image n in the MLP Benchmark and the",
    "start": "1236919",
    "end": "1245159"
  },
  {
    "text": "alac algorithm kind of searching what types of changes or how can it change",
    "start": "1245159",
    "end": "1252080"
  },
  {
    "text": "the original architecture of reset 50 in order to get a better architecture that",
    "start": "1252080",
    "end": "1257919"
  },
  {
    "text": "preserves the accuracy of 76% of accuracy for example and minimize the",
    "start": "1257919",
    "end": "1263919"
  },
  {
    "text": "latency or maximize the triput and it is very interesting to see that first of",
    "start": "1263919",
    "end": "1270000"
  },
  {
    "text": "all it replaces some of the operations from dance convolution layers to",
    "start": "1270000",
    "end": "1275039"
  },
  {
    "text": "depthwise and some other variants of convolutional layers that are let's say less memory bounded because the cach",
    "start": "1275039",
    "end": "1282039"
  },
  {
    "text": "memory also limit the inference speed of the network second thing that we can learn from that idea is is that some",
    "start": "1282039",
    "end": "1290480"
  },
  {
    "text": "layers are more important than others so for example if you think about what size",
    "start": "1290480",
    "end": "1295840"
  },
  {
    "text": "to have on each layer in the network we have some understanding that the initial",
    "start": "1295840",
    "end": "1301080"
  },
  {
    "text": "layers that are doing the feature extraction in the in the image classification example are more important than the later layers in the",
    "start": "1301080",
    "end": "1308520"
  },
  {
    "text": "network so putting kind of most of the computation at the beginning of the network seems to have better accuracy",
    "start": "1308520",
    "end": "1316400"
  },
  {
    "text": "preserving properties this is kind of what we see when we observe on the results of that run of uton on this",
    "start": "1316400",
    "end": "1324200"
  },
  {
    "text": "example and we end up with a model that is faster in something like 3x compared",
    "start": "1324200",
    "end": "1330200"
  },
  {
    "text": "to the Baseline and having the same accuracy so this is kind of how we take",
    "start": "1330200",
    "end": "1335400"
  },
  {
    "text": "an algorithm that is fully automatic and try to do a postmortem to understand",
    "start": "1335400",
    "end": "1340640"
  },
  {
    "text": "what happened there why the output of these algorithms look like that why the initial layers haven't changed or almost",
    "start": "1340640",
    "end": "1347919"
  },
  {
    "text": "haven't changed but the later layer have changed significantly replaced to other types of",
    "start": "1347919",
    "end": "1353440"
  },
  {
    "text": "layers their size was smaller and some other changes that we observe on the",
    "start": "1353440",
    "end": "1358720"
  },
  {
    "text": "result of the algorithm so I want to actually get back to that later because it's really fascinating that you can",
    "start": "1358720",
    "end": "1364159"
  },
  {
    "text": "sort of use these tools to learn more about the types of things that work well",
    "start": "1364159",
    "end": "1370480"
  },
  {
    "text": "architecture design wise on certain Hardware that's really interesting but first just to sort of bring that example",
    "start": "1370480",
    "end": "1377039"
  },
  {
    "text": "into Focus it's sounds like you had this base resnet model I'm just thinking about the sort of inputs outputs of the",
    "start": "1377039",
    "end": "1384559"
  },
  {
    "text": "automatic Network architecture search like if I'm a practitioner and maybe I",
    "start": "1384559",
    "end": "1389880"
  },
  {
    "text": "have my own custom model for object detection or a custom model for speech",
    "start": "1389880",
    "end": "1395279"
  },
  {
    "text": "recognition or whatever it is and I've trained that then in terms of doing this automatic neural architecture search I'm",
    "start": "1395279",
    "end": "1402799"
  },
  {
    "text": "assuming one input to that is the sort of serialized version of my model and",
    "start": "1402799",
    "end": "1408120"
  },
  {
    "text": "whatever format it's in you also mentioned that a data set was input to that maybe you could give some details",
    "start": "1408120",
    "end": "1414840"
  },
  {
    "text": "on that so why is the data set input is that so that you can make sure that you're not optimizing just for the",
    "start": "1414840",
    "end": "1421520"
  },
  {
    "text": "hardware but you're also optimizing to make sure that performance doesn't degrade in terms of prediction",
    "start": "1421520",
    "end": "1427640"
  },
  {
    "text": "performance and then also do I need to have access to like the specific Hardware that I'm targeting so I need",
    "start": "1427640",
    "end": "1435120"
  },
  {
    "text": "like this data set and then do I need like the specific Hardware that I'm targeting and run the automated neural",
    "start": "1435120",
    "end": "1441600"
  },
  {
    "text": "architecture search on on that Hardware or how does that work yeah so you got it right let's formulate the equation that",
    "start": "1441600",
    "end": "1448440"
  },
  {
    "text": "the neural architecture search is trying to solve we are talking about minimize",
    "start": "1448440",
    "end": "1453720"
  },
  {
    "text": "the latency of the model on a given Hardware subject to getting an accuracy",
    "start": "1453720",
    "end": "1459640"
  },
  {
    "text": "that is above the given threshold so the latency of the architecture or the model",
    "start": "1459640",
    "end": "1465559"
  },
  {
    "text": "can be measured without training in most of the cases say on the hardware so we don't need the data to understand what",
    "start": "1465559",
    "end": "1471600"
  },
  {
    "text": "is going to be the latency of Resident 50 on a CPU of Intel but the accuracy is",
    "start": "1471600",
    "end": "1476679"
  },
  {
    "text": "data dependent and if we want to put that constraint and obviously we want to put that we need to have the data and",
    "start": "1476679",
    "end": "1484360"
  },
  {
    "text": "verify that the model that is selected by the minimization problem of the latency still meet the accuracy",
    "start": "1484360",
    "end": "1490880"
  },
  {
    "text": "requirements because if we not put the accuracy constraint we'll end up with a model with one neon that predicting",
    "start": "1490880",
    "end": "1498000"
  },
  {
    "text": "nothing so this is kind of the composition between the latency that is measured on the hardware and the",
    "start": "1498000",
    "end": "1503159"
  },
  {
    "text": "accuracy that is measured on the data could you talk a little bit more about kind of the output path on that in the",
    "start": "1503159",
    "end": "1508279"
  },
  {
    "text": "sense of if let's say that you have a model that's doing object detection or",
    "start": "1508279",
    "end": "1514360"
  },
  {
    "text": "it could be anything really and you want to put it on maybe a platform that's on",
    "start": "1514360",
    "end": "1519640"
  },
  {
    "text": "the edge that is has a bunch of sensors maybe a bunch of cameras pulling in when you're going through the training",
    "start": "1519640",
    "end": "1525720"
  },
  {
    "text": "process how are you accounting for kind of that variability out there you know kind of going back to if you're not",
    "start": "1525720",
    "end": "1532159"
  },
  {
    "text": "targeting the hardware in the architecture search that you're going to deploy to how do you all account for",
    "start": "1532159",
    "end": "1539000"
  },
  {
    "text": "that how do you say ah there's a very unique configuration for my output",
    "start": "1539000",
    "end": "1544559"
  },
  {
    "text": "Target my deployment Target how do you approach that are you asking what happens when we don't know the Hardway",
    "start": "1544559",
    "end": "1550240"
  },
  {
    "text": "in production well yeah I guess just like if you're targeting a particular environment that may be customized",
    "start": "1550240",
    "end": "1556240"
  },
  {
    "text": "fairly significantly for deployment as more and more practitioners are now kind of getting out of the data center and",
    "start": "1556240",
    "end": "1562440"
  },
  {
    "text": "they are putting things on drones and they're putting things into Automotive being a big one obviously anything like",
    "start": "1562440",
    "end": "1568440"
  },
  {
    "text": "that that's out on the edge and kind of has a custom environment what does that look like from a practitioner",
    "start": "1568440",
    "end": "1573919"
  },
  {
    "text": "perspective kind of out of the theory and into the Hands-On so we prefer to",
    "start": "1573919",
    "end": "1578960"
  },
  {
    "text": "connect to the actual Hardware that the model is going to be deployed on so for example if it's Jetson we connect the",
    "start": "1578960",
    "end": "1585600"
  },
  {
    "text": "exact Jetson model to the newer architecture cell if we don't know that we need to have",
    "start": "1585600",
    "end": "1591000"
  },
  {
    "text": "some proxies so a good proxy might be the number of floating Point operation",
    "start": "1591000",
    "end": "1596120"
  },
  {
    "text": "but we know that this is not such a good proxy and some other methods like",
    "start": "1596120",
    "end": "1601240"
  },
  {
    "text": "pruning Target this metric but this metric correlates to latency only on CPUs so having proxies there it's not",
    "start": "1601240",
    "end": "1609480"
  },
  {
    "text": "the right approach in our perspective measuring the metric that really matters on the actual device is the way to go in",
    "start": "1609480",
    "end": "1617279"
  },
  {
    "text": "my perspective like measuring the actual latency measuring the actual throughput",
    "start": "1617279",
    "end": "1622600"
  },
  {
    "text": "on the device is the way to go in order to understand the exact performance that you going to see in production because I",
    "start": "1622600",
    "end": "1629039"
  },
  {
    "text": "can show examples where I cut the flops in factor of2 and the latency is getting",
    "start": "1629039",
    "end": "1634799"
  },
  {
    "text": "slower on GPU on Jetson on some types of devices so these boxes are very",
    "start": "1634799",
    "end": "1640279"
  },
  {
    "text": "problematic today and I think that this is kind of the interesting part of doing Hardware where newal architecture search",
    "start": "1640279",
    "end": "1647559"
  },
  {
    "text": "compared to to other compression techniques like I mentioned as pruning that reduces the number of flops or the",
    "start": "1647559",
    "end": "1654039"
  },
  {
    "text": "number of parameters or any proxy for the size or the complexity of the network so janathan as you're working in",
    "start": "1654039",
    "end": "1661919"
  },
  {
    "text": "this space one of the big things that I always start thinking about when I think of optimizing networks in this way is",
    "start": "1661919",
    "end": "1669279"
  },
  {
    "text": "that there's just so many different types of layers and custom layers that people are using and new stuff coming",
    "start": "1669279",
    "end": "1676600"
  },
  {
    "text": "out all the time so what has it been like sort of maintaining your search",
    "start": "1676600",
    "end": "1682840"
  },
  {
    "text": "space over time and growing that space to sort of include new things as they're",
    "start": "1682840",
    "end": "1688480"
  },
  {
    "text": "coming out how do you approach that as an organization and figure out how to expand that search space and what to",
    "start": "1688480",
    "end": "1694600"
  },
  {
    "text": "include and what not to include so that's a good question because the research field of deep learning is",
    "start": "1694600",
    "end": "1700320"
  },
  {
    "text": "progressing very fast and we have a team of researchers that sitting on the",
    "start": "1700320",
    "end": "1706880"
  },
  {
    "text": "latest academic papers that propose all those new layers and those new operators",
    "start": "1706880",
    "end": "1713279"
  },
  {
    "text": "and kind of reproduce all these models to understand which type of layers which",
    "start": "1713279",
    "end": "1718840"
  },
  {
    "text": "types of techniques are worth adding to the search space of the neural architecture search and which are not",
    "start": "1718840",
    "end": "1725679"
  },
  {
    "text": "and actually the result is very interesting in most cases for example if",
    "start": "1725679",
    "end": "1730840"
  },
  {
    "text": "we'll take the computer vision domain the basic operators that are well known",
    "start": "1730840",
    "end": "1736120"
  },
  {
    "text": "for the last five years or something like that are performing the best so some of the tricks that we see now are",
    "start": "1736120",
    "end": "1742919"
  },
  {
    "text": "not improving so much compared to using those blocks and operators that build",
    "start": "1742919",
    "end": "1749080"
  },
  {
    "text": "res net and mobile net and efficient net and those networks so having the right",
    "start": "1749080",
    "end": "1754279"
  },
  {
    "text": "composition of operators is more crucial that having all those fancy tricks that",
    "start": "1754279",
    "end": "1759840"
  },
  {
    "text": "showed up in in the last two or three years this is something that we see it's quite General claim there are some cases",
    "start": "1759840",
    "end": "1767360"
  },
  {
    "text": "that we see that wor adding to the SE space but in general I would say that",
    "start": "1767360",
    "end": "1772799"
  },
  {
    "text": "it's not so easy to beat a resonant model that is quantized and use the",
    "start": "1772799",
    "end": "1778440"
  },
  {
    "text": "graph compiler like 10 or something like that and you need to work hard in order to build it so this is kind of how we",
    "start": "1778440",
    "end": "1785919"
  },
  {
    "text": "see all the advancement of course we have other advancement in other fields like training tricks optimizers and",
    "start": "1785919",
    "end": "1793360"
  },
  {
    "text": "stuff like this and we have to be on the front line on all of those and this is something that that we are working",
    "start": "1793360",
    "end": "1798720"
  },
  {
    "text": "really hard in order to reproduce all state-of-the-art all the types of model all those new models that just announc",
    "start": "1798720",
    "end": "1806440"
  },
  {
    "text": "and having their results and their operators in our self space and a followup on that I guess which is",
    "start": "1806440",
    "end": "1812519"
  },
  {
    "text": "related to that approach is on the one side you have all of these different types of architectures on the other side",
    "start": "1812519",
    "end": "1818000"
  },
  {
    "text": "you have all of these different tasks being solved with deep learning models",
    "start": "1818000",
    "end": "1823799"
  },
  {
    "text": "and and AI models so I'm curious as you've experienced this over a number of",
    "start": "1823799",
    "end": "1829120"
  },
  {
    "text": "years now is it harder to optimize the inference of certain tasks versus other",
    "start": "1829120",
    "end": "1836640"
  },
  {
    "text": "tasks so maybe like NLP tasks versus computer vision tasks versus audio tasks",
    "start": "1836640",
    "end": "1842679"
  },
  {
    "text": "versus you know maybe it's time series modeling or something are there certain",
    "start": "1842679",
    "end": "1848039"
  },
  {
    "text": "domains of AI tasks that are harder to optimize than others so at the moment we",
    "start": "1848039",
    "end": "1853919"
  },
  {
    "text": "are mostly focused on computer vision and NLP and in those domains we see that",
    "start": "1853919",
    "end": "1858960"
  },
  {
    "text": "the principles that we are using that are machine learning based are working across the board yes I can tell that",
    "start": "1858960",
    "end": "1865519"
  },
  {
    "text": "there are some tasks in a domain that are a little bit more complex for example semantic segmentation networks",
    "start": "1865519",
    "end": "1871799"
  },
  {
    "text": "are more complex than classification networks and they have to preserve the",
    "start": "1871799",
    "end": "1877320"
  },
  {
    "text": "information along the network in order to do kind of imageo image tasks but",
    "start": "1877320",
    "end": "1882639"
  },
  {
    "text": "also on those type of newal architectures we can optimize them and the principle is very simple most of the",
    "start": "1882639",
    "end": "1889519"
  },
  {
    "text": "networks and the new fancy algorithms are kind of built on top of three components one of is the stem is the few",
    "start": "1889519",
    "end": "1896960"
  },
  {
    "text": "layers connected to the input the second component is the backbone and the third",
    "start": "1896960",
    "end": "1902120"
  },
  {
    "text": "component is the prediction Block in most cases 80 to 90% of the compute is",
    "start": "1902120",
    "end": "1908000"
  },
  {
    "text": "happening in the backbone and usually the backbone are just a bunch of convolutional layers in the case of",
    "start": "1908000",
    "end": "1913960"
  },
  {
    "text": "computer vision and by optimizing that significant part of the network which is",
    "start": "1913960",
    "end": "1920000"
  },
  {
    "text": "similar across classification semantic segmentation and object detection you can get that boost of performance that",
    "start": "1920000",
    "end": "1926360"
  },
  {
    "text": "we're looking to have in all tasks as we're talking about this I'm just visualizing everything in my head as you",
    "start": "1926360",
    "end": "1933320"
  },
  {
    "text": "get to the output and you've targeted a particular deployment Target and accounted for the hardware and what the",
    "start": "1933320",
    "end": "1939760"
  },
  {
    "text": "capabilities are I'm curious how does your platform integrate in with whatever devops pipeline a practitioner might",
    "start": "1939760",
    "end": "1946159"
  },
  {
    "text": "have put into place what is a typical scenario for actually pushing the deployment out to the hardware that it's",
    "start": "1946159",
    "end": "1952000"
  },
  {
    "text": "going to run on for inference look like so we look at our platform as an end to end platform from development to",
    "start": "1952000",
    "end": "1957720"
  },
  {
    "text": "production we develop two production tools one of them called infer and and the second is called RTI infer is",
    "start": "1957720",
    "end": "1965080"
  },
  {
    "text": "lightweight Edge inference Engine That Could Be integrated to a monolite",
    "start": "1965080",
    "end": "1970320"
  },
  {
    "text": "application easily and RAC is a containerized inference server so if",
    "start": "1970320",
    "end": "1976639"
  },
  {
    "text": "you'll take that solution it could be easily deployed by a devops with the model inside fetch from the model",
    "start": "1976639",
    "end": "1983840"
  },
  {
    "text": "repository that we provide as part of our s sofware ring and kind of ser the model in a standardized API that",
    "start": "1983840",
    "end": "1991240"
  },
  {
    "text": "contains all the packages libraries environment details that you need in order to go over kubernetes for",
    "start": "1991240",
    "end": "1997679"
  },
  {
    "text": "inference and scale sometimes we see companies that already have their own infrastructure",
    "start": "1997679",
    "end": "2003399"
  },
  {
    "text": "and don't want to change their existing infrastructure and we provide them with this specific model just the exact model",
    "start": "2003399",
    "end": "2009799"
  },
  {
    "text": "that ran the optimization the model after the optimized model in their format we support all the types of",
    "start": "2009799",
    "end": "2016360"
  },
  {
    "text": "format from Onix to tens oflow pyou caras and all of these Frameworks that",
    "start": "2016360",
    "end": "2021799"
  },
  {
    "text": "you can run inference on so this is kind of the two ways to get Desi optimized",
    "start": "2021799",
    "end": "2026880"
  },
  {
    "text": "model to a production environment either by our deployment tools or getting the model and using your existing stack I'm",
    "start": "2026880",
    "end": "2034000"
  },
  {
    "text": "just kind of browsing around on some of the information about Desi and super fascinating one of the things that I see",
    "start": "2034000",
    "end": "2040360"
  },
  {
    "text": "is this idea of desin Nets which you share about and share some of the sort",
    "start": "2040360",
    "end": "2045559"
  },
  {
    "text": "of successes that you've had taking this approach in various domains for various",
    "start": "2045559",
    "end": "2052079"
  },
  {
    "text": "types of models could you just share a few success stories in terms of what",
    "start": "2052079",
    "end": "2057240"
  },
  {
    "text": "you've been able to achieve performance- wise with this approach yeah so deset is",
    "start": "2057240",
    "end": "2062560"
  },
  {
    "text": "a good example for that we took a few well-known tasks like image",
    "start": "2062560",
    "end": "2067760"
  },
  {
    "text": "classification object detection and semantic segmentation and we've taken the most",
    "start": "2067760",
    "end": "2073398"
  },
  {
    "text": "famous open source data set for example imet cocko and data sets like that and",
    "start": "2073399",
    "end": "2079079"
  },
  {
    "text": "we built kind of a catalog of pre-optimized model for each and every hardware and what we are doing we are",
    "start": "2079079",
    "end": "2086280"
  },
  {
    "text": "kind of plotting what we call an efficient Frontier chart with the latency on the xaxis and the accuracy on",
    "start": "2086280",
    "end": "2093839"
  },
  {
    "text": "the y- axis and plotting all the models that we know the reset the YOLO and",
    "start": "2093839",
    "end": "2099880"
  },
  {
    "text": "those models on those charts and putting those Des Nets or desd dats for detection and kind of seeing the what we",
    "start": "2099880",
    "end": "2107079"
  },
  {
    "text": "call the efficient Frontier how those models reach better accuracy and better latency and dominate this tradeoff",
    "start": "2107079",
    "end": "2114560"
  },
  {
    "text": "between accuracy and latency and now we provide those deset for data scientist",
    "start": "2114560",
    "end": "2120280"
  },
  {
    "text": "in order to try fit them to their specific data fine-tune them for their specific data and having kind of a pre-",
    "start": "2120280",
    "end": "2127320"
  },
  {
    "text": "optim results of Aon that is ready for use immediately and I'm wondering as you",
    "start": "2127320",
    "end": "2133200"
  },
  {
    "text": "plot out this landscape I love how you term that like the efficiency landscape it's a really cool way to think about",
    "start": "2133200",
    "end": "2140240"
  },
  {
    "text": "space as you plot this out and explore that space yourself I'm wondering one",
    "start": "2140240",
    "end": "2145960"
  },
  {
    "text": "way to think about what you're doing and how you've expressed it is I'm a data scientist I've trained my model now I",
    "start": "2145960",
    "end": "2151920"
  },
  {
    "text": "run it through Auto neural architecture search and get out my you know better model faster and more efficient for the",
    "start": "2151920",
    "end": "2159240"
  },
  {
    "text": "architecture while still performing well for prediction but I'm wondering if this sort of cycle as you do that more and",
    "start": "2159240",
    "end": "2165960"
  },
  {
    "text": "more you start sort of building some intuition as a data scientist or a practitioner to like start with a better",
    "start": "2165960",
    "end": "2173160"
  },
  {
    "text": "model in the first place like if I look at your plots of the efficiency landscape can I learn some things about",
    "start": "2173160",
    "end": "2180880"
  },
  {
    "text": "maybe maybe I start with better models in in the first place rather than sort of relying as much on the neural",
    "start": "2180880",
    "end": "2188760"
  },
  {
    "text": "architecture search do you think there's that sort of feedback and that learning that can happen from like what architectures are learned by the",
    "start": "2188760",
    "end": "2195800"
  },
  {
    "text": "automatic neural architecture search yeah absolutely we share some information intuition behind those",
    "start": "2195800",
    "end": "2203000"
  },
  {
    "text": "architectures that found for a given hardware for example one of the things that we see that those architectures are",
    "start": "2203000",
    "end": "2210000"
  },
  {
    "text": "faster but having more parameters for example so this is kind of an intuition that we see around these models that can",
    "start": "2210000",
    "end": "2217640"
  },
  {
    "text": "be used to understand that we not always look to smaller models but for more",
    "start": "2217640",
    "end": "2223359"
  },
  {
    "text": "faster model and accurate model we provide those models as a starting point so for example if you're considering",
    "start": "2223359",
    "end": "2229480"
  },
  {
    "text": "taking resonant 54 ride or efficient and b0 you can take the corresponding",
    "start": "2229480",
    "end": "2235200"
  },
  {
    "text": "destinate model and start from that and trick that for your application whether you're doing object detection",
    "start": "2235200",
    "end": "2242040"
  },
  {
    "text": "classification or or anything like that and this is kind of giving the ability",
    "start": "2242040",
    "end": "2247079"
  },
  {
    "text": "to use use a nce produced model compared to having a model that is off the shelf",
    "start": "2247079",
    "end": "2252760"
  },
  {
    "text": "for General use so for example efficient Nets is a result by Google from two",
    "start": "2252760",
    "end": "2258160"
  },
  {
    "text": "years ago or something like that that are supposed to be efficient but it appears to that efficient that are not",
    "start": "2258160",
    "end": "2264400"
  },
  {
    "text": "so efficient for gpus even when they are over quantization and graph compiler",
    "start": "2264400",
    "end": "2270240"
  },
  {
    "text": "like Tor T so having a pre-optimized model for the given Hardware that you're",
    "start": "2270240",
    "end": "2276240"
  },
  {
    "text": "going to use is very crucial and in our example of deset we show that you can have a model that is something",
    "start": "2276240",
    "end": "2284280"
  },
  {
    "text": "like three times faster than efficient and b z for a Jetson GPU while having",
    "start": "2284280",
    "end": "2290280"
  },
  {
    "text": "even better accuracy so this is kind of a results that you can take off the",
    "start": "2290280",
    "end": "2295440"
  },
  {
    "text": "shelf instead of using efficient and b z you can take that model and train it to",
    "start": "2295440",
    "end": "2300680"
  },
  {
    "text": "your application build on top of that some other prediction heads some other",
    "start": "2300680",
    "end": "2306160"
  },
  {
    "text": "tasks that you want to solve with that backbone and get a Aon result without",
    "start": "2306160",
    "end": "2311680"
  },
  {
    "text": "running all the neural architecture search for that specific desk that's",
    "start": "2311680",
    "end": "2316760"
  },
  {
    "text": "very cool this is so interesting as you were talking about model optimization and the things that desie has done with",
    "start": "2316760",
    "end": "2323200"
  },
  {
    "text": "it what are you envisioning as you guys are looking into the future at this point what kinds of things are",
    "start": "2323200",
    "end": "2329000"
  },
  {
    "text": "aspirational for desie in terms of where you want to take the platform and what you envision will be the next thing in",
    "start": "2329000",
    "end": "2336440"
  },
  {
    "text": "model optimization that you'd like to implement I think that we feel at a good point in the model optimization space",
    "start": "2336440",
    "end": "2343319"
  },
  {
    "text": "that now we are seeing that we need to expand to the wall development life",
    "start": "2343319",
    "end": "2348920"
  },
  {
    "text": "cycle so after you have the data we we look on controlling all the training",
    "start": "2348920",
    "end": "2354079"
  },
  {
    "text": "optimization and deployment of theep learning model on our platform whether you're using those deset or using some",
    "start": "2354079",
    "end": "2361599"
  },
  {
    "text": "of the Shelf models and kind of having a full workflow of development ization and",
    "start": "2361599",
    "end": "2368079"
  },
  {
    "text": "deployment based on our platform that's because we understand that there's kind",
    "start": "2368079",
    "end": "2373359"
  },
  {
    "text": "of a triangle that we can draw that is on one Edge we have the model on the",
    "start": "2373359",
    "end": "2378480"
  },
  {
    "text": "other one we have the data and on the last one we have the hardware and this is kind of a combined optimization that",
    "start": "2378480",
    "end": "2385560"
  },
  {
    "text": "every data scientist need to understand how they solve that and we are kind of providing the tools to solve this",
    "start": "2385560",
    "end": "2392560"
  },
  {
    "text": "optimize this triangle and for now we are mostly focused on the model side side but in the future we will be also",
    "start": "2392560",
    "end": "2399920"
  },
  {
    "text": "focused on the data and the hardware side in terms of not having them fixed but having some techniques for data",
    "start": "2399920",
    "end": "2407280"
  },
  {
    "text": "enrichment data augmentation self-supervised learning having some",
    "start": "2407280",
    "end": "2412440"
  },
  {
    "text": "Hardware recommendation system maybe having some fpga capabilities and having",
    "start": "2412440",
    "end": "2418079"
  },
  {
    "text": "Hardware Hardware that is optimized for the given model and this is kind of the far future about how I see optimization",
    "start": "2418079",
    "end": "2425839"
  },
  {
    "text": "in its full that's awesome awesome well I'm really excited that we got to talk through this because I know I learned a",
    "start": "2425839",
    "end": "2432839"
  },
  {
    "text": "lot about this neural architecture search and the things that you're doing so really impressed with with where",
    "start": "2432839",
    "end": "2438680"
  },
  {
    "text": "you're headed with this and appreciate you taking time to join us and chat with us about it sure thank you very much it",
    "start": "2438680",
    "end": "2444920"
  },
  {
    "text": "was great talking to you and I look forward to he the",
    "start": "2444920",
    "end": "2450400"
  },
  {
    "text": "episodes thank you for listening to practical AI we have a bundle of awesome",
    "start": "2451160",
    "end": "2456680"
  },
  {
    "text": "podcasts for you at Chang log.com including our brand new show ship it with Gard Lazo a podcast about getting",
    "start": "2456680",
    "end": "2463720"
  },
  {
    "text": "your best ideas into the world and seeing what happens it's about the code the Ops the infra and the people that",
    "start": "2463720",
    "end": "2470480"
  },
  {
    "text": "make it happen yes we focus on the people because everything else is an implementation detail subscribe now at",
    "start": "2470480",
    "end": "2478280"
  },
  {
    "text": "changelog.txt all change Log podcasts including",
    "start": "2481160",
    "end": "2487400"
  },
  {
    "text": "practical Ai and ship it in one place search changelog Master feed or head to",
    "start": "2487400",
    "end": "2492560"
  },
  {
    "text": "changel log.com slm And subscribe today practical AI is hosted by Daniel whack",
    "start": "2492560",
    "end": "2497960"
  },
  {
    "text": "and Chris Benson with music by breakmaster cylinder we're brought to you by fley laun darkley and Leno that's",
    "start": "2497960",
    "end": "2503680"
  },
  {
    "text": "all for now we'll talk to you again next [Music]",
    "start": "2503680",
    "end": "2513760"
  },
  {
    "text": "week [Music]",
    "start": "2516240",
    "end": "2532739"
  },
  {
    "text": "King l",
    "start": "2533480",
    "end": "2536680"
  }
]