[
  {
    "text": "one thing that we think is really interesting is that unlike generic model hubs like tensorflow's Hub or pie",
    "start": "120",
    "end": "5640"
  },
  {
    "text": "torches Hub because our models are all of the same form we can build a lot of tools and Machinery around using them so",
    "start": "5640",
    "end": "12599"
  },
  {
    "text": "for instance we have a visualizer that works for all of our models you can just upload your own model and get really",
    "start": "12599",
    "end": "18199"
  },
  {
    "text": "interesting visualization of its internal structure or this open source project called text attack built a",
    "start": "18199",
    "end": "24240"
  },
  {
    "text": "adversarial attack system and it's able to generically build attacks to any of our models in our so because they all",
    "start": "24240",
    "end": "30920"
  },
  {
    "text": "have the same interface it allows people to do these really longitudinal research projects across everything that's going",
    "start": "30920",
    "end": "36079"
  },
  {
    "text": "on in the hub itself and then I should mention that now we have a kind of an inference API on any of the pages you",
    "start": "36079",
    "end": "42680"
  },
  {
    "text": "can just type in some text and it will run against that model and you can even call that from your own code directly",
    "start": "42680",
    "end": "48920"
  },
  {
    "text": "without ever running anything on your machine just run it on one of these servers and we even have a Twitter bot",
    "start": "48920",
    "end": "54160"
  },
  {
    "text": "that we just put up last week where you can tweet at it and it will run a model against your Tweet",
    "start": "54160",
    "end": "61239"
  },
  {
    "text": "BWI for change log is provided by fastly learn more at fastly.com we move fast",
    "start": "61239",
    "end": "67080"
  },
  {
    "text": "and fix things here at changelog because of rbar check them out at rar.com and we're hosted on linode cloud servers",
    "start": "67080",
    "end": "73840"
  },
  {
    "text": "head to lin.com changelog this episode is brought to you",
    "start": "73840",
    "end": "80280"
  },
  {
    "text": "by digital ocean droplets manage kubernetes manage databases spaces",
    "start": "80280",
    "end": "86079"
  },
  {
    "text": "object storage volume block storage Advanced networking like virtual private clouds and Cloud firewalls developer",
    "start": "86079",
    "end": "92680"
  },
  {
    "text": "tooling like the robust API and CLI to make sure you can interact with your infrastructure the way you want to",
    "start": "92680",
    "end": "98920"
  },
  {
    "text": "digital ocean is designed for developers and built for businesses join Over",
    "start": "98920",
    "end": "104399"
  },
  {
    "text": "150,000 businesses that develop manage and scale their applications with digital ocean head to do. co/ change law",
    "start": "104399",
    "end": "111920"
  },
  {
    "text": "to get started with a $100 credit again do. [Music]",
    "start": "111920",
    "end": "118840"
  },
  {
    "text": "changelog [Music]",
    "start": "118840",
    "end": "125489"
  },
  {
    "text": "welcome to practical AI a weekly podcast that makes artificial intelligence practical productive and accessible to",
    "start": "126439",
    "end": "132959"
  },
  {
    "text": "everyone this is where conversations around AI machine learning and data science happen join the community and",
    "start": "132959",
    "end": "138959"
  },
  {
    "text": "slack with us around various topics of the show at chain.com commmunity and follow us on Twitter we're at practical",
    "start": "138959",
    "end": "147120"
  },
  {
    "text": "aifm well welcome to another episode of",
    "start": "148840",
    "end": "154560"
  },
  {
    "text": "practical AI this is Daniel whack I am a data scientist with s International and",
    "start": "154560",
    "end": "161680"
  },
  {
    "text": "I'm joined as always by my co-host Chris Benson who is a principal AI strategist",
    "start": "161680",
    "end": "167319"
  },
  {
    "text": "at Lockheed Martin how are you doing Chris I'm hanging in there uh how you doing Daniel doing pretty good as we",
    "start": "167319",
    "end": "173959"
  },
  {
    "text": "talked about the last couple weeks I've been um ordering parts for a AI workstation computer",
    "start": "173959",
    "end": "180360"
  },
  {
    "text": "and um it's sitting next to me and it's running oh nice I am successfully uh or",
    "start": "180360",
    "end": "186760"
  },
  {
    "text": "at least it appears that I'm successfully overfitting a model on the GPU so I'll have to deal with that you",
    "start": "186760",
    "end": "193680"
  },
  {
    "text": "know after recording but it's running and it's not overheating yet it's kind of stable at a I think reasonable",
    "start": "193680",
    "end": "200840"
  },
  {
    "text": "temperature so I'm happy on that front so it's funny cuz you know as we're on this call we're on zoom and in the video",
    "start": "200840",
    "end": "208439"
  },
  {
    "text": "you have the data center and the background so I just find it funny yeah a bunch of dgx Nidia machines or",
    "start": "208439",
    "end": "214680"
  },
  {
    "text": "whatever yeah but that's not what you're using the workstation mine as much smaller although it's bigger than I",
    "start": "214680",
    "end": "219920"
  },
  {
    "text": "thought because after I put the GPU in the case would not close so oh I guess",
    "start": "219920",
    "end": "225799"
  },
  {
    "text": "that just is like airf flow perfect there you go it solved itself yeah I'm",
    "start": "225799",
    "end": "231400"
  },
  {
    "text": "just uh doing okay I did something really stupid this morning I was reminded that I'm a Kutz I fell when I",
    "start": "231400",
    "end": "237640"
  },
  {
    "text": "was running and it looks like broke a rib and you'd think that I'd do something about that but I I'm I'm lucky",
    "start": "237640",
    "end": "244000"
  },
  {
    "text": "I have a fourth year med student for a stepdaughter so I called her up and we agreed because Co is running rampant we",
    "start": "244000",
    "end": "250319"
  },
  {
    "text": "were not going to have me go to the emergency room so wow she said the treatment would be the same either way",
    "start": "250319",
    "end": "255480"
  },
  {
    "text": "so I'm just kind of cranking through the day doing my thing and we're now we're recording we're having fun man you're",
    "start": "255480",
    "end": "261000"
  },
  {
    "text": "really pushing through the pain for AI podcast there you go you got to be practical yeah that's something okay",
    "start": "261000",
    "end": "268000"
  },
  {
    "text": "yeah feel free to uh M and scream a couple times or whatever you need to do",
    "start": "268000",
    "end": "273759"
  },
  {
    "text": "okay we'll do well going from that note to something completely different as the",
    "start": "273759",
    "end": "279160"
  },
  {
    "text": "show might say we're uh really excited today because we have a followup on a",
    "start": "279160",
    "end": "284840"
  },
  {
    "text": "show that we did quite a while ago actually this was episode 35 so quite a while ago we had CLM Delong on from",
    "start": "284840",
    "end": "292280"
  },
  {
    "text": "hugging face to talk about what they were doing and now we're uh very excited to have Sasha Rush joining us who is an",
    "start": "292280",
    "end": "299720"
  },
  {
    "text": "associate professor at Cornell Tech and is also working at hugging face on a bunch of different things and involved",
    "start": "299720",
    "end": "306520"
  },
  {
    "text": "in the Transformers Library um and so we're really excited to have you Sasha to hear more about hugging face oh",
    "start": "306520",
    "end": "312840"
  },
  {
    "text": "thanks thanks for having me on yeah before we jump into all of that could you just give us a little bit of a sense",
    "start": "312840",
    "end": "319120"
  },
  {
    "text": "of uh your background and how you came to into the field of AI and uh",
    "start": "319120",
    "end": "324240"
  },
  {
    "text": "eventually into NLP and what you're doing now sure yeah so I've been at Cornell Tech uh for last year and if um",
    "start": "324240",
    "end": "331440"
  },
  {
    "text": "you don't know about Coronel Tech it's a new University it's about seven years old but we've had uh buildings for the",
    "start": "331440",
    "end": "338800"
  },
  {
    "text": "last two years our buildings are right in the center of New York City on uh an",
    "start": "338800",
    "end": "344080"
  },
  {
    "text": "island in the middle of the East River so um every day we kind of take a little Gondola over to the island uh and teach",
    "start": "344080",
    "end": "351479"
  },
  {
    "text": "courses there ah how romantic yeah it's a pretty fun place and yes I've been a professor here for the last year uh",
    "start": "351479",
    "end": "358360"
  },
  {
    "text": "before that I was professor at Harvard for about four and a half years and uh",
    "start": "358360",
    "end": "364440"
  },
  {
    "text": "before that I was a postto at Facebook AI research also in New York so uh",
    "start": "364440",
    "end": "370280"
  },
  {
    "text": "that's my background uh these days I have a lab here at Cornell Tech and I",
    "start": "370280",
    "end": "375599"
  },
  {
    "text": "work with the team at hugging face uh who are in Brooklyn so it's nice everything's kind of uh centered in New",
    "start": "375599",
    "end": "382599"
  },
  {
    "text": "York City lots of interesting Ai and machine learning going around here these days so my background uh after",
    "start": "382599",
    "end": "390080"
  },
  {
    "text": "graduating college I worked as a software engineer for about three years I'm kind of a person who very much",
    "start": "390080",
    "end": "397080"
  },
  {
    "text": "enjoys coding and uh I kind of have that as kind of the first part of my personality I then went to graduate",
    "start": "397080",
    "end": "404000"
  },
  {
    "text": "school to study natural language processing when I got into natural language processing I think I really got",
    "start": "404000",
    "end": "409440"
  },
  {
    "text": "into it because I was very interested in language and particularly kind of the algorithms and data structures involved",
    "start": "409440",
    "end": "415639"
  },
  {
    "text": "in studying and understanding how language works at that time I did a lot of machine learning but machine learning",
    "start": "415639",
    "end": "422120"
  },
  {
    "text": "wasn't kind of the primary way we studied language there were all sorts of other aspects about kind of how",
    "start": "422120",
    "end": "428840"
  },
  {
    "text": "computers and language interacted and actually my dissertation was much more about say the optimization aspects of",
    "start": "428840",
    "end": "436479"
  },
  {
    "text": "language in a discret sense kind of how you construct trees that represent",
    "start": "436479",
    "end": "442039"
  },
  {
    "text": "different linguistic phenomena and how these interact with kind of classical",
    "start": "442039",
    "end": "447360"
  },
  {
    "text": "computer science algorithms and when I graduated my PhD I kind of graduated right into the beginning of really kind",
    "start": "447360",
    "end": "454680"
  },
  {
    "text": "of intense deep learning for uh language and doing my postto at uh Facebook",
    "start": "454680",
    "end": "461360"
  },
  {
    "text": "everyone was kind of intensely interested in how we could do translation how we could do question",
    "start": "461360",
    "end": "468680"
  },
  {
    "text": "answering kind of completely from data using deep learning based systems so I",
    "start": "468680",
    "end": "474319"
  },
  {
    "text": "kind of dived right into that world I sat next to the folks who are working on",
    "start": "474319",
    "end": "480120"
  },
  {
    "text": "Torch at the time at then it was written in Lua and a couple years later they converted it to Python and it became",
    "start": "480120",
    "end": "487080"
  },
  {
    "text": "pytorch so I've always been very fascinated by kind of the tools and structures that make it possible to do",
    "start": "487080",
    "end": "493800"
  },
  {
    "text": "these sort of systems in a kind of Open Source way some other things I've worked on in the past um I worked on a library",
    "start": "493800",
    "end": "500479"
  },
  {
    "text": "called open nmt which was an open source translation Library written in pytorch",
    "start": "500479",
    "end": "506520"
  },
  {
    "text": "and tensorflow and we worked with a a lot of translation companies particularly uh in Europe to try to",
    "start": "506520",
    "end": "513560"
  },
  {
    "text": "build open source tools to let them build their own kind of custom Google translate like services and that was a",
    "start": "513560",
    "end": "520719"
  },
  {
    "text": "really fun project and it kind of tied together the research we were doing in my lab which was on kind of questions of",
    "start": "520719",
    "end": "527680"
  },
  {
    "text": "how to improve translation how to speed it up how to make it work on devices with questions of how in an open source",
    "start": "527680",
    "end": "534720"
  },
  {
    "text": "World these were used so I'm kind of curious since you kind of alluded a little bit to uh one thing that's kind",
    "start": "534720",
    "end": "541800"
  },
  {
    "text": "of happened in recent years in terms of how I guess people maybe used to think",
    "start": "541800",
    "end": "547720"
  },
  {
    "text": "about NLP and and still do for many tasks as far as like computational linguists have been thinking about these",
    "start": "547720",
    "end": "553720"
  },
  {
    "text": "things for a very long time but now there's been all of this focus on kind",
    "start": "553720",
    "end": "559200"
  },
  {
    "text": "of extending these tasks to maybe generalized machine learning type",
    "start": "559200",
    "end": "564920"
  },
  {
    "text": "problems could you give your perspective on kind of how that shift has happened and like what that's meant both in terms",
    "start": "564920",
    "end": "571120"
  },
  {
    "text": "of momentum in the field and and people getting involved in the field and all of",
    "start": "571120",
    "end": "576399"
  },
  {
    "text": "that what are your thoughts on that yeah let's see so I think there's a couple different perspectives I don't want to",
    "start": "576399",
    "end": "582760"
  },
  {
    "text": "make it seem like kind of data driven or machine learning systems were kind of new to NLP there's a long history of use",
    "start": "582760",
    "end": "590839"
  },
  {
    "text": "of learning both in NLP but also kind of learning systems developed in NLP being",
    "start": "590839",
    "end": "596399"
  },
  {
    "text": "used in other areas so I think it's a field that's always kind of interacted with these methods in a kind of open",
    "start": "596399",
    "end": "603640"
  },
  {
    "text": "dialogue I think the phenomenon we're seeing now is uh kind of more extreme",
    "start": "603640",
    "end": "609600"
  },
  {
    "text": "and it's extreme for a couple reasons I mean one is the sheer growth of all these fields we're seeing kind of",
    "start": "609600",
    "end": "615800"
  },
  {
    "text": "exponential growth in conference sizes and paper submissions in kind of usage",
    "start": "615800",
    "end": "621160"
  },
  {
    "text": "of this technology which I honestly think is a great problem to have but it obviously brings with it a lot of",
    "start": "621160",
    "end": "627519"
  },
  {
    "text": "challenges so they're kind of organiz gational questions of kind of running communities or kind of trying to kind of",
    "start": "627519",
    "end": "633839"
  },
  {
    "text": "make progress in this world I think the other question is what does it mean in",
    "start": "633839",
    "end": "639880"
  },
  {
    "text": "terms of methods and um we're seeing lots of interesting things along those",
    "start": "639880",
    "end": "645279"
  },
  {
    "text": "lines I think that people in the field are adapting to the challenges that kind",
    "start": "645279",
    "end": "651160"
  },
  {
    "text": "of come kind of from the world around like as researchers were interested in solving the problems that exist now and",
    "start": "651160",
    "end": "658800"
  },
  {
    "text": "a lot of the problems in NLP are suddenly kind of data set problems how",
    "start": "658800",
    "end": "664639"
  },
  {
    "text": "do we construct interesting novel and difficult data sets how do we analyze",
    "start": "664639",
    "end": "671480"
  },
  {
    "text": "models to understand what they're doing and how they're structured and what they're learning kind of societal",
    "start": "671480",
    "end": "679160"
  },
  {
    "text": "questions of how do we understand what biases they might have or what issues they might bring or",
    "start": "679160",
    "end": "686000"
  },
  {
    "text": "even how they might learn like from what what signals they picking up on and so",
    "start": "686000",
    "end": "691040"
  },
  {
    "text": "there's no shortage of interesting research going on it's just that what's interesting is maybe less so the kind of",
    "start": "691040",
    "end": "699000"
  },
  {
    "text": "how do you make the Benchmark problem go up X number of points so I'm kind of",
    "start": "699000",
    "end": "704040"
  },
  {
    "text": "curious I've been thinking about listening to this and and you know we had Clen back on the show back I think",
    "start": "704040",
    "end": "711079"
  },
  {
    "text": "it was episode 35 35 yeah going way back it was before the Transformers Library",
    "start": "711079",
    "end": "717680"
  },
  {
    "text": "came out which we we'll definitely talk about later yeah totally I think that was what I was thinking about was the",
    "start": "717680",
    "end": "723200"
  },
  {
    "text": "fact that when we were talking to Clum we were really kind of focused on like social Ai and chat Bots and similar",
    "start": "723200",
    "end": "729959"
  },
  {
    "text": "tools and approaches and then in that time between talking to you today and talking to Clen you know Transformers",
    "start": "729959",
    "end": "736240"
  },
  {
    "text": "came out and you guys really created the definitive Transformer library and you",
    "start": "736240",
    "end": "741279"
  },
  {
    "text": "know we've been talking about hugging face in the context to Transformers since then and I I guess how did hugging",
    "start": "741279",
    "end": "747079"
  },
  {
    "text": "face make that transition what caused that and it's an interesting turn for the you know for for the history of the",
    "start": "747079",
    "end": "753480"
  },
  {
    "text": "company yeah so I mean I guess I should give some perspective so I've actually only worked at hugging face for about",
    "start": "753480",
    "end": "759880"
  },
  {
    "text": "eight months now and honestly I ended up working there because I was such a fan I",
    "start": "759880",
    "end": "766920"
  },
  {
    "text": "observed them in the same way that you did which was as an external Observer",
    "start": "766920",
    "end": "772360"
  },
  {
    "text": "seeing them make this transition so impressively from uh kind of working on",
    "start": "772360",
    "end": "779720"
  },
  {
    "text": "chat Bots to being this kind of Open Source Powerhouse and I I guess as someone who I guess I mean who knows",
    "start": "779720",
    "end": "786240"
  },
  {
    "text": "what it means in open source but as a competitor as someone building his own libraries in this space they were just",
    "start": "786240",
    "end": "791760"
  },
  {
    "text": "doing it so much better than I was and so uh I think that um that that always",
    "start": "791760",
    "end": "797600"
  },
  {
    "text": "impressed me now I should say even before Transformers came out as an official Library I have memories of well",
    "start": "797600",
    "end": "804839"
  },
  {
    "text": "I guess now we're getting into some of the technical terminology when Bert came out as a paper there was a kind of rush",
    "start": "804839",
    "end": "810959"
  },
  {
    "text": "to Port Bert to a pi torch version and um I was working a little bit on this at",
    "start": "810959",
    "end": "817240"
  },
  {
    "text": "my own pace and hugging face very very quickly put out their own version of this maybe part of their chapot Library",
    "start": "817240",
    "end": "823839"
  },
  {
    "text": "maybe it was a separate thing and I think it was really useful just to have that immediately right after the research came and so I was really",
    "start": "823839",
    "end": "830639"
  },
  {
    "text": "appreciative of that even at the time what's the state of hugging face now in terms of I know that they raised a round",
    "start": "830639",
    "end": "837839"
  },
  {
    "text": "of funding it's seems like from what I'm picking up on Twitter that the team is growing a little bit but uh from",
    "start": "837839",
    "end": "843399"
  },
  {
    "text": "chatting with you before it seems like it's still also very distributed there might be some kind of creative",
    "start": "843399",
    "end": "848759"
  },
  {
    "text": "relationships like of course you're in Academia but you're also with hugging face so sort of what's the state of the",
    "start": "848759",
    "end": "854920"
  },
  {
    "text": "hugging face team now and how's it growing to support this uh really rich ecosystem of tools yeah so um we have",
    "start": "854920",
    "end": "861519"
  },
  {
    "text": "about 15 to 20 people depending on how you count we're mainly focused or entirely focused on these open source uh",
    "start": "861519",
    "end": "868160"
  },
  {
    "text": "tool development the main library is Transformers which we've talked about and kind of is the center of what we're",
    "start": "868160",
    "end": "875040"
  },
  {
    "text": "developing but now there's also several other really interesting open source projects going on so we have a a Project",
    "start": "875040",
    "end": "881839"
  },
  {
    "text": "based on NLP data sets that now has almost 150 different open data sets that",
    "start": "881839",
    "end": "889279"
  },
  {
    "text": "you can easily browse and download and use and they're very efficient and um",
    "start": "889279",
    "end": "894839"
  },
  {
    "text": "kind of easy to extend way we also have a library of tokenizers that's written",
    "start": "894839",
    "end": "899880"
  },
  {
    "text": "in in Rust kind of lowlevel library that lets you do very fast uh tokenization",
    "start": "899880",
    "end": "905240"
  },
  {
    "text": "and training and then all this is kind of joined together by a kind of Hub of different models and structures that",
    "start": "905240",
    "end": "912560"
  },
  {
    "text": "people have uploaded um and if you go to the website you can kind of see this kind of really rich ecosystem of uh",
    "start": "912560",
    "end": "918320"
  },
  {
    "text": "different models of different data sets and of different tokenizers that kind of",
    "start": "918320",
    "end": "924519"
  },
  {
    "text": "build this all together practically it is an interesting question of what the company is like I mentioned earlier that",
    "start": "924519",
    "end": "930800"
  },
  {
    "text": "I've been there longer now probably in Co than not in covid yeah I guess that's true but it's always been a distributed",
    "start": "930800",
    "end": "937959"
  },
  {
    "text": "company there are there's a team in Paris and a team in New York it's about half and half but now we also have",
    "start": "937959",
    "end": "943480"
  },
  {
    "text": "interns in California and some interns in China some people in in different places so we mostly kind of communicate",
    "start": "943480",
    "end": "949759"
  },
  {
    "text": "through slack and through other distributed [Music]",
    "start": "949759",
    "end": "956519"
  },
  {
    "text": "means [Music]",
    "start": "957920",
    "end": "967920"
  },
  {
    "text": "change log news is the best way to keep up with the fast moving software world",
    "start": "968920",
    "end": "974000"
  },
  {
    "text": "we track log and contextualize the coolest projects the best practices and",
    "start": "974000",
    "end": "979800"
  },
  {
    "text": "the biggest stories each and every week make Chang law.com your daily destination or hit this snooze button",
    "start": "979800",
    "end": "986240"
  },
  {
    "text": "and subscribe to our Weekly Newsletter that hits inbox on Sunday mornings join more than 15,000 enthusiastic readers",
    "start": "986240",
    "end": "993920"
  },
  {
    "text": "it'll cost you exactly $0 and you can subscribe right now at chain.com",
    "start": "993920",
    "end": "1000920"
  },
  {
    "text": "weekly so I guess you know we've alluded to Transformers several times now and kind of talked around it a little bit",
    "start": "1005079",
    "end": "1011319"
  },
  {
    "text": "for those who are new to the topic could you kind of Define what is a Transformer I mean it's been a big big deal uh in",
    "start": "1011319",
    "end": "1018319"
  },
  {
    "text": "recent months and has really changed NLP but a lot of people may not be familiar with it or have not kept up to date",
    "start": "1018319",
    "end": "1023800"
  },
  {
    "text": "could you kind of just give us a basic run through from the way you see it sure so I think the term Transformer really",
    "start": "1023800",
    "end": "1031038"
  },
  {
    "text": "kind of implicitly applies two different Innovations and both of these were actually connected to each other but",
    "start": "1031039",
    "end": "1038038"
  },
  {
    "text": "both pretty transformative in their own right so I'll start with the first so the first is the Transformer as an",
    "start": "1038039",
    "end": "1044760"
  },
  {
    "text": "architecture so this is the particular kind of development of a very specific",
    "start": "1044760",
    "end": "1050600"
  },
  {
    "text": "type of architecture that came out and the kind of dominant architecture in",
    "start": "1050600",
    "end": "1056440"
  },
  {
    "text": "natural language processing for about five years had been recurrent neural networks particularly the lstm network",
    "start": "1056440",
    "end": "1063320"
  },
  {
    "text": "and was used basically for everything that we did in the field and the Transformer proposed a different and in",
    "start": "1063320",
    "end": "1069640"
  },
  {
    "text": "fact kind of simpler architecture that instead of kind of relyant on these recurrent connections kind of",
    "start": "1069640",
    "end": "1075480"
  },
  {
    "text": "connections over time instead used the kind of random addressing style of",
    "start": "1075480",
    "end": "1082159"
  },
  {
    "text": "architecture based on a mechanism called attention and the way it works is that",
    "start": "1082159",
    "end": "1087280"
  },
  {
    "text": "you basically have everything you've seen in the past ready to access at every point in time and the main kind of",
    "start": "1087280",
    "end": "1094080"
  },
  {
    "text": "uh neural network step that you take is a kind of soft random addressing over",
    "start": "1094080",
    "end": "1099960"
  },
  {
    "text": "all your previous history and you use that in order to compute the next stage",
    "start": "1099960",
    "end": "1105280"
  },
  {
    "text": "in your sequence so instead of kind of keeping a fixed length vector that gets transformed over time you keep around",
    "start": "1105280",
    "end": "1110880"
  },
  {
    "text": "everything and you basically search through it at every stage in the process and this architecture wasn't kind of new",
    "start": "1110880",
    "end": "1117480"
  },
  {
    "text": "on its own right but kind of demonstrating that it was more effective than recurrent neural networks and that",
    "start": "1117480",
    "end": "1123520"
  },
  {
    "text": "particularly could scale to both uh kind of fast training and also um very very",
    "start": "1123520",
    "end": "1129559"
  },
  {
    "text": "large models better than recurrent neural networks was kind of a big breakthrough in the field and the first",
    "start": "1129559",
    "end": "1135720"
  },
  {
    "text": "results showed kind of large improvements on trans ation accuracy just quick question you mentioned",
    "start": "1135720",
    "end": "1141559"
  },
  {
    "text": "attention and you sort of defined it in the larger thing but just because it's a kind of a a key uh aspect of that could",
    "start": "1141559",
    "end": "1148840"
  },
  {
    "text": "you talk about what part of that was attention just to differentiate it from the larger process sure yeah and I",
    "start": "1148840",
    "end": "1154400"
  },
  {
    "text": "should say the the original Transformer paper has the title attention is all you need it's kind of the key aspect of what",
    "start": "1154400",
    "end": "1160919"
  },
  {
    "text": "makes a Transformer attention itself is actually quite simple um and uh it's actually kind of very kind of",
    "start": "1160919",
    "end": "1167159"
  },
  {
    "text": "intuitively appealing idea so imagine you have a set of objects say five different objects and you want to have a",
    "start": "1167159",
    "end": "1173720"
  },
  {
    "text": "neural network decide which one of those objects you want to use you might have a",
    "start": "1173720",
    "end": "1179159"
  },
  {
    "text": "softmax layer where the softmax gives you a distribution a probability",
    "start": "1179159",
    "end": "1184240"
  },
  {
    "text": "distribution over which aspect you want to pick so which of the five things you should choose you could just end there",
    "start": "1184240",
    "end": "1191520"
  },
  {
    "text": "and if you ended there we would just call it multiclass classification what attention does is it",
    "start": "1191520",
    "end": "1197799"
  },
  {
    "text": "uses that distribution the probability of each of the five things and feeds that probability back into the model",
    "start": "1197799",
    "end": "1204640"
  },
  {
    "text": "itself so it would give a weight to each of the five items and then feed them back in with that weight so imagine I",
    "start": "1204640",
    "end": "1211679"
  },
  {
    "text": "have a sentence like the man walked the dog and I want to predict the next word",
    "start": "1211679",
    "end": "1216880"
  },
  {
    "text": "in that sentence those previous five words would be the five items I'd want to choose from and attention would say",
    "start": "1216880",
    "end": "1224520"
  },
  {
    "text": "how much weight should I give to each of those previous five words when trying to decide on the next word so maybe I'll",
    "start": "1224520",
    "end": "1232000"
  },
  {
    "text": "give it 80% to man 5% to the ETC and use",
    "start": "1232000",
    "end": "1238120"
  },
  {
    "text": "those in the next step of the process all the Transformer is is a kind of repeated version of that game for say",
    "start": "1238120",
    "end": "1246600"
  },
  {
    "text": "six toh 24 different rounds where each time you look back at what you've previously decided use it uh to feed it",
    "start": "1246600",
    "end": "1254039"
  },
  {
    "text": "back into your network and then use that to try to predict the next step along the line so you mentioned men that this",
    "start": "1254039",
    "end": "1259840"
  },
  {
    "text": "architecture also in in addition to kind of having this new structure also",
    "start": "1259840",
    "end": "1265360"
  },
  {
    "text": "allowed some performance benefits and scaling as well I was wondering if you could just give a sense of because I",
    "start": "1265360",
    "end": "1271480"
  },
  {
    "text": "know this is something people see out there and in particular I think there was a thread on Twitter about um how",
    "start": "1271480",
    "end": "1277880"
  },
  {
    "text": "many parameters are in the the hugging face model Hub and all of that yeah so I was wondering if you could just give us",
    "start": "1277880",
    "end": "1283840"
  },
  {
    "text": "a sense of you know what are the sort of scale of models that are out there",
    "start": "1283840",
    "end": "1289120"
  },
  {
    "text": "people hear about like bird and gpt2 and now of course we're getting flooded with",
    "start": "1289120",
    "end": "1294880"
  },
  {
    "text": "gpt3 things what are the sort of scale of these models both in terms of like",
    "start": "1294880",
    "end": "1299960"
  },
  {
    "text": "parameters and also like the data needed to actually train them it's a good question I never know these numbers off",
    "start": "1299960",
    "end": "1306559"
  },
  {
    "text": "hand uh models range from uh 50 million parameters to tens of billions of",
    "start": "1306559",
    "end": "1312400"
  },
  {
    "text": "parameters at the the top end in practice some of the larger models it's unclear how you would even use them say",
    "start": "1312400",
    "end": "1319000"
  },
  {
    "text": "a standard GPU Hardware but scale has been a big kind of main aspect of kind",
    "start": "1319000",
    "end": "1325640"
  },
  {
    "text": "of Transformers usage but actually maybe let me pick that question to talk a little bit about the second main",
    "start": "1325640",
    "end": "1331240"
  },
  {
    "text": "innovation of the Transformer I talked about the architecture but I think it's important to also get a sense of the",
    "start": "1331240",
    "end": "1337400"
  },
  {
    "text": "second Innovation because I think it actually matters even more this is a kind of innovation that started or",
    "start": "1337400",
    "end": "1343039"
  },
  {
    "text": "around the use of a model called Elmo there were a couple other variants one",
    "start": "1343039",
    "end": "1348080"
  },
  {
    "text": "called Cove and then it's all kind of peaked with the release of a model called Bert and the kind of idea behind",
    "start": "1348080",
    "end": "1355360"
  },
  {
    "text": "these models is to take a neural network in the case of Bert a Transformer and to",
    "start": "1355360",
    "end": "1360880"
  },
  {
    "text": "train it on a very simple task at a very very massive data scale so in the case",
    "start": "1360880",
    "end": "1366240"
  },
  {
    "text": "of Bert the task is similar to the one I described previously you're given a",
    "start": "1366240",
    "end": "1371600"
  },
  {
    "text": "bunch of words and you randomly remove some of the words and try to predict them back it's a game that you can play",
    "start": "1371600",
    "end": "1377480"
  },
  {
    "text": "yourself and try to get a sense of how easy or hard it is to do sometimes it's really easy sometimes it's really",
    "start": "1377480",
    "end": "1383200"
  },
  {
    "text": "challenging but the point isn't the task itself the point is to give the model a task that would require it to know",
    "start": "1383200",
    "end": "1390520"
  },
  {
    "text": "something about language in order to complete and then train it at as biggest scale as you can so it's hard to give",
    "start": "1390520",
    "end": "1396520"
  },
  {
    "text": "you a sense of this I mean uh one thing that's nice about language is you can store a ton of it in very little space",
    "start": "1396520",
    "end": "1403360"
  },
  {
    "text": "so if you have all of Wikipedia just basically fit it on your computer and",
    "start": "1403360",
    "end": "1408480"
  },
  {
    "text": "companies like Google basically have um a non-trivial amount of all the text that's ever been produced and so you can",
    "start": "1408480",
    "end": "1415679"
  },
  {
    "text": "kind of take all that texts throw it into one of these models and then train it on this simple task and it turns out",
    "start": "1415679",
    "end": "1422159"
  },
  {
    "text": "that in the process of trying to complete this task the model learns a lot about how language works we say it",
    "start": "1422159",
    "end": "1428279"
  },
  {
    "text": "uh it learns very good features for language so once you've done that once you've kind of trained it on all the",
    "start": "1428279",
    "end": "1434440"
  },
  {
    "text": "language that you have you can then apply it to a much smaller task that you maybe have a small amount of supervised",
    "start": "1434440",
    "end": "1440720"
  },
  {
    "text": "data for so this idea which people call pre-training is kind of central to how a",
    "start": "1440720",
    "end": "1447559"
  },
  {
    "text": "lot of NLP works these days and also to how the Transformers library is designed",
    "start": "1447559",
    "end": "1452600"
  },
  {
    "text": "so yeah I I think that's a such a great and important point is that people kind",
    "start": "1452600",
    "end": "1458679"
  },
  {
    "text": "of get hung up on the the size of these models and it's it's kind of cool to talk about those things and in some",
    "start": "1458679",
    "end": "1464720"
  },
  {
    "text": "cases annoying to work with them because they're so large and in some cases hard to perform inference with but yeah I",
    "start": "1464720",
    "end": "1472480"
  },
  {
    "text": "guess what you're saying is you know that the task that they're trained on is",
    "start": "1472480",
    "end": "1477919"
  },
  {
    "text": "just intended to help them learn good features and then the task that you actually want to use them for involves",
    "start": "1477919",
    "end": "1485679"
  },
  {
    "text": "some like fine-tuning or or transfer learning is is that right yeah I think I mean I don't want to claim that this is",
    "start": "1485679",
    "end": "1492360"
  },
  {
    "text": "finished as an idea right I think a lot of the the tasks we work on now will have a kind of fine tuning stage where",
    "start": "1492360",
    "end": "1499640"
  },
  {
    "text": "you take the model and and learn it for a given task open AI has a slightly different model of what they're trying",
    "start": "1499640",
    "end": "1505360"
  },
  {
    "text": "to achieve which is they're not super interested in fine tuning they want to kind of just use the model directly kind",
    "start": "1505360",
    "end": "1511919"
  },
  {
    "text": "of feed it some more sentences and try to directly kind of predict Tas yeah so there is this like uh because I've seen",
    "start": "1511919",
    "end": "1518600"
  },
  {
    "text": "um and maybe you could kind of help us through some of this jargon it seems like people talk about some of these",
    "start": "1518600",
    "end": "1524240"
  },
  {
    "text": "models they just like they have so much knowledge that you can perform a task that they just write off the bat like I",
    "start": "1524240",
    "end": "1530840"
  },
  {
    "text": "I don't know if it's question answering or information retrieval or whatever it is uh without really much fine tuning is",
    "start": "1530840",
    "end": "1536960"
  },
  {
    "text": "that what you're kind of getting at in that other model well I do want to distinguish uh kind of two aspects I",
    "start": "1536960",
    "end": "1542039"
  },
  {
    "text": "think that all the kind of state-ofthe-art models on kind of standard Benchmark tasks all use some",
    "start": "1542039",
    "end": "1548279"
  },
  {
    "text": "sort of fine-tuning that's like a become a very standard uh procedure and we kind",
    "start": "1548279",
    "end": "1553799"
  },
  {
    "text": "of understand how that works but to do fine tuning you still need some amount of supervised data I guess we would say",
    "start": "1553799",
    "end": "1560840"
  },
  {
    "text": "it's a small to medium amount but you need something in domain for the task you're interested in I think there's a",
    "start": "1560840",
    "end": "1566799"
  },
  {
    "text": "lot of recent excitement for kind of a a crazier idea which is this kind of zero shot or one shot uh idea of just the",
    "start": "1566799",
    "end": "1574520"
  },
  {
    "text": "model should know how to do your task immediately right off the bat I think that's where I was going because they",
    "start": "1574520",
    "end": "1579559"
  },
  {
    "text": "throw throw around this idea of zero shot and to some degree it seems sort of like magical in many in many ways to",
    "start": "1579559",
    "end": "1585720"
  },
  {
    "text": "people I think yeah I don't want to say anything on record it's on the research Frontier yeah it might turn out that's",
    "start": "1585720",
    "end": "1592039"
  },
  {
    "text": "the way to do lots of language tasks but I think still an open question I would say so turning to the Transformer",
    "start": "1592039",
    "end": "1598399"
  },
  {
    "text": "Library itself I'm kind of curious so and recognizing that you've only been there at the company for a limited",
    "start": "1598399",
    "end": "1603960"
  },
  {
    "text": "amount of time yeah do you have any insight into kind of the motivation that moved the company into this Transformer",
    "start": "1603960",
    "end": "1610320"
  },
  {
    "text": "Library itself was it supporting the other operations or was it just something that was an opportunity that",
    "start": "1610320",
    "end": "1615799"
  },
  {
    "text": "came up what kind of took the company there as far as your familiar that's a good question",
    "start": "1615799",
    "end": "1620960"
  },
  {
    "text": "uh the the graph of the usage of this library on on GitHub kind of blows me away like it it went from pretty insane",
    "start": "1620960",
    "end": "1628480"
  },
  {
    "text": "no users to about 30,000 so I think they just hit on something that was like uh I",
    "start": "1628480",
    "end": "1633760"
  },
  {
    "text": "guess when you have a hit maybe that changes the mode of of thinking yeah so maybe you could describe like along with",
    "start": "1633760",
    "end": "1640080"
  },
  {
    "text": "that what is the sort of main usage pattern that people are kind of grabbing",
    "start": "1640080",
    "end": "1646080"
  },
  {
    "text": "on to Transformers for I I know that there are multiple of course like quite a few different things that you could",
    "start": "1646080",
    "end": "1652600"
  },
  {
    "text": "use the library for but what do you see as the sort of uh like the main thrust",
    "start": "1652600",
    "end": "1658080"
  },
  {
    "text": "of what people are grabbing Transformers for um what what is that and you know",
    "start": "1658080",
    "end": "1663679"
  },
  {
    "text": "how is that being supported I guess yeah yeah this is a great question and I think in some ways you guys maybe have",
    "start": "1663679",
    "end": "1669840"
  },
  {
    "text": "insight into this that I would be also interested to hear about let me start at the high level one thing that fascinates",
    "start": "1669840",
    "end": "1675559"
  },
  {
    "text": "me about kind of current usage of deep learning is that you have people who approach it from many different angles",
    "start": "1675559",
    "end": "1682760"
  },
  {
    "text": "and um in one of our papers we kind of broke this down into three different classes so we talk about they being",
    "start": "1682760",
    "end": "1688200"
  },
  {
    "text": "Architects they're being trainers and then they're being end users and I think",
    "start": "1688200",
    "end": "1693519"
  },
  {
    "text": "within the ecosystem Transformers kind of has different meanings to all three of those people so if you're a company",
    "start": "1693519",
    "end": "1700679"
  },
  {
    "text": "like open AI or like Allen AI kind of companies at the kind of cutting edge of",
    "start": "1700679",
    "end": "1705840"
  },
  {
    "text": "research training you use transform or kind of related libraries to try to build the next architecture or the next",
    "start": "1705840",
    "end": "1712880"
  },
  {
    "text": "pre-trained model and that often means running these very large training jobs on multi-gpus over many days and then",
    "start": "1712880",
    "end": "1721720"
  },
  {
    "text": "using Transformers as a way to distribute your model through our Hub and make it easy for people to use it or",
    "start": "1721720",
    "end": "1727480"
  },
  {
    "text": "to adapt it for their tasks if you're like an expert but maybe not kind of at the kind of front end uh",
    "start": "1727480",
    "end": "1735120"
  },
  {
    "text": "of the like Frontier of research another common use case is this kind of fine-tuning use case where you have data",
    "start": "1735120",
    "end": "1742480"
  },
  {
    "text": "for your company or for a given problem that you want to solve and you bring that data into the library use it in",
    "start": "1742480",
    "end": "1749720"
  },
  {
    "text": "training mode to fine-tune on your data set it may take a couple hours and",
    "start": "1749720",
    "end": "1754799"
  },
  {
    "text": "require some gpus but out of that you get a really accurate model for the task",
    "start": "1754799",
    "end": "1760200"
  },
  {
    "text": "you're interested in but then at the other end you have just end users who want to use the library as a way of kind",
    "start": "1760200",
    "end": "1766880"
  },
  {
    "text": "of Performing kind of stand NLP tasks you might want to use it as a way to do summarization or translation or named",
    "start": "1766880",
    "end": "1774960"
  },
  {
    "text": "entity recognition or question answering and you can often just use it completely in inference mode maybe not even using",
    "start": "1774960",
    "end": "1781480"
  },
  {
    "text": "python just kind of taking a pre-trained model using it directly for your task in",
    "start": "1781480",
    "end": "1786720"
  },
  {
    "text": "in that kind of setting so I think all of these people are within the machine learning ecosystem um but they kind of",
    "start": "1786720",
    "end": "1793240"
  },
  {
    "text": "have different end goals or different use cases and and I think we're kind trying to aim to support any of those uh",
    "start": "1793240",
    "end": "1799360"
  },
  {
    "text": "those kind of outcomes so I know you have a model Hub and was wondering if",
    "start": "1799360",
    "end": "1804880"
  },
  {
    "text": "you could kind of talk about you know what users can find there and start incorporating into their own projects",
    "start": "1804880",
    "end": "1810880"
  },
  {
    "text": "what does the growth of that Hub look like you know just what kind of ecosystem has developed around it yeah",
    "start": "1810880",
    "end": "1816200"
  },
  {
    "text": "so the model Hub is um kind of part of the open source Library if you want to",
    "start": "1816200",
    "end": "1821720"
  },
  {
    "text": "use a model in the library you say model. load and you pull off you just",
    "start": "1821720",
    "end": "1826799"
  },
  {
    "text": "pull it directly down from the model Hub uh and you can do that with any of the models that are there we have kind of a",
    "start": "1826799",
    "end": "1833279"
  },
  {
    "text": "set of models that kind of have brand names that are are very often used so those include models like",
    "start": "1833279",
    "end": "1839080"
  },
  {
    "text": "gpd2 or variance of Bert or Roberta or new models like this model called Bart",
    "start": "1839080",
    "end": "1844240"
  },
  {
    "text": "or a model called T5 but then it also includes a long taale of other models from the community so this includes",
    "start": "1844240",
    "end": "1850720"
  },
  {
    "text": "models that are pre-trained to Target say biomedical text or extraction from",
    "start": "1850720",
    "end": "1855840"
  },
  {
    "text": "scientific documents or models that are trained in many different languages kind",
    "start": "1855840",
    "end": "1860880"
  },
  {
    "text": "of by the communities interested in those languages themselves or models that are experimental or try to do other",
    "start": "1860880",
    "end": "1867200"
  },
  {
    "text": "things or one popular aspect is models that are very small models that you could run on your phone um so the idea",
    "start": "1867200",
    "end": "1873600"
  },
  {
    "text": "of the model how is kind of have all of those have the same API and have the same easy way to use them and one thing",
    "start": "1873600",
    "end": "1881120"
  },
  {
    "text": "that we think is really interesting is that unlike kind of generic model hubs like uh tensor flows Hub or pie torches",
    "start": "1881120",
    "end": "1887600"
  },
  {
    "text": "hub because our models are all of the same form we can build a lot of kind of tools",
    "start": "1887600",
    "end": "1893240"
  },
  {
    "text": "and Machinery around using them so for instance we have a visualizer that works for all of our models you can uh just",
    "start": "1893240",
    "end": "1900200"
  },
  {
    "text": "upload your own model and get really interesting visualization of its internal structure or this open source",
    "start": "1900200",
    "end": "1906440"
  },
  {
    "text": "project called I think it's called text attack built uh adversarial attack system and it's able to kind of",
    "start": "1906440",
    "end": "1913039"
  },
  {
    "text": "generically uh build ATT tax to any of our models in our Hub so because they all have the same interface that allows",
    "start": "1913039",
    "end": "1919039"
  },
  {
    "text": "people to do these really kind of longitudinal research projects across everything that's going on in the hub",
    "start": "1919039",
    "end": "1924600"
  },
  {
    "text": "itself itself and then I should mention that now we have a kind of an inference API on any of the pages you can just",
    "start": "1924600",
    "end": "1931279"
  },
  {
    "text": "type in some text and it will run against that model and you can even call that from your own code directly without",
    "start": "1931279",
    "end": "1937880"
  },
  {
    "text": "ever running anything on your machine just run it on one of these servers uh and we even have a a Twitter bot that we",
    "start": "1937880",
    "end": "1944080"
  },
  {
    "text": "just put up last week where you can kind of tweet at it and it will run a model again tweet yeah that's great I was",
    "start": "1944080",
    "end": "1950519"
  },
  {
    "text": "wondering before we leave the topic of the the open source projects you also mentioned the these other libraries",
    "start": "1950519",
    "end": "1956639"
  },
  {
    "text": "tokenizers and NLP which includes the data sets and evaluation metrics how do those fit into the puzzle and maybe",
    "start": "1956639",
    "end": "1963960"
  },
  {
    "text": "interact and influence one another yeah I mean end of the day our interest is in",
    "start": "1963960",
    "end": "1969200"
  },
  {
    "text": "building open source NLP and I think there will continue to be kind of new",
    "start": "1969200",
    "end": "1974960"
  },
  {
    "text": "variants of Transformers and new pre-trained models but kind of a as I",
    "start": "1974960",
    "end": "1981080"
  },
  {
    "text": "mentioned earlier an increasing area of innovation in NLP is to try to find the",
    "start": "1981080",
    "end": "1986320"
  },
  {
    "text": "right data sets to kind of challenge these models in interesting ways and so there's a lot of energy in data set",
    "start": "1986320",
    "end": "1993399"
  },
  {
    "text": "construction these days and a proliferation of really interesting data sets of different sizes and Scopes and",
    "start": "1993399",
    "end": "2000600"
  },
  {
    "text": "so um Tom Wolf who's our main open source engineer uh got very passionate about building up open source data sets",
    "start": "2000600",
    "end": "2008320"
  },
  {
    "text": "and build a library that uh makes it very easy to use these models in Python",
    "start": "2008320",
    "end": "2013960"
  },
  {
    "text": "and really makes it extremely efficient to use kind of complex data sets",
    "start": "2013960",
    "end": "2019120"
  },
  {
    "text": "directly within your code across kind of many different aspects of of NLP and so",
    "start": "2019120",
    "end": "2025080"
  },
  {
    "text": "you can uh we have a website that you can go to where you can kind of browse through any of these data sets and kind",
    "start": "2025080",
    "end": "2030799"
  },
  {
    "text": "of use them in various tasks um and one nice aspect of this is that we have a lot of examples of how to use",
    "start": "2030799",
    "end": "2036880"
  },
  {
    "text": "Transformers and they had a lot of kind of custom data set code just to run the examples but now that code has all kind",
    "start": "2036880",
    "end": "2043000"
  },
  {
    "text": "of been factored out you can just kind of Pull It in from NLP and then run the examples kind of focusing on the machine",
    "start": "2043000",
    "end": "2048679"
  },
  {
    "text": "learning [Music]",
    "start": "2048679",
    "end": "2062839"
  },
  {
    "text": "parts we deserve a better internet and the brave team has the recipe for bringing it to us start with Google",
    "start": "2062839",
    "end": "2069398"
  },
  {
    "text": "Chrome keep the extension the dev tools and the rendering engine that make Chrome great rip out the Google bits we",
    "start": "2069399",
    "end": "2075040"
  },
  {
    "text": "don't need them mix in ad and tracker blocking by default quick access to the tour Network for True private browsing",
    "start": "2075040",
    "end": "2081760"
  },
  {
    "text": "and an opt-in reward system so you can get paid to view privacy respecting ads then turn around and use those rewards",
    "start": "2081760",
    "end": "2087919"
  },
  {
    "text": "to support your favorite web creators like us Download Brave today using the link in the show notes and give tipping",
    "start": "2087919",
    "end": "2093358"
  },
  {
    "text": "a try on change.com [Music]",
    "start": "2093359",
    "end": "2105190"
  },
  {
    "text": "so to take the conversation in a slightly different direction for a moment I know from talking before the",
    "start": "2105280",
    "end": "2110920"
  },
  {
    "text": "show that you put together iclr and you kind of manage that process this morning",
    "start": "2110920",
    "end": "2116040"
  },
  {
    "text": "and for uh which is a research conference and I'm really interested at this point you know we are in the time",
    "start": "2116040",
    "end": "2121800"
  },
  {
    "text": "of of covid-19 and so much has changed uh across all of work but particularly",
    "start": "2121800",
    "end": "2127720"
  },
  {
    "text": "conferences many of them are going online uh becoming virtual like that really interested in what that was like",
    "start": "2127720",
    "end": "2133480"
  },
  {
    "text": "uh and what what you know what your experience doing it this way was and you know what worked what didn't that kind",
    "start": "2133480",
    "end": "2139040"
  },
  {
    "text": "of I'm just curious because I think a lot of people are kind of waiting to see what conferences are turning into and",
    "start": "2139040",
    "end": "2145359"
  },
  {
    "text": "you know do they want to continue to to go down that route or something yeah this year I was the General chair of",
    "start": "2145359",
    "end": "2151280"
  },
  {
    "text": "iclr the International Conference of learning representations it's a big machine learning conference and really",
    "start": "2151280",
    "end": "2157359"
  },
  {
    "text": "the only one focused uh completely on deep learning and it was interesting I had the chance of being the program",
    "start": "2157359",
    "end": "2163400"
  },
  {
    "text": "chair for the conference last year uh where we had the conference in New Orleans and then this year I was a",
    "start": "2163400",
    "end": "2169480"
  },
  {
    "text": "general chair and by about December we were getting prepped and uh then by know",
    "start": "2169480",
    "end": "2176200"
  },
  {
    "text": "February March it became increasingly clear that we weren't going to be able to have this conference live and so I",
    "start": "2176200",
    "end": "2182240"
  },
  {
    "text": "think we were the first AI conference to really have to be completely virtual we",
    "start": "2182240",
    "end": "2187920"
  },
  {
    "text": "had about a month and a half before the conference to really come up with something new uh and we had this",
    "start": "2187920",
    "end": "2194200"
  },
  {
    "text": "wonderful team led by the program chair this year Shakir Muhammad and we wanted",
    "start": "2194200",
    "end": "2199560"
  },
  {
    "text": "to do something that kind of fit the spirit of the conference and so we sat down and and wrote a a website for the",
    "start": "2199560",
    "end": "2205240"
  },
  {
    "text": "conference from scratch and we built a website that was based around this idea that everyone in the conference would be",
    "start": "2205240",
    "end": "2211119"
  },
  {
    "text": "in kind of a zoom like sorry a slack like chat room um we used an open source",
    "start": "2211119",
    "end": "2217079"
  },
  {
    "text": "platform for that and that every paper would have its own page with a video of",
    "start": "2217079",
    "end": "2223280"
  },
  {
    "text": "the work and a chat room for that paper so people would be able to kind of talk about it or discuss it within that uh",
    "start": "2223280",
    "end": "2230400"
  },
  {
    "text": "setting itself and um in addition we build out kind of a bunch of social",
    "start": "2230400",
    "end": "2238480"
  },
  {
    "text": "Gatherings that people could have and a a kind of calendar for the whole event and the kind of main challenge is how do",
    "start": "2238480",
    "end": "2246359"
  },
  {
    "text": "you run a conference a synchronous in this way uh we didn't really think it was possible to have everyone in the",
    "start": "2246359",
    "end": "2252640"
  },
  {
    "text": "same place at the same time and so we wanted it to kind of use things like",
    "start": "2252640",
    "end": "2257680"
  },
  {
    "text": "like chat rooms that kind of feel more asynchronous particularly with kind of international audience and the",
    "start": "2257680",
    "end": "2263280"
  },
  {
    "text": "conference itself actually was really fun we had actually a pretty large increase in attendance over past years",
    "start": "2263280",
    "end": "2269640"
  },
  {
    "text": "we had people from all over the world particularly from some places that would have been difficult to attend a",
    "start": "2269640",
    "end": "2276200"
  },
  {
    "text": "conference in other year and a ton of Engagement a lot of the the",
    "start": "2276200",
    "end": "2281280"
  },
  {
    "text": "posters were viewed tremendous amount of times and maybe about 100,000 messages over the chat system uh over a couple",
    "start": "2281280",
    "end": "2288240"
  },
  {
    "text": "days I think there were challenges I think it's hard to get that same kind of spirit of having coffee or kind of just",
    "start": "2288240",
    "end": "2296040"
  },
  {
    "text": "chatting informally in this sort of event things like Twitter are helpful",
    "start": "2296040",
    "end": "2301160"
  },
  {
    "text": "but don't have the same kind of intimacy but there were also kind of nice things we ran these kind of mentorship sessions",
    "start": "2301160",
    "end": "2307800"
  },
  {
    "text": "where one person was able to chat with 10 to 20 folks who were interested in mentorship and a kind of one to many",
    "start": "2307800",
    "end": "2314839"
  },
  {
    "text": "model that actually I think might have been difficult at a conference so but kind of works actually pretty nicely",
    "start": "2314839",
    "end": "2320160"
  },
  {
    "text": "over Zoom anyway it was an experimental setup since then we open- sourced all",
    "start": "2320160",
    "end": "2325640"
  },
  {
    "text": "the tools that we built for the conference you can get it online it's if you search for mini comp and uh the",
    "start": "2325640",
    "end": "2331240"
  },
  {
    "text": "software has been used for about five or six other major conferences since then including um ACL this year which is the",
    "start": "2331240",
    "end": "2337760"
  },
  {
    "text": "big NLP conference and icml which is another machine learning uh conference venue I don't think it's I don't think",
    "start": "2337760",
    "end": "2344599"
  },
  {
    "text": "we've cracked it but uh in the meantime it's nice to have something we build as a community yeah I um I attended uh the",
    "start": "2344599",
    "end": "2352280"
  },
  {
    "text": "the conference I clear and um I was super impressed with everything that was put together especially given the time",
    "start": "2352280",
    "end": "2358079"
  },
  {
    "text": "frame I know you you must have had some uh very late nights fueled by very much",
    "start": "2358079",
    "end": "2364880"
  },
  {
    "text": "coffee so congratulations on on uh in such a short time period putting",
    "start": "2364880",
    "end": "2370839"
  },
  {
    "text": "together something that was so good I know one of the things that I appreciated you know I've been to other research conferences in person and you",
    "start": "2370839",
    "end": "2378960"
  },
  {
    "text": "know posters or talks or something like that there's just so much going on that",
    "start": "2378960",
    "end": "2384440"
  },
  {
    "text": "it is hard to kind of do that like you can't go to this talk at the same time",
    "start": "2384440",
    "end": "2389480"
  },
  {
    "text": "as this talk and it's hard to find that person afterwards and ask them some questions about their work maybe you",
    "start": "2389480",
    "end": "2395240"
  },
  {
    "text": "walk by their poster or something so it was kind kind of nice to just scroll through and look at the different videos",
    "start": "2395240",
    "end": "2400599"
  },
  {
    "text": "especially given the time zone differences and you know shoot the authors a message that they could",
    "start": "2400599",
    "end": "2406319"
  },
  {
    "text": "respond to asynchronously so that that question didn't get lost or something like that I found that extremely useful",
    "start": "2406319",
    "end": "2413720"
  },
  {
    "text": "what are your thoughts on assuming maybe that at some point in the future research conferences we'll have an",
    "start": "2413720",
    "end": "2420240"
  },
  {
    "text": "in-person component again do you see a sort of hybrid scenario developing",
    "start": "2420240",
    "end": "2425680"
  },
  {
    "text": "because I know one of the things that like with nurs and all of that was a struggle for so many years was were",
    "start": "2425680",
    "end": "2431280"
  },
  {
    "text": "people getting visas as well which is just such a shame as like so many people from uh Africa or from Asia that were",
    "start": "2431280",
    "end": "2440200"
  },
  {
    "text": "doing amazing work but couldn't actually be at the conference because of Visa issues or cost issues or whatever it is",
    "start": "2440200",
    "end": "2446960"
  },
  {
    "text": "so how do you see that that future happening yeah uh it's a question we're talking actually a lot about in at iclr",
    "start": "2446960",
    "end": "2453839"
  },
  {
    "text": "right now I don't think we have an answer and I think a lot of it will depend on kind of what the world looks",
    "start": "2453839",
    "end": "2459040"
  },
  {
    "text": "like in a couple years so um one thing we're committed to at iclr is is having",
    "start": "2459040",
    "end": "2464440"
  },
  {
    "text": "the conference at uh venues in other locations or kind of locations that have",
    "start": "2464440",
    "end": "2470000"
  },
  {
    "text": "kind of not been visited as much in the past so one thing that was very disappointing was that this year's",
    "start": "2470000",
    "end": "2475680"
  },
  {
    "text": "conference for icr was supposed to be in um Ethiopia in adaba and we were all",
    "start": "2475680",
    "end": "2481640"
  },
  {
    "text": "really disappointed that we couldn't make it out there it would have been a really interesting event so hopefully we",
    "start": "2481640",
    "end": "2487359"
  },
  {
    "text": "continue to kind of have conferences in in kind of a wider range of locations but I think as I was saying earlier all",
    "start": "2487359",
    "end": "2494280"
  },
  {
    "text": "these uh areas are experiencing such kind of hyper growth that uh kind of",
    "start": "2494280",
    "end": "2500000"
  },
  {
    "text": "ways of kind of dealing with scale that that doesn't lose a kind of sense of",
    "start": "2500000",
    "end": "2505079"
  },
  {
    "text": "interaction is a kind of major challenge for the community and so I think we need to kind of be creative about ways to",
    "start": "2505079",
    "end": "2511839"
  },
  {
    "text": "handle that problem and ways of kind of maybe giving people the same experience that I think at least I feel like I had",
    "start": "2511839",
    "end": "2517880"
  },
  {
    "text": "when I was a first a graduate student that kind of inspired me to continue in the field so I don't know what that would look like maybe it looks like",
    "start": "2517880",
    "end": "2524240"
  },
  {
    "text": "something more distributed with a virtual component so kind of wondering and also turning the corner a little bit",
    "start": "2524240",
    "end": "2530319"
  },
  {
    "text": "on just NLP in general yeah and you know you're doing the work that you're doing you're right at the center of the NLP",
    "start": "2530319",
    "end": "2536720"
  },
  {
    "text": "World in that way and it's certainly you know Daniel and I talk all the time on these episodes about the fact that the",
    "start": "2536720",
    "end": "2542880"
  },
  {
    "text": "last couple of years has felt like you know NLP has really come of age you know you might say golden age of NLP is how",
    "start": "2542880",
    "end": "2549359"
  },
  {
    "text": "it feels like we're in and kind of before that you know we had seen like CNN's have their moment and stuff as",
    "start": "2549359",
    "end": "2555440"
  },
  {
    "text": "we've arrived where we are so far in NLP you know what does the future look like to you what kind of big challenges are",
    "start": "2555440",
    "end": "2562559"
  },
  {
    "text": "are open and should be focused on you know what are your thoughts there on on from this point",
    "start": "2562559",
    "end": "2568559"
  },
  {
    "text": "forward that's a hard question big one yeah in some ways as someone who's been working at LP for a while it's been",
    "start": "2568559",
    "end": "2575440"
  },
  {
    "text": "really neat I mean uh I think think uh it's it's way better than I could have possibly expected seeing things like",
    "start": "2575440",
    "end": "2582119"
  },
  {
    "text": "translation get to the point where it's at now is just all inspiring to me like it's such a useful thing and have it worked the way it does is is awesome so",
    "start": "2582119",
    "end": "2589480"
  },
  {
    "text": "what are the challenges now I think there's a bunch I think computer vision for all its successes has also had a lot",
    "start": "2589480",
    "end": "2596599"
  },
  {
    "text": "of issues and uh there's a lot of conversation in NLP about kind of how",
    "start": "2596599",
    "end": "2602319"
  },
  {
    "text": "to avoid some of the issues or to kind of have those conversations ear earlier",
    "start": "2602319",
    "end": "2607640"
  },
  {
    "text": "rather than later things like what we've seen with facial recognition as a technology and kind of questions about",
    "start": "2607640",
    "end": "2615160"
  },
  {
    "text": "ethicacy there is is I think it kind of challenging point and we've somehow",
    "start": "2615160",
    "end": "2620839"
  },
  {
    "text": "managed to to solve a lot of the natural language processing questions without solving some of the computational",
    "start": "2620839",
    "end": "2626599"
  },
  {
    "text": "Linguistics questions like things work but we have no real sense of why and as",
    "start": "2626599",
    "end": "2633599"
  },
  {
    "text": "a scientist that can be a little bit frustrating like we don't really know what signals these models are using to",
    "start": "2633599",
    "end": "2639280"
  },
  {
    "text": "make predictions and it's very hard to know or to even ask that sort of",
    "start": "2639280",
    "end": "2645200"
  },
  {
    "text": "question in a falsifiable way why did this model classify this sentence in",
    "start": "2645200",
    "end": "2650240"
  },
  {
    "text": "this way why did it decide to choose this decision I mean these models are at least from a publicistic sense",
    "start": "2650240",
    "end": "2656319"
  },
  {
    "text": "completely Global uh and so it's kind of challenging to kind of do any sort of",
    "start": "2656319",
    "end": "2662200"
  },
  {
    "text": "analysis along those lines but then more kind of practically I think there's a lot of practical questions that are not",
    "start": "2662200",
    "end": "2668359"
  },
  {
    "text": "solved yet you mentioned this idea of dealing with massive massive models it's",
    "start": "2668359",
    "end": "2673440"
  },
  {
    "text": "not clear if we're going to need Hardware that is 100 times bigger to run these models or whether you can use um",
    "start": "2673440",
    "end": "2680720"
  },
  {
    "text": "pruning and distillation to make them super small or what does it mean to run it locally or does it just make us more",
    "start": "2680720",
    "end": "2686880"
  },
  {
    "text": "reliant on kind of cloud systems um I think these these all become interesting kind of systems research questions in",
    "start": "2686880",
    "end": "2692319"
  },
  {
    "text": "the short term awesome well um we appreciate you taking a stab at the future predictions cuz uh I know I think",
    "start": "2692319",
    "end": "2699440"
  },
  {
    "text": "we've said on the podcast before any of the predictions that we make I feel like are are definitely going to be false",
    "start": "2699440",
    "end": "2704720"
  },
  {
    "text": "because it's always something unexpected that that happens but I appreciate you giving giving your perspective being",
    "start": "2704720",
    "end": "2710720"
  },
  {
    "text": "part of the kind of the center of all of this work um and appreciate you taking time to talk with us and kind of explain",
    "start": "2710720",
    "end": "2718079"
  },
  {
    "text": "a bit about the Transformers library and things that are going on in LP thank you so much for your contributions to the",
    "start": "2718079",
    "end": "2723880"
  },
  {
    "text": "community as well in terms of helping you know conferences and really pushing",
    "start": "2723880",
    "end": "2729440"
  },
  {
    "text": "forward open source so appreciate you taking time to join us and uh looking forward to digging into all the great",
    "start": "2729440",
    "end": "2736800"
  },
  {
    "text": "things that hugging face uh is releasing and is doing thanks so much thanks for having me",
    "start": "2736800",
    "end": "2742170"
  },
  {
    "text": "[Music] on if you're listening to this in the month of July you've got a shot at some",
    "start": "2742170",
    "end": "2749319"
  },
  {
    "text": "free goodies we are doing a giveaway in celebration of our friend and open source Wiz Zeno roach's new book 14",
    "start": "2749319",
    "end": "2756280"
  },
  {
    "text": "habits of highly productive developers if you don't know Zeno by name you may have heard of his wildly popular Dracula",
    "start": "2756280",
    "end": "2762280"
  },
  {
    "text": "theme it's an awesome dark mode theme for text editors terminals Etc and we have three bundles of Dracula Pro and 14",
    "start": "2762280",
    "end": "2769319"
  },
  {
    "text": "habits to give away for absolutely free that's a $60 value and there are three ways to enter you can be the reviewer",
    "start": "2769319",
    "end": "2776440"
  },
  {
    "text": "the socializer and the recommender hit up the link in your show notes to get started there will be three lucky",
    "start": "2776440",
    "end": "2782280"
  },
  {
    "text": "winners and you could be one of them thanks to our longtime sponsors fastly lenoe and rollbar for their continued",
    "start": "2782280",
    "end": "2788760"
  },
  {
    "text": "support to break master cylinder for our amazing beats and to you for listening to practical AI we appreciate your time",
    "start": "2788760",
    "end": "2795599"
  },
  {
    "text": "and attention that's all for this week we'll talk to you next [Music]",
    "start": "2795599",
    "end": "2802000"
  },
  {
    "text": "time",
    "start": "2802000",
    "end": "2805000"
  }
]