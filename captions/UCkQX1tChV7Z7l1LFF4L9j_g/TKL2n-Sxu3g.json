[
  {
    "text": "[Music]",
    "start": "3270",
    "end": "8730"
  },
  {
    "text": "hi everyone uh my name is lbna and I'm a machine learning engineer at Hain face and I work on large language models for",
    "start": "9400",
    "end": "15720"
  },
  {
    "text": "code and today I will tell you how these models are trained and how you can leverage them for your own use cases uh",
    "start": "15720",
    "end": "22480"
  },
  {
    "text": "so a few words about me uh I work at Hagin face and I did my studies in Paris on engineering and deep learning and I",
    "start": "22480",
    "end": "28640"
  },
  {
    "text": "mainly Now work on LM for code in synthetic data so let's see a little bit how all",
    "start": "28640",
    "end": "34719"
  },
  {
    "text": "of this started uh larg language for models for code have been there for a while but this topic became very trendy",
    "start": "34719",
    "end": "41760"
  },
  {
    "text": "in the AI world when GitHub copilot was introduced in late 2021 so this is a vs",
    "start": "41760",
    "end": "47000"
  },
  {
    "text": "code extension by Microsoft that aut completes your code and it uses a model by open AI called codex and this was a",
    "start": "47000",
    "end": "53960"
  },
  {
    "text": "very huge breakthrough in the field because this model was so much better than all the other code completion models before it which were so much",
    "start": "53960",
    "end": "60879"
  },
  {
    "text": "smaller and much less performance and this is important because it kind of improves the",
    "start": "60879",
    "end": "67119"
  },
  {
    "text": "productivity of the engineers who use it not all of them but some of them for example it can help you unit test or",
    "start": "67119",
    "end": "73400"
  },
  {
    "text": "documentation or even do more complex tasks with the recent elements that we have now for example in this blog post",
    "start": "73400",
    "end": "79520"
  },
  {
    "text": "by Google AI uh I think it was in 2022 they found the six% reduction in code",
    "start": "79520",
    "end": "85240"
  },
  {
    "text": "iterations if you scale that to hundreds of thousands of Engineers that's this is",
    "start": "85240",
    "end": "90759"
  },
  {
    "text": "a significant gain of time and money uh this was very exciting GitHub",
    "start": "90759",
    "end": "96600"
  },
  {
    "text": "compilot aw code whisper and other models but the issue was that they were",
    "start": "96600",
    "end": "101799"
  },
  {
    "text": "only available through an API so we don't have the model checkpoints you can use the model to find unit on your own",
    "start": "101799",
    "end": "108079"
  },
  {
    "text": "use case you also don't have information on the data that was used to train these models so there isn't a lot of data",
    "start": "108079",
    "end": "114719"
  },
  {
    "text": "transparency around and also the code to do the training and data processing is not available so all of this makes these",
    "start": "114719",
    "end": "121479"
  },
  {
    "text": "results not reproducible but if you go to the huging face Hub where you can find all the open",
    "start": "121479",
    "end": "127399"
  },
  {
    "text": "source models and you search for the tag code you can see that now we have over 1,700 models that are trained on code",
    "start": "127399",
    "end": "134720"
  },
  {
    "text": "some of them are llms that included code in their training data but a lot of them are pure code completion models so we've",
    "start": "134720",
    "end": "141360"
  },
  {
    "text": "made a lot of progress in this field you might be wondering how did we get",
    "start": "141360",
    "end": "147360"
  },
  {
    "text": "here this is the product of the community work to train code models that not only can autocomplete your code but",
    "start": "147360",
    "end": "154640"
  },
  {
    "text": "that can also follow instructions and that we call them instruction tuned and if you go for example to this code",
    "start": "154640",
    "end": "160040"
  },
  {
    "text": "leaderboard that we made where you kind of rank code completion models and compare them on different programming",
    "start": "160040",
    "end": "165440"
  },
  {
    "text": "languages you can see that on some benchmarks the scores are pretty high for example solving more than 80% of the",
    "start": "165440",
    "end": "171400"
  },
  {
    "text": "problems in The Benchmark for some of uh the instruction models for code that are out there so there's a lot of progress",
    "start": "171400",
    "end": "177440"
  },
  {
    "text": "in this field and there are a lot of interesting models that you can use is but how are these models",
    "start": "177440",
    "end": "183840"
  },
  {
    "text": "trained if you want to train an llm in general or an nlm from Co for code from",
    "start": "183840",
    "end": "189480"
  },
  {
    "text": "scratch you should know that this requires a lot of resources you first you need to have a lot of gpus to be",
    "start": "189480",
    "end": "195440"
  },
  {
    "text": "able to train the models in a reasonable amount of time this can go from hundreds of gpus to thousands or even more you",
    "start": "195440",
    "end": "203040"
  },
  {
    "text": "also need to have a lot of data because generally these llms they require uh",
    "start": "203040",
    "end": "208080"
  },
  {
    "text": "terabytes of data to train them efficiently so we should be able to SC to scrape all of the this data to filter",
    "start": "208080",
    "end": "213959"
  },
  {
    "text": "it and then to train on it you should also be able to scale the performance because once you have the data it's it's",
    "start": "213959",
    "end": "219959"
  },
  {
    "text": "not like you're going to take the to figure out which filtering makes sense for your data set uh which hyperparameters make sense for the model",
    "start": "219959",
    "end": "226640"
  },
  {
    "text": "so that will require like a team who's able to dedicate effort and time to do all of these experiments so training",
    "start": "226640",
    "end": "232879"
  },
  {
    "text": "models from scratch are not from every for everyone it requires a lot of resources and if we look at the",
    "start": "232879",
    "end": "239040"
  },
  {
    "text": "technical details very quickly these models they usually all have the same architecture which is a Transformer",
    "start": "239040",
    "end": "244400"
  },
  {
    "text": "model and then first you start from a model that is untrained and then you show it a lot of data it becomes a",
    "start": "244400",
    "end": "251120"
  },
  {
    "text": "pre-trained base model and then you show it some data where you have ground truth",
    "start": "251120",
    "end": "256400"
  },
  {
    "text": "so this is called supervised fine tuning and after that you can do another step called lhf which is alignment to get the",
    "start": "256400",
    "end": "263759"
  },
  {
    "text": "model to hallucinate less uh to generate less biased content and toxic content",
    "start": "263759",
    "end": "269120"
  },
  {
    "text": "the aim is to to align it with human preferences and this is when you get a chat model for example like chat GPT",
    "start": "269120",
    "end": "274759"
  },
  {
    "text": "chat GPT went through all of these steps and for us we want code models so we're",
    "start": "274759",
    "end": "280400"
  },
  {
    "text": "not going to train on just the web we need to train the model on code where do we get that we get the data set from",
    "start": "280400",
    "end": "285800"
  },
  {
    "text": "GitHub and then if you want to use a build a model that can follow instructions then you need to build a",
    "start": "285800",
    "end": "291680"
  },
  {
    "text": "data set for the sft step and it looks like this for example you have an instruction write a function or solve a",
    "start": "291680",
    "end": "298240"
  },
  {
    "text": "bug and then you have the solution which are the ground truth and then you train your instruction tuned",
    "start": "298240",
    "end": "303520"
  },
  {
    "text": "model and that's how you get a chat llm but specific to",
    "start": "303520",
    "end": "309039"
  },
  {
    "text": "code um so I told you that you need a lot of resources to train these models from scratch and not everyone can do",
    "start": "309039",
    "end": "314919"
  },
  {
    "text": "that uh we're very proud to be part of these people who train these models for the community through a project called",
    "start": "314919",
    "end": "321199"
  },
  {
    "text": "Big code uh in this project we released the stack data set which is the largest open data set of source code we also",
    "start": "321199",
    "end": "328319"
  },
  {
    "text": "released two families of code generation models star coder and star coder 2 in",
    "start": "328319",
    "end": "333440"
  },
  {
    "text": "different sizes along with an instruction tuned model called Star Chat 2 other players in this field are meta",
    "start": "333440",
    "end": "339800"
  },
  {
    "text": "with their code llama models and deep seek coder with the Deep seek models there are also other models for example",
    "start": "339800",
    "end": "345240"
  },
  {
    "text": "from stability AI Coen from Salesforce and other llms so all of these models",
    "start": "345240",
    "end": "350680"
  },
  {
    "text": "are open it depends how open depends on the license that they are released under but you can find the checkpoints and you",
    "start": "350680",
    "end": "357039"
  },
  {
    "text": "can adopt them to your own use cases um so I've told you some of the things",
    "start": "357039",
    "end": "363240"
  },
  {
    "text": "that we developed in big code uh but what is actually big code it's a collaboration that is between hugging",
    "start": "363240",
    "end": "369479"
  },
  {
    "text": "face and service now and our goal was to have an open project where everyone can join and can help us train these models",
    "start": "369479",
    "end": "375840"
  },
  {
    "text": "in an open and transparent approach so for example in our slack Channel we have over a thousand people who joined these",
    "start": "375840",
    "end": "382199"
  },
  {
    "text": "are researchers Engineers but also lawyers and policy makers because we really also care about the data",
    "start": "382199",
    "end": "388240"
  },
  {
    "text": "governance and privacy aspect and we wanted to invest some time into that and the pillars of this project are",
    "start": "388240",
    "end": "394880"
  },
  {
    "text": "three full data transparency for example if you want to use our models you can know exactly on which data they were",
    "start": "394880",
    "end": "401160"
  },
  {
    "text": "trained this data is public and you can inspect it we also open source the code for processing the data set and also",
    "start": "401160",
    "end": "407599"
  },
  {
    "text": "training the models to try to make our work reproducible and the model weights are released under commercially friendly",
    "start": "407599",
    "end": "416240"
  },
  {
    "text": "license um in the motivation behind this project was to kind of uh uh try to",
    "start": "416240",
    "end": "422199"
  },
  {
    "text": "encourage better practices in the field of AI and the LMS in general uh these",
    "start": "422199",
    "end": "427639"
  },
  {
    "text": "practices are not always respected in the closed Source field for example you might have a model whose weights are",
    "start": "427639",
    "end": "433680"
  },
  {
    "text": "public but the details about the training data are not disclosed uh this might be because they're afraid of uh",
    "start": "433680",
    "end": "440599"
  },
  {
    "text": "lwuit but also to not give up their Competitive Edge to others uh sometimes",
    "start": "440599",
    "end": "445919"
  },
  {
    "text": "the model weights are not public for example which is the case for chpt and if you want to use these models or for",
    "start": "445919",
    "end": "451639"
  },
  {
    "text": "example even use GitHub Coop Pilots the issue is that you will have to send your data to a third party because you're",
    "start": "451639",
    "end": "457240"
  },
  {
    "text": "just qu an API and in a lot of cases your data might be sensitive so you don't want it to be sent to third",
    "start": "457240",
    "end": "463599"
  },
  {
    "text": "parties that's when you want something that is deployed on premise and that is secure and needless to say that all of",
    "start": "463599",
    "end": "470039"
  },
  {
    "text": "this makes this work not reproducible and doesn't um encourage progress in the open source",
    "start": "470039",
    "end": "476759"
  },
  {
    "text": "field uh what we're trying to achieve with this project is to have for example public data the data that we trained on",
    "start": "476759",
    "end": "483800"
  },
  {
    "text": "is available and people can inspect it if they want to be removed and not be included in our future trainings they",
    "start": "483800",
    "end": "490000"
  },
  {
    "text": "can just fill a form and opt out uh and the model weights are public for fine-tuning and you can also deploy them",
    "start": "490000",
    "end": "497039"
  },
  {
    "text": "in premise this is the data set we released uh how we built it that we basically",
    "start": "497039",
    "end": "502960"
  },
  {
    "text": "scraped all of GitHub and then we filtered the data set for licenses we can use and then we did additional",
    "start": "502960",
    "end": "508639"
  },
  {
    "text": "filtering like move in files that look similar in a step called the duplication we also have a tool called m in the",
    "start": "508639",
    "end": "514839"
  },
  {
    "text": "stack where you can go you can just type your GitHub username and you can check if any of your repositories are in our",
    "start": "514839",
    "end": "520479"
  },
  {
    "text": "data sets if you don't want to be in the data set and in the model trainings you can just fill a form and will make sure",
    "start": "520479",
    "end": "526720"
  },
  {
    "text": "to not use your data um this is the first model that we",
    "start": "526720",
    "end": "531800"
  },
  {
    "text": "trained uh it was released last April it's called star coder it has 15 billion parameters and it was trained on 500 a00",
    "start": "531800",
    "end": "540200"
  },
  {
    "text": "for 24 days and when it was released it was the best code generation",
    "start": "540200",
    "end": "545560"
  },
  {
    "text": "model um last month we released a new data set called the stack V2 which is uh",
    "start": "545560",
    "end": "551399"
  },
  {
    "text": "much larger than the first version of the stack and also still the largest open data sets of source code in",
    "start": "551399",
    "end": "557560"
  },
  {
    "text": "collaboration with software Heritage and we also trained a new model called star coder 2 which is much better than star",
    "start": "557560",
    "end": "564560"
  },
  {
    "text": "coder and also better than a lot of other models there so for example if you want a 50 B model the best model is",
    "start": "564560",
    "end": "570920"
  },
  {
    "text": "Soder 2 it even outperforms other models like uh uh I think it's not here but",
    "start": "570920",
    "end": "576440"
  },
  {
    "text": "it's Cod Lama 34d and it's good on code but also math",
    "start": "576440",
    "end": "581720"
  },
  {
    "text": "uh another thing we added in this model compared to Star coder one is report context so before when we were training",
    "start": "581720",
    "end": "588320"
  },
  {
    "text": "we would just grab files from GitHub and randomly concatenate them and train on",
    "start": "588320",
    "end": "593399"
  },
  {
    "text": "them so we kind of lost the repository level structure but for Star coder 2 we made sure to keep files that are in the",
    "start": "593399",
    "end": "599720"
  },
  {
    "text": "same repository next to each other during the training so this starer 2 model is aware of repository context and",
    "start": "599720",
    "end": "605959"
  },
  {
    "text": "if you were to use it for example in a vs code extension and concatenate files from your repositories it can give you",
    "start": "605959",
    "end": "611519"
  },
  {
    "text": "completions that are in other files and not necessarily in the file that you're",
    "start": "611519",
    "end": "616600"
  },
  {
    "text": "editing uh we also built an instruction tuned version of star coder 2 called",
    "start": "616600",
    "end": "621839"
  },
  {
    "text": "Star Chat 2 in collaboration with hugging face H4 team and this model is available on uh on a space at hugging",
    "start": "621839",
    "end": "628760"
  },
  {
    "text": "face I think I put the link there and you can query it it can also complete your code but it can also follow",
    "start": "628760",
    "end": "634440"
  },
  {
    "text": "instructions not just on python but also on other programming languages because star coder true was trained on more than",
    "start": "634440",
    "end": "641000"
  },
  {
    "text": "600 programming languages um this kind of creates a big",
    "start": "641000",
    "end": "647680"
  },
  {
    "text": "code ecosystem where people would take the data set that we released because it has so many programming languages and",
    "start": "647680",
    "end": "653480"
  },
  {
    "text": "develop new models on top of it so this can be new pre-training from scratch for example like St code or codeen 2.5 and",
    "start": "653480",
    "end": "661399"
  },
  {
    "text": "other models or people would just start from the models released for example star coder and star coder base and they",
    "start": "661399",
    "end": "667519"
  },
  {
    "text": "would fine- tune them for their own use cases for example here you can see that there's wizard coder which is an",
    "start": "667519",
    "end": "672839"
  },
  {
    "text": "instruct tuned version of star coder there's also defog SQL coder which is very interesting because it outperforms",
    "start": "672839",
    "end": "679560"
  },
  {
    "text": "gbt 4 on SQL and they just started from Star coder and they find you in this on a lot of human annotated data for SQL",
    "start": "679560",
    "end": "687040"
  },
  {
    "text": "and they were able to outperform gbt for on that so this shows that uh the power",
    "start": "687040",
    "end": "692399"
  },
  {
    "text": "of Open Source when you release models and tools for the community they can use them and build new things that even you",
    "start": "692399",
    "end": "698480"
  },
  {
    "text": "haven't thought of um now let's see how you can go from",
    "start": "698480",
    "end": "704320"
  },
  {
    "text": "a model to an API or a tool like vs code extension so first you have the model",
    "start": "704320",
    "end": "710040"
  },
  {
    "text": "and then you wrap it around an inference end point with an API then you have a chat interface and each time you go to",
    "start": "710040",
    "end": "716560"
  },
  {
    "text": "another level you need to add new parameters for example when you go to the mo from the model to the API you",
    "start": "716560",
    "end": "722560"
  },
  {
    "text": "need to add moderation in compute uh if when you go to the chat interface you need to have a system prompt for the",
    "start": "722560",
    "end": "728440"
  },
  {
    "text": "model to kind of control Its Behavior you also need to have some hyperparameters to have a very nice user",
    "start": "728440",
    "end": "734279"
  },
  {
    "text": "experience so this is for chat models in general if we go to code models you can",
    "start": "734279",
    "end": "740000"
  },
  {
    "text": "maybe just swap the last component of a chat model with a code interface that could be Vis code extension or jet",
    "start": "740000",
    "end": "746360"
  },
  {
    "text": "brains or something like that um hugging face released two things for",
    "start": "746360",
    "end": "751920"
  },
  {
    "text": "that hugging chat which is like chat GPT but only uses open source models it's free to use there's we also have an",
    "start": "751920",
    "end": "758680"
  },
  {
    "text": "extension called llms vs code and uh we deployed star coder there and we also",
    "start": "758680",
    "end": "764360"
  },
  {
    "text": "deployed other open source other Open Access models like code Lama and deeps you can all find them there if you want",
    "start": "764360",
    "end": "770760"
  },
  {
    "text": "to use an alternative to GitHub copilot that uses open models uh now let's go back to the slide",
    "start": "770760",
    "end": "779360"
  },
  {
    "text": "um I said that to train codee models from scratch you need a lot of resources but the good news is that if you only",
    "start": "779360",
    "end": "785800"
  },
  {
    "text": "want to customize existing models to your codebase or to new data sets you don't need as much",
    "start": "785800",
    "end": "792240"
  },
  {
    "text": "resources and this is possible thanks to fine tuning and uh the reason why we would",
    "start": "792240",
    "end": "798680"
  },
  {
    "text": "like to fine tune and not just train from scratch might be related to the resources if you don't have enough gpus",
    "start": "798680",
    "end": "805959"
  },
  {
    "text": "or if you don't have enough data because training from scratch it requires a lot of data to get good performance or if",
    "start": "805959",
    "end": "811440"
  },
  {
    "text": "the model that is already out there is good at your task but you just want to improve the performance a little bit you",
    "start": "811440",
    "end": "816560"
  },
  {
    "text": "want to reduce the hallucinations you want to include information that is more up to date that's when you would go to",
    "start": "816560",
    "end": "822880"
  },
  {
    "text": "other Solutions than just training from scratch um if you want to adapt an",
    "start": "822880",
    "end": "828480"
  },
  {
    "text": "existing code llm or llms in general to a new task um there are some less",
    "start": "828480",
    "end": "834160"
  },
  {
    "text": "intensive ways and other ways that are more intensive the easiest one would",
    "start": "834160",
    "end": "839560"
  },
  {
    "text": "would be to just do prompt engineering you would take a chat model and just tweak the prompt to get it to solve your",
    "start": "839560",
    "end": "845600"
  },
  {
    "text": "task another thing that is a little bit more complex is in context learning where you would add examples in the",
    "start": "845600",
    "end": "852040"
  },
  {
    "text": "prompts to kind of teach the model new behavior for example there are a lot of papers who are trying to use uh models",
    "start": "852040",
    "end": "858519"
  },
  {
    "text": "for example chat GPT and try to teach it new libraries that are probably were not included during the training and just by",
    "start": "858519",
    "end": "865240"
  },
  {
    "text": "adding documentation to the context to see if the model is able to pick up new skills that it wasn't trained on another",
    "start": "865240",
    "end": "872320"
  },
  {
    "text": "thing is tool use I'm going to show later how to do that and then there's fine tuning and then there's continued",
    "start": "872320",
    "end": "878759"
  },
  {
    "text": "pre-training which is like fine tuning but it is so much longer for prompt engineering as I",
    "start": "878759",
    "end": "884040"
  },
  {
    "text": "explained you just need to change the prompt and add instructions to uh kind of get the model to follow what you want",
    "start": "884040",
    "end": "891040"
  },
  {
    "text": "and some examples are for example few shots where you add examples of the task you want there's also Chain of Thought",
    "start": "891040",
    "end": "897839"
  },
  {
    "text": "where you try to get the model to solve a problem using stepbystep reasoning this is for example very useful for math",
    "start": "897839",
    "end": "904519"
  },
  {
    "text": "problems where instead of just asking the model to solve it you ask it to reason step by step and kind of split",
    "start": "904519",
    "end": "910600"
  },
  {
    "text": "the mo the problem to smaller easier problems the other thing that is also",
    "start": "910600",
    "end": "917079"
  },
  {
    "text": "very interesting to use with Co code llms is tool use uh for example if you",
    "start": "917079",
    "end": "922759"
  },
  {
    "text": "pick a general code LM or just LM in general they might be very bad that's arithmetic and if you can plug in a",
    "start": "922759",
    "end": "929480"
  },
  {
    "text": "calculator to do the competitions for you or just uh add an interpreter to interpret the code that might help you",
    "start": "929480",
    "end": "936199"
  },
  {
    "text": "improve the performance a lot other techniques that are used are for example rag retrieval you can do that with the",
    "start": "936199",
    "end": "943000"
  },
  {
    "text": "chat code LMS if you want to if you wanted to retrieve some documentation and add it to the context or if you want",
    "start": "943000",
    "end": "948639"
  },
  {
    "text": "to add more upto-date information to the context um and for fine tuning you would",
    "start": "948639",
    "end": "956880"
  },
  {
    "text": "start from a base PR trained model and then you can show it some sft data",
    "start": "956880",
    "end": "962000"
  },
  {
    "text": "for example for llms this can be any domain Finance medical domain or",
    "start": "962000",
    "end": "967160"
  },
  {
    "text": "summarization and then the model is able to pick up this task the only drawback is that like you need to have this sft",
    "start": "967160",
    "end": "973639"
  },
  {
    "text": "data that kind of has pairs of questions and answers to be able to train the model on that so if you have it you",
    "start": "973639",
    "end": "979639"
  },
  {
    "text": "might be able to adapt the model to your own use case um now let's see what are some good",
    "start": "979639",
    "end": "988199"
  },
  {
    "text": "practices if if you have a code base and you want to take an existing code llm and adopt it to your own code base in",
    "start": "988199",
    "end": "995399"
  },
  {
    "text": "here I'm going to put this blog po which is very interesting uh by Engineers as huging face who try to build a personal",
    "start": "995399",
    "end": "1001720"
  },
  {
    "text": "co-pilot so they took some of the hugging face libraries that are were",
    "start": "1001720",
    "end": "1007199"
  },
  {
    "text": "released after star coder training and they built a new code base and got the model to learn these new libraries and",
    "start": "1007199",
    "end": "1013360"
  },
  {
    "text": "to kind of follow our practices for coding and in this blog post they go through the steps of data processing",
    "start": "1013360",
    "end": "1020279"
  },
  {
    "text": "model fine-tuning and also provide the code to be able to reproduce their",
    "start": "1020279",
    "end": "1026160"
  },
  {
    "text": "results if you want to do that I think the first step is you need to prepare your data set um so the steps for",
    "start": "1026160",
    "end": "1033600"
  },
  {
    "text": "preparing the data set are very similar to the steps that we use for pre-training when we were pre-training",
    "start": "1033600",
    "end": "1039319"
  },
  {
    "text": "star coder we first gathered all this data from GitHub and then we did some filtering to kind of remove files that",
    "start": "1039319",
    "end": "1046199"
  },
  {
    "text": "wouldn't help the model during PR training and we did another filtering that is very important it's called the duplication where you remove files that",
    "start": "1046199",
    "end": "1053039"
  },
  {
    "text": "look similar because for llms this really hurts their performance if you show them files that are very similar",
    "start": "1053039",
    "end": "1058760"
  },
  {
    "text": "early in the training so it's very important to remove duplicates before you start the training and for filtering",
    "start": "1058760",
    "end": "1065039"
  },
  {
    "text": "we I think if you have a custom code base maybe you already are cleaning it but if it's not the case um you might",
    "start": "1065039",
    "end": "1071039"
  },
  {
    "text": "want to have some filters to remove autogenerated files and uh maybe filter configs filter SSV data because that's",
    "start": "1071039",
    "end": "1078200"
  },
  {
    "text": "not something that's is going to teach the model how to code after you've done all this filtering then you can tokenize",
    "start": "1078200",
    "end": "1084799"
  },
  {
    "text": "the data set um oh sorry I forgot another step which is pii detection um",
    "start": "1084799",
    "end": "1090760"
  },
  {
    "text": "because in public repositories although people shouldn't put their SSH any API Keys you must be surprised that there's",
    "start": "1090760",
    "end": "1097280"
  },
  {
    "text": "still a lot of them on public repositories on GitHub so we try to filter all of them and we release the",
    "start": "1097280",
    "end": "1102559"
  },
  {
    "text": "tools that we used we also try to remove names on the in the emails so you might also want to do that before you TR your",
    "start": "1102559",
    "end": "1109080"
  },
  {
    "text": "model otherwise you might risk having a model that would spit out an API key or",
    "start": "1109080",
    "end": "1114280"
  },
  {
    "text": "someone's name when you use it during inference um I put here some links that you can use uh to check how to train",
    "start": "1114280",
    "end": "1122320"
  },
  {
    "text": "these kind of models we also have resources for data D duplication with the library called data",
    "start": "1122320",
    "end": "1129080"
  },
  {
    "text": "drob and we also have published all the code that we used for our own data pre processing for Star coder",
    "start": "1129080",
    "end": "1136640"
  },
  {
    "text": "models uh let me go back to this pii reduction step because it might be interesting for you um first we started",
    "start": "1136640",
    "end": "1143880"
  },
  {
    "text": "by using regexes to detect these keys and names and emails but the rxes were",
    "start": "1143880",
    "end": "1149640"
  },
  {
    "text": "not very good at catching some types of keys so we tried to hire human annotators to kind of annotate the data",
    "start": "1149640",
    "end": "1156960"
  },
  {
    "text": "set for secrets and pii and we released this data set it is gated you have to",
    "start": "1156960",
    "end": "1162960"
  },
  {
    "text": "fill form because we don't want to expose people's information and then you can train we train the name the entity",
    "start": "1162960",
    "end": "1168200"
  },
  {
    "text": "recognition model you just show it a file and it is able to detect where is the name where is an email and where's",
    "start": "1168200",
    "end": "1173679"
  },
  {
    "text": "the key and then we run all of that on our data set it was quite intensive",
    "start": "1173679",
    "end": "1178720"
  },
  {
    "text": "because it's still an ner model it's a neural network and if you want to run it on terabytes of data you need a lot of",
    "start": "1178720",
    "end": "1184480"
  },
  {
    "text": "gpus so for us I think it cost around 800 GPU hours on a 100 machines uh but",
    "start": "1184480",
    "end": "1191440"
  },
  {
    "text": "if your data base is smaller maybe it will be faster to run this pipeline the code is also available on the big code",
    "start": "1191440",
    "end": "1198039"
  },
  {
    "text": "project repository um one other thing that you",
    "start": "1198039",
    "end": "1203520"
  },
  {
    "text": "might want to do is fine tune in but it's not just fine tuning on 10,000 samples or 100,000 samples it's fine",
    "start": "1203520",
    "end": "1210320"
  },
  {
    "text": "tuning on a data set that is kind of large itself and uh why would you want to do that for example if you want to",
    "start": "1210320",
    "end": "1217000"
  },
  {
    "text": "take a model uh that is very good at python but maybe not very good at Swift",
    "start": "1217000",
    "end": "1222120"
  },
  {
    "text": "and I think most models now kind of focus on python because it's um the default programming language for machine",
    "start": "1222120",
    "end": "1228880"
  },
  {
    "text": "learning and there's a lot of hypee around it but maybe you want to adopt it for a low resource language and if you",
    "start": "1228880",
    "end": "1235799"
  },
  {
    "text": "have a data set for that you can train the model on this data set in this stack",
    "start": "1235799",
    "end": "1241280"
  },
  {
    "text": "we also have a lot of programming languages so you can take one of these languages and continue training the star",
    "start": "1241280",
    "end": "1246720"
  },
  {
    "text": "coder models for example on them uh a lot of people try doing that and it seems to help for some low resource",
    "start": "1246720",
    "end": "1253000"
  },
  {
    "text": "languages there are even benchmarks to test the performance not just on python",
    "start": "1253000",
    "end": "1258919"
  },
  {
    "text": "but also on these languages there are even data sets that were developed for these low reson languages like multi PLT",
    "start": "1258919",
    "end": "1265440"
  },
  {
    "text": "data set um so we saw how to findun on your",
    "start": "1265440",
    "end": "1271679"
  },
  {
    "text": "custom code base and also how to do continued pre trining on data set that is a little bit larger and now we'll see",
    "start": "1271679",
    "end": "1278559"
  },
  {
    "text": "how to build a chat assistant so if you have a code model and you want to turn it into a chat assistant that could help",
    "start": "1278559",
    "end": "1285240"
  },
  {
    "text": "you with your coding related questions you need to follow what we call instruction tuning you need to have a",
    "start": "1285240",
    "end": "1290880"
  },
  {
    "text": "data set of instructions and all the answers to these instructions and this could be used for bug fixing or for just",
    "start": "1290880",
    "end": "1298480"
  },
  {
    "text": "code completion to or a lot of other use cases here there's a paper called Magic coder where they try to use uh just",
    "start": "1298480",
    "end": "1305679"
  },
  {
    "text": "normal GitHub code uh from the stack and they kind of got used another llm in",
    "start": "1305679",
    "end": "1311240"
  },
  {
    "text": "their case it was gp4 to generate instructions and the answers to these instructions just from some code file",
    "start": "1311240",
    "end": "1319279"
  },
  {
    "text": "and this is a good approach because if you were to just select few topics maybe you will lose in diversity but but",
    "start": "1319279",
    "end": "1325200"
  },
  {
    "text": "because you start from General code data set you kind of get files for different topics and you just need an llm that can",
    "start": "1325200",
    "end": "1332799"
  },
  {
    "text": "generate good instructions and solutions that actually work I think they even try to execute the solutions to see if they",
    "start": "1332799",
    "end": "1339360"
  },
  {
    "text": "are correct and they don't generate errors and they kind of have a refinement pipeline to only keep the instructions and solutions that are",
    "start": "1339360",
    "end": "1345720"
  },
  {
    "text": "actually relevant and that will help them during the training there's also the code interpreter paper",
    "start": "1345720",
    "end": "1353080"
  },
  {
    "text": "where they uh also try to do some code interpretation to increase the quality and remove the files and the",
    "start": "1353080",
    "end": "1359200"
  },
  {
    "text": "instructions that fail uh if you see their evaluations they are able to get some pretty good performances with their",
    "start": "1359200",
    "end": "1367960"
  },
  {
    "text": "approach um and for example in Our Star Chat to model how we built the",
    "start": "1368919",
    "end": "1374320"
  },
  {
    "text": "instruction tuning data set we just took a lot of instruction data sets not not all of them were made for code some of",
    "start": "1374320",
    "end": "1380880"
  },
  {
    "text": "them are just general instructions for example write me a poem uh who is the president of the United States so these",
    "start": "1380880",
    "end": "1386760"
  },
  {
    "text": "are instruction data sets that are used for llms but it also helps to include them with other instruction data sets",
    "start": "1386760",
    "end": "1393279"
  },
  {
    "text": "that are just dedicated to code this way your model doesn't lose its ability to know normal facts and also",
    "start": "1393279",
    "end": "1401840"
  },
  {
    "text": "English um the model is available in this demo as well as all the data set that we included and there's even an",
    "start": "1402799",
    "end": "1409760"
  },
  {
    "text": "alignment handbook where you can use these techniques to do the fine tuning and one other good thing about like fine",
    "start": "1409760",
    "end": "1416159"
  },
  {
    "text": "tuning techniques is that recently they are very cheap you can find techniques like Laura and PFT and you can find tune",
    "start": "1416159",
    "end": "1422960"
  },
  {
    "text": "7B models in just few hours on one GPU compared to two years ago it was so much",
    "start": "1422960",
    "end": "1428039"
  },
  {
    "text": "expensive to train these models because you basically needed to train all of the weights but now it's really",
    "start": "1428039",
    "end": "1435480"
  },
  {
    "text": "manageable um so we saw how these models are trained how you can fine- tune them but you might be asking yourselves okay",
    "start": "1435960",
    "end": "1442679"
  },
  {
    "text": "I trained a model how do you know how do I know it's good um how the models are evaluated in the code domain is that we",
    "start": "1442679",
    "end": "1450720"
  },
  {
    "text": "have some benchmarks and we test on these benchmarks to see what the score",
    "start": "1450720",
    "end": "1456080"
  },
  {
    "text": "what is the score that we would get for example there's a benchmark called human eval and it is basically some function",
    "start": "1456080",
    "end": "1461640"
  },
  {
    "text": "signatures and you ask the model to complete the function implementation and then in the Benchmark you have unit",
    "start": "1461640",
    "end": "1468440"
  },
  {
    "text": "tests for each problem and then you execute the solution that your model generated against these unit tests to",
    "start": "1468440",
    "end": "1475320"
  },
  {
    "text": "see if any of them pass and then you kind of report an average of the problems that pass um this is a lot in",
    "start": "1475320",
    "end": "1483360"
  },
  {
    "text": "intensive compared to just natural language evaluation because you need to First generate the solutions and then",
    "start": "1483360",
    "end": "1488720"
  },
  {
    "text": "you need to execute them which might take some time luckily the Benchmark is not very big but that's also a drawback",
    "start": "1488720",
    "end": "1495279"
  },
  {
    "text": "because you only test on a smaller on a small benchmark and it might be hard to see the differences between models that",
    "start": "1495279",
    "end": "1501480"
  },
  {
    "text": "are very close just because you don't have a lot of problems in your benchmark there are some other benchmarks that are",
    "start": "1501480",
    "end": "1506840"
  },
  {
    "text": "larger uh but they all require this generation then execution step one thing to be also aware of is that what you're",
    "start": "1506840",
    "end": "1513520"
  },
  {
    "text": "going to execute is code that is generated by an llm it might contain malicious code so we don't want to",
    "start": "1513520",
    "end": "1520240"
  },
  {
    "text": "execute it on your machine it's better to do it on a Sandbox or at least a Docker uh to make sure it doesn't alter",
    "start": "1520240",
    "end": "1527520"
  },
  {
    "text": "your system and this Benchmark is not just for python it's uh I think it was translated",
    "start": "1527520",
    "end": "1534880"
  },
  {
    "text": "to 18 other programming languages so we can find it for example even for Lua Swift and",
    "start": "1534880",
    "end": "1541559"
  },
  {
    "text": "rust and what I showed you was a leaderboard to kind of rank these models to compare them there are a lot of",
    "start": "1541559",
    "end": "1547440"
  },
  {
    "text": "leader boards for code now there's also this one called eval plus where they compared not just open models between",
    "start": "1547440",
    "end": "1553600"
  },
  {
    "text": "each other but they also compare to other closed models like clo 3 and GPT 4",
    "start": "1553600",
    "end": "1560240"
  },
  {
    "text": "there's also this Live code bench uh and it is very interesting because a lot of",
    "start": "1561279",
    "end": "1566600"
  },
  {
    "text": "people argue that maybe some of the code models are doing very well on the benchmarks because they were trained on",
    "start": "1566600",
    "end": "1572919"
  },
  {
    "text": "them this is what we call data leakage or data contamination because everyone could have taken a benchmark and put it",
    "start": "1572919",
    "end": "1579640"
  },
  {
    "text": "on GitHub so if you just scrape GitHub and you train your model there's a very high chance that you probably trained on",
    "start": "1579640",
    "end": "1585360"
  },
  {
    "text": "the test in Benchmark and so it's very normal to have model that is good at The Benchmark you're testing on um usually",
    "start": "1585360",
    "end": "1592039"
  },
  {
    "text": "we try to do decontamination for example for the start coder models we checked all of our training data to make sure",
    "start": "1592039",
    "end": "1598039"
  },
  {
    "text": "that none of the test benchmarks are included but this is very hard to check if people don't say that they explicitly",
    "start": "1598039",
    "end": "1604279"
  },
  {
    "text": "did that in the paper and now there are a lot of models who say nothing about the data processing pipeline so this",
    "start": "1604279",
    "end": "1610240"
  },
  {
    "text": "leaderboard kind of tries to only use very recent repositories on GitHub and",
    "start": "1610240",
    "end": "1615440"
  },
  {
    "text": "build the Benchmark out of them so there's a very high a very low chance that the current code llm saw what is in",
    "start": "1615440",
    "end": "1622200"
  },
  {
    "text": "this Benchmark and you can they also compare the open models but also the closed",
    "start": "1622200",
    "end": "1629000"
  },
  {
    "text": "models if you trained the an llm a code LM on your custom code base and you want",
    "start": "1629360",
    "end": "1635000"
  },
  {
    "text": "to evaluate it I think you should first start by the standard benchmarks to make sure you're not losing performance on",
    "start": "1635000",
    "end": "1641080"
  },
  {
    "text": "the language you're training it on but if you did the fine tuning for a specific use case for example you want",
    "start": "1641080",
    "end": "1646679"
  },
  {
    "text": "the model to follow a certain style when programming you probably want to build a new Benchmark that would test this",
    "start": "1646679",
    "end": "1653840"
  },
  {
    "text": "ability for that you would need to have human annotators or you can use other powerful llms maybe even closed to build",
    "start": "1653840",
    "end": "1661200"
  },
  {
    "text": "this Benchmark a lot of people are doing this because it's not enough to just run on the standard benchmarks that don't",
    "start": "1661200",
    "end": "1666919"
  },
  {
    "text": "necessarily test what you want to implement another thing is to do Just Vibe checks uh test the model and see",
    "start": "1666919",
    "end": "1673799"
  },
  {
    "text": "what it generates and maybe the most efficient one would be to deploy the model and then have some users tested for",
    "start": "1673799",
    "end": "1680880"
  },
  {
    "text": "example you could deploy the model in a vs code extension and then have number of program of software Engineers tested",
    "start": "1680880",
    "end": "1687600"
  },
  {
    "text": "and then you can use metrics like the acceptance rate for example how many times is the code generated by the model",
    "start": "1687600",
    "end": "1694360"
  },
  {
    "text": "accepted by the user if this acceptance rate acceptance rate is high it means",
    "start": "1694360",
    "end": "1699519"
  },
  {
    "text": "that probably the generations are useful and uh the people want to keep them so you can do multiple iterations and see",
    "start": "1699519",
    "end": "1706760"
  },
  {
    "text": "uh when the acceptance rates declines or improves to kind of Judge the quality of the models you just trained and deployed",
    "start": "1706760",
    "end": "1714240"
  },
  {
    "text": "but this is still kind of an intensive approach but if you want to really test if your fine is working you need to go",
    "start": "1714240",
    "end": "1719799"
  },
  {
    "text": "through that um the last thing we're going to see is how these llms are served um so",
    "start": "1719799",
    "end": "1728200"
  },
  {
    "text": "you can train them fine-tune them evaluate them but then you need to serve them and maybe deploy them to hundreds",
    "start": "1728200",
    "end": "1733880"
  },
  {
    "text": "of users to hundreds of thousands so depending on your use case you might choose some options or others we have",
    "start": "1733880",
    "end": "1742159"
  },
  {
    "text": "some inference end points that you can use if you don't have an engineering team that will do all the envelops uh",
    "start": "1742159",
    "end": "1748399"
  },
  {
    "text": "side of stuff you can just uh purchase an endpoint and will take care of the deployment you can just query this",
    "start": "1748399",
    "end": "1755360"
  },
  {
    "text": "endpoint and pay as you go otherwise we have an open source Library called text",
    "start": "1755360",
    "end": "1760679"
  },
  {
    "text": "generation inference uh this Library tries to T the most popular models and deploy them in a",
    "start": "1760679",
    "end": "1766880"
  },
  {
    "text": "very efficient approach uh so you have your gpus you can use this library in the model and then be",
    "start": "1766880",
    "end": "1773279"
  },
  {
    "text": "able to serve it and what's interesting in TGI is that it has tensor parm implemented so if the model is very",
    "start": "1773279",
    "end": "1780279"
  },
  {
    "text": "large and it cannot fit in one GPU it can split the model on multiple shards",
    "start": "1780279",
    "end": "1786039"
  },
  {
    "text": "and be able to load it for example we use this with very large models like Falcon 180 billion parameters in which",
    "start": "1786039",
    "end": "1792519"
  },
  {
    "text": "was able to work efficiently it also has a token streaming it means that when the",
    "start": "1792519",
    "end": "1798880"
  },
  {
    "text": "you send a query or a prompt you have the option of like not waiting until the generation has ended and then sh in",
    "start": "1798880",
    "end": "1805480"
  },
  {
    "text": "token stream is that like you show the tokens and the text as it is generated and this is very useful because it",
    "start": "1805480",
    "end": "1811559"
  },
  {
    "text": "improves the user experience user doesn't have to wait until the full generation has ended but it can also see",
    "start": "1811559",
    "end": "1817679"
  },
  {
    "text": "the generations as they come and if it doesn't the user doesn't like what is being generated they can just stop so",
    "start": "1817679",
    "end": "1824480"
  },
  {
    "text": "this kind of reduces the perceived lency uh there also metrics for monitoring ingi and it is production",
    "start": "1824480",
    "end": "1831840"
  },
  {
    "text": "ready once you launch it you and you tested it you're sure that it's not going to fail in production it is what",
    "start": "1831840",
    "end": "1838000"
  },
  {
    "text": "is being used now in our inference stand points uh it also has techniques for quantization and optimization and it",
    "start": "1838000",
    "end": "1845240"
  },
  {
    "text": "supports most of the popular code llms but also General llms there are other",
    "start": "1845240",
    "end": "1850559"
  },
  {
    "text": "open source libraries by the community that are similar for example VM which also offers uh similar implementations",
    "start": "1850559",
    "end": "1858000"
  },
  {
    "text": "so you can test both and see which one is good for your use case um if you want to explore more",
    "start": "1858000",
    "end": "1865240"
  },
  {
    "text": "models in data sets related to code you can go and search on the Hub and find the ones that are relevant for your use",
    "start": "1865240",
    "end": "1871639"
  },
  {
    "text": "case you can also build demos to Showcase your models and test them and use gpus if you don't have",
    "start": "1871639",
    "end": "1879399"
  },
  {
    "text": "them um now let's see what are the future directions of this field I think",
    "start": "1879399",
    "end": "1884840"
  },
  {
    "text": "what we really need is to have better open-source models and high quality data sets even though",
    "start": "1884840",
    "end": "1890480"
  },
  {
    "text": "we made a lot of progress we're still for example behind gbt 4 in code completion and other models like clo 3",
    "start": "1890480",
    "end": "1897799"
  },
  {
    "text": "so we still need to investigate a little bit more what is missing to kind of catch up and close the gap to closed",
    "start": "1897799",
    "end": "1903679"
  },
  {
    "text": "Source uh we also need high quality data sets so for example the stack was a good",
    "start": "1903679",
    "end": "1908919"
  },
  {
    "text": "step towards democratizing training code models because it is now available and everyone can pre-train code models from",
    "start": "1908919",
    "end": "1915760"
  },
  {
    "text": "scratch if they have the resources we also need better data transparency and governance that means telling the users",
    "start": "1915760",
    "end": "1922919"
  },
  {
    "text": "exactly which data was trained uh was used for the training and also alerting about the possible biases in the data",
    "start": "1922919",
    "end": "1929519"
  },
  {
    "text": "set regarding uh privacy and security we also need better evaluations that not",
    "start": "1929519",
    "end": "1934679"
  },
  {
    "text": "only focus on the high resource languages but also the low resource ones and eval currently we focus on",
    "start": "1934679",
    "end": "1941399"
  },
  {
    "text": "functional level so there are some benchmarks to test that the repository context works that the model can",
    "start": "1941399",
    "end": "1948200"
  },
  {
    "text": "retrieve information from another file and generate it uh we also need better",
    "start": "1948200",
    "end": "1953559"
  },
  {
    "text": "evaluations that test also class implementations not just functions overall just evaluation that catch",
    "start": "1953559",
    "end": "1959120"
  },
  {
    "text": "things that are more complex now we're making some progress towards that but I think there's still room for",
    "start": "1959120",
    "end": "1964639"
  },
  {
    "text": "improvements um and the last thing is that like we need better smaller specialized models but not not everyone",
    "start": "1964639",
    "end": "1970320"
  },
  {
    "text": "has enough resources to deploy 15 billion model or 7B model so if we could generate smaller models that are better",
    "start": "1970320",
    "end": "1976840"
  },
  {
    "text": "that would be good too um so here I try to show you a little",
    "start": "1976840",
    "end": "1982320"
  },
  {
    "text": "bit how you go from just data on GitHub to actually products that you can use",
    "start": "1982320",
    "end": "1987360"
  },
  {
    "text": "for example hugging chat and the vs code that extension that we have and we saw that between the two you kind of have to",
    "start": "1987360",
    "end": "1993960"
  },
  {
    "text": "do a lot of things not just train on a lot of gpus but you have to do a lot of data curation data governance you have",
    "start": "1993960",
    "end": "2000919"
  },
  {
    "text": "to also work on inference to be able to serve a lot of users in evaluation and also fine tuning",
    "start": "2000919",
    "end": "2008799"
  },
  {
    "text": "uh so thank you all for your attention and uh if you have any questions uh feel free to ask them and you can also leave",
    "start": "2008799",
    "end": "2015120"
  },
  {
    "text": "us some",
    "start": "2015120",
    "end": "2017480"
  },
  {
    "text": "feedback thank you Lua any questions there's one",
    "start": "2023559",
    "end": "2030360"
  },
  {
    "text": "here all right thank you for your excellent talk uh I've just uh got one question that has uh bothered me for a",
    "start": "2030440",
    "end": "2037120"
  },
  {
    "text": "while um all the code that you suck in somehow from GitHub you say uh you take",
    "start": "2037120",
    "end": "2043039"
  },
  {
    "text": "care that the licenses are okay so I guess there will be no GPL code in your",
    "start": "2043039",
    "end": "2049720"
  },
  {
    "text": "uh or in all those uh large language models but what about other licenses and",
    "start": "2049720",
    "end": "2055200"
  },
  {
    "text": "what is a permissive license because almost all the licenses I know say that",
    "start": "2055200",
    "end": "2060280"
  },
  {
    "text": "you must leave intact copyright header that you must add uh the the the license",
    "start": "2060280",
    "end": "2066398"
  },
  {
    "text": "file to the code that you producing uh or is there any discussions in the",
    "start": "2066399",
    "end": "2072079"
  },
  {
    "text": "community how to deal with that yeah that's a very good question if you use MIT or apach to that doesn't mean that",
    "start": "2072079",
    "end": "2077440"
  },
  {
    "text": "you shouldn't attribute the author if the model generates exactly the same code that's something we thought about",
    "start": "2077440",
    "end": "2083079"
  },
  {
    "text": "and we implemented code attribution tools for example if you use the vas code extension with star coder we have a",
    "start": "2083079",
    "end": "2089240"
  },
  {
    "text": "membership test that when the model generates something we go and check the data set and if we find that it is an",
    "start": "2089240",
    "end": "2094679"
  },
  {
    "text": "exact copy of something that was in the data set we can you have like a red alert and if you click on the link you",
    "start": "2094679",
    "end": "2100560"
  },
  {
    "text": "can find exactly which repository that was from and then you can attribute the author so that kind of tries to help a",
    "start": "2100560",
    "end": "2106440"
  },
  {
    "text": "little bit with the code attribution side so we try to develop some tools to help the users who are using the models",
    "start": "2106440",
    "end": "2112240"
  },
  {
    "text": "to attribute the authors if the model ever generates exact copies of what was in the training",
    "start": "2112240",
    "end": "2118920"
  },
  {
    "text": "data any more questions",
    "start": "2120880",
    "end": "2125359"
  },
  {
    "text": "um so I have a question so in training the model do you care about um the",
    "start": "2128640",
    "end": "2134920"
  },
  {
    "text": "quality of the code say like if the code is NE enough and and uh the the",
    "start": "2134920",
    "end": "2140160"
  },
  {
    "text": "algorithms uh computational complexity um yeah so for when we were",
    "start": "2140160",
    "end": "2146160"
  },
  {
    "text": "training on the data set I think you can Implement a lot of filters to only keep uh the code files that you believe have",
    "start": "2146160",
    "end": "2152720"
  },
  {
    "text": "a higher quality and we did a lot of experiments and a lot of ablations to try to find the adequate filters and we",
    "start": "2152720",
    "end": "2159599"
  },
  {
    "text": "found that you can filter but you shouldn't filter too aggressively otherwise you will end up using losing a",
    "start": "2159599",
    "end": "2164920"
  },
  {
    "text": "lot of data and you wouldn't have enough to chain your model for example we have a paper called Santa coder don't reach",
    "start": "2164920",
    "end": "2170359"
  },
  {
    "text": "for the stars and that's because we try to use stars as a filtering approach so for example we kept only files that had",
    "start": "2170359",
    "end": "2176960"
  },
  {
    "text": "more than five stars but this significantly reduced the size of the data set and we ended up with a model",
    "start": "2176960",
    "end": "2182800"
  },
  {
    "text": "that was like the worst of all the models we trained so some filters might seem like good to you but maybe for the",
    "start": "2182800",
    "end": "2188839"
  },
  {
    "text": "model training um it's not worth using them so for example filtering on stars is not a good idea although we might be",
    "start": "2188839",
    "end": "2195319"
  },
  {
    "text": "thinking that repositories with a lot of stars probably have a higher quality um but now we use some kind of basic",
    "start": "2195319",
    "end": "2201280"
  },
  {
    "text": "filters to remove autogenerated files for example we count the average line length and if we find that is very long",
    "start": "2201280",
    "end": "2208280"
  },
  {
    "text": "for some specific programming languages maybe it is autogenerated we also try to remove data for example CSV and Jon we",
    "start": "2208280",
    "end": "2214839"
  },
  {
    "text": "only keep a smaller subset because that is not called just dat so we have other filters like that but they are not too",
    "start": "2214839",
    "end": "2221119"
  },
  {
    "text": "aggressive thank you",
    "start": "2221119",
    "end": "2224799"
  },
  {
    "text": "sure hello am I already go um my question is",
    "start": "2226880",
    "end": "2234359"
  },
  {
    "text": "uh regarding obviously the legal and ethical considerations around script and data from uh GitHub um for example I",
    "start": "2234359",
    "end": "2241560"
  },
  {
    "text": "know in the past there's been uh legal cases uh what one notable one would be",
    "start": "2241560",
    "end": "2247160"
  },
  {
    "text": "the legal case between LinkedIn and hiq where hiq was scraping data from",
    "start": "2247160",
    "end": "2252599"
  },
  {
    "text": "LinkedIn using fake accounts and I think link LinkedIn took an injunction against them I'm just wondering um what what",
    "start": "2252599",
    "end": "2258599"
  },
  {
    "text": "what considerations have been made uh in that sense um is is there some kind of",
    "start": "2258599",
    "end": "2265280"
  },
  {
    "text": "agreement with L uh is there some kind of agreement uh between yourself and",
    "start": "2265280",
    "end": "2270680"
  },
  {
    "text": "GitHub uh on the on the scraping of the data or did you just go ahead and scrip",
    "start": "2270680",
    "end": "2276720"
  },
  {
    "text": "the data yeah um no there's no agreement between us and the GitHub because we only use",
    "start": "2276720",
    "end": "2282680"
  },
  {
    "text": "the repositories that are public and then once we scraped them we filtered out for example um licenses that don't",
    "start": "2282680",
    "end": "2289960"
  },
  {
    "text": "allow commercial use or GPL code uh and then we trained on a subset of the data",
    "start": "2289960",
    "end": "2295880"
  },
  {
    "text": "that we scraped and we have also these opt out tools so we can consider that for example given users kind of a choice",
    "start": "2295880",
    "end": "2303560"
  },
  {
    "text": "um to decide whether they want their code to be included in the prain or not so in this opt out tool users can fill a",
    "start": "2303560",
    "end": "2311040"
  },
  {
    "text": "request and if they see that some of their GitHub repositories are in the data set and they want them removed they",
    "start": "2311040",
    "end": "2317079"
  },
  {
    "text": "can ask to remove that and we also have the code attribution tools so these are kind of the three things we try to",
    "start": "2317079",
    "end": "2323640"
  },
  {
    "text": "consider but we don't have an agreement with GitHub because the code was",
    "start": "2323640",
    "end": "2329560"
  },
  {
    "text": "public does that answer your question okay uh have one over here hi",
    "start": "2329560",
    "end": "2337920"
  },
  {
    "text": "thank you for this great presentation uh do you have any uh mitigation uh for",
    "start": "2337920",
    "end": "2344319"
  },
  {
    "text": "against the risk of model collapse where you put uh generated code back into the",
    "start": "2344319",
    "end": "2351720"
  },
  {
    "text": "stack and where you train the new models on AI generated code yeah I think there",
    "start": "2351720",
    "end": "2358119"
  },
  {
    "text": "was a study about like you're talking about using synthetic data what the model generated to train again on it yes",
    "start": "2358119",
    "end": "2364040"
  },
  {
    "text": "and then and and then uh with a couple of Cycles the model collapses and then",
    "start": "2364040",
    "end": "2369960"
  },
  {
    "text": "just spit out uh random yeah things yeah I think maybe for talking about the same",
    "start": "2369960",
    "end": "2375560"
  },
  {
    "text": "study in that study they kind of used the very small model I think it was opt 125 million and they found that it",
    "start": "2375560",
    "end": "2382400"
  },
  {
    "text": "collapses but now a lot of people are train synthetic data a little bit models that are larger and we haven't seen that",
    "start": "2382400",
    "end": "2388599"
  },
  {
    "text": "that happens um the worst thing that happen that the models performance that just does not improve on the task you're",
    "start": "2388599",
    "end": "2394040"
  },
  {
    "text": "training on uh but maybe in the future when we have a lot of these cycles that will happen but now that hasn't really",
    "start": "2394040",
    "end": "2400160"
  },
  {
    "text": "happened",
    "start": "2400160",
    "end": "2403160"
  },
  {
    "text": "okay another",
    "start": "2405240",
    "end": "2408599"
  },
  {
    "text": "one hi thank you very much for the talk uh the question is if we come to a future where most of",
    "start": "2417079",
    "end": "2425319"
  },
  {
    "text": "the code is AI generated and this is for sure being used to train more of these",
    "start": "2425319",
    "end": "2431560"
  },
  {
    "text": "models what do you think will happen um that's a good question for",
    "start": "2431560",
    "end": "2437280"
  },
  {
    "text": "which I'm not sure I have the answer I think we're gonna have to wait and see what happens um but if we see like the",
    "start": "2437280",
    "end": "2443880"
  },
  {
    "text": "code that is being generated a lot of it are actually might be higher quality than if you take the average of what is",
    "start": "2443880",
    "end": "2449280"
  },
  {
    "text": "on GitHub so it's not just like garbage code I think maybe something we can do is to kind of have things to detect and",
    "start": "2449280",
    "end": "2456599"
  },
  {
    "text": "distinguish EI generated code from code that was used by humans for example I",
    "start": "2456599",
    "end": "2462160"
  },
  {
    "text": "saw very recent paper that tried no just a study where they tried to see which uh",
    "start": "2462160",
    "end": "2467880"
  },
  {
    "text": "papers have been written by Chad GPT and they used the word delve and they found that like after Chad GPT was released",
    "start": "2467880",
    "end": "2474599"
  },
  {
    "text": "the number of papers that use the word Del just increased and chpt has the tendency to generate that so that's kind",
    "start": "2474599",
    "end": "2481119"
  },
  {
    "text": "of a silly filter but it still detects what was generated by a model so maybe for code if we can have some water",
    "start": "2481119",
    "end": "2487440"
  },
  {
    "text": "marking approach or something that will help us distinguish AI generator from non AI generator that would be very",
    "start": "2487440",
    "end": "2493480"
  },
  {
    "text": "helpful but I think that's still under exploration and we haven't made much progress on that but it's hard to",
    "start": "2493480",
    "end": "2498920"
  },
  {
    "text": "predict what will happen when we will have more AI generated code than human gener",
    "start": "2498920",
    "end": "2505920"
  },
  {
    "text": "code uh thank you for your talk uh my question is uh looking forward to the future can",
    "start": "2508240",
    "end": "2515480"
  },
  {
    "text": "you sort of talk about how we might get to the point where you could generate",
    "start": "2515480",
    "end": "2520640"
  },
  {
    "text": "multifile or multi like project size uh completions with um one of these tools",
    "start": "2520640",
    "end": "2529000"
  },
  {
    "text": "yeah I think yeah that's something everyone's looking forward to because for example when gab out was introduced",
    "start": "2529000",
    "end": "2534920"
  },
  {
    "text": "I personally was using it just for documentation in tests it was not like very good at like generating new code",
    "start": "2534920",
    "end": "2540839"
  },
  {
    "text": "coming up with new ideas now it has improved a lot so you can use it for other tasks that are more complex but I",
    "start": "2540839",
    "end": "2546119"
  },
  {
    "text": "think we're still a little bit far from having something like an agent that can work on your whole code base and just",
    "start": "2546119",
    "end": "2551559"
  },
  {
    "text": "you give it a question you don't need to say where exactly it has to change the code so I think that is kind of the end",
    "start": "2551559",
    "end": "2557480"
  },
  {
    "text": "goal to have something like that can take in multiple files and do the required changes one step forward to",
    "start": "2557480",
    "end": "2564040"
  },
  {
    "text": "that is training on repository context for example which we did for Star coder 2 compared to Star coder one uh but if",
    "start": "2564040",
    "end": "2571280"
  },
  {
    "text": "we saw see things for example like div if you saw it it's this AI Chang agent that can change anything and and I think",
    "start": "2571280",
    "end": "2577800"
  },
  {
    "text": "that's uh that means that we're making progress towards that just need to find the right recipe for that and I think",
    "start": "2577800",
    "end": "2583200"
  },
  {
    "text": "that means that you need to have good instruction tuning data that not only asks the model and instruction about a",
    "start": "2583200",
    "end": "2588280"
  },
  {
    "text": "specific code snippers but something about a code base and you also should start from a base model that is already",
    "start": "2588280",
    "end": "2593559"
  },
  {
    "text": "very good uh because that will significantly impact the performance of what you get so I think that means",
    "start": "2593559",
    "end": "2599000"
  },
  {
    "text": "having better base models and better data set that you train on and also better evaluations to be able to track if you're moving forward or not with",
    "start": "2599000",
    "end": "2606040"
  },
  {
    "text": "regards to that aspect",
    "start": "2606040",
    "end": "2609119"
  },
  {
    "text": "there's one of here yeah training your llm seems to be quite an involved",
    "start": "2611559",
    "end": "2617160"
  },
  {
    "text": "process do you think there'll be a time where when I'm in an Enterprise and I've got a couple of G repos that I can just",
    "start": "2617160",
    "end": "2623760"
  },
  {
    "text": "say hey model please in ingest these um models and then the model trains itself",
    "start": "2623760",
    "end": "2630760"
  },
  {
    "text": "on on my code without me going through this whole song and dance here yeah I think there are a lot of startups who",
    "start": "2630760",
    "end": "2636680"
  },
  {
    "text": "are trying to do that because that's kind of like use case I don't want to be involved in all of this training just",
    "start": "2636680",
    "end": "2641720"
  },
  {
    "text": "give me a service that I can launch the training so I think there's one probably called poolside and they're trying to",
    "start": "2641720",
    "end": "2647680"
  },
  {
    "text": "have that because basically now we have all the components there's the model there's the find scripts and there's your data and we kind of know which",
    "start": "2647680",
    "end": "2653839"
  },
  {
    "text": "filters will help so we just need someone who will combine these components so maybe that's a product",
    "start": "2653839",
    "end": "2659760"
  },
  {
    "text": "that someone can develop and you can use um a bit of a followup on that and I",
    "start": "2659760",
    "end": "2666079"
  },
  {
    "text": "think you touched uh in in in your talk but I didn't get the like the the",
    "start": "2666079",
    "end": "2671480"
  },
  {
    "text": "guidance uh with respect to fine-tuning you you talked about your libraries right that you did find tun for",
    "start": "2671480",
    "end": "2677440"
  },
  {
    "text": "libraries so can you give some guidance of how many I'm guessing million lines",
    "start": "2677440",
    "end": "2682760"
  },
  {
    "text": "of code would you need to fine tune let's say I have you know my own uh",
    "start": "2682760",
    "end": "2688559"
  },
  {
    "text": "Frameworks and libraries internally in my Enterprise and I want to find you in a model roughly what what are we talking",
    "start": "2688559",
    "end": "2695680"
  },
  {
    "text": "about we need the million 5 10 5050 million lines of code yeah um",
    "start": "2695680",
    "end": "2701839"
  },
  {
    "text": "I can give you a number but like that number does not really apply depends on your use case which model you started",
    "start": "2701839",
    "end": "2707480"
  },
  {
    "text": "from what you're trying to adapt it but I think usually for like fine tune in on new programming languages that's a new",
    "start": "2707480",
    "end": "2713800"
  },
  {
    "text": "one one that the model has already seen but you want to improve the performance a little bit maybe a few tens of gigabytes of data should do the trick if",
    "start": "2713800",
    "end": "2721839"
  },
  {
    "text": "you not just just do like instruction tuning or get the model to follow instructions you need much less I think",
    "start": "2721839",
    "end": "2727000"
  },
  {
    "text": "people are just using tens of thousands of uh samples um but yeah maybe a few",
    "start": "2727000",
    "end": "2733359"
  },
  {
    "text": "gigabytes of data could work if it is just taking a model that has already seen that language and you just want to",
    "start": "2733359",
    "end": "2738480"
  },
  {
    "text": "adapt it that should work for example if you go to the blog post that I mentioned I think they had even less because they",
    "start": "2738480",
    "end": "2743720"
  },
  {
    "text": "just took the hugging face libraries I don't know how many we have but like they just compiled them into a data set",
    "start": "2743720",
    "end": "2749680"
  },
  {
    "text": "and they found that it works to train on that um one other thing that's kind of made it that you need less data is these",
    "start": "2749680",
    "end": "2756160"
  },
  {
    "text": "new fine tuning techniques that don't take the whole model and change the whole weight during the fine tuning they",
    "start": "2756160",
    "end": "2761599"
  },
  {
    "text": "only change specific weights and specific layers that they add this means that you don't need a lot of data for",
    "start": "2761599",
    "end": "2766839"
  },
  {
    "text": "the adaptation um so yeah you will need to try and see at was threshold your model",
    "start": "2766839",
    "end": "2773200"
  },
  {
    "text": "gets thank",
    "start": "2773200",
    "end": "2775760"
  },
  {
    "text": "you hi thanks for the talk um I have a quick question around if you've done any work on like deprecation of libraries or",
    "start": "2778480",
    "end": "2785079"
  },
  {
    "text": "stuff like CV so um um I've got loads of examples I guess where chat GPT will generate like a package Jason or",
    "start": "2785079",
    "end": "2791880"
  },
  {
    "text": "dependency file but then the library or API Co that it's referenced in the next bit of code is deicated or doesn't exist",
    "start": "2791880",
    "end": "2798480"
  },
  {
    "text": "in like the V2 version um have you done any work around how to sort of avoid that happening in code generation models",
    "start": "2798480",
    "end": "2805440"
  },
  {
    "text": "yeah I haven't personally worked on that but that is also like hallucination for llms when you would ask the model who's",
    "start": "2805440",
    "end": "2811520"
  },
  {
    "text": "the president of the US and they would say Obama because like this is something that was dicated this is",
    "start": "2811520",
    "end": "2817440"
  },
  {
    "text": "and one of the techniques to solve that is to use for example retrieval rag to kind of retrieve information that is",
    "start": "2817440",
    "end": "2823319"
  },
  {
    "text": "more recent and add it in the context in your case you could try to retrieve the documentation for the new uh version of",
    "start": "2823319",
    "end": "2830160"
  },
  {
    "text": "the library and add it to the context to kind of tell the model okay these are the new Changs that are changed or you",
    "start": "2830160",
    "end": "2837119"
  },
  {
    "text": "can also add the logs of the changes uh but that is something that is worth exporing otherwise you would need to",
    "start": "2837119",
    "end": "2842680"
  },
  {
    "text": "fine tune on like recent code uh but these are two things either retrieval or or find new",
    "start": "2842680",
    "end": "2849480"
  },
  {
    "text": "find anybody else over",
    "start": "2851319",
    "end": "2857280"
  },
  {
    "text": "there hello thank you for the talk so we've had a truly explosive growth in AI over the last few years do you think",
    "start": "2859680",
    "end": "2866200"
  },
  {
    "text": "we're close to plateauing or is this just a start I sorry I haven't heard the last part sorry what I didn't hear the",
    "start": "2866200",
    "end": "2872880"
  },
  {
    "text": "last part ah uh are we close to plateauing on AI or is this just a start",
    "start": "2872880",
    "end": "2878119"
  },
  {
    "text": "I think uh We've made a lot of progress but we still have a long way to go first",
    "start": "2878119",
    "end": "2883319"
  },
  {
    "text": "by like matching the performance of the closed models but then trying to address some issues that even these closed",
    "start": "2883319",
    "end": "2888520"
  },
  {
    "text": "models still have which are the hallucinations the biases and also all the data governance issues so we think",
    "start": "2888520",
    "end": "2895079"
  },
  {
    "text": "we're off to a good start but we still have a lot of work um like you asked we",
    "start": "2895079",
    "end": "2900400"
  },
  {
    "text": "want code models that can act as agents on programming code bases and not just",
    "start": "2900400",
    "end": "2906240"
  },
  {
    "text": "complete simple function so we still have a a long way to go but I think we're off to a good path thank you",
    "start": "2906240",
    "end": "2915480"
  },
  {
    "text": "sure we have time probably for just one more question if somebody has",
    "start": "2916720",
    "end": "2923160"
  },
  {
    "text": "one no um do you have an experience on using",
    "start": "2923160",
    "end": "2932520"
  },
  {
    "text": "Code generated to improve an existing code base and how how this is working",
    "start": "2932520",
    "end": "2939559"
  },
  {
    "text": "well or not uh with Cod LMS um I think yeah this can happen",
    "start": "2939559",
    "end": "2946400"
  },
  {
    "text": "implicitly when you have some Engineers who are using the model and then they use the code that is generated for what",
    "start": "2946400",
    "end": "2954440"
  },
  {
    "text": "they will push for example to production so that's kind of an implicit Improvement um otherwise you can maybe",
    "start": "2954440",
    "end": "2961079"
  },
  {
    "text": "try to use the chat models and maybe integrate parts of your code base and try to get feedback back on these",
    "start": "2961079",
    "end": "2968240"
  },
  {
    "text": "different components um but yeah it's still kind of um I think maybe early",
    "start": "2968240",
    "end": "2973920"
  },
  {
    "text": "stage because we still don't have models that can act on the code based level but they still all act on the file level um",
    "start": "2973920",
    "end": "2981440"
  },
  {
    "text": "yeah but maybe it's bit always thanks a lot lbda",
    "start": "2981440",
    "end": "2988760"
  },
  {
    "text": "[Music]",
    "start": "2988990",
    "end": "2994440"
  }
]