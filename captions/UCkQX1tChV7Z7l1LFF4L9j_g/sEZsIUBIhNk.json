[
  {
    "start": "0",
    "end": "117000"
  },
  {
    "text": "hello everyone if you're like me and follow what goes on in the mission",
    "start": "3919",
    "end": "10670"
  },
  {
    "text": "learning community or if you're new to machine learning and try to get a peek",
    "start": "10670",
    "end": "15679"
  },
  {
    "text": "in your impression will be something like this we discuss a lot on deep learning different algorithms are you",
    "start": "15679",
    "end": "22430"
  },
  {
    "text": "are made to believe knowing machine learning doing machine learning is all about knowing the",
    "start": "22430",
    "end": "28010"
  },
  {
    "text": "boosting algorithms and the ordinance but in practice my experience working on",
    "start": "28010",
    "end": "33980"
  },
  {
    "text": "machine learning and real-world products it looks more like this it's about",
    "start": "33980",
    "end": "39770"
  },
  {
    "text": "figuring out how to configure data pipelines how do we manage the different models we have and how do you monitor",
    "start": "39770",
    "end": "46760"
  },
  {
    "text": "what goes on in the predictions is it doing fine so in real world",
    "start": "46760",
    "end": "52489"
  },
  {
    "text": "it looks more like 10 percent of the time and 10 percent of the effort is on the algorithms a 90 percent is on the",
    "start": "52489",
    "end": "59780"
  },
  {
    "text": "systems that support your machine learning models and that is the focus of",
    "start": "59780",
    "end": "65088"
  },
  {
    "text": "my talk what if we could turn this around and instead focus more on the",
    "start": "65089",
    "end": "73760"
  },
  {
    "text": "algorithms have systems in place that lets you focus more on the algorithms I'm here to share how we did that how we",
    "start": "73760",
    "end": "81020"
  },
  {
    "text": "go about doing that at door - through",
    "start": "81020",
    "end": "86180"
  },
  {
    "text": "many iterations of applying machine learning in our products we identified",
    "start": "86180",
    "end": "91689"
  },
  {
    "text": "best practice solutions for a lot of the common problems that occur with using",
    "start": "91689",
    "end": "97909"
  },
  {
    "text": "machine learning and production build that into robust tools to form our image systems as I go through the talk as I",
    "start": "97909",
    "end": "106490"
  },
  {
    "text": "talk about the experience I would talk about the implementation details but it would focus more on the underlying",
    "start": "106490",
    "end": "112040"
  },
  {
    "text": "concepts in the hopes that that's more useful to you in your own applications",
    "start": "112040",
    "end": "117310"
  },
  {
    "start": "117000",
    "end": "117000"
  },
  {
    "text": "quick are we on the agenda I'll start with what doe dashes and vary use",
    "start": "117310",
    "end": "123020"
  },
  {
    "text": "machine learning at door - talk about how ml evolves in our product or in",
    "start": "123020",
    "end": "128989"
  },
  {
    "text": "products in general and then go deep on the different systems we have for",
    "start": "128989",
    "end": "134150"
  },
  {
    "text": "machine learning and end it with couple of examples of how these systems have helped us ship more machine",
    "start": "134150",
    "end": "142250"
  },
  {
    "text": "learning and ship it more confidently the start dough - how many of you have",
    "start": "142250",
    "end": "149720"
  },
  {
    "text": "heard of dough - quick show of hands how many of you have used door - looks like",
    "start": "149720",
    "end": "158360"
  },
  {
    "text": "we're doing well that we also have good growth opportunity here so dough - is a",
    "start": "158360",
    "end": "166010"
  },
  {
    "text": "technology company that's focused on being the last mile Logistics layer in every city we do this by partnering with",
    "start": "166010",
    "end": "173810"
  },
  {
    "text": "local businesses by empowering local businesses to offer delivery we connect them with consumers who are looking for",
    "start": "173810",
    "end": "180530"
  },
  {
    "text": "delivery and dashes for the delivery personnel who ready to fulfill the delivery this gives rise to a",
    "start": "180530",
    "end": "186710"
  },
  {
    "text": "three-sided marketplace consumers merchants and dashes we started about",
    "start": "186710",
    "end": "192230"
  },
  {
    "text": "five years ago focused primarily on restaurant delivery for those of you are in New York Restaurant delivery food",
    "start": "192230",
    "end": "199520"
  },
  {
    "text": "delivery is something pretty common that's not true across the u.s. it's a",
    "start": "199520",
    "end": "204590"
  },
  {
    "text": "new and rapidly growing industry and we aim to be in 1600 cities by in the end of this year to give you a sense of the",
    "start": "204590",
    "end": "212480"
  },
  {
    "text": "scale that we work with we work with hundred thousand plus restaurants on the platform and 300,000 plus dashes and in",
    "start": "212480",
    "end": "220430"
  },
  {
    "text": "the five years of experience five years of operations we are fulfilled tens of millions of deliveries so let's start",
    "start": "220430",
    "end": "230990"
  },
  {
    "start": "230000",
    "end": "230000"
  },
  {
    "text": "out the marketplace I believe the best",
    "start": "230990",
    "end": "236210"
  },
  {
    "text": "way to apply machine learning in any industry is to start with the customers",
    "start": "236210",
    "end": "241270"
  },
  {
    "text": "understand what they look for and use machine learning as a problem-solving tool so far for us there are three",
    "start": "241270",
    "end": "249800"
  },
  {
    "text": "different customers they are merchants and the reason they use door - is to",
    "start": "249800",
    "end": "255230"
  },
  {
    "text": "increase the reach and they won't increase the reach in order to make more revenue for consumers they use door - so",
    "start": "255230",
    "end": "264980"
  },
  {
    "text": "that they could focus on what they care about more it's more about convenience and what they look for in achieving",
    "start": "264980",
    "end": "271460"
  },
  {
    "text": "convenience is the selection that they could get the ability to order from many different restaurants for dashes the",
    "start": "271460",
    "end": "278630"
  },
  {
    "text": "main motivation is flexibility that door - provides they're independent contractors which makes them which makes",
    "start": "278630",
    "end": "286310"
  },
  {
    "text": "the work schedule really flexible and what they want really is more earnings so understanding these motivations for",
    "start": "286310",
    "end": "294349"
  },
  {
    "text": "the different customers and this leads to lot of machine learning problems between the different customer segments",
    "start": "294349",
    "end": "303160"
  },
  {
    "text": "between the merchants and the consumers it's the traditional e-commerce application how do we surface the best",
    "start": "303160",
    "end": "309020"
  },
  {
    "text": "content we have either on the app or on the website through emails to push so",
    "start": "309020",
    "end": "314449"
  },
  {
    "text": "that's a lot of recommendation and personalization algorithms between the",
    "start": "314449",
    "end": "320030"
  },
  {
    "text": "merchants and the dashes that's the core Logistics engine how we have a bunch of",
    "start": "320030",
    "end": "326090"
  },
  {
    "text": "dashes who are ready to fulfill the delivery we have a bunch of deliveries on the platform how do we make the best",
    "start": "326090",
    "end": "331880"
  },
  {
    "text": "possible combination of this the best possible assignment so that we increase",
    "start": "331880",
    "end": "337070"
  },
  {
    "text": "the efficiency of the market place and we fulfill deliveries as soon as possible between the dashes and the",
    "start": "337070",
    "end": "343729"
  },
  {
    "text": "consumers we have the big responsibility of making sure the marketplace is well",
    "start": "343729",
    "end": "349159"
  },
  {
    "text": "balanced which is we have the right supply right number of dashes we need for the demand we have for the number of",
    "start": "349159",
    "end": "356360"
  },
  {
    "text": "orders that they're coming through this involves machine learning in the sense",
    "start": "356360",
    "end": "361610"
  },
  {
    "text": "we we have to predict the demand that you're going to have we predict the supply will have and how do we balance",
    "start": "361610",
    "end": "368060"
  },
  {
    "text": "these two a lot of different pricing algorithms other demand shaping and",
    "start": "368060",
    "end": "373550"
  },
  {
    "text": "supply shaping strategies in addition to the applications between the mark between the customers we also have",
    "start": "373550",
    "end": "380960"
  },
  {
    "text": "specific applications for each customer for merchants given the selection we",
    "start": "380960",
    "end": "387500"
  },
  {
    "text": "have on door - we have all different types of food starting from late let's",
    "start": "387500",
    "end": "392780"
  },
  {
    "text": "say the fast food places where the food could get prepared really quickly or on the other hand let's say deep which",
    "start": "392780",
    "end": "398509"
  },
  {
    "text": "piece of it probably takes 20-25 minutes to prepare there's a lot of variants involved in the merchant operations how",
    "start": "398509",
    "end": "404449"
  },
  {
    "text": "do we how do you know there's an spike in the orders spike an in-house orders at the",
    "start": "404449",
    "end": "409490"
  },
  {
    "text": "merchant maybe it's weakened demand maybe there's a dinner peak how do we incorporate these into our machine",
    "start": "409490",
    "end": "416150"
  },
  {
    "text": "learning algorithms to be able to predict food preparation time for the consumers you could think of traditional",
    "start": "416150",
    "end": "423770"
  },
  {
    "text": "e-commerce applications again on predicting the lifetime value of the customer how do we decide promotion",
    "start": "423770",
    "end": "429919"
  },
  {
    "text": "structures for different custom different consumers for dashes the most",
    "start": "429919",
    "end": "435530"
  },
  {
    "text": "important machine learning application is the paid calculation daughter chooses",
    "start": "435530",
    "end": "441949"
  },
  {
    "text": "an effort based pay model which means we look at the effort involved in fulfilling a delivery looking into the",
    "start": "441949",
    "end": "449479"
  },
  {
    "text": "complexity of the order the distance of the the cents if you need to travel the time would you take to process the order",
    "start": "449479",
    "end": "456469"
  },
  {
    "text": "and we we try to make the pay consistent and fair for every delivery so this is",
    "start": "456469",
    "end": "464210"
  },
  {
    "text": "powered by a machine learning model as I mentioned before we do supply forecasting and there is a lot of work",
    "start": "464210",
    "end": "470270"
  },
  {
    "text": "on our own incentives for different dashes given we have wide variety of",
    "start": "470270",
    "end": "475909"
  },
  {
    "text": "applications before building systems for these it's good to understand what the",
    "start": "475909",
    "end": "482029"
  },
  {
    "text": "requirements are I like to look at this in this offline to online axis on one",
    "start": "482029",
    "end": "488120"
  },
  {
    "text": "hand on the offline side you have recommendation algorithms think think of",
    "start": "488120",
    "end": "494330"
  },
  {
    "text": "emails that you get from different people different products these do not",
    "start": "494330",
    "end": "500389"
  },
  {
    "text": "have to be computed in real time these are computed offline in a in a batch",
    "start": "500389",
    "end": "505550"
  },
  {
    "text": "setting on the other hand we have let's say predicting the delivery times as a",
    "start": "505550",
    "end": "511130"
  },
  {
    "text": "consumer if you are on door - today will tell you you will get the food in 42 minutes 36 minutes whatever it is this",
    "start": "511130",
    "end": "518870"
  },
  {
    "text": "has to be real time we don't know who is going to order we don't know which restaurant they're going to pick so once",
    "start": "518870",
    "end": "524209"
  },
  {
    "text": "we get a delivery we make the prediction so that's more on the online side of things and then if you take the example",
    "start": "524209",
    "end": "530180"
  },
  {
    "text": "of demand forecasting that's somewhere in the middle we probably want to predict demand let's say two weeks in",
    "start": "530180",
    "end": "536180"
  },
  {
    "text": "advance to know what we are getting into you also want to predict demand let's say 20 minutes out so that we can manage",
    "start": "536180",
    "end": "543079"
  },
  {
    "text": "the marketplace in real time for most of",
    "start": "543079",
    "end": "548870"
  },
  {
    "text": "the talk the systems I describe cover both the offline and the online set of things just to keep it focused I will",
    "start": "548870",
    "end": "555559"
  },
  {
    "text": "run with an example of the online side particularly I will focus on the delivery time prediction when you start",
    "start": "555559",
    "end": "566629"
  },
  {
    "text": "with machine learning in a product it doesn't start off with the super",
    "start": "566629",
    "end": "573199"
  },
  {
    "text": "sophisticated models imagine it's the early days of the product and you are",
    "start": "573199",
    "end": "578870"
  },
  {
    "start": "575000",
    "end": "575000"
  },
  {
    "text": "tasked with the project of predicting delivery times how do you go about doing this you'd start not with the",
    "start": "578870",
    "end": "587120"
  },
  {
    "text": "state-of-the-art model but with just simple rules for this particular",
    "start": "587120",
    "end": "592129"
  },
  {
    "text": "application it could be something like this let's say in Manhattan any delivery",
    "start": "592129",
    "end": "597980"
  },
  {
    "text": "takes 40 minutes or in Austin it takes 35 minutes this is the general idea you could extend this out you could say on",
    "start": "597980",
    "end": "604639"
  },
  {
    "text": "Friday as the deliveries take certain time on Monday as it takes out another time you also don't have a lot of data",
    "start": "604639",
    "end": "611660"
  },
  {
    "text": "this time so this is sort of the only approach you could take as you get more data the next evolution is you run a",
    "start": "611660",
    "end": "621499"
  },
  {
    "text": "regression you identify the coefficients you get some sort of an equation that says this this is the ETA and this is",
    "start": "621499",
    "end": "627800"
  },
  {
    "text": "how you would add certain factors to it the way you would productionize such a",
    "start": "627800",
    "end": "632899"
  },
  {
    "text": "system is you take this coefficients these numbers and pretty much just load",
    "start": "632899",
    "end": "638149"
  },
  {
    "text": "it into your production system that's that's the simple ml system possible",
    "start": "638149",
    "end": "643550"
  },
  {
    "text": "there as things of all you a mortgage on",
    "start": "643550",
    "end": "648679"
  },
  {
    "text": "to more sophisticated models for example",
    "start": "648679",
    "end": "653749"
  },
  {
    "text": "your first real Lehrman model might be some using a psycho to learn model that you can get off the shelf you write your",
    "start": "653749",
    "end": "659809"
  },
  {
    "text": "own feature extractors here is where you're actually getting into the real",
    "start": "659809",
    "end": "664819"
  },
  {
    "text": "production ml set of things and quickly enough you see this is pretty powerful",
    "start": "664819",
    "end": "672019"
  },
  {
    "text": "you have many different animal models one for let's say the predicting the preparation time one for predicting",
    "start": "672019",
    "end": "677929"
  },
  {
    "text": "demand predicting a TAS and more often than not each of this evolved independently you end up with many",
    "start": "677929",
    "end": "685459"
  },
  {
    "text": "different stacks for each model presumably different people do ordered and at this stage you're left with",
    "start": "685459",
    "end": "693439"
  },
  {
    "text": "addition of designing a true ml system what do we need there as your product",
    "start": "693439",
    "end": "699350"
  },
  {
    "text": "grows the performance of your ml system becomes really critical when you start",
    "start": "699350",
    "end": "706009"
  },
  {
    "text": "you you're probably not thinking too much about accuracy guarantees but at scale that becomes really crucial for",
    "start": "706009",
    "end": "715899"
  },
  {
    "text": "example let's say the case of predicting ETA s let's say I tell you it's going to",
    "start": "715899",
    "end": "723529"
  },
  {
    "text": "you're going to get your food in 80 minutes and it's coming from a prediction you're probably not going to",
    "start": "723529",
    "end": "728929"
  },
  {
    "text": "order you own your food as soon as possible so that affects the revenue that the business makes if on the other",
    "start": "728929",
    "end": "735920"
  },
  {
    "text": "end we tell you you'll get you the food in 20 minutes and then not deliver on it that's not good either and this becomes",
    "start": "735920",
    "end": "743480"
  },
  {
    "text": "all the more important when your business is fulfilling lots of deliveries every day",
    "start": "743480",
    "end": "748809"
  },
  {
    "text": "so your Emin system needs to provide certain accuracy SLS similarly system",
    "start": "748809",
    "end": "755779"
  },
  {
    "text": "sls when you're checking out your order card you probably don't want to be waiting too long to get know when you",
    "start": "755779",
    "end": "761839"
  },
  {
    "text": "food's going to be there so there is recommends on latency recommends and ability to scale as you grow but the",
    "start": "761839",
    "end": "770420"
  },
  {
    "text": "most important part of the requirement is the debug ability let's say you're running a model in production it's going",
    "start": "770420",
    "end": "777470"
  },
  {
    "text": "to there's going to be some case where it fails and when it fails you want to be able to identify what is going wrong",
    "start": "777470",
    "end": "785589"
  },
  {
    "text": "for example let's say the prediction for ETS is going up someone calls you up and",
    "start": "785589",
    "end": "793910"
  },
  {
    "text": "says what's going on you want to be able to quickly identify whether it's a data issue its model issue as a model still",
    "start": "793910",
    "end": "801589"
  },
  {
    "text": "is a a dependency issue you depend on someone that's not behaving correctly and the ability",
    "start": "801589",
    "end": "811670"
  },
  {
    "text": "to debug a system is how you can iterate faster on it innovation is usually powered through",
    "start": "811670",
    "end": "817970"
  },
  {
    "text": "experimentation and the crucial requirement to experiment is to be able to move fast and which is where time to",
    "start": "817970",
    "end": "825080"
  },
  {
    "text": "iterate on the model is time to ship them or it becomes important and as you build more ml models you end up with a",
    "start": "825080",
    "end": "832340"
  },
  {
    "text": "lot of dependencies for example the system to predict parking times would be",
    "start": "832340",
    "end": "839390"
  },
  {
    "text": "a dependent of the system to predict ETS or the system to predict real-time",
    "start": "839390",
    "end": "845480"
  },
  {
    "text": "supply is going to impact the delivery times so how do you manage this",
    "start": "845480",
    "end": "851780"
  },
  {
    "text": "dependencies how do we avoid writing a lot of custom code and actually leverage",
    "start": "851780",
    "end": "856880"
  },
  {
    "text": "the economies of scale which is you don't want to 2x the effort if you have 2x the number of models you want the",
    "start": "856880",
    "end": "863300"
  },
  {
    "text": "effort required to reduce given this the",
    "start": "863300",
    "end": "869540"
  },
  {
    "text": "way we approach this problem is we take we take the best practices with template",
    "start": "869540",
    "end": "875750"
  },
  {
    "text": "eyes them so we don't have to duplicate a lot of the effort to attain the SLA",
    "start": "875750",
    "end": "880940"
  },
  {
    "text": "requirements to attain quicker iteration speeds we build robust tools and put",
    "start": "880940",
    "end": "886820"
  },
  {
    "text": "these two things together make your ml system as you try to build this ml",
    "start": "886820",
    "end": "893840"
  },
  {
    "text": "system what principles did we follow template acing the best practices you",
    "start": "893840",
    "end": "899750"
  },
  {
    "start": "896000",
    "end": "896000"
  },
  {
    "text": "don't have to work doubly hard for it ease of use when the system is deployed",
    "start": "899750",
    "end": "906730"
  },
  {
    "text": "you want your data scientists and engineers to be able to use the system easily and effectively you want the",
    "start": "906730",
    "end": "916640"
  },
  {
    "text": "configurability to be easy maybe put in",
    "start": "916640",
    "end": "922220"
  },
  {
    "text": "as some put in as config files not as not requiring a lot of code the other",
    "start": "922220",
    "end": "929890"
  },
  {
    "text": "requirement we said on our Celsius to go to bias - it's simple tools door - is a",
    "start": "929890",
    "end": "935870"
  },
  {
    "text": "fast growing business we would don't we don't want to over Inge early or optimist early focus on",
    "start": "935870",
    "end": "942980"
  },
  {
    "text": "building modulators tools so each of these tools can evolve independently and",
    "start": "942980",
    "end": "949120"
  },
  {
    "text": "the last point is to integrate with the rest of the ecosystem no dashes fast",
    "start": "949120",
    "end": "955940"
  },
  {
    "text": "moving into micro service architecture and when we build this prediction and training systems we want it to integrate",
    "start": "955940",
    "end": "962900"
  },
  {
    "text": "well with the micro service architecture we use lot of the monitoring and the logging systems that are in place just a",
    "start": "962900",
    "end": "973670"
  },
  {
    "text": "quick intro on what goes on in machine learning there's traditionally two different",
    "start": "973670",
    "end": "979460"
  },
  {
    "text": "phases there is training and prediction training is where you prepare your data explore different model options do model",
    "start": "979460",
    "end": "987500"
  },
  {
    "text": "selection run different algorithms and then when you decide you have a good model you take it into production once",
    "start": "987500",
    "end": "995840"
  },
  {
    "text": "it's there in production you want to monitor the system and there is a maintenance involved going with the",
    "start": "995840",
    "end": "1003070"
  },
  {
    "text": "running example of predicting delivery times training is done offline you you",
    "start": "1003070",
    "end": "1010450"
  },
  {
    "start": "1005000",
    "end": "1005000"
  },
  {
    "text": "get your data from your data bases run use many different scientific libraries",
    "start": "1010450",
    "end": "1015930"
  },
  {
    "text": "try different models possible do it all offline so the system we develop for",
    "start": "1015930",
    "end": "1021640"
  },
  {
    "text": "that needs to be agnostic of the model choices made it focuses on the iteration",
    "start": "1021640",
    "end": "1027220"
  },
  {
    "text": "speed so so you can move quickly the",
    "start": "1027220",
    "end": "1032430"
  },
  {
    "text": "prediction part though that has to be real-time which means it integrates with",
    "start": "1032430",
    "end": "1037810"
  },
  {
    "start": "1033000",
    "end": "1033000"
  },
  {
    "text": "the with the production systems and scale becomes an important requirement",
    "start": "1037810",
    "end": "1043319"
  },
  {
    "text": "so let's talk of training first this is",
    "start": "1043320",
    "end": "1048370"
  },
  {
    "start": "1046000",
    "end": "1046000"
  },
  {
    "text": "a prototypical training pipeline you start with the data you have your",
    "start": "1048370",
    "end": "1053500"
  },
  {
    "text": "features extractor and then you have a training job and you end up with the model that you put in the model",
    "start": "1053500",
    "end": "1058960"
  },
  {
    "text": "repository that's going to each individual component here features",
    "start": "1058960",
    "end": "1065710"
  },
  {
    "text": "extractor the power of any supervised learning model comes from the features",
    "start": "1065710",
    "end": "1070990"
  },
  {
    "text": "you use the features you put into this so for example you're predicting delivery times you decide a lot of",
    "start": "1070990",
    "end": "1079179"
  },
  {
    "text": "features let's say you just look at historical delivery times in the particular market you look at historical",
    "start": "1079179",
    "end": "1085720"
  },
  {
    "text": "delivery times on the particular day historical delivery times for this particular merchant what's the course is",
    "start": "1085720",
    "end": "1092559"
  },
  {
    "text": "subtotal of the order how many items does this order contain these are different features so your design job",
    "start": "1092559",
    "end": "1098049"
  },
  {
    "text": "you write a job that looks at looks at the data calculates these features",
    "start": "1098049",
    "end": "1104070"
  },
  {
    "text": "cleans the features standardizes them and makes it ready for training to make",
    "start": "1104070",
    "end": "1111159"
  },
  {
    "text": "this more robust we integrated this with the ETL pipeline what is he dl pipeline there we we have",
    "start": "1111159",
    "end": "1123779"
  },
  {
    "text": "many different jobs that run on a nightly basis that calculate different",
    "start": "1123779",
    "end": "1130330"
  },
  {
    "text": "business metrics for example at the end of the day we look at all deliveries that happen yesterday calculate how long",
    "start": "1130330",
    "end": "1137499"
  },
  {
    "text": "each delivery took to be delivered how long each delivery how long the merchant",
    "start": "1137499",
    "end": "1142960"
  },
  {
    "text": "took to prepare the food how long the dash had traveled from one location to another these are metrics that are",
    "start": "1142960",
    "end": "1148419"
  },
  {
    "text": "calculated for business reporting purposes what if if we integrate the",
    "start": "1148419",
    "end": "1156779"
  },
  {
    "text": "model training on the features generation part into the system we could",
    "start": "1156779",
    "end": "1162220"
  },
  {
    "text": "manage the dependencies more effectively which is you can have the job that",
    "start": "1162220",
    "end": "1167440"
  },
  {
    "text": "calculates the average historical delia times for the merchant depend on the job",
    "start": "1167440",
    "end": "1172869"
  },
  {
    "text": "that calculates delivery times for each delivery so in order to do this we use a job scheduler in particular we use an",
    "start": "1172869",
    "end": "1179440"
  },
  {
    "text": "air flow based system as we build a lot",
    "start": "1179440",
    "end": "1184869"
  },
  {
    "text": "of these features we identified a significant overlap between different",
    "start": "1184869",
    "end": "1190450"
  },
  {
    "text": "jobs the features job for delivery times and the features job for let's say the draft Daschle time would would require",
    "start": "1190450",
    "end": "1198360"
  },
  {
    "text": "same set of features for example what's the parking time historically so we",
    "start": "1198360",
    "end": "1205059"
  },
  {
    "text": "found a lot of value in extracting this comment just out into what we call a common feature store these enable us to have",
    "start": "1205059",
    "end": "1213760"
  },
  {
    "text": "standardized features so not just the particular ml application could benefit",
    "start": "1213760",
    "end": "1219700"
  },
  {
    "text": "from it it could also be any future application could directly use the job",
    "start": "1219700",
    "end": "1224880"
  },
  {
    "text": "the best way to think of this is it's a key value store let's say there is a",
    "start": "1224880",
    "end": "1230610"
  },
  {
    "text": "features table that contains all features of a store of a merchant the",
    "start": "1230610",
    "end": "1235930"
  },
  {
    "text": "key could be what's the average delivery times in the last one week and the value is whatever the average delivery time",
    "start": "1235930",
    "end": "1241780"
  },
  {
    "text": "was and expressing this for all different ml pipelines helps us",
    "start": "1241780",
    "end": "1247200"
  },
  {
    "text": "standardize the practices make sure the data is robust once you have the",
    "start": "1247200",
    "end": "1253420"
  },
  {
    "text": "features the next part is training the pipeline we rent we have a job there we",
    "start": "1253420",
    "end": "1259570"
  },
  {
    "text": "use many different libraries like scikit-learn light GBM carers so in",
    "start": "1259570",
    "end": "1265000"
  },
  {
    "text": "order to wrap all this and make it more elastic we use an m1 wrapper and again",
    "start": "1265000",
    "end": "1270540"
  },
  {
    "text": "to make use of the dependencies we already have in place we integrate this",
    "start": "1270540",
    "end": "1275890"
  },
  {
    "text": "into airflow system so the training job could depend on the features job so it runs after the features job is ready it",
    "start": "1275890",
    "end": "1283060"
  },
  {
    "text": "also helps with scheduling so you could schedule the training job to run let's say every day or every week however the",
    "start": "1283060",
    "end": "1288940"
  },
  {
    "text": "requirement is once it's trained we add the data we add the model file you can",
    "start": "1288940",
    "end": "1296200"
  },
  {
    "text": "which is a model object into a repository in particular we use s3 as",
    "start": "1296200",
    "end": "1301660"
  },
  {
    "text": "this data source for all models in addition to the model object we store metadata on let's say the training gear",
    "start": "1301660",
    "end": "1307630"
  },
  {
    "text": "I used the training time parameters used",
    "start": "1307630",
    "end": "1312060"
  },
  {
    "text": "moving on to the predictions part",
    "start": "1313500",
    "end": "1317610"
  },
  {
    "start": "1316000",
    "end": "1316000"
  },
  {
    "text": "the use case we have here is again crediting delivery times which is real",
    "start": "1319590",
    "end": "1326139"
  },
  {
    "text": "time you get a predictions request it comes with it under service you need a predictions response back you get a",
    "start": "1326139",
    "end": "1332229"
  },
  {
    "text": "delivery end you process it into a service you get a the value of the delivery time prediction back so we",
    "start": "1332229",
    "end": "1340179"
  },
  {
    "text": "build this in keeping line with the micro service architecture into an HTTP",
    "start": "1340179",
    "end": "1345580"
  },
  {
    "text": "service it's a it's a single service that handles multiple predictors main",
    "start": "1345580",
    "end": "1351489"
  },
  {
    "text": "requirement is low latency of the service and we build it behind a load balancer so we could scale this",
    "start": "1351489",
    "end": "1357279"
  },
  {
    "text": "horizontally as much as possible in particular we use kubernetes containers",
    "start": "1357279",
    "end": "1363970"
  },
  {
    "text": "to deploy the service and the service is built on flask let's dive into a little",
    "start": "1363970",
    "end": "1369970"
  },
  {
    "text": "more detail on what this prediction services and what the dependencies are three main ones there is a model",
    "start": "1369970",
    "end": "1377379"
  },
  {
    "text": "metadata service a model service and a feature service let's go into the",
    "start": "1377379",
    "end": "1383169"
  },
  {
    "text": "details model metadata service is the is the command center it keeps track of all",
    "start": "1383169",
    "end": "1390789"
  },
  {
    "text": "models that are live currently running that are registered with the prediction service it gives you you can think of",
    "start": "1390789",
    "end": "1398289"
  },
  {
    "text": "having some sort of a JSON interface to it you have the IDE of the model you have different model status and type and",
    "start": "1398289",
    "end": "1404979"
  },
  {
    "text": "version whether it's an experiment model is it model that's live in production what type of form order it is is it a",
    "start": "1404979",
    "end": "1411129"
  },
  {
    "text": "regression model or a classification model",
    "start": "1411129",
    "end": "1414869"
  },
  {
    "text": "the model service is responsible for fetching the model from the repository",
    "start": "1417100",
    "end": "1424110"
  },
  {
    "text": "so when you get a request for a delivery let's say you get a request for",
    "start": "1424110",
    "end": "1430420"
  },
  {
    "text": "predicting preparation times or predicting delivery times model service is responsible for figuring out what the",
    "start": "1430420",
    "end": "1436360"
  },
  {
    "text": "right model is to use it uses the metadata service to know what the models available are and fetches these models",
    "start": "1436360",
    "end": "1442660"
  },
  {
    "text": "and since we are going to have multiple requests for the same model there is a caching layer and an important part of",
    "start": "1442660",
    "end": "1450790"
  },
  {
    "text": "having this models in production is the ability to test them so when you have a new model let's say you come up with a",
    "start": "1450790",
    "end": "1456850"
  },
  {
    "text": "new model for your application there are two ways we test it one is we do what we",
    "start": "1456850",
    "end": "1464200"
  },
  {
    "text": "call shadowing when a request comes in we send it to the shadow model to make a",
    "start": "1464200",
    "end": "1470680"
  },
  {
    "text": "prediction based on it but not really use it so that gets recorded so we can",
    "start": "1470680",
    "end": "1475810"
  },
  {
    "text": "go back and do a valuation on it and look at what the performance is and the",
    "start": "1475810",
    "end": "1482050"
  },
  {
    "text": "second step once the shadowing is done is experimentation which is mostly we",
    "start": "1482050",
    "end": "1489190"
  },
  {
    "text": "run a lot of a B experiments let's say we have two different ways to predict two different models to predict delivery",
    "start": "1489190",
    "end": "1494980"
  },
  {
    "text": "times we send half the traffic there and the other half the traffic to the other model and see which one performs better",
    "start": "1494980",
    "end": "1501430"
  },
  {
    "text": "in terms of business metrics and this",
    "start": "1501430",
    "end": "1506740"
  },
  {
    "text": "model service supports multiple model types in the previous talk Michael mentioned ensemble and segmentation is",
    "start": "1506740",
    "end": "1513820"
  },
  {
    "text": "different methods which is you probably are using different models for different markets the request comes in for let's",
    "start": "1513820",
    "end": "1520210"
  },
  {
    "text": "say delivery times you want to be able to use different models underneath or you want to use multiple models and",
    "start": "1520210",
    "end": "1527410"
  },
  {
    "text": "combine the predictions so the model service abstracts out all the details of it it",
    "start": "1527410",
    "end": "1532600"
  },
  {
    "text": "handles all the complexity so it makes it easy to productionize models",
    "start": "1532600",
    "end": "1538860"
  },
  {
    "text": "feature service when you have a predictions request you know where you",
    "start": "1540830",
    "end": "1547049"
  },
  {
    "text": "are getting the model from all that's left is figuring out what features to use feature service is responsible for",
    "start": "1547049",
    "end": "1554759"
  },
  {
    "text": "talking to the feature store and retrieving these features if you",
    "start": "1554759",
    "end": "1560759"
  },
  {
    "text": "remember common feature store from the training pipeline it's extremely important to make sure use the same",
    "start": "1560759",
    "end": "1568710"
  },
  {
    "text": "features in training and prediction using this common feature store helps us achieve this I'll get into little more",
    "start": "1568710",
    "end": "1577679"
  },
  {
    "text": "detail on how this is exactly useful when I go into the case study similar to",
    "start": "1577679",
    "end": "1584190"
  },
  {
    "text": "how model service supports multiple model types feature service supports multiple more feature types we could use",
    "start": "1584190",
    "end": "1589799"
  },
  {
    "text": "aggregate features we could use store embeddings which are pretty popular",
    "start": "1589799",
    "end": "1595370"
  },
  {
    "text": "similar to the concept of word to wake and natural language processing the",
    "start": "1595370",
    "end": "1604769"
  },
  {
    "text": "fourth service here is monitoring when you get a pretty when you get a request",
    "start": "1604769",
    "end": "1610320"
  },
  {
    "text": "for prediction you generate the prediction and you could give it back to the consumer but also asynchronously we",
    "start": "1610320",
    "end": "1619190"
  },
  {
    "text": "send it to the monitoring service to track the predictions three particular",
    "start": "1619190",
    "end": "1624509"
  },
  {
    "text": "things we track predictions what's the output coming out of the model second is features distribution what's",
    "start": "1624509",
    "end": "1633480"
  },
  {
    "text": "the distribution of the features that are being used by this model and the",
    "start": "1633480",
    "end": "1639600"
  },
  {
    "text": "third part is logging we log all the features and predictions you generated",
    "start": "1639600",
    "end": "1646139"
  },
  {
    "text": "for different models again I'll go more into this in the case study the",
    "start": "1646139",
    "end": "1652679"
  },
  {
    "text": "technologies we use here we use stats t server to record all these stats that",
    "start": "1652679",
    "end": "1658320"
  },
  {
    "text": "are used for predictions and features monitoring and we use segments to log different events systems in action now",
    "start": "1658320",
    "end": "1667950"
  },
  {
    "text": "that describe how the system works I want to take two particular examples of",
    "start": "1667950",
    "end": "1673470"
  },
  {
    "text": "how the system hel this build better ml products first part",
    "start": "1673470",
    "end": "1681180"
  },
  {
    "start": "1679000",
    "end": "1679000"
  },
  {
    "text": "features integrity features power most of the supervised models performance and it's critical to under",
    "start": "1681180",
    "end": "1687860"
  },
  {
    "text": "understand that we have the same set of features we have consistency of features",
    "start": "1687860",
    "end": "1694380"
  },
  {
    "text": "across different environments and also we have consistency over time consistency across in ramens means there",
    "start": "1694380",
    "end": "1702420"
  },
  {
    "text": "are no differences in training and prediction time there are two particular",
    "start": "1702420",
    "end": "1708150"
  },
  {
    "text": "sub parts of the system that helps us achieve this first is the common feature",
    "start": "1708150",
    "end": "1713340"
  },
  {
    "text": "store extracting out a lot of the features that are used by the different",
    "start": "1713340",
    "end": "1718680"
  },
  {
    "text": "models we use in production into this common feature store and using the same features across both training and",
    "start": "1718680",
    "end": "1725490"
  },
  {
    "text": "prediction removes any differences that are possible between the two environments you are pretty much",
    "start": "1725490",
    "end": "1732180"
  },
  {
    "text": "executing the exact code it works for most features there are certain features which might not be able to be powered",
    "start": "1732180",
    "end": "1739320"
  },
  {
    "text": "this way which is where the second piece comes in feature logging when you make a",
    "start": "1739320",
    "end": "1746070"
  },
  {
    "text": "prediction you have access to all the features you're using and the output of the model how would you just log it",
    "start": "1746070",
    "end": "1753270"
  },
  {
    "text": "right there log it in the production system so that you could retrain an on it later if you do that",
    "start": "1753270",
    "end": "1761760"
  },
  {
    "text": "you're pretty much training on the same data that your production system is predicting on so you eliminate more",
    "start": "1761760",
    "end": "1767480"
  },
  {
    "text": "discrepancies between training and prediction time the only caveat here is",
    "start": "1767480",
    "end": "1772920"
  },
  {
    "text": "it only works for new features sorry it doesn't work for new features let's say",
    "start": "1772920",
    "end": "1777950"
  },
  {
    "text": "you have a model running in production it's using 100 features you come up with a new idea let's say you come up with a",
    "start": "1777950",
    "end": "1784800"
  },
  {
    "text": "new sort of embedding for representing a merchant that cannot be integrated into the system because you know you still",
    "start": "1784800",
    "end": "1791280"
  },
  {
    "text": "don't have access to it in production it works for all other features consistency",
    "start": "1791280",
    "end": "1799560"
  },
  {
    "text": "over time when you have a system that's predicting and it uses features there's",
    "start": "1799560",
    "end": "1805620"
  },
  {
    "text": "a good chance things change over time especially when you depend on multiple data sources the way we work",
    "start": "1805620",
    "end": "1813270"
  },
  {
    "text": "around this is the features monitoring the monitoring service establishes",
    "start": "1813270",
    "end": "1820560"
  },
  {
    "text": "metrics to track four different features for example you have a feature that's",
    "start": "1820560",
    "end": "1825720"
  },
  {
    "text": "number of items in the order we track the median values the mean values and",
    "start": "1825720",
    "end": "1832200"
  },
  {
    "text": "the p90 values similar to how you would track an application performance I'm doing this let's say doing this helps us",
    "start": "1832200",
    "end": "1840960"
  },
  {
    "text": "when there is an issue let's say we get paged on predictions not being okay having the ability to go look back at",
    "start": "1840960",
    "end": "1848430"
  },
  {
    "text": "the features that are being fed into the system helps us identify areas of discrepancy that we could focus on the",
    "start": "1848430",
    "end": "1860580"
  },
  {
    "start": "1859000",
    "end": "1859000"
  },
  {
    "text": "second area that having the system helps us is in launching new models previously",
    "start": "1860580",
    "end": "1870080"
  },
  {
    "text": "once you have the model ready which is the output of the training stage someone",
    "start": "1870080",
    "end": "1875760"
  },
  {
    "text": "has to go write the feature extractors on prod write out the prediction code which is write out the logic to figure",
    "start": "1875760",
    "end": "1882150"
  },
  {
    "text": "out how to fix the model apply the features on the model have their own rollout logic run an experiment run a",
    "start": "1882150",
    "end": "1889830"
  },
  {
    "text": "shadow and then go on this monitor debug loop this would take days two weeks but",
    "start": "1889830",
    "end": "1899880"
  },
  {
    "text": "with the prediction service once you have the model ready you could register",
    "start": "1899880",
    "end": "1905700"
  },
  {
    "text": "the model in the metadata service and the model service figures out how to",
    "start": "1905700",
    "end": "1911730"
  },
  {
    "text": "fetch the model so you no longer have to write code for it the feature service has access to different features so it",
    "start": "1911730",
    "end": "1918000"
  },
  {
    "text": "it knows how to fetch those features and the ability to shadow and experiment",
    "start": "1918000",
    "end": "1924830"
  },
  {
    "text": "means you don't have to worry about the rollout logic so all you do is you",
    "start": "1924830",
    "end": "1930600"
  },
  {
    "text": "register the model in the service monitor the metrics and focus more on building the model this meant the time",
    "start": "1930600",
    "end": "1941550"
  },
  {
    "text": "to take a model production reduce significantly it takes a few minutes to hours to production as",
    "start": "1941550",
    "end": "1947020"
  },
  {
    "text": "a model now what are the biggest",
    "start": "1947020",
    "end": "1952419"
  },
  {
    "start": "1952000",
    "end": "1952000"
  },
  {
    "text": "takeaways establishing robust tools are critical to deploying a Malian",
    "start": "1952419",
    "end": "1958510"
  },
  {
    "text": "production template icing the ML best practices you",
    "start": "1958510",
    "end": "1964179"
  },
  {
    "text": "identify lets you focus more on the algorithms which is where all the furnace particularly useful tools we",
    "start": "1964179",
    "end": "1971230"
  },
  {
    "text": "identified the common feature store the ability the experimentation excrement and shadow and the predictions and",
    "start": "1971230",
    "end": "1978309"
  },
  {
    "text": "features monitoring if any of these problems are interesting to you either",
    "start": "1978309",
    "end": "1984760"
  },
  {
    "text": "on the infrastructure side or on the algorithms that I mentioned earlier hit me up we are definitely hiring those are",
    "start": "1984760",
    "end": "1991659"
  },
  {
    "text": "materials first name our door - calm or you can find me on Twitter and LinkedIn thank you so much",
    "start": "1991659",
    "end": "1998640"
  }
]