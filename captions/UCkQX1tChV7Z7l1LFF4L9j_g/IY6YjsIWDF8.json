[
  {
    "text": "thank you Thomas hi everyone thank you so much for joining us today myself and my colleague",
    "start": "10240",
    "end": "17039"
  },
  {
    "text": "Karthik here we are going to be walking you through as part of this session on how we solve for scaling the Uber Eats",
    "start": "17039",
    "end": "24640"
  },
  {
    "text": "backend and infra architecture so that we can infinitely scale the number of",
    "start": "24640",
    "end": "30880"
  },
  {
    "text": "merchants that are deliverable to any particular reader so to start with intro",
    "start": "30880",
    "end": "37760"
  },
  {
    "text": "uh I'm Jani Narayan i'm a senior staff engineer working on search ranking",
    "start": "37760",
    "end": "42800"
  },
  {
    "text": "recommendations for the f past few years and we have Karthik here uh hi everyone",
    "start": "42800",
    "end": "47920"
  },
  {
    "text": "uh I am Karthik Ramas Swami i'm a senior staff engineer i work in the search platform team our team builds the search",
    "start": "47920",
    "end": "56000"
  },
  {
    "text": "search solutions that powers various search use cases at Uber including Uber Eats uh location search rights and other",
    "start": "56000",
    "end": "64920"
  },
  {
    "text": "things a fun Easter egg here so if you have your uh Uber Eatats app open you",
    "start": "64920",
    "end": "70720"
  },
  {
    "text": "could go there into the search bar and then look up who built this and you can see the list of all of the engineers who",
    "start": "70720",
    "end": "77040"
  },
  {
    "text": "are part of the team so uh with that uh let's get",
    "start": "77040",
    "end": "82600"
  },
  {
    "text": "started as part of this talk just to make it fun and interactive we have bunch of trivia sprinkled over the talk",
    "start": "82600",
    "end": "90000"
  },
  {
    "text": "and whoever gets the answers um or whoever gets to answers fast they will",
    "start": "90000",
    "end": "95040"
  },
  {
    "text": "get a goodie from Uber one of the volunteers will find you and then they will give you a good so the first trivia",
    "start": "95040",
    "end": "100400"
  },
  {
    "text": "for the day which of the following is not among the top 10 searches in Uber",
    "start": "100400",
    "end": "107200"
  },
  {
    "text": "Eats that was my guess too",
    "start": "107799",
    "end": "112680"
  },
  {
    "text": "so it is kind of unexpected for me when I first saw this it was actually Mexican apparently not many people care about",
    "start": "113720",
    "end": "120000"
  },
  {
    "text": "eating healthy when they order from outside so um volunteers will take care of whoever uh give the answers they will",
    "start": "120000",
    "end": "126719"
  },
  {
    "text": "take care of the goodie so the problem that we want to talk",
    "start": "126719",
    "end": "133599"
  },
  {
    "text": "about as part of this is how did we go about expanding selection by NX so",
    "start": "133599",
    "end": "139360"
  },
  {
    "text": "before we go into the what does NX mean I want to spend a little bit of time talking about what selection means it",
    "start": "139360",
    "end": "146319"
  },
  {
    "text": "means different things for different people if I were to talk to the merchant onboarding team the operations they would say that hey uh onboarding as many",
    "start": "146319",
    "end": "153040"
  },
  {
    "text": "merchants as possible that is considered as uh success metric that is considered as good selection if I were to speak to",
    "start": "153040",
    "end": "159280"
  },
  {
    "text": "eaters different kind of eaters um will have a different answer to this someone could say that hey I care about getting",
    "start": "159280",
    "end": "165200"
  },
  {
    "text": "my food within a particular um ETA so that is good selection or my restaurant my favorite restaurant if it is on this",
    "start": "165200",
    "end": "171760"
  },
  {
    "text": "platform then this platform has good selection some other folks can also say that if I get to find new stores",
    "start": "171760",
    "end": "179680"
  },
  {
    "text": "discover more stores which I wouldn't have normally found it or instead of going out of the app and then finding",
    "start": "179680",
    "end": "184959"
  },
  {
    "text": "somewhere else or word of mouth recommendation and then coming back to the app if the app in itself can curate",
    "start": "184959",
    "end": "190000"
  },
  {
    "text": "my preferences based on my order history based on my search history you know everything about me so why don't you give me something that I haven't tried",
    "start": "190000",
    "end": "195360"
  },
  {
    "text": "before surprise me so that is considered as good selection so what we are here to talk about is given all of the real",
    "start": "195360",
    "end": "203120"
  },
  {
    "text": "world aspect of Uber Eats out of the picture what is a technical challenge that we could solve where the",
    "start": "203120",
    "end": "208800"
  },
  {
    "text": "restaurants are stores are getting onboarded onto this platform and the eaters want to have access to more and",
    "start": "208800",
    "end": "215440"
  },
  {
    "text": "more of selection more and more of restaurants available to them when they are trying to look it up in any of the discovery services so to get a sense of",
    "start": "215440",
    "end": "223519"
  },
  {
    "text": "how the business is growing how the market is growing as we can see just before the pandemic and through the",
    "start": "223519",
    "end": "229280"
  },
  {
    "text": "course of pandemic the business has expanded and branched down to multiple different line of businesses why this is",
    "start": "229280",
    "end": "234720"
  },
  {
    "text": "important is because that is all part of the scale that is inclusive of what we were trying to solve so it is not just",
    "start": "234720",
    "end": "242239"
  },
  {
    "text": "about restaurants starting from pandemic we have grocery stores then retail",
    "start": "242239",
    "end": "247599"
  },
  {
    "text": "stores uh people who are trying to uh deliver packages all of these things are part of the same infra same ranking and",
    "start": "247599",
    "end": "254640"
  },
  {
    "text": "recommendation text stack which powers it under the hood why this matters is that up until now we are talking about",
    "start": "254640",
    "end": "261840"
  },
  {
    "text": "it in terms of restaurants and stores but the index and complexity comes in where in case of a restaurant if we talk",
    "start": "261840",
    "end": "268400"
  },
  {
    "text": "about a single document it would probably have like 20 30 uh items and that's about it but if we take about",
    "start": "268400",
    "end": "274080"
  },
  {
    "text": "grocery as a line of business for every single store there are going to be 100,000 SKUs for each and every store so",
    "start": "274080",
    "end": "281040"
  },
  {
    "text": "all of those items also need to be indexed so onboarding a single grocery store is very different in terms of",
    "start": "281040",
    "end": "286960"
  },
  {
    "text": "scale comparison with onboarding a restaurant and another example is before",
    "start": "286960",
    "end": "293120"
  },
  {
    "text": "we started working on this people were able to order from restaurants which are 10 to 15 minutes away from them now you",
    "start": "293120",
    "end": "299919"
  },
  {
    "text": "can order from a restaurant which is sitting from San Francisco you could order it all the way to Berkeley and let's say if you want to order something",
    "start": "299919",
    "end": "306240"
  },
  {
    "text": "from Home Depot and the item that you're looking for is not here but it is somewhere in Sacramento you should be",
    "start": "306240",
    "end": "311520"
  },
  {
    "text": "able to order it from Uber Eats and then get it delivered to you and that is the breadth of the line of businesses that",
    "start": "311520",
    "end": "318320"
  },
  {
    "text": "we wanted to unlock and also the different challenges in terms of scale that different line of business offers",
    "start": "318320",
    "end": "324400"
  },
  {
    "text": "for us so with that in place in terms of selection we are specifically focusing",
    "start": "324400",
    "end": "329680"
  },
  {
    "text": "on the quantity of selection that is exposed to the eater when they're going to any of these discovery surfaces the",
    "start": "329680",
    "end": "335360"
  },
  {
    "text": "personalization aspect of it that's a completely different uh topic al together so what we mean by discovery",
    "start": "335360",
    "end": "341759"
  },
  {
    "text": "surfaces let's start with terminologies there are four different discovery surfaces and the next trivia for the day",
    "start": "341759",
    "end": "349280"
  },
  {
    "text": "which of the surfaces uh do you guys think most of the orders come from",
    "start": "349280",
    "end": "356240"
  },
  {
    "text": "home feed and depending on whether it is e-commerce or let's say um uh online",
    "start": "361360",
    "end": "366720"
  },
  {
    "text": "streaming services the uh surface area is going to change and for uh specifically delivery business it is a",
    "start": "366720",
    "end": "373120"
  },
  {
    "text": "home feed so in terms of the text stack that we are looking at there are",
    "start": "373120",
    "end": "378560"
  },
  {
    "text": "different entry points to this different use cases that we serve as part of the work that we did if we take search for",
    "start": "378560",
    "end": "383840"
  },
  {
    "text": "example there are different kinds of search restaurant name search dish search cuisine search and home feed has",
    "start": "383840",
    "end": "390319"
  },
  {
    "text": "multiple different um compartments to this there is uh the cruisers which are",
    "start": "390319",
    "end": "395759"
  },
  {
    "text": "all thematic based on the users order history and then we have storefronts which is list of stores uh based on the",
    "start": "395759",
    "end": "400880"
  },
  {
    "text": "location and at the top of your home feed if you uh look at your Uber Eats app there would be shortcuts which will",
    "start": "400880",
    "end": "406560"
  },
  {
    "text": "be either cuisine based or promotion based and whatnot all of these entry points need to work in cohesion in other",
    "start": "406560",
    "end": "413520"
  },
  {
    "text": "words regardless of whether someone goes through suggestions where someone is searching for pasta and you are trying",
    "start": "413520",
    "end": "418800"
  },
  {
    "text": "to um also show pastrami based on the look ahead search or uh we are uh",
    "start": "418800",
    "end": "424000"
  },
  {
    "text": "looking at me as a search and we also want to show uh Burger King as part of the suggestions that come up all of",
    "start": "424000",
    "end": "430319"
  },
  {
    "text": "these different use cases need to be addressed as part of this because if I'm",
    "start": "430319",
    "end": "435360"
  },
  {
    "text": "able to find a store or a restaurant through my home feed but I'm not able to locate it through my suggestions at the same time that is considered as a poor",
    "start": "435360",
    "end": "442000"
  },
  {
    "text": "customer experience so we needed to address all parts of the text stack in one go in one XP so given this let's",
    "start": "442000",
    "end": "448560"
  },
  {
    "text": "take a look at the overall architecture from the infraite to the application layer so the leftmost thing Karthik will",
    "start": "448560",
    "end": "454319"
  },
  {
    "text": "go over a little bit in detail in the next few slides that is all the infra layer of it it is the corpus of all of",
    "start": "454319",
    "end": "459919"
  },
  {
    "text": "our uh stores and indexes how we index how do we ingest all of that goes into",
    "start": "459919",
    "end": "465440"
  },
  {
    "text": "that then we have the retrieval layer that is where kind of the application layer and my team and my org starts",
    "start": "465440",
    "end": "471240"
  },
  {
    "text": "where the retrieval layer focuses on improving for or optimizing for recall",
    "start": "471240",
    "end": "476560"
  },
  {
    "text": "the more and more stores that we could fetch the more and more stores that we could uh send it to the next set of rankers so they can figure out what is",
    "start": "476560",
    "end": "482960"
  },
  {
    "text": "an appropriate um restaurant to show at that time and the first pass ranker is",
    "start": "482960",
    "end": "488000"
  },
  {
    "text": "looking for precision and um efficiency so what this means is that as the",
    "start": "488000",
    "end": "493919"
  },
  {
    "text": "restaurants are uh or stores are fetched we want to start looking at uh how do we",
    "start": "493919",
    "end": "498960"
  },
  {
    "text": "do a lexical match so the users query and the document are matched as much as",
    "start": "498960",
    "end": "504240"
  },
  {
    "text": "possible in terms of relevance then we have the hydration layer where a lot of the business logic comes into picture in",
    "start": "504240",
    "end": "509599"
  },
  {
    "text": "terms of does it have a promotion does it have um membership benefits and all is there um any other bogo buy one get",
    "start": "509599",
    "end": "515440"
  },
  {
    "text": "one order um that we could present and whatnot and ETD information store images all of those things come into picture",
    "start": "515440",
    "end": "520719"
  },
  {
    "text": "there then we have the second pass anchor which optimizes for um the",
    "start": "520719",
    "end": "525760"
  },
  {
    "text": "precision so this is where lot of the business metrics get addressed so we look at conversion rate we also look at",
    "start": "525760",
    "end": "533279"
  },
  {
    "text": "um given the previous order history all of the other things that I know from different surfaces of interaction from this eater how do I build the personaliz",
    "start": "533279",
    "end": "540640"
  },
  {
    "text": "personalization aspect of it so we will be able to give more and more relevant results to the item so given this",
    "start": "540640",
    "end": "546399"
  },
  {
    "text": "overall text stack want to give a sense of scale in terms of the text stack particularly I would like you I would",
    "start": "546399",
    "end": "552000"
  },
  {
    "text": "like to draw your attention to two of the line items here one is the number of stores which are part of the corpus",
    "start": "552000",
    "end": "557120"
  },
  {
    "text": "millions of them and the other one is the number of match documents that you use to fetch out of the retrieval layer",
    "start": "557120",
    "end": "562720"
  },
  {
    "text": "so when you look at it the scale sounds like oh there is only thousands of documents which are matched what match means is when I look at the EA's",
    "start": "562720",
    "end": "570320"
  },
  {
    "text": "location how many stores can deliver to that EA's location when we had these tens of thousands of them when we",
    "start": "570320",
    "end": "576000"
  },
  {
    "text": "started we said \"Oh if you wanted to make it NX or increase it more all that",
    "start": "576000",
    "end": "581279"
  },
  {
    "text": "we needed to do is fetch more so let's just increase our early termination count which Karthik will go into uh",
    "start": "581279",
    "end": "586320"
  },
  {
    "text": "detail later let's uh fetch more candidates and then go from there.\" So we did a very basic approach of fetching",
    "start": "586320",
    "end": "593360"
  },
  {
    "text": "200 more stores to week XP and it blew up on our face where we saw that the",
    "start": "593360",
    "end": "598480"
  },
  {
    "text": "latency increase P50 latency increased by 4x so in this example we could see",
    "start": "598480",
    "end": "604480"
  },
  {
    "text": "that the red dot is where the eater is that is the eer's location and as it expands which is the um new red dots",
    "start": "604480",
    "end": "611120"
  },
  {
    "text": "that we started um adding that is where the latency started increasing so this is a serious problem then we needed to",
    "start": "611120",
    "end": "616480"
  },
  {
    "text": "look into where exactly the lat agency is coming from so the root cause as we started diving",
    "start": "616480",
    "end": "623360"
  },
  {
    "text": "into it had multiple aspects to it where we need we needed to look at different",
    "start": "623360",
    "end": "628720"
  },
  {
    "text": "parts of the text stack to make sure that some decisions design decisions made in let's say injest how does it how",
    "start": "628720",
    "end": "635680"
  },
  {
    "text": "does it impact the query layer and how does some of the mechanisms that we have in the query layer doesn't really gel",
    "start": "635680",
    "end": "641200"
  },
  {
    "text": "well with our sharding strategy so it was a um whole new can of worms that we opened as we started looking into it",
    "start": "641200",
    "end": "647279"
  },
  {
    "text": "first we started with benchmarking so if there is a latency increase especially at the retrieval layer let's just figure",
    "start": "647279",
    "end": "653279"
  },
  {
    "text": "out where exactly it is coming from so in the search and for layer we added a bunch of different metrics and depending",
    "start": "653279",
    "end": "658959"
  },
  {
    "text": "on whether we are looking at u grocery or eats there is one step particularly",
    "start": "658959",
    "end": "664560"
  },
  {
    "text": "which stood out where we were trying to match a particular document to the query",
    "start": "664560",
    "end": "670079"
  },
  {
    "text": "and then okay this document match put it into your bucket then move on to the next document when we iterate over to",
    "start": "670079",
    "end": "675839"
  },
  {
    "text": "the next document that matches that took anywhere between 2.5 milliseconds latency for grocery and.5 millconds for",
    "start": "675839",
    "end": "682640"
  },
  {
    "text": "eats and that is unexplainable for us that was unexplainable at that time it",
    "start": "682640",
    "end": "688320"
  },
  {
    "text": "is supposed to take nanconds especially if you have optimized index so then we started looking at okay This is a problem area that we needed to uh start",
    "start": "688320",
    "end": "695040"
  },
  {
    "text": "looking into uh the other area that I want to talk about is um how we are",
    "start": "695040",
    "end": "700240"
  },
  {
    "text": "ingesting the data and the pieces will fall in place in next few slides for",
    "start": "700240",
    "end": "706160"
  },
  {
    "text": "those of you who are following Uber's engineering blogs you would now be familiar that Uber does most of its uh",
    "start": "706160",
    "end": "713200"
  },
  {
    "text": "geo representation representation using H3 and H3 library is uh what we use to",
    "start": "713200",
    "end": "719680"
  },
  {
    "text": "figure out how we tessillate the world and how we make sense out of uh the different levels that we have in place",
    "start": "719680",
    "end": "725600"
  },
  {
    "text": "depending on the resolution the different levers optimize for different behaviors that we want for the eaters",
    "start": "725600",
    "end": "730959"
  },
  {
    "text": "and the merchants so given this we represent any merchant and the delivery",
    "start": "730959",
    "end": "736240"
  },
  {
    "text": "using the hexagons to say that merchant A can deliver to ABC locations by using",
    "start": "736240",
    "end": "742240"
  },
  {
    "text": "hexagons and the resolutions how this gets indexed is if we take this example where we have restaurants A B and C and",
    "start": "742240",
    "end": "750480"
  },
  {
    "text": "hexagons are delivery areas which are numbered the index will be a reverse lookup where going by the hexagons we",
    "start": "750480",
    "end": "758480"
  },
  {
    "text": "would say that in this hexagon two or three different uh restaurants can deliver to me fairly straightforward so",
    "start": "758480",
    "end": "764880"
  },
  {
    "text": "far so from here what we did is now that we understand how the index layout looks",
    "start": "764880",
    "end": "770560"
  },
  {
    "text": "this is the second uh problem that we identified as part of um selection expansion so at the time of ingestion we",
    "start": "770560",
    "end": "778480"
  },
  {
    "text": "had this concept of close by and far away and that is the column that we use to ingest the data so at a store level",
    "start": "778480",
    "end": "786320"
  },
  {
    "text": "the upstream service had the decision to say hey I'm going to give you list of stores and the deliverable hexagons and",
    "start": "786320",
    "end": "792320"
  },
  {
    "text": "I'm going to categorize them as close by and far away and when we did that if we look at this example hexagon 7 considers",
    "start": "792320",
    "end": "800800"
  },
  {
    "text": "both A and B as far away but if we look at the real data B is actually close by",
    "start": "800800",
    "end": "805839"
  },
  {
    "text": "much close by in comparison with A but the injection doesn't have that information naturally the query stage",
    "start": "805839",
    "end": "812639"
  },
  {
    "text": "also doesn't have this information only the upstream service have this information which we lost as part of this so this ETD information without",
    "start": "812639",
    "end": "820880"
  },
  {
    "text": "that we are treating A and B together at the time of rankers and that was another problem in terms of search area even",
    "start": "820880",
    "end": "828240"
  },
  {
    "text": "though we say that we only increase by 100 and 200 of 200s of restaurants going from let's say 5",
    "start": "828240",
    "end": "834560"
  },
  {
    "text": "kilometer to 10 kilometer to so and so would mean that we are increasing the",
    "start": "834560",
    "end": "839680"
  },
  {
    "text": "area by square so the search space increases exponentially even though we",
    "start": "839680",
    "end": "845279"
  },
  {
    "text": "say that hey I'm only trying to deliver from 10 miles to 12 m or 15 miles and whatn not this meant that we are",
    "start": "845279",
    "end": "850560"
  },
  {
    "text": "processing a lot number of candidates which will tie in into why going from one document to the other was taking",
    "start": "850560",
    "end": "856720"
  },
  {
    "text": "such a long time the next thing is the store distribution if we were to make it as a",
    "start": "856720",
    "end": "863040"
  },
  {
    "text": "simple concentric circle around where uh the eater's location is and the latl longlong is what we could see is as we",
    "start": "863040",
    "end": "869040"
  },
  {
    "text": "start expanding further and further into more geos the number of stores or the",
    "start": "869040",
    "end": "874720"
  },
  {
    "text": "distribution of stores in the innermost circle versus the outer circle and whatnot is going to be like anywhere",
    "start": "874720",
    "end": "880000"
  },
  {
    "text": "between 1 is to9 kind of ratio so we will get more and more of farway stores",
    "start": "880000",
    "end": "885279"
  },
  {
    "text": "than the close by stores and ranking them becomes harder and harder another thing to note is if we are going to uh",
    "start": "885279",
    "end": "892320"
  },
  {
    "text": "find a restaurant which has much higher conversion rate because uh that restaurant is more popular and whatn not",
    "start": "892320",
    "end": "898000"
  },
  {
    "text": "but that is in the second circle or the third most circle then it is highly likely that in the second pass ranker",
    "start": "898000",
    "end": "904480"
  },
  {
    "text": "that store will get a higher precedence because it has higher conversion rate but in reality people would want to see",
    "start": "904480",
    "end": "910639"
  },
  {
    "text": "more of their close by stores because a burger is a burger at some point in time so that was one of the problems that we",
    "start": "910639",
    "end": "916560"
  },
  {
    "text": "uh saw as we started fetching more and more stores where good stores were trumping the close by stores and the",
    "start": "916560",
    "end": "922800"
  },
  {
    "text": "ranking also needed to account for that so with that in place uh Karthik",
    "start": "922800",
    "end": "928560"
  },
  {
    "text": "here will talk more about the search platform and how we index yeah next we'll go uh share some insights about",
    "start": "928560",
    "end": "936240"
  },
  {
    "text": "the search platform that powers the Uber Eat search and then we will talk about some optimizations that we did to",
    "start": "936240",
    "end": "943040"
  },
  {
    "text": "improve the retrieval limit and also the latency so here's another trivia how",
    "start": "943040",
    "end": "950959"
  },
  {
    "text": "much traffic does Uber Eats gets per day any",
    "start": "950959",
    "end": "956320"
  },
  {
    "text": "guesses yeah B is the right answer it's tens of millions of requests per",
    "start": "957399",
    "end": "964720"
  },
  {
    "text": "day so Uber has a in-house search platform that is built on Apache lucine",
    "start": "965880",
    "end": "971360"
  },
  {
    "text": "uh we use a typical lambda architecture for injection so we have batch injection through spark and then we have realtime",
    "start": "971360",
    "end": "979360"
  },
  {
    "text": "uh injection through the streaming path one of the notable features that we support in the real time injection is",
    "start": "979360",
    "end": "985279"
  },
  {
    "text": "the priority aware injection so the callers can prioritize requests and the system will give precedence to the",
    "start": "985279",
    "end": "992079"
  },
  {
    "text": "higher priority requests to ensure a data freshness high degree of data freshness for the high priority request",
    "start": "992079",
    "end": "999680"
  },
  {
    "text": "and at Uber we use geosshing quite heavily and and this because most of our use cases are geospatial in nature so",
    "start": "999680",
    "end": "1006880"
  },
  {
    "text": "I'll share some insights on some of the geosarding techniques that we use at Uber and then finally we build custom",
    "start": "1006880",
    "end": "1014000"
  },
  {
    "text": "index layouts and query operators that are turned for Uber Eats cases that take advantage of the offline uh document",
    "start": "1014000",
    "end": "1021279"
  },
  {
    "text": "ranking and and and query uh early termination to speed up the",
    "start": "1021279",
    "end": "1027160"
  },
  {
    "text": "queries here's the architectural overview of the search platform um there are uh three key components here the",
    "start": "1027160",
    "end": "1034798"
  },
  {
    "text": "first one is the uh batch uh indexing pipeline and and the second component is",
    "start": "1034799",
    "end": "1041360"
  },
  {
    "text": "the streaming or real-time updates path and and the third component is the",
    "start": "1041360",
    "end": "1046400"
  },
  {
    "text": "serving stack so we start with the batch injection so usually these are spark jobs that takes the data from the source",
    "start": "1046400",
    "end": "1053600"
  },
  {
    "text": "of truth convert them into search documents find the partition them into",
    "start": "1053600",
    "end": "1058799"
  },
  {
    "text": "shots and then builds lucine index in spark the output of the lucine output of",
    "start": "1058799",
    "end": "1064400"
  },
  {
    "text": "the spark shops are lucine indexes which are then get stored into the object store then updates are then constantly",
    "start": "1064400",
    "end": "1072000"
  },
  {
    "text": "consumed through the streaming path there is an injection service that consumes the updates from the upstream",
    "start": "1072000",
    "end": "1078000"
  },
  {
    "text": "again converts them into search documents and then writes finds the shard the document maps to and then",
    "start": "1078000",
    "end": "1084240"
  },
  {
    "text": "writes to the respective shard so one thing to note here is that we use Kafka as the right-handed lock uh which which",
    "start": "1084240",
    "end": "1091280"
  },
  {
    "text": "provides several benefits uh one of them that we talked earlier is implementing priority aware injection um because we",
    "start": "1091280",
    "end": "1098320"
  },
  {
    "text": "use Kafka as a righthead log it enables us to implement such features and and it also provides us to uh implement",
    "start": "1098320",
    "end": "1107039"
  },
  {
    "text": "replication and other things using Kafka and the uh searcher node when it comes",
    "start": "1107039",
    "end": "1112240"
  },
  {
    "text": "up uh it takes the index from the remote store and then it also catches up the uh",
    "start": "1112240",
    "end": "1118000"
  },
  {
    "text": "updates from the streaming path to the right block and then it exposes query",
    "start": "1118000",
    "end": "1123039"
  },
  {
    "text": "operators to run the search queries there is another uh component here which is called the aggregator service um it",
    "start": "1123039",
    "end": "1130160"
  },
  {
    "text": "is actually a stateless service uh its main responsibility is to take the request from upstream find the shard the",
    "start": "1130160",
    "end": "1137120"
  },
  {
    "text": "request maps to then send it to the respective searcher node and and execute the queries and and also aggregate the",
    "start": "1137120",
    "end": "1144320"
  },
  {
    "text": "results and uh send it back to the caller if there are query fanots and things like that that's the highle",
    "start": "1144320",
    "end": "1150559"
  },
  {
    "text": "overview of the search platform uh that powers Uber Eat",
    "start": "1150559",
    "end": "1156280"
  },
  {
    "text": "search next I will talk about sharding techniques that we use um as we have",
    "start": "1156280",
    "end": "1161840"
  },
  {
    "text": "been talking earlier that most of our queries are geospatial in nature we are looking for find me restaurants for a",
    "start": "1161840",
    "end": "1169039"
  },
  {
    "text": "given coordinates or find me grocery stores for a given coordinates so we use geossharding to make these queries more",
    "start": "1169039",
    "end": "1176000"
  },
  {
    "text": "efficient the main advantage of geos sharding is that you can um place or we",
    "start": "1176000",
    "end": "1181679"
  },
  {
    "text": "can locate all the data for a given location in a single shot so that the queries are executed in a single chart",
    "start": "1181679",
    "end": "1187919"
  },
  {
    "text": "right so at scale this is quite important because if you fan out the request um to multiple shots then there",
    "start": "1187919",
    "end": "1194640"
  },
  {
    "text": "is an overhead of overfetching and aggregating the results which can be avoided by using geos sharding and the",
    "start": "1194640",
    "end": "1202080"
  },
  {
    "text": "other benefit is like first pass ranking can be executed on the data nodes um the",
    "start": "1202080",
    "end": "1208000"
  },
  {
    "text": "reason being is that the data node has the full view of the results for a given query and then you can push the first",
    "start": "1208000",
    "end": "1213600"
  },
  {
    "text": "part ranker down to the data node to make it efficient the two uh geosharding",
    "start": "1213600",
    "end": "1218799"
  },
  {
    "text": "techniques that we use are latitude sharding and hex sharding i'll talk about both of them in detail in the next",
    "start": "1218799",
    "end": "1225440"
  },
  {
    "text": "slides so latitude sharding works this way uh where you imagine the world as a slice",
    "start": "1225440",
    "end": "1232320"
  },
  {
    "text": "of latitude bands and each band maps to a shard right the latitude ranges are",
    "start": "1232320",
    "end": "1237760"
  },
  {
    "text": "computed offline uh we use spark job to compute it the way we compute is a two-step process first is we divide the",
    "start": "1237760",
    "end": "1245039"
  },
  {
    "text": "map into several narrow stripes uh you can imagine uh this in order of",
    "start": "1245039",
    "end": "1251440"
  },
  {
    "text": "thousands of stripes and then we group the adjust sites to get a roughly equal",
    "start": "1251440",
    "end": "1257679"
  },
  {
    "text": "size shots right so when we in the first step we also get the count of documents that maps to each narrow stripe and then",
    "start": "1257679",
    "end": "1264240"
  },
  {
    "text": "then we group the adjacent stripes such that you get a roughly equal size shots n being the number of shots here there's",
    "start": "1264240",
    "end": "1271520"
  },
  {
    "text": "a special thing to note here like how we handle the documents that falls on the boundary of the shots that is the green",
    "start": "1271520",
    "end": "1278159"
  },
  {
    "text": "zone that is in this in this picture so those are documents that falls on the",
    "start": "1278159",
    "end": "1283200"
  },
  {
    "text": "boundary of two shots uh what we do is we index those shots in both of the",
    "start": "1283200",
    "end": "1288400"
  },
  {
    "text": "neighboring shots so that way the queries can go to a single shard and get all the documents relevant for the given",
    "start": "1288400",
    "end": "1294039"
  },
  {
    "text": "query the boundary or the buffer degree is calculated based on the search radius",
    "start": "1294039",
    "end": "1299600"
  },
  {
    "text": "we all we know that the queries are at the max going to go for a 50 mile or or",
    "start": "1299600",
    "end": "1304720"
  },
  {
    "text": "100 mile radius then we find the latitude degree that maps to that radius and then that's the buffer zone and any",
    "start": "1304720",
    "end": "1311280"
  },
  {
    "text": "document that falls in the buffer buffer zone are indexed in both the",
    "start": "1311280",
    "end": "1316720"
  },
  {
    "text": "shots so with latitude sharding we get this benefit of uh cities from different",
    "start": "1317080",
    "end": "1323840"
  },
  {
    "text": "time zones get collocated in the same shot so in this example you can see um",
    "start": "1323840",
    "end": "1329679"
  },
  {
    "text": "Europe cities and uh cities from America mixed in the same shot uh why is this",
    "start": "1329679",
    "end": "1335200"
  },
  {
    "text": "important um this is because the traffic in Uber uh especially follows the sun",
    "start": "1335200",
    "end": "1340880"
  },
  {
    "text": "pattern where the activities are higher during the day and it slows downs during the night and this sharding naturally",
    "start": "1340880",
    "end": "1349360"
  },
  {
    "text": "avoids clustering cities with same BCRs in the same shard and that that's helps",
    "start": "1349360",
    "end": "1354960"
  },
  {
    "text": "us a lot uh in in managing the capacity and",
    "start": "1354960",
    "end": "1359840"
  },
  {
    "text": "stuff but we also see some problems or challenges with the latitude sharding",
    "start": "1361240",
    "end": "1366720"
  },
  {
    "text": "one of them is the bands are too narrow at the center that's because the cities",
    "start": "1366720",
    "end": "1372159"
  },
  {
    "text": "are more denser um in the space and then you reach a point in some use cases",
    "start": "1372159",
    "end": "1378000"
  },
  {
    "text": "where it's difficult to divide further especially considering you have a buffer zone right there's no there so over time",
    "start": "1378000",
    "end": "1385200"
  },
  {
    "text": "the shots become um uneven and and some shots especially towards the center are",
    "start": "1385200",
    "end": "1391679"
  },
  {
    "text": "larger when compared to the uh rest of the shots and this creates problems like your index bills take longer time",
    "start": "1391679",
    "end": "1398559"
  },
  {
    "text": "because you're bound by the larger shard and also those shards as experienced uh",
    "start": "1398559",
    "end": "1404480"
  },
  {
    "text": "can be uh larger latencies and stuff so the optimization for this",
    "start": "1404480",
    "end": "1410320"
  },
  {
    "text": "problem is the hex sharding right hex sharding we imagine the world as tiles",
    "start": "1410320",
    "end": "1415679"
  },
  {
    "text": "of hexagons and as Jenneny said at Uber we use H3 library very extensively H3",
    "start": "1415679",
    "end": "1422480"
  },
  {
    "text": "library provides different resolutions of hexagons the lowest resolution which means large of hexagons results in about",
    "start": "1422480",
    "end": "1430640"
  },
  {
    "text": "100 tiles for the whole world and the highest resolution results in trillions of tiles so selecting the right uh",
    "start": "1430640",
    "end": "1438240"
  },
  {
    "text": "resolution is the key for using kick sharding and we use some observations",
    "start": "1438240",
    "end": "1443840"
  },
  {
    "text": "and empirical data to decide the uh hex sizes in a tuber we generally use for",
    "start": "1443840",
    "end": "1450320"
  },
  {
    "text": "hex sharding hex size two or three and again we use the same uh approach of",
    "start": "1450320",
    "end": "1455919"
  },
  {
    "text": "offline jobs to compute the shard boundaries so basically we compute the",
    "start": "1455919",
    "end": "1462000"
  },
  {
    "text": "we pick a resolution we compute the number of docs that maps to each resolution and then group them into n",
    "start": "1462000",
    "end": "1469600"
  },
  {
    "text": "shards uh basically and equal shards using bin packing and and we also handle",
    "start": "1469600",
    "end": "1475520"
  },
  {
    "text": "the buffer zones similar to latitude sharding in hex sharding you have to imagine the buffer zones also in terms",
    "start": "1475520",
    "end": "1481200"
  },
  {
    "text": "of hexagons uh the key here is choose a resolution that is smaller than the uh",
    "start": "1481200",
    "end": "1487120"
  },
  {
    "text": "the first the main resolution hex for the buffer zones and then you index the",
    "start": "1487120",
    "end": "1493120"
  },
  {
    "text": "documents that falls in buffer zone in both the hex's so in this case the uh the right side chart shows that the main",
    "start": "1493120",
    "end": "1499919"
  },
  {
    "text": "blue area is the main hexagon and the outside are the buffer zone hexagons",
    "start": "1499919",
    "end": "1505440"
  },
  {
    "text": "that gets indexed into it as well to avoid crossout queries",
    "start": "1505440",
    "end": "1511039"
  },
  {
    "text": "that's that's the uh details on sharding and the architecture of the search",
    "start": "1511039",
    "end": "1516600"
  },
  {
    "text": "platform next we will talk about some specific optimizations to be did for the",
    "start": "1516600",
    "end": "1522799"
  },
  {
    "text": "Uber Eats use case taking advantage of the query patterns and other other uh",
    "start": "1522799",
    "end": "1529360"
  },
  {
    "text": "data from the use case to make the to improve the recall and also reduce the",
    "start": "1529360",
    "end": "1534880"
  },
  {
    "text": "latency the first first thing that we did is building a data layout uh that",
    "start": "1534880",
    "end": "1540720"
  },
  {
    "text": "can take advantage of the query patterns so I will share a couple of data layouts uh that we used one for the each use",
    "start": "1540720",
    "end": "1548080"
  },
  {
    "text": "case and other for the grocery use case and I'll also walk through how those layouts helped us to improve the latency",
    "start": "1548080",
    "end": "1555679"
  },
  {
    "text": "and the second technique is like how we'll talk about how we use the ETD information that Chenny was talking",
    "start": "1555679",
    "end": "1562240"
  },
  {
    "text": "about earlier how we index that into the uh search index and then how we divide",
    "start": "1562240",
    "end": "1568559"
  },
  {
    "text": "the search space into non-over overlapping uh ranges and then execute execute them in parallel to improve the",
    "start": "1568559",
    "end": "1574600"
  },
  {
    "text": "latency and then finally we'll talk about how moving some of the computations that were happening in the",
    "start": "1574600",
    "end": "1580480"
  },
  {
    "text": "query time such as far away versus nearby computation that Jenny was talking earlier and and and how that",
    "start": "1580480",
    "end": "1587279"
  },
  {
    "text": "helped to improve the uh recall and the latency i will go through the data",
    "start": "1587279",
    "end": "1592880"
  },
  {
    "text": "layout and then I will hand over to Jenny to talk about the rest of the two optimizations",
    "start": "1592880",
    "end": "1599720"
  },
  {
    "text": "okay so this is a data layout that we um use for the eats index so if you look at",
    "start": "1599919",
    "end": "1607120"
  },
  {
    "text": "the eats index or eats query pattern to begin with you're basically looking for",
    "start": "1607120",
    "end": "1612799"
  },
  {
    "text": "restaurants or items within the restaurants for a given store so we we use uh this insight to also organize the",
    "start": "1612799",
    "end": "1620880"
  },
  {
    "text": "documents in the index right so in this case we take the city and we colllocate",
    "start": "1620880",
    "end": "1626400"
  },
  {
    "text": "all the restaurants for a given city first so you get you can see subway mcto McDonald and subway they all the",
    "start": "1626400",
    "end": "1633200"
  },
  {
    "text": "restaurants under the city SF and then we uh then order the items or the menus",
    "start": "1633200",
    "end": "1639840"
  },
  {
    "text": "uh under those restaurants uh in the same order so you go with this order where city followed by all the",
    "start": "1639840",
    "end": "1646559"
  },
  {
    "text": "restaurants in that city and then items for each of the restaurants in the same order as the",
    "start": "1646559",
    "end": "1652760"
  },
  {
    "text": "store so the benefit we get is the faster iteration right um so a query",
    "start": "1652760",
    "end": "1658320"
  },
  {
    "text": "comes from SF you can skip over all the documents of other cities that may be in the shard and just go move the pointer",
    "start": "1658320",
    "end": "1665279"
  },
  {
    "text": "right to the SF and then find out all the stores that makes the query faster",
    "start": "1665279",
    "end": "1670320"
  },
  {
    "text": "and the other benefit nice benefit that we get is that uh if your data is",
    "start": "1670320",
    "end": "1676399"
  },
  {
    "text": "denormalized in our case sometimes we denormalize all the um store fields into",
    "start": "1676399",
    "end": "1682080"
  },
  {
    "text": "the items as well so in that case uh you have lot of common attributes for the",
    "start": "1682080",
    "end": "1688159"
  },
  {
    "text": "docs so the item docs for the store will have all similar attributes uh similar store level attributes adjacent to each",
    "start": "1688159",
    "end": "1695200"
  },
  {
    "text": "other uh this provides better compression ratio and and that's because lucine uses delta encoding and if you",
    "start": "1695200",
    "end": "1702640"
  },
  {
    "text": "have very sequential doc ids uh then your compression is better and then",
    "start": "1702640",
    "end": "1707679"
  },
  {
    "text": "finally we also order the documents by static rank which helps us to early terminate the queries once we reach the",
    "start": "1707679",
    "end": "1715919"
  },
  {
    "text": "budget next I will share a slightly modified version of the index layout",
    "start": "1716440",
    "end": "1722000"
  },
  {
    "text": "that we use for grocery um that's because of the nature of the grocery data it's pretty similar again we first",
    "start": "1722000",
    "end": "1729360"
  },
  {
    "text": "sort by city then um we sort by we take the stores sort by stores stores are",
    "start": "1729360",
    "end": "1736080"
  },
  {
    "text": "ranked by the order um uh ranked by the offline conversion rate order and here",
    "start": "1736080",
    "end": "1741840"
  },
  {
    "text": "the difference is we place the items of the store next to each other and I will",
    "start": "1741840",
    "end": "1747120"
  },
  {
    "text": "share uh uh in a minute that why that is important so this is how the layout right layout looks city then store and",
    "start": "1747120",
    "end": "1754240"
  },
  {
    "text": "the items of the store go to the second city uh second store items of the store and third store items of the",
    "start": "1754240",
    "end": "1761240"
  },
  {
    "text": "store so one benefit is like uh the posting lists uh of let's say if you if",
    "start": "1761240",
    "end": "1766880"
  },
  {
    "text": "you look for a specific uh posting list with the title as chicken then you get the store one all the uh items with the",
    "start": "1766880",
    "end": "1774880"
  },
  {
    "text": "title chicken for that store and store two all the items with this title chicken for that store and store three",
    "start": "1774880",
    "end": "1781840"
  },
  {
    "text": "and as Jenny was saying earlier grocery scale is very high compared to each right so you have like hundreds or",
    "start": "1781840",
    "end": "1788960"
  },
  {
    "text": "thousands of items in a single store that can match the given title And when you're executing a query uh you",
    "start": "1788960",
    "end": "1795600"
  },
  {
    "text": "don't want to be looking for all the items from the same store so you can give a budget for each store and then",
    "start": "1795600",
    "end": "1801520"
  },
  {
    "text": "once you reach that limit then you can skip over to the next store so this layout allows us to skip over the stores",
    "start": "1801520",
    "end": "1809039"
  },
  {
    "text": "pretty quickly but also collecting enough items for a from a given store so",
    "start": "1809039",
    "end": "1815039"
  },
  {
    "text": "uh and and um the uh that the other benefit that we get is from the business",
    "start": "1815039",
    "end": "1821440"
  },
  {
    "text": "point of view is it skills us to get diverse results right your results are not coming from a single store you also",
    "start": "1821440",
    "end": "1828000"
  },
  {
    "text": "cover all the stores in search space so that's that's that's the main advantage of this",
    "start": "1828000",
    "end": "1834279"
  },
  {
    "text": "layout next uh here's some interesting numbers that um that we uh observed when",
    "start": "1834279",
    "end": "1840960"
  },
  {
    "text": "we moved from uh one uh unsorted or uncclustered layout to the clustered",
    "start": "1840960",
    "end": "1846399"
  },
  {
    "text": "layouts from location and and store so here's the latency of a single query",
    "start": "1846399",
    "end": "1851679"
  },
  {
    "text": "that is executed before and after clustering and this query returns about",
    "start": "1851679",
    "end": "1856799"
  },
  {
    "text": "4k docs as you can see the retrieval time before uh clustering is around 145",
    "start": "1856799",
    "end": "1862880"
  },
  {
    "text": "milliseconds and the retrieval time after clustering is 60 milliseconds which is about 60% better after",
    "start": "1862880",
    "end": "1869039"
  },
  {
    "text": "clustering the docs based on the query pattern and the down below we share the",
    "start": "1869039",
    "end": "1874559"
  },
  {
    "text": "graph shows the dock ids right time taken to get get each kit in the",
    "start": "1874559",
    "end": "1880080"
  },
  {
    "text": "retrieval loop and as you can see before sorting the kits take and then the",
    "start": "1880080",
    "end": "1885120"
  },
  {
    "text": "latency here is in microsconds the kids can take anywhere from 10 to 60",
    "start": "1885120",
    "end": "1890159"
  },
  {
    "text": "microsconds for a single dock and after sorting as you can see the latency is a",
    "start": "1890159",
    "end": "1896399"
  },
  {
    "text": "lot better like each hits takes less than five",
    "start": "1896399",
    "end": "1900960"
  },
  {
    "text": "microsconds here's the overall improvement in latency uh that we observed when we rolled out this",
    "start": "1901559",
    "end": "1907679"
  },
  {
    "text": "clustered index layout so you can see like more than 50% improvement in P95 we",
    "start": "1907679",
    "end": "1912960"
  },
  {
    "text": "also see equal improvement on on P50 latencies as well and the b other",
    "start": "1912960",
    "end": "1918559"
  },
  {
    "text": "benefit is index size reduced by 20% uh next I'll hand over to Jenny to talk",
    "start": "1918559",
    "end": "1925440"
  },
  {
    "text": "about the ETA indexing thank you Karth",
    "start": "1925440",
    "end": "1932519"
  },
  {
    "text": "so one of the aspects that we talked about as part of inition is the metadata",
    "start": "1932600",
    "end": "1938320"
  },
  {
    "text": "that we get from the upstream services was not passed on to the rest of the stack to be able to do meaningful",
    "start": "1938320",
    "end": "1944159"
  },
  {
    "text": "optimizations on top of it so what this means is that if we take restaurant one and two as part of this",
    "start": "1944159",
    "end": "1950760"
  },
  {
    "text": "example as we index that restaurant one can deliver to hexagon 1 2 3 4 we do not",
    "start": "1950760",
    "end": "1956720"
  },
  {
    "text": "know relative to H1 how far away is H2 how far away is H3 and whatn not this is",
    "start": "1956720",
    "end": "1962240"
  },
  {
    "text": "an important metadata that we needed to pass it to the rankers so the rankers can penalize the farway stores in",
    "start": "1962240",
    "end": "1969200"
  },
  {
    "text": "conjunction with the conversion rate that they have available so uh this information needed to be passed on from",
    "start": "1969200",
    "end": "1975679"
  },
  {
    "text": "the upstream uh team al together so we started off with this so now that we",
    "start": "1975679",
    "end": "1980880"
  },
  {
    "text": "have one more dimension that we needed to index data we were benchmarking a",
    "start": "1980880",
    "end": "1986000"
  },
  {
    "text": "couple of different approaches of how we could have both the hexagon and the ETD indexed and used in the retrieval layer",
    "start": "1986000",
    "end": "1994080"
  },
  {
    "text": "so what we finally ended up doing is that for each and every range after",
    "start": "1994080",
    "end": "2000240"
  },
  {
    "text": "discussions with product and science team and whatnot we aligned on what ranges make sense in terms of our query",
    "start": "2000240",
    "end": "2006480"
  },
  {
    "text": "pattern and we said let's break them down into few ranges that um overall reflects how people are querying the um",
    "start": "2006480",
    "end": "2014000"
  },
  {
    "text": "its ecosystem so we dissected it by multiple of these time ranges 0 to 10",
    "start": "2014000",
    "end": "2019120"
  },
  {
    "text": "minutes 10 to 20 minutes 20 to 30 and whatnot and after we dissected it we also said from this eaters location",
    "start": "2019120",
    "end": "2025679"
  },
  {
    "text": "let's say um hexagon one what are the restaurants which are available in range one range two range three and so on and",
    "start": "2025679",
    "end": "2032000"
  },
  {
    "text": "we did that for every single hexagon available for those of those of you who are following along and then thinking",
    "start": "2032000",
    "end": "2038159"
  },
  {
    "text": "about okay I smell something here so how about there are other different ways of doing things for example in this case",
    "start": "2038159",
    "end": "2044080"
  },
  {
    "text": "there is a trade-off that we make in terms of where do we want the complexity to be should it be in the storage layer",
    "start": "2044080",
    "end": "2050878"
  },
  {
    "text": "or should it be in the CPU in this case specifically if we take a particular restaurant A that restaurant can be in",
    "start": "2050879",
    "end": "2057040"
  },
  {
    "text": "multiple different hex ETV ranges so restaurant A could be 10 minutes from",
    "start": "2057040",
    "end": "2062158"
  },
  {
    "text": "hexagon one 30 minutes from hexagon 2 which means that we store it couple of times or multiple times in this case and",
    "start": "2062159",
    "end": "2068240"
  },
  {
    "text": "that is a case where at the cost of storage and injection level offline optimization we get the benefit of uh",
    "start": "2068240",
    "end": "2076240"
  },
  {
    "text": "making the query faster so we even for this we tried a couple of different",
    "start": "2076240",
    "end": "2081520"
  },
  {
    "text": "approaches and we will soon enough have a blog post which talks more and more about the multiple alternate benchmarks",
    "start": "2081520",
    "end": "2087280"
  },
  {
    "text": "that we did where we would go in depth into one of the other approaches we try we tried a BKD3 approach to see can we",
    "start": "2087280",
    "end": "2094000"
  },
  {
    "text": "do this in login operation and also couple of other approaches around I would only maintain the hexagons as part",
    "start": "2094000",
    "end": "2100480"
  },
  {
    "text": "of the K tree but in the retrieval layer I could make a gRPC call to get the ETD",
    "start": "2100480",
    "end": "2105599"
  },
  {
    "text": "information and then sort it in memory will that work so we did bunch of different things to get there so finally",
    "start": "2105599",
    "end": "2111839"
  },
  {
    "text": "this is how our query layer looks like like Karthik mentioned this is like GRP's layer between delivery and u the",
    "start": "2111839",
    "end": "2118480"
  },
  {
    "text": "search platform so we added a new paradigm of these range queries and we started um having multiple ranges of u r",
    "start": "2118480",
    "end": "2126400"
  },
  {
    "text": "multiple ranges that we can operate with and this enabled us to leverage the power of uh parallelization so to",
    "start": "2126400",
    "end": "2134720"
  },
  {
    "text": "visualize this if a request comes in let's say somewhere in the yellow circle for that request there will be",
    "start": "2134720",
    "end": "2142000"
  },
  {
    "text": "multiple queries which would be sent from the application layer all the way to the storage layer one query would be for the yellow layer which is the",
    "start": "2142000",
    "end": "2148000"
  },
  {
    "text": "closest bucket and another query for uh the light green and dark green and so on and so forth this is how we were able to",
    "start": "2148000",
    "end": "2154800"
  },
  {
    "text": "get this NX and selection expansion at constant latency regardless of which line of business that we care about but",
    "start": "2154800",
    "end": "2161520"
  },
  {
    "text": "it involved changes in every single part of search and delivery ecosystem",
    "start": "2161520",
    "end": "2166560"
  },
  {
    "text": "multiple services multiple engineers like to uh get it to the finish",
    "start": "2166560",
    "end": "2172040"
  },
  {
    "text": "line after we made all of these changes we put it into production and we saw",
    "start": "2172040",
    "end": "2177119"
  },
  {
    "text": "that the latency decreased by 50% which we thought was originally not possible",
    "start": "2177119",
    "end": "2182800"
  },
  {
    "text": "and the cherry on top is we were also able to increase the recall before this",
    "start": "2182800",
    "end": "2188800"
  },
  {
    "text": "we had a different algorithm to query the concentric circle of expanding radius and in that algorithm we made a",
    "start": "2188800",
    "end": "2196480"
  },
  {
    "text": "trade-off between recall and latency in this case we were able to get more and",
    "start": "2196480",
    "end": "2202400"
  },
  {
    "text": "more strokes because that is how the rankers are able to see more and more candidates to make the optimization",
    "start": "2202400",
    "end": "2208920"
  },
  {
    "text": "so one of the other use cases that we haven't spoken about so far but also important enough in terms of customer um",
    "start": "2208920",
    "end": "2216480"
  },
  {
    "text": "customer experience is the non-deliverable stores in Uber at least in the",
    "start": "2216480",
    "end": "2223119"
  },
  {
    "text": "restaurant side there can be many cases where you would look for a store but it is not at available not at deliverable",
    "start": "2223119",
    "end": "2228480"
  },
  {
    "text": "but it is available for pickup the reason this is this exists is based on",
    "start": "2228480",
    "end": "2234560"
  },
  {
    "text": "marketplace conditions where the um merch where where the merchants are located where we could send couers the",
    "start": "2234560",
    "end": "2240320"
  },
  {
    "text": "time of the day and whatnot at some time of the day we won't be able to deliver to a restaurant and this deliverability",
    "start": "2240320",
    "end": "2246240"
  },
  {
    "text": "of a particular uh restaurant is also um dynamic given this we still want the",
    "start": "2246240",
    "end": "2251599"
  },
  {
    "text": "eater to know that hey we do have this restaurant but for some of the reasons we won't be able to deliver uh at this",
    "start": "2251599",
    "end": "2257440"
  },
  {
    "text": "particular point in time so we wanted to support this even in this use case we moved bunch of um uh bunch of complexity",
    "start": "2257440",
    "end": "2265520"
  },
  {
    "text": "from the query layer into the injection layer at the injection layer we did an intersection of how many of these um",
    "start": "2265520",
    "end": "2271599"
  },
  {
    "text": "hexagons are deliverable from the store how many of them are discoverable and we did that discoverable minus deliverable",
    "start": "2271599",
    "end": "2278359"
  },
  {
    "text": "intersection stored it in the index so at the time of query layer we would quite simply be able to say that okay",
    "start": "2278359",
    "end": "2283599"
  },
  {
    "text": "it's either in the deliverable or in the discoverable and I can get it from there so key takeaways in overall what we u",
    "start": "2283599",
    "end": "2292400"
  },
  {
    "text": "wanted to get out of this is we started from first principles when the latency",
    "start": "2292400",
    "end": "2298000"
  },
  {
    "text": "uh shot up we did a benchmark to understand where it is coming from and started to narrow it down to the",
    "start": "2298000",
    "end": "2304560"
  },
  {
    "text": "sharding strategy of hey I have a San Francisco document and I have a bunch of Japan documents because Japan has a most",
    "start": "2304560",
    "end": "2310160"
  },
  {
    "text": "concentrated uh restaurants possible so given that if I were to take a San",
    "start": "2310160",
    "end": "2316240"
  },
  {
    "text": "Francisco request and go through bunch of Japan restaurants that is obviously going to increase the latency that is where the millisecond latency in get",
    "start": "2316240",
    "end": "2322640"
  },
  {
    "text": "next talk came in and index layout is one of the most overlooked uh overlooked",
    "start": "2322640",
    "end": "2328800"
  },
  {
    "text": "piece of uh software that we don't even look at where we needed to spend the two",
    "start": "2328800",
    "end": "2334079"
  },
  {
    "text": "to three years to understand the query pattern and then figure out what is it that we needed to do in our index layout so that it can be optimized for the",
    "start": "2334079",
    "end": "2340480"
  },
  {
    "text": "request pattern that we care about then the sharding strategy uh needed to",
    "start": "2340480",
    "end": "2345920"
  },
  {
    "text": "be aligned based on what we are trying to get at we even saw test stores which were part of the production index which",
    "start": "2345920",
    "end": "2352480"
  },
  {
    "text": "was adding to this latency three times the stores that we had originally were test stores and we",
    "start": "2352480",
    "end": "2358640"
  },
  {
    "text": "were processing all of those things when we were trying to process a realtime request so we needed to create a separate cluster for the test",
    "start": "2358640",
    "end": "2363880"
  },
  {
    "text": "stores so apart from this there were few other",
    "start": "2363880",
    "end": "2369119"
  },
  {
    "text": "expensive operations which used to happen in the query layer we had some fallbacks available at the query layer",
    "start": "2369119",
    "end": "2375839"
  },
  {
    "text": "in any kind of distributed system there is always going to be a timeout there's always going to be some data which is not available from the upstream and when",
    "start": "2375839",
    "end": "2382079"
  },
  {
    "text": "those things happen we used to have a query layer fallback to say hey try to get it from this place or if it if you",
    "start": "2382079",
    "end": "2387440"
  },
  {
    "text": "don't get it from this service get it from this other service or get it from bunch of other places we moved all of this u fallback logic to the injection",
    "start": "2387440",
    "end": "2394560"
  },
  {
    "text": "layer so at the query layer we just know that I'm going to query and get the data that I need and all of the corner cases",
    "start": "2394560",
    "end": "2400480"
  },
  {
    "text": "are being handled apart from the parallelization based on ETD we also had bunch of other",
    "start": "2400480",
    "end": "2406880"
  },
  {
    "text": "parallelizations in terms of hey this is the strong match in terms of query this is a fuzzy match and this is either or",
    "start": "2406880",
    "end": "2412160"
  },
  {
    "text": "match say burger and king would mean that I'm going to look for uh stores",
    "start": "2412160",
    "end": "2417359"
  },
  {
    "text": "which have burger and also look for stores stores which have king and then do much so we did all of these different things to leverage the non-over",
    "start": "2417359",
    "end": "2424640"
  },
  {
    "text": "overlapping subqueries and get the power of parallelization so with that we are",
    "start": "2424640",
    "end": "2429839"
  },
  {
    "text": "towards the end of the talk and the final trivia for the day how much time",
    "start": "2429839",
    "end": "2436560"
  },
  {
    "text": "uh do you think was expected uh to be spent to solve this problem",
    "start": "2436560",
    "end": "2443680"
  },
  {
    "text": "two weeks so we spent about two to three months to identify where the problem is",
    "start": "2446000",
    "end": "2452000"
  },
  {
    "text": "because there are multiple different teams like feed is a separate team ads is a separate team suggestions is a separate team uh thousand engineers",
    "start": "2452000",
    "end": "2458880"
  },
  {
    "text": "together so we needed to instrument in multiple different parts to even identify the problem for like 2 to 3",
    "start": "2458880",
    "end": "2464240"
  },
  {
    "text": "months and it took us like four to 6 months to get to the first um round of XP and right now I think this uh Q1 or",
    "start": "2464240",
    "end": "2472160"
  },
  {
    "text": "so we are available in rest of the world too thank you so much for uh joining us today and we will take the Q&A",
    "start": "2472160",
    "end": "2480680"
  },
  {
    "text": "well thank you so um is there anything that you did all this moving of stuff to the ingestion side is there anything you",
    "start": "2484240",
    "end": "2490800"
  },
  {
    "text": "put in there that you don't need anymore that you want to go back and take out",
    "start": "2490800",
    "end": "2497078"
  },
  {
    "text": "so this architecture that we have is also something which is evolving um so",
    "start": "2497680",
    "end": "2504480"
  },
  {
    "text": "from that perspective there are some changes that we uh did in terms of live",
    "start": "2504480",
    "end": "2510079"
  },
  {
    "text": "edition um I would give an example uh we went with the idea that many use cases",
    "start": "2510079",
    "end": "2516960"
  },
  {
    "text": "need to be live ingested and then we realized that there are some cases which don't even need to be ingested at that",
    "start": "2516960",
    "end": "2522240"
  },
  {
    "text": "time which would also help us in building the indexes faster the SLAs will become better and better so one",
    "start": "2522240",
    "end": "2527520"
  },
  {
    "text": "thing that we decided to take out later is when a new store when a store moves a",
    "start": "2527520",
    "end": "2532720"
  },
  {
    "text": "location uh that location update used to be a live in which will go through Kafka and then get it into the index and they",
    "start": "2532720",
    "end": "2539040"
  },
  {
    "text": "said operation said hey we needed to get it in right after the store moves And it has to be in milliseconds of latency to",
    "start": "2539040",
    "end": "2544079"
  },
  {
    "text": "get it ingested and when we started understanding more and more of what the use case is there is a time period",
    "start": "2544079",
    "end": "2550160"
  },
  {
    "text": "involved between when the store decides to move a location when ops gets to know that and when tech will start moving it",
    "start": "2550160",
    "end": "2557119"
  },
  {
    "text": "they usually have made this decision two or three months in advance and we have time for about a week or two weeks to",
    "start": "2557119",
    "end": "2563839"
  },
  {
    "text": "actually make that transition so we decided that oh uh the priority Q approach that he talked about as part of",
    "start": "2563839",
    "end": "2569760"
  },
  {
    "text": "the injection so we don't need this as a priority because this can go as part of the base index build that is not going",
    "start": "2569760",
    "end": "2575920"
  },
  {
    "text": "to use my computer resources uh any other questions out there did we get the",
    "start": "2575920",
    "end": "2581359"
  },
  {
    "text": "swag handed out during the trivia no uh if you have a question you're going to get a prize now are there any",
    "start": "2581359",
    "end": "2588520"
  },
  {
    "text": "questions there's one right here",
    "start": "2588520",
    "end": "2593160"
  },
  {
    "text": "thank you yeah my question is about the um you you mentioned about the two month",
    "start": "2597440",
    "end": "2603280"
  },
  {
    "text": "to identify the problem and it takes two weeks to solve it so what kind of obserability platform do you use to to",
    "start": "2603280",
    "end": "2610880"
  },
  {
    "text": "measure this metrics do you use any open telemetry to identify where those query",
    "start": "2610880",
    "end": "2617520"
  },
  {
    "text": "is slowing down and what queries are performing so a quick clarification",
    "start": "2617520",
    "end": "2622880"
  },
  {
    "text": "there the expectation when we started the problem was that we will land it in",
    "start": "2622880",
    "end": "2627920"
  },
  {
    "text": "two weeks not that it took us two weeks to solve the problem i see so to your",
    "start": "2627920",
    "end": "2633359"
  },
  {
    "text": "question on open telemetry so we have in-house uh telemetry services that we",
    "start": "2633359",
    "end": "2639200"
  },
  {
    "text": "have in place in fact there is one company branched out of some of the engineers who worked in the team so uh",
    "start": "2639200",
    "end": "2644480"
  },
  {
    "text": "we use M3 that is our metric system and that is what we integrated when we started",
    "start": "2644480",
    "end": "2650319"
  },
  {
    "text": "aer for tracing tracing and when we started instrumenting it at that time our search infrastructure wasn't",
    "start": "2650319",
    "end": "2656240"
  },
  {
    "text": "integrated with M3 so that was also something that we needed to do along the way to get it instrumented and then get",
    "start": "2656240",
    "end": "2661520"
  },
  {
    "text": "it out the door um one reason we didn't do that at that time was because of uh",
    "start": "2661520",
    "end": "2666960"
  },
  {
    "text": "inmemory usage for it's a sidecar agent so because of that we didn't want to have that inmemory usage at the time of",
    "start": "2666960",
    "end": "2672560"
  },
  {
    "text": "production so we spun off a separate cluster which was very identical in terms of uh hardware configurations and",
    "start": "2672560",
    "end": "2679760"
  },
  {
    "text": "um capacity and that is where we did all of our benchmarks so that it doesn't impact production okay that's my",
    "start": "2679760",
    "end": "2686240"
  },
  {
    "text": "question yeah thank you time for one more",
    "start": "2686240",
    "end": "2692079"
  },
  {
    "text": "there is one here too okay so my question is more around the",
    "start": "2692079",
    "end": "2697440"
  },
  {
    "text": "indexing side of things so um you said you use lucine for indexing uh does that",
    "start": "2697440",
    "end": "2704000"
  },
  {
    "text": "mean that for searching specifically you have a separate cluster that is specifically used just for searching",
    "start": "2704000",
    "end": "2710319"
  },
  {
    "text": "versus indexing i mean or is it the same cluster that serves both reads and writes um so so we use the same cluster",
    "start": "2710319",
    "end": "2718880"
  },
  {
    "text": "for um search and indexing at the time but if you notice the architecture uh we",
    "start": "2718880",
    "end": "2726000"
  },
  {
    "text": "have uh two components of injection one is the batch injection and the realtime injection so what we do is we move all",
    "start": "2726000",
    "end": "2733119"
  },
  {
    "text": "of the heavy lifting on the indexing to the batch side and the live injection or realtime injection is kept lightweight",
    "start": "2733119",
    "end": "2740240"
  },
  {
    "text": "so that the serving system is so the utilization the CPU or the resources of",
    "start": "2740240",
    "end": "2746800"
  },
  {
    "text": "the serving search node is utilized mostly for queries very little is used for indexing that that's the current",
    "start": "2746800",
    "end": "2753359"
  },
  {
    "text": "state and and we are also working on the next generation system where we are going to fully separate the searcher and",
    "start": "2753359",
    "end": "2760079"
  },
  {
    "text": "the indexer I hope that answers your question so do",
    "start": "2760079",
    "end": "2765280"
  },
  {
    "text": "you have one more quick one oh we got more i'm going to take one more um and I know lunch is up next so we're gonna",
    "start": "2765280",
    "end": "2772640"
  },
  {
    "text": "hold the questions at this hey so uh I would think that most",
    "start": "2772640",
    "end": "2779839"
  },
  {
    "text": "people are are querying for food when they're at home or at work and so like subsequent queries um are going to be",
    "start": "2779839",
    "end": "2786640"
  },
  {
    "text": "like pretty common do you all do anything where uh in order to reduce the search space you",
    "start": "2786640",
    "end": "2792480"
  },
  {
    "text": "effectively like cache the the hex cells on the like subsequent calls so for",
    "start": "2792480",
    "end": "2797920"
  },
  {
    "text": "example like if I'm at my house you know like on the first query you have to go out and do the work to determine what",
    "start": "2797920",
    "end": "2803520"
  },
  {
    "text": "the boundaries are but then on the subsequent queries like the geography isn't changing the only thing that's changing is the the restaurants and so",
    "start": "2803520",
    "end": "2810560"
  },
  {
    "text": "have you all looked at that uh like type of caching for subsequent queries we do have a cache in place not for the",
    "start": "2810560",
    "end": "2817920"
  },
  {
    "text": "purposes that you're looking for so we don't cache some of these requests",
    "start": "2817920",
    "end": "2824400"
  },
  {
    "text": "because if we look at uh the session so throughout the session we do have some information that we maintain in memory",
    "start": "2824400",
    "end": "2830240"
  },
  {
    "text": "and then we could serve from there so uh we haven't done a distributed cache there and many a times we want to also",
    "start": "2830240",
    "end": "2837760"
  },
  {
    "text": "be able to dynamically look at store availability item availability which changes very often especially during the",
    "start": "2837760",
    "end": "2844240"
  },
  {
    "text": "peak times um people run out of things like restaurants run out of things so because of that we don't intentionally",
    "start": "2844240",
    "end": "2849440"
  },
  {
    "text": "do intentionally do caching for that particular purpose and also the delivery radius or the deliverability",
    "start": "2849440",
    "end": "2856960"
  },
  {
    "text": "also expands and shrinks based on surge based on whether there is accidents in",
    "start": "2856960",
    "end": "2863119"
  },
  {
    "text": "that area rerouting happens and whatnot so there is lot of those things in place",
    "start": "2863119",
    "end": "2868960"
  },
  {
    "text": "and if there is an incident someone could go change rejigger this delivery zones too so we want that real time to",
    "start": "2868960",
    "end": "2875839"
  },
  {
    "text": "reflect because the last thing someone wants is to be able to add everything into their cart and then see that the store is no longer available that is the",
    "start": "2875839",
    "end": "2882480"
  },
  {
    "text": "reason we don't invest heavily in caching at the time of retrieval in that path but we use it for a different path",
    "start": "2882480",
    "end": "2888240"
  },
  {
    "text": "in the query layer well that was really a good question um uh another thank you for Janani and",
    "start": "2888240",
    "end": "2894560"
  },
  {
    "text": "Cararthik",
    "start": "2894560",
    "end": "2897560"
  }
]