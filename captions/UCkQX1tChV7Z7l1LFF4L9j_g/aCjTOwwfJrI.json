[
  {
    "start": "0",
    "end": "126000"
  },
  {
    "text": "so about two and a half years ago I found myself in a room with four other",
    "start": "5160",
    "end": "11620"
  },
  {
    "text": "engineers at Skyscanner and we were tasked with answering a question that I thought should be fairly simple to",
    "start": "11620",
    "end": "17619"
  },
  {
    "text": "answer and that question was where does our company's revenue come from as it",
    "start": "17619",
    "end": "23440"
  },
  {
    "text": "turned out it wasn't quite that easy to answer but the reason we had this question in the first place was we now had to comply",
    "start": "23440",
    "end": "29140"
  },
  {
    "text": "with regulations known as Sox but it's in the u.s. its stands for sarbanes-oxley and it essentially meant",
    "start": "29140",
    "end": "35980"
  },
  {
    "text": "that we needed to be able to show for every line in our financial statements",
    "start": "35980",
    "end": "41380"
  },
  {
    "text": "and our records every line there was traceable and that we could prove that the data there was reliable and from",
    "start": "41380",
    "end": "47680"
  },
  {
    "text": "reliable sources and that it couldn't have been altered in any fundamental way by anyone inside or outside our company",
    "start": "47680",
    "end": "53860"
  },
  {
    "text": "and in this case it like I said it",
    "start": "53860",
    "end": "60040"
  },
  {
    "text": "wasn't that easy to answer like we were scribbling on whiteboards trolling through clear github answering asking",
    "start": "60040",
    "end": "66130"
  },
  {
    "text": "questions to other teams trying to figure out how does revenue actually flow through our streaming pipeline and",
    "start": "66130",
    "end": "71909"
  },
  {
    "text": "it actually took us a couple of weeks you get enough confidence in the answer to give a recommendation say yeah this",
    "start": "71909",
    "end": "78190"
  },
  {
    "text": "is exactly how we're going to secure it this is how we're gonna make sure that these numbers don't get fundamentally",
    "start": "78190",
    "end": "83740"
  },
  {
    "text": "changed and it gave me quite a bit of pause like how did we end up in this situation where no one individual or one",
    "start": "83740",
    "end": "91720"
  },
  {
    "text": "team even could tell from beginning to end from producer all the way to financial statements like how the data",
    "start": "91720",
    "end": "98560"
  },
  {
    "text": "flowed and what was happening in there so but I'd like to do today is share a",
    "start": "98560",
    "end": "103570"
  },
  {
    "text": "story with you of how the data platform at Skyscanner evolved over time from",
    "start": "103570",
    "end": "109420"
  },
  {
    "text": "batch to streaming and to both and by sharing your stories I hope to share",
    "start": "109420",
    "end": "116470"
  },
  {
    "text": "some of the lessons and help you avoid some of the traps that we fell into along this journey doing things in a",
    "start": "116470",
    "end": "123430"
  },
  {
    "text": "fairly agile fashion as we did now when I originally submitted this talk the",
    "start": "123430",
    "end": "130060"
  },
  {
    "start": "126000",
    "end": "224000"
  },
  {
    "text": "title was from batch to streaming to both and and two weeks funneling I've been",
    "start": "130060",
    "end": "136319"
  },
  {
    "text": "rethinking it and thought that a more accurate title might in fact be from batch to streaming to both and then back",
    "start": "136319",
    "end": "142560"
  },
  {
    "text": "again perhaps and I'll get into why",
    "start": "142560",
    "end": "148049"
  },
  {
    "text": "there's a question mark at the end there and this this new title wasn't quite as catchy so I didn't change it but just to",
    "start": "148049",
    "end": "153870"
  },
  {
    "text": "give you some idea of where we're going and the kind of thing I'll be talking about later on my name is Herman like",
    "start": "153870",
    "end": "160859"
  },
  {
    "text": "you said I work at Skyscanner if you haven't heard of Skyscanner we",
    "start": "160859",
    "end": "166500"
  },
  {
    "text": "help you plan trips and we do that by helping you find the best flights from A",
    "start": "166500",
    "end": "172859"
  },
  {
    "text": "to B when you get to B you can also find good accommodation and car hire all",
    "start": "172859",
    "end": "178620"
  },
  {
    "text": "these things and when I say good flight that might actually differ depending on",
    "start": "178620",
    "end": "184560"
  },
  {
    "text": "the person that might be in cheapest it might mean lowest carbon footprint it might mean most comfortable and we allow",
    "start": "184560",
    "end": "192989"
  },
  {
    "text": "you to search across all these different things and over the years we've grown to",
    "start": "192989",
    "end": "198120"
  },
  {
    "text": "be fairly popular so it's a peak we have about a hundred million user monthly",
    "start": "198120",
    "end": "205079"
  },
  {
    "text": "active users it's available in 30 plus languages we've had 100 million or more",
    "start": "205079",
    "end": "211739"
  },
  {
    "text": "app downloads and we have about a thousand 200 partners by which I mean",
    "start": "211739",
    "end": "218310"
  },
  {
    "text": "online travel agents and airlines and hotels and so on so fairly big and with",
    "start": "218310",
    "end": "227400"
  },
  {
    "start": "224000",
    "end": "280000"
  },
  {
    "text": "that scale comes also a pretty big data platform now what I'm showing you here",
    "start": "227400",
    "end": "233190"
  },
  {
    "text": "is the number of messages per second that we were processing from 2016 to",
    "start": "233190",
    "end": "238530"
  },
  {
    "text": "fairly recently in our streaming data platform now our platform wasn't always streaming so this isn't the full",
    "start": "238530",
    "end": "244859"
  },
  {
    "text": "timeline but back in 2016 we were doing about 20,000 messages per second later",
    "start": "244859",
    "end": "251639"
  },
  {
    "text": "the year about 80,000 messages per second and that grew to about two million messages per second now that's",
    "start": "251639",
    "end": "258989"
  },
  {
    "text": "fairly big scale to put that into perspective it's about 172",
    "start": "258989",
    "end": "264010"
  },
  {
    "text": "per day it would be several petabytes if we stored everything which we don't",
    "start": "264010",
    "end": "271920"
  },
  {
    "text": "but even if you sorted in an efficient format like Parque like it would be quite a lot of data now our story",
    "start": "271920",
    "end": "281350"
  },
  {
    "start": "280000",
    "end": "337000"
  },
  {
    "text": "started with a batch my plan like I alluded to and like many organizations",
    "start": "281350",
    "end": "288540"
  },
  {
    "text": "something you're probably familiar with we started with data being in lots of disparate sources we had Mixpanel we had",
    "start": "288540",
    "end": "295360"
  },
  {
    "text": "Google Analytics we had own app database that we lovingly referred to as the cube",
    "start": "295360",
    "end": "300970"
  },
  {
    "text": "and this cube allowed you to do slice and dice queries fairly effectively and",
    "start": "300970",
    "end": "306550"
  },
  {
    "text": "efficiently but it was hosted on premise because we had not yet transition to the cloud and we were running into some",
    "start": "306550",
    "end": "314170"
  },
  {
    "text": "scaling problems like firstly we didn't really want to wait 24 hours for patch extracts to happen and as we started",
    "start": "314170",
    "end": "323080"
  },
  {
    "text": "processing more and more and more messages the cube was starting to show its age and so together with a",
    "start": "323080",
    "end": "328930"
  },
  {
    "text": "transition to the cloud and AWS we started moving and exploring the",
    "start": "328930",
    "end": "335170"
  },
  {
    "text": "possibilities of streaming data pipeline so we went from batch to streaming and",
    "start": "335170",
    "end": "342760"
  },
  {
    "start": "337000",
    "end": "449000"
  },
  {
    "text": "we didn't really know whether this whole streaming thing was actually going to catch on we put Kafka in the middle",
    "start": "342760",
    "end": "349180"
  },
  {
    "text": "looked like a really promising technology at the time and we said all",
    "start": "349180",
    "end": "354910"
  },
  {
    "text": "right on the producer side we'll have our servers communicating directly with",
    "start": "354910",
    "end": "361990"
  },
  {
    "text": "Kafka writing messages there and we'll have an HTTP API with which we can use",
    "start": "361990",
    "end": "368440"
  },
  {
    "text": "for web clients and wha-la clients they can write to that instead using rest and JSON and this HTTP API would convey",
    "start": "368440",
    "end": "376810"
  },
  {
    "text": "convert the JSON to credible so pretty early on we had this convention that all",
    "start": "376810",
    "end": "382390"
  },
  {
    "text": "topics in Kafka should be pretty buff this is a really good thing if you're",
    "start": "382390",
    "end": "387880"
  },
  {
    "text": "not using something like beretta buff as a message format like Avro or Swift or any other number of ones you're not",
    "start": "387880",
    "end": "395110"
  },
  {
    "text": "using a binary format like that I really reckon and you go out and do that and we'll see",
    "start": "395110",
    "end": "400220"
  },
  {
    "text": "some of the benefits later later in this talk but an interesting thing here is that we used our Catholic Laster for",
    "start": "400220",
    "end": "406190"
  },
  {
    "text": "three very different kinds of data as well so we had metrics we had business events and we had application logs",
    "start": "406190",
    "end": "412840"
  },
  {
    "text": "metrics would be pulled from Kafka and delivered to open TS DB which is a very",
    "start": "412840",
    "end": "418370"
  },
  {
    "text": "scalable time series database based on HBase and our data platform users will",
    "start": "418370",
    "end": "423949"
  },
  {
    "text": "be able to create that through gov fauna paying business events that would go to s3 as park' and be variable through",
    "start": "423949",
    "end": "432199"
  },
  {
    "text": "systems like Athena or redshift or any number of engines that we later added and application logs would be sent to",
    "start": "432199",
    "end": "441530"
  },
  {
    "text": "elasticsearch and our customers within Skyscanner the engineers would be able to create their application logs in",
    "start": "441530",
    "end": "447139"
  },
  {
    "text": "Campania and I just want to say that this is still a good design in my",
    "start": "447139",
    "end": "453680"
  },
  {
    "start": "449000",
    "end": "591000"
  },
  {
    "text": "opinion I think it's fundamentally a really good thing to do because it's sort of paradoxically both decoupled and",
    "start": "453680",
    "end": "459320"
  },
  {
    "text": "unified it's decoupled in the sense that the producers no longer need to know",
    "start": "459320",
    "end": "464539"
  },
  {
    "text": "which consumers are going to be interested in the data they could just produce do Kafka and kind of forget",
    "start": "464539",
    "end": "470180"
  },
  {
    "text": "about it that's good and you the consumers similarly don't need to know",
    "start": "470180",
    "end": "475699"
  },
  {
    "text": "exactly who produced the data and you don't have this web of interactions that you might have if all the consumers need",
    "start": "475699",
    "end": "481490"
  },
  {
    "text": "to be aware of the producers and vice versa so it's decoupled that's a good",
    "start": "481490",
    "end": "486710"
  },
  {
    "text": "thing but it's simultaneously unified in the sense that all the messages now have",
    "start": "486710",
    "end": "492020"
  },
  {
    "text": "to go through Kafka and Kafka becomes do something your single source of truth and in fact around that time we started",
    "start": "492020",
    "end": "499580"
  },
  {
    "text": "talking about this concept of the single unified log which basically just means that all the messages would now go",
    "start": "499580",
    "end": "507500"
  },
  {
    "text": "through Kafka if you had a question you could answer it from Kafka using the data in Kafka and all the other systems",
    "start": "507500",
    "end": "512959"
  },
  {
    "text": "would derive their truth from Kafka from the single unified log now this was all",
    "start": "512959",
    "end": "521120"
  },
  {
    "text": "good our users really liked it but pretty soon they were telling us hey",
    "start": "521120",
    "end": "526819"
  },
  {
    "text": "you know what what we'd really like to do is add some transformations to the topics that we have in Africa now we",
    "start": "526819",
    "end": "533730"
  },
  {
    "text": "would like to merge topics we'd like to split them into only the information",
    "start": "533730",
    "end": "538829"
  },
  {
    "text": "that we care about or we'd like to enrich them with further information from other api's and we said that's",
    "start": "538829",
    "end": "544620"
  },
  {
    "text": "great this is exactly what we think stream processing is for and we wanted to",
    "start": "544620",
    "end": "550139"
  },
  {
    "text": "exercise a little bit of control though we didn't want everyone to reinvent the wheel and build their own stream processing system so we standardized on",
    "start": "550139",
    "end": "557430"
  },
  {
    "text": "Sam's Apache Samsa if you haven't heard this you can also think of storm patchy",
    "start": "557430",
    "end": "563670"
  },
  {
    "text": "storm is another similar one we standardized in Samsa and we basically made a templated version of this that",
    "start": "563670",
    "end": "569550"
  },
  {
    "text": "any team in Skyscanner could easily create within a couple of clicks by filling out two very simple java",
    "start": "569550",
    "end": "576420"
  },
  {
    "text": "functions you could have a stream processor up and running within minutes and fairly soon we had hundreds of these",
    "start": "576420",
    "end": "582029"
  },
  {
    "text": "sansa jobs running and I could do any kind of transformation we didn't put any",
    "start": "582029",
    "end": "587279"
  },
  {
    "text": "limits on that and things were great we were pretty happy that was one problem",
    "start": "587279",
    "end": "592439"
  },
  {
    "start": "591000",
    "end": "705000"
  },
  {
    "text": "however that we found also fairly early on which was that as we were",
    "start": "592439",
    "end": "599939"
  },
  {
    "text": "transitioning to the cloud we had some services that had hundreds of containers and when they did blue/green deploy that",
    "start": "599939",
    "end": "608339"
  },
  {
    "text": "would suddenly add hundreds of new connections to the caster cluster that",
    "start": "608339",
    "end": "614069"
  },
  {
    "text": "would then have a cascading effect of lots of extra metadata on the cluster and would cause some temporary",
    "start": "614069",
    "end": "619439"
  },
  {
    "text": "degradation and we decided that for that reason mainly we would add we would make",
    "start": "619439",
    "end": "627240"
  },
  {
    "text": "sure that the servers within Skyscanner worked are now deployed a bilious would also talk through this HTTP API to Kafka",
    "start": "627240",
    "end": "634500"
  },
  {
    "text": "so they weren't talking to Cass go directly anymore and this was a really really good move in retrospect because",
    "start": "634500",
    "end": "642720"
  },
  {
    "text": "it actually allowed us to decouple the production from Kafka itself which means",
    "start": "642720",
    "end": "649980"
  },
  {
    "text": "we now have a place we could implement any kind of business logic so we could enforce the usage of protobuf rather",
    "start": "649980",
    "end": "655649"
  },
  {
    "text": "than how as a convention we could check that certain fields are set we can make sure that messages are good enough quality to",
    "start": "655649",
    "end": "662250"
  },
  {
    "text": "go in all these things and also some other surprising benefits like we could have this API deployed in multiple",
    "start": "662250",
    "end": "669779"
  },
  {
    "text": "regions where as our calf Custer was only deployed in one region at the time",
    "start": "669779",
    "end": "675240"
  },
  {
    "text": "and this API would be in multiple regions we could have local syncs that",
    "start": "675240",
    "end": "681420"
  },
  {
    "text": "we write the data to and then forwarded the data on to our cluster the central one which means that write resiliency is",
    "start": "681420",
    "end": "688440"
  },
  {
    "text": "a lot higher even if our central kappa cluster was having an issue we would not",
    "start": "688440",
    "end": "693450"
  },
  {
    "text": "lose messages we would still be able to take those weather from server side or from mobile devices or from the web so",
    "start": "693450",
    "end": "700950"
  },
  {
    "text": "this proxy was a really great move and",
    "start": "700950",
    "end": "706610"
  },
  {
    "start": "705000",
    "end": "806000"
  },
  {
    "text": "we we felt like this was we were in a pretty good place but then we realized",
    "start": "706610",
    "end": "716790"
  },
  {
    "text": "that maybe we were we have become the victims of our own success to some",
    "start": "716790",
    "end": "723390"
  },
  {
    "text": "extent so we now had about a hundred thousand messages giving per second going through the platform and it was in",
    "start": "723390",
    "end": "730230"
  },
  {
    "text": "this room where we were trying to trace our revenue events that we saw that actually these revenue events were going",
    "start": "730230",
    "end": "735959"
  },
  {
    "text": "through an epic journey so first they would go from the server to the HTTP API",
    "start": "735959",
    "end": "741209"
  },
  {
    "text": "that's all easy enough then to Kafka that's all as we expected",
    "start": "741209",
    "end": "748080"
  },
  {
    "text": "then it's time the job would pick that up it'll pick up this revenue event it might add some extra information something like device information",
    "start": "748080",
    "end": "756029"
  },
  {
    "text": "extracted from the user agent for example okay that's good then another",
    "start": "756029",
    "end": "761430"
  },
  {
    "text": "Center job would read from the second topic and write out to a third topic and in this case it might add some",
    "start": "761430",
    "end": "767520"
  },
  {
    "text": "information like geolocation information and as we started tracing it we realized",
    "start": "767520",
    "end": "773970"
  },
  {
    "text": "it was a third Center job writing to a fourth topic and this fourth job and a",
    "start": "773970",
    "end": "779220"
  },
  {
    "text": "fifth and so we went on long long epic journey all the way until finally the",
    "start": "779220",
    "end": "785430"
  },
  {
    "text": "final times a job number of hops later at this point we probably needed a sense of job to do",
    "start": "785430",
    "end": "791180"
  },
  {
    "text": "some TV application because as you know distributed systems introduced duplicates pretty easily so before we",
    "start": "791180",
    "end": "797090"
  },
  {
    "text": "commit the final results that we will use in our reports to s3 we have a DJ",
    "start": "797090",
    "end": "802220"
  },
  {
    "text": "plication job in there as well this was fairly complicated and so the first",
    "start": "802220",
    "end": "808310"
  },
  {
    "start": "806000",
    "end": "893000"
  },
  {
    "text": "lesson that I want to share is that Conway's law is true for data platforms",
    "start": "808310",
    "end": "814000"
  },
  {
    "text": "now if you haven't heard Conway's law before the basically says is that",
    "start": "814000",
    "end": "819550"
  },
  {
    "text": "organizations which design systems are constrained to produce designs which are",
    "start": "819550",
    "end": "825710"
  },
  {
    "text": "copies of their communication structures and here I've substituted systems for data platforms because I think the same",
    "start": "825710",
    "end": "833150"
  },
  {
    "text": "thing goes for data platforms in particular and when we were looking at",
    "start": "833150",
    "end": "838460"
  },
  {
    "text": "our revenue events in this case we realized that all these different hops all these different sums of jobs",
    "start": "838460",
    "end": "844550"
  },
  {
    "text": "basically mirrored all the different organizational hops that the day that I",
    "start": "844550",
    "end": "850040"
  },
  {
    "text": "had to go through like and to understand this it helps to give them to give a little bit of context about how Skyscanner is structured we operate",
    "start": "850040",
    "end": "856760"
  },
  {
    "text": "using squads and tribes model which basically means we have very small 67",
    "start": "856760",
    "end": "862340"
  },
  {
    "text": "people economist teams and they solve problems in an independent way they try",
    "start": "862340",
    "end": "868820"
  },
  {
    "text": "not to have too many dependencies and so what happened was for example the",
    "start": "868820",
    "end": "874220"
  },
  {
    "text": "producers would produce the data it wasn't in their interest really to make",
    "start": "874220",
    "end": "879230"
  },
  {
    "text": "sure it made it all the way through they didn't have the full picture similarly other teams would create stands of jobs",
    "start": "879230",
    "end": "884870"
  },
  {
    "text": "in rich that write it back and so forth all the way through to the financial reports but no one single team had the",
    "start": "884870",
    "end": "890900"
  },
  {
    "text": "full picture so Conway's law is true and we perhaps",
    "start": "890900",
    "end": "898340"
  },
  {
    "start": "893000",
    "end": "949000"
  },
  {
    "text": "didn't realize this enough early on but one thing we did say was that we're a",
    "start": "898340",
    "end": "904670"
  },
  {
    "text": "small platform team we're trying to do this stream processing thing for quite a large company and we're trying to be to",
    "start": "904670",
    "end": "912770"
  },
  {
    "text": "make that that for myself serve as possible and I still thinks being self-serve is very good basically means",
    "start": "912770",
    "end": "917900"
  },
  {
    "text": "we we enable our customers as data platform users to use our",
    "start": "917900",
    "end": "923990"
  },
  {
    "text": "platform without us needing to get involved women without us need to have deep knowledge of every single data set",
    "start": "923990",
    "end": "929810"
  },
  {
    "text": "because as a small platform team you just can't do that what we didn't realize there was that metadata then",
    "start": "929810",
    "end": "936410"
  },
  {
    "text": "becomes absolutely critical and it was metadata to some extent that we lacked",
    "start": "936410",
    "end": "942980"
  },
  {
    "text": "when we were looking at the pipeline and trying to figure out how events flowed through the system so let's talk about",
    "start": "942980",
    "end": "951110"
  },
  {
    "start": "949000",
    "end": "1072000"
  },
  {
    "text": "metadata you may have noticed if you",
    "start": "951110",
    "end": "956890"
  },
  {
    "text": "very eagerly in the architecture diagram I showed earlier that the that there was",
    "start": "956890",
    "end": "965990"
  },
  {
    "text": "no schema registry in that diagram so how did consumers know which schema to",
    "start": "965990",
    "end": "972560"
  },
  {
    "text": "use and how did producers like tell",
    "start": "972560",
    "end": "978740"
  },
  {
    "text": "consumers which came up to use and the answer to that in Skyscanner case at least is convention",
    "start": "978740",
    "end": "984410"
  },
  {
    "text": "we had a simple convention for topic names and every topic that's kind of looks like this it's like prod or",
    "start": "984410",
    "end": "991100"
  },
  {
    "text": "sandbox or local to the environment name and then a dot and the service name dot",
    "start": "991100",
    "end": "996200"
  },
  {
    "text": "event name dot schema dot from a glance which schema was used and",
    "start": "996200",
    "end": "1005070"
  },
  {
    "text": "we all we store all the schemas in a central repository where everyone has access to it and you can easily see ok",
    "start": "1005070",
    "end": "1011980"
  },
  {
    "text": "this is the schema and the myth the schema file name and the message and can look it up in that repository and this was a pretty good convention because it",
    "start": "1011980",
    "end": "1019240"
  },
  {
    "text": "actually allowed us as a small platform team to share quite a bit of data about",
    "start": "1019240",
    "end": "1024640"
  },
  {
    "text": "each topic and know how to interpret the data in that topic without us needing to go build an API to support this nowadays",
    "start": "1024640",
    "end": "1032620"
  },
  {
    "text": "confluence topically schema registry might actually do the same thing however until recently I believe it at least",
    "start": "1032620",
    "end": "1039730"
  },
  {
    "text": "didn't support crota buff back and maybe build this and stop the schema she didn't exist so pretty soon we had",
    "start": "1039730",
    "end": "1048459"
  },
  {
    "text": "topics looking like this product identity service audit identity coded message that's",
    "start": "1048459",
    "end": "1054530"
  },
  {
    "text": "fairly good it tells you what it is and what schema to use when you read it we",
    "start": "1054530",
    "end": "1059690"
  },
  {
    "text": "also had topics like product flying circus blogged about that message which is less useful but it came down to the",
    "start": "1059690",
    "end": "1066440"
  },
  {
    "text": "team how useful they want to make these topic names and how much context they want to give in it and it was it was a very useful thing now I want to just",
    "start": "1066440",
    "end": "1074360"
  },
  {
    "start": "1072000",
    "end": "1161000"
  },
  {
    "text": "think we share between three different types of metadata descriptive structural",
    "start": "1074360",
    "end": "1081520"
  },
  {
    "text": "administrative with descriptive metadata you're answering questions like what",
    "start": "1081520",
    "end": "1088520"
  },
  {
    "text": "does this dataset mean who owns it what does it contain where does it come",
    "start": "1088520",
    "end": "1093860"
  },
  {
    "text": "from structural metadata is more about how does this relate to other datasets",
    "start": "1093860",
    "end": "1100690"
  },
  {
    "text": "how is it organized how is this datasets sorted and petitioned an administrative",
    "start": "1100690",
    "end": "1107630"
  },
  {
    "text": "which is about how far does this data set date back how frequently is it updated how large",
    "start": "1107630",
    "end": "1113270"
  },
  {
    "text": "is it how complete is it and all three gowns are very important when we started",
    "start": "1113270",
    "end": "1119060"
  },
  {
    "text": "looking into it we had some descriptive metadata by virtue of using protobuf and by having",
    "start": "1119060",
    "end": "1124640"
  },
  {
    "text": "our topic naming convention that was good we had some structural metadata but",
    "start": "1124640",
    "end": "1130700"
  },
  {
    "text": "we didn't know not really not without a lot of effort at least how datasets relate to one and out there this was",
    "start": "1130700",
    "end": "1136820"
  },
  {
    "text": "something we missed and we could we could tell if customers were to ask us how something was",
    "start": "1136820",
    "end": "1142940"
  },
  {
    "text": "partitioned we could go and look that up but it wasn't easy for our customers to tell that and then administrative",
    "start": "1142940",
    "end": "1149060"
  },
  {
    "text": "metadata we didn't have much of it at all similarly we could go look it up if someone asked was a fair bit of trouble",
    "start": "1149060",
    "end": "1155810"
  },
  {
    "text": "but we didn't surface this to our customers and we didn't have a lot of visibility into this ourselves",
    "start": "1155810",
    "end": "1161980"
  },
  {
    "start": "1161000",
    "end": "1238000"
  },
  {
    "text": "so the second lesson is that metadata is critical especially when you're building a self-serve platform that you want a",
    "start": "1161980",
    "end": "1169130"
  },
  {
    "text": "lot of people to use in your organization and especially the relationships between data sets like if",
    "start": "1169130",
    "end": "1175250"
  },
  {
    "text": "we had this information determining the revenue pipeline and determining where you came from would have been a lot",
    "start": "1175250",
    "end": "1181280"
  },
  {
    "text": "easier and ideally of course it would be automated and ideally we'd have all this from the start",
    "start": "1181280",
    "end": "1186690"
  },
  {
    "text": "but we didn't so we did the next best thing which was we started to gather this information manually when users",
    "start": "1186690",
    "end": "1193710"
  },
  {
    "text": "wanted to archive something to s3 we asked them could you please use your",
    "start": "1193710",
    "end": "1199140"
  },
  {
    "text": "knowledge of this system to tell us what is the hierarchy and how do I load events actually flow to make it all the",
    "start": "1199140",
    "end": "1205800"
  },
  {
    "text": "way to the final topic that you want now once you commit to s3 and that was the start it gave us some form of metadata",
    "start": "1205800",
    "end": "1211350"
  },
  {
    "text": "we did it knowing that this would get out of date and it would be better if",
    "start": "1211350",
    "end": "1216600"
  },
  {
    "text": "this was automated but a bit of metadata is already better than not at all and",
    "start": "1216600",
    "end": "1223010"
  },
  {
    "text": "like I mentioned the tools like schema registry or start here but they're not",
    "start": "1223010",
    "end": "1228240"
  },
  {
    "text": "the full solution you need to also think about how you can make these relationships between data sets and the",
    "start": "1228240",
    "end": "1234180"
  },
  {
    "text": "three different types of metadata we just talked about make those visible as well now many of you have robably seen",
    "start": "1234180",
    "end": "1241770"
  },
  {
    "start": "1238000",
    "end": "1350000"
  },
  {
    "text": "this picture before it's from an X sed comic and it shows the flow of characters through Jurassic Park and I",
    "start": "1241770",
    "end": "1249660"
  },
  {
    "text": "like to compare this two topics blowing through Kafka so just to explain this diagram briefly on the x-axis we have",
    "start": "1249660",
    "end": "1255360"
  },
  {
    "text": "time and on the y-axis we have the proximity of different characters to one",
    "start": "1255360",
    "end": "1260430"
  },
  {
    "text": "another so fear if the characters are in the same scene then the lines are close together and if they're far apart",
    "start": "1260430",
    "end": "1266510"
  },
  {
    "text": "geographically then they are the lines are further apart and as you might",
    "start": "1266510",
    "end": "1272700"
  },
  {
    "text": "expect in Jurassic Park some characters make it all the way through some characters don't make it all the way through and the reason I'm saying I'm",
    "start": "1272700",
    "end": "1281490"
  },
  {
    "text": "saying this is a bit like a Kefka cluster is that topic so where are we",
    "start": "1281490",
    "end": "1294150"
  },
  {
    "text": "Jurassic Park we throw this with what our pipeline looked like it looked a bit",
    "start": "1294150",
    "end": "1299760"
  },
  {
    "text": "more like the movie primer which I don't know if everyone's seen it it's a good",
    "start": "1299760",
    "end": "1305490"
  },
  {
    "text": "movie go and watch it so let me start looking into it things are pretty",
    "start": "1305490",
    "end": "1310770"
  },
  {
    "text": "difficult to follow that's kind of the point here and there engineers control the pot line and by this I mean that",
    "start": "1310770",
    "end": "1318150"
  },
  {
    "text": "when you design the system the restrictions you put in place on your data platform users kind of determine",
    "start": "1318150",
    "end": "1325530"
  },
  {
    "text": "the plot line you're gonna get it determines whether you're gonna get primer or 12 Angry Men both very good",
    "start": "1325530",
    "end": "1333240"
  },
  {
    "text": "movies you haven't watched them I recommend you go and watch them but if I had to choose a plot line for my",
    "start": "1333240",
    "end": "1340590"
  },
  {
    "text": "platform I would rather have a plot like 12 Angry Men then for primer then like primer um and",
    "start": "1340590",
    "end": "1348570"
  },
  {
    "text": "so that's kind of what we did we realized that these sands are jobs these",
    "start": "1348570",
    "end": "1355290"
  },
  {
    "text": "stream processing jobs they were causing us a bit of pain because they would think that firstly we didn't enforce",
    "start": "1355290",
    "end": "1363510"
  },
  {
    "text": "using thread above there for example but we trust the people with use protocol but more to the point we couldn't really",
    "start": "1363510",
    "end": "1369840"
  },
  {
    "text": "trace the lineage through sounds we didn't have a lot of control because it was essentially free code and people",
    "start": "1369840",
    "end": "1375780"
  },
  {
    "text": "could do in them whatever they wanted it's made it pretty difficult to trace things through the system so what if we",
    "start": "1375780",
    "end": "1381150"
  },
  {
    "text": "could just go straight from the HTTP API to s3 this would also avoid the problem",
    "start": "1381150",
    "end": "1389940"
  },
  {
    "text": "that we had with Sansa jobs in terms of repeatability so when things go wrong in",
    "start": "1389940",
    "end": "1397170"
  },
  {
    "text": "your streaming pipeline it's really we're thinking about how what you're gonna do in that situation for example",
    "start": "1397170",
    "end": "1403230"
  },
  {
    "text": "if a Sansa jobs stopped then it would affect all the downstream sounds of jobs",
    "start": "1403230",
    "end": "1409350"
  },
  {
    "text": "and users would come to us and ask what happened where did my data go which team do I talk to and another problem here is",
    "start": "1409350",
    "end": "1418440"
  },
  {
    "text": "that when things when you inevitably do discover a bug in your Sam's a stream",
    "start": "1418440",
    "end": "1423810"
  },
  {
    "text": "processor long after the fact it's very difficult to go and fix it you basically have to do a replay you have",
    "start": "1423810",
    "end": "1429960"
  },
  {
    "text": "to send that data through the Samsa job again that would have unpredictable consequences again on the downstream seams of jobs if",
    "start": "1429960",
    "end": "1436170"
  },
  {
    "text": "you don't have a good handle on what's happening in them so what if we could go straight from this HTTP API that we",
    "start": "1436170",
    "end": "1442710"
  },
  {
    "text": "introduced straight us three but less like primer but more like 12 Angry Men so that's",
    "start": "1442710",
    "end": "1453660"
  },
  {
    "text": "kind of what we did we said we're going to still have Kafka we're going to send",
    "start": "1453660",
    "end": "1459000"
  },
  {
    "text": "all our metrics and application logs and even business events everything we're standing there before we're still",
    "start": "1459000",
    "end": "1464550"
  },
  {
    "text": "getting sent to Kafka and that will still be going through stream processing all the usual things but in parallel",
    "start": "1464550",
    "end": "1471720"
  },
  {
    "text": "we're going to also send it to Kinesis now AWS Kinesis it's a managed service",
    "start": "1471720",
    "end": "1479430"
  },
  {
    "text": "that little bit is offers it's it's a bit like Kafka we chose it not for any big technical reason other than it's",
    "start": "1479430",
    "end": "1485370"
  },
  {
    "text": "very easy to spin up in multiple regions and payable yes pretty much manages it for us so we chose Kinesis here and we",
    "start": "1485370",
    "end": "1493740"
  },
  {
    "text": "started sending things to Kinesis and then reading them from flink link is another stateful stateful stream",
    "start": "1493740",
    "end": "1500280"
  },
  {
    "text": "processor and using flink we could kind of abstract away firstly that whether",
    "start": "1500280",
    "end": "1506430"
  },
  {
    "text": "it's Kafka or Kinesis it has adapters for both and then send those messages to",
    "start": "1506430",
    "end": "1512220"
  },
  {
    "text": "ice 3 every couple of minutes so it would batch them together into a bigger",
    "start": "1512220",
    "end": "1517800"
  },
  {
    "text": "file into a park a file and then send it to s3 but this time we also learnt a",
    "start": "1517800",
    "end": "1523980"
  },
  {
    "text": "lesson and we said that ok of course we still need transformations we can't",
    "start": "1523980",
    "end": "1530880"
  },
  {
    "text": "completely do away with them transformations are useful data sets aren't in their final form the moment",
    "start": "1530880",
    "end": "1537270"
  },
  {
    "text": "they're produced so we allowed transformations to be done but this time",
    "start": "1537270",
    "end": "1543120"
  },
  {
    "text": "only on the archive and only in a repeatable way it meant that we done we",
    "start": "1543120",
    "end": "1549660"
  },
  {
    "text": "don't need to do any kind of replay anymore we could just read that data with our spark job for example do",
    "start": "1549660",
    "end": "1556780"
  },
  {
    "text": "the transformation again and write the data again and it wouldn't necessarily need to have an impact on any other jobs",
    "start": "1556780",
    "end": "1562930"
  },
  {
    "text": "but we also realize that we need to make sure that these spark jobs that do these",
    "start": "1562930",
    "end": "1568360"
  },
  {
    "text": "transformations on the archive have to at least register their dependencies and register where they're going to write so",
    "start": "1568360",
    "end": "1574180"
  },
  {
    "text": "that when we do find errors and we do",
    "start": "1574180",
    "end": "1580090"
  },
  {
    "text": "correct them that we know what we need to do to correct it in the downstream systems as well this is really important",
    "start": "1580090",
    "end": "1586290"
  },
  {
    "text": "we also decided that this is a good opportunity for us to measure completeness and tell our users give",
    "start": "1586290",
    "end": "1592480"
  },
  {
    "text": "them a bit more metadata about the datasets that they were working with and tell them that yeah this is this all the",
    "start": "1592480",
    "end": "1598690"
  },
  {
    "text": "data that we let be found in our HTTP API has been delivered to s3 and all the transformations you were expecting to be",
    "start": "1598690",
    "end": "1604600"
  },
  {
    "text": "done have been done we can verify that for you as a platform so lesson 4 here",
    "start": "1604600",
    "end": "1612430"
  },
  {
    "start": "1610000",
    "end": "1683000"
  },
  {
    "text": "is that the peat ability is very important ability is something that is",
    "start": "1612430",
    "end": "1618970"
  },
  {
    "text": "not always discussed and it's more of a failure case in some cases and for streaming platforms and it seems like",
    "start": "1618970",
    "end": "1626800"
  },
  {
    "text": "streams often have to choose between replays and accepting errors as permanent and replays can be really",
    "start": "1626800",
    "end": "1635020"
  },
  {
    "text": "complicated in in our revenue pipeline example we had about a 30 step process",
    "start": "1635020",
    "end": "1640810"
  },
  {
    "text": "for doing replays they were really really complicated and we didn't have a lot of confidence in doing them because",
    "start": "1640810",
    "end": "1645970"
  },
  {
    "text": "you don't do them very often so if you can avoid them entirely that might be better and that's sort of the approach",
    "start": "1645970",
    "end": "1651130"
  },
  {
    "text": "we went with here might not work in every case but we found that to be a bit simpler best processing is good because",
    "start": "1651130",
    "end": "1661150"
  },
  {
    "text": "it can be done again any time the source data is always there it's a bit easier to manage and going straight to the",
    "start": "1661150",
    "end": "1667990"
  },
  {
    "text": "archive in small batches you could say micro batches sort of get you the",
    "start": "1667990",
    "end": "1674920"
  },
  {
    "text": "benefits of both you you still have very low latency but you your replays or can",
    "start": "1674920",
    "end": "1680710"
  },
  {
    "text": "be at least quite a bit simpler now some of you familiar with data",
    "start": "1680710",
    "end": "1686920"
  },
  {
    "start": "1683000",
    "end": "1781000"
  },
  {
    "text": "architectures may be asking is this a lambda architecture is this what you're telling me about Herman and yes it is",
    "start": "1686920",
    "end": "1694690"
  },
  {
    "text": "for those of you that haven't heard this term I just want to make here that land architecture here doesn't have anything",
    "start": "1694690",
    "end": "1700240"
  },
  {
    "text": "to do with AWS lambda or lambda functions I believe the name derives from the fact that the lambda character",
    "start": "1700240",
    "end": "1707380"
  },
  {
    "text": "in Greek has two little legs that you can see there and this corresponds to",
    "start": "1707380",
    "end": "1712780"
  },
  {
    "text": "the two legs in the red platform there's a streaming leg and there's a match leg and yeah this is sort of what we built",
    "start": "1712780",
    "end": "1719350"
  },
  {
    "text": "we had a stream processing side and we had a batter side that we introduced now",
    "start": "1719350",
    "end": "1724510"
  },
  {
    "text": "and it's a little bit different like this has been used in other companies",
    "start": "1724510",
    "end": "1730330"
  },
  {
    "text": "like Yahoo and in Netflix quite successfully for a number of years they use land architectures but they came at",
    "start": "1730330",
    "end": "1735640"
  },
  {
    "text": "it from a different angle they actually had a dupe batch processing pipelines and they weren't",
    "start": "1735640",
    "end": "1742510"
  },
  {
    "text": "satisfied with the high latency they weren't satisfied with the 24-hour wake they they needed and so they introduced",
    "start": "1742510",
    "end": "1748540"
  },
  {
    "text": "a parallel streaming architecture and we came at it from the other direction we came we had a streaming pipeline latency",
    "start": "1748540",
    "end": "1755050"
  },
  {
    "text": "was fine but the error case the the replays that we had to do those were pretty complicated and we and we wanted",
    "start": "1755050",
    "end": "1760270"
  },
  {
    "text": "to simplify the lineage and and increase our visibility over what was going on so we added the batch of my function and it",
    "start": "1760270",
    "end": "1768940"
  },
  {
    "text": "almost feels like this is an inevitable architecture to some extent but then I want a question is this really a land",
    "start": "1768940",
    "end": "1775120"
  },
  {
    "text": "architecture because what we're doing is pretty low latency on both these legs in",
    "start": "1775120",
    "end": "1782430"
  },
  {
    "start": "1781000",
    "end": "1931000"
  },
  {
    "text": "fact we realized that we could write files to s3 every five minutes but we",
    "start": "1782430",
    "end": "1788860"
  },
  {
    "text": "could also use the technology known as Delta Lake which is an open source",
    "start": "1788860",
    "end": "1795370"
  },
  {
    "text": "technology and use that as a as a sink for streaming and for batch and both and",
    "start": "1795370",
    "end": "1800980"
  },
  {
    "text": "as a source was teeming and for batch so just to talk a little bit about Delta Lake this was initially developed by a",
    "start": "1800980",
    "end": "1806590"
  },
  {
    "text": "company called data breaks and it's essentially metadata on top of park' in",
    "start": "1806590",
    "end": "1812170"
  },
  {
    "text": "your archive could be HDFS will be s3 and what it gives you is acid compliant",
    "start": "1812170",
    "end": "1820470"
  },
  {
    "text": "features and semantics on your s3 archive so basically you can do updates",
    "start": "1820470",
    "end": "1827490"
  },
  {
    "text": "and deletes on data in your archive you can do very nice things like sort the",
    "start": "1827490",
    "end": "1833850"
  },
  {
    "text": "data they call it Z ordering which can make certain types of queries very efficient so in our case because of gdpr",
    "start": "1833850",
    "end": "1842520"
  },
  {
    "text": "we need to be able to look up individual users and delete their information",
    "start": "1842520",
    "end": "1848010"
  },
  {
    "text": "requested and something like Delta Lake allows us to do this and in our experiments we found that using Delta",
    "start": "1848010",
    "end": "1855390"
  },
  {
    "text": "Lake instead of just thanked Parque could speed up these kinds of needle-in-a-haystack queries up to a",
    "start": "1855390",
    "end": "1861000"
  },
  {
    "text": "hundred times but it also does something really nice which is it gives us the ability to roll up these very small",
    "start": "1861000",
    "end": "1868050"
  },
  {
    "text": "park' files that we're constantly delivering into bigger files between no",
    "start": "1868050",
    "end": "1873060"
  },
  {
    "text": "improves of query performance as well like from our experiment experiments those feet up typical queries by about",
    "start": "1873060",
    "end": "1878640"
  },
  {
    "text": "two times just those rode ups and and Delta Lake is able to do these roll-ups",
    "start": "1878640",
    "end": "1884690"
  },
  {
    "text": "without affecting running queries so that's where the acid database compliance comes in it's really nice",
    "start": "1884690",
    "end": "1891570"
  },
  {
    "text": "technology and we put this in place in our pipeline so we had blinky delivering",
    "start": "1891570",
    "end": "1897540"
  },
  {
    "text": "its files to s3 at the moment it's delivering them as 4k but then we have",
    "start": "1897540",
    "end": "1902910"
  },
  {
    "text": "another transformation step that live loads them into Delta Lake I'm hoping in the future these will merge into one and",
    "start": "1902910",
    "end": "1908760"
  },
  {
    "text": "we'll write you'll be writing directly to that Lake but in essence we have very low latency on both sides we have one",
    "start": "1908760",
    "end": "1915570"
  },
  {
    "text": "minute latency in our Kafka pipeline and we have about five minutes latency right now in our so called batch pipeline but",
    "start": "1915570",
    "end": "1923010"
  },
  {
    "text": "I think it's more about if you call it a micro batch pipeline and that's pretty",
    "start": "1923010",
    "end": "1932430"
  },
  {
    "start": "1931000",
    "end": "2165000"
  },
  {
    "text": "much it so I think in the future here it",
    "start": "1932430",
    "end": "1937950"
  },
  {
    "text": "seems to me like our streaming architecture that we originally started with we had some reasons to go back to",
    "start": "1937950",
    "end": "1944570"
  },
  {
    "text": "batch but it's more like a micro batch this time and our micro batch architecture in this case",
    "start": "1944570",
    "end": "1950580"
  },
  {
    "text": "the latency is getting lower and lower in the future it looks like the latency could be low enough that most of the use",
    "start": "1950580",
    "end": "1956910"
  },
  {
    "text": "cases we were using Kafka for before might not need to be done in calf",
    "start": "1956910",
    "end": "1961980"
  },
  {
    "text": "ganimard we could do this in our batch by play instead and perhaps with only reserved calf calf for very very real",
    "start": "1961980",
    "end": "1968850"
  },
  {
    "text": "time use cases like metrics and application logs so the key takeaways",
    "start": "1968850",
    "end": "1975450"
  },
  {
    "text": "here I'm gonna quickly go through all the points I made because I realize it's",
    "start": "1975450",
    "end": "1981480"
  },
  {
    "text": "been quite a long time now the first one I talked about was Conway's law and how",
    "start": "1981480",
    "end": "1986580"
  },
  {
    "text": "that's two four data platforms this basically means that your data platform will structure and the data in it",
    "start": "1986580",
    "end": "1993840"
  },
  {
    "text": "specifically will mirror that of your organization and so luckily for me the",
    "start": "1993840",
    "end": "1999170"
  },
  {
    "text": "advice I can give you here is gonna be fairly generic because it will depend on your exact organizational layout what",
    "start": "1999170",
    "end": "2007370"
  },
  {
    "text": "this means but it's really worth thinking about it upfront and realizing that this is something you need to account for the second thing was that",
    "start": "2007370",
    "end": "2015380"
  },
  {
    "text": "metadata is absolutely critical like having a self-serve architecture is very good as your organization grows you",
    "start": "2015380",
    "end": "2022610"
  },
  {
    "text": "don't want your platform team the data platform team to become a bottleneck for using data and so you can't know about",
    "start": "2022610",
    "end": "2028910"
  },
  {
    "text": "every data set but that means you need to provide your users and yourself with the metadata you need to work with those",
    "start": "2028910",
    "end": "2035690"
  },
  {
    "text": "data sets efficiently and make good decisions it's as important as the data data engineers control the flat line",
    "start": "2035690",
    "end": "2044410"
  },
  {
    "text": "with this I mean that the restrictions you put on the platform there's the ways",
    "start": "2044410",
    "end": "2050570"
  },
  {
    "text": "that you are allowed to use the data that actually matters and it it",
    "start": "2050570",
    "end": "2055850"
  },
  {
    "text": "determines whether you get a really complicated plot line like primer or whether you get a pretty simple pipeline",
    "start": "2055850",
    "end": "2061010"
  },
  {
    "text": "like 12 Angry Men or Jurassic Park and then I talked about repeatability and",
    "start": "2061010",
    "end": "2067159"
  },
  {
    "text": "how that's important and how it's often overlooked in streaming pipelines it's it's it's a difficult topic getting a",
    "start": "2067160",
    "end": "2074270"
  },
  {
    "text": "piece ability right in our case meant completely avoiding streaming that's not necessarily the solution like I think",
    "start": "2074270",
    "end": "2080780"
  },
  {
    "text": "you can do repeatability with good partitioning schemes on Kafka for",
    "start": "2080780",
    "end": "2085990"
  },
  {
    "text": "example you can achieve some of these same semantics or by keeping your system fairly simple then repeatability is not",
    "start": "2085990",
    "end": "2091179"
  },
  {
    "text": "such a big problem but it is very important do you know what your plan is here and know how you're going to keep",
    "start": "2091179",
    "end": "2097118"
  },
  {
    "text": "it under control and then finally I really touched on Delta Lake and it's a",
    "start": "2097119",
    "end": "2102369"
  },
  {
    "text": "technology that we're starting to use now it's guys kind of we're using it right now as a bit of a batch sink and",
    "start": "2102369",
    "end": "2109170"
  },
  {
    "text": "probably a batch source but I'm hoping soon also as a streaming source which",
    "start": "2109170",
    "end": "2116109"
  },
  {
    "text": "will give our customers the ability to choose whether they want very low latency maybe five minutes maybe one",
    "start": "2116109",
    "end": "2122500"
  },
  {
    "text": "minute or whether they are okay with having batch daily batch transformations done on the archive either way they can",
    "start": "2122500",
    "end": "2129430"
  },
  {
    "text": "use spark either way it's the same code and that's quite a nice thing it's quite a nice thing to be able to do so if you",
    "start": "2129430",
    "end": "2135700"
  },
  {
    "text": "haven't heard of Delta Lake that's a good technology to go explore next and we're really seeing quite a bit of",
    "start": "2135700",
    "end": "2141580"
  },
  {
    "text": "benefit from this that's kind of scanner and I think we'll see more of that in the year to come and if you do all of",
    "start": "2141580",
    "end": "2149260"
  },
  {
    "text": "this right all these many different things then I hope you don't one day",
    "start": "2149260",
    "end": "2155170"
  },
  {
    "text": "like me find yourself in a room where you have to answer the question where",
    "start": "2155170",
    "end": "2160420"
  },
  {
    "text": "does your company's revenue come from thank you",
    "start": "2160420",
    "end": "2165119"
  },
  {
    "start": "2165000",
    "end": "2507000"
  },
  {
    "text": "[Music] do we have any questions for Herman",
    "start": "2169550",
    "end": "2177280"
  },
  {
    "text": "thank you for the presentation have you considered introducing lineage",
    "start": "2190870",
    "end": "2197560"
  },
  {
    "text": "technologies such as aperture atlas or igc type of technologies yeah that's",
    "start": "2197560",
    "end": "2204730"
  },
  {
    "text": "really important that's something we're actively looking at now and deciding what the best system will be for us so",
    "start": "2204730",
    "end": "2211540"
  },
  {
    "text": "we've looked at atlas we've looked interestingly at Blu specifically",
    "start": "2211540",
    "end": "2216850"
  },
  {
    "text": "because we use glue WSQ for pretty much everything and we're thinking what we'll",
    "start": "2216850",
    "end": "2222550"
  },
  {
    "text": "do is store the metadata for lineage in that instead of using existing open",
    "start": "2222550",
    "end": "2230590"
  },
  {
    "text": "source solutions because our system is quite customized but just having",
    "start": "2230590",
    "end": "2235750"
  },
  {
    "text": "something like this having this metadata at all is the important thing Apache atlas I believe yeah it's a good",
    "start": "2235750",
    "end": "2241270"
  },
  {
    "text": "option we will likely be using it soon but aren't yet",
    "start": "2241270",
    "end": "2246780"
  },
  {
    "text": "I am the only first wondering so looking at what you're showing with your stream sort of processing we're seen like you",
    "start": "2253680",
    "end": "2259440"
  },
  {
    "text": "put your issue there wasn't so much batch versus stream but was almost a fight you had so many different dependencies in the stream engine so",
    "start": "2259440",
    "end": "2264900"
  },
  {
    "text": "yeah so many teams where you know one team has a little bit of data has a new topic the next team reads out the next",
    "start": "2264900",
    "end": "2270420"
  },
  {
    "text": "for the next one yeah I mean how is that working with the batch one you know when you change it later to the spark end and",
    "start": "2270420",
    "end": "2276780"
  },
  {
    "text": "we're having lots of batch jobs that all depend on each other or was that really what you fixed having so many dependencies and actually said that each",
    "start": "2276780",
    "end": "2283170"
  },
  {
    "text": "team instead of depending on the previous team it's going to kind of start from the beginning actually and add most of their own data at that point",
    "start": "2283170",
    "end": "2289250"
  },
  {
    "text": "yeah we'll see how how it ends up like this is all this side of the pipeline is",
    "start": "2289250",
    "end": "2295020"
  },
  {
    "text": "fairly new so time will tell whether we were successful in simplifying the whole thing but what I can say is that we by",
    "start": "2295020",
    "end": "2305310"
  },
  {
    "text": "having the metadata at least this time to know how other dependencies are teams can decide whether they want to start",
    "start": "2305310",
    "end": "2312600"
  },
  {
    "text": "from scratch from source or use some of the existing metadata and you can get the picture and you can see what's available already quite a bit more",
    "start": "2312600",
    "end": "2319590"
  },
  {
    "text": "easily than you could before like we also added some some useful distinctions",
    "start": "2319590",
    "end": "2325290"
  },
  {
    "text": "here in terms of the quality of the data so we're thinking of like putting things into let's put it into data bricks terms",
    "start": "2325290",
    "end": "2333930"
  },
  {
    "text": "like bronze silver gold and so you can know the quality of each data set and we",
    "start": "2333930",
    "end": "2339600"
  },
  {
    "text": "will we will prefer that people build their data sets on top of gold data sets so that the transformations happen in as",
    "start": "2339600",
    "end": "2346680"
  },
  {
    "text": "few places as possible and not all over the place but that meant that that's again metadata is important to make this",
    "start": "2346680",
    "end": "2352440"
  },
  {
    "text": "visible and for teams to know which data sets to build their own transformations",
    "start": "2352440",
    "end": "2358020"
  },
  {
    "text": "on and not necessarily do everything that other teams are already done yeah but it's a good question time will tell",
    "start": "2358020",
    "end": "2364020"
  },
  {
    "text": "whether we will we were completely successful here and well there anymore",
    "start": "2364020",
    "end": "2370080"
  },
  {
    "text": "this one",
    "start": "2370080",
    "end": "2373130"
  },
  {
    "text": "what was the relationship between those architectural choices and your original performance graph and also in your",
    "start": "2380270",
    "end": "2387300"
  },
  {
    "text": "lambda with your two flows of data in the stream in batch space what was the split of the two million per second",
    "start": "2387300",
    "end": "2393090"
  },
  {
    "text": "across that sorry could you repeat the question so you got a performance graph that was shown your increase in rate",
    "start": "2393090",
    "end": "2398490"
  },
  {
    "text": "over time and all those different architectural changes at what points did they happen on though on those",
    "start": "2398490",
    "end": "2404010"
  },
  {
    "text": "improvements were that whether the improvements a result of the architectural choices at all and then on",
    "start": "2404010",
    "end": "2409200"
  },
  {
    "text": "your final picture with the split of metadata versus the business events",
    "start": "2409200",
    "end": "2414290"
  },
  {
    "text": "where does the two million the second fit across that is that both of those or one of them okay okay good question oh",
    "start": "2414290",
    "end": "2420780"
  },
  {
    "text": "yeah so the Kefka plaster right now in",
    "start": "2420780",
    "end": "2428580"
  },
  {
    "text": "all the pictures are is doing two million like we haven't reduced the amount of data and Kafka at this stage",
    "start": "2428580",
    "end": "2434280"
  },
  {
    "text": "what might happen in the future and the batch side is doing quite a bit less I",
    "start": "2434280",
    "end": "2440010"
  },
  {
    "text": "think I'm gonna just estimate it about 20% of that in terms of how things",
    "start": "2440010",
    "end": "2446550"
  },
  {
    "text": "played out in the timeline the batch pipeline we introduced we started",
    "start": "2446550",
    "end": "2452130"
  },
  {
    "text": "introducing this new batch might find about a year ago and so it's been rolling out it's being it's it's kind of",
    "start": "2452130",
    "end": "2458520"
  },
  {
    "text": "still in the early phases of seeing how it works but it's processing only the business events and not the metrics and",
    "start": "2458520",
    "end": "2463770"
  },
  {
    "text": "application logs and actually as time goes on we're splitting these completely we're trying to not use the same Kafka",
    "start": "2463770",
    "end": "2469740"
  },
  {
    "text": "cluster for all these different data use cases because they are actually quite different I have different profiles so",
    "start": "2469740",
    "end": "2475980"
  },
  {
    "text": "what seems like will happen is we will use one Kafka cluster for metrics and",
    "start": "2475980",
    "end": "2481350"
  },
  {
    "text": "other for logs and then probably this this batch pipeline for business events only you're probably gonna leave it",
    "start": "2481350",
    "end": "2489870"
  },
  {
    "text": "there just to give you some time to get to the next talks they're still gonna start at 14 there may be some people",
    "start": "2489870",
    "end": "2496230"
  },
  {
    "text": "still in the previous room though if they're over running a little bit so just be patience and hopefully we can",
    "start": "2496230",
    "end": "2501720"
  },
  {
    "text": "start on time but thanks so much Herman thank you you",
    "start": "2501720",
    "end": "2506860"
  }
]