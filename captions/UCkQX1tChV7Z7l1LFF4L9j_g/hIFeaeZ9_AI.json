[
  {
    "start": "0",
    "end": "130000"
  },
  {
    "text": "choosing to move to a microservices architecture is something that almost every engineering team is seduced by at",
    "start": "3950",
    "end": "12139"
  },
  {
    "text": "some point no matter what size a company or product you have okay there's so much content out there",
    "start": "12139",
    "end": "18680"
  },
  {
    "text": "warning you about the trade-offs but your developer productivity is declining",
    "start": "18680",
    "end": "23750"
  },
  {
    "text": "due to the fault isolation and modularity that come inherently with the monolith however if they are implemented",
    "start": "23750",
    "end": "32540"
  },
  {
    "text": "incorrectly or used as a band-aid without addressing some of the root flaws in your system you'll find",
    "start": "32540",
    "end": "39020"
  },
  {
    "text": "yourself no longer to make any new product development because you'll be drowning in the complexity at segments",
    "start": "39020",
    "end": "46220"
  },
  {
    "text": "we decided to break apart our monolith to micro-services about a year after we launched however it failed to address",
    "start": "46220",
    "end": "54110"
  },
  {
    "text": "some of the flaws in our system so about three years of after continuing to add to our micro services we were drowning",
    "start": "54110",
    "end": "61040"
  },
  {
    "text": "the complexity in 2017 we took a step",
    "start": "61040",
    "end": "66890"
  },
  {
    "text": "back and decided to move back to a monolith the trade-offs that came with micro services caused more issues than",
    "start": "66890",
    "end": "74179"
  },
  {
    "text": "they fixed and moving back to a monolith is what allowed us to be able to fix",
    "start": "74179",
    "end": "79789"
  },
  {
    "text": "some of the fundamental flaws in our system so my hope today is that you'll",
    "start": "79789",
    "end": "84920"
  },
  {
    "text": "understand and learn to deeply consider the trade-offs that come with micro services and make the decision that is",
    "start": "84920",
    "end": "91310"
  },
  {
    "text": "right for your team so what we're gonna cover is I'll go over what segments does",
    "start": "91310",
    "end": "97880"
  },
  {
    "text": "to help give you some context on the infrastructure decisions that we made and I'll go over our basic basic",
    "start": "97880",
    "end": "104029"
  },
  {
    "text": "monolithic architecture and then why we move to micro services and why they",
    "start": "104029",
    "end": "109700"
  },
  {
    "text": "worked for us in production for so long then I'll cover what led us to hit the tipping point and inspired us to move",
    "start": "109700",
    "end": "115729"
  },
  {
    "text": "back to a monolith and how we did that and then as with every big infrastructure decision there are always",
    "start": "115729",
    "end": "121700"
  },
  {
    "text": "trade-offs so I'll cover the trade-offs that we dealt with moving back to a monolith and how we think about them",
    "start": "121700",
    "end": "127219"
  },
  {
    "text": "moving forward so what does that meant do assuming most of you don't know this",
    "start": "127219",
    "end": "134870"
  },
  {
    "start": "130000",
    "end": "524000"
  },
  {
    "text": "maybe some of you do but segment we have a data pipeline and that data pipeline",
    "start": "134870",
    "end": "141739"
  },
  {
    "text": "ingest hundreds of thousands of events per second and this is what a sample event looks like it's a JSON payload",
    "start": "141739",
    "end": "149480"
  },
  {
    "text": "that contains basic information about users and their actions and these events",
    "start": "149480",
    "end": "156769"
  },
  {
    "text": "are generated by the software that our customers built so a lot of people build",
    "start": "156769",
    "end": "163010"
  },
  {
    "text": "really great software and they want to understand how their customers are using that software and there are a ton of",
    "start": "163010",
    "end": "169220"
  },
  {
    "text": "great tools out there that let you do that for example Google has one called Google Analytics so if you want to get",
    "start": "169220",
    "end": "175069"
  },
  {
    "text": "up and started with Google Analytics you have to go and add code and every single piece of your product to start sending",
    "start": "175069",
    "end": "181310"
  },
  {
    "text": "data to Google Analytics is API and your marketing team maybe wants to use Mixpanel",
    "start": "181310",
    "end": "186620"
  },
  {
    "text": "your sales team wants to use Salesforce and every time you want to add a new tool you have to go and in write code",
    "start": "186620",
    "end": "192409"
  },
  {
    "text": "and all the different sources of your product and eventually you can kind of",
    "start": "192409",
    "end": "197629"
  },
  {
    "text": "turn into something like this I stole this slide from our marketing team but it's basically this big mesh of sources",
    "start": "197629",
    "end": "203599"
  },
  {
    "text": "and places tools that you're sending your data to maybe one of them is leaking PII the data is not consistent",
    "start": "203599",
    "end": "209720"
  },
  {
    "text": "across tools and you want to try using a new tool but you have to write code",
    "start": "209720",
    "end": "214730"
  },
  {
    "text": "again in all of your sources to get started with that tool so segments goal",
    "start": "214730",
    "end": "219859"
  },
  {
    "text": "is to make that easier we provide a single API that you could send your data to and then we'll take care of sending",
    "start": "219859",
    "end": "226940"
  },
  {
    "text": "your events to any end tool that you want so we have a single API and as a developer you have to write",
    "start": "226940",
    "end": "233419"
  },
  {
    "text": "significantly less code because you just implement us once and then we handle sending that data for you we provide",
    "start": "233419",
    "end": "241430"
  },
  {
    "text": "tools along the way to help you do things such as strip any sensitive data or run any analysis on these tools and",
    "start": "241430",
    "end": "248269"
  },
  {
    "text": "if you want to start using a new tool it's as simple as just going into our app and enabling that tool so the piece",
    "start": "248269",
    "end": "255169"
  },
  {
    "text": "of our infrastructure that I'm going to focus on today for this talk is these end tools and the forwarding of events",
    "start": "255169",
    "end": "260840"
  },
  {
    "text": "to those tools and we refer to these tools as destinations in 2013",
    "start": "260840",
    "end": "268560"
  },
  {
    "text": "first launched our product we launched with the model if we needed the low operational overhead that came with",
    "start": "268560",
    "end": "275190"
  },
  {
    "text": "managing a monolith it was just our four founders and they needed whatever infrastructure was easiest for them to",
    "start": "275190",
    "end": "280650"
  },
  {
    "text": "move and iterate on and this was the original architecture we had a single",
    "start": "280650",
    "end": "287400"
  },
  {
    "text": "API that ingested events and forwarded them to a distributed message queue then",
    "start": "287400",
    "end": "292830"
  },
  {
    "text": "there was a single monolithic destination worker at the end that consumed from this queue and each one of",
    "start": "292830",
    "end": "299610"
  },
  {
    "text": "these destination api's expects events to be in a specific format so the",
    "start": "299610",
    "end": "305460"
  },
  {
    "text": "destination worker would consume an event from the queue check customer manage settings to see which destination",
    "start": "305460",
    "end": "311729"
  },
  {
    "text": "the event needed to go to it would then transform the event to be compatible with that destination API send the",
    "start": "311729",
    "end": "319290"
  },
  {
    "text": "request over the Internet to that destination wait for the response back and then move on to the next destination",
    "start": "319290",
    "end": "326810"
  },
  {
    "text": "now requests to destination actually fail pretty frequently and we categorize",
    "start": "326810",
    "end": "334200"
  },
  {
    "text": "those errors into two different types there's a retry Bellaire in an honor tribal error and a non reliable error is",
    "start": "334200",
    "end": "341990"
  },
  {
    "text": "something that we know will never be accepted by the destination api so this could be you have invalid credentials or",
    "start": "341990",
    "end": "348930"
  },
  {
    "text": "your event is missing a required field like a user ID or an email and then",
    "start": "348930",
    "end": "354210"
  },
  {
    "text": "there's retrial errors and reach variable errors are something that we think potentially be accepted later by",
    "start": "354210",
    "end": "360539"
  },
  {
    "text": "the destination with no changes and so this is what retries looked like in our",
    "start": "360539",
    "end": "366630"
  },
  {
    "text": "infrastructure the destination worker would get a response back from the destination api and if it needed to be",
    "start": "366630",
    "end": "372360"
  },
  {
    "text": "retried it would put it back in the queue in line with everything else and we keep retrying events for a specific",
    "start": "372360",
    "end": "379680"
  },
  {
    "text": "amount of time before giving up it's usually anywhere from about two to ten attempts and what this caused though is",
    "start": "379680",
    "end": "389130"
  },
  {
    "text": "something called head-of-line blocking and head-of-line blocking is a performance issue where things are",
    "start": "389130",
    "end": "395310"
  },
  {
    "text": "blocked by whatever is first in line this is kind of first-in first-out design",
    "start": "395310",
    "end": "400830"
  },
  {
    "text": "if you zoom in on our queue in particular you'll see we have the newest events in line with retry events across",
    "start": "400830",
    "end": "408300"
  },
  {
    "text": "all our customers and all destinations so what would happen is let's say",
    "start": "408300",
    "end": "414150"
  },
  {
    "text": "Salesforce for example is having a temporary outage now every event sent to",
    "start": "414150",
    "end": "419640"
  },
  {
    "text": "Salesforce fails and is put back in the queue to be sent again at a later time",
    "start": "419640",
    "end": "425900"
  },
  {
    "text": "but and we have auto scaling at the time to add more workers to the pool to be",
    "start": "426380",
    "end": "431700"
  },
  {
    "text": "able to handle this increase in queue depth but this sudden flood what outpace our ability to scale up which then",
    "start": "431700",
    "end": "437970"
  },
  {
    "text": "resulted in delays across all our destinations for all of our customers and customers rely on the timeliness of",
    "start": "437970",
    "end": "445260"
  },
  {
    "text": "this data so we can't afford these types of delays anywhere in our pipeline so",
    "start": "445260",
    "end": "450930"
  },
  {
    "text": "now we're at a point we have this monolith in production for about a year and the operational overhead is great",
    "start": "450930",
    "end": "456480"
  },
  {
    "text": "but this environmental isolation between the destinations is really starting to bite us about 10% of requests that we",
    "start": "456480",
    "end": "464850"
  },
  {
    "text": "send out to destinations fail with a retrial err so this is something that we were constantly dealing with and it was",
    "start": "464850",
    "end": "471150"
  },
  {
    "text": "a huge point of frustration a customer that would not even be using Salesforce would be impacted by a Salesforce outage",
    "start": "471150",
    "end": "478100"
  },
  {
    "text": "and so this trade-off is what inspired us to move to micro services so in 2014",
    "start": "478100",
    "end": "485730"
  },
  {
    "text": "we make a switch to micro services because environmental isolation is something that comes naturally when you",
    "start": "485730",
    "end": "491160"
  },
  {
    "text": "make the switch to micro services and this is what the new architecture looked",
    "start": "491160",
    "end": "496290"
  },
  {
    "text": "like so now we have the API that's ingesting events and forwarding them to",
    "start": "496290",
    "end": "501330"
  },
  {
    "text": "a router process this router process is responsible for fetching customer manage",
    "start": "501330",
    "end": "507420"
  },
  {
    "text": "settings to see which destination the event needs to go to and then it makes a copy of the event and distribute it to",
    "start": "507420",
    "end": "513930"
  },
  {
    "text": "each destination specific queue and then at the end of each queue there's a destination specific worker that is",
    "start": "513930",
    "end": "521190"
  },
  {
    "text": "result responsible for handling messages and our micro service architecture",
    "start": "521190",
    "end": "528000"
  },
  {
    "start": "524000",
    "end": "656000"
  },
  {
    "text": "allowed us to scale our platform like we needed to at the time for a few different reasons",
    "start": "528000",
    "end": "534320"
  },
  {
    "text": "the first was it was our attempt to solve that head-of-line blocking issue that we saw now if a destination was",
    "start": "534320",
    "end": "540920"
  },
  {
    "text": "having issues only its queue would back up and no other destinations would be impacted it also allowed us to quickly",
    "start": "540920",
    "end": "550280"
  },
  {
    "text": "add two destinations to the platform a common request that we would get is our sales team would come in say hey we had",
    "start": "550280",
    "end": "557330"
  },
  {
    "text": "this big new customer but they want to use Marketo we don't support Marketo can you guys add that in maybe like sure so",
    "start": "557330",
    "end": "564680"
  },
  {
    "text": "we'd add another queue and another destination worker to support Marketo now I can go off and build this Marketo",
    "start": "564680",
    "end": "571040"
  },
  {
    "text": "destination worker without worrying about any of the other destinations and in this situation the majority of my",
    "start": "571040",
    "end": "579200"
  },
  {
    "text": "time is actually spent understanding how Marquette OHS API works and ironing out",
    "start": "579200",
    "end": "585080"
  },
  {
    "text": "all the edge cases with their API so for example remember some destinations your",
    "start": "585080",
    "end": "592370"
  },
  {
    "text": "each destination requires events to be in a specific format so for example one",
    "start": "592370",
    "end": "597710"
  },
  {
    "text": "destination would expect birthday as date of birth our IP I accepted its birthday so here's an example of a",
    "start": "597710",
    "end": "603860"
  },
  {
    "text": "relatively basic transform that the destination worker would be responsible for some of them are pretty simple like",
    "start": "603860",
    "end": "609920"
  },
  {
    "text": "this one but some of them can be really complex one of our destinations for example requires payloads to be in XML",
    "start": "609920",
    "end": "618580"
  },
  {
    "text": "another thing that the worker is responsible for is not every destination",
    "start": "618580",
    "end": "625150"
  },
  {
    "text": "gives sends responses back that are in a standard HTTP format for example this is",
    "start": "625150",
    "end": "632060"
  },
  {
    "text": "one of our destinations and it returns a 200 and if you look you'll see it says success true but then if you look at the",
    "start": "632060",
    "end": "639230"
  },
  {
    "text": "results array it was actually not found there so this is the code that we have",
    "start": "639230",
    "end": "645410"
  },
  {
    "text": "to write for this destination now to parse every single response we're getting back from that destination to",
    "start": "645410",
    "end": "651920"
  },
  {
    "text": "understand if we need to retry the error or not and when you move to micro",
    "start": "651920",
    "end": "658850"
  },
  {
    "start": "656000",
    "end": "853000"
  },
  {
    "text": "services something that comes up often is do you keep everything in a mono repo or do you break it out into micro repos",
    "start": "658850",
    "end": "665990"
  },
  {
    "text": "and have one repo per service and when we",
    "start": "665990",
    "end": "671339"
  },
  {
    "text": "first broke out everything into micro services we actually kept everything in one repo and the destinations were",
    "start": "671339",
    "end": "677399"
  },
  {
    "text": "broken out into their sub directories in these sub directories lived all the custom code that we just went over for",
    "start": "677399",
    "end": "683760"
  },
  {
    "text": "each destination as well as any unit test that they had to verify this custom code but something that was causing a",
    "start": "683760",
    "end": "691199"
  },
  {
    "text": "lot of frustration with the team was if I had to go in and make a change to Salesforce mercado's tests were breaking",
    "start": "691199",
    "end": "697199"
  },
  {
    "text": "I'd have to spend time fixing the Marketo tests to get out my change for Salesforce and in response to that we",
    "start": "697199",
    "end": "703920"
  },
  {
    "text": "broke out all the destinations into their own repos and this isolation",
    "start": "703920",
    "end": "709170"
  },
  {
    "text": "allowed us to move quickly when maintaining destinations but we'll find out a bit later that this turned out to",
    "start": "709170",
    "end": "715440"
  },
  {
    "text": "be a false advantage and came back to bite us the next and final benefit that",
    "start": "715440",
    "end": "721889"
  },
  {
    "text": "we got from micro services which is a little specific to segments as we got really good visibility out of the box",
    "start": "721889",
    "end": "728910"
  },
  {
    "text": "into our infrastructure and specifically the visibility that I'm talking about is program execution like hot code pass and",
    "start": "728910",
    "end": "737209"
  },
  {
    "text": "stack size most tools aggregated these types of metrics on the host or the",
    "start": "737209",
    "end": "743880"
  },
  {
    "text": "service level so you can see here we have a memory leak and with all the",
    "start": "743880",
    "end": "749970"
  },
  {
    "text": "destinations separated out we knew exactly which destination was responsible for it and something we were",
    "start": "749970",
    "end": "757560"
  },
  {
    "text": "constantly paged tour was queued up and queued up is generally a good indicator that something is wrong",
    "start": "757560",
    "end": "763079"
  },
  {
    "text": "and with our one to one destination per cute or destination worker queue setup",
    "start": "763079",
    "end": "768930"
  },
  {
    "text": "we knew exactly which destination was having issues the uncle engineer could then go check the logs for that service",
    "start": "768930",
    "end": "775500"
  },
  {
    "text": "and understand relatively quickly what was happening and you definitely can get",
    "start": "775500",
    "end": "781019"
  },
  {
    "text": "this type of visibility from a monolith but you don't get it like for free right",
    "start": "781019",
    "end": "786180"
  },
  {
    "text": "away it takes a bit of time and effort to implement so 2015 we only had about",
    "start": "786180",
    "end": "794040"
  },
  {
    "text": "10 engineers total and our micro service set up is allowing us to scale our products like we needed to it the",
    "start": "794040",
    "end": "799889"
  },
  {
    "text": "time if we look at the trade-offs this is kind of what we see we have really",
    "start": "799889",
    "end": "805619"
  },
  {
    "text": "good environmental isolation now one destination having issues doesn't impact anybody else we have good improved",
    "start": "805619",
    "end": "812369"
  },
  {
    "text": "modularity with all the repos or all the destinations broken out into their own repos failing tests don't impact any of",
    "start": "812369",
    "end": "819929"
  },
  {
    "text": "the other destinations we got this good default visibility the metrics and",
    "start": "819929",
    "end": "825389"
  },
  {
    "text": "logging that come out of the box with micro-services significantly cut down on our time having to spent debug whenever",
    "start": "825389",
    "end": "831869"
  },
  {
    "text": "we got paged and the operational overhead story isn't great but it's not",
    "start": "831869",
    "end": "837779"
  },
  {
    "text": "a real problem for us yet because we only have about 20 destinations at this time however with the way our micro",
    "start": "837779",
    "end": "846809"
  },
  {
    "text": "services are set up this specific trade-off will soon be our downfall so",
    "start": "846809",
    "end": "854129"
  },
  {
    "text": "now we're gonna go into 2016 and our product is starting to gain some real traction and we're entering that like",
    "start": "854129",
    "end": "861149"
  },
  {
    "text": "hyper growth startup phase and that's requests where the sales team would come",
    "start": "861149",
    "end": "867119"
  },
  {
    "text": "in was happening more and more they would come in make hey we have this other new customer that's an even bigger deal but they want webhooks and we don't",
    "start": "867119",
    "end": "874470"
  },
  {
    "text": "support webhooks can you add that in back of course why not spin up a new queue your destination",
    "start": "874470",
    "end": "880589"
  },
  {
    "text": "worker another thing we were seeing it was we're at the point where destinations were actually reaching out",
    "start": "880589",
    "end": "886319"
  },
  {
    "text": "to us to be supported on our platform which was really cool and again in our",
    "start": "886319",
    "end": "891509"
  },
  {
    "text": "micro service architecture it was really easy to just spin them up a new queue and in your worker but this keeps",
    "start": "891509",
    "end": "898589"
  },
  {
    "text": "happening again and again and again over time and so in 2017 we've now added over",
    "start": "898589",
    "end": "905459"
  },
  {
    "text": "50 new destinations and I couldn't quite fit 50 on here but you get the idea the thing is though",
    "start": "905459",
    "end": "913559"
  },
  {
    "text": "with 50 new destinations that meant 50 new repos and so remember each",
    "start": "913559",
    "end": "920549"
  },
  {
    "text": "destination worker is responsible for transforming events to be compatible with the destination API and our API",
    "start": "920549",
    "end": "928709"
  },
  {
    "text": "will accept name in any of these formats either just name or first and lastname camelcase first",
    "start": "928709",
    "end": "935790"
  },
  {
    "text": "last name snake cased so let's say I want to get the name from this event I",
    "start": "935790",
    "end": "942000"
  },
  {
    "text": "have to check each of these cases in that destination codebase and I'm",
    "start": "942000",
    "end": "948210"
  },
  {
    "text": "already having to do a decent amount of context switching to understand the differences in destination api's and now",
    "start": "948210",
    "end": "954090"
  },
  {
    "text": "all of our destination code bases also have specific code to get the name from segment event and so this made",
    "start": "954090",
    "end": "959970"
  },
  {
    "text": "maintenance for us a bit of a headache we wanted to decrease that customization or cross our repos as much as possible",
    "start": "959970",
    "end": "967880"
  },
  {
    "text": "so we wrote some shared libraries so now for that name example I could go into",
    "start": "967880",
    "end": "973230"
  },
  {
    "text": "any of our destination code bases call event dot name and this is what would happen under the hood the Harbury would",
    "start": "973230",
    "end": "980460"
  },
  {
    "text": "check for traits not name that didn't exist it would go through and check for first name and last name check in all",
    "start": "980460",
    "end": "986460"
  },
  {
    "text": "the cases and these familiar methods made the code bases a lot more uniform",
    "start": "986460",
    "end": "992670"
  },
  {
    "text": "which made maintenance less of a headache I could go into any code base and quickly understand what was going on",
    "start": "992670",
    "end": "1000310"
  },
  {
    "text": "but what happens when there's a bug and one of these shared libraries let's say",
    "start": "1000310",
    "end": "1005930"
  },
  {
    "text": "for argument's sake that we forgot to check snake case here now any customer",
    "start": "1005930",
    "end": "1011570"
  },
  {
    "text": "that is sending us an event that snake case won't have the name of their users in their end tools so I go into the show",
    "start": "1011570",
    "end": "1018920"
  },
  {
    "text": "library write a fix for it to start checking snake case and I released a new version of it all of our infrastructure",
    "start": "1018920",
    "end": "1026150"
  },
  {
    "text": "at the time was hosted on AWS and we were using terraform to manage the state of it so the state of our infrastructure",
    "start": "1026150",
    "end": "1032750"
  },
  {
    "text": "lived in github and this was the standard deploy process for just one of",
    "start": "1032750",
    "end": "1038000"
  },
  {
    "text": "these services if everything went perfectly smoothly I could probably get",
    "start": "1038000",
    "end": "1044060"
  },
  {
    "text": "this change out to one service in about an hour but remember we had over 50 new",
    "start": "1044060",
    "end": "1050900"
  },
  {
    "text": "destinations now which meant 50 new queues and 50 new services so if I",
    "start": "1050900",
    "end": "1056330"
  },
  {
    "text": "wanted to get this fix out to just check snake case on a name I now have to test and deploy dozens of services",
    "start": "1056330",
    "end": "1065440"
  },
  {
    "text": "and with over 50 destinations that's at least one week of work for me to get this change out or the whole team is",
    "start": "1065490",
    "end": "1072300"
  },
  {
    "text": "with me heads down in a room and we're powering through these destinations and so changes to our shared libraries began",
    "start": "1072300",
    "end": "1079530"
  },
  {
    "text": "to require a ton of time and effort to maintain and two things started",
    "start": "1079530",
    "end": "1084660"
  },
  {
    "text": "happening because of that one we just stopped making changes to our shared",
    "start": "1084660",
    "end": "1090450"
  },
  {
    "text": "libraries even when we desperately needed them and it started to cause a lot of friction in our code base or the",
    "start": "1090450",
    "end": "1097770"
  },
  {
    "text": "second was I would go in and just update the version in that specific destination",
    "start": "1097770",
    "end": "1103470"
  },
  {
    "text": "codebase and because of that eventually the versions of our shared libraries",
    "start": "1103470",
    "end": "1108570"
  },
  {
    "text": "started to diverge across these code bases and that amazing benefit we once",
    "start": "1108570",
    "end": "1114150"
  },
  {
    "text": "had a reduced customization completely reversed on us eventually all of our",
    "start": "1114150",
    "end": "1120090"
  },
  {
    "text": "destinations were using different versions of these shared libraries and we probably could have built to lean to",
    "start": "1120090",
    "end": "1127860"
  },
  {
    "text": "help us with testing and automating the deploys of all of these services but not only was our productivity suffering",
    "start": "1127860",
    "end": "1134790"
  },
  {
    "text": "because of this customization we were starting to run into some other issues with our micro services something that",
    "start": "1134790",
    "end": "1143220"
  },
  {
    "start": "1141000",
    "end": "1234000"
  },
  {
    "text": "we noticed was that some of our destinations were handling little to no",
    "start": "1143220",
    "end": "1148980"
  },
  {
    "text": "traffic so for example you have destination Y here and they're handling less than one event per second and one",
    "start": "1148980",
    "end": "1156929"
  },
  {
    "text": "of our larger customers who's sending us thousands of event per second sees destination Y wants to try it out so",
    "start": "1156929",
    "end": "1163530"
  },
  {
    "text": "they turn on destination Y and now all the sudden destination Y's Q is flooded",
    "start": "1163530",
    "end": "1168540"
  },
  {
    "text": "with these new events just because one customer enabled them and now I'm getting paged to have to go in and scale",
    "start": "1168540",
    "end": "1174780"
  },
  {
    "text": "up destination Y because similar to before our it outpaced our ability to scale up and handle this type of load",
    "start": "1174780",
    "end": "1181490"
  },
  {
    "text": "and this happened very frequently and at",
    "start": "1181490",
    "end": "1186929"
  },
  {
    "text": "the time we had one auto scaling rule applied to all of these services and we",
    "start": "1186929",
    "end": "1192990"
  },
  {
    "text": "would play around with it to try and master the configuration to help us with these load spikes but each of the",
    "start": "1192990",
    "end": "1199380"
  },
  {
    "text": "services also had a distinct amount of CPU and memory resources that they used",
    "start": "1199380",
    "end": "1205100"
  },
  {
    "text": "so there wasn't really any a set of auto-scaling rules that worked and one",
    "start": "1205100",
    "end": "1212250"
  },
  {
    "text": "solution to this could be to just over provision and just have a bunch of minimum workers in the pool but one that",
    "start": "1212250",
    "end": "1218160"
  },
  {
    "text": "gets expensive and we had also thought about having dedicated auto scaling",
    "start": "1218160",
    "end": "1224610"
  },
  {
    "text": "rules per destination worker but we were already drowning in the complexity across the code bases so that wasn't",
    "start": "1224610",
    "end": "1231720"
  },
  {
    "text": "really an option for us and so as time goes on we're still adding to our micro",
    "start": "1231720",
    "end": "1237840"
  },
  {
    "start": "1234000",
    "end": "1337000"
  },
  {
    "text": "service architecture and we eventually hit a breaking point we were rapidly",
    "start": "1237840",
    "end": "1243240"
  },
  {
    "text": "adding new destinations to this platform on average it was about three per month and with our setup our operational",
    "start": "1243240",
    "end": "1249990"
  },
  {
    "text": "overhead was increasing linearly with each new destination added and unless",
    "start": "1249990",
    "end": "1256200"
  },
  {
    "text": "you're adding more bodies to the problem what tends to happen is your developer productivity will start to decrease as",
    "start": "1256200",
    "end": "1262500"
  },
  {
    "text": "your operational overhead increases okay",
    "start": "1262500",
    "end": "1269210"
  },
  {
    "text": "well sorry about that clearly somebody is just as upset about micro services as we were so we're okay yes and so we're",
    "start": "1269210",
    "end": "1279660"
  },
  {
    "text": "at this point now where our operational overhead is increasing linearly with",
    "start": "1279660",
    "end": "1285120"
  },
  {
    "text": "every new destination that we're adding and unless you add more bodies to that",
    "start": "1285120",
    "end": "1291900"
  },
  {
    "text": "problem generally what tends to happen is you're operational as your operational overhead increases your",
    "start": "1291900",
    "end": "1298260"
  },
  {
    "text": "developer productivity will start to decline managing all of these services was a huge tax on our team we were",
    "start": "1298260",
    "end": "1306840"
  },
  {
    "text": "literally losing sleep over it because it was so common for us to get paged to have to go in and scale up our smaller",
    "start": "1306840",
    "end": "1313470"
  },
  {
    "text": "destinations and we actually got in to a point where we'd become so numb to pages",
    "start": "1313470",
    "end": "1319170"
  },
  {
    "text": "and issues that we weren't responding to them as much anymore and people were reaching out to our CEO to be like what",
    "start": "1319170",
    "end": "1325800"
  },
  {
    "text": "is going on over there and not only did this operational overhead",
    "start": "1325800",
    "end": "1331720"
  },
  {
    "text": "cause our productivity to decline but it also halted any new product development a really common feature request we got",
    "start": "1331720",
    "end": "1341020"
  },
  {
    "start": "1337000",
    "end": "1534000"
  },
  {
    "text": "for our destinations was customers wanted to understand if their events were successfully sent to the",
    "start": "1341020",
    "end": "1346990"
  },
  {
    "text": "destination or not and at the time our product was a bit of a black box we'd given customers",
    "start": "1346990",
    "end": "1353169"
  },
  {
    "text": "visibility into whether their events had made it to our API like I'm showing you here but then if they wanted to know if",
    "start": "1353169",
    "end": "1359919"
  },
  {
    "text": "their destination if their events had made it to their destination that'd have to go into that destination and check for themselves and the",
    "start": "1359919",
    "end": "1368049"
  },
  {
    "text": "solution for that in our micro service worlds would have been to create a shared library that all the destinations",
    "start": "1368049",
    "end": "1373360"
  },
  {
    "text": "could use to easily publish metrics to a separate queue but we were already in",
    "start": "1373360",
    "end": "1380740"
  },
  {
    "text": "that situation with our current shared libraries that were on different versions for each of these services so",
    "start": "1380740",
    "end": "1386110"
  },
  {
    "text": "we knew that that was not going to be an option now if we ever wanted to make a change to that feature we'd have to go",
    "start": "1386110",
    "end": "1391840"
  },
  {
    "text": "through and test and deploy every single one of these services and so if we take",
    "start": "1391840",
    "end": "1398110"
  },
  {
    "text": "a look now at the trade-offs the operational overhead is what is killing us if we're struggling just to",
    "start": "1398110",
    "end": "1406179"
  },
  {
    "text": "keep this system alive and this overhead of managing this infrastructure is no",
    "start": "1406179",
    "end": "1411610"
  },
  {
    "text": "longer maintainable and was only getting worse as we added new destinations so",
    "start": "1411610",
    "end": "1417460"
  },
  {
    "text": "because of this our productivity and velocity was quickly declining the",
    "start": "1417460",
    "end": "1422950"
  },
  {
    "text": "operational overhead was so great that we weren't able to test and deploy these destinations like we needed to to keep",
    "start": "1422950",
    "end": "1429309"
  },
  {
    "text": "them in sync which resulted in the complexity of our code bases exploding and the new product",
    "start": "1429309",
    "end": "1436630"
  },
  {
    "text": "development just didn't happen on the platform anymore not only because we were drowning just trying to keep the",
    "start": "1436630",
    "end": "1442510"
  },
  {
    "text": "system alive but because we knew that any new any additions that we made would",
    "start": "1442510",
    "end": "1447850"
  },
  {
    "text": "make the complexity worse and when we started to really think about it",
    "start": "1447850",
    "end": "1453669"
  },
  {
    "text": "micro-services didn't actually solve that fundamental head-of-line blocking problem in our system it really just",
    "start": "1453669",
    "end": "1461289"
  },
  {
    "text": "decreased the blast radius of it so if you look at our individual cute now we still have new events in line",
    "start": "1461289",
    "end": "1469760"
  },
  {
    "text": "with retry events and 2017 we have some pretty large customers that are sending",
    "start": "1469760",
    "end": "1475700"
  },
  {
    "text": "a ton of events and then we also are now supporting some destinations who can",
    "start": "1475700",
    "end": "1480890"
  },
  {
    "text": "only handle about ten requests a minute so rate limiting was something we were constantly dealing with across all our",
    "start": "1480890",
    "end": "1487490"
  },
  {
    "text": "destinations if you remember rate limiting is an error that we want to retry on so now one deaths:1 customers",
    "start": "1487490",
    "end": "1494180"
  },
  {
    "text": "rate limits would impact would cause delays for all of our customers using that destination so we still have this",
    "start": "1494180",
    "end": "1502340"
  },
  {
    "text": "head-of-line blocking issue it's now just isolated at the destination level and the ideal setup to actually solve",
    "start": "1502340",
    "end": "1511790"
  },
  {
    "text": "for that issue in our micro service world would have been one queue in one destination worker per customer per",
    "start": "1511790",
    "end": "1518770"
  },
  {
    "text": "destination but since we were already reaching our breaking point with micro services that was not an option",
    "start": "1518770",
    "end": "1525020"
  },
  {
    "text": "and I'm not talking about adding like a couple hundred more micro services in queues I'm talking tens of thousands of",
    "start": "1525020",
    "end": "1531350"
  },
  {
    "text": "more micro services and queues so its 2017 now and we have officially reached",
    "start": "1531350",
    "end": "1538820"
  },
  {
    "start": "1534000",
    "end": "1899000"
  },
  {
    "text": "the breaking point at this point we have over a hundred and forty services and queues and repose and the micro service",
    "start": "1538820",
    "end": "1547640"
  },
  {
    "text": "set up that originally allowed us to scale is now completely crippling us we",
    "start": "1547640",
    "end": "1553430"
  },
  {
    "text": "brought on one of our most senior engineers to help us with the situation and this is the picture that he drew to",
    "start": "1553430",
    "end": "1558920"
  },
  {
    "text": "depict what the situation was we're in in case you can't tell that the big hole in the ship and there's three engineers",
    "start": "1558920",
    "end": "1565100"
  },
  {
    "text": "struggling to ship water out of it and it's kind of what it felt like so the",
    "start": "1565100",
    "end": "1572480"
  },
  {
    "text": "first thing that we knew we wanted to do is we wanted to move back to a monolith the operational overhead of managing all",
    "start": "1572480",
    "end": "1578000"
  },
  {
    "text": "these micro services was the root of all of our problems and if you look at the",
    "start": "1578000",
    "end": "1585110"
  },
  {
    "text": "trade-offs and the burdens of this operational overhead was actually out",
    "start": "1585110",
    "end": "1590450"
  },
  {
    "text": "weighing all of the benefits that micro services gave us",
    "start": "1590450",
    "end": "1595480"
  },
  {
    "text": "we'd already made the switch once before so this time we actually knew that we had to really consider the trade-offs",
    "start": "1595539",
    "end": "1601989"
  },
  {
    "text": "that were gonna come with it and think deeply about each one and be comfortable with potentially losing some of the",
    "start": "1601989",
    "end": "1607840"
  },
  {
    "text": "benefits that microservices gave us so arc so we knew we wanted to move",
    "start": "1607840",
    "end": "1613539"
  },
  {
    "text": "everything back into a service but the architecture at the time would have made that a bit difficult if we had put",
    "start": "1613539",
    "end": "1619029"
  },
  {
    "text": "everything in one service but kept aky√ºz now this monolithic destination worker",
    "start": "1619029",
    "end": "1624070"
  },
  {
    "text": "would have been responsible for checking each queue for work and that would have added a layer of complexity to the",
    "start": "1624070",
    "end": "1630460"
  },
  {
    "text": "destinations with which we weren't comfortable with it also doesn't solve that fundamental head-of-line blocking",
    "start": "1630460",
    "end": "1636249"
  },
  {
    "text": "issue that we see one customers rate limits can still impact everybody using that destination and moving back to a",
    "start": "1636249",
    "end": "1643149"
  },
  {
    "text": "single queue puts us back in the same situation we were in when we first launched we're now one customers retries",
    "start": "1643149",
    "end": "1649599"
  },
  {
    "text": "impact all destinations and all customers and this was the main",
    "start": "1649599",
    "end": "1654669"
  },
  {
    "text": "inspiration for centrifuge centrifuge would replace all of our queues and be",
    "start": "1654669",
    "end": "1660249"
  },
  {
    "text": "responsible for sending events to this single monolithic worker and you could think of centrifuge as providing a queue",
    "start": "1660249",
    "end": "1667929"
  },
  {
    "text": "per customer per destination but without all the external complexity and it",
    "start": "1667929",
    "end": "1673090"
  },
  {
    "text": "finally solved that head-of-line blocking issue that we'd experienced since we launched once and for all so",
    "start": "1673090",
    "end": "1680229"
  },
  {
    "text": "now the destinations team there's about 12 of us and we are dedicated to just building centrifuge and this destination",
    "start": "1680229",
    "end": "1687429"
  },
  {
    "text": "worker so after we kind of designed the system we were ready to start building and we knew that we were since we were",
    "start": "1687429",
    "end": "1694299"
  },
  {
    "text": "gonna go back to one worker we wanted to move back to a mono repo and we deemed",
    "start": "1694299",
    "end": "1699340"
  },
  {
    "text": "this specific part of the project call we called it the great mono Gration and so the destinations were divided up",
    "start": "1699340",
    "end": "1706419"
  },
  {
    "text": "amongst each of us and we started porting them over back into the mono repo and with this change we saw an",
    "start": "1706419",
    "end": "1713379"
  },
  {
    "text": "opportunity to fix two fundamental issues one we were going to put everything back on the same versions of",
    "start": "1713379",
    "end": "1719830"
  },
  {
    "text": "our dependencies at this point we had a hundred and twenty unique dependencies and we were committed to having one",
    "start": "1719830",
    "end": "1726179"
  },
  {
    "text": "person or one party one version of the shared birth dependency excuse me",
    "start": "1726179",
    "end": "1732000"
  },
  {
    "text": "and so as we move destinations over we'd update to the latest and then we'd run",
    "start": "1732000",
    "end": "1737909"
  },
  {
    "text": "the tests and fix anything that broke the next part of the Monta Gration this",
    "start": "1737909",
    "end": "1743309"
  },
  {
    "text": "is really the meat of it was we needed to build a test suite that we could quickly and easily run for all of our",
    "start": "1743309",
    "end": "1750480"
  },
  {
    "text": "destinations now if you remember the original motivation for breaking destinations out into their own repos",
    "start": "1750480",
    "end": "1757470"
  },
  {
    "text": "was these failing tests but this turned",
    "start": "1757470",
    "end": "1763919"
  },
  {
    "text": "out to be a false advantage because these destination tests were actually",
    "start": "1763919",
    "end": "1768929"
  },
  {
    "text": "making outbound HTTP requests over the Internet to destination api's to verify",
    "start": "1768929",
    "end": "1774690"
  },
  {
    "text": "that we're handling the response to the requests and responses properly so what",
    "start": "1774690",
    "end": "1779759"
  },
  {
    "text": "what happened is I would go into Salesforce which hadn't been touched in six months and need to make a quick",
    "start": "1779759",
    "end": "1785549"
  },
  {
    "text": "update but then the tests are failing because the our test credentials aren't valid so",
    "start": "1785549",
    "end": "1790769"
  },
  {
    "text": "I go into our shared tool and I try and find updated credentials and of course they're not there so now I have to reach",
    "start": "1790769",
    "end": "1796799"
  },
  {
    "text": "out to our partnerships team or Salesforce directly to get new test credentials just so I can get out my",
    "start": "1796799",
    "end": "1803039"
  },
  {
    "text": "small change to Salesforce something that should have taken me only a few hours is now taking me over a week of",
    "start": "1803039",
    "end": "1809279"
  },
  {
    "text": "work because I don't have valid test credentials and stuff like that should never fail tests and with the",
    "start": "1809279",
    "end": "1818279"
  },
  {
    "text": "destinations broken out into separate repos there was very little motivation for us to go in and clean up these",
    "start": "1818279",
    "end": "1823980"
  },
  {
    "text": "failing tests and this poor hygiene led to a constant source of frustrating",
    "start": "1823980",
    "end": "1829289"
  },
  {
    "text": "technical debt we also knew that some destination api's are much slower than",
    "start": "1829289",
    "end": "1835950"
  },
  {
    "text": "others one destination test suite could take up to five minutes to run waiting for the responses back from the",
    "start": "1835950",
    "end": "1842370"
  },
  {
    "text": "destination and with over a hundred and forty destinations that means our test suite could have taken up to an hour to",
    "start": "1842370",
    "end": "1848190"
  },
  {
    "text": "run which was not acceptable so we built something called traffic recorder and",
    "start": "1848190",
    "end": "1854720"
  },
  {
    "text": "traffic recorder was responsible for recording and saving all destination test traffic so on the first test",
    "start": "1854720",
    "end": "1862140"
  },
  {
    "text": "what it would do is the requests and responses were recorded to a file like this and then on the next run the",
    "start": "1862140",
    "end": "1869040"
  },
  {
    "text": "requests and response in the file is played back instead of actually sending the request to the destination API all",
    "start": "1869040",
    "end": "1876180"
  },
  {
    "text": "these files are checked into a repo as well so that they're consistent with every change and that's made our test",
    "start": "1876180",
    "end": "1882360"
  },
  {
    "text": "sleep significantly more resilient which we knew is going to be a must-have moving back to a mono repo I remember",
    "start": "1882360",
    "end": "1889290"
  },
  {
    "text": "running the test for every destination for the first time it only took milliseconds when most tests before",
    "start": "1889290",
    "end": "1894900"
  },
  {
    "text": "would take me a matter of minutes and it just felt like magic so we finished the",
    "start": "1894900",
    "end": "1901320"
  },
  {
    "start": "1899000",
    "end": "2018000"
  },
  {
    "text": "great mono Gration in the summer of 2017 and then the team shifts to focus on building and scaling out centrifuge and",
    "start": "1901320",
    "end": "1908120"
  },
  {
    "text": "we're slowly moving destinations over into the monolithic worker now as we're",
    "start": "1908120",
    "end": "1913290"
  },
  {
    "text": "learning to scale centrifuge and we complete this migration at the beginning of 2018 and overall it was a massive",
    "start": "1913290",
    "end": "1921540"
  },
  {
    "text": "improvement so if we look back at some of the auto scaling issues that we had before if you remember some of our",
    "start": "1921540",
    "end": "1928320"
  },
  {
    "text": "smaller destinations weren't able to handle big increases in load and it was",
    "start": "1928320",
    "end": "1933390"
  },
  {
    "text": "a constant source of pages for us so with every destination now living in one",
    "start": "1933390",
    "end": "1938640"
  },
  {
    "text": "service we had a good mix of CPU and memory intensive destinations which made",
    "start": "1938640",
    "end": "1943950"
  },
  {
    "text": "scaling a service to meet demand significantly easier this large worker",
    "start": "1943950",
    "end": "1949500"
  },
  {
    "text": "pool is able to absorb spikes and loads so we're no longer getting paged for the destinations that only handle a small",
    "start": "1949500",
    "end": "1955920"
  },
  {
    "text": "number of events we could also add new destinations to this and it wouldn't add anything to our operational overhead",
    "start": "1955920",
    "end": "1964010"
  },
  {
    "text": "next was our productivity with every destination living in one service our",
    "start": "1964010",
    "end": "1969660"
  },
  {
    "text": "developer productivity substantially improved because we no longer had to deploy over a hundred and forty",
    "start": "1969660",
    "end": "1975600"
  },
  {
    "text": "destinations to get a change out to one of our shared libraries one engineer was able to deploy this service in a matter",
    "start": "1975600",
    "end": "1982080"
  },
  {
    "text": "of minutes all our destinations were using the same versions of the shared",
    "start": "1982080",
    "end": "1987360"
  },
  {
    "text": "libraries as well which significantly cut down on the complexity of the code bases and our testing story was much",
    "start": "1987360",
    "end": "1993780"
  },
  {
    "text": "better we had traffic recorder so one engineer could run the tests for every destination in under a minute",
    "start": "1993780",
    "end": "2000289"
  },
  {
    "text": "when making changes to the shared libraries we also started building new",
    "start": "2000289",
    "end": "2005630"
  },
  {
    "text": "products again a few months after we finished centrifuge and moved back to monolith myself and one other engineer",
    "start": "2005630",
    "end": "2011419"
  },
  {
    "text": "were able to build out a delivery system for our destinations and only a matter of months so we're able to new products",
    "start": "2011419",
    "end": "2020990"
  },
  {
    "start": "2018000",
    "end": "2450000"
  },
  {
    "text": "again and were able to scale our platform without adding to our operation overhead but it wasn't all sunshine and",
    "start": "2020990",
    "end": "2026960"
  },
  {
    "text": "roses moving back to a monolith and there were some trade-offs so if you",
    "start": "2026960",
    "end": "2033169"
  },
  {
    "text": "take a quick high-level look at the trade-offs it doesn't look great our operational story had improved greatly",
    "start": "2033169",
    "end": "2039409"
  },
  {
    "text": "and that was the root of all of our issues we almost need like a weight on here to signify how important each",
    "start": "2039409",
    "end": "2045019"
  },
  {
    "text": "trade-off is and traffic a quarter had made us comfortable with the loss and",
    "start": "2045019",
    "end": "2051138"
  },
  {
    "text": "improved modularity we now yes if I had to go in and fix Salesforce in marcado's tests are failing I would have to fix",
    "start": "2051139",
    "end": "2057560"
  },
  {
    "text": "those tests to get the change out but it allowed it it made us more resilient to",
    "start": "2057560",
    "end": "2063138"
  },
  {
    "text": "those types of failures that we saw in the beginning we're invalid credentials would fail tests and centrifuge fixed",
    "start": "2063139",
    "end": "2071540"
  },
  {
    "text": "the head-of-line blocking issue which was one of the main causes of environmental isolation but I also go",
    "start": "2071540",
    "end": "2077868"
  },
  {
    "text": "over in a few minutes we actually ran into another issue with that and then",
    "start": "2077869",
    "end": "2083148"
  },
  {
    "text": "there was the default visibility that we no longer had which this was the first thing we ran into so a few months after",
    "start": "2083149",
    "end": "2092329"
  },
  {
    "text": "we moved to a monolith we started seeing these destination workers running out of memory and crashing this is commonly",
    "start": "2092329",
    "end": "2099500"
  },
  {
    "text": "referred to as om kills and it wasn't happening in an alarming rate but it was",
    "start": "2099500",
    "end": "2105560"
  },
  {
    "text": "happening with enough frequency that we wanted to fix it and with our destination workers broken out to",
    "start": "2105560",
    "end": "2111560"
  },
  {
    "text": "microservices before we would have known exactly which destination was responsible but now it was a bit more",
    "start": "2111560",
    "end": "2117349"
  },
  {
    "text": "difficult so to give you a sense of what we had to do now was we use this tool",
    "start": "2117349",
    "end": "2123530"
  },
  {
    "text": "called cystic and cystic lets you capture and filter and decode system",
    "start": "2123530",
    "end": "2128780"
  },
  {
    "text": "calls so we go on the hosts and we run system monitoring these workers and we",
    "start": "2128780",
    "end": "2134060"
  },
  {
    "text": "think we find a connection link and so we go in and we kind of blindly make some updates to this service to fix this",
    "start": "2134060",
    "end": "2140840"
  },
  {
    "text": "leak but workers are still crashing and we didn't fix it and it took a few more",
    "start": "2140840",
    "end": "2147110"
  },
  {
    "text": "weeks of different attempts and trying to almost blindly debug this memory issue and what ended up working was we",
    "start": "2147110",
    "end": "2153320"
  },
  {
    "text": "used a package called node heap dump and how that works is it synchronously",
    "start": "2153320",
    "end": "2158420"
  },
  {
    "text": "writes a snapshot to a file on disk that you can later go and inspect with google",
    "start": "2158420",
    "end": "2163490"
  },
  {
    "text": "chrome dev tools so if you look really closely at the last two fields here you'll see that the issue is in our",
    "start": "2163490",
    "end": "2169730"
  },
  {
    "text": "intercom destination and for some reason there's a 100 megabyte array so but now",
    "start": "2169730",
    "end": "2175820"
  },
  {
    "text": "we're able to know we know which destination it is and were able to go and debug from there and you know it",
    "start": "2175820",
    "end": "2182600"
  },
  {
    "text": "took some time and effort to implement this kind of visibility into the worker that we got for free with micro services",
    "start": "2182600",
    "end": "2188390"
  },
  {
    "text": "and we probably could have implemented something similar and our original monolith but we didn't have the",
    "start": "2188390",
    "end": "2194090"
  },
  {
    "text": "resources on the team like we did now so environmental isolation this was the",
    "start": "2194090",
    "end": "2202010"
  },
  {
    "text": "most frequent issue an unforeseen issue that we run into moving back to a monolith so at any given time we run",
    "start": "2202010",
    "end": "2209180"
  },
  {
    "text": "about 2,000 to 4,000 of these workers in production and something that we were",
    "start": "2209180",
    "end": "2215360"
  },
  {
    "text": "constantly running into was this workers written in node and we would constantly",
    "start": "2215360",
    "end": "2221390"
  },
  {
    "text": "have uncaught exceptions and uncaught exceptions are unfortunately common and",
    "start": "2221390",
    "end": "2226820"
  },
  {
    "text": "very easily can get into a code base undetected so for example I remember I",
    "start": "2226820",
    "end": "2233330"
  },
  {
    "text": "was on call one weekend and I happened to be in a different state with my whole family it was my grandma's 90th birthday",
    "start": "2233330",
    "end": "2239510"
  },
  {
    "text": "and we're all crammed into this hotel room and it's 2:00 a.m. on a Friday and I'm getting paged and I'm like what",
    "start": "2239510",
    "end": "2245300"
  },
  {
    "text": "could this possibly be it's 2:00 a.m. on a Friday nobody is deploying code right now and I go in and workers are crashing and I bug",
    "start": "2245300",
    "end": "2252770"
  },
  {
    "text": "it down to this catch block here and I go and look at github to see when this",
    "start": "2252770",
    "end": "2258170"
  },
  {
    "text": "was committed and it had been put in codebase two weeks ago but only just now we're events coming in triggering this",
    "start": "2258170",
    "end": "2264790"
  },
  {
    "text": "uncaught exception because the response here is not defined and so what happens",
    "start": "2264790",
    "end": "2270790"
  },
  {
    "text": "with an uncaught exception in node is that the process will exit which causes one of these workers to exit and when",
    "start": "2270790",
    "end": "2279070"
  },
  {
    "text": "one of them exits this event is considered fails and centrifuge will actually retry that event so when there",
    "start": "2279070",
    "end": "2286990"
  },
  {
    "text": "are multiple events that are coming in and hitting this code path we kind of see this cascading failure because now",
    "start": "2286990",
    "end": "2293410"
  },
  {
    "text": "new events are coming in causing these uncaught exceptions causing these workers to exit and then centrifuge is",
    "start": "2293410",
    "end": "2299230"
  },
  {
    "text": "retry and those events causing those workers to exit and we host these",
    "start": "2299230",
    "end": "2305170"
  },
  {
    "text": "workers on AWS as ECS platform and ECS will bring up a new worker whenever one",
    "start": "2305170",
    "end": "2311050"
  },
  {
    "text": "exits but not at a fast enough rate with how quickly they are exiting here and we",
    "start": "2311050",
    "end": "2317080"
  },
  {
    "text": "also have found that with when workers have a really high turnover like this ECS will actually stop scheduling new",
    "start": "2317080",
    "end": "2324220"
  },
  {
    "text": "tasks to come up so now we're in the situation where our worker pool is quickly shrinking but we're spending it",
    "start": "2324220",
    "end": "2330310"
  },
  {
    "text": "more traffic because of all the retries and then that creates this back pressure",
    "start": "2330310",
    "end": "2335800"
  },
  {
    "text": "situation where every customer and destination is impacted which should sound a bit familiar and what's really",
    "start": "2335800",
    "end": "2344080"
  },
  {
    "text": "interesting is that there was a new engineer on the team and one of the first suggestions that came from them",
    "start": "2344080",
    "end": "2349810"
  },
  {
    "text": "was how about we break all these destinations into their own service that way if there's an uncaught exception in",
    "start": "2349810",
    "end": "2356470"
  },
  {
    "text": "one now it's only reduced to that destination in those customers and I",
    "start": "2356470",
    "end": "2361900"
  },
  {
    "text": "mean there is some truth to that right like it would isolate the destinations very nicely so that an uncaught",
    "start": "2361900",
    "end": "2366910"
  },
  {
    "text": "exception would only impact that destination but we already been down that road and we already been burned by",
    "start": "2366910",
    "end": "2373540"
  },
  {
    "text": "the operational overhead that came with micro services all living or all the destinations living in their own service",
    "start": "2373540",
    "end": "2379660"
  },
  {
    "text": "and that doesn't actually solve the root issue here it really just decreases the blast",
    "start": "2379660",
    "end": "2386470"
  },
  {
    "text": "radius and so we thought about it a bit more and what we ended up doing was we created",
    "start": "2386470",
    "end": "2392140"
  },
  {
    "text": "and how this wrapper works is it's a container runtime that acts as a",
    "start": "2392140",
    "end": "2397690"
  },
  {
    "text": "middleware between the application and the infrastructure so a very simplified version of how it works in the uncaught",
    "start": "2397690",
    "end": "2403990"
  },
  {
    "text": "exception situation is when an uncaught exception happens that'll catch it discard it and then restart the worker",
    "start": "2403990",
    "end": "2410920"
  },
  {
    "text": "and this is really just a workaround because ECS can't restart tasks quick",
    "start": "2410920",
    "end": "2416320"
  },
  {
    "text": "enough with how quickly they exit and again this also doesn't solve the uncaught exception problem but now at",
    "start": "2416320",
    "end": "2423550"
  },
  {
    "text": "least our worker pool isn't shrinking and we're able to set up alerts in metrics when we have these uncaught",
    "start": "2423550",
    "end": "2428920"
  },
  {
    "text": "exceptions and then go in and fix them and so if you look at the trade-offs",
    "start": "2428920",
    "end": "2435010"
  },
  {
    "text": "again we've actually made some improvements on our environmentally she asian isolation and visibility we now",
    "start": "2435010",
    "end": "2441970"
  },
  {
    "text": "have the resources and time to build this type of wrapper or to add heap dumps on these hosts and as we've been",
    "start": "2441970",
    "end": "2451840"
  },
  {
    "start": "2450000",
    "end": "2678000"
  },
  {
    "text": "running into new issues it's been really interesting to see that almost every time that like our gut reaction is we",
    "start": "2451840",
    "end": "2458020"
  },
  {
    "text": "should break this apart into microservices but it never really solves",
    "start": "2458020",
    "end": "2463300"
  },
  {
    "text": "the root of our issues and I think the biggest lessons that we learned as we move from monolith to microservices and",
    "start": "2463300",
    "end": "2470020"
  },
  {
    "text": "then back to monolith is that it's really all about trade-offs and really understanding the issues that you're",
    "start": "2470020",
    "end": "2476650"
  },
  {
    "text": "dealing with and making the decision that's right for your team at the time if we had started with microservices",
    "start": "2476650",
    "end": "2484180"
  },
  {
    "text": "right away in 2013 there's a chance we never would have made it off the ground because of the operational overhead that comes with",
    "start": "2484180",
    "end": "2490600"
  },
  {
    "text": "them but then we quickly ran into environmental isolation where destinations were impacting one another",
    "start": "2490600",
    "end": "2497140"
  },
  {
    "text": "and with micro services you get good environmental isolation so we moved to",
    "start": "2497140",
    "end": "2502210"
  },
  {
    "text": "micro services and as you know we had more resources on the team so the operational overhead we were willing to",
    "start": "2502210",
    "end": "2507670"
  },
  {
    "text": "trade off a little bit and it isolated the destinations like we needed from one another at the time you know we were",
    "start": "2507670",
    "end": "2513940"
  },
  {
    "text": "having sales deals coming in saying you don't support this destination we need that otherwise we're not signing with",
    "start": "2513940",
    "end": "2519820"
  },
  {
    "text": "you and I think when we moved to micro service at this time didn't have a full understanding of what",
    "start": "2519820",
    "end": "2527280"
  },
  {
    "text": "the rear issue was we didn't understand that it was actually our cueing system that was causing a lot of our problems",
    "start": "2527280",
    "end": "2533100"
  },
  {
    "text": "and even if we did understand this and really took the time to think critically about it we had only 10 engineers total",
    "start": "2533100",
    "end": "2541020"
  },
  {
    "text": "at the time and I don't know if we would have been able to build something like centrifuge",
    "start": "2541020",
    "end": "2546150"
  },
  {
    "text": "that actually solved that problem for us centrifuge took us a full year and two of our most senior engineers to get it",
    "start": "2546150",
    "end": "2553110"
  },
  {
    "text": "into production and so micro-services provided that isolation that we needed at the time to scale but then after four",
    "start": "2553110",
    "end": "2561030"
  },
  {
    "text": "years in 2017 we hit a tipping point and the operational overhead is too great",
    "start": "2561030",
    "end": "2566160"
  },
  {
    "text": "for us we lacked the proper tooling and for testing and deploying these services",
    "start": "2566160",
    "end": "2571590"
  },
  {
    "text": "so our developer productivity was quickly declining and we were just trying to keep the system alive and the",
    "start": "2571590",
    "end": "2578820"
  },
  {
    "text": "trade-offs were no longer worth it but we now had the resources to really think critically about this problem and fixed",
    "start": "2578820",
    "end": "2585690"
  },
  {
    "text": "the system so we moved back to a monolith and this time we made sure to",
    "start": "2585690",
    "end": "2591900"
  },
  {
    "text": "think through every one of the trade-offs and have a good story around each we built traffic recorder proactively",
    "start": "2591900",
    "end": "2597480"
  },
  {
    "text": "since we knew we were gonna lose some of that modularity centrifuged helped us solve some of those environmental",
    "start": "2597480",
    "end": "2602580"
  },
  {
    "text": "isolation issues and as we iterate on this infrastructure and are continuing",
    "start": "2602580",
    "end": "2608550"
  },
  {
    "text": "to encounter new issues we're doing a much better job of really thinking through these trade-offs and doing our",
    "start": "2608550",
    "end": "2614820"
  },
  {
    "text": "best to attempt to assault the root issues that we're dealing with and I",
    "start": "2614820",
    "end": "2620310"
  },
  {
    "text": "think you're gonna take anything away from this talk it's that there is no silver bullet and you really need to",
    "start": "2620310",
    "end": "2626790"
  },
  {
    "text": "look at the trade-offs and you have to be comfortable with some of the sacrifices that you're gonna make when",
    "start": "2626790",
    "end": "2632490"
  },
  {
    "text": "we move to micro services and whenever it's suggested again it's never really solved the root of our problems and like",
    "start": "2632490",
    "end": "2639750"
  },
  {
    "text": "when we first moved it only decreased the blast radius and then those problems came back to bite us later but were much",
    "start": "2639750",
    "end": "2645960"
  },
  {
    "text": "much bigger nothing can really replace really critically thinking about your",
    "start": "2645960",
    "end": "2651690"
  },
  {
    "text": "problem and making sure you weigh the trade-offs and make the decision that is right for your team at the",
    "start": "2651690",
    "end": "2657470"
  },
  {
    "text": "and other parts of our infrastructure we actually still use micro services and they were great but our destinations are",
    "start": "2657470",
    "end": "2664760"
  },
  {
    "text": "a perfect example of how this trend can end up hurting your productivity and the",
    "start": "2664760",
    "end": "2670160"
  },
  {
    "text": "solution that works for us was moving back to a monolith",
    "start": "2670160",
    "end": "2674800"
  },
  {
    "text": "[Applause]",
    "start": "2675440",
    "end": "2680019"
  }
]