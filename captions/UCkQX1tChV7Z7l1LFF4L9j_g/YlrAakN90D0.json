[
  {
    "start": "0",
    "end": "497000"
  },
  {
    "text": "thank you all for being here wow this is a lot of people who are really introduced interested in debugging",
    "start": "3889",
    "end": "9469"
  },
  {
    "text": "production micro services and it turns out that this is a very important problem to think about why well it turns",
    "start": "9469",
    "end": "16580"
  },
  {
    "text": "out that 40 to 90 percent of the total cost of ownership of your services is",
    "start": "16580",
    "end": "21790"
  },
  {
    "text": "after your finished building it you will spend 10 times as much time debugging",
    "start": "21790",
    "end": "27470"
  },
  {
    "text": "and running your system in production as you will building it think back to this morning to the keynote right where as",
    "start": "27470",
    "end": "34190"
  },
  {
    "text": "described where Wilkes said I could spend an entire lifetime debugging bugs",
    "start": "34190",
    "end": "39770"
  },
  {
    "text": "in my own system right and our systems have gotten so much more complicated since then so it's really important that",
    "start": "39770",
    "end": "46400"
  },
  {
    "text": "we be able to understand what's going on in our systems in order to make sure that our users are happy so why are we",
    "start": "46400",
    "end": "53239"
  },
  {
    "text": "here today so I'm from the Google cloud of reliability engineering team and we",
    "start": "53239",
    "end": "58970"
  },
  {
    "text": "focus on teaching people who are potential cloud users how to build and operate reliable systems because if we",
    "start": "58970",
    "end": "65239"
  },
  {
    "text": "don't teach people this people are not going to feel prepared to run applications in the cloud and then",
    "start": "65239",
    "end": "71060"
  },
  {
    "text": "they're going to be unhappy with the reliability of their services so this is not a vendor pitch right like I want to",
    "start": "71060",
    "end": "76070"
  },
  {
    "text": "teach you how to be successful in the cloud because this is where the future is similarly I'm here because I I'm",
    "start": "76070",
    "end": "83510"
  },
  {
    "text": "actually unlikely as relatively new at Google I have only been here for about 18 months but I've been in the space",
    "start": "83510",
    "end": "88820"
  },
  {
    "text": "doing sre and DevOps II type things for about a decade now but the most remarkable thing I realized when I joined Google was I was working with",
    "start": "88820",
    "end": "94909"
  },
  {
    "text": "much much bigger systems more complex more replicas more everything but they were actually simpler to debug and I",
    "start": "94909",
    "end": "100880"
  },
  {
    "text": "realized this was not because of some magical Google thing they have going on but because of the techniques did they",
    "start": "100880",
    "end": "106970"
  },
  {
    "text": "use and it was nothing specific to Google about those techniques so I'm hoping to share some of those techniques with you so maybe your debugging can",
    "start": "106970",
    "end": "113090"
  },
  {
    "text": "become a little bit easier too so let's suppose that we have a service how many",
    "start": "113090",
    "end": "119090"
  },
  {
    "text": "of you in the audience raise your hands nice and tall are you running microcircuits almost everyone that's",
    "start": "119090",
    "end": "124909"
  },
  {
    "text": "good how many of you are running microsoft says it get running on dozens or hundreds of different replicas like",
    "start": "124909",
    "end": "132080"
  },
  {
    "text": "hundreds are different hundreds of different replicas ok some of you at least dozens how many of you have at least dozens of diff",
    "start": "132080",
    "end": "137480"
  },
  {
    "text": "machines that you're running across okay good so the idea that we ran into at",
    "start": "137480",
    "end": "143540"
  },
  {
    "text": "Google was that we had an explosion of metrics that we had an explosion of",
    "start": "143540",
    "end": "148640"
  },
  {
    "text": "cardinality that was caused by the fact that we were collecting a lot of different metrics about every single job",
    "start": "148640",
    "end": "154849"
  },
  {
    "text": "that we run and that each of those jobs is running on hundreds thousands of different hosts and as a result it was",
    "start": "154849",
    "end": "161390"
  },
  {
    "text": "difficult to get to the right data that we needed for the purposes of this talk we're going to assume that you're doing",
    "start": "161390",
    "end": "167030"
  },
  {
    "text": "latency-sensitive user query serving there are other approaches for bat serving but that could be an entirely",
    "start": "167030",
    "end": "172579"
  },
  {
    "text": "separate talk let's also talk about how we monitor our systems how we detect",
    "start": "172579",
    "end": "179030"
  },
  {
    "text": "failures in our systems I'd usually assume when I give this talk to an",
    "start": "179030",
    "end": "184819"
  },
  {
    "text": "audience of a series that people are using service level objectives but I know that many of you here are not",
    "start": "184819",
    "end": "189829"
  },
  {
    "text": "acessories raise your hand if you know what a service level objective is okay",
    "start": "189829",
    "end": "194900"
  },
  {
    "text": "that's about 10% of the audience so let's define it so a service level objective is a way of measuring whether",
    "start": "194900",
    "end": "201980"
  },
  {
    "text": "users are able to successfully use your service and you can design your systems",
    "start": "201980",
    "end": "207019"
  },
  {
    "text": "such that the only alert that you have is on that one metric are your users are",
    "start": "207019",
    "end": "212930"
  },
  {
    "text": "able to use your service or not because it turns out everything else is just noise it doesn't matter does it matter",
    "start": "212930",
    "end": "218359"
  },
  {
    "text": "if your hard disk is ninety percent full and it goes to ninety point zero zero one percent phone and alert buyers no as",
    "start": "218359",
    "end": "224750"
  },
  {
    "text": "long as your users are serving queries you're fine so the idea line the service",
    "start": "224750",
    "end": "230299"
  },
  {
    "text": "level objective is that we need to define what user happiness looks like and measure it at the highest possible",
    "start": "230299",
    "end": "237260"
  },
  {
    "text": "level and then if there is a problem we'll get alerted and start debugging through the staff to figure it out but",
    "start": "237260",
    "end": "244069"
  },
  {
    "text": "even if you aren't using service level objectives you can still get value out of this talk because it'll teach you how",
    "start": "244069",
    "end": "250639"
  },
  {
    "text": "to debug the N known and knowns in your service that you haven't set up explicit monitoring for and maybe our hope is",
    "start": "250639",
    "end": "256910"
  },
  {
    "text": "that it will persuade you to switch off some of those alerts because you know you will be able to debug problems using",
    "start": "256910",
    "end": "262370"
  },
  {
    "text": "the 10-8 techniques we describe if you want to know more about SL O's again that could be an entirely separate talk",
    "start": "262370",
    "end": "268310"
  },
  {
    "text": "but I do have a five minute talk that is and video that's the QR code on your screen so you can watch that afterwards",
    "start": "268310",
    "end": "275260"
  },
  {
    "text": "so let's suppose that we have our running micro service system and something goes wrong in our system and",
    "start": "275260",
    "end": "282230"
  },
  {
    "text": "this is at Google what happens well our users start experiencing errors and then",
    "start": "282230",
    "end": "287810"
  },
  {
    "text": "our service level indicators start failing either the ones that are set up on error counts or the ones there step",
    "start": "287810",
    "end": "294320"
  },
  {
    "text": "on latency that those values become unacceptable to us and we start burning through the budget allowed errors that",
    "start": "294320",
    "end": "301190"
  },
  {
    "text": "we allow ourselves to have in our production systems knowing that we can never really achieve a hundred percent",
    "start": "301190",
    "end": "307000"
  },
  {
    "text": "so that means that our service level objective that we set is in danger and that pagers need to go off and s eries",
    "start": "307000",
    "end": "313430"
  },
  {
    "text": "need to be woken up now what do we do as s eries we prioritize the speed with",
    "start": "313430",
    "end": "320000"
  },
  {
    "text": "which we mitigate the problem and recover from the problem to restore service as fast as possible so what does",
    "start": "320000",
    "end": "326300"
  },
  {
    "text": "this look like well I can kind of think about this in terms of a loop a loop of",
    "start": "326300",
    "end": "331730"
  },
  {
    "text": "of how we debug our problems so we start off meeting our service level objectives and then a fault happens somewhere in",
    "start": "331730",
    "end": "338180"
  },
  {
    "text": "our system and then we get woken up and we have to quickly figure out how big is",
    "start": "338180",
    "end": "346070"
  },
  {
    "text": "this problem is my entire service down is a hundred percent of my traffic serving errors or is it a smaller",
    "start": "346070",
    "end": "352730"
  },
  {
    "text": "problem that I can take my time resolving without dragging in additional people can I do something quickly to",
    "start": "352730",
    "end": "358310"
  },
  {
    "text": "mitigate like draining a bad region if so then I'm going to do that first but after that I have to go and start",
    "start": "358310",
    "end": "364850"
  },
  {
    "text": "figuring out what's going wrong with my collection of Micra services so the way",
    "start": "364850",
    "end": "370430"
  },
  {
    "text": "that I go about this is I think about what could be going wrong right like formulating a hypothesis as to what is",
    "start": "370430",
    "end": "376550"
  },
  {
    "text": "broken and then I'll test it I'll figure out is that actually what broke and",
    "start": "376550",
    "end": "382450"
  },
  {
    "text": "hopefully I've identified the right cause to begin with and then I can go",
    "start": "382450",
    "end": "387830"
  },
  {
    "text": "ahead and figure out a fix and I can test the fix and then hopefully I go back to meeting my service level objective right so everything's",
    "start": "387830",
    "end": "394700"
  },
  {
    "text": "happening but unfortunately that's not usually where I spend the majority of my time because it turns out a lot of the",
    "start": "394700",
    "end": "402590"
  },
  {
    "text": "time when we develop a solution it's not the right solution or when we",
    "start": "402590",
    "end": "407660"
  },
  {
    "text": "develop a hypothesis as to what we think is broken in our systems that's not actually what is broken and",
    "start": "407660",
    "end": "413419"
  },
  {
    "text": "we're back to square one so we wind up spending as site reliability engineers or as anyone who is on call for a",
    "start": "413419",
    "end": "419960"
  },
  {
    "text": "service we wind up spending a lot of time in that loop of formulating hypotheses and testing them so how can",
    "start": "419960",
    "end": "428449"
  },
  {
    "text": "we make our systems more resilient how can we make them more reliable for our users and how can we increase our",
    "start": "428449",
    "end": "434449"
  },
  {
    "text": "velocity with which we're able to experiment and push features out to users the way that we can do that is by",
    "start": "434449",
    "end": "440180"
  },
  {
    "text": "speeding up that loop by formulating hypotheses faster and testing them faster in decision science this is",
    "start": "440180",
    "end": "447440"
  },
  {
    "text": "described as the OODA loop the absorb sorry observe orient decide act loop so",
    "start": "447440",
    "end": "452870"
  },
  {
    "text": "today we're going to work through a set of practical examples using three best practices layer peeling dynamic data",
    "start": "452870",
    "end": "459860"
  },
  {
    "text": "joins and exemplars as a note about the technologies that we're showing you today we're showing you google x'",
    "start": "459860",
    "end": "466909"
  },
  {
    "text": "internal monitoring system which is called panopticon which is a front-end for monarch because our Google's two",
    "start": "466909",
    "end": "472400"
  },
  {
    "text": "internal technologies but the techniques are fully generic you can implant them and take them home with you and do them",
    "start": "472400",
    "end": "478580"
  },
  {
    "text": "today we'll name public implementations both open source and vendor supplied and",
    "start": "478580",
    "end": "484039"
  },
  {
    "text": "we're also going to mention that some of these features are already present in Google's particular implementation in",
    "start": "484039",
    "end": "489860"
  },
  {
    "text": "stature iver so adam you're on call and your pager goes off beep okay here we go",
    "start": "489860",
    "end": "499509"
  },
  {
    "start": "497000",
    "end": "537000"
  },
  {
    "text": "this is this is sort of a text version of basically what I would receive when my pager goes off it's not the most",
    "start": "499509",
    "end": "506569"
  },
  {
    "text": "helpful thing but because all of our alerts are based on s ellos all it's telling me is one of these SLO s is in",
    "start": "506569",
    "end": "512270"
  },
  {
    "text": "danger we have a service level indicator of how fast the latency of our queries",
    "start": "512270",
    "end": "517789"
  },
  {
    "text": "and it looks like there are too many of them which are not meeting their objective there are too many slow queries it's telling me that the the SLA",
    "start": "517789",
    "end": "525200"
  },
  {
    "text": "is being violated in uswest one the region uswest one because we we only",
    "start": "525200",
    "end": "530420"
  },
  {
    "text": "have regional SLO s on this imaginary service there are a lot of there are a",
    "start": "530420",
    "end": "536240"
  },
  {
    "text": "lot of there are a lot of different services",
    "start": "536240",
    "end": "541260"
  },
  {
    "start": "537000",
    "end": "568000"
  },
  {
    "text": "which are running inside of us West one some of which are in different zones we don't actually have zone level indicator",
    "start": "541260",
    "end": "547200"
  },
  {
    "text": "so all that this alert is telling me is that something in u.s. West one is essentially too slow now this could be",
    "start": "547200",
    "end": "552780"
  },
  {
    "text": "caused by all of the replicas in u.s. West one being slowed a little bit slow or it could be one replica which is",
    "start": "552780",
    "end": "559530"
  },
  {
    "text": "being very slow all it means is some percentile of our queries are being served too slow so I'm gonna dig in and",
    "start": "559530",
    "end": "566220"
  },
  {
    "text": "try and find out exactly what's wrong first technique I'm gonna use here is called lay appealing because as I",
    "start": "566220",
    "end": "572550"
  },
  {
    "start": "568000",
    "end": "588000"
  },
  {
    "text": "mentioned all of these all these are lies they're composites we're not going to set off an alarm",
    "start": "572550",
    "end": "578430"
  },
  {
    "text": "every single time one of our replicas goes slow so we have to merge all those together somehow and then when we want",
    "start": "578430",
    "end": "583560"
  },
  {
    "text": "to find out what's going wrong we need to peel back those layers so first I'm going to start here show a diagram first",
    "start": "583560",
    "end": "592500"
  },
  {
    "start": "588000",
    "end": "612000"
  },
  {
    "text": "I'm gonna start looking at the region which is going wrong I can disregard US central annuus East one because those",
    "start": "592500",
    "end": "597750"
  },
  {
    "text": "aren't paging me right now I'm going to dive into there and find out which zone is going wrong and once I've isolated",
    "start": "597750",
    "end": "604170"
  },
  {
    "text": "that I'm going to drill down into the individual replicas so this is roughly how imaginary services is architected so",
    "start": "604170",
    "end": "612840"
  },
  {
    "start": "612000",
    "end": "645000"
  },
  {
    "text": "here's pecan there was a link in the or panopticon rival we call it pecan internally there was a link inside of",
    "start": "612840",
    "end": "619500"
  },
  {
    "text": "the page that I received which takes me to this page and you can see pretty clearly what's happening here we have we",
    "start": "619500",
    "end": "625410"
  },
  {
    "text": "have a constant line at hundred milliseconds which is our SLO we promise that within a certain region thank you",
    "start": "625410",
    "end": "632220"
  },
  {
    "text": "within a certain region all of the queries are going to be well 99% of the queries are going to be served below",
    "start": "632220",
    "end": "638160"
  },
  {
    "text": "this line so the fact that this red line has gone over this constant means that I'm getting paged this happened a couple",
    "start": "638160",
    "end": "644160"
  },
  {
    "text": "of minutes ago so the first thing I do is pop out the sidebar panopticon it's a very interactive tool it's roughly our",
    "start": "644160",
    "end": "650670"
  },
  {
    "start": "645000",
    "end": "668000"
  },
  {
    "text": "internal equivalent of something like graphite but it's it's much more deeply integrated with the backend so as you",
    "start": "650670",
    "end": "656820"
  },
  {
    "text": "can see the query which is being shown on the screen now which is which is representing the metric which paged me",
    "start": "656820",
    "end": "664050"
  },
  {
    "text": "is actually editable I if we zoom in a little bit we can see that there's a",
    "start": "664050",
    "end": "671100"
  },
  {
    "start": "668000",
    "end": "693000"
  },
  {
    "text": "because we have all of the data for all of the regions of course but there's only one which is paging me so panopticon has helpfully filtered all of",
    "start": "671100",
    "end": "677940"
  },
  {
    "text": "the other regions out but as Liz mentioned the very first thing I need to do is establish the blast radius of of",
    "start": "677940",
    "end": "683400"
  },
  {
    "text": "this outage it's possible that this is the first region which is paged but all of them are on the way up and the rest",
    "start": "683400",
    "end": "688770"
  },
  {
    "text": "are going to page me soon so I can remove this filter now there are no filters and if we look back at the chart",
    "start": "688770",
    "end": "695220"
  },
  {
    "start": "693000",
    "end": "704000"
  },
  {
    "text": "now we can see that all of the regions are plotted now one of them is clearly in danger but the other two are",
    "start": "695220",
    "end": "701250"
  },
  {
    "text": "basically fine I don't see any kind of problem there so that's a relief and I'm gonna filter them away oops sorry I",
    "start": "701250",
    "end": "709080"
  },
  {
    "start": "704000",
    "end": "715000"
  },
  {
    "text": "skipped forward a few too many slides there what happened so obvious question",
    "start": "709080",
    "end": "716700"
  },
  {
    "start": "715000",
    "end": "746000"
  },
  {
    "text": "why don't we just look at all of the streams straight away all of the streams that make this composite SLI because if",
    "start": "716700",
    "end": "722880"
  },
  {
    "text": "we did it would just be a complete jumble on the screen there's there's no way that I can discern any kind of useful signal out of this the whole",
    "start": "722880",
    "end": "728730"
  },
  {
    "text": "point of this exercise is to turn that jumble into one stream where I can identify that this is the actual problem",
    "start": "728730",
    "end": "734670"
  },
  {
    "text": "and you can see on the right hand side there if we look at the key each one of these different streams is representing",
    "start": "734670",
    "end": "740040"
  },
  {
    "text": "a different replica in a different region or or zone each one has its own latency metric so we'll go back to",
    "start": "740040",
    "end": "747180"
  },
  {
    "start": "746000",
    "end": "888000"
  },
  {
    "text": "bisecting we now we're now showing the problematic region again one of the",
    "start": "747180",
    "end": "752700"
  },
  {
    "text": "really cool things about panopticon about monarch in general is that these are aggregates that were looking at now",
    "start": "752700",
    "end": "758160"
  },
  {
    "text": "and these aggregates are computed behind the scenes I'm not doing a big query here where I'm taking the latency",
    "start": "758160",
    "end": "763260"
  },
  {
    "text": "metrics or latency streams all of the latency streams in the world and aggregating them at query time they've",
    "start": "763260",
    "end": "769440"
  },
  {
    "text": "already been aggregated for me and this this is it's a little bit like like a",
    "start": "769440",
    "end": "774690"
  },
  {
    "text": "materialized view in a sequel database and I know that influx calls these are standing queries but we call these pre",
    "start": "774690",
    "end": "781440"
  },
  {
    "text": "computations internally we just have we have query evaluators running behind the scenes we just do these queries all day",
    "start": "781440",
    "end": "787950"
  },
  {
    "text": "and then rather than outputting to the screen like we are now they're just output into another metric and then that metric is the one that",
    "start": "787950",
    "end": "793740"
  },
  {
    "text": "alerts me so I'm looking at the output of this pre-computation right now but I want to drill down so I've highlighted",
    "start": "793740",
    "end": "799370"
  },
  {
    "text": "this link we have replaced with the definition and essentially this replaces in line the query with the query which",
    "start": "799370",
    "end": "807030"
  },
  {
    "text": "behind the scenes was run to generate the query that I'm seeing now if we skip back and forth you can see the actual",
    "start": "807030",
    "end": "813170"
  },
  {
    "text": "the line itself the data it doesn't change at all it's the same data of course this one was derived from the",
    "start": "813170",
    "end": "818490"
  },
  {
    "text": "other but in the query editor on the left hand side now we've got a lot more control over my query this query is a",
    "start": "818490",
    "end": "824940"
  },
  {
    "text": "little bit slower not noticeably slow because we're pulling in more data and rendering it out query time as opposed",
    "start": "824940",
    "end": "830220"
  },
  {
    "text": "to doing a pre computation time but this is cool because it means I can modify it which is what I'm going to do look at",
    "start": "830220",
    "end": "835470"
  },
  {
    "text": "the very last one the other thing about pre computations is that it's a level of abstraction right it kind of lets you",
    "start": "835470",
    "end": "841320"
  },
  {
    "text": "form libraries remember the talk from this morning about libraries right if you can make parts of your monitoring",
    "start": "841320",
    "end": "846390"
  },
  {
    "text": "queries libraries that you can pull into other parts of your modern queries that reduces the amount of detail that you",
    "start": "846390",
    "end": "851730"
  },
  {
    "text": "need to keep in your head at any given time look at how much more complex this query is yeah yeah this is much more",
    "start": "851730",
    "end": "857100"
  },
  {
    "text": "complex and this isn't even the raw data we're still looking at a pre computation here which we're going to drill down into further if you look at the last the",
    "start": "857100",
    "end": "865470"
  },
  {
    "text": "last I guess operation in the query editor it's a group operation and these group operations are essentially a",
    "start": "865470",
    "end": "870960"
  },
  {
    "text": "lexical group operations we take a lot of streams and then we output a smaller number of stream space by aggregating",
    "start": "870960",
    "end": "876030"
  },
  {
    "text": "the input streams in some way when I when I'm looking at this the chart here",
    "start": "876030",
    "end": "882960"
  },
  {
    "text": "this is grouped by region because we only want to see it by region but I want to see it by zone now so I can go down to the bottom group it by zone as well",
    "start": "882960",
    "end": "889710"
  },
  {
    "start": "888000",
    "end": "937000"
  },
  {
    "text": "and now the output will show me the aggregated latency of all of the",
    "start": "889710",
    "end": "894960"
  },
  {
    "text": "replicas in each of these zones you can see one of these zones is looking totally fine one of them really",
    "start": "894960",
    "end": "900840"
  },
  {
    "text": "interestingly is above our regional SLO which you may think why isn't this",
    "start": "900840",
    "end": "906210"
  },
  {
    "text": "paging me already but we don't have a zonal SLO in this imaginary service we don't we don't offer any guarantees",
    "start": "906210",
    "end": "911250"
  },
  {
    "text": "about how fast individual zones will be only that the within a region 99% of all",
    "start": "911250",
    "end": "916590"
  },
  {
    "text": "of the queries will be less than 100 milliseconds so that's why that didn't page me but you can see one of these",
    "start": "916590",
    "end": "922740"
  },
  {
    "text": "lines very clearly is going up it looks like we have some kind of problem in this region so we're going to drill down into that if we pop up in the sidebar we",
    "start": "922740",
    "end": "930150"
  },
  {
    "text": "can see which zone it's is in exactly it's us West 1a so let's focus on that we're going to go back to the other side",
    "start": "930150",
    "end": "936360"
  },
  {
    "text": "bar and we're going to add another filter so this time we want to throw away all of",
    "start": "936360",
    "end": "942540"
  },
  {
    "start": "937000",
    "end": "947000"
  },
  {
    "text": "the input data except for the replicas which are in u.s. West 1a so we've done",
    "start": "942540",
    "end": "948900"
  },
  {
    "start": "947000",
    "end": "1002000"
  },
  {
    "text": "that and if we scroll down to the one at this point we we could keep drilling in but I want to show you something really",
    "start": "948900",
    "end": "955710"
  },
  {
    "text": "interesting which is which is often the first step that I take when diagnosing these problems the very last step here",
    "start": "955710",
    "end": "961020"
  },
  {
    "text": "is transforming if you can see the types of these queries they're wedged in between the operations one of them is a",
    "start": "961020",
    "end": "966600"
  },
  {
    "text": "distribution so we met we measure a lot of things with distributions latency distributions that is and the very last",
    "start": "966600",
    "end": "973920"
  },
  {
    "text": "item is is a double it's just a number essentially and so the last step is transforming this distribution by taking",
    "start": "973920",
    "end": "978990"
  },
  {
    "text": "the 98th I think percentile and transforming it into a number now we have to do this because the distribution",
    "start": "978990",
    "end": "984900"
  },
  {
    "text": "itself is very hard to interpret I mean we could it would be very hard to write a rule on that distribution to alert",
    "start": "984900",
    "end": "990930"
  },
  {
    "text": "people or yeah for a machine to evaluate into a lot of people but this is this is pretty easy for a machine to evaluate",
    "start": "990930",
    "end": "996510"
  },
  {
    "text": "it's a number if it gets too big send me a page easy but when I'm debugging I want to look at a bit more detail so I",
    "start": "996510",
    "end": "1001550"
  },
  {
    "text": "can remove by clicking on this little cross the last operation and now the output of this query is a distribution",
    "start": "1001550",
    "end": "1008840"
  },
  {
    "start": "1002000",
    "end": "1125000"
  },
  {
    "text": "so the way that pecan renders this I think is really cool this is a heat map and I've deliberately I've made it lower",
    "start": "1008840",
    "end": "1015200"
  },
  {
    "text": "resolution and we would usually look at so it's a little bit easier to understand him but essentially the black",
    "start": "1015200",
    "end": "1020690"
  },
  {
    "text": "area at the top or the left hand the y-axis I guess represents the latency of",
    "start": "1020690",
    "end": "1026540"
  },
  {
    "text": "course it's it's the same as the previous chart and the y axis or x axis rather represents the time so you can",
    "start": "1026540",
    "end": "1032180"
  },
  {
    "text": "see at some point this this heat map started to extend upwards the black area at the top essentially means there are",
    "start": "1032180",
    "end": "1037459"
  },
  {
    "text": "no there are no queries which are within this latency earlier on five minutes ago there were no queries which were taking",
    "start": "1037460",
    "end": "1043069"
  },
  {
    "text": "140 milliseconds but now there are some like we've crept up and we can see those lines which are overlaid the fiftieth I",
    "start": "1043070",
    "end": "1049490"
  },
  {
    "text": "think 90 F 98 and 99 percentile have started creeping up and so now there are some number of queries not a tremendous",
    "start": "1049490",
    "end": "1055940"
  },
  {
    "text": "amount it's really only affecting the top few percentiles but there are some of them what's really interesting as you can see the bottom of the heat map it",
    "start": "1055940",
    "end": "1062270"
  },
  {
    "text": "hasn't changed at all the vast majority the 50th percentile are still basically where they were so we're not looking at",
    "start": "1062270",
    "end": "1069050"
  },
  {
    "text": "a situation where the whole zone is you can imagine why that may happen some kind of network contention or something",
    "start": "1069050",
    "end": "1074290"
  },
  {
    "text": "but we're not looking about here we're looking at a subset of the replicas inside of this zone are serving slowly",
    "start": "1074290",
    "end": "1080240"
  },
  {
    "text": "so we're gonna drill in a little bit further back to the line again we're going to replace this definition now",
    "start": "1080240",
    "end": "1086570"
  },
  {
    "text": "that we've focused it on one zone we're going to replace this query with its definition to look at all of the replicas in this zone and I think we can",
    "start": "1086570",
    "end": "1093500"
  },
  {
    "text": "pretty clearly see what the problem is here most of our replicas are behaving as normally but two of them the 1998 I",
    "start": "1093500",
    "end": "1101240"
  },
  {
    "text": "think percentile has just gone through the roof recently so let's assume at this point that there's something wrong with these replicas we skipped a slide",
    "start": "1101240",
    "end": "1109250"
  },
  {
    "text": "here but if we if we hovered over one of those one of those lines we would see",
    "start": "1109250",
    "end": "1114380"
  },
  {
    "text": "that the replica idea of this was for one of them so we an add another filter so now we're looking exclusively at the",
    "start": "1114380",
    "end": "1121190"
  },
  {
    "text": "latency metrics which are coming out of replicas for in uswest 1a again we look",
    "start": "1121190",
    "end": "1127070"
  },
  {
    "start": "1125000",
    "end": "1164000"
  },
  {
    "text": "at the bottom operation in in this query and it's a distribution to percentile or distribution to double operation and",
    "start": "1127070",
    "end": "1134330"
  },
  {
    "text": "we're going to remove it so we're looking at the heatmap this is the raw distribution of latency coming out of that one metric and this is really",
    "start": "1134330",
    "end": "1139700"
  },
  {
    "text": "interesting we're not seeing the same heat map as we were at the zone level we've seen quite clearly that where",
    "start": "1139700",
    "end": "1145880"
  },
  {
    "text": "previously our 50th percentile was around 60 milliseconds it's gone through the roof in fact there are now very few",
    "start": "1145880",
    "end": "1151220"
  },
  {
    "text": "queries coming out of this replica or being served by this replica which are being completed within a hundred milliseconds at all so this this replica",
    "start": "1151220",
    "end": "1157910"
  },
  {
    "text": "is completely broken at this point so I'm gonna go ahead and restart it and hope that that solves the problem that",
    "start": "1157910",
    "end": "1163160"
  },
  {
    "text": "would there would be a good stabbing at mitigation I'm optimistic so that's lay appealing lay appealing is",
    "start": "1163160",
    "end": "1170300"
  },
  {
    "start": "1164000",
    "end": "1198000"
  },
  {
    "text": "really helpful in situations where some kind of aggregate of a signal which is",
    "start": "1170300",
    "end": "1175940"
  },
  {
    "text": "being emitted by a very large number of targets or let's say replicas is causing an alert and you need to drill down or",
    "start": "1175940",
    "end": "1182570"
  },
  {
    "text": "bisect into that to find out what or which replica is causing the problem but",
    "start": "1182570",
    "end": "1188630"
  },
  {
    "text": "of course much much the time when we're dealing with micro-services there are far too many to just start by looking at all from you need some way of",
    "start": "1188630",
    "end": "1194540"
  },
  {
    "text": "intelligently drilling down and that's that's lay appealing but that isn't",
    "start": "1194540",
    "end": "1199670"
  },
  {
    "start": "1198000",
    "end": "1209000"
  },
  {
    "text": "always the case sometimes it's not so much that there's one time which is going wrong with some number of",
    "start": "1199670",
    "end": "1205669"
  },
  {
    "text": "tasks to have something in common like their location it's a thank you it's a",
    "start": "1205669",
    "end": "1211730"
  },
  {
    "start": "1209000",
    "end": "1228000"
  },
  {
    "text": "there's some other cause some other quality that those replicas have that",
    "start": "1211730",
    "end": "1216859"
  },
  {
    "text": "there is where there is no path between the alert and the actual the actual",
    "start": "1216859",
    "end": "1223070"
  },
  {
    "text": "problem you've got to form a more elaborate hypothesis and you've got a pull together data to figure out what's wrong so just to illustrate that we may",
    "start": "1223070",
    "end": "1231409"
  },
  {
    "start": "1228000",
    "end": "1265000"
  },
  {
    "text": "be being alerted in one region but actually the problems kind of everywhere and there's no way there's no clear way",
    "start": "1231409",
    "end": "1237980"
  },
  {
    "text": "to like drill down and find out what is the problem just based on that one query so we're gonna have to join it where",
    "start": "1237980",
    "end": "1243799"
  },
  {
    "text": "other queries when we call that dynamic query joining I would open up to if I were to let's imagine that my first",
    "start": "1243799",
    "end": "1250369"
  },
  {
    "text": "solution did not work out at all they were there were no replicas which I found for sure at fault I would open up",
    "start": "1250369",
    "end": "1255499"
  },
  {
    "text": "a new tab and peek on cuz I'm gonna make my own query because I have a hypothesis about what might be wrong with this I remember a few days ago there was some",
    "start": "1255499",
    "end": "1260929"
  },
  {
    "text": "kind of I overheard the kernel team talking about latency so maybe I'm going",
    "start": "1260929",
    "end": "1266210"
  },
  {
    "start": "1265000",
    "end": "1336000"
  },
  {
    "text": "to look into that I'm gonna look in my service I'm going to pull out a totally different metric all of our all of our",
    "start": "1266210",
    "end": "1272289"
  },
  {
    "text": "jobs or tasks that we run at Google export as Liz said a tremendous number of metrics one of which is the kernel",
    "start": "1272289",
    "end": "1278749"
  },
  {
    "text": "version and it's worth noting that not I know not all micro services do this",
    "start": "1278749",
    "end": "1283759"
  },
  {
    "text": "not everyone runs on a standardized codebase where you have all of these useful metrics coming out all of the",
    "start": "1283759",
    "end": "1289009"
  },
  {
    "text": "time but I owed you two because it's great user envoy or any other service",
    "start": "1289009",
    "end": "1295220"
  },
  {
    "text": "mesh because it'll give you lots of these out-of-the-box metrics absolutely and then even though you know I'm not going to rely on my kernel version of",
    "start": "1295220",
    "end": "1301519"
  },
  {
    "text": "course but when something's going wrong then I might want to join that data back to the the error signal that I'm going",
    "start": "1301519",
    "end": "1307580"
  },
  {
    "text": "to see if I can diagnose what's wrong in it so I'm going to do here so I've pulled up the kernel versions of each of",
    "start": "1307580",
    "end": "1312649"
  },
  {
    "text": "my different replicas so there are quite a few of them too many to be useful but I am going to and if we were to zoom in",
    "start": "1312649",
    "end": "1320779"
  },
  {
    "text": "on one of those rows we would see each of those rows has all the standard information about it it means that for",
    "start": "1320779",
    "end": "1327259"
  },
  {
    "text": "each of each replica we export the region we export the zone and we export",
    "start": "1327259",
    "end": "1332539"
  },
  {
    "text": "the kernel version it's running in this case it's one one one one so I'm going to create a group by",
    "start": "1332539",
    "end": "1338220"
  },
  {
    "start": "1336000",
    "end": "1360000"
  },
  {
    "text": "operation I'm rather than editing queries which are being given to me by my pager I'm now creating queries from",
    "start": "1338220",
    "end": "1344910"
  },
  {
    "text": "scratch so I've added a group I've grouped these by version and I'm using a count reducer so essentially what this",
    "start": "1344910",
    "end": "1351450"
  },
  {
    "text": "means is given all of those streams I want to see one stream for each different kernel version and I want to",
    "start": "1351450",
    "end": "1357240"
  },
  {
    "text": "know how many of each of those streams have that kernel version and this is how it goes we had it looked like we have",
    "start": "1357240",
    "end": "1362880"
  },
  {
    "text": "about 138 running on version one one one one and some of our older versions we",
    "start": "1362880",
    "end": "1368010"
  },
  {
    "text": "have a few stragglers running down there one is as old as 1 0-0 to which we have coincidentally two tasks running hours",
    "start": "1368010",
    "end": "1374310"
  },
  {
    "text": "so if I jump back into the latency metrics that we saw before these were completely indecipherable so you may",
    "start": "1374310",
    "end": "1380220"
  },
  {
    "start": "1375000",
    "end": "1415000"
  },
  {
    "text": "recall we had to drill into them to find out what was wrong but I can take these metrics and I can join the kernel",
    "start": "1380220",
    "end": "1385260"
  },
  {
    "text": "version on to this to make something which is more useful I've joined these and if you if you see in the in the",
    "start": "1385260",
    "end": "1391950"
  },
  {
    "text": "query visualizer on the left-hand side we've essentially taken this query the the latency and the previous query and",
    "start": "1391950",
    "end": "1398430"
  },
  {
    "text": "we've joined them together so rather than dealing with two separate queries we're now dealing with one which outputs a tuple containing the latency and",
    "start": "1398430",
    "end": "1406650"
  },
  {
    "text": "kernel version you see on the right hand side we stored all over all of those streams for latency but we've augmented",
    "start": "1406650",
    "end": "1412530"
  },
  {
    "text": "them with the kernel version so now we can do something clever with this we can group the we can group the latency we",
    "start": "1412530",
    "end": "1421260"
  },
  {
    "start": "1415000",
    "end": "1428000"
  },
  {
    "text": "can take the mean latency of each of those different kernel versions because again I have a hypothesis that this",
    "start": "1421260",
    "end": "1426690"
  },
  {
    "text": "might be related and what do you know it is one of these kernel versions has a",
    "start": "1426690",
    "end": "1432390"
  },
  {
    "start": "1428000",
    "end": "1438000"
  },
  {
    "text": "mean latency which is much higher and it started about the same time as the other kernel versions so if I pop up in a",
    "start": "1432390",
    "end": "1439230"
  },
  {
    "start": "1438000",
    "end": "1445000"
  },
  {
    "text": "sidebar then yeah it's this straggler kernel which for some reason is still rolled out in production which is exhibiting a lot higher latency so at",
    "start": "1439230",
    "end": "1446040"
  },
  {
    "text": "this point I don't know maybe it would be a good time to firstly kill those jobs to mitigate the problem but",
    "start": "1446040",
    "end": "1451140"
  },
  {
    "text": "secondly to maybe file a bug and say hey why are these all turn around maybe we could set up some kind of non paging alert based on old kernel versions as",
    "start": "1451140",
    "end": "1458160"
  },
  {
    "text": "now we know that they're causing problems but I want to see that's that's a it's a different talk really the summary of dynamic data joins is you",
    "start": "1458160",
    "end": "1464670"
  },
  {
    "text": "don't necessarily have to export every single piece of information which might be useful we call them tags all",
    "start": "1464670",
    "end": "1470690"
  },
  {
    "text": "or I guess internally we call them target fields but I've heard them referred to as tags and labels if you",
    "start": "1470690",
    "end": "1476989"
  },
  {
    "text": "were to export every single one of those for example the kernel version with every single metric you would just be you would be completely overwhelmed of",
    "start": "1476989",
    "end": "1482450"
  },
  {
    "text": "these useless data what you can do instead is export this data from each of your tasks without necessarily alerting",
    "start": "1482450",
    "end": "1488899"
  },
  {
    "text": "on it and then later on at alert time when you're debugging you can join it back together at query time and drive a lot more value from it so that's dynamic",
    "start": "1488899",
    "end": "1496460"
  },
  {
    "start": "1495000",
    "end": "1521000"
  },
  {
    "text": "data joining so you'll notice that atom has spent the past ten or fifteen",
    "start": "1496460",
    "end": "1501529"
  },
  {
    "text": "minutes telling you about how he might debug problems starting kind of from first principle starting from the tack",
    "start": "1501529",
    "end": "1506899"
  },
  {
    "text": "of top of the stack working his way all the way down to the bottom in fine grain detail but what if I told you that I",
    "start": "1506899",
    "end": "1513320"
  },
  {
    "text": "could beat Adam to debugging the problem Juvenal by using a really cool technique that we developed called exemplars the",
    "start": "1513320",
    "end": "1521599"
  },
  {
    "text": "theory behind exemplars is that if you have high cardinality data like individual task level data where you",
    "start": "1521599",
    "end": "1528320"
  },
  {
    "text": "have distributed tracing turned on in your systems that you can use sampling",
    "start": "1528320",
    "end": "1533359"
  },
  {
    "text": "you can probabilistically keep small amounts of that data around and that you",
    "start": "1533359",
    "end": "1538940"
  },
  {
    "text": "can use that data to jump directly to where your problem is rather than having to repeatedly bisect to find the problem",
    "start": "1538940",
    "end": "1546019"
  },
  {
    "text": "in your stack now you can do this with any kind of graphing tool in fact I know",
    "start": "1546019",
    "end": "1551599"
  },
  {
    "text": "that light step has implemented this inside of gravano today and that other providers such as honeycomb also have it",
    "start": "1551599",
    "end": "1558200"
  },
  {
    "text": "integrated into their UI so you have to mark in your distribution graphing tool which individual buckets have samples",
    "start": "1558200",
    "end": "1566539"
  },
  {
    "text": "associated with them and then you can highlight which samples have the same tags so we'll let you more easily cross",
    "start": "1566539",
    "end": "1573859"
  },
  {
    "text": "correlate the problem and then potentially go off into Zipkin or your tracing tool of choice in order to get",
    "start": "1573859",
    "end": "1580700"
  },
  {
    "text": "the find ring details so let's walk through what this might look like so essentially what we have is this is the",
    "start": "1580700",
    "end": "1588590"
  },
  {
    "text": "same system that I was showing you with layer peeling except for I have a tool that will let me say hey by the way",
    "start": "1588590",
    "end": "1594979"
  },
  {
    "text": "replicas 1 & 3 are bad that's where high latency is coming from just with one click of a button so what",
    "start": "1594979",
    "end": "1600859"
  },
  {
    "text": "does this look like so let's remember back when we are debugging the problem where we had a few tasks",
    "start": "1600859",
    "end": "1607140"
  },
  {
    "text": "that were slow although I didn't know it at the time and we were trying to figure out globally kind of where is this rise",
    "start": "1607140",
    "end": "1613800"
  },
  {
    "text": "in 98th percentile latency coming from so what I can do is I can inside of the",
    "start": "1613800",
    "end": "1620370"
  },
  {
    "text": "group by where I would normally throw out all of the task level data and say you know what I don't care that task one",
    "start": "1620370",
    "end": "1626730"
  },
  {
    "text": "two three or four all I care about is that it's in u.s. West one a but I can",
    "start": "1626730",
    "end": "1631800"
  },
  {
    "text": "say I want to retain some of the task IDs so if I do that what happens",
    "start": "1631800",
    "end": "1638600"
  },
  {
    "text": "well winds up happening is that suddenly I get a bunch of little little green",
    "start": "1638600",
    "end": "1644309"
  },
  {
    "text": "lines over here I know it's hard to see on a conference projector but I see a bunch of little green lines indicating",
    "start": "1644309",
    "end": "1650730"
  },
  {
    "text": "that there is data associated with each of those buckets and if I click on one it will highlight all the other buckets",
    "start": "1650730",
    "end": "1658110"
  },
  {
    "text": "that sure the exact same tag so if I click on it it will tell me hey by the",
    "start": "1658110",
    "end": "1663840"
  },
  {
    "text": "way Liz that task that is slow is task number four and that'll let me jump",
    "start": "1663840",
    "end": "1671070"
  },
  {
    "text": "directly to the task that's failing and start investigating or maybe mitigate by killing it in the short term right so",
    "start": "1671070",
    "end": "1677700"
  },
  {
    "text": "what took Adam about 10 minutes to bisect took me 20 seconds to look at because of the availability of this",
    "start": "1677700",
    "end": "1684150"
  },
  {
    "text": "fine-grained data inside of my distribution graphing here's a real-world non contrived example we have",
    "start": "1684150",
    "end": "1691740"
  },
  {
    "start": "1687000",
    "end": "1735000"
  },
  {
    "text": "many many machines running at Google so many that I can't tell you and we need",
    "start": "1691740",
    "end": "1697350"
  },
  {
    "text": "to keep track of for instance how long does it take each machine to reboot what was the current boot time on the most",
    "start": "1697350",
    "end": "1703800"
  },
  {
    "text": "recent reboot and as you can imagine retaining the task level data for every",
    "start": "1703800",
    "end": "1709050"
  },
  {
    "text": "single machine at Google might be very very expensive but if I only keep one",
    "start": "1709050",
    "end": "1714630"
  },
  {
    "text": "example for the most common value say a hundred seconds or 80 seconds to boot then that lets me focus on the outliers",
    "start": "1714630",
    "end": "1721530"
  },
  {
    "text": "and retain the task level data for those as well and there will be much less duplication of that data and I can go",
    "start": "1721530",
    "end": "1728460"
  },
  {
    "text": "and click through and find out hey that one machine is slow I should look at the logs to figure out why it rebooted",
    "start": "1728460",
    "end": "1733980"
  },
  {
    "text": "slowly but let's talk about tracing as well so in the case of tracing how do I make",
    "start": "1733980",
    "end": "1740540"
  },
  {
    "start": "1735000",
    "end": "1766000"
  },
  {
    "text": "this useful this is an example from these team that I used to manage until two years ago so this is the this is the",
    "start": "1740540",
    "end": "1748580"
  },
  {
    "text": "black box monitoring system where we've said we care about the right latency as",
    "start": "1748580",
    "end": "1753890"
  },
  {
    "text": "performed by the black box prober and we've told it hey by the way sample one percent of the queries that you issue",
    "start": "1753890",
    "end": "1760280"
  },
  {
    "text": "and send them to dapper or tracing system as it can is the public equivalent so I can go and look here",
    "start": "1760280",
    "end": "1768080"
  },
  {
    "start": "1766000",
    "end": "1830000"
  },
  {
    "text": "because today for example I've just gotten paged and what I can see is I can see that on one hand I have overlaid",
    "start": "1768080",
    "end": "1774290"
  },
  {
    "text": "here than 98 the 99th percentile line right like it's this little purple line that's weaving its way jump way up right",
    "start": "1774290",
    "end": "1779450"
  },
  {
    "text": "and so I may have gotten page an hour ago and I'm staring at this right and the number one thing that I see when I",
    "start": "1779450",
    "end": "1785270"
  },
  {
    "text": "look at that graph is well that's a lot of queries at X thousand milliseconds",
    "start": "1785270",
    "end": "1790730"
  },
  {
    "text": "right here that's a lot of queries and that even was happening before I got paged and they're just started being",
    "start": "1790730",
    "end": "1797059"
  },
  {
    "text": "more and more of them as time went on that's an example of your percentiles",
    "start": "1797059",
    "end": "1803000"
  },
  {
    "text": "lying to you right you're percentiles don't tell you that you really need to have distribution bucket heat maps to be",
    "start": "1803000",
    "end": "1809780"
  },
  {
    "text": "able to see stuff like this that gives you useful hypotheses about what's going on so I say to myself ok I want to find",
    "start": "1809780",
    "end": "1817429"
  },
  {
    "text": "out what's going on with those X thousand millisecond traces so let's go and look at one sample trace so if I",
    "start": "1817429",
    "end": "1823820"
  },
  {
    "text": "click on the little T symbol I have highlighted it'll give me a link to go directly to the dapper or Zipkin trace",
    "start": "1823820",
    "end": "1830090"
  },
  {
    "text": "and then I can see and expand out the trace to find out what were all of the",
    "start": "1830090",
    "end": "1836630"
  },
  {
    "text": "calls made throughout all of the various microservices in the stack and what this",
    "start": "1836630",
    "end": "1841760"
  },
  {
    "text": "happens to tell me is that the Micra service dedicated to distributed file system operations called Colossus that",
    "start": "1841760",
    "end": "1850150"
  },
  {
    "text": "prot that process of finalising files is repeatedly taking X a thousand",
    "start": "1850150",
    "end": "1856220"
  },
  {
    "text": "milliseconds that sounds an awful lot like some kind of timeout right that gives me a very useful starting point",
    "start": "1856220",
    "end": "1862940"
  },
  {
    "text": "for saying okay now I know exactly what dashboard I need to look at I need to look at the distributed file system",
    "start": "1862940",
    "end": "1868910"
  },
  {
    "text": "performance dashboards and at the at the file closing and finalization flush metrics right and",
    "start": "1868910",
    "end": "1876500"
  },
  {
    "text": "then I could potentially even overlay them on the same graph or do some kind of other join according to what Adam was",
    "start": "1876500",
    "end": "1882320"
  },
  {
    "text": "demonstrating right before me so how do we do this the way that we do this is",
    "start": "1882320",
    "end": "1887980"
  },
  {
    "start": "1884000",
    "end": "2000000"
  },
  {
    "text": "that we collect the data in high fidelity and we decide what to throw out later so every single job of running at",
    "start": "1887980",
    "end": "1896030"
  },
  {
    "text": "Google has the monitoring library built in if you were to use a service mesh right then you would get all of that",
    "start": "1896030",
    "end": "1903140"
  },
  {
    "text": "stuff built into your service mesh and it exports the streams into a collection service and then when we aggregate the",
    "start": "1903140",
    "end": "1910430"
  },
  {
    "text": "data when you actually try to query the data that's when we can say you know what if there is more than one example",
    "start": "1910430",
    "end": "1916250"
  },
  {
    "text": "for that bucket for instance if I have two traces I only need to keep one of them or if I have two different to",
    "start": "1916250",
    "end": "1922310"
  },
  {
    "text": "different tags two different like individual replicated level data I only need to remember one of them and that",
    "start": "1922310",
    "end": "1928610"
  },
  {
    "text": "lets me go ahead and make sure that I have this fine-grained data stored alongside my aggregated distribution so",
    "start": "1928610",
    "end": "1936440"
  },
  {
    "text": "as example right like inside of my client I might say if I'm inside of a trace context and I'm supposed to be",
    "start": "1936440",
    "end": "1943250"
  },
  {
    "text": "sampling exemplars then hey by the way as I'm storing data inside of the distribution go ahead and record that",
    "start": "1943250",
    "end": "1950060"
  },
  {
    "text": "trace ID alongside it or hypothetically",
    "start": "1950060",
    "end": "1955640"
  },
  {
    "text": "actually no this is a real example on YouTube uses this if you can imagine",
    "start": "1955640",
    "end": "1961070"
  },
  {
    "text": "every single video ID encoded by YouTube right like that thing on the end of the YouTube string you know iid equals foo",
    "start": "1961070",
    "end": "1967880"
  },
  {
    "text": "right we can't possibly store a data stream for every single video that we",
    "start": "1967880",
    "end": "1973010"
  },
  {
    "text": "encode or transcode but what we do instead is that we record a bit of",
    "start": "1973010",
    "end": "1978020"
  },
  {
    "text": "custom data associated with the latency of every single video recording and",
    "start": "1978020",
    "end": "1983780"
  },
  {
    "text": "encoding request and that way when we're looking at the distribution to see what videos were slow to encode I'll be able",
    "start": "1983780",
    "end": "1991250"
  },
  {
    "text": "to see a sample video ID for every individual latency bucket and for in",
    "start": "1991250",
    "end": "1997370"
  },
  {
    "text": "every individual slice of time which is pretty cool so when it gets time to deduplicate right when i'm figuring out",
    "start": "1997370",
    "end": "2003370"
  },
  {
    "start": "2000000",
    "end": "2025000"
  },
  {
    "text": "what to throw away or concept is that we do a weighted average right that if there are more samples associated with a given bucket",
    "start": "2003370",
    "end": "2009670"
  },
  {
    "text": "from one source and fewer from another then I'm going to weight towards retaining the higher level the more",
    "start": "2009670",
    "end": "2014710"
  },
  {
    "text": "granular data from that source that's present more often so I do the weighted average and then I store only one of",
    "start": "2014710",
    "end": "2020350"
  },
  {
    "text": "them and I throw the rest of it away so with the example traces or with the",
    "start": "2020350",
    "end": "2029110"
  },
  {
    "text": "example tags associated with a problematic thing or anything I want to investigate in more detail that's",
    "start": "2029110",
    "end": "2036040"
  },
  {
    "text": "displayed alongside my graphs that gives me much easier tools to form hypotheses",
    "start": "2036040",
    "end": "2041110"
  },
  {
    "text": "about what's going on in my system like maybe I want to go directly to task number four and go look at the logs at a",
    "start": "2041110",
    "end": "2047380"
  },
  {
    "text": "specific time stamp because I know that I will find a slow query there so I still need to be able to do the dynamic",
    "start": "2047380",
    "end": "2053830"
  },
  {
    "text": "query building and joining an evaluation but it lets me take a lot of the time out of bisecting because I'm able to",
    "start": "2053830",
    "end": "2060580"
  },
  {
    "text": "jump faster to these solutions by keeping a small amount of the sample data around so like I was mentioning",
    "start": "2060580",
    "end": "2068169"
  },
  {
    "text": "earlier this is present in light step this is present I believe in cirque onus and this is also present in honeycomb so",
    "start": "2068169",
    "end": "2075429"
  },
  {
    "text": "there are many providers that already provide this and this is coming soon to stack driver as well so to wrap up we",
    "start": "2075429",
    "end": "2084070"
  },
  {
    "start": "2081000",
    "end": "2107000"
  },
  {
    "text": "improved resiliency by reducing the amount of time that it took us to find problems in our system that we",
    "start": "2084070",
    "end": "2090158"
  },
  {
    "text": "formulated hypotheses faster and tested them faster and we did this using the three techniques layer peeling the",
    "start": "2090159",
    "end": "2097148"
  },
  {
    "text": "bisect the problem dynamic joint data joins in order to find correlations between our data and exemplars to be",
    "start": "2097149",
    "end": "2103990"
  },
  {
    "text": "able to more quickly jump to examples of things that are failing of course after any incident it's always",
    "start": "2103990",
    "end": "2110800"
  },
  {
    "text": "important to be able to record what did you do how did you arrive at the results that you came to so for that reason we",
    "start": "2110800",
    "end": "2117880"
  },
  {
    "text": "typically add critical queries that we use into our playbook however I discourage you from adding them to your",
    "start": "2117880",
    "end": "2124150"
  },
  {
    "text": "dashboards as a knee-jerk reaction why is that well it turns out that storing",
    "start": "2124150",
    "end": "2131380"
  },
  {
    "text": "extra dashboards and precomputing extra queries has a compute and storage cost but that's not even why I'm concerned",
    "start": "2131380",
    "end": "2137480"
  },
  {
    "text": "what I'm concerned about is the cognitive overload of having too many dashboards this is a problem I see way",
    "start": "2137480",
    "end": "2144410"
  },
  {
    "text": "too many teams jump into they define a dashboard for every single thing and what you wind up doing when you're on",
    "start": "2144410",
    "end": "2151010"
  },
  {
    "text": "calls you look at the list of 50 different dashboards each with 50 different individual queries on them and",
    "start": "2151010",
    "end": "2156380"
  },
  {
    "text": "you can't make heads or tails of it and you spend 20 or 30 minutes just picking random graphs to stare out did that",
    "start": "2156380",
    "end": "2163100"
  },
  {
    "text": "graph wiggle at the same time as that other breath I'm not sure right like there's a huge cognitive cost to adding",
    "start": "2163100",
    "end": "2168800"
  },
  {
    "text": "dashboards much of the time those dashboards are going to be leading me in a wrong direction as well it's just",
    "start": "2168800",
    "end": "2174200"
  },
  {
    "text": "because a dashboard is wiggling if your SLO isn't in danger then there's really no reason to be looking at it yes so no",
    "start": "2174200",
    "end": "2180320"
  },
  {
    "text": "two outages are identical so it's important to figure out what are the common patterns that you can use to generically debug any outage and what",
    "start": "2180320",
    "end": "2187580"
  },
  {
    "text": "are the things that are specific to that one outage so we reduce time to resolve",
    "start": "2187580",
    "end": "2194119"
  },
  {
    "start": "2191000",
    "end": "2247000"
  },
  {
    "text": "and spend less error budget which means that we can spend that error budget instead on pushing our code out faster",
    "start": "2194119",
    "end": "2201380"
  },
  {
    "text": "and the way that we do that is to be able to do a hel ad hoc queries to be able to write our own monitoring queries",
    "start": "2201380",
    "end": "2207590"
  },
  {
    "text": "to look at the data to play with it to drill down you really don't want to build a hundred different dashboards and",
    "start": "2207590",
    "end": "2213910"
  },
  {
    "text": "instead if you are able to quickly and confidently root cause things you're",
    "start": "2213910",
    "end": "2219590"
  },
  {
    "text": "going to have a much better time running distributed systems and operating in a microcircuit environment so I urge folks",
    "start": "2219590",
    "end": "2225530"
  },
  {
    "text": "to take a look at open census which is an interoperability layer between many different monitoring and observability",
    "start": "2225530",
    "end": "2230600"
  },
  {
    "text": "tools and you can read more from my team at cloud platform that Google Blog com",
    "start": "2230600",
    "end": "2235880"
  },
  {
    "text": "and in particular I really encourage people to look at Yanis blog post about observability which talks in a lot more",
    "start": "2235880",
    "end": "2241700"
  },
  {
    "text": "written detail about the concepts that we describe today so that is our talk",
    "start": "2241700",
    "end": "2246859"
  },
  {
    "text": "and you can also read the site reliability engineering book today and to you books are coming out this summer",
    "start": "2246859",
    "end": "2253430"
  },
  {
    "text": "both in early access on O'Reilly the site reliability workbook and seeking essary thank you so much and I think we have",
    "start": "2253430",
    "end": "2259790"
  },
  {
    "text": "time for about five or ten minutes of questions thank you you",
    "start": "2259790",
    "end": "2265990"
  }
]