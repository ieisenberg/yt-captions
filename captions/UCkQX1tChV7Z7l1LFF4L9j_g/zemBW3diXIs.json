[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "all right thank you everyone thank you for everyone coming out uh this afternoon so as Thomas said GitHub",
    "start": "12160",
    "end": "18640"
  },
  {
    "text": "Copilot is the largest LLM powered code completion service in the world we serve",
    "start": "18640",
    "end": "24800"
  },
  {
    "text": "hundreds of millions of requests a day um with an average response time of under 200 milliseconds and so the story",
    "start": "24800",
    "end": "30800"
  },
  {
    "text": "I'm going to cover today in this track is the story of how we built this service just quickly about me uh I'm the",
    "start": "30800",
    "end": "38239"
  },
  {
    "text": "cat on the internet with glasses um in real life I'm just some guy that wears glasses i've been at GitHub for nearly 5",
    "start": "38239",
    "end": "45520"
  },
  {
    "text": "years i've worked on a bunch of back-end components which none of you know about but you interact with every day",
    "start": "45520",
    "end": "51760"
  },
  {
    "text": "i'm currently the tech lead on Copilot Proxy which is the service that connects your IDs to LLMs hosted in",
    "start": "51760",
    "end": "59239"
  },
  {
    "text": "Azure um I've got a pretty full pretty full schedule here so we'll probably take questions at the end so let's get",
    "start": "59239",
    "end": "68080"
  },
  {
    "text": "started given this is San Francisco it almost seems redundant to ask this question but for possibly the one person",
    "start": "68439",
    "end": "73920"
  },
  {
    "text": "who hasn't heard of GitHub in this entire room we are the largest social coding site in the world we have over",
    "start": "73920",
    "end": "80720"
  },
  {
    "text": "100 million users uh and we we're very proud to call ourselves the home of all",
    "start": "80720",
    "end": "86680"
  },
  {
    "text": "developers now the product I'm going to talk to you about today is GitHub C-Pilot specifically the code completion",
    "start": "86680",
    "end": "92240"
  },
  {
    "text": "part of the product that's the bit that I work on copilot does uh many other things uh chat interactive refactoring",
    "start": "92240",
    "end": "100799"
  },
  {
    "text": "uh things like that and so on and they broadly use the same uh architecture and infrastructure as I'm going to talk",
    "start": "100799",
    "end": "106000"
  },
  {
    "text": "about today but the details vary subtly if you're really interested come find me",
    "start": "106000",
    "end": "112240"
  },
  {
    "text": "afterwards now co GitHub Copilot is available as an extension you install it in your IDE we support most of the major",
    "start": "112439",
    "end": "119520"
  },
  {
    "text": "IDEs VS Code Visual Studio obviously the Pantheon of IntelliJes",
    "start": "119520",
    "end": "125680"
  },
  {
    "text": "uh NeoVim and we recently announced support for Xcode so pretty much you can get it wherever you want we serve more",
    "start": "125680",
    "end": "132239"
  },
  {
    "text": "than 400 million um completion requests that was when I pitched this talk um I had a look at the number it's much",
    "start": "132239",
    "end": "137360"
  },
  {
    "text": "higher than that these days uh and we peak at about 8,000 requests a second during that kind of peak period between",
    "start": "137360",
    "end": "144160"
  },
  {
    "text": "the European afternoon and the US workday that's our peak period and during that time we see a mean response",
    "start": "144160",
    "end": "149680"
  },
  {
    "text": "time of less than 200 milliseconds just in case there is again",
    "start": "149680",
    "end": "156800"
  },
  {
    "text": "one person who hasn't seen GitHub Copilot in action here's a recording of me in my hotel room just working on some",
    "start": "156800",
    "end": "162319"
  },
  {
    "text": "throwaway code the goal what you'll see here is that we have uh the inbuilt IDE",
    "start": "162319",
    "end": "169040"
  },
  {
    "text": "completions those are the ones in the box and co-pilots completions which are the gray ones which we um we notionally",
    "start": "169040",
    "end": "176560"
  },
  {
    "text": "call ghost text because it's gray and ethereal um and you can see as I go through here",
    "start": "176560",
    "end": "183920"
  },
  {
    "text": "it's just every time that I stop typing or I kind of pause takes",
    "start": "183920",
    "end": "189959"
  },
  {
    "text": "over you can see you can write a function uh kind of a comment describing",
    "start": "189959",
    "end": "195040"
  },
  {
    "text": "what you want and copilot will do its best to write for write it for you it really likes patterns so as you can see",
    "start": "195040",
    "end": "200879"
  },
  {
    "text": "it's it's telling it has kind of figured out the pattern of what I'm",
    "start": "200879",
    "end": "206480"
  },
  {
    "text": "doing okay we all know how to make prime numbers you pretty much got the idea",
    "start": "214760",
    "end": "222239"
  },
  {
    "text": "so that's the product in action let's talk about the requirements of how we built the service that powers this on",
    "start": "222239",
    "end": "227840"
  },
  {
    "text": "the back end because the goal is interactive code completion in the IDE",
    "start": "227840",
    "end": "233680"
  },
  {
    "text": "um and in this respect we're competing with all of the other interactive uh",
    "start": "233680",
    "end": "238959"
  },
  {
    "text": "autocomplete built into most IDEs that's your anything LSP powered um your code",
    "start": "238959",
    "end": "244959"
  },
  {
    "text": "sense your IntelliSense all of that stuff and this is a pretty tall order because uh those things running locally",
    "start": "244959",
    "end": "251519"
  },
  {
    "text": "on your machine don't have to like they don't have to deal with network latency they don't have to deal with like shared",
    "start": "251519",
    "end": "259600"
  },
  {
    "text": "server resources they don't have to deal with the inevitable cloud outage so we've got a pretty high bar that's been",
    "start": "259600",
    "end": "264880"
  },
  {
    "text": "set for us so to be competitive we need to do a bunch of things the first one is that we",
    "start": "264880",
    "end": "272160"
  },
  {
    "text": "need to minimize latency between uh for and between requests",
    "start": "272160",
    "end": "277680"
  },
  {
    "text": "we need to amortitize any kind of uh setup costs that we might make because this is a network",
    "start": "277680",
    "end": "283000"
  },
  {
    "text": "service um and to a to a point we need to avoid as much network latency as we",
    "start": "283000",
    "end": "289040"
  },
  {
    "text": "can because that's overhead that our competitors that sit locally in IDs don't have to pay and the last one is that the the",
    "start": "289040",
    "end": "298240"
  },
  {
    "text": "length and thus time to generate a co a code coco completion response is very much a function of the size of the",
    "start": "298240",
    "end": "304240"
  },
  {
    "text": "request which is completely variable and so one of the other things that we do is rather than waiting for the whole",
    "start": "304240",
    "end": "309759"
  },
  {
    "text": "request to be uh completed and then send it back to the user we work in a streaming mode and so it doesn't really",
    "start": "309759",
    "end": "315039"
  },
  {
    "text": "matter how long the request is we're immediately start streaming it as soon as it starts this is quite a useful property",
    "start": "315039",
    "end": "321039"
  },
  {
    "text": "because it unlocks other optimizations i want to dig into this uh connection",
    "start": "321039",
    "end": "328320"
  },
  {
    "text": "setup is expensive idea because this is a network service use TCP tcp uses the",
    "start": "328320",
    "end": "334479"
  },
  {
    "text": "so-called three-way handshake sin sac act on top of that because this is the",
    "start": "334479",
    "end": "340800"
  },
  {
    "text": "internet and it's 2024 everything needs to be secured by TLS and TLS takes between five to seven additional legs to",
    "start": "340800",
    "end": "348800"
  },
  {
    "text": "do that handshaking negotiate keys in both in both directions now some of these steps can be pipelined",
    "start": "348800",
    "end": "355440"
  },
  {
    "text": "a lot of work has been gone into uh reducing uh reducing these setup costs",
    "start": "355440",
    "end": "360479"
  },
  {
    "text": "and kind of overlaying the TLS handshake with the TCP handshake and these are these are great optimizations but",
    "start": "360479",
    "end": "366240"
  },
  {
    "text": "they're not a panacea there's no way to drive this network cost down to zero",
    "start": "366240",
    "end": "372400"
  },
  {
    "text": "and so because of that the latency uh because of that you end up with about five or six round",
    "start": "372400",
    "end": "379039"
  },
  {
    "text": "trips between you and the server and back again to make a a new connection to a",
    "start": "379039",
    "end": "384680"
  },
  {
    "text": "service and the duration of each of those legs is highly correlated with",
    "start": "384680",
    "end": "390240"
  },
  {
    "text": "distance this graph that I shame shamelessly stole from the internet says about 50 milliseconds a leg which is",
    "start": "390240",
    "end": "396560"
  },
  {
    "text": "probably you know east coast to west coast kind of time where I live on the other side of an ocean we see uh",
    "start": "396560",
    "end": "403440"
  },
  {
    "text": "roundtrip times far in excess of 100 milliseconds and when you add all that up and doing five or six of them that",
    "start": "403440",
    "end": "410000"
  },
  {
    "text": "makes connection setup really expensive so you want to do it once and you want to keep it open for as long as",
    "start": "410000",
    "end": "417120"
  },
  {
    "text": "possible so those are the kind of high level requirements let's take a little bit of a a trip back in time and look at",
    "start": "418440",
    "end": "425919"
  },
  {
    "text": "the history of Copilot as evolved because when we started out we had an extension in VS Code and to use it the",
    "start": "425919",
    "end": "434240"
  },
  {
    "text": "offer users would go to OpenAI get an account get their key added to a special group then go and plug that key into the",
    "start": "434240",
    "end": "442879"
  },
  {
    "text": "IDE this worked i mean it was it was great for an alpha product and it scaled to literally dozens of",
    "start": "443319",
    "end": "449000"
  },
  {
    "text": "users well and at that point everyone got tired of being in the business of user management i mean OpenAI don't want",
    "start": "449000",
    "end": "456080"
  },
  {
    "text": "to be in the business of knowing who our users are frankly we don't want that either what we want is a service",
    "start": "456080",
    "end": "462000"
  },
  {
    "text": "provider relationship and it's that kind of thing is what you're used to when you can consumer consumer cloud service you",
    "start": "462000",
    "end": "468720"
  },
  {
    "text": "get a key to access the server resources anytime someone uses that key a bill is emitted the who is allowed to do that",
    "start": "468720",
    "end": "475759"
  },
  {
    "text": "and under what circumstances entirely your job as the product team",
    "start": "475759",
    "end": "481360"
  },
  {
    "text": "so we're left with this problem of how do we manage this service key um well",
    "start": "481360",
    "end": "486639"
  },
  {
    "text": "let's talk about the wrong way to do it so the wrong way to do it would be to encode the key somehow like in the",
    "start": "486639",
    "end": "492639"
  },
  {
    "text": "extension that we give to users in a way that it can be extracted and used by the service but is invisible to like casual",
    "start": "492639",
    "end": "500080"
  },
  {
    "text": "malicious onlookers um this is this is impossible um this is at best security",
    "start": "500080",
    "end": "505680"
  },
  {
    "text": "by obscurity it doesn't work just ask the rabbit R1 folks",
    "start": "505680",
    "end": "511599"
  },
  {
    "text": "so the solution that we arrived at is to build an authenticating proxy which sits in the middle of this network",
    "start": "511599",
    "end": "518680"
  },
  {
    "text": "transaction um the name of the product is copilot proxy because it's just an internal name i'm just going to call it",
    "start": "518680",
    "end": "524800"
  },
  {
    "text": "proxy for the rest of this talk and it was added shortly after our alpha release to move us out of this period of",
    "start": "524800",
    "end": "530399"
  },
  {
    "text": "having user provided keys uh to a more scalable authentication mechanism so what does this workflow",
    "start": "530399",
    "end": "537279"
  },
  {
    "text": "look like now well you install the extension in your ID just as normal and you authenticate to GitHub uh just as",
    "start": "537279",
    "end": "543600"
  },
  {
    "text": "normal and that creates a kind of oorthth relationship like each with there's an oorthth key which identifies",
    "start": "543600",
    "end": "549200"
  },
  {
    "text": "that installation on that particular machine for that person who was logged in at that",
    "start": "549200",
    "end": "554600"
  },
  {
    "text": "time the ID can use that uh now can use that or credential to go to GitHub and",
    "start": "554600",
    "end": "560640"
  },
  {
    "text": "exchange that for what we call a shortlived code completion token the token is just is just it ju just",
    "start": "560640",
    "end": "568800"
  },
  {
    "text": "like a like a train ticket it's just an authorization to use it for a short period of time it's signed and so when",
    "start": "568800",
    "end": "574800"
  },
  {
    "text": "the request lands on the proxy all we have to do is just check that that signature is valid if it's good we swap",
    "start": "574800",
    "end": "580160"
  },
  {
    "text": "the request we swap the key out for the actual API service key forward it on and stream back the results we don't need to",
    "start": "580160",
    "end": "586720"
  },
  {
    "text": "do any other further validation and this is important because it means we don't have to like for every request we get we",
    "start": "586720",
    "end": "592959"
  },
  {
    "text": "don't have to call out to an external authentication service the shortlived token is the",
    "start": "592959",
    "end": "598680"
  },
  {
    "text": "authentication and from the client's point of view nothing's really changed in their world they still think they're talking to a model and they still get",
    "start": "598680",
    "end": "604959"
  },
  {
    "text": "the response as as usual this token lives it's got a",
    "start": "604959",
    "end": "610560"
  },
  {
    "text": "lifetime in the order of minutes you know 10 20 30 minutes and this is mainly",
    "start": "610560",
    "end": "615920"
  },
  {
    "text": "to limit the liability if say it was stolen which is highly unlikely um the",
    "start": "615920",
    "end": "621920"
  },
  {
    "text": "much more uh much more likely and sad case is that in the cases of abuse we need the ability to uh to shut down an",
    "start": "621920",
    "end": "628480"
  },
  {
    "text": "account and therefore not generate new tokens and that's generally why the token has a short lifetime now in the background the",
    "start": "628480",
    "end": "635920"
  },
  {
    "text": "client knows the expiration time of the token it was given and a couple of minutes before that it kicks off a re",
    "start": "635920",
    "end": "641600"
  },
  {
    "text": "refresh cycle gets a new token swaps it out the world",
    "start": "641600",
    "end": "646760"
  },
  {
    "text": "continues so cool we solved the access problem that's half the",
    "start": "646760",
    "end": "652279"
  },
  {
    "text": "problem now to talk about another part of the another part of the problem as a product design we don't have an",
    "start": "652279",
    "end": "658640"
  },
  {
    "text": "autocomplete key like you don't I remember in Eclipse you would use you know command command space things like",
    "start": "658640",
    "end": "664240"
  },
  {
    "text": "that to trigger to trigger the autocomplete or to trigger the refactoring tools I didn't use that uh",
    "start": "664240",
    "end": "669519"
  },
  {
    "text": "in the example I showed whenever I stop typing copilot takes",
    "start": "669519",
    "end": "675040"
  },
  {
    "text": "over and so that creates the question of when should it take over when should we",
    "start": "676040",
    "end": "681440"
  },
  {
    "text": "when should we switch from user typing to copilot typing and this is um it it is it's not",
    "start": "681440",
    "end": "688959"
  },
  {
    "text": "a straight for problem i mean some of the solutions we could use is just a fixed timer we hook the key presses and",
    "start": "688959",
    "end": "696240"
  },
  {
    "text": "after each key press we start a timer and if that timer elapses without another key press then we say right",
    "start": "696240",
    "end": "702079"
  },
  {
    "text": "we're ready to issue the request and move into completion mode this is good because it provides an upper bound on",
    "start": "702079",
    "end": "708480"
  },
  {
    "text": "how long we wait and that waiting is additional latency um but it's bad",
    "start": "708480",
    "end": "714240"
  },
  {
    "text": "because it provides a lower bound we always wait at least this long before starting even if that was the last key",
    "start": "714240",
    "end": "719920"
  },
  {
    "text": "press the user was going to make we could try something a bit more sciency and use a tiny prediction model",
    "start": "719920",
    "end": "726160"
  },
  {
    "text": "to look at the stream of characters as they're typed and kind of predict are they approaching the end of the word or",
    "start": "726160",
    "end": "731760"
  },
  {
    "text": "are they in the middle of a word and like nudge that timer forward and back we can just do things like a blind guess",
    "start": "731760",
    "end": "738320"
  },
  {
    "text": "anytime there's a key press we can just assume that's it no more input from the user and always issue the completion",
    "start": "738320",
    "end": "743600"
  },
  {
    "text": "request now in reality we use a we use a mixture of all all these",
    "start": "743600",
    "end": "751120"
  },
  {
    "text": "strategies and that leads us to the next problem which is despite all this work and tuning that went into this around",
    "start": "752920",
    "end": "760320"
  },
  {
    "text": "half of the requests that we issued are what we call type through don't forget we're doing autocomplete and so if you",
    "start": "760320",
    "end": "766560"
  },
  {
    "text": "continue to type after we've made a request you've now diverged from the data we had and our request is now out",
    "start": "766560",
    "end": "772880"
  },
  {
    "text": "of date we can't use that result we could try a few things to work",
    "start": "772880",
    "end": "779600"
  },
  {
    "text": "around this like we could wait longer before a request um that might reduce",
    "start": "779600",
    "end": "784800"
  },
  {
    "text": "the number of times that we issue a request and then have to have to not use the result but that additional latency",
    "start": "784800",
    "end": "791920"
  },
  {
    "text": "that additional waiting penalizes every user who had stopped and was waiting and",
    "start": "791920",
    "end": "797360"
  },
  {
    "text": "of course we wait too long then users might think that Copilot is broken because it's not saying anything to them",
    "start": "797360",
    "end": "802480"
  },
  {
    "text": "anymore so instead what we've built is a system",
    "start": "802480",
    "end": "808399"
  },
  {
    "text": "that allows us to cancel requests once they've been issued now this cancellation of requests",
    "start": "808399",
    "end": "815279"
  },
  {
    "text": "using HTTP is it's it's potentially novel i don't claim it to be unique but",
    "start": "815279",
    "end": "821120"
  },
  {
    "text": "it's certainly the first time I've come across this um in my career so I want to spend a little a little bit of time a",
    "start": "821120",
    "end": "827120"
  },
  {
    "text": "few slides digging into like what it means to cancel a HTTP request",
    "start": "827120",
    "end": "834079"
  },
  {
    "text": "so if I if I said to you how would you like you're you're at your web browser and you decide you don't want to wait",
    "start": "835839",
    "end": "840880"
  },
  {
    "text": "for that page how do you say I want to stop i want to cancel that well you",
    "start": "840880",
    "end": "846160"
  },
  {
    "text": "press the stop button could close the browser tab like drop off the network",
    "start": "846160",
    "end": "853040"
  },
  {
    "text": "get your laptop away these are all kind of very final",
    "start": "853040",
    "end": "859839"
  },
  {
    "text": "actions they imply that you're canceling the quest the request because you're done either you're done with using of",
    "start": "859839",
    "end": "866399"
  },
  {
    "text": "the site or you're just frustrated and you've given up it's an act of finality you don't intend to make another",
    "start": "866399",
    "end": "873399"
  },
  {
    "text": "request and under the under the hood they all have the same kind of networking behavior you cancel the you",
    "start": "873399",
    "end": "880320"
  },
  {
    "text": "reset the TCP stream the connection that we talked about setting up on previous slides",
    "start": "880320",
    "end": "887720"
  },
  {
    "text": "now that's on the that's on the browser side if we look on the server side",
    "start": "889120",
    "end": "894920"
  },
  {
    "text": "either at the application layer or in your web framework this idea of",
    "start": "894920",
    "end": "901360"
  },
  {
    "text": "cancellation is not something that is very common inside web frameworks so if a user using your application your",
    "start": "901360",
    "end": "909279"
  },
  {
    "text": "site like presses stop on the browser or if they control C their curl",
    "start": "909279",
    "end": "914880"
  },
  {
    "text": "command that that underlying thing translates into a TCP reset of the",
    "start": "915240",
    "end": "920760"
  },
  {
    "text": "connection but on the other end in your ser in your your server code when do you",
    "start": "920760",
    "end": "926240"
  },
  {
    "text": "get to see that signal when you get to see that they've done that now the generally times that you",
    "start": "926240",
    "end": "932480"
  },
  {
    "text": "can spot that the TCP connection has been reset is either when you're reading the request body so early in the request",
    "start": "932480",
    "end": "938639"
  },
  {
    "text": "when you're reading the headers and the body or later on when you're going to start writing your response back there",
    "start": "938639",
    "end": "946000"
  },
  {
    "text": "and this is a really big problem for LLMs because the cost of the request",
    "start": "946000",
    "end": "951199"
  },
  {
    "text": "that initial inference before you generate the first token is the majority of the cost and that happens before you",
    "start": "951199",
    "end": "957519"
  },
  {
    "text": "produce any output so all that work is",
    "start": "957519",
    "end": "963000"
  },
  {
    "text": "performed we've done we've done the inference we're ready to start streaming back tokens and only then do we find that the user closed the socket and",
    "start": "963000",
    "end": "969360"
  },
  {
    "text": "there's they've gone as you saw in our case around",
    "start": "969360",
    "end": "974800"
  },
  {
    "text": "that's about 40 40% 45% of the requests so half of the time we'd be performing",
    "start": "974800",
    "end": "981279"
  },
  {
    "text": "that inference and then throwing the the result on the floor which is an enormous waste of money time and energy which in",
    "start": "981279",
    "end": "989279"
  },
  {
    "text": "AI terms is all the same",
    "start": "989279",
    "end": "992560"
  },
  {
    "text": "thing now if this situation wasn't bad enough it it gets worse because",
    "start": "994440",
    "end": "999600"
  },
  {
    "text": "cancellation in HB World is the result of closing that connection but in our",
    "start": "999600",
    "end": "1004720"
  },
  {
    "text": "case in the usage of of of networking TCP to talk to uh the proxy the reason",
    "start": "1004720",
    "end": "1013600"
  },
  {
    "text": "we cancelled that request is because we want to make another one straight away but to make that make that request",
    "start": "1013600",
    "end": "1020079"
  },
  {
    "text": "straight away we don't have a connection anymore we have to pay those five or six round trips to set up a new TCP TLS",
    "start": "1020079",
    "end": "1026798"
  },
  {
    "text": "connection so in this naive idea in this normal usage cancellation occurs every other",
    "start": "1026799",
    "end": "1034480"
  },
  {
    "text": "request on average and this would mean that users are constantly closing and",
    "start": "1034480",
    "end": "1039839"
  },
  {
    "text": "reestablishing their TCP connections in this kind of signaling they want to cancel and then reestablishing",
    "start": "1039839",
    "end": "1045038"
  },
  {
    "text": "connection to make a new request and the latency of that far exceeds the cost of",
    "start": "1045039",
    "end": "1050080"
  },
  {
    "text": "just letting the the request that we didn't need run to completion and then just ignoring it",
    "start": "1050080",
    "end": "1057720"
  },
  {
    "text": "now most of what I said on the previous slides applies to HP1 which has this one request per connection model as you're",
    "start": "1060240",
    "end": "1067840"
  },
  {
    "text": "reading on this slide HTTP version numbers go above number one they go up to two and three so I'm going to spend a",
    "start": "1067840",
    "end": "1073760"
  },
  {
    "text": "little bit of time talking about HTTP2 and how that was very important to our",
    "start": "1073760",
    "end": "1079160"
  },
  {
    "text": "solution as a side note um Copilot proxy is written in Go because it has a quite",
    "start": "1079160",
    "end": "1084640"
  },
  {
    "text": "robust HTTP library it gave us the HTB2 support and control over that that we needed for this part of the product um",
    "start": "1084640",
    "end": "1092000"
  },
  {
    "text": "and that's why I'm here today rather than a rust station talking to you but this is mostly an implementation",
    "start": "1092000",
    "end": "1098559"
  },
  {
    "text": "detail for the general room htb2 is more like SSH than good old HP1 plus",
    "start": "1100440",
    "end": "1108760"
  },
  {
    "text": "TLS like SSH HTTP2 is a tunnneled tunnneled connection you have a single a",
    "start": "1108760",
    "end": "1115760"
  },
  {
    "text": "single connection and multiple requests multipplexed on top of that uh in both",
    "start": "1115760",
    "end": "1120880"
  },
  {
    "text": "SSH and HP2 they're called streams so a single network connection can carry",
    "start": "1120880",
    "end": "1127360"
  },
  {
    "text": "multiple streams and where each stream is a request so we use HP2 between the client",
    "start": "1127360",
    "end": "1133360"
  },
  {
    "text": "and the proxy because that allows us to create a connection once and reuse it over and",
    "start": "1133360",
    "end": "1140000"
  },
  {
    "text": "over again instead of canceling the TCP connection itself of resetting the TCP connection itself you just reset the",
    "start": "1140000",
    "end": "1146640"
  },
  {
    "text": "individual stream representing the request you made the underlying connection stays open we do the same between the proxy",
    "start": "1146640",
    "end": "1153840"
  },
  {
    "text": "and our LLM model um because uh the proxy is effectually effectively",
    "start": "1153840",
    "end": "1159360"
  },
  {
    "text": "concatenating requests like fanning them in from thousands of clients down onto a small set of connections to the LL model",
    "start": "1159360",
    "end": "1167440"
  },
  {
    "text": "we use we use a a a bunch like a connection pool a bunch of uh",
    "start": "1167440",
    "end": "1172960"
  },
  {
    "text": "connections to talk to the model and this is just to spread the load across multiple uh multiple TCP streams avoid",
    "start": "1172960",
    "end": "1179520"
  },
  {
    "text": "networking issues avoid head of line blocking things like that now we found just like the client",
    "start": "1179520",
    "end": "1185240"
  },
  {
    "text": "behavior these connections between the proxy and our LLM model uh are",
    "start": "1185240",
    "end": "1190480"
  },
  {
    "text": "established when we we start the process and they live assuming there's no uh upstream problems they remain open for",
    "start": "1190480",
    "end": "1197280"
  },
  {
    "text": "the lifetime of the process until we redeploy it so minutes to hours to days depending on when we choose to",
    "start": "1197280",
    "end": "1203960"
  },
  {
    "text": "redeploy and by keeping these long live HTB2 connections open we get additional benefits to the TCP layer basically TCP",
    "start": "1203960",
    "end": "1211120"
  },
  {
    "text": "has this trust thing the longer a connection is open the more it trusts it the more allows more data to be in fly",
    "start": "1211120",
    "end": "1216640"
  },
  {
    "text": "before it has to be acknowledged so you get these nice warmed up TCP pipes that",
    "start": "1216640",
    "end": "1221919"
  },
  {
    "text": "go all the way from the client through the proxy up to the model and",
    "start": "1221919",
    "end": "1227280"
  },
  {
    "text": "back this is not intended to be a tutorial on Go but for those who do speak it socially um this is what",
    "start": "1228120",
    "end": "1234240"
  },
  {
    "text": "basically every Go HTTP handle looks like and the key here is this",
    "start": "1234240",
    "end": "1239360"
  },
  {
    "text": "request.ext context object context is uh effectively a handle it allows efficient",
    "start": "1239360",
    "end": "1246000"
  },
  {
    "text": "transmission of cancellations and timeouts and those kind of uh request",
    "start": "1246000",
    "end": "1251039"
  },
  {
    "text": "specific type meta information the important thing here is that the other end of this request",
    "start": "1251039",
    "end": "1257760"
  },
  {
    "text": "context is effectively connected out into userland to the user's IDE and when",
    "start": "1257760",
    "end": "1264320"
  },
  {
    "text": "when by continuing to type they need to cancel a request that stream",
    "start": "1264320",
    "end": "1269640"
  },
  {
    "text": "reset causes this context object to be cancelled and so that makes it immediately visible to the HTP server",
    "start": "1269640",
    "end": "1276960"
  },
  {
    "text": "without having to wait until we get to a point of actually trying to write any data to the stream and of course we this context can",
    "start": "1276960",
    "end": "1284159"
  },
  {
    "text": "be passed up and down the call stack and used for anything that wants to know should it stop early and so we use it in",
    "start": "1284159",
    "end": "1291520"
  },
  {
    "text": "the HTP client and we make that call onwards to the model we pass in that same context and so the cancellation",
    "start": "1291520",
    "end": "1297440"
  },
  {
    "text": "that now occurs that happens in the IDE propagates to the proxy and into the model effectively",
    "start": "1297440",
    "end": "1305240"
  },
  {
    "text": "immediately and this is all rather neatly expressed here but it requires that all parties speak HTB2",
    "start": "1305240",
    "end": "1313760"
  },
  {
    "text": "natively and it turns out this wasn't all beer and Skittles because",
    "start": "1314919",
    "end": "1320480"
  },
  {
    "text": "In practice this turned out to getting this end to- end HTP2 turned out to be more difficult uh than we expected this",
    "start": "1320480",
    "end": "1328159"
  },
  {
    "text": "is despite HTB2 being uh nearly a decade old just general support for this in",
    "start": "1328159",
    "end": "1333760"
  },
  {
    "text": "just general in just general life was not as good as it could be for example most load balancers are",
    "start": "1333760",
    "end": "1340880"
  },
  {
    "text": "happy to speak HP2 on the front end but downgrade to HP1 on the back end um this",
    "start": "1340880",
    "end": "1346840"
  },
  {
    "text": "includes uh most of the major you know ALB and NLBs you get in your cloud providers um it it at the time included",
    "start": "1346840",
    "end": "1355280"
  },
  {
    "text": "all the CDN providers that were available to us um and that was that fact alone was enough to spawn us doing",
    "start": "1355280",
    "end": "1363200"
  },
  {
    "text": "this project there also other weird things we ran into um at the time and I don't",
    "start": "1363200",
    "end": "1369039"
  },
  {
    "text": "believe it's been fixed yet uh OpenAI was fronted with EngineX and EngineX just has an arbitrary limit of 100",
    "start": "1369039",
    "end": "1375520"
  },
  {
    "text": "requests per connection after that they've just closed the connection um",
    "start": "1375520",
    "end": "1380640"
  },
  {
    "text": "and at at the request rates that you saw it doesn't take long to chew through 100 requests and then EngineX will drop the",
    "start": "1380640",
    "end": "1386640"
  },
  {
    "text": "connection and force you to reestablish it which that was just a buzzkill and all this is just a",
    "start": "1386640",
    "end": "1392080"
  },
  {
    "text": "long-winded way of saying that the kind of generic advice of yep just stick your app behind your cloud provider's load",
    "start": "1392080",
    "end": "1398799"
  },
  {
    "text": "balancer it'll be fine didn't work out for us out of the",
    "start": "1398799",
    "end": "1404000"
  },
  {
    "text": "box but something that did work out for us and this is a good opportunity to give it a shout is",
    "start": "1404679",
    "end": "1411799"
  },
  {
    "text": "GB gb is the G stands for GitHub's load balancer it was introduced eight years",
    "start": "1411799",
    "end": "1418240"
  },
  {
    "text": "ago it's a one of the many things that is spun out of our uh engineering",
    "start": "1418240",
    "end": "1423480"
  },
  {
    "text": "group and GB is based on haroxy it uses haroxy under the hood um and haroxy",
    "start": "1423480",
    "end": "1430240"
  },
  {
    "text": "turns out to be one of the very few uh open- source load balancers that has um",
    "start": "1430240",
    "end": "1437120"
  },
  {
    "text": "it offers just exquisite HP2 control um I've never found anything like it it it",
    "start": "1437120",
    "end": "1442799"
  },
  {
    "text": "so not only did it speak HP2 end to end but offered exquisite control over the whole connection and so what we have is",
    "start": "1442799",
    "end": "1449760"
  },
  {
    "text": "GB being the GitHub load balancer which sits in front of everything that you interact with in GitHub actually owns",
    "start": "1449760",
    "end": "1455840"
  },
  {
    "text": "the client connection so the client connects to GB and GB holds that connection open if we when we deploy",
    "start": "1455840",
    "end": "1463120"
  },
  {
    "text": "when we redeploy our proxy pods um th those connections are gracefully torn",
    "start": "1463120",
    "end": "1469679"
  },
  {
    "text": "down and then reestablished for new pods but GB keeps the connection to the the client open they never see that we've",
    "start": "1469679",
    "end": "1475679"
  },
  {
    "text": "we've done a redeploy and never disconnected during that",
    "start": "1475679",
    "end": "1480000"
  },
  {
    "text": "time now with success and growth come yet more problems we serve millions of",
    "start": "1483080",
    "end": "1489679"
  },
  {
    "text": "users around the globe we uh we have co-pilot users in all the major markets um where I live in APAC Europe Americas",
    "start": "1489679",
    "end": "1496720"
  },
  {
    "text": "EMA all over the world um there is not a time that cope that we are not busy serving requests",
    "start": "1496720",
    "end": "1504080"
  },
  {
    "text": "so that's a this presents the problem that even though all this HTB2 stuff is really",
    "start": "1504080",
    "end": "1510000"
  },
  {
    "text": "good um it still can't change the speed of light and so the roundtrip time of",
    "start": "1510000",
    "end": "1515120"
  },
  {
    "text": "just being able to send the bits of your request across an ocean or through a",
    "start": "1515120",
    "end": "1520400"
  },
  {
    "text": "long geopolitical boundary or something like that can easily exceed the actual",
    "start": "1520400",
    "end": "1526159"
  },
  {
    "text": "meanantime to to to process that request and send back the answer",
    "start": "1526159",
    "end": "1531360"
  },
  {
    "text": "so this is another problem now the good news is that Azure",
    "start": "1531360",
    "end": "1536559"
  },
  {
    "text": "through its partnership with OpenAI offers open a models in effectively every region that Azure has they've got",
    "start": "1536559",
    "end": "1544240"
  },
  {
    "text": "dozens of regions around the world this is great we can put a model like we can",
    "start": "1544240",
    "end": "1549760"
  },
  {
    "text": "put a model in Europe we can put a model in Asia we can put a model where wherever we need one wherever the users",
    "start": "1549760",
    "end": "1555320"
  },
  {
    "text": "are and now we have a few more few more problems to solve i mean we want in",
    "start": "1555320",
    "end": "1560559"
  },
  {
    "text": "terms of requirements we want users therefore to be routed to their quote unquote closest proxy",
    "start": "1560559",
    "end": "1567480"
  },
  {
    "text": "region if that region is unhealthy we want them to automatically be routed somewhere else so they continue to get",
    "start": "1567480",
    "end": "1575000"
  },
  {
    "text": "service and the flip side is also true because if we have multiple regions",
    "start": "1575000",
    "end": "1580159"
  },
  {
    "text": "around the world this uh increases our capacity and our reliability we no longer have all our eggs in one basket",
    "start": "1580159",
    "end": "1586480"
  },
  {
    "text": "in one giant model somewhere let's just say in the US by spreading them around the world there's uh we we uh we're",
    "start": "1586480",
    "end": "1593919"
  },
  {
    "text": "never going to be in a situation where you know the service is down because it's spread around the",
    "start": "1593919",
    "end": "1599880"
  },
  {
    "text": "world and to do this uh we use another product again that's spun out of GitHub's engineering team called",
    "start": "1599880",
    "end": "1608080"
  },
  {
    "text": "octadns now octadns despite its name is not actually a DNS server it's actually",
    "start": "1608520",
    "end": "1613600"
  },
  {
    "text": "just a a configuration language to describe uh DNS configur urations that you want and it supports all the good",
    "start": "1613600",
    "end": "1620000"
  },
  {
    "text": "things uh arbitrary waitings load balancing you know splitting sharing uh",
    "start": "1620000",
    "end": "1626240"
  },
  {
    "text": "health checks it allows us to uh identify uh users in terms of the continent",
    "start": "1626240",
    "end": "1632400"
  },
  {
    "text": "they're in the country um here in the United States we can even work down to the state level sometimes so it gives us",
    "start": "1632400",
    "end": "1638240"
  },
  {
    "text": "exquisite control to say you over there you should primarily be going to that instance and you over there should be",
    "start": "1638240",
    "end": "1643840"
  },
  {
    "text": "primarily going to that instance and do a lot of and do a lot of testing to say uh for you know for for a user who is",
    "start": "1643840",
    "end": "1650640"
  },
  {
    "text": "you know roughly two roughly halfway between two instances which is the best one to send them to so they have the",
    "start": "1650640",
    "end": "1656400"
  },
  {
    "text": "lowest latency now on the flip side each proxy instance",
    "start": "1656400",
    "end": "1661840"
  },
  {
    "text": "is looking at the success rate of requests that it sees and it handles and if that success rate fail um drops if it",
    "start": "1661840",
    "end": "1669520"
  },
  {
    "text": "goes below the SLO those proxy instances we use the standard healthc endpoint",
    "start": "1669520",
    "end": "1674720"
  },
  {
    "text": "pattern they set their their health healthc status to 500 the upstream DNS",
    "start": "1674720",
    "end": "1680399"
  },
  {
    "text": "providers who are who've been programmed with those health checks notice that and so they effectively if if a proxy",
    "start": "1680399",
    "end": "1686240"
  },
  {
    "text": "instance starts seeing its success rate drop they vote themselves out of DNS",
    "start": "1686240",
    "end": "1691760"
  },
  {
    "text": "they go take a little quiet time by themselves and when they're feeling better they they're they're um above the",
    "start": "1691760",
    "end": "1698799"
  },
  {
    "text": "SLO they raise the health check status and bring themselves back into DNS and so this is mostly now mostly",
    "start": "1698799",
    "end": "1705399"
  },
  {
    "text": "self-healing it turns a regional outage and we're like oh my god God all of Europe can't do completions into a just",
    "start": "1705399",
    "end": "1712799"
  },
  {
    "text": "reduction in capacity because traffic is routed to other",
    "start": "1712799",
    "end": "1718200"
  },
  {
    "text": "regions one thing I'll talk touch on briefly is one model we experimented with and then rejected uh because it",
    "start": "1718200",
    "end": "1724880"
  },
  {
    "text": "just didn't work out for us was the so-called point of presence model you might have heard it called pop um if",
    "start": "1724880",
    "end": "1730720"
  },
  {
    "text": "you're used to working with big CDNs they will have points of presence like imagine even one of these dots on this",
    "start": "1730720",
    "end": "1736000"
  },
  {
    "text": "map they have a a data center in where where they're serving from and so the idea is that users will connect and do",
    "start": "1736000",
    "end": "1741440"
  },
  {
    "text": "that expensive connection as closely as close to them as possible and speed up that bit and then those CDN providers",
    "start": "1741440",
    "end": "1748880"
  },
  {
    "text": "will c that data and if need to they can call back to the um the edge and to the origin server so in in our scenario",
    "start": "1748880",
    "end": "1757120"
  },
  {
    "text": "where I live uh where I live in uh in Asia we might put uh we might put a",
    "start": "1757120",
    "end": "1762559"
  },
  {
    "text": "point of presence in Singapore that's a good equidistant place from most of most of Asia and so a user in Japan would be",
    "start": "1762559",
    "end": "1769120"
  },
  {
    "text": "attracted to that uh to that Singapore server but there's a problem because the",
    "start": "1769120",
    "end": "1774320"
  },
  {
    "text": "model is actually still hosted back here on the west coast and so we have traffic that flows that flows westward to",
    "start": "1774320",
    "end": "1780720"
  },
  {
    "text": "Singapore only to turn around and go all the way back to uh to the west coast and",
    "start": "1780720",
    "end": "1786399"
  },
  {
    "text": "we the the kind of networking colloquialism for this is traffic tromboning",
    "start": "1786399",
    "end": "1791440"
  },
  {
    "text": "this is okay for um CDN providers because CDM providers uh their goal is to c as much of the information so they",
    "start": "1791440",
    "end": "1797919"
  },
  {
    "text": "rarely call back to the origin server so any kind of like round like round",
    "start": "1797919",
    "end": "1803039"
  },
  {
    "text": "tripping or or hairpinning of traffic isn't really a problem but for us doing code completions it's always going back",
    "start": "1803039",
    "end": "1808799"
  },
  {
    "text": "to a model so what we ended up after a lot of experimentation was the idea of",
    "start": "1808799",
    "end": "1814080"
  },
  {
    "text": "having many many regions calling back to a few models just didn't pay for itself um the latency wasn't as good and it",
    "start": "1814080",
    "end": "1821600"
  },
  {
    "text": "carried with it a very high operational burden like every every point of presence that you deploy is now a thing",
    "start": "1821600",
    "end": "1826960"
  },
  {
    "text": "you have to monitor and upgrade and deploy to and fix when it breaks so it",
    "start": "1826960",
    "end": "1832640"
  },
  {
    "text": "just didn't pay for us so we went with a much simpler model which is simply if there is a model in an Azure region we",
    "start": "1832640",
    "end": "1839200"
  },
  {
    "text": "colllocate a proxy instance in that same Azure region and we say that is the location that users traffic is sent",
    "start": "1839200",
    "end": "1847278"
  },
  {
    "text": "to now we started out with a proxy whose job was to do this authentication to",
    "start": "1850520",
    "end": "1856240"
  },
  {
    "text": "authenticate uh users requests and then mediate that towards a an",
    "start": "1856240",
    "end": "1862279"
  },
  {
    "text": "LLM but it turns out it's very very handy to be in the middle of all these",
    "start": "1862279",
    "end": "1867480"
  },
  {
    "text": "requests one uh some some examples I'll give of this",
    "start": "1867480",
    "end": "1873240"
  },
  {
    "text": "are we obviously look at latency from the point of view of the client but that's a very fraugh thing to do um and",
    "start": "1873240",
    "end": "1879919"
  },
  {
    "text": "it's something I I caution you it's okay to track that number just don't put it on a dashboard because someone will be",
    "start": "1879919",
    "end": "1887279"
  },
  {
    "text": "very incentivized to uh like take the the 50th percentile of it the average of",
    "start": "1887279",
    "end": "1892559"
  },
  {
    "text": "it or something like that so you you're essentially averaging up the experience of everybody on the internet's requests",
    "start": "1892559",
    "end": "1899120"
  },
  {
    "text": "from somebody who lives way way out on bush on a satellite link to someone who lives next to Amzix's data center and",
    "start": "1899120",
    "end": "1906240"
  },
  {
    "text": "you're effectively trying to say take the average of all their experiences what you get when you do that is",
    "start": "1906240",
    "end": "1911919"
  },
  {
    "text": "effectively the belief that all your your users live on a boat in the middle of the Atlantic Ocean",
    "start": "1911919",
    "end": "1918440"
  },
  {
    "text": "now this this v this this vantage point uh is also good because um while our",
    "start": "1919039",
    "end": "1924640"
  },
  {
    "text": "upstream provider does give us um lots of statistics they're really targeted to their how they view how they view",
    "start": "1924640",
    "end": "1930399"
  },
  {
    "text": "running the service their kind of metrics they have basic kind of like request counts and error rates and",
    "start": "1930399",
    "end": "1935440"
  },
  {
    "text": "things like that but they're not really the granuality we want and more fundamentally the way that",
    "start": "1935440",
    "end": "1942320"
  },
  {
    "text": "I think about it um to take take food delivery as an example use the app you you you request some food and about 5",
    "start": "1942320",
    "end": "1949039"
  },
  {
    "text": "minutes later you get a notification saying your food your food's ready you know we finished cooking it we're just",
    "start": "1949039",
    "end": "1954240"
  },
  {
    "text": "waiting for the driver to pick it up so from the restaurant's point of view their job is done they did it their SLO",
    "start": "1954240",
    "end": "1960559"
  },
  {
    "text": "5 minutes done but it's another 45 minutes before there's a knock on the door with your food you don't care how",
    "start": "1960559",
    "end": "1965840"
  },
  {
    "text": "quickly the restaurant prepared your food what you care about is the total endto end time of the of the request and",
    "start": "1965840",
    "end": "1972159"
  },
  {
    "text": "so we do that by by defining in our SLOs's that the point we are measuring is at our proxy we it's it's okay for",
    "start": "1972159",
    "end": "1980640"
  },
  {
    "text": "it's okay for our service provider to have their own metrics but we negotiate our SLOs's as the kind of the data plane",
    "start": "1980640",
    "end": "1987760"
  },
  {
    "text": "the split where the where the proxy",
    "start": "1987760",
    "end": "1991919"
  },
  {
    "text": "is now you saw that we have we support a variety of of ideides and within each",
    "start": "1995640",
    "end": "2001120"
  },
  {
    "text": "IDE there is a flirtillaa of different client versions out there dealing with",
    "start": "2001120",
    "end": "2006960"
  },
  {
    "text": "the longtailed client versions is the bane of my life yes there is always a long tail we",
    "start": "2006960",
    "end": "2014320"
  },
  {
    "text": "see about when we do a new client release we get to about 80% population within 12 within 24 to 36 hours and that",
    "start": "2014320",
    "end": "2021840"
  },
  {
    "text": "last 20% will take until the heat death of the universe i I cannot understand",
    "start": "2021840",
    "end": "2028200"
  },
  {
    "text": "why clients can use such old software like what I mean the auto update mechanisms are so",
    "start": "2028200",
    "end": "2036159"
  },
  {
    "text": "pervasive and penicious about getting you to update i don't quite understand how they can do this but they",
    "start": "2036159",
    "end": "2041559"
  },
  {
    "text": "do so what this means is that if we have a bug and we need to make a fix we can't",
    "start": "2041559",
    "end": "2047200"
  },
  {
    "text": "do it in the client it just takes too long and we never get to the population that would lead be successful with the",
    "start": "2047200",
    "end": "2053520"
  },
  {
    "text": "with rolling out that fix but this is good because we have a service that sits in the middle the proxy that sits in the",
    "start": "2053520",
    "end": "2059919"
  },
  {
    "text": "middle that we can do a fix up on the fly hopefully and over time that that",
    "start": "2059919",
    "end": "2066398"
  },
  {
    "text": "fix will make it into the client versions and they'll sufficiently roll out that there'll be a sufficient enough population",
    "start": "2066399",
    "end": "2073118"
  },
  {
    "text": "an example of this uh one one day out of the blue we got a call from our model from our model provider that said um you",
    "start": "2073119",
    "end": "2080560"
  },
  {
    "text": "can't send this particular parameter it was something to do with log probabilities you can't send that because it'll cause the model to",
    "start": "2080560",
    "end": "2086919"
  },
  {
    "text": "crash which is it's pretty bad because this is a poison pill it will if a",
    "start": "2086919",
    "end": "2092720"
  },
  {
    "text": "particular form request will cause a model instance to crash it will blow that one out of the water and that request will be retrieded and it'll blow",
    "start": "2092720",
    "end": "2098720"
  },
  {
    "text": "the next one out of the water and keep working its way down the line we couldn't fix it in the client because it",
    "start": "2098720",
    "end": "2104079"
  },
  {
    "text": "wouldn't be fast enough but because we have a proxy in the middle we can just mutate the request quietly on the way",
    "start": "2104079",
    "end": "2110760"
  },
  {
    "text": "through and that takes the pressure off our upstream provider to get a real fix so we can restore that functionality",
    "start": "2110760",
    "end": "2119280"
  },
  {
    "text": "and the last thing that we do is when we have clients that are very very old and we need to deprecate like some API",
    "start": "2119280",
    "end": "2125920"
  },
  {
    "text": "endpoint or something like that rather than just letting them get weird 404 errors we actually have a a special",
    "start": "2125920",
    "end": "2131280"
  },
  {
    "text": "status code which triggers logic in the client that puts up a giant modal dialogue box that asks them very",
    "start": "2131280",
    "end": "2136640"
  },
  {
    "text": "politely would they please just push the upgrade button and there's even more that we can",
    "start": "2136640",
    "end": "2143359"
  },
  {
    "text": "do with this because logically the proxy is uh transparent the clients uh through",
    "start": "2143359",
    "end": "2150800"
  },
  {
    "text": "all of all of the shenanigans the clients still believe that they're making a request to model and they get a response the rest is",
    "start": "2150800",
    "end": "2158119"
  },
  {
    "text": "transparent but from the point of view of us in the middle who are routing",
    "start": "2158119",
    "end": "2163520"
  },
  {
    "text": "requests we can now split traffic across multiple models um quite often the",
    "start": "2163520",
    "end": "2169760"
  },
  {
    "text": "capacity we see we receive in one region won't all be in one kind of unit it",
    "start": "2169760",
    "end": "2175119"
  },
  {
    "text": "might be spread across multiple units especially if it arrives at different times and so being able to do traffic splits to combine all that together into",
    "start": "2175119",
    "end": "2181599"
  },
  {
    "text": "one logical model is very very handy we can do the opposite we can uh",
    "start": "2181599",
    "end": "2188079"
  },
  {
    "text": "mirror traffic so we can take a readonly tap of requests and send that to a new version of the model that we might be",
    "start": "2188079",
    "end": "2194160"
  },
  {
    "text": "either performance testing or validating or something like that and then we can",
    "start": "2194160",
    "end": "2199280"
  },
  {
    "text": "take these two ideas and kind of you know mix and match them and stack them on top of each other and make AB tests",
    "start": "2199280",
    "end": "2204640"
  },
  {
    "text": "experiments all those kind of things all without involving the clients the from the client's point of view it just",
    "start": "2204640",
    "end": "2210640"
  },
  {
    "text": "thinks it's talking to the the same model it has yesterday and",
    "start": "2210640",
    "end": "2215920"
  },
  {
    "text": "today so this is the basic gist of how you build a low latency code completion",
    "start": "2218680",
    "end": "2224720"
  },
  {
    "text": "system with the aim of competing with ideides but I want to step back and in",
    "start": "2224720",
    "end": "2230880"
  },
  {
    "text": "these last 10 minutes just ask like as an engineering effort was this worth it",
    "start": "2230880",
    "end": "2236839"
  },
  {
    "text": "did the engineering effort we put into this proxy system pay for",
    "start": "2236839",
    "end": "2242320"
  },
  {
    "text": "itself now one way to look at this is for low latency you want to minimize",
    "start": "2242760",
    "end": "2248320"
  },
  {
    "text": "hops you sort of want to minimize the number of middlemen that are in the middle uh the middleware anything that's",
    "start": "2248320",
    "end": "2253680"
  },
  {
    "text": "kind of in that request path adding value but also adding latency so what if we just went straight",
    "start": "2253680",
    "end": "2260160"
  },
  {
    "text": "to Azure instead clients connect straight to Azure this would have left",
    "start": "2260160",
    "end": "2266760"
  },
  {
    "text": "authentication as the big problem as well as observability they would have really been open",
    "start": "2266760",
    "end": "2273240"
  },
  {
    "text": "questions it would have been possible to teach Azure to understand GitHub's O token so the the token that the IDE",
    "start": "2273240",
    "end": "2280079"
  },
  {
    "text": "natively has from GitHub could be presented to Azure um as an authentication method i'm sure that",
    "start": "2280079",
    "end": "2285280"
  },
  {
    "text": "would be possible but it would probably result in Azure building effectively what I just",
    "start": "2285280",
    "end": "2291040"
  },
  {
    "text": "demonstrated on this um certainly if if our roles were a person I was the Azure engineer I would build this with an",
    "start": "2291040",
    "end": "2296640"
  },
  {
    "text": "authentication layer in front of my service some customers coming to me with a strange authentication mechanism I'm",
    "start": "2296640",
    "end": "2303119"
  },
  {
    "text": "going to build a layer which converts that into my real authentication mechanism so we would have probably ended up with exactly the same number of",
    "start": "2303119",
    "end": "2310000"
  },
  {
    "text": "moving parts just with more of them kind of behind the curtain in the Azure",
    "start": "2310000",
    "end": "2316799"
  },
  {
    "text": "side instead by colloccating proxy instances and model instances in the same Azure region um we to to most",
    "start": "2319000",
    "end": "2328079"
  },
  {
    "text": "extent ameliated the cost of that extra hop um certainly inter interreion traffic is not free it's not zero but",
    "start": "2328079",
    "end": "2335200"
  },
  {
    "text": "it's pretty close to zero and it's fairly constant in terms of in in terms of the latency you see there so you can",
    "start": "2335200",
    "end": "2341280"
  },
  {
    "text": "characterize it and effectively ignore",
    "start": "2341280",
    "end": "2345200"
  },
  {
    "text": "it and I'm going to tell you a few more stories of a few more war stories of what's happened over the life of this",
    "start": "2347000",
    "end": "2352800"
  },
  {
    "text": "product just to kind of emphasize that the value of having this this intermediary um really paid for itself",
    "start": "2352800",
    "end": "2358880"
  },
  {
    "text": "over and over one day uh we we upgraded to a new version",
    "start": "2358880",
    "end": "2365599"
  },
  {
    "text": "of the model which seemed to be very attracted to a particular token it really liked emitting this token it was",
    "start": "2365599",
    "end": "2370720"
  },
  {
    "text": "some kind of end of file marker and it was something to do with a mistake in how it was trained that um it it just",
    "start": "2370720",
    "end": "2376480"
  },
  {
    "text": "really like to emit this token now we can work around this by using uh by",
    "start": "2376480",
    "end": "2382880"
  },
  {
    "text": "using essentially in the request by saying in your response wait this very particular token you know weight it you",
    "start": "2382880",
    "end": "2388880"
  },
  {
    "text": "know down negative affinity never want to see it but if we didn't have an intermediary",
    "start": "2388880",
    "end": "2395359"
  },
  {
    "text": "like the proxy to do that we would have had to do that in the client so we would have had to do a client roll out which",
    "start": "2395359",
    "end": "2400560"
  },
  {
    "text": "would have been slow and ultimately would not have got all the users then we would have had to then the",
    "start": "2400560",
    "end": "2408079"
  },
  {
    "text": "model would have been fixed and we'd have to do another client to reverse what we just another client change to reverse what we just",
    "start": "2408079",
    "end": "2415520"
  },
  {
    "text": "did instead it was super easy to add this add this parameter to the request",
    "start": "2415800",
    "end": "2421119"
  },
  {
    "text": "on the fly it was as it was on its way to the model um and that solved the problem immediately and it gave us",
    "start": "2421119",
    "end": "2426400"
  },
  {
    "text": "breathing room to figure out what had gone wrong with the model training and fix that without the you know the the",
    "start": "2426400",
    "end": "2432400"
  },
  {
    "text": "the sort of damicles hanging over our head another another",
    "start": "2432400",
    "end": "2438920"
  },
  {
    "text": "story is that one day I was looking at the distribution of uh cancellation how",
    "start": "2438920",
    "end": "2445040"
  },
  {
    "text": "for a request that was canceled how long did it live until it was cancelled and",
    "start": "2445040",
    "end": "2450560"
  },
  {
    "text": "there was this bizarre spike effectively at like 1 millisecond oh",
    "start": "2450560",
    "end": "2456440"
  },
  {
    "text": "dear there we go there was a this bizarre spike effectively at 1 millisecond effectively immediately it",
    "start": "2456440",
    "end": "2462960"
  },
  {
    "text": "was saying a lot of requests come from the clients and are immediately cancelled as in you read the you read",
    "start": "2462960",
    "end": "2469760"
  },
  {
    "text": "the request and then instantly afterwards the client is like whoop I'm sorry I didn't mean to send that to you",
    "start": "2469760",
    "end": "2475520"
  },
  {
    "text": "let me take it back the problem is by that time we've already started the process of forwarding that to Azure and",
    "start": "2475520",
    "end": "2482079"
  },
  {
    "text": "they're they're they're mulling on it and so we immediately send the request to Azure and then say to them sorry I",
    "start": "2482079",
    "end": "2487119"
  },
  {
    "text": "didn't mean to send that to you may I please have it back now cancellation",
    "start": "2487119",
    "end": "2492720"
  },
  {
    "text": "frees up model resources quicker but it's not as as cheap as just not sending",
    "start": "2492720",
    "end": "2497920"
  },
  {
    "text": "requests that we know we're going to cancel is now it took us some time to figure",
    "start": "2497920",
    "end": "2504480"
  },
  {
    "text": "out what was exactly happening in the client to cause this fast cancellation behavior but because we had the proxy in",
    "start": "2504480",
    "end": "2511280"
  },
  {
    "text": "the middle we could add a little check that just before we made the request to the model we would check has it actually",
    "start": "2511280",
    "end": "2517520"
  },
  {
    "text": "been cancelled there was there were mechanisms in the HTP library to ask that question and we saved ourselves",
    "start": "2517520",
    "end": "2523119"
  },
  {
    "text": "making and then retracting that request and another another point",
    "start": "2523119",
    "end": "2528720"
  },
  {
    "text": "talking to metrics um from the the metrics that our",
    "start": "2528720",
    "end": "2533760"
  },
  {
    "text": "upstream uh model provider provides us there's no way like we don't get histograms we don't get distributions we",
    "start": "2533760",
    "end": "2539680"
  },
  {
    "text": "barely get averages there would be no way we would have been able to spot this without our own uh observability at that",
    "start": "2539680",
    "end": "2546720"
  },
  {
    "text": "at that client proxy If we didn't have the proxy as an",
    "start": "2546720",
    "end": "2552880"
  },
  {
    "text": "intermediary we still could have had multiple models around the world there would as you saw you can have uh OpenAI",
    "start": "2552880",
    "end": "2558880"
  },
  {
    "text": "models in any Azure region you want we would just not have a proxy in front of them so we probably would have used",
    "start": "2558880",
    "end": "2564560"
  },
  {
    "text": "something like Octad DNS to still do the um kind of geographic geographic",
    "start": "2564560",
    "end": "2570119"
  },
  {
    "text": "routing but it would have left open the question of what do we do about health checks like when models are unhealthy or",
    "start": "2570119",
    "end": "2575920"
  },
  {
    "text": "overloaded how do we take them out of DNS and what we probably would have had to do is build something like a",
    "start": "2575920",
    "end": "2583400"
  },
  {
    "text": "uh yeah build a some kind of thing that's issuing synthetic requests or pinging the model or something like that",
    "start": "2583400",
    "end": "2590160"
  },
  {
    "text": "and then making calls to upstream DNS providers to manually thumbs up and thumbs thumbs down regions",
    "start": "2590160",
    "end": "2598520"
  },
  {
    "text": "so HP2 is critical to the co-pilot latency story without cancellation",
    "start": "2599520",
    "end": "2605599"
  },
  {
    "text": "um we we make twice as many requests and waste half of",
    "start": "2605599",
    "end": "2611000"
  },
  {
    "text": "them and it was surprisingly difficult to do with off-the-shelf tools uh at the time CDNs didn't support",
    "start": "2611000",
    "end": "2618720"
  },
  {
    "text": "HP2 on the back end that was a non that was an absolute non-starter most cloud providers didn't support HTB2 on the",
    "start": "2618720",
    "end": "2625359"
  },
  {
    "text": "back end um if you want to do that you have to terminate TLS yourself so for the first",
    "start": "2625359",
    "end": "2632000"
  },
  {
    "text": "year of our existence of our product TLS like the actual network connection was terminated directly on the Kubernetes",
    "start": "2632000",
    "end": "2637599"
  },
  {
    "text": "pod um you can imagine our security team were absolutely over overjoyed with this situation it also meant that every time",
    "start": "2637599",
    "end": "2644960"
  },
  {
    "text": "we did a deploy we were literally disconnecting everybody and they would reconnect but that goes against the that",
    "start": "2644960",
    "end": "2651599"
  },
  {
    "text": "goes against the uh the theory that we want to have these connections and keep them open for as long as",
    "start": "2651599",
    "end": "2659000"
  },
  {
    "text": "possible",
    "start": "2659000",
    "end": "2662000"
  },
  {
    "text": "yeah this is very GitHub specific but a lot of you work for medium to large",
    "start": "2664599",
    "end": "2670160"
  },
  {
    "text": "large scale companies you probably have a tech stack that is in GitHub pilots we call it the the paved path it is the",
    "start": "2670160",
    "end": "2677280"
  },
  {
    "text": "blessed way the way that you're supposed to deploy applications inside the company everything behind GB everything",
    "start": "2677280",
    "end": "2683520"
  },
  {
    "text": "managed by OctaDNS um made our compliance story and you can imagine we're selling this to large enterprise",
    "start": "2683520",
    "end": "2689599"
  },
  {
    "text": "companies you need to have your certifications you need to have your sock 2 tick in the box and using these",
    "start": "2689599",
    "end": "2695119"
  },
  {
    "text": "shared components really made that compliance story much easier the auditors say \"Oh this is another GB",
    "start": "2695119",
    "end": "2701359"
  },
  {
    "text": "hosted service.\" Yep using all the regular stuff not exactly a tick in the box but got a long way towards solving",
    "start": "2701359",
    "end": "2708319"
  },
  {
    "text": "our compliance story and the flip side is because these are shared components rather than every",
    "start": "2708319",
    "end": "2714640"
  },
  {
    "text": "individual team knowing every detail up and down of terminating TLS connections on pods hosted in Kubernetes clusters",
    "start": "2714640",
    "end": "2721200"
  },
  {
    "text": "that they run themselves we delegate that work to shared teams who are much better at it than that",
    "start": "2721200",
    "end": "2728599"
  },
  {
    "text": "so just to wrap up this is a story of what made Copilot",
    "start": "2729040",
    "end": "2734720"
  },
  {
    "text": "a success um and although I have been around the sponsors booth it is possible that not all of you are building your",
    "start": "2734720",
    "end": "2741520"
  },
  {
    "text": "own LLM as a service service so are there broader takeaways uh for the rest of",
    "start": "2741520",
    "end": "2747800"
  },
  {
    "text": "you the first one is definitely use HP2 it's dope",
    "start": "2747800",
    "end": "2753440"
  },
  {
    "text": "uh I heard I was saw a presentation by uh the C the CTO of Fastley um and he he",
    "start": "2753440",
    "end": "2760000"
  },
  {
    "text": "was he he viewed HP2 as an intermediary he really says HTTP3 is the the the real",
    "start": "2760000",
    "end": "2766720"
  },
  {
    "text": "standard the one that they really wanted to make and from his position as a content delivery partner whose job is",
    "start": "2766720",
    "end": "2772880"
  },
  {
    "text": "just to ship bits as fast as possible I I agree completely with that so perhaps",
    "start": "2772880",
    "end": "2777920"
  },
  {
    "text": "the advice is not use HTB2 the advice would perhaps be something like use",
    "start": "2777920",
    "end": "2783520"
  },
  {
    "text": "something better than HTTP1 if you're interested in learning more if you look that up on YouTube",
    "start": "2783520",
    "end": "2789359"
  },
  {
    "text": "that's a presentation by Jeff Hust talking about HTTP2 from the point of view of application writers and clients",
    "start": "2789359",
    "end": "2796400"
  },
  {
    "text": "and how it totally disintermediates most of the kind of SSL in the middle VPN",
    "start": "2796400",
    "end": "2801599"
  },
  {
    "text": "nonsense that we live with day-to-day in current kind of web stuff",
    "start": "2801599",
    "end": "2807359"
  },
  {
    "text": "the second one is I I didn't have a Capernacus quote but I do have a a a Bezos quote if",
    "start": "2807359",
    "end": "2814880"
  },
  {
    "text": "you're gluing your product together from parts from off-the-shelf suppliers and",
    "start": "2814880",
    "end": "2820079"
  },
  {
    "text": "your role in that is only supplying the silly party in the glue what are your customers paying you",
    "start": "2820079",
    "end": "2825920"
  },
  {
    "text": "for like where's your moat now as an engineer I understand very",
    "start": "2825920",
    "end": "2832000"
  },
  {
    "text": "very deeply the ide the desire not to reinvent the wheel so the challenge to",
    "start": "2832000",
    "end": "2837359"
  },
  {
    "text": "you is find the place where investing your limited engineering budget is going to in a bespoke solution is going to",
    "start": "2837359",
    "end": "2844240"
  },
  {
    "text": "give you a marketable return in our case it was writing HP2 proxy that accelerated one API call",
    "start": "2844240",
    "end": "2853040"
  },
  {
    "text": "now we're very lucky that copilot proxy as a product is more or less done and has been done for quite a long time",
    "start": "2853040",
    "end": "2858960"
  },
  {
    "text": "which is great because it gives our small team essentially 90% of our time to get dedicate to the operational",
    "start": "2858960",
    "end": "2864240"
  },
  {
    "text": "issues of running this service and the last one is if you if",
    "start": "2864240",
    "end": "2871760"
  },
  {
    "text": "you care about latency if your cloud provider is trying",
    "start": "2871760",
    "end": "2877920"
  },
  {
    "text": "to sell you this siren song that they can solve your latency issues with their super fast network backbone that can be",
    "start": "2877920",
    "end": "2884880"
  },
  {
    "text": "true to a point but remember the words of Montgomery Scott um you cannot change",
    "start": "2884880",
    "end": "2890000"
  },
  {
    "text": "the laws of physics despite what your engineering title is if you want low latency you have to",
    "start": "2890000",
    "end": "2896480"
  },
  {
    "text": "bring your application closer to your users in our case that was fairly straightforward because code completion",
    "start": "2896480",
    "end": "2902400"
  },
  {
    "text": "at least in the in the request path is essentially stateless uh your your uh",
    "start": "2902400",
    "end": "2907920"
  },
  {
    "text": "situation may not be as easy by having multiple models around the globe that turns sev one incidents",
    "start": "2907920",
    "end": "2915359"
  },
  {
    "text": "into just SE 2 alerts um if a region is down or overloaded traffic just flows",
    "start": "2915359",
    "end": "2921040"
  },
  {
    "text": "somewhere else those users instead of getting a busy signal still get a service albeit at a marginally higher",
    "start": "2921040",
    "end": "2927800"
  },
  {
    "text": "latency and in I I've talked to a bunch of people at lunch and I said that we we",
    "start": "2927800",
    "end": "2933119"
  },
  {
    "text": "we would fight fight holy wars over 20 milliseconds so the kind of the the kind",
    "start": "2933119",
    "end": "2938240"
  },
  {
    "text": "of latencies we're talking about here are in the range of 50 to 100 milliseconds so really not noticeable",
    "start": "2938240",
    "end": "2943680"
  },
  {
    "text": "for the average user and with that thank you very much",
    "start": "2943680",
    "end": "2948720"
  },
  {
    "text": "for your time i've really appreciated the opportunity to speak to you today",
    "start": "2948720",
    "end": "2953760"
  },
  {
    "text": "[Music]",
    "start": "2957320",
    "end": "2962770"
  }
]