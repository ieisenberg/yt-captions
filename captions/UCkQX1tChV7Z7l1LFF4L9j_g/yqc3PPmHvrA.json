[
  {
    "text": "good morning and welcome to cubecon so as nikki mentioned we're going to start with something that",
    "start": "4640",
    "end": "9920"
  },
  {
    "text": "has a 90 chance of failure um so i want to request all of you to",
    "start": "9920",
    "end": "15440"
  },
  {
    "text": "take out your phones or laptops and go to this url on the screen tiny dot cc slash",
    "start": "15440",
    "end": "20720"
  },
  {
    "text": "qcon live you can also scan this qr code and log in with your linkedin",
    "start": "20720",
    "end": "26160"
  },
  {
    "text": "credentials if you don't have a linkedin account please make one right now",
    "start": "26160",
    "end": "31840"
  },
  {
    "text": "uh you should see a live stream of this very talk delayed by a minute and my lovely wife",
    "start": "32399",
    "end": "38800"
  },
  {
    "text": "sitting here in the audience is streaming the demo live from a phone",
    "start": "38800",
    "end": "44559"
  },
  {
    "text": "if you're successful you should see something like this on your screen",
    "start": "44559",
    "end": "50960"
  },
  {
    "text": "how many of you are able to see something awesome all right i guess you're getting",
    "start": "51760",
    "end": "57280"
  },
  {
    "text": "there you're still making a linkedin account right all right so",
    "start": "57280",
    "end": "62559"
  },
  {
    "text": "this comments area is where you can interact with each other you can post comments you can ask",
    "start": "62559",
    "end": "69520"
  },
  {
    "text": "questions and i will try and answer some of them at the end of this talk the only thing i request is to not share",
    "start": "69520",
    "end": "75439"
  },
  {
    "text": "the stream because this is just a demo and i will delete it after the end of the stock",
    "start": "75439",
    "end": "81920"
  },
  {
    "text": "all right let's start with a question anyone know what the largest live stream",
    "start": "84479",
    "end": "89759"
  },
  {
    "text": "in the world was think of something that grinds an entire nation to halt yes",
    "start": "89759",
    "end": "100640"
  },
  {
    "text": "oh that was the second largest yes yes cricket all right so believe it or",
    "start": "100640",
    "end": "106720"
  },
  {
    "text": "not it was the semi-final of the cricket world cup last year between india and new zealand more than 25 million viewers watch the",
    "start": "106720",
    "end": "114320"
  },
  {
    "text": "match at the same time also overall the match crossed 100 million viewers",
    "start": "114320",
    "end": "120000"
  },
  {
    "text": "and as this gentleman said the second largest was the british royal wedding which was more than 18 million viewers",
    "start": "120000",
    "end": "127200"
  },
  {
    "text": "concurrently so remember this we'll come back to this this",
    "start": "127200",
    "end": "134400"
  },
  {
    "text": "so this is me and my team uh we call ourselves the unreal real time team",
    "start": "135599",
    "end": "140800"
  },
  {
    "text": "and we love cricket and we love coding",
    "start": "140800",
    "end": "145040"
  },
  {
    "text": "this has a range not not very great and we believe in",
    "start": "147200",
    "end": "153760"
  },
  {
    "text": "something we believe that problems and distributed systems can be solved by starting small solve",
    "start": "153760",
    "end": "159519"
  },
  {
    "text": "the first problem and add simple layers in your architecture to solve bigger problems",
    "start": "159519",
    "end": "165519"
  },
  {
    "text": "and today i'm going to tell you a story of how we built a platform called the real-time platform using that",
    "start": "167840",
    "end": "172959"
  },
  {
    "text": "principle to solve how we can stream or have many many people interact simultaneously",
    "start": "172959",
    "end": "180640"
  },
  {
    "text": "on live videos i hope that a lot of you will be able to learn something from this and also apply",
    "start": "180640",
    "end": "187120"
  },
  {
    "text": "it to your own systems so what is a live video a live telecast",
    "start": "187120",
    "end": "193120"
  },
  {
    "text": "a live conference broadcast a sports match all of them are examples of live videos how many of",
    "start": "193120",
    "end": "199920"
  },
  {
    "text": "you have interacted with a live stream on youtube or facebook perfect so you know the general idea the",
    "start": "199920",
    "end": "206560"
  },
  {
    "text": "difference that these streams have is that they allow you users or viewers to interact with each other",
    "start": "206560",
    "end": "213599"
  },
  {
    "text": "so what is a real-time interaction on live video so the easiest way is to just look at it or try it on your phones with the demo",
    "start": "213599",
    "end": "221200"
  },
  {
    "text": "people are able to like comment and just ask questions and interact with each other so this is an example",
    "start": "221200",
    "end": "227280"
  },
  {
    "text": "of a linkedin ios app doing a linkedin live video and similarly on desktop the experience",
    "start": "227280",
    "end": "233280"
  },
  {
    "text": "is very similar there are many rich interactions that happen there this particular live stream is from the linkedin talent connect",
    "start": "233280",
    "end": "239760"
  },
  {
    "text": "conference that happened in september of last year so i want to talk about the simplest",
    "start": "239760",
    "end": "246080"
  },
  {
    "text": "interaction here which is how do you how do these likes get distributed to all these viewers in real",
    "start": "246080",
    "end": "252159"
  },
  {
    "text": "time all at the same time so let's say that the sender s likes the video and wants to send it to a receiver",
    "start": "252159",
    "end": "258720"
  },
  {
    "text": "a so the sender s sends the like to the to the server with a simple http request",
    "start": "258720",
    "end": "266639"
  },
  {
    "text": "but how do we send the like from the server back to the receiver a publishing data to clients is not that",
    "start": "266639",
    "end": "272560"
  },
  {
    "text": "straightforward so this brings us to the first challenge",
    "start": "272560",
    "end": "277919"
  },
  {
    "text": "what is the delivery pipe to send stuff to the clients by the way this is how structured the presentation today i'm going to",
    "start": "277919",
    "end": "284080"
  },
  {
    "text": "introduce problems and i'm going to talk about how we solve them in in our in our platform and these are all like",
    "start": "284080",
    "end": "291520"
  },
  {
    "text": "simple layers of architecture that we added to our platform to solve each of them",
    "start": "291520",
    "end": "297840"
  },
  {
    "text": "so as discussed before the sender s sends the like to what we call the likes backend which is the backend that stores",
    "start": "299440",
    "end": "305280"
  },
  {
    "text": "all these likes with a simple http request and this backend system",
    "start": "305280",
    "end": "311120"
  },
  {
    "text": "now needs to publish the like over to the real-time delivery system again that happens with a simple http",
    "start": "311120",
    "end": "317199"
  },
  {
    "text": "request so the thing we need is a persistent connection between the real-time delivery system",
    "start": "317199",
    "end": "323039"
  },
  {
    "text": "and the receiver a so let's talk a little bit more about the nature of this connection",
    "start": "323039",
    "end": "328639"
  },
  {
    "text": "because that's the one that we care about here so once again log in at linkedin.com",
    "start": "328639",
    "end": "335680"
  },
  {
    "text": "hopefully you've created an account by now and go to this url tiny dot cc real time",
    "start": "335680",
    "end": "341680"
  },
  {
    "text": "this time you will actually see what's happening behind the scenes",
    "start": "341680",
    "end": "346639"
  },
  {
    "text": "you should be logged in for this",
    "start": "350160",
    "end": "353840"
  },
  {
    "text": "all right so for those of you who were successful you should see something like this on your screen this is literally your persistent",
    "start": "357360",
    "end": "363680"
  },
  {
    "text": "connection with linkedin this is the pipe that we have to send data over to your",
    "start": "363680",
    "end": "368720"
  },
  {
    "text": "phones or to your laptops and this thing is extremely simple",
    "start": "368720",
    "end": "374240"
  },
  {
    "text": "it's a simple http long pole which is a regular http connection where the server holds on to the request",
    "start": "374240",
    "end": "381039"
  },
  {
    "text": "it just doesn't disconnect it so over this connection we use a technology called server sent events",
    "start": "381039",
    "end": "386880"
  },
  {
    "text": "and this allows us to stream chunks of data over what is called the event source",
    "start": "386880",
    "end": "392160"
  },
  {
    "text": "interface and the client doesn't need to make subsequent requests we can just keep streaming data",
    "start": "392160",
    "end": "397759"
  },
  {
    "text": "on the same open connection so the client makes a normal http get request it's as",
    "start": "397759",
    "end": "404000"
  },
  {
    "text": "simple as a regular http connect connection request the only difference is that this the accept header says",
    "start": "404000",
    "end": "411280"
  },
  {
    "text": "event stream that's the only difference from a regular http connection request",
    "start": "411280",
    "end": "417120"
  },
  {
    "text": "so the server responds with a normal http 200 okay and sets the content type to event",
    "start": "417120",
    "end": "423360"
  },
  {
    "text": "stream and the connection is not disconnected",
    "start": "423360",
    "end": "428160"
  },
  {
    "text": "chunks of data are sent down without closing the connection so you might receive for example a like",
    "start": "428479",
    "end": "434880"
  },
  {
    "text": "object and later you might receive a comment object",
    "start": "434880",
    "end": "440560"
  },
  {
    "text": "without closing the connection the server is just streaming chunks of data over the same open http connection",
    "start": "440560",
    "end": "446400"
  },
  {
    "text": "request each chunk is processed independently on the client through what is called the",
    "start": "446400",
    "end": "452319"
  },
  {
    "text": "event source interface and as you can see there is nothing terribly different from a regular http connection",
    "start": "452319",
    "end": "457759"
  },
  {
    "text": "except that the content type is different and you can stream multiple chunks of bodies",
    "start": "457759",
    "end": "463520"
  },
  {
    "text": "on the same open http connection request how does this look like on the client",
    "start": "463520",
    "end": "469280"
  },
  {
    "text": "side on web the client first creates an event source",
    "start": "469280",
    "end": "474560"
  },
  {
    "text": "object with the target url on the server and then it defines these event handlers which",
    "start": "474560",
    "end": "481360"
  },
  {
    "text": "will process each chunk of data independently on the client so most browsers support the event",
    "start": "481360",
    "end": "487360"
  },
  {
    "text": "source interface natively on android and ios there are lightweight",
    "start": "487360",
    "end": "492960"
  },
  {
    "text": "libraries that are available to implement the event source interface on these clients",
    "start": "492960",
    "end": "500080"
  },
  {
    "text": "okay so we now know how to stream data from the server to the client and we did this by using http long poll",
    "start": "500639",
    "end": "507360"
  },
  {
    "text": "with service and events what is the next challenge",
    "start": "507360",
    "end": "512638"
  },
  {
    "text": "yes so so think of think of all the thousands of indians trying to watch cricket right the next challenge is multiple",
    "start": "517279",
    "end": "524320"
  },
  {
    "text": "connections maybe thousands of them and we need to figure out how to manage these connections right",
    "start": "524320",
    "end": "531839"
  },
  {
    "text": "so connection management at linkedin we manage these connections using aka",
    "start": "532080",
    "end": "537440"
  },
  {
    "text": "akai is a toolkit for building highly concurrent message driven applications",
    "start": "537440",
    "end": "542560"
  },
  {
    "text": "anyone familiar with akka actors wow room full so yes not hollywood",
    "start": "542560",
    "end": "548800"
  },
  {
    "text": "actors they're this very simple thing it's this little small guy",
    "start": "548800",
    "end": "554480"
  },
  {
    "text": "so this is the only concept you need to know to understand the rest of the presentation",
    "start": "554480",
    "end": "559600"
  },
  {
    "text": "actors are objects which have some state and they have some behavior the behavior defines how the state",
    "start": "559600",
    "end": "566959"
  },
  {
    "text": "should be modified when they receive certain messages each actor has a mailbox and they",
    "start": "566959",
    "end": "574320"
  },
  {
    "text": "communicate exclusively by exchanging messages an actor is assigned a lightweight",
    "start": "574320",
    "end": "581040"
  },
  {
    "text": "thread every time there is a message to be processed and that thread will look at the",
    "start": "581040",
    "end": "587040"
  },
  {
    "text": "behavior that is defined for that message and modify the state of the actor based on that definition",
    "start": "587040",
    "end": "593120"
  },
  {
    "text": "and then once that is done this thread is actually free to be assigned to the next actor",
    "start": "593120",
    "end": "600080"
  },
  {
    "text": "since actors are so lightweight there can be millions of them in the system and each can have their own state and",
    "start": "600240",
    "end": "605760"
  },
  {
    "text": "their own behavior and a relatively small number of threads",
    "start": "605760",
    "end": "610800"
  },
  {
    "text": "which is proportional to the number of cores can be serving these millions of actors all at the same time",
    "start": "610800",
    "end": "616640"
  },
  {
    "text": "because a thread is assigned to an actor only when there is something to process",
    "start": "616640",
    "end": "622320"
  },
  {
    "text": "so in our case each actor is managing one persistent connection",
    "start": "622800",
    "end": "628079"
  },
  {
    "text": "that's the state that it is managing and as it receives an event uh the behavior here is defining",
    "start": "628079",
    "end": "634480"
  },
  {
    "text": "how to publish that event to the event source connection and those many connections can be managed by the same machine",
    "start": "634480",
    "end": "641200"
  },
  {
    "text": "using these concept of ag actors so let's look at how in our characters are assigned to an event source",
    "start": "641200",
    "end": "647440"
  },
  {
    "text": "connection almost every major server framework supports the event source interface natively",
    "start": "647440",
    "end": "653440"
  },
  {
    "text": "at linkedin we use the play framework and if you're familiar with play we just",
    "start": "653440",
    "end": "659279"
  },
  {
    "text": "use a regular play controller to accept the incoming connection",
    "start": "659279",
    "end": "664880"
  },
  {
    "text": "and then we use the play event source api to convert it into a persistent connection",
    "start": "665120",
    "end": "670160"
  },
  {
    "text": "and assign it a random connection id now we need something to manage the life",
    "start": "670160",
    "end": "675600"
  },
  {
    "text": "cycle of these connections and this is where our characters fit in",
    "start": "675600",
    "end": "681120"
  },
  {
    "text": "and so this is where we create an actor to manage this connection and we instantiate an act with the",
    "start": "681120",
    "end": "688000"
  },
  {
    "text": "connection id and the handle to the event source connection that it is supposed to manage",
    "start": "688000",
    "end": "694240"
  },
  {
    "text": "so let's get back out of code and see how the concept of our characters allows you to manage multiple",
    "start": "694640",
    "end": "699839"
  },
  {
    "text": "connections at the same time so each client connection here is",
    "start": "699839",
    "end": "706640"
  },
  {
    "text": "managed by its own actor and each actor in turn all of them are",
    "start": "706640",
    "end": "714079"
  },
  {
    "text": "managed by an akka supervisor actor so let's see how like can be distributed to all these clients",
    "start": "714079",
    "end": "720720"
  },
  {
    "text": "using this concept so the likes backend publishes the like object to the supervisor arc actor",
    "start": "720720",
    "end": "727279"
  },
  {
    "text": "over a regular http request the supervisor character simply",
    "start": "727279",
    "end": "734639"
  },
  {
    "text": "broadcasts the like object to all of its child actors here",
    "start": "734639",
    "end": "740160"
  },
  {
    "text": "and then these arc actors have a very simple thing to do they just need to take the handle of the event source",
    "start": "741200",
    "end": "746560"
  },
  {
    "text": "connection that they have and send the event down through that connection so for that it",
    "start": "746560",
    "end": "751680"
  },
  {
    "text": "looks something very similar sorry very very simple it's eventsource.send",
    "start": "751680",
    "end": "757040"
  },
  {
    "text": "and the like object that they need to send so they will use that to send the like objects down to the",
    "start": "757040",
    "end": "762959"
  },
  {
    "text": "clients and what does this look like on the client side the client sees",
    "start": "762959",
    "end": "770160"
  },
  {
    "text": "a new chunk of data as you saw before and we'll simply use that to render the like on the screen it's as",
    "start": "770160",
    "end": "776480"
  },
  {
    "text": "simple as that okay so in this section we saw how an",
    "start": "776480",
    "end": "782240"
  },
  {
    "text": "event source connection can be managed using and therefore you can manage many many",
    "start": "782240",
    "end": "788079"
  },
  {
    "text": "connections on a single machine what's the next challenge",
    "start": "788079",
    "end": "793600"
  },
  {
    "text": "sorry fan out is one built before that",
    "start": "794320",
    "end": "801839"
  },
  {
    "text": "oh mailbox q size so we're now talking already about big big scale even before that something simple i'll give you a hint my",
    "start": "802160",
    "end": "809200"
  },
  {
    "text": "wife and i always want to watch different shows on netflix",
    "start": "809200",
    "end": "815279"
  },
  {
    "text": "no no yes so we just we just the thing that we",
    "start": "815279",
    "end": "821279"
  },
  {
    "text": "did just now is just broadcast the like blindly to everybody right without knowing which particular live video they're currently actually",
    "start": "821279",
    "end": "827040"
  },
  {
    "text": "watching right so the next challenge is different clients watching different live videos",
    "start": "827040",
    "end": "834000"
  },
  {
    "text": "how do we handle that we we don't know how to make sure that a like for let's say the red light video",
    "start": "834000",
    "end": "840240"
  },
  {
    "text": "goes to the red client and the green live video goes to the green client all right so let's assume that this",
    "start": "840240",
    "end": "847519"
  },
  {
    "text": "client here with connection id 3 is watching the red light video",
    "start": "847519",
    "end": "852720"
  },
  {
    "text": "and this client here with connection id5 is watching the green live video so what we need is a concept of",
    "start": "852720",
    "end": "859519"
  },
  {
    "text": "subscription that so the client can inform the server that this is the particular live video",
    "start": "859519",
    "end": "865839"
  },
  {
    "text": "that they're currently watching so when client 3 starts watching the red live video",
    "start": "865839",
    "end": "871360"
  },
  {
    "text": "all it does is that it sends a simple subscription request using a simple http request to our server",
    "start": "871360",
    "end": "877920"
  },
  {
    "text": "and the server will store the subscription in an in-memory subscriptions table",
    "start": "877920",
    "end": "884320"
  },
  {
    "text": "so now the server knows that the client with connection id3 is watching the deadline video",
    "start": "884320",
    "end": "890959"
  },
  {
    "text": "question why does in-memory work right so there are two reasons",
    "start": "890959",
    "end": "898880"
  },
  {
    "text": "the the subscription table is completely local",
    "start": "898880",
    "end": "904240"
  },
  {
    "text": "it is only for the clients that are connected to this machine and secondly the connections are",
    "start": "904240",
    "end": "911199"
  },
  {
    "text": "strongly tied to the life cycle of this machine if the machine dies the connection is also lost and",
    "start": "911199",
    "end": "917760"
  },
  {
    "text": "therefore you can actually store these subscriptions in memory inside these front-end nodes",
    "start": "917760",
    "end": "923519"
  },
  {
    "text": "and we'll talk a little bit more about this later so similarly client 5 also subscribes to live video 2 which is",
    "start": "923519",
    "end": "930800"
  },
  {
    "text": "the green live video so once all the subscriptions are done this is the state",
    "start": "930800",
    "end": "936160"
  },
  {
    "text": "of the front end or the real-time delivery system the server knows which clients are",
    "start": "936160",
    "end": "941360"
  },
  {
    "text": "watching which live videos so when the backend publishes a like for",
    "start": "941360",
    "end": "946399"
  },
  {
    "text": "the green live video this time all that the supervisor actor has to do is figure out which are all the clients",
    "start": "946399",
    "end": "952639"
  },
  {
    "text": "that are subscribed to the green live video which in this case is clients one two and five",
    "start": "952639",
    "end": "959759"
  },
  {
    "text": "and so the corresponding actors are able to send the likes to just those client devices similarly when",
    "start": "959759",
    "end": "967279"
  },
  {
    "text": "like happens on the red live video the supervisor actor is able to decide that it is destined only for connection",
    "start": "967279",
    "end": "973199"
  },
  {
    "text": "ids three and four and is able to send them the likes for the videos that they're currently",
    "start": "973199",
    "end": "978560"
  },
  {
    "text": "watching all right so in this section we introduced the concept of subscription",
    "start": "978560",
    "end": "984079"
  },
  {
    "text": "and now we know how to make sure that clients are only receiving likes for the videos that they're currently watching so what's the next challenge",
    "start": "984079",
    "end": "991680"
  },
  {
    "text": "now we can go back to the gentleman here so somebody already said here that there",
    "start": "991680",
    "end": "998480"
  },
  {
    "text": "could be billions and millions of connections right like now that just more number of connections than what a",
    "start": "998480",
    "end": "1004000"
  },
  {
    "text": "single machine can handle right that's the next challenge so we thought",
    "start": "1004000",
    "end": "1009440"
  },
  {
    "text": "really hard about this right like this this is where we were a little stuck",
    "start": "1009440",
    "end": "1014720"
  },
  {
    "text": "and that's us thinking really hard and we finally did what every back-end",
    "start": "1014720",
    "end": "1021040"
  },
  {
    "text": "engineer does to solve scaling challenges right you already know we added a machine",
    "start": "1021040",
    "end": "1026959"
  },
  {
    "text": "right so we add a machine and we start calling these front-end servers",
    "start": "1026959",
    "end": "1033120"
  },
  {
    "text": "the animation worked and we introduce a real-time dispatcher",
    "start": "1033360",
    "end": "1040160"
  },
  {
    "text": "right whose job is to dispatch a published event between the newly introduced front of machines because now we have",
    "start": "1040160",
    "end": "1046400"
  },
  {
    "text": "more than one now can the dispatcher node simply send",
    "start": "1046400",
    "end": "1052320"
  },
  {
    "text": "a published event to all the front end nodes",
    "start": "1052320",
    "end": "1056320"
  },
  {
    "text": "yes it can it's not that hard it can but turns out that it's not very efficient",
    "start": "1060559",
    "end": "1065679"
  },
  {
    "text": "if you have a small live video with only a few viewers that are connected to just",
    "start": "1065679",
    "end": "1070720"
  },
  {
    "text": "a few frontal machines right and there's a second reason which i'll come back to a little later but for now let's assume that the",
    "start": "1070720",
    "end": "1077679"
  },
  {
    "text": "dispatcher can't simply send a like to all the front-end machines blindly",
    "start": "1077679",
    "end": "1082799"
  },
  {
    "text": "so given that the dispatcher now needs to know which front-end machine is has connections that are subscribed",
    "start": "1082799",
    "end": "1090080"
  },
  {
    "text": "to a particular live video and so we need these front-end machines to tell the dispatcher whether it has",
    "start": "1090080",
    "end": "1096720"
  },
  {
    "text": "connections that are subscribed to a particular live video so let's assume that front-end node one",
    "start": "1096720",
    "end": "1102559"
  },
  {
    "text": "here has connections that are subscribed to the red light video and front-end note 2",
    "start": "1102559",
    "end": "1108880"
  },
  {
    "text": "here has connections that are subscribed to both the red and the green live video",
    "start": "1108880",
    "end": "1114160"
  },
  {
    "text": "so front-end node 1 would then send a simple subscription request just like the",
    "start": "1114160",
    "end": "1119280"
  },
  {
    "text": "clients were sending to the front-end service and tell the real-time dispatcher",
    "start": "1119280",
    "end": "1124880"
  },
  {
    "text": "that it has connections that are watching the red light video and the dispatcher will create an entry",
    "start": "1124880",
    "end": "1131760"
  },
  {
    "text": "in its own subscriptions table to figure out which front end nodes are subscribed to which live videos",
    "start": "1131760",
    "end": "1139440"
  },
  {
    "text": "similarly note 2 here subscribes to both the red live video and the green live video",
    "start": "1139440",
    "end": "1146080"
  },
  {
    "text": "now let's look at what happens when an event is published",
    "start": "1146080",
    "end": "1151840"
  },
  {
    "text": "so after a few subscriptions let's assume that this is the state of the subscriptions in the real-time dispatcher",
    "start": "1152000",
    "end": "1157600"
  },
  {
    "text": "and note that a single front-end node could be subscribed to more than one live videos right because now it can have connections",
    "start": "1157600",
    "end": "1163360"
  },
  {
    "text": "that are watching multiple live videos at the same time so in this case for example note 2 is subscribed to both",
    "start": "1163360",
    "end": "1168720"
  },
  {
    "text": "the red light video and the green light video so this time the likes back and publishes a like on the green live video",
    "start": "1168720",
    "end": "1176080"
  },
  {
    "text": "to the real-time dispatcher and the dispatcher is able to look up its local subscriptions table",
    "start": "1176080",
    "end": "1181919"
  },
  {
    "text": "to know that nodes two three and five have connections that are subscribed to the green live",
    "start": "1181919",
    "end": "1187600"
  },
  {
    "text": "video and it will dispatch them to those front front-end nodes over a regular http request",
    "start": "1187600",
    "end": "1194960"
  },
  {
    "text": "and what happens next that you've already seen these front-end nodes will look up their own in-memory subscriptions table that",
    "start": "1195039",
    "end": "1201520"
  },
  {
    "text": "is inside them to figure out which of their connections are watching the green light video",
    "start": "1201520",
    "end": "1206799"
  },
  {
    "text": "and dispatch the likes to just those clients make sense all right so we now have this",
    "start": "1206799",
    "end": "1214240"
  },
  {
    "text": "beautiful system where the system is able to dispatch between multiple front end nodes",
    "start": "1214240",
    "end": "1219679"
  },
  {
    "text": "which are then able to dispatch to many many clients that are connected to them and we can scale to almost any number of",
    "start": "1219679",
    "end": "1225360"
  },
  {
    "text": "connections but what is the bottleneck in the system",
    "start": "1225360",
    "end": "1230559"
  },
  {
    "text": "the dispatcher is the bottleneck in the system it never ends so the next challenge is that we have",
    "start": "1231360",
    "end": "1238000"
  },
  {
    "text": "this one node which is which we're calling the dispatcher and if it gets a very high published rate of events then it may not be able",
    "start": "1238000",
    "end": "1244320"
  },
  {
    "text": "to cope up right so that takes us to challenge number five",
    "start": "1244320",
    "end": "1249440"
  },
  {
    "text": "which is a very high rate of likes being published per second so once again how do we solve scaling",
    "start": "1249440",
    "end": "1255440"
  },
  {
    "text": "challenges we had a machine right and i mean",
    "start": "1255440",
    "end": "1260559"
  },
  {
    "text": "engineers just do the most lazy thing and usually works out pretty well so we add another dispatcher node to",
    "start": "1260559",
    "end": "1267039"
  },
  {
    "text": "handle the high rate of likes being published",
    "start": "1267039",
    "end": "1270799"
  },
  {
    "text": "and something important to note here the dispatcher nodes are completely",
    "start": "1272240",
    "end": "1278080"
  },
  {
    "text": "independent of the front-end nodes any front-end node can subscribe to any",
    "start": "1278080",
    "end": "1283120"
  },
  {
    "text": "dispatcher node and any dispatcher node can publish to any front-end node there is no persistent connections here",
    "start": "1283120",
    "end": "1288720"
  },
  {
    "text": "right the persistent connections are all only between front-end nodes and the clients not here",
    "start": "1288720",
    "end": "1295520"
  },
  {
    "text": "and this results in another challenge the subscriptions table can no longer be",
    "start": "1295520",
    "end": "1301760"
  },
  {
    "text": "local to just one dispatcher load right any dispatcher node should be able to access",
    "start": "1301760",
    "end": "1307039"
  },
  {
    "text": "that subscriptions table to figure out which front-end node a particular published event is destined for",
    "start": "1307039",
    "end": "1313360"
  },
  {
    "text": "and secondly i tricked you a little bit before this subscriptions table can't really live in memory in the",
    "start": "1313360",
    "end": "1319280"
  },
  {
    "text": "dispatcher node i mean it can live in the memory in the in the front end node but not in the dispatcher node",
    "start": "1319280",
    "end": "1324960"
  },
  {
    "text": "why because even if a dispatcher node is lost let's say this one just dies",
    "start": "1324960",
    "end": "1330000"
  },
  {
    "text": "then we can't afford to lose this entire subscriptions data right and so for both of these reasons",
    "start": "1330000",
    "end": "1336960"
  },
  {
    "text": "we pull out this subscriptions table into its own key value store",
    "start": "1336960",
    "end": "1342640"
  },
  {
    "text": "which is accessible by any dispatcher node at any time so now when a like is published by the",
    "start": "1342640",
    "end": "1350320"
  },
  {
    "text": "likes backend for the red live video on a random dispatcher node and on the green live video to some",
    "start": "1350320",
    "end": "1356880"
  },
  {
    "text": "other random dispatcher node each of them are able to independently",
    "start": "1356880",
    "end": "1362000"
  },
  {
    "text": "query the subscriptions table that is residing in the key value store and",
    "start": "1362000",
    "end": "1368320"
  },
  {
    "text": "this and and they're able to do that because the subscription table is is completely independent of these",
    "start": "1368320",
    "end": "1374240"
  },
  {
    "text": "dispatcher nodes and they're the data is safe there so another dispatcher nodes dispatch the",
    "start": "1374240",
    "end": "1380720"
  },
  {
    "text": "likes based on what is in the subscriptions table over regular http requests to the frontal",
    "start": "1380720",
    "end": "1386840"
  },
  {
    "text": "loads cool all right so i think we now have",
    "start": "1386840",
    "end": "1393039"
  },
  {
    "text": "all the components to show you how we can do what i promised in the title",
    "start": "1393039",
    "end": "1398320"
  },
  {
    "text": "of the stock anyone remember the title of this talk",
    "start": "1398320",
    "end": "1403360"
  },
  {
    "text": "oh thank god i'm not that boring huh all right so",
    "start": "1405440",
    "end": "1412720"
  },
  {
    "text": "if a hundred likes are published per second by the likes backend to the dispatcher",
    "start": "1412720",
    "end": "1418960"
  },
  {
    "text": "and there are 10k viewers that are watching the live video at the same time then we're effectively distributing",
    "start": "1418960",
    "end": "1424480"
  },
  {
    "text": "a million likes per second so i'm gonna start from the beginning and show you everything in one flow",
    "start": "1424480",
    "end": "1430720"
  },
  {
    "text": "because everyone tells me that i got to repeat myself if i want to make sure that you will remember something when you walk out of the stock so this",
    "start": "1430720",
    "end": "1438240"
  },
  {
    "text": "is how a viewer starts to watch a live video and at this time the viewer is",
    "start": "1438240",
    "end": "1445840"
  },
  {
    "text": "the first thing that the viewer needs to do is subscribe to the frontend node and subscribe to the live video topic",
    "start": "1445840",
    "end": "1452240"
  },
  {
    "text": "that they're currently watching so the client sends a subscription request to the front-end node",
    "start": "1452240",
    "end": "1459840"
  },
  {
    "text": "and the front-end node stores the subscription in the in-memory subscription table",
    "start": "1461279",
    "end": "1467039"
  },
  {
    "text": "and the same happens for all such subscriptions from all the clients",
    "start": "1467120",
    "end": "1473840"
  },
  {
    "text": "so let's go back to our overall diagram perfect so now the subscript the",
    "start": "1474240",
    "end": "1479919"
  },
  {
    "text": "subscription has reached the front end nodes so the front end node as i said before now has to subscribe",
    "start": "1479919",
    "end": "1486240"
  },
  {
    "text": "to the dispatcher nodes because the dispatcher will need to know during the publish step which front-end nodes have connections",
    "start": "1486240",
    "end": "1492960"
  },
  {
    "text": "that are subscribed to particular live video so let's look at that flow the front-end node sends a subscription request to the",
    "start": "1492960",
    "end": "1499520"
  },
  {
    "text": "dispatcher which creates an entry in the key value",
    "start": "1499520",
    "end": "1504960"
  },
  {
    "text": "store that is accessible by any dispatcher node so in this case node 1 has subscribed to live video 1",
    "start": "1504960",
    "end": "1511840"
  },
  {
    "text": "and node 2 is subscribing to live video 2. so this is the end of the subscriptions",
    "start": "1511840",
    "end": "1518320"
  },
  {
    "text": "flow so now we need to look at what happens during the publish flow so the publish flow starts when a viewer",
    "start": "1518320",
    "end": "1525360"
  },
  {
    "text": "starts to actually like a live video and so different viewers are watching",
    "start": "1525360",
    "end": "1530960"
  },
  {
    "text": "different live videos and they are continuously liking them and all these requests are sent over",
    "start": "1530960",
    "end": "1536960"
  },
  {
    "text": "regular http requests to the likes backend which stores them and then dispatches",
    "start": "1536960",
    "end": "1542000"
  },
  {
    "text": "them to the dispatcher so it does so with a regular http",
    "start": "1542000",
    "end": "1548880"
  },
  {
    "text": "request to any random dispatcher node and they look up the subscriptions table to figure out which front-end nodes are",
    "start": "1548880",
    "end": "1555360"
  },
  {
    "text": "subscribed to those likes and dispatch them to the subscribed front-end nodes",
    "start": "1555360",
    "end": "1561679"
  },
  {
    "text": "okay so the likes have now reached the front-end nodes and we have the last step which we began",
    "start": "1564400",
    "end": "1570240"
  },
  {
    "text": "the presentation with they need to send it to the right client devices",
    "start": "1570240",
    "end": "1575600"
  },
  {
    "text": "so each front-end node will look up its local subscriptions table and this is done by the supervisor actor",
    "start": "1575600",
    "end": "1582000"
  },
  {
    "text": "to figure out which actors to send this these like objects to",
    "start": "1582000",
    "end": "1587440"
  },
  {
    "text": "and they will dispatch the likes to the appropriate connections based on what they see in the subscriptions table",
    "start": "1587440",
    "end": "1594640"
  },
  {
    "text": "and done we just distributed a million likes per second with a fairly straightforward and",
    "start": "1595760",
    "end": "1601679"
  },
  {
    "text": "iteratively designed scalable distributed system so this is the system",
    "start": "1601679",
    "end": "1606720"
  },
  {
    "text": "that we call the real-time platform at linkedin by the way it doesn't just distribute",
    "start": "1606720",
    "end": "1612799"
  },
  {
    "text": "likes it can also do comments typing indicators scene receipts all of our",
    "start": "1612799",
    "end": "1618159"
  },
  {
    "text": "instant messaging works on this platform and even presence those green online indicators that you",
    "start": "1618159",
    "end": "1623600"
  },
  {
    "text": "see on linkedin are all driven by this system in real time",
    "start": "1623600",
    "end": "1629120"
  },
  {
    "text": "so everything is great we're really happy and then linkedin adds another data center",
    "start": "1629120",
    "end": "1641840"
  },
  {
    "text": "now this made us really stressed we don't know what to do so we went back",
    "start": "1641919",
    "end": "1649120"
  },
  {
    "text": "to our principle we said okay how can we use our principle to make sure that we can use",
    "start": "1649120",
    "end": "1655360"
  },
  {
    "text": "our existing architecture and make it work with multiple data centers",
    "start": "1655360",
    "end": "1661039"
  },
  {
    "text": "so let's look at that so let's take the scenario where a like is published to a red live",
    "start": "1661039",
    "end": "1668880"
  },
  {
    "text": "video in the first data center so this is dc1 which is",
    "start": "1668880",
    "end": "1674000"
  },
  {
    "text": "assume that this is the first data center and let's also assume that there are no viewers of the red",
    "start": "1674000",
    "end": "1680320"
  },
  {
    "text": "live video in the first data center so this is where remember i spoke about subscriptions in the dispatcher",
    "start": "1680320",
    "end": "1687039"
  },
  {
    "text": "it helps here because now we might prevent a lot of work in dc one because",
    "start": "1687039",
    "end": "1692960"
  },
  {
    "text": "we know whether we have any subscriptions for the red light video in dc one",
    "start": "1692960",
    "end": "1699600"
  },
  {
    "text": "we also know that there are well in this case there are no viewers for the red live video in dc2",
    "start": "1702399",
    "end": "1710399"
  },
  {
    "text": "but there are viewers of the red light video in dc3 right so somehow we need to take this",
    "start": "1710399",
    "end": "1717039"
  },
  {
    "text": "like and send it to this guy over here right really far away",
    "start": "1717039",
    "end": "1722960"
  },
  {
    "text": "so let's start the likes back in gets the like for the red live video from the viewer",
    "start": "1722960",
    "end": "1729279"
  },
  {
    "text": "in dc one and it does exactly what it was doing before right because it's not the likes back ends responsibility it's",
    "start": "1729279",
    "end": "1736320"
  },
  {
    "text": "the platform's responsibility right we are building a platform here and therefore hiding all the complexity",
    "start": "1736320",
    "end": "1741360"
  },
  {
    "text": "of the multiple data centers from the users that are trying to use this platform right",
    "start": "1741360",
    "end": "1746559"
  },
  {
    "text": "so it will just publish the like to the dispatcher in the first data center just like it was doing before nothing changes",
    "start": "1746559",
    "end": "1751840"
  },
  {
    "text": "there and now that the dispatcher in the first",
    "start": "1751840",
    "end": "1757279"
  },
  {
    "text": "data center has received the like the dispatcher will check for any subscriptions again just like before",
    "start": "1757279",
    "end": "1763200"
  },
  {
    "text": "in its local data center and this time it saved a ton of work because there are no viewers of the red",
    "start": "1763200",
    "end": "1768880"
  },
  {
    "text": "light video in dc one but how do we get the like across to all the viewers in the other data centers",
    "start": "1768880",
    "end": "1775679"
  },
  {
    "text": "right like that's the challenge any guesses",
    "start": "1775679",
    "end": "1781840"
  },
  {
    "text": "no no don't add another dispatcher we already have too many dispatchers",
    "start": "1783120",
    "end": "1787679"
  },
  {
    "text": "okay okay so we can do cross-color",
    "start": "1792840",
    "end": "1798799"
  },
  {
    "text": "subscriptions cross data center subscriptions what's another idea",
    "start": "1798799",
    "end": "1804799"
  },
  {
    "text": "good broadcast to any dc and we'll talk a little bit about like the the trade-off between",
    "start": "1806080",
    "end": "1811279"
  },
  {
    "text": "subscribing in uh in across data center fashion versus publishing in a cross data center",
    "start": "1811279",
    "end": "1817840"
  },
  {
    "text": "fashion so it turns out that publishing in a cross data center fashion is better here and we'll talk a little bit about that a little later",
    "start": "1817840",
    "end": "1825600"
  },
  {
    "text": "so yes this is where we do a cross color or a cross data center publish to dispatchers in all of the",
    "start": "1825600",
    "end": "1832799"
  },
  {
    "text": "peer nodes right and this is we're doing that so",
    "start": "1832799",
    "end": "1837840"
  },
  {
    "text": "that we can capture capture viewers that are subscribed to the red live video in all the other data centers so the",
    "start": "1837840",
    "end": "1845200"
  },
  {
    "text": "dispatcher in the first data center simply dispatches the likes to all of its peers dispatchers in all the other data",
    "start": "1845200",
    "end": "1850960"
  },
  {
    "text": "centers and in this case a subscriber is found in dc3",
    "start": "1850960",
    "end": "1856320"
  },
  {
    "text": "but not in dc2 right that's the onl and by the way this dispatcher is doing exactly what it would have done if it",
    "start": "1856320",
    "end": "1862559"
  },
  {
    "text": "received this like locally in this data center there's nothing special that it is doing right it's just that this dispatcher",
    "start": "1862559",
    "end": "1869840"
  },
  {
    "text": "uh distributed the like all over to all the dispatchers in the peer data centers",
    "start": "1869840",
    "end": "1876240"
  },
  {
    "text": "and the viewer in dc3 simply gets the like just like it would normally do because",
    "start": "1876720",
    "end": "1882640"
  },
  {
    "text": "the dispatcher was able to find the subscription information in dc3 and this green this viewer with the",
    "start": "1882640",
    "end": "1890080"
  },
  {
    "text": "green live video does not get anything all right so this is how the platform can support multiple",
    "start": "1890080",
    "end": "1896159"
  },
  {
    "text": "data centers across the globe by keeping subscriptions local to the data center",
    "start": "1896159",
    "end": "1901679"
  },
  {
    "text": "while doing a cross color fanout during publish make sense all right so finally",
    "start": "1901679",
    "end": "1909919"
  },
  {
    "text": "i want to talk a little bit about the performance of the system because like everybody here is here",
    "start": "1909919",
    "end": "1915919"
  },
  {
    "text": "because hey scale right all right so we did this experiment",
    "start": "1915919",
    "end": "1921519"
  },
  {
    "text": "where we kept adding more and more connections to the same front-end machine like we just kept on going and",
    "start": "1921519",
    "end": "1930159"
  },
  {
    "text": "if you wanted to figure out how many persistent connections a single machine can hold so any guesses",
    "start": "1930159",
    "end": "1937039"
  },
  {
    "text": "oh million wow no not that many we also are doing a lot of work",
    "start": "1937679",
    "end": "1945200"
  },
  {
    "text": "sorry yeah so he's very close so turns out that we were able to have a hundred thousand connections on the same",
    "start": "1945279",
    "end": "1951600"
  },
  {
    "text": "machine yes you can go to a million but at the same time because we're also doing all this work and because we use the system not just",
    "start": "1951600",
    "end": "1959039"
  },
  {
    "text": "for distributing likes but also for all the other things that linkedin has we we",
    "start": "1959039",
    "end": "1966320"
  },
  {
    "text": "were able to get to 100 000 connections per front-end machine all right so anyone remember the second",
    "start": "1966320",
    "end": "1972159"
  },
  {
    "text": "largest live stream royal wedding all right so the royal",
    "start": "1972159",
    "end": "1977200"
  },
  {
    "text": "wedding had 18 million viewers at peak and so we could do that oh",
    "start": "1977200",
    "end": "1985200"
  },
  {
    "text": "yeah so we could do that with uh just eight 180 machines right because a single machine can do",
    "start": "1985200",
    "end": "1991519"
  },
  {
    "text": "100 000 connections and so with 180 machines you are able to have persistent connections for all the",
    "start": "1991519",
    "end": "1997600"
  },
  {
    "text": "18 million viewers that are currently streaming the royal",
    "start": "1997600",
    "end": "2002840"
  },
  {
    "text": "wedding of course we just didn't get to this number easily so we hit a bunch of file descriptor",
    "start": "2002840",
    "end": "2008960"
  },
  {
    "text": "limits port exhaustion uh even memory limits and luckily we documented all of that at",
    "start": "2008960",
    "end": "2016080"
  },
  {
    "text": "this link tiny dot cc slash linkedin scaling and i hope that you will be able to get",
    "start": "2016080",
    "end": "2022000"
  },
  {
    "text": "something out of reading something like this because it's it's very interesting it's just like regular scaling challenges",
    "start": "2022000",
    "end": "2027600"
  },
  {
    "text": "it's just that we hit it in in context of trying to expand",
    "start": "2027600",
    "end": "2034000"
  },
  {
    "text": "the number of connections that we could hold on a single machine all right how about other parts of the",
    "start": "2034000",
    "end": "2039279"
  },
  {
    "text": "system how many events per second can be",
    "start": "2039279",
    "end": "2045519"
  },
  {
    "text": "published to the dispatcher node again before you answer this question i",
    "start": "2045519",
    "end": "2051118"
  },
  {
    "text": "want to talk about something really important about the design of the system which makes it massively scalable",
    "start": "2051119",
    "end": "2057839"
  },
  {
    "text": "the dispatcher node only has to publish an incoming event to a maximum",
    "start": "2057839",
    "end": "2065040"
  },
  {
    "text": "of the number of front-end machines right it doesn't have to worry about all the",
    "start": "2065040",
    "end": "2070560"
  },
  {
    "text": "connections that these front-end machines are in turn holding",
    "start": "2070560",
    "end": "2075839"
  },
  {
    "text": "it only cares about this green fan out here which is the number of fronted",
    "start": "2076320",
    "end": "2083358"
  },
  {
    "text": "machines that this dispatcher can possibly publish an event to but it doesn't have to worry about this red fan",
    "start": "2083359",
    "end": "2089358"
  },
  {
    "text": "out because that's the part that the front-end machines are handling and they're doing that",
    "start": "2089359",
    "end": "2094480"
  },
  {
    "text": "with in-memory subscriptions with akka actors which are highly highly efficient at this right okay so now with that context what",
    "start": "2094480",
    "end": "2102160"
  },
  {
    "text": "do you think is the maximum events that you can publish to this dispatcher per second",
    "start": "2102160",
    "end": "2108160"
  },
  {
    "text": "very close that's a very very good guess so turns out for us that number turned out to be",
    "start": "2110079",
    "end": "2115599"
  },
  {
    "text": "close to 5000 so 5000 events can be published per second to the dispatcher node to a single dispatcher node and effectively",
    "start": "2115599",
    "end": "2123040"
  },
  {
    "text": "we can publish a 50 000 likes per second to these front-end machines with just 10 dispatcher machines by the",
    "start": "2123040",
    "end": "2129119"
  },
  {
    "text": "way this is just the first part of the fan out right these 50 000 likes per second will then be fanned",
    "start": "2129119",
    "end": "2136079"
  },
  {
    "text": "out even more by all the front-end machines that are able to do that very very efficiently",
    "start": "2136079",
    "end": "2142000"
  },
  {
    "text": "so that's a multiplicative factor there and that will result in millions of likes being distributed per second",
    "start": "2142000",
    "end": "2149040"
  },
  {
    "text": "so lastly let's look at the time because everybody really really cares about latency right like you're you're building a real-time system",
    "start": "2149040",
    "end": "2155200"
  },
  {
    "text": "so you got to make sure that things are super fast right so let's talk about the end to end",
    "start": "2155200",
    "end": "2161280"
  },
  {
    "text": "latency all right so if we record the time t1",
    "start": "2161280",
    "end": "2166560"
  },
  {
    "text": "at which the likes backend publishes the like to our real-time platform which is the",
    "start": "2166560",
    "end": "2173839"
  },
  {
    "text": "dispatcher machine and we record the time t2 at which point",
    "start": "2173839",
    "end": "2180000"
  },
  {
    "text": "we have sent the like over over the persistent connection to the clients and the reason we're measuring it there",
    "start": "2180000",
    "end": "2186320"
  },
  {
    "text": "is because you can't really control the the latency outside your data center",
    "start": "2186320",
    "end": "2191680"
  },
  {
    "text": "right i mean you have some control over it but that's the one that the platform really really cares about",
    "start": "2191680",
    "end": "2197440"
  },
  {
    "text": "then the delta turns out to be just 75 milliseconds at p90 the system is very fast as there is just",
    "start": "2197440",
    "end": "2206160"
  },
  {
    "text": "one key value look up here right and one in memory look up here and the",
    "start": "2206160",
    "end": "2212880"
  },
  {
    "text": "rest is just network hops and very few network hubs",
    "start": "2212880",
    "end": "2220078"
  },
  {
    "text": "so these are some performance characteristics of the system now this end-to-end latency measurement",
    "start": "2221200",
    "end": "2227599"
  },
  {
    "text": "is also a very interesting thing how do you really do that right",
    "start": "2227599",
    "end": "2232720"
  },
  {
    "text": "because most of you must be familiar with measuring latencies for a request response system",
    "start": "2232720",
    "end": "2238000"
  },
  {
    "text": "like you send an incoming request and the same machine can measure when the response is sent",
    "start": "2238000",
    "end": "2243280"
  },
  {
    "text": "out and therefore you can say that hey it took this much time in this case there are multiple systems involved",
    "start": "2243280",
    "end": "2249200"
  },
  {
    "text": "you're going from the dispatcher to the front end node and then to the client how do you measure latencies for such",
    "start": "2249200",
    "end": "2255680"
  },
  {
    "text": "one-way flows across many many systems so that is also a very interesting problem",
    "start": "2255680",
    "end": "2262480"
  },
  {
    "text": "and we wrote about it we wrote about a system that we built using using near-line processing",
    "start": "2262480",
    "end": "2269760"
  },
  {
    "text": "using samsa so sams is another technology that we use at linkedin and you can use that to understand how",
    "start": "2269760",
    "end": "2276079"
  },
  {
    "text": "latencies uh sorry you can measure latencies across end-to-end systems",
    "start": "2276079",
    "end": "2281280"
  },
  {
    "text": "across many many machines so we wrote about it at tiny dot cc slash linkedin",
    "start": "2281280",
    "end": "2286320"
  },
  {
    "text": "latency don't have the time to dive into it here well i would love to and you should i",
    "start": "2286320",
    "end": "2291440"
  },
  {
    "text": "hope that you get something out of creating something like this and if you have a system where you want to measure latencies across",
    "start": "2291440",
    "end": "2298720"
  },
  {
    "text": "many different parts of the stack you can use something like this to measure latencies",
    "start": "2298720",
    "end": "2305280"
  },
  {
    "text": "so why does the system scale i think it scales because it is uh you can add more front-end",
    "start": "2306160",
    "end": "2312800"
  },
  {
    "text": "machines or more dispatcher machines as your traffic increases it's just completely horizontally scalable",
    "start": "2312800",
    "end": "2320720"
  },
  {
    "text": "the other thing that i mentioned in the beginning of the stock is that we also extended the system to build presence",
    "start": "2322720",
    "end": "2329359"
  },
  {
    "text": "which is this technology where you can understand when somebody goes online and offline and now that we have these persistent",
    "start": "2329359",
    "end": "2335200"
  },
  {
    "text": "connections we know when they were made we know when they were disconnected so we also know when somebody came online",
    "start": "2335200",
    "end": "2341119"
  },
  {
    "text": "and when somebody went offline but it isn't that easy because uh mobile devices",
    "start": "2341119",
    "end": "2348400"
  },
  {
    "text": "are notorious right like they will sometimes just have a bad network they might disconnect",
    "start": "2348400",
    "end": "2353520"
  },
  {
    "text": "and reconnect without any reason so how do we kind of average out or produce all that noise to kind of figure",
    "start": "2353520",
    "end": "2359680"
  },
  {
    "text": "out when somebody's actually online and when they're offline and not just jitter all the way where you keep going",
    "start": "2359680",
    "end": "2364880"
  },
  {
    "text": "offline and online because you have connections and disconnections simply because of the network that you have so we wrote",
    "start": "2364880",
    "end": "2371119"
  },
  {
    "text": "about that at linkedin.cc sorry tiny.cc at slash linkedin presence",
    "start": "2371119",
    "end": "2376160"
  },
  {
    "text": "where we use the concept of persistent connections to understand how uh how somebody goes online and",
    "start": "2376160",
    "end": "2382480"
  },
  {
    "text": "offline and we built the presence technology on top of the exact same platform so i hope that is also useful to you",
    "start": "2382480",
    "end": "2390480"
  },
  {
    "text": "all right so that was probably a lot to consume in the last few minutes so i'll try to see if i can help you remember some of this",
    "start": "2390720",
    "end": "2397920"
  },
  {
    "text": "real-time content delivery real-time content delivery can enable dynamic interactions between users of your apps",
    "start": "2397920",
    "end": "2404319"
  },
  {
    "text": "uh you can do likes you can do comments you can do polls discussions very powerful stuff because",
    "start": "2404319",
    "end": "2410000"
  },
  {
    "text": "it really engages your users and the first piece you need is a persistent connection for that there is",
    "start": "2410000",
    "end": "2416560"
  },
  {
    "text": "built-in support for event source in most browsers and also on most server frameworks",
    "start": "2416560",
    "end": "2422800"
  },
  {
    "text": "so there are also easily available client libraries that you can use for ios and android",
    "start": "2422800",
    "end": "2428319"
  },
  {
    "text": "play and actors are powerful frameworks to manage connections in a very efficient way",
    "start": "2428319",
    "end": "2433920"
  },
  {
    "text": "they can they can allow millions of connections to be managed on on your server side and therefore",
    "start": "2433920",
    "end": "2439760"
  },
  {
    "text": "they can allow millions of viewers to interact with each other so everyone remember our actors way cooler than hollywood actors",
    "start": "2439760",
    "end": "2448480"
  },
  {
    "text": "the principal i started this presentation with that challenges in distributed systems can be solved by starting small",
    "start": "2448480",
    "end": "2454319"
  },
  {
    "text": "solve the first problem and then build on top of it add simple layers in your architecture",
    "start": "2454319",
    "end": "2459440"
  },
  {
    "text": "to solve bigger challenges this is all we did throughout this presentation",
    "start": "2459440",
    "end": "2464480"
  },
  {
    "text": "and when you hit a limit horizontally scaling the system is usually a good idea add a machine distribute your work",
    "start": "2464480",
    "end": "2472480"
  },
  {
    "text": "and the real-time platform that i described to you can be built on almost any server or storage technology",
    "start": "2472480",
    "end": "2477520"
  },
  {
    "text": "you can use node.js you can use python all of these server frameworks support",
    "start": "2477520",
    "end": "2482800"
  },
  {
    "text": "some methodology of maintaining persistent connections and for the key value store you can use",
    "start": "2482800",
    "end": "2488480"
  },
  {
    "text": "couch base redis mongodb anything that makes you the happiest anything that you are already using and",
    "start": "2488480",
    "end": "2495920"
  },
  {
    "text": "so most importantly you can do the same for your app real time interactions are very powerful and i",
    "start": "2495920",
    "end": "2502880"
  },
  {
    "text": "feel that if you use some of the principles that i shared with you you can do some pretty interesting stuff",
    "start": "2502880",
    "end": "2508000"
  },
  {
    "text": "and pretty dynamic experiences in your own apps thank you everyone uh for attending the",
    "start": "2508000",
    "end": "2513520"
  },
  {
    "text": "session i'm a proud indian i work at linkedin in the us and i'm so glad that i got this",
    "start": "2513520",
    "end": "2519200"
  },
  {
    "text": "opportunity to talk to her talk to you here at kubecon london this talk",
    "start": "2519200",
    "end": "2524720"
  },
  {
    "text": "and all of its slides will be available at tiny dot cc slash qcon 2020 i'm assuming very soon",
    "start": "2524720",
    "end": "2531680"
  },
  {
    "text": "and there is also an ama session at 1 40 pm where you can come and ask me anything",
    "start": "2531680",
    "end": "2536720"
  },
  {
    "text": "not just related to this but anything else that you have in your mind and that's happening at guild one more",
    "start": "2536720",
    "end": "2543599"
  },
  {
    "text": "thing so now that we have about nine minutes left i wanted to check on the stream i don't",
    "start": "2543599",
    "end": "2549119"
  },
  {
    "text": "know if it's still playing",
    "start": "2549119",
    "end": "2551838"
  },
  {
    "text": "it is nobody said anything",
    "start": "2554240",
    "end": "2558000"
  },
  {
    "text": "wow all right questions",
    "start": "2560319",
    "end": "2566000"
  },
  {
    "text": "[Applause]",
    "start": "2566220",
    "end": "2570330"
  },
  {
    "text": "it looks like we're going to have all have an orderly queue if you can just come here then i think that's how it works in",
    "start": "2575040",
    "end": "2581520"
  },
  {
    "text": "this one what",
    "start": "2581520",
    "end": "2585520"
  },
  {
    "text": "hello thanks for that very interesting um do you have any full bags for clients that",
    "start": "2589280",
    "end": "2594560"
  },
  {
    "text": "don't support service and events or do you just say modern browsers are our focus here great question so the question was uh do",
    "start": "2594560",
    "end": "2602640"
  },
  {
    "text": "we have any fall backs if servers and events don't work so the beauty of servers and events is",
    "start": "2602640",
    "end": "2608400"
  },
  {
    "text": "that they are literally a regular http request there's absolutely no difference between",
    "start": "2608400",
    "end": "2614160"
  },
  {
    "text": "what a regular http connection would do in fact websockets is something that sometimes gets blocked",
    "start": "2614160",
    "end": "2619760"
  },
  {
    "text": "by firewalls in in certain certain systems and we have never experienced a case",
    "start": "2619760",
    "end": "2625839"
  },
  {
    "text": "where service and events don't work because it's a regular http connection most firewalls will not block it",
    "start": "2625839",
    "end": "2631680"
  },
  {
    "text": "and most clients would understand it and we would like we've never seen a case where service and events doesn't work",
    "start": "2631680",
    "end": "2639760"
  },
  {
    "text": "hello so how do you synchronize your video stream with your likes with time",
    "start": "2640880",
    "end": "2648480"
  },
  {
    "text": "oh i see um so i think the question here is that once these likes have happened how do you make sure",
    "start": "2648480",
    "end": "2655599"
  },
  {
    "text": "that the next time somebody watches this video the likes show up at the same time",
    "start": "2655599",
    "end": "2660800"
  },
  {
    "text": "is that what you're asking yeah and also i guess the video streams are delayed a little",
    "start": "2660800",
    "end": "2666880"
  },
  {
    "text": "bit on different servers and your likes are happening in different places i see i see so yes i think you must have",
    "start": "2666880",
    "end": "2673760"
  },
  {
    "text": "noticed here that there is a delay right and i think the question here is that hey i i liked at moment x",
    "start": "2673760",
    "end": "2680160"
  },
  {
    "text": "but maybe the broadcaster sees it at moment y right so so yes there is a delay and",
    "start": "2680160",
    "end": "2686079"
  },
  {
    "text": "some of it is simply because of natural causes like you just speed of light and the other is that there's also",
    "start": "2686079",
    "end": "2692240"
  },
  {
    "text": "sometimes uh something that we do to make sure that the broadcaster can like cut something off",
    "start": "2692240",
    "end": "2697680"
  },
  {
    "text": "if something is seriously wrong the good thing here is that once",
    "start": "2697680",
    "end": "2703359"
  },
  {
    "text": "somebody has pressed like it will show up to all the viewers almost instantaneously like you should have actually you can",
    "start": "2703359",
    "end": "2709440"
  },
  {
    "text": "actually try it right now if you press like you should actually be able to see it almost immediately",
    "start": "2709440",
    "end": "2715760"
  },
  {
    "text": "so the distribution is real time but yes there may be a delay between",
    "start": "2715760",
    "end": "2720800"
  },
  {
    "text": "when you think that the broadcaster said something versus when you actually liked it",
    "start": "2720800",
    "end": "2727520"
  },
  {
    "text": "and that is natural i think there are also security reasons to do so",
    "start": "2727520",
    "end": "2733838"
  },
  {
    "text": "yeah hi first of all thanks for your talk thank you um my question is do you have any consistency guarantees",
    "start": "2735440",
    "end": "2741040"
  },
  {
    "text": "especially in view of dispatch dispatcher failure even",
    "start": "2741040",
    "end": "2746160"
  },
  {
    "text": "across data centers yes great question what about consistency right like how how do you",
    "start": "2746160",
    "end": "2751599"
  },
  {
    "text": "show guarantees uh how do you make sure that a like will actually get to it",
    "start": "2751599",
    "end": "2756880"
  },
  {
    "text": "get to its destination short answer is that we don't right because in in this case uh what we",
    "start": "2756880",
    "end": "2763520"
  },
  {
    "text": "are going for is speed and we're not going for complete guarantees",
    "start": "2763520",
    "end": "2768880"
  },
  {
    "text": "for whether something will make it to the end having said that we measure everything right we measure the cross color",
    "start": "2768880",
    "end": "2775119"
  },
  {
    "text": "dispatch we measure the dispatchers sending requests to the front ends and we also measure whether",
    "start": "2775119",
    "end": "2780880"
  },
  {
    "text": "something that was sent by the front end was actually received by the client and if we see our",
    "start": "2780880",
    "end": "2787680"
  },
  {
    "text": "four nines or five nines falling we will figure out what the cause is and we will fix it now i do want to share something else",
    "start": "2787680",
    "end": "2794800"
  },
  {
    "text": "now that you asked this question which is",
    "start": "2794800",
    "end": "2803838"
  },
  {
    "text": "kafka right i mean a natural question is like why not just do this with kafka",
    "start": "2804079",
    "end": "2810880"
  },
  {
    "text": "and if we do it with kafka then yes you do get that because the way you would do it with kafka is that the likes back end",
    "start": "2810880",
    "end": "2818160"
  },
  {
    "text": "would publish a like over to a live video topic that is defined in kafka and then each of these front-end",
    "start": "2818160",
    "end": "2824000"
  },
  {
    "text": "machines would be consumers for all the live video topics that are currently right so you already",
    "start": "2824000",
    "end": "2830640"
  },
  {
    "text": "see a little bit of a problem here which is that these front-end servers are now responsible for consuming every",
    "start": "2830640",
    "end": "2836160"
  },
  {
    "text": "single live video topic and each of them needs to consume all of them because you never know which connection",
    "start": "2836160",
    "end": "2841839"
  },
  {
    "text": "is subscribed to which live video and connect it to this front-end server but what this gives you is guarantees",
    "start": "2841839",
    "end": "2848240"
  },
  {
    "text": "right you cannot drop an event anywhere here in the stack but you can drop an event when you send",
    "start": "2848240",
    "end": "2855359"
  },
  {
    "text": "it to the client from the front-end server but you can detect that in fact event source interface provides a built-in",
    "start": "2855359",
    "end": "2862640"
  },
  {
    "text": "uh support for it uh it it has this concept of where you are like it's like a it's like",
    "start": "2862640",
    "end": "2869440"
  },
  {
    "text": "a uh it's like a number that tells you where you are in the stream and then if if that things get dropped",
    "start": "2869440",
    "end": "2875920"
  },
  {
    "text": "the front end's over the next time it connects it will tell you that it was at point x and the front-end server can start consuming",
    "start": "2875920",
    "end": "2881280"
  },
  {
    "text": "from the topic at point x but what you give away here is speed",
    "start": "2881280",
    "end": "2886640"
  },
  {
    "text": "and also the fact that the front-end servers would stop scaling after a while because each of them need to consume",
    "start": "2886640",
    "end": "2892880"
  },
  {
    "text": "from each of these streams and as scale grows that doesn't like as you add front-end machines that doesn't help",
    "start": "2892880",
    "end": "2899119"
  },
  {
    "text": "because each printed machine now needs to still consume all the events from all the kafka topics",
    "start": "2899119",
    "end": "2905040"
  },
  {
    "text": "yeah thanks a lot hello so question related to the so you",
    "start": "2905040",
    "end": "2911359"
  },
  {
    "text": "have 100 connections to the clients yes some other clients are very slow they might not be consuming your data properly on the pipe",
    "start": "2911359",
    "end": "2918319"
  },
  {
    "text": "okay how do you ensure that you don't have memory exhaustion on the servers great question so notice that when the",
    "start": "2918319",
    "end": "2924800"
  },
  {
    "text": "front end server sends the data or has the persistent connection to the client it is actually a fire and forget",
    "start": "2924800",
    "end": "2930960"
  },
  {
    "text": "right the front end server itself is not blocking on sending the data to the client it just shoves it into the pipe and",
    "start": "2930960",
    "end": "2937040"
  },
  {
    "text": "forgets about it so there is no process or no thread that is waiting",
    "start": "2937040",
    "end": "2942319"
  },
  {
    "text": "to figure out whether the data actually went to the client and so therefore no matter what these clients are doing",
    "start": "2942319",
    "end": "2948800"
  },
  {
    "text": "they might be dropping events they might not be accepting it because something is wrong on the client side",
    "start": "2948800",
    "end": "2954160"
  },
  {
    "text": "the front-end server is not impacted by that the front-end server's job is to just dispatch it over the connection and be done with it",
    "start": "2954160",
    "end": "2960240"
  },
  {
    "text": "again because we are going for speed and not for uh yes",
    "start": "2960240",
    "end": "2971839"
  },
  {
    "text": "thank you",
    "start": "2972960",
    "end": "2977040"
  }
]