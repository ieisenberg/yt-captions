[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "that was a great introduction this year I've probably heard from a thousand plus",
    "start": "10480",
    "end": "16000"
  },
  {
    "text": "people that are interested in the topic and talk to a ton of different Enterprise companies who here knows what reddis is hands yep so a lot of",
    "start": "16000",
    "end": "25119"
  },
  {
    "text": "customers who here knows that reddis is a vector database see",
    "start": "25119",
    "end": "30640"
  },
  {
    "text": "so lot of talking to do and that is the point is that I've had to do a lot of",
    "start": "30640",
    "end": "35760"
  },
  {
    "text": "talking to educate people on how you can use rdus in these scenarios and help these customers actually build it and",
    "start": "35760",
    "end": "41239"
  },
  {
    "text": "put it in production um and not always successfully to be completely honest with you but what I'm going to share",
    "start": "41239",
    "end": "48120"
  },
  {
    "text": "today is what I've learned from all of that and this is uh previous talk if",
    "start": "48120",
    "end": "53640"
  },
  {
    "text": "you're at large language models in production the mlops community one this is essentially an advanced version of",
    "start": "53640",
    "end": "60160"
  },
  {
    "text": "that talk since I was asked to be technical this is going to be a lot of technical information I promise to get",
    "start": "60160",
    "end": "65680"
  },
  {
    "text": "through it so there's some questions but um you know please enjoy okay so I'm not going to beat this",
    "start": "65680",
    "end": "72680"
  },
  {
    "text": "one up because you've probably heard about this a ton today large language models are used almost everywhere right",
    "start": "72680",
    "end": "78960"
  },
  {
    "text": "now in all types of applications and they're used for a number of different things from summarization to question",
    "start": "78960",
    "end": "84200"
  },
  {
    "text": "answering and all types and they're often supplied by external knowledge bases called Vector databases",
    "start": "84200",
    "end": "90640"
  },
  {
    "text": "they do this through Vector similarity search and this is something that uh is",
    "start": "90640",
    "end": "95759"
  },
  {
    "text": "to together you can see platforms like Amazon Bedrock being able to string infrastructure together to be able to do",
    "start": "95759",
    "end": "101720"
  },
  {
    "text": "this and so there's a couple things that come along with using large language models",
    "start": "101720",
    "end": "108799"
  },
  {
    "text": "that are going to help us ground why rag exists in the first place and then we're going to build up into how it's actually",
    "start": "108799",
    "end": "115680"
  },
  {
    "text": "used and then how do you do it so cost quality performance and security cost",
    "start": "115680",
    "end": "122399"
  },
  {
    "text": "because large language models are expensive especially generative ones that are really good and quality because",
    "start": "122399",
    "end": "128720"
  },
  {
    "text": "they often make stuff up if you ask them to do things they do it they have no knowledge of right and wrong they are",
    "start": "128720",
    "end": "135400"
  },
  {
    "text": "instruction-based models you instruct them they do and so if you instruct them to do something and they don't know they",
    "start": "135400",
    "end": "142480"
  },
  {
    "text": "will make something up and so their quality is often bad in certain scenarios that you're not necessarily",
    "start": "142480",
    "end": "148640"
  },
  {
    "text": "always thinking about and has been a case of many times of those failures that I was talking about in the systems",
    "start": "148640",
    "end": "155080"
  },
  {
    "text": "that we had and a lot of times people call those hallucinations I don't love that term really it's just wrong",
    "start": "155080",
    "end": "162239"
  },
  {
    "text": "information because it's it kind of personifies the llm to say hallucinations like it's hallucinating",
    "start": "162239",
    "end": "168319"
  },
  {
    "text": "but really it's just wrong it's just not the right answer not the right information performance because does",
    "start": "168319",
    "end": "174840"
  },
  {
    "text": "anybody know what the QPS of a lot of these large language models is queries per second it's like two it's like two",
    "start": "174840",
    "end": "182120"
  },
  {
    "text": "queries per second and you know what the queries per second of a single open source Rance",
    "start": "182120",
    "end": "187440"
  },
  {
    "text": "is it's in the thousands let's put it that way and so you have systems where you",
    "start": "187440",
    "end": "193760"
  },
  {
    "text": "suddenly have a large bottleneck and do you span that llm out and create copies",
    "start": "193760",
    "end": "199799"
  },
  {
    "text": "well what happens to that first point there your cost goes way up and so you have to start thinking about how can I",
    "start": "199799",
    "end": "206040"
  },
  {
    "text": "take these attributes and make them work together and lastly security what happens if you want to deploy a rag system where you have something that is",
    "start": "206040",
    "end": "212680"
  },
  {
    "text": "internal and external right how are you separating that",
    "start": "212680",
    "end": "218080"
  },
  {
    "text": "data okay so this is where I start to say grounding rag we're going to rethink the data strategy okay first everybody",
    "start": "218080",
    "end": "226000"
  },
  {
    "text": "wants this it's a private jat gbt I I've probably built 30 of these this year",
    "start": "226000",
    "end": "231120"
  },
  {
    "text": "it's just chat gbt plus our internal knowledge base of a bunch of PDFs and",
    "start": "231120",
    "end": "236360"
  },
  {
    "text": "notes and whatnot that we have at the company PowerPoints you name it and there's all types of modalities",
    "start": "236360",
    "end": "242120"
  },
  {
    "text": "everybody wants that that's the goal now do I fine-tune to get this do I take all of that data and then create the let's",
    "start": "242120",
    "end": "249920"
  },
  {
    "text": "say the reddis llm on all of our internal docs I could and it would certainly become better but usually",
    "start": "249920",
    "end": "256160"
  },
  {
    "text": "fine-tuning is much better for Behavioral changes and how it speaks how it acts rather than what it knows and",
    "start": "256160",
    "end": "263440"
  },
  {
    "text": "this can also lead to security issues so that internal external that I talked about what happens when you want to use",
    "start": "263440",
    "end": "269240"
  },
  {
    "text": "the same l LM that knowledge is in its parameter set you can't then ask it to go be an external llm CU it has that",
    "start": "269240",
    "end": "276600"
  },
  {
    "text": "knowledge if prompted correctly it will say that information so do you just feed",
    "start": "276600",
    "end": "282520"
  },
  {
    "text": "everything into the context window just say okay I'm just going to slam all this information in at runtime well first of",
    "start": "282520",
    "end": "288919"
  },
  {
    "text": "all you have the limitation of even in the best model is 32k but what happens then when your cost start to go up and",
    "start": "288919",
    "end": "295240"
  },
  {
    "text": "then on top of that relevance if you say that this is a context for the question",
    "start": "295240",
    "end": "301120"
  },
  {
    "text": "that you're supposed to answer and then there's IR irrelevant context that model is going to use that",
    "start": "301120",
    "end": "308039"
  },
  {
    "text": "context that's irrelevant to answer the question you're instructing it to do so and so there's a middle",
    "start": "308039",
    "end": "315680"
  },
  {
    "text": "ground and this is where we're going to talk about Vector databases and this is kind of how I talk about this because a",
    "start": "315680",
    "end": "322039"
  },
  {
    "text": "lot of times it's not either or you do one and then maybe you do another you find tuna model to change how it acts or",
    "start": "322039",
    "end": "328880"
  },
  {
    "text": "how it talks and then you also have an external knowledge basee in the form of a vector database what do Vector",
    "start": "328880",
    "end": "334440"
  },
  {
    "text": "databases do they perform Vector search Vector similarity search if you want to",
    "start": "334440",
    "end": "339479"
  },
  {
    "text": "expand it out this is an oversimplified Vector space that you see here it's obviously not two-dimensional these",
    "start": "339479",
    "end": "345160"
  },
  {
    "text": "vectors are of 15 36 dimensions in the case of open Ai and they're often you",
    "start": "345160",
    "end": "350400"
  },
  {
    "text": "know there's a lot of times they're even bigger than that honestly if you go on hugging face um but what you see here is",
    "start": "350400",
    "end": "356479"
  },
  {
    "text": "a semantic surge space of three sentences it's today's sunny day that is a very happy person that is very happy",
    "start": "356479",
    "end": "362000"
  },
  {
    "text": "dog and then you have a query okay and that query is that is a happy person and what you want to calculate is how far is",
    "start": "362000",
    "end": "369039"
  },
  {
    "text": "that sentence in its semantic representation from the other sentences in my search space and you do this by",
    "start": "369039",
    "end": "376479"
  },
  {
    "text": "having those vectors and doing the same thing you did in fifth grade when they taught you soaa which is cosine cosine",
    "start": "376479",
    "end": "382280"
  },
  {
    "text": "similarity between the two angles in that search space and so then you get a distance a vector distance this sentence",
    "start": "382280",
    "end": "389919"
  },
  {
    "text": "is this Vector distance away from this sentence and I always do this because really that is all you're doing and that",
    "start": "389919",
    "end": "395919"
  },
  {
    "text": "is an extremely efficient computation that's why these things can have billions of vectors and they can perform",
    "start": "395919",
    "end": "402800"
  },
  {
    "text": "billion scale Vector similarity search because they're doing a very very simple mathematical operation and in this case",
    "start": "402800",
    "end": "410680"
  },
  {
    "text": "it's only three vectors but you can scale these into the millions hundreds of millions billions and that's because",
    "start": "410680",
    "end": "417360"
  },
  {
    "text": "these representations are very computation efficient think about taking a extremely large paragraph and boiling",
    "start": "417360",
    "end": "423319"
  },
  {
    "text": "that down into a vector with 1536 floating 32 numbers you're going from",
    "start": "423319",
    "end": "429199"
  },
  {
    "text": "megabytes to kilobytes and so that representation is more compact space efficient and in",
    "start": "429199",
    "end": "436919"
  },
  {
    "text": "runtime more computationally efficient and on top of that it's better than things like bm25 because it accounts for",
    "start": "436919",
    "end": "444599"
  },
  {
    "text": "things like synonyms if you say brother and you say relative a bm25 is not going to count those two words as the same but",
    "start": "444599",
    "end": "451720"
  },
  {
    "text": "a semantic search that has the knowledge of a model that's read all of Wikipedia and all of Reddit encoded into that",
    "start": "451720",
    "end": "457919"
  },
  {
    "text": "semantic Vector we'll know that relative and brother belong to the same category at least relatively pun intended of",
    "start": "457919",
    "end": "465800"
  },
  {
    "text": "words now what do Vector databases do they essentially just put this operation",
    "start": "465800",
    "end": "471400"
  },
  {
    "text": "into production and I won't spend a lot of time on this because I got a lot to get through so the one thing you should know here as I mentioned earlier in the",
    "start": "471400",
    "end": "477440"
  },
  {
    "text": "talk rdus is a great Vector database I won't talk a ton about why rdus is a great Vector database today but if you",
    "start": "477440",
    "end": "482680"
  },
  {
    "text": "want to know come talk to me afterwards so what is retrieval",
    "start": "482680",
    "end": "487759"
  },
  {
    "text": "augmented generation how do I use a vector database as a knowledge base well I always love this one because",
    "start": "487759",
    "end": "495479"
  },
  {
    "text": "a lot of people are like oh veter databases everybody wants them you know everybody the the Market's so flooded",
    "start": "495479",
    "end": "501000"
  },
  {
    "text": "and the truth is is that this is why is that you have people like sko putting out stats that 80 they believe 88% of",
    "start": "501000",
    "end": "507120"
  },
  {
    "text": "these large language model applications are going to use a retrieval Mech M whether it's a vector database or not and so in this case you can see they're",
    "start": "507120",
    "end": "514320"
  },
  {
    "text": "saying it's it's doing those things that I talked about that middle ground between those two operations where",
    "start": "514320",
    "end": "519599"
  },
  {
    "text": "people do use large language models and so that middle ground is filled by",
    "start": "519599",
    "end": "524640"
  },
  {
    "text": "retrieval in this case so what is the process this is really simple definitely",
    "start": "524640",
    "end": "531000"
  },
  {
    "text": "oversimplified diagram but you have a user query let's say what is redus that goes to an embedding model that",
    "start": "531000",
    "end": "536600"
  },
  {
    "text": "embedding model then gives you a list of numbers and it's just simple floating Point 32 you know B float 16 if you're",
    "start": "536600",
    "end": "543600"
  },
  {
    "text": "doing an optimize or a quantized approach and then you have a vector search which using that embedding",
    "start": "543600",
    "end": "549600"
  },
  {
    "text": "returns passages of text in this case I'm saying documents just imagine it returns you a bunch of PDFs that are",
    "start": "549600",
    "end": "555440"
  },
  {
    "text": "relevant to the user query and then you create a prompt says here's a question",
    "start": "555440",
    "end": "561600"
  },
  {
    "text": "here's the context now answer that question and that's all rag is that's the entire concept it's saying I have",
    "start": "561600",
    "end": "569640"
  },
  {
    "text": "and it doesn't necessarily have to be Q&A by the way and I'll show you that later but in that case all I'm saying is",
    "start": "569640",
    "end": "575320"
  },
  {
    "text": "I need more information than present in the large language model to do something whether it's summarization question",
    "start": "575320",
    "end": "581720"
  },
  {
    "text": "answering what have you you might use something like coherer hugging face to use the embedding model",
    "start": "581720",
    "end": "587920"
  },
  {
    "text": "obviously you'd use redus as your vector database and you might use something like open AI because their generative",
    "start": "587920",
    "end": "593360"
  },
  {
    "text": "models are currently really good if you look at benchmarks and then to chain this all together you might use",
    "start": "593360",
    "end": "599800"
  },
  {
    "text": "something like Lang chain or if you know you really want a high level guey you might use relevance or llama index with",
    "start": "599800",
    "end": "606040"
  },
  {
    "text": "Jerry Lou's company so the principle here and I it's",
    "start": "606040",
    "end": "613120"
  },
  {
    "text": "it's really simple more relevancy more relevant context you get a better answer",
    "start": "613120",
    "end": "619640"
  },
  {
    "text": "and that's one of the most important things about it there's a thing called a range query which does uh if you were",
    "start": "619640",
    "end": "624880"
  },
  {
    "text": "looking at that uh space that diagram earlier it you can specify Vector distance a range away from it such that",
    "start": "624880",
    "end": "632279"
  },
  {
    "text": "if you don't get any context if it's not within that range if it's not similar",
    "start": "632279",
    "end": "637519"
  },
  {
    "text": "enough then you can say don't say anything you don't know anything if you want that model to be strictly bounded",
    "start": "637519",
    "end": "643760"
  },
  {
    "text": "to that surf space then no hallucinations because it's only ever going to generate something with that",
    "start": "643760",
    "end": "649320"
  },
  {
    "text": "context if it gets retrieved and that's relying on the retrieval you're then flipping the problem around and saying",
    "start": "649320",
    "end": "655279"
  },
  {
    "text": "I'm relying completely on my retrieval process in order to generate this information which leads you to just have",
    "start": "655279",
    "end": "661880"
  },
  {
    "text": "to curate the retrieval process and then you can rely on other people for the llm",
    "start": "661880",
    "end": "668000"
  },
  {
    "text": "sometimes the benefits of this are it's cheaper and faster than fine- tuning it's better security like we mentioned",
    "start": "668000",
    "end": "673680"
  },
  {
    "text": "with fine- tuning earlier in a case of databases like redis or even some other Vector databases you can update this in",
    "start": "673680",
    "end": "679720"
  },
  {
    "text": "real time imagine if you had a network of sensor data and you were an incident engineer and something happened and you",
    "start": "679720",
    "end": "685000"
  },
  {
    "text": "said what just happened to machine 4 that data has to be new you can't refine",
    "start": "685000",
    "end": "690320"
  },
  {
    "text": "tune on that you can't stuff all of that information into the context window it has to be in a real-time data platform",
    "start": "690320",
    "end": "696800"
  },
  {
    "text": "ready to go and then lastly like I was mentioning it allows you to have multi-tenancy it allows you to separate",
    "start": "696800",
    "end": "704240"
  },
  {
    "text": "these users from those users that company from this company and those documents from those documents and that",
    "start": "704240",
    "end": "711040"
  },
  {
    "text": "is a really important part of actually having knowledge in these systems so as",
    "start": "711040",
    "end": "716680"
  },
  {
    "text": "I mentioned it's used for more than just question and answering although currently a lot of them seem like",
    "start": "716680",
    "end": "722200"
  },
  {
    "text": "question and answering but there's more than uh you know just that in the system right",
    "start": "722200",
    "end": "728720"
  },
  {
    "text": "now okay summarization is also really important if you have a we have an",
    "start": "728720",
    "end": "734120"
  },
  {
    "text": "archive demo that I'll show later and you can take a bunch of archive papers and say hey I don't really understand this summarize it in English that's",
    "start": "734120",
    "end": "740120"
  },
  {
    "text": "essentially what it does and then you have a the customer service applications that say users oh hey uh what was the",
    "start": "740120",
    "end": "747079"
  },
  {
    "text": "last thing I ordered I forgot what it was called and then you use a feature store to inject their last order right into the",
    "start": "747079",
    "end": "753440"
  },
  {
    "text": "prompt so that a customer service box can tell your users exactly the last thing they ordered and do so in a",
    "start": "753440",
    "end": "760160"
  },
  {
    "text": "reasonable amount of time they never have to call an operator again you never have to sit there and go agent agent",
    "start": "760160",
    "end": "765800"
  },
  {
    "text": "agent ever again I mean that's significantly better in terms of user experience and these are the types of",
    "start": "765800",
    "end": "771440"
  },
  {
    "text": "things that people get really excited about is the change of user experience from one methodology to a whole",
    "start": "771440",
    "end": "777279"
  },
  {
    "text": "completely new way of doing it okay so taking a step back what really matters here how do I take these systems",
    "start": "777279",
    "end": "783720"
  },
  {
    "text": "to prod you roughly understand the process now I want to talk about two abstraction levels first a service level",
    "start": "783720",
    "end": "792399"
  },
  {
    "text": "okay and this is the service around the rag system everybody wants to talk about",
    "start": "792399",
    "end": "797920"
  },
  {
    "text": "the one on the right the very kind of simp the in my mind somewhat simpler system because of the results it's",
    "start": "797920",
    "end": "804399"
  },
  {
    "text": "really flashy it's cool these lolms produce great content and it's interesting",
    "start": "804399",
    "end": "810240"
  },
  {
    "text": "yet these systems are still hard to put into prop you still have to do things like caching and monitoring and we're going to talk a little bit about how",
    "start": "810240",
    "end": "817839"
  },
  {
    "text": "it's more like this the service encompasses the rag system and it may even be like this where you may have",
    "start": "817839",
    "end": "823120"
  },
  {
    "text": "multiple rag applications deployed on the same infrastructure and we'll show examples of how to reduce the",
    "start": "823120",
    "end": "829560"
  },
  {
    "text": "complexities there so first we're going to talk though about the rag level system this is an example of Q&A system",
    "start": "829560",
    "end": "837800"
  },
  {
    "text": "and this is also on the red as V GitHub which you can go check out but there are essentially two processes here that you have to accomplish and this is an",
    "start": "837800",
    "end": "843720"
  },
  {
    "text": "oversimplification again but I'm going to get more complicated as I go first you have a background process the",
    "start": "843720",
    "end": "848759"
  },
  {
    "text": "background process is I take some number of documents let's assume that I take the entire Corpus of the document for",
    "start": "848759",
    "end": "854440"
  },
  {
    "text": "now I use an embedding model to create a vector and then I put that into my Vector database and Associate that",
    "start": "854440",
    "end": "860279"
  },
  {
    "text": "Vector with some text that it was used to create it and then there is the online system the online system is where",
    "start": "860279",
    "end": "866519"
  },
  {
    "text": "a user enters that query that query then gets embedded just like you did before that embedding is used to look up",
    "start": "866519",
    "end": "873279"
  },
  {
    "text": "information text in the vector database by performing Vector search and returning the associated text and then",
    "start": "873279",
    "end": "879560"
  },
  {
    "text": "you send that in a newly constructed prompt to something like open AI to perform a generation this again is an",
    "start": "879560",
    "end": "885680"
  },
  {
    "text": "oversimplified process but this is really the kind of high level of the rag level",
    "start": "885680",
    "end": "892079"
  },
  {
    "text": "system now let's talk about some specifics okay I just said we'd take the",
    "start": "892079",
    "end": "897880"
  },
  {
    "text": "entire Corpus of the doc document you almost never do that almost never and the reason is is because of what I was",
    "start": "897880",
    "end": "904440"
  },
  {
    "text": "talking about earlier it's hard to search semantically over an entire PDF there are different meanings in an",
    "start": "904440",
    "end": "910360"
  },
  {
    "text": "entire Corpus of words and so if you have a user query even if you're using",
    "start": "910360",
    "end": "915440"
  },
  {
    "text": "like an approach I'll show later that query doesn't look like almost anything in that document and especially at the",
    "start": "915440",
    "end": "922680"
  },
  {
    "text": "size of that document even if you're taking that entire document so in this case the simple approach would be just",
    "start": "922680",
    "end": "929440"
  },
  {
    "text": "take that raw text and use it for the embeddings and then have that system we described before it looks something like",
    "start": "929440",
    "end": "935480"
  },
  {
    "text": "this okay now the problem with approach as I mentioned is the unrelated context",
    "start": "935480",
    "end": "942600"
  },
  {
    "text": "surrounding the text that you actually care about and that makes the retrieval",
    "start": "942600",
    "end": "947800"
  },
  {
    "text": "and as in I said in most of these rag systems you're actually relying on retrieval and so the retrieval is",
    "start": "947800",
    "end": "953880"
  },
  {
    "text": "actually the most important part if you're using a vector database like this and so that filler text actually really",
    "start": "953880",
    "end": "960959"
  },
  {
    "text": "degrades the search in terms of even recall like very measurable statistics",
    "start": "960959",
    "end": "966639"
  },
  {
    "text": "and so instead we want to do some other things this one's courtesy of Jerry Lou at llama index great guy instead you",
    "start": "966639",
    "end": "973279"
  },
  {
    "text": "could take that entire PDF and create a document summary think about like an abstract of a paper that is much more",
    "start": "973279",
    "end": "979639"
  },
  {
    "text": "concise the semantics of that information are much broader they're much richer they're much closer to what",
    "start": "979639",
    "end": "985240"
  },
  {
    "text": "a user query may be even if you do have specific you can then use the summaries as the",
    "start": "985240",
    "end": "991880"
  },
  {
    "text": "highle Retriever and then do another semantic search through the chunks",
    "start": "991880",
    "end": "997240"
  },
  {
    "text": "retrieved from that summary so these are actually two separate Vector searches that are going to happen first you use",
    "start": "997240",
    "end": "1004440"
  },
  {
    "text": "the summaries and do a vector search across the summaries and then you return",
    "start": "1004440",
    "end": "1009519"
  },
  {
    "text": "the document chunks and do either local or in database if you can do chained operations in database Vector search to",
    "start": "1009519",
    "end": "1016639"
  },
  {
    "text": "retrieve between those chunks so in the paper example we use abstracts and then",
    "start": "1016639",
    "end": "1022480"
  },
  {
    "text": "we go through the entire paper imagine like do you want to talk to this paper on you're on archive.com but instead of",
    "start": "1022480",
    "end": "1029000"
  },
  {
    "text": "a bm25 text bar at the top right it's semantic search through the abstracts and titles and then you can click on the",
    "start": "1029000",
    "end": "1035600"
  },
  {
    "text": "paper and say hey I've got a question about this and what it's going to do is only semantic search through that paper",
    "start": "1035600",
    "end": "1042240"
  },
  {
    "text": "there's a great feature in R is called temporary indices to do that by the way anyway um next you can also",
    "start": "1042240",
    "end": "1049360"
  },
  {
    "text": "spend a bunch of money on embeddings so this approach you take uh something smaller something like every sentence",
    "start": "1049360",
    "end": "1056880"
  },
  {
    "text": "and then you take all of the context around it as what you return instead so",
    "start": "1056880",
    "end": "1062559"
  },
  {
    "text": "you're only ever doing semantic search on that specific sentence right and so that query is much more likely to be",
    "start": "1062559",
    "end": "1068360"
  },
  {
    "text": "similar to one specific sentence which does improve the retrieval but then for",
    "start": "1068360",
    "end": "1073799"
  },
  {
    "text": "the actual context retrieve for the llm you return much more surrounding text",
    "start": "1073799",
    "end": "1079760"
  },
  {
    "text": "and so you can do this through things like overlap but essentially uh this is used in cases where more context is",
    "start": "1079760",
    "end": "1086280"
  },
  {
    "text": "often needed so things where more information is going to be better as I mentioned earlier that's not always the",
    "start": "1086280",
    "end": "1092240"
  },
  {
    "text": "case and so going through these data preparation strategies and going through your problem what is my user query going",
    "start": "1092240",
    "end": "1098159"
  },
  {
    "text": "to look like what is the endup system going to look like in terms of the retrieval how do I test and evaluate",
    "start": "1098159",
    "end": "1103480"
  },
  {
    "text": "that retrieval and all of those steps matter for data preparation because it basically all starts there and your",
    "start": "1103480",
    "end": "1110600"
  },
  {
    "text": "quality is going to largely depend on data preparation which is why it's my first slide and this is the archive rag demo",
    "start": "1110600",
    "end": "1117280"
  },
  {
    "text": "that I was talking about um it's right now the deployed one is just semantic",
    "start": "1117280",
    "end": "1123000"
  },
  {
    "text": "search um and you can use open Ai and hugging face because we have two indices up there um but if you want to look at",
    "start": "1123000",
    "end": "1129240"
  },
  {
    "text": "the rag one it's on red as Ventures um okay so that would just be",
    "start": "1129240",
    "end": "1134440"
  },
  {
    "text": "sematic search to the abstracts if you want to go check it out it's hosted it's a small ac2 server though so be nice um",
    "start": "1134440",
    "end": "1140960"
  },
  {
    "text": "hybrid quering probably the most important feature of vector databases and it's ironic because it's the feature",
    "start": "1140960",
    "end": "1146320"
  },
  {
    "text": "that doesn't actually use Vector search and so this this example here let's say I have two sets of documents documents",
    "start": "1146320",
    "end": "1153360"
  },
  {
    "text": "written by Paul Graham and documents written by David Zachs and then in those documents I have pieces of text from",
    "start": "1153360",
    "end": "1160000"
  },
  {
    "text": "those documents and the problem is is what if I only want information from",
    "start": "1160000",
    "end": "1165520"
  },
  {
    "text": "what did I say David sachs's documents or uh Paul Graham whoops what if I only had",
    "start": "1165520",
    "end": "1171559"
  },
  {
    "text": "articles written by Paul Graham semantic search isn't going to be good at that what if in a lot of those passages of",
    "start": "1171559",
    "end": "1179720"
  },
  {
    "text": "David Sachs Paul Graham is mentioned right then you have a problem so hybrid",
    "start": "1179720",
    "end": "1185080"
  },
  {
    "text": "search allows you to do things like use bm25 like keyword frequency which I just",
    "start": "1185080",
    "end": "1190480"
  },
  {
    "text": "spent a long time saying wasn't good in terms of vector search you can use them together and so you can actually do a",
    "start": "1190480",
    "end": "1197159"
  },
  {
    "text": "it's like a pre-filter you can say I actually only want documents written by Paul Graham and now do my Vector search",
    "start": "1197159",
    "end": "1204039"
  },
  {
    "text": "and at least in Redd you can also swap that operation and so this allows you to say oh let me get this subset of users",
    "start": "1204039",
    "end": "1210720"
  },
  {
    "text": "or I want that user's documents or that user's conversations or that customer's orders or that and allows you to do this",
    "start": "1210720",
    "end": "1216919"
  },
  {
    "text": "with text tag Fields bm25 geographic regions and radiuses or polygons those",
    "start": "1216919",
    "end": "1223120"
  },
  {
    "text": "kinds of features are really important when you're someone who I don't know delivers food and you have a a radius",
    "start": "1223120",
    "end": "1229360"
  },
  {
    "text": "around your store that says oh should I search through these specific products in this particular store and that's what",
    "start": "1229360",
    "end": "1235440"
  },
  {
    "text": "hybrid search allows you to do so let's give you an example of this this is the rag one so um we're using",
    "start": "1235440",
    "end": "1242159"
  },
  {
    "text": "Lang chain here fangs Harrison and we're just going to pull papers from archive this actually just uses bm25 okay so",
    "start": "1242159",
    "end": "1247760"
  },
  {
    "text": "this is very simple just bm25 and it's going to use the archive loader and it's going to say load me 20 documents about",
    "start": "1247760",
    "end": "1254520"
  },
  {
    "text": "retrieval augmented generation just going to check my time okay um and those are going to get they're going",
    "start": "1254520",
    "end": "1261440"
  },
  {
    "text": "to get raw documents back so these are PDFs pars with like Pi PDF or pied PDF or something like that and then we're",
    "start": "1261440",
    "end": "1268039"
  },
  {
    "text": "going to index them into red doing this with open AI embeddings now usually you don't actually have to use open AI embeddings there's a lot of other",
    "start": "1268039",
    "end": "1273799"
  },
  {
    "text": "embedding providers but it's the most recognizable so I use it at talks so",
    "start": "1273799",
    "end": "1278880"
  },
  {
    "text": "then it's really easy actually using Lang chain this all loads up all the documents and all the metadata and I just recently implemented automatic meta",
    "start": "1278880",
    "end": "1286080"
  },
  {
    "text": "metadata generation so in this case if you use the archive loader of Lang chain every time it's going to load you",
    "start": "1286080",
    "end": "1293200"
  },
  {
    "text": "see load all metadata true it's going to load all of these categories so then see if the laser pointer works all of these",
    "start": "1293200",
    "end": "1300720"
  },
  {
    "text": "fields I can then do hybrid search on so now in my rag I can",
    "start": "1300720",
    "end": "1307360"
  },
  {
    "text": "say oh well get to Red SP in my rag I can say filter by the category in year",
    "start": "1307360",
    "end": "1314320"
  },
  {
    "text": "okay and so now I could say I only want machine learning papers cslg and I only want them to be published in something",
    "start": "1314320",
    "end": "1321080"
  },
  {
    "text": "that starts with 200 2020 something so anything in 2020s",
    "start": "1321080",
    "end": "1327520"
  },
  {
    "text": "and Beyond right and then I realized I said 2023 there it's anything in 2020",
    "start": "1327520",
    "end": "1332960"
  },
  {
    "text": "it's a fuzzy Tech search um but then you can combine those by saying and and",
    "start": "1332960",
    "end": "1339919"
  },
  {
    "text": "that's really cool so this is all new features of flying chain and so you can combine those two filters together using",
    "start": "1339919",
    "end": "1346360"
  },
  {
    "text": "bullan operators and you can do this arbitrarily WR and so you can have SQL like Expressions now in your rag system",
    "start": "1346360",
    "end": "1354200"
  },
  {
    "text": "to do a hybrid search which is super cool um and then you get your results",
    "start": "1354200",
    "end": "1359640"
  },
  {
    "text": "and so here it's going to be doing relevancy scores which is going to be one minus the cosine distance in this case and so that would be the score and",
    "start": "1359640",
    "end": "1366120"
  },
  {
    "text": "so this is how simil similar those documents were to my query and so you can see retrieval augmented generation",
    "start": "1366120",
    "end": "1373080"
  },
  {
    "text": "is the top paper um and I'll go back and show you that that is the query that we wrote",
    "start": "1373080",
    "end": "1379840"
  },
  {
    "text": "and now you might say that seems similar to something that bm25 would do right but in this case we're able to do this",
    "start": "1379840",
    "end": "1387240"
  },
  {
    "text": "across all different types of documents and we're able to say that you only want a specific range of semantic similarity",
    "start": "1387240",
    "end": "1394159"
  },
  {
    "text": "included and so I'm only ever going to be returning documents that have a certain level of semantic similarity to",
    "start": "1394159",
    "end": "1401000"
  },
  {
    "text": "that specific query and inhance and retri uh enhancing my retrieval system and hence my llm application so also",
    "start": "1401000",
    "end": "1408960"
  },
  {
    "text": "this is redis VL it's a command line tool to look at your uh redis and library to look at your uh you know",
    "start": "1408960",
    "end": "1414720"
  },
  {
    "text": "redis schema is what I'm doing right there it's really cool it's purpose built I don't know who built it but he's probably really cool um",
    "start": "1414720",
    "end": "1422600"
  },
  {
    "text": "and next hybrid queries and Lang chain just talked about this a little bit but you can see a bunch of examples there um",
    "start": "1422600",
    "end": "1428919"
  },
  {
    "text": "I'm going to make sure I get to the rest of this so I'm actually going to skip over this a little bit okay next approach hide one of the more fun ones",
    "start": "1428919",
    "end": "1435720"
  },
  {
    "text": "okay using fake answers to look up context so like let's think of a Q&A system here",
    "start": "1435720",
    "end": "1441440"
  },
  {
    "text": "I'm going to have a user's question then I'm going to use an llm at first to generate a fake answer now why would I",
    "start": "1441440",
    "end": "1448880"
  },
  {
    "text": "do that a fake answer is often more semantically similar to a real answer",
    "start": "1448880",
    "end": "1455520"
  },
  {
    "text": "than a query think about it like what is redus and an llm generated answer to",
    "start": "1455520",
    "end": "1462000"
  },
  {
    "text": "what is redus something like an inmemory database that can be an awesome Vector database is the second part of that that",
    "start": "1462000",
    "end": "1468679"
  },
  {
    "text": "answer is more semantically similar to what I'm looking for in terms of context",
    "start": "1468679",
    "end": "1474600"
  },
  {
    "text": "than what is redus as a query and so if I'm using that to create an embedding",
    "start": "1474600",
    "end": "1480039"
  },
  {
    "text": "and do my Vector search then often times it's not as good as if I actually have",
    "start": "1480039",
    "end": "1485559"
  },
  {
    "text": "an llm purposefully hallucinate an answer and then create an embedding from",
    "start": "1485559",
    "end": "1491279"
  },
  {
    "text": "that hallucinated answer and search with that and it's a really cool technique it",
    "start": "1491279",
    "end": "1497399"
  },
  {
    "text": "doesn't always work and it's for specific situations but it can be really",
    "start": "1497399",
    "end": "1503120"
  },
  {
    "text": "impactful to retrieval and this is actually I think uh Jerry and Harrison have both sorry uh llama index and Lang",
    "start": "1503120",
    "end": "1508880"
  },
  {
    "text": "chain have both implemented this now and so you can use this like hide like approach that will do it for you",
    "start": "1508880",
    "end": "1515000"
  },
  {
    "text": "automatically now this is really slow it pings an llm twice so you got to be able to have like a 30- second response time",
    "start": "1515000",
    "end": "1522000"
  },
  {
    "text": "and I can show you an app about this but um you should have it as often a backup",
    "start": "1522000",
    "end": "1527279"
  },
  {
    "text": "plan for when something like no context retrieved so you do a vector search right which is often really fast cheap",
    "start": "1527279",
    "end": "1533760"
  },
  {
    "text": "embedding model Vector search I didn't get anything what do I do in the background you can be doing an",
    "start": "1533760",
    "end": "1539039"
  },
  {
    "text": "asynchronous call to a hide service that is running that and so while you kick",
    "start": "1539039",
    "end": "1544360"
  },
  {
    "text": "you kick them off at the same time and then in the background if you don't retrieve any context you can return that",
    "start": "1544360",
    "end": "1550240"
  },
  {
    "text": "hide answer and then that's a really efficient way to say and that's how we speed up a lot of the Hide ones is that",
    "start": "1550240",
    "end": "1555679"
  },
  {
    "text": "it's like your backup plan in case none of the context gets retrieved and so you run concurrently like an a async iio",
    "start": "1555679",
    "end": "1562399"
  },
  {
    "text": "gather or something like that okay I'm going to show you an example of a hide one that's really funny I built this for fun this had",
    "start": "1562399",
    "end": "1569960"
  },
  {
    "text": "nothing no practical commercial application but that really shows off the hide approach pretty well so this is",
    "start": "1569960",
    "end": "1576440"
  },
  {
    "text": "online you can get on the red ventur GitHub uh so I'm not going to show a lot of this but I'm going to you see the",
    "start": "1576440",
    "end": "1581480"
  },
  {
    "text": "separation of offline and online that I was talking about here so the bottom part there's a line bottom part is",
    "start": "1581480",
    "end": "1587440"
  },
  {
    "text": "offline top Parts online offline we're creating embeddings and sometimes that is online too that you have to do you",
    "start": "1587440",
    "end": "1593399"
  },
  {
    "text": "know uh constant updating but I like to separate those two online in this case",
    "start": "1593399",
    "end": "1599360"
  },
  {
    "text": "we have a streamlit guey we have a generative model an embedding model and then we have a fast API back end and",
    "start": "1599360",
    "end": "1605600"
  },
  {
    "text": "redus that's it and those those things allow us to create a process that uses",
    "start": "1605600",
    "end": "1612279"
  },
  {
    "text": "hide to generate a recommendation for a hotel now how does it do this it's going",
    "start": "1612279",
    "end": "1618600"
  },
  {
    "text": "to take user inputs positive and negative qualities of a hotel so I'm",
    "start": "1618600",
    "end": "1623919"
  },
  {
    "text": "going to say something like nice amenities like a pool in a gym and what do you want to avoid in a hotel me and",
    "start": "1623919",
    "end": "1631000"
  },
  {
    "text": "staff okay and then it's asked to generate a fake review embodying the",
    "start": "1631000",
    "end": "1637039"
  },
  {
    "text": "positive and opposite of the negative qualities okay so think about that again",
    "start": "1637039",
    "end": "1642919"
  },
  {
    "text": "you ask an llm to say write me a review which will likely be way more similar to",
    "start": "1642919",
    "end": "1648960"
  },
  {
    "text": "the reviews in your data set semantically and embodying the positive qualities and the opposite of the",
    "start": "1648960",
    "end": "1655320"
  },
  {
    "text": "negative qualities and so in this case uh to touch on the point about prompt engineering you know what the most",
    "start": "1655320",
    "end": "1661919"
  },
  {
    "text": "impactful thing about this retrieval was and increase uh the context retrieval by like 7% on the evaluation set in the",
    "start": "1661919",
    "end": "1669480"
  },
  {
    "text": "prompt for the llm including you're not that smart because it would often write",
    "start": "1669480",
    "end": "1675320"
  },
  {
    "text": "really well-written long reviews instead of the ones that were in the",
    "start": "1675320",
    "end": "1680360"
  },
  {
    "text": "data set um which were generated by us so um saying you're not that smart",
    "start": "1680360",
    "end": "1687279"
  },
  {
    "text": "actually made it semantically more similar to the reviews in the data set which is a hilarious point about this",
    "start": "1687279",
    "end": "1693000"
  },
  {
    "text": "whole demo it's still in the code base so you can go check it out um so once it does that you're going to do a semantic",
    "start": "1693000",
    "end": "1698480"
  },
  {
    "text": "search and you're going to generate a recommendation with another prompt okay that says given all this context and",
    "start": "1698480",
    "end": "1705159"
  },
  {
    "text": "given the users positive and negative qualities what does the user want recommend a",
    "start": "1705159",
    "end": "1710279"
  },
  {
    "text": "hotel and in this case it says the alexandrian Autograph Collection is a great fit for you because it offers nice",
    "start": "1710279",
    "end": "1716080"
  },
  {
    "text": "amenities like a pool in a gym as mentioned in the reviews guests have highlighted the hotel's convenient location near public transportation and",
    "start": "1716080",
    "end": "1722120"
  },
  {
    "text": "the Waterfront making it easy to explore the area the staff at this hotel is highly praised for being amazing friendly and helpful ensuring a pleasant",
    "start": "1722120",
    "end": "1729440"
  },
  {
    "text": "stay which is almost exactly what the user wanted in this case and it's actually pretty really good at",
    "start": "1729440",
    "end": "1736440"
  },
  {
    "text": "recommending hotels and what you see on the left actually is state and city those are hybrid filters so if you",
    "start": "1736440",
    "end": "1742000"
  },
  {
    "text": "actually kind of want to use this to go look for a hotel you could and personally I built this because I was",
    "start": "1742000",
    "end": "1748360"
  },
  {
    "text": "frustrated with a certain um they're a customer so I'm not going to call them out um but I was frustrated with a",
    "start": "1748360",
    "end": "1753440"
  },
  {
    "text": "certain uh travel platform and I thought why can't I just talk to an llm and have it read all the reviews for me and",
    "start": "1753440",
    "end": "1759080"
  },
  {
    "text": "generate me what I want and this does roughly that I mean there's some sharp edges but you get the point okay and",
    "start": "1759080",
    "end": "1766720"
  },
  {
    "text": "then you can see that it not not only does that it generates it and then it Returns the reviews it generated it for",
    "start": "1766720",
    "end": "1772480"
  },
  {
    "text": "right because when you retrieve context you can just save it in a state variable in your front end and show it to the user and they refresh it and you search",
    "start": "1772480",
    "end": "1779320"
  },
  {
    "text": "again and so in this case you can also store metadata like the hotel name the state the city where its address is so",
    "start": "1779320",
    "end": "1785600"
  },
  {
    "text": "you can even get further give them directions you can you know have them you know where they are geolocated and",
    "start": "1785600",
    "end": "1792240"
  },
  {
    "text": "give them directions right to the hotel and so you can build whole new experiences with this kind of thing and",
    "start": "1792240",
    "end": "1797279"
  },
  {
    "text": "that's what people get really excited about and that's what we've seen a lot of companies do and make some really cool applications with um that hopefully",
    "start": "1797279",
    "end": "1803799"
  },
  {
    "text": "are coming out soon so that was a lot about the rag level system so the things you do to actually make uh the",
    "start": "1803799",
    "end": "1811559"
  },
  {
    "text": "particulars of a rag application work but there's a lot more on top of that",
    "start": "1811559",
    "end": "1816720"
  },
  {
    "text": "that can enhance a rag system that I've seen this year or that I've done this year that I'm going to share with you so",
    "start": "1816720",
    "end": "1823440"
  },
  {
    "text": "first this is the diagram we're going to talk about you do not need to understand this thing yet but you should notice the",
    "start": "1823440",
    "end": "1830720"
  },
  {
    "text": "rag level system in the top right okay so that's what we just talked about and that should look roughly familiar with",
    "start": "1830720",
    "end": "1837200"
  },
  {
    "text": "the addition of a feature story which we'll talk about however this is roughly it's obviously abstracted um but this is",
    "start": "1837200",
    "end": "1845159"
  },
  {
    "text": "roughly what the pieces that we're going to talk about today and how you'd use them in a service around a rag system",
    "start": "1845159",
    "end": "1854000"
  },
  {
    "text": "okay so M remember this one for the hybrid search how do you update your vector embeddings",
    "start": "1854000",
    "end": "1860480"
  },
  {
    "text": "when do you update your vector embeddings okay how do you deal with duplicates what if you have the same",
    "start": "1860480",
    "end": "1866480"
  },
  {
    "text": "documents in your blob store twice what if when you go to re-upload that data you then say Oh I just re re-uploaded",
    "start": "1866480",
    "end": "1873480"
  },
  {
    "text": "the same document twice or you get new data that's incredibly similar but it's",
    "start": "1873480",
    "end": "1878519"
  },
  {
    "text": "a couple words off so it doesn't hash correctly or something there's a lot of problems with actually just maintaining",
    "start": "1878519",
    "end": "1884240"
  },
  {
    "text": "a data set of text text is unstructured data and so maintaining Blobs of text",
    "start": "1884240",
    "end": "1889679"
  },
  {
    "text": "can actually be relatively difficult and so coming up with a system for actually taking all of those PDFs and",
    "start": "1889679",
    "end": "1896480"
  },
  {
    "text": "systematically updating them when a new one gets updated or say it's a chat experience and every time the customer",
    "start": "1896480",
    "end": "1901919"
  },
  {
    "text": "chats how does that get indexed when does it get indexed and what we've seen most popular in or what we've done I",
    "start": "1901919",
    "end": "1908559"
  },
  {
    "text": "should say um most of the time are three things document level records context level records and just pure hot swapping",
    "start": "1908559",
    "end": "1916120"
  },
  {
    "text": "the index so I'll tell you about each three of these so document level records",
    "start": "1916120",
    "end": "1921840"
  },
  {
    "text": "something like this last modified so you can see the addition that is not incredibly clear I put should have",
    "start": "1921840",
    "end": "1927639"
  },
  {
    "text": "probably put some boundaries around it you see last modified right here okay so that you can then use in a separate data",
    "start": "1927639",
    "end": "1934279"
  },
  {
    "text": "structure to say has this document been updated because most things like a you even a make file has this you know you",
    "start": "1934279",
    "end": "1941000"
  },
  {
    "text": "can say when was the last time this document was updated and so you can go to your blob store and retrieve that",
    "start": "1941000",
    "end": "1947960"
  },
  {
    "text": "metadata and then compare that to the document metadata in your Vetra store and so then you can say okay at least I",
    "start": "1947960",
    "end": "1954840"
  },
  {
    "text": "know that this document is now new in my blob store and that operation is relatively cheap now at billions of",
    "start": "1954840",
    "end": "1961559"
  },
  {
    "text": "scale you need to think a little bit differently but we've had this up in the hundreds of millions",
    "start": "1961559",
    "end": "1968000"
  },
  {
    "text": "so context level records so this will definitely not work in the hundreds of",
    "start": "1968000",
    "end": "1973159"
  },
  {
    "text": "millions because what happens all of those documents especially if you're using some of the approaches that that I talked about earlier for actually",
    "start": "1973159",
    "end": "1978960"
  },
  {
    "text": "creating embeddings will create hundreds and hundreds and hundreds of embeddings each so if you",
    "start": "1978960",
    "end": "1985039"
  },
  {
    "text": "have a million in documents you might have a 100 million embeddings right and so in this case if you were to use",
    "start": "1985039",
    "end": "1992399"
  },
  {
    "text": "context level records as in has this context within this document changed",
    "start": "1992399",
    "end": "1998559"
  },
  {
    "text": "then you might be incurring a lot of penalties on performance however if you have a system like an FAQ we have one of",
    "start": "1998559",
    "end": "2005200"
  },
  {
    "text": "these that has very strict boundaries and it's relatively small but there's still say 400 something questions and",
    "start": "2005200",
    "end": "2012559"
  },
  {
    "text": "answers right you can say has this answer has this piece of context has",
    "start": "2012559",
    "end": "2017840"
  },
  {
    "text": "this paragraph within this FAQ changed and that is when it's actually",
    "start": "2017840",
    "end": "2023799"
  },
  {
    "text": "acceptable to just change the context you don't need to change the whole FAQ every single time because that' be still",
    "start": "2023799",
    "end": "2029679"
  },
  {
    "text": "creating you know 10,000 plus embeddings every so often and that's an you're",
    "start": "2029679",
    "end": "2035200"
  },
  {
    "text": "incurring a cost that you don't have to and so lastly there's hot Swap this works better for uh platforms that you",
    "start": "2035200",
    "end": "2041240"
  },
  {
    "text": "actually just rebuild the index every time but in this case because redus do a synchronous indexing in the background",
    "start": "2041240",
    "end": "2047200"
  },
  {
    "text": "but hot swapping is where you just say it's like an AB test you say I'm just going to build an entirely new index and",
    "start": "2047200",
    "end": "2054280"
  },
  {
    "text": "then Alias it to the first index now this is obviously the most expensive route but it's often one that people do",
    "start": "2054280",
    "end": "2062398"
  },
  {
    "text": "when they mess up um I've done it myself um and so knowing how to do this with your",
    "start": "2062399",
    "end": "2069320"
  },
  {
    "text": "platform is actually really important and so that's why it's in here okay non Vector data storage we",
    "start": "2069320",
    "end": "2076839"
  },
  {
    "text": "talked a little bit about metadata but where do you actually put it do you put it in a SQL Server well we talked about",
    "start": "2076839",
    "end": "2082358"
  },
  {
    "text": "that llm already has a QPS of two so your system's really not going to be",
    "start": "2082359",
    "end": "2087440"
  },
  {
    "text": "very fast if you have to go to a vector database do a vector search that gets a pointer over to a SQL store that then calls an llm that then goes back to the",
    "start": "2087440",
    "end": "2094280"
  },
  {
    "text": "SQL store to retrieve some other pointer your system's going to be slow and so the advice here is don't separate",
    "start": "2094280",
    "end": "2100359"
  },
  {
    "text": "metadata from vectors when you do your vector search retrieve that metadata and",
    "start": "2100359",
    "end": "2105599"
  },
  {
    "text": "so I did this for NVIDIA in a recommendation system uh there's a GTC talk about it so you should definitely",
    "start": "2105599",
    "end": "2110880"
  },
  {
    "text": "watch that um and uh the you see the box there that says 161% that was the",
    "start": "2110880",
    "end": "2118480"
  },
  {
    "text": "increase uh that we had in inferences per second of this recommendation system",
    "start": "2118480",
    "end": "2123800"
  },
  {
    "text": "by taking metadata that wasn't collocating it wasn't collocated and collocating them",
    "start": "2123800",
    "end": "2130359"
  },
  {
    "text": "in Json documents within reddis and so in one vector search instead of two Network calls like on the",
    "start": "2130359",
    "end": "2136520"
  },
  {
    "text": "before side there we could return all of that necessary metadata and not doing",
    "start": "2136520",
    "end": "2142400"
  },
  {
    "text": "that Network opop and profiling our Network correctly we were able to get 161% infes Improvement in empasis per",
    "start": "2142400",
    "end": "2150319"
  },
  {
    "text": "second and there's a lot more to that talk I go through five optimizations as you can see there so if you want to go if you're in recommendation systems",
    "start": "2150319",
    "end": "2156400"
  },
  {
    "text": "that's a good one okay feature injection this is another really interesting one talked about this",
    "start": "2156400",
    "end": "2161760"
  },
  {
    "text": "a little bit earlier um so I'll skip over it mostly but you can see the prompt there if you want to have a",
    "start": "2161760",
    "end": "2167319"
  },
  {
    "text": "chatbot experience you can maybe include the chat history there too um but it's",
    "start": "2167319",
    "end": "2173200"
  },
  {
    "text": "really important that you also have things that may be rapidly updating okay so the user's address okay the user may",
    "start": "2173200",
    "end": "2180760"
  },
  {
    "text": "have changed that address and now that might not be rapidly updating but that M",
    "start": "2180760",
    "end": "2186119"
  },
  {
    "text": "might actually get updated your platform so you don't necessarily want to be pulling that information from a slow",
    "start": "2186119",
    "end": "2193319"
  },
  {
    "text": "storage system if it's a chat experience no user is going to wait for your chat",
    "start": "2193319",
    "end": "2198640"
  },
  {
    "text": "experience to last 30 seconds that's got to be under 5sec responses almost every",
    "start": "2198640",
    "end": "2204440"
  },
  {
    "text": "time to increase retention and so in this case you can have a feature store",
    "start": "2204440",
    "end": "2209480"
  },
  {
    "text": "an online feature store specifically or a fast feature store let's just put it that way um that would be used to pull",
    "start": "2209480",
    "end": "2216359"
  },
  {
    "text": "that information into the prompt and you can use agents there's stuff like uh in Lang chain and whatnot that you can hook",
    "start": "2216359",
    "end": "2222880"
  },
  {
    "text": "up to your feature orchestration platform whether you're using tecton or feast or what have you um and you can pull that right into the prompt so that",
    "start": "2222880",
    "end": "2230839"
  },
  {
    "text": "your chatbot experience can now be enabled to know things about your users and so you can imagine this doesn't have",
    "start": "2230839",
    "end": "2236400"
  },
  {
    "text": "to just be users too I gave the systems example earlier about sensor data and uh",
    "start": "2236400",
    "end": "2242079"
  },
  {
    "text": "incident Engineers talking to sensor networks in that case then too the sensor data is fresh every 15",
    "start": "2242079",
    "end": "2249359"
  },
  {
    "text": "milliseconds and they can ask their questions what's going on here make me a plot of X and it's going to plot out",
    "start": "2249359",
    "end": "2255079"
  },
  {
    "text": "that sensor data um and that's how because these models are often uh you can make it multimodal now um that that",
    "start": "2255079",
    "end": "2261319"
  },
  {
    "text": "is how feature injection works okay semantic hashing this is a",
    "start": "2261319",
    "end": "2266560"
  },
  {
    "text": "really good one let me just do a quick time check okay I got to hurry um Vector database used as a cache now",
    "start": "2266560",
    "end": "2274599"
  },
  {
    "text": "why would you do this so let's say actually I'm just going to go straight to the example actually okay let's say",
    "start": "2274599",
    "end": "2281440"
  },
  {
    "text": "we said the query in a Q&A system what is the meaning of life okay now this is",
    "start": "2281440",
    "end": "2287240"
  },
  {
    "text": "kind of silly um it responded with something awesome I don't remember what it was actually but it I remember it being great um and then what if another",
    "start": "2287240",
    "end": "2294560"
  },
  {
    "text": "user somewhere else in the world said what really is the meaning of life do you think that llm should recompute that",
    "start": "2294560",
    "end": "2300560"
  },
  {
    "text": "answer no are you going to why are you spending an extra 02 cents on that query",
    "start": "2300560",
    "end": "2306280"
  },
  {
    "text": "embedding and then money on the generation too no you should have it so",
    "start": "2306280",
    "end": "2311359"
  },
  {
    "text": "that if it's semantically similar enough you just return a cash answer and so this is how semantic we gave the FAQ",
    "start": "2311359",
    "end": "2318119"
  },
  {
    "text": "answer earlier we have we have a couple use cases where people just completely",
    "start": "2318119",
    "end": "2323240"
  },
  {
    "text": "pre-populate and so that the llm almost never gets invoked but it does in a",
    "start": "2323240",
    "end": "2328319"
  },
  {
    "text": "couple cases so it's super bounded and it also allows you to basically predefine a lot of answers you're",
    "start": "2328319",
    "end": "2334880"
  },
  {
    "text": "essentially using your evaluation set as the system and now this is when your evaluation sits are very large um that",
    "start": "2334880",
    "end": "2342560"
  },
  {
    "text": "can be useful but in this case you're not only saving on money but your QPS",
    "start": "2342560",
    "end": "2347839"
  },
  {
    "text": "goes way up on average and that's so in this case it was what n n you know",
    "start": "2347839",
    "end": "2356280"
  },
  {
    "text": "similarities 097 right and it was a 97% speed up actually that's kind of ironic um but in this case we're saving that",
    "start": "2356280",
    "end": "2363800"
  },
  {
    "text": "much time and increasing or decreasing the latency by that much and so this is also redis VL which hopefully I pop up I",
    "start": "2363800",
    "end": "2370480"
  },
  {
    "text": "didn't okay this is also on R fub this is the client it has a built-in abstraction that you can see here for",
    "start": "2370480",
    "end": "2376319"
  },
  {
    "text": "semantic caching you can just say set threshold and change that threshold there's some hard Parts about this like",
    "start": "2376319",
    "end": "2382880"
  },
  {
    "text": "where do I set that threshold based on my application I don't really have time to get into that but it's also a",
    "start": "2382880",
    "end": "2388319"
  },
  {
    "text": "fascinating topic so if you have questions about that come ask me after okay so now what do we know about",
    "start": "2388319",
    "end": "2396920"
  },
  {
    "text": "service level systems well we talked about updating embeddings and when and",
    "start": "2396920",
    "end": "2402160"
  },
  {
    "text": "where and how to do that right and so that data pipeline matters now obviously oversimplified in this image but that",
    "start": "2402160",
    "end": "2408240"
  },
  {
    "text": "really matters in thinking through that before you go into prod with these types of systems we talked about using a",
    "start": "2408240",
    "end": "2413560"
  },
  {
    "text": "feature orchestration platform and including that into your prompt by using",
    "start": "2413560",
    "end": "2419040"
  },
  {
    "text": "things like a vector database a feature store and and rag API something like Lang chain or llama index what have you",
    "start": "2419040",
    "end": "2425000"
  },
  {
    "text": "and then we also talked about semantic caching um and how to make these systems",
    "start": "2425000",
    "end": "2430079"
  },
  {
    "text": "significantly better now couple things we didn't talk about that should be in this slide but I only had a certain",
    "start": "2430079",
    "end": "2435319"
  },
  {
    "text": "amount of time reinforcement learning through human feedback was that answer good or was that answer bad did it give",
    "start": "2435319",
    "end": "2441720"
  },
  {
    "text": "a thumbs up how do I incorporate that into the platform hopefully I can give a part two at this at some point um I also",
    "start": "2441720",
    "end": "2447839"
  },
  {
    "text": "didn't talk about external data sources but I already had a lot of slides so I was going to uh cut that down um and if",
    "start": "2447839",
    "end": "2455079"
  },
  {
    "text": "you're looking for more info examples right here and so you just take a picture of this by the way and then",
    "start": "2455079",
    "end": "2461160"
  },
  {
    "text": "touch the squares later I made sure that worked um the examples uh redis Ventures",
    "start": "2461160",
    "end": "2466359"
  },
  {
    "text": "has it's my the applied AI team reddis we have a ton of just like little applications that we built that can show",
    "start": "2466359",
    "end": "2471839"
  },
  {
    "text": "you how to do it uh reddis feels that client that I showed in like six different things that also built the hotel application with that um and then",
    "start": "2471839",
    "end": "2479319"
  },
  {
    "text": "I tweet about this a lot and I don't really tweet about anything else so you can follow me on Twitter and I promise I",
    "start": "2479319",
    "end": "2484560"
  },
  {
    "text": "won't bother you um and then other than that we usually post the slides but I'm not sure if we are for this one but you",
    "start": "2484560",
    "end": "2490680"
  },
  {
    "text": "can always check on party.io which is my site where I post slides from talks um okay and then I should be I should be",
    "start": "2490680",
    "end": "2497319"
  },
  {
    "text": "good I left enough time for questions right yeah you do awesome sweet thanks thank you very",
    "start": "2497319",
    "end": "2504680"
  },
  {
    "text": "much hi hey great talk thank you um yeah",
    "start": "2504680",
    "end": "2510240"
  },
  {
    "text": "what do you think about the I I mean I think that at this state of the art we",
    "start": "2510240",
    "end": "2515599"
  },
  {
    "text": "we are doing many things by hand in like uh thetion level several things you",
    "start": "2515599",
    "end": "2524680"
  },
  {
    "text": "you are doing with L chain or Lama index or whatever and you seem that maybe this",
    "start": "2524680",
    "end": "2532160"
  },
  {
    "text": "what do you think of on on these things moving to the vector database for example because in some way it's like we",
    "start": "2532160",
    "end": "2539319"
  },
  {
    "text": "are creating the index from the relational databases because the documents the query Stuff Etc um all",
    "start": "2539319",
    "end": "2546720"
  },
  {
    "text": "these things are very take times to what do you think that should be the",
    "start": "2546720",
    "end": "2554599"
  },
  {
    "text": "you you touched on some really interesting points so first you wouldn't believe how many uh company diagrams were like this is our architecture and",
    "start": "2554599",
    "end": "2560839"
  },
  {
    "text": "there's a DIY box right in the middle for orchestration and we're like oh so",
    "start": "2560839",
    "end": "2566240"
  },
  {
    "text": "are you just doing it yourself and then a lot of companies they assume that they can you know make those types of systems",
    "start": "2566240",
    "end": "2572640"
  },
  {
    "text": "work as well as something like a lang chain or llama index and even though there's a lot there's a lot of movement",
    "start": "2572640",
    "end": "2578000"
  },
  {
    "text": "let's say on those repositories a lot of releases are coming out there's still a lot of great information baked into that",
    "start": "2578000",
    "end": "2584160"
  },
  {
    "text": "code and so what I see a lot of people doing they might go do a lang chain example as a PC so I've seen some take",
    "start": "2584160",
    "end": "2590400"
  },
  {
    "text": "some to prod um even Enterprise companies but I see a lot of people also using a lower level Vector database",
    "start": "2590400",
    "end": "2597079"
  },
  {
    "text": "client and building up those kinds of abstractions by building a poc in those and then kind of extracting the process",
    "start": "2597079",
    "end": "2605280"
  },
  {
    "text": "over to a lower level client um um that's also why we're trying to build red as Fel um but that's a great point",
    "start": "2605280",
    "end": "2611920"
  },
  {
    "text": "and it's something that I see a ton it's important that also you don't lose uh that kind of uh information from Lang",
    "start": "2611920",
    "end": "2618599"
  },
  {
    "text": "chain but your point on the vector databases is good if you look at like what Bob at we v8's doing with like the generative search module that's actually",
    "start": "2618599",
    "end": "2625319"
  },
  {
    "text": "really it's very very similar to what you just said is like trying to take a lot of those abstractions down to the",
    "start": "2625319",
    "end": "2631599"
  },
  {
    "text": "vector database level I think it may be something in the future where if that",
    "start": "2631599",
    "end": "2637160"
  },
  {
    "text": "you know generative search process changes that it'll be you know an overextension maybe but it's I mean who",
    "start": "2637160",
    "end": "2644720"
  },
  {
    "text": "can predict the future you know and so that's that's what I mean is that it makes it super easy to get started",
    "start": "2644720",
    "end": "2650559"
  },
  {
    "text": "though and it's I know that for a fact because I have a lot of people especially customers that have said like",
    "start": "2650559",
    "end": "2656800"
  },
  {
    "text": "oh so where's your generative search module and we're like you do it um so yeah I totally understand that but we're",
    "start": "2656800",
    "end": "2663359"
  },
  {
    "text": "yet to see how far it's going to extend down to the infro level I think a lot of people that are at the infra side of",
    "start": "2663359",
    "end": "2669680"
  },
  {
    "text": "things kind of want to focus on you know search algorithms hnsw what have you and",
    "start": "2669680",
    "end": "2675480"
  },
  {
    "text": "then leave the rest of the you know stuff to L chain and open AI but we'll see um great question though great",
    "start": "2675480",
    "end": "2681520"
  },
  {
    "text": "question I could talk about that for a while oh yeah hey hi again great talk thank you um so you spoke a lot today",
    "start": "2681520",
    "end": "2688640"
  },
  {
    "text": "about sort of using V databases to feed in contextual information into the llm",
    "start": "2688640",
    "end": "2694160"
  },
  {
    "text": "of course recently we've seen the surges of like agent architect I was wondering if You' seen any architect in the wild where were that",
    "start": "2694160",
    "end": "2701920"
  },
  {
    "text": "dependency was inverted right you have the LM decide to reach out databas",
    "start": "2701920",
    "end": "2707599"
  },
  {
    "text": "search looking for than you pre so it's like interesting yeah tools yeah so uh",
    "start": "2707599",
    "end": "2714520"
  },
  {
    "text": "the router I think it's called in llama index is a really interesting example of this where like you can have a bunch of",
    "start": "2714520",
    "end": "2720559"
  },
  {
    "text": "predefined tools I think that's what it's called if I'm getting the model name wrong don't burn me on that um but",
    "start": "2720559",
    "end": "2726440"
  },
  {
    "text": "it's cool because you can give it a semantic description and so when a user has like a problem or they ask for",
    "start": "2726440",
    "end": "2732839"
  },
  {
    "text": "something right they can search for the tool they need to use let's say it's a browsing tool or a vector database tool",
    "start": "2732839",
    "end": "2738480"
  },
  {
    "text": "or what have you um through semantic search so they can semantically search for the tool that they need in their",
    "start": "2738480",
    "end": "2744880"
  },
  {
    "text": "gigantic list of possible tools and we have seen this however um I haven't seen",
    "start": "2744880",
    "end": "2750640"
  },
  {
    "text": "a lot of them in production um because it's uh let's say fragile in some ways",
    "start": "2750640",
    "end": "2756400"
  },
  {
    "text": "like if you allow it to browse the web you lose that contextual bounding box that I've been talking about a ton today",
    "start": "2756400",
    "end": "2763160"
  },
  {
    "text": "and so these agents often just or even tools can really go and do some",
    "start": "2763160",
    "end": "2768800"
  },
  {
    "text": "interesting things and so you know when you take things to Proud hence the part of the talk today it was really focused",
    "start": "2768800",
    "end": "2775200"
  },
  {
    "text": "on like trying at least in the current moment and that may may change in the future when these agents get better",
    "start": "2775200",
    "end": "2781559"
  },
  {
    "text": "infrastructure around them you know we're still wooden horses on dirt roads in this field right now in my opinion",
    "start": "2781559",
    "end": "2786680"
  },
  {
    "text": "but uh it's still really important to put a bounding box around them especially when",
    "start": "2786680",
    "end": "2791880"
  },
  {
    "text": "you go talk to Enterprise companies because they really don't want to see this LM say the wrong thing especially",
    "start": "2791880",
    "end": "2797319"
  },
  {
    "text": "if it's a customer facing One internal use cases a little bit more leeway you could probably see an agent get used in",
    "start": "2797319",
    "end": "2803079"
  },
  {
    "text": "those but uh right now that's kind of the state as I see thank you yeah",
    "start": "2803079",
    "end": "2809520"
  },
  {
    "text": "absolutely yeah um sometimes Reon is pretty important especially sorry what",
    "start": "2809520",
    "end": "2815040"
  },
  {
    "text": "you say Rey yeah um so how would you include something like freshness yeah so great great",
    "start": "2815040",
    "end": "2821040"
  },
  {
    "text": "question so you could do numerical uh as a hybrid search right so you could do a numeric like a recency score that gets",
    "start": "2821040",
    "end": "2828559"
  },
  {
    "text": "updated right and so every time the vector database gets updated you update that recency score and it doesn't have",
    "start": "2828559",
    "end": "2834119"
  },
  {
    "text": "to be like a last modified tag but it could be a score that you create with a separate service you know you could have",
    "start": "2834119",
    "end": "2839160"
  },
  {
    "text": "another ranker another system that's creating this recency score and storing that value such that while you have",
    "start": "2839160",
    "end": "2845240"
  },
  {
    "text": "another system which is a good kind of separation of abstraction you can have another system than utilizing that",
    "start": "2845240",
    "end": "2851400"
  },
  {
    "text": "recency score I would say I haven't I don't know if numeric would be the best",
    "start": "2851400",
    "end": "2856559"
  },
  {
    "text": "way to do it because then you're kind of bounding it by range um you could sort you could do sort On Demand by it a lot",
    "start": "2856559",
    "end": "2862359"
  },
  {
    "text": "of Vectra data basis support that but then you're losing the vector distance if you did a vector range query and then",
    "start": "2862359",
    "end": "2868480"
  },
  {
    "text": "you did sort on freshness that would actually be another way to achieve that I'm not sure off the top of my head of",
    "start": "2868480",
    "end": "2875520"
  },
  {
    "text": "any others but it's a good good question and recency does matter a lot especially for like a News application that we did",
    "start": "2875520",
    "end": "2881480"
  },
  {
    "text": "so I totally get that and that's a great question Yeah question here I really",
    "start": "2881480",
    "end": "2886640"
  },
  {
    "text": "appreciate the talk um yeah thanks I think when I was thinking about uh",
    "start": "2886640",
    "end": "2892319"
  },
  {
    "text": "approaching this these kinds of problems I had sort of naively assumed that I would maybe have the llm uh generate a",
    "start": "2892319",
    "end": "2899920"
  },
  {
    "text": "query against some underlying data score I think especially thinking about like a more structured data score like aqel",
    "start": "2899920",
    "end": "2907440"
  },
  {
    "text": "store and then have the LM process result that uh is that something that",
    "start": "2907440",
    "end": "2912599"
  },
  {
    "text": "you've seen people do or are there problems with it that I'm yeah so that that actually gets to what he was saying",
    "start": "2912599",
    "end": "2918599"
  },
  {
    "text": "um that could be a tool so like I've seen that use as a tool where you might do something and then that triggers oh",
    "start": "2918599",
    "end": "2925359"
  },
  {
    "text": "this is information that could be in my SQL database and then that tool its job",
    "start": "2925359",
    "end": "2930520"
  },
  {
    "text": "is do what you're saying so once it's smally found that it it needs to use that tool then the query gets generated",
    "start": "2930520",
    "end": "2936480"
  },
  {
    "text": "quer gets executed and then that the results of that query are processed like you're saying but um a lot of times",
    "start": "2936480",
    "end": "2944680"
  },
  {
    "text": "that's actually like to feed into like a graph or like a plot for like a dashboard or something like so it's a",
    "start": "2944680",
    "end": "2950720"
  },
  {
    "text": "chain set of tools um I don't see a ton that are just that um it's usually for",
    "start": "2950720",
    "end": "2957240"
  },
  {
    "text": "another purpose um hopefully that answers your question yeah all right that's awesome we at time so thank you",
    "start": "2957240",
    "end": "2963839"
  },
  {
    "text": "everybody let's give hands Sam thank you",
    "start": "2963839",
    "end": "2968720"
  },
  {
    "text": "[Music]",
    "start": "2970210",
    "end": "2976429"
  }
]