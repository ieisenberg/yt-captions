[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "[Music]",
    "start": "680",
    "end": "8549"
  },
  {
    "text": "good morning everyone my name is alex uh today we're going to talk about co-design and raft on a threat per core",
    "start": "15120",
    "end": "21119"
  },
  {
    "text": "model uh for the kafka api all of the code will be available on github today this",
    "start": "21119",
    "end": "27279"
  },
  {
    "text": "is for qcon",
    "start": "27279",
    "end": "31240"
  },
  {
    "start": "33000",
    "end": "33000"
  },
  {
    "text": "so background my name is alex feel free to find me on on twitter if it's probably the easiest",
    "start": "34079",
    "end": "40640"
  },
  {
    "text": "way to engage with me but i'm a developer i originally wrote the storage engine and all of the techniques that",
    "start": "40640",
    "end": "46719"
  },
  {
    "text": "we're going to cover today i i was heavily involved with either uh as the developer that that wrote the code or or",
    "start": "46719",
    "end": "53600"
  },
  {
    "text": "as a reviewer on on the code we've been hacking on on red panda which",
    "start": "53600",
    "end": "59199"
  },
  {
    "text": "is the the project that we're going to talk about today it's a streaming platform for mission critical systems prior to this i was an",
    "start": "59199",
    "end": "66240"
  },
  {
    "text": "engineer at akamai uh through an acquisition of concord that i o you can think of it like spark streaming but",
    "start": "66240",
    "end": "72960"
  },
  {
    "text": "we wrote it in c plus on top of mesos the agenda today will cover",
    "start": "72960",
    "end": "79920"
  },
  {
    "start": "77000",
    "end": "77000"
  },
  {
    "text": "um two observations a new way of building software using a throttle core model and the practical",
    "start": "79920",
    "end": "86640"
  },
  {
    "text": "implementation specific will cover eight techniques for building low latency software",
    "start": "86640",
    "end": "92640"
  },
  {
    "text": "and so the the first real observation here is how hardware is fundamentally different",
    "start": "92640",
    "end": "98400"
  },
  {
    "text": "than it was a decade ago the second observation is that given the",
    "start": "98400",
    "end": "103759"
  },
  {
    "text": "fundamental shifts in in hardware cpu becomes the new bottleneck for the storage system like",
    "start": "103759",
    "end": "110240"
  },
  {
    "text": "ours and so embracing the hardware where everything is asynchronous is really the second observation",
    "start": "110240",
    "end": "117360"
  },
  {
    "text": "and so with these two premises we can reason about how to build new software or what are what are",
    "start": "117360",
    "end": "123600"
  },
  {
    "text": "what are new techniques to build in software in particular uh we will walk through",
    "start": "123600",
    "end": "130720"
  },
  {
    "text": "the the most impactful techniques for red panda to achieve 10 to 100 x better",
    "start": "130720",
    "end": "136400"
  },
  {
    "text": "tail latencies um i'll highlight that all of the source code that i'll be talking about today is available on",
    "start": "136400",
    "end": "142800"
  },
  {
    "text": "github so if you have any questions if you want to interact with the code or the software",
    "start": "142800",
    "end": "147920"
  },
  {
    "text": "feel free to go on github and ask us questions etc",
    "start": "147920",
    "end": "154000"
  },
  {
    "text": "one of our advisors really loves this this saying and and it goes sometimes you get to reinvent the wheel when the",
    "start": "154800",
    "end": "160239"
  },
  {
    "text": "road changes and the the road is is the hardware in this in this analogy um the idea here is",
    "start": "160239",
    "end": "167760"
  },
  {
    "text": "that hardware is fundamentally different from what it was a decade ago in particular for storage engine and in",
    "start": "167760",
    "end": "174239"
  },
  {
    "text": "the things that is materially different is one disks are a thousand times faster",
    "start": "174239",
    "end": "181599"
  },
  {
    "text": "and at the same time they are 100x cheaper the second material improvement on",
    "start": "181599",
    "end": "187840"
  },
  {
    "text": "hardware over the last decade is that computers are 20 times taller so you can",
    "start": "187840",
    "end": "194159"
  },
  {
    "text": "go on on gcp and rent a 225 core vcpu vm",
    "start": "194159",
    "end": "200319"
  },
  {
    "text": "and in in so so massively different scales and and and and the contention moves",
    "start": "200319",
    "end": "206959"
  },
  {
    "text": "uh for for a storage engine out of the spinning disc subsystem for older",
    "start": "206959",
    "end": "213599"
  },
  {
    "text": "storage systems to cpu coordination and then the last one that i think is",
    "start": "213599",
    "end": "218720"
  },
  {
    "text": "worth noting and and i didn't highlight it here in bold but it is an important distinction is",
    "start": "218720",
    "end": "225360"
  },
  {
    "text": "that in the in the land basically on on on amazon",
    "start": "225360",
    "end": "230480"
  },
  {
    "text": "you can go and rent a hundred gigabit per second nix and actually get that sustained traffic delivered to all of",
    "start": "230480",
    "end": "237280"
  },
  {
    "text": "the vms and so the idea is that software doesn't run on category theory it runs on the",
    "start": "237280",
    "end": "242400"
  },
  {
    "text": "super scalar cpus with massive amounts of memory multi-socket motherboards discs that are",
    "start": "242400",
    "end": "250080"
  },
  {
    "text": "two you actually three orders of magnitude uh faster and so you start to move the bottleneck and compute to a",
    "start": "250080",
    "end": "256320"
  },
  {
    "text": "different subsystem this is an evolution of this the system",
    "start": "256320",
    "end": "262479"
  },
  {
    "start": "259000",
    "end": "259000"
  },
  {
    "text": "that i'll i'll talk to you guys about today in particular the mental modeling",
    "start": "262479",
    "end": "268080"
  },
  {
    "text": "techniques and how it evolved through history superimposed with the evolutions of",
    "start": "268080",
    "end": "273759"
  },
  {
    "text": "hardware and so rabbit and q really came about and replaced a bunch of older enterprise",
    "start": "273759",
    "end": "280080"
  },
  {
    "text": "kind of old-school uh message queuing like tipco and and solas and around that",
    "start": "280080",
    "end": "287040"
  },
  {
    "text": "time ssds were about two thousand five hundred dollars in uh per terabyte",
    "start": "287040",
    "end": "292240"
  },
  {
    "text": "in 2010-2011 um kafka came out uh really bringing a lot of the mapreduce style",
    "start": "292240",
    "end": "298720"
  },
  {
    "text": "thinking to streaming the idea is can you do streaming on cheap spinning disc type",
    "start": "298720",
    "end": "304800"
  },
  {
    "text": "computers with four vcpus perhaps maybe a gigabit of network",
    "start": "304800",
    "end": "309919"
  },
  {
    "text": "and and the the the point i want to highlight here is that",
    "start": "309919",
    "end": "316720"
  },
  {
    "text": "the threading model and the mental model for building",
    "start": "316720",
    "end": "321840"
  },
  {
    "text": "software a decade ago was for a totally different bottleneck in computing namely that to write a page",
    "start": "321840",
    "end": "329199"
  },
  {
    "text": "to spin and disk it was in the order of milliseconds rather than microseconds",
    "start": "329199",
    "end": "334800"
  },
  {
    "text": "pulsar came about as the evolution of streaming here and added the disaggregation of computer and",
    "start": "334800",
    "end": "339919"
  },
  {
    "text": "store uh largely with the context of hdfs in the public clouds uh the s3 api became",
    "start": "339919",
    "end": "347039"
  },
  {
    "text": "the true disaggregation of compute and store and so uh as of this year red panda is",
    "start": "347039",
    "end": "352880"
  },
  {
    "text": "is now stable and ready for production using and what i wanted to highlight here",
    "start": "352880",
    "end": "358080"
  },
  {
    "text": "is that is software design for the modern platform and so when we ask ourselves what could we do",
    "start": "358080",
    "end": "365120"
  },
  {
    "text": "differently if we were to start from zero we knew where we wanted to be which was to provide",
    "start": "365120",
    "end": "371360"
  },
  {
    "text": "you know effectively a raft implementation on on with the kafka api",
    "start": "371360",
    "end": "377840"
  },
  {
    "text": "given the modern platform given the modern hardware",
    "start": "377840",
    "end": "382800"
  },
  {
    "start": "384000",
    "end": "384000"
  },
  {
    "text": "the second observation here is that everything in hardware is actually asynchronous the the most important point of this slide",
    "start": "385039",
    "end": "392240"
  },
  {
    "text": "is really that even on an nvme ssd device the one the one on the right is western",
    "start": "392240",
    "end": "398960"
  },
  {
    "text": "uh digital nvme ssd device you can have sustained writes throughput",
    "start": "398960",
    "end": "405440"
  },
  {
    "text": "of 1.2 gigabytes per second however if you use the classical model of io",
    "start": "405440",
    "end": "412800"
  },
  {
    "text": "where it's it's blocking right so you do a file right and you give it the file handle and some",
    "start": "412800",
    "end": "419120"
  },
  {
    "text": "bytes and and then you do file close right so basically synchronous and sequential in",
    "start": "419120",
    "end": "424560"
  },
  {
    "text": "your in your program to write a single page to disk you're roughly wasting 60 to 420",
    "start": "424560",
    "end": "432639"
  },
  {
    "text": "million clock cycles and so the observation here is could the cpu be actually doing something else",
    "start": "432639",
    "end": "439840"
  },
  {
    "text": "instead of waiting for the hardware to complete the operation",
    "start": "439840",
    "end": "444960"
  },
  {
    "text": "and uh you know obviously the answer is yes and but from a software construction",
    "start": "444960",
    "end": "450800"
  },
  {
    "text": "perspective is if we embrace the platform if we embrace that hardware",
    "start": "450800",
    "end": "456639"
  },
  {
    "text": "is always asynchronous and we design our software from the ground up to be asynchronous first",
    "start": "456639",
    "end": "463520"
  },
  {
    "text": "then we get to take advantage of this 420 million clock cycles that are wasted",
    "start": "463520",
    "end": "469039"
  },
  {
    "text": "to do other interesting work like compression or verifying you know checksum and verifying the integrity of",
    "start": "469039",
    "end": "475520"
  },
  {
    "text": "your data or pulling more data out of the network while your nvme ssd device is busy writing it to the to the actual",
    "start": "475520",
    "end": "482639"
  },
  {
    "text": "sectors of the nvme controller the architecture",
    "start": "482639",
    "end": "487919"
  },
  {
    "text": "that we chose to build red panda is uh on on threat per core architecture",
    "start": "487919",
    "end": "493680"
  },
  {
    "text": "and for building a storage engine we think that",
    "start": "493680",
    "end": "498960"
  },
  {
    "text": "we've measured right that the the new bottleneck is a cpu and so how do we how",
    "start": "498960",
    "end": "504080"
  },
  {
    "text": "do we start to build on on a new way to to build new storage engine or new",
    "start": "504080",
    "end": "510400"
  },
  {
    "text": "software using a threat record architecture the distinction here is that",
    "start": "510400",
    "end": "515839"
  },
  {
    "text": "asynchronicity is a first-class citizen and it becomes explicitly how you start to reason about",
    "start": "515839",
    "end": "522080"
  },
  {
    "text": "uh your your your software building a word of warning though is there's no",
    "start": "522080",
    "end": "529680"
  },
  {
    "text": "real free lunch here and when you move to a threat record architecture you will end up rewriting large parts of your",
    "start": "529680",
    "end": "536480"
  },
  {
    "text": "software that come with built-in threading models a classical example is that often you add a login library to",
    "start": "536480",
    "end": "542720"
  },
  {
    "text": "your program and it has this thread pooling implementation while that doesn't really work that well with the",
    "start": "542720",
    "end": "548320"
  },
  {
    "text": "threat per core architecture where every every logical core presented to",
    "start": "548320",
    "end": "554160"
  },
  {
    "text": "the program is an actual p thread and inside that particular p thread",
    "start": "554160",
    "end": "559760"
  },
  {
    "text": "there is a comparative scheduling implementation and it doesn't necessarily have to be",
    "start": "559760",
    "end": "566160"
  },
  {
    "text": "the the way that i'm going to explain but this is rather an example of an architectural thinking",
    "start": "566160",
    "end": "573040"
  },
  {
    "text": "that you can that you can leverage so the idea that we're going to cover is uh in",
    "start": "573040",
    "end": "580720"
  },
  {
    "text": "nc plus plus because we wrote the storage engine in c plus in large because we're we're trying to extract",
    "start": "580720",
    "end": "587200"
  },
  {
    "text": "every ounce of performance of the hardware and so it was important to us to be able to be able to access all of",
    "start": "587200",
    "end": "594160"
  },
  {
    "text": "the lowest level primitives so uh this comparative scheduling framework",
    "start": "594160",
    "end": "599920"
  },
  {
    "text": "with with c plus plus futures and the we use the library called c star",
    "start": "599920",
    "end": "605600"
  },
  {
    "text": "that gives us this threat record architecture um and a comparative scheduling",
    "start": "605600",
    "end": "611200"
  },
  {
    "text": "mechanism and so uh go has a similar thing where every time you return on a function call the",
    "start": "611200",
    "end": "617279"
  },
  {
    "text": "comparative scheduler gets to choose what is the next go routine that is going to run similarly for",
    "start": "617279",
    "end": "623440"
  },
  {
    "text": "cstar every time you return a future the future is either executing or",
    "start": "623440",
    "end": "630000"
  },
  {
    "text": "suspended there is no possible there is no other state about this particular future and so what it is",
    "start": "630000",
    "end": "636160"
  },
  {
    "text": "is that you start to worry about software about the concurrent structure of your",
    "start": "636160",
    "end": "641519"
  },
  {
    "text": "program rather than the parallelism and the parallelism is a free variable that is decided at runtime",
    "start": "641519",
    "end": "649120"
  },
  {
    "text": "so you can take a program that was built for concurrent structure on your four core laptop and run it on",
    "start": "649120",
    "end": "656720"
  },
  {
    "text": "225 physical course or you know present and",
    "start": "656720",
    "end": "661760"
  },
  {
    "text": "say virtual course um present it to to your program on on google",
    "start": "661760",
    "end": "667839"
  },
  {
    "text": "on on gcp and know that because you've programmed to the correct",
    "start": "667839",
    "end": "674079"
  },
  {
    "text": "concurrent structure parallelism is a variable that you can turn that you can tune at runtime",
    "start": "674079",
    "end": "680480"
  },
  {
    "text": "the idea here is that these futures are viral primitives and so just like actors and you know in",
    "start": "680480",
    "end": "686640"
  },
  {
    "text": "frameworks like orleans or akka or pony the language they're really composable and so you can",
    "start": "686640",
    "end": "692959"
  },
  {
    "text": "mix them map reduce them for an aggregation you can filter them change them complete a particular future you",
    "start": "692959",
    "end": "699040"
  },
  {
    "text": "can have a generator sleeves for example is a really powerful mechanism because",
    "start": "699040",
    "end": "705839"
  },
  {
    "text": "sleep on a future is actually just a suspension of the particular frame",
    "start": "711600",
    "end": "718000"
  },
  {
    "text": "um in in systar the framework that we're",
    "start": "718000",
    "end": "723200"
  },
  {
    "text": "using it gives you no no locks once once you",
    "start": "723200",
    "end": "729120"
  },
  {
    "text": "enter the thread local domain you don't have to worry about other",
    "start": "729120",
    "end": "734240"
  },
  {
    "text": "actual parallel access to the same variable because",
    "start": "734240",
    "end": "740399"
  },
  {
    "text": "there's exactly one thread to do everything and so it's really freeing it is limited",
    "start": "740399",
    "end": "745839"
  },
  {
    "text": "in that there's exactly one unit one way to write code but it is freeing in that as long as your",
    "start": "745839",
    "end": "752160"
  },
  {
    "text": "domain fits into this threadbar core model and we made the kafka api fit in this",
    "start": "752160",
    "end": "757279"
  },
  {
    "text": "bottle so we think it's fairly flexible then you no longer have to worry about parallel access to a particular variable",
    "start": "757279",
    "end": "764480"
  },
  {
    "text": "and so a lot of problems with with parallel programming just sort of go",
    "start": "764480",
    "end": "769519"
  },
  {
    "text": "away because you are sort of explicitly doing this from from the get-go",
    "start": "769519",
    "end": "777440"
  },
  {
    "start": "777000",
    "end": "777000"
  },
  {
    "text": "that one of the so we'll cover eight techniques in this talk the first technique that we use to achieve 10 to",
    "start": "777440",
    "end": "782480"
  },
  {
    "text": "100x lower to latency inward panda and exposing the kafka api",
    "start": "782480",
    "end": "788079"
  },
  {
    "text": "was to use no virtual memory and this is rather a contentious object but what we do is remember that we're",
    "start": "788079",
    "end": "794240"
  },
  {
    "text": "using a threat record architecture and so when the program starts up we allocate the entire machine's memory",
    "start": "794240",
    "end": "801680"
  },
  {
    "text": "and we actually do some reservation for the operating system and demons etc but the point is that you",
    "start": "801680",
    "end": "807920"
  },
  {
    "text": "take the memory that you present to the program and the program allocates a hundred",
    "start": "807920",
    "end": "814480"
  },
  {
    "text": "percent of the memory presented and then you split the memory evenly across the number of",
    "start": "814480",
    "end": "821279"
  },
  {
    "text": "course so let's say that you have 10 cores and 20 gigabytes of ram",
    "start": "821279",
    "end": "826399"
  },
  {
    "text": "well then every core is gonna have two two gigabytes of memory for it",
    "start": "826399",
    "end": "831600"
  },
  {
    "text": "and that is it a hundred percent like as in if your core allocates more than two gigabytes of memory you will oom and",
    "start": "831600",
    "end": "838639"
  },
  {
    "text": "crash your program but what it gives you is this low latency thread local",
    "start": "838639",
    "end": "845519"
  },
  {
    "text": "memory allocation access and at the lowest level we use a body allocator and the body allocator says",
    "start": "845519",
    "end": "853440"
  },
  {
    "text": "it's it's this basically um it divides memory by half every time he",
    "start": "853440",
    "end": "858720"
  },
  {
    "text": "needs a new region and so the mental model is that for allocation pools right every",
    "start": "858720",
    "end": "864240"
  },
  {
    "text": "time you say new integer it has to come from somewhere right and so we swap the allocator library so that",
    "start": "864240",
    "end": "871440"
  },
  {
    "text": "all allocation pools below 64 kilobytes are you know part of part of a range of",
    "start": "871440",
    "end": "876720"
  },
  {
    "text": "allocations so that you could do you could recycle the allocations etc and everything above",
    "start": "876720",
    "end": "882399"
  },
  {
    "text": "64 kilobytes just gets added to the to the large uh memory pool allocation",
    "start": "882399",
    "end": "888720"
  },
  {
    "text": "and so one thing to highlight here is it is difficult to use this technique in practice because every data structure",
    "start": "888720",
    "end": "895839"
  },
  {
    "text": "you have to think a priori you're like oh how much memory am i going to dedicate to my rpc subsystem versus my",
    "start": "895839",
    "end": "903120"
  },
  {
    "text": "compaction and my background threading model versus you know any any other part of the subsystem or",
    "start": "903120",
    "end": "909839"
  },
  {
    "text": "my reader head for the disc or my right behind you know my my my training for my",
    "start": "909839",
    "end": "915920"
  },
  {
    "text": "dictionary compression etc as a programmer just like you have explicitly opted into",
    "start": "915920",
    "end": "923040"
  },
  {
    "text": "a concurrent and parallel execution model you're also opting in to deciding",
    "start": "923040",
    "end": "930000"
  },
  {
    "text": "explicitly where and how your memory is allocated and this is perhaps one of the more",
    "start": "930000",
    "end": "936720"
  },
  {
    "text": "difficult mental sort of modeling techniques that we have to retreat that we had to retrain",
    "start": "936720",
    "end": "943040"
  },
  {
    "text": "ourselves so that we we don't own uh especially this is really hard as memory",
    "start": "943040",
    "end": "948639"
  },
  {
    "text": "fragmentation increases over time the second technique is we call io buff",
    "start": "948639",
    "end": "954079"
  },
  {
    "start": "951000",
    "end": "951000"
  },
  {
    "text": "and it's a throw per core buffer management system so the idea uh with",
    "start": "954079",
    "end": "960079"
  },
  {
    "text": "threat record architecture is that once you land on a particular core and let's reduce the mental complexity model for",
    "start": "960079",
    "end": "966320"
  },
  {
    "text": "the sake of this talk into having a single core right let's say two cars so that the mental",
    "start": "966320",
    "end": "972560"
  },
  {
    "text": "model is simple for fragmentation as your program",
    "start": "972560",
    "end": "978399"
  },
  {
    "text": "runs over time let's say weeks or months uh your memory is going to to start to",
    "start": "978399",
    "end": "983759"
  },
  {
    "text": "look like uh you know great cheese right like it has this particular holes of memory",
    "start": "983759",
    "end": "989199"
  },
  {
    "text": "and so what an i above optimization does for us is that instead of forcing a reallocation and compaction of memory",
    "start": "989199",
    "end": "996959"
  },
  {
    "text": "where it's it's similar to the disk defragmentation in the windows you know",
    "start": "996959",
    "end": "1002880"
  },
  {
    "text": "on the spinning disk days where you would click you know defragment disk and you would take a bunch of fragments and",
    "start": "1002880",
    "end": "1009360"
  },
  {
    "text": "put them together and all of a sudden your program would get um faster so similar things happen to memory where",
    "start": "1009360",
    "end": "1017680"
  },
  {
    "text": "as a function of time your program is going to allocate memory in in the entire memory region allocated to it and",
    "start": "1017680",
    "end": "1024240"
  },
  {
    "text": "it's never going to be uh pretty it's never going to be like the program always allocates from the",
    "start": "1024240",
    "end": "1029918"
  },
  {
    "text": "bottom part of the memory it's just the the allocator just keeps a pointer to the next to where it is that the next",
    "start": "1029919",
    "end": "1036079"
  },
  {
    "text": "free slot and then you know it moves around and scans until it finds the next pointer and so it's a really low latency in that",
    "start": "1036079",
    "end": "1042400"
  },
  {
    "text": "it doesn't do any compaction at runtime this is fundamentally different how some of the you know virtual memory",
    "start": "1042400",
    "end": "1048640"
  },
  {
    "text": "subsystems work but fundamentally we never allocate a large linearized memory because that",
    "start": "1048640",
    "end": "1055280"
  },
  {
    "text": "puts a lot of pressure on our memory allocator the body allocator and if not done correctly if if you do",
    "start": "1055280",
    "end": "1061679"
  },
  {
    "text": "that for really large allocations then um you can actually end up um in your system remember that we only have",
    "start": "1061679",
    "end": "1067760"
  },
  {
    "text": "usually around two gigabytes per core and we don't use virtual memory and so what this is saying is like let's",
    "start": "1067760",
    "end": "1073840"
  },
  {
    "text": "embrace that you are going to be mostly in a memory",
    "start": "1073840",
    "end": "1078880"
  },
  {
    "text": "fragmented world and let's just use memory um",
    "start": "1078880",
    "end": "1084000"
  },
  {
    "text": "as efficiently as possible without causing this latency spike of um",
    "start": "1084000",
    "end": "1089600"
  },
  {
    "text": "you know of of causing a compaction where you take all of your memory you re-compact it to the bottom part of the",
    "start": "1089600",
    "end": "1094799"
  },
  {
    "text": "region and you leave this the upper regions of memory free so you could do linearized memory allocation instead we",
    "start": "1094799",
    "end": "1101520"
  },
  {
    "text": "built iterate we built this structure that allows us to iterate over data structures as though they were",
    "start": "1101520",
    "end": "1107360"
  },
  {
    "text": "contiguous memory in code but in reality it's backed by a bunch of fragmented buffers we weren't",
    "start": "1107360",
    "end": "1114000"
  },
  {
    "text": "the first ones to invent this this thing this the linux kernel has a k-buff",
    "start": "1114000",
    "end": "1119200"
  },
  {
    "text": "implementation that is very similar and the freebsd kernel has another implementation called mbuff which is",
    "start": "1119200",
    "end": "1125120"
  },
  {
    "text": "exactly the same and these techniques have been around for a long time what we've added here",
    "start": "1125120",
    "end": "1131360"
  },
  {
    "text": "is we've sort of tied it to our threat record architecture and so instead of",
    "start": "1131360",
    "end": "1136640"
  },
  {
    "text": "having a global deallocation pool when you send memory to a remote region",
    "start": "1136640",
    "end": "1142400"
  },
  {
    "text": "instead of calling free on that remote region because that region doesn't own that particular core doesn't actually",
    "start": "1142400",
    "end": "1147760"
  },
  {
    "text": "own the data it'll send a message back to the originating core and then the",
    "start": "1147760",
    "end": "1153200"
  },
  {
    "text": "originated core has to delete the memory and this is sort of all handled for you as a as a buffer abstraction and so as",
    "start": "1153200",
    "end": "1159679"
  },
  {
    "text": "long as you build everything on on on top of this buffer abstraction uh you",
    "start": "1159679",
    "end": "1164720"
  },
  {
    "text": "know largely your code ends up looking roughly dissimilar or the same",
    "start": "1164720",
    "end": "1170559"
  },
  {
    "start": "1170000",
    "end": "1170000"
  },
  {
    "text": "the next technique is out of order rights and this is really important for trying to maximize uh",
    "start": "1170559",
    "end": "1176240"
  },
  {
    "text": "throughput of the device and it's in like your physical uh",
    "start": "1176240",
    "end": "1181520"
  },
  {
    "text": "hard drive has a particular set of parallelism and and um and also concurrent structure",
    "start": "1181520",
    "end": "1189120"
  },
  {
    "text": "but to to highlight this technique is we",
    "start": "1189120",
    "end": "1194480"
  },
  {
    "text": "ask the the file system hey give me your your um your alignment for writing a",
    "start": "1194480",
    "end": "1200320"
  },
  {
    "text": "page to effectively the nvme ssd controller and it tells you uh let's say 4096 bytes is the alignment",
    "start": "1200320",
    "end": "1207679"
  },
  {
    "text": "and so when you allocate something you say okay give me a hardware align",
    "start": "1207679",
    "end": "1213080"
  },
  {
    "text": "4096 byte boundary and then i'm going to write the page in this particular case the size is 128",
    "start": "1213080",
    "end": "1220240"
  },
  {
    "text": "kilobytes for this example instead of dispatching one waiting for",
    "start": "1220240",
    "end": "1225280"
  },
  {
    "text": "acknowledgement dispatching the next one waiting for acknowledgement and so on we dispatch them all and simply do this",
    "start": "1225280",
    "end": "1231200"
  },
  {
    "text": "scatter gather approach to writing a particular set of pages so that what we are doing is we're",
    "start": "1231200",
    "end": "1238720"
  },
  {
    "text": "trying to saturate the hardware while minimizing latency so you're doing both",
    "start": "1238720",
    "end": "1244000"
  },
  {
    "text": "things here you are reducing the latency for a particular batch of operations not",
    "start": "1244000",
    "end": "1249200"
  },
  {
    "text": "a particular operation but a particular batch of operations and uh so the average latency is much",
    "start": "1249200",
    "end": "1256080"
  },
  {
    "text": "lower right so the average latency for writing a block in this case is higher but the",
    "start": "1256080",
    "end": "1261200"
  },
  {
    "text": "average latency for writing all of them is much much lower it's probably like 1.5 x the cost of writing one",
    "start": "1261200",
    "end": "1268640"
  },
  {
    "text": "and your throughput in this case you know could potentially quadruple um so this is a really powerful",
    "start": "1268640",
    "end": "1274320"
  },
  {
    "text": "technique when you're trying to saturate the dma device uh sorry the nvme uh device",
    "start": "1274320",
    "end": "1281280"
  },
  {
    "start": "1280000",
    "end": "1280000"
  },
  {
    "text": "the next techniques that we use uh you know sort of building on top of this swept record architecture is no page",
    "start": "1281280",
    "end": "1288240"
  },
  {
    "text": "cache and this is uh you know the second most most uh controversial thing for us",
    "start": "1288240",
    "end": "1294559"
  },
  {
    "text": "uh for for the particular application that we're building which is a red panda a a drop in kafka replacement right so",
    "start": "1294559",
    "end": "1301520"
  },
  {
    "text": "we give the user a kafka api you can take your 100 000 you know kafka api",
    "start": "1301520",
    "end": "1307440"
  },
  {
    "text": "compatible code and point it at red panda and it should just work there should be no code changes so that's the",
    "start": "1307440",
    "end": "1312960"
  },
  {
    "text": "api we give the user kafka as an api has a very specific you",
    "start": "1312960",
    "end": "1319280"
  },
  {
    "text": "know it is a very specific problem it is not a generic problem and so",
    "start": "1319280",
    "end": "1324640"
  },
  {
    "text": "to us the linux kernel page cache actually introduces non-determinism in the i o path and at worst it introduces",
    "start": "1324640",
    "end": "1332240"
  },
  {
    "text": "correctness bugs so let's walk through it every time you instantiate a file handle the kernel will instantiate a",
    "start": "1332240",
    "end": "1338400"
  },
  {
    "text": "page cache object that has global locking semantics for that particular page cache object",
    "start": "1338400",
    "end": "1344640"
  },
  {
    "text": "so if you have two writers or if you have a writer and a reader they're all competing and",
    "start": "1344640",
    "end": "1350559"
  },
  {
    "text": "contending on that particular lock um especially if you have tailing iterators",
    "start": "1350559",
    "end": "1357280"
  },
  {
    "text": "uh to to get access to get exclusive access to that particular memory region so so",
    "start": "1357280",
    "end": "1362720"
  },
  {
    "text": "that's the first one but we've architected our code in a way that is a thread per core which means we have a single thing at any point in time",
    "start": "1362720",
    "end": "1370159"
  },
  {
    "text": "accessing files and so perhaps the way i like to frame this as a general rule of thumb is that",
    "start": "1370159",
    "end": "1377280"
  },
  {
    "text": "the page cache the generic page cache is never a bad choice it's just always not a good choice",
    "start": "1377280",
    "end": "1383039"
  },
  {
    "text": "if you have an application specific problem that you're trying to solve like offering our users",
    "start": "1383039",
    "end": "1388480"
  },
  {
    "text": "you know effectively a log interface with the kafka api access patterns and so it is always",
    "start": "1388480",
    "end": "1395360"
  },
  {
    "text": "really a good middle ground to use and and and writing your code against cma is really tricky and hard to get right and",
    "start": "1395360",
    "end": "1402320"
  },
  {
    "text": "it took us a couple of years to make sure that is that it's solid and and it crashes correctly and so on",
    "start": "1402320",
    "end": "1407760"
  },
  {
    "text": "um and the last one that i'll mention here is that it introduces hearts to trackbox",
    "start": "1407760",
    "end": "1414400"
  },
  {
    "text": "and i linked in the presentation uh a postgres blog where",
    "start": "1414400",
    "end": "1420559"
  },
  {
    "text": "you know it was it was basically uh the bug was there for 20 years in postgres and",
    "start": "1420559",
    "end": "1426559"
  },
  {
    "text": "postgres is one of the better engineers out in the world and so so you can understand you can start to understand",
    "start": "1426559",
    "end": "1432320"
  },
  {
    "text": "why a generic scheduling algorithm is not really that useful when you have",
    "start": "1432320",
    "end": "1438480"
  },
  {
    "text": "specific embedded domain knowledge about the problem you're trying to solve what eliminating the page catch gives us",
    "start": "1438480",
    "end": "1444640"
  },
  {
    "text": "though in particular for performance because remember the the goal is to get 10 to 100 x better tail latencies is that it",
    "start": "1444640",
    "end": "1451760"
  },
  {
    "text": "gives us a thread local so really low latency cache that the format is ready to be shipped",
    "start": "1451760",
    "end": "1457679"
  },
  {
    "text": "straight onto the wire so there's very little translation right we move away from caching pages into caching objects",
    "start": "1457679",
    "end": "1463919"
  },
  {
    "text": "the reason that works for us is because the kafka model is just a header with a bunch of payload data",
    "start": "1463919",
    "end": "1469279"
  },
  {
    "text": "and so we paid the cost of materializing the header a priori so that we can just take that entire byte array and ship it over onto",
    "start": "1469279",
    "end": "1476559"
  },
  {
    "text": "the wire when the next request comes in so there's really no translation",
    "start": "1476559",
    "end": "1481919"
  },
  {
    "text": "and perhaps more fundamentally is that we can learn the statistics from",
    "start": "1481919",
    "end": "1487039"
  },
  {
    "text": "the file system latency back pressure and actually fail the next request one example is assume",
    "start": "1487039",
    "end": "1492880"
  },
  {
    "text": "that you're trying to heartbeat a particular endpoint um and you know that in order to do the",
    "start": "1492880",
    "end": "1499279"
  },
  {
    "text": "next heartbeat you have to write some data on the file system well we know if the current latency is twice as much as",
    "start": "1499279",
    "end": "1505039"
  },
  {
    "text": "the next heartbeat then we can simply fail it without adding additional throughput or latency onto you know that",
    "start": "1505039",
    "end": "1511760"
  },
  {
    "text": "is effectively wasted work on on the disk and so",
    "start": "1511760",
    "end": "1516960"
  },
  {
    "text": "that transparency really bases a lot of performance when you get to the saturation when you get the to the to the tail latency scale",
    "start": "1516960",
    "end": "1525039"
  },
  {
    "text": "so above 99.9 percentile and and every percentile between that and the max",
    "start": "1525039",
    "end": "1530080"
  },
  {
    "text": "latency another big allocation that we have three left is adaptive allocation",
    "start": "1530080",
    "end": "1537279"
  },
  {
    "start": "1531000",
    "end": "1531000"
  },
  {
    "text": "this is really a trivial thing that i think a lot of applications could could really benefit from you don't really",
    "start": "1537279",
    "end": "1543360"
  },
  {
    "text": "need even a threat record architecture here the idea is that every time you update a global metadata like your file size it",
    "start": "1543360",
    "end": "1551360"
  },
  {
    "text": "incurs all sorts of additional costs in the linux kernel it might have to update other inodes or other metadata whether",
    "start": "1551360",
    "end": "1557360"
  },
  {
    "text": "it's folder metadata or just the file system and the data but anyways",
    "start": "1557360",
    "end": "1562559"
  },
  {
    "text": "that operation is a global operation and it requires a lock on the kernel and so what we're doing",
    "start": "1562559",
    "end": "1568799"
  },
  {
    "text": "here is you can use the follow cases call to effectively write zeros every 32",
    "start": "1568799",
    "end": "1574159"
  },
  {
    "text": "megabytes and so instead of doing f sync we do f data sync because we know we have the",
    "start": "1574159",
    "end": "1579520"
  },
  {
    "text": "size and every time we're about to heat over that 32 megabytes we pre-allocated or 32 megabytes",
    "start": "1579520",
    "end": "1585600"
  },
  {
    "text": "and in real life use case that has anywhere from 15 to 20 latency improvement by doing this ahead of time",
    "start": "1585600",
    "end": "1592960"
  },
  {
    "text": "you know effectively what you're doing is you're amortizing the cost of updating metadata in the kernel",
    "start": "1592960",
    "end": "1599840"
  },
  {
    "text": "how does this all of this relate to to raft by bypassing the page cache and virtual",
    "start": "1600080",
    "end": "1606000"
  },
  {
    "text": "memory and all of these techniques we get to embed hardware level knowledge into uh",
    "start": "1606000",
    "end": "1613520"
  },
  {
    "text": "interrupt but one really interesting use case that we found while trying to optimize trying to reduce the flushes",
    "start": "1613520",
    "end": "1620000"
  },
  {
    "text": "which are a global file system operation is um every time you're right in raft you",
    "start": "1620000",
    "end": "1627120"
  },
  {
    "text": "have to flush to guarantee durability of the rights on this and so what we did is we learned from",
    "start": "1627120",
    "end": "1633279"
  },
  {
    "text": "the guys that were building hardware where they get to scan the next series of operations and reorder them as long as",
    "start": "1633279",
    "end": "1640320"
  },
  {
    "text": "the resulting set doesn't change the semantics so we can't",
    "start": "1640320",
    "end": "1645360"
  },
  {
    "text": "change the right order but what we can do is we can drop the flushes if you have the right flush right flush right",
    "start": "1645360",
    "end": "1652880"
  },
  {
    "text": "flush ratio what we do is we say okay for this batch of operations",
    "start": "1652880",
    "end": "1658080"
  },
  {
    "text": "we're going to write three times drop two flushes and then flush at the end of",
    "start": "1658080",
    "end": "1664159"
  },
  {
    "text": "the call and that has this huge performance improvement at the cost of a throughput improvement",
    "start": "1664159",
    "end": "1671120"
  },
  {
    "text": "at the cost of actually artificially debouncing latency so actually increasing the",
    "start": "1671120",
    "end": "1676399"
  },
  {
    "text": "latency just a little bit reduces the overall system latency and",
    "start": "1676399",
    "end": "1681679"
  },
  {
    "text": "increases the throughput and so this is really useful for us and we do",
    "start": "1681679",
    "end": "1688080"
  },
  {
    "text": "this because we get to learn transparently what the latencies being propagated from this are",
    "start": "1688080",
    "end": "1694080"
  },
  {
    "text": "all the way into higher application level protocols and so this gives if you really think about it what we're doing",
    "start": "1694080",
    "end": "1700240"
  },
  {
    "text": "here is we're just doing better buffer packing at the lowest level bits right instead of writing half a page flush",
    "start": "1700240",
    "end": "1706399"
  },
  {
    "text": "read the next half page etc what we're doing is we're writing bigger buffers and and then we're flushing bigger",
    "start": "1706399",
    "end": "1712320"
  },
  {
    "text": "buffers and we're reducing the number of flash operations",
    "start": "1712320",
    "end": "1716799"
  },
  {
    "start": "1716000",
    "end": "1716000"
  },
  {
    "text": "then the next one that i think is really important when you build the threat per core model is",
    "start": "1717840",
    "end": "1723039"
  },
  {
    "text": "an execution model for pipelining and pipelining is really critical in order to get latency why without pipelining",
    "start": "1723039",
    "end": "1730720"
  },
  {
    "text": "you can do the de-bouncing trick that i mentioned right so if you dispatch one request wait for it to finish and then",
    "start": "1730720",
    "end": "1736880"
  },
  {
    "text": "you know do the next one and do the next one that sequential order and it's really really slow and so what we do is",
    "start": "1736880",
    "end": "1742640"
  },
  {
    "text": "the dispatching once you get a tcp connection accepted let's say on in this example is on core 0",
    "start": "1742640",
    "end": "1749360"
  },
  {
    "text": "and you write into both core 0 and core 1 in this particular example",
    "start": "1749360",
    "end": "1755360"
  },
  {
    "text": "what we do is we we figure out the assignment",
    "start": "1755360",
    "end": "1760399"
  },
  {
    "text": "of okay which cores are actually going to handle this particular data and then for those cores we simply",
    "start": "1760399",
    "end": "1767039"
  },
  {
    "text": "pipeline the writes so if they're on two different course you're actually getting true parallelism and if you're in the",
    "start": "1767039",
    "end": "1773039"
  },
  {
    "text": "same core then you're getting pipelining and you can take advantage of a lot of other optimizations like the debouncing",
    "start": "1773039",
    "end": "1779440"
  },
  {
    "text": "for lowering the number of flush operations",
    "start": "1779440",
    "end": "1784240"
  },
  {
    "start": "1784000",
    "end": "1784000"
  },
  {
    "text": "and the last one perhaps the most important one that you'll do is",
    "start": "1785520",
    "end": "1790720"
  },
  {
    "text": "when you adopt a threepercore execution model its metadata becomes really difficult",
    "start": "1790720",
    "end": "1797039"
  },
  {
    "text": "and the idea is that you no longer have global metadata and so if you don't have global metadata",
    "start": "1797039",
    "end": "1802640"
  },
  {
    "text": "you always are going to be operating on a stale metadata and so you have to embrace that idea right you are will",
    "start": "1802640",
    "end": "1809039"
  },
  {
    "text": "never have a global view of the data it is very expensive to do a synchronization event across 225 course",
    "start": "1809039",
    "end": "1816080"
  },
  {
    "text": "right that that includes basically pushing a barrier to all of the cores serializing the metadata and then making",
    "start": "1816080",
    "end": "1822240"
  },
  {
    "text": "sure that every core has a copy of it instead what we do is every time we send data to a remote core",
    "start": "1822240",
    "end": "1829039"
  },
  {
    "text": "we piggyback information on the way back that is useful to the protocol dispatcher so for the kafka model we say",
    "start": "1829039",
    "end": "1835279"
  },
  {
    "text": "okay go write this partition on a remote core and on your way back tell me what are the latency statistics of disk for",
    "start": "1835279",
    "end": "1841440"
  },
  {
    "text": "that particular core how much data is there on that particular core if this",
    "start": "1841440",
    "end": "1846480"
  },
  {
    "text": "tcp connection is a consumer let me know what is the gap between my head and the tail of the log so that i can start to",
    "start": "1846480",
    "end": "1853200"
  },
  {
    "text": "do an execution planning on the next phase that comes in and this particular metadata",
    "start": "1853200",
    "end": "1858960"
  },
  {
    "text": "piggybacking has had some of the most profound effects in simplifying the architecture so",
    "start": "1858960",
    "end": "1866240"
  },
  {
    "text": "we could do this many different ways you could you know you could add an exception to the rule and simply say",
    "start": "1866240",
    "end": "1871679"
  },
  {
    "text": "okay for for metadata we're going to have a global lock etc but what we found",
    "start": "1871679",
    "end": "1876960"
  },
  {
    "text": "is that piggybacking information is very cheap computationally speaking right you're copying integers around",
    "start": "1876960",
    "end": "1884080"
  },
  {
    "text": "and two really gives the tcp connection the view that it needs to do the job it",
    "start": "1884080",
    "end": "1890480"
  },
  {
    "text": "doesn't have to be correct at all times or and by correct it just can't be",
    "start": "1890480",
    "end": "1895919"
  },
  {
    "text": "incorrect right it can have a partial view of the data which is fine because not every core needs to have an",
    "start": "1895919",
    "end": "1902080"
  },
  {
    "text": "understanding of what every other core is doing it just needs to know the information that it needs to know and so it's a much smaller problem",
    "start": "1902080",
    "end": "1909440"
  },
  {
    "text": "and piggybacking information on the way back from a remote core to the originating core was one of the most profound architectural differences",
    "start": "1909440",
    "end": "1917600"
  },
  {
    "start": "1917000",
    "end": "1917000"
  },
  {
    "text": "with that i will wrap it up the bottom two graphs show",
    "start": "1917600",
    "end": "1922880"
  },
  {
    "text": "uh you know what are what are the difference in impact in terms of performance and what",
    "start": "1922880",
    "end": "1928640"
  },
  {
    "text": "i'll highlight here is that a lot of these techniques are really aim for anything above the p90th percentile",
    "start": "1928640",
    "end": "1934320"
  },
  {
    "text": "as you can see um i'm comparing kafka in black with red panda and red",
    "start": "1934320",
    "end": "1940080"
  },
  {
    "text": "and one once red panda understand the hardware saturation right if your hardware isn't being saturated there's",
    "start": "1940080",
    "end": "1945120"
  },
  {
    "text": "really very little that we could do if there's lots of room for computation and growth and and there's really no no",
    "start": "1945120",
    "end": "1950960"
  },
  {
    "text": "back pressure all of these techniques matter when you start to care about the tail latency when you start to care",
    "start": "1950960",
    "end": "1957840"
  },
  {
    "text": "about you know basically the entire distribution of your users and when you care to give predictable performance to",
    "start": "1957840",
    "end": "1964640"
  },
  {
    "text": "your application all of this code is available online if you have any questions i'm always on on",
    "start": "1964640",
    "end": "1969919"
  },
  {
    "text": "slack feel free to chat me there or feel free to say hi on twitter i hope you enjoy all right thanks",
    "start": "1969919",
    "end": "1977840"
  },
  {
    "text": "excellent talk uh we have only about five minutes for questions uh i think",
    "start": "1980320",
    "end": "1985760"
  },
  {
    "text": "can you join the hangouts just after this sure it's on zoom so if people have",
    "start": "1985760",
    "end": "1991760"
  },
  {
    "text": "questions that's the best way to talk to you um i think you've answered a bunch of",
    "start": "1991760",
    "end": "1997200"
  },
  {
    "text": "questions already um i think listening to this one question i have is",
    "start": "1997200",
    "end": "2003600"
  },
  {
    "text": "do you still need an operating system or is to have your operating system become your enemy",
    "start": "2003600",
    "end": "2009760"
  },
  {
    "text": "yeah that's a that's a great question um [Music] not not we you know",
    "start": "2009760",
    "end": "2016000"
  },
  {
    "text": "i've thought i've thought a lot about this um not not as much as as as as other programs do um",
    "start": "2016000",
    "end": "2022520"
  },
  {
    "text": "[Music] you know we really kind of want to own most of it for for predictable latencies",
    "start": "2022520",
    "end": "2028399"
  },
  {
    "text": "and so the next step for us would be to write our own file system and we're actually not very far from it so if you look at",
    "start": "2028399",
    "end": "2035279"
  },
  {
    "text": "the kafka api we keep track of of timestamps of offset of metadata we have",
    "start": "2035279",
    "end": "2041440"
  },
  {
    "text": "multiple indexes and so you know i've thought about actually writing our own file system so that it",
    "start": "2041440",
    "end": "2048000"
  },
  {
    "text": "just like you open up a block device file that's supposed to open up a file handle file you just open up",
    "start": "2048000",
    "end": "2054158"
  },
  {
    "text": "just the device block file and you do a little bit of recovery and we have to do a lot of these techniques already if",
    "start": "2054159",
    "end": "2060398"
  },
  {
    "text": "you're any kind of storage system because if the system crashes you have to look up some metadata and",
    "start": "2060399",
    "end": "2066320"
  },
  {
    "text": "then you have to recover and recheck some that the data is the thing that you you know that the program said it is",
    "start": "2066320",
    "end": "2071440"
  },
  {
    "text": "stored and so if we have recovery if we have time stamp if we have offset we have",
    "start": "2071440",
    "end": "2076638"
  },
  {
    "text": "sizes we have minute data compression then what is the file system really doing for us you know what it's really doing is it's sort of",
    "start": "2076639",
    "end": "2082878"
  },
  {
    "text": "being compatible with other programs and the reason we we haven't gone into just a fool on include os is that there's",
    "start": "2082879",
    "end": "2090800"
  },
  {
    "text": "just other programs that people need to run often on the same machine whether it's for like",
    "start": "2090800",
    "end": "2096240"
  },
  {
    "text": "you know authorization authentication like in our cloud there's like an ssh program that verifies that you know only",
    "start": "2096240",
    "end": "2102160"
  },
  {
    "text": "the sre is allowed to ssh into that machine and so it becomes",
    "start": "2102160",
    "end": "2107280"
  },
  {
    "text": "uh kind of a an operational hassle to to to just own them you know really create your own os image",
    "start": "2107280",
    "end": "2113680"
  },
  {
    "text": "and um but yeah right now um you're right i think that's a really",
    "start": "2113680",
    "end": "2118720"
  },
  {
    "text": "fantastic observation we really try to to just own the whole thing which is funny",
    "start": "2118720",
    "end": "2124240"
  },
  {
    "text": "yeah it seems forget the name now but there's a trend of uh things of exokernel type",
    "start": "2124240",
    "end": "2131839"
  },
  {
    "text": "development where you basically link your program into the kernel if as long as you use a safe language",
    "start": "2131839",
    "end": "2136960"
  },
  {
    "text": "it's fine so if you consider that maybe as an option or ramp kernels i think they're also",
    "start": "2136960",
    "end": "2142839"
  },
  {
    "text": "called yeah no and you know there are there isn't that much like all we need is really the initialization sequence",
    "start": "2142839",
    "end": "2150320"
  },
  {
    "text": "um of of like okay bootstrap and really start this process but in terms of owning it in terms of like managing",
    "start": "2150320",
    "end": "2157040"
  },
  {
    "text": "memory like you mentioned cpu cross-core one of i think the hardest thing was to",
    "start": "2157040",
    "end": "2162160"
  },
  {
    "text": "do proper cpu accounting and and i posted a link on the chat",
    "start": "2162160",
    "end": "2168160"
  },
  {
    "text": "it's really not just crossing the memory boundary is actually crossing the memory bounding boundary with",
    "start": "2168160",
    "end": "2174640"
  },
  {
    "text": "with accounting so you say okay what is what is my budget to to leave my core",
    "start": "2174640",
    "end": "2180880"
  },
  {
    "text": "and and otherwise i need to do local operations before i spend all my budget all my latency going to remote course",
    "start": "2180880",
    "end": "2187839"
  },
  {
    "text": "so once once we got that worked out in red panda um then the rest you know i",
    "start": "2187839",
    "end": "2193520"
  },
  {
    "text": "really started thinking about that there's so many cool problems to solve that it's just it's hard to",
    "start": "2193520",
    "end": "2198640"
  },
  {
    "text": "hard to work on that i think it's only next year so um",
    "start": "2198640",
    "end": "2204880"
  },
  {
    "text": "two minutes um maybe it's a bad question to ask with two minutes to go but",
    "start": "2204880",
    "end": "2210000"
  },
  {
    "text": "what do you think about um has the the rise of arm on the server",
    "start": "2210000",
    "end": "2216320"
  },
  {
    "text": "uh is this interesting for you or is it uh hugely interesting yeah",
    "start": "2216320",
    "end": "2221599"
  },
  {
    "text": "so we uh we're releasing um arm support hopefully this week or next",
    "start": "2221599",
    "end": "2227119"
  },
  {
    "text": "week our rva support and what it does is actually your price per byte on on a on",
    "start": "2227119",
    "end": "2233119"
  },
  {
    "text": "a storage subsystem even for your active workload it's hugely cheaper it's like 30 20 20 to 30",
    "start": "2233119",
    "end": "2240560"
  },
  {
    "text": "cheaper on the on the price per byte or or price per",
    "start": "2240560",
    "end": "2245680"
  },
  {
    "text": "dollar per cycle basically spent it's very very cheap um",
    "start": "2245680",
    "end": "2251440"
  },
  {
    "text": "i know i think it just has to do with the cost of how amazon is renting those computers to you is just cheaper um but",
    "start": "2251440",
    "end": "2257040"
  },
  {
    "text": "yeah and you know what's an interesting stuff it's someone in the in the audience asked it which is is uh is this designed for super high",
    "start": "2257040",
    "end": "2264320"
  },
  {
    "text": "you know like super tall machines and and the answer is actually the opposite and the idea is that when you design",
    "start": "2264320",
    "end": "2270480"
  },
  {
    "text": "your system concurrently then parallelism becomes a free variable so we have this embedded firewall company",
    "start": "2270480",
    "end": "2276720"
  },
  {
    "text": "that is using us in an arm setting actually not an arm sense uh just in a single core in a distributed",
    "start": "2276720",
    "end": "2283680"
  },
  {
    "text": "uh router and instead of using the file system they use the kafka api because they get",
    "start": "2283680",
    "end": "2289280"
  },
  {
    "text": "apples and you know tls and a bunch of like security and user management and so",
    "start": "2289280",
    "end": "2294640"
  },
  {
    "text": "uh you know encryption and so um yeah anyways i i think that is a that is",
    "start": "2294640",
    "end": "2300720"
  },
  {
    "text": "a really interesting thing to to think about parallelism and concurrency where parallelism is just a number of cores",
    "start": "2300720",
    "end": "2306720"
  },
  {
    "text": "and it's this free runtime variable and so you can run from one core all the way up to 225 cores right so i get the",
    "start": "2306720",
    "end": "2313920"
  },
  {
    "text": "signal that we have to wrap up uh but i think we'll see you in the hangout room thank you alex yeah all right thank you",
    "start": "2313920",
    "end": "2320480"
  },
  {
    "text": "guys bye everyone [Music]",
    "start": "2320480",
    "end": "2331270"
  },
  {
    "text": "you",
    "start": "2333520",
    "end": "2335599"
  }
]