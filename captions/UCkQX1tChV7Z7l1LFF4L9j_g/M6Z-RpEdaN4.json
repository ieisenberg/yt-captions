[
  {
    "text": "[Music]",
    "start": "3350",
    "end": "14249"
  },
  {
    "text": "Hello everybody welcome to this talk about change data capture for microservices let me set the scene a",
    "start": "15559",
    "end": "21760"
  },
  {
    "text": "little bit with a maybe blunt statement and an observation so the world around us this is happening in real time people",
    "start": "21760",
    "end": "28279"
  },
  {
    "text": "buy stuff in online store maybe they do some sort of payment transactions maybe you have Machinery or iot devices which",
    "start": "28279",
    "end": "35079"
  },
  {
    "text": "send over measurements or all kinds of sensor data and now we build software to",
    "start": "35079",
    "end": "40360"
  },
  {
    "text": "model to represent this soft this real world and now well the data in this software also should be real time and it",
    "start": "40360",
    "end": "46960"
  },
  {
    "text": "should be really up to date because otherwise well it's just losing very quickly in its value so let's say you",
    "start": "46960",
    "end": "53320"
  },
  {
    "text": "build some sort of um dashboard which tells you how your business is going this needs to be really up to date",
    "start": "53320",
    "end": "58840"
  },
  {
    "text": "because well if it works on data from yesterday or on the data from last week well it would just not be as insightful",
    "start": "58840",
    "end": "64799"
  },
  {
    "text": "to you and the information wouldn't be as relevant to you so that's the idea the world is real time our data also",
    "start": "64799",
    "end": "70960"
  },
  {
    "text": "needs to be real time and in this talk I would like to talk about one concept and one tool change data capture which can",
    "start": "70960",
    "end": "77360"
  },
  {
    "text": "help us to build software and systems which live up to this realtime um promise and this is what we are going to",
    "start": "77360",
    "end": "84400"
  },
  {
    "text": "talk about today so change data capture as a tool as a um um part of our toolbox",
    "start": "84400",
    "end": "90360"
  },
  {
    "text": "secondly a few use cases in the context specifically of microservices for change",
    "start": "90360",
    "end": "95840"
  },
  {
    "text": "data capture or CDC for short and lastly also I want to talk a little bit about some of the challenges which you might",
    "start": "95840",
    "end": "101640"
  },
  {
    "text": "encounter when deploying CDC into practice a few words about myself I work",
    "start": "101640",
    "end": "107719"
  },
  {
    "text": "as a software engineer at decodable where we built a fully managed uh data",
    "start": "107719",
    "end": "113439"
  },
  {
    "text": "stream processing platform and one of the tools it includes is um theum it's all based on a pat of Link um so this",
    "start": "113439",
    "end": "120039"
  },
  {
    "text": "explains a little bit why I do this talk and um why I'm interested in this space maybe you have also seen uh previous",
    "start": "120039",
    "end": "126479"
  },
  {
    "text": "talks from me where I spoke um about deum and CD CDC in more depth um just",
    "start": "126479",
    "end": "132000"
  },
  {
    "text": "because I've been the deum Project Lead for quite a while in my previous job um at redhead all right so let's get into",
    "start": "132000",
    "end": "138080"
  },
  {
    "text": "it let's get let's get into it let's talk about deum and I did spoke I did",
    "start": "138080",
    "end": "143879"
  },
  {
    "text": "speak about debm at cucum before so I just want to do a brief recap here what the general idea is um and then touch on",
    "start": "143879",
    "end": "151400"
  },
  {
    "text": "a few features which are maybe not as widely known yet and I would like to get out the word about those but what it is",
    "start": "151400",
    "end": "157840"
  },
  {
    "text": "about in a nutshell it is about capturing changes from your database so whenever something happens in your",
    "start": "157840",
    "end": "164000"
  },
  {
    "text": "database let's say postris or MySQL then um you know this insert or update or delete it will be appended to the",
    "start": "164000",
    "end": "170840"
  },
  {
    "text": "transaction lock and CDC in a nutshell is just a process of extracting those change events from the transaction log",
    "start": "170840",
    "end": "178120"
  },
  {
    "text": "and propagating them to all kinds of consumers and now maybe you wonder hey",
    "start": "178120",
    "end": "183319"
  },
  {
    "text": "there might be different approaches for reacting to changes out of a database so maybe I could Implement a trigger based",
    "start": "183319",
    "end": "188840"
  },
  {
    "text": "approach where where I essentially set up triggers for all the tables I'm interested in um or maybe you think",
    "start": "188840",
    "end": "194760"
  },
  {
    "text": "about some sort of polling based approach where in a continuously running Loop you would go to your tables and",
    "start": "194760",
    "end": "200680"
  },
  {
    "text": "query them for what are the latest changes let's say happening in the last few minutes um but really this log based",
    "start": "200680",
    "end": "207200"
  },
  {
    "text": "approach I would say it's the most Advan this one because it gives you a very low latency it doesn't impact your data",
    "start": "207200",
    "end": "214000"
  },
  {
    "text": "model um it also will never miss any changes let's say if you do a polling based approach you might for instance",
    "start": "214000",
    "end": "220200"
  },
  {
    "text": "miss two changes which happen between uh two of your polling Loop runs this could",
    "start": "220200",
    "end": "225840"
  },
  {
    "text": "never happen with a log based approach so that's essentially what it is and now the question is why would you be",
    "start": "225840",
    "end": "231519"
  },
  {
    "text": "interested in using such a tool or such a concept and um well I would always think about it as a huge enabler for",
    "start": "231519",
    "end": "238280"
  },
  {
    "text": "your database so once you for your data so once you are able to react to your data changes with a low latency and it",
    "start": "238280",
    "end": "244879"
  },
  {
    "text": "could for instance be like two digigit or three digigit digit milliseconds of latency um well you can use that",
    "start": "244879",
    "end": "252040"
  },
  {
    "text": "information for driving all kinds of use cases so just for instance to replicate your data into a data warehouse such as",
    "start": "252040",
    "end": "258160"
  },
  {
    "text": "snowflake or a Pipino maybe over to replicate your data into a search index or a cache um so that's all replication",
    "start": "258160",
    "end": "265000"
  },
  {
    "text": "in the widest sense but then you also could use CTC for instance to run streaming cues to um continuously",
    "start": "265000",
    "end": "272120"
  },
  {
    "text": "recompute queries and have um upto-date results uh or maybe anal analytical",
    "start": "272120",
    "end": "278840"
  },
  {
    "text": "query resights over your data you could use it to exchange data between microservices and I will talk a little",
    "start": "278840",
    "end": "284560"
  },
  {
    "text": "bit about this in more depth later on um you could use it to build all the loocks again we will also see this later on so",
    "start": "284560",
    "end": "290360"
  },
  {
    "text": "it's a it's a very powerful tool and I mean I've been working with the beum and CDC for the last well five and a bit",
    "start": "290360",
    "end": "296440"
  },
  {
    "text": "years and I still discover new use cases all the time so I feel it's a very versatile tool which you should have in",
    "start": "296440",
    "end": "302880"
  },
  {
    "text": "your um tool belt if you work on data driven applications now um just to give a",
    "start": "302880",
    "end": "310639"
  },
  {
    "text": "little bit of an overview what there is in dium well it's a fully open source project it's open source it's A2",
    "start": "310639",
    "end": "316360"
  },
  {
    "text": "licensed um it's a fully capable CDC platform so it means it comes with all",
    "start": "316360",
    "end": "322400"
  },
  {
    "text": "kinds of uh things like initial step shotting support there's a UI there's a",
    "start": "322400",
    "end": "327800"
  },
  {
    "text": "all way all kinds of ways for massaging your data like filtering it out or transforming your data all this kind of",
    "start": "327800",
    "end": "333680"
  },
  {
    "text": "stuff and it has a very active very Lively Community around it so up to this",
    "start": "333680",
    "end": "338759"
  },
  {
    "text": "point more than 350 people have contributed to uh debu in one way or another and there's huge massive",
    "start": "338759",
    "end": "345400"
  },
  {
    "text": "deployments out in the field where people use it to capture changes out of hundreds or even thousands of databases",
    "start": "345400",
    "end": "351520"
  },
  {
    "text": "so it really is Battle proven and has proven its value in this sort of large scale",
    "start": "351520",
    "end": "357400"
  },
  {
    "text": "environment um just in terms of how how do those change events look like just to again to recap a little bit if you",
    "start": "357400",
    "end": "363400"
  },
  {
    "text": "haven't been using it before so essentially the change events um well most importantly they describe the old",
    "start": "363400",
    "end": "369520"
  },
  {
    "text": "and the new um state of a row in your database and by the way it could also be for instance a document in in terms of",
    "start": "369520",
    "end": "376039"
  },
  {
    "text": "mongodb um or like a rose in Cassandra so it's not only limited to relational",
    "start": "376039",
    "end": "381240"
  },
  {
    "text": "databases now within those before and after Parts they are structured as your actual database tables um so for each",
    "start": "381240",
    "end": "388960"
  },
  {
    "text": "column as ENT there will be a a field by default um and then there's all kinds of metadata like uh what's the table where",
    "start": "388960",
    "end": "396039"
  },
  {
    "text": "this event is coming from the database um maybe transaction ID the position in the in the transaction log maybe even",
    "start": "396039",
    "end": "402919"
  },
  {
    "text": "the query which caused the change as for instance we can capture that in in the case of my SQL so all sorts of metadata",
    "start": "402919",
    "end": "409800"
  },
  {
    "text": "and then of course also things like a Tim stamp type discriminator all this kind of stuff so you can process this",
    "start": "409800",
    "end": "415720"
  },
  {
    "text": "event in a meaningful way all right so that's really just a short recap about",
    "start": "415720",
    "end": "421240"
  },
  {
    "text": "uh what theum is now I would like to touch a little bit um on some of the maybe newer or lesser known features and",
    "start": "421240",
    "end": "428199"
  },
  {
    "text": "one of the things I'm particularly excited about is that really is establishing itself as a defao standard",
    "start": "428199",
    "end": "434560"
  },
  {
    "text": "for CDC so that's a very interesting development over the last few um years I",
    "start": "434560",
    "end": "439639"
  },
  {
    "text": "would say that uh not only the deum project itself is working on uh more and",
    "start": "439639",
    "end": "445319"
  },
  {
    "text": "more connectors but also other Wenders so let's take a DB as an example who",
    "start": "445319",
    "end": "451199"
  },
  {
    "text": "work on a Cassandra compatible nosql database so they decided to build their",
    "start": "451199",
    "end": "456759"
  },
  {
    "text": "Kafka CDC connector based on the deum connector framework or just recently a",
    "start": "456759",
    "end": "462720"
  },
  {
    "text": "few weeks back um Google announced their CDC connector for Google Cloud spinner their um highly scalable database again",
    "start": "462720",
    "end": "470759"
  },
  {
    "text": "also based on theum and actually they even developed this as part of the deum",
    "start": "470759",
    "end": "475919"
  },
  {
    "text": "project umbrella and all this is to say that um well all those connectors they",
    "start": "475919",
    "end": "481319"
  },
  {
    "text": "give you one unified change event format as we have seen on the previous slides and this means now for you as a user",
    "start": "481319",
    "end": "487879"
  },
  {
    "text": "really it doesn't matter um does this event come from postest does it come from SB does it come from Google Cloud",
    "start": "487879",
    "end": "493400"
  },
  {
    "text": "spanner or any of the other supported databases you can Implement your consuming logic in a uniform way which",
    "start": "493400",
    "end": "500120"
  },
  {
    "text": "makes it of course rather easy and uh simple for you to adopt this so really that's very interesting for me to see",
    "start": "500120",
    "end": "506080"
  },
  {
    "text": "also for instance yugabyte they also develop a deum based CDC connector now most of the times uh",
    "start": "506080",
    "end": "514399"
  },
  {
    "text": "people use deum actually with Kafka I mention it briefly and I would say maybe 90% of the deum users they are working",
    "start": "514399",
    "end": "521080"
  },
  {
    "text": "with deum on top of a Pache Kafka um which essentially means deum in this sort of deployment scenario as we see it",
    "start": "521080",
    "end": "527680"
  },
  {
    "text": "here on the left hand side it's essentially a set of Kafka connect Source connectors so they take data out",
    "start": "527680",
    "end": "534279"
  },
  {
    "text": "of a database and put those change events into a Pache Kafka uh by fault",
    "start": "534279",
    "end": "539440"
  },
  {
    "text": "that would be one topic per table you capturing and then you could use any kinds of Kafka sync connectors to take",
    "start": "539440",
    "end": "546040"
  },
  {
    "text": "them to take the change events to your external systems but then we also realized uh",
    "start": "546040",
    "end": "551519"
  },
  {
    "text": "well there's people who would like to use um CDC they would like to use theum but maybe they are not on Kafka maybe",
    "start": "551519",
    "end": "558120"
  },
  {
    "text": "they use something like aache palar um AWS Kinesis Google Cloud Google Cloud",
    "start": "558120",
    "end": "563760"
  },
  {
    "text": "pops up um red streams all those kinds of things and of course we thought also",
    "start": "563760",
    "end": "569399"
  },
  {
    "text": "those users should be able to benefit from theum and this is where deum here deum server on the right hand side comes",
    "start": "569399",
    "end": "575240"
  },
  {
    "text": "into picture so essentially that's a readymade runtime for connecting the",
    "start": "575240",
    "end": "580360"
  },
  {
    "text": "deum connectors with all those kinds of messaging uh or streaming",
    "start": "580360",
    "end": "585480"
  },
  {
    "text": "platforms and at the heart of theum server uh there's what we call the deum engine so essentially that's just a",
    "start": "585480",
    "end": "591720"
  },
  {
    "text": "library module um which allows you to take the beum and its connectors and",
    "start": "591720",
    "end": "596959"
  },
  {
    "text": "integrate them into any kind of Java or jvm based application and this is also",
    "start": "596959",
    "end": "602839"
  },
  {
    "text": "what is driving theum server but it's also what many integrators of deum into",
    "start": "602839",
    "end": "609000"
  },
  {
    "text": "external systems use so for instance for Pache Flink there's the Flink CDC project and Flink CDC to a large part is",
    "start": "609000",
    "end": "615519"
  },
  {
    "text": "based on deum engine and that way you can ingest change events out of uh those",
    "start": "615519",
    "end": "621560"
  },
  {
    "text": "deum connectors without even having to go through kfka or any other streaming platform it would be ingested straight",
    "start": "621560",
    "end": "627880"
  },
  {
    "text": "into Flink and there's all kinds of other integrators of the deum embedded engine and you also could use you could",
    "start": "627880",
    "end": "634279"
  },
  {
    "text": "use it in your own application so let's say you're working on a spring boot application or you're working on a quarcus application you could use the",
    "start": "634279",
    "end": "641079"
  },
  {
    "text": "embedded engine and essentially that would just be a call back method which you register and then whenever a change",
    "start": "641079",
    "end": "646399"
  },
  {
    "text": "when arrives from the database um this callback method would be invoked and you would be free to do whatever you would",
    "start": "646399",
    "end": "652880"
  },
  {
    "text": "like to do with this change event so that's different deployment options for theum again most of the times people use",
    "start": "652880",
    "end": "659440"
  },
  {
    "text": "it with CFA but there's quite a few other options which give you much more flexibility if you need",
    "start": "659440",
    "end": "665120"
  },
  {
    "text": "it all right so then let me talk a little bit about Cloud events because I think that's also a lesser known feature",
    "start": "665120",
    "end": "671279"
  },
  {
    "text": "and I think it definitely um you know deserves more attention so we have seen this change event format uh before now",
    "start": "671279",
    "end": "679040"
  },
  {
    "text": "the thing is well that's like a bepoke uh standard and maybe um or a bepoke",
    "start": "679040",
    "end": "685399"
  },
  {
    "text": "format I I should say maybe you are really working in a very wide ecosystem of companies or maybe",
    "start": "685399",
    "end": "692639"
  },
  {
    "text": "just distributed teams in your organization and for them it would make sense to align on one envelope for all",
    "start": "692639",
    "end": "699519"
  },
  {
    "text": "your message or event driven um event payloads so let's say a CDC changed",
    "start": "699519",
    "end": "704600"
  },
  {
    "text": "their capture that's just one kind of Event Source which you have in this sort of architecture maybe you also ingest",
    "start": "704600",
    "end": "710320"
  },
  {
    "text": "iot data maybe have other sources um of realtime data flowing through this uh",
    "start": "710320",
    "end": "717200"
  },
  {
    "text": "streaming platform and now in that sort of scenario it makes sense to align on a",
    "start": "717200",
    "end": "723320"
  },
  {
    "text": "uh standardized event envelope and this is essentially what a cloud events is providing you so that's a standard",
    "start": "723320",
    "end": "729720"
  },
  {
    "text": "developed by the cncf the cloud native compute Foundation which defines um a",
    "start": "729720",
    "end": "735440"
  },
  {
    "text": "specification for describing event data in a common way so that's the ver from the spe and essentially what the aim for",
    "start": "735440",
    "end": "742120"
  },
  {
    "text": "is to give you um this unified event format which you see here which has like well defined specified attributes like",
    "start": "742120",
    "end": "749199"
  },
  {
    "text": "what's the source in this case it's theum uh post class connector what's the type so that's a post class change event",
    "start": "749199",
    "end": "756199"
  },
  {
    "text": "time um what's the content type Jason and so on and now having those well",
    "start": "756199",
    "end": "762639"
  },
  {
    "text": "defined attributes this allows you again to implement your consumers in a",
    "start": "762639",
    "end": "768360"
  },
  {
    "text": "homogeneous way um working with all kinds of different event sources and it makes it very easy for them to you know",
    "start": "768360",
    "end": "775800"
  },
  {
    "text": "reason about the type of the event filter out events and so on and of course now the actual payload which we",
    "start": "775800",
    "end": "781440"
  },
  {
    "text": "see here in this data part that's the part that's the core part of the B",
    "start": "781440",
    "end": "786959"
  },
  {
    "text": "change event so with the before and after part in this case the before part it's now because it's an insert event",
    "start": "786959",
    "end": "792680"
  },
  {
    "text": "and so that's the actual payload of the of the event and then you have all those",
    "start": "792680",
    "end": "797800"
  },
  {
    "text": "attributes which are prefixed with iodium so those are those metadata attributes with the um time stem",
    "start": "797800",
    "end": "804839"
  },
  {
    "text": "information the connector version and so on allowing your consumer to handle those events and essentially well all",
    "start": "804839",
    "end": "811920"
  },
  {
    "text": "this just helps you to implement your consumers in a consistent way to have",
    "start": "811920",
    "end": "816959"
  },
  {
    "text": "portability across different platforms all this kind of stuff so Cloud events it's supported as a message converter in",
    "start": "816959",
    "end": "823920"
  },
  {
    "text": "the beum and it's very easy for you to use if you uh are in this sort of um",
    "start": "823920",
    "end": "830800"
  },
  {
    "text": "environment next um there's a very interesting feature I wanted to briefly",
    "start": "830839",
    "end": "836079"
  },
  {
    "text": "mention and this is the support for what we call transaction metadata so transaction metadata that's essentially",
    "start": "836079",
    "end": "842759"
  },
  {
    "text": "about giving you the tools to correlate the events which originate from one and the same transaction so very often",
    "start": "842759",
    "end": "850519"
  },
  {
    "text": "that's a use case people have um let's say you are working on some sort of uh domain driven design application and",
    "start": "850519",
    "end": "857639"
  },
  {
    "text": "there you aggregate uh manifests in multiple tables in a relational database",
    "start": "857639",
    "end": "862839"
  },
  {
    "text": "and now you would like to whenever there's a change to this kind of agregate structure you would like to",
    "start": "862839",
    "end": "868040"
  },
  {
    "text": "combine the events um and only Adit them to Downstream consumers once all the",
    "start": "868040",
    "end": "873680"
  },
  {
    "text": "events have arrived for uh this Aggregate and a particular transaction",
    "start": "873680",
    "end": "878720"
  },
  {
    "text": "and this is exactly what uh this transaction metadata events allow you so if you enable this then all your actual",
    "start": "878720",
    "end": "885279"
  },
  {
    "text": "change events as we see them here on the left hand side they will have this transaction block where we essentially",
    "start": "885279",
    "end": "890639"
  },
  {
    "text": "tell you this is the um transaction ID and then also what's the um order number",
    "start": "890639",
    "end": "897279"
  },
  {
    "text": "of this particular event type and like a global order number within the transaction so they're ordered and then",
    "start": "897279",
    "end": "904680"
  },
  {
    "text": "you also get on another topic you get those U messages which tell you a",
    "start": "904680",
    "end": "909759"
  },
  {
    "text": "transaction has started or a transaction has been completed and in the completed",
    "start": "909759",
    "end": "914800"
  },
  {
    "text": "case we also tell you how many events there are per um table type or",
    "start": "914800",
    "end": "921000"
  },
  {
    "text": "essentially per topic if you were to break it down to Kafka terms and now this allows you to implement some sort",
    "start": "921000",
    "end": "926839"
  },
  {
    "text": "of buffering logic where you can essentially await all the events originating from the same transaction",
    "start": "926839",
    "end": "932959"
  },
  {
    "text": "and only once you are sure you have consumed all those events then you would go and Emmit them to your Downstream",
    "start": "932959",
    "end": "939519"
  },
  {
    "text": "consumers um and one of the users of this is actually um stripe so they did a",
    "start": "939519",
    "end": "944880"
  },
  {
    "text": "very interesting presentation at the conference last year Flink forward where they used this transaction metadata",
    "start": "944880",
    "end": "950279"
  },
  {
    "text": "support um for exactly doing this buffering the events from one transaction and emitting them once they",
    "start": "950279",
    "end": "956680"
  },
  {
    "text": "have received all the events from one transaction so I very much recommend to check out this talk I think it's a super",
    "start": "956680",
    "end": "961759"
  },
  {
    "text": "interesting uh capability which deum provides and which isn't as widely used",
    "start": "961759",
    "end": "966880"
  },
  {
    "text": "yet all right so that uh being said about deum itself change their capture",
    "start": "966880",
    "end": "972240"
  },
  {
    "text": "itself um let's talk a little bit more in depth about some of the use cases in particular in the context of",
    "start": "972240",
    "end": "979120"
  },
  {
    "text": "microservices and the first one I would like to talk about a little bit is the outbox pattern and the outbox pattern",
    "start": "979120",
    "end": "985399"
  },
  {
    "text": "well when when would you want to use it or what's the challenge you often face well typically if you are in a microservices environment or scenario",
    "start": "985399",
    "end": "992399"
  },
  {
    "text": "well then very often it happens you receive let's say a web request and what needs to happen is you need to update",
    "start": "992399",
    "end": "998959"
  },
  {
    "text": "your own database um let's say a request comes in to place a purchase order you need to update your own database and at",
    "start": "998959",
    "end": "1005600"
  },
  {
    "text": "the same time you would like to send out a message to external Services maybe a shipment service which should be",
    "start": "1005600",
    "end": "1012000"
  },
  {
    "text": "notified so that the shipment can be built and typically you would use something like Kafka for that and now",
    "start": "1012000",
    "end": "1017959"
  },
  {
    "text": "the problem is updating your own local database and sending a message to Kafka",
    "start": "1017959",
    "end": "1023560"
  },
  {
    "text": "well those two things they cannot happen as part of one shared transaction because there is just no means of um",
    "start": "1023560",
    "end": "1030558"
  },
  {
    "text": "distributed transactions which would also involve kfka so essentially that's what we call dual rights trying to",
    "start": "1030559",
    "end": "1036480"
  },
  {
    "text": "update those two resources a Services database and Kafka without shared transactional semantics and it's just",
    "start": "1036480",
    "end": "1043038"
  },
  {
    "text": "prone to inconsistencies because one of the operations might fail the other might succeed and then you end up in an",
    "start": "1043039",
    "end": "1049520"
  },
  {
    "text": "overall inconsistent state so that's definitely something which you want to avoid well the idea for the outbox",
    "start": "1049520",
    "end": "1055840"
  },
  {
    "text": "pattern essentially is well if we don't if we cannot update multiple resources we can always update a single resource",
    "start": "1055840",
    "end": "1062000"
  },
  {
    "text": "so the idea is our application only writes to its database so in this case it would update uh let's say its order",
    "start": "1062000",
    "end": "1069080"
  },
  {
    "text": "and order line tables and then at the same time it would insert a message into What's called the outbox table and this",
    "start": "1069080",
    "end": "1075640"
  },
  {
    "text": "outbox table um only this one then will be captured and it essentially is a",
    "start": "1075640",
    "end": "1081000"
  },
  {
    "text": "message which you send there so this is an externally phasing contract and now you don't have this issue with those",
    "start": "1081000",
    "end": "1087200"
  },
  {
    "text": "dual rights because writing to your business tables and the outbox table this happens as part of one local",
    "start": "1087200",
    "end": "1093840"
  },
  {
    "text": "transaction so either both things happen or the entire transaction would be rolled back and nothing gets committed",
    "start": "1093840",
    "end": "1100919"
  },
  {
    "text": "so that's the idea and now in terms of how does this out books table look like well um again coming back to the ideas",
    "start": "1100919",
    "end": "1106960"
  },
  {
    "text": "of domain driven design you could have columns there like an aggregate type which defines what is the type of my of",
    "start": "1106960",
    "end": "1113760"
  },
  {
    "text": "my um aggregate this is about let's say a purchase order or a customer and you could then for instance use this",
    "start": "1113760",
    "end": "1119120"
  },
  {
    "text": "information and the museum provides you with support for that for routing all the events for the same aggregate type",
    "start": "1119120",
    "end": "1125320"
  },
  {
    "text": "to one topic within Kafka you would have something there like an aggregate ID",
    "start": "1125320",
    "end": "1130360"
  },
  {
    "text": "which defines in terms of Kafka the message key making sure that all the events which pertain to one aggregate",
    "start": "1130360",
    "end": "1137120"
  },
  {
    "text": "that they all end up in the the same transaction you could have something like an event type to further",
    "start": "1137120",
    "end": "1142559"
  },
  {
    "text": "discriminate different types of Events maybe something like order created order line changed order deleted and so on so",
    "start": "1142559",
    "end": "1149760"
  },
  {
    "text": "for instance consumers could have different event handlers come in and then well the actual payload itself this",
    "start": "1149760",
    "end": "1156159"
  },
  {
    "text": "is now uh what you want to send to your consumers so this could be Json structure it could be AO really what you",
    "start": "1156159",
    "end": "1162080"
  },
  {
    "text": "want uh to have there but what's important to keep in mind is it's a external uh user facing contract right",
    "start": "1162080",
    "end": "1169679"
  },
  {
    "text": "so that's a contract between your source application and your Downstream consumers so you should evolve it with a",
    "start": "1169679",
    "end": "1176600"
  },
  {
    "text": "very strong consideration of backwards compatibility in mind so not to break those consumers and now having this",
    "start": "1176600",
    "end": "1183679"
  },
  {
    "text": "bespoke um contract also of course um abstracts from your internal model so",
    "start": "1183679",
    "end": "1189640"
  },
  {
    "text": "some one concern some users have with CD with CDC sometimes is it exposes the",
    "start": "1189640",
    "end": "1195000"
  },
  {
    "text": "internal table schema which would be right if you were to capture the order and the order line tables themselves but",
    "start": "1195000",
    "end": "1200679"
  },
  {
    "text": "now as we only here captured this outbox table we don't have this concern any",
    "start": "1200679",
    "end": "1205919"
  },
  {
    "text": "longer by the way coming back to deum this all is integrated with um distributed tracing uh right now it's",
    "start": "1205919",
    "end": "1212320"
  },
  {
    "text": "still um open tracing but there's work happening to move it all to open Telemetry which then allows you for",
    "start": "1212320",
    "end": "1217880"
  },
  {
    "text": "instance to use tools like Jagger to to examine and analyze this distributed um",
    "start": "1217880",
    "end": "1224919"
  },
  {
    "text": "messaging flow you would have deep inside you would learn uh where is slowness um and you could for instance",
    "start": "1224919",
    "end": "1230799"
  },
  {
    "text": "do performance analysis all this kind of stuff again deeply integrated with the mum now there's one interesting",
    "start": "1230799",
    "end": "1237400"
  },
  {
    "text": "variation of this pattern which I would like to mention for a postest so most of the times this outbox pattern is",
    "start": "1237400",
    "end": "1243039"
  },
  {
    "text": "implemented using a bepoke table but actually in the case of postest we don't even have to use a table we also could",
    "start": "1243039",
    "end": "1249960"
  },
  {
    "text": "use what's called logical decoding messages and logical decoding messages essentially are just messages which are",
    "start": "1249960",
    "end": "1257000"
  },
  {
    "text": "just written to devolve you write the headlock in postest the transaction log there and now this means um we don't",
    "start": "1257000",
    "end": "1263120"
  },
  {
    "text": "even have to write into a table for instance we also don't need to think about deleting an outbox message from",
    "start": "1263120",
    "end": "1268840"
  },
  {
    "text": "this table once it has been sent also we would be sure that a message will never be updated because well that should be",
    "start": "1268840",
    "end": "1275440"
  },
  {
    "text": "an append only um outbox of messages really all this cannot happen if you use",
    "start": "1275440",
    "end": "1281200"
  },
  {
    "text": "this PG logical emit message function so the way it works is you essentially just",
    "start": "1281200",
    "end": "1286799"
  },
  {
    "text": "say should it be transactional meaning does it participate in transactions yes or no so this is what we want to do um",
    "start": "1286799",
    "end": "1293600"
  },
  {
    "text": "there is a this prefix notion so in this case we say this is an outbox message which just is used to group different um",
    "start": "1293600",
    "end": "1300559"
  },
  {
    "text": "logical decoding messages and then again we have our payload which pretty much would be the same as we have seen uh for",
    "start": "1300559",
    "end": "1307480"
  },
  {
    "text": "the outbox table structure on the slide before but in this case it will never materialize in a table it will only be",
    "start": "1307480",
    "end": "1314120"
  },
  {
    "text": "written to the wall and still deum can be used to extract those changes and propagate them to external consumers",
    "start": "1314120",
    "end": "1321080"
  },
  {
    "text": "so I feel that's a very interesting variation um very smart way of implementing the outbox",
    "start": "1321080",
    "end": "1327679"
  },
  {
    "text": "pattern all right so then let's talk a little bit more about this scenario",
    "start": "1327679",
    "end": "1333240"
  },
  {
    "text": "where you would like to move to microservices before uh maybe you are",
    "start": "1333240",
    "end": "1338320"
  },
  {
    "text": "still in the world of U monolithic application architecture now you would like to move to microservices and now",
    "start": "1338320",
    "end": "1343799"
  },
  {
    "text": "the challenge there is well this should happen in a gradual fashion you don't want to do a Big Bang migration just",
    "start": "1343799",
    "end": "1349279"
  },
  {
    "text": "because it's too risky so what you want to do is you want to take parts of the modist step by step and extract them",
    "start": "1349279",
    "end": "1356039"
  },
  {
    "text": "gradually into new bespoke microservices and let's see how change their capture can help us a little bit with that so",
    "start": "1356039",
    "end": "1363200"
  },
  {
    "text": "let's assume for the sake of the example we have this sort of Monolithic application maybe it's still buil using",
    "start": "1363200",
    "end": "1368760"
  },
  {
    "text": "an Old Spring version an old Java version um and we would like to extract those components there into micros",
    "start": "1368760",
    "end": "1374960"
  },
  {
    "text": "services and by the way I have components there because because well you often times we think about monolith",
    "start": "1374960",
    "end": "1381120"
  },
  {
    "text": "as like one big ball of mud but still what I've realized or what I've often observed as well actually they have a",
    "start": "1381120",
    "end": "1386320"
  },
  {
    "text": "structure so there's different components they're actually quite well organized it's just part of one large",
    "start": "1386320",
    "end": "1392039"
  },
  {
    "text": "deployment monolith so let's take those components and move them over into microservices now is this Strangler fig",
    "start": "1392039",
    "end": "1398760"
  },
  {
    "text": "pattern by the way it's name or the term the name was uh coined by Martin Fowler who saw the Strangler Strangler hick",
    "start": "1398760",
    "end": "1405799"
  },
  {
    "text": "plant which like wraps an old tree and kind of strangles it until it dies off",
    "start": "1405799",
    "end": "1410919"
  },
  {
    "text": "so it this reminded him of this pattern apparently so what we would do there is well we would put a proxy component in",
    "start": "1410919",
    "end": "1416600"
  },
  {
    "text": "front of it so could be something like engine X or Envoy or whatever and first of all all the requests to this",
    "start": "1416600",
    "end": "1422799"
  },
  {
    "text": "application would go to this proxy and now we would start and we would set up a CDC process so in this case uh it's a",
    "start": "1422799",
    "end": "1429880"
  },
  {
    "text": "mySQL database so we use the MySQL deum connector put the data into Kafka and",
    "start": "1429880",
    "end": "1435880"
  },
  {
    "text": "maybe we want to also change the database of our new servers and maybe this should use mongod Tob so we would",
    "start": "1435880",
    "end": "1441320"
  },
  {
    "text": "use the mongodb sync connector to put the data into this mongod Tob database",
    "start": "1441320",
    "end": "1446640"
  },
  {
    "text": "and then we could have another new server so let's say we have this owner component so that's from a pet store",
    "start": "1446640",
    "end": "1452640"
  },
  {
    "text": "scenario want to have this owner component and move this into an owner service or maybe we change also of the",
    "start": "1452640",
    "end": "1458720"
  },
  {
    "text": "stack so maybe that's buil using quarkus Java 11 maybe Java 17 um so that's a new thing now and now first of all we would",
    "start": "1458720",
    "end": "1466159"
  },
  {
    "text": "say all the read requests they would be served by this new microservice so maybe there's just specific views in this",
    "start": "1466159",
    "end": "1472440"
  },
  {
    "text": "application and we would like to serve them from this new microservice um so we would have to set up the proxy component",
    "start": "1472440",
    "end": "1478520"
  },
  {
    "text": "so that it routes all the reads which pertain to owners to the new microservice and everything else",
    "start": "1478520",
    "end": "1484760"
  },
  {
    "text": "including the rights for the owners uh they will still go to the old monolith",
    "start": "1484760",
    "end": "1490000"
  },
  {
    "text": "and whenever the changes are happening they are propagated using CDC to the Ser to the new Services database so that we",
    "start": "1490000",
    "end": "1496960"
  },
  {
    "text": "can serve those re requests and then while we can continue we could say um at some point everything",
    "start": "1496960",
    "end": "1503919"
  },
  {
    "text": "which pertains to pet owners this is now managed by this new microservice so also the RS they are managed by the new",
    "start": "1503919",
    "end": "1510720"
  },
  {
    "text": "microservice and all the other parts of our domain they are still handled by the monolith and this essentially would",
    "start": "1510720",
    "end": "1517360"
  },
  {
    "text": "allow us already to um you know have to to take one component and extract it and",
    "start": "1517360",
    "end": "1524320"
  },
  {
    "text": "now what could happen is maybe we still have dependencies in the model which pertain to pet owners so we need",
    "start": "1524320",
    "end": "1530840"
  },
  {
    "text": "to have pet owners data still in the monolithic database and now you could uh",
    "start": "1530840",
    "end": "1536000"
  },
  {
    "text": "realize that well we also can use CDC again to take this data back from the microservice and propagate it back into",
    "start": "1536000",
    "end": "1542799"
  },
  {
    "text": "the uh Mony so we could do it both ways what we just need to realize is we should not we should always have one um",
    "start": "1542799",
    "end": "1551039"
  },
  {
    "text": "component which is in charge one side which is in charge of a particular part of our domain so owners should be either",
    "start": "1551039",
    "end": "1558440"
  },
  {
    "text": "managed which means right it should be it should manage the right side of things it should either be this micros",
    "start": "1558440",
    "end": "1564000"
  },
  {
    "text": "service or it should be the mod we shouldn't have rights to the same part of the domain uh in both components in",
    "start": "1564000",
    "end": "1570039"
  },
  {
    "text": "both Services because well then we would essentially end up circulating the same kind of changes potentially forever",
    "start": "1570039",
    "end": "1576000"
  },
  {
    "text": "forth and back so that's not something which we want to do all right um there's a demo repo",
    "start": "1576000",
    "end": "1582440"
  },
  {
    "text": "which you can check out which shows you all this uh in action and you can follow along if you want to do it and now well",
    "start": "1582440",
    "end": "1588960"
  },
  {
    "text": "what we gain by this well we can we can do this credal approach right so we can",
    "start": "1588960",
    "end": "1594640"
  },
  {
    "text": "uh take take component by component we would always update this routing uh logic in front of it and essentially we",
    "start": "1594640",
    "end": "1601480"
  },
  {
    "text": "would be able to do this migration step by step we could pause it even maybe we",
    "start": "1601480",
    "end": "1607000"
  },
  {
    "text": "could say we want to continue later on but those two things could uh coexist for quite a while or maybe we even could",
    "start": "1607000",
    "end": "1613279"
  },
  {
    "text": "say um that's a desired State maybe we are happy just having extracted the owner's component into its service and",
    "start": "1613279",
    "end": "1620000"
  },
  {
    "text": "maybe another one and then we want to leave the rest in the in the monolith by having this proxy in front well that's",
    "start": "1620000",
    "end": "1626080"
  },
  {
    "text": "something which we can do and all this is really done to minimize the risk and to be sure our business can uh continue",
    "start": "1626080",
    "end": "1633039"
  },
  {
    "text": "to function now there are some questions which you might ask yourself at this point in particular aren't we uh",
    "start": "1633039",
    "end": "1640880"
  },
  {
    "text": "exposing the internals of our old monolithic database coming a little bit back to the outbox pattern and that's a",
    "start": "1640880",
    "end": "1646640"
  },
  {
    "text": "valid concern um also maybe we just don't want to have this Ono one uh",
    "start": "1646640",
    "end": "1651919"
  },
  {
    "text": "replication of our tables maybe if we are in mongodb we want to do something",
    "start": "1651919",
    "end": "1657360"
  },
  {
    "text": "like putting multiple data structures into a single document and Nest that uh",
    "start": "1657360",
    "end": "1662919"
  },
  {
    "text": "which we could do in this sort of document store so let's see how we could go about those things um and one means",
    "start": "1662919",
    "end": "1669960"
  },
  {
    "text": "of doing that is what's called Kafka connect message single message Transformations so essentially they are",
    "start": "1669960",
    "end": "1676279"
  },
  {
    "text": "there for modifying or routing and changing the single messages so single",
    "start": "1676279",
    "end": "1681720"
  },
  {
    "text": "change events in our case we could use such an smt for instance to um adjust",
    "start": "1681720",
    "end": "1688440"
  },
  {
    "text": "the structure of our data Maybe we have like old Legacy column names which we want to rename or maybe some we date",
    "start": "1688440",
    "end": "1694880"
  },
  {
    "text": "formats maybe some we data representations like booleans and so on so we could use such an smt to Shield",
    "start": "1694880",
    "end": "1701000"
  },
  {
    "text": "our newly extracted microservice from all those audities in the monolithic database and provide a clean",
    "start": "1701000",
    "end": "1708279"
  },
  {
    "text": "um provide a clean data contract the Same by the way if we change schemas while all this is running so maybe we",
    "start": "1708279",
    "end": "1715320"
  },
  {
    "text": "add or we rename a column in the in the monolithic database we could use such an",
    "start": "1715320",
    "end": "1720440"
  },
  {
    "text": "smt to Shield the consumers from from such a schema change because we could",
    "start": "1720440",
    "end": "1725640"
  },
  {
    "text": "for instance just add back the field using an old using the old name and having uh the the field within",
    "start": "1725640",
    "end": "1732080"
  },
  {
    "text": "changements twice using the old and the new name so there's quite a few things also in regards to schema change which",
    "start": "1732080",
    "end": "1737799"
  },
  {
    "text": "we can do with those message Transformations but then they only go so far because well they only work on a",
    "start": "1737799",
    "end": "1743480"
  },
  {
    "text": "single message on each of on a specific message and um maybe coming back to this",
    "start": "1743480",
    "end": "1749519"
  },
  {
    "text": "nesting example this is not enough so maybe we want to actually take multiple table streams and merge them into a",
    "start": "1749519",
    "end": "1756360"
  },
  {
    "text": "single document and this is where we need something more powerful something more capable um stateful stream",
    "start": "1756360",
    "end": "1762440"
  },
  {
    "text": "processing for instance in the form of Kafka streams or um Apache Flink which",
    "start": "1762440",
    "end": "1767679"
  },
  {
    "text": "seems to establish itself as the dominating solution I would say in the Stream processing space so we could use",
    "start": "1767679",
    "end": "1773720"
  },
  {
    "text": "a patri Flink and for instance implement this sort of a joining logic so let's say we have owners and pets and they are",
    "start": "1773720",
    "end": "1779919"
  },
  {
    "text": "stored within two tables in the monolith um so this means we would have two CDC",
    "start": "1779919",
    "end": "1786120"
  },
  {
    "text": "topics one uh topic with owners one topic with pets and what we actually would like to have is a nested structure",
    "start": "1786120",
    "end": "1793120"
  },
  {
    "text": "shown here in in Jason for the sake of the example which contains the data of one owner and the data of all their pets",
    "start": "1793120",
    "end": "1800640"
  },
  {
    "text": "aggregated into a single document and this should be written back to uh in",
    "start": "1800640",
    "end": "1805720"
  },
  {
    "text": "owners with pets topic which we then could take and ingest into mongodb U and Flink uh provides us with",
    "start": "1805720",
    "end": "1812919"
  },
  {
    "text": "different means for doing that one way for instance could be Flink SQL as we see it here um where essentially we just",
    "start": "1812919",
    "end": "1819480"
  },
  {
    "text": "use a few custom functions which I'm not showing here to take the data from those",
    "start": "1819480",
    "end": "1825519"
  },
  {
    "text": "uh two topics um join them based on the owner's ID then essentially aggregate a",
    "start": "1825519",
    "end": "1832720"
  },
  {
    "text": "group and aggregate the structure data per owner so we have all the changes",
    "start": "1832720",
    "end": "1838039"
  },
  {
    "text": "from one owner in a single data structure and um this then we insert",
    "start": "1838039",
    "end": "1843360"
  },
  {
    "text": "into another Kafka topic or let's say in this case first of all in a Flink SQL table which then could be taken into a",
    "start": "1843360",
    "end": "1849559"
  },
  {
    "text": "Kafka topic or which could even be written directly to mongodb using a Flink connector for um mongodb but the",
    "start": "1849559",
    "end": "1858799"
  },
  {
    "text": "bottom line of all this being is just by means of having this declarative uh",
    "start": "1858799",
    "end": "1864039"
  },
  {
    "text": "logic expressed in Sequel we are capable of expressing this rather complex",
    "start": "1864039",
    "end": "1869120"
  },
  {
    "text": "joining grouping and aggregating logic just a natural Flink um you know there's",
    "start": "1869120",
    "end": "1874480"
  },
  {
    "text": "definitely way more which could be said around it but I hope it gives you like a glimpse of what could be",
    "start": "1874480",
    "end": "1879760"
  },
  {
    "text": "possible all right having spoken about CDC and uh two",
    "start": "1879760",
    "end": "1885200"
  },
  {
    "text": "use cases the outbox pattern and the Strangler fake pattern um let's talk a little bit about some of the challenges",
    "start": "1885200",
    "end": "1891840"
  },
  {
    "text": "which you might encounter when using CDC Solutions like uh DB because well it's a",
    "start": "1891840",
    "end": "1896919"
  },
  {
    "text": "powerful concept a powerful tool but where there's great power often times well there's great responsibility but",
    "start": "1896919",
    "end": "1902399"
  },
  {
    "text": "also sometimes just you need to be careful of how employing those tools and this is what I would like to cover in",
    "start": "1902399",
    "end": "1908080"
  },
  {
    "text": "this last part of the talk um and the first thing would be how do we actually",
    "start": "1908080",
    "end": "1913720"
  },
  {
    "text": "go about capturing intent so this means well if you change if you apply change",
    "start": "1913720",
    "end": "1918880"
  },
  {
    "text": "data capture to the tables of our business application uh what we typically are missing is metadata um",
    "start": "1918880",
    "end": "1926720"
  },
  {
    "text": "which tells us what's the business user who did a specific change or maybe what's the client uh device",
    "start": "1926720",
    "end": "1932919"
  },
  {
    "text": "configuration what's the client IP address uh something like a user user ident use case identifier all this kind",
    "start": "1932919",
    "end": "1939679"
  },
  {
    "text": "of metadata which you typically don't store within the tables of your application itself which means deum or",
    "start": "1939679",
    "end": "1946799"
  },
  {
    "text": "any other CDC solution for that met would not be able to extract that meta metadata so and still you would like to",
    "start": "1946799",
    "end": "1953279"
  },
  {
    "text": "have this metadata as part of a change events and again stateful stream",
    "start": "1953279",
    "end": "1958320"
  },
  {
    "text": "processing for instance with a pat of Link provides us with an interesting way for capturing that metadata and",
    "start": "1958320",
    "end": "1965360"
  },
  {
    "text": "enriching our change WIS change WIS with the metadata and this is how it works so",
    "start": "1965360",
    "end": "1970919"
  },
  {
    "text": "this again uses this nice function and postgress PG logical emit message um but",
    "start": "1970919",
    "end": "1976000"
  },
  {
    "text": "we could also of course Implement another form of this and we could just insert that metadata into a bebook table",
    "start": "1976000",
    "end": "1982080"
  },
  {
    "text": "but in postest logical decoding messages are very interesting way of doing this so here um we again would use this",
    "start": "1982080",
    "end": "1988720"
  },
  {
    "text": "function and we would at the beginning of each transaction we would write um a",
    "start": "1988720",
    "end": "1994559"
  },
  {
    "text": "message a logical decoding message with the metadata we are after into uh into",
    "start": "1994559",
    "end": "2001000"
  },
  {
    "text": "our transaction log so here we would have something like client date use case identifier username but really this is",
    "start": "2001000",
    "end": "2006639"
  },
  {
    "text": "on uh up to you what you would like to have so this is the first message which gets written to the transaction log for",
    "start": "2006639",
    "end": "2013039"
  },
  {
    "text": "each transaction um and then the idea is well we would like to use stateful stream",
    "start": "2013039",
    "end": "2019200"
  },
  {
    "text": "processing to take this event and put it into all the subsequent change events which are originating from the same",
    "start": "2019200",
    "end": "2025760"
  },
  {
    "text": "transaction so to give you an example here let's say we have this first transaction there's a begin event then",
    "start": "2025760",
    "end": "2032000"
  },
  {
    "text": "this first message which captures the trans the metadata so we have have that",
    "start": "2032000",
    "end": "2037320"
  },
  {
    "text": "stored and then we have two actual data changes two inserts in this case and then this transaction gets committed and",
    "start": "2037320",
    "end": "2044000"
  },
  {
    "text": "now we would use uh deum we would use Flink CDC which takes the deum and",
    "start": "2044000",
    "end": "2049800"
  },
  {
    "text": "integrates it into the uh Flink Universe um and we would then propagate this",
    "start": "2049800",
    "end": "2056599"
  },
  {
    "text": "metadata into each of the change events so then we wouldn't have an event anymore for just this metadata this",
    "start": "2056599",
    "end": "2063480"
  },
  {
    "text": "would be part of the actual change events themselves and just to give you an idea how this could look like so",
    "start": "2063480",
    "end": "2069358"
  },
  {
    "text": "before that I showed you uh how you can Implement um this sort of stream",
    "start": "2069359",
    "end": "2075398"
  },
  {
    "text": "processing logic in a declarative way using Flink SQL but there's other means",
    "start": "2075399",
    "end": "2080480"
  },
  {
    "text": "for implementing stream processing in Flink for instance in this case this data streaming API which allow or data",
    "start": "2080480",
    "end": "2086480"
  },
  {
    "text": "streams API which allows you to use Java or Scola or python or or other kinds of",
    "start": "2086480",
    "end": "2092398"
  },
  {
    "text": "languages for implementing more advanced logic which you maybe cannot express in",
    "start": "2092399",
    "end": "2098240"
  },
  {
    "text": "in SQL by itself so in this case it would give you this API where you have like operators for mapping your data",
    "start": "2098240",
    "end": "2103920"
  },
  {
    "text": "filtering your data and also for instance this flatmap operator and this means essentially for each event which",
    "start": "2103920",
    "end": "2110480"
  },
  {
    "text": "is coming in to our stream processor we would emit either non-event at all or",
    "start": "2110480",
    "end": "2117480"
  },
  {
    "text": "one or even more events and this is exactly what we want to do because well if we receive um um such a logical",
    "start": "2117480",
    "end": "2125440"
  },
  {
    "text": "decoding message for in we don't want to emit this right away we just want to capture disordered metadata which was",
    "start": "2125440",
    "end": "2132599"
  },
  {
    "text": "inserted and we want to put this into a state store so this is what's happening here on the left hand side if the",
    "start": "2132599",
    "end": "2138480"
  },
  {
    "text": "operation type equals m so that's a logical decoding message then we would take the metadata and put it into an",
    "start": "2138480",
    "end": "2145000"
  },
  {
    "text": "internal state store so we remember that um and now on the on on the right hand",
    "start": "2145000",
    "end": "2151240"
  },
  {
    "text": "side what you see there is now if an actual change event comes in then we would go to our state store and we would",
    "start": "2151240",
    "end": "2158079"
  },
  {
    "text": "check well if is this the same transaction still so because there might also be events uh which didn't receive",
    "start": "2158079",
    "end": "2164520"
  },
  {
    "text": "any metadata in their transaction so we would just emit them unenriched but in the case uh the the metadata which we",
    "start": "2164520",
    "end": "2172240"
  },
  {
    "text": "have in the state store it's from the current transaction then we will retrieve it from the state store and put",
    "start": "2172240",
    "end": "2177800"
  },
  {
    "text": "it into our um change events and then send them out to uh whatever is on the",
    "start": "2177800",
    "end": "2184839"
  },
  {
    "text": "syn side of the stream typically would be a cafka topic um so that our consumers could then consume from that",
    "start": "2184839",
    "end": "2190599"
  },
  {
    "text": "cafka topic those enriched changins another challenge this uh is",
    "start": "2190599",
    "end": "2197800"
  },
  {
    "text": "about snapshotting so as I mentioned um deum and CDC or in particular log based",
    "start": "2197800",
    "end": "2204520"
  },
  {
    "text": "CDC this is about changing sorry extracting change events from a",
    "start": "2204520",
    "end": "2209599"
  },
  {
    "text": "transaction log now the problem there is well very often we don't have the",
    "start": "2209599",
    "end": "2215040"
  },
  {
    "text": "transaction logs from a long long time ago because the database essentially it uses transaction locks for its own",
    "start": "2215040",
    "end": "2221200"
  },
  {
    "text": "purposes for replication um or for transaction recovery and typically once it figures it doesn't need those",
    "start": "2221200",
    "end": "2227720"
  },
  {
    "text": "transaction locks any longer then it would discard them this means if you set up such a new CDC pipeline we wouldn't",
    "start": "2227720",
    "end": "2233800"
  },
  {
    "text": "have the transaction locks from six months ago um and still we would often times to start with one complete uh",
    "start": "2233800",
    "end": "2240640"
  },
  {
    "text": "snapshot of our data so we want to do the snapshot or a backfill of our of our data and in this simplest case this is",
    "start": "2240640",
    "end": "2247640"
  },
  {
    "text": "how it could look like so again um I guess you realize but I'm I'm a big fan of postest so this is how essentially it",
    "start": "2247640",
    "end": "2253640"
  },
  {
    "text": "works in postest there we uh create What's called the replication slot slot so this means that's our handle for",
    "start": "2253640",
    "end": "2260400"
  },
  {
    "text": "getting change changes out of the transaction lock and we also say we want to export a snapshot right at this",
    "start": "2260400",
    "end": "2267520"
  },
  {
    "text": "offset in the transaction loog and this export a snapshot allows us essentially to run a separate read transaction right",
    "start": "2267520",
    "end": "2274640"
  },
  {
    "text": "at this uh snapshot offset and within this transaction we would scan all the tables which there are um and",
    "start": "2274640",
    "end": "2282000"
  },
  {
    "text": "essentially Amit something which looks like an insert event and once this snapshot is done we would re we would",
    "start": "2282000",
    "end": "2287319"
  },
  {
    "text": "Commit This R transaction we would have an entire version of the data in the",
    "start": "2287319",
    "end": "2292640"
  },
  {
    "text": "change EV stream and then we would continue to read from this replication slot right at this offset where the",
    "start": "2292640",
    "end": "2298359"
  },
  {
    "text": "snapshot was taken so that's a very efficient um and or let's say effective",
    "start": "2298359",
    "end": "2303680"
  },
  {
    "text": "means of implementing such an initial backfill logic but there's a few problems um which are coming with that",
    "start": "2303680",
    "end": "2310760"
  },
  {
    "text": "and which we also got reported in the beum project again and again so for instance it meant you couldn't update",
    "start": "2310760",
    "end": "2316920"
  },
  {
    "text": "the filter list so in the musum you can say I want to just capture those 10 out of my 100 tables and now maybe um you",
    "start": "2316920",
    "end": "2324880"
  },
  {
    "text": "said okay there's another table which I also want to capture and there was no good way for doing that also such a",
    "start": "2324880",
    "end": "2330240"
  },
  {
    "text": "snapshot can run for for for quite a long time it can take um hours if you have like many millions of uh table rows",
    "start": "2330240",
    "end": "2338400"
  },
  {
    "text": "to complete such a snapshot and this was like one Atomic operation you couldn't pause it um this this meant um if for",
    "start": "2338400",
    "end": "2345839"
  },
  {
    "text": "whatever reason you had to restart this connector or maybe there was a failure then it had to redo the entire snapshot",
    "start": "2345839",
    "end": "2353280"
  },
  {
    "text": "Al although maybe it already was done by 90% but still you had to redo the entire snapshot also it meant you couldn't uh",
    "start": "2353280",
    "end": "2360880"
  },
  {
    "text": "begin to stre to read changes from the transaction lock while the the snap was",
    "start": "2360880",
    "end": "2366800"
  },
  {
    "text": "still run so quite a few shortcomings in this classic snapshotting approach and this is um when we learned about this",
    "start": "2366800",
    "end": "2374440"
  },
  {
    "text": "very interesting approach uh implemented by the project dblock so dblock is a",
    "start": "2374440",
    "end": "2380640"
  },
  {
    "text": "internal CDC project by Netflix U by those two Engineers Andreas andreakis",
    "start": "2380640",
    "end": "2386359"
  },
  {
    "text": "and yanis papap gatu I hope I say that right so they implemented a bepoke CDC",
    "start": "2386359",
    "end": "2392920"
  },
  {
    "text": "solution at Netflix and they came up with a very interesting um snapshotting approach and thankfully they wrote a",
    "start": "2392920",
    "end": "2400839"
  },
  {
    "text": "research paper about this Watermark based snapshotting approach which allowed us to also implement this in the",
    "start": "2400839",
    "end": "2406359"
  },
  {
    "text": "beum and this is essentially how it looks like so the idea there is instead of doing those two things essentially",
    "start": "2406359",
    "end": "2413119"
  },
  {
    "text": "after each other first doing the snapshot then doing the uh the read from",
    "start": "2413119",
    "end": "2418280"
  },
  {
    "text": "the transaction log those two things happen simultaneously in an interweaved",
    "start": "2418280",
    "end": "2424160"
  },
  {
    "text": "way and there's some kind of D duplication logic in place which um",
    "start": "2424160",
    "end": "2429560"
  },
  {
    "text": "allows us to not emit uh duplicated events from the snapshot and from the uh",
    "start": "2429560",
    "end": "2436560"
  },
  {
    "text": "what we retrieve from the transaction log so that's that's basic idea and to make it a bit more specific so this is",
    "start": "2436560",
    "end": "2442040"
  },
  {
    "text": "how it works so first of all we don't have a single transaction for reading or a single select for reading all the",
    "start": "2442040",
    "end": "2449160"
  },
  {
    "text": "changes all the data from a table but instead we we do this in in chunks so here let's say we step through this",
    "start": "2449160",
    "end": "2455599"
  },
  {
    "text": "customer table in chunks of thousand thousand rows each time but could it's configurable could be more could be less",
    "start": "2455599",
    "end": "2462359"
  },
  {
    "text": "but let's say we do a th customers at a time and now what we also do is we",
    "start": "2462359",
    "end": "2468440"
  },
  {
    "text": "insert uh water marks into the transaction Lo so essentially whenever",
    "start": "2468440",
    "end": "2474040"
  },
  {
    "text": "one of those snapshotting transaction runs the process is that first of all we insert What's called the low Watermark",
    "start": "2474040",
    "end": "2481000"
  },
  {
    "text": "into the transaction log and um of course then if you can if you read from the transaction loog we will re receive",
    "start": "2481000",
    "end": "2486800"
  },
  {
    "text": "uh this this Mark event so we insert the low Watermark then we run this next uh",
    "start": "2486800",
    "end": "2492280"
  },
  {
    "text": "snapshot chunk uh transaction or selection so let's say the next thousand customers and then we insert a high",
    "start": "2492280",
    "end": "2499760"
  },
  {
    "text": "Watermark so this concludes this uh this chunk of data and this is what's",
    "start": "2499760",
    "end": "2504960"
  },
  {
    "text": "happening step by step and now those watermarks they allow us to correlate a",
    "start": "2504960",
    "end": "2512400"
  },
  {
    "text": "specific snapshot select chunk um with a part which we read from the transaction",
    "start": "2512400",
    "end": "2518680"
  },
  {
    "text": "box so let's look a little bit uh closer at one of those um chunk processing",
    "start": "2518680",
    "end": "2524400"
  },
  {
    "text": "steps so here we have our low Watermark then a few um operations as they were",
    "start": "2524400",
    "end": "2530680"
  },
  {
    "text": "happening in the database so an update and insert and update delete few more updates and then uh the high Watermark",
    "start": "2530680",
    "end": "2537599"
  },
  {
    "text": "and at the same time we have the results set from this uh snapshot select for",
    "start": "2537599",
    "end": "2543040"
  },
  {
    "text": "this particular chunk now what happens is that um essentially all this gets buffered in memory and once we receive",
    "start": "2543040",
    "end": "2550079"
  },
  {
    "text": "uh this high water mark we apply this uh D duplication step and the logic there",
    "start": "2550079",
    "end": "2555559"
  },
  {
    "text": "is well whatever we we retrieve for this Chunk from the transaction lock this",
    "start": "2555559",
    "end": "2561400"
  },
  {
    "text": "will take uh precedence over what we read from the snapshot select so in this case um there's for the key one we don't",
    "start": "2561400",
    "end": "2570640"
  },
  {
    "text": "take the read event from the snapshot chunk because well there was a delete event for for that key same for key2",
    "start": "2570640",
    "end": "2578280"
  },
  {
    "text": "there were updates for that key so whatever is in the transaction log for this chunk it will be taken from there",
    "start": "2578280",
    "end": "2584160"
  },
  {
    "text": "only for the key number three and five and six they were not modified during this uh this uh period Well then we will",
    "start": "2584160",
    "end": "2592200"
  },
  {
    "text": "backfill that data from uh from the snapshot select and this essentially",
    "start": "2592200",
    "end": "2597280"
  },
  {
    "text": "means um on a on a high level we step through",
    "start": "2597280",
    "end": "2602599"
  },
  {
    "text": "the data in our database with those uh chunk selects at the same time we continue to retrieve data from the",
    "start": "2602599",
    "end": "2609440"
  },
  {
    "text": "transaction lock and we give precedence to what we retrieve from the transaction lock now very importantly this means uh",
    "start": "2609440",
    "end": "2617040"
  },
  {
    "text": "that is that there is a few differences to the behavior and to the semantics you would get from a from a classic snapshot",
    "start": "2617040",
    "end": "2624319"
  },
  {
    "text": "so in particular uh you wouldn't never you wouldn't be guaranteed to get snapshot events or read events for all",
    "start": "2624319",
    "end": "2631200"
  },
  {
    "text": "the data um which are capturing it could also be update events events uh from the",
    "start": "2631200",
    "end": "2637240"
  },
  {
    "text": "transaction lock or it could even be delete events if a record was deleted um during that time but um and for instance",
    "start": "2637240",
    "end": "2644920"
  },
  {
    "text": "it could also happen that you receive an update event and you didn't get an insert event before that because that's",
    "start": "2644920",
    "end": "2650400"
  },
  {
    "text": "just what you got from the from what we read from the transaction up but what is guaranteed and that's what you really",
    "start": "2650400",
    "end": "2656760"
  },
  {
    "text": "want to do or what you want to have is once this entire logic is completed once the snapshot is completed you have an",
    "start": "2656760",
    "end": "2663319"
  },
  {
    "text": "complete set of your data in in your streaming platform in Kafka or wherever",
    "start": "2663319",
    "end": "2669040"
  },
  {
    "text": "you take the data so that's guaranteed to you and this is a very powerful uh",
    "start": "2669040",
    "end": "2674359"
  },
  {
    "text": "tool because now um if you think about all those things well for instance now we can uh IT addresses all those",
    "start": "2674359",
    "end": "2681119"
  },
  {
    "text": "shortcomings which I touched on before so for instance now we can update our filter list or we can re bootstrap one",
    "start": "2681119",
    "end": "2688040"
  },
  {
    "text": "particular table because those snapshots they can be run in at hoc fashion so the way it works and the beum is you inter",
    "start": "2688040",
    "end": "2695240"
  },
  {
    "text": "interact with the connector using what we call a signal table so you insert into the signal table a command where",
    "start": "2695240",
    "end": "2701640"
  },
  {
    "text": "you say hey I want to do a snapshot for this table inventory orders and then this will run this um Watermark based s",
    "start": "2701640",
    "end": "2710160"
  },
  {
    "text": "just for this table so you could re snapshot specific tables maybe you lost that Kafka topic also it means um we",
    "start": "2710160",
    "end": "2717440"
  },
  {
    "text": "keep track of how far have we gotten in the snapshotting process so those snapshot chunk selects we essentially",
    "start": "2717440",
    "end": "2725000"
  },
  {
    "text": "store the IDS at the boundaries uh in the connector offsets which means if you",
    "start": "2725000",
    "end": "2731680"
  },
  {
    "text": "restart the connector or you stop the connector and pause it and you then you restart it then um well we will be able",
    "start": "2731680",
    "end": "2739200"
  },
  {
    "text": "to continue the snapshot from from that particular point in time and also it means well and it's the gist of it",
    "start": "2739200",
    "end": "2746240"
  },
  {
    "text": "streaming and snapshotting happens at the same time so it means you don't have to wait for a multi-hour snapshot to",
    "start": "2746240",
    "end": "2751800"
  },
  {
    "text": "complete before receiving the latest changes from your database you will start to receive changes from the database um happening right now so",
    "start": "2751800",
    "end": "2759119"
  },
  {
    "text": "that's very a very interesting concept I definitely would recommend you to read this research paper by the Netflix guys",
    "start": "2759119",
    "end": "2765440"
  },
  {
    "text": "um so I think that's a huge Improvement and I'm really excited about this means of implementing backfills for uh CDC",
    "start": "2765440",
    "end": "2773240"
  },
  {
    "text": "systems and the last challenge I wanted to mention um is about wall growth uh",
    "start": "2773240",
    "end": "2779559"
  },
  {
    "text": "right a headlock growth in in post and this is a little bit of an oddity in post and just because so many people",
    "start": "2779559",
    "end": "2785400"
  },
  {
    "text": "stumbl opponent I really want to touch on this and the challenge there is in postgress you can have essentially uh a",
    "start": "2785400",
    "end": "2792640"
  },
  {
    "text": "physical database host so like a machine or Wim and then there can be multiple logical databases on this host so in",
    "start": "2792640",
    "end": "2799839"
  },
  {
    "text": "this case I have database one and database 2 on the same postris host um",
    "start": "2799839",
    "end": "2805400"
  },
  {
    "text": "the problem there is the wall the wrer headlock that's Global so that's shared",
    "start": "2805400",
    "end": "2811000"
  },
  {
    "text": "between those two logical tables whereas The Logical replication slot which is",
    "start": "2811000",
    "end": "2816160"
  },
  {
    "text": "our handle for getting changes out of a database this is specific to one of",
    "start": "2816160",
    "end": "2821280"
  },
  {
    "text": "those logical databases so we could have a replication slot either for database 1",
    "start": "2821280",
    "end": "2827280"
  },
  {
    "text": "or for database 2 now the situation is um or a challenge can arise if there is",
    "start": "2827280",
    "end": "2834720"
  },
  {
    "text": "differences in terms of how frequently are changes done to those databases so maybe on database 1 we do many changes",
    "start": "2834720",
    "end": "2842200"
  },
  {
    "text": "and on database 2 we don't do any changes at all and now this means if we have a",
    "start": "2842200",
    "end": "2847640"
  },
  {
    "text": "replication slot for replication for database 2 then we will not be able to advance this replication slot because we",
    "start": "2847640",
    "end": "2854599"
  },
  {
    "text": "will never receive changes from the database and it also means we will never be able to acknowledge this is the",
    "start": "2854599",
    "end": "2860640"
  },
  {
    "text": "latest offset we have consumed because we never consume any offsets and this means the as the wall is shared that",
    "start": "2860640",
    "end": "2868000"
  },
  {
    "text": "this replication slot retains larger and larger chunks of the of the transaction log so here in this case you you have",
    "start": "2868000",
    "end": "2874640"
  },
  {
    "text": "this nice query where you can essentially take uh the the offset where the replication slot is at and you can",
    "start": "2874640",
    "end": "2881160"
  },
  {
    "text": "compare this to the current replic to the current um uh position in the transaction lock and you would see this",
    "start": "2881160",
    "end": "2888480"
  },
  {
    "text": "retained wall gets bigger and bigger because we have this replication slot on this low traffic database and it has no",
    "start": "2888480",
    "end": "2895119"
  },
  {
    "text": "way for making any progress because no changes are coming in so that's a challenge and in the end of the day your",
    "start": "2895119",
    "end": "2901559"
  },
  {
    "text": "database might run out of dis space unless you delete this replication slot and then of course the connector state",
    "start": "2901559",
    "end": "2907040"
  },
  {
    "text": "would be messed up in a way so one solution is um to artificially induce",
    "start": "2907040",
    "end": "2914359"
  },
  {
    "text": "changes into this low traffic database and this is what theum allows you to do",
    "start": "2914359",
    "end": "2919480"
  },
  {
    "text": "why this feature of heartbeat action queries and again it's a very nice use case for those logical decoding messages",
    "start": "2919480",
    "end": "2925720"
  },
  {
    "text": "because there we could just emit messages into the wall um for this particular database just some heartbeat",
    "start": "2925720",
    "end": "2932440"
  },
  {
    "text": "record and now then this would allow this replication slot for this database to make progress and we wouldn't have",
    "start": "2932440",
    "end": "2938799"
  },
  {
    "text": "this issue so it's a rather peculiar specific issue which you have with post but it can happen quite easily so",
    "start": "2938799",
    "end": "2945359"
  },
  {
    "text": "definitely something to keep in mind if you have this scenario with a high traffic and a low traffic logical",
    "start": "2945359",
    "end": "2950480"
  },
  {
    "text": "database on the same host and with that um pretty much done",
    "start": "2950480",
    "end": "2955720"
  },
  {
    "text": "um I I touched on what I wanted to explain today just just to do a quick",
    "start": "2955720",
    "end": "2961319"
  },
  {
    "text": "recap so well I hope we all agree well the fresher our data is the more",
    "start": "2961319",
    "end": "2967720"
  },
  {
    "text": "actionable is the more valuable it is so if you build dashboards if you want streaming queries against our data all",
    "start": "2967720",
    "end": "2973000"
  },
  {
    "text": "this needs to happen on fresh data otherwise the insights we gain from that AR just not is valuable and if it comes",
    "start": "2973000",
    "end": "2979440"
  },
  {
    "text": "to processing data from a database well then log based CDC as implemented with theum it's a very powerful means for",
    "start": "2979440",
    "end": "2987400"
  },
  {
    "text": "giving you fresh data for giving you upto-date data and then you could of course use tools like uh kka streams or",
    "start": "2987400",
    "end": "2994400"
  },
  {
    "text": "pet link for processing those chain event streams and combine them filter",
    "start": "2994400",
    "end": "2999720"
  },
  {
    "text": "them map them doing aggregated uh operations doing window analytics all",
    "start": "2999720",
    "end": "3006119"
  },
  {
    "text": "this kind of stuff so really taking it to the next level that's pretty much all I had for",
    "start": "3006119",
    "end": "3011799"
  },
  {
    "text": "you I will be available for uh Q&A um at the conference so for those people who",
    "start": "3011799",
    "end": "3016920"
  },
  {
    "text": "are there you can meet me there and I will be very happy to answer any questions otherwise please feel free to",
    "start": "3016920",
    "end": "3022520"
  },
  {
    "text": "reach out to me on Twitter or send me an email and I would be very happy to engage in a discussion around chain Cher",
    "start": "3022520",
    "end": "3028280"
  },
  {
    "text": "capture deum aache Flink and all the paradigms related to that thank you so",
    "start": "3028280",
    "end": "3035280"
  },
  {
    "text": "[Music]",
    "start": "3037950",
    "end": "3043400"
  },
  {
    "text": "much",
    "start": "3044599",
    "end": "3047599"
  }
]