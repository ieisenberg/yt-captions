[
  {
    "text": "foreign [Music]",
    "start": "0",
    "end": "14249"
  },
  {
    "text": "for many customers who are moving to the cloud they want to know how to build and orchestrate data pipelines across",
    "start": "15240",
    "end": "21600"
  },
  {
    "text": "on-premise remote and Cloud environments hello my name is Ricardo Suarez and this",
    "start": "21600",
    "end": "27359"
  },
  {
    "text": "session I'm going to show you how you can leverage patchy airflow to orchestrate workflows using data sources",
    "start": "27359",
    "end": "32820"
  },
  {
    "text": "both inside and out of the cloud I'll be covering why customers care about orchestrating these hybrid",
    "start": "32820",
    "end": "38160"
  },
  {
    "text": "workflows and some of the typical use cases you might see explore some of the options you might have within Apache",
    "start": "38160",
    "end": "43320"
  },
  {
    "text": "airflow and some of the trade-offs you need to think about before diving into some code and a demo of building and",
    "start": "43320",
    "end": "48480"
  },
  {
    "text": "orchestrating a hybrid workflow I'll be sharing the resources including the code I use in the demos at the end of the",
    "start": "48480",
    "end": "54239"
  },
  {
    "text": "presentation Apache airflow has become a very popular solution for customers who want to solve",
    "start": "54239",
    "end": "61680"
  },
  {
    "text": "the problem of how to orchestrate data pipelines rather than building and relying on a homegrown solution Apache",
    "start": "61680",
    "end": "68939"
  },
  {
    "text": "airflow provides a proven set of capabilities to help you create deploy and manage your workflows it also has a",
    "start": "68939",
    "end": "75900"
  },
  {
    "text": "great community of contributors who are driving and innovating the project forward and for customers who have a strong",
    "start": "75900",
    "end": "82439"
  },
  {
    "text": "preference for open source it has become a key part of their data engineering infrastructure",
    "start": "82439",
    "end": "89240"
  },
  {
    "text": "customers move to the cloud they look for help in how they build these hybrid solutions that can integrate with",
    "start": "89520",
    "end": "95820"
  },
  {
    "text": "existing Legacy systems and leverage the data that resides in those wherever that data might be",
    "start": "95820",
    "end": "101579"
  },
  {
    "text": "as customers build those data pipelines they encounter however a number of challenges for some data for example",
    "start": "101579",
    "end": "107340"
  },
  {
    "text": "there may be some stronger regulatory or compliance controls the limit where that data can be processed or where that data",
    "start": "107340",
    "end": "113880"
  },
  {
    "text": "can be reside yet they still want to get insights from that data customers may want to integrate into",
    "start": "113880",
    "end": "120060"
  },
  {
    "text": "Legacy and Heritage systems who can't move to the cloud for whatever reason",
    "start": "120060",
    "end": "125340"
  },
  {
    "text": "and they want to do this in a way that's simple and doesn't rely on overly complex Solutions and they also want a",
    "start": "125340",
    "end": "131940"
  },
  {
    "text": "cost-effective solution in case they want to move large volumes of data",
    "start": "131940",
    "end": "137360"
  },
  {
    "text": "so it's no surprise Apache airflow has a number of operators that allow you to",
    "start": "137940",
    "end": "143879"
  },
  {
    "text": "simplify how you can orchestrate your data pipelines but which one of these is",
    "start": "143879",
    "end": "149099"
  },
  {
    "text": "best suited for these hybrid scenarios now the reality is that most of the Apache airflow operators can work as",
    "start": "149099",
    "end": "155520"
  },
  {
    "text": "long as you've got network connectivity to the systems that you're connecting to architecting solutions that work across",
    "start": "155520",
    "end": "162360"
  },
  {
    "text": "hybrid environments require some planning and consideration of the different strengths of each approach",
    "start": "162360",
    "end": "168180"
  },
  {
    "text": "especially when you're considering the compliance and Regulatory concerns",
    "start": "168180",
    "end": "173519"
  },
  {
    "text": "for example if we wanted to build a workflow that performed a regular batch processing of some data for example in a",
    "start": "173519",
    "end": "180120"
  },
  {
    "text": "mySQL database that we've got deployed across our remote our data center and our Cloud environments we have a number",
    "start": "180120",
    "end": "187080"
  },
  {
    "text": "of different options we might want to consider we could use for example the python operator to remotely access our data our",
    "start": "187080",
    "end": "193800"
  },
  {
    "text": "data sources now we can run and create code in Python and that code would run",
    "start": "193800",
    "end": "199440"
  },
  {
    "text": "in an Apache airflow worker node we could reuse that code we can package it",
    "start": "199440",
    "end": "204599"
  },
  {
    "text": "up as a plug-in to make it simpler and more reusable but",
    "start": "204599",
    "end": "209879"
  },
  {
    "text": "the code would still be running on the worker node in the cloud so it may not meet our compliance and regulatory",
    "start": "209879",
    "end": "216720"
  },
  {
    "text": "requirements now if you've got kubernetes skills you may be able to implement your elt logic within a",
    "start": "216720",
    "end": "222299"
  },
  {
    "text": "container image and then deploy and run this kubernetes uh this image in a",
    "start": "222299",
    "end": "227400"
  },
  {
    "text": "kubernetes uh cluster that you might have and this is a very popular option for many customers but you would need to",
    "start": "227400",
    "end": "234480"
  },
  {
    "text": "implement a solution locally that would allow you to deploy your container and you then have to make sure you've got",
    "start": "234480",
    "end": "240720"
  },
  {
    "text": "the right skills locally to manage and maintain those kubernetes clusters",
    "start": "240720",
    "end": "247700"
  },
  {
    "text": "you can also use the MySQL operator um again if you've got the network connectivity and network networking",
    "start": "247860",
    "end": "254280"
  },
  {
    "text": "infrastructure in place you will also need to put some additional controls in place to enable access to the various",
    "start": "254280",
    "end": "259500"
  },
  {
    "text": "components but again like the python operator this would potentially be limited in those use cases where",
    "start": "259500",
    "end": "265919"
  },
  {
    "text": "compliance and regulatory requirements meant that you needed you weren't able to process that data centrally",
    "start": "265919",
    "end": "274160"
  },
  {
    "text": "now if we're using AWS managed Services we could use a number of operators such as the Athena operator which allows you",
    "start": "274440",
    "end": "280139"
  },
  {
    "text": "to create Federated queries using an open source SDK that helps you build connectors to whatever data sources",
    "start": "280139",
    "end": "287400"
  },
  {
    "text": "you've got and as long as you've got that Network VPN connectivity you create your Federated query over a Lambda",
    "start": "287400",
    "end": "292979"
  },
  {
    "text": "function and that access the data but again this requires this the processing",
    "start": "292979",
    "end": "298440"
  },
  {
    "text": "of that to be done in the cloud and so may not meet your Regulatory and compliance needs",
    "start": "298440",
    "end": "305160"
  },
  {
    "text": "and um this is what it would typically look like when you would deploy it",
    "start": "305160",
    "end": "311940"
  },
  {
    "text": "now similar to the kubernetes operator the ECS operator allows you to run container workloads but with a much",
    "start": "311940",
    "end": "318600"
  },
  {
    "text": "simpler deployment model rather than having to manage and operate a kubernetes cluster remotely we can use",
    "start": "318600",
    "end": "324240"
  },
  {
    "text": "something called the ECS anywhere agent that allows you to deploy deploy and run containers easily via something called",
    "start": "324240",
    "end": "331620"
  },
  {
    "text": "the ECS agent now I'm going to explore more how you can use this",
    "start": "331620",
    "end": "337400"
  },
  {
    "text": "container-based approach together with that operator for the rest of the presentation",
    "start": "337400",
    "end": "343280"
  },
  {
    "text": "ECS anywhere allows you to extend the data plane from AWS into your own environments it could be on-prem in a",
    "start": "343440",
    "end": "350520"
  },
  {
    "text": "remote office it could even be in other Cloud providers and it provides a simplified deployment that makes it",
    "start": "350520",
    "end": "356460"
  },
  {
    "text": "really easy to get up and running and requires only outbound connectivity and",
    "start": "356460",
    "end": "361860"
  },
  {
    "text": "it runs on a number of Linux operating systems as you can see there and very recently we added also Windows support",
    "start": "361860",
    "end": "368940"
  },
  {
    "text": "as well and once you've installed this agent on a host you effectively have a managed",
    "start": "368940",
    "end": "375120"
  },
  {
    "text": "container which you enable which allows you to then deploy via the ECS control",
    "start": "375120",
    "end": "380400"
  },
  {
    "text": "plane into that environment",
    "start": "380400",
    "end": "384740"
  },
  {
    "text": "now a common pattern for Apache airflow is to actually to containerize your ETL",
    "start": "386819",
    "end": "393000"
  },
  {
    "text": "logic and then run this one of the container operators whether it's the kubernetes one I talked about or the ECS",
    "start": "393000",
    "end": "399479"
  },
  {
    "text": "operator so we're going to take this approach um in a second in a demo and we're going",
    "start": "399479",
    "end": "406800"
  },
  {
    "text": "to typically show you how we can develop some simple ETL logic we can package",
    "start": "406800",
    "end": "412199"
  },
  {
    "text": "that as a container image we can push that contained image to our repository and then we can deploy and run it",
    "start": "412199",
    "end": "418620"
  },
  {
    "text": "anywhere and when using ECS anywhere that means anywhere where the ECS anywhere agent is running and we can",
    "start": "418620",
    "end": "425699"
  },
  {
    "text": "then use the Apache airflow operator to then allow us to orchestrate the running of those contained images so anywhere",
    "start": "425699",
    "end": "433560"
  },
  {
    "text": "where that agent is running so let's take a look at actually what we're going to build in the demo",
    "start": "433560",
    "end": "440099"
  },
  {
    "text": "so this is going to be our hypothetical scenario we have got a cloud environment we've got a number of MySQL databases",
    "start": "440099",
    "end": "448020"
  },
  {
    "text": "and we want to store extracts of that in our Central data Lake",
    "start": "448020",
    "end": "453979"
  },
  {
    "text": "and we want to bring that information from all of those environments but we",
    "start": "454199",
    "end": "459240"
  },
  {
    "text": "don't want to um deploy complex vpns we want a really simple solution we've got",
    "start": "459240",
    "end": "464580"
  },
  {
    "text": "limited it resources at these external locations we want to create a workflow in Apache",
    "start": "464580",
    "end": "471300"
  },
  {
    "text": "airflow to orchestrate this on a regular basis ensure that we can centralize the management and operations of those workflows so that we can execute them",
    "start": "471300",
    "end": "477780"
  },
  {
    "text": "schedule them but also see the output and make sure they're running as they need to be",
    "start": "477780",
    "end": "483300"
  },
  {
    "text": "and these workflows um you know will use the ECS operator in",
    "start": "483300",
    "end": "489180"
  },
  {
    "text": "conjunction with the ECS anywhere agent to allow us to deploy our container from",
    "start": "489180",
    "end": "495960"
  },
  {
    "text": "AWS into that remote environment and then run beta logic and then push that",
    "start": "495960",
    "end": "502440"
  },
  {
    "text": "data into our data Lake now this is the",
    "start": "502440",
    "end": "507479"
  },
  {
    "text": "actual architecture of what we're going to build in the demo we can break this down and we can see",
    "start": "507479",
    "end": "512760"
  },
  {
    "text": "that these are our MySQL databases the one at the top is the cloud-based one",
    "start": "512760",
    "end": "518159"
  },
  {
    "text": "the one at the bottom are local and then we've got our data Lake there",
    "start": "518159",
    "end": "524640"
  },
  {
    "text": "we're going to create our um ETL image container image we're going",
    "start": "524640",
    "end": "530040"
  },
  {
    "text": "to dockerize it and we're going to push that to a repository we're then going to run that and test it",
    "start": "530040",
    "end": "536820"
  },
  {
    "text": "locally on the ECS control plane and then we're going to actually deploy",
    "start": "536820",
    "end": "543420"
  },
  {
    "text": "that locally via the ECS agent",
    "start": "543420",
    "end": "547940"
  },
  {
    "text": "and then finally we're going to create a workflow that puts all these pieces together and shows us how we can",
    "start": "548700",
    "end": "554220"
  },
  {
    "text": "orchestrate and start to use all those moving parts to manage and kick off",
    "start": "554220",
    "end": "559440"
  },
  {
    "text": "those ETL processors both in the cloud as well as in our remote location right",
    "start": "559440",
    "end": "565640"
  },
  {
    "text": "that's enough of the presentations I'm going to switch now to code and we'll",
    "start": "565640",
    "end": "571200"
  },
  {
    "text": "take it from there so let's go into",
    "start": "571200",
    "end": "576300"
  },
  {
    "text": "um code so this is the repository I'm going to be sharing the link later on that contains all the code I'm going to",
    "start": "576300",
    "end": "581580"
  },
  {
    "text": "use and I've checked it out already into this Repository",
    "start": "581580",
    "end": "587580"
  },
  {
    "text": "we got a couple of databases one is in RDS and it contains a sample",
    "start": "587580",
    "end": "595620"
  },
  {
    "text": "table customers if we have a look at those we can see just some sample which I",
    "start": "595620",
    "end": "601019"
  },
  {
    "text": "generated using a sample data generator and we also have a ec2 instance running",
    "start": "601019",
    "end": "607040"
  },
  {
    "text": "uh the same or similar rather database schema but different data",
    "start": "607040",
    "end": "612480"
  },
  {
    "text": "and um the ec2 instance here is simulating a remote office it's not connected to",
    "start": "612480",
    "end": "618959"
  },
  {
    "text": "anything at the moment and we'll we'll show how we can install the ECS agent",
    "start": "618959",
    "end": "624060"
  },
  {
    "text": "and bring that in in a second so we've got",
    "start": "624060",
    "end": "629399"
  },
  {
    "text": "the data let's come out of here and we've got our ETL logic our code",
    "start": "629399",
    "end": "637140"
  },
  {
    "text": "which we can see in the app here and if we look at this code here this is the",
    "start": "637140",
    "end": "642779"
  },
  {
    "text": "sample python code that takes a number of parameters which we can see at the bottom here first of all it's an S3",
    "start": "642779",
    "end": "649560"
  },
  {
    "text": "bucket we want to create and store this the file name we want to use the query we want to pass in and then the AWS",
    "start": "649560",
    "end": "657260"
  },
  {
    "text": "parameters now the reason why we need the address parameters is because we need to know which database to connect",
    "start": "657260",
    "end": "663240"
  },
  {
    "text": "to so if we go on to the AdWords Secrets where we're using and storing this information we can we can come to this",
    "start": "663240",
    "end": "670860"
  },
  {
    "text": "sample here which I'm not using and these values are stored here we have a",
    "start": "670860",
    "end": "676079"
  },
  {
    "text": "database name a username a password and host these are encrypted and this allows",
    "start": "676079",
    "end": "681360"
  },
  {
    "text": "you just to access that information from your code without having to know the the details all I need to know is the name",
    "start": "681360",
    "end": "688680"
  },
  {
    "text": "of the secret itself so that allows everything to be nice and secure so we can try this out we can run",
    "start": "688680",
    "end": "695640"
  },
  {
    "text": "I've got some sample queries here and if we run this we should get some two",
    "start": "695640",
    "end": "701940"
  },
  {
    "text": "things should happen um because what this what this code does is it it runs a query and then it",
    "start": "701940",
    "end": "707160"
  },
  {
    "text": "uploads this to our sample day take on Amazon S3 bucket so if we run this",
    "start": "707160",
    "end": "712980"
  },
  {
    "text": "May the demo Gods be happy with us and we've got a result which is good we",
    "start": "712980",
    "end": "720060"
  },
  {
    "text": "can see it's it's returned just um uh the data from the Polish users and",
    "start": "720060",
    "end": "726300"
  },
  {
    "text": "if we go to our data Lake which is on S3 we refresh we can see we've got this folder here",
    "start": "726300",
    "end": "733140"
  },
  {
    "text": "which contains the name of the file and that if we let's download it",
    "start": "733140",
    "end": "740000"
  },
  {
    "text": "and have a look at this file sugary",
    "start": "740000",
    "end": "747800"
  },
  {
    "text": "[Applause]",
    "start": "750610",
    "end": "753829"
  },
  {
    "text": "you can see that it's our it's our data okay",
    "start": "755640",
    "end": "760860"
  },
  {
    "text": "right so we've now got um this script working uh which is the first part so the next part is to",
    "start": "760860",
    "end": "767040"
  },
  {
    "text": "actually containerize this so within this folder I've got a setup script which goes through the process of",
    "start": "767040",
    "end": "774060"
  },
  {
    "text": "actually how you typically would do this now I'm using AWS so I'm going to build and package this up and store this in",
    "start": "774060",
    "end": "781139"
  },
  {
    "text": "Amazon ecl which is the container repository that Amazon provides",
    "start": "781139",
    "end": "786480"
  },
  {
    "text": "um and we're going to create a manifest and tag it so that we can then run this so to make this easier I've created a",
    "start": "786480",
    "end": "793380"
  },
  {
    "text": "setup script so I'm just going to run this script here and that's going to go through all the",
    "start": "793380",
    "end": "798959"
  },
  {
    "text": "processes I'm going to put the video on on pause whilst it's doing this because it takes about five to ten minutes whilst doing that on my home broadband",
    "start": "798959",
    "end": "806040"
  },
  {
    "text": "connection okay that's now finished took about 15",
    "start": "806040",
    "end": "812459"
  },
  {
    "text": "actually minutes on my slow broadband connection let's go over to the console and go to the ECR I was an ECR",
    "start": "812459",
    "end": "820139"
  },
  {
    "text": "repository and we should be able to see that we've now got a new repository called hybrid airflow and within it",
    "start": "820139",
    "end": "825720"
  },
  {
    "text": "we've got a container image so we can try running this if we try and run it through Docker run",
    "start": "825720",
    "end": "833660"
  },
  {
    "text": "we should hopefully get it running and it's doing exactly what we expect which is basically to",
    "start": "834540",
    "end": "839760"
  },
  {
    "text": "return the fact that we haven't provided enough command line arguments which if we look at the script it's expecting a",
    "start": "839760",
    "end": "847200"
  },
  {
    "text": "number of arguments otherwise it generates this error message right so what we can do is we can take the",
    "start": "847200",
    "end": "853860"
  },
  {
    "text": "um parameters we did here before and append that",
    "start": "853860",
    "end": "860120"
  },
  {
    "text": "and let's see what happens and it's actually failed but this is",
    "start": "860220",
    "end": "866700"
  },
  {
    "text": "actually to be expected the reason why is we've created a container and we're",
    "start": "866700",
    "end": "871980"
  },
  {
    "text": "now executing that container and that container is now trying to access AWS services but there's no credentials in",
    "start": "871980",
    "end": "877620"
  },
  {
    "text": "that so what we need to do is we need to pass in the AWS credentials so what",
    "start": "877620",
    "end": "883019"
  },
  {
    "text": "we're going to do is I've got um because it's a long command line string I've actually got this already prepared",
    "start": "883019",
    "end": "891199"
  },
  {
    "text": "and what I'm what I've done is I've created two environment variables which contain my access key and secret key",
    "start": "891199",
    "end": "897839"
  },
  {
    "text": "pass them in and that will actually then be passed into the container and this time if it were hopefully it should",
    "start": "897839",
    "end": "905040"
  },
  {
    "text": "return exactly the same results the next step is to take our container",
    "start": "905040",
    "end": "910320"
  },
  {
    "text": "and now deploy it in the cloud for this we're going to use AWS ECS elastic",
    "start": "910320",
    "end": "916380"
  },
  {
    "text": "container service and we're going to provision and ECS cluster create a",
    "start": "916380",
    "end": "922740"
  },
  {
    "text": "configuration a task which will take our container that we've just created and then run that so how do we do that so",
    "start": "922740",
    "end": "930300"
  },
  {
    "text": "what we're going to do is we're going to create an environment using infrastructure as code for that I'm",
    "start": "930300",
    "end": "935519"
  },
  {
    "text": "going to use an open source project called cdk or Cloud development kits I've already got the code ready to go",
    "start": "935519",
    "end": "942959"
  },
  {
    "text": "which I'll share here and it's in the repository we have got",
    "start": "942959",
    "end": "948899"
  },
  {
    "text": "some parameters we need to Define for our classes such as the cluster name we need the contain image which we just",
    "start": "948899",
    "end": "955440"
  },
  {
    "text": "created in the script so we Define that here and then the S3 bucket which we're",
    "start": "955440",
    "end": "960600"
  },
  {
    "text": "going to use later on we then have two stacks the first creates some networking",
    "start": "960600",
    "end": "965880"
  },
  {
    "text": "infrastructure our virtual private cloud and then the second one actually creates",
    "start": "965880",
    "end": "970920"
  },
  {
    "text": "the actual cluster itself including all the permissions we need scope down to the bare minimum so that we only can do",
    "start": "970920",
    "end": "977880"
  },
  {
    "text": "what we need to do in order to run this task so to deploy this what we do is we from",
    "start": "977880",
    "end": "985440"
  },
  {
    "text": "the command line if we go to the cdk folder",
    "start": "985440",
    "end": "991680"
  },
  {
    "text": "we do cdk list which lists all the available Stacks we can deploy and we can then do a cdk deploy and I've",
    "start": "991680",
    "end": "1001579"
  },
  {
    "text": "already deployed the VPC once so let's deploy the",
    "start": "1001579",
    "end": "1006920"
  },
  {
    "text": "um the actual ECS cluster and this will prompt me to confirm what I want to do",
    "start": "1006920",
    "end": "1012860"
  },
  {
    "text": "this and also alerting me to any security stuff that's changing or being added",
    "start": "1012860",
    "end": "1018440"
  },
  {
    "text": "and this is going to take about five minutes to do so I'm going to put this on pause whilst it deploys",
    "start": "1018440",
    "end": "1025600"
  },
  {
    "text": "so this is now finished we can see it's generated some output if we go to the AWS console if we want to actually see",
    "start": "1027740",
    "end": "1034459"
  },
  {
    "text": "what actually was deployed we can go to cloudflamations",
    "start": "1034459",
    "end": "1039678"
  },
  {
    "text": "and we should see up here a number of stacks so here we can see",
    "start": "1040040",
    "end": "1046160"
  },
  {
    "text": "the VPC stack and this is the one we've just deployed and we can see all the resources that um this stack has",
    "start": "1046160",
    "end": "1054020"
  },
  {
    "text": "deployed now what we can do is we can go actually to",
    "start": "1054020",
    "end": "1059780"
  },
  {
    "text": "the console for Amazon ECS and if we refresh we should now see our",
    "start": "1059780",
    "end": "1065299"
  },
  {
    "text": "cluster so this is the cluster here we go we look at the code",
    "start": "1065299",
    "end": "1071179"
  },
  {
    "text": "we can see that we gave it a name here and it matches to that",
    "start": "1071179",
    "end": "1077299"
  },
  {
    "text": "we can see that this cluster has got some resources so in this instance it's a virtual machine so when we run and",
    "start": "1077299",
    "end": "1085039"
  },
  {
    "text": "execute our one of our containers it's going to be running in the cloud the actual configuration for the",
    "start": "1085039",
    "end": "1091700"
  },
  {
    "text": "application I that script we ran earlier on is defined in a task definition and",
    "start": "1091700",
    "end": "1097280"
  },
  {
    "text": "we can see here it's called Apache airflow and when we click on here we can see the",
    "start": "1097280",
    "end": "1103220"
  },
  {
    "text": "configuration of this task and if we go down to here we can see that we've got the container image name",
    "start": "1103220",
    "end": "1110840"
  },
  {
    "text": "that we created and then we've got the command that we're going to pass into that container",
    "start": "1110840",
    "end": "1117140"
  },
  {
    "text": "so we can quickly test this out to make sure it's working by running the task",
    "start": "1117140",
    "end": "1123559"
  },
  {
    "text": "and then defining where we want this to run so in this instance we're going to run it on that in the cloud so we run on ec2 instance",
    "start": "1123559",
    "end": "1129799"
  },
  {
    "text": "we click on run task and it's now pass that task to the",
    "start": "1129799",
    "end": "1135200"
  },
  {
    "text": "resources of the cluster so the cluster is going to take that take that task run it on the available resource so that one",
    "start": "1135200",
    "end": "1140600"
  },
  {
    "text": "virtual machine it's running here we can see it's in the state of pending because the",
    "start": "1140600",
    "end": "1147020"
  },
  {
    "text": "first time the image is running it's going to download it from Amazon ECR and",
    "start": "1147020",
    "end": "1153260"
  },
  {
    "text": "then it's going to execute it subsequent runs will be quick unless you change the image of course that's now finished we",
    "start": "1153260",
    "end": "1160940"
  },
  {
    "text": "can click on here and we can see that the task is finished and we can go over to logs and we can",
    "start": "1160940",
    "end": "1167960"
  },
  {
    "text": "see that we've actually had the output which should match what we had before so",
    "start": "1167960",
    "end": "1174080"
  },
  {
    "text": "what we've done here is we've shown we've taken that ETR script which we developed locally containerized locally",
    "start": "1174080",
    "end": "1179960"
  },
  {
    "text": "push the cloud and we're now running on the ECS cluster",
    "start": "1179960",
    "end": "1185380"
  },
  {
    "text": "the next step now is to show how we can create a hybrid version of this eye",
    "start": "1185480",
    "end": "1191600"
  },
  {
    "text": "running in uh that your remote location so it could be your data center a remote",
    "start": "1191600",
    "end": "1196760"
  },
  {
    "text": "office or even another public cloud so for that we're going to use ECS",
    "start": "1196760",
    "end": "1203179"
  },
  {
    "text": "anywhere which I talked about during the presentation and I have an ec2 instance",
    "start": "1203179",
    "end": "1210679"
  },
  {
    "text": "which if I go",
    "start": "1210679",
    "end": "1215860"
  },
  {
    "text": "and I can connect to it I've got the",
    "start": "1217700",
    "end": "1223120"
  },
  {
    "text": "details here and I've got the IP address oops",
    "start": "1223160",
    "end": "1228620"
  },
  {
    "text": "not sure what happened there",
    "start": "1228620",
    "end": "1231940"
  },
  {
    "text": "okay that's there we go and now I can connect to it because I",
    "start": "1234620",
    "end": "1241100"
  },
  {
    "text": "have the key here it's running Amazon Linux so I connect using ec2 user",
    "start": "1241100",
    "end": "1249500"
  },
  {
    "text": "and I'm now connecting now on this ec2 instance even though it's running on AWS I'm mimicking a remote office and on",
    "start": "1249500",
    "end": "1257059"
  },
  {
    "text": "this ec2 instance I've actually got MySQL running",
    "start": "1257059",
    "end": "1262659"
  },
  {
    "text": "which I can log into hopefully use local demo and",
    "start": "1266660",
    "end": "1274360"
  },
  {
    "text": "customers and we can see it's got a similar set of data to what we showed",
    "start": "1275780",
    "end": "1281360"
  },
  {
    "text": "when I was in Visual Studio code so we've got this machine and if you",
    "start": "1281360",
    "end": "1290539"
  },
  {
    "text": "recall when we were in the cluster we could see that this cluster has",
    "start": "1290539",
    "end": "1297320"
  },
  {
    "text": "under ECS instances we have one node and this is the node currently that is being used to run all",
    "start": "1297320",
    "end": "1305840"
  },
  {
    "text": "containers now to create a hybrid inversion what we",
    "start": "1305840",
    "end": "1311600"
  },
  {
    "text": "can do is we can use this button here called register external instances and",
    "start": "1311600",
    "end": "1316640"
  },
  {
    "text": "when we click on this it's going to walk us through a couple of screens that will allow us to",
    "start": "1316640",
    "end": "1321740"
  },
  {
    "text": "generate a configuration file which we can then Deploy on this ec2 instance and it will install the ECS anywhere agent",
    "start": "1321740",
    "end": "1328360"
  },
  {
    "text": "configure all the permissions and roles and then integrate that into this cluster that's allowing us to",
    "start": "1328360",
    "end": "1334820"
  },
  {
    "text": "effectively extend the resource capabilities of this ECS cluster and allow us to run that container in that",
    "start": "1334820",
    "end": "1341539"
  },
  {
    "text": "ec2 instance or that remote machine now when I created via cdk",
    "start": "1341539",
    "end": "1349039"
  },
  {
    "text": "the ECS cluster we created a number of roles and I'm going to use one of those",
    "start": "1349039",
    "end": "1354440"
  },
  {
    "text": "roles here which is this one here and this is the one that is going to be used on the remote machine to give it just",
    "start": "1354440",
    "end": "1361460"
  },
  {
    "text": "enough permissions to access the ECR repository so can download containers as well as access Cloud watch so it can",
    "start": "1361460",
    "end": "1367760"
  },
  {
    "text": "write logs so we specify this one here it should be",
    "start": "1367760",
    "end": "1375140"
  },
  {
    "text": "there you go and we only want one instance the",
    "start": "1375140",
    "end": "1380600"
  },
  {
    "text": "activation key you can leave the default we're going to do it right now and we can see here that because I'm",
    "start": "1380600",
    "end": "1387140"
  },
  {
    "text": "running a next command sorry running to Links machine I use this command very recently added we added windows and if",
    "start": "1387140",
    "end": "1392900"
  },
  {
    "text": "you're running a Windows machine you could do this I click on copy here I go to my Linux machine here and",
    "start": "1392900",
    "end": "1401179"
  },
  {
    "text": "I'm going to run this as root",
    "start": "1401179",
    "end": "1405039"
  },
  {
    "text": "sure yes",
    "start": "1407480",
    "end": "1412240"
  },
  {
    "text": "and this is I'm going to put this on pause because it takes a while um during this process it's downloading",
    "start": "1414220",
    "end": "1419480"
  },
  {
    "text": "the agent installing configuring the agent registering it into AWS assistance manager and then the ETS control plane",
    "start": "1419480",
    "end": "1425900"
  },
  {
    "text": "so I'll come back once it's finished it took about five minutes to run and we",
    "start": "1425900",
    "end": "1433039"
  },
  {
    "text": "can see that it's been successful it's giving us a message saying that it's been successfully integrated and if we",
    "start": "1433039",
    "end": "1438080"
  },
  {
    "text": "now go back to our ECS cluster and we refresh we should now see we've",
    "start": "1438080",
    "end": "1445220"
  },
  {
    "text": "got two resources um we've got this new one here which is got a different identifier and the MI",
    "start": "1445220",
    "end": "1451700"
  },
  {
    "text": "stands for managed instance and we can see that external instance equals to true",
    "start": "1451700",
    "end": "1456919"
  },
  {
    "text": "so what this means is we can now run our task um on this external instance",
    "start": "1456919",
    "end": "1463940"
  },
  {
    "text": "so again what we can do is take the task we had and we can run it",
    "start": "1463940",
    "end": "1469539"
  },
  {
    "text": "and this time we're going to select the external launch type and the external",
    "start": "1469539",
    "end": "1475220"
  },
  {
    "text": "launch type will match that instance that we've just added via ECS anywhere",
    "start": "1475220",
    "end": "1480380"
  },
  {
    "text": "and we can click on run and it's going to start running and",
    "start": "1480380",
    "end": "1486860"
  },
  {
    "text": "again because it's the first time that the containers run on this external instance it's going to take a few",
    "start": "1486860",
    "end": "1492020"
  },
  {
    "text": "seconds to download that image before it runs and hopefully won't take too long",
    "start": "1492020",
    "end": "1499340"
  },
  {
    "text": "one of the advantages of it being an ec2 instance is that it's got a good speed when I was doing this on my machine here",
    "start": "1499340",
    "end": "1506080"
  },
  {
    "text": "it took a while my internet speed here is not great so give it a few seconds it's run",
    "start": "1506080",
    "end": "1513919"
  },
  {
    "text": "and we can see that it's run here we can check the logs and we can see",
    "start": "1513919",
    "end": "1520700"
  },
  {
    "text": "that the scripts Run Okay so we've now demonstrated that we've actually run our",
    "start": "1520700",
    "end": "1525799"
  },
  {
    "text": "ETL script the container code on the remote machine so if I was in a remote office somewhere it would have been",
    "start": "1525799",
    "end": "1531799"
  },
  {
    "text": "running there rather than in AWS but so far what it's done is it's accessed the",
    "start": "1531799",
    "end": "1537620"
  },
  {
    "text": "same database if we look here uh within the code I've instrumented it so we can see that it's connecting to",
    "start": "1537620",
    "end": "1545000"
  },
  {
    "text": "the database that's in RDS The Next Step really is to change this so that we run",
    "start": "1545000",
    "end": "1552140"
  },
  {
    "text": "and access that local database we've got running so to do that we're going to create a new task definition so I click on my",
    "start": "1552140",
    "end": "1559340"
  },
  {
    "text": "task definition I click on new revision and I then",
    "start": "1559340",
    "end": "1565520"
  },
  {
    "text": "can configure the",
    "start": "1565520",
    "end": "1570860"
  },
  {
    "text": "database I'm going to connect to by changing this value here if you recall from the beginning of this presentation I Define the database",
    "start": "1570860",
    "end": "1579140"
  },
  {
    "text": "environments that I want to connect to via some parameters I store in sequence manager so if I go back to here this is",
    "start": "1579140",
    "end": "1586640"
  },
  {
    "text": "the current one RDS if I select this one here this points to and contains the",
    "start": "1586640",
    "end": "1591799"
  },
  {
    "text": "information to connect to the local database and go back to ECS cluster and select",
    "start": "1591799",
    "end": "1600340"
  },
  {
    "text": "this one here and click on update",
    "start": "1601039",
    "end": "1609279"
  },
  {
    "text": "and click on create I now have a new vision of my task and if I click on run",
    "start": "1609679",
    "end": "1615620"
  },
  {
    "text": "and select external and click on run this time it should not",
    "start": "1615620",
    "end": "1622700"
  },
  {
    "text": "take very long to run because the container image is already run click on stopped and we can see that is it wrong",
    "start": "1622700",
    "end": "1631100"
  },
  {
    "text": "okay which which one is it is this one here",
    "start": "1631100",
    "end": "1638059"
  },
  {
    "text": "you can check the logs and we can see that it's connected to",
    "start": "1638059",
    "end": "1643640"
  },
  {
    "text": "the local database and it's brought back this data so now we've actually",
    "start": "1643640",
    "end": "1649779"
  },
  {
    "text": "got the ECS to execute this running container running in that remote",
    "start": "1649779",
    "end": "1656059"
  },
  {
    "text": "instance running on against a local database and then if we look ultimately",
    "start": "1656059",
    "end": "1661520"
  },
  {
    "text": "what we're trying to do here on RS3 bucket go back to here we can see that we've",
    "start": "1661520",
    "end": "1667580"
  },
  {
    "text": "got some data and if we look back actually what we were looking to",
    "start": "1667580",
    "end": "1672679"
  },
  {
    "text": "configure here in the task I can't remember which bucket we were trying to put it into but",
    "start": "1672679",
    "end": "1679520"
  },
  {
    "text": "we can quickly find it out um it was basically period one HQ data",
    "start": "1679520",
    "end": "1685580"
  },
  {
    "text": "and if we look here period one we can see uh the time the timestamp matches up so this is the data",
    "start": "1685580",
    "end": "1691820"
  },
  {
    "text": "that we've just got from that local machine so we can we've now demonstrated how we can containerize that ETL script",
    "start": "1691820",
    "end": "1698480"
  },
  {
    "text": "run it both in the cloud and remote it could have been you know a remote branch",
    "start": "1698480",
    "end": "1703520"
  },
  {
    "text": "office it could have been in another cloud or our data center the next stage now is to see how we can incorporate",
    "start": "1703520",
    "end": "1709760"
  },
  {
    "text": "that into our workflow within Apache airflow",
    "start": "1709760",
    "end": "1715000"
  },
  {
    "text": "to configure and provision an Apache airflow environment I am using an AWS",
    "start": "1715220",
    "end": "1720380"
  },
  {
    "text": "service called manage workflows for Apache airflow and to create the environment again I've used more",
    "start": "1720380",
    "end": "1725720"
  },
  {
    "text": "infrastructure service code um again that this code is all in the repository which uses another file to Via",
    "start": "1725720",
    "end": "1733220"
  },
  {
    "text": "parameters configure the key values in this instance is the name and the location of the S3 bucket where we will",
    "start": "1733220",
    "end": "1739880"
  },
  {
    "text": "store our dags or our workflow files there are two stacks uh one configures",
    "start": "1739880",
    "end": "1744980"
  },
  {
    "text": "the VPC Network the second one creates the actual environment itself now the",
    "start": "1744980",
    "end": "1752299"
  },
  {
    "text": "provisioning creating this environment takes about 30 minutes so I've already pre-provisioned this environment and",
    "start": "1752299",
    "end": "1757419"
  },
  {
    "text": "again I can look at what gets provisioned by looking at cloud",
    "start": "1757419",
    "end": "1762559"
  },
  {
    "text": "formation so I can see all the different resources that get created and I can go to the actual",
    "start": "1762559",
    "end": "1768100"
  },
  {
    "text": "Apache airflow manage workflows for Apache airflow console I can see my environment here and then I can actually",
    "start": "1768100",
    "end": "1774320"
  },
  {
    "text": "access the console this way and by default in this code I deployed two very simple workflows so I have my",
    "start": "1774320",
    "end": "1781460"
  },
  {
    "text": "environment ready to go I now need to work on the actual workflows themselves",
    "start": "1781460",
    "end": "1786740"
  },
  {
    "text": "so I have actually these um as well within the code within the repository",
    "start": "1786740",
    "end": "1793460"
  },
  {
    "text": "within the dag folder I've got um three that are of Interest one is uh",
    "start": "1793460",
    "end": "1799700"
  },
  {
    "text": "called ec2 uh ECS hybrid ec2 one is called hybrid external and then the",
    "start": "1799700",
    "end": "1805820"
  },
  {
    "text": "other one's called hybrid local so what these do is they take the things that I've just shown you how we've run",
    "start": "1805820",
    "end": "1812480"
  },
  {
    "text": "them through the ECS console but they wrap them around an Apache airflow",
    "start": "1812480",
    "end": "1818539"
  },
  {
    "text": "operator called esis operator and then get the Apache airflow worker to execute these on these on our behalf",
    "start": "1818539",
    "end": "1825799"
  },
  {
    "text": "so let's walk through this so we have our standard Apache airflow Imports",
    "start": "1825799",
    "end": "1831799"
  },
  {
    "text": "we're going to be using the esys operator so we import that from the Amazon provider we then create some",
    "start": "1831799",
    "end": "1837620"
  },
  {
    "text": "default args we Define the name of our workflow we're not going to put this on",
    "start": "1837620",
    "end": "1844220"
  },
  {
    "text": "a schedule so we're just going to run it on demand and then we actually have the parameters",
    "start": "1844220",
    "end": "1849980"
  },
  {
    "text": "of our actual operator itself so we need to give it a task ID so in",
    "start": "1849980",
    "end": "1855919"
  },
  {
    "text": "this instance I've called it Cloud query and we need to define the ECS cluster",
    "start": "1855919",
    "end": "1861260"
  },
  {
    "text": "that we want to run the task on so this is the name of the ECS cluster so if you",
    "start": "1861260",
    "end": "1866779"
  },
  {
    "text": "recall it's called qcon hybrid airflow we then need to give it the name of a task definition so the task definition",
    "start": "1866779",
    "end": "1873820"
  },
  {
    "text": "contains the actual task this is this screen here",
    "start": "1873820",
    "end": "1878899"
  },
  {
    "text": "um of what we want to run and as you can see it's called Apache airflow and you",
    "start": "1878899",
    "end": "1884000"
  },
  {
    "text": "can Define just the name and it will always use the latest you'll notice that it has a number every time you create a",
    "start": "1884000",
    "end": "1891140"
  },
  {
    "text": "new version the the number increments if you have no number here it will always use the latest if you want to be",
    "start": "1891140",
    "end": "1897260"
  },
  {
    "text": "specific you can actually put a code on a number to tie it and pin it to a",
    "start": "1897260",
    "end": "1902840"
  },
  {
    "text": "specific version we can then pass in overrides to the containable we'll see see an example of",
    "start": "1902840",
    "end": "1909200"
  },
  {
    "text": "that in a minute and then here we Define the launch type as I've mentioned before the first time we ran the uh the ETL",
    "start": "1909200",
    "end": "1916700"
  },
  {
    "text": "script we ran on ec2 launch type then when we ran via the hybrid ecs8 anywhere",
    "start": "1916700",
    "end": "1922820"
  },
  {
    "text": "mode we selected external so here we're going to be running on ec2 just so we can see it running we then Define the",
    "start": "1922820",
    "end": "1930140"
  },
  {
    "text": "log group that this ECS cluster has been configured as well as the prefix for",
    "start": "1930140",
    "end": "1935240"
  },
  {
    "text": "this particular task so we can see the output of this task when it runs",
    "start": "1935240",
    "end": "1941179"
  },
  {
    "text": "if I look at the external one this actually looks exactly the same the",
    "start": "1941179",
    "end": "1947240"
  },
  {
    "text": "only difference being is the launch type is external and then if um but both this one and the",
    "start": "1947240",
    "end": "1955940"
  },
  {
    "text": "ec2 one are running the task as is and if we recall that is running it against",
    "start": "1955940",
    "end": "1962120"
  },
  {
    "text": "the RDS instance but what if we wanted to for example pass in uh new new",
    "start": "1962120",
    "end": "1967159"
  },
  {
    "text": "queries or connect to different databases well here we can actually use the override",
    "start": "1967159",
    "end": "1972440"
  },
  {
    "text": "and here we specify the same information as for the external one and we override",
    "start": "1972440",
    "end": "1979880"
  },
  {
    "text": "the actual command we want to pass in so here for example I'm specifying the fact",
    "start": "1979880",
    "end": "1985940"
  },
  {
    "text": "that I want to use um this folder and and this file I want to",
    "start": "1985940",
    "end": "1993080"
  },
  {
    "text": "upload I select a query um and here I specify a different",
    "start": "1993080",
    "end": "2000039"
  },
  {
    "text": "um MySQL instance um now just to just to make this",
    "start": "2000039",
    "end": "2005740"
  },
  {
    "text": "interesting what I can do is I can change that to a different um name",
    "start": "2005740",
    "end": "2012760"
  },
  {
    "text": "and um we get we can we can deploy these into our Apache airflow cluster now um",
    "start": "2012760",
    "end": "2020260"
  },
  {
    "text": "typically you would do this through a ccic pipeline I'm going to just upload them into the",
    "start": "2020260",
    "end": "2027880"
  },
  {
    "text": "folder here so I'm going to just upload these",
    "start": "2027880",
    "end": "2032919"
  },
  {
    "text": "and the project qcon and they should so local external so if",
    "start": "2032919",
    "end": "2041980"
  },
  {
    "text": "I upload these here so this is uploaded into the Apache",
    "start": "2041980",
    "end": "2048700"
  },
  {
    "text": "airflow dags folder we can see them here it will take a few seconds before they",
    "start": "2048700",
    "end": "2054339"
  },
  {
    "text": "actually appear in the Apache Apple console so we'll give it a few seconds I'm going",
    "start": "2054339",
    "end": "2060339"
  },
  {
    "text": "to put on pause while this um while we wait for this to happen so that took a couple of minutes and we",
    "start": "2060339",
    "end": "2066339"
  },
  {
    "text": "can now see that the dags workflows are in the Apache airflow UI",
    "start": "2066339",
    "end": "2071858"
  },
  {
    "text": "if we open up this one and check the code",
    "start": "2071859",
    "end": "2077200"
  },
  {
    "text": "we can see that it's the same code that we saw in visual studio so let's enable",
    "start": "2077200",
    "end": "2083858"
  },
  {
    "text": "this and Trigger it and what we should see",
    "start": "2083859",
    "end": "2091619"
  },
  {
    "text": "is after a short while the Apache airflow is handled this over to the",
    "start": "2093040",
    "end": "2098500"
  },
  {
    "text": "scheduler schedulers is running it on a Apache worker working ode the task is",
    "start": "2098500",
    "end": "2104320"
  },
  {
    "text": "being executed on the worker node and as we can see here the task as completed and if we look at the logs we should see",
    "start": "2104320",
    "end": "2112839"
  },
  {
    "text": "the SQL return as we can see here now as we can see from here this was act this",
    "start": "2112839",
    "end": "2119560"
  },
  {
    "text": "was accessing the database in the cloud on RDS so let's now try and do the same",
    "start": "2119560",
    "end": "2126760"
  },
  {
    "text": "thing with the local so here we can actually",
    "start": "2126760",
    "end": "2134020"
  },
  {
    "text": "look at the code and we can see here that we're passing on new parameters in this instance we're",
    "start": "2134020",
    "end": "2141700"
  },
  {
    "text": "specifying the local database so let's",
    "start": "2141700",
    "end": "2147760"
  },
  {
    "text": "trigger this one and we should see exactly the same thing",
    "start": "2147760",
    "end": "2154660"
  },
  {
    "text": "we should see it again passing over to the worker node the worker node is going to pass it onto ECS because it's set to",
    "start": "2154660",
    "end": "2161619"
  },
  {
    "text": "external it's going to run on the ECS anywhere so this is our our fake",
    "start": "2161619",
    "end": "2167020"
  },
  {
    "text": "external remote branch and we can see it's run and we can take a look at the logs",
    "start": "2167020",
    "end": "2174180"
  },
  {
    "text": "and we can see that it's this time it's only brought back one record but we can",
    "start": "2174579",
    "end": "2180099"
  },
  {
    "text": "see actually from The Source IP um that is actually our remote",
    "start": "2180099",
    "end": "2185680"
  },
  {
    "text": "um instances Alice accessing the database are locally here so now I've showed um",
    "start": "2185680",
    "end": "2191920"
  },
  {
    "text": "these as two separate workflows but in reality when you create your own hybrid workflows you would use these in",
    "start": "2191920",
    "end": "2198040"
  },
  {
    "text": "conjunction with other other activities other other tasks in order to create your data pipeline",
    "start": "2198040",
    "end": "2204540"
  },
  {
    "text": "now one thing I didn't cover in any depth during the demo was the security and permissions and how these are",
    "start": "2205660",
    "end": "2212020"
  },
  {
    "text": "configured so there are a number of important policies and roles that we need to",
    "start": "2212020",
    "end": "2217540"
  },
  {
    "text": "create in order to make sure that we only provide access to the bare minimum",
    "start": "2217540",
    "end": "2223900"
  },
  {
    "text": "Services when setting this up if we start from the bottom of the diagram and the local environment we",
    "start": "2223900",
    "end": "2230440"
  },
  {
    "text": "have the ECS anywhere agent and this needs a role and a set of permissions in",
    "start": "2230440",
    "end": "2235900"
  },
  {
    "text": "order for it to access and interact with AWS to do things such as access the ECR",
    "start": "2235900",
    "end": "2241020"
  },
  {
    "text": "repository download containers send logging information to cloudwatch that kind of thing and this is called the ECS",
    "start": "2241020",
    "end": "2247780"
  },
  {
    "text": "anywhere task execution role and so we will have a very small scoped",
    "start": "2247780",
    "end": "2254680"
  },
  {
    "text": "um and narrow set for Missions uh for that task to run",
    "start": "2254680",
    "end": "2260020"
  },
  {
    "text": "um within the ECS cluster environment we have a similar",
    "start": "2260020",
    "end": "2265300"
  },
  {
    "text": "um a set of permissions which is the task execution role and these are the permissions that the ECS cluster has for",
    "start": "2265300",
    "end": "2272859"
  },
  {
    "text": "doing similar access so downloading container images Cloud watch logs and",
    "start": "2272859",
    "end": "2279400"
  },
  {
    "text": "running tasks now within Apache airflow the worker nodes have a policy which is called the",
    "start": "2279400",
    "end": "2287440"
  },
  {
    "text": "a role rather which is called the mar execution role and that contains a set of policies that allowed it to interact",
    "start": "2287440",
    "end": "2293500"
  },
  {
    "text": "with other AWS services so for example the it will it allows us to allow us to",
    "start": "2293500",
    "end": "2300040"
  },
  {
    "text": "run and execute ECS tasks and then finally within the actual containerized",
    "start": "2300040",
    "end": "2307119"
  },
  {
    "text": "application itself we have then the task permissions So within the actual code",
    "start": "2307119",
    "end": "2312520"
  },
  {
    "text": "that we're running what permissions do we need that code to have in order for",
    "start": "2312520",
    "end": "2318400"
  },
  {
    "text": "it to work so in this instance we need to make sure that we've got access to RDS but we've got access to the S3 so we",
    "start": "2318400",
    "end": "2325300"
  },
  {
    "text": "can copy the files and that is the same policy that's used both in ECS cluster",
    "start": "2325300",
    "end": "2332200"
  },
  {
    "text": "when it's running in the cloud but also when it's running locally in the ECS anywhere tasks so it's still using the",
    "start": "2332200",
    "end": "2338619"
  },
  {
    "text": "same policy but the difference is that one when it's running on easy anyway it's running and using that policy",
    "start": "2338619",
    "end": "2343839"
  },
  {
    "text": "locally in your local environmental remote environment and then when it's running in the Cs cluster it's running in the cloud",
    "start": "2343839",
    "end": "2351660"
  },
  {
    "text": "to further illustrate um this approach is how different sets",
    "start": "2352480",
    "end": "2358420"
  },
  {
    "text": "of users within your environment need different sets of access to make a",
    "start": "2358420",
    "end": "2364060"
  },
  {
    "text": "solution like this work so typically we've got two kind of personas here at the top we've got the data engineer and",
    "start": "2364060",
    "end": "2369880"
  },
  {
    "text": "who typically are and what they care about is actually creating the code and orchestrating the data pipelines and",
    "start": "2369880",
    "end": "2376660"
  },
  {
    "text": "they don't really want to know much about the plumbing underneath that and then we've got the system administration",
    "start": "2376660",
    "end": "2383380"
  },
  {
    "text": "or maybe your devops team who build the infrastructure of your infrastructure code who manage Secrets who provision",
    "start": "2383380",
    "end": "2391599"
  },
  {
    "text": "everything so that the data Engineers can do the work and here we can see this",
    "start": "2391599",
    "end": "2396640"
  },
  {
    "text": "solution in the context of the kind of activities which um would each of those percenters would",
    "start": "2396640",
    "end": "2403119"
  },
  {
    "text": "do so the sysadmin would typically deploy the ECS anywhere locally and get",
    "start": "2403119",
    "end": "2409119"
  },
  {
    "text": "that up and running they would configure and and make sure that your easy yes clusters up and running they would",
    "start": "2409119",
    "end": "2414700"
  },
  {
    "text": "configure and Define the secrets they would set up your RDS database they will set up the permissions and policies as",
    "start": "2414700",
    "end": "2421599"
  },
  {
    "text": "well as creating and deploying the Apache airflow environment and then the data engineers at the top they would",
    "start": "2421599",
    "end": "2428859"
  },
  {
    "text": "actually create the ETL scripts and containerize those using typically your standard",
    "start": "2428859",
    "end": "2434760"
  },
  {
    "text": "software development lifecycle approach but they would also create the dags the",
    "start": "2434760",
    "end": "2440380"
  },
  {
    "text": "workflows within Apache airflow and deploy those and typically most organizations will have a kind of cic",
    "start": "2440380",
    "end": "2446680"
  },
  {
    "text": "system for both of those approaches both for creating the dags but also for their ETL workflow the output of that being a",
    "start": "2446680",
    "end": "2454540"
  },
  {
    "text": "workflow dag file which gets deployed onto the Apache airflow or a container",
    "start": "2454540",
    "end": "2460780"
  },
  {
    "text": "image that gets pushed into the uh your container repository so here in this example I'm I'm showing that it's S3 for",
    "start": "2460780",
    "end": "2469240"
  },
  {
    "text": "Apache airflow and the ECR Amazon ECR as the repository the image repository and",
    "start": "2469240",
    "end": "2475119"
  },
  {
    "text": "we can see here how this enables a nice clean separation of duties for both",
    "start": "2475119",
    "end": "2480220"
  },
  {
    "text": "those activities so that as a data engineer I can focus on writing just the",
    "start": "2480220",
    "end": "2485260"
  },
  {
    "text": "scripts I have um committing my code and it will automatically deploy those into the",
    "start": "2485260",
    "end": "2491800"
  },
  {
    "text": "right repositories and I can then use the ECR operator to actually execute those and I don't have to provision any",
    "start": "2491800",
    "end": "2497800"
  },
  {
    "text": "of that stuff and I don't need to know things such as the secrets for the",
    "start": "2497800",
    "end": "2502960"
  },
  {
    "text": "resources I'm accessing now one of the things I like about using",
    "start": "2502960",
    "end": "2509740"
  },
  {
    "text": "containers and specifically the ECS operator approach in conjunction with ECS anywhere is there are a couple of",
    "start": "2509740",
    "end": "2515320"
  },
  {
    "text": "strengths first of all implementing the solution is super simple",
    "start": "2515320",
    "end": "2520380"
  },
  {
    "text": "from an infrastructure perspective there's no complex firewalls to manage it's a simple local install of the ECS",
    "start": "2520380",
    "end": "2527740"
  },
  {
    "text": "agent which integrates that system into the ECS control plane and also the",
    "start": "2527740",
    "end": "2535060"
  },
  {
    "text": "system AWS System Manager as well from a development perspective",
    "start": "2535060",
    "end": "2540339"
  },
  {
    "text": "um you can reuse your existing development patterns to create and manage your container images and from a",
    "start": "2540339",
    "end": "2546160"
  },
  {
    "text": "management perspective you can manage everything from a single pane of glass I.E the ECS control plane it's easy to",
    "start": "2546160",
    "end": "2551560"
  },
  {
    "text": "separate the duties as I've just shown in the previous slide so you've got clear separation from what the data",
    "start": "2551560",
    "end": "2557200"
  },
  {
    "text": "Engineers are doing versus your system admin and potentially uh the people who manage your secrets and they have your",
    "start": "2557200",
    "end": "2565119"
  },
  {
    "text": "data engines have standard operators they need they work with they don't have to learn or create new operators the",
    "start": "2565119",
    "end": "2571660"
  },
  {
    "text": "easiest operator is a standard operator that is part of the Amazon provider package",
    "start": "2571660",
    "end": "2578460"
  },
  {
    "text": "so the code that I use the demo um can be found at this link there's",
    "start": "2578740",
    "end": "2583780"
  },
  {
    "text": "also a link to a blog post which actually walk you through building the whole thing if you are interested in",
    "start": "2583780",
    "end": "2588940"
  },
  {
    "text": "doing that um I have also got a link here to",
    "start": "2588940",
    "end": "2594640"
  },
  {
    "text": "um the presentation and if you have got a couple of minutes to share that feedback I'll be much appreciated it's",
    "start": "2594640",
    "end": "2601420"
  },
  {
    "text": "only about three or four questions and you'll get a copy of the slides as well so thank you very much for your time I",
    "start": "2601420",
    "end": "2607960"
  },
  {
    "text": "hope this has been helpful and I hope to see many more of you building these hybrid workflows using Apache airflow",
    "start": "2607960",
    "end": "2614980"
  },
  {
    "text": "and the ECS operator thank you",
    "start": "2614980",
    "end": "2618900"
  },
  {
    "text": "hi Ricardo that was insightful as always I see there is a demand for the Gita",
    "start": "2621460",
    "end": "2627040"
  },
  {
    "text": "blank people already wants to run your tutorial and see how they can actually",
    "start": "2627040",
    "end": "2632920"
  },
  {
    "text": "yes I hope so it's um it should be straightforward actually and um I I I'm",
    "start": "2632920",
    "end": "2639040"
  },
  {
    "text": "interested in what people do with that because obviously this is just an idea right hybrid is super interesting and I'm kind of like interested in what",
    "start": "2639040",
    "end": "2645700"
  },
  {
    "text": "other people might do how they might take it you know use different variants you know",
    "start": "2645700",
    "end": "2651760"
  },
  {
    "text": "um so I'm at kubecon at the moment and um I've been having interesting conversations about um with data",
    "start": "2651760",
    "end": "2657280"
  },
  {
    "text": "Engineers about how they're using containers to do exactly what um we talk about here in the demo",
    "start": "2657280",
    "end": "2662800"
  },
  {
    "text": "um so watch the space the the kubernetes version hopefully coming soon",
    "start": "2662800",
    "end": "2667900"
  },
  {
    "text": "this is faster it's fascinating can you play can you please share more what's them like around hybrid",
    "start": "2667900",
    "end": "2673240"
  },
  {
    "text": "so yeah so I think um the conversation really is around you know um customers are moving to the cloud but",
    "start": "2673240",
    "end": "2680619"
  },
  {
    "text": "they've still got a significant amount of assets um data applications",
    "start": "2680619",
    "end": "2687520"
  },
  {
    "text": "um in in their current environments okay and these are important applications you can't just leave them quite often",
    "start": "2687520",
    "end": "2692980"
  },
  {
    "text": "they're still running important business processes um other customers who are in say industrial segments they've got remote",
    "start": "2692980",
    "end": "2699819"
  },
  {
    "text": "offices with very little local I.T skills and so what's really needed or at least what what why they're telling me",
    "start": "2699819",
    "end": "2706119"
  },
  {
    "text": "is that this is a nice solution is that they don't have the expertise to run a remote kubernetes cluster right they",
    "start": "2706119",
    "end": "2711400"
  },
  {
    "text": "want a really simple solution that literally they can just run install and then leave it okay and I think that's",
    "start": "2711400",
    "end": "2717880"
  },
  {
    "text": "the beauty of these kinds of solutions it's fascinating I remember when hybrid",
    "start": "2717880",
    "end": "2724180"
  },
  {
    "text": "the conversation about hybrid just started was always confusing between um do you mean hybrid like having some",
    "start": "2724180",
    "end": "2731260"
  },
  {
    "text": "of the workloads on-prem and then on the cloud or hybrid like among the different clouds so it could actually be it could",
    "start": "2731260",
    "end": "2738880"
  },
  {
    "text": "actually be both right it could be absolutely both um I think you know it depends on really",
    "start": "2738880",
    "end": "2744400"
  },
  {
    "text": "what the customer's trying to do you know sometimes I have had customers who are moving from One Cloud to another and",
    "start": "2744400",
    "end": "2751480"
  },
  {
    "text": "so they've got data assets in one cloud and they want to access them and incorporate them in their data pipelines",
    "start": "2751480",
    "end": "2757660"
  },
  {
    "text": "um I think the key thing though is um you know we've always been able to do this with Apache airflow as long as",
    "start": "2757660",
    "end": "2763900"
  },
  {
    "text": "you've got network connectivity but network connectivity comes out you know some complexity right so I think",
    "start": "2763900",
    "end": "2769420"
  },
  {
    "text": "Solutions like this make it really really super simple because effectively it's just an outbound connection from",
    "start": "2769420",
    "end": "2775119"
  },
  {
    "text": "the agent there's no vpns required it's all encrypted so it's secure and I think that's the that's the key and I think",
    "start": "2775119",
    "end": "2781359"
  },
  {
    "text": "that um you know the the re the reason why um I think we'll still see that hybrid",
    "start": "2781359",
    "end": "2787720"
  },
  {
    "text": "is that you know especially in Europe you know there are you know lots of regulatory and compliance",
    "start": "2787720",
    "end": "2793240"
  },
  {
    "text": "um use cases where you can't do the processing using using um your Apache airflow operators in the cloud right so",
    "start": "2793240",
    "end": "2800260"
  },
  {
    "text": "you've got to do them locally you've got you've got to do that locally um and so that that's where solution",
    "start": "2800260",
    "end": "2805420"
  },
  {
    "text": "like this can help you out got it interesting well we have uh the",
    "start": "2805420",
    "end": "2810760"
  },
  {
    "text": "last question before we have to wrap up uh so how does Apache airflow compares to AWS glue so I think um",
    "start": "2810760",
    "end": "2818819"
  },
  {
    "text": "so so it's I would say that a better comparison is",
    "start": "2818819",
    "end": "2823900"
  },
  {
    "text": "Apache airflow and step functions um they do very very similar thing they're kind of orchestrators",
    "start": "2823900",
    "end": "2829960"
  },
  {
    "text": "um and I think what um I'm a little bit familiar with with as glue but that that I would say that",
    "start": "2829960",
    "end": "2835480"
  },
  {
    "text": "is more used for doing more of the ETL stuff so the stuff that I was doing in the container is typically what you",
    "start": "2835480",
    "end": "2840880"
  },
  {
    "text": "would do with them eight years ago and you can actually obviously orchestrate a Lewis glue with Apache airflow which is",
    "start": "2840880",
    "end": "2846160"
  },
  {
    "text": "a very common common pattern oh that fascinating Ricardo thank you so",
    "start": "2846160",
    "end": "2851440"
  },
  {
    "text": "thank you so for joining us today um folks everyone who wants to continue",
    "start": "2851440",
    "end": "2857560"
  },
  {
    "text": "the conversation with Ricardo please join the hangout with uh with Ricardo by clicking on the yellow Banner on the",
    "start": "2857560",
    "end": "2864339"
  },
  {
    "text": "schedule page once you go to the schedule page you can see uh yellow Banner right there so just click it it's",
    "start": "2864339",
    "end": "2869980"
  },
  {
    "text": "going to take you to a zoom uh and during the break when you have the time check out the sponsor area and you can",
    "start": "2869980",
    "end": "2875319"
  },
  {
    "text": "opt in to unlock ebooks white papers demos and giveaways um Ricardo thank you so much uh for",
    "start": "2875319",
    "end": "2882579"
  },
  {
    "text": "joining us today and sharing your knowledge and experience with everyone thank you Addie for the opportunity it's",
    "start": "2882579",
    "end": "2888400"
  },
  {
    "text": "been really great and I hope everyone's enjoying a great coupon likewise take care thanks I see you on",
    "start": "2888400",
    "end": "2896079"
  },
  {
    "text": "this on the zoom chat yes",
    "start": "2896079",
    "end": "2899760"
  },
  {
    "text": "[Music]",
    "start": "2902180",
    "end": "2907630"
  }
]