[
  {
    "start": "0",
    "end": "119000"
  },
  {
    "text": "so good morning my name is Francis Perry and I'm Tyler Aikido and we are both engineers at Google and also committers",
    "start": "3930",
    "end": "10889"
  },
  {
    "text": "on the Apache beam project and we're here today to talk to you about string processing and more specifically this",
    "start": "10889",
    "end": "17130"
  },
  {
    "text": "unified model for both stream processing and batch processing but this unified model within apache beam that we've",
    "start": "17130",
    "end": "22920"
  },
  {
    "text": "developed over a number of years that we think really makes it relatively easy to think about stream processing and you",
    "start": "22920",
    "end": "29009"
  },
  {
    "text": "know build stream processing applications that are robust and you know able to give you the kinds of results that you want all right so we're",
    "start": "29009",
    "end": "35970"
  },
  {
    "text": "gonna start today by talking about data shapes what kind of data are we interested in processing after that",
    "start": "35970",
    "end": "42120"
  },
  {
    "text": "we're gonna move on to the second section where we're gonna look at the four questions that underlie this model",
    "start": "42120",
    "end": "47250"
  },
  {
    "text": "what where when and how then we want to explain to you why those are the right questions for asking these solving these",
    "start": "47250",
    "end": "53520"
  },
  {
    "text": "types of problems and how they make it very intuitive to reason about a very complex set of use cases and then at the",
    "start": "53520",
    "end": "59220"
  },
  {
    "text": "end we're gonna wrap it all up by taking a little more concrete and looking specifically at what this incubating",
    "start": "59220",
    "end": "64650"
  },
  {
    "text": "Apache bean project is all right so let's get started with data shapes so throughout this talk we're going to be",
    "start": "64650",
    "end": "70830"
  },
  {
    "text": "using a running example and in this example we're a mobile gaming company and we've just launched a new mobile",
    "start": "70830",
    "end": "76290"
  },
  {
    "text": "game for users across the world so we've got users all over the globe and they're",
    "start": "76290",
    "end": "81420"
  },
  {
    "text": "frantically doing some sort of mind-numbing task on their mobile phone candy killing works better what our kids",
    "start": "81420",
    "end": "87810"
  },
  {
    "text": "are doing these days yeah we're not kids every time they do this they're earning points for their",
    "start": "87810",
    "end": "92820"
  },
  {
    "text": "team and we'd like to analyze those logs and figure out what's going on how our games being used and so as we do this",
    "start": "92820",
    "end": "100110"
  },
  {
    "text": "you know we're generating some user events some data so each each of the the colored squares in here represents um",
    "start": "100110",
    "end": "105870"
  },
  {
    "text": "some score for a team or something like that and as we start out our you know our app is new so our data is relatively small and and it looks something like",
    "start": "105870",
    "end": "112470"
  },
  {
    "text": "this maybe now our game gets a little more popular so our data set gets bigger and then it gets really really popular",
    "start": "112470",
    "end": "119100"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "and our data starts to get so big that we have to start introducing some sort of structure to it to keep it manageable so you know maybe we start siloing it",
    "start": "119100",
    "end": "125400"
  },
  {
    "text": "into these daily groups of data so that we can we can deal with the fact that you know our popularity has massively",
    "start": "125400",
    "end": "131580"
  },
  {
    "text": "scaled up our data size but really this kind of organized from structure is just a cheap way to",
    "start": "131580",
    "end": "137209"
  },
  {
    "text": "represent an infinite stream of data right once our game launches and users start addictively playing our awesome",
    "start": "137209",
    "end": "143150"
  },
  {
    "text": "game they're never gonna stop and those events will just keep streaming into our system for processing but given that we've got this massive",
    "start": "143150",
    "end": "149480"
  },
  {
    "text": "scale data we're doing this all on top of distributed systems and as you know distributed systems being distributed",
    "start": "149480",
    "end": "154810"
  },
  {
    "text": "can introduce them ambiguities so let's look at some points that were scored",
    "start": "154810",
    "end": "160939"
  },
  {
    "text": "right about eight o'clock in the morning to understand what distributed systems are gonna cause here so this red score",
    "start": "160939",
    "end": "166609"
  },
  {
    "text": "was happened right around 8:00 a.m. and arrived in our system for processing almost immediately and the second one",
    "start": "166609",
    "end": "172639"
  },
  {
    "text": "here this yellow one also happened at 8 a.m. arrived a little bit later of 8 28 30 something like that maybe there was",
    "start": "172639",
    "end": "178250"
  },
  {
    "text": "network congestion who know some some some blip in the distributed systems caused it to show up later and over here",
    "start": "178250",
    "end": "184879"
  },
  {
    "text": "we've got this green score this happened at 8 a.m. the user crushed the candy or whatever they were doing but didn't make",
    "start": "184879",
    "end": "190310"
  },
  {
    "text": "it our system till closer to 2:30 in the afternoon for processing so perhaps this was a score where a user was online or",
    "start": "190310",
    "end": "197569"
  },
  {
    "text": "playing our game on their phone and c-26 be on a transatlantic flight with no elbow room right in airplane mode it",
    "start": "197569",
    "end": "204530"
  },
  {
    "text": "wasn't until that plane landed and that user reconnected to a network that they were a their phone was able to send that",
    "start": "204530",
    "end": "210379"
  },
  {
    "text": "event to our system for processing so now we've got this unordered infinite data set how do we process it so some",
    "start": "210379",
    "end": "218629"
  },
  {
    "start": "218000",
    "end": "218000"
  },
  {
    "text": "kinds of processing are actually pretty simple so element-wise processing is anything that looks at an individual element and",
    "start": "218629",
    "end": "224689"
  },
  {
    "text": "can process it by itself so this would be something like parsing translating filtering any of those types of things",
    "start": "224689",
    "end": "231729"
  },
  {
    "text": "but most other types of things you would want to do involve you know some sort of aggregation of multiple events counting",
    "start": "231729",
    "end": "237620"
  },
  {
    "text": "joining things like that and so you have to be able to combine these multiple events together whenever you're doing",
    "start": "237620",
    "end": "242719"
  },
  {
    "text": "this kind of aggregation you need some way to take that infinite stream of data and chop it up into finite chunks so",
    "start": "242719",
    "end": "248959"
  },
  {
    "text": "that you can actually complete your aggregation and a bit of results the simplest way to do this is is to just",
    "start": "248959",
    "end": "254989"
  },
  {
    "text": "say you know as my data arrive I'm gonna kind of chop it up into these fixed intervals that are just just contained",
    "start": "254989",
    "end": "260269"
  },
  {
    "text": "whatever data arrives in that that hour of time or whatever it is when it shows up and will process it there and but the",
    "start": "260269",
    "end": "266120"
  },
  {
    "text": "drawback of this if you remember that element that green score from shows up about 2:30 we're now",
    "start": "266120",
    "end": "271229"
  },
  {
    "text": "going to process it with other scores from the two to three o'clock window and that means it's being processed out of",
    "start": "271229",
    "end": "276270"
  },
  {
    "text": "context that depending on our algorithm this may or may not be a problem but it could lead to incorrect results so what",
    "start": "276270",
    "end": "282810"
  },
  {
    "text": "we really want to be able to do is to process events in the context that they actually occurred and so so that's sort",
    "start": "282810",
    "end": "288750"
  },
  {
    "text": "of what this diagram is trying to show you know as inputs arrive we want to we actually want to shuffle them into the right space to process them within that",
    "start": "288750",
    "end": "295770"
  },
  {
    "text": "context when they occurred so if you look at this red element here it arrived in the system just after 12:30 maybe it",
    "start": "295770",
    "end": "302009"
  },
  {
    "text": "also happened pretty much immediately then so we'll keep it in that 12 to 1 o'clock window the green one on the",
    "start": "302009",
    "end": "307259"
  },
  {
    "text": "other hand arrived around the same time as the red one but it actually happened much much earlier so if we just processed it in that that 12:00 to 1:00",
    "start": "307259",
    "end": "314340"
  },
  {
    "text": "o'clock window when it arrived that would have been the wrong set of you know the wrong context for that event so what we actually want to do is shuffle",
    "start": "314340",
    "end": "319740"
  },
  {
    "text": "it back into that 11 to 12 window and process it with all the other events that happened in that period so",
    "start": "319740",
    "end": "325110"
  },
  {
    "text": "conceptually this kind of reordering makes sense but to do this in practice we're going to have to formalize that",
    "start": "325110",
    "end": "330330"
  },
  {
    "text": "difference between event time and processing time so we're gonna sit be seeing a bunch of graphs that look kind",
    "start": "330330",
    "end": "335820"
  },
  {
    "start": "333000",
    "end": "333000"
  },
  {
    "text": "of like this throughout the talk the basic format is you've got these two axes of times we've got event time so",
    "start": "335820",
    "end": "341340"
  },
  {
    "text": "this is these are the times that events actually happen and it's on the bottom access that the x-axis in blue then",
    "start": "341340",
    "end": "347070"
  },
  {
    "text": "you've got processing time which is you know the time that the system itself observes the events when they come into the system so when are they actually",
    "start": "347070",
    "end": "353010"
  },
  {
    "text": "processed and that's on the y-axis in green now if distributed systems weren't so gosh-darn distributed and everything",
    "start": "353010",
    "end": "360060"
  },
  {
    "text": "just arrived in our system immediately we'd see everything arrive on that ideal line where it's event time and",
    "start": "360060",
    "end": "365130"
  },
  {
    "text": "processing time we're indistinguishable but reality looks a little bit more like this this red squiggly line on",
    "start": "365130",
    "end": "371539"
  },
  {
    "text": "processing time this is slightly to delay it off of an end time and what you'll see is that delay is variable it",
    "start": "371539",
    "end": "377699"
  },
  {
    "text": "changes over time depending on the behaviors of that distributed system and we call this distance the skew so we",
    "start": "377699",
    "end": "385650"
  },
  {
    "text": "need to track this in order to reason about completeness and correctness from the system so so within the beam model",
    "start": "385650",
    "end": "392610"
  },
  {
    "text": "we refer to this red line as the watermark and it's a it's a statement that you know no event times early",
    "start": "392610",
    "end": "399020"
  },
  {
    "text": "this point are expected to appear in the future now this watermark can be either perfect of heuristic a perfect modern",
    "start": "399020",
    "end": "406099"
  },
  {
    "text": "mark is possible in a few rare situations where we have perfect knowledge of our input data so an",
    "start": "406099",
    "end": "412129"
  },
  {
    "text": "example of that might be if you have sequential log files likelike Kafka that neha was talking about you know if you",
    "start": "412129",
    "end": "417590"
  },
  {
    "text": "know a static set of log files ahead of time and you know that within a given log file you know event times are",
    "start": "417590",
    "end": "422629"
  },
  {
    "text": "continually increasing then you actually have perfect knowledge of all of these events and you can construct a perfect water market but our more common in the",
    "start": "422629",
    "end": "429830"
  },
  {
    "text": "distributed system is that we have to use a heuristic watermark the system has to make its best guess of when all the",
    "start": "429830",
    "end": "435800"
  },
  {
    "text": "events for a given time have arrived and the data is complete so this is especially true in the case of mobile",
    "start": "435800",
    "end": "442430"
  },
  {
    "text": "events we talked about mobile mobile devices go online and offline all the time they may stay offline for large",
    "start": "442430",
    "end": "448400"
  },
  {
    "text": "periods of time and there's just no way for the system to know about this and so given that there's just no way to provide a completely accurate prediction",
    "start": "448400",
    "end": "456229"
  },
  {
    "text": "about when you're gonna see all the events for a given point in time and what do we start dealing with the watermark we have to deal with cases",
    "start": "456229",
    "end": "462710"
  },
  {
    "text": "where it's a little too slow or too fast so if it's too slow the watermark is basically holding up our results this is",
    "start": "462710",
    "end": "469580"
  },
  {
    "text": "going to introduce unnecessary latency into the system we're gonna have to wait a really long time because maybe there's",
    "start": "469580",
    "end": "475099"
  },
  {
    "text": "just one more late element coming even though we have the bulk of our data and we can't give you any result if we're",
    "start": "475099",
    "end": "480169"
  },
  {
    "text": "holding on waiting for that that flight to land then the other problem is that they can be too fast and when you're",
    "start": "480169",
    "end": "485270"
  },
  {
    "text": "using these heuristic watermarks doing your best to estimate completeness it's still an estimate and when that estimate",
    "start": "485270",
    "end": "491690"
  },
  {
    "text": "is wrong you may end up getting events that show up late and you've got to figure out what am I gonna do with these am I gonna throw them away am I gonna",
    "start": "491690",
    "end": "497180"
  },
  {
    "text": "incorporate them somehow so this is the environment we're in these types of infinite out of order",
    "start": "497180",
    "end": "502610"
  },
  {
    "text": "data sets and we'd like to do our data processing within this environment so we think this can actually be pretty",
    "start": "502610",
    "end": "508190"
  },
  {
    "text": "intuitive if we break it down into these four questions so the four questions are",
    "start": "508190",
    "end": "515450"
  },
  {
    "start": "513000",
    "end": "513000"
  },
  {
    "text": "what are you computing so this is essentially you know what what transformations do you have in your interview system are you",
    "start": "515450",
    "end": "521870"
  },
  {
    "text": "you know summing integers you know counting up total scores you're building histograms or you're computing machine",
    "start": "521870",
    "end": "526970"
  },
  {
    "text": "learning models this is essentially the question that you that folks have traditionally answered using a classic",
    "start": "526970",
    "end": "533520"
  },
  {
    "text": "batch processing what what is it that I'm actually calculating once you figured that out you want to think about",
    "start": "533520",
    "end": "538770"
  },
  {
    "text": "how the event time is going to affect results so does the time each event originally occurred affect how you'd",
    "start": "538770",
    "end": "545310"
  },
  {
    "text": "like to create the results are you a grenading results based on event time fixed windows or sliding windows or",
    "start": "545310",
    "end": "551760"
  },
  {
    "text": "bursts of user activity the third question independent from the event time is when in processing time",
    "start": "551760",
    "end": "558000"
  },
  {
    "text": "does the system actually materialize these results do you want exactly one answer for you know a given window when",
    "start": "558000",
    "end": "565050"
  },
  {
    "text": "you think that all the input for that is complete or do you want to see speculative results for things build up",
    "start": "565050",
    "end": "570180"
  },
  {
    "text": "over time and then what do you do if there are late data in the system do you do you want to just throw that that",
    "start": "570180",
    "end": "575580"
  },
  {
    "text": "those late data away or do you want to incorporate into the results somehow just when as the system is processing",
    "start": "575580",
    "end": "580980"
  },
  {
    "text": "things do you actually create results and then finally how do refinements relate so if I choose to omit multiple",
    "start": "580980",
    "end": "587580"
  },
  {
    "text": "results over time as more data arrives in the system and my results become more complete do I want those different",
    "start": "587580",
    "end": "593880"
  },
  {
    "text": "results to build on each other or do I want them to be independent and distinct so now we're gonna dive into each",
    "start": "593880",
    "end": "600540"
  },
  {
    "text": "question in a little more detail and we're going to do it over the course of an example as we build a pipeline so the",
    "start": "600540",
    "end": "607350"
  },
  {
    "text": "first thing we have to figure out is what we're actually computing so we talked to briefly earlier about these different types of transformations right",
    "start": "607350",
    "end": "614010"
  },
  {
    "text": "we have those element wise transformations we're just looking at a single element in isolation and you can",
    "start": "614010",
    "end": "619470"
  },
  {
    "text": "think of this very much like the map function in Map Reduce very it's very easy to parallelize because of that",
    "start": "619470",
    "end": "626010"
  },
  {
    "text": "independent computation right so you can farm it out over as many machines as you like so then we have aggregations these",
    "start": "626010",
    "end": "632430"
  },
  {
    "text": "are these are counts these are joins things like they need any time that you are combining multiple elements together and this this tends to correspond more",
    "start": "632430",
    "end": "639630"
  },
  {
    "text": "towards the reduced portion of a MapReduce now there's also a set of operations that are really just made up of other",
    "start": "639630",
    "end": "645540"
  },
  {
    "text": "operations so things like joins counts a lot of these are basically just built up",
    "start": "645540",
    "end": "650820"
  },
  {
    "text": "with more primitive operations but it's useful to roll them up into a composite operation and not have to worry about",
    "start": "650820",
    "end": "655980"
  },
  {
    "text": "the details of how it's actually implemented alright so now let's actually see a code snippet for",
    "start": "655980",
    "end": "661360"
  },
  {
    "text": "our gaming example so the code snippets in this talk are pseudo Java they almost compile but we wanted them",
    "start": "661360",
    "end": "667420"
  },
  {
    "start": "663000",
    "end": "663000"
  },
  {
    "text": "to fit on a slide so we sort of allied in some of the details and so what we have here are essentially three three",
    "start": "667420",
    "end": "673780"
  },
  {
    "text": "different statements to begin with we're gonna read a collection of raw log lines from some IO source and store that in so",
    "start": "673780",
    "end": "680020"
  },
  {
    "text": "so in beam a P collection is essentially you can think of a representation of a potentially massive scale collection in",
    "start": "680020",
    "end": "685990"
  },
  {
    "text": "this case it's a bunch of strings so we're we're saying we're gonna read from from some IO source this raw set of log",
    "start": "685990",
    "end": "691540"
  },
  {
    "text": "strings so then what we'd like to do is do an element-wise transformation to take each of those log lines and parse",
    "start": "691540",
    "end": "697630"
  },
  {
    "text": "it into a more structured format so here we're gonna take each log line and create a key value pair extracting the",
    "start": "697630",
    "end": "703510"
  },
  {
    "text": "team and the number of points scored for that team and then once we have these these individual scores that have been",
    "start": "703510",
    "end": "710250"
  },
  {
    "text": "tallied for forgiven teams we're going to take those and we're going to apply this composite transformation some",
    "start": "710250",
    "end": "716080"
  },
  {
    "text": "integers by key and we're gonna sum those up so this is going to give us per team totals of scores all right so now",
    "start": "716080",
    "end": "722260"
  },
  {
    "text": "let's look at some sample data so we can start seeing how this executes again same graph as before so we've got event",
    "start": "722260",
    "end": "729520"
  },
  {
    "text": "time in blue on the x-axis and the green axis showing processing time and that ideal line is shown here where there",
    "start": "729520",
    "end": "737440"
  },
  {
    "text": "everything would arrive if there wasn't a delay in the system and then all these these circles with numbers in them are",
    "start": "737440",
    "end": "742570"
  },
  {
    "text": "essentially individual scores for the team we're gonna we're going to go through and sum these up and you'll",
    "start": "742570",
    "end": "747910"
  },
  {
    "text": "notice looking and looking at a couple of these like the the three here that we've highlighted this one this event",
    "start": "747910",
    "end": "753190"
  },
  {
    "text": "occurred and then was also observed by the pipeline relatively quickly so it's it exists relatively close to that ideal",
    "start": "753190",
    "end": "759820"
  },
  {
    "text": "dashed line the score of nine however is more like seven minutes delayed right it",
    "start": "759820",
    "end": "764830"
  },
  {
    "text": "happened just after 12:01 arrives in the system for processing just after 1208 so",
    "start": "764830",
    "end": "770170"
  },
  {
    "text": "maybe this user was playing our game in an elevator or subway somewhere with a temporary lack of network connectivity",
    "start": "770170",
    "end": "775600"
  },
  {
    "text": "and this graph isn't even large enough to start to show the types of delays that you would get with folks playing",
    "start": "775600",
    "end": "781300"
  },
  {
    "text": "the game on the transatlantic flight or something like that so now we're gonna start animating this to show how processing time alright show",
    "start": "781300",
    "end": "788200"
  },
  {
    "text": "how things execute so what you see here is time as progressing as this thick white line as",
    "start": "788200",
    "end": "794230"
  },
  {
    "text": "the computation is going we're summing up the scores we encounter into that intermediate state that's tracking the",
    "start": "794230",
    "end": "800290"
  },
  {
    "text": "white line and remember this is still just classic batch processing here and so the system continues to build up the",
    "start": "800290",
    "end": "807280"
  },
  {
    "text": "sum until it's seen all the inputs and once it's finally seen all the inputs then it generates the the final output",
    "start": "807280",
    "end": "812560"
  },
  {
    "text": "answer which is when the rectangle turns blue and here you'll notice we're covering all of event time with one",
    "start": "812560",
    "end": "818350"
  },
  {
    "text": "result so we're not really paying attention to when the events happened we're just summing them up so this is",
    "start": "818350",
    "end": "823480"
  },
  {
    "text": "basically equivalent to traditional batch style processing so so let's now",
    "start": "823480",
    "end": "828730"
  },
  {
    "text": "see what happens when we go and start playing with the other questions so the",
    "start": "828730",
    "end": "833770"
  },
  {
    "text": "first thing we're gonna look at is when doing this is the question that talks about event time so windowing is going",
    "start": "833770",
    "end": "839380"
  },
  {
    "text": "to let us create independent results for slice of finite slices of event time and",
    "start": "839380",
    "end": "845350"
  },
  {
    "text": "so on this slide we have three examples of relatively common patterns of one doing so the first one is fixed windows",
    "start": "845350",
    "end": "851740"
  },
  {
    "text": "this is just essentially saying we're gonna take take time we're gonna slice it up into these these fixed divisions",
    "start": "851740",
    "end": "857560"
  },
  {
    "text": "that you know ahead of time they apply across all the keys you know hourly windows daily windows those kinds of",
    "start": "857560",
    "end": "864100"
  },
  {
    "text": "things a sliding window so something like every hour give me the last 24 hours worth of data",
    "start": "864100",
    "end": "870100"
  },
  {
    "text": "so the cool thing about sliding windows is they actually overlap so a single element can conceptually be in multiple",
    "start": "870100",
    "end": "876550"
  },
  {
    "text": "overlapping windows and effect I can contribute to multiple results and then the last example we have here is session",
    "start": "876550",
    "end": "883030"
  },
  {
    "text": "sessions are really interesting for a couple of reasons one one is that they're based off the data themselves so you don't know a priori what what a",
    "start": "883030",
    "end": "889900"
  },
  {
    "text": "session for any given user is going to be and then the other interesting thing is that they're unaligned they're not the same for for each given",
    "start": "889900",
    "end": "896560"
  },
  {
    "text": "user in the system because because I didn't define upfront a session is essentially something that tries to capture a burst of activity or you know",
    "start": "896560",
    "end": "903730"
  },
  {
    "text": "a sequence of events that are somehow related typically typically in time and so when you're talking about like user",
    "start": "903730",
    "end": "910240"
  },
  {
    "text": "interaction over a website naturally different users are going to have different bursts of activity and bursts of interaction and so cheston's sessions",
    "start": "910240",
    "end": "916900"
  },
  {
    "text": "themselves inherently capture information that is that is part of the data itself not just something that you assigned",
    "start": "916900",
    "end": "922360"
  },
  {
    "text": "ahead of time so that makes them both very powerful but also very complicated to build yourself",
    "start": "922360",
    "end": "927640"
  },
  {
    "text": "now using windowing it's very common when you're processing these unbounded infinite datasets and streaming process",
    "start": "927640",
    "end": "933850"
  },
  {
    "text": "and stream processing but it's actually also something we've done for a very long time and traditional batch processing however in that situation we",
    "start": "933850",
    "end": "940960"
  },
  {
    "text": "usually use something like a composite key and don't tend to think of it as when doing but the same kind of computation applies so let's now go back",
    "start": "940960",
    "end": "951400"
  },
  {
    "text": "to the code and we're going to start adding in you know these answers to these other questions so we've we've alighted the first two steps of the code",
    "start": "951400",
    "end": "957280"
  },
  {
    "text": "because those aren't going to change for the rest of the pipeline so we're just looking at that last step where we're doing that integer summation and so what",
    "start": "957280",
    "end": "964180"
  },
  {
    "text": "we have here then highlighted in black is the code that we've added and so all we all we're saying is you know once we",
    "start": "964180",
    "end": "969280"
  },
  {
    "text": "have our input that's been parsed into these these teams score key value pairs we're gonna go ahead and window it in to",
    "start": "969280",
    "end": "975310"
  },
  {
    "text": "fix windows of two minutes and then we'll go ahead and apply our summation transformation so now if you look you'll",
    "start": "975310",
    "end": "982240"
  },
  {
    "text": "see that we're computing for independent results each for two minutes slice of event time as as before this is still",
    "start": "982240",
    "end": "991660"
  },
  {
    "text": "batch processing so we wait until we've seen all the inputs before we provide any any results and that's fine for",
    "start": "991660",
    "end": "999490"
  },
  {
    "text": "bounded datasets but that's obviously not going to be practical for unbounded datasets because we can't wait until the end there is no end so what we want to",
    "start": "999490",
    "end": "1006150"
  },
  {
    "text": "do there is start decreasing the latency for individual results and to do that we have to look at the third question the",
    "start": "1006150",
    "end": "1012270"
  },
  {
    "start": "1012000",
    "end": "1012000"
  },
  {
    "text": "processing time so to answer this question of when in processing time we're gonna want to start using a tool",
    "start": "1012270",
    "end": "1017790"
  },
  {
    "text": "called triggers so triggers are a way for you to dictate when results within",
    "start": "1017790",
    "end": "1023040"
  },
  {
    "text": "your pipeline are going to be emitted the triggers are often going to be relative to the watermark which again is",
    "start": "1023040",
    "end": "1028470"
  },
  {
    "text": "that systems best guess of data completeness and event time progress so",
    "start": "1028470",
    "end": "1034110"
  },
  {
    "start": "1034000",
    "end": "1034000"
  },
  {
    "text": "again let's go back to our code and we're gonna insert a triggering statement here that simply says I want to trigger each of these two minute fix",
    "start": "1034110",
    "end": "1040920"
  },
  {
    "text": "windows when the watermark passes the end of it so that when the watermark pad reaches the end of that window that's",
    "start": "1040920",
    "end": "1046140"
  },
  {
    "text": "basically a state and from the system saying we believe that all the input that you're ever going to see that is relevant to this window has been",
    "start": "1046140",
    "end": "1051540"
  },
  {
    "text": "consumed and so it's it's safe to go ahead and get what we think is a complete answer so now that's look at how things execute",
    "start": "1051540",
    "end": "1058000"
  },
  {
    "text": "with this triggering so the graph here shows a perfect watermark this is what happened would happen if the system knew",
    "start": "1058000",
    "end": "1063610"
  },
  {
    "text": "exactly when the data for a given event time had was complete and so the the",
    "start": "1063610",
    "end": "1069730"
  },
  {
    "text": "watermark is represented here by that green dashed line and you can see it essentially follows the the curve of the",
    "start": "1069730",
    "end": "1074770"
  },
  {
    "text": "the data as they arrive and once the once the dashed line passes the end of a window then the system is free to",
    "start": "1074770",
    "end": "1080710"
  },
  {
    "text": "materialize the results so we first get a result for the first window and then very quickly afterwards the watermark",
    "start": "1080710",
    "end": "1085960"
  },
  {
    "text": "traverses to the right we get outputs for the other two windows and here you can see one of the drawbacks that comes",
    "start": "1085960",
    "end": "1092020"
  },
  {
    "text": "from waiting for late data right that first window we're waiting a long time",
    "start": "1092020",
    "end": "1097210"
  },
  {
    "text": "after that windows passed just for that one late element 9 right so here we only have two elements but you could imagine",
    "start": "1097210",
    "end": "1102490"
  },
  {
    "text": "a situation where the vast bulk of our data has arrived but we're just waiting on one or two that late late elements so",
    "start": "1102490",
    "end": "1109600"
  },
  {
    "text": "we're getting introduced in this unnecessary latency into our system so let's go ahead and now look at the",
    "start": "1109600",
    "end": "1115630"
  },
  {
    "text": "second graph on the right so this is you know same input data but this time we're using a heuristic watermark that's with",
    "start": "1115630",
    "end": "1121840"
  },
  {
    "text": "the solid green line and you can see that being a heuristic it's it's less conservative than the perfect watermark",
    "start": "1121840",
    "end": "1128200"
  },
  {
    "text": "on the left but the the big omission here is that the watermark doesn't take into account that that value of nine",
    "start": "1128200",
    "end": "1134590"
  },
  {
    "text": "that in this case showed up late so here we've got two problems right sometimes we're waiting too long to get any",
    "start": "1134590",
    "end": "1141730"
  },
  {
    "text": "results out of the system and sometimes we're moving on too quickly leaving elements abandoned behind so we can",
    "start": "1141730",
    "end": "1147790"
  },
  {
    "text": "address both of these problems with using more advanced rigor so back to the code again what we what we've done now",
    "start": "1147790",
    "end": "1154030"
  },
  {
    "start": "1151000",
    "end": "1151000"
  },
  {
    "text": "is we've added a two modification structure we basically said well you know we want to we want to solve the the",
    "start": "1154030",
    "end": "1160600"
  },
  {
    "text": "2slow problem we'd like to have early speculative results over time so we had this with early firings clause that just",
    "start": "1160600",
    "end": "1167350"
  },
  {
    "text": "says you know every every minute of processing time if you have anything new to report to me go ahead and give me an update that way I'm going to get",
    "start": "1167350",
    "end": "1172930"
  },
  {
    "text": "continuous updates to my results over time we've also added another clause to ask for late firings so here if we",
    "start": "1172930",
    "end": "1179500"
  },
  {
    "text": "expect our elements to be relatively telemon's to be relatively we might decide that every time we see a late element we'd like to go ahead and",
    "start": "1179500",
    "end": "1185760"
  },
  {
    "text": "update our results so if we go back to the animations now and look at this you can see it looks very similar to what we",
    "start": "1185760",
    "end": "1192000"
  },
  {
    "text": "had before but for for certain windows you know as time progresses we're gonna give early updates anytime that we've",
    "start": "1192000",
    "end": "1199980"
  },
  {
    "text": "had updates or we've had data that it's worth reporting about for more than a minute and those are all annotated as",
    "start": "1199980",
    "end": "1206220"
  },
  {
    "text": "early once the watermark passes the end of the window if there's something new to report about again we'll give an",
    "start": "1206220",
    "end": "1211500"
  },
  {
    "text": "on-time firing and then on the the graph on the right where that nine shows up late you'll see we also give that",
    "start": "1211500",
    "end": "1216720"
  },
  {
    "text": "immediately that that late firing saying you know we you know we we've given you an on-time firing we thought the value",
    "start": "1216720",
    "end": "1222360"
  },
  {
    "text": "of this window was five but then this nine showed up and we need we need to give you a new update we need to tell you it's actually 14 but every time we",
    "start": "1222360",
    "end": "1228659"
  },
  {
    "text": "choose to give you multiple updates we have to decide how they're going to relate so if you look at the last window",
    "start": "1228659",
    "end": "1233909"
  },
  {
    "text": "we have a speculative firing with three and then we get some morph elements in with a total of nine right before the",
    "start": "1233909",
    "end": "1240570"
  },
  {
    "text": "watermark now we can choose how what we'd like that final result to be in this case we've accumulated so we're",
    "start": "1240570",
    "end": "1246510"
  },
  {
    "text": "giving a final score of twelve and including the results from that earlier speculative result but that behavior is",
    "start": "1246510",
    "end": "1252270"
  },
  {
    "text": "actually configurable and that brings us to the final question in the model so this last question how do these",
    "start": "1252270",
    "end": "1257760"
  },
  {
    "start": "1254000",
    "end": "1254000"
  },
  {
    "text": "refinements relate in order to walk you through the the different options we have here we're gonna have a little",
    "start": "1257760",
    "end": "1263610"
  },
  {
    "text": "example here on the table so the idea what this table is we're gonna imagine that we're getting four elements in that",
    "start": "1263610",
    "end": "1270120"
  },
  {
    "text": "we're gonna process so that's what the three the five the one and the two are and as they arrive we're gonna have three trigger firings we're gonna have a",
    "start": "1270120",
    "end": "1275880"
  },
  {
    "text": "single early speculative firing we're gonna have a single on time watermark firing and then we're gonna have a",
    "start": "1275880",
    "end": "1280919"
  },
  {
    "text": "single late firing and we're gonna see what the actual materialized outputs for those firings are with depending on",
    "start": "1280919",
    "end": "1287250"
  },
  {
    "text": "which accumulation mode we've we've chosen so the first mode is discarding mode and here you only include values",
    "start": "1287250",
    "end": "1294149"
  },
  {
    "text": "that have come in since the previous firing so you can see we our first firing includes that three but our",
    "start": "1294149",
    "end": "1299940"
  },
  {
    "text": "second firing only includes the new elements that have come in and then if you look at the bottom then we have we",
    "start": "1299940",
    "end": "1306240"
  },
  {
    "text": "have two rows down there last observed in total observe so the lassiter serve tells you what is the last thing I saw from this this this pipeline and it gave",
    "start": "1306240",
    "end": "1314640"
  },
  {
    "text": "us the value of to which all B's not the right correct total sum so this is this is sort of the shortcoming of discarding mode but if you're sending",
    "start": "1314640",
    "end": "1321150"
  },
  {
    "text": "these outputs into a system downstream that is also itself doing summations then you get exactly what you want that",
    "start": "1321150",
    "end": "1326370"
  },
  {
    "text": "that downstream system is going to take these individual disjoint outputs and sum them up to give you the the right",
    "start": "1326370",
    "end": "1331710"
  },
  {
    "text": "total so you get that 11 in that case now another option is accumulating mode which is what we had on the previous",
    "start": "1331710",
    "end": "1337470"
  },
  {
    "text": "slide so here the total running sum is emitted every time we do a firing but",
    "start": "1337470",
    "end": "1342690"
  },
  {
    "text": "you can see now the last observed value is correct but however the total observed may over count so again",
    "start": "1342690",
    "end": "1349290"
  },
  {
    "text": "depending on your what you're doing with this data downstream that if that system isn't able to handle these multiple",
    "start": "1349290",
    "end": "1355980"
  },
  {
    "text": "firings you could end up with incorrect results the last example we have is",
    "start": "1355980",
    "end": "1362160"
  },
  {
    "text": "accumulating or attracting mode so in this case we're still doing the accumulation that we were doing before",
    "start": "1362160",
    "end": "1367440"
  },
  {
    "text": "but in addition every time we have a refinement of a previous result we not only give you the new updated result but",
    "start": "1367440",
    "end": "1372840"
  },
  {
    "text": "we also give you a retraction saying hey you know previously we said like for the second firing we'll give you the 9 this",
    "start": "1372840",
    "end": "1378630"
  },
  {
    "text": "is the new value but also previously we'd said that the the result was 3 well we're gonna get we're gonna say you take that 3 back here's here's sort of a",
    "start": "1378630",
    "end": "1384990"
  },
  {
    "text": "negative 3 all right so let's go ahead and change our running example to use accumulating and retracting mode so",
    "start": "1384990",
    "end": "1391590"
  },
  {
    "text": "again just a small code tweak and then back to the animation here you can see",
    "start": "1391590",
    "end": "1396660"
  },
  {
    "text": "this looks a lot like what we had before where you know as results evolve over time we give these speculative updates",
    "start": "1396660",
    "end": "1403110"
  },
  {
    "text": "and new values but you'll notice each time we provide a refinement to to a value we also in addition to in addition",
    "start": "1403110",
    "end": "1410160"
  },
  {
    "text": "to giving the new value we give this retraction that tells you you know previously we'd said you know a certain",
    "start": "1410160",
    "end": "1415800"
  },
  {
    "text": "value now we would take that away and this is really important and like if you have a system that's that's performing",
    "start": "1415800",
    "end": "1421290"
  },
  {
    "text": "multiple aggregations in a row you know that that early firing and then the later on time firing or late firing if",
    "start": "1421290",
    "end": "1427920"
  },
  {
    "text": "you wreak your data downstream those might end up going to different places and you can't just assume that you can overwrite the previous value with a new",
    "start": "1427920",
    "end": "1433440"
  },
  {
    "text": "value you may need to actually have a retraction for the first value sent to one place and then this new value sent to another place so being able to have",
    "start": "1433440",
    "end": "1440280"
  },
  {
    "text": "have both of these both of these sides of the coins sent throughout the pipeline or sent downstream that's",
    "start": "1440280",
    "end": "1445440"
  },
  {
    "text": "really important for having correct everywhere so those are the four questions the what where when and how of data processing right and we think",
    "start": "1445440",
    "end": "1452500"
  },
  {
    "text": "that's a really intuitive framework for understanding these complex infinite out of ordered data datasets and how to",
    "start": "1452500",
    "end": "1459760"
  },
  {
    "text": "process them so now we'd like to try to convince you why this framework is so powerful so we have here five five",
    "start": "1459760",
    "end": "1466450"
  },
  {
    "text": "reasons why we think this model is really powerful well just kind of walk through more quick the first one is correctness this isn't",
    "start": "1466450",
    "end": "1473470"
  },
  {
    "text": "something we've historically thought we could get from streaming systems right so this is this is a pretty big deal yeah so you know many of you are maybe",
    "start": "1473470",
    "end": "1480310"
  },
  {
    "text": "aware of sort of the the the history of streaming systems and how there's for a long time then this this baggage of",
    "start": "1480310",
    "end": "1487300"
  },
  {
    "text": "streaming systems not being able to compute correct results you know having having inherent systematic things built",
    "start": "1487300",
    "end": "1494350"
  },
  {
    "text": "into them that make it difficult to actually get correctness but if you build your systems properly and you also",
    "start": "1494350",
    "end": "1499720"
  },
  {
    "start": "1498000",
    "end": "1498000"
  },
  {
    "text": "approach the problems properly that's not actually true so let's look at an example here that's going to highlight this a bit so again remember that the",
    "start": "1499720",
    "end": "1506950"
  },
  {
    "text": "the processing time the time things arrived in the system is really at the whim of the distributed system that",
    "start": "1506950",
    "end": "1512260"
  },
  {
    "text": "you're dealing with so depending on the day depending on the time the wind direction right your results might",
    "start": "1512260",
    "end": "1517930"
  },
  {
    "text": "arrive in a slightly different manner so what we have here you know we've got our ten events and the white the white",
    "start": "1517930",
    "end": "1523240"
  },
  {
    "text": "version is what we've been seeing all along right but you know if things had happened differently or if we're reprocessing data and you know the the",
    "start": "1523240",
    "end": "1529810"
  },
  {
    "text": "stuff everything percolates through the system slightly differently they might have arrived in the order that you see in the purple version the thing that's a",
    "start": "1529810",
    "end": "1536410"
  },
  {
    "text": "really note here is that the expositions for all of these events is the same here the events the events occurred once",
    "start": "1536410",
    "end": "1542290"
  },
  {
    "text": "right they always occurred at the same point in time it's just when we actually observe them with the system may differ",
    "start": "1542290",
    "end": "1547330"
  },
  {
    "text": "depending on how the system sees the events or when we do the processing things like that let's look at the",
    "start": "1547330",
    "end": "1552550"
  },
  {
    "text": "impact of those changes right processing time he's gonna look look like this so",
    "start": "1552550",
    "end": "1559150"
  },
  {
    "start": "1553000",
    "end": "1553000"
  },
  {
    "text": "this these graphs deserve a bit of explanation so what we're doing here is trying to explain to you what it looks like when you use processing time to to",
    "start": "1559150",
    "end": "1566080"
  },
  {
    "text": "to compute windowed aggregates of your of your inputs so in the light gray you",
    "start": "1566080",
    "end": "1572380"
  },
  {
    "text": "can see that the the orderings of when these events actually happened but process when you're doing windowing",
    "start": "1572380",
    "end": "1578140"
  },
  {
    "text": "within processing time what's what's really effectively happening is as an event shows up you're you're throwing",
    "start": "1578140",
    "end": "1583300"
  },
  {
    "text": "away whatever event time that event actually had and saying you know what actually I'm going to just pretend like the time that this event arrived in the",
    "start": "1583300",
    "end": "1589060"
  },
  {
    "text": "system is its event time so that's why all of these events are effectively being shifted over onto that ideal but",
    "start": "1589060",
    "end": "1595180"
  },
  {
    "text": "the results of that you can see is that depending on which order they show up and you actually get different results so here I'm an undefined behavior of",
    "start": "1595180",
    "end": "1603220"
  },
  {
    "text": "your distributed system it's actually leaking through and affecting your results what would really like what",
    "start": "1603220",
    "end": "1608980"
  },
  {
    "text": "would be ideal is for regardless of the ordering of when things arrive we end up with the same answers so let's look at",
    "start": "1608980",
    "end": "1615070"
  },
  {
    "start": "1610000",
    "end": "1610000"
  },
  {
    "text": "what happens if we aggregate based on event time right so here you'll see that we do have different intermediate",
    "start": "1615070",
    "end": "1620200"
  },
  {
    "text": "results those speculative firings beyond time the late firings are giving us different results because the data is",
    "start": "1620200",
    "end": "1625900"
  },
  {
    "text": "arriving in a different order however the final result for each of these windows is identical regardless of the",
    "start": "1625900",
    "end": "1632080"
  },
  {
    "text": "input ordering all right so next up",
    "start": "1632080",
    "end": "1637210"
  },
  {
    "text": "power so what we're really getting at here is that with with the beam model you can do really powerful things with",
    "start": "1637210",
    "end": "1643870"
  },
  {
    "text": "without a whole lot of effort so as you may recall earlier when we were talking about session windows I'd mentioned how",
    "start": "1643870",
    "end": "1650050"
  },
  {
    "text": "those are those are really powerful concepts that allow you to really learn very interesting things about your user",
    "start": "1650050",
    "end": "1656410"
  },
  {
    "text": "data but they're also really complicated to implement and so let's talk quickly about what we mean by sessions so what",
    "start": "1656410",
    "end": "1662260"
  },
  {
    "text": "we have here is we've looked to identify the points that we have for a given user and we're dividing it up into two bursts",
    "start": "1662260",
    "end": "1668380"
  },
  {
    "text": "of user activity so you can see the user was sort of playing for a while here took a two minute break got distracted",
    "start": "1668380",
    "end": "1673960"
  },
  {
    "text": "by something then came back and played our game some more so there are many cases where we'd like to treat these as",
    "start": "1673960",
    "end": "1679120"
  },
  {
    "text": "two different sessions of user activity and process the elements based on the session in which they arrived so doing",
    "start": "1679120",
    "end": "1685030"
  },
  {
    "text": "this yourself in code is actually quite a lot of code word there's there's blog posts out there that kind of explain how to do it on various systems and it's",
    "start": "1685030",
    "end": "1691270"
  },
  {
    "text": "non-trivial but the way that you can do it within the system like beam where we've we've carefully abstract it apart",
    "start": "1691270",
    "end": "1697840"
  },
  {
    "text": "these different questions that you're answering allowed it to swap in and out answers to the questions means that you",
    "start": "1697840",
    "end": "1703260"
  },
  {
    "text": "just walk in here and you throw in this one-line change of saying you know we used to have fixed windows in here well now we're just going to say you know we",
    "start": "1703260",
    "end": "1708690"
  },
  {
    "text": "actually want to window by sessions everything else stays the same and now you can see that as we're going we're",
    "start": "1708690",
    "end": "1714120"
  },
  {
    "text": "really building up these sessions over time as elements come in in close conjunction with each other we're able",
    "start": "1714120",
    "end": "1720390"
  },
  {
    "text": "to treat them as a single session so you can see we're starting to build up sessions we start merging things as we",
    "start": "1720390",
    "end": "1725640"
  },
  {
    "text": "process new elements and what you see here is that late element of nine is really key and getting the correct",
    "start": "1725640",
    "end": "1731520"
  },
  {
    "text": "results here because we only have two sessions but until we see that late element it looks like we have three",
    "start": "1731520",
    "end": "1736800"
  },
  {
    "text": "sessions all right so next composability so we've shown you a bunch of examples",
    "start": "1736800",
    "end": "1744060"
  },
  {
    "text": "where you kind of ask you know ask the four questions and answer them but you actually can ask these questions",
    "start": "1744060",
    "end": "1750120"
  },
  {
    "text": "multiple times and that that just extends further the amount of power you have with the system so what we have",
    "start": "1750120",
    "end": "1755370"
  },
  {
    "start": "1754000",
    "end": "1754000"
  },
  {
    "text": "here is some code that's calculating the length of a user session so we're when",
    "start": "1755370",
    "end": "1760530"
  },
  {
    "text": "doing in two sessions trigger at the watermark and then we're just calculating the length of each section each session we're not trying to sum the",
    "start": "1760530",
    "end": "1766860"
  },
  {
    "text": "scores right now just figure out how long that burst of user activity was so this this length of user activity is a",
    "start": "1766860",
    "end": "1773700"
  },
  {
    "text": "reasonable proxy for user engagement so this is a metric that's commonly used just to see help you know how long our",
    "start": "1773700",
    "end": "1779460"
  },
  {
    "text": "users you know playing our game crushing candy killing works so remember these graphs we've been showing you are perky",
    "start": "1779460",
    "end": "1785910"
  },
  {
    "text": "so Tyler and I as two users playing the game each have our own graph here right because we each have distinct sessions",
    "start": "1785910",
    "end": "1792180"
  },
  {
    "text": "our own patterns of playing this game and this is oh that's fine so so once we",
    "start": "1792180",
    "end": "1798480"
  },
  {
    "start": "1796000",
    "end": "1796000"
  },
  {
    "text": "calculated session links then the next thing we're going to do is go ask the questions again and this time we're",
    "start": "1798480",
    "end": "1804090"
  },
  {
    "text": "gonna say okay well we've computed individual session links for all of our users but in order to really infer",
    "start": "1804090",
    "end": "1809280"
  },
  {
    "text": "something interesting about our system let's go and we're gonna window those those sessions into fixed windows so we",
    "start": "1809280",
    "end": "1815520"
  },
  {
    "text": "can sort of group together sessions within a given time period and then we're over that time period we're gonna compute a global average so we're going",
    "start": "1815520",
    "end": "1822120"
  },
  {
    "text": "to say with you know within this fixed window of an hour or two minutes or whatever you know what is the average session length and this will give us",
    "start": "1822120",
    "end": "1828480"
  },
  {
    "text": "over time a view of user engagement as it evolves as our system changes",
    "start": "1828480",
    "end": "1834030"
  },
  {
    "text": "so you can see here we could use this for something like tracking new rollouts so we just pushed a new version of our",
    "start": "1834030",
    "end": "1839700"
  },
  {
    "text": "game to prod and what's rolling out and our users are starting to engage with it if over time then we see a sudden",
    "start": "1839700",
    "end": "1845460"
  },
  {
    "text": "increase in the length the average length of a user session then that new game that new feature that we rolled out",
    "start": "1845460",
    "end": "1850740"
  },
  {
    "text": "must be really engaging right it's causing users to not get distracted and to focus on our game for a longer time",
    "start": "1850740",
    "end": "1856640"
  },
  {
    "text": "okay so now to move on to the last two so so flexibility the system gives you a",
    "start": "1856640",
    "end": "1862740"
  },
  {
    "text": "lot of a lot of power to to choose exactly what the shape of your output looks like we walked you through so far",
    "start": "1862740",
    "end": "1869390"
  },
  {
    "text": "six different examples so here we've covered everything from your classic batch job processing all the way up",
    "start": "1869390",
    "end": "1875670"
  },
  {
    "text": "through some very complex stream processing and what's more moving on to the last point the modularity we've done",
    "start": "1875670",
    "end": "1882000"
  },
  {
    "text": "this with really quite minimal code changes you know very small changes that are still maintainable and leave your",
    "start": "1882000",
    "end": "1887790"
  },
  {
    "text": "leave your code readable and and allow it to evolve still over time as needs change so here the core algorithm that we're",
    "start": "1887790",
    "end": "1894240"
  },
  {
    "text": "running was just integer summation right it's very simple but this same kind of modularity applies to much more complex",
    "start": "1894240",
    "end": "1900330"
  },
  {
    "text": "algorithms so the bulk of your code the bulk of what you're doing is you're really complex user and logic that will",
    "start": "1900330",
    "end": "1906090"
  },
  {
    "text": "all stay the same as you tweak these other three questions and so there you",
    "start": "1906090",
    "end": "1912390"
  },
  {
    "text": "go five reasons that we think the four questions are awesome all right so we've got these four questions but that's no",
    "start": "1912390",
    "end": "1917820"
  },
  {
    "text": "fun unless we actually have a framework that's concrete right well you can get your hands dirty and start playing with them so that brings us to apache bheem",
    "start": "1917820",
    "end": "1926330"
  },
  {
    "text": "so to begin with we'll look and see where this model and then eventually Apache be came from perspective so so",
    "start": "1926330",
    "end": "1934110"
  },
  {
    "start": "1927000",
    "end": "1927000"
  },
  {
    "text": "for us it all began back with with MapReduce so the original MapReduce paper was published back in 2004 and it",
    "start": "1934110",
    "end": "1940680"
  },
  {
    "text": "fundamentally changed the way that we think about massive scale distributed data processing now inside Google we kept working after",
    "start": "1940680",
    "end": "1947460"
  },
  {
    "text": "MapReduce and it evolved into a number of new systems and as we were building these systems and using them internally",
    "start": "1947460",
    "end": "1952980"
  },
  {
    "text": "we continued to publish papers so we're sort of writing these papers lobbing them over the wall but really just",
    "start": "1952980",
    "end": "1958380"
  },
  {
    "text": "focused on our own internal needs externally then as you're all well aware an open source ecosystem",
    "start": "1958380",
    "end": "1964950"
  },
  {
    "text": "flourished around it initially you know these ideas around MapReduce but then you know adding in all sorts of other",
    "start": "1964950",
    "end": "1970260"
  },
  {
    "text": "innovations and other ideas and really turn into this this massive ecosystem of open source software that you all",
    "start": "1970260",
    "end": "1977190"
  },
  {
    "text": "utilize today so in 2014 Google announced Google Cloud dataflow which",
    "start": "1977190",
    "end": "1982620"
  },
  {
    "text": "was part of our cloud platform and it was a product and a fully managed service based on these years of internal",
    "start": "1982620",
    "end": "1988080"
  },
  {
    "text": "data processing now there were really two parts to cloud dataflow there was the SDK and the programming model",
    "start": "1988080",
    "end": "1993330"
  },
  {
    "text": "originally called the data flow model which is what we've been discussing in this talk that you used for constructing your pipelines and then a no-no Nobbs",
    "start": "1993330",
    "end": "2000170"
  },
  {
    "text": "managed service for executing them but we didn't want to stop there we there",
    "start": "2000170",
    "end": "2005180"
  },
  {
    "text": "are two things really we wanted to share this model more broadly both because it's awesome but also because we realized looking at",
    "start": "2005180",
    "end": "2012230"
  },
  {
    "text": "the success of open source ecosystem that users really benefit from having this larger ecosystem and having portability across systems and the",
    "start": "2012230",
    "end": "2018800"
  },
  {
    "text": "ability to choose amongst these other open systems so Google along with a",
    "start": "2018800",
    "end": "2024080"
  },
  {
    "text": "number of our partners when I hadn't donated this programming model and SDKs to the Apache Software Foundation as the",
    "start": "2024080",
    "end": "2030350"
  },
  {
    "text": "incubating project Apache bean so today Apache beam includes essentially the",
    "start": "2030350",
    "end": "2037010"
  },
  {
    "start": "2033000",
    "end": "2033000"
  },
  {
    "text": "following three things firstly it is this this conceptual beam model that we've been talking about that that the",
    "start": "2037010",
    "end": "2043280"
  },
  {
    "text": "underpinnings of which are these this what where when how a model of thinking about data processing then we have what",
    "start": "2043280",
    "end": "2048679"
  },
  {
    "text": "we hope becomes a very large set of SDKs that let you actually express the concepts in this model so we're starting",
    "start": "2048680",
    "end": "2054169"
  },
  {
    "text": "with the Java SDK which came almost directly from Google's original dataflow SDK modulo a very large package renamed",
    "start": "2054170",
    "end": "2061220"
  },
  {
    "text": "and we're also working on a Python SDK which is on a feature branch in the Apache bean repository and then lastly",
    "start": "2061220",
    "end": "2067639"
  },
  {
    "text": "the part that actually makes this portability thing happened is that we have runners for a number of existing distributed processing backends so we",
    "start": "2067640",
    "end": "2075020"
  },
  {
    "text": "have runners for Apache flink Apache spark and Google Cloud dataflow there's a direct runner for local development",
    "start": "2075020",
    "end": "2080179"
  },
  {
    "text": "and testing and then you know as part of you know community involvement and actually building an ecosystem around",
    "start": "2080180",
    "end": "2085850"
  },
  {
    "text": "this within the greater open source community we now have two other runners in development contributed by folks from",
    "start": "2085850",
    "end": "2092240"
  },
  {
    "text": "these two communities a patch of gear pump and Apache apex with apex is either as both of those are",
    "start": "2092240",
    "end": "2098630"
  },
  {
    "text": "currently in future branches has apex it's gonna merge in any day now apex is eminently moving into the main feature",
    "start": "2098630",
    "end": "2104480"
  },
  {
    "text": "branch or the main branch of beam because our goal with Apache beam is",
    "start": "2104480",
    "end": "2109640"
  },
  {
    "start": "2106000",
    "end": "2106000"
  },
  {
    "text": "really to support three distinct user communities right the first is end-users these are folks who have data they have",
    "start": "2109640",
    "end": "2116210"
  },
  {
    "text": "business needs they'd like to write their logic and process their data and they want to do this in the environment",
    "start": "2116210",
    "end": "2121430"
  },
  {
    "text": "they're familiar with the languages their company is already chosen the distributed processing backends are",
    "start": "2121430",
    "end": "2126859"
  },
  {
    "text": "already using either on-premise or in the cloud the second is is SDK writers",
    "start": "2126859",
    "end": "2132470"
  },
  {
    "text": "so these are the folks that want to make the beam concepts the beam model available to folks that want to develop",
    "start": "2132470",
    "end": "2137960"
  },
  {
    "text": "in new languages so you know you want to you want to have a go SDK or you wanted to have a Visual Basic SDK you know go",
    "start": "2137960",
    "end": "2144440"
  },
  {
    "text": "for it and then finally we have the runner communities and these are the folks who you know are part of and",
    "start": "2144440",
    "end": "2150230"
  },
  {
    "text": "deeply involved in a distributed processing backend we get flanked SPARC apex whatever and what they'd like to do",
    "start": "2150230",
    "end": "2156470"
  },
  {
    "text": "is add support for being pipelines so that they can take the concepts and the programs written against them and go",
    "start": "2156470",
    "end": "2161509"
  },
  {
    "text": "ahead and execute them on their back-end so having a vision and reaching it lowered are two very different things so",
    "start": "2161509",
    "end": "2168140"
  },
  {
    "text": "so be mentored incubation in very early february of this year and then very quickly after we dumped 48,000 lines of",
    "start": "2168140",
    "end": "2175339"
  },
  {
    "text": "initial code into the apache repository so what follow then was a period of",
    "start": "2175339",
    "end": "2180410"
  },
  {
    "text": "chaos deagle deagle off' occation generalization you know a lot of work",
    "start": "2180410",
    "end": "2185420"
  },
  {
    "text": "around making this into a an actual system that can be you know worked on by",
    "start": "2185420",
    "end": "2191059"
  },
  {
    "text": "a large community outside of google in a very general way so we've been getting",
    "start": "2191059",
    "end": "2196160"
  },
  {
    "text": "into the cadence of doing incubating releases we've grown some new committers in the community working on things like like your pump and in Apex you've seen",
    "start": "2196160",
    "end": "2204650"
  },
  {
    "text": "us do a lot of API changes as we try to stabilize the API is between components and get ready but we're really reaching",
    "start": "2204650",
    "end": "2210740"
  },
  {
    "text": "the part getting to the point where you can this promise of portability is true where you can take a single beam",
    "start": "2210740",
    "end": "2216170"
  },
  {
    "text": "pipeline and smoothly run it across any of these runners and then once we start once you get to that point and you have",
    "start": "2216170",
    "end": "2222049"
  },
  {
    "text": "folks actually interested in picking a runner to Toronto then you now have this other problem you",
    "start": "2222049",
    "end": "2227590"
  },
  {
    "text": "have to address of how do they understand what different runners can do so that brings us to the capability",
    "start": "2227590",
    "end": "2233710"
  },
  {
    "start": "2232000",
    "end": "2232000"
  },
  {
    "text": "matrix so what we're trying to do with the beam model is really generalize the semantics of this style of data",
    "start": "2233710",
    "end": "2239770"
  },
  {
    "text": "processing right we're trying to come up with the best most intuitive way for you to represent your computations and so",
    "start": "2239770",
    "end": "2246340"
  },
  {
    "text": "what we've done is we've we've categorized features within the model and these are these are broken out into categories that match the what where",
    "start": "2246340",
    "end": "2252220"
  },
  {
    "text": "when how questions know that the color sort of match the colors we've had throughout here and for each of the",
    "start": "2252220",
    "end": "2257380"
  },
  {
    "text": "features on each of the columns it shows you you know for the given runners that we have what level of support we have",
    "start": "2257380",
    "end": "2262480"
  },
  {
    "text": "and you can see in general most folks within the industry are moving towards supporting everything within the B model",
    "start": "2262480",
    "end": "2267580"
  },
  {
    "text": "one thing worth calling out here is that the SPARC column is is missing a few",
    "start": "2267580",
    "end": "2272590"
  },
  {
    "text": "things there that's primarily because this is built on top of SPARC 1 X so as SPARC 2 L matures which is bringing with",
    "start": "2272590",
    "end": "2279550"
  },
  {
    "text": "it a lot of the stuff within the B model into the SPARC itself we expect this to change and also the folks that are that",
    "start": "2279550",
    "end": "2286210"
  },
  {
    "text": "are working hard on our SPARC runner are also working at building in support for this within beam on top of SPARC 1x so",
    "start": "2286210",
    "end": "2293200"
  },
  {
    "text": "you'll actually be able to do things with beam that you otherwise would have to do manually and require much more",
    "start": "2293200",
    "end": "2298360"
  },
  {
    "text": "effort on top of SPARC our next once they once they manage to flush out all of those columns alright so I think",
    "start": "2298360",
    "end": "2303610"
  },
  {
    "text": "we've got really different takeaways here for different folks if you're just getting started with stream processing and you're still trying to figure out",
    "start": "2303610",
    "end": "2309610"
  },
  {
    "text": "the concepts and how you can think about these style things I really encourage you to go check out Tyler's blog posts",
    "start": "2309610",
    "end": "2315250"
  },
  {
    "text": "on streaming 101 and 102 they're basically a much longer more detailed and slightly snarky ER version",
    "start": "2315250",
    "end": "2321820"
  },
  {
    "text": "of this talk but they're a lot of fun and then if you you know want to learn more about beam come to our website",
    "start": "2321820",
    "end": "2327310"
  },
  {
    "text": "there's lots of information there both if you're interested as a user as an sdk writer or as a runner builder and if",
    "start": "2327310",
    "end": "2332860"
  },
  {
    "text": "you'd like to join the community there's our mailing lists and we would love to have you come and help us build you know the sort of future vision of data",
    "start": "2332860",
    "end": "2339370"
  },
  {
    "text": "processing the way we've seen the community grow over the last few months has just been amazing what happens when you take something it was designed in a",
    "start": "2339370",
    "end": "2345580"
  },
  {
    "text": "very hermetic environment and you generalize it you bring in the power of new ideas so we're really excited about the bean",
    "start": "2345580",
    "end": "2351500"
  },
  {
    "text": "community and its growth all right so we'd love to take questions either a here offline we'll be around",
    "start": "2351500",
    "end": "2356630"
  },
  {
    "text": "all day so thank you very much [Applause]",
    "start": "2356630",
    "end": "2363229"
  }
]