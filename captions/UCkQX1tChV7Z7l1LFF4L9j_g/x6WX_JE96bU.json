[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "hello everyone welcome to my talk I know it's almost time for lunch thank you for staying with me for the next 15",
    "start": "10320",
    "end": "17000"
  },
  {
    "text": "minutes so today I'm going to talk about streaming from iceb data Lake we all want to build data pipelines",
    "start": "17000",
    "end": "24519"
  },
  {
    "text": "that have low latency are cost effective and easy to operate today I'm going to show you how",
    "start": "24519",
    "end": "31279"
  },
  {
    "text": "to achieve those goals by using apach Iceberg as a streaming",
    "start": "31279",
    "end": "36640"
  },
  {
    "text": "Source I first give a quick introduction about apach iceberg and apach Flink now",
    "start": "36640",
    "end": "42399"
  },
  {
    "text": "look at some the motivation why I was thinking about using apach Iceberg as a streaming Source next we'll talk about high level",
    "start": "42399",
    "end": "49039"
  },
  {
    "text": "design of the ice streaming source and some of the benefit it brings in after that we're going to dive deeper",
    "start": "49039",
    "end": "55719"
  },
  {
    "text": "into the fling Watermark alignment and how it works in the ice streaming source and finally we can look at the",
    "start": "55719",
    "end": "61920"
  },
  {
    "text": "evaluation results apach iceberg is a Open Table",
    "start": "61920",
    "end": "68040"
  },
  {
    "text": "format for huge analytic data sets what is the table format you might",
    "start": "68040",
    "end": "74000"
  },
  {
    "text": "be familiar with five format like apach pet it's about organized record within a",
    "start": "74000",
    "end": "79200"
  },
  {
    "text": "data file table format like apach iceberg is about organized data files",
    "start": "79200",
    "end": "85439"
  },
  {
    "text": "within a table a iceberg was designed to address",
    "start": "85439",
    "end": "91200"
  },
  {
    "text": "the correctness and performance issue of the old high table format ice brings numerous benefit first",
    "start": "91200",
    "end": "99320"
  },
  {
    "text": "it provides seriable isolation all table changes are automic readers will never",
    "start": "99320",
    "end": "105880"
  },
  {
    "text": "see partial or uncommitted data IO support fast scan planning with",
    "start": "105880",
    "end": "113079"
  },
  {
    "text": "Advanced filtering using partitions and column level",
    "start": "113079",
    "end": "118360"
  },
  {
    "text": "statistics I suppos support the safe schema and the partition",
    "start": "118360",
    "end": "123680"
  },
  {
    "text": "Evolution it support time travel which enables reproducible query using the",
    "start": "123680",
    "end": "129759"
  },
  {
    "text": "exact same snapshot in the io table recently iceo added a feature like",
    "start": "129759",
    "end": "135480"
  },
  {
    "text": "a brch taging with branching we can Implement write auditor and pop pattern",
    "start": "135480",
    "end": "141760"
  },
  {
    "text": "we can write new data to a staging Branch first run the data validation once the validation passed we can merge",
    "start": "141760",
    "end": "148760"
  },
  {
    "text": "the staging branch to the main chunk so you can manage the data at the code like",
    "start": "148760",
    "end": "153920"
  },
  {
    "text": "you're familiar with the GitHub git workflow so where does apach iceberg fit",
    "start": "153920",
    "end": "159879"
  },
  {
    "text": "in the Big Data",
    "start": "159879",
    "end": "162760"
  },
  {
    "text": "ecosystem at yeah at the bottom level we have the",
    "start": "166319",
    "end": "171920"
  },
  {
    "text": "data files like a pet or or that's stored on store distributed file system",
    "start": "171920",
    "end": "177560"
  },
  {
    "text": "like hdfs or cloud storage like S3 then we have table format like apach",
    "start": "177560",
    "end": "184319"
  },
  {
    "text": "Iceberg Delta l or apach hoodi they provid the SQL table semantics table",
    "start": "184319",
    "end": "190720"
  },
  {
    "text": "format is all about metadata management and computer engines like",
    "start": "190720",
    "end": "197120"
  },
  {
    "text": "apach Fring apach spark tro they integrate with a table format to access",
    "start": "197120",
    "end": "202920"
  },
  {
    "text": "underline data files apach Fring is a distributed posit",
    "start": "202920",
    "end": "211200"
  },
  {
    "text": "engine it is one of the most popular stream posit engines it is highly scalable a single",
    "start": "211200",
    "end": "218040"
  },
  {
    "text": "fling job can process trillions of events per day it can run on thousand of course and maintains teley of State in a",
    "start": "218040",
    "end": "226080"
  },
  {
    "text": "single fling job FL checkpoint provides strong state",
    "start": "226080",
    "end": "231519"
  },
  {
    "text": "state consistency or exact one person semantics if the sync also support",
    "start": "231519",
    "end": "237519"
  },
  {
    "text": "transaction right like Iceberg we can achieve end to end exact ones",
    "start": "237519",
    "end": "242920"
  },
  {
    "text": "also fing sport event time process semantics it use Watermark to reason",
    "start": "242920",
    "end": "249120"
  },
  {
    "text": "about event time progress within the application Watermark also provide a flexible way to trade off between the",
    "start": "249120",
    "end": "255920"
  },
  {
    "text": "completeness and the latency of the results fling provide a layout API from",
    "start": "255920",
    "end": "261880"
  },
  {
    "text": "low level data stream API to high level table API and the",
    "start": "261880",
    "end": "267040"
  },
  {
    "text": "SQL next I'm going to explain why we're thinking about iceberg the streaming",
    "start": "267160",
    "end": "273320"
  },
  {
    "text": "Source here the data pipeline you might be familiar with we have device sent the raw events",
    "start": "273320",
    "end": "280039"
  },
  {
    "text": "to a edge API services with ingest raw data into a Capcom mq this inest delay",
    "start": "280039",
    "end": "286680"
  },
  {
    "text": "is typically fast like seconds or less then with stream stre po engines",
    "start": "286680",
    "end": "292560"
  },
  {
    "text": "like Flink read data from Kafka and write a Raw event to a data Lake like",
    "start": "292560",
    "end": "297759"
  },
  {
    "text": "Iceberg after that is mostly best jobs for",
    "start": "297759",
    "end": "303199"
  },
  {
    "text": "ETL feature engineering and maybe offline mod training those batch jobs are typically",
    "start": "303199",
    "end": "310840"
  },
  {
    "text": "scheduled hourly or daily so the over latency is at least a",
    "start": "310840",
    "end": "317280"
  },
  {
    "text": "few hours if not days we know latency matters for machine learning pipelines the industry is",
    "start": "317280",
    "end": "324840"
  },
  {
    "text": "increasing shifting toward realtime machine learning for online learning and online influence",
    "start": "324840",
    "end": "330479"
  },
  {
    "text": "so how can we reduce the latency of our data pipeline kka is probably the most",
    "start": "330479",
    "end": "336880"
  },
  {
    "text": "popular streaming storage for think it can achieve subsec R",
    "start": "336880",
    "end": "343400"
  },
  {
    "text": "latency so if you care about latency why don't we switch everything to cka and the Flink they are pipeline build that",
    "start": "344039",
    "end": "352680"
  },
  {
    "text": "way Kafka is a fantastic streaming storage I love Kafka it can achieve",
    "start": "352680",
    "end": "358000"
  },
  {
    "text": "subse read latency which is great if you have low really low lat",
    "start": "358000",
    "end": "364199"
  },
  {
    "text": "requirement but as we know system design all about trade offs nobody is good at",
    "start": "364400",
    "end": "369680"
  },
  {
    "text": "everything there are a few things in cop is not well present to work",
    "start": "369680",
    "end": "374919"
  },
  {
    "text": "with if you operate a state for storage system you know it's not",
    "start": "374919",
    "end": "379960"
  },
  {
    "text": "easy syst upgrade can be painful and because we cannot difficult",
    "start": "379960",
    "end": "385360"
  },
  {
    "text": "to autoscale a state for storage system so we have to do careful capacity plan",
    "start": "385360",
    "end": "391080"
  },
  {
    "text": "and we have to learn about the burst backfield workload and how to achieve isolation so",
    "start": "391080",
    "end": "397960"
  },
  {
    "text": "that the busty backfield workload does not affect live",
    "start": "397960",
    "end": "403120"
  },
  {
    "text": "traffic in a FL meet up stown from Netflix Dem is 38 times more expensive",
    "start": "403800",
    "end": "411240"
  },
  {
    "text": "to stop long-term data in Kafka compared to Iceberg K also recently introduced the",
    "start": "411240",
    "end": "417919"
  },
  {
    "text": "te storage within the Kafka that can help R reduce difference but they don't",
    "start": "417919",
    "end": "424039"
  },
  {
    "text": "be completely able to completely bridge the gap so that's why many company adopt",
    "start": "424039",
    "end": "430039"
  },
  {
    "text": "the practice of Steel storage they use c car to store the recent data for the last a few hours or",
    "start": "430039",
    "end": "437360"
  },
  {
    "text": "maybe a few days then they store the long-term data on the iceb data link C",
    "start": "437360",
    "end": "443680"
  },
  {
    "text": "used for serving stream procing workload and Iceberg used to serve the batch workload",
    "start": "443680",
    "end": "451319"
  },
  {
    "text": "for aity reasons kka Brokers are typically placed on different avability",
    "start": "452720",
    "end": "458919"
  },
  {
    "text": "zones that can incur cross AG Network traffic from producer to",
    "start": "458919",
    "end": "464960"
  },
  {
    "text": "broker or broker to broker for replication or broker to",
    "start": "464960",
    "end": "470240"
  },
  {
    "text": "Consumer I did a back envelope calculation using public listing pricing for major car provider my calculation",
    "start": "470240",
    "end": "477800"
  },
  {
    "text": "shows that the cross a net cost can be 10 times more expensive compared to the B cost computer and Stor",
    "start": "477800",
    "end": "486639"
  },
  {
    "text": "combined kafa do provide a Rec aware partition assignment it can avoid the",
    "start": "488800",
    "end": "494599"
  },
  {
    "text": "cross 80 traffic from the broker to Consumer but we still have the producer",
    "start": "494599",
    "end": "501440"
  },
  {
    "text": "to broker and the broker to broker traffic cross a traffic that's still pretty",
    "start": "501440",
    "end": "508000"
  },
  {
    "text": "significant K Source doesn't provide filtering or projection at Bokus side if",
    "start": "508240",
    "end": "514919"
  },
  {
    "text": "we had different consumer jobs they interested in different subset of the data like by filter objection all of",
    "start": "514919",
    "end": "522518"
  },
  {
    "text": "them have to pull down the big pipe and apply the field and projection at the consumer site this obiously",
    "start": "522519",
    "end": "531880"
  },
  {
    "text": "inefficient people have been setting about routing jobs just to apply the field and projection to produce smaller",
    "start": "532519",
    "end": "540000"
  },
  {
    "text": "substream such as all the downstream job they only need to consume the smaller substreams they definitely help with it",
    "start": "540000",
    "end": "546959"
  },
  {
    "text": "improve the efficiency but also equate an actual routing job to maintain and the data",
    "start": "546959",
    "end": "554120"
  },
  {
    "text": "duplications in kka a partition is unbounded stream of Records kka Source",
    "start": "554800",
    "end": "561240"
  },
  {
    "text": "statically assigned the partitions to the workers during the job initialization this assignment remains",
    "start": "561240",
    "end": "568120"
  },
  {
    "text": "static throughout the life cycle of the job unless let topology change for example at a new",
    "start": "568120",
    "end": "574680"
  },
  {
    "text": "worker this leads to a few limitations first in production we",
    "start": "574680",
    "end": "580720"
  },
  {
    "text": "sometime we can see outlier work Noe whose performance is significant lower compared to other",
    "start": "580720",
    "end": "586720"
  },
  {
    "text": "peers in this case because the parti is on bound stream there's no work sharing",
    "start": "586720",
    "end": "592120"
  },
  {
    "text": "of what stealing even if other workers they have extra capacity to spell they cannot pick up the slack",
    "start": "592120",
    "end": "601120"
  },
  {
    "text": "second Kafka s parm is limited by the number of Kaa partitions adding a new",
    "start": "602560",
    "end": "608560"
  },
  {
    "text": "worker won't help improve the risk throughput because not getting a new petition assigned to it over provision",
    "start": "608560",
    "end": "615640"
  },
  {
    "text": "number of C petition can help Elevate the problem here but the too many",
    "start": "615640",
    "end": "621040"
  },
  {
    "text": "partition does have performance penalty on the broker side and also on the consumer",
    "start": "621040",
    "end": "626480"
  },
  {
    "text": "side how do in can help improve the cost efficiency and reduce the operation",
    "start": "627480",
    "end": "633399"
  },
  {
    "text": "burden let's assume initially we have six partitions assign those three workers each worker get two partition",
    "start": "633399",
    "end": "639959"
  },
  {
    "text": "will get a well balanced workload assignment now let's say Auto scaling need to decide to increase scale up the",
    "start": "639959",
    "end": "647639"
  },
  {
    "text": "number of workers because the traffic grow so now I add a new worker now we're",
    "start": "647639",
    "end": "652720"
  },
  {
    "text": "assigning six partition to four workers so we get unbalanced workload",
    "start": "652720",
    "end": "657800"
  },
  {
    "text": "assignment again and over provis number of partion can help but it also have",
    "start": "657800",
    "end": "663639"
  },
  {
    "text": "performance implications those things le start to",
    "start": "663639",
    "end": "669240"
  },
  {
    "text": "think any other automative streaming storage we can leverage next I'm going to show you the",
    "start": "669240",
    "end": "676600"
  },
  {
    "text": "high level design of the iso streaming source and what the benefit it brings",
    "start": "676600",
    "end": "682199"
  },
  {
    "text": "in as I showed up earlier in the data pipeline graph we typically have a stream person like Flink read data from",
    "start": "683279",
    "end": "690680"
  },
  {
    "text": "Kafka write data files and commit a data file to the io table Flink I sync committed data files",
    "start": "690680",
    "end": "699200"
  },
  {
    "text": "after every successful checkpoint in Flink one to 10 minutes committ inter",
    "start": "699200",
    "end": "704519"
  },
  {
    "text": "are pretty common if we commit too frequently like every second we going create we're going to create a lot of",
    "start": "704519",
    "end": "710880"
  },
  {
    "text": "small files and also create a lot of metadata files for iceo to keep track",
    "start": "710880",
    "end": "716639"
  },
  {
    "text": "too many metad files can also stress the ice metadata system but if we commit to infrequently",
    "start": "716639",
    "end": "723560"
  },
  {
    "text": "like every hour we can delay the availability of the data to the downstream",
    "start": "723560",
    "end": "729959"
  },
  {
    "text": "consumers so the question we're asking can a downstream fling job stream data",
    "start": "730800",
    "end": "736279"
  },
  {
    "text": "out of the iceo table as they are committed by the Upstream",
    "start": "736279",
    "end": "741399"
  },
  {
    "text": "job the answer is is let's say that currently the I table have snapshot",
    "start": "741399",
    "end": "747880"
  },
  {
    "text": "n the Upstream job commited through new data files it will create a new snapshot",
    "start": "747880",
    "end": "754240"
  },
  {
    "text": "in IO table called n plus1 so ice provide this incremental",
    "start": "754240",
    "end": "760560"
  },
  {
    "text": "scan API to discover the new data files appended between two",
    "start": "760560",
    "end": "766199"
  },
  {
    "text": "snapshots this is called the incremental scan then the is Source will discover",
    "start": "766199",
    "end": "773000"
  },
  {
    "text": "the new files and the readers will read the records from those data files and admit them to the downstream of",
    "start": "773000",
    "end": "780839"
  },
  {
    "text": "operators for streaming job this cycle continues to a because stream job by",
    "start": "781320",
    "end": "786560"
  },
  {
    "text": "definition never finish it LS forever so this split Discovery cycle continues",
    "start": "786560",
    "end": "794279"
  },
  {
    "text": "Forever on the left I have the code snit for using construct CA Source in fling",
    "start": "794760",
    "end": "801440"
  },
  {
    "text": "basically with set the bootstrap servers and C topics and starting offet strategy",
    "start": "801440",
    "end": "807519"
  },
  {
    "text": "here I use lat offset and the disiz to construct an iceb Source in",
    "start": "807519",
    "end": "814160"
  },
  {
    "text": "Flink we need to provide table loader basic you tell the ice Source how to load the I table from a catalog like",
    "start": "814160",
    "end": "821680"
  },
  {
    "text": "like high Met store or glue if you're familiar with then we also need set a",
    "start": "821680",
    "end": "827040"
  },
  {
    "text": "starting strategy here I choose starting from the latest snapshot in the ice table and also monitor interval how",
    "start": "827040",
    "end": "834720"
  },
  {
    "text": "often we want is source to discover new data files from the",
    "start": "834720",
    "end": "841000"
  },
  {
    "text": "table both car and I S are available in the fting",
    "start": "841240",
    "end": "847600"
  },
  {
    "text": "SQL this Paradigm of streaming from Iceberg works because we observe many",
    "start": "848120",
    "end": "854920"
  },
  {
    "text": "stream use cases they actually fine with minute level latency they're not looking",
    "start": "854920",
    "end": "860320"
  },
  {
    "text": "for subsec level latency so if that with that we can",
    "start": "860320",
    "end": "868040"
  },
  {
    "text": "build a low data pipelines Changed by fling jobs streaming from Iceberg we can",
    "start": "868040",
    "end": "874120"
  },
  {
    "text": "achieve an to latency maybe in minut",
    "start": "874120",
    "end": "878360"
  },
  {
    "text": "level in 2019 fling forward presentation stepan Owen put the data processing",
    "start": "880279",
    "end": "886279"
  },
  {
    "text": "application in the spectrum of latency at most most real time we have",
    "start": "886279",
    "end": "892040"
  },
  {
    "text": "the transaction processing they like the request response model in a microservice",
    "start": "892040",
    "end": "897240"
  },
  {
    "text": "so latency here is typically milliseconds then we have the event",
    "start": "897240",
    "end": "902320"
  },
  {
    "text": "application this problem mostly the C Orion conf from the latency may be the",
    "start": "902320",
    "end": "908000"
  },
  {
    "text": "subsec then with stream streaming analytics data",
    "start": "908000",
    "end": "913720"
  },
  {
    "text": "pipelines continued processing and batch processing batch processing that typic scheduled in",
    "start": "913720",
    "end": "920320"
  },
  {
    "text": "hourly or daily so that latency is at this hours or days",
    "start": "920320",
    "end": "926199"
  },
  {
    "text": "level St that the four category in the middle fit the stream POS and",
    "start": "927000",
    "end": "934399"
  },
  {
    "text": "Paradigm I think this fling is streaming Source feeds well for the data pipelines",
    "start": "934399",
    "end": "939800"
  },
  {
    "text": "and continuous processing categories where the latency expectation are probably in",
    "start": "939800",
    "end": "946759"
  },
  {
    "text": "minutes you may wonder that this seems like a micro batch model it is so how is different with the increment incremental",
    "start": "947440",
    "end": "954240"
  },
  {
    "text": "badge processing I think this also relate to a question was asked earlier after Shar talk so let me first explain",
    "start": "954240",
    "end": "961079"
  },
  {
    "text": "what what do I mean by implemental batch processing it means that with schedu the batch ones at a much shorter interval",
    "start": "961079",
    "end": "967920"
  },
  {
    "text": "than we typically do like every a few minutes or even every minutes because typically bad most commonly is the a",
    "start": "967920",
    "end": "975959"
  },
  {
    "text": "daily and each B one process new files added since the last one so each bat one",
    "start": "975959",
    "end": "982240"
  },
  {
    "text": "will produce a bookmark tell you where to end then you discover the implemental data files",
    "start": "982240",
    "end": "989959"
  },
  {
    "text": "as a schedule interval shortens from AIO daily to every few minutes or every",
    "start": "989959",
    "end": "995440"
  },
  {
    "text": "minutes the line between streaming and the badge actually becomes blly what is the badge what streaming yeah probably",
    "start": "995440",
    "end": "1002079"
  },
  {
    "text": "cannot tell so what are the limitation of the incremented batch procing compared to",
    "start": "1002079",
    "end": "1007480"
  },
  {
    "text": "the streaming execution we're talking about in this talk first as we shorten the schedule",
    "start": "1007480",
    "end": "1014199"
  },
  {
    "text": "interval to every minute or every couple minutes it might be more EXP expensive to tear down the job and bring it up",
    "start": "1014199",
    "end": "1021079"
  },
  {
    "text": "again just a few seconds later we might as well just keep the job running all",
    "start": "1021079",
    "end": "1026160"
  },
  {
    "text": "the time that's essential of streaming job because streaming job runs forever they never",
    "start": "1026160",
    "end": "1033120"
  },
  {
    "text": "terminate second I have heard from batch users they favoring daily scheduling",
    "start": "1034240",
    "end": "1039400"
  },
  {
    "text": "over the a scheduling although AUD scheduling can BRS lower",
    "start": "1039400",
    "end": "1044438"
  },
  {
    "text": "latency the reason is that with all scheduling we can 24 times more BS and",
    "start": "1044439",
    "end": "1050960"
  },
  {
    "text": "higher chance of job failures and the need to backfield that operation burden",
    "start": "1050960",
    "end": "1056000"
  },
  {
    "text": "push them away from the a scheduling to the Daily scheduling now imagine if we're going to",
    "start": "1056000",
    "end": "1061240"
  },
  {
    "text": "schedule the b r every minute or every couple minutes you will get a lot more BS and lot higher chance of job failures",
    "start": "1061240",
    "end": "1068360"
  },
  {
    "text": "and the need to do backfield this operation burden can be too high so most non-trivial data processing",
    "start": "1068360",
    "end": "1077240"
  },
  {
    "text": "invol state for state for badge processing intermate results are discarded after each one and",
    "start": "1077240",
    "end": "1085720"
  },
  {
    "text": "recomputed in the next one with stream processing the intimated",
    "start": "1085720",
    "end": "1092159"
  },
  {
    "text": "result are stored in FL State and they also checkpoint for for tolerance",
    "start": "1092159",
    "end": "1098360"
  },
  {
    "text": "purpose flip stands for Flink Improvement proposal is used to describe",
    "start": "1099840",
    "end": "1105720"
  },
  {
    "text": "like major new features or public API changes flip 27 introduced a new source",
    "start": "1105720",
    "end": "1112159"
  },
  {
    "text": "interface in Flink it was designed to address the limitations for the Old Source function in",
    "start": "1112159",
    "end": "1118320"
  },
  {
    "text": "Flink the key idea in flip 7 Source interface is to separate the work",
    "start": "1118320",
    "end": "1124039"
  },
  {
    "text": "Discovery with actual reading the enumerator runs on the job manager which",
    "start": "1124039",
    "end": "1129200"
  },
  {
    "text": "is the coordinator it is responsible for discover work the par readers runs on",
    "start": "1129200",
    "end": "1135760"
  },
  {
    "text": "the task managers those are the work notes they are responsible for actually",
    "start": "1135760",
    "end": "1140880"
  },
  {
    "text": "reading the data from the storage in flipp 7 a unit of work is",
    "start": "1140880",
    "end": "1147799"
  },
  {
    "text": "defined as a split in the CFA Source a split is a c",
    "start": "1147799",
    "end": "1154159"
  },
  {
    "text": "partition in ISO Source a split is a file a slice of a large file or a group",
    "start": "1154159",
    "end": "1161480"
  },
  {
    "text": "of small files a split can be unbounded like",
    "start": "1161480",
    "end": "1167240"
  },
  {
    "text": "carer Source case all bounded like ice",
    "start": "1167240",
    "end": "1172480"
  },
  {
    "text": "Source in the ice Source The Eliminator will discover spit from the ice table",
    "start": "1173960",
    "end": "1181200"
  },
  {
    "text": "and keep track of the pending spit in internal queue the readers will request a split",
    "start": "1181200",
    "end": "1189080"
  },
  {
    "text": "during the job initialization or when it's done with colon split so the we assign a split to the",
    "start": "1189080",
    "end": "1197679"
  },
  {
    "text": "reader reader requir one spit at time is a pool",
    "start": "1197679",
    "end": "1202720"
  },
  {
    "text": "based Model F 7 unify the batch and the",
    "start": "1202720",
    "end": "1209559"
  },
  {
    "text": "streaming sources the only difference is whether the split Discovery is one time",
    "start": "1209559",
    "end": "1215480"
  },
  {
    "text": "for the batch execution or it's periodic for the streaming",
    "start": "1215480",
    "end": "1221158"
  },
  {
    "text": "execution so what are the benefit of using Iceberg as a streaming Source it",
    "start": "1221400",
    "end": "1226600"
  },
  {
    "text": "tied to some of the paino we talk about earlier first in iceburg can leverage",
    "start": "1226600",
    "end": "1232200"
  },
  {
    "text": "magical storage like S3 it offload operation burden to the car provider",
    "start": "1232200",
    "end": "1237720"
  },
  {
    "text": "like for the system upgrade capacity planning busty workload and",
    "start": "1237720",
    "end": "1243240"
  },
  {
    "text": "isolation and cloud storage is also very scalable and cost",
    "start": "1243240",
    "end": "1249880"
  },
  {
    "text": "effective this simplify the storage architecture to a unified storage both both recent data and the",
    "start": "1250559",
    "end": "1258120"
  },
  {
    "text": "historic data are stored in iceberg and iceberg is used to search both streaming workload and batch",
    "start": "1258120",
    "end": "1265440"
  },
  {
    "text": "workload it unifies the live job and the backfield job source to",
    "start": "1265440",
    "end": "1271760"
  },
  {
    "text": "Iceberg and most clouds Dr storage like S3 don't charge network uh cross eating",
    "start": "1271760",
    "end": "1278440"
  },
  {
    "text": "networ cost within a region so there's no cross eating Network C you seen",
    "start": "1278440",
    "end": "1284919"
  },
  {
    "text": "earlier is Source support the once data p uh data ping by filter and projection",
    "start": "1285000",
    "end": "1292360"
  },
  {
    "text": "you can provide a filter expression so the ice scan planning will effectively P",
    "start": "1292360",
    "end": "1298039"
  },
  {
    "text": "out data files not matching the FI expression and with projection the high",
    "start": "1298039",
    "end": "1304960"
  },
  {
    "text": "source we only decate the columns it is interested in because we use column",
    "start": "1304960",
    "end": "1310440"
  },
  {
    "text": "level uh column oriented uh F form",
    "start": "1310440",
    "end": "1315559"
  },
  {
    "text": "paret in the I Source the speed assignment is",
    "start": "1317039",
    "end": "1322159"
  },
  {
    "text": "dynamic it's pool based Tri triggered by the reader if let outl work node it allows",
    "start": "1322159",
    "end": "1330039"
  },
  {
    "text": "other worker to pick up a slack because the split is bonded there's opportunity",
    "start": "1330039",
    "end": "1336200"
  },
  {
    "text": "for the other workers to steal the work from the outli work node so the other",
    "start": "1336200",
    "end": "1342640"
  },
  {
    "text": "workers they can just pick up more data files and process them",
    "start": "1342640",
    "end": "1349840"
  },
  {
    "text": "in with ice s we can typically get a lot more file segments and the number of C competitions this can bring some uh oper",
    "start": "1349840",
    "end": "1357559"
  },
  {
    "text": "operational benefit for example with a higher number",
    "start": "1357559",
    "end": "1362760"
  },
  {
    "text": "of file segment we can support higher parm which can be useful in some cases",
    "start": "1362760",
    "end": "1368919"
  },
  {
    "text": "like Back Field you may want to be able to backfield much faster than the live job so you want a higher paradism with",
    "start": "1368919",
    "end": "1375000"
  },
  {
    "text": "more file segment you can do that with a more file segment is also more",
    "start": "1375000",
    "end": "1380880"
  },
  {
    "text": "Autos scaling friendly it's easier to assign the number of uh the files to the readers in more balanced",
    "start": "1380880",
    "end": "1389278"
  },
  {
    "text": "fashion we have merged this f27 Flink ice Source into the apach iso",
    "start": "1389960",
    "end": "1396960"
  },
  {
    "text": "project it's fully merged but the only thing I want to call out is that for the streaming rate we right now we only",
    "start": "1396960",
    "end": "1403400"
  },
  {
    "text": "support the append only records it does not support the CDC rate on updat and",
    "start": "1403400",
    "end": "1408840"
  },
  {
    "text": "the deletes that's that it need to be adjust in the",
    "start": "1408840",
    "end": "1414279"
  },
  {
    "text": "future next I'm going to dive deeper into the fling Watermark alignment and how it works in the fling in the fling I",
    "start": "1415159",
    "end": "1424519"
  },
  {
    "text": "Source I want to give a quick recap what is a watermark in Flink event based",
    "start": "1425240",
    "end": "1431679"
  },
  {
    "text": "application record can come out of order it's necessary to to buffer and",
    "start": "1431679",
    "end": "1438320"
  },
  {
    "text": "wait to tolerate the later R data but we cannot wait forever",
    "start": "1438320",
    "end": "1444120"
  },
  {
    "text": "otherwise the the uh the result will be delayed for a long time and the fling State can grow unboundedly so at a",
    "start": "1444120",
    "end": "1451200"
  },
  {
    "text": "certain time we have to stop waiting that exactly Watermark is doing it basically tells us all the data before",
    "start": "1451200",
    "end": "1458360"
  },
  {
    "text": "the watermark have already arrived so it's okay to emit result but occasionally The Watermark",
    "start": "1458360",
    "end": "1465440"
  },
  {
    "text": "could be holistic could be wrong hence the watermarket is kind of tradeoff between the complet L and the latency of",
    "start": "1465440",
    "end": "1472720"
  },
  {
    "text": "the results so so we talk about Watermark",
    "start": "1472720",
    "end": "1479640"
  },
  {
    "text": "then what is Watermark alignment why is it needed let's look at the use case of stateful join for example let's say we",
    "start": "1479640",
    "end": "1486240"
  },
  {
    "text": "joined add impression with ADD click for state of join we typically",
    "start": "1486240",
    "end": "1491399"
  },
  {
    "text": "need to Define drawing window which is typically determined by the how late",
    "start": "1491399",
    "end": "1496799"
  },
  {
    "text": "data can come and let's in this case let's assume we have six our join",
    "start": "1496799",
    "end": "1502880"
  },
  {
    "text": "window let's assume everything works well in a stady state with the live",
    "start": "1502880",
    "end": "1509639"
  },
  {
    "text": "traffic let's say we need to replace the two CA source to 24 hours ago maybe",
    "start": "1510600",
    "end": "1515760"
  },
  {
    "text": "that's out digit or there some like a data uh data",
    "start": "1515760",
    "end": "1521279"
  },
  {
    "text": "issue because those two data streams they may have different volume data let's assume that as impression stream",
    "start": "1521279",
    "end": "1527840"
  },
  {
    "text": "4X amount of data compared to click stream because the different data volume",
    "start": "1527840",
    "end": "1533240"
  },
  {
    "text": "those two c Source they will perceive that at different pace in fling the pace",
    "start": "1533240",
    "end": "1538799"
  },
  {
    "text": "is measured as a",
    "start": "1538799",
    "end": "1541880"
  },
  {
    "text": "watermark if we zoom into the fling job a little bit it can be simplified as a four operator it have two c Source",
    "start": "1544360",
    "end": "1551559"
  },
  {
    "text": "operator each read from one cap stream then we have the co-process function",
    "start": "1551559",
    "end": "1557200"
  },
  {
    "text": "which does the actual state for joint this is the state for operator then a sync operator which",
    "start": "1557200",
    "end": "1562679"
  },
  {
    "text": "write the joint output to the sync storage because the click stream is much",
    "start": "1562679",
    "end": "1569480"
  },
  {
    "text": "smaller right so let's assume the source to was able to catch up much faster it able to catch up the live traffic which",
    "start": "1569480",
    "end": "1576640"
  },
  {
    "text": "emit Watermark as now and Source One was catching off from",
    "start": "1576640",
    "end": "1581799"
  },
  {
    "text": "much bigger stream so it will catch up much slower let's assume it will IM waterm Mark now minus 18",
    "start": "1581799",
    "end": "1589559"
  },
  {
    "text": "hour because the fck water mark is calculate the Minal value of all the inputs so the state for co-process",
    "start": "1589559",
    "end": "1596799"
  },
  {
    "text": "function will have a water mark at now minus 18 hour this is the problem because the",
    "start": "1596799",
    "end": "1605200"
  },
  {
    "text": "watermark was at one slower in the state state of operator now it need to buff for 24 hours of the click stream data",
    "start": "1605200",
    "end": "1613240"
  },
  {
    "text": "was a 6 hour during stady State this excessive data buffering can",
    "start": "1613240",
    "end": "1619880"
  },
  {
    "text": "lead to Performance issue and stability issue for the state foring job this actually one of the biggest pinpoint we",
    "start": "1619880",
    "end": "1626760"
  },
  {
    "text": "experience when running large state for job in Flink that's why Flink introduce this",
    "start": "1626760",
    "end": "1634039"
  },
  {
    "text": "Watermark alignment uh strategy in the source it's basically try to ensure that",
    "start": "1634039",
    "end": "1640600"
  },
  {
    "text": "both sources they progress at similar Pace The Source tool will stop reading",
    "start": "1640600",
    "end": "1647120"
  },
  {
    "text": "from car when local Watermark is too far ahead compared to the global Watermark this is the alignment part",
    "start": "1647120",
    "end": "1655000"
  },
  {
    "text": "this award except data ping in the downstream State operator for example in",
    "start": "1655000",
    "end": "1660399"
  },
  {
    "text": "this case if we allow one hour of Maximum water mark drift then the",
    "start": "1660399",
    "end": "1666039"
  },
  {
    "text": "co-process operator function only need to buffer seven hours of Click data which is very close to the steady",
    "start": "1666039",
    "end": "1673799"
  },
  {
    "text": "state this is how fling Watermark align Works internally let's assume we have",
    "start": "1674840",
    "end": "1680000"
  },
  {
    "text": "three readers each reader from one CA petition the ca readers extract The",
    "start": "1680000",
    "end": "1686559"
  },
  {
    "text": "Watermark from a Time Field in the record it consume from",
    "start": "1686559",
    "end": "1691600"
  },
  {
    "text": "kfun and they periodically send a local Watermark to the enumerator for Global",
    "start": "1691600",
    "end": "1699240"
  },
  {
    "text": "aggregations the enumerator calcat the global wmark as the minimum value of all",
    "start": "1700000",
    "end": "1705840"
  },
  {
    "text": "the reported local water mark which should be 10 10 in this case The Eliminator then broadcast a go Mark to",
    "start": "1705840",
    "end": "1713000"
  },
  {
    "text": "all the readers then the readers need to check the difference between the local",
    "start": "1713000",
    "end": "1718360"
  },
  {
    "text": "Watermark and the global Watermark to determine if Sor is needed let's assume",
    "start": "1718360",
    "end": "1723960"
  },
  {
    "text": "the max allow Watermark drift is 30 minutes in this case read zero will stop",
    "start": "1723960",
    "end": "1729120"
  },
  {
    "text": "reading because it's local Watermark too far ahead this is where the alignment part come in we need to align the",
    "start": "1729120",
    "end": "1736080"
  },
  {
    "text": "progress of all the source read",
    "start": "1736080",
    "end": "1739398"
  },
  {
    "text": "before I talk about how the waterb alignment work in the iceb streaming Source let's recap some of the",
    "start": "1741120",
    "end": "1747480"
  },
  {
    "text": "difference between the KFA source and iceo Source in CA Source The Spits are",
    "start": "1747480",
    "end": "1753360"
  },
  {
    "text": "unbounded partitions in iceo Sword the split are bounded data files or file",
    "start": "1753360",
    "end": "1760000"
  },
  {
    "text": "segments in cap Source the spit assignment is static during the job ination in the isos source the spit",
    "start": "1760000",
    "end": "1767000"
  },
  {
    "text": "assignment is dynamic is pool based from the",
    "start": "1767000",
    "end": "1772000"
  },
  {
    "text": "readers in kka workers are ordered within a partition first in first out in",
    "start": "1772159",
    "end": "1778519"
  },
  {
    "text": "is Source record may not be sorted by the time stand field within a data",
    "start": "1778519",
    "end": "1785480"
  },
  {
    "text": "file in C Source case only readers can extract Watermark information from the",
    "start": "1785480",
    "end": "1791080"
  },
  {
    "text": "recers that read from KFAN that's the only way in the iceberg Source because",
    "start": "1791080",
    "end": "1796679"
  },
  {
    "text": "iceberg keep track of a column Lev statistics like mean Max values in the metadata files the enumerator actually",
    "start": "1796679",
    "end": "1804360"
  },
  {
    "text": "can extract water mark information from the minmax St column level statistic in the metadata file without actually",
    "start": "1804360",
    "end": "1811200"
  },
  {
    "text": "downloading or reading the data files the key idea is to the The Ice",
    "start": "1811200",
    "end": "1819360"
  },
  {
    "text": "Source ulator need to assign the splits ordered by the",
    "start": "1819360",
    "end": "1825000"
  },
  {
    "text": "time let's assume the discover the nine files and we order those nine files by",
    "start": "1825200",
    "end": "1831720"
  },
  {
    "text": "the minimum time stamp value from the column level statistic here the notation",
    "start": "1831720",
    "end": "1836880"
  },
  {
    "text": "means that F1 contains time stamp with records from 9:00 to",
    "start": "1836880",
    "end": "1843840"
  },
  {
    "text": "903 let's assume The Eliminator assigned the first three files to the first three",
    "start": "1844200",
    "end": "1849320"
  },
  {
    "text": "readers and the readers extract the local order Mark by the minimum time stamp value from the statea file so read",
    "start": "1849320",
    "end": "1856840"
  },
  {
    "text": "zero which calc local Watermark at 9:00 because that's the minimum time St value",
    "start": "1856840",
    "end": "1862200"
  },
  {
    "text": "we have from F1 the global wmark is calculated at the",
    "start": "1862200",
    "end": "1868480"
  },
  {
    "text": "minimum of the all the local Watermark so the global Watermark here will be n",
    "start": "1868480",
    "end": "1875039"
  },
  {
    "text": "CL the readers will check the difference between the local Mark and the gold Mark",
    "start": "1875039",
    "end": "1881120"
  },
  {
    "text": "is to decide if s is needed in this case all the three reers are okay to proceed",
    "start": "1881120",
    "end": "1886399"
  },
  {
    "text": "because they with in the threshold of 10 minutes with mft let's assume sometime later reader",
    "start": "1886399",
    "end": "1893600"
  },
  {
    "text": "two finishes data file so it's requesting a new file and got the F4",
    "start": "1893600",
    "end": "1900240"
  },
  {
    "text": "back now read two we the one is local order Mark to 9913 because that's",
    "start": "1900240",
    "end": "1906480"
  },
  {
    "text": "minimum time 10 value from F4 but in this case reader two need to",
    "start": "1906480",
    "end": "1913880"
  },
  {
    "text": "stop reading because it's local Water Market is too far ahead because Max allow drift is 10 minutes so that's the",
    "start": "1913880",
    "end": "1921200"
  },
  {
    "text": "startling alignment part let's assume sometime later read",
    "start": "1921200",
    "end": "1927120"
  },
  {
    "text": "zero also finishes file and got a new file F5 and read zero at once is a local",
    "start": "1927120",
    "end": "1933919"
  },
  {
    "text": "Watermark to 916 because that's a minimum time St value from F5 Now read zero also need to stop",
    "start": "1933919",
    "end": "1941399"
  },
  {
    "text": "reading because it local Water Market too far ahead after some publication delay maybe",
    "start": "1941399",
    "end": "1948120"
  },
  {
    "text": "a couple second or two and the gold one Mark will add once to 94 because that's",
    "start": "1948120",
    "end": "1953760"
  },
  {
    "text": "the minimum minimal time stamp value of the all the local water",
    "start": "1953760",
    "end": "1959799"
  },
  {
    "text": "mark sorry in this case the reader to can resume reading because it's local",
    "start": "1961799",
    "end": "1967880"
  },
  {
    "text": "Watermark now is within the threshold when compared to the lat is go the board Mark so now the video two will resume",
    "start": "1967880",
    "end": "1975279"
  },
  {
    "text": "the normal reading so the max out ordinance we can achieve",
    "start": "1975279",
    "end": "1982919"
  },
  {
    "text": "in the ice s Watermark alignment is equal to the max allowed Watermark drift",
    "start": "1982919",
    "end": "1989240"
  },
  {
    "text": "plus the max time stamp time stamp range in any data files let's assume in this",
    "start": "1989240",
    "end": "1995880"
  },
  {
    "text": "case the max time stamp range is from F6 is 8 minutes and if the max water mark",
    "start": "1995880",
    "end": "2002320"
  },
  {
    "text": "drift allowed is a 10 minutes then we can achieve max out allness is 18",
    "start": "2002320",
    "end": "2008360"
  },
  {
    "text": "minutes so why is important to keep the max out ordin small remember the earlier",
    "start": "2008360",
    "end": "2013600"
  },
  {
    "text": "gra I showed you when we keep the max out Orly small we can avoid excessive data buffering in the fling state for",
    "start": "2013600",
    "end": "2022639"
  },
  {
    "text": "operator my colleague Peter W is working on the op contribution for this Watermark alignment in the fling I",
    "start": "2024760",
    "end": "2031399"
  },
  {
    "text": "Source this is not fully is not fully merged yet finally we're going to look at some",
    "start": "2031399",
    "end": "2038200"
  },
  {
    "text": "of the evalution results this is a test pipeline setup we",
    "start": "2038200",
    "end": "2043960"
  },
  {
    "text": "have a CA Source job and read data from Kafka and write the same data into ier",
    "start": "2043960",
    "end": "2049480"
  },
  {
    "text": "table then we have IO Source job streaming data from the input iook table",
    "start": "2049480",
    "end": "2055040"
  },
  {
    "text": "and write the same data in output iook table it's like iceb mdle",
    "start": "2055040",
    "end": "2061078"
  },
  {
    "text": "maker the traffic volume is about 4,000 message per second and each message is about 1",
    "start": "2061760",
    "end": "2068679"
  },
  {
    "text": "kiloby the task manag container has one CPU and 4 GB memory so it's a small test",
    "start": "2068679",
    "end": "2075679"
  },
  {
    "text": "setup what are we evaluating first we want to look at the Rel latency of the ice streaming Source",
    "start": "2075679",
    "end": "2082599"
  },
  {
    "text": "because for streaming job Laten matters we want to understand that second we want to understand how",
    "start": "2082599",
    "end": "2088919"
  },
  {
    "text": "the Upstream commit interval affect the downstream consumption The Bu",
    "start": "2088919",
    "end": "2094878"
  },
  {
    "text": "consumption the downstream third we want to compare the CPU",
    "start": "2094879",
    "end": "2099920"
  },
  {
    "text": "utilization between the Kafka source and iceo Source we measure the latency from the",
    "start": "2099920",
    "end": "2108320"
  },
  {
    "text": "Kafka broker to the is Source re uh uh processing time if we zoom into a little bit there",
    "start": "2108320",
    "end": "2115440"
  },
  {
    "text": "actually three segment first we have the Kafka Source CFA Rel latency which",
    "start": "2115440",
    "end": "2121359"
  },
  {
    "text": "typically very very fast like subsec then we have the commit interval how often Upstream job commit the data",
    "start": "2121359",
    "end": "2128400"
  },
  {
    "text": "file to the iceo table third segment is the PO interval how often the io Source P the io table",
    "start": "2128400",
    "end": "2136480"
  },
  {
    "text": "to discover the new data files so the latency is most determined about by the last two segments the commit interval",
    "start": "2136480",
    "end": "2143800"
  },
  {
    "text": "and the PO interval here we're going to look at some of the latency histogram with 10",
    "start": "2143800",
    "end": "2151160"
  },
  {
    "text": "seconds commit interal and 5 Seconds P interval the X accesses is time Y aises",
    "start": "2151160",
    "end": "2158640"
  },
  {
    "text": "is latency in millisecond we can say the max latency is less than 40",
    "start": "2158640",
    "end": "2165680"
  },
  {
    "text": "seconds and the median latency is fluctuated around 10 second which corresponds to the 10 seconds upam",
    "start": "2165839",
    "end": "2173040"
  },
  {
    "text": "committing the world which makes sense right because we commit a batch of files and downam read a batch of",
    "start": "2173040",
    "end": "2180559"
  },
  {
    "text": "files because the I data commit is transaction that can lead to stop and go",
    "start": "2183800",
    "end": "2189040"
  },
  {
    "text": "consumption pattern the top graph shows the the CPU usage for the C car Source",
    "start": "2189040",
    "end": "2195200"
  },
  {
    "text": "job so the xais time y AIS is the number of Calles it used remember we're using",
    "start": "2195200",
    "end": "2201359"
  },
  {
    "text": "one core uh container from this gra we can see the kafa S job is always busy it's always",
    "start": "2201359",
    "end": "2209040"
  },
  {
    "text": "pulling data from Kaa processing them but if you look at the i s jop here",
    "start": "2209040",
    "end": "2215359"
  },
  {
    "text": "we're using five minutes commit interval and 30 second po interval we can see the",
    "start": "2215359",
    "end": "2220480"
  },
  {
    "text": "is Source job was busy process a batch of files for maybe two to three minutes",
    "start": "2220480",
    "end": "2225520"
  },
  {
    "text": "after done with the batch is what idle for the next two minutes maybe until the",
    "start": "2225520",
    "end": "2230760"
  },
  {
    "text": "new batch comes in and it will Bey again then idle for next two minutes this kind",
    "start": "2230760",
    "end": "2236359"
  },
  {
    "text": "of stop and go consumption pattern is what we expect out of the global transaction writing",
    "start": "2236359",
    "end": "2242119"
  },
  {
    "text": "iBook but I want to show you as we shorten the Upstream commit interval and",
    "start": "2242119",
    "end": "2247599"
  },
  {
    "text": "IO Source po interval the CPU usage can become much smoother for the iceo source job the top graph is from the previous",
    "start": "2247599",
    "end": "2255079"
  },
  {
    "text": "slides is for 5 minutes com interval and 30 second po interval the middle graph is for 60",
    "start": "2255079",
    "end": "2262960"
  },
  {
    "text": "seconds committing the wall and 10 second point the you can see we don't have those long idle gap of 2 minutes or",
    "start": "2262960",
    "end": "2271000"
  },
  {
    "text": "three minutes anymore although it is still much choppier compared to C",
    "start": "2271000",
    "end": "2277280"
  },
  {
    "text": "so still a burer compared to cover Source but as we further shorten the",
    "start": "2277280",
    "end": "2283400"
  },
  {
    "text": "schedule interval to 10 seconds com interval and 5c po interval we prob",
    "start": "2283400",
    "end": "2289720"
  },
  {
    "text": "cannot see much different in term of CP usage between ice jop and C jop the",
    "start": "2289720",
    "end": "2295800"
  },
  {
    "text": "pattern maybe looks very similar finally we're going to compare",
    "start": "2295800",
    "end": "2302280"
  },
  {
    "text": "the CPU usage between the CFA Source job and the is Source job we can say the only difference between",
    "start": "2302280",
    "end": "2309000"
  },
  {
    "text": "those two job is in the streaming Source One is the CF car the other one is the iceberg AB El actually pretty much the",
    "start": "2309000",
    "end": "2317400"
  },
  {
    "text": "same here the CPU usage comparison between the c s job and the ice we apply",
    "start": "2317400",
    "end": "2324480"
  },
  {
    "text": "smooth function so that the CP us does not look so choppy so it's easier to",
    "start": "2324480",
    "end": "2329599"
  },
  {
    "text": "read the over train you can say the car job have CPU tion at 60% isce job has CP",
    "start": "2329599",
    "end": "2337440"
  },
  {
    "text": "36% and The Benchmark is is tricky I think the main takeway is that I sour",
    "start": "2337440",
    "end": "2342800"
  },
  {
    "text": "job is as performance as CER Source streaming Source if not",
    "start": "2342800",
    "end": "2348680"
  },
  {
    "text": "better I want to close my presentation with the next slides let take away this",
    "start": "2349800",
    "end": "2356680"
  },
  {
    "text": "one next one we can build the low latency data pipelines chained by fling",
    "start": "2356680",
    "end": "2362119"
  },
  {
    "text": "job streaming from Iceberg we can achieve overall latency in minutes",
    "start": "2362119",
    "end": "2367920"
  },
  {
    "text": "level they're cost effective and they also operation",
    "start": "2367920",
    "end": "2372960"
  },
  {
    "text": "friendly thank",
    "start": "2372960",
    "end": "2376200"
  },
  {
    "text": "you I the question was for people coming from like batch background is it like a",
    "start": "2382560",
    "end": "2388160"
  },
  {
    "text": "difference for like a different computer engine like fling spark to they work",
    "start": "2388160",
    "end": "2393280"
  },
  {
    "text": "with better with one data Le solution like I iberg or hood or Del",
    "start": "2393280",
    "end": "2398800"
  },
  {
    "text": "Lake um I think initially for example the iceberg were coming from the batch",
    "start": "2398800",
    "end": "2405200"
  },
  {
    "text": "analytic data set background and the hoodie was coming from the stream use cases so that's why I think a who",
    "start": "2405200",
    "end": "2412720"
  },
  {
    "text": "probably have better support for the CDC read all those but last couple years we can see some convergency on those like",
    "start": "2412720",
    "end": "2419599"
  },
  {
    "text": "data L solutions they also trying to learn from each other trying to kind of address the weakness is the OWN system",
    "start": "2419599",
    "end": "2425920"
  },
  {
    "text": "so I think all the common de leag solution like Iceberg dat leag who now they all",
    "start": "2425920",
    "end": "2434119"
  },
  {
    "text": "pretty very good integration with all the popular engines like fling spark tro",
    "start": "2434119",
    "end": "2439560"
  },
  {
    "text": "so yeah I don't think that's I would say there a significance one is working better with the",
    "start": "2439560",
    "end": "2445400"
  },
  {
    "text": "other yeah so sometime depend on what kind of Technology of you you are",
    "start": "2445400",
    "end": "2451000"
  },
  {
    "text": "familiar with or you're more comfortable with",
    "start": "2451000",
    "end": "2458319"
  },
  {
    "text": "question [Music]",
    "start": "2468920",
    "end": "2473339"
  },
  {
    "text": "also how are based on Ice we still",
    "start": "2476240",
    "end": "2483800"
  },
  {
    "text": "get the same assurance you will",
    "start": "2483800",
    "end": "2488800"
  },
  {
    "text": "get that's great question I'll repeat the question the question is CA support key topic where the same the record for",
    "start": "2493920",
    "end": "2501240"
  },
  {
    "text": "same key go the same C partition so with Iber Source how how is that handled that's great question so ice do support",
    "start": "2501240",
    "end": "2508760"
  },
  {
    "text": "a partition scheme called a pareting it's kind of like a hasion you can you",
    "start": "2508760",
    "end": "2514079"
  },
  {
    "text": "can send the all the records from with the same bucket ID which is basically",
    "start": "2514079",
    "end": "2519240"
  },
  {
    "text": "hashing function what by number buckets to the same bucket partition bucket so",
    "start": "2519240",
    "end": "2524720"
  },
  {
    "text": "that way you can group all the records for the same key go to same bucket then when you read you can also assign the uh",
    "start": "2524720",
    "end": "2531400"
  },
  {
    "text": "you can assign the file for the same bucket to a particular",
    "start": "2531400",
    "end": "2536280"
  },
  {
    "text": "reader",
    "start": "2545160",
    "end": "2548160"
  },
  {
    "text": "yeah so the question is in the uh CPU Benchmark The carer Source 60% C usage",
    "start": "2558160",
    "end": "2564720"
  },
  {
    "text": "and I Source have 36% usage is that why is such a difference do we look into",
    "start": "2564720",
    "end": "2570040"
  },
  {
    "text": "that so I think I I did not look into",
    "start": "2570040",
    "end": "2575359"
  },
  {
    "text": "the what exactly C different but I think couple things might Factor because in IP",
    "start": "2575359",
    "end": "2580599"
  },
  {
    "text": "Source the if the pulling data in much bigger chunk because data come for three",
    "start": "2580599",
    "end": "2586520"
  },
  {
    "text": "we get a data file and deip so it's a much bigger batch compared to the Capa Source R that's probably one of the one",
    "start": "2586520",
    "end": "2594079"
  },
  {
    "text": "of the reasons",
    "start": "2594079",
    "end": "2598000"
  },
  {
    "text": "yeah but again I don't want to read it too much he I SP is more efficient than the Catra as it is I want to say point",
    "start": "2602200",
    "end": "2609680"
  },
  {
    "text": "I'm trying to make it as performant as it is um when you showed the graphs about",
    "start": "2609680",
    "end": "2616480"
  },
  {
    "text": "the spiky loads versus the smoother loads when you um went to the the smaller intervals do you run a greater",
    "start": "2616480",
    "end": "2623400"
  },
  {
    "text": "risk of backup when you're short shortening the intervals sorry can you repeat the",
    "start": "2623400",
    "end": "2629040"
  },
  {
    "text": "question again um in the graphs you had the smoother graph versus the very very spiky graphs um and it got smoother as",
    "start": "2629040",
    "end": "2635440"
  },
  {
    "text": "you short you're saying have it go even lower let's say 1 second committable I'm just",
    "start": "2635440",
    "end": "2640960"
  },
  {
    "text": "asking is there a risk of more backups in the system as usual interal um not",
    "start": "2640960",
    "end": "2646760"
  },
  {
    "text": "really well I think the problem if we as as we shorten the interval the problem is a small data",
    "start": "2646760",
    "end": "2653160"
  },
  {
    "text": "files if we let's say commit data file every let's say every second we can only",
    "start": "2653160",
    "end": "2658319"
  },
  {
    "text": "write one second of data into Data file that data file typically to be much smaller maybe a few megabytes compared",
    "start": "2658319",
    "end": "2663920"
  },
  {
    "text": "to Ideal data file size maybe like 100 meaby we're looking for so yes they are producing much small data file which is",
    "start": "2663920",
    "end": "2670160"
  },
  {
    "text": "the downside yeah that's why we typically",
    "start": "2670160",
    "end": "2675319"
  },
  {
    "text": "don't recommend go go very low that's why I think 1 to 10 minutes com pretty",
    "start": "2675319",
    "end": "2681760"
  },
  {
    "text": "common uh thank you for the presentation I had one question about um the",
    "start": "2690760",
    "end": "2696559"
  },
  {
    "text": "optimization right to Iceberg based on latency of Iceberg itself said 10 11",
    "start": "2696559",
    "end": "2704680"
  },
  {
    "text": "minutes was optimal interval interestingly you not considering fling",
    "start": "2704680",
    "end": "2710160"
  },
  {
    "text": "checkpoints in recoverability right because you mentioned operational costs",
    "start": "2710160",
    "end": "2715800"
  },
  {
    "text": "the larger the checkpoints the more cost will become to maintain this can you",
    "start": "2715800",
    "end": "2721920"
  },
  {
    "text": "comment on that yeah so the question is uh earlier we talk about the ice commit",
    "start": "2721920",
    "end": "2728079"
  },
  {
    "text": "interval is 1 to 10 minutes and let also bring checkpoint interval how how fine",
    "start": "2728079",
    "end": "2733359"
  },
  {
    "text": "that is because depend how frequent the fling checkpoint complete maybe the when you have failure you're going to replace",
    "start": "2733359",
    "end": "2739920"
  },
  {
    "text": "more or less data so the I Source I sync only commit data after every successful",
    "start": "2739920",
    "end": "2746960"
  },
  {
    "text": "checkpoint that's why the committing the wall and the FL checkpoint the wall they're actually identical they're the",
    "start": "2746960",
    "end": "2752200"
  },
  {
    "text": "same so I think yeah the main thing is we you don't want to commit too frequently or too check too frequent",
    "start": "2752200",
    "end": "2758680"
  },
  {
    "text": "because it can produce small data files that's related to the earlier question asked and also too many metadata files",
    "start": "2758680",
    "end": "2765440"
  },
  {
    "text": "so but that's why but they also don't want to commit too infrequent that can lead to latency and the delay of the",
    "start": "2765440",
    "end": "2771119"
  },
  {
    "text": "data result so in production most common I have seen is one to 10 minutes some people do fast shorter like every 5",
    "start": "2771119",
    "end": "2777839"
  },
  {
    "text": "minutes or every 10 every 5 second or 10 second some people do them longer than 15 minutes but one to 10 minut more",
    "start": "2777839",
    "end": "2783880"
  },
  {
    "text": "common than",
    "start": "2783880",
    "end": "2787799"
  },
  {
    "text": "maybe",
    "start": "2793520",
    "end": "2795720"
  },
  {
    "text": "I'll uh so in the previous presentation given my sharing uh so she talked about",
    "start": "2801040",
    "end": "2808359"
  },
  {
    "text": "uh the the hybrid Source the Lambda Source where you uh read historical data",
    "start": "2808359",
    "end": "2813800"
  },
  {
    "text": "from da and latest data using C so how do you see the the pros and cons um of",
    "start": "2813800",
    "end": "2822240"
  },
  {
    "text": "you know purely relying on the ice Source comparing with the the notion of",
    "start": "2822240",
    "end": "2827839"
  },
  {
    "text": "the Lambda Source yeah quick question I I repeat question for the recording purpose so",
    "start": "2827839",
    "end": "2834240"
  },
  {
    "text": "the question in the previous presentation by Sharon she talk about the Lambda Source where you have old",
    "start": "2834240",
    "end": "2841079"
  },
  {
    "text": "data histo data in the S3 and recent data in kesis and how is different with",
    "start": "2841079",
    "end": "2846720"
  },
  {
    "text": "here with I'm talking about with Iceberg stream source so as I mention the",
    "start": "2846720",
    "end": "2852559"
  },
  {
    "text": "presentation Iceberg can use to store longterm data you can store months years data in iceb that's no problem because",
    "start": "2852559",
    "end": "2858200"
  },
  {
    "text": "we put data in the CL prop storage three so I think the main problem with that",
    "start": "2858200",
    "end": "2865319"
  },
  {
    "text": "when you boost strap let's say for 30 days of data the main problem is actually not to do all the St dat data",
    "start": "2865319",
    "end": "2873280"
  },
  {
    "text": "in memory you want to kind of um use The Watermark alignment to proc the old data first then move them along uh nicely you",
    "start": "2873280",
    "end": "2881119"
  },
  {
    "text": "don't want to read both new data and old data at the same time and the other thing with the I",
    "start": "2881119",
    "end": "2888960"
  },
  {
    "text": "think by by the way the Lambda source is implementing fling as called hybrid Source my colleague Thomas Thomas was",
    "start": "2888960",
    "end": "2896040"
  },
  {
    "text": "Implement that so basic the idea with yeah the key idea with the um",
    "start": "2896040",
    "end": "2902880"
  },
  {
    "text": "hypers source is you finish reading the historic data first then you transition to the streaming Source but I was just",
    "start": "2902880",
    "end": "2910040"
  },
  {
    "text": "reminder where time is running out yeah feel free to chat with me afterwards",
    "start": "2910040",
    "end": "2915079"
  },
  {
    "text": "thank you very much thank you",
    "start": "2915079",
    "end": "2919119"
  },
  {
    "text": "[Music]",
    "start": "2922190",
    "end": "2927639"
  },
  {
    "text": "everyone",
    "start": "2927839",
    "end": "2930839"
  }
]