[
  {
    "text": "welcome to the info-cube podcast my name is roland mertens and today i am interviewing omar santifero he is a",
    "start": "4960",
    "end": "12320"
  },
  {
    "text": "machine learning engineer at hugging face and he works on the intersection of open source and product",
    "start": "12320",
    "end": "18400"
  },
  {
    "text": "he gave a talk at cucumber and i am actually speaking to him in person at the venue of the cucumber conference in",
    "start": "18400",
    "end": "25599"
  },
  {
    "text": "this interview i will talk with him about hugging face and the work he is doing there but we will also touch on",
    "start": "25599",
    "end": "30960"
  },
  {
    "text": "some societal topics such as the impact deep learning models have on carbon emissions and what it means to",
    "start": "30960",
    "end": "37680"
  },
  {
    "text": "democratize good machine learning as i said before omar presented at cucon london and he is also presenting at qcon",
    "start": "37680",
    "end": "44399"
  },
  {
    "text": "plus if you registered for any of these conferences you can view his video there",
    "start": "44399",
    "end": "49440"
  },
  {
    "text": "we will also release his presentation on the infoq website so keep an eye on that now on to the interview itself welcome",
    "start": "49440",
    "end": "56160"
  },
  {
    "text": "over to cucum london how are you enjoying the conference so far hey it's going great i'm enjoying it quite a bit",
    "start": "56160",
    "end": "62000"
  },
  {
    "text": "yeah the talks have been great yeah it has been an amazing experience yeah so you gave a talk yesterday can you talk a",
    "start": "62000",
    "end": "67360"
  },
  {
    "text": "bit about who you are what your talk was about sure of course so i'm ahmad i'm a ml engineer at cooking face which is a",
    "start": "67360",
    "end": "73680"
  },
  {
    "text": "open source startup that is trying to democratize good machine learning the talk about yesterday was about open",
    "start": "73680",
    "end": "79759"
  },
  {
    "text": "machine learning which is about how we can do machine learning in a way that everyone can use these models that",
    "start": "79759",
    "end": "85680"
  },
  {
    "text": "research that is being done is something that is accessible to everyone and that is very transparent right so it's about",
    "start": "85680",
    "end": "91119"
  },
  {
    "text": "transparently communicating what these models do how they work what could be the impacts of these models and yeah we",
    "start": "91119",
    "end": "97600"
  },
  {
    "text": "talked quite a bit about this about good practices about finding models created by the community about collaboration so",
    "start": "97600",
    "end": "103680"
  },
  {
    "text": "yeah it was a fun experience yeah so what kind of models are there right now on hanging face i think you guys are",
    "start": "103680",
    "end": "109200"
  },
  {
    "text": "mostly specialized in transformers or do you have all kinds of different models yeah yeah that's a great question how we",
    "start": "109200",
    "end": "114799"
  },
  {
    "text": "began a couple of years ago maybe for a bit of context we have a platform called the hub the hub allows anyone to upload",
    "start": "114799",
    "end": "121600"
  },
  {
    "text": "any ml model or data set or demos by now we have over 30 000 models and the",
    "start": "121600",
    "end": "127360"
  },
  {
    "text": "growth has been a bit wide about a year ago we had six thousand models on the hub six thousand dollars that's already",
    "start": "127360",
    "end": "133680"
  },
  {
    "text": "big yeah yeah one year it has multiplied time size right so it's quite exciting and originally what we had a couple of",
    "start": "133680",
    "end": "139760"
  },
  {
    "text": "years ago three years ago we launched an open source library in python called transformers which allows anyone to load",
    "start": "139760",
    "end": "146000"
  },
  {
    "text": "a transformer model from d-hub which is this central platform with a single line of code and then they were able to use",
    "start": "146000",
    "end": "151200"
  },
  {
    "text": "these models and that's how we began and transformers have their origins their roots in nlp so they come from the",
    "start": "151200",
    "end": "157599"
  },
  {
    "text": "natural language processing domain but they have expanded quite a bit to other domains so now they are being used in",
    "start": "157599",
    "end": "162640"
  },
  {
    "text": "computer vision in audio or speech and also in reinforcement learning time series so it has expanded quite a bit",
    "start": "162640",
    "end": "169120"
  },
  {
    "text": "and most of the models on the hub right now maybe about 20 000 so two thirds are transformer models but now we're also",
    "start": "169120",
    "end": "175680"
  },
  {
    "text": "supporting other open source like rising ecosystem so you have to use scikit-learn tensorflow by torch spacey",
    "start": "175680",
    "end": "182800"
  },
  {
    "text": "or really any other machine learning library you can just upload your model to github and use it as a central",
    "start": "182800",
    "end": "188080"
  },
  {
    "text": "platform with which you can collaborate with the community and so if i download a model from hugging face does this then",
    "start": "188080",
    "end": "195040"
  },
  {
    "text": "work with all these different backends or do you have specific models for specific machine learning libraries yeah",
    "start": "195040",
    "end": "201040"
  },
  {
    "text": "so we have different models for different libraries right what we have in the platform is that we have tags so",
    "start": "201040",
    "end": "206959"
  },
  {
    "text": "you can easily discover and filter models depending on your use cases or interests there are some libraries that",
    "start": "206959",
    "end": "212799"
  },
  {
    "text": "have interoperability so for example there's a library called sentence transformers or adapter transformers so",
    "start": "212799",
    "end": "218480"
  },
  {
    "text": "these are two other libraries and they are able to load models that are for transformers but for example cyclic",
    "start": "218480",
    "end": "224000"
  },
  {
    "text": "learn of course they cannot simply load the transformer model but in the case of certain libraries you can but what is",
    "start": "224000",
    "end": "229920"
  },
  {
    "text": "really important to us is that people can easily find the best model for their use cases so we are working in good",
    "start": "229920",
    "end": "235280"
  },
  {
    "text": "search experience so people can find between these 30 000 models which are the best models for their use cases so",
    "start": "235280",
    "end": "241200"
  },
  {
    "text": "what are kind of popular models on there right now that's an excellent question i think that what is quite powerful about",
    "start": "241200",
    "end": "246959"
  },
  {
    "text": "transformers is that you can easily pick a model from the hub and then do something called fine tuning or transfer",
    "start": "246959",
    "end": "252400"
  },
  {
    "text": "learning and then you can just modify this model for your own use case modify is probably a bit of a",
    "start": "252400",
    "end": "257479"
  },
  {
    "text": "oversimplification but you can really pick a model from the hub and just with a bit of your data you can modify or",
    "start": "257479",
    "end": "263199"
  },
  {
    "text": "fine tune or just train a bit more this model for your own specific use case so what many people do is that they go and",
    "start": "263199",
    "end": "268720"
  },
  {
    "text": "pick a very classic model such as bird is a very common language model that",
    "start": "268720",
    "end": "273919"
  },
  {
    "text": "then they can use to do something like test classification but there are many many popular models in the auto domain",
    "start": "273919",
    "end": "279680"
  },
  {
    "text": "there's a model called wave to back to that this is from facebook and this is being used quite a bit for audio so you",
    "start": "279680",
    "end": "285360"
  },
  {
    "text": "can see it the same way as bird is very common pre-trained model in nlp wave to back to is a very common pre-trained",
    "start": "285360",
    "end": "291440"
  },
  {
    "text": "model for audio so you get the right features or you get at least a nicely calibrated feature space once you enter",
    "start": "291440",
    "end": "298720"
  },
  {
    "text": "like an audio file in this case right yeah exactly so what we took to do in this case is that once you pass an audio",
    "start": "298720",
    "end": "304560"
  },
  {
    "text": "file it will map this to a embedding or hidden states that then you can use for",
    "start": "304560",
    "end": "309759"
  },
  {
    "text": "your end task right so you can use wave tobacco to do automatic speech recognition which is you pass an audio",
    "start": "309759",
    "end": "315919"
  },
  {
    "text": "file and then you get a transcription of that file so you can see it as speech to text but there is also work to do dts",
    "start": "315919",
    "end": "322240"
  },
  {
    "text": "which is text-to-speech so you generate synthetic audio but there are many many other applications so for example audio",
    "start": "322240",
    "end": "327680"
  },
  {
    "text": "classification so you pass an audio and then you might classify which is the language being spoken here or which is a",
    "start": "327680",
    "end": "333280"
  },
  {
    "text": "speaker speaking here things like this yeah i also think i saw like speaker verification being used where you have",
    "start": "333280",
    "end": "338880"
  },
  {
    "text": "two vectors where you can compare the latent spaces to see if it's a similar person or the same person actually in",
    "start": "338880",
    "end": "344240"
  },
  {
    "text": "this case yeah exactly and something that is very fun about all of this is that since you are mapping to these",
    "start": "344240",
    "end": "349520"
  },
  {
    "text": "feature spaces which are dnr just vectors or arrays of numbers you can then start to compare vectors of",
    "start": "349520",
    "end": "355120"
  },
  {
    "text": "different modalities so you can start to compare images or the embedding of an image with the embedding of a text or a",
    "start": "355120",
    "end": "360960"
  },
  {
    "text": "paragraph or a document or or an audio file right and then you can start to get a cross modality search so you can start",
    "start": "360960",
    "end": "368160"
  },
  {
    "text": "to have a search systems in which you might pass an image and you might get retrieved all of the text relevant to",
    "start": "368160",
    "end": "373840"
  },
  {
    "text": "this image or the other way around right so you search funny cards and then you will get funny cats images yeah i really",
    "start": "373840",
    "end": "379280"
  },
  {
    "text": "liked it about the clip although from open ai that you can easily build a natural language image search system",
    "start": "379280",
    "end": "385039"
  },
  {
    "text": "where you can just align the medics of your text to to the embedding space that's really brilliant but yeah you",
    "start": "385039",
    "end": "391440"
  },
  {
    "text": "save it like what you're working on this is open source ecosystem so how is this work are many people publishing models",
    "start": "391440",
    "end": "397840"
  },
  {
    "text": "or are many people collaborating to make models better yeah so there are many many many efforts",
    "start": "397840",
    "end": "403039"
  },
  {
    "text": "going on i've been working at hooking face for about a year by now and we have many efforts so in the team we really",
    "start": "403039",
    "end": "409440"
  },
  {
    "text": "don't train models from scratch we do have a science team and we are training some models and doing some open and",
    "start": "409440",
    "end": "416000"
  },
  {
    "text": "science work so we do have research that is trying to push the state of the art with a very big effort called big",
    "start": "416000",
    "end": "421520"
  },
  {
    "text": "science we can talk a bit more about that but from the open source team what we are doing is we are building collaborations with the open source",
    "start": "421520",
    "end": "427520"
  },
  {
    "text": "ecosystem and this goes in many many ways so one is that as soon as there is new research related to transformers we",
    "start": "427520",
    "end": "434319"
  },
  {
    "text": "are adding these new architectures to the transformers library so people can easily train these models and use them",
    "start": "434319",
    "end": "439919"
  },
  {
    "text": "because what happens very often is that researchers share github repository if they do the code might not be super",
    "start": "439919",
    "end": "446479"
  },
  {
    "text": "clean or might not follow good practices or it's not intended for production and that's totally okay that's how academia",
    "start": "446479",
    "end": "451520"
  },
  {
    "text": "research works but once you want to start using these models in a real world setting you really need to have like",
    "start": "451520",
    "end": "456639"
  },
  {
    "text": "good practices so we have collaborations with many open source libraries as we were talking before so we have",
    "start": "456639",
    "end": "462160"
  },
  {
    "text": "collaborations with about 15 or 20 different open source teams from other companies or just from volunteers that",
    "start": "462160",
    "end": "468479"
  },
  {
    "text": "are working in other external to cooking phase right so that is quite exciting and then there are people that are just",
    "start": "468479",
    "end": "474000"
  },
  {
    "text": "training models and pushing the state of the art for example there is a very cool group called somas nlp which is focused",
    "start": "474000",
    "end": "479680"
  },
  {
    "text": "in nlp for spanish and this is a group of volunteers that they just like to push the boundaries of the state of the",
    "start": "479680",
    "end": "485599"
  },
  {
    "text": "art in nlp in spanish and they are training very interesting models organizing events creating content in",
    "start": "485599",
    "end": "491840"
  },
  {
    "text": "spanish and it has been quite exciting and from the coding face side we try to support them in whatever they need sort of a few examples yes there is basically",
    "start": "491840",
    "end": "498800"
  },
  {
    "text": "really the safely art models the best of the best published by the big companies but then replicated in a more",
    "start": "498800",
    "end": "505599"
  },
  {
    "text": "reproducible way by volunteers and kind of other people yeah so in the team we do have people that are implementing",
    "start": "505599",
    "end": "512000"
  },
  {
    "text": "these models in the transformers library so it's not that it's done just by people outside us but yeah so we are",
    "start": "512000",
    "end": "517680"
  },
  {
    "text": "trying to replicate these model implementations in a good standardized way with common interfaces that we work",
    "start": "517680",
    "end": "524640"
  },
  {
    "text": "with similar models for the same task so if you want to use a model for text classification and you want to try",
    "start": "524640",
    "end": "530399"
  },
  {
    "text": "different architectures you can very very very easily change from one model to the other even if they are different",
    "start": "530399",
    "end": "535600"
  },
  {
    "text": "models you can do that right so that's quite powerful and how does it work in terms of the data do you also host the",
    "start": "535600",
    "end": "541680"
  },
  {
    "text": "data sets or do people download their own data sets or are all the data sets",
    "start": "541680",
    "end": "546959"
  },
  {
    "text": "by openingi for example public yeah that's a great question so many of the data sets for these very very famous",
    "start": "546959",
    "end": "553680"
  },
  {
    "text": "large models such as gpt3 neither the model neither the datasets are public the papers for example do explain quite",
    "start": "553680",
    "end": "561279"
  },
  {
    "text": "a bit about how this data was gathered so there is information about it but the data was not published itself most of",
    "start": "561279",
    "end": "566320"
  },
  {
    "text": "the time this is data script from the web so they go to reddit for example it's just from many many websites it's",
    "start": "566320",
    "end": "571600"
  },
  {
    "text": "not just reddit right but they go too many websites a wikipedia reddit and other platforms and they describe data",
    "start": "571600",
    "end": "577680"
  },
  {
    "text": "from the web and they use this to train these models that will then gain a statistical understanding of the language from the coding phase side what",
    "start": "577680",
    "end": "584399"
  },
  {
    "text": "we do is that we have something called data sets and this is a bit branch in the tools so we have an open source",
    "start": "584399",
    "end": "589760"
  },
  {
    "text": "library in python called datasets and what it allows is to load data sets with a single line of code with a single line",
    "start": "589760",
    "end": "595360"
  },
  {
    "text": "of code like load from hub you can load the data set with the speed you want and it's quite nice because you can work with huge data sets of many terabytes",
    "start": "595360",
    "end": "602640"
  },
  {
    "text": "you can easily do filtering or mapping or do any operations you want to do about it and then what we have is the",
    "start": "602640",
    "end": "608480"
  },
  {
    "text": "data sets hub so data set scope is a in the hub is a platform that allows people to share their data sets what most",
    "start": "608480",
    "end": "614399"
  },
  {
    "text": "people do is that they just share scripts to load data from other sources so what usually happens is that people",
    "start": "614399",
    "end": "620640"
  },
  {
    "text": "might have their data set stored in google cloud or amazon web services and then they will have a script on the hub",
    "start": "620640",
    "end": "626959"
  },
  {
    "text": "and the hooking face hub that will load this data set and do all the splitting for example or any kind of post",
    "start": "626959",
    "end": "632880"
  },
  {
    "text": "processing that is required and they also do the documentation for their data set on the hub but some people also",
    "start": "632880",
    "end": "639120"
  },
  {
    "text": "share their data sets directly on the host so that means if their datasets are csv files or jsons or whatever they will",
    "start": "639120",
    "end": "644399"
  },
  {
    "text": "upload those files to the hub and they can also use that if they want so you can store the data set on the hub and what is very nice is that on the hub you",
    "start": "644399",
    "end": "650800"
  },
  {
    "text": "have a data set preview thing which allows you to explore and play with the data set directly on the browser so",
    "start": "650800",
    "end": "656560"
  },
  {
    "text": "without having to run any code or download the data set you can just explore a bit this data set directly in",
    "start": "656560",
    "end": "662240"
  },
  {
    "text": "the web browser so i think the other thing you mentioned in your talk where these hollow cards right where if you",
    "start": "662240",
    "end": "667680"
  },
  {
    "text": "have a model that you have some basic information about how it's trained whether it's trained on lottery dusky maybe tell me about that yeah sure so",
    "start": "667680",
    "end": "675360"
  },
  {
    "text": "the concept is will sound quite obvious for anyone that has done a lot of software development you document your",
    "start": "675360",
    "end": "680720"
  },
  {
    "text": "code you should also document your models but in practice almost nobody was doing this so this is a concept the",
    "start": "680720",
    "end": "686480"
  },
  {
    "text": "modern cars that was coined by google make michelle specifically and her research group three years ago in 2019",
    "start": "686480",
    "end": "693440"
  },
  {
    "text": "and the idea here is that you can have an artifact that will document your model and of course how you document",
    "start": "693440",
    "end": "699200"
  },
  {
    "text": "that model is completely different as how you document your software but usually you have things such as what",
    "start": "699200",
    "end": "705200"
  },
  {
    "text": "this model is right what is it supposed to do how it was trained which data was used to train this model how it was",
    "start": "705200",
    "end": "710880"
  },
  {
    "text": "evaluated maybe a snippet of code to run this model so showing how to run",
    "start": "710880",
    "end": "716240"
  },
  {
    "text": "inference because if you don't do that people want people to use your model and then you might have things such as limitations and biases so most if not",
    "start": "716240",
    "end": "723360"
  },
  {
    "text": "all models have some biases and we can talk a bit more about this in a second so it's very important to document that",
    "start": "723360",
    "end": "728720"
  },
  {
    "text": "this model might not work for certain minority group for example right if not what can happen is that a company or",
    "start": "728720",
    "end": "734959"
  },
  {
    "text": "anyone can go and pick this model and try to use this model for this use case without knowing that there were these limitations and that will just lead to",
    "start": "734959",
    "end": "741760"
  },
  {
    "text": "many different issues how much do people know what the limitations of a model are because i personally when i try",
    "start": "741760",
    "end": "748240"
  },
  {
    "text": "something i frequently discover the weirdest things not working which you don't really expect do people really",
    "start": "748240",
    "end": "753760"
  },
  {
    "text": "also explore what the model kind of captures in the latent space yeah so this is something that some",
    "start": "753760",
    "end": "760160"
  },
  {
    "text": "people are doing but not all people and what we're trying to do we have a research group completely focused on",
    "start": "760160",
    "end": "765360"
  },
  {
    "text": "this it's great tools to allow people to easily explore and really dive into the model and how",
    "start": "765360",
    "end": "772480"
  },
  {
    "text": "it works both model and data sets so we're trying to create those to enable researchers but also practitioners to",
    "start": "772480",
    "end": "778399"
  },
  {
    "text": "really explore how their model works there are many explainability libraries that try to",
    "start": "778399",
    "end": "783680"
  },
  {
    "text": "show for example the tension mechanism and how the model is working and which words are important ones for the model",
    "start": "783680",
    "end": "789360"
  },
  {
    "text": "but in practice this is not super actionable let's say so we're trying to create tools that give actionable",
    "start": "789360",
    "end": "794639"
  },
  {
    "text": "feedback that will help people to easily know what is going on here but what people do is they give or they provide",
    "start": "794639",
    "end": "801680"
  },
  {
    "text": "high level description of which biases the model might have so this is something that does exist already yeah",
    "start": "801680",
    "end": "806720"
  },
  {
    "text": "yeah so what do you mean in terms of biases like specific things it is good at recognizing specific things which is",
    "start": "806720",
    "end": "812800"
  },
  {
    "text": "bad at recognizing yeah so this is quite depending on the use case but let me give you an example with gpt so gpt is a",
    "start": "812800",
    "end": "819120"
  },
  {
    "text": "very famous model that predicts which is the next word right so if you say i don't know today is a very x day for",
    "start": "819120",
    "end": "825519"
  },
  {
    "text": "example so today will be a very new text it might say i don't know sunny or nice day something like that right so it will",
    "start": "825519",
    "end": "831440"
  },
  {
    "text": "predict which is the next word for example if you say this man is working as a the next word that the mother will",
    "start": "831440",
    "end": "838079"
  },
  {
    "text": "predict might be architect doctor or something like that for example with woman it will say something like this",
    "start": "838079",
    "end": "844320"
  },
  {
    "text": "woman is working as a and one of the predictions might be prostitutes for example right how is this happening well the model was",
    "start": "844320",
    "end": "851040"
  },
  {
    "text": "trained with lots of data scraped from the web what is quite powerful about these huge transformer models is that you don't require any labeled data so",
    "start": "851040",
    "end": "857600"
  },
  {
    "text": "that means that you don't need to spend a lot of money leveling data so you can just grab as much data as you want you can pass it to the model and the model",
    "start": "857600",
    "end": "864079"
  },
  {
    "text": "will in an unsupervised way learn the patterns in the data right and then as we were talking before people sometimes",
    "start": "864079",
    "end": "870560"
  },
  {
    "text": "just go grab this model and then they do fine-tuning or transfer learning to train a new model and what is interesting is that these biases are",
    "start": "870560",
    "end": "877199"
  },
  {
    "text": "also learned with the transfer learning so this fine-tune model will also have the biases from this source original",
    "start": "877199",
    "end": "882800"
  },
  {
    "text": "model right so even if you didn't have any data that would be concerning in the",
    "start": "882800",
    "end": "888000"
  },
  {
    "text": "fine tuning stage if this first model had biases this fine-tuned model will also have iss so this is a very clear",
    "start": "888000",
    "end": "894560"
  },
  {
    "text": "example of what biases would mean here yeah i guess that in this case if you mostly download the reddit data for",
    "start": "894560",
    "end": "900000"
  },
  {
    "text": "example that's not a very accurate requestation of the world or i mean",
    "start": "900000",
    "end": "905199"
  },
  {
    "text": "maybe even in worst cases it's not an accurate representation of what you actually try to predict yeah exactly so",
    "start": "905199",
    "end": "911040"
  },
  {
    "text": "we have an effort called big science big science started as an effort that hogan face proposed with the french government",
    "start": "911040",
    "end": "917920"
  },
  {
    "text": "so the french government is giving us a couple millions of compute power from their super computer in paris and the",
    "start": "917920",
    "end": "924560"
  },
  {
    "text": "idea here is to train a very very very very large language model but in a fully",
    "start": "924560",
    "end": "929839"
  },
  {
    "text": "transparent open science and open source way so this is something that hogging face is pushing but it's not an effort",
    "start": "929839",
    "end": "935040"
  },
  {
    "text": "from hawking phase so right now it's a collaboration between 700 researchers from different universities so anyone",
    "start": "935040",
    "end": "940320"
  },
  {
    "text": "that is a researcher in academia or industry can just join the effort it's quite interesting because what we are trying to do is put all the questions up",
    "start": "940320",
    "end": "947600"
  },
  {
    "text": "front which by assessing this model might have how are we collecting this data so we had a lot a lot of work so",
    "start": "947600",
    "end": "953759"
  },
  {
    "text": "this is for the third march of last year we just started to train the model last month like three weeks ago yeah so we",
    "start": "953759",
    "end": "959360"
  },
  {
    "text": "spent eight nine months just exploring the data defining which data sets we want to have which languages we want to",
    "start": "959360",
    "end": "965279"
  },
  {
    "text": "work with great like cleaning all of this data making sure that there are no biases exploring this data so it was",
    "start": "965279",
    "end": "971360"
  },
  {
    "text": "quite interesting and it was an effort between many people from different backgrounds and for us that's quite",
    "start": "971360",
    "end": "976480"
  },
  {
    "text": "important right so when you want to train these very large powerful models if you just go and get all data from the",
    "start": "976480",
    "end": "982079"
  },
  {
    "text": "web you will have many issues and it's very hard to get or find these issues if you really don't work with this data",
    "start": "982079",
    "end": "988320"
  },
  {
    "text": "so yeah and are there any kind of practical things you can do as a machine learning practitioner when setting up a",
    "start": "988320",
    "end": "994480"
  },
  {
    "text": "data set are there specific kind of tips and tricks to try to prevent these biases or is it just something you have",
    "start": "994480",
    "end": "1000320"
  },
  {
    "text": "to keep in mind when deploying your model that they are there yeah i think you need to do both the second thing you",
    "start": "1000320",
    "end": "1005440"
  },
  {
    "text": "mentioned about just keeping in mind making sure is something that you will always need to do in machine learning so",
    "start": "1005440",
    "end": "1011360"
  },
  {
    "text": "it's very very easy for your model to have iss and especially when your models are impacting humans lives you need to",
    "start": "1011360",
    "end": "1016560"
  },
  {
    "text": "be especially careful about how you are using this model because of course if your model is just doing something very silly or not having a huge impact or",
    "start": "1016560",
    "end": "1023839"
  },
  {
    "text": "something that is in a future that won't have direct impact in a human life maybe that that's okay right but when you're impacting i don't know something related",
    "start": "1023839",
    "end": "1030558"
  },
  {
    "text": "to health care or something related to loans right then you need to be extremely careful but then in terms of",
    "start": "1030559",
    "end": "1035678"
  },
  {
    "text": "if you are training a model and you want to be careful about this you need to be very careful and really dive into your data before going and training a model",
    "start": "1035679",
    "end": "1042319"
  },
  {
    "text": "and it can be quite easy really like you can just go and see a bit your data many people train models without really",
    "start": "1042319",
    "end": "1047600"
  },
  {
    "text": "spending one hour just looking at their data and even unrelated to biases this is a good practice in machine learning in general because for example if you",
    "start": "1047600",
    "end": "1054960"
  },
  {
    "text": "have different ways in which people are doing spaces or current tokenization methods of your data you might end up",
    "start": "1054960",
    "end": "1060880"
  },
  {
    "text": "with terrible results and just because you didn't really look into your data and once you have your model trained there are many things you can do for",
    "start": "1060880",
    "end": "1067120"
  },
  {
    "text": "example what you can do is maybe i will go a bit too deep on this but you can do things such as looking at data in which",
    "start": "1067120",
    "end": "1074720"
  },
  {
    "text": "the model prediction and the label diverge quite a bit so if your model is predicting 0.99 but the label was zero that means",
    "start": "1074720",
    "end": "1082480"
  },
  {
    "text": "that the model is terribly grown or the label is wrong right so that's an easy way to just get some data that might be",
    "start": "1082480",
    "end": "1088160"
  },
  {
    "text": "interesting to look at if you are using a threshold boundary you might want to look data near the threshold because",
    "start": "1088160",
    "end": "1093280"
  },
  {
    "text": "those are samples in which the model might be struggling a bit so you should probably go because there might be or an",
    "start": "1093280",
    "end": "1098720"
  },
  {
    "text": "issue with the model or an issue with data right and then you can do other things for example you can look at data",
    "start": "1098720",
    "end": "1105360"
  },
  {
    "text": "based on how much loss it causes so once you train the model you can pass a lot of data and see which was the change of",
    "start": "1105360",
    "end": "1111679"
  },
  {
    "text": "the loss based on each of these samples and the ones with the big largest loss those are the ones making a biggest",
    "start": "1111679",
    "end": "1117520"
  },
  {
    "text": "change in the model so you probably want to take a look at those samples as well maybe those are too practical but yeah those are nice ways and it helps not",
    "start": "1117520",
    "end": "1124320"
  },
  {
    "text": "just for biases but it helps to find interesting data in your data set yeah i think at least it will probably give you",
    "start": "1124320",
    "end": "1129919"
  },
  {
    "text": "a hint on what extra data to collect and how to do it another thing i sometimes read is that if you",
    "start": "1129919",
    "end": "1135679"
  },
  {
    "text": "know where your bias might be coming from just giving it as an explicit feature into your model so if you know",
    "start": "1135679",
    "end": "1141200"
  },
  {
    "text": "that maybe people from certain areas are kind of actively being biased on school",
    "start": "1141200",
    "end": "1147200"
  },
  {
    "text": "results just adding the area as a feature might actually teach the model that this feature is unreliable but that",
    "start": "1147200",
    "end": "1153280"
  },
  {
    "text": "you have a different representation for these people yeah yeah i think in terms of iss there are many things that you can do but i don't think there is a",
    "start": "1153280",
    "end": "1160240"
  },
  {
    "text": "single solution for every problem right now we are talking a bit about text data right but once we talk about image data",
    "start": "1160240",
    "end": "1165760"
  },
  {
    "text": "for example then it gets complicated as well or if you're talking about tabular data so tables for example it happens a",
    "start": "1165760",
    "end": "1173600"
  },
  {
    "text": "lot that you have some columns or some features that are strongly correlated with some i don't know with gender for",
    "start": "1173600",
    "end": "1179440"
  },
  {
    "text": "example right or some race so even if you are not fitting the model the woman's a labeled woman has zero in i",
    "start": "1179440",
    "end": "1185679"
  },
  {
    "text": "don't know something but you might have a feature that is strongly correlated so i think really exploring your data is",
    "start": "1185679",
    "end": "1191360"
  },
  {
    "text": "extremely important if you want to avoid biases which if you are deploying models in production is something that you",
    "start": "1191360",
    "end": "1196880"
  },
  {
    "text": "should do to be responsible yeah now indeed the other thing i think you mentioned in your talk was where the",
    "start": "1196880",
    "end": "1201919"
  },
  {
    "text": "carbon emissions so if you're trading massive models carbon emissions can be quite large so compute power can be",
    "start": "1201919",
    "end": "1207440"
  },
  {
    "text": "quite large can you say anything about that sure i think this is linked as well to the model cards so when you",
    "start": "1207440",
    "end": "1213600"
  },
  {
    "text": "share models and you publish the model card which is documenting how this model works you should also document which are",
    "start": "1213600",
    "end": "1219600"
  },
  {
    "text": "the ecological implications of your model right so right now what we have on the hub is that in the model card you",
    "start": "1219600",
    "end": "1225039"
  },
  {
    "text": "can specify in the metadata which is some information some metadata about the model you can specify which are the co2",
    "start": "1225039",
    "end": "1231600"
  },
  {
    "text": "emissions and at least to the transformers like we have added callbacks and functions that allow people to very easily track the co2",
    "start": "1231600",
    "end": "1238559"
  },
  {
    "text": "emissions while the model is being trained now maybe to take a step back and talk a bit about what is ecological",
    "start": "1238559",
    "end": "1243760"
  },
  {
    "text": "impact mean or why are we talking about co2 emissions now so these models are very very very large they are really",
    "start": "1243760",
    "end": "1249760"
  },
  {
    "text": "huge so we were talking about hundreds of billions of parameters for some of these cases right so there are models",
    "start": "1249760",
    "end": "1255280"
  },
  {
    "text": "that are just two or three billion parameters or some millions but training these models can have a bit of impact so",
    "start": "1255280",
    "end": "1260799"
  },
  {
    "text": "right now some of the largest models might have the impact maybe of a flight an intercontinental flight or maybe the",
    "start": "1260799",
    "end": "1268400"
  },
  {
    "text": "life span of a car in the us for example but this is for very large models so these very large models is not something",
    "start": "1268400",
    "end": "1274880"
  },
  {
    "text": "that everyone is training so most people are training very small models that really don't have as much ecological",
    "start": "1274880",
    "end": "1280640"
  },
  {
    "text": "impact but it's very important to document this there is something called the loss of scaling which shows that as",
    "start": "1280640",
    "end": "1286159"
  },
  {
    "text": "you grow the model the better the performance will be right so the trend that has been happening already for the",
    "start": "1286159",
    "end": "1291760"
  },
  {
    "text": "last four years is that people are just going with larger and larger and larger models just two days ago today is",
    "start": "1291760",
    "end": "1296960"
  },
  {
    "text": "wednesday right so yeah so on monday google released a model with 500 billion parameters i think and of course if we",
    "start": "1296960",
    "end": "1303200"
  },
  {
    "text": "keep seeing this trend and i think we'll keep seeing this trend the models will just consume more co2 just to train them",
    "start": "1303200",
    "end": "1309120"
  },
  {
    "text": "right and then when you put them in production they will also consume more compared to a psychic learning model",
    "start": "1309120",
    "end": "1314240"
  },
  {
    "text": "with a i don't know which is super small right yeah like a gt3 models you can't run on your laptop and you can't even",
    "start": "1314240",
    "end": "1320240"
  },
  {
    "text": "run them on a single machine you really have to have a data center and to have an api to interact with them yeah",
    "start": "1320240",
    "end": "1325760"
  },
  {
    "text": "exactly exactly and even if some of these models are open source yeah as you're saying that you need to have many gpus just to load the model right there",
    "start": "1325760",
    "end": "1332400"
  },
  {
    "text": "yeah so yeah so it's very important to document them and there were some other",
    "start": "1332400",
    "end": "1337440"
  },
  {
    "text": "talks yesterday about this so it's quite interesting so i would suggest you to look at that talk but i think something",
    "start": "1337440",
    "end": "1342799"
  },
  {
    "text": "important is that this doesn't mean that we should not train these models but we should make sure that we are training",
    "start": "1342799",
    "end": "1348640"
  },
  {
    "text": "models that are solving relevant issues and that we are being careful about reducing work that already exists right",
    "start": "1348640",
    "end": "1354799"
  },
  {
    "text": "so this concept of fine tuning or using pre-trained models really saves not only in money but also in ecological impact",
    "start": "1354799",
    "end": "1361600"
  },
  {
    "text": "right so you can just go fine tune a model and it will just take you a couple of minutes or hours in a gpu and that's",
    "start": "1361600",
    "end": "1367280"
  },
  {
    "text": "it but if you want to drink one of these large models it will take a lot of money hundreds of thousands of dollars and many gpus right so i think it's quite",
    "start": "1367280",
    "end": "1373919"
  },
  {
    "text": "important to try to leverage existing work and if you train models even if they are small starting to document the",
    "start": "1373919",
    "end": "1380159"
  },
  {
    "text": "co2 emissions is i think a good practice that we'll start to see more and more because the models are becoming larger",
    "start": "1380159",
    "end": "1386159"
  },
  {
    "text": "and when we are comparing our benchmarking models also measuring not just the accuracy or the precision or",
    "start": "1386159",
    "end": "1392400"
  },
  {
    "text": "just a single number but also look at metrics such as how fast this model is if it can be deployed or what's the co2",
    "start": "1392400",
    "end": "1398720"
  },
  {
    "text": "mission to train this model are things that are important to compare between different models so if you have a model",
    "start": "1398720",
    "end": "1404400"
  },
  {
    "text": "that is extremely small and have two percent less in precision which can happen when you do distillation maybe",
    "start": "1404400",
    "end": "1410320"
  },
  {
    "text": "you can take that hit right maybe that two percent less precision is completely okay if that means that you will be",
    "start": "1410320",
    "end": "1416480"
  },
  {
    "text": "saving in co2 emissions and a lot of money in compute as well so yeah yeah yeah that's",
    "start": "1416480",
    "end": "1421760"
  },
  {
    "text": "a really good point so the last thing maybe you mentioned in your talk was a version control for deep learning models",
    "start": "1421760",
    "end": "1427120"
  },
  {
    "text": "would that be a solution that you always kind of start already with existing features and then you have to train a",
    "start": "1427120",
    "end": "1432640"
  },
  {
    "text": "tiny bit yeah so having version control is a good practice but what i was talking about it was comparing how we do",
    "start": "1432640",
    "end": "1439520"
  },
  {
    "text": "software to how we do machine learning right so when you're working in a software project you won't just go and release your large software related",
    "start": "1439520",
    "end": "1446400"
  },
  {
    "text": "project right or your i don't know like open source library you will work and iterate on it and do like an iterative",
    "start": "1446400",
    "end": "1452000"
  },
  {
    "text": "approach with it so we try to do the same with these models so for example a model you might train for 100 bucks or",
    "start": "1452000",
    "end": "1457600"
  },
  {
    "text": "100 iterations so instead of just pushing the last model you might want to save every 10 epochs for example and",
    "start": "1457600",
    "end": "1463120"
  },
  {
    "text": "this is a good practice because maybe your last model was not the best one but a previous one so having this is a nice",
    "start": "1463120",
    "end": "1468720"
  },
  {
    "text": "way right to be able to just go to different versions and really compare the different versions what some people",
    "start": "1468720",
    "end": "1474559"
  },
  {
    "text": "have been doing already for many many years is that they might have some callback for example in keras i think many people do this and you just save",
    "start": "1474559",
    "end": "1480799"
  },
  {
    "text": "the best model but then we are going again to this word in which you just look at one or two numbers right so you",
    "start": "1480799",
    "end": "1486880"
  },
  {
    "text": "just look at the best model with related to precision but there are really many numbers that you should look at when you",
    "start": "1486880",
    "end": "1492000"
  },
  {
    "text": "want to push a model to production so having version control is important if you are turning a model for a few weeks",
    "start": "1492000",
    "end": "1497679"
  },
  {
    "text": "for example you might want to just keep pushing models to the hub and then you might want to keep updating the metrics and at one point you might be able to",
    "start": "1497679",
    "end": "1503840"
  },
  {
    "text": "compare all these metrics across time so that's quite useful and then if your model just breaks you",
    "start": "1503840",
    "end": "1509039"
  },
  {
    "text": "can also just go back to our previous version yeah so for example if you would be normally trained your model for 100",
    "start": "1509039",
    "end": "1515600"
  },
  {
    "text": "epochs when you update your data set you want to continue trading on the 50 that box so your kind of latent features are",
    "start": "1515600",
    "end": "1521200"
  },
  {
    "text": "already relatively well but then still have enough space to learn right yeah yeah exactly and do you also see that",
    "start": "1521200",
    "end": "1527120"
  },
  {
    "text": "that's something on the hugging phase where people keep building on top of existing models to keep building wise advanced models versus something which",
    "start": "1527120",
    "end": "1533679"
  },
  {
    "text": "is relatively new uh this is happening this is happening i would not say it's happening that much but i'm seeing this",
    "start": "1533679",
    "end": "1540000"
  },
  {
    "text": "more and more so i think this trend will just keep going up yeah yeah and then maybe the last thing we want to talk a",
    "start": "1540000",
    "end": "1545039"
  },
  {
    "text": "bit about democratizing good machine learning can you say something about what you mean with that sure yeah so",
    "start": "1545039",
    "end": "1550240"
  },
  {
    "text": "really i think this is a very complex topic and we could talk a few hours about this but something important to",
    "start": "1550240",
    "end": "1555520"
  },
  {
    "text": "keep in mind with machine learning is that as a software engineer it's very easy to see machine learning us okay we're solving this problem and that's it",
    "start": "1555520",
    "end": "1562080"
  },
  {
    "text": "but at the end we are impacting humans lives so it's very important to always keep that in mind and when you are doing",
    "start": "1562080",
    "end": "1568080"
  },
  {
    "text": "machine learning there are many aspects to this so for example from the research perspective when you're doing machine learning research and you are proposing",
    "start": "1568080",
    "end": "1574799"
  },
  {
    "text": "or publishing these huge models that nobody can train us at these large institutions then you're making science",
    "start": "1574799",
    "end": "1580480"
  },
  {
    "text": "or research not accessible to everyone right so if you want other researchers to be able to evaluate your model you",
    "start": "1580480",
    "end": "1586159"
  },
  {
    "text": "cannot do that when we are talking about these huge models people that are working in low digital resource",
    "start": "1586159",
    "end": "1591440"
  },
  {
    "text": "languages so for example if there are not that many data sets in spanish people won't be able to go and train any",
    "start": "1591440",
    "end": "1596799"
  },
  {
    "text": "mobile right so these are problems that you always need to keep in mind and not just that you need to involve people",
    "start": "1596799",
    "end": "1602480"
  },
  {
    "text": "that are being impacted by them so something i talked quite a bit in my talk was about demos so building demos",
    "start": "1602480",
    "end": "1608159"
  },
  {
    "text": "that people can use to interact and play with the models so something we are seeing is that people are creating demos to for their models and then they are",
    "start": "1608159",
    "end": "1614720"
  },
  {
    "text": "sharing those demos with people that will be impacted by these systems and this helps because most likely in a",
    "start": "1614720",
    "end": "1620159"
  },
  {
    "text": "software related team you will have a group of if you are a team of 10 people nine of those will be men or eight of",
    "start": "1620159",
    "end": "1625919"
  },
  {
    "text": "those statistically speaking most of them will be white men and then you will have a set of people that will have a",
    "start": "1625919",
    "end": "1632080"
  },
  {
    "text": "very narrow view of the world so when you do this kind of demos and you share them with people from different contexts",
    "start": "1632080",
    "end": "1637360"
  },
  {
    "text": "different backgrounds even if they are not ml engineers you're going to start to get interesting perspective a couple of weeks ago someone released a imagine",
    "start": "1637360",
    "end": "1644080"
  },
  {
    "text": "captioning model and it was working extremely well with white people but with black people it was having labels such as i think it was korean and it was",
    "start": "1644080",
    "end": "1650640"
  },
  {
    "text": "similar to what happened with google photos like five years ago and it's very important to really make sure that",
    "start": "1650640",
    "end": "1656320"
  },
  {
    "text": "people from different contexts play with these models just because of this so involving people that will be impacted",
    "start": "1656320",
    "end": "1661520"
  },
  {
    "text": "or any stakeholder is extremely important and that's a huge part of good machine learning democratizing with",
    "start": "1661520",
    "end": "1666559"
  },
  {
    "text": "machine learning so we try to involve people both from the okay we have a model so let's validate it and let's see",
    "start": "1666559",
    "end": "1672399"
  },
  {
    "text": "how it works with different groups so that's something that we do we try to do these open science efforts to really",
    "start": "1672399",
    "end": "1677919"
  },
  {
    "text": "enable people to to do things and then we're doing many different things to enable people from underrepresented",
    "start": "1677919",
    "end": "1683440"
  },
  {
    "text": "groups to be able to use machine learning we had a very fun effort with yandex in july of last year and it was",
    "start": "1683440",
    "end": "1690799"
  },
  {
    "text": "published in eurips this year and the idea was to do distributed computing so these people didn't have gpus and what",
    "start": "1690799",
    "end": "1696960"
  },
  {
    "text": "we wanted to do was train a very large model for their local language what we did instead was you could see it as",
    "start": "1696960",
    "end": "1702640"
  },
  {
    "text": "distributed computing and each of these persons provided a bit of their compute power in from their own computer and",
    "start": "1702640",
    "end": "1708799"
  },
  {
    "text": "then we had 40 or 50 people just with our computer open all the time and at the end we had an open source model",
    "start": "1708799",
    "end": "1714960"
  },
  {
    "text": "owned by them and it was published in a paper and it was also owned by them so it's in an organization",
    "start": "1714960",
    "end": "1721120"
  },
  {
    "text": "and everyone collaborated on this and even if they were not writing any code or anything they were just running some",
    "start": "1721120",
    "end": "1726960"
  },
  {
    "text": "cells of code they were contributing to this larger model and this is an interesting effort because this could",
    "start": "1726960",
    "end": "1732000"
  },
  {
    "text": "enable people from languages or from universities or communities that really don't have access to gpus or any like",
    "start": "1732000",
    "end": "1738640"
  },
  {
    "text": "fancy compute power this could enable them to train these larger models and this was a state of the art so this was",
    "start": "1738640",
    "end": "1743840"
  },
  {
    "text": "quite interesting yeah i think or especially the underlying data can be so important because for gt3 you can see",
    "start": "1743840",
    "end": "1749760"
  },
  {
    "text": "the underlying language distribution and you can also when you're playing around with the model it works perfectly for",
    "start": "1749760",
    "end": "1755279"
  },
  {
    "text": "english and then once you get into more obscure languages like swedish dutch amount of content on the internet in",
    "start": "1755279",
    "end": "1761919"
  },
  {
    "text": "that language is maybe a bit lower but also harder to find there's not that many dutch speaking subreddits so yeah",
    "start": "1761919",
    "end": "1768720"
  },
  {
    "text": "everything is influenced but it's good to try to involve those people like i think you mentioned the spanish people",
    "start": "1768720",
    "end": "1774000"
  },
  {
    "text": "right who know exactly where the spanish language resources are exactly yeah",
    "start": "1774000",
    "end": "1779679"
  },
  {
    "text": "exactly and even then spanish is quite interesting because spanish is one of the i think it's the second most spoken",
    "start": "1779679",
    "end": "1785440"
  },
  {
    "text": "language it has 500 million speakers or i think it's much more than that probably but it's interesting because",
    "start": "1785440",
    "end": "1791039"
  },
  {
    "text": "the spanish from spain is will be completely different than the spanish from argentina or spanish from mexico and then you need to involve people not",
    "start": "1791039",
    "end": "1797679"
  },
  {
    "text": "from spain but also people from different countries in latin america to be able to get a very distributed very",
    "start": "1797679",
    "end": "1803279"
  },
  {
    "text": "fair data set distribution so you get data from these different sources from these different spanish variations right",
    "start": "1803279",
    "end": "1809120"
  },
  {
    "text": "if you just train spanish models from spanish from spain then you will get a complete it will be a biased model right",
    "start": "1809120",
    "end": "1814720"
  },
  {
    "text": "so because it will be just focusing the spanish from spain so that's a a key topic as well so really involving the",
    "start": "1814720",
    "end": "1821520"
  },
  {
    "text": "people to get these data sets right so if you just speak english or you just live in an english-speaking country and",
    "start": "1821520",
    "end": "1826559"
  },
  {
    "text": "then you want to train a large model for italian without involving anyone that speaks italian you probably cannot do",
    "start": "1826559",
    "end": "1832480"
  },
  {
    "text": "that right yeah yeah you will get some numbers that will show you that the model is doing great but if you don't have any idea of how this model is",
    "start": "1832480",
    "end": "1838799"
  },
  {
    "text": "really working that's not great yeah and also just getting a feeling for what are relevant resources so every time i'm",
    "start": "1838799",
    "end": "1843840"
  },
  {
    "text": "learning a new language you're trying to figure out where is the content where's good quality content and as a non-native",
    "start": "1843840",
    "end": "1850320"
  },
  {
    "text": "speaker you have no clue about all the kind of cultural aspects or the social aspects of a new country so yeah",
    "start": "1850320",
    "end": "1856559"
  },
  {
    "text": "involving a lot of people is so important yeah so for example with big science which is this effort with 700",
    "start": "1856559",
    "end": "1861760"
  },
  {
    "text": "researchers i was talking about we are targeting seven languages i think i'm not that involved in the project but",
    "start": "1861760",
    "end": "1867919"
  },
  {
    "text": "between those languages i think there was french german and italian and something that they did was a data set",
    "start": "1867919",
    "end": "1874080"
  },
  {
    "text": "catalog so even before creating any data set they just just explored many many many potential sources of data so news",
    "start": "1874080",
    "end": "1881919"
  },
  {
    "text": "places they then analyze a bit the licensing or data quality and if there",
    "start": "1881919",
    "end": "1887120"
  },
  {
    "text": "were tools to access this data from the publisher of this data and this was a distributed effort because of course for",
    "start": "1887120",
    "end": "1892960"
  },
  {
    "text": "each of these languages there was a group of native speakers and people that were living in those countries that were exploring which are the best sources for",
    "start": "1892960",
    "end": "1899360"
  },
  {
    "text": "this data and then we collected it and then we did the all the next relevant parts but as you were saying really",
    "start": "1899360",
    "end": "1904880"
  },
  {
    "text": "finding the right sources for these languages needs to involve people from these languages so if there's people",
    "start": "1904880",
    "end": "1910159"
  },
  {
    "text": "listening who want to get involved in this is there any way to get started you mean to big science yeah or any other",
    "start": "1910159",
    "end": "1915679"
  },
  {
    "text": "projects which you can recommend people to check out help democratize in our systems yeah so from the big science effort you",
    "start": "1915679",
    "end": "1922000"
  },
  {
    "text": "can just search big science for that project in particular you need to be a researcher so you need to be either at a",
    "start": "1922000",
    "end": "1927600"
  },
  {
    "text": "company or at a university but really there are many many ways in which you can contribute as a first step i always",
    "start": "1927600",
    "end": "1933760"
  },
  {
    "text": "recommend just go to the hub and cib which are the data sets available because there are 3000 data sets see",
    "start": "1933760",
    "end": "1939039"
  },
  {
    "text": "which are the models available and many many times if you're already doing machine learning you can potentially really open source your work right of",
    "start": "1939039",
    "end": "1945600"
  },
  {
    "text": "course it really varies of what you are doing within the machine learning ecosystem but there are many things that you can do to contribute back to the",
    "start": "1945600",
    "end": "1951919"
  },
  {
    "text": "community and everything we do is open source so if you're interested in data sales or in transformers you can go to",
    "start": "1951919",
    "end": "1957440"
  },
  {
    "text": "this libraries and contribute with a new model architecture or a new data set if that's something you would like so yeah",
    "start": "1957440",
    "end": "1962559"
  },
  {
    "text": "i think there are many potential venues in which you can contribute to the ecosystem yeah sounds good thank you",
    "start": "1962559",
    "end": "1967600"
  },
  {
    "text": "very much for this talk enjoy cucumber have a good day thank you and thank you for the invitation that was the",
    "start": "1967600",
    "end": "1972880"
  },
  {
    "text": "interview with omar thanks again omar for participating i hope you enjoyed listening to this",
    "start": "1972880",
    "end": "1978080"
  },
  {
    "text": "in-person interview recorded at qcon london and thank you very much for listening to the info-cute podcast",
    "start": "1978080",
    "end": "1985799"
  },
  {
    "text": "[Music]",
    "start": "1989330",
    "end": "2005339"
  },
  {
    "text": "you",
    "start": "2005600",
    "end": "2007679"
  }
]