[
  {
    "text": "so I'm going to talk today about uh machine learning at Uber um and there's sort of three phases of the talk the",
    "start": "4440",
    "end": "10480"
  },
  {
    "text": "first one is to go over um some of the interesting sort of use cases of of ml at Uber um second piece is around uh",
    "start": "10480",
    "end": "17160"
  },
  {
    "text": "looking at the sort of first version of the platform that we built to support those use cases many more and then uh",
    "start": "17160",
    "end": "23160"
  },
  {
    "text": "the final section is is kind of more tailored to this uh this track which is around um kind of developer experience",
    "start": "23160",
    "end": "28960"
  },
  {
    "text": "and how we're working right now to um kind of accelerate machine learning um",
    "start": "28960",
    "end": "34239"
  },
  {
    "text": "uh usage and adoption and and and and Innovation at Uber um through better tooling and better experience um and to",
    "start": "34239",
    "end": "41280"
  },
  {
    "text": "end all right so first thing ml at Uber um uh Uber I think is one of the most exciting places right now to do machine",
    "start": "41280",
    "end": "46920"
  },
  {
    "text": "learning for a bunch of reasons um you know first is that uh you know there's not sort of one or two big use cases",
    "start": "46920",
    "end": "53559"
  },
  {
    "text": "that consume all of the ml attention horsepower um uh there's a wide a wide",
    "start": "53559",
    "end": "58879"
  },
  {
    "text": "array of of of projects of more equal weight across the whole company and and we'll go through a bunch of those um",
    "start": "58879",
    "end": "65960"
  },
  {
    "text": "second one is uh the um sort of interestingness of the data like uber is",
    "start": "65960",
    "end": "71040"
  },
  {
    "text": "you know Uber operates in the physical world all the the rider partner and and Driver apps um uh you know have GPS's",
    "start": "71040",
    "end": "77439"
  },
  {
    "text": "they have accelerometers um and we collect a lot of interesting data about the physical world and of course the cars you know move around the physical world and so um we're not just dealing",
    "start": "77439",
    "end": "84600"
  },
  {
    "text": "with people clicking on web pages we're dealing with with things out there in the world um the third thing is you know",
    "start": "84600",
    "end": "90119"
  },
  {
    "text": "Uber is a younger company and so in a lot of cases uh you know we're applying machine learning in areas for the very",
    "start": "90119",
    "end": "95799"
  },
  {
    "text": "first time and so you're not trying to you know um uh grind out a few extra",
    "start": "95799",
    "end": "101280"
  },
  {
    "text": "fractions of a percent of accuracy you're actually seeing you know giant swings the first time you get a new model deployed in production for some",
    "start": "101280",
    "end": "106759"
  },
  {
    "text": "use case um and the final one is that ml is really Central and strategic to Uber at this point um you know the the",
    "start": "106759",
    "end": "113240"
  },
  {
    "text": "decisions and features that we can um we can base on the data that we collect um you know very very hard to copy um then",
    "start": "113240",
    "end": "120119"
  },
  {
    "text": "also you know ml is is one of the things that helps Uber um kind of run the whole machine uh more efficiently um it's",
    "start": "120119",
    "end": "126240"
  },
  {
    "text": "applied lots of places to to make the product and a lot of the internal operations um run much much more",
    "start": "126240",
    "end": "132120"
  },
  {
    "text": "efficiently all right so data Uber you know data is I mean Uber has grown a lot in the last um bunch of years uh we have",
    "start": "132120",
    "end": "138400"
  },
  {
    "text": "75 million Riders now 3 million drivers um completed four billion trips um uh",
    "start": "138400",
    "end": "144920"
  },
  {
    "text": "last year and so it's even bigger this year um we operate in 600 cities and we're completing you know more than a",
    "start": "144920",
    "end": "150760"
  },
  {
    "text": "million 15 million trips um every single day um and to give you a sense again this is several years ago so things have",
    "start": "150760",
    "end": "157000"
  },
  {
    "text": "grown a lot since then but this is sort of GPS traces of uh the driver phones um in London over the course of I think six",
    "start": "157000",
    "end": "163720"
  },
  {
    "text": "hours but you can see how you know the cars very quickly cover a lot of the city um and this is all data that we can",
    "start": "163720",
    "end": "170000"
  },
  {
    "text": "use for for machine learning um and so this is you know",
    "start": "170000",
    "end": "175560"
  },
  {
    "text": "there's I think over 100 um sort of ml use cases are problems being solved at",
    "start": "175560",
    "end": "180760"
  },
  {
    "text": "Uber right now so this is a a small sampling of ones um but you can see it really cuts across the whole company from Uber Eats and we'll talk about that",
    "start": "180760",
    "end": "187239"
  },
  {
    "text": "one in more depth um to self-driving cars to customer support um pricing uh",
    "start": "187239",
    "end": "193440"
  },
  {
    "text": "forecasting and then even things um kind of more removed from the product um like doing an alal detection on on system",
    "start": "193440",
    "end": "200120"
  },
  {
    "text": "metrics and and uh you know on the backend services and then even like capacity planning our data centers to",
    "start": "200120",
    "end": "205480"
  },
  {
    "text": "make sure we have you know adequate Hardware capacity for both a longterm as well as as shorter term spikes that we",
    "start": "205480",
    "end": "211159"
  },
  {
    "text": "get on on big holidays like New Year's Eve and and and Halloween so here kind of walk through a",
    "start": "211159",
    "end": "218120"
  },
  {
    "text": "few interesting ones so Uber Eats every time you open the Uber Eats app we score um I think hundreds of",
    "start": "218120",
    "end": "224720"
  },
  {
    "text": "different models to generate the homepage for you um so uh we use models to try to figure out which restaurants",
    "start": "224720",
    "end": "230799"
  },
  {
    "text": "you're most likely interested in ordering from and so we do ranking of restaurants within some of the screens",
    "start": "230799",
    "end": "236040"
  },
  {
    "text": "there's actually meals and so we rank the meals again uh trying to see which on you're more likely to want all of the",
    "start": "236040",
    "end": "242560"
  },
  {
    "text": "delivery times that you can see below the below the restaurant um are ml models trying to predict you know how",
    "start": "242560",
    "end": "249079"
  },
  {
    "text": "long will it take once you place the order for the order to get prepared for them to notify the driver partner to",
    "start": "249079",
    "end": "255120"
  },
  {
    "text": "drive to the restaurant to get out of the car and walk into the restaurant to pick up the meal walk back to the car and then drive it to your house and so",
    "start": "255120",
    "end": "260880"
  },
  {
    "text": "ml is used um you know to to model that whole problem and give you know pretty good um etas for delivery time for um",
    "start": "260880",
    "end": "268400"
  },
  {
    "text": "for the meals and then finally search ranking when you try to search for a meal it will again not just do prefix",
    "start": "268400",
    "end": "274080"
  },
  {
    "text": "based searching but also try to predict um your intent and and what you're looking for and this is you know this is you",
    "start": "274080",
    "end": "280120"
  },
  {
    "text": "know an AB test this this makes a big difference for Uber's business um self-driving cars uh you know um you",
    "start": "280120",
    "end": "286400"
  },
  {
    "text": "know the cars have to use you know they have lar and cameras they try to understand the world around them um and",
    "start": "286400",
    "end": "291440"
  },
  {
    "text": "they also uh so they use ml for for that for object detection um trying to find where the streets go looking out for",
    "start": "291440",
    "end": "297199"
  },
  {
    "text": "pedestrians at their cars and then also um other parts in the process for for planning um and Route finding and so",
    "start": "297199",
    "end": "302800"
  },
  {
    "text": "forth um and the cars are are mostly deeping these days um etas um so etas",
    "start": "302800",
    "end": "308680"
  },
  {
    "text": "are the you know in the app when you request a ride or are about to request a ride it tells you how far away the",
    "start": "308680",
    "end": "313800"
  },
  {
    "text": "driver is um and this is uh super important both for the product experience for our users because you",
    "start": "313800",
    "end": "319560"
  },
  {
    "text": "know if the ETA is incorrect it's quite frustrating and it may sort of um affect how you use it but it's also fed into um",
    "start": "319560",
    "end": "325199"
  },
  {
    "text": "lots and lots of other internal systems it drives pricing and routing and a bunch of other things and so accurate etas is super super important to Uber",
    "start": "325199",
    "end": "332080"
  },
  {
    "text": "and and it's a it's a hard problem um and so Uber for a long time has had a route based uh um you know ETA predictor",
    "start": "332080",
    "end": "340199"
  },
  {
    "text": "that will look at the segments of the road you're going to travel over and you know average uh you know travel you know average uh speeds over over in the past",
    "start": "340199",
    "end": "347639"
  },
  {
    "text": "and it will use that to predict kind of a base ETA um what we found is those etas are are are usually you know wrong",
    "start": "347639",
    "end": "353600"
  },
  {
    "text": "to some degree but they're wrong in in consistent ways or or predictable ways and so we can fit models to to the error",
    "start": "353600",
    "end": "360280"
  },
  {
    "text": "and then use the prediction to correct the error and and give um you know dramatically more accurate etas across",
    "start": "360280",
    "end": "365759"
  },
  {
    "text": "the board um mapmaking you know Uber you know used to use Google Maps and now",
    "start": "365759",
    "end": "371720"
  },
  {
    "text": "we're building out our own mapping infrastructure and as part of the mapmaking process there's sort of a layering of evidence collection you",
    "start": "371720",
    "end": "377680"
  },
  {
    "text": "start with a a base um uh you know Street um map and then you layer on",
    "start": "377680",
    "end": "383000"
  },
  {
    "text": "evidence to make it more and more accurate and so one of the things we do is we drive cars around with cameras on top and take pictures of all of the",
    "start": "383000",
    "end": "388520"
  },
  {
    "text": "buildings and street signs and then use and and also tag those with the GPS coordinates of where the picture was taken from and then use ml models to",
    "start": "388520",
    "end": "396319"
  },
  {
    "text": "um to try to find uh um you know addresses and street signs such that we",
    "start": "396319",
    "end": "401919"
  },
  {
    "text": "can uh um you add them to the database and help to um you know make the overall",
    "start": "401919",
    "end": "407120"
  },
  {
    "text": "the the map itself more accurate and consistent so you you get sort of a a base map and you layer on evidence that",
    "start": "407120",
    "end": "412960"
  },
  {
    "text": "we collect you know with sensors and cars and machine learning to um to actually find first first we' figure out",
    "start": "412960",
    "end": "419039"
  },
  {
    "text": "the um the objects we're interested in and so in this case you can see you know street signs and addresses and then we",
    "start": "419039",
    "end": "425120"
  },
  {
    "text": "apply um text extraction algorithms to actually pull the text out of the image and then and then the actual text",
    "start": "425120",
    "end": "430919"
  },
  {
    "text": "whether it's an address or a street sign or a you know restaurant name can be fed into the",
    "start": "430919",
    "end": "436199"
  },
  {
    "text": "database um destination prediction when you open the app and you are starting to search for where you want to go um ml",
    "start": "436199",
    "end": "442039"
  },
  {
    "text": "again is used um you know like in the E case to try to help you know help help you you know help help you find the place you want to go",
    "start": "442039",
    "end": "450160"
  },
  {
    "text": "um on in forecasting in Marketplace um you know Uber is a Marketplace we try to connect Riders and drivers um for rides",
    "start": "451360",
    "end": "458879"
  },
  {
    "text": "and it's you know for the thing to work it's very important that the Riders and drivers you know be close to each other in both space and time if you request a",
    "start": "458879",
    "end": "465680"
  },
  {
    "text": "ride and the driver is very very far away across the city it doesn't work because it takes too long to drive across the city to get to you um if you",
    "start": "465680",
    "end": "471840"
  },
  {
    "text": "request a ride and there's no drivers available um even ones that are close doesn't work either and so the the sort of proximity and space and time of",
    "start": "471840",
    "end": "478080"
  },
  {
    "text": "supply and demand is quite important um and you you can sort of contrast that with a business like eBay which is also Marketplace but you can you know you can",
    "start": "478080",
    "end": "484960"
  },
  {
    "text": "order a futon today from LA and they can ship it next week and and that all works out even though the distance and time are spread out but for Uber the the the",
    "start": "484960",
    "end": "492120"
  },
  {
    "text": "sort of spatial temporal thing is quite important and so um uh and Uber's maps you can see the little hexagons there we",
    "start": "492120",
    "end": "498680"
  },
  {
    "text": "we divide up the maps into hexagons it's a more efficient way than than a grid to to organize Maps but we use deep",
    "start": "498680",
    "end": "504199"
  },
  {
    "text": "learning models to predict a variety of of marketplace metrics um at various points of time in the future um so uh",
    "start": "504199",
    "end": "511159"
  },
  {
    "text": "you know drivers who will be available Riders who will want rides and then can um identify um uh um you know gaps",
    "start": "511159",
    "end": "519000"
  },
  {
    "text": "between supply and demand in the future and then use that to help um uh encourage drivers to go where there will",
    "start": "519000",
    "end": "524720"
  },
  {
    "text": "be um demand to help again keep etas low and and utilization",
    "start": "524720",
    "end": "530080"
  },
  {
    "text": "High um customer support uh you know there are 15 million rides a day people",
    "start": "530080",
    "end": "535279"
  },
  {
    "text": "leave phones and backpacks in the back of cars and they file customer support tickets and those get routed to call",
    "start": "535279",
    "end": "540680"
  },
  {
    "text": "centers and Uber spend lots and lots of money for people to answer support tickets and what happens when a ticket",
    "start": "540680",
    "end": "545720"
  },
  {
    "text": "comes in is the person has to read the ticket figure out what the problem is and then pick from a big menu of of",
    "start": "545720",
    "end": "551000"
  },
  {
    "text": "responses um you know for the prop for the proper response for lost and found or whatever else um we can use deep",
    "start": "551000",
    "end": "556600"
  },
  {
    "text": "learning models um looking at the text of the message to try to predict what the actual problem was and then reduce",
    "start": "556600",
    "end": "562519"
  },
  {
    "text": "the um menu options from I think 30 down to three like the three most likely response templates and so you know that",
    "start": "562519",
    "end": "568880"
  },
  {
    "text": "gave I think initially a 10% boost but I think we have another model which gave another another 6% boost in in the speed at",
    "start": "568880",
    "end": "575240"
  },
  {
    "text": "which um these people can answer tickets which is you know 16% off of the cost of the of the thing which is huge for",
    "start": "575240",
    "end": "581480"
  },
  {
    "text": "us um and then actually another one that's quite similar although different application is uh this this new oneclick",
    "start": "581480",
    "end": "589360"
  },
  {
    "text": "Chat thing that we released recently and the idea here is that when um when when a car is coming to pick you up you often",
    "start": "589360",
    "end": "595680"
  },
  {
    "text": "want to communicate with a driver to tell you him or her you know exactly where you're standing or if you're you know running down the block um but it's",
    "start": "595680",
    "end": "602480"
  },
  {
    "text": "hard for drivers to to type um and it's sort of easier to chat and so we have a",
    "start": "602480",
    "end": "607959"
  },
  {
    "text": "um an NLP model that basically predicts the likely response um the next response in a conversation and so you can very",
    "start": "607959",
    "end": "613560"
  },
  {
    "text": "quickly communicate with the driver um and vice versa via just picking um responses uh out of menu as opposed to",
    "start": "613560",
    "end": "620160"
  },
  {
    "text": "typing um I forget the exact accuracy rate but it's but it's quite high and you're able to carry on pretty good conversations without actually typing um",
    "start": "620160",
    "end": "626880"
  },
  {
    "text": "any text which is pretty cool all right um so that's you know that was",
    "start": "626880",
    "end": "632959"
  },
  {
    "text": "like 10 or something out of you know close to 100 different use cases around uber where ml is being used um and so",
    "start": "632959",
    "end": "638480"
  },
  {
    "text": "over the last three years we built a platform called michelangela which supports um you know the majority of those use cases um and so talk a bit now",
    "start": "638480",
    "end": "645600"
  },
  {
    "text": "about sort of the philosophy of the platform and and sort of the first version and what it covers so the you know overall mission",
    "start": "645600",
    "end": "652240"
  },
  {
    "text": "of my team is to you know build software and tools that will enable data scientists and Engineers around the company you know to you know kind of own",
    "start": "652240",
    "end": "659360"
  },
  {
    "text": "the end to end to you know deploy and operate these ml solutions that we just saw before um and to do it sort of at",
    "start": "659360",
    "end": "665959"
  },
  {
    "text": "full Uber scale and there's a big sort of Dev experience component to that because you want to empower um the same",
    "start": "665959",
    "end": "671519"
  },
  {
    "text": "person to own you know own the end to end you know from the mod from the you know idea and the prototyping the model",
    "start": "671519",
    "end": "676680"
  },
  {
    "text": "all the way through deployment and production you know the more you can have one person own that process um you know the the the the faster uh um you",
    "start": "676680",
    "end": "683920"
  },
  {
    "text": "can move through it and and you know modeling work being very iterative um you know the faster you can move move through things a compounding effect",
    "start": "683920",
    "end": "690240"
  },
  {
    "text": "because there are lots of Cycles as you experiment with new models um this is we had a blog post",
    "start": "690240",
    "end": "696560"
  },
  {
    "text": "recently in addition to technology there's been a lot of kind of organizational and process um aspects to ml Uber um that have been quite",
    "start": "696560",
    "end": "702680"
  },
  {
    "text": "important in in making it work well um at scale in terms of the the system scale but also the organizational scale",
    "start": "702680",
    "end": "708320"
  },
  {
    "text": "and there's a blog post we put out recently that that describes a bunch of this all right so um you know V1 of of",
    "start": "708320",
    "end": "715800"
  },
  {
    "text": "ml at Uber was um really just to enable people to do it um and and that's been",
    "start": "715800",
    "end": "720959"
  },
  {
    "text": "quite successful and Powerful but it's uh you know it wasn't always the easiest thing and so as we look at um you know",
    "start": "720959",
    "end": "727279"
  },
  {
    "text": "V2 was is more around how do we improve developer um productivity and and experience and and increase uh sort of",
    "start": "727279",
    "end": "733600"
  },
  {
    "text": "velocity of of modeling work and deployment work um again to facilitate um kind of",
    "start": "733600",
    "end": "738839"
  },
  {
    "text": "innovation all right so this is um a walkthrough of the platform um sort of the first version of the platform and",
    "start": "738839",
    "end": "743959"
  },
  {
    "text": "then we'll talk about the things we're doing now to to make it better and faster",
    "start": "743959",
    "end": "749399"
  },
  {
    "text": "so one of the early um kind of hypotheses that we had or or or or Vision around the platform was that you",
    "start": "750120",
    "end": "755560"
  },
  {
    "text": "know machine learning is much more than just training models um that there's a whole end-end workflow that you have to support to make it to make it actually",
    "start": "755560",
    "end": "760880"
  },
  {
    "text": "work well and it starts with uh managing data and this you know actually in most cases ends up being the most complicated",
    "start": "760880",
    "end": "767320"
  },
  {
    "text": "part of the of the process um you have to manage the you know data sets that",
    "start": "767320",
    "end": "772440"
  },
  {
    "text": "use for training the model which is the the features and the labels um and and you know it has to be accurate you have",
    "start": "772440",
    "end": "778639"
  },
  {
    "text": "to be able to manage it for training retraining and then when you deploy the model you have to get that same data um",
    "start": "778639",
    "end": "784519"
  },
  {
    "text": "to the model in production and at Uber most models are deployed into um you know a real-time prediction service for",
    "start": "784519",
    "end": "790880"
  },
  {
    "text": "request response based predictions and so in many times you know the data that you need for the model is sitting in h",
    "start": "790880",
    "end": "796519"
  },
  {
    "text": "dupe somewhere and so you have to wire up the pipelines for running kind of analytical queries against historical",
    "start": "796519",
    "end": "801560"
  },
  {
    "text": "data and then delivering that into a key Value Store where the model can read it and so a lot of kind of complicated",
    "start": "801560",
    "end": "807120"
  },
  {
    "text": "pipelines um for for getting the right right data delivered to the right time um in place uh for the model to use it",
    "start": "807120",
    "end": "812959"
  },
  {
    "text": "at scoring time um training models obviously you have to actually train the models and that's you know we we do a",
    "start": "812959",
    "end": "818199"
  },
  {
    "text": "bunch there U model evaluation um you you you know modeling work is very iterative and so you want to be able to",
    "start": "818199",
    "end": "824079"
  },
  {
    "text": "have good tools for comparing models and finding out which ones are are good or not um deployment you know once you have",
    "start": "824079",
    "end": "829440"
  },
  {
    "text": "a model that you like you want to be able to click a button or call an API and have it deployed out across um your",
    "start": "829440",
    "end": "834560"
  },
  {
    "text": "serving infrastructure and then making predictions that's the obvious part and then monitoring is interesting um and",
    "start": "834560",
    "end": "841519"
  },
  {
    "text": "that you you know you you train models against historical data you evaluate against historical data and then when",
    "start": "841519",
    "end": "846920"
  },
  {
    "text": "you are when you deploy model in production you then don't actually know if it's doing the right thing anymore because you're seeing new data against",
    "start": "846920",
    "end": "852519"
  },
  {
    "text": "the model and so being able to um you know monitor the accuracy of predictions going forward and time becomes quite",
    "start": "852519",
    "end": "857839"
  },
  {
    "text": "important and what we found is that the same workflow you know applies across um you know all sorts of or most of the of",
    "start": "857839",
    "end": "865000"
  },
  {
    "text": "the ml um problems we've seen from traditional you know trees and linear models RS to deep learning um you know",
    "start": "865000",
    "end": "871320"
  },
  {
    "text": "supervised and unsupervised um you know online learning where you're learning more continuously um you know whether",
    "start": "871320",
    "end": "876759"
  },
  {
    "text": "you're deploying a model in a in a batch pipeline or online or on a mobile phone and then even as we saw in that",
    "start": "876759",
    "end": "882360"
  },
  {
    "text": "Marketplace case you know it works for you know classification regression but also for time series forecasting so for",
    "start": "882360",
    "end": "887720"
  },
  {
    "text": "all these things the same basic workflow holds true um and so we spent time building out you know platform to",
    "start": "887720",
    "end": "893360"
  },
  {
    "text": "support to support these things all right so managing data I hit a bit of this before already but um you",
    "start": "893360",
    "end": "899600"
  },
  {
    "text": "know most cases data is the hardest part of ML and um we've built a variety of things including a centralized feature",
    "start": "899600",
    "end": "906079"
  },
  {
    "text": "store where teams can can register and curate and share features that are used across different models um and that you",
    "start": "906079",
    "end": "912639"
  },
  {
    "text": "know that that uh uh facilitates modeling work because rather than having to write new queries to find your features you can just pick and choose",
    "start": "912639",
    "end": "918519"
  },
  {
    "text": "them from a feature store and then you know more important as importantly you know once your model goes into",
    "start": "918519",
    "end": "923759"
  },
  {
    "text": "production we can automatically wire up the pipeline to deliver those features uh to the model at prediction time",
    "start": "923759",
    "end": "930959"
  },
  {
    "text": "uh training models uh you know we run large scale distributed uh training for both uh um on CPU clusters for trees and",
    "start": "931600",
    "end": "940040"
  },
  {
    "text": "linear models and then on on GPU clusters for um for deep learning",
    "start": "940040",
    "end": "946160"
  },
  {
    "text": "models um in the case of deep learning um you know we base a lot of it around uh tensor flow and and pytorch but we",
    "start": "946639",
    "end": "953800"
  },
  {
    "text": "built our own distributed training infrastructure called horovod I won't go into too much detail here but and watch",
    "start": "953800",
    "end": "959120"
  },
  {
    "text": "back to this in the experience section but horod has two interesting aspects one is that it makes distributed training um more efficient by getting",
    "start": "959120",
    "end": "965880"
  },
  {
    "text": "rid of the parameter server and and using a different technique involving MPI and ring reduction to more",
    "start": "965880",
    "end": "971639"
  },
  {
    "text": "efficiently um uh uh you know uh Shuffle data around during distributed training",
    "start": "971639",
    "end": "977120"
  },
  {
    "text": "but it also um makes the the apis for managing the distributed training jobs are much much easier for the modeling um",
    "start": "977120",
    "end": "984360"
  },
  {
    "text": "developers and we'll come back to that later so it's it's quite strong in terms of of scale and and and speed but also",
    "start": "984360",
    "end": "990680"
  },
  {
    "text": "much much easier to use managing eval models again after you",
    "start": "990680",
    "end": "996319"
  },
  {
    "text": "you train models um you know you often train you know tens or hundreds tens or hundreds of models before you find one",
    "start": "996319",
    "end": "1002560"
  },
  {
    "text": "that's that's sort of good enough for your use case and so being able to keep a rigorous uh um recording of all the",
    "start": "1002560",
    "end": "1009560"
  },
  {
    "text": "models you train the training data who train them as well as a lot of uh metrics and reports around um you know",
    "start": "1009560",
    "end": "1016639"
  },
  {
    "text": "accuracy of the model and even debug reports helps the modelers um you know iterate and eventually find the model",
    "start": "1016639",
    "end": "1022920"
  },
  {
    "text": "that they that they want to use in production and so we invest a lot of work here in and and sort of you know",
    "start": "1022920",
    "end": "1028160"
  },
  {
    "text": "collecting metadata about the models and then and then exposing it in ways that are very easy for developers to make",
    "start": "1028160",
    "end": "1033480"
  },
  {
    "text": "sense of and and move the modeling process forward and so we have um this is for a um for a uh um a regression",
    "start": "1033480",
    "end": "1040839"
  },
  {
    "text": "model and so there's the standard kind of error metrics as well as reporting to show um you know kind of the accuracy of the model sort of very standard things",
    "start": "1040839",
    "end": "1047319"
  },
  {
    "text": "that data scientists are are used to doing um for a classification model again different set of metrics but again the",
    "start": "1047319",
    "end": "1052720"
  },
  {
    "text": "things that that people need to use to hone in on on the best model for their for their use case um and then you know",
    "start": "1052720",
    "end": "1059440"
  },
  {
    "text": "for all of the different features that go into the model we look at the importance of the feature to the model as well as um statistics about that data",
    "start": "1059440",
    "end": "1067400"
  },
  {
    "text": "so the you know the the mean the Min the max standard deviation as well as distribution um again all things that",
    "start": "1067400",
    "end": "1072880"
  },
  {
    "text": "help you understand um the data in the model um and accelerate the the work here",
    "start": "1072880",
    "end": "1079720"
  },
  {
    "text": "and this is a uh for for tree models we expose a a tool that lets you actually dig into the structure of the Learned",
    "start": "1079720",
    "end": "1085840"
  },
  {
    "text": "trees um to help understand you know how the model works and and to help explain",
    "start": "1085840",
    "end": "1090919"
  },
  {
    "text": "you know why a certain um set of of input features generates a certain prediction and so um you know across the",
    "start": "1090919",
    "end": "1096640"
  },
  {
    "text": "top you can see this is a boosted tree with can't each each column in that top grid is a is One Tree in the forest um",
    "start": "1096640",
    "end": "1103679"
  },
  {
    "text": "each row is a feature um and then as you click on a tree you'll see the tree at the bottom with the with the all the",
    "start": "1103679",
    "end": "1109480"
  },
  {
    "text": "split points and distributions and then you can actually um fill in data on the left there and it will it will light up",
    "start": "1109480",
    "end": "1116120"
  },
  {
    "text": "the path through the tree so you can see how the tree handles that that feature Vector so again if a model's not behaving correctly you can pull up the",
    "start": "1116120",
    "end": "1122480"
  },
  {
    "text": "screen and figure out exactly why the model is generating is generating a certain prediction for a certain um",
    "start": "1122480",
    "end": "1127679"
  },
  {
    "text": "input set of of of features and then deployment and serving",
    "start": "1127679",
    "end": "1134360"
  },
  {
    "text": "so once you've um once you've you know found the model that you want it's important to be able to deploy it um",
    "start": "1134360",
    "end": "1140159"
  },
  {
    "text": "Uber does both uh batch predictions meaning you you run a job once a day or once an hour to generate lots and lots",
    "start": "1140159",
    "end": "1145440"
  },
  {
    "text": "of predictions or you can deploy a model um into a essentially web service so a",
    "start": "1145440",
    "end": "1150880"
  },
  {
    "text": "container that will receive um you know Network requests and then return predictions and and you know most models",
    "start": "1150880",
    "end": "1156919"
  },
  {
    "text": "at Uber and a lot of the ones that I showed before um are all of that nature so you open your your Uber Eats app and",
    "start": "1156919",
    "end": "1163159"
  },
  {
    "text": "it calls the backend services and it will score a bunch of models to render to render your homepage um you know in whatever it is",
    "start": "1163159",
    "end": "1169480"
  },
  {
    "text": "milliseconds um and so we have built and operate uh the prediction clusters that",
    "start": "1169480",
    "end": "1174720"
  },
  {
    "text": "scale out um and are used across the company",
    "start": "1174720",
    "end": "1179919"
  },
  {
    "text": "um and kind of a quick architectural diagram but the idea is from the client sends the the feature Vector in we have",
    "start": "1179919",
    "end": "1185600"
  },
  {
    "text": "a kind of routing infrastructure and then within the model or sorry within the prediction service you can have multiple models um loaded and So based",
    "start": "1185600",
    "end": "1192400"
  },
  {
    "text": "on a on a header it will go find the right model um send the feature Vector to that model get the prediction back in",
    "start": "1192400",
    "end": "1198120"
  },
  {
    "text": "some cases load more data from Cassandra which is the feature St we talked about um and then return the prediction back",
    "start": "1198120",
    "end": "1203400"
  },
  {
    "text": "to the client and I think right now we're running close to a million predictions a second across all the different use cases at Uber which is",
    "start": "1203400",
    "end": "1210159"
  },
  {
    "text": "quite a bit all right um so yeah so the yeah we're at 1",
    "start": "1210159",
    "end": "1216480"
  },
  {
    "text": "million plus and then um you know in for for trees and linear models the the scoring time is quite fast um I think",
    "start": "1216480",
    "end": "1223520"
  },
  {
    "text": "typically we're we're less than 5 milliseconds for P95 I think it is for if if there's no um if if there's no",
    "start": "1223520",
    "end": "1229880"
  },
  {
    "text": "Cassandra in the path for the for the online features and then when you have to call Cassandra to get features it adds another you know five or 10 or 20",
    "start": "1229880",
    "end": "1236200"
  },
  {
    "text": "milliseconds so all in all is still quite fast for predictions which is good we're starting to work on more deep",
    "start": "1236200",
    "end": "1241480"
  },
  {
    "text": "loaring models and those are trickier because depending on the complexity of the model the inference time can actually go up quite a bit um but for",
    "start": "1241480",
    "end": "1246919"
  },
  {
    "text": "trees it's it's it's it's usually very very fast um and the final bit I talked about",
    "start": "1246919",
    "end": "1253120"
  },
  {
    "text": "is you've you know trained against historical data and now you deploy your model production and you want to make sure that it's you know when you",
    "start": "1253120",
    "end": "1259760"
  },
  {
    "text": "evaluate it against historical data you know your model was good for last week's data but now it's running production you",
    "start": "1259760",
    "end": "1264960"
  },
  {
    "text": "want to make sure it's actually good for you know for the data that you're seeing right now and um uh so we can do and and we'll",
    "start": "1264960",
    "end": "1274039"
  },
  {
    "text": "come back to this at the end there's another kind of newer piece of this but there's a few different ways you can you know monitor uh your predictions um the",
    "start": "1274039",
    "end": "1281880"
  },
  {
    "text": "the ideal way is where you can actually log the predictions that you make and then join them back to um the uh",
    "start": "1281880",
    "end": "1288640"
  },
  {
    "text": "outcomes That You observe um as part of the running of the system um later on and then see how you know see whether",
    "start": "1288640",
    "end": "1293679"
  },
  {
    "text": "you got the the prediction right or wrong and so you can imagine you know for the Uber Eats case we predict the",
    "start": "1293679",
    "end": "1299480"
  },
  {
    "text": "ETA for a certain restaurant and you order the meal and then 20 minutes later it delivers and then we know the actual",
    "start": "1299480",
    "end": "1305440"
  },
  {
    "text": "um arrival time from that meal and that's collected to one of our backend systems and if we log the prediction that we made for your um when you viewed",
    "start": "1305440",
    "end": "1312200"
  },
  {
    "text": "the screen and then join that back to the actual delivery time we can see you know how right or wrong that prediction",
    "start": "1312200",
    "end": "1318000"
  },
  {
    "text": "was and you col those in Aggregate and then you can generate um very accurate um you know ongoing accuracy reports um",
    "start": "1318000",
    "end": "1324240"
  },
  {
    "text": "for your model in production and uh this one you know because you have to wait for batch",
    "start": "1324240",
    "end": "1330400"
  },
  {
    "text": "processes to run to collect the outcomes you can get good monitoring but there's I think an hour delay before you can you",
    "start": "1330400",
    "end": "1335720"
  },
  {
    "text": "know know know you know how correct the prediction",
    "start": "1335720",
    "end": "1339880"
  },
  {
    "text": "was so then from an architecture perspective you know again walking through you know along the top you have",
    "start": "1341559",
    "end": "1346600"
  },
  {
    "text": "the the different um workflow steps and then the bottom you have both our offline batch systems at the bottom and",
    "start": "1346600",
    "end": "1353039"
  },
  {
    "text": "then our online systems at the top um and we'll kind of just kind of walk through the stages of of the architecture um and so in the offline",
    "start": "1353039",
    "end": "1359919"
  },
  {
    "text": "world we start in the lower left with our data Lake and so all of Uber's data funnels into Hadoop you know Hadoop And",
    "start": "1359919",
    "end": "1365520"
  },
  {
    "text": "Hive tables and that's the starting point for most um you know mo most batch data data work um was part of the ml",
    "start": "1365520",
    "end": "1372480"
  },
  {
    "text": "platform we let developers write either spark or um or SQL jobs to to do the",
    "start": "1372480",
    "end": "1378200"
  },
  {
    "text": "kind of core scin you know joining and aggregation and collection of feature data um and outcome data and then those",
    "start": "1378200",
    "end": "1383799"
  },
  {
    "text": "are are fed back into to Hive tables that are used for for training batch prediction and then in cases uh where",
    "start": "1383799",
    "end": "1391520"
  },
  {
    "text": "you want those features available online for prediction time um th those those values that were calculated in those",
    "start": "1391520",
    "end": "1398000"
  },
  {
    "text": "those um batch jobs can be copied into Cassandra for online serving and so for example um in the Uber Eats uh delivery",
    "start": "1398000",
    "end": "1406159"
  },
  {
    "text": "time case you know one of the features is is something like you know what's the average meal prep time for a restaurant",
    "start": "1406159",
    "end": "1411679"
  },
  {
    "text": "over the last two weeks and um and so that's computed via you know a spark job",
    "start": "1411679",
    "end": "1417400"
  },
  {
    "text": "and because it's a two we average it's kind of okay if if that only gets refresh in Cassandra once or twice a day",
    "start": "1417400",
    "end": "1422559"
  },
  {
    "text": "because two weeks plus or minus 12 hours doesn't make that much difference for that kind of metric and so that's that one is fine flowing through the bottom",
    "start": "1422559",
    "end": "1428600"
  },
  {
    "text": "batch path it gets computed once a day loaded to Cassandra and then we can use that same value um for for every single",
    "start": "1428600",
    "end": "1434240"
  },
  {
    "text": "prediction um however there are cases where you want you know more you want the features to be a lot fresher and so",
    "start": "1434240",
    "end": "1440559"
  },
  {
    "text": "in addition to the two we meal prep time for restaurant you may also want to know you know which gives you a sense of how",
    "start": "1440559",
    "end": "1445919"
  },
  {
    "text": "you just how fast the restaurant is in general you may also want to know um you know how busy is the restaurant right",
    "start": "1445919",
    "end": "1451760"
  },
  {
    "text": "now so what was the what's the meal prep time over the last you know one hour or last five minutes and obviously if",
    "start": "1451760",
    "end": "1457600"
  },
  {
    "text": "you're Computing things with that freshness you can't afford to go run you know offline jobs and so we have a streaming path across the top or we can",
    "start": "1457600",
    "end": "1463679"
  },
  {
    "text": "get metrics coming out of Kafka we can run you know Flink job to aggregate across the stream of data and then um",
    "start": "1463679",
    "end": "1469720"
  },
  {
    "text": "write uh those numbers into Cassandra and then double write them back to Hive so you have the exact same numbers",
    "start": "1469720",
    "end": "1475799"
  },
  {
    "text": "available later on for training and so sort of the parody between online offline is super important to get right",
    "start": "1475799",
    "end": "1480960"
  },
  {
    "text": "um and the way we've solved that generally is by having like only compute the the um only compute the feature once",
    "start": "1480960",
    "end": "1488480"
  },
  {
    "text": "and then double write it to um the other store uh so then batch training um you",
    "start": "1488480",
    "end": "1496000"
  },
  {
    "text": "know pulls data from uh these you know Hive tables and runs it through the algorithm which could be a tree or",
    "start": "1496000",
    "end": "1501720"
  },
  {
    "text": "linear model or deeping model and then writes the output uh it's actually not",
    "start": "1501720",
    "end": "1506919"
  },
  {
    "text": "cassendra anymore but into a model database that stores you know all the metadata about the model um you know",
    "start": "1506919",
    "end": "1512159"
  },
  {
    "text": "that we talked about before who trained it when it was trained what data set and then as well as all of the actual um uh",
    "start": "1512159",
    "end": "1518320"
  },
  {
    "text": "you know learn parameters the artifacts of the model and so if it's a a tree model it's the all the split points we saw before with The Zing model it's all",
    "start": "1518320",
    "end": "1524640"
  },
  {
    "text": "of the the learn weights um in the network um and so so we capture both the you know all the metadata all the",
    "start": "1524640",
    "end": "1530600"
  },
  {
    "text": "configuration plus the actual parameters of the model and store that in database and then at the at at",
    "start": "1530600",
    "end": "1536799"
  },
  {
    "text": "deployment time um you can click a button or through an API uh uh you know",
    "start": "1536799",
    "end": "1542480"
  },
  {
    "text": "take one of those models you've trained and and push that out into either an online serving container that we talked about before that will do network-based",
    "start": "1542480",
    "end": "1550080"
  },
  {
    "text": "you know request response predictions um or you can deploy it into a um a batch",
    "start": "1550080",
    "end": "1555640"
  },
  {
    "text": "job that will run you know on a Cadence and generate you know lots and lots of predictions and and then send them somewhere",
    "start": "1555640",
    "end": "1562000"
  },
  {
    "text": "else um and then finally if you look at how the predictions actually happen so along the top you know again the the the",
    "start": "1562000",
    "end": "1568760"
  },
  {
    "text": "realtime case imagine you open the Uber Eats app and you want to see your your meal um delivery time estimates um",
    "start": "1568760",
    "end": "1575880"
  },
  {
    "text": "you'll send you know the features coming from the phone would be you know your location time and day a bunch of things that are relevant to the current context",
    "start": "1575880",
    "end": "1582720"
  },
  {
    "text": "and that will go to the model and then um you know the model as part of the configuration knows that in addition to",
    "start": "1582720",
    "end": "1587760"
  },
  {
    "text": "the features that come as part of the current request we have to get a bunch of other ones that are waiting for it in the feature store and so we have the um",
    "start": "1587760",
    "end": "1594760"
  },
  {
    "text": "you know the one or the 1our meal prep time and the twoe meal prep time and probably a bunch of others are pulled",
    "start": "1594760",
    "end": "1599799"
  },
  {
    "text": "out at Cassandra and then join to the features you sent um from the phone and",
    "start": "1599799",
    "end": "1605039"
  },
  {
    "text": "then that whole feature Vector is then sent to the model for for scoring and so you can see we're kind of blending the",
    "start": "1605039",
    "end": "1610320"
  },
  {
    "text": "the the request context features with a bunch that are computed either via streaming jobs or via batch jobs um and",
    "start": "1610320",
    "end": "1616840"
  },
  {
    "text": "again like a lot of the the challenges here are are getting the system set up and and and and in a way that where it's",
    "start": "1616840",
    "end": "1622880"
  },
  {
    "text": "very easy for developers to wire up all these pipelines and not have to do it one off each time because that's where",
    "start": "1622880",
    "end": "1628000"
  },
  {
    "text": "you know without this that's where most of the work in ml goes is getting these data pipelines set up and then for the uh monitoring case",
    "start": "1628000",
    "end": "1635640"
  },
  {
    "text": "um either for uh real time or batch predictions we can you know log the",
    "start": "1635640",
    "end": "1641480"
  },
  {
    "text": "predictions back um to Hadoop and then join them to the outcomes you know once we learn about them as part of the",
    "start": "1641480",
    "end": "1646880"
  },
  {
    "text": "regular processing of of data and then we can you know push those out to metric systems for for alerting and and and and",
    "start": "1646880",
    "end": "1653240"
  },
  {
    "text": "and and monitoring um and again because these are batch jobs you know I think it's we run these things once an hour so",
    "start": "1653240",
    "end": "1658360"
  },
  {
    "text": "it's not super real time um yet but uh we'll come back to that later and then zooming out uh we have a",
    "start": "1658360",
    "end": "1665200"
  },
  {
    "text": "sort of a management plane um that we use for um you know the monitoring we pump to Central monitoring system that",
    "start": "1665200",
    "end": "1671960"
  },
  {
    "text": "that the drive dashboards we have an API tier that uh kind of orchestrates in sort of the brains of the system and",
    "start": "1671960",
    "end": "1677760"
  },
  {
    "text": "then it Al um is a kind of a public API surface for the web UI that's used to",
    "start": "1677760",
    "end": "1683720"
  },
  {
    "text": "doing a lot of the the workflow management and deployment and then uh you can write uh you know python or Java",
    "start": "1683720",
    "end": "1689919"
  },
  {
    "text": "you know automation code or integration code to to drive the system from the",
    "start": "1689919",
    "end": "1694799"
  },
  {
    "text": "outside and I have a quick little video here showing the UI but um you know it's it's organized around projects and these",
    "start": "1695039",
    "end": "1701320"
  },
  {
    "text": "are all kind of dummy names but um a project is a container for modeling problem you can go connect to you know",
    "start": "1701320",
    "end": "1707679"
  },
  {
    "text": "Hive table able um to train your model on um you",
    "start": "1707679",
    "end": "1712960"
  },
  {
    "text": "can let's see here go you know look at all of the models you've trained um we talked about this before um so it's a",
    "start": "1712960",
    "end": "1719600"
  },
  {
    "text": "bunch of boosted tree models you can drill into one of these oops I click",
    "start": "1719600",
    "end": "1727158"
  },
  {
    "text": "something um oops me try that",
    "start": "1727240",
    "end": "1734000"
  },
  {
    "text": "again so projects",
    "start": "1736000",
    "end": "1740039"
  },
  {
    "text": "go grab a hive",
    "start": "1743720",
    "end": "1746600"
  },
  {
    "text": "table um and then drill in I think we're going to drill in and see some of the",
    "start": "1749600",
    "end": "1755600"
  },
  {
    "text": "visualizations or Imports on one of these models so that's one's already deployed",
    "start": "1755600",
    "end": "1761000"
  },
  {
    "text": "we click in you can see that this is a classification model so you can see the the confusion Matrix and a bunch of the different metrics used to assess",
    "start": "1761000",
    "end": "1767279"
  },
  {
    "text": "accuracy this is the the tree thing we saw before that has um uh you know this",
    "start": "1767279",
    "end": "1772679"
  },
  {
    "text": "model has whatever 162 features and a lot of trees and you can see the actual you know split points in",
    "start": "1772679",
    "end": "1778360"
  },
  {
    "text": "the in the trees and then here's the feature report for all the features in this model with the the",
    "start": "1778360",
    "end": "1784360"
  },
  {
    "text": "distributions and so forth and I think we're going to go deploy model here you can see how fast it goes out so you click so this is",
    "start": "1784360",
    "end": "1792000"
  },
  {
    "text": "model's not deployed you click deploy click okay and it spins for a few minutes and sort of packages up the",
    "start": "1792000",
    "end": "1797840"
  },
  {
    "text": "model mod and pushes it out to the serving infrastructure and then boom it's ready to",
    "start": "1797840",
    "end": "1803080"
  },
  {
    "text": "go and then here you can see the you know the history of all the different models you've deployed over time um you",
    "start": "1807039",
    "end": "1812760"
  },
  {
    "text": "know sort of logs of of who deployed and when cool all right so that's the you",
    "start": "1812760",
    "end": "1818440"
  },
  {
    "text": "know that's sort of the V1 of the platform and and this is what's you know we built a last few years to um you know",
    "start": "1818440",
    "end": "1824200"
  },
  {
    "text": "to support mlu cases at scale um you know it's worked well in some is you know things weren't as fast as easy as",
    "start": "1824200",
    "end": "1829720"
  },
  {
    "text": "they could be and and so the next wave of of our efforts on the platform around how do you um how do you think this",
    "start": "1829720",
    "end": "1835399"
  },
  {
    "text": "Foundation that we have now and how do you make it sort of faster and easier for people to go from idea um through",
    "start": "1835399",
    "end": "1841080"
  },
  {
    "text": "prototyping to first model and then deploy that and then sort of scale that model up up into production and we'll go through a few recent projects that we've",
    "start": "1841080",
    "end": "1847679"
  },
  {
    "text": "um either finished building or are building right now to address those problems so on the right side um you",
    "start": "1847679",
    "end": "1853880"
  },
  {
    "text": "know are sort of working now on on on on accelerating ML and so we have a new python ml project that helps um people",
    "start": "1853880",
    "end": "1860639"
  },
  {
    "text": "work with um kind of bringing the the tool set to the to to the data scientists who who prefer working in",
    "start": "1860639",
    "end": "1866080"
  },
  {
    "text": "Python over over web uis or over over Scola um horovod is our distributed deep",
    "start": "1866080",
    "end": "1871159"
  },
  {
    "text": "loing system that has a really elegant API um autotune is our first uh piece of of automl so allowing the system to help",
    "start": "1871159",
    "end": "1878240"
  },
  {
    "text": "you train good models as opposed to having the data scientist or engineer kind of have to figure out all the right settings",
    "start": "1878240",
    "end": "1884679"
  },
  {
    "text": "themselves and some new visualization tools to help understand you know why models are working well or not and then",
    "start": "1884679",
    "end": "1890080"
  },
  {
    "text": "some newer features around uh um understanding more in real time how the models behaving in production the the",
    "start": "1890080",
    "end": "1895760"
  },
  {
    "text": "thing I showed you earlier was you know refresh once an hour and now we have more realtime um uh uh monitoring of of",
    "start": "1895760",
    "end": "1902120"
  },
  {
    "text": "the model and production all right so um as we've you know started to look at how do we make",
    "start": "1902120",
    "end": "1907880"
  },
  {
    "text": "how do we accelerate model development and sort of address the developer experience problem um with machine learning we've kind of looked at a few",
    "start": "1907880",
    "end": "1913799"
  },
  {
    "text": "things one is you know ml is this long workflow from getting data to training models all the way through and and you know there's friction points in every",
    "start": "1913799",
    "end": "1919880"
  },
  {
    "text": "single step and so we've been quite you know rigorous around trying to identify where those friction points are and and kind of grinding off the rough edges and",
    "start": "1919880",
    "end": "1926120"
  },
  {
    "text": "making the workflow faster um one of the guiding uh kind of principles or philosophies has been that and this kind",
    "start": "1926120",
    "end": "1932360"
  },
  {
    "text": "of goes back in many ways to the you know devops philosophy where if you let the engineer own you know own the code",
    "start": "1932360",
    "end": "1939159"
  },
  {
    "text": "from um from prototype through hardening through through QA through production you can accelerate the loop of of trying",
    "start": "1939159",
    "end": "1945519"
  },
  {
    "text": "something out getting a production and you also build better systems because the engineers are are on the hook to support the thing in production and we",
    "start": "1945519",
    "end": "1952760"
  },
  {
    "text": "found the same thing applies to machine learning too if you can empower the data scientists to own you know more and more",
    "start": "1952760",
    "end": "1958080"
  },
  {
    "text": "of the workflow ideally the whole thing um that they're able to you know to Traverse the workflow faster and they",
    "start": "1958080",
    "end": "1963679"
  },
  {
    "text": "also have more ownership of the of the problem end to end um you know bringing the tools to developers we made a few",
    "start": "1963679",
    "end": "1969639"
  },
  {
    "text": "mistakes early on around um you know not not embracing the tools that the data",
    "start": "1969639",
    "end": "1975080"
  },
  {
    "text": "scientists um were already very familiar with ie python um so we're bringing that back um and then more investments in in",
    "start": "1975080",
    "end": "1981760"
  },
  {
    "text": "sort of visual tools to help um understand and debug U Models All right so",
    "start": "1981760",
    "end": "1989880"
  },
  {
    "text": "piml the general problem here is that um you know Michelangelo initially targeted those super high scale use cases so high",
    "start": "1989880",
    "end": "1995679"
  },
  {
    "text": "scale training on giant data sets and and highs scale um predictions um you know at very low latency and that was",
    "start": "1995679",
    "end": "2003399"
  },
  {
    "text": "great for you know the first couple years of use cases and a lot of the the highest value ones um however we found",
    "start": "2003399",
    "end": "2008840"
  },
  {
    "text": "that um the system is not as easy to use and not as flexible um as is desired by",
    "start": "2008840",
    "end": "2014360"
  },
  {
    "text": "many scientists and also as is required by sort of a long tale of of more unique problems across Uber and so the solution",
    "start": "2014360",
    "end": "2021399"
  },
  {
    "text": "was you know how can we um just support sort of plain Python and the rich ecosystem of python tools um throughout",
    "start": "2021399",
    "end": "2027480"
  },
  {
    "text": "this endend workflow and do it um you know at at somewhat limited scale because you're you're dealing with",
    "start": "2027480",
    "end": "2032919"
  },
  {
    "text": "python you're dealing with a non-distributed environment but but make it scale and make it work as well as we possibly can",
    "start": "2032919",
    "end": "2038760"
  },
  {
    "text": "and so the basic idea is to allow people to build models in you know using essentially any any python code in any",
    "start": "2038760",
    "end": "2044919"
  },
  {
    "text": "python libraries um uh you know Implement a you know serving interface",
    "start": "2044919",
    "end": "2050200"
  },
  {
    "text": "in Python and then um have sort of packaging and deployment tools that will treat it like any other model that we",
    "start": "2050200",
    "end": "2056280"
  },
  {
    "text": "have and and be able to push it out to our serving infrastructure and I'll go through a quick thing um but sort of the",
    "start": "2056280",
    "end": "2062520"
  },
  {
    "text": "trade-offs between uh the PML and the other system is really kind of a trade between Flex ibility and sort of",
    "start": "2062520",
    "end": "2069000"
  },
  {
    "text": "resource efficiency and scale and and latency but the general idea is that",
    "start": "2069000",
    "end": "2074720"
  },
  {
    "text": "this is a pretty simple I think is a kaggle case but um we're going to build a I think a logistic aggression model is",
    "start": "2074720",
    "end": "2080280"
  },
  {
    "text": "that right um but we're going to build a panas data frame yeah we train a logistic aggression model um and then",
    "start": "2080280",
    "end": "2086118"
  },
  {
    "text": "and then run some test predictions at the very bottom and so very simple you know kind of standard um pyit learn",
    "start": "2086119",
    "end": "2091440"
  },
  {
    "text": "model and this is this is actually all happening in in a jupyter notebook I didn't show the whole context this is all happening in jupyter so you can have",
    "start": "2091440",
    "end": "2097880"
  },
  {
    "text": "a requirements file that that selects all of your dependencies you can then import your python libraries um you uh",
    "start": "2097880",
    "end": "2105160"
  },
  {
    "text": "um you then pick you save the model file back out to your directory um and then",
    "start": "2105160",
    "end": "2112200"
  },
  {
    "text": "you can um this is a serving interface you implement an interface that knows how to load that model um back into the",
    "start": "2112200",
    "end": "2118520"
  },
  {
    "text": "file and then implements a predict method that can do you know simple feature Transformations and then feed",
    "start": "2118520",
    "end": "2123760"
  },
  {
    "text": "the data through the model for scoring and so you can kind of see how these pieces you know you know give you interface to score the",
    "start": "2123760",
    "end": "2130640"
  },
  {
    "text": "model and then through the API we can you know test the model and then at the bottom we can actually um call upload",
    "start": "2131839",
    "end": "2138160"
  },
  {
    "text": "model and this will package up the model and all its dependencies and send it up to you know the Michelangelo backend",
    "start": "2138160",
    "end": "2143400"
  },
  {
    "text": "such that it can be managed in our UI and then deployed um the same way other models can be",
    "start": "2143400",
    "end": "2148560"
  },
  {
    "text": "deployed and so this model has been uploaded so now you can see it in the UI the same way you saw the other models that were all trained on the high scale",
    "start": "2148560",
    "end": "2154760"
  },
  {
    "text": "system and then either through the UI or through the API you can then deploy the model out to the exact same serving",
    "start": "2154760",
    "end": "2161319"
  },
  {
    "text": "infrastructure to do you know real time request response scoring and we also I don't have an example here but you can",
    "start": "2161319",
    "end": "2166599"
  },
  {
    "text": "also deploy it out to a spark job to do batch batch scoring but again so this is an attempt",
    "start": "2166599",
    "end": "2171720"
  },
  {
    "text": "to you know embrace the flexibility of python and and the tools that data scientists like to use already and then",
    "start": "2171720",
    "end": "2177079"
  },
  {
    "text": "provide the infrastructure and Scaffolding to kind of make it work at at the high scale that these tools um you know can",
    "start": "2177079",
    "end": "2184280"
  },
  {
    "text": "support architecturally um uh you know the left side there is the you know in",
    "start": "2184280",
    "end": "2189720"
  },
  {
    "text": "your environment where you're working whether it's jupyter or any other python environment you basically you know train",
    "start": "2189720",
    "end": "2194760"
  },
  {
    "text": "your model save it locally you know using whatever techniques you want you build your your model.py file which is",
    "start": "2194760",
    "end": "2199920"
  },
  {
    "text": "that one that had the serving the predict interface in it um and then you have your typical requirements and packages that tell the system um for all",
    "start": "2199920",
    "end": "2206800"
  },
  {
    "text": "of the libraries and and and system um libraries that you need and then there is a um you know basically a packaging",
    "start": "2206800",
    "end": "2213440"
  },
  {
    "text": "and build step that builds up a Docker container um that includes your your mod and all the dependencies and that can be",
    "start": "2213440",
    "end": "2219560"
  },
  {
    "text": "pushed out um I we cut off the screen there can be pushed out to our online serving system on the top or to um our",
    "start": "2219560",
    "end": "2228319"
  },
  {
    "text": "sorry offline on the top or our online system on the bottom for doing either batch predictions via spark job or",
    "start": "2228319",
    "end": "2233720"
  },
  {
    "text": "online predictions via um the request response thing that we saw before and looking a little more closer",
    "start": "2233720",
    "end": "2239920"
  },
  {
    "text": "so on the left is the is the uh online serving of the high Scale Models which is you know the picture we saw before um",
    "start": "2239920",
    "end": "2247599"
  },
  {
    "text": "and on the left side you can see that we we actually deploy a nested Docker container containing all of the Python",
    "start": "2247599",
    "end": "2253480"
  },
  {
    "text": "resources and so our existing prediction service acts as a sort of proxy or Gateway and then routes to a local um",
    "start": "2253480",
    "end": "2259960"
  },
  {
    "text": "Docker container that contains all the python code um so you get all of the same monitoring and and um support of",
    "start": "2259960",
    "end": "2266640"
  },
  {
    "text": "the um and sort of the the the network stuff of our high scale prediction container but then we can route the",
    "start": "2266640",
    "end": "2272119"
  },
  {
    "text": "request to um the nested python service um that that's used for the actual scoring",
    "start": "2272119",
    "end": "2278240"
  },
  {
    "text": "and then you can use psychic learn you can use deep learning you can write custom algorithms and this is super flexible now the trade-offs are you'll",
    "start": "2278240",
    "end": "2283960"
  },
  {
    "text": "you'll have slightly higher latency it's it doesn't scale as cheaply because Python's running and then you know if",
    "start": "2283960",
    "end": "2289000"
  },
  {
    "text": "you're using pyit learn you can't train on giant data sets because it's not distributed um but in terms of of",
    "start": "2289000",
    "end": "2294520"
  },
  {
    "text": "developer friendliness and speed um it's great and I think the the way people are approaching it is they can use this to",
    "start": "2294520",
    "end": "2300040"
  },
  {
    "text": "very quickly get out of Model you know say for one city um and then once they've proved that the model matters",
    "start": "2300040",
    "end": "2305599"
  },
  {
    "text": "then they're then they're it's sort of an easy your cell to go rebuild it on the high skill",
    "start": "2305599",
    "end": "2311480"
  },
  {
    "text": "system so horovod is our deploying um distribute deing system and it has as we",
    "start": "2312839",
    "end": "2318079"
  },
  {
    "text": "mentioned before kind of two interesting facets um you know one is that it scales",
    "start": "2318079",
    "end": "2323200"
  },
  {
    "text": "more efficiently than the other distributed deep learning approaches um but also the API for it is much much",
    "start": "2323200",
    "end": "2330079"
  },
  {
    "text": "simpler and much easier to set up your much easier to go from a a single node training job to a distributed training",
    "start": "2330079",
    "end": "2336240"
  },
  {
    "text": "job and so in this case um we pulled an example from I think it's from the tensorflow documentation for how to set",
    "start": "2336240",
    "end": "2341720"
  },
  {
    "text": "up a a distributed trading job in tensorflow using a parameter server and you can see um you know there's",
    "start": "2341720",
    "end": "2347440"
  },
  {
    "text": "basically one there's one little method in the middle there which is train the model and everything else is is setting",
    "start": "2347440",
    "end": "2353000"
  },
  {
    "text": "up the distributed trending environment which is not stuff that that a that a modeler should have to care about in the",
    "start": "2353000",
    "end": "2358520"
  },
  {
    "text": "horovod case um we're able to do you know sort of better distributed training with a lot less work and so we have the",
    "start": "2358520",
    "end": "2365520"
  },
  {
    "text": "the train method um up there the middle and then um around it are a few um you",
    "start": "2365520",
    "end": "2370960"
  },
  {
    "text": "know API calls to set up um you know horod to do the training there's a initialization um and then a few other",
    "start": "2370960",
    "end": "2376839"
  },
  {
    "text": "calls to to set up the environment but s much much easier and friendlier than than some of the other",
    "start": "2376839",
    "end": "2382960"
  },
  {
    "text": "approaches and this has been sort of quite popular in the community as well for for both those reasons uh manifold um so one of the",
    "start": "2382960",
    "end": "2391200"
  },
  {
    "text": "challenges you know in the vi reports we showed before is you tend to you know train models you tend to get a global",
    "start": "2391200",
    "end": "2396760"
  },
  {
    "text": "accuracy metri and so you know what's the Au or mean squ error for the whole model across the whole data set and",
    "start": "2396760",
    "end": "2402040"
  },
  {
    "text": "that's you know a good starting point but often different segments of the data will have very different um uh",
    "start": "2402040",
    "end": "2407800"
  },
  {
    "text": "characteristics and and the model will treat them very very differently and we've had cases where we've seen cases where you know a model on the whole",
    "start": "2407800",
    "end": "2413560"
  },
  {
    "text": "works great but then there's you know one slice of data where where behaves very very poorly and it maybe a very important slice of",
    "start": "2413560",
    "end": "2420359"
  },
  {
    "text": "data and so manifold we're building visualization tools that let you um kind of dive in more to the data and",
    "start": "2420359",
    "end": "2426480"
  },
  {
    "text": "understand you know how a model works on on smaller pieces on smaller segments that data and trying to um help you",
    "start": "2426480",
    "end": "2433280"
  },
  {
    "text": "identify ones where you know they're anomalous or or look different or",
    "start": "2433280",
    "end": "2438680"
  },
  {
    "text": "problematic um so autotune this is a system um you know once you have the you",
    "start": "2439720",
    "end": "2446280"
  },
  {
    "text": "know once you figured out the features for your model there's often a lot of work to figure out the right combination of of hyperparameters um that gives you",
    "start": "2446280",
    "end": "2453079"
  },
  {
    "text": "the the best accuracy and so for tree model you have the number of trees you have um the depth of the forest or sorry",
    "start": "2453079",
    "end": "2459440"
  },
  {
    "text": "the number of trees the depth of the trees um you have binning there's you know probably six or 10 different hyper",
    "start": "2459440",
    "end": "2464800"
  },
  {
    "text": "parameters that you can tweak and it's impossible to know upfront what the right combination is and so it's often a",
    "start": "2464800",
    "end": "2470599"
  },
  {
    "text": "sort of Brute Force process of finding um the right combination of those and and you know a common approach is either",
    "start": "2470599",
    "end": "2476400"
  },
  {
    "text": "a Brute Force grid search where you just generate a essentially hyper cube of all the different options and try everyone",
    "start": "2476400",
    "end": "2481560"
  },
  {
    "text": "you can do a random search where you do the same thing but then just search random pieces and find a pretty good one um or you can use what's called blackbox",
    "start": "2481560",
    "end": "2488440"
  },
  {
    "text": "optimization and you can actually um more efficiently uh and experimentally",
    "start": "2488440",
    "end": "2493640"
  },
  {
    "text": "um you know try um try different combinations and and learn the sort of shape of the of the space and then",
    "start": "2493640",
    "end": "2499200"
  },
  {
    "text": "Traverse more directly to a more optimal um a more optimal uh a set of of",
    "start": "2499200",
    "end": "2504800"
  },
  {
    "text": "parameters um and so we we collaborated with the uh research team at Uber to to build this um and as you can see on the",
    "start": "2504800",
    "end": "2511359"
  },
  {
    "text": "right um the the um the the line in the bottom is the uh uh is the the beian",
    "start": "2511359",
    "end": "2519160"
  },
  {
    "text": "blackbox optimized uh hyperr search which which gets to a a better model in",
    "start": "2519160",
    "end": "2524560"
  },
  {
    "text": "much few iterations than a than a grid search does because it can do it can it sort of learns the process and can do it much more efficiently and so again this",
    "start": "2524560",
    "end": "2530880"
  },
  {
    "text": "is kind of getting into automl how do you how do you help developers you know build models faster in a more automated fashion and and save um you know so you",
    "start": "2530880",
    "end": "2538480"
  },
  {
    "text": "know deploy the human intelligence where's really needed not where it can be you know automated",
    "start": "2538480",
    "end": "2543920"
  },
  {
    "text": "away and then the final one is around you know big part of you know once you have the model you like and you deploy",
    "start": "2544319",
    "end": "2549599"
  },
  {
    "text": "it a big part of the thing is is how do you make sure that model's behaving correctly in production and we we talked about being able to join you know",
    "start": "2549599",
    "end": "2555880"
  },
  {
    "text": "predictions back to to outcomes to to know your model's behaving well um there's a couple problems with that one",
    "start": "2555880",
    "end": "2561119"
  },
  {
    "text": "is um you know we we run that as a batch job and so there's a delay you know in the case of credit card fraud it can",
    "start": "2561119",
    "end": "2567480"
  },
  {
    "text": "take you know 90 days for the bank to send you the outcomes and so you you often can't sort of join to the outcomes",
    "start": "2567480",
    "end": "2573240"
  },
  {
    "text": "as quickly as you'd want to and so the other approach is to um it's less uh",
    "start": "2573240",
    "end": "2578599"
  },
  {
    "text": "less accurate in a sense less precise but it but it's much much quicker and that is just kind of looking at distributions of of both feature data so",
    "start": "2578599",
    "end": "2585559"
  },
  {
    "text": "the input data as well as the predictions coming out over time and you know for most models um you know the",
    "start": "2585559",
    "end": "2592880"
  },
  {
    "text": "there should be a pretty regular distribution of features and and predictions over time um and then maybe there's a seasonality to it as as the",
    "start": "2592880",
    "end": "2599800"
  },
  {
    "text": "day and the week flows by but it's often easy to sort identify you know big anomalies that that often cause problems",
    "start": "2599800",
    "end": "2606400"
  },
  {
    "text": "and and usually result usually they're caused by um bad data coming in either from a broken service or a broken",
    "start": "2606400",
    "end": "2612319"
  },
  {
    "text": "pipeline um and so you can see in this case we have um this is a classification model so the top we're just looking at",
    "start": "2612319",
    "end": "2618319"
  },
  {
    "text": "the distribution of um true versus false which you can see the distribution is pretty steady over time um and then we",
    "start": "2618319",
    "end": "2624480"
  },
  {
    "text": "have a few other slices looking at um the actual class probability over time",
    "start": "2624480",
    "end": "2629839"
  },
  {
    "text": "and then a few sort of histograms cast this time series so you can see you know how the how the bucketing of of data",
    "start": "2629839",
    "end": "2635839"
  },
  {
    "text": "works over time and I I think these ones are all you know things kind of look okay um and in this case we're now",
    "start": "2635839",
    "end": "2642720"
  },
  {
    "text": "looking at uh at the top um the prediction result but then also looking against the distribution of features in",
    "start": "2642720",
    "end": "2647800"
  },
  {
    "text": "the model and you can see at the top we actually had a you know the predictions themselves kind of deviated from what",
    "start": "2647800",
    "end": "2653240"
  },
  {
    "text": "looks more normal and at the bottom um you can see there was actually a feature that had some some bad data that was",
    "start": "2653240",
    "end": "2658680"
  },
  {
    "text": "sort of triggering the the abnormal predictions and so um again sort like like with software engineering you know",
    "start": "2658680",
    "end": "2665319"
  },
  {
    "text": "we're running a production system having all the monitoring set up automatically for you is super important and and you",
    "start": "2665319",
    "end": "2670520"
  },
  {
    "text": "know for ML that matters uh you know at the system level but it also matters at the data and the model level you want to",
    "start": "2670520",
    "end": "2675800"
  },
  {
    "text": "make sure that not only is the system not erring out but that the predictions and data are are correct and that there aren't breakages and data pipelines um",
    "start": "2675800",
    "end": "2682640"
  },
  {
    "text": "which is which is a sort of common failure mode for for ML Models All right so key Lessons Learned",
    "start": "2682640",
    "end": "2689520"
  },
  {
    "text": "recently um you know one mentioned before you know around sort of productivity ml is is bringing the tools",
    "start": "2689520",
    "end": "2695319"
  },
  {
    "text": "to developers you know we again Focus very early on on high scale and and you know that was the right first choice for",
    "start": "2695319",
    "end": "2701680"
  },
  {
    "text": "Uber but but now as we focus on on on velocity we're now bringing the tools closer to developers and even even",
    "start": "2701680",
    "end": "2707760"
  },
  {
    "text": "compromising on scale to make that um easier and faster and then providing a path to scale up you know as the as the",
    "start": "2707760",
    "end": "2713079"
  },
  {
    "text": "problem is more deeply understood um you know data is generally the hardest part of ML and so having",
    "start": "2713079",
    "end": "2718880"
  },
  {
    "text": "really good infrastructure and tooling and and automation around the the data management um lets the modelers focus on",
    "start": "2718880",
    "end": "2725760"
  },
  {
    "text": "the modeling problem and not on on the plumbing um you know on the on the",
    "start": "2725760",
    "end": "2732760"
  },
  {
    "text": "system Dev side you know we we've leveraged a lot of Open Source um but and and and we've we've you know",
    "start": "2732760",
    "end": "2738119"
  },
  {
    "text": "struggled in many cases to make it like it's taken a lot longer in many cases to make things actually work well at scale",
    "start": "2738119",
    "end": "2743400"
  },
  {
    "text": "um nothing's free um um and the last one is that uh you",
    "start": "2743400",
    "end": "2749760"
  },
  {
    "text": "know real time ml is is is is quite challenging and and hard to get right and and hard to empower modelers to own",
    "start": "2749760",
    "end": "2755920"
  },
  {
    "text": "the end to end and and we're investing a lot in Uber to uh to make those systems kind of run themselves so developers can",
    "start": "2755920",
    "end": "2761319"
  },
  {
    "text": "focus on on the modeling work and not worry about um the systems",
    "start": "2761319",
    "end": "2766640"
  },
  {
    "text": "themselves all right thank you [Applause]",
    "start": "2766800",
    "end": "2773369"
  }
]