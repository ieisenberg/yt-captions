[
  {
    "text": "[Music]",
    "start": "3360",
    "end": "9618"
  },
  {
    "text": "putting all of the machine learning algorithms models uh and everything else into the database didn't always sound",
    "start": "10400",
    "end": "17000"
  },
  {
    "text": "like a good idea to me a lot of this is based on things that I learned working",
    "start": "17000",
    "end": "22800"
  },
  {
    "text": "on at in instacart over the last decade trying to scale our machine learning infrastructure our data infrastructure",
    "start": "22800",
    "end": "29480"
  },
  {
    "text": "and our uh real time inference systems when I got to instacart one of",
    "start": "29480",
    "end": "35640"
  },
  {
    "text": "the first things I did actually was help pull all of our product catalog data out",
    "start": "35640",
    "end": "41239"
  },
  {
    "text": "of our monolithic postgress database that was already hitting scalability constraints you know eight nine years",
    "start": "41239",
    "end": "48399"
  },
  {
    "text": "ago and I moved all of that into elastic search so that we would have this beautiful horizontally scalable amazing",
    "start": "48399",
    "end": "54800"
  },
  {
    "text": "natural language processing framework um and that actually carried the company for about the next 5 years it became the",
    "start": "54800",
    "end": "60440"
  },
  {
    "text": "real heart of our data infrastructure um and it allowed us to grow the business to a multi-billion dollar Revenue",
    "start": "60440",
    "end": "67040"
  },
  {
    "text": "generating Enterprise um you know our atlasic church cluster grew to several hundred nodes it was powering uh several",
    "start": "67040",
    "end": "76360"
  },
  {
    "text": "thousand stores worth of data with hundreds of thousands of personal shoppers and millions of customers using",
    "start": "76360",
    "end": "82759"
  },
  {
    "text": "it on a regular basis um we we were doing everything with that cluster we",
    "start": "82759",
    "end": "87840"
  },
  {
    "text": "were taking our machine learning embeddings and put it in there we were putting our our feature store data in",
    "start": "87840",
    "end": "93040"
  },
  {
    "text": "there some of the Json blobs for our elastic documents reached the size of megabytes um and and over time this this",
    "start": "93040",
    "end": "101399"
  },
  {
    "text": "became slightly less enable once once you have the god uh object in your data",
    "start": "101399",
    "end": "109079"
  },
  {
    "text": "architecture that everybody wants to put something in and get something out of uh there's all kinds of organizational",
    "start": "109079",
    "end": "115600"
  },
  {
    "text": "problems there's also all kinds of scalability problems um regardless of of the technology and you really need tight",
    "start": "115600",
    "end": "122079"
  },
  {
    "text": "control over everything going on um the The Crucible moment I think for me came",
    "start": "122079",
    "end": "128479"
  },
  {
    "text": "during the covid pandemic when most of the world moved from an offline grocery",
    "start": "128479",
    "end": "134840"
  },
  {
    "text": "experience to an online grocery experience we were already you know a multi-billion dollar company uh and then",
    "start": "134840",
    "end": "140400"
  },
  {
    "text": "we started doubling on a weekly basis and at that point uh everything you know",
    "start": "140400",
    "end": "145599"
  },
  {
    "text": "about scalability goes out the window everything you know about engineering best practices goes out the window because the company's going to die um if",
    "start": "145599",
    "end": "152640"
  },
  {
    "text": "you can't actually you know scale double in a week and then you have to do it again the next week and so you find all",
    "start": "152640",
    "end": "159519"
  },
  {
    "text": "of these microservices uh that had their individual feature stores based on reddis or Cassandra or Druid or we're",
    "start": "159519",
    "end": "167080"
  },
  {
    "text": "talking directly to snowflake um you know all of the concurrency issues that you thought you know you would slowly",
    "start": "167080",
    "end": "174159"
  },
  {
    "text": "deal with all come to AE and so what we did was we had a postgress cluster we",
    "start": "174159",
    "end": "181280"
  },
  {
    "text": "had learned a lot about scaling postgress in the intervening years between the time that I get there you can get pretty far with read replicas",
    "start": "181280",
    "end": "189159"
  },
  {
    "text": "especially in a machine learning uh context where a few seconds of data",
    "start": "189159",
    "end": "194799"
  },
  {
    "text": "latency is actually pretty state-of-the-art when you start thinking about kofka and Flink streaming uh then",
    "start": "194799",
    "end": "202640"
  },
  {
    "text": "then you're used to tolerating uh and not having you know assd compliant transactions and so we can actually",
    "start": "202640",
    "end": "208879"
  },
  {
    "text": "scale post in a very similar manner horizontally we get really good control of the sharding capabilities of our",
    "start": "208879",
    "end": "214920"
  },
  {
    "text": "database so that we can very specifically Target exactly what criteria we need um and we we abstracted",
    "start": "214920",
    "end": "223599"
  },
  {
    "text": "all of that logic that we built internally and inart into a different project that I'm not going to talk about today called PG cat that sits in front",
    "start": "223599",
    "end": "230360"
  },
  {
    "text": "of a massive cluster of postgress databases to help scale it horizontally um but just know that",
    "start": "230360",
    "end": "237680"
  },
  {
    "text": "that's always the other half of the pro project that I I rarely talk about um",
    "start": "237680",
    "end": "242720"
  },
  {
    "text": "because scaling postgress is so important when you're talking about",
    "start": "242720",
    "end": "247760"
  },
  {
    "text": "machine learning uh talking about machine learning you know when we think about",
    "start": "247760",
    "end": "254280"
  },
  {
    "text": "machine learning a lot of people in the world think that it's this mystical Dark Art um I and I think that's really",
    "start": "254280",
    "end": "261680"
  },
  {
    "text": "unfortunate I think that we should try to take a engineering first approach",
    "start": "261680",
    "end": "267000"
  },
  {
    "text": "because I think engineering is actually where many of the hard problem s in deploying machine learning systems",
    "start": "267000",
    "end": "273639"
  },
  {
    "text": "are and I think Engineers are very used to dealing with systems as blackboxes",
    "start": "275280",
    "end": "281759"
  },
  {
    "text": "you have inputs you expect outputs it's just a function in the middle a machine learning model is just a function it",
    "start": "281759",
    "end": "288199"
  },
  {
    "text": "just takes inputs it just produces outputs you don't need to know how it works to use it we use hundreds of apis",
    "start": "288199",
    "end": "294759"
  },
  {
    "text": "hundreds of functions hundreds of sdks Without Really knowing how they work internally and we do this very",
    "start": "294759",
    "end": "300360"
  },
  {
    "text": "effectively uh we have unit tests we have integration tests we have end to end QA and we can deal with machine",
    "start": "300360",
    "end": "307160"
  },
  {
    "text": "Learning Systems very similarly and get very far uh so this is this is an",
    "start": "307160",
    "end": "314199"
  },
  {
    "text": "example this very contrived uh it it's just a function that takes your age and years and it returns a float or I'm",
    "start": "314199",
    "end": "321000"
  },
  {
    "text": "sorry it takes a birth date and it returns a float this is implemented in Python I hope that uh I don't have any",
    "start": "321000",
    "end": "327440"
  },
  {
    "text": "bugs in my code but it it gives you a rough idea of what we're talking about we can actually",
    "start": "327440",
    "end": "334720"
  },
  {
    "text": "pretend that we're a machine we don't understand birthdays we don't understand age as a concept and so if we're trying",
    "start": "334720",
    "end": "341880"
  },
  {
    "text": "to teach a machine about how to calculate a birthday we we'll take a data driven approach we'll collect some",
    "start": "341880",
    "end": "348160"
  },
  {
    "text": "samples from the audience you know somebody's born in 1960 somebody's born in 1980 somebody's born in the year 2000",
    "start": "348160",
    "end": "354600"
  },
  {
    "text": "and we know we can ask them what is your age we don't know how they're calculating their age it's some internal",
    "start": "354600",
    "end": "359919"
  },
  {
    "text": "mystical black box that they know how old they are and they know when they were born um they'll actually do some",
    "start": "359919",
    "end": "366000"
  },
  {
    "text": "computation very similar to that python function but we don't need to know that so we can just put their data in a",
    "start": "366000",
    "end": "371880"
  },
  {
    "text": "lookup table and now our function implementation doesn't need to know anything about dates or ages you pass in",
    "start": "371880",
    "end": "378919"
  },
  {
    "text": "a birth date we'll look up that birth date in our data table and we'll return the answer uh this is actually pretty",
    "start": "378919",
    "end": "385440"
  },
  {
    "text": "terrible because most people in this world in this room wouldn't have an exact birth date and so we'll get a not",
    "start": "385440",
    "end": "392919"
  },
  {
    "text": "found error uh rather than actually returning a useful age so this is where",
    "start": "392919",
    "end": "398560"
  },
  {
    "text": "machine learning comes into play what we can do with a with this very meager sample of data is we want to generalize",
    "start": "398560",
    "end": "404960"
  },
  {
    "text": "right we want to be able to tell people their age and years given any birth date",
    "start": "404960",
    "end": "410960"
  },
  {
    "text": "not just some birth date that somebody's already told us the answer for one of the simplest machine learning",
    "start": "410960",
    "end": "416639"
  },
  {
    "text": "algorithms is linear regression and all linear aggression is is you take your three data points that you have you put",
    "start": "416639",
    "end": "423759"
  },
  {
    "text": "them on a plot graph you draw a line through them and if you remember you",
    "start": "423759",
    "end": "428879"
  },
  {
    "text": "know this the slope intercept form of a line from uh early algebra in high",
    "start": "428879",
    "end": "434280"
  },
  {
    "text": "school is yal MX plus b in this case we can we know that you know every year you",
    "start": "434280",
    "end": "439840"
  },
  {
    "text": "get one year older and it happens to be the year 2023 today so m is negative 1 B",
    "start": "439840",
    "end": "447720"
  },
  {
    "text": "is 2023 this gives us we've we've now solved the equation for the linear regression that passes through these",
    "start": "447720",
    "end": "454800"
  },
  {
    "text": "points you don't need to know how linear regression actually works actually implemented that's just another function",
    "start": "454800",
    "end": "461840"
  },
  {
    "text": "call it'll give you this data there's libraries that do this it's all",
    "start": "461840",
    "end": "466919"
  },
  {
    "text": "implemented um we can now rewrite our python function we have a couple constants we have M andb you can now",
    "start": "466919",
    "end": "474199"
  },
  {
    "text": "pass in your date time the year of the date time is the only feature we care about in our a very simple model we can",
    "start": "474199",
    "end": "481120"
  },
  {
    "text": "now multiply that by m and add B and now we've actually generalized our three",
    "start": "481120",
    "end": "486199"
  },
  {
    "text": "data points into predicting the correct age from any birth date Without Really",
    "start": "486199",
    "end": "491400"
  },
  {
    "text": "knowing anything about ages or birth dates and so if you start to think about all the hidden functions in our",
    "start": "491400",
    "end": "496440"
  },
  {
    "text": "applications from our user perspective we don't know why users behave the way that they do or what they want but we",
    "start": "496440",
    "end": "503039"
  },
  {
    "text": "can start to gather data about their behavior and they can start explaining to us these hidden functions and we can",
    "start": "503039",
    "end": "509639"
  },
  {
    "text": "model them and then we can actually generalize those models across population so machine learning is a very",
    "start": "509639",
    "end": "515360"
  },
  {
    "text": "very powerful technique when you're dealing in a murky environment um to move this forward you",
    "start": "515360",
    "end": "522039"
  },
  {
    "text": "know beyond the simplest linear regression neural networks deep learning and llms are a much much more advanced",
    "start": "522039",
    "end": "529240"
  },
  {
    "text": "topic this is a diagram of a very simple neural network it has three layers the",
    "start": "529240",
    "end": "534920"
  },
  {
    "text": "three layers are the inputs the hidden layer and the output this is just a function fun it takes three inputs it",
    "start": "534920",
    "end": "541079"
  },
  {
    "text": "produces one output U what happens in the middle is a black bar Box nobody",
    "start": "541079",
    "end": "546200"
  },
  {
    "text": "needs it to know but I'll I'll walk through an example and just just for reference all machine learning models",
    "start": "546200",
    "end": "553000"
  },
  {
    "text": "only operate on math they only take numbers but what we've seen lately is that llms they take text and so how does",
    "start": "553000",
    "end": "559920"
  },
  {
    "text": "that work well you start with your words and you assign all of your words and ID",
    "start": "559920",
    "end": "565200"
  },
  {
    "text": "so a is the first word in our dictionary it gets the ID number one upon is the",
    "start": "565200",
    "end": "570279"
  },
  {
    "text": "74th word in our dictionary so it gets the ID number 74 and so on and so forth until you have all of the words in your",
    "start": "570279",
    "end": "576560"
  },
  {
    "text": "dictionary assigned numbers then you multiply those numbers",
    "start": "576560",
    "end": "581640"
  },
  {
    "text": "you add those numbers every single line in this graph represents a function very",
    "start": "581640",
    "end": "586800"
  },
  {
    "text": "similar to linear regression there's hundreds of difference of functions and hundreds of different ways that you can",
    "start": "586800",
    "end": "592519"
  },
  {
    "text": "Implement those lines um but you don't need to know that right now you don't need to be a machine learning expert to",
    "start": "592519",
    "end": "598880"
  },
  {
    "text": "know this is just math it's a lot of math there are a lot of lines and you know that's why gpus can actually",
    "start": "598880",
    "end": "605480"
  },
  {
    "text": "execute all of those lines in parallel they're all independent but what you'll get is just some more magic numbers in",
    "start": "605480",
    "end": "611399"
  },
  {
    "text": "in an array in the middle in the hidden layer you just repeat that process with more lines more functions more math and",
    "start": "611399",
    "end": "618360"
  },
  {
    "text": "you'll get an output and the output is just some number some magic number like 42 so to actually understand what 42",
    "start": "618360",
    "end": "624760"
  },
  {
    "text": "means um and we look it up in the dictionary and we get the word time out and so now we have this model that given",
    "start": "624760",
    "end": "631200"
  },
  {
    "text": "the three inputs once upon a time is predicting the next word in the sentence is",
    "start": "631200",
    "end": "637760"
  },
  {
    "text": "time and this is this is how llms work this is the magic that they",
    "start": "637760",
    "end": "643120"
  },
  {
    "text": "do but for this talk we're going to focus on embeddings and vectors what are",
    "start": "643120",
    "end": "648839"
  },
  {
    "text": "embeddings and vectors an embedding is just that hidden layer this is some mystical intermediate",
    "start": "648839",
    "end": "656440"
  },
  {
    "text": "representation that the model has uh state-of-the-art research doesn't really",
    "start": "656440",
    "end": "662040"
  },
  {
    "text": "understand why these numbers are the way that they are they are the way that they are because that's what gets the right",
    "start": "662040",
    "end": "667480"
  },
  {
    "text": "answer is basically what it boils down to there are lots of clever ways to figure out how to generate those numbers",
    "start": "667480",
    "end": "673920"
  },
  {
    "text": "to make sure that you are getting the right answer but again we don't need to know any of that it's just a black box",
    "start": "673920",
    "end": "679680"
  },
  {
    "text": "it's just a function the very cool thing is that we'll have lots of various uh ways to",
    "start": "679680",
    "end": "688200"
  },
  {
    "text": "start a story like once upon a um that you know may all be similar to us and",
    "start": "688200",
    "end": "696760"
  },
  {
    "text": "they will also be similar in this hidden layer in this embedding even though they may use completely different words it",
    "start": "696760",
    "end": "703279"
  },
  {
    "text": "may be a completely different phrase it may be a completely different language um as long as the model has been trained",
    "start": "703279",
    "end": "711200"
  },
  {
    "text": "well then the embeding the intermediate representation of that language will be",
    "start": "711200",
    "end": "716720"
  },
  {
    "text": "very similar and so and and when I say similar I just mean like by ukian",
    "start": "716720",
    "end": "722440"
  },
  {
    "text": "geometry like the the number three or negative3 will be close it'll be some number close tog3 it'll be some number",
    "start": "722440",
    "end": "729519"
  },
  {
    "text": "close to 23 in the second box um and we have lots of ways to measure similarity",
    "start": "729519",
    "end": "734680"
  },
  {
    "text": "of large arrays uh you can do The Manhattan distance since we're in New York um or you might choose the dot",
    "start": "734680",
    "end": "740920"
  },
  {
    "text": "product or you might choose cosine similarity these are all just four Loops",
    "start": "740920",
    "end": "746040"
  },
  {
    "text": "um to implement these things",
    "start": "746040",
    "end": "750880"
  },
  {
    "text": "again modern processors can do a lot of those computations very very quickly and",
    "start": "751480",
    "end": "757079"
  },
  {
    "text": "tell you how similar all all of these things are the reason you need a",
    "start": "757079",
    "end": "762760"
  },
  {
    "text": "embedding database or you might want an embedding database is think about the ex elastic search case that I mentioned",
    "start": "762760",
    "end": "769160"
  },
  {
    "text": "before you have hundreds of thousands or millions or or even billions of documents and they're all text in",
    "start": "769160",
    "end": "775959"
  },
  {
    "text": "traditional search you know you're going to do keyword match ing against an inverted index using English language",
    "start": "775959",
    "end": "782120"
  },
  {
    "text": "but if somebody uses the wrong word uh if they say um trying to come up with an",
    "start": "782120",
    "end": "787800"
  },
  {
    "text": "example um Vanilla Ice Cream versus old-fashioned vanilla um you may not",
    "start": "787800",
    "end": "794800"
  },
  {
    "text": "actually get the right keyword match and you may return the wrong product so synonym matching is something that",
    "start": "794800",
    "end": "801360"
  },
  {
    "text": "embeddings are very very good at um and so you can actually take all of your",
    "start": "801360",
    "end": "807760"
  },
  {
    "text": "natural language doc doents you run them all through in their Network like this you generate the embedding for that",
    "start": "807760",
    "end": "814639"
  },
  {
    "text": "document uh and then you save just that embedding just that array in your database now pretty much every database",
    "start": "814639",
    "end": "821959"
  },
  {
    "text": "I know of has an array data type for storage it's a pretty primitive data type pretty much every programming",
    "start": "821959",
    "end": "828480"
  },
  {
    "text": "language supports arrays um even uh garbage collected runtime",
    "start": "828480",
    "end": "834279"
  },
  {
    "text": "languages like python can Implement operations on arrays very quick quickly",
    "start": "834279",
    "end": "839519"
  },
  {
    "text": "especially with OP uh optimized libraries like numpy and pandas um so",
    "start": "839519",
    "end": "845880"
  },
  {
    "text": "this this is not new stuff uh even though Vector databases seem very new",
    "start": "845880",
    "end": "851160"
  },
  {
    "text": "today people have been doing these things for decades um and and a lot of a lot of the functionality you need has",
    "start": "851160",
    "end": "857959"
  },
  {
    "text": "actually been baked into Hardware by Intel and by Nvidia um because this is such a generally useful thing to be able",
    "start": "857959",
    "end": "864600"
  },
  {
    "text": "to say add up the numbers into arrays like everybody needs to do it for all kinds of things but if you have a",
    "start": "864600",
    "end": "871360"
  },
  {
    "text": "database and it's full of these arrays with millions of them and then you have a user query come in you can also",
    "start": "871360",
    "end": "878600"
  },
  {
    "text": "generate the embedding of that user query and now you can actually calculate the distance between that user embedding",
    "start": "878600",
    "end": "885519"
  },
  {
    "text": "array and each and every one of the arrays in your database and you'll get some distance function for all of those",
    "start": "885519",
    "end": "892480"
  },
  {
    "text": "you can sort that list find the one with the smallest distance that will give you",
    "start": "892480",
    "end": "898000"
  },
  {
    "text": "the array you translate that array is mapped back to a text document and now you have the",
    "start": "898000",
    "end": "903600"
  },
  {
    "text": "English language document that is most similar to the English language query coming in and again this is just a",
    "start": "903600",
    "end": "911839"
  },
  {
    "text": "function you you take a single input which is English language you produce a single output which is English language",
    "start": "911839",
    "end": "918360"
  },
  {
    "text": "even though there it's all math under the hood so I've just given you a pretty",
    "start": "918360",
    "end": "925079"
  },
  {
    "text": "lowlevel um or high level description of how machine Learning Works you can",
    "start": "925079",
    "end": "930920"
  },
  {
    "text": "basically forget all of that now because if you want to generate an embedding you don't really need to know how any of it",
    "start": "930920",
    "end": "936920"
  },
  {
    "text": "actually happens we've created a very simple function in postgress ml if you install postgress ml in your postgress",
    "start": "936920",
    "end": "943680"
  },
  {
    "text": "database you can select the pgml embed function and you pass it two arguments",
    "start": "943680",
    "end": "950800"
  },
  {
    "text": "the first argument is the model name and this can be any model published on hugging face there are hundreds of open-",
    "start": "950800",
    "end": "957040"
  },
  {
    "text": "source models that will generate embeddings Um this can be a model that you've trained yourself if you are a",
    "start": "957040",
    "end": "963600"
  },
  {
    "text": "natural language processing expert and then it of course takes the text that you want to create an embedding for and",
    "start": "963600",
    "end": "970040"
  },
  {
    "text": "so you can use this function both to index all of your documents and create",
    "start": "970040",
    "end": "977000"
  },
  {
    "text": "uh embeddings and this is this is the output of that function for a single",
    "start": "977000",
    "end": "983040"
  },
  {
    "text": "call it's always a vector uh and again a vector is just a massive blob of numbers",
    "start": "983040",
    "end": "989079"
  },
  {
    "text": "keep in mind though that when you're talking about these embedding vectors typically they're on the scale of a",
    "start": "989079",
    "end": "994440"
  },
  {
    "text": "thousand floats each um and so if you're talking about you know a million",
    "start": "994440",
    "end": "1000079"
  },
  {
    "text": "documents that you're creating embeddings for that's a million times a thousand a billion floats that you need",
    "start": "1000079",
    "end": "1005120"
  },
  {
    "text": "to store that's a gigabyte um of yeah so",
    "start": "1005120",
    "end": "1010880"
  },
  {
    "text": "that these things can quickly grow large um and so you you do need to be",
    "start": "1010880",
    "end": "1017279"
  },
  {
    "text": "thoughtful about how your storing them how you're indexing them etc etc but postgress makes this again very",
    "start": "1017279",
    "end": "1024959"
  },
  {
    "text": "simple if you have a documents table and that table would normally have a text",
    "start": "1024959",
    "end": "1030079"
  },
  {
    "text": "column that represents the body of the document we can add a separate column or a second column to that table and this",
    "start": "1030079",
    "end": "1038000"
  },
  {
    "text": "table will represent or hold the embedding in this case the EM embedding is a vector it has 768 elements in it uh",
    "start": "1038000",
    "end": "1046798"
  },
  {
    "text": "these need to be sized um it's really nice to have a typed schema in postgress",
    "start": "1046799",
    "end": "1052000"
  },
  {
    "text": "where you can check correctness of everything but this particular vcum",
    "start": "1052000",
    "end": "1057120"
  },
  {
    "text": "Vector column postgress has this really nice feature where you can say that a column is",
    "start": "1057120",
    "end": "1063559"
  },
  {
    "text": "generated and in this case the generation of this embedding column is our pgml embed function so anybody who",
    "start": "1064760",
    "end": "1072720"
  },
  {
    "text": "inserts some text into this document table is automatically going to also create an embedding alongside that",
    "start": "1072720",
    "end": "1079799"
  },
  {
    "text": "document um they don't even need to know that they're creating an embedding um it",
    "start": "1079799",
    "end": "1084840"
  },
  {
    "text": "it it eliminates a lot of issues with data staleness if they update the document the embedding is also",
    "start": "1084840",
    "end": "1091240"
  },
  {
    "text": "regenerated and updated for them uh this is an example of a slightly newer model",
    "start": "1091240",
    "end": "1098320"
  },
  {
    "text": "um it's the Hong Kong University's natural language processing instructor extra large model um at the time that I",
    "start": "1098320",
    "end": "1105720"
  },
  {
    "text": "made these slides it was the leading model it's no longer the leading model for embeddings these things change on a",
    "start": "1105720",
    "end": "1112240"
  },
  {
    "text": "weekly basis so being able to just swap out your model name regenerate all your",
    "start": "1112240",
    "end": "1117720"
  },
  {
    "text": "embeddings is actually really nice um so",
    "start": "1117720",
    "end": "1122919"
  },
  {
    "text": "uh but this this model is interesting because it takes a prompt for how to actually generate the embedding similar",
    "start": "1122919",
    "end": "1128919"
  },
  {
    "text": "to all the prompt engineering that people are doing with other large language models so it's worth worth",
    "start": "1128919",
    "end": "1135360"
  },
  {
    "text": "calling out that everything is moving very very quickly now you needs a lot of",
    "start": "1135360",
    "end": "1142880"
  },
  {
    "text": "flexibility um and you need to constantly be updating dependencies uh python dependencies are not fun to",
    "start": "1142880",
    "end": "1149760"
  },
  {
    "text": "maintain there's a very large operational burden there if you're doing machine learning on a bunch of laptops",
    "start": "1149760",
    "end": "1156039"
  },
  {
    "text": "uh for your data scientists it's very hard to make sure that every data scientist in a large organization has",
    "start": "1156039",
    "end": "1161720"
  },
  {
    "text": "the latest updates and that their laptops are working on the other hand if there's one large Central database",
    "start": "1161720",
    "end": "1167320"
  },
  {
    "text": "cluster that thing can be managed very effectively by an operational team who",
    "start": "1167320",
    "end": "1172960"
  },
  {
    "text": "has complete control and then the data scientists can still get access to the latest models they can still get access",
    "start": "1172960",
    "end": "1179520"
  },
  {
    "text": "to do all of the things that they need to do Post gml isn't just about large language models it has all of the",
    "start": "1179520",
    "end": "1185840"
  },
  {
    "text": "classical machine learning algorithms that you expect um we have native bindings for XG boost that are super",
    "start": "1185840",
    "end": "1192760"
  },
  {
    "text": "super fast we're very proud of the implementation is highly optimized so that when you have you know what is an",
    "start": "1192760",
    "end": "1199360"
  },
  {
    "text": "array a c array in memory in postgress in a buffer from a table we take a",
    "start": "1199360",
    "end": "1205960"
  },
  {
    "text": "pointer to that we don't even copy it we pass that pointer to XG boost XG boost says fine I know what an array is again",
    "start": "1205960",
    "end": "1213360"
  },
  {
    "text": "these things are very primitive this is the goal of Apache Arrow um this is the goal of protuff from Google they have",
    "start": "1213360",
    "end": "1221120"
  },
  {
    "text": "spanner uh spanner is a very similar concept that when you have a Proto buff will store the data in the array or the",
    "start": "1221120",
    "end": "1229159"
  },
  {
    "text": "data in the table in the row in the exact protuff format so there is no",
    "start": "1229159",
    "end": "1234360"
  },
  {
    "text": "serialization they can just copy it straight out over the wire um serialization and going over the wire",
    "start": "1234360",
    "end": "1241200"
  },
  {
    "text": "kill so many machine learning applications they really limit the amount of data that you can bring to",
    "start": "1241200",
    "end": "1247840"
  },
  {
    "text": "bear in a interactive online context um but that's on as side for",
    "start": "1247840",
    "end": "1254039"
  },
  {
    "text": "later umore",
    "start": "1254039",
    "end": "1260120"
  },
  {
    "text": "in yeah so the the the Hong Kong University natural language processing instructor XL model it has an a hidden",
    "start": "1261440",
    "end": "1269720"
  },
  {
    "text": "layer in the middle of it that is 768 nodes wide and it will always have",
    "start": "1269720",
    "end": "1275760"
  },
  {
    "text": "768 floats as its intermediate hidden layer representation and so regardless",
    "start": "1275760",
    "end": "1282080"
  },
  {
    "text": "of the string size that you pass in it's going to have that hidden State once",
    "start": "1282080",
    "end": "1287200"
  },
  {
    "text": "it's read the entire string and so that is the size of the vector that we will",
    "start": "1287200",
    "end": "1292559"
  },
  {
    "text": "always be storing in this table regardless if it's one-word input or a th word inputs a lot of these models",
    "start": "1292559",
    "end": "1298480"
  },
  {
    "text": "have what's called a context window which means they can only consider 512 or 2,000 tokens at a time um if you pass",
    "start": "1298480",
    "end": "1307440"
  },
  {
    "text": "fewer than that it will get padded out with zeros or the empty word token if",
    "start": "1307440",
    "end": "1313400"
  },
  {
    "text": "you pass more than that it'll probably just get truncated um there's a whole bunch of techniques that you might want",
    "start": "1313400",
    "end": "1319120"
  },
  {
    "text": "to do called chunking where if your documents are larger than the context window of your model you will want to",
    "start": "1319120",
    "end": "1325440"
  },
  {
    "text": "split those documents up into chunks create an embedding for each chunk there's lots of cool tricks you",
    "start": "1325440",
    "end": "1332360"
  },
  {
    "text": "can do with embeddings I don't know if I'm covering this later in the in the talk but um you can you can do all kinds",
    "start": "1332360",
    "end": "1338159"
  },
  {
    "text": "of math with arrays you can add like let's say you have a document and you break it up into 10 chunks and you",
    "start": "1338159",
    "end": "1343919"
  },
  {
    "text": "generate 10 different embeddings for those 10 chunks you can actually add up all the vectors and the vector that you",
    "start": "1343919",
    "end": "1350640"
  },
  {
    "text": "get by adding up those 10 vectors will actually be the average Vector of the entire document and so you can even",
    "start": "1350640",
    "end": "1358880"
  },
  {
    "text": "though your your model may be limited to a context window of 512 the embedding um",
    "start": "1358880",
    "end": "1365200"
  },
  {
    "text": "practically that it can consider is unlimited but if you think about what happens when you start adding up a bunch",
    "start": "1365200",
    "end": "1371200"
  },
  {
    "text": "of embeddings together some of will have positive numbers some of them will have negative numbers a lot of them will just",
    "start": "1371200",
    "end": "1376880"
  },
  {
    "text": "cancel out and trend toward zero and if you add up you know the embeddings for all of Wikipedia you might just get",
    "start": "1376880",
    "end": "1383360"
  },
  {
    "text": "zeros across the board meaning that this is not special in any way it's equally relevant to everything or equally close",
    "start": "1383360",
    "end": "1390240"
  },
  {
    "text": "to everything um well it'll actually be closer to things that are also very",
    "start": "1390240",
    "end": "1395720"
  },
  {
    "text": "genetic or generic or general um and so",
    "start": "1395720",
    "end": "1401120"
  },
  {
    "text": "that that covers I think the 768 generated always as is just this the",
    "start": "1401120",
    "end": "1406279"
  },
  {
    "text": "postgress syntax that tells postgress anytime someone inserts or updates a row",
    "start": "1406279",
    "end": "1412760"
  },
  {
    "text": "in this table it needs to run this function um and I'll step over here the",
    "start": "1412760",
    "end": "1419600"
  },
  {
    "text": "this body here is actually a variable that references this this body column up",
    "start": "1419600",
    "end": "1425080"
  },
  {
    "text": "here so you only have to pass the body text in once postgress has it in memory",
    "start": "1425080",
    "end": "1430240"
  },
  {
    "text": "once it will reuse it and it will repass it to the model here um and stored is",
    "start": "1430240",
    "end": "1437360"
  },
  {
    "text": "another option option you can have this be stored in the table physically where it will take up space um and this is",
    "start": "1437360",
    "end": "1444360"
  },
  {
    "text": "good if you read things more than you write them but you don't have to store it it can be generated on the Fly um at",
    "start": "1444360",
    "end": "1451640"
  },
  {
    "text": "read time so perhaps you're storing a lot of documents that you never need the embedding for generating embeddings is",
    "start": "1451640",
    "end": "1457559"
  },
  {
    "text": "expensive so in that case you only want to run the generation function if you somebody's reading the embedding column",
    "start": "1457559",
    "end": "1464760"
  },
  {
    "text": "U and that that might be a savings there",
    "start": "1464760",
    "end": "1469799"
  },
  {
    "text": "um this is an example of the cosine distance operator uh that's also provided by PG Vector um there are three",
    "start": "1469960",
    "end": "1478000"
  },
  {
    "text": "of these operators passed this will give you this will do the cosine similarity function there's one for the Manhattan",
    "start": "1478000",
    "end": "1485080"
  },
  {
    "text": "distance there's one for the dot product these three functions have different tradeoffs Um Manhattan doesn't involve",
    "start": "1485080",
    "end": "1491960"
  },
  {
    "text": "any square roots you just add up the you know East West plus the north south uh",
    "start": "1491960",
    "end": "1498320"
  },
  {
    "text": "and that's your final distance um whereas dot product and cosine similarity have a little bit more math",
    "start": "1498320",
    "end": "1505360"
  },
  {
    "text": "involved they're a little bit more expensive to compute but the distance is",
    "start": "1505360",
    "end": "1510840"
  },
  {
    "text": "truer you know if we actually take the hypotenuse of the of the triangle that's",
    "start": "1510840",
    "end": "1516640"
  },
  {
    "text": "better than if we look at just the East West North South Manhattan distance although we've just had to calculate a",
    "start": "1516640",
    "end": "1523080"
  },
  {
    "text": "square root um so you know my my preference is to always start with",
    "start": "1523080",
    "end": "1528399"
  },
  {
    "text": "cosine distance which is the most flexible if your vectors are",
    "start": "1528399",
    "end": "1533600"
  },
  {
    "text": "normalized um then you can you get the dot product will give you the same",
    "start": "1533600",
    "end": "1538880"
  },
  {
    "text": "accuracy for free um but not all Vector spaces are fully normalized so this is",
    "start": "1538880",
    "end": "1545480"
  },
  {
    "text": "the safest highest quality answer if you need more performance later you can see",
    "start": "1545480",
    "end": "1550960"
  },
  {
    "text": "you can test empirically and you really have to test empirically um there's all",
    "start": "1550960",
    "end": "1556520"
  },
  {
    "text": "kinds of metrics you can get for these models that will give you",
    "start": "1556520",
    "end": "1564240"
  },
  {
    "text": "a quantitative answer of like oh the perplexity is blah blah blah and like what does that mean I don't know it",
    "start": "1564240",
    "end": "1570440"
  },
  {
    "text": "means it's better than the other one maybe kind of um what you really have to",
    "start": "1570440",
    "end": "1575520"
  },
  {
    "text": "do is you have to actually run some queries against your database you have to look at the things that you're getting back um and you can actually you",
    "start": "1575520",
    "end": "1583720"
  },
  {
    "text": "know uh later later on we'll get into some other ways to look at look at",
    "start": "1583720",
    "end": "1589000"
  },
  {
    "text": "quality um so I'll leave that for then if you have say a thousand",
    "start": "1589000",
    "end": "1598039"
  },
  {
    "text": "factors um you can a modern CPU core or",
    "start": "1598039",
    "end": "1603640"
  },
  {
    "text": "GPU core can with you know several gigahertz of speed can probably",
    "start": "1603640",
    "end": "1610240"
  },
  {
    "text": "compute 100,000 cosign similarities or dot products per",
    "start": "1610240",
    "end": "1615679"
  },
  {
    "text": "second um and so if you have a 100,000 documents in your database and you have",
    "start": "1615679",
    "end": "1622960"
  },
  {
    "text": "a single core CPU and one second is fast enough for your application then you're",
    "start": "1622960",
    "end": "1630000"
  },
  {
    "text": "done you don't need anything special if you have fewer documents you're done you don't need anything special um a lot of",
    "start": "1630000",
    "end": "1635279"
  },
  {
    "text": "a lot of document collections that I see are on the order of like thousands they're not on the order of millions um",
    "start": "1635279",
    "end": "1643520"
  },
  {
    "text": "and you can just Brute Force the computation on a single core now if you have a GPU that has 5,000 cores then you",
    "start": "1643520",
    "end": "1650880"
  },
  {
    "text": "know you can do 5,000 times 100,000 in a second which will give you five billion",
    "start": "1650880",
    "end": "1657279"
  },
  {
    "text": "documents um five billion is an interesting number uh for for",
    "start": "1657279",
    "end": "1663039"
  },
  {
    "text": "scalability reasons but that that's probably a different conversation in this case though like let's say you you",
    "start": "1663039",
    "end": "1670279"
  },
  {
    "text": "have a million documents um and you don't want to wait 10 seconds for a user query to come in you need a way to",
    "start": "1670279",
    "end": "1677200"
  },
  {
    "text": "actually find your closest vectors in your data set uh much more quickly and",
    "start": "1677200",
    "end": "1683360"
  },
  {
    "text": "so you you want to avoid doing a lot of the direct comparisons and so you know we use use",
    "start": "1683360",
    "end": "1690159"
  },
  {
    "text": "indexes to create shortcuts all of the time most people are probably familiar",
    "start": "1690159",
    "end": "1696000"
  },
  {
    "text": "with keyword indexes when they're doing text recall um we just build the list of",
    "start": "1696000",
    "end": "1701480"
  },
  {
    "text": "all of the documents that contain that keyword and we sort that list ahead of",
    "start": "1701480",
    "end": "1706799"
  },
  {
    "text": "time uh by by how many times that keyword appears in that document and so",
    "start": "1706799",
    "end": "1712840"
  },
  {
    "text": "then when somebody searches for that keyword you just go get your list and you take the head of it uh and that gives you your documents that that match",
    "start": "1712840",
    "end": "1719640"
  },
  {
    "text": "and so it's very quick it's much more quick than actually having to scan every single document and see if the keyword",
    "start": "1719640",
    "end": "1726159"
  },
  {
    "text": "actually exists in it that would be terribly slow similarly with uh vectors",
    "start": "1726159",
    "end": "1731640"
  },
  {
    "text": "there are indexing operations that we do PG Vector",
    "start": "1731640",
    "end": "1736720"
  },
  {
    "text": "supports the IVF flat indexing type uh and this index type what it does is",
    "start": "1736720",
    "end": "1745600"
  },
  {
    "text": "let's say you've got a million vectors that you want to build an index over in",
    "start": "1745600",
    "end": "1751720"
  },
  {
    "text": "this case we've said we want 2,000 lists in this index and so it's going to",
    "start": "1751720",
    "end": "1757399"
  },
  {
    "text": "create 2,000 lists of vectors it's going to automatically cluster all of your",
    "start": "1757399",
    "end": "1763720"
  },
  {
    "text": "million vectors into 2,000 different clusters where that are most compact so",
    "start": "1763720",
    "end": "1769600"
  },
  {
    "text": "that every Vector in each list is most similar to all the other vectors in that list and less similar to vectors in",
    "start": "1769600",
    "end": "1776360"
  },
  {
    "text": "other lists and then it's going to compute the centroid of all of those lists this is just Vector math um and so",
    "start": "1776360",
    "end": "1783720"
  },
  {
    "text": "each each list can then be looked at as the centroid and everything will be",
    "start": "1783720",
    "end": "1789240"
  },
  {
    "text": "closest to that centroid in each list so now when I do a query and I want",
    "start": "1789240",
    "end": "1795720"
  },
  {
    "text": "to look up and find the nearest vectors to my query Vector I only have to Brute",
    "start": "1795720",
    "end": "1801720"
  },
  {
    "text": "Force the 2,000 centroids I can find the list from those 2000 that has the most",
    "start": "1801720",
    "end": "1809440"
  },
  {
    "text": "likely candidates that are closest to my input and then I can brute force all of",
    "start": "1809440",
    "end": "1815480"
  },
  {
    "text": "the ones in that list I don't want to do the math of 1 million divided by 2000 I",
    "start": "1815480",
    "end": "1821799"
  },
  {
    "text": "think it's 500 but that would be another 500 uh Vector comparisons so in total we",
    "start": "1821799",
    "end": "1827519"
  },
  {
    "text": "would do 2,000 to find the list we would do 500 across that list that's 2500",
    "start": "1827519",
    "end": "1833279"
  },
  {
    "text": "comparisons or vector distance calculations rather than the full million so 2500 is much faster than a",
    "start": "1833279",
    "end": "1839960"
  },
  {
    "text": "million the tradeoff here is that is around edge cases like if your vector is",
    "start": "1839960",
    "end": "1845799"
  },
  {
    "text": "on the boundary between two lists and it only slightly Falls closer to one list",
    "start": "1845799",
    "end": "1850840"
  },
  {
    "text": "than the other um then you might actually miss some other vectors from",
    "start": "1850840",
    "end": "1856120"
  },
  {
    "text": "that other list that are also so near the boundary and that's why this is an",
    "start": "1856120",
    "end": "1861559"
  },
  {
    "text": "approximate nearest neighbor search what we can do in those case to handle those cases if we find that our recall is bad",
    "start": "1861559",
    "end": "1868559"
  },
  {
    "text": "that we're frequently missing vectors near the edge uh in our results is we can turn up the number of probes and the",
    "start": "1868559",
    "end": "1875440"
  },
  {
    "text": "number of probes is basically how many lists do we want to actually look in or consider we can go from one probe of",
    "start": "1875440",
    "end": "1882240"
  },
  {
    "text": "just considering the 500 vectors in the very nearest list to 10 probes and then we'll consider 5,000 additional factors",
    "start": "1882240",
    "end": "1890519"
  },
  {
    "text": "across 10 different lists U and that will make sure that if anything is on in of the edges uh we'll get that but again",
    "start": "1890519",
    "end": "1897639"
  },
  {
    "text": "this is you know 10 times more expensive um given the fixed cost of",
    "start": "1897639",
    "end": "1902880"
  },
  {
    "text": "2000 up front you can do the math but the run times are actually very predictable if you're doing this on a",
    "start": "1902880",
    "end": "1909200"
  },
  {
    "text": "CPU core it's always the same number of floating Point operations and so you can",
    "start": "1909200",
    "end": "1914240"
  },
  {
    "text": "measure you can say how much latency budget do I have have how important is",
    "start": "1914240",
    "end": "1919600"
  },
  {
    "text": "it to actually get every single record back or not um if you good",
    "start": "1919600",
    "end": "1927600"
  },
  {
    "text": "Yeahs was thatly great great question yeah you you",
    "start": "1927799",
    "end": "1934080"
  },
  {
    "text": "actually need to build build these um with the matching operator that you will",
    "start": "1934080",
    "end": "1940159"
  },
  {
    "text": "use and so in this case uh we we are doing a cosine operator against our",
    "start": "1940159",
    "end": "1946000"
  },
  {
    "text": "index so we generate the index to match against the cosign operator if you want to use the Idan distance or the dot",
    "start": "1946000",
    "end": "1952519"
  },
  {
    "text": "product you would substitute that here as well you can build as many of these indexes as you want so you could have",
    "start": "1952519",
    "end": "1958440"
  },
  {
    "text": "three indexes one for each operator if you wanted to cover it that way um we",
    "start": "1958440",
    "end": "1965279"
  },
  {
    "text": "there's a lot you can do with these indexes you can build partial indexes and postgress that only cover certain",
    "start": "1965279",
    "end": "1970679"
  },
  {
    "text": "portions of the table so let's say you have some other criteria in the table where like maybe it's some document is",
    "start": "1970679",
    "end": "1979679"
  },
  {
    "text": "part of a collection um and you only want to search for documents in collection 32 you can build a partial",
    "start": "1979679",
    "end": "1986760"
  },
  {
    "text": "index that only includes documents from collection 32 and actually what we found at",
    "start": "1986760",
    "end": "1993720"
  },
  {
    "text": "instacart was we didn't need Vector indexes because all of our queries were",
    "start": "1993720",
    "end": "2001320"
  },
  {
    "text": "scoped to very tight constraints where given given the user ID given the store",
    "start": "2001320",
    "end": "2007799"
  },
  {
    "text": "that they were shopping at given all of this extra criteria we could winnow down the number of possible vectors to some",
    "start": "2007799",
    "end": "2014760"
  },
  {
    "text": "subset far smaller than 10,000 and then we could just Brute Force the answer and it would be an exact answer U and it it",
    "start": "2014760",
    "end": "2023039"
  },
  {
    "text": "would be very fast so so what are the pairs I that's using like are the pairs from the documents or is one part of the",
    "start": "2023039",
    "end": "2030120"
  },
  {
    "text": "vector pair the search the search and the other part PA document",
    "start": "2030120",
    "end": "2037480"
  },
  {
    "text": "repos so the the question was um what are the what the pairs that we're calculating distance from and it's",
    "start": "2037480",
    "end": "2044679"
  },
  {
    "text": "whatever you want it to be uh in the in the clustering case you know the pair is going to be two documents in the query",
    "start": "2044679",
    "end": "2051919"
  },
  {
    "text": "case you you'll be comparing the incoming Vector the query Vector to a document vector and to actually all of",
    "start": "2051919",
    "end": "2058720"
  },
  {
    "text": "document vectors clustering is very very expensive operation because you actually have to compare every Vector to every",
    "start": "2058720",
    "end": "2065720"
  },
  {
    "text": "other vector to find out what vectors are closest to each other so this is like an ins squared operation on your",
    "start": "2065720",
    "end": "2072200"
  },
  {
    "text": "vector size uh creating these indexes is a timec consuming process and so you",
    "start": "2072200",
    "end": "2077638"
  },
  {
    "text": "might want to consider and if you've already created your index one of the very cool things about postgress and PG",
    "start": "2077639",
    "end": "2084599"
  },
  {
    "text": "Vector is that anytime you insert a new record into the table it goes into the",
    "start": "2084599",
    "end": "2089960"
  },
  {
    "text": "index the index is persisted to dis if your database catches on fire or whatever you can be sure that that uh",
    "start": "2089960",
    "end": "2098079"
  },
  {
    "text": "that data is safe and sound as as long as you've gotten the response back from postgress that in fact your transaction",
    "start": "2098079",
    "end": "2104640"
  },
  {
    "text": "has completed but for the other first case where you have a search query and you're trying to match it to Something",
    "start": "2104640",
    "end": "2110079"
  },
  {
    "text": "in the repository you wouldn't use an index right because index has to happen at before",
    "start": "2110079",
    "end": "2116920"
  },
  {
    "text": "search uh that that's correct you have to create the index before you can use the index if you don't have an index on",
    "start": "2116920",
    "end": "2123440"
  },
  {
    "text": "the table and you run that query let's see like this um postgress will accept this query with",
    "start": "2123440",
    "end": "2129440"
  },
  {
    "text": "or without an index postgress has a very Advanced query planner it will look at all of the indexes on the table given a",
    "start": "2129440",
    "end": "2136599"
  },
  {
    "text": "particular query uh and it will use statistics about the data the number of rows uh the different predicates in your",
    "start": "2136599",
    "end": "2143640"
  },
  {
    "text": "query in this case the query has no predicate um and if the table has no Index this will be a full table scan it",
    "start": "2143640",
    "end": "2150560"
  },
  {
    "text": "will actually do this comparison against every Vector now if you have an index on the other hand it will use the index to",
    "start": "2150560",
    "end": "2158079"
  },
  {
    "text": "take this shortcut that I've described and so that's the magic that PG Vector gives",
    "start": "2158079",
    "end": "2164280"
  },
  {
    "text": "you in this case like this is for the clustering case right where you're doing all n pairs in a joint table called",
    "start": "2164280",
    "end": "2171040"
  },
  {
    "text": "Vector that's what this example um this this is a very generic example so I",
    "start": "2171040",
    "end": "2177400"
  },
  {
    "text": "probably could have picked something more more explicit you could say that the left hand side is query vector and",
    "start": "2177400",
    "end": "2183319"
  },
  {
    "text": "the right hand side is documents. embedding and in that case that's what you would write and then you would be",
    "start": "2183319",
    "end": "2189720"
  },
  {
    "text": "selecting from documents uh and that that's how you would query that table but again you would have to generate",
    "start": "2189720",
    "end": "2197319"
  },
  {
    "text": "that user query um at runtime because it is they're what they're going to give",
    "start": "2197319",
    "end": "2202720"
  },
  {
    "text": "you is a piece of text so then you would call pgml embed on that piece of text",
    "start": "2202720",
    "end": "2207960"
  },
  {
    "text": "and I think we'll get into more examples like that in in a little bit further down um so I'm going to go I think we've",
    "start": "2207960",
    "end": "2213480"
  },
  {
    "text": "covered indexing and some of the trade-offs there there's there's more work being done on",
    "start": "2213480",
    "end": "2219319"
  },
  {
    "text": "PG Vector right now to in improve there's there's there's a lot of",
    "start": "2219319",
    "end": "2224520"
  },
  {
    "text": "research going on around Vector indexes there's a lot of tradeoffs there's at least a dozen different Vector indexing",
    "start": "2224520",
    "end": "2231760"
  },
  {
    "text": "algorithms but they are all some kind of trade-off between build cost runtime",
    "start": "2231760",
    "end": "2237680"
  },
  {
    "text": "cost accuracy um IVF flat is a is a pretty good first first",
    "start": "2237680",
    "end": "2244280"
  },
  {
    "text": "stab that that that makes pretty balanc trade-offs amongst all of those things",
    "start": "2244280",
    "end": "2250480"
  },
  {
    "text": "um so I I think said to your question this is a better example of what you",
    "start": "2250480",
    "end": "2256280"
  },
  {
    "text": "might do and in postgress uh if you if you want to start building these queries in postgress",
    "start": "2256280",
    "end": "2264240"
  },
  {
    "text": "um let's pretend that we've taken there's there's a blog post I will mention on the website that walks",
    "start": "2266079",
    "end": "2273119"
  },
  {
    "text": "through all of this in in depth of what you might want to do but the you have the let let's say we",
    "start": "2273119",
    "end": "2280720"
  },
  {
    "text": "have a set of the Amazon movie reviews from all customer reviews for all of the",
    "start": "2280720",
    "end": "2287839"
  },
  {
    "text": "Amazon DVDs for sale so there are millions of these customer reviews we",
    "start": "2287839",
    "end": "2293119"
  },
  {
    "text": "can create embeddings out of all those customer reviews we can store them in a table um and that'll be our documents",
    "start": "2293119",
    "end": "2299960"
  },
  {
    "text": "table for this example where we have millions of user reviews now when somebody comes in with a query and they",
    "start": "2299960",
    "end": "2306040"
  },
  {
    "text": "want to find a movie similar to that query we can use pgml to generate the embedding for that",
    "start": "2306040",
    "end": "2314160"
  },
  {
    "text": "in this case we're looking for the best 1980 sci-fi movie and some people may describe in",
    "start": "2314160",
    "end": "2321079"
  },
  {
    "text": "their review they may use the keyword best 1980s sci-fi movie but they might also say best 1970s sci-fi movie or",
    "start": "2321079",
    "end": "2328359"
  },
  {
    "text": "worst 1980s sci-fi movie and it's these like very subtle modifiers and nuances",
    "start": "2328359",
    "end": "2334200"
  },
  {
    "text": "that embeddings are really magical about capturing the entire sentiment and not just over indexing on particular",
    "start": "2334200",
    "end": "2341240"
  },
  {
    "text": "keywords so so the quality you can get with a recall like this is pretty cool this when you have a with request",
    "start": "2341240",
    "end": "2349359"
  },
  {
    "text": "as um this creates a virtual table in postgress this is called a Common Table expression you can begin any query with",
    "start": "2349359",
    "end": "2356880"
  },
  {
    "text": "this and the content is any other query whatever comes out of that query in this",
    "start": "2356880",
    "end": "2361920"
  },
  {
    "text": "case it'll be an embedding um so for the rest of this query and we can chain",
    "start": "2361920",
    "end": "2367240"
  },
  {
    "text": "multiple ones of these that's what the select dot dot dot is going to do in the next slide um but we'll have we'll have",
    "start": "2367240",
    "end": "2375240"
  },
  {
    "text": "a virtual table in memory it will have one row in it that row will be a single Vector named embedding and it'll be",
    "start": "2375240",
    "end": "2382079"
  },
  {
    "text": "generated on the fly from the best 1980 sci-fi movie using our model so then we",
    "start": "2382079",
    "end": "2387359"
  },
  {
    "text": "can actually do the cosine operator because we've actually built a an index",
    "start": "2387359",
    "end": "2394280"
  },
  {
    "text": "on this table in the previous slide this will be very fast this will be very efficient um and you know we we get the",
    "start": "2394280",
    "end": "2402160"
  },
  {
    "text": "full power of SQL this doesn't have a limit you probably should put a limit otherwise you're going to be pulling",
    "start": "2402160",
    "end": "2408079"
  },
  {
    "text": "back five million documents and postgress will know that it'll know that there's five million documents and you",
    "start": "2408079",
    "end": "2413119"
  },
  {
    "text": "didn't put a limit so it'll just ignore your index because it's not going to do any good anyway because you're going to need to calculate the five million dot",
    "start": "2413119",
    "end": "2419960"
  },
  {
    "text": "products anyway um because that's what you're actually selecting you're",
    "start": "2419960",
    "end": "2425400"
  },
  {
    "text": "selecting the cosine distance so the postrest query planner is smart it will try to help you out but you also need to",
    "start": "2425400",
    "end": "2432880"
  },
  {
    "text": "be a little bit diligent when you're thinking about what do I really want what do I really",
    "start": "2432880",
    "end": "2439079"
  },
  {
    "text": "need the post crml isn't only limited to embeddings um there's lots of things",
    "start": "2441359",
    "end": "2447800"
  },
  {
    "text": "that you can do with these large language models uh hugging face has this concept of tasks that we've also adopted",
    "start": "2447800",
    "end": "2454880"
  },
  {
    "text": "uh some of these tasks it can be text generation it can be text classification it can be translation from one language",
    "start": "2454880",
    "end": "2461920"
  },
  {
    "text": "to another uh there's several dozen of these but this is an example of using",
    "start": "2461920",
    "end": "2467359"
  },
  {
    "text": "pgml transform which is a second function that will also download a model from hugging face in this case we",
    "start": "2467359",
    "end": "2474000"
  },
  {
    "text": "haven't even specified the the model hugging face has some default model this is a bit of a foot gun because they may",
    "start": "2474000",
    "end": "2481240"
  },
  {
    "text": "change that default model and your application may be using a new model that you haven't tested uh so it you",
    "start": "2481240",
    "end": "2488839"
  },
  {
    "text": "know be be wary H but you can also specify the model to this function",
    "start": "2488839",
    "end": "2494839"
  },
  {
    "text": "um although there I was trying to keep these slides slightly simple um they do take keyword arguments and you can",
    "start": "2494839",
    "end": "2500760"
  },
  {
    "text": "pretty anything you can specify to any of these API models there's a way to pass the keyword arguments down all the",
    "start": "2500760",
    "end": "2507800"
  },
  {
    "text": "way through um the output of this uh example here",
    "start": "2507800",
    "end": "2516480"
  },
  {
    "text": "will be whatever the output of the hugging",
    "start": "2516480",
    "end": "2521760"
  },
  {
    "text": "face uh model would be so in this case hugging face returns some",
    "start": "2521760",
    "end": "2527640"
  },
  {
    "text": "Json it's going to have like a sentiment colon floating Point number postgress",
    "start": "2527640",
    "end": "2533839"
  },
  {
    "text": "has all of the Json operators you need so then you can dig into that Json object coming",
    "start": "2533839",
    "end": "2541558"
  },
  {
    "text": "back and and this is where I think think the true power of you know SQL as a",
    "start": "2543559",
    "end": "2549480"
  },
  {
    "text": "platform becomes really interesting because if you think about all of the",
    "start": "2549480",
    "end": "2555839"
  },
  {
    "text": "thousands of chat Bots being built with Lang chain today that are that you know it's sort of like the iPhone moment when",
    "start": "2555839",
    "end": "2562520"
  },
  {
    "text": "everybody was building a flashlight app back in 2008 uh the very the very standard model",
    "start": "2562520",
    "end": "2569280"
  },
  {
    "text": "is I I have my user prompt that I want",
    "start": "2569280",
    "end": "2574480"
  },
  {
    "text": "to chat with you take that you pass it to open AI That's a remote data center call it takes several hundred",
    "start": "2574480",
    "end": "2581200"
  },
  {
    "text": "milliseconds uh they have to actually then run their model on it they give you back no longer one of the best quality",
    "start": "2581200",
    "end": "2587280"
  },
  {
    "text": "embeddings but they'll charge you for it anyway then you take that embedding you pass it to your vector database you look",
    "start": "2587280",
    "end": "2594760"
  },
  {
    "text": "up a bunch of context that you want to use for prompt engineering with your chatbot maybe those are help documents",
    "start": "2594760",
    "end": "2602319"
  },
  {
    "text": "that your support center has or whatever maybe that's a movie catalog because this is a movie Guru chatbot um you you",
    "start": "2602319",
    "end": "2610480"
  },
  {
    "text": "get that context that you've written all these documents about uh movies or about",
    "start": "2610480",
    "end": "2616960"
  },
  {
    "text": "help for your support center you pull back the English language text out of your database and you pull back as much",
    "start": "2616960",
    "end": "2624119"
  },
  {
    "text": "English language text as open AI models will take in their context Windows you",
    "start": "2624119",
    "end": "2629920"
  },
  {
    "text": "put your prompt on that you say given this information you are a helpful chatbot please respond to this prompt",
    "start": "2629920",
    "end": "2636720"
  },
  {
    "text": "and you paste all that together you send off you know 30 30 kilobytes worth of data to open AI again they run it",
    "start": "2636720",
    "end": "2642839"
  },
  {
    "text": "through their text generation model they send you a response back then you go ahead and um send that response back to",
    "start": "2642839",
    "end": "2650079"
  },
  {
    "text": "the user with post crml you can do all of that in a single query inside the database so there's no network Transit",
    "start": "2650079",
    "end": "2657880"
  },
  {
    "text": "there's no memory copies you you would you know you you create your embedding in the first CTE you retrieve all of the",
    "start": "2657880",
    "end": "2666640"
  },
  {
    "text": "re relevant documents in the second CTE and then in the third query you you pass",
    "start": "2666640",
    "end": "2673599"
  },
  {
    "text": "those as concatenated inputs with your prompt to another large language model and you can actually start stacking and",
    "start": "2673599",
    "end": "2679720"
  },
  {
    "text": "chaining models with these CTE as much as you want uh one of the things that we",
    "start": "2679720",
    "end": "2684920"
  },
  {
    "text": "found very useful is that Co cosine similarity is okay uh it it's good and",
    "start": "2684920",
    "end": "2690000"
  },
  {
    "text": "fast relative to say an XG boost model that ranks but an XG boost model that ranks is going to be much more accurate",
    "start": "2690000",
    "end": "2697760"
  },
  {
    "text": "if you're if rankings really matter so so you might actually want a",
    "start": "2697760",
    "end": "2703440"
  },
  {
    "text": "step number three to be reranked with XG boost and then step number four to be um",
    "start": "2703440",
    "end": "2709000"
  },
  {
    "text": "go ahead can be an online end access you",
    "start": "2709000",
    "end": "2715040"
  },
  {
    "text": "don't want to load a model in your datab call open",
    "start": "2715040",
    "end": "2722160"
  },
  {
    "text": "um I the task the task can it be a remote execution can the task be a",
    "start": "2724599",
    "end": "2730800"
  },
  {
    "text": "normal execution form it's going to happen in the database the transform is going to happen in the database so all",
    "start": "2730800",
    "end": "2736880"
  },
  {
    "text": "of all of that text document for the prompt engineering and context will stay",
    "start": "2736880",
    "end": "2742079"
  },
  {
    "text": "inside of the database and so there will be a much lower network data transit",
    "start": "2742079",
    "end": "2747200"
  },
  {
    "text": "cost um it it's it's going to be pointers in memory or a M Copy if",
    "start": "2747200",
    "end": "2752640"
  },
  {
    "text": "necessary and so you're you're talking about many kilobytes potentially megabytes worth of data that doesn't",
    "start": "2752640",
    "end": "2758440"
  },
  {
    "text": "need to get sent over the wire and so we get uh you know compared to open Ai and",
    "start": "2758440",
    "end": "2763720"
  },
  {
    "text": "pine cone implementations doing the same thing it's usually a 10x speed",
    "start": "2763720",
    "end": "2769720"
  },
  {
    "text": "up I think the question was can the model be hosted somewhere else not on",
    "start": "2769720",
    "end": "2775640"
  },
  {
    "text": "your database you you could um you could there if you wanted to do it with postgress ML you could have a foreign",
    "start": "2775640",
    "end": "2782400"
  },
  {
    "text": "data wrapper to a different database instance um and you can set up that and you can query that different database",
    "start": "2782400",
    "end": "2788040"
  },
  {
    "text": "instance if you wanted to retrieve all of your documents from postgress into your python application and then send",
    "start": "2788040",
    "end": "2794040"
  },
  {
    "text": "those off to open Ai and do a hybrid approach you can do that absolutely um",
    "start": "2794040",
    "end": "2799280"
  },
  {
    "text": "but you're losing you're then now paying the the network transit",
    "start": "2799280",
    "end": "2804839"
  },
  {
    "text": "cost I think I think we're almost done uh but I want to close with this SDK",
    "start": "2807880",
    "end": "2814040"
  },
  {
    "text": "slide I I've just showed you know we we covered machine learning in theory we",
    "start": "2814040",
    "end": "2819240"
  },
  {
    "text": "covered SQL and how to do all this in depth we've we've gone a step further",
    "start": "2819240",
    "end": "2824760"
  },
  {
    "text": "and we've created a python SDK that implements all of these SQL queries for you it gives you again the functions",
    "start": "2824760",
    "end": "2831880"
  },
  {
    "text": "with inputs and outputs that you actually want and need to know you connect your python application to a",
    "start": "2831880",
    "end": "2838040"
  },
  {
    "text": "postgress database you create a collection of documents you give it some arbitrary name for your collection you",
    "start": "2838040",
    "end": "2844920"
  },
  {
    "text": "can then upsert documents into that collection if you want to go ahead and generate the indexes you can do that",
    "start": "2844920",
    "end": "2851800"
  },
  {
    "text": "with a single function call um will enforce all of the best practices and the most efficient query patterns with",
    "start": "2851800",
    "end": "2858520"
  },
  {
    "text": "this SDK and finally you can just do your vector search and you can get those documents back that you might then want",
    "start": "2858520",
    "end": "2865040"
  },
  {
    "text": "to pass on to open AI as as you mentioned so we're extending this though",
    "start": "2865040",
    "end": "2870920"
  },
  {
    "text": "to create chainable API SDK function calls so that you can do a vector search",
    "start": "2870920",
    "end": "2878200"
  },
  {
    "text": "and then instead of materializing the results immediately you can call do text generation and you can actually do it",
    "start": "2878200",
    "end": "2885040"
  },
  {
    "text": "all in a single execution for to get the efficiency back at the application",
    "start": "2885040",
    "end": "2891440"
  },
  {
    "text": "layer uh it's it's worth mentioning that if you try to do a bunch of llms inside",
    "start": "2892240",
    "end": "2899319"
  },
  {
    "text": "of the database inside of a single database you're probably going to knock that database over pretty quickly um",
    "start": "2899319",
    "end": "2905720"
  },
  {
    "text": "pgat I think I mentioned early on is our other project that acts as a router it",
    "start": "2905720",
    "end": "2912559"
  },
  {
    "text": "handles charting it handles replication it handles failover and load balancing across many different postgress replicas",
    "start": "2912559",
    "end": "2919760"
  },
  {
    "text": "and so you might want to look into that project as",
    "start": "2919760",
    "end": "2924599"
  },
  {
    "text": "[Music]",
    "start": "2927240",
    "end": "2932880"
  },
  {
    "text": "well",
    "start": "2932880",
    "end": "2935880"
  }
]