[
  {
    "start": "0",
    "end": "151000"
  },
  {
    "text": "hi folks my name is Sharon I'm a software engineer at lyft and today I",
    "start": "3920",
    "end": "10370"
  },
  {
    "text": "will be talking about how we incorporated streaming into a machine learning platform and why we need it and",
    "start": "10370",
    "end": "17510"
  },
  {
    "text": "all the steps we went through all the hoops we had to jump to get it to work",
    "start": "17510",
    "end": "22910"
  },
  {
    "text": "properly for us and just an FYI I've had like six cups of coffee to fight jet lag",
    "start": "22910",
    "end": "28970"
  },
  {
    "text": "so if I sprout wings and fly off that's why so how many of you have used or have",
    "start": "28970",
    "end": "37729"
  },
  {
    "text": "heard of lyft before quite a few of you and and how many of",
    "start": "37729",
    "end": "43220"
  },
  {
    "text": "you have used any other ride-sharing service uber okay great so for those who",
    "start": "43220",
    "end": "48650"
  },
  {
    "text": "are not familiar lyft is a ride-sharing service that connects passengers with",
    "start": "48650",
    "end": "56000"
  },
  {
    "text": "drivers through our platform so when you use the lyft app when you want to go to",
    "start": "56000",
    "end": "62049"
  },
  {
    "text": "some place you open the lyft app you enter your destination and lyft gives",
    "start": "62049",
    "end": "67310"
  },
  {
    "text": "you a few options to get there you can take a shared ride you can take a luxury car and gives you an ETA a price",
    "start": "67310",
    "end": "75799"
  },
  {
    "text": "estimate and so on and then when you pick an option lift platform matches you",
    "start": "75799",
    "end": "81020"
  },
  {
    "text": "with a nearby driver who takes you to your destination so as you can imagine",
    "start": "81020",
    "end": "86539"
  },
  {
    "text": "in this whole interaction in the world of ride-sharing there are a lot of",
    "start": "86539",
    "end": "91700"
  },
  {
    "text": "decisions that depend on the most recent state of the world for example ETA",
    "start": "91700",
    "end": "98420"
  },
  {
    "text": "calculation would depend on traffic whether if there is a road closure or an",
    "start": "98420",
    "end": "104299"
  },
  {
    "text": "accident that may impact the ETA pricing depends on supply and demand and demand",
    "start": "104299",
    "end": "110509"
  },
  {
    "text": "can vary a lot based on the time of the day based on similar things like weather",
    "start": "110509",
    "end": "115579"
  },
  {
    "text": "traffic if there's a train cancelation they may suddenly be a higher demand for lift for example so to solve the all",
    "start": "115579",
    "end": "124100"
  },
  {
    "text": "these problems we use machine learning to make predictions about what ETA",
    "start": "124100",
    "end": "129740"
  },
  {
    "text": "should be pricing should be and so on but a machine learning algorithms are",
    "start": "129740",
    "end": "134959"
  },
  {
    "text": "only as good as the data that is used called the the algorithms and that is",
    "start": "134959",
    "end": "141300"
  },
  {
    "text": "where streaming comes in and apart from that there are a lot of instances like fraud as well that we want to fight",
    "start": "141300",
    "end": "148110"
  },
  {
    "text": "where real time information is super critical which so - talking about fraud",
    "start": "148110",
    "end": "155370"
  },
  {
    "start": "151000",
    "end": "286000"
  },
  {
    "text": "I want to tell you a short story a story about Alex and Tracy so alex is as a",
    "start": "155370",
    "end": "165299"
  },
  {
    "text": "driver on lyft and this is a totally fictional but a totally plausible story so alex is a driver and lyft and he got",
    "start": "165299",
    "end": "172920"
  },
  {
    "text": "a ride request from someone and he is on his way to pick that person up and while",
    "start": "172920",
    "end": "178980"
  },
  {
    "text": "he is driving he gets a call from Tracy and Tracy says hey Alex you are an",
    "start": "178980",
    "end": "186060"
  },
  {
    "text": "amazing driver on a platform and we want to reward you so we want to offer you $200 so alex is obviously really happy",
    "start": "186060",
    "end": "195510"
  },
  {
    "text": "and so he and then he carries on the conversation now Tracy says but I see",
    "start": "195510",
    "end": "204720"
  },
  {
    "text": "that you are in the middle of the middle of a write you're going to pick someone up so let me go ahead and cancel your",
    "start": "204720",
    "end": "210900"
  },
  {
    "text": "write from the backend so that you can pull to the side of the road and we can continue this conversation so at this",
    "start": "210900",
    "end": "218400"
  },
  {
    "text": "point alex is thinking oh I mean Tracy has to be from lyft if she can cancel my ride",
    "start": "218400",
    "end": "225540"
  },
  {
    "text": "from the back end so he trusts her they continue this conversation and but at",
    "start": "225540",
    "end": "233609"
  },
  {
    "text": "this time Tracy says between before I give you the $200 I want to verify your",
    "start": "233609",
    "end": "241230"
  },
  {
    "text": "identity and I will be sending you a verification code can you repeat that to",
    "start": "241230",
    "end": "247859"
  },
  {
    "text": "me at this point I guess you all know where this is going so Tracy is not",
    "start": "247859",
    "end": "252930"
  },
  {
    "text": "really a support agent she's an imposter and our poor gullible Alex gives her the",
    "start": "252930",
    "end": "258750"
  },
  {
    "text": "verification code and oops now she has access to his driver account and she can",
    "start": "258750",
    "end": "265560"
  },
  {
    "text": "do quite a bit of damage with that chicken she has access to his bank details she can take his",
    "start": "265560",
    "end": "271370"
  },
  {
    "text": "and stuff like that so we want to combat these kind of phishing attacks in real",
    "start": "271370",
    "end": "276830"
  },
  {
    "text": "time and a thing with fraudsters is they keep on changing their their schemes and",
    "start": "276830",
    "end": "282500"
  },
  {
    "text": "we want to evolve and keep up with that so to solve this problem research",
    "start": "282500",
    "end": "289790"
  },
  {
    "start": "286000",
    "end": "402000"
  },
  {
    "text": "scientists at lyft they figured out that there is a pattern to this there is a",
    "start": "289790",
    "end": "294889"
  },
  {
    "text": "pattern to all these fraudsters so if we can fingerprint if we can find these patterns in their behavior",
    "start": "294889",
    "end": "300949"
  },
  {
    "text": "we may be able to predict when a fraud is going to happen and combat that in real time for example in this case",
    "start": "300949",
    "end": "309710"
  },
  {
    "text": "Tracy was actually a passenger she requested a ride then she made driver",
    "start": "309710",
    "end": "315949"
  },
  {
    "text": "contact so in the lyft app once you are matched with a driver you're you can make you can call the driver through the",
    "start": "315949",
    "end": "322460"
  },
  {
    "text": "app so she did that and then she canceled the right taken together these",
    "start": "322460",
    "end": "329270"
  },
  {
    "text": "are these indicate like who veered sequence of events which doesn't occur",
    "start": "329270",
    "end": "334699"
  },
  {
    "text": "normally so this is a fingerprint that can tell you a little bit about whether this can be a potential for fraud so our",
    "start": "334699",
    "end": "342620"
  },
  {
    "text": "research scientists decided to turn to deep learning and they wanted to use",
    "start": "342620",
    "end": "348370"
  },
  {
    "text": "convolutional neural networks to to look find these patterns and act on that and",
    "start": "348370",
    "end": "356560"
  },
  {
    "text": "this is the input that they wanted to give to the to the convolutional neural",
    "start": "356560",
    "end": "361970"
  },
  {
    "text": "network they wanted to get the last X events or the last X actions a person",
    "start": "361970",
    "end": "369740"
  },
  {
    "text": "took on on the platform historic context",
    "start": "369740",
    "end": "375590"
  },
  {
    "text": "was also important because just one of action doesn't tell you as much they wanted to look at what the person has",
    "start": "375590",
    "end": "383240"
  },
  {
    "text": "been doing in the past and they've needed event time processing so all",
    "start": "383240",
    "end": "388940"
  },
  {
    "text": "these actions needed to be taken in the context of when they actually happened",
    "start": "388940",
    "end": "394870"
  },
  {
    "text": "so this is an example of a situation where real-time features are super",
    "start": "394870",
    "end": "401930"
  },
  {
    "text": "important and this for this and may other such use cases we wanted to make",
    "start": "401930",
    "end": "407919"
  },
  {
    "start": "402000",
    "end": "865000"
  },
  {
    "text": "it really easy for our research scientists to be effective at their work and make a generation of streaming",
    "start": "407919",
    "end": "416319"
  },
  {
    "text": "features super easy so when we started thinking about building such a platform",
    "start": "416319",
    "end": "422110"
  },
  {
    "text": "so the first step was what would we use as a stream processing engine like what",
    "start": "422110",
    "end": "427989"
  },
  {
    "text": "would be the end of underlying engine and for that we turn towards flink",
    "start": "427989",
    "end": "435629"
  },
  {
    "text": "Apache fling is an open source framework it provides low latency stateful",
    "start": "436259",
    "end": "443439"
  },
  {
    "text": "computations on streaming data and you can achieve latency in the order of",
    "start": "443439",
    "end": "449409"
  },
  {
    "text": "milliseconds through that event time processing is supported which was really",
    "start": "449409",
    "end": "454719"
  },
  {
    "text": "important for us and what event and processing gives us there is",
    "start": "454719",
    "end": "459899"
  },
  {
    "text": "replayability first of all so you get you can you get the same results no matter how many times you play the",
    "start": "459899",
    "end": "467139"
  },
  {
    "text": "algorithm on the same set of data and exactly once processing for correctness",
    "start": "467139",
    "end": "473529"
  },
  {
    "text": "so no matter how many times the data is retried you always there is exactly once processing then failure recovery was",
    "start": "473529",
    "end": "481419"
  },
  {
    "text": "also important in a distributed system anything can happen things go down we want our system to gracefully recover",
    "start": "481419",
    "end": "488139"
  },
  {
    "text": "from that and the other thing was ease of use of link provides a sequel API so",
    "start": "488139",
    "end": "494289"
  },
  {
    "text": "stateful computations can be can be specified in terms of sequel which was",
    "start": "494289",
    "end": "500319"
  },
  {
    "text": "really desirable for us so at lyft whenever any action happens",
    "start": "500319",
    "end": "509379"
  },
  {
    "text": "on the platform whenever user takes any action events are generated and I was",
    "start": "509379",
    "end": "514839"
  },
  {
    "text": "listening to Benjamin's talk earlier and he described events in a really excellent way so events are something",
    "start": "514839",
    "end": "521740"
  },
  {
    "text": "that happens so in this case for example someone took a ride at time t0 so that's",
    "start": "521740",
    "end": "528880"
  },
  {
    "text": "an event so these events are generated on the platform our event ingestion",
    "start": "528880",
    "end": "534639"
  },
  {
    "text": "pipeline ingests all these vents and these are streamed into multiple",
    "start": "534639",
    "end": "541689"
  },
  {
    "text": "Kinesis streams and also persisted in s3 for offline computation for just to",
    "start": "541689",
    "end": "548110"
  },
  {
    "text": "persisted for historic reasons now the other important thing I want to talk",
    "start": "548110",
    "end": "555369"
  },
  {
    "text": "about is why event time processing is important and why it's tricky in a",
    "start": "555369",
    "end": "561189"
  },
  {
    "text": "streaming world why it's tricky to understand the difference between processing and event time so processing",
    "start": "561189",
    "end": "569139"
  },
  {
    "text": "time is the time when your system",
    "start": "569139",
    "end": "574179"
  },
  {
    "text": "observes an event observes in event and this time is maintained by the processor",
    "start": "574179",
    "end": "580509"
  },
  {
    "text": "itself so when when a processor looks at the event it looks at the clock and that's the processing time event time on",
    "start": "580509",
    "end": "587110"
  },
  {
    "text": "the other hand is the time at which an event actually happened and this is",
    "start": "587110",
    "end": "592899"
  },
  {
    "text": "supplied by the event so that your event will come with a timestamp in them in a",
    "start": "592899",
    "end": "600730"
  },
  {
    "text": "perfect world when there are no Layton sees everything is great our events",
    "start": "600730",
    "end": "606639"
  },
  {
    "text": "would be somewhere on this axis so there is no latency between event time and",
    "start": "606639",
    "end": "612490"
  },
  {
    "text": "processing time but in reality it will be something more like this nine over",
    "start": "612490",
    "end": "618910"
  },
  {
    "text": "there so this nine has an event time of 12:01 but a processing time of 1208 so",
    "start": "618910",
    "end": "625480"
  },
  {
    "text": "our system saw it seven minutes later and then things can be like completely",
    "start": "625480",
    "end": "634119"
  },
  {
    "text": "out of order right like complete madness all out of order so Star Wars Episode",
    "start": "634119",
    "end": "639490"
  },
  {
    "text": "four five and six came first I don't know why and then one two and three so",
    "start": "639490",
    "end": "646499"
  },
  {
    "text": "so what do you do in this in such a world where things are out of order and",
    "start": "646499",
    "end": "652029"
  },
  {
    "text": "there are latency is unpredictable agencies now suppose we want to",
    "start": "652029",
    "end": "658509"
  },
  {
    "text": "calculate integer sum in two minute windows suppose that's the problem we",
    "start": "658509",
    "end": "663759"
  },
  {
    "text": "are trying to solve in a batch world suppose you are running a MapReduce job",
    "start": "663759",
    "end": "669669"
  },
  {
    "text": "on data you run that on complete data set so before you start the program your data is fully available",
    "start": "669669",
    "end": "677639"
  },
  {
    "text": "so in that in that situation it's easy so our MapReduce program starts looking",
    "start": "677639",
    "end": "684220"
  },
  {
    "text": "at the data and then it slots everything into Windows and then it produces the results once it has seen all the data so",
    "start": "684220",
    "end": "691810"
  },
  {
    "text": "you're sure to receive correct results but in this in a streaming world data is",
    "start": "691810",
    "end": "697720"
  },
  {
    "text": "never complete it's an infinite constantly arriving set of events so how",
    "start": "697720",
    "end": "704259"
  },
  {
    "text": "do you know when to how long to wait or when to finish processing or commit",
    "start": "704259",
    "end": "711459"
  },
  {
    "text": "these windows so for this the data flow",
    "start": "711459",
    "end": "717240"
  },
  {
    "text": "data flow model which was introduced at Google they came up with this property",
    "start": "717240",
    "end": "724720"
  },
  {
    "text": "of this thing called watermarks and a passive link is actually based on the data flow model so the simplest way to",
    "start": "724720",
    "end": "732610"
  },
  {
    "text": "understand a watermark is a watermark is the way time flows through a streaming",
    "start": "732610",
    "end": "738519"
  },
  {
    "text": "system so what marks are like markers that are like that are in your stream",
    "start": "738519",
    "end": "746439"
  },
  {
    "text": "along with your data these watermarks will flow through and when the system sees the event a",
    "start": "746439",
    "end": "752589"
  },
  {
    "text": "watermark what it tells the system is you can stop waiting for events having",
    "start": "752589",
    "end": "759250"
  },
  {
    "text": "timestamp smaller than mine so when the system sees this what about 1202 it",
    "start": "759250",
    "end": "766720"
  },
  {
    "text": "knows that I should have received everything having a lower time stone time stamped in this so if there are any",
    "start": "766720",
    "end": "774100"
  },
  {
    "text": "windows that need to be committed I can go ahead and do that now our system it's",
    "start": "774100",
    "end": "781630"
  },
  {
    "text": "now in the streaming world or whenever the system sees the watermark it commits",
    "start": "781630",
    "end": "787029"
  },
  {
    "text": "results then so we have low latency with the system doesn't have to wait",
    "start": "787029",
    "end": "792279"
  },
  {
    "text": "endlessly and as a developer you can choose what your watermarking strategy",
    "start": "792279",
    "end": "799089"
  },
  {
    "text": "should be so if you know that your p99 latency is 5 minutes you can account for",
    "start": "799089",
    "end": "804220"
  },
  {
    "text": "that when what amongst so now we get low latency",
    "start": "804220",
    "end": "809779"
  },
  {
    "text": "we get results as quickly as possible but still data can arrive late like if",
    "start": "809779",
    "end": "815450"
  },
  {
    "text": "you are what about strategy is not conservative enough or by an off chance when there are backfills happening and",
    "start": "815450",
    "end": "822460"
  },
  {
    "text": "data is coming in late it can still get missed out like that nine over there but",
    "start": "822460",
    "end": "830839"
  },
  {
    "text": "flink has a remedy for this too so fling in flink you can read trigger",
    "start": "830839",
    "end": "836630"
  },
  {
    "text": "windows when when late data writes so you can trigger windows once when the",
    "start": "836630",
    "end": "842690"
  },
  {
    "text": "system sees the watermark and then when the late event arise it can read trigger it so you have low latency with eventual",
    "start": "842690",
    "end": "850690"
  },
  {
    "text": "correctness and this was really desirable for us because we wanted eventually correct data but we also",
    "start": "850690",
    "end": "857180"
  },
  {
    "text": "wanted our system to get results as quickly as possible even though they",
    "start": "857180",
    "end": "862220"
  },
  {
    "text": "were like one-off so with the engine taken care of the second thing we",
    "start": "862220",
    "end": "868490"
  },
  {
    "start": "865000",
    "end": "1123000"
  },
  {
    "text": "started thinking of was the usability so our users our data scientists and",
    "start": "868490",
    "end": "876230"
  },
  {
    "text": "research scientists and we started thinking about what do they care most so",
    "start": "876230",
    "end": "882500"
  },
  {
    "text": "they care most about model development about feature engineering data quality",
    "start": "882500",
    "end": "889370"
  },
  {
    "text": "and probably the least about figuring out compute resources dealing with all",
    "start": "889370",
    "end": "896060"
  },
  {
    "text": "the things related to distributed systems but often that becomes a bigger",
    "start": "896060",
    "end": "903560"
  },
  {
    "text": "chunk of the whole problem like they spend a lot of they end up spending a lot of time on data collection data",
    "start": "903560",
    "end": "910910"
  },
  {
    "text": "discovery scheme it ization compute resources so we wanted to abstract all",
    "start": "910910",
    "end": "916820"
  },
  {
    "text": "that out for them then we also started thinking about what a typical machine",
    "start": "916820",
    "end": "923060"
  },
  {
    "text": "learning workflow looks like so a bigger part of the machine learning workflow is",
    "start": "923060",
    "end": "929300"
  },
  {
    "text": "the data prep so finding out what sources are available cleaning it up generating features and only after that",
    "start": "929300",
    "end": "937310"
  },
  {
    "text": "comes the modeling and training and valuation and the data input and data prep is often",
    "start": "937310",
    "end": "944699"
  },
  {
    "text": "the most complicated and time-consuming aspect of doing anything machine",
    "start": "944699",
    "end": "949740"
  },
  {
    "text": "learning so with that in mind what came",
    "start": "949740",
    "end": "954779"
  },
  {
    "text": "about was something that we lovingly called drift because every project at lift has to have a y in its name",
    "start": "954779",
    "end": "962149"
  },
  {
    "text": "so this is our self-service streaming framework and the user plain which is",
    "start": "962149",
    "end": "968999"
  },
  {
    "text": "the only thing that the user interacts with is a simple UI that accepts a",
    "start": "968999",
    "end": "974699"
  },
  {
    "text": "declarative configuration as input and I will get to that in one minute the",
    "start": "974699",
    "end": "980519"
  },
  {
    "text": "control plane does all the heavy lifting of data discovery it does the query",
    "start": "980519",
    "end": "986579"
  },
  {
    "text": "analysis it figures out how much resources do I need to run the streaming program it does all of that and then",
    "start": "986579",
    "end": "993329"
  },
  {
    "text": "finally when features materialized they are written to several different places to kafka to DynamoDB which is our main",
    "start": "993329",
    "end": "1002089"
  },
  {
    "text": "feature storage druid for analytics hive and elastic search the user only need to",
    "start": "1002089",
    "end": "1013730"
  },
  {
    "text": "give us this like it's us like it's as simple as like a one-page declarative",
    "start": "1013730",
    "end": "1019250"
  },
  {
    "text": "configuration so users can specify their logic in terms of sequel and which was",
    "start": "1019250",
    "end": "1025788"
  },
  {
    "text": "great because fling sequel API is really powerful so almost 90% of our use cases can be expressed in in terms of fling",
    "start": "1025789",
    "end": "1033380"
  },
  {
    "text": "sequel and then they give us a simple job config which contains like metadata",
    "start": "1033380",
    "end": "1038990"
  },
  {
    "text": "about the job what the retention policy should be where the features need to go",
    "start": "1038990",
    "end": "1044089"
  },
  {
    "text": "any versioning information and and so on and this is what a general ecosystem of",
    "start": "1044089",
    "end": "1053539"
  },
  {
    "text": "our streaming service looks like the blue box on top that represents a user's",
    "start": "1053539",
    "end": "1061429"
  },
  {
    "text": "application so when the user gives us the declarative configuration and when",
    "start": "1061429",
    "end": "1066649"
  },
  {
    "text": "they hit submit on on the UI the application starts it starts consuming",
    "start": "1066649",
    "end": "1073190"
  },
  {
    "text": "data from the data sources applies the logic rights to a Kinesis tree then a",
    "start": "1073190",
    "end": "1080660"
  },
  {
    "text": "separate feature of fan-out job reads data from the from the skinny stream and",
    "start": "1080660",
    "end": "1088210"
  },
  {
    "text": "materializes it to all the things where it needs to go and the reason why we had",
    "start": "1088210",
    "end": "1093620"
  },
  {
    "text": "this two-step process is so that users user applications can run completely",
    "start": "1093620",
    "end": "1100070"
  },
  {
    "text": "agnostic of where the data needs to go so that way our things can evolve",
    "start": "1100070",
    "end": "1105910"
  },
  {
    "text": "irrespective of what's happening with the user app so the feature fan-out job actually does the work of any kind of",
    "start": "1105910",
    "end": "1113090"
  },
  {
    "text": "transformation managers throughputs to sync it does all everything that's",
    "start": "1113090",
    "end": "1119030"
  },
  {
    "text": "needed to materialize materialize the data and when we were building this we",
    "start": "1119030",
    "end": "1125510"
  },
  {
    "start": "1123000",
    "end": "1171000"
  },
  {
    "text": "also decided to eat our own dog food so the feature fan-out jobs are also",
    "start": "1125510",
    "end": "1131630"
  },
  {
    "text": "written using drifts declarative configuration and runs on flink and this",
    "start": "1131630",
    "end": "1137900"
  },
  {
    "text": "makes it really easy for us because now anyone like even in the middle of the night when we get paged like we are",
    "start": "1137900",
    "end": "1145970"
  },
  {
    "text": "you're writing too much too dynamodb our SRS can go and quickly change a config lower the right rate fix any small",
    "start": "1145970",
    "end": "1154220"
  },
  {
    "text": "parameters and quickly update the job like OneTouch so it's really really really easy no coding required and any",
    "start": "1154220",
    "end": "1161420"
  },
  {
    "text": "kind of transformation can also be expressed in terms of sequel so even though these feature fan-out jobs are",
    "start": "1161420",
    "end": "1167120"
  },
  {
    "text": "stateless we can still use sequel for that now let's talk about deployment and",
    "start": "1167120",
    "end": "1174380"
  },
  {
    "start": "1171000",
    "end": "1466000"
  },
  {
    "text": "this is where things got really interesting and we went through several roadblocks to figure out what worked for",
    "start": "1174380",
    "end": "1180650"
  },
  {
    "text": "us so previously until like about a year",
    "start": "1180650",
    "end": "1186170"
  },
  {
    "text": "ago our platform ran on AWS ec2",
    "start": "1186170",
    "end": "1192350"
  },
  {
    "text": "instances using our custom deployment there was basically one big cluster",
    "start": "1192350",
    "end": "1198350"
  },
  {
    "text": "there was one job manager cluster and the job managers only job is to submit",
    "start": "1198350",
    "end": "1204380"
  },
  {
    "text": "jobs or create tasks and submit it to another cluster which has a task manager cluster all our jobs and there were like",
    "start": "1204380",
    "end": "1212720"
  },
  {
    "text": "60 jobs running on the platform at that time they were all running on the same cluster and this is what it looked like",
    "start": "1212720",
    "end": "1222169"
  },
  {
    "text": "so all the jobs in one cluster so if one job takes down the whole cluster all the",
    "start": "1222169",
    "end": "1227630"
  },
  {
    "text": "jobs around and it was multi-tenancy health as you can imagine so at this",
    "start": "1227630",
    "end": "1235340"
  },
  {
    "text": "time kubernetes came to our rescue with kubernetes now we deploy each",
    "start": "1235340",
    "end": "1241970"
  },
  {
    "text": "application in its own separate cluster it has its own dedicated job manager",
    "start": "1241970",
    "end": "1247580"
  },
  {
    "text": "pods and task manager pods and they are there is separation of concerns so now",
    "start": "1247580",
    "end": "1253309"
  },
  {
    "text": "the problem with the problems of the multi-tenancy architecture is removed",
    "start": "1253309",
    "end": "1258940"
  },
  {
    "text": "another thing over here that I want to mention so because it was running on one",
    "start": "1258940",
    "end": "1265010"
  },
  {
    "text": "cluster compute resources were kind of manually computed so we would anticipate",
    "start": "1265010",
    "end": "1272480"
  },
  {
    "text": "okay by next week we might have like 60 jobs if each job needs like a",
    "start": "1272480",
    "end": "1279260"
  },
  {
    "text": "parallelism of 10 maybe we need 600 instances so we will do these manual",
    "start": "1279260",
    "end": "1284510"
  },
  {
    "text": "calculations when we deploy I've used to ask for 600 instances which is obviously not a great use of our time or a good",
    "start": "1284510",
    "end": "1292880"
  },
  {
    "text": "model so now so to solve all these",
    "start": "1292880",
    "end": "1299570"
  },
  {
    "text": "problems we built something called a fling kubernetes operator at lyft and",
    "start": "1299570",
    "end": "1305240"
  },
  {
    "text": "which is available in open source you can find that in the link over there the",
    "start": "1305240",
    "end": "1311620"
  },
  {
    "text": "this Swift fling kubernetes operator it's this operator is essentially like",
    "start": "1311620",
    "end": "1317270"
  },
  {
    "text": "you can think of it like a chrome cron job you provide it a custom resource",
    "start": "1317270",
    "end": "1323480"
  },
  {
    "text": "descriptor the scuba knit is operator is like constantly poling for custom resource descriptors it looks for any",
    "start": "1323480",
    "end": "1330919"
  },
  {
    "text": "new custom resource descriptor or any changes and it basically materializes",
    "start": "1330919",
    "end": "1337470"
  },
  {
    "text": "whatever your desires that you write in that custom resource descriptor into a cluster a custom resource descriptor",
    "start": "1337470",
    "end": "1347070"
  },
  {
    "text": "looks something like this so now in one page you can tell the flink kubernetes",
    "start": "1347070",
    "end": "1352679"
  },
  {
    "text": "operator what your cluster should look like so I can say that my cluster will",
    "start": "1352679",
    "end": "1358830"
  },
  {
    "text": "be running a flink application I can give it an image name and my image will contain all my my taco image will",
    "start": "1358830",
    "end": "1365460"
  },
  {
    "text": "contain all the dependencies and I can also say that this is my task manager",
    "start": "1365460",
    "end": "1371309"
  },
  {
    "text": "configuration I want all my task managers to have 15 gigs of memory for CPUs and so on and when the flink",
    "start": "1371309",
    "end": "1380760"
  },
  {
    "text": "kubernetes operator will read that and just make it happen so if I go back and",
    "start": "1380760",
    "end": "1386250"
  },
  {
    "text": "change this update this over here like to change the CPU to 1 I don't need to do anything because this cust the flink",
    "start": "1386250",
    "end": "1393270"
  },
  {
    "text": "kubernetes operator is constantly listening to these changes so it will listen to these changes and it will",
    "start": "1393270",
    "end": "1398309"
  },
  {
    "text": "update it so now this is what our entire deployment step looks like the user",
    "start": "1398309",
    "end": "1405840"
  },
  {
    "text": "writes a decorative configuration our control plane validates this",
    "start": "1405840",
    "end": "1413340"
  },
  {
    "text": "configuration does query analysis based on the query analysis it figures out how",
    "start": "1413340",
    "end": "1419070"
  },
  {
    "text": "much memory is required how much CPU what the parallelism should be and it generates the kubernetes CRD and from",
    "start": "1419070",
    "end": "1427919"
  },
  {
    "text": "there the flink kubernetes operator takes over and it deploys the application and everything is everything",
    "start": "1427919",
    "end": "1435690"
  },
  {
    "text": "just works fine so it's all that is completely isolated from the user with",
    "start": "1435690",
    "end": "1444090"
  },
  {
    "text": "flink on kubernetes as I mentioned we have each flink application runs on a separate cluster we were able to scale",
    "start": "1444090",
    "end": "1451440"
  },
  {
    "text": "to hundreds of flink applications and automatic updates as well is supported",
    "start": "1451440",
    "end": "1456600"
  },
  {
    "text": "and the best thing is resource calculation and allocation is done on a",
    "start": "1456600",
    "end": "1462990"
  },
  {
    "text": "per job basis now let's talk about bootstrapping this was another big",
    "start": "1462990",
    "end": "1470010"
  },
  {
    "start": "1466000",
    "end": "1862000"
  },
  {
    "text": "problem that we spent of time trying to solve so first let's",
    "start": "1470010",
    "end": "1475400"
  },
  {
    "text": "understand what is bootstrapping so let's look at this simple program so",
    "start": "1475400",
    "end": "1483660"
  },
  {
    "text": "here we want to count the number of rides a person took over a 30 day period",
    "start": "1483660",
    "end": "1492410"
  },
  {
    "text": "so the point to note here is this is a streaming application which is reading",
    "start": "1492410",
    "end": "1498510"
  },
  {
    "text": "streaming data and it is it wants to count the number of rides over 30 days",
    "start": "1498510",
    "end": "1504350"
  },
  {
    "text": "so now suppose I start the program today and it starts accumulating data in two",
    "start": "1504350",
    "end": "1510060"
  },
  {
    "text": "states for this job to correctly answer the query it needs to run for at least",
    "start": "1510060",
    "end": "1516300"
  },
  {
    "text": "30 days to have enough data to be able to answer this query correctly and",
    "start": "1516300",
    "end": "1521870"
  },
  {
    "text": "that's definitely not a good solution because what if after 30 days we realized that there was something wrong",
    "start": "1521870",
    "end": "1528510"
  },
  {
    "text": "with the logic and we need to update it so that's definitely not a scalable solution so what we what you can do is",
    "start": "1528510",
    "end": "1536430"
  },
  {
    "text": "we can bootstrap the first state looking back using historic information and then",
    "start": "1536430",
    "end": "1543600"
  },
  {
    "text": "we keep on updating the state as new data comes in through our streams which",
    "start": "1543600",
    "end": "1549270"
  },
  {
    "text": "is great in theory it works really well the problem arises when the sources that",
    "start": "1549270",
    "end": "1556200"
  },
  {
    "text": "you read from do not have 30 days worth of data which is often the case Kinesis",
    "start": "1556200",
    "end": "1562080"
  },
  {
    "text": "has maximum retention I think of seven days if I'm not wrong with Kafka also it's really expensive to",
    "start": "1562080",
    "end": "1569280"
  },
  {
    "text": "have like really long retention policy so you cannot have it's unless you have",
    "start": "1569280",
    "end": "1574650"
  },
  {
    "text": "an infinite retention source it's it's not possible to get all that data from",
    "start": "1574650",
    "end": "1580230"
  },
  {
    "text": "one source which was the case with us so what we did was we decided to read from",
    "start": "1580230",
    "end": "1588360"
  },
  {
    "text": "two sources so we would read real time in from a real time data from Kinesis",
    "start": "1588360",
    "end": "1595410"
  },
  {
    "text": "which has a retention policy of seven days and everything else from our s3",
    "start": "1595410",
    "end": "1601170"
  },
  {
    "text": "which is our historic source and we have the in s3 forever and in order to remove any",
    "start": "1601170",
    "end": "1609370"
  },
  {
    "text": "kind of duplicates when we are unioning these two streams we use like a target",
    "start": "1609370",
    "end": "1615010"
  },
  {
    "text": "time as a bridge between when you are unioning this data and this works great",
    "start": "1615010",
    "end": "1620890"
  },
  {
    "text": "this this is what we currently use so this is our bootstrapping strategy now",
    "start": "1620890",
    "end": "1627880"
  },
  {
    "text": "one thing to note here is when the job starts when it is bootstrapping it is reading like 30 days worth of data all",
    "start": "1627880",
    "end": "1635170"
  },
  {
    "text": "at once from s3 and we see this huge spike in the number of records being",
    "start": "1635170",
    "end": "1640960"
  },
  {
    "text": "processed once the job once the bootstrapping is over and it's just",
    "start": "1640960",
    "end": "1646890"
  },
  {
    "text": "processing real-time data it's now caching and in real time it's obviously it's not at the sale data is not being",
    "start": "1646890",
    "end": "1653380"
  },
  {
    "text": "read at the same rate all at once so the",
    "start": "1653380",
    "end": "1659440"
  },
  {
    "text": "problem with this is you can clearly see that during bootstrapping it needs a lot more resources to do its job and",
    "start": "1659440",
    "end": "1666390"
  },
  {
    "text": "bootstrapping time can vary between based on the kind of job it is so either",
    "start": "1666390",
    "end": "1673780"
  },
  {
    "text": "we could provision the job for the maximum require maximum whatever is",
    "start": "1673780",
    "end": "1680410"
  },
  {
    "text": "required for bootstrapping which is not great which is a really costly option or",
    "start": "1680410",
    "end": "1685660"
  },
  {
    "text": "we had to deal with it in some other way so what we did we would start the job",
    "start": "1685660",
    "end": "1693510"
  },
  {
    "text": "with a really high parallelism so that it has enough resources to go through",
    "start": "1693510",
    "end": "1700540"
  },
  {
    "text": "this huge backlog of installed data then our system will automatically detect",
    "start": "1700540",
    "end": "1707890"
  },
  {
    "text": "when bootstrapping is over and we do this based on watermarks so remember the",
    "start": "1707890",
    "end": "1715060"
  },
  {
    "text": "watermarks that I talked about earlier so our system would keep on listening to the watermarks to see how time is",
    "start": "1715060",
    "end": "1721110"
  },
  {
    "text": "progressing within the streaming system and when the watermark approaches",
    "start": "1721110",
    "end": "1726700"
  },
  {
    "text": "somewhere within a reasonable limit of of real-time we realize that ok",
    "start": "1726700",
    "end": "1733600"
  },
  {
    "text": "bootstrapping should be over and at that time we obtained it our CRD with fewer resources so that",
    "start": "1733600",
    "end": "1741550"
  },
  {
    "text": "the job update takes place and then our job can proceed with fewer resources so",
    "start": "1741550",
    "end": "1746980"
  },
  {
    "text": "we save on cost another thing that we",
    "start": "1746980",
    "end": "1753010"
  },
  {
    "text": "had to deal with was the output volume spike so because during bootstrapping our job",
    "start": "1753010",
    "end": "1759880"
  },
  {
    "text": "is reading a lot more data so understandably it's also producing a lot more data and how do you manage through",
    "start": "1759880",
    "end": "1769390"
  },
  {
    "text": "put in that situation like do you over-provision your sinks to handle that",
    "start": "1769390",
    "end": "1775270"
  },
  {
    "text": "huge spike during bootstrapping that can be a strategy but that is again really",
    "start": "1775270",
    "end": "1781150"
  },
  {
    "text": "costly like over provisioning for a short bootstrapping period so what we",
    "start": "1781150",
    "end": "1790000"
  },
  {
    "text": "did was we we realized that for us feature needs to be fresh but eventual",
    "start": "1790000",
    "end": "1797320"
  },
  {
    "text": "completeness is ok so instead of over-provisioning our sinks to handle",
    "start": "1797320",
    "end": "1804850"
  },
  {
    "text": "that really high spikey throughput during the bootstrap phase we decided to",
    "start": "1804850",
    "end": "1812140"
  },
  {
    "text": "write the data produce during bootstrap into a separate kinase history and then",
    "start": "1812140",
    "end": "1817540"
  },
  {
    "text": "the feature fan-out job that reads from the kinases stream would smooth it out and write it over a longer period of",
    "start": "1817540",
    "end": "1824410"
  },
  {
    "text": "time so now our sinks do not have to be over provision to handle that throughput",
    "start": "1824410",
    "end": "1829990"
  },
  {
    "text": "because they are just receiving results in in one rate and the data the results",
    "start": "1829990",
    "end": "1837970"
  },
  {
    "text": "produced during steady-state go through a high-priority Kinesis and they and",
    "start": "1837970",
    "end": "1844000"
  },
  {
    "text": "they are written to an idempotent sink and it's important for the sink to be",
    "start": "1844000",
    "end": "1850720"
  },
  {
    "text": "idempotent because there are two different sources now writing data to it so you need a way to make sure the think",
    "start": "1850720",
    "end": "1858100"
  },
  {
    "text": "knows how to dig up with that now that",
    "start": "1858100",
    "end": "1864160"
  },
  {
    "start": "1862000",
    "end": "2147000"
  },
  {
    "text": "is all well and good now two obstacles over the the another third thing that we",
    "start": "1864160",
    "end": "1869679"
  },
  {
    "text": "had to deal deal with was the times cube and that was a really interesting problem that it",
    "start": "1869679",
    "end": "1875809"
  },
  {
    "text": "took us a while to figure out so let's",
    "start": "1875809",
    "end": "1880909"
  },
  {
    "text": "think about what is happening during bootstrapping so I mentioned that our our program is reading from two sources",
    "start": "1880909",
    "end": "1888559"
  },
  {
    "text": "at the same time so we are getting really old data from s3 and we are getting like new fresh brand new data",
    "start": "1888559",
    "end": "1895309"
  },
  {
    "text": "from Genesis because we Union it all these all this data is going on and",
    "start": "1895309",
    "end": "1902720"
  },
  {
    "text": "updating windowed States so suppose our window is like two minute windows with all these older data is updating like",
    "start": "1902720",
    "end": "1909830"
  },
  {
    "text": "older two minute windows new data is coming in and updating new two minute windows the challenge arises because of",
    "start": "1909830",
    "end": "1918559"
  },
  {
    "text": "the way watermarks flow through the system so remember I mentioned watermark",
    "start": "1918559",
    "end": "1926450"
  },
  {
    "text": "is a way time moved through a streaming platform so watermark will be based on",
    "start": "1926450",
    "end": "1933980"
  },
  {
    "text": "whatever the oldest data is that is being processed right now so if your system is still processing old data",
    "start": "1933980",
    "end": "1941299"
  },
  {
    "text": "the watermark will not proceed time will not move forward until it is done with that and because watermark is not",
    "start": "1941299",
    "end": "1948889"
  },
  {
    "text": "proceeding all these windows are waiting open over there because they are only triggered once based on watermarks so",
    "start": "1948889",
    "end": "1958039"
  },
  {
    "text": "what we saw was like the state size explosion or Liguria the our memory usage just went up because of all these",
    "start": "1958039",
    "end": "1965570"
  },
  {
    "text": "open windows that are just waiting for watermark to proceed to solve this",
    "start": "1965570",
    "end": "1971179"
  },
  {
    "text": "problem we built source synchronization",
    "start": "1971179",
    "end": "1976610"
  },
  {
    "text": "into our sources the way source synchronization works now each source",
    "start": "1976610",
    "end": "1984620"
  },
  {
    "text": "each source task that is reading from either Kinesis or s3 they would publish",
    "start": "1984620",
    "end": "1992840"
  },
  {
    "text": "their watermark their view of the time in the system to a global state which is",
    "start": "1992840",
    "end": "1999860"
  },
  {
    "text": "accessible to all the consumers all that oken's humor tasks now each consumer task when",
    "start": "1999860",
    "end": "2007960"
  },
  {
    "text": "it's reading data from a source it puts it in a priority queue and then it looks",
    "start": "2007960",
    "end": "2014050"
  },
  {
    "text": "at the global water marks and it makes a decision it looks at the data that it has and and it questions itself like",
    "start": "2014050",
    "end": "2022120"
  },
  {
    "text": "based on this watermark is this data going to be used soon enough like am I is my data to new or is the watermark to",
    "start": "2022120",
    "end": "2031120"
  },
  {
    "text": "behind and if the watermark is to behind the global watermark is too old it means that I might just forward this data to",
    "start": "2031120",
    "end": "2039640"
  },
  {
    "text": "the other task and the state will be just sitting there and he'll not be used so the consumer task just forces over",
    "start": "2039640",
    "end": "2047440"
  },
  {
    "text": "there it pauses and then it waits for it keeps on polling the global watermark",
    "start": "2047440",
    "end": "2053230"
  },
  {
    "text": "State and it waits for time to advance in the system and when time has advanced",
    "start": "2053230",
    "end": "2060840"
  },
  {
    "text": "beyond a certain point at that time it starts proceeding with that data it",
    "start": "2060840",
    "end": "2067120"
  },
  {
    "text": "starts reading from the sources again so because of this problem because of this",
    "start": "2067120",
    "end": "2073540"
  },
  {
    "text": "waiting that we introduced now that now our sources wait to produce new data the",
    "start": "2073540",
    "end": "2080620"
  },
  {
    "text": "time moves forward and the issue that we saw with memory was removed now we have",
    "start": "2080620",
    "end": "2091080"
  },
  {
    "text": "with all these improvements we were able to scale the system and now we have",
    "start": "2091080",
    "end": "2097000"
  },
  {
    "text": "about 120 plus features that are being produced by our by our system we write",
    "start": "2097000",
    "end": "2104350"
  },
  {
    "text": "features to DynamoDB and and several other things now the time to write test",
    "start": "2104350",
    "end": "2112090"
  },
  {
    "text": "and deploy our streaming application has reduced considerably now our research",
    "start": "2112090",
    "end": "2117580"
  },
  {
    "text": "scientists can write and test and prototype and run start off streaming jobs in half a day and a VR p99",
    "start": "2117580",
    "end": "2126880"
  },
  {
    "text": "latencies are five seconds and remember I mentioned that 90% of our use",
    "start": "2126880",
    "end": "2134890"
  },
  {
    "text": "cases are solved by fling sequel but not everything is so we are looking into Python support as well and for that we",
    "start": "2134890",
    "end": "2142300"
  },
  {
    "text": "are currently looking into beam so that may be coming up with that I would like",
    "start": "2142300",
    "end": "2149230"
  },
  {
    "text": "to conclude thank you so much for attending my talk and let me know if you have questions all right I see your hand",
    "start": "2149230",
    "end": "2163359"
  },
  {
    "text": "already let's start there first thanks",
    "start": "2163359",
    "end": "2170589"
  },
  {
    "text": "for the presentation at the beginning you said you have you had a pattern and",
    "start": "2170589",
    "end": "2177940"
  },
  {
    "text": "you wanted to find some some frauds did you find this pattern through machine",
    "start": "2177940",
    "end": "2184150"
  },
  {
    "text": "learning or is machine learning or what's the reason for machine learning",
    "start": "2184150",
    "end": "2190330"
  },
  {
    "text": "if in this case yeah so so we the way we",
    "start": "2190330",
    "end": "2197109"
  },
  {
    "text": "train our so we used like we produce batch features through like art through",
    "start": "2197109",
    "end": "2204869"
  },
  {
    "text": "presto and batch queries for training the machine learning algorithms the streaming feature is actually used for",
    "start": "2204869",
    "end": "2211300"
  },
  {
    "text": "scoring it so this is the input to our so the way it works is we have one some machine learning algorithms are trained",
    "start": "2211300",
    "end": "2217750"
  },
  {
    "text": "and they are deployed to in production we have something called as a model",
    "start": "2217750",
    "end": "2222849"
  },
  {
    "text": "model executor so the model executor knows it has it knows where what the",
    "start": "2222849",
    "end": "2228790"
  },
  {
    "text": "model is and then it needs features to score or as input to the model to make a",
    "start": "2228790",
    "end": "2233980"
  },
  {
    "text": "decision or make a prediction and that is what the streaming features are used for so so for that we needed like really",
    "start": "2233980",
    "end": "2240099"
  },
  {
    "text": "instant real-time information so this feature this particular feature it",
    "start": "2240099",
    "end": "2245230"
  },
  {
    "text": "required like a would look at like last 2000 or so actions and with timestamp",
    "start": "2245230",
    "end": "2252640"
  },
  {
    "text": "and in this case there is a not I'm not completely sure about what the actual",
    "start": "2252640",
    "end": "2258220"
  },
  {
    "text": "way it finds this pattern is but this is the data that it would use as an input in production to",
    "start": "2258220",
    "end": "2264460"
  },
  {
    "text": "the prediction some more questions yep",
    "start": "2264460",
    "end": "2272250"
  },
  {
    "text": "when you're doing the historical sort of backfill and then you're gonna reconfigure the job and start it again",
    "start": "2275920",
    "end": "2281589"
  },
  {
    "text": "yeah do you ever run into issues where your job stores a lot of state and thats",
    "start": "2281589",
    "end": "2287440"
  },
  {
    "text": "a point might take a long time or I don't know have issues is there any challenges around that part that",
    "start": "2287440",
    "end": "2293859"
  },
  {
    "text": "transition yeah so are for us the biggest so there were like two issues that we saw with that update so one",
    "start": "2293859",
    "end": "2302800"
  },
  {
    "text": "thing is with flink like whenever the job draft changes in any way like if",
    "start": "2302800",
    "end": "2309609"
  },
  {
    "text": "suppose a new operator is added something like that so at that time save points become incompatible so that was one reason why",
    "start": "2309609",
    "end": "2317080"
  },
  {
    "text": "we virus we kept the job draft the same so even though in steady state the s3",
    "start": "2317080",
    "end": "2323710"
  },
  {
    "text": "operator is not doing anything it's not reading any s3 data but we Vickers just so that we could keep job trough the",
    "start": "2323710",
    "end": "2330550"
  },
  {
    "text": "same we we basically just keep the operator around so that was one that was",
    "start": "2330550",
    "end": "2336760"
  },
  {
    "text": "one of the issues related to state not really the main issues with like",
    "start": "2336760",
    "end": "2342040"
  },
  {
    "text": "checkpoint size was the the memory that the data skew that I was talking about so because of that times Q because of",
    "start": "2342040",
    "end": "2350650"
  },
  {
    "text": "data check point sizes used to be really high and then check point alignment",
    "start": "2350650",
    "end": "2355660"
  },
  {
    "text": "would take a long time and then we saw a lot of issues around that especially for like really high volume events any more",
    "start": "2355660",
    "end": "2364599"
  },
  {
    "text": "questions",
    "start": "2364599",
    "end": "2367080"
  },
  {
    "text": "hey thanks how do you deal with with schema changes like if some of the",
    "start": "2372410",
    "end": "2377760"
  },
  {
    "text": "people actually change like the DES queried see Coker they're actually executing yeah so the way it works",
    "start": "2377760",
    "end": "2385350"
  },
  {
    "text": "internally so we have suppose there is a schema changes that is actually like a",
    "start": "2385350",
    "end": "2392760"
  },
  {
    "text": "breaking change like it completely makes save points incompatible checkpoints and the job cannot proceed so the way we do",
    "start": "2392760",
    "end": "2399150"
  },
  {
    "text": "that is we so one so the way our features are stored in dynamo DB they",
    "start": "2399150",
    "end": "2405480"
  },
  {
    "text": "are named spaced under a feature name and a version so suppose I have a feature that's counting like the number",
    "start": "2405480",
    "end": "2412290"
  },
  {
    "text": "of Rights so suppose the version one is running with like schema one and I know",
    "start": "2412290",
    "end": "2418680"
  },
  {
    "text": "that the schema change is going to break this so the way we do it is we Bri",
    "start": "2418680",
    "end": "2424710"
  },
  {
    "text": "bootstrap a version two which is written under a completely different namespace and then we automatically cut over the",
    "start": "2424710",
    "end": "2431280"
  },
  {
    "text": "clients once it is fully bootstrap and like caught up and it has picked up all the updates we just got over to the new",
    "start": "2431280",
    "end": "2437570"
  },
  {
    "text": "new version yeah any more hands don't",
    "start": "2437570",
    "end": "2449910"
  },
  {
    "text": "say any more but I'm sure Sharon would be very happy to answer any more questions that you that you have so if",
    "start": "2449910",
    "end": "2456000"
  },
  {
    "text": "we can give her a warm round of applause thank you Sarah [Applause]",
    "start": "2456000",
    "end": "2463059"
  }
]