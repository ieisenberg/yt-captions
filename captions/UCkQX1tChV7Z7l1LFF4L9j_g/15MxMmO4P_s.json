[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "thank you Yeah thank you for the introduction Hello everyone I'm Jun Uh",
    "start": "10320",
    "end": "15599"
  },
  {
    "text": "it's great pleasure to be here today to talk about about our project efficient inmental processing using the MRO and",
    "start": "15599",
    "end": "22640"
  },
  {
    "text": "the iceberg uh I'm the uh techn of the uh data platform at Netflix uh uh doing",
    "start": "22640",
    "end": "30240"
  },
  {
    "text": "maining in the job and workflow tracing area work and uh",
    "start": "30240",
    "end": "35880"
  },
  {
    "text": "um so hope you enjoying the ice cream bar and uh as this is the last talk I",
    "start": "35880",
    "end": "42879"
  },
  {
    "text": "hope the talk can bring many values and fun to you",
    "start": "42879",
    "end": "48680"
  },
  {
    "text": "too okay uh here's outline of the talk I will first give a brief introduction of",
    "start": "48680",
    "end": "55520"
  },
  {
    "text": "the problem space and then I will give a u kind uh overview of the architectural",
    "start": "55520",
    "end": "61920"
  },
  {
    "text": "design After that I will show some use cases and examples Finally summarize the",
    "start": "61920",
    "end": "67840"
  },
  {
    "text": "talk with the key takeaways and future works So introduction",
    "start": "67840",
    "end": "74799"
  },
  {
    "text": "uh let's get started by looking at the landscape of the data insights at Netflix Uh just quick away can you raise",
    "start": "74799",
    "end": "81840"
  },
  {
    "text": "your hand if you are a Netflix subscriber",
    "start": "81840",
    "end": "87439"
  },
  {
    "text": "Okay great Great Really glad to see we have many audiences subscribing Netflix",
    "start": "87439",
    "end": "92960"
  },
  {
    "text": "Thank you enjoying the show at Netflix Um as you might experience while",
    "start": "92960",
    "end": "99600"
  },
  {
    "text": "navigating uh at Netflix.com the personalized show recommendation results is quite good It",
    "start": "99600",
    "end": "106880"
  },
  {
    "text": "kind of sometime not always but many times give you the shows that you are",
    "start": "106880",
    "end": "112000"
  },
  {
    "text": "interested in Right Those are powered by the data insights based on multiple data",
    "start": "112000",
    "end": "118799"
  },
  {
    "text": "pipelines and also m machine learning workflows And as Netflix is datadriven",
    "start": "118799",
    "end": "124799"
  },
  {
    "text": "company many many decisions and Netflix are entirely driven by the data insights",
    "start": "124799",
    "end": "130000"
  },
  {
    "text": "Data practition practitioners like a data engineer data scientist uh machine",
    "start": "130000",
    "end": "135520"
  },
  {
    "text": "learning engineer uh software engineer even non-engineer like a data producer",
    "start": "135520",
    "end": "140640"
  },
  {
    "text": "they all run their data pipelines to get insights they need It can be like just the color used in the landing page when",
    "start": "140640",
    "end": "147280"
  },
  {
    "text": "you visit Netflix.com or it can be that as mentioned the personalized",
    "start": "147280",
    "end": "152440"
  },
  {
    "text": "recommendation or the content producer may decide if they should renew the uh",
    "start": "152440",
    "end": "157920"
  },
  {
    "text": "list or like terminate the next season of the show based on data insights also like we can even with also like scan",
    "start": "157920",
    "end": "165680"
  },
  {
    "text": "data to get like a security issue or anything So data used widely Um so they those okay yeah let's",
    "start": "165680",
    "end": "175760"
  },
  {
    "text": "go next one So as the business continues to expand to new areas like from",
    "start": "175760",
    "end": "181920"
  },
  {
    "text": "streaming to the games to the ads to the lives right you may uh watch the recent",
    "start": "181920",
    "end": "187120"
  },
  {
    "text": "live events uh on uh past Friday and so the demand for the data continues to",
    "start": "187120",
    "end": "194720"
  },
  {
    "text": "grow and those new initiatives also bring lots of new requirements for",
    "start": "194720",
    "end": "200800"
  },
  {
    "text": "example like a security requirement privacy requirement or latency requirements and they are or bring a",
    "start": "200800",
    "end": "207680"
  },
  {
    "text": "wide variety of use cases to the platform as",
    "start": "207680",
    "end": "212159"
  },
  {
    "text": "well So while users working with the data the data practitioners usually face",
    "start": "212760",
    "end": "219680"
  },
  {
    "text": "three common problems Data accuracy data freshness and the cost efficiency Uh",
    "start": "219680",
    "end": "225599"
  },
  {
    "text": "with a huge amount of data right The data accuracy is a critical important Business decisions has to be made based",
    "start": "225599",
    "end": "232400"
  },
  {
    "text": "on the high quality data Also with that amount of data if there are some issues",
    "start": "232400",
    "end": "238319"
  },
  {
    "text": "you have to correct data then it might be expensive and also time consuming as you have to back field a lot of data Um",
    "start": "238319",
    "end": "246400"
  },
  {
    "text": "data freshness is also very important Users needs to process large data sets",
    "start": "246400",
    "end": "252239"
  },
  {
    "text": "quickly to enable fast business decision Um cost efficiency is always important",
    "start": "252239",
    "end": "258720"
  },
  {
    "text": "as we running uh run the business uh Netflix spend $150 million per year just",
    "start": "258720",
    "end": "264240"
  },
  {
    "text": "on the compute and the storage Also I would like to call out these three problems are general and",
    "start": "264240",
    "end": "271440"
  },
  {
    "text": "common problems No matter how big or small your data is it might be more impactful if the uh data size is large",
    "start": "271440",
    "end": "278639"
  },
  {
    "text": "right It cost a lot Um but if we can solve them this will be a gamecher and",
    "start": "278639",
    "end": "285680"
  },
  {
    "text": "enable lots of new patterns and also allows us to re rethink about the batch",
    "start": "285680",
    "end": "290960"
  },
  {
    "text": "ETL in analytics domain Uh there are lots of challenges",
    "start": "290960",
    "end": "296800"
  },
  {
    "text": "to solve those problems Uh one of the important one is later data Um this",
    "start": "296800",
    "end": "303759"
  },
  {
    "text": "graph shows a visual example Uh for instance like at like last night 10 20",
    "start": "303759",
    "end": "310479"
  },
  {
    "text": "p.m I watch Netflix right I open my app Then my battery is my phone's battery was dead Then the event generated at",
    "start": "310479",
    "end": "318320"
  },
  {
    "text": "that my device at 10:20 p.m won't be able to send it to the server So it",
    "start": "318320",
    "end": "324960"
  },
  {
    "text": "buffered my device Right Then I put my my iPhone to the charger and then I went",
    "start": "324960",
    "end": "330880"
  },
  {
    "text": "to sleep This morning I got up at 8:20 Then I uh open my phone and uh start",
    "start": "330880",
    "end": "338720"
  },
  {
    "text": "Netflix app Then the events at two uh 8:20 a.m plus the events uh generated",
    "start": "338720",
    "end": "345440"
  },
  {
    "text": "last night both sent to Netflix server got processed So those n those uh events",
    "start": "345440",
    "end": "352880"
  },
  {
    "text": "generated last night got processed with a few hours of multiple hours of delay",
    "start": "352880",
    "end": "358320"
  },
  {
    "text": "and then that's cause trouble Uh so the key is that the event",
    "start": "358320",
    "end": "366759"
  },
  {
    "text": "time matters a lot to business not the processing time So many times the",
    "start": "366759",
    "end": "373600"
  },
  {
    "text": "streaming or ingesting pipeline uh use the processing time so they can quickly",
    "start": "373600",
    "end": "378720"
  },
  {
    "text": "uh ingest data and pend data to the staging table uh which is partitioned by the processing time This can greatly",
    "start": "378720",
    "end": "385919"
  },
  {
    "text": "simplify the uh streaming pipeline which is wonderful and then it leaves the uh",
    "start": "385919",
    "end": "390960"
  },
  {
    "text": "later uh badge analytics pipeline to handle the latering",
    "start": "390960",
    "end": "396280"
  },
  {
    "text": "data as the data land right so the data processed few hours ago or in the past",
    "start": "396280",
    "end": "403600"
  },
  {
    "text": "becomes incomplete which then caused the data accuracy issue We can fix that by",
    "start": "403600",
    "end": "409680"
  },
  {
    "text": "reprocessing the data But given that amount of data um the that might be",
    "start": "409680",
    "end": "415199"
  },
  {
    "text": "expensive or time consuming Also um while we deal with those kind of large",
    "start": "415199",
    "end": "420560"
  },
  {
    "text": "data sets we usually have to carefully design the pition schema uh to fit the business needs right And then the",
    "start": "420560",
    "end": "427440"
  },
  {
    "text": "latering data might block the downstream pipelines to start because those",
    "start": "427440",
    "end": "433280"
  },
  {
    "text": "pipelines would like to start to process data only when the data is complete as much as possible So that will reduce the",
    "start": "433280",
    "end": "441360"
  },
  {
    "text": "uh data uh freshness as well",
    "start": "441360",
    "end": "445879"
  },
  {
    "text": "So to assist uh data practitioners and Netflix to work with data solve those problem or develop any solutions we",
    "start": "448240",
    "end": "455440"
  },
  {
    "text": "developed this uh big data analytics platform as a uh high level abstraction",
    "start": "455440",
    "end": "460880"
  },
  {
    "text": "to offer the best user experience and high level abstractions for users to interact with those engines computer",
    "start": "460880",
    "end": "467199"
  },
  {
    "text": "engines So uh users use our mRO workflow traator which abstract a lot so the",
    "start": "467199",
    "end": "472720"
  },
  {
    "text": "complexity from users they don't need deal with the spark directly they can easily write their jobs and then uh use",
    "start": "472720",
    "end": "479440"
  },
  {
    "text": "the engine they like to process their data eventually data is saved to the iceberg",
    "start": "479440",
    "end": "485960"
  },
  {
    "text": "table So so we observe the users in our",
    "start": "485960",
    "end": "491599"
  },
  {
    "text": "platform um usually follow those two common patterns to deal with later in data uh first is called uh look back",
    "start": "491599",
    "end": "499520"
  },
  {
    "text": "window So in that scenario uh the workflow owner or the job owner usually",
    "start": "499520",
    "end": "505440"
  },
  {
    "text": "have a lot of business domain knowledge So they kind of can tell how long they",
    "start": "505440",
    "end": "510560"
  },
  {
    "text": "should look back right Uh if the data is older than that window likely there's",
    "start": "510560",
    "end": "515839"
  },
  {
    "text": "not much business value there So they can discard So then they can um for example always reprocess the past three",
    "start": "515839",
    "end": "523518"
  },
  {
    "text": "days of data every day to insert overrites to target table every day to",
    "start": "523519",
    "end": "528800"
  },
  {
    "text": "bring back the data accuracy after three days Uh another approach is that we can just",
    "start": "528800",
    "end": "536880"
  },
  {
    "text": "like ignore the later in data Uh that sometimes works especially the business",
    "start": "536880",
    "end": "542000"
  },
  {
    "text": "decision may have to make uh have to be made right at real time or at that moment Then if data is not there then",
    "start": "542000",
    "end": "548800"
  },
  {
    "text": "you cannot make uh we have to make a decision then we make a decision so later that doesn't matter Um but then we",
    "start": "548800",
    "end": "555600"
  },
  {
    "text": "got freshness we got a cost efficiency but we lose the data accuracy",
    "start": "555600",
    "end": "561760"
  },
  {
    "text": "Um another well-known pattern called incremental processing can address those",
    "start": "562760",
    "end": "568080"
  },
  {
    "text": "uh problems Uh incremental processing is just approach that to process data but",
    "start": "568080",
    "end": "573519"
  },
  {
    "text": "only the new or changed data Um here we focus on the analytical use cases Uh so",
    "start": "573519",
    "end": "580959"
  },
  {
    "text": "to support that we have to solve two problems right One is how we can capture the change Secondly how we can track the",
    "start": "580959",
    "end": "587839"
  },
  {
    "text": "states Uh in this talk we are I'm going to show",
    "start": "587839",
    "end": "594320"
  },
  {
    "text": "how we can use the iceberg plus the mroer together to efficiently support",
    "start": "594320",
    "end": "599920"
  },
  {
    "text": "the change capturing and also uh give us a very great experience to integrate",
    "start": "599920",
    "end": "605600"
  },
  {
    "text": "with their pipelines Uh so let's talk about iceberg",
    "start": "605600",
    "end": "611839"
  },
  {
    "text": "first Uh have you heard about iceberg Can you raise your hand Oh some okay great I still give a very quick uh brief",
    "start": "611839",
    "end": "618720"
  },
  {
    "text": "Uh so D is a high performance uh table format for huge analytics uh tables and",
    "start": "618720",
    "end": "625120"
  },
  {
    "text": "this project started about eight years ago and uh Netflix is it now becomes the",
    "start": "625120",
    "end": "630160"
  },
  {
    "text": "top a batch project and one of the most popular open table formats Um so it",
    "start": "630160",
    "end": "636480"
  },
  {
    "text": "brings a lots of great features at least some of here and um it simplify a lot",
    "start": "636480",
    "end": "642079"
  },
  {
    "text": "for the uh data management for our project We leverage the iceberg metadata",
    "start": "642079",
    "end": "648160"
  },
  {
    "text": "layer It provides lots of information to help us build a mechanism to be able to",
    "start": "648160",
    "end": "653600"
  },
  {
    "text": "catch the change without actually reading the user data at all I'm going",
    "start": "653600",
    "end": "659040"
  },
  {
    "text": "to talk about later Uh let's go over some basic uh iceberg table concept",
    "start": "659040",
    "end": "664240"
  },
  {
    "text": "first So iceberg table are saved in the catalog This catalog can be pluggable",
    "start": "664240",
    "end": "670720"
  },
  {
    "text": "like hive catalog or glue catalog or j like a JDBC or rest catalog Uh and then",
    "start": "670720",
    "end": "678240"
  },
  {
    "text": "because we have our own internal uh catalog service as well Um then the tables will save the um metadata in the",
    "start": "678240",
    "end": "686160"
  },
  {
    "text": "metadata file which has a list of manifest files which uh map to the snapshots Then those manifest files are",
    "start": "686160",
    "end": "693839"
  },
  {
    "text": "just files saved the lots of uh additional information related to the data files It also keep the reference of",
    "start": "693839",
    "end": "700000"
  },
  {
    "text": "data file as well And then um iceberg will produce the partitioning",
    "start": "700000",
    "end": "706720"
  },
  {
    "text": "value by taking a column value and optionally transform",
    "start": "706720",
    "end": "712600"
  },
  {
    "text": "it Then the iceberg track this kind of relationship The table partitioning",
    "start": "712600",
    "end": "718560"
  },
  {
    "text": "scheme uh data design is purely based on this relationships Uh so no longer",
    "start": "718560",
    "end": "724079"
  },
  {
    "text": "depends on the table physical layout That's a very uh important property that",
    "start": "724079",
    "end": "729519"
  },
  {
    "text": "we can leverage to build that efficient incremental change capturing feature",
    "start": "729519",
    "end": "736279"
  },
  {
    "text": "So data practitioners at Netflix use iceberg create like a more than one million tables there and develop",
    "start": "737120",
    "end": "744240"
  },
  {
    "text": "hundreds of thousands workflows to kind of read or write or transform the data",
    "start": "744240",
    "end": "749440"
  },
  {
    "text": "of those tables we then need to orchestrating those workflows which",
    "start": "749440",
    "end": "754639"
  },
  {
    "text": "that's why we invented maestro u maestro is a horizontal scalable workflow trader",
    "start": "754639",
    "end": "761519"
  },
  {
    "text": "um that's manage the large scale um data and machine learning workflows it manage",
    "start": "761519",
    "end": "767920"
  },
  {
    "text": "end to end whole life cycle so give user kind of a serverless experience they just like own their business code and",
    "start": "767920",
    "end": "774079"
  },
  {
    "text": "then they ship the code to the platform um so it offers multiple uh reusable",
    "start": "774079",
    "end": "780480"
  },
  {
    "text": "patterns like for each conditional branching and some workflow and so on So",
    "start": "780480",
    "end": "786480"
  },
  {
    "text": "our users also build additional patterns using those usable patterns Um it is",
    "start": "786480",
    "end": "792160"
  },
  {
    "text": "also designed to expansion and integration with others like a metalflow You might hear that talk yesterday about",
    "start": "792160",
    "end": "799040"
  },
  {
    "text": "metalflow metalflow integrate with maestro uh then iceberg in this talk we",
    "start": "799040",
    "end": "804240"
  },
  {
    "text": "are going to talk about that So we initiated mro project about",
    "start": "804240",
    "end": "810560"
  },
  {
    "text": "four years ago Um the decision to build our own workflow orchestrator instead of",
    "start": "810560",
    "end": "816560"
  },
  {
    "text": "using those popular ones like airflow is bas is just because that the challenges",
    "start": "816560",
    "end": "821920"
  },
  {
    "text": "that Netflix were facing um for example like scalability we need like horizontal scalable workflow orchestrator and also",
    "start": "821920",
    "end": "828959"
  },
  {
    "text": "like a usability and accessibility uh we had the alpha release in like 2021",
    "start": "828959",
    "end": "835440"
  },
  {
    "text": "later we got uh in 2022 we have the beta release and then uh later 2022 we got a",
    "start": "835440",
    "end": "842000"
  },
  {
    "text": "J internally uh with then the team spend like one year to move hundred of thousand workflows from the old system",
    "start": "842000",
    "end": "848560"
  },
  {
    "text": "to the maestro it's a fully managed uh migration user don't actually make any",
    "start": "848560",
    "end": "853600"
  },
  {
    "text": "line of code change um after that this summer uh this year uh we make the major",
    "start": "853600",
    "end": "860560"
  },
  {
    "text": "code uh to be public available So you can try it out after this talk I also",
    "start": "860560",
    "end": "866720"
  },
  {
    "text": "put some QR code here Uh if you like you can visit there Those are our blog post uh uh describing with more details about",
    "start": "866720",
    "end": "874399"
  },
  {
    "text": "the MRO and also the open source project Okay",
    "start": "874399",
    "end": "880680"
  },
  {
    "text": "Um so I would like to show this simple examples just like to give you some sense like how users interact with Vio",
    "start": "880680",
    "end": "888000"
  },
  {
    "text": "or write their workflows at Netflix So this is like like a kind of",
    "start": "888000",
    "end": "893279"
  },
  {
    "text": "configuration like a definition but users uh uh define this and then they",
    "start": "893279",
    "end": "899199"
  },
  {
    "text": "also can include some business logic there So the first section like description uh section that the user can",
    "start": "899199",
    "end": "905760"
  },
  {
    "text": "put some uh information even like some call instruction there This supports",
    "start": "905760",
    "end": "910880"
  },
  {
    "text": "markdown syntax and then when there's the alerts or there's something wrong with this workflow when we send email we",
    "start": "910880",
    "end": "916959"
  },
  {
    "text": "can include this description in the email body as well And then user can uh say I want to trigger it run daily So we",
    "start": "916959",
    "end": "924079"
  },
  {
    "text": "support like a crown trigger and also single trigger as well For example if the uh upstream table is ready then",
    "start": "924079",
    "end": "931440"
  },
  {
    "text": "please run this workflow kind of uh uh single trigger supports and then uh here",
    "start": "931440",
    "end": "936800"
  },
  {
    "text": "you see in the workflow users can define parameters parameter can reference another parameter and doing the uh",
    "start": "936800",
    "end": "942639"
  },
  {
    "text": "runtime evaluation here the parameter of my query include the SQL query trying to",
    "start": "942639",
    "end": "948480"
  },
  {
    "text": "uh do something um then user can define oh I want to run this query in the spark",
    "start": "948480",
    "end": "954480"
  },
  {
    "text": "okay um then uh they just like simply put this configuration there they don't need to worry about like which cluster",
    "start": "954480",
    "end": "960800"
  },
  {
    "text": "they uh need to route to or like what's the memory or settings they need to use they can always let the platform to",
    "start": "960800",
    "end": "966880"
  },
  {
    "text": "decide that so they just need to pass a query um then once they have this uh",
    "start": "966880",
    "end": "973120"
  },
  {
    "text": "workflow finishing defined when they save it they can use the our CI tool trying to push it and then run it So",
    "start": "973120",
    "end": "980079"
  },
  {
    "text": "during the development right the query may not be perfect or they may need like a multiple iterations you can just",
    "start": "980079",
    "end": "985759"
  },
  {
    "text": "simply run the query use like second one or something first until you are satisfied with your results you can like",
    "start": "985759",
    "end": "991199"
  },
  {
    "text": "plug in the production query and then user can also use a UI to take a look",
    "start": "991199",
    "end": "997199"
  },
  {
    "text": "what's wrong or what happens um so MRO provides a kind of workflow",
    "start": "997199",
    "end": "1004000"
  },
  {
    "text": "platform for everyone uh serving thousands of internal Netflix uh users",
    "start": "1004000",
    "end": "1009120"
  },
  {
    "text": "including engineers and the lung engineers Um so it offers multiple uh",
    "start": "1009120",
    "end": "1014320"
  },
  {
    "text": "interfaces and also a flexible integration and uh dynamic",
    "start": "1014320",
    "end": "1020800"
  },
  {
    "text": "uh workflow engine and uh this uh extensible execution supports With all",
    "start": "1020800",
    "end": "1026720"
  },
  {
    "text": "these features Netflix become very uh sorry maestro has become very successful Netflix um as the uh data and machine",
    "start": "1026720",
    "end": "1035120"
  },
  {
    "text": "learning workflow traders our thousands of our users use that develop 100 thousand workflows there Uh it runs like",
    "start": "1035120",
    "end": "1042720"
  },
  {
    "text": "uh have million jobs and some in some like busy days it even run like a two million uh jobs per",
    "start": "1042720",
    "end": "1048520"
  },
  {
    "text": "day Yeah I put a QR code here You may learn more from this blog post",
    "start": "1048520",
    "end": "1054240"
  },
  {
    "text": "Um again we we would like to provide a clean and uh easy to adopt the solution",
    "start": "1054240",
    "end": "1060960"
  },
  {
    "text": "for users to be able to do the efficient increment processing uh with data",
    "start": "1060960",
    "end": "1066160"
  },
  {
    "text": "accuracy freshness and the cost efficiency Okay Uh now let's start the",
    "start": "1066160",
    "end": "1073679"
  },
  {
    "text": "fun part architectural design So there are two major goal of this design Firstly we need to efficiently capture",
    "start": "1073679",
    "end": "1080799"
  },
  {
    "text": "the change Um it this is important not only because",
    "start": "1080799",
    "end": "1087120"
  },
  {
    "text": "like um efficiency So but also because like the lots of times right we cannot",
    "start": "1087120",
    "end": "1094640"
  },
  {
    "text": "access user data because of security requirement or the privacy requirement So in those cases then this kind of uh",
    "start": "1094640",
    "end": "1102320"
  },
  {
    "text": "requirement efficiently capture change without reading user data become really important uh fortunately that the",
    "start": "1102320",
    "end": "1108559"
  },
  {
    "text": "iceberg provide all these supports in the metadata layer help us to achieve this Uh second is to get the best user",
    "start": "1108559",
    "end": "1116280"
  },
  {
    "text": "experiences You can imagine that data manual users develop that amount of the workflows in our platform right we",
    "start": "1116280",
    "end": "1122559"
  },
  {
    "text": "cannot break them or we cannot ask them to make changes or significantly So we would like to offer the best user",
    "start": "1122559",
    "end": "1128720"
  },
  {
    "text": "experience The key is that to decouple the change capturing from the user",
    "start": "1128720",
    "end": "1133760"
  },
  {
    "text": "business logic Um so in that way the implementation or the support of the",
    "start": "1133760",
    "end": "1139600"
  },
  {
    "text": "incurren processing can be engine or language agnostic In that case is user can use whatever langu language they",
    "start": "1139600",
    "end": "1145440"
  },
  {
    "text": "like or computer engine they like to implement their business logic and leave the incremental processing to be handled",
    "start": "1145440",
    "end": "1150880"
  },
  {
    "text": "by the platform So mure provide all the support to kind of uh uh develop this",
    "start": "1150880",
    "end": "1157360"
  },
  {
    "text": "kind of interface Okay Now first let's see how we can efficiently capture chain As I",
    "start": "1157360",
    "end": "1164480"
  },
  {
    "text": "mentioned like iceberg met data provide lots of useful information The uh the snapshots contains information about",
    "start": "1164480",
    "end": "1171039"
  },
  {
    "text": "like how many uh change rows or addit data files and then the metadata uh uh",
    "start": "1171039",
    "end": "1178000"
  },
  {
    "text": "file per data file gives us the information about like uh the reference to the data file and also the upper and",
    "start": "1178000",
    "end": "1185520"
  },
  {
    "text": "lower bound of the change of a given column from that data file and so on All",
    "start": "1185520",
    "end": "1190880"
  },
  {
    "text": "those information can help us build a very efficient way to capture the change or capture even the range of the",
    "start": "1190880",
    "end": "1198000"
  },
  {
    "text": "specific column But then uh they only need to uh we only need to access those",
    "start": "1198000",
    "end": "1204400"
  },
  {
    "text": "metadata and get a reference of data file and then use that we can build a mechanism to track the changes So then",
    "start": "1204400",
    "end": "1211120"
  },
  {
    "text": "we can capture those changes It's a zero data copy and we don't touch user data",
    "start": "1211120",
    "end": "1217080"
  },
  {
    "text": "So the the change captured and will be included in a table which then becomes",
    "start": "1217080",
    "end": "1223039"
  },
  {
    "text": "an interface to hand to our users to consume So they can this table is like same table as the original table with",
    "start": "1223039",
    "end": "1229760"
  },
  {
    "text": "the same schema uh like security access everything but the only difference is",
    "start": "1229760",
    "end": "1235440"
  },
  {
    "text": "that this table only contains the chain data and then the table name can be a parameter pass to the user job then user",
    "start": "1235440",
    "end": "1242240"
  },
  {
    "text": "just consume everything from this table then they got the chain",
    "start": "1242240",
    "end": "1247279"
  },
  {
    "text": "data Next I will use example to show how this approach works So here uh I I have",
    "start": "1247559",
    "end": "1255120"
  },
  {
    "text": "this simple table called DB1 table one right at uh workload time T1 they",
    "start": "1255120",
    "end": "1260799"
  },
  {
    "text": "mentioned that uh there only one single slapshots there and then it has like two",
    "start": "1260799",
    "end": "1265919"
  },
  {
    "text": "manifest files which uh has five data files there those five data files uh",
    "start": "1265919",
    "end": "1271760"
  },
  {
    "text": "actually maps to uh two partitions those partition are virtual as I mentioned right so uh the data files are immutable",
    "start": "1271760",
    "end": "1278799"
  },
  {
    "text": "in the uh in the storage and then What at query time you got those virtual kind of uh table",
    "start": "1278799",
    "end": "1286240"
  },
  {
    "text": "position Okay Um so yeah I had here and",
    "start": "1287480",
    "end": "1292880"
  },
  {
    "text": "then next uh you see here I got at the t2 I got a new snapshot s as one where",
    "start": "1292880",
    "end": "1300240"
  },
  {
    "text": "uh here either has three new data files append to this table and uh they the",
    "start": "1300240",
    "end": "1308080"
  },
  {
    "text": "they somehow have the lettering data So they goes to the partition P 0 and P1",
    "start": "1308080",
    "end": "1313360"
  },
  {
    "text": "and P2 P2 is a new partition but then those data goes to P 0 and P1",
    "start": "1313360",
    "end": "1318919"
  },
  {
    "text": "but you want to process right the using like a traditional way high right you",
    "start": "1318919",
    "end": "1324480"
  },
  {
    "text": "want process the uh all the data we have to select data from like P 0 P1 P2",
    "start": "1324480",
    "end": "1330240"
  },
  {
    "text": "everything which means that we actually reprocess those f data files again it's",
    "start": "1330240",
    "end": "1335919"
  },
  {
    "text": "not efficient Yeah thinking about like if this like 40 days or like something uh huge amount of data then uh instead",
    "start": "1335919",
    "end": "1343600"
  },
  {
    "text": "using the uh iceberg features we can create a kind of uh",
    "start": "1343600",
    "end": "1351200"
  },
  {
    "text": "table called like IBP table one that has the same schema as the original iceberg table is a is indeed an iceberg table",
    "start": "1351200",
    "end": "1359760"
  },
  {
    "text": "and then we add a new snapshots which has a manifest file but at this moment",
    "start": "1359760",
    "end": "1364880"
  },
  {
    "text": "we don't uh create or copy those data files to create a new data file Instead",
    "start": "1364880",
    "end": "1370080"
  },
  {
    "text": "we can read the snapshot information from the uh S1 to get a reference of",
    "start": "1370080",
    "end": "1375200"
  },
  {
    "text": "those data files and then we just simply add reference to the manifest file of the S2 in the new table In this way we",
    "start": "1375200",
    "end": "1382799"
  },
  {
    "text": "can actually reference them without copying the data right Zero data copy and",
    "start": "1382799",
    "end": "1388280"
  },
  {
    "text": "then when user query they select start from this table they will get three partition P 0 P1 P2 as well But in this",
    "start": "1388280",
    "end": "1395280"
  },
  {
    "text": "new virtual partitioning uh they don't have those old data file at all which",
    "start": "1395280",
    "end": "1400400"
  },
  {
    "text": "means that they will not reprocess data also uh after a while right the",
    "start": "1400400",
    "end": "1408159"
  },
  {
    "text": "platform is responsible to delete this uh temporary table while after the ETL finishes",
    "start": "1408159",
    "end": "1415640"
  },
  {
    "text": "uh there are some other alternatives to achieve the similar goal um like to enable this increment processing using",
    "start": "1415840",
    "end": "1421679"
  },
  {
    "text": "the iceberg like iceberg with spring with spark uh spark streaming and so on we didn't go with those approach mainly",
    "start": "1421679",
    "end": "1428159"
  },
  {
    "text": "just because they are coupled with engine tightly so this is not only like requires user to interact with those",
    "start": "1428159",
    "end": "1434799"
  },
  {
    "text": "library using their API or or some implementation client and add into their",
    "start": "1434799",
    "end": "1440080"
  },
  {
    "text": "business logic also require user sometimes have to rewrite their code or for example if they use a train uh if",
    "start": "1440080",
    "end": "1445600"
  },
  {
    "text": "they use other engines they have to rewrite use the engine that supporting processing Um but I think underlying",
    "start": "1445600",
    "end": "1452640"
  },
  {
    "text": "underlying implementation all those implementations are kind of similar uh which uh I show some code here Um",
    "start": "1452640",
    "end": "1460640"
  },
  {
    "text": "basically we kind of load table and then use the iceberg APIs get the the",
    "start": "1460640",
    "end": "1467120"
  },
  {
    "text": "snapshots between the two snapshots ID and with some filters say we only care",
    "start": "1467120",
    "end": "1472640"
  },
  {
    "text": "the append partition information and then we create empty table with the same schema as the original one Then we scan",
    "start": "1472640",
    "end": "1480080"
  },
  {
    "text": "through the snapshots table add data files one by one to this uh new uh ICDC",
    "start": "1480080",
    "end": "1485360"
  },
  {
    "text": "table After that we commit it we're done So of course this code is simple and",
    "start": "1485360",
    "end": "1490559"
  },
  {
    "text": "it's not like for production but it demonstrate idea clearly Uh so you might",
    "start": "1490559",
    "end": "1496320"
  },
  {
    "text": "consider similar approach uh in your solution uh evaporable So this new capability",
    "start": "1496320",
    "end": "1503919"
  },
  {
    "text": "enables many patterns Uh here are three emerging ones uh that we observed or",
    "start": "1503919",
    "end": "1509520"
  },
  {
    "text": "discovered at Netflix So I will talk one by one Firstly uh we are going to talk about",
    "start": "1509520",
    "end": "1515120"
  },
  {
    "text": "incrementally process the chain data and directly append it to the uh target table So as we have",
    "start": "1515120",
    "end": "1522279"
  },
  {
    "text": "show the chain data will include reference to the real data file right",
    "start": "1522279",
    "end": "1528159"
  },
  {
    "text": "the ch the ch uh this table have the reference to the real data files and then in the ETL workflows they can use",
    "start": "1528159",
    "end": "1535840"
  },
  {
    "text": "simply just consume the ICD table select star from it and then they will can they",
    "start": "1535840",
    "end": "1541679"
  },
  {
    "text": "can get the uh those change uh data file and then append to the target table um",
    "start": "1541679",
    "end": "1547840"
  },
  {
    "text": "it does not need to uh reprocess the whole original P 0 P1 P2 and it will",
    "start": "1547840",
    "end": "1553440"
  },
  {
    "text": "bring a lots of savings and also in the future um this might be supported by the",
    "start": "1553440",
    "end": "1560000"
  },
  {
    "text": "those SQL extension or something right you can select star between these two slot or something uh we actually are",
    "start": "1560000",
    "end": "1565679"
  },
  {
    "text": "working on this okay a second pattern is that we",
    "start": "1565679",
    "end": "1572159"
  },
  {
    "text": "can use this capture chain data as a row level filters uh so in many times Right Especially in the analytics domain the",
    "start": "1572159",
    "end": "1579039"
  },
  {
    "text": "chain data itself does not actually give the give us the full data sets to process Um just for example if I want to",
    "start": "1579039",
    "end": "1587120"
  },
  {
    "text": "get the watch time for all the Netflix users right Um in that case is the chain",
    "start": "1587120",
    "end": "1593279"
  },
  {
    "text": "data from the t past time window only tell me the users that recently watch",
    "start": "1593279",
    "end": "1599520"
  },
  {
    "text": "Netflix right But it does not give me a total time However this chain data itself if we just take a look at the",
    "start": "1599520",
    "end": "1606159"
  },
  {
    "text": "chain data and select a unique user ID from it right We are going to get a",
    "start": "1606159",
    "end": "1611200"
  },
  {
    "text": "small set of the user ID those user at least watch Netflix in that window With",
    "start": "1611200",
    "end": "1616919"
  },
  {
    "text": "that while we doing the processing we can use that as the filter We can join the original table on this table by this",
    "start": "1616919",
    "end": "1623520"
  },
  {
    "text": "kind of user ids that we find we can quickly like uh prone the uh the data sets to process the transform to be able",
    "start": "1623520",
    "end": "1630720"
  },
  {
    "text": "to get the sum easily Here I show example Uh so by looking at the Chinese",
    "start": "1630720",
    "end": "1636480"
  },
  {
    "text": "data we found that circle and diamond right uh actually are the keys that in",
    "start": "1636480",
    "end": "1641840"
  },
  {
    "text": "the Chinese data and then ETL can load the table quickly filter and find those",
    "start": "1641840",
    "end": "1647120"
  },
  {
    "text": "kind of diamond and the circle uh uh data points and then doing the aggregation and save to uh overrides the",
    "start": "1647120",
    "end": "1654320"
  },
  {
    "text": "target table So uh in this example u I use a simple sum So it's you might think",
    "start": "1654320",
    "end": "1661440"
  },
  {
    "text": "about some other incremental way to doing the sum but know that this kind business logic can be uh anything right",
    "start": "1661440",
    "end": "1667919"
  },
  {
    "text": "can be very complicated you may not be able to do that kind of incremental uh summation or something so this is just",
    "start": "1667919",
    "end": "1674399"
  },
  {
    "text": "for demo okay the third pattern is use the u uh actually uh capture change rate",
    "start": "1674399",
    "end": "1681039"
  },
  {
    "text": "parameter uh that's a little different from the uh u uh change uh data capture",
    "start": "1681039",
    "end": "1686640"
  },
  {
    "text": "we even don't need to create a table Instead of um we just need to get the",
    "start": "1686640",
    "end": "1692399"
  },
  {
    "text": "upper and lower bound of some specific change columns from the source tables",
    "start": "1692399",
    "end": "1697600"
  },
  {
    "text": "actually uh this is very common especially if you are going to join multiple tables right that's is a common",
    "start": "1697600",
    "end": "1703600"
  },
  {
    "text": "pattern in the batch world where you join two tables then then you have to find like those common range that you",
    "start": "1703600",
    "end": "1710720"
  },
  {
    "text": "need to select all the data between them to be able to run a join or run any complicated uh uh like processing in",
    "start": "1710720",
    "end": "1716880"
  },
  {
    "text": "that cases we can read uh the iceberg metadata to quickly get",
    "start": "1716880",
    "end": "1722240"
  },
  {
    "text": "get the range like min or mean of max of the from all the tables for a given",
    "start": "1722240",
    "end": "1728000"
  },
  {
    "text": "column and then we can tell like oh this is a mean",
    "start": "1728000",
    "end": "1734120"
  },
  {
    "text": "oops okay the mean is p1 and max is p2 and then I can load the table uh the",
    "start": "1734120",
    "end": "1740399"
  },
  {
    "text": "parian p1 p2 and par p1 p2 to the etl workflow and then doing the processing",
    "start": "1740399",
    "end": "1747000"
  },
  {
    "text": "Okay So this is pattern is very useful in the analytics batch workflows because uh it's very common to join many many",
    "start": "1747000",
    "end": "1753279"
  },
  {
    "text": "tables not just two Um are we done No we",
    "start": "1753279",
    "end": "1758559"
  },
  {
    "text": "are not done yet On boarding class is another major concern Thousands of our",
    "start": "1758559",
    "end": "1764640"
  },
  {
    "text": "users develop hundreds of thousands workflows in the metro and then um we",
    "start": "1764640",
    "end": "1770720"
  },
  {
    "text": "cannot break those workflows or users or ask a user thousand of user to rewrite that pipeline and also many times even",
    "start": "1770720",
    "end": "1778960"
  },
  {
    "text": "like a user uh pipeline is not independent or completely uh independent",
    "start": "1778960",
    "end": "1785360"
  },
  {
    "text": "So many times they are dependent with each other So you have multi-stage pipelines In those cases they might have",
    "start": "1785360",
    "end": "1790720"
  },
  {
    "text": "some stage is enabling incremental processing but some other stage are not incremental processing So you have to we",
    "start": "1790720",
    "end": "1796880"
  },
  {
    "text": "have to like support a mix of those kind of pipelines as well and then um those",
    "start": "1796880",
    "end": "1801919"
  },
  {
    "text": "extra costs like development operational maintenance might completely offsets the",
    "start": "1801919",
    "end": "1807919"
  },
  {
    "text": "benefit of the incremental processing So some of the feedback we heard from user is that I don't have benefits to rewrite",
    "start": "1807919",
    "end": "1815919"
  },
  {
    "text": "right So can it just magically work with little",
    "start": "1815919",
    "end": "1821240"
  },
  {
    "text": "changes The answer is yes So to address those concerns uh we provide other",
    "start": "1821240",
    "end": "1827200"
  },
  {
    "text": "interfaces in addition to table interface There are two new maestro step types Why is called IB capture step type",
    "start": "1827200",
    "end": "1834559"
  },
  {
    "text": "which encapsulate all the business logic from the platform to be able to capture the change of the table and then the IB",
    "start": "1834559",
    "end": "1842960"
  },
  {
    "text": "commit step which can commit checkpoint uh based on the IB capture step information So uh with this design user",
    "start": "1842960",
    "end": "1850720"
  },
  {
    "text": "workflows can simply onboard to mro uh IP support by just adding one job of app",
    "start": "1850720",
    "end": "1857039"
  },
  {
    "text": "capture front and one job of every complete app user jobs So as I mentioned",
    "start": "1857039",
    "end": "1862240"
  },
  {
    "text": "like we are going to have the uh ICDS table right include all the changes and then that table name will be passed as a",
    "start": "1862240",
    "end": "1868799"
  },
  {
    "text": "parameter to the user drops So that's a great interface for user to be able to simply just consume that table User most",
    "start": "1868799",
    "end": "1876640"
  },
  {
    "text": "of user business logic will be exactly same They don't need to worry about like uh how to capture the change at all In",
    "start": "1876640",
    "end": "1883440"
  },
  {
    "text": "case you we maintain or upgrade or fix bugs in our change capturing logic user",
    "start": "1883440",
    "end": "1888480"
  },
  {
    "text": "won't get impacted They just need to redund everything will works They don't need to any they don't need to uh add",
    "start": "1888480",
    "end": "1894320"
  },
  {
    "text": "any library dependency to kind of get increment processing support and also",
    "start": "1894320",
    "end": "1899679"
  },
  {
    "text": "multi-stage pipelines can work as well because this is just like a typical standardized mro workflow All the",
    "start": "1899679",
    "end": "1905440"
  },
  {
    "text": "workflows can work using the mro",
    "start": "1905440",
    "end": "1909440"
  },
  {
    "text": "features Okay Um with those interfaces all existing uh workflows can work",
    "start": "1910519",
    "end": "1917600"
  },
  {
    "text": "together seamlessly and so user can use the best way to implement their business logic can use the engine they like and",
    "start": "1917600",
    "end": "1924960"
  },
  {
    "text": "uh also mash my step is kind of configurable as I just showed you with very low uh code solution there too Uh I",
    "start": "1924960",
    "end": "1932159"
  },
  {
    "text": "would show a kind of a complicated example here just demonstrate how powerful and simple that uh approach is",
    "start": "1932159",
    "end": "1938720"
  },
  {
    "text": "So here's a complicated workflow uh example uh that our user developer",
    "start": "1938720",
    "end": "1944080"
  },
  {
    "text": "trying to auto remedate the issues So many times like uh in your ETL pipeline right it may fail uh because of some",
    "start": "1944080",
    "end": "1951440"
  },
  {
    "text": "kind of small problems here there and then the own code will wake up and then rerun some script to fix a pro data",
    "start": "1951440",
    "end": "1957840"
  },
  {
    "text": "problem or something and then just kick off the restart again and then uh in",
    "start": "1957840",
    "end": "1964080"
  },
  {
    "text": "this auto remediation approach user doing the similar thing in this flow that it calls this typical ETL pipeline",
    "start": "1964080",
    "end": "1970640"
  },
  {
    "text": "plus auditing process and then they can tell Mro okay if this ser fail Please",
    "start": "1970640",
    "end": "1976080"
  },
  {
    "text": "don't fail the workflow and the page me Please uh just uh ignore that first and",
    "start": "1976080",
    "end": "1981200"
  },
  {
    "text": "then uh run a recovery job So this check status job will see oh if you actually",
    "start": "1981200",
    "end": "1988880"
  },
  {
    "text": "ser fails or not you failed it goes to this recovery step and run the recovery",
    "start": "1988880",
    "end": "1994399"
  },
  {
    "text": "job and then run this workflow again to see if it succeeds If not then page the",
    "start": "1994399",
    "end": "1999840"
  },
  {
    "text": "user Um yeah so this is how user define that using the mRO uh so they can go",
    "start": "1999840",
    "end": "2008320"
  },
  {
    "text": "define this uh uh workflow right with this special flag saying ignore failure",
    "start": "2008320",
    "end": "2014080"
  },
  {
    "text": "then uh they define the data information which I mentioned like if else condition here trying to route the uh workflow to",
    "start": "2014080",
    "end": "2021679"
  },
  {
    "text": "different paths if based on the status of that workflow job to enable inquiry processing for",
    "start": "2021679",
    "end": "2028799"
  },
  {
    "text": "this complicated pattern It's very simple You just need the IB capture step at the beginning and the IB",
    "start": "2028799",
    "end": "2035840"
  },
  {
    "text": "commit step at the end That's it They don't need to actually modify lots of their code And then you can see here is",
    "start": "2035840",
    "end": "2042480"
  },
  {
    "text": "a visual the new uh pipeline So this is a workflow which points to the workflow",
    "start": "2042480",
    "end": "2047679"
  },
  {
    "text": "that I just I just showed uh it's a auto remediation pipeline and then it pass",
    "start": "2047679",
    "end": "2053280"
  },
  {
    "text": "the source table which is now become ICD table name to the down to the subflow where it also can pass a query as well",
    "start": "2053280",
    "end": "2062520"
  },
  {
    "text": "and then it adds this IB capture step trying to capture changes uh it says",
    "start": "2062520",
    "end": "2068638"
  },
  {
    "text": "it's ICDC mode and then uh the table is membership table and then only care the",
    "start": "2068639",
    "end": "2074000"
  },
  {
    "text": "append uh only in snapshots then in the commit step user just need to tell us",
    "start": "2074000",
    "end": "2079839"
  },
  {
    "text": "what's the step ID of the IB capture That's it So I c IPS can efficiently capture",
    "start": "2079839",
    "end": "2086800"
  },
  {
    "text": "the incremental change uh and handle the later data With those clean interfaces",
    "start": "2086800",
    "end": "2092000"
  },
  {
    "text": "the solution is compatible with the existing user experience with really low on boarding cost as I just showed U next",
    "start": "2092000",
    "end": "2099520"
  },
  {
    "text": "uh let's over walk over a few user uh cases and examples together",
    "start": "2099520",
    "end": "2104960"
  },
  {
    "text": "Uh so this is is a two-stage detail pipeline with two measure workflows and three tables and uh so this is a",
    "start": "2104960",
    "end": "2112160"
  },
  {
    "text": "playback table which saves the uh ingested events from the uh streaming pipeline and then the the the workflow",
    "start": "2112160",
    "end": "2121400"
  },
  {
    "text": "owner decide like I have to take a look back like 14 two weeks of data uh",
    "start": "2121400",
    "end": "2126880"
  },
  {
    "text": "because there's a lot of events come late and then uh in their daily pipeline they kind of aggregate this table and",
    "start": "2126880",
    "end": "2134079"
  },
  {
    "text": "and save it to target table by changing the position key from the processing to",
    "start": "2134079",
    "end": "2139119"
  },
  {
    "text": "the event time and then they have to do this uh every day and rewrite the past",
    "start": "2139119",
    "end": "2144960"
  },
  {
    "text": "14 days of data This pipeline can take quite a while So they have to run daily They cannot run hourly and then for the",
    "start": "2144960",
    "end": "2153440"
  },
  {
    "text": "this table consumers we may have like multiple hundreds of workflows consumed",
    "start": "2153440",
    "end": "2158800"
  },
  {
    "text": "from this table they build a kind of aggregation pipelines to power their business use cases and then here in this",
    "start": "2158800",
    "end": "2165680"
  },
  {
    "text": "case is uh this doing the data aggregation also have to consume the data from the past 14 days as well Um so",
    "start": "2165680",
    "end": "2173520"
  },
  {
    "text": "you can see it's time consuming plus that is also fragile uh and many times",
    "start": "2173520",
    "end": "2178720"
  },
  {
    "text": "if there's a traffic pattern change right so suddenly there's a huge events then or there's like",
    "start": "2178720",
    "end": "2186000"
  },
  {
    "text": "uh business logical change user have to adjust this look this look back window which then will be affect all the",
    "start": "2186000",
    "end": "2192000"
  },
  {
    "text": "downream workflows and everyone as well that's not a great pattern and then let's see how we can",
    "start": "2192000",
    "end": "2198960"
  },
  {
    "text": "make that to be um IPS enabled So here's a one that uh using the pattern one you",
    "start": "2198960",
    "end": "2205520"
  },
  {
    "text": "can see that we can easily just like have a ICPS table to hold the chain data and then let this pipeline to consume",
    "start": "2205520",
    "end": "2212320"
  },
  {
    "text": "from ICD table instead of from the original table Then we can easily merge into the uh uh target table",
    "start": "2212320",
    "end": "2219480"
  },
  {
    "text": "and also because this run so fast we can doing this hourly instead of",
    "start": "2219480",
    "end": "2225480"
  },
  {
    "text": "daily and this shows the uh uh some kind of uh uh changes in the user job write",
    "start": "2225480",
    "end": "2231800"
  },
  {
    "text": "job I just use a SQL as uh like a demonstration there's also scala or",
    "start": "2231800",
    "end": "2237680"
  },
  {
    "text": "other logic you can see that instead of instead of all right we can use merging two and then just change the front table",
    "start": "2237680",
    "end": "2245359"
  },
  {
    "text": "uh from the ICD table and then add the DUB logic Also you can use the insert",
    "start": "2245359",
    "end": "2251119"
  },
  {
    "text": "into if you like or there is a DUB logic or other workflows running trying to",
    "start": "2251119",
    "end": "2256640"
  },
  {
    "text": "distribute the distribute the data Okay for the stage two we use the",
    "start": "2256640",
    "end": "2264000"
  },
  {
    "text": "pattern two where we can uh have the ICD table hold the chain data then join with",
    "start": "2264000",
    "end": "2271359"
  },
  {
    "text": "the IC uh IPS table uh join with the original table and then doing this aggregation again and merging to target",
    "start": "2271359",
    "end": "2278160"
  },
  {
    "text": "table Same thing it can run super fast and then we can update the workflows cadence from daily to hourly as well And",
    "start": "2278160",
    "end": "2285599"
  },
  {
    "text": "this shows the change in SQL Uh so basically same thing uh inserted change inserted into override uh merging to and",
    "start": "2285599",
    "end": "2293200"
  },
  {
    "text": "then we join with the ICD table on those aggregation group by keys then we add the uh uh D2 logic as",
    "start": "2293200",
    "end": "2303680"
  },
  {
    "text": "well Okay So after change this two stage",
    "start": "2303880",
    "end": "2309200"
  },
  {
    "text": "pipeline from the arena way to this new ICD IPS enabled approach we see a huge",
    "start": "2309200",
    "end": "2315760"
  },
  {
    "text": "savings It's a like that the you see that look because look back window is about 14 days right and also the later",
    "start": "2315760",
    "end": "2322720"
  },
  {
    "text": "data is sparse right it's not always had lots of later data so actually the cost of the new pipeline is less or like only",
    "start": "2322720",
    "end": "2330240"
  },
  {
    "text": "like 10% the original pipeline also it improved the data",
    "start": "2330240",
    "end": "2336079"
  },
  {
    "text": "freshness as we can now run hourly yeah so the next example shows",
    "start": "2336079",
    "end": "2341440"
  },
  {
    "text": "the uh um multi tableable cases using the pattern uh three where we have like",
    "start": "2341440",
    "end": "2348800"
  },
  {
    "text": "uh three row table staging tables row tables which have the events from stream pipeline and the first uh the first one",
    "start": "2348800",
    "end": "2357040"
  },
  {
    "text": "and last one has like a later data and the middle one does not So middle one can be a normal pipeline and this first",
    "start": "2357040",
    "end": "2362640"
  },
  {
    "text": "and uh third pipeline will be the one with the IPS enabled Then they produce",
    "start": "2362640",
    "end": "2368880"
  },
  {
    "text": "this hourly table one two three and then the uh this final pipeline will based on",
    "start": "2368880",
    "end": "2375440"
  },
  {
    "text": "the range captured right uh here is three min is three and max is uh you",
    "start": "2375440",
    "end": "2381359"
  },
  {
    "text": "know max is six so it are going to load the data from all three tables from",
    "start": "2381359",
    "end": "2386640"
  },
  {
    "text": "position three to six our six to uh three to six together and then doing the",
    "start": "2386640",
    "end": "2391839"
  },
  {
    "text": "complicated uh join applica uh operations to produce data to the target",
    "start": "2391839",
    "end": "2398240"
  },
  {
    "text": "Okay Yeah I hope this example um demonstrate how simple and powerful it",
    "start": "2401280",
    "end": "2407200"
  },
  {
    "text": "is for users work with M show using the uh uh like IPS Uh now let's do a quick",
    "start": "2407200",
    "end": "2415440"
  },
  {
    "text": "recap So IPS uh enables new patterns and also let us rethink about ETL a batch",
    "start": "2415440",
    "end": "2421680"
  },
  {
    "text": "ETL which is sometimes kind of considered to be replaced by the streaming pipeline which also have some",
    "start": "2421680",
    "end": "2427920"
  },
  {
    "text": "other troubles as well and then uh so actually uh the batch are still very",
    "start": "2427920",
    "end": "2433119"
  },
  {
    "text": "popular and widely used especially in the analytics domain With this uh IPS",
    "start": "2433119",
    "end": "2439560"
  },
  {
    "text": "support we can uh power the ETL batch ETL to have the data accuracy freshness",
    "start": "2439560",
    "end": "2447040"
  },
  {
    "text": "and the cost efficiency All of this that can fill lots of gaps and enable lots of use cases even sometimes that we may not",
    "start": "2447040",
    "end": "2454640"
  },
  {
    "text": "need to move to the streaming and can stay with the batch Um so in the talk we",
    "start": "2454640",
    "end": "2460560"
  },
  {
    "text": "show like how we can use the iceberg metadata to be able to efficiently capture change and then uh we also",
    "start": "2460560",
    "end": "2467359"
  },
  {
    "text": "showed the power of the decoupling that either can help the to address concerns differently and then let a user uh have",
    "start": "2467359",
    "end": "2474400"
  },
  {
    "text": "minimum changes Um also with a clean interface that mash provides we provide",
    "start": "2474400",
    "end": "2479440"
  },
  {
    "text": "a greater user experience and also with minimum effort user can adopt to this new approach Um also hope you can",
    "start": "2479440",
    "end": "2487200"
  },
  {
    "text": "leverage some of those patterns that we discovered in our uh new work if",
    "start": "2487200",
    "end": "2492359"
  },
  {
    "text": "applicable Um yeah uh so what's next So we are going to uh move some of the",
    "start": "2492359",
    "end": "2500040"
  },
  {
    "text": "implementation by uh from the uh our own side to the iceberg like SQL extension",
    "start": "2500040",
    "end": "2506160"
  },
  {
    "text": "to be able to create a view instead of a table to reduce the maintenance class So we don't need delay table and also we",
    "start": "2506160",
    "end": "2513040"
  },
  {
    "text": "add a support we are going to add a support for other types of snapshots beyond the just append and also we are",
    "start": "2513040",
    "end": "2518960"
  },
  {
    "text": "working with iceber community to add a cookbook to show how doing the change capturing and the last but not least we",
    "start": "2518960",
    "end": "2525359"
  },
  {
    "text": "are working on the auto cascading data back field features using the",
    "start": "2525359",
    "end": "2530839"
  },
  {
    "text": "IPS So lastly I would like to acknowledge the uh team uh the Netflix team for the great team work and uh this",
    "start": "2530839",
    "end": "2537920"
  },
  {
    "text": "concludes my presentation Thank you [Applause]",
    "start": "2537920",
    "end": "2547359"
  },
  {
    "text": "All right June thank you so much for a great presentation Uh if anyone has question this is a good time to raise",
    "start": "2547359",
    "end": "2552720"
  },
  {
    "text": "your hand June will also be there later on if you want to take questions and he can take questions in private as well",
    "start": "2552720",
    "end": "2561318"
  },
  {
    "text": "Uh thanks for the talk So I um I have a question regarding the join operation",
    "start": "2564079",
    "end": "2570560"
  },
  {
    "text": "that you mentioned for incremental processing So um if let's say I'm doing",
    "start": "2570560",
    "end": "2576240"
  },
  {
    "text": "uh like a seven like seven day or 14 days of drawing right even for the",
    "start": "2576240",
    "end": "2582079"
  },
  {
    "text": "incremental part you will still need to load the data on left side and right",
    "start": "2582079",
    "end": "2587599"
  },
  {
    "text": "hand side both for 14 days right because the incremental data might be joining",
    "start": "2587599",
    "end": "2593680"
  },
  {
    "text": "with the historical data in the past so I'm trying to understand uh does that",
    "start": "2593680",
    "end": "2598800"
  },
  {
    "text": "how would you be able to achieve like 10 uh x of the cost reduction in that case",
    "start": "2598800",
    "end": "2604640"
  },
  {
    "text": "Got it Yeah that's a great one Uh so that reduction is based on the fact that",
    "start": "2604640",
    "end": "2609760"
  },
  {
    "text": "if you guesstimate the rent you need to do the drawing right It may not accurate or many times you have to kind of conser",
    "start": "2609760",
    "end": "2616720"
  },
  {
    "text": "be conservative to handle the worst scenario So you actually have a really long window then you kind of have to",
    "start": "2616720",
    "end": "2623200"
  },
  {
    "text": "then you join that So but but you are right that the cost efficiency we got",
    "start": "2623200",
    "end": "2628480"
  },
  {
    "text": "here is mainly for the pattern one and two but not for that capture range cases and the capture range cases is more like",
    "start": "2628480",
    "end": "2634720"
  },
  {
    "text": "we can achieve the like the optimal like data accuracy as we know exactly what's",
    "start": "2634720",
    "end": "2640319"
  },
  {
    "text": "the change but the gain the cost efficiency gain from there may not be that huge like 10x",
    "start": "2640319",
    "end": "2648440"
  },
  {
    "text": "Thank you again June for a great presentation Please give June a big round of applause",
    "start": "2654480",
    "end": "2661560"
  },
  {
    "text": "[Music]",
    "start": "2664960",
    "end": "2670409"
  }
]