[
  {
    "text": "thanks everyone I'm pleased to be here at Q Khan it's my first time here and I know that this is an awesome conference",
    "start": "3909",
    "end": "10120"
  },
  {
    "text": "and I always wanted to either come here and well have a pleasure to meet",
    "start": "10120",
    "end": "15129"
  },
  {
    "text": "everyone or be able to present and Here I am that's I'm very pleased so as it",
    "start": "15129",
    "end": "21040"
  },
  {
    "text": "was already mentioned my name is Alex I'm an Apache Cassandra committer and today we're going to talk about the on",
    "start": "21040",
    "end": "26740"
  },
  {
    "text": "disk storage and if you're new to the subject it's quite hard to find the",
    "start": "26740",
    "end": "32680"
  },
  {
    "text": "information and kind of put the pieces together some systems have existed for like decades that for many years and",
    "start": "32680",
    "end": "39489"
  },
  {
    "text": "this somewhat skews the information that is easier to find that it's more",
    "start": "39489",
    "end": "45489"
  },
  {
    "text": "accessible and other things are like somewhat difficult to find and you kind",
    "start": "45489",
    "end": "50679"
  },
  {
    "text": "of stop knowing what is good and what is bad and do not really know where to",
    "start": "50679",
    "end": "55690"
  },
  {
    "text": "start and I couldn't find a good summary about what's going on in the storage world so I thought that why not just do",
    "start": "55690",
    "end": "64059"
  },
  {
    "text": "a talk or actually it started all with the serious of the blog post that you",
    "start": "64059",
    "end": "69100"
  },
  {
    "text": "can also read online and now it's kind of turning into the book about well",
    "start": "69100",
    "end": "74590"
  },
  {
    "text": "everything database is related and it should help you to understand and give you an introduction to this world of",
    "start": "74590",
    "end": "82439"
  },
  {
    "text": "algorithms behind base storage let's give let's talk about the reasons",
    "start": "82439",
    "end": "88750"
  },
  {
    "text": "to give this talk over the last decade or so the amount of data processed by even an average application has grown",
    "start": "88750",
    "end": "95890"
  },
  {
    "text": "tremendously I would say if you were to shop for a database in like 2006 you",
    "start": "95890",
    "end": "101049"
  },
  {
    "text": "would have just a few choices so it was like rather obvious I won't name names but we all know those and now we have",
    "start": "101049",
    "end": "108640"
  },
  {
    "text": "all sorts of databases even here like there were several presented and if you were to shop for database today what",
    "start": "108640",
    "end": "115390"
  },
  {
    "text": "would you pick like what do they have underneath what are they using is this good like from theoretical perspective",
    "start": "115390",
    "end": "121929"
  },
  {
    "text": "how do we compare the databases how do we say what is good for my application so because now you have all sorts of",
    "start": "121929",
    "end": "130240"
  },
  {
    "text": "good databases it's kind of hard to answer these questions because it's also sometimes hard to know what's inside of",
    "start": "130240",
    "end": "136180"
  },
  {
    "text": "course this doesn't make of neither database developers or application developers easier because",
    "start": "136180",
    "end": "142090"
  },
  {
    "text": "database folks are trying to squeeze out the last bits of performance out of their systems and application developers",
    "start": "142090",
    "end": "148560"
  },
  {
    "text": "require every day new tools potentially having more features and at some point",
    "start": "148560",
    "end": "154810"
  },
  {
    "text": "the when the obvious optimizations are already done and we still need to",
    "start": "154810",
    "end": "161200"
  },
  {
    "text": "increase like density of the node or decrease the latency we kind of have to",
    "start": "161200",
    "end": "167560"
  },
  {
    "text": "start making trade-offs because everything else has already optimized the way and knowing these trade-offs and",
    "start": "167560",
    "end": "173530"
  },
  {
    "text": "understanding why where they're coming from can be instrumental for creating better tools or building the",
    "start": "173530",
    "end": "179890"
  },
  {
    "text": "applications on top of them in this talk I will summarize the modern practices build vocabulary and again it will give",
    "start": "179890",
    "end": "187570"
  },
  {
    "text": "you or help us all to gain some more intuition to get started with the",
    "start": "187570",
    "end": "193300"
  },
  {
    "text": "subject and systematize it a little bit for the folks who are already familiar with this song by the way who has",
    "start": "193300",
    "end": "201310"
  },
  {
    "text": "already some familiarity with the concepts such as like B trees and LS m",
    "start": "201310",
    "end": "207430"
  },
  {
    "text": "trees okay so I have at least something to talk about this is good this is very",
    "start": "207430",
    "end": "212740"
  },
  {
    "text": "good so B tree is obviously most of the people know already but at least I can",
    "start": "212740",
    "end": "218700"
  },
  {
    "text": "hopefully shed some some light on and compare them to the other ones so but",
    "start": "218700",
    "end": "225550"
  },
  {
    "text": "let's not run in front of ourselves given a talk format is quite hard to",
    "start": "225550",
    "end": "230620"
  },
  {
    "text": "cover all the latest developments but I'm also writing a book as it was mentioned with O'Reilly and hope it will",
    "start": "230620",
    "end": "236800"
  },
  {
    "text": "give you even more details and help to gain even better understanding if you're",
    "start": "236800",
    "end": "242530"
  },
  {
    "text": "curious hit me up I let's chat about it a little bit and stay tuned for that",
    "start": "242530",
    "end": "247980"
  },
  {
    "text": "this is not a simple subject and in 45 minutes of the talk I'll only be able to",
    "start": "247980",
    "end": "253420"
  },
  {
    "text": "scratch the surface of course and to learn more you will have to dig into the books papers and code I personally had",
    "start": "253420",
    "end": "261430"
  },
  {
    "text": "to read approximately 120 papers in order to cover well the subject in the",
    "start": "261430",
    "end": "266650"
  },
  {
    "text": "book and approximately six books which will cover different aspects for each one of the things that we're",
    "start": "266650",
    "end": "273669"
  },
  {
    "text": "gonna be talking about so it's a vast topic but this hopefully is going to get",
    "start": "273669",
    "end": "278800"
  },
  {
    "text": "you started I'll try my best to make this talk free of empty promises and all",
    "start": "278800",
    "end": "284710"
  },
  {
    "text": "sorts of single silver bullets and present only face facts and make it",
    "start": "284710",
    "end": "290020"
  },
  {
    "text": "unbiased even though I work for pachysandra n or I mean I work on Apache Cassandra",
    "start": "290020",
    "end": "295659"
  },
  {
    "text": "not for on Apache Cassandra and I I mean",
    "start": "295659",
    "end": "301659"
  },
  {
    "text": "I've seen the code and I know the trade-offs that Apache Cassandra is making however it doesn't mean that I'm",
    "start": "301659",
    "end": "307839"
  },
  {
    "text": "going to be saying that this is the only way to go this would be unfair of me and after all there are no clear winners and",
    "start": "307839",
    "end": "315039"
  },
  {
    "text": "we're yet to see absolutely optimal system and actually I doubt that this is going to happen before we dig into the",
    "start": "315039",
    "end": "321789"
  },
  {
    "text": "storage systems themselves let's just start with some terminology one of the most important things that sets",
    "start": "321789",
    "end": "327550"
  },
  {
    "text": "different storage systems apart is access patterns if we were to store data on the let's say vinyl record access",
    "start": "327550",
    "end": "335439"
  },
  {
    "text": "pattern would describe the needle movement usually we distinguish between random sequential and mixed access",
    "start": "335439",
    "end": "342430"
  },
  {
    "text": "pattern and while mixed is kind of clear so this is a mix of sequential and",
    "start": "342430",
    "end": "348430"
  },
  {
    "text": "random let's take a look at what sequential and random actually means by sequential accesses we usually mean",
    "start": "348430",
    "end": "354520"
  },
  {
    "text": "reading contiguous memory segments without six in between this is as if you",
    "start": "354520",
    "end": "360639"
  },
  {
    "text": "were playing your favorite album where tracks are perfectly laid down by artists and exactly in the way that you",
    "start": "360639",
    "end": "368169"
  },
  {
    "text": "would enjoy listening to them so this happens not very often but it does and random access differs in that it doesn't",
    "start": "368169",
    "end": "375839"
  },
  {
    "text": "read contiguous memory segments so it",
    "start": "375839",
    "end": "380889"
  },
  {
    "text": "has to perform seeks in order to find the locations for the next read the essence seeks these seeks are hard to",
    "start": "380889",
    "end": "388719"
  },
  {
    "text": "predict we say it's random it's kind of a like you are listening an album where",
    "start": "388719",
    "end": "393759"
  },
  {
    "text": "you only like a couple of songs so you listen the first one and then you skip a couple then you listen to the other one",
    "start": "393759",
    "end": "401019"
  },
  {
    "text": "etc etc so you kind of have to seek for the songs maybe even performed and not in the",
    "start": "401019",
    "end": "407320"
  },
  {
    "text": "order that the artist has written them on the vinyl record so what we have",
    "start": "407320",
    "end": "414070"
  },
  {
    "text": "discussed so far is well related only to",
    "start": "414070",
    "end": "419080"
  },
  {
    "text": "the reeds of course the terminology doesn't change for rights at all so sequential and random rides are have",
    "start": "419080",
    "end": "426880"
  },
  {
    "text": "pretty much the same meaning but semantics are slightly different for example sequential writes do not always",
    "start": "426880",
    "end": "432460"
  },
  {
    "text": "result into the sequential reads data that is written closely is not necessarily going to be read together so",
    "start": "432460",
    "end": "439810"
  },
  {
    "text": "if you have data stored in the B Tree on disk and in order to maintain key order",
    "start": "439810",
    "end": "446320"
  },
  {
    "text": "and allow sequential access for like range scans which is often done like whenever you have in SQL statement with",
    "start": "446320",
    "end": "453850"
  },
  {
    "text": "like where X larger than 1 and then you scan like a large range of data you have",
    "start": "453850",
    "end": "463420"
  },
  {
    "text": "to perform random seeks in order to locate the data but then you can start",
    "start": "463420",
    "end": "468790"
  },
  {
    "text": "reading at one point and continue reading until you reach either the end of the data or the end of the range that",
    "start": "468790",
    "end": "474820"
  },
  {
    "text": "you were searching in order to achieve sequential writes however we use either",
    "start": "474820",
    "end": "480780"
  },
  {
    "text": "in-memory buffering or append-only storage of course happened only storage",
    "start": "480780",
    "end": "485950"
  },
  {
    "text": "due to its nature results in two random reads because well the order in which we",
    "start": "485950",
    "end": "492070"
  },
  {
    "text": "are randomly writing the data into the database is definitely almost definitely",
    "start": "492070",
    "end": "497080"
  },
  {
    "text": "not going to be the order in which we are we would like to have it read unless",
    "start": "497080",
    "end": "502870"
  },
  {
    "text": "it's some some sort of a log storage like for instance in Kafka so whenever",
    "start": "502870",
    "end": "509260"
  },
  {
    "text": "you actually would like to do that so to put the data that will be read together",
    "start": "509260",
    "end": "516669"
  },
  {
    "text": "we have to kind of prepare it for it it's done by collecting and buffering the records in memory then sorting them",
    "start": "516670",
    "end": "523630"
  },
  {
    "text": "and then writing them down and disk you see that we can achieve sequential reads",
    "start": "523630",
    "end": "528940"
  },
  {
    "text": "by either random writes or sequential writes but we'll we always have a trade-off locating right point or",
    "start": "528940",
    "end": "536610"
  },
  {
    "text": "buffering and sorting the records right so it's one of the two overheads this dichotomy",
    "start": "536610",
    "end": "543460"
  },
  {
    "text": "or dichotomy is going to hunt us throughout the talk and we'll see that",
    "start": "543460",
    "end": "548980"
  },
  {
    "text": "in its implications are going to be extremely important for database and",
    "start": "548980",
    "end": "554110"
  },
  {
    "text": "storage design and while SSDs have eliminated the coast of random i/o reeds",
    "start": "554110",
    "end": "560590"
  },
  {
    "text": "compared to sequential we have to keep in mind that it applies only to block reads and writes since blocks block is",
    "start": "560590",
    "end": "567580"
  },
  {
    "text": "kind of a smallest atomic entry that can be transferred between well the block",
    "start": "567580",
    "end": "573850"
  },
  {
    "text": "device and your application well with many layers in between reading just a",
    "start": "573850",
    "end": "579010"
  },
  {
    "text": "few bytes out of the block means that you have to transfer the whole block",
    "start": "579010",
    "end": "584080"
  },
  {
    "text": "from disk anyways so for example things like range scans can be only effectively implemented in",
    "start": "584080",
    "end": "590680"
  },
  {
    "text": "the storage where data is laid out and prepared for sequential reads the",
    "start": "590680",
    "end": "597550"
  },
  {
    "text": "situation with writes is that they're both bad both for SSDs and hardest",
    "start": "597550",
    "end": "603490"
  },
  {
    "text": "drives but for slightly different reasons on hard disk drives random i/o",
    "start": "603490",
    "end": "609190"
  },
  {
    "text": "is bad because the arm has to seek in order like to the correct track in order to well navigate from one position to",
    "start": "609190",
    "end": "616690"
  },
  {
    "text": "another to perform every right and with SSDs the reason is well pretty much how",
    "start": "616690",
    "end": "623950"
  },
  {
    "text": "they're built modes most of the modern SSDs are built using NAND gates and a",
    "start": "623950",
    "end": "630070"
  },
  {
    "text": "and D gates so due to optimizations and cost efficiency the smallest erasable entity",
    "start": "630070",
    "end": "637420"
  },
  {
    "text": "is series of blocks and the data can be written only after the blocks are erased",
    "start": "637420",
    "end": "644800"
  },
  {
    "text": "fully so this means that the right operation can only set the single bits",
    "start": "644800",
    "end": "651840"
  },
  {
    "text": "but erase operations can unset but not single bits but entire units like",
    "start": "651840",
    "end": "658630"
  },
  {
    "text": "several blocks at a time and this is all like this like very low level then one",
    "start": "658630",
    "end": "666340"
  },
  {
    "text": "level above that there is an flash translation layer which is doing a job of keeping record",
    "start": "666340",
    "end": "671900"
  },
  {
    "text": "of these abandoned pages like the pages that were once at one place then were",
    "start": "671900",
    "end": "676990"
  },
  {
    "text": "well-read and has to have to be rewritten now they have to be relocated and the previous is abandoned and now",
    "start": "676990",
    "end": "683570"
  },
  {
    "text": "like if there are memory segments which are close to it we have to relocate them as well in order to well clear out the",
    "start": "683570",
    "end": "691070"
  },
  {
    "text": "entire block so the flash translation layer will have to do all this process",
    "start": "691070",
    "end": "698660"
  },
  {
    "text": "of garbage collection so eventually we run out of empty pages after doing lots",
    "start": "698660",
    "end": "704840"
  },
  {
    "text": "of Rights and have to garbage collect and nullify basically unset those blocks possibly relocating their neighbors this",
    "start": "704840",
    "end": "712010"
  },
  {
    "text": "operation is performed during a write and can increase the cost of Rights since every write implies block",
    "start": "712010",
    "end": "719150"
  },
  {
    "text": "relocation and subsequent kind of garbage collection we have to keep the",
    "start": "719150",
    "end": "726260"
  },
  {
    "text": "right and blue key amplification as low as we can in order to reduce the amount",
    "start": "726260",
    "end": "732440"
  },
  {
    "text": "of i/o in general which will subsequently also reduce the amount of GC that we're gonna have in order to",
    "start": "732440",
    "end": "738980"
  },
  {
    "text": "summarize everything that we were just saying about the sequential and random i/o we say that sequential i/o is",
    "start": "738980",
    "end": "745400"
  },
  {
    "text": "generally good since it's more predictable and it gives you just more material to work with to optimize to",
    "start": "745400",
    "end": "751970"
  },
  {
    "text": "make it faster and better but because we cannot avoid random i/o altogether in",
    "start": "751970",
    "end": "757850"
  },
  {
    "text": "life we have to kind of balance between what makes sense and what's best for the",
    "start": "757850",
    "end": "763460"
  },
  {
    "text": "database and another important concept and distinction between that storage",
    "start": "763460",
    "end": "770380"
  },
  {
    "text": "database storage designers taken to the consideration is the mutability or",
    "start": "770380",
    "end": "778160"
  },
  {
    "text": "immutability of the story of the well disk format or of the file a database",
    "start": "778160",
    "end": "785510"
  },
  {
    "text": "structure this also has significant implications to the disk layout construction on the maintenance process",
    "start": "785510",
    "end": "791630"
  },
  {
    "text": "and many other things mutable data structures usually pre allocate the",
    "start": "791630",
    "end": "796760"
  },
  {
    "text": "memory and do in-place updates so basically find the thing on the disk and kind of update it in place wherever it",
    "start": "796760",
    "end": "803990"
  },
  {
    "text": "was in order to emerge is the cost of Rights for the data that is stored together this usually results",
    "start": "803990",
    "end": "811190"
  },
  {
    "text": "in two random i/o in order to perform a right a bunch of reads have to be done in order to locate the destination and",
    "start": "811190",
    "end": "817520"
  },
  {
    "text": "then we can do the actual right and this the rights which are well close to each",
    "start": "817520",
    "end": "824240"
  },
  {
    "text": "other in time will not most likely be in the continuous can well not be written",
    "start": "824240",
    "end": "831950"
  },
  {
    "text": "together since updates are made in place all the data is read from a single",
    "start": "831950",
    "end": "838280"
  },
  {
    "text": "source and reads will not have to merge and reconcile data from different sources because we basically have a",
    "start": "838280",
    "end": "844520"
  },
  {
    "text": "single source of truth so a single file we just read from it because it would be",
    "start": "844520",
    "end": "849620"
  },
  {
    "text": "unsafe to concurrently modify the data that can change anytime underneath of us",
    "start": "849620",
    "end": "856270"
  },
  {
    "text": "excesses and modifications have to be guarded with concurrency primitives that help with mutual exclusions these are",
    "start": "856270",
    "end": "863810"
  },
  {
    "text": "locks which guard b-tree data integrity and latches which guard between or",
    "start": "863810",
    "end": "870590"
  },
  {
    "text": "excuse me the locks which guard B tree data integrity and latches which guard",
    "start": "870590",
    "end": "876710"
  },
  {
    "text": "between structure integrity these are two different concepts slightly",
    "start": "876710",
    "end": "882550"
  },
  {
    "text": "overloaded terms and I would really recommend listening to for instance CMU",
    "start": "882550",
    "end": "888460"
  },
  {
    "text": "talks by Andy Pablo he really well explains the concepts of locks and",
    "start": "888460",
    "end": "894770"
  },
  {
    "text": "latches in many details the immutable data structures on other hand require no",
    "start": "894770",
    "end": "900440"
  },
  {
    "text": "memory overhead for the subsequent updates since well files are pretty much",
    "start": "900440",
    "end": "905960"
  },
  {
    "text": "written on this just once and will never be changed anymore however in order to",
    "start": "905960",
    "end": "911690"
  },
  {
    "text": "perform a read since all the files are immutable that will be well kind of multiple versions of the same record of",
    "start": "911690",
    "end": "919820"
  },
  {
    "text": "data for the same key located in several several different files we have will",
    "start": "919820",
    "end": "925220"
  },
  {
    "text": "have to read all of these files reconcile merge the data together and only then will be able to return it to",
    "start": "925220",
    "end": "933080"
  },
  {
    "text": "the user rights here are sequential so data is backed up in memory and sorted",
    "start": "933080",
    "end": "938360"
  },
  {
    "text": "and written out sick and since files are not modified on disk and updates would effectively mean well",
    "start": "938360",
    "end": "944930"
  },
  {
    "text": "pretty much rewriting the whole file immutable data structure require merged from different different sources before",
    "start": "944930",
    "end": "952309"
  },
  {
    "text": "returning the data to different clients now let's move on to the first data",
    "start": "952309",
    "end": "957860"
  },
  {
    "text": "structure that I thought would be great to discuss today and it would be log",
    "start": "957860",
    "end": "963290"
  },
  {
    "text": "structured merge trees we covered the basic vocabulary and well we can start",
    "start": "963290",
    "end": "969470"
  },
  {
    "text": "talking about it LSM trees is an immutable disk resident right optimize",
    "start": "969470",
    "end": "974629"
  },
  {
    "text": "the data structure Allisyn trees have been getting more attention because the insert update and",
    "start": "974629",
    "end": "981740"
  },
  {
    "text": "delete operations even for their sordid variants do not require random i/o so",
    "start": "981740",
    "end": "988490"
  },
  {
    "text": "which is good so you do not to locate the place where you're gonna write on disk at all in order to use LSM trees in",
    "start": "988490",
    "end": "995929"
  },
  {
    "text": "order to allow sequential writes LSM trees batch up writes and updates in",
    "start": "995929",
    "end": "1001899"
  },
  {
    "text": "memory resident table so it's like somewhere in RAM it's often implemented using some sort of like sorted data",
    "start": "1001899",
    "end": "1008679"
  },
  {
    "text": "structures such as binary search tree or a skip list so anything that would allow",
    "start": "1008679",
    "end": "1013749"
  },
  {
    "text": "you quickly searching like logarithmic lookup time would be pretty good and when the size of this table memory base",
    "start": "1013749",
    "end": "1021579"
  },
  {
    "text": "table is reaching a certain threshold its contents are written on disk and this operation is called flush after a",
    "start": "1021579",
    "end": "1029709"
  },
  {
    "text": "few flushes are performed and data ends up being split between multiple on disk",
    "start": "1029709",
    "end": "1037000"
  },
  {
    "text": "tables now in order to retrieve the data we have to search all these resident parts of the tree check in memory table",
    "start": "1037000",
    "end": "1043990"
  },
  {
    "text": "and their contents before well return and the result itself so writes are only addressing the memory resident table",
    "start": "1043990",
    "end": "1051100"
  },
  {
    "text": "while reads have to reconcile the data from everywhere from memory and disk",
    "start": "1051100",
    "end": "1057539"
  },
  {
    "text": "disk resident and table files are immutable they have written only once",
    "start": "1057539",
    "end": "1063370"
  },
  {
    "text": "and never modified and can only be deleted on later stage and we're going",
    "start": "1063370",
    "end": "1069220"
  },
  {
    "text": "to be talking about it later in next few slides on the last slide we just seen that when",
    "start": "1069220",
    "end": "1075040"
  },
  {
    "text": "the data is read it has to be merged from the several multiple sources now",
    "start": "1075040",
    "end": "1080470"
  },
  {
    "text": "let's consider a specific example here we have two tables and they hold the",
    "start": "1080470",
    "end": "1085960"
  },
  {
    "text": "data for the same key this means that a record was first inserted and then",
    "start": "1085960",
    "end": "1091600"
  },
  {
    "text": "updated and now we have and now the well",
    "start": "1091600",
    "end": "1098230"
  },
  {
    "text": "former version of this data has to be discarded now let's check out how this",
    "start": "1098230",
    "end": "1103510"
  },
  {
    "text": "is happening on the world from the keys and values perspective record with the key Alex was written with one time step",
    "start": "1103510",
    "end": "1110620"
  },
  {
    "text": "and then on the later stage updated with higher timestamp so the resulting table",
    "start": "1110620",
    "end": "1116050"
  },
  {
    "text": "the merged table will only have the updated value for this key and record",
    "start": "1116050",
    "end": "1122520"
  },
  {
    "text": "with the key John gets deleted in the later table and will be discarded during",
    "start": "1122520",
    "end": "1129400"
  },
  {
    "text": "the merge process and not show up in the merge table at all and the other two",
    "start": "1129400",
    "end": "1135250"
  },
  {
    "text": "records are just well copied verbatim so to say because they're not yet out so they do not appear they appear only in",
    "start": "1135250",
    "end": "1142600"
  },
  {
    "text": "one of the tables and not the other to summarize the merge arithmetics we",
    "start": "1142600",
    "end": "1147760"
  },
  {
    "text": "have well quite simple operations going on here from the data standpoint merge",
    "start": "1147760",
    "end": "1153310"
  },
  {
    "text": "is reconciling the records from multiple sources so records have a key and timestamp associated with it and if two",
    "start": "1153310",
    "end": "1161040"
  },
  {
    "text": "tables hold the same value for the same key meaning that an update has a cured",
    "start": "1161040",
    "end": "1167410"
  },
  {
    "text": "only the record would alight latter or later timestamp will be picked and the",
    "start": "1167410",
    "end": "1173500"
  },
  {
    "text": "previous one will be discarded since the lead operation cannot physically remove the data from",
    "start": "1173500",
    "end": "1179410"
  },
  {
    "text": "immutable data structure we have to use something called dormant certificates sometimes also called Tung tombstones",
    "start": "1179410",
    "end": "1186900"
  },
  {
    "text": "they indicate that a record for a certain key has to be removed starting with a certain timestamp and during the",
    "start": "1186900",
    "end": "1193900"
  },
  {
    "text": "merge process records shadowed by these Tom stones are going to be discarded and",
    "start": "1193900",
    "end": "1199030"
  },
  {
    "text": "many modern element implementations such as rocks DB Cassandra Apache Cassandra are choose sorted string tables as they",
    "start": "1199030",
    "end": "1206840"
  },
  {
    "text": "file format because of its simplicity SS tables are persistent maps from keys to",
    "start": "1206840",
    "end": "1214220"
  },
  {
    "text": "values ordered by the key structurally SS tables are split in two parts it's an",
    "start": "1214220",
    "end": "1219440"
  },
  {
    "text": "index and the data block the index block contains keys mapped to the block",
    "start": "1219440",
    "end": "1224690"
  },
  {
    "text": "offsets pointing to where the actual record is stored or located and index is",
    "start": "1224690",
    "end": "1231470"
  },
  {
    "text": "implemented using a format you also with the good loop lookup guarantees such as",
    "start": "1231470",
    "end": "1238820"
  },
  {
    "text": "b3 which is good for range scans or if you just need point queries you can use something like a hash table and this",
    "start": "1238820",
    "end": "1246320"
  },
  {
    "text": "table has some several very interesting and nice properties first of all for the",
    "start": "1246320",
    "end": "1251390"
  },
  {
    "text": "point queries in order to find the value by the key it can be done quickly by",
    "start": "1251390",
    "end": "1256400"
  },
  {
    "text": "simply looking up the index and locating data then following up set offset and",
    "start": "1256400",
    "end": "1263290"
  },
  {
    "text": "reading the data from the data file and the range scans can eat well pretty much",
    "start": "1263290",
    "end": "1269660"
  },
  {
    "text": "do the same thing so they can first jump to the beginning of the range and then kind of keep reading the the data from",
    "start": "1269660",
    "end": "1277550"
  },
  {
    "text": "the sorted well data file without performing any extra 6 or anything else",
    "start": "1277550",
    "end": "1283520"
  },
  {
    "text": "so logically access table like the 1st SS table after flush represents a",
    "start": "1283520",
    "end": "1289490"
  },
  {
    "text": "snapshot of all the database operations over the period in time so because SS",
    "start": "1289490",
    "end": "1295700"
  },
  {
    "text": "tables created in this flash process from the memory base data structure as we have discussed on previous slides so",
    "start": "1295700",
    "end": "1304490"
  },
  {
    "text": "far we've covered that SS tables are created by the flush process and from memory resident tables but because SS",
    "start": "1304490",
    "end": "1311750"
  },
  {
    "text": "tables are immutable and are written sequentially and hold no reserved space",
    "start": "1311750",
    "end": "1317570"
  },
  {
    "text": "for like in-place modifications if we had a single SS table any insert update",
    "start": "1317570",
    "end": "1324290"
  },
  {
    "text": "or delete operation would probably mean that we have to rewrite the entire file so over the time amount of such tables",
    "start": "1324290",
    "end": "1331370"
  },
  {
    "text": "created by flush will only grow and reads will be getting more and more and more expensive because we will have to",
    "start": "1331370",
    "end": "1337970"
  },
  {
    "text": "merge more and more data files and data for the same key will be well pretty much located everywhere and",
    "start": "1337970",
    "end": "1344869"
  },
  {
    "text": "records will be shadowed by the way its etc etc so we will not only waste time",
    "start": "1344869",
    "end": "1351419"
  },
  {
    "text": "but we will also waste a bunch of space because of all these duplicates everywhere in order to reduce the cost",
    "start": "1351419",
    "end": "1359159"
  },
  {
    "text": "of reeds reconcile disk space shadow by caused by these shadowed records and",
    "start": "1359159",
    "end": "1366989"
  },
  {
    "text": "reduce the amount of these tables that have to be merged the LS entries propose",
    "start": "1366989",
    "end": "1372539"
  },
  {
    "text": "a process that reads complete asses tables from disk merges them and then writes them back this process is called",
    "start": "1372539",
    "end": "1378989"
  },
  {
    "text": "compaction as we have discussed as these tables are sorted by the key and compaction works sort of like merge sort",
    "start": "1378989",
    "end": "1385850"
  },
  {
    "text": "which makes it very efficient so records are read from several sources",
    "start": "1385850",
    "end": "1390950"
  },
  {
    "text": "sequentially then merged and appended to the file also in a sequential manner and",
    "start": "1390950",
    "end": "1396619"
  },
  {
    "text": "one of the advantages of this merge iteration is that can it it can work",
    "start": "1396619",
    "end": "1402239"
  },
  {
    "text": "very efficiently also for the datasets that do not fit in memory and resulting",
    "start": "1402239",
    "end": "1409679"
  },
  {
    "text": "table preserves the order of the original SS table so you will be able to repeat the process of compaction over",
    "start": "1409679",
    "end": "1417359"
  },
  {
    "text": "and over over your data set during the",
    "start": "1417359",
    "end": "1422669"
  },
  {
    "text": "process emerged SS tables are discarded and they are replaced by their compacted",
    "start": "1422669",
    "end": "1428039"
  },
  {
    "text": "version after compaction is done the amount of SS tables is reduced and the queries are made more efficient to",
    "start": "1428039",
    "end": "1435239"
  },
  {
    "text": "summarize what we have just learned about Alice entries posting from first and foremost they are immutable and well",
    "start": "1435239",
    "end": "1443309"
  },
  {
    "text": "their contents are never changed on disk in order to reconcile the disk space we",
    "start": "1443309",
    "end": "1449609"
  },
  {
    "text": "use the process which is called compaction which merges things together and discards the old data and everything",
    "start": "1449609",
    "end": "1456299"
  },
  {
    "text": "that we do not need anymore and Alice M trees are right optimized so any non-durable insert update or delete",
    "start": "1456299",
    "end": "1462950"
  },
  {
    "text": "does not require any disk 6 whatsoever and well let's call it a day",
    "start": "1462950",
    "end": "1472590"
  },
  {
    "text": "so that's it so now let's talk about the b-trees which most of you seem to know but I",
    "start": "1472590",
    "end": "1479350"
  },
  {
    "text": "will only scratch the surface so I will not go into the details that you probably already know and try to",
    "start": "1479350",
    "end": "1485020"
  },
  {
    "text": "concentrate on the things which are lesser now so one of the prominent papers on the bee trees is called",
    "start": "1485020",
    "end": "1490450"
  },
  {
    "text": "ubiquitous Big B trees which describes in great detail several variants of B",
    "start": "1490450",
    "end": "1496420"
  },
  {
    "text": "trees and their applications the original paper was published by but",
    "start": "1496420",
    "end": "1501730"
  },
  {
    "text": "buyer and Emma crate back in 1972 but this paper or this data structure",
    "start": "1501730",
    "end": "1508090"
  },
  {
    "text": "remains important and useful up til today researchers still come up with new ways to optimize B trees and come up",
    "start": "1508090",
    "end": "1514630"
  },
  {
    "text": "with their different variants and some of them are extremely useful before we",
    "start": "1514630",
    "end": "1520540"
  },
  {
    "text": "dig into the vitreous let's remember their predecessor the binary search tree which was kind of logically preceded",
    "start": "1520540",
    "end": "1527560"
  },
  {
    "text": "them the binary search trees are useful for in memory as an in-memory sorted",
    "start": "1527560",
    "end": "1534670"
  },
  {
    "text": "data structure but they are not very good to you to be used on the disk because of the balancing so you have to",
    "start": "1534670",
    "end": "1541720"
  },
  {
    "text": "perform like balanced notes of the bsts in order to well keep them balanced or",
    "start": "1541720",
    "end": "1546940"
  },
  {
    "text": "keep the height to the minimum and because of the low fan-out meaning that they only have like two child nodes per",
    "start": "1546940",
    "end": "1554140"
  },
  {
    "text": "node which would not be very efficient given the block size of even four",
    "start": "1554140",
    "end": "1559630"
  },
  {
    "text": "kilobytes so this all doesn't work very well in disk therefore the community came up with B",
    "start": "1559630",
    "end": "1566710"
  },
  {
    "text": "trees and B trees allow storing more than two pointers per node and work very",
    "start": "1566710",
    "end": "1572110"
  },
  {
    "text": "well with block devices by matching node size to the page size and today well of",
    "start": "1572110",
    "end": "1577270"
  },
  {
    "text": "course that there are many implementations that are used larger page sizes but I think it's kind of irrelevant to the talk and betrays have",
    "start": "1577270",
    "end": "1584860"
  },
  {
    "text": "the following properties first of all they're sorted which allows the sequential scans and simplifies lookups",
    "start": "1584860",
    "end": "1591250"
  },
  {
    "text": "pretty much the same as with SS tables that we just talked B trees are self",
    "start": "1591250",
    "end": "1597400"
  },
  {
    "text": "balancing so there is no need to balance the tree during insertions and deletions so it's going to be like the hide going",
    "start": "1597400",
    "end": "1604660"
  },
  {
    "text": "to be uniform across the tree and when the bee tree node is full it is",
    "start": "1604660",
    "end": "1609790"
  },
  {
    "text": "being split in two parts when the occupancy of the neighboring nodes is",
    "start": "1609790",
    "end": "1615460"
  },
  {
    "text": "low enough the merged are going to be merged together this also means that leaves the bottom layer of the tree are",
    "start": "1615460",
    "end": "1623500"
  },
  {
    "text": "going to be always equidistant from the root of the tree and probably the most",
    "start": "1623500",
    "end": "1630430"
  },
  {
    "text": "important thing is that be trees are mutable so inserts updates and deletes are performed on discs in place in order",
    "start": "1630430",
    "end": "1637720"
  },
  {
    "text": "to make this updates on in place possible B trees first have to locate",
    "start": "1637720",
    "end": "1642940"
  },
  {
    "text": "the data unlike LS entries that we just talked about additionally in order to avoid",
    "start": "1642940",
    "end": "1649050"
  },
  {
    "text": "resizing nodes during each update B trees reserve some space for the future",
    "start": "1649050",
    "end": "1655450"
  },
  {
    "text": "inserts and update operations so these two things really set them apart from the LS entries and let's take a how the",
    "start": "1655450",
    "end": "1664080"
  },
  {
    "text": "well and at an anatomy of the B tree how they're built B trees consists of disk",
    "start": "1664080",
    "end": "1670510"
  },
  {
    "text": "segments usually called nodes or sometimes pages and we distinguish between several types of them and we say",
    "start": "1670510",
    "end": "1678280"
  },
  {
    "text": "there is a root the one that has no parents so meaning that it is not child",
    "start": "1678280",
    "end": "1684880"
  },
  {
    "text": "to any other node and internal nodes which connect the top layer to the bottom layer and the bottom layer which",
    "start": "1684880",
    "end": "1692020"
  },
  {
    "text": "stores holds the data and it's usually called leaf and B trees are",
    "start": "1692020",
    "end": "1700330"
  },
  {
    "text": "characterized by their first of all branching factor and a branching factor is an amount of child pointers like an",
    "start": "1700330",
    "end": "1707200"
  },
  {
    "text": "fan-out like how many child pointers that will be per every node in the tree and they also this also means that there",
    "start": "1707200",
    "end": "1715420"
  },
  {
    "text": "will be more pretty much as many keys as pointers and the B trees are also",
    "start": "1715420",
    "end": "1724240"
  },
  {
    "text": "characterized by their occupancy meaning that how many how much free space pretty",
    "start": "1724240",
    "end": "1729730"
  },
  {
    "text": "much the B tree node has or each node has they're also characterized by their",
    "start": "1729730",
    "end": "1736450"
  },
  {
    "text": "height and height is a amount of B three levels how",
    "start": "1736450",
    "end": "1743900"
  },
  {
    "text": "many pointers you have to follow between the levels in order to finish your look up process and height is a logarithm of",
    "start": "1743900",
    "end": "1751760"
  },
  {
    "text": "the amount of keys over the notes and this also explains you how many disk transfers you will have to perform in",
    "start": "1751760",
    "end": "1759770"
  },
  {
    "text": "order to finalize your read and every non-leaf node in the tree holds keys and",
    "start": "1759770",
    "end": "1765920"
  },
  {
    "text": "pointers to the subtrees leaf nodes may also hold a pointer to the previous and the next well note in order to have more",
    "start": "1765920",
    "end": "1775010"
  },
  {
    "text": "efficient navigation on the leaf level as you just seen on the previous slide",
    "start": "1775010",
    "end": "1782059"
  },
  {
    "text": "the node consists of sorted keys and pointers the keys are sometimes called separator keys and because they separate",
    "start": "1782059",
    "end": "1789410"
  },
  {
    "text": "the tree into the sub trees and the amount of pointers is always greater",
    "start": "1789410",
    "end": "1796520"
  },
  {
    "text": "than amount of keys by one so separator keys also can be thought as they were",
    "start": "1796520",
    "end": "1802040"
  },
  {
    "text": "splitting the range to like before the first ski between middle keys like any",
    "start": "1802040",
    "end": "1809960"
  },
  {
    "text": "pair of middle keys and after the last key one performing lookups you start",
    "start": "1809960",
    "end": "1816770"
  },
  {
    "text": "with the top node then follow recursively the pointers down to the",
    "start": "1816770",
    "end": "1823059"
  },
  {
    "text": "leaf level when the point queries performed the search is complete after",
    "start": "1823059",
    "end": "1828350"
  },
  {
    "text": "while you have located the leaf node in case of the range scan you have to",
    "start": "1828350",
    "end": "1833929"
  },
  {
    "text": "traverse the keys and values until you're well pretty much range scan has",
    "start": "1833929",
    "end": "1839600"
  },
  {
    "text": "complete in tourism of complexity B tree is guarantee logarithm of the base to",
    "start": "1839600",
    "end": "1847549"
  },
  {
    "text": "look up because finding a key is performed using the binary search so in",
    "start": "1847549",
    "end": "1852950"
  },
  {
    "text": "order to find a key within each node we still have to binary search so it would be incorrect to say",
    "start": "1852950",
    "end": "1860270"
  },
  {
    "text": "that we have a blogger rhythm of height of tree complexity here when performing",
    "start": "1860270",
    "end": "1867110"
  },
  {
    "text": "insertions we have to locate the target leaf for that we can use well the lookup",
    "start": "1867110",
    "end": "1872620"
  },
  {
    "text": "that we just discussed on the previous slide and after the target leaf is located and key and value appended to it",
    "start": "1872620",
    "end": "1879539"
  },
  {
    "text": "in case there is not enough space in the",
    "start": "1879539",
    "end": "1884679"
  },
  {
    "text": "in the node we have to well the operation the situation is called",
    "start": "1884679",
    "end": "1890380"
  },
  {
    "text": "overflow and the leaf has to be split in two parts this is done by allocating the",
    "start": "1890380",
    "end": "1897100"
  },
  {
    "text": "new leaf moving half of the elements to the new one and pulling the first key of the newly allocated node to its parent",
    "start": "1897100",
    "end": "1904630"
  },
  {
    "text": "and if the parent doesn't have the space either the split is performed on the",
    "start": "1904630",
    "end": "1909909"
  },
  {
    "text": "parent level and so on and so forth until we reach the root level this also",
    "start": "1909909",
    "end": "1916090"
  },
  {
    "text": "implies that the tree always grows from the root level so there is no other way",
    "start": "1916090",
    "end": "1921100"
  },
  {
    "text": "to change the root of the B tree except for by splitting all the nodes recursively up to the root level and",
    "start": "1921100",
    "end": "1927520"
  },
  {
    "text": "then splitting the root deletions are similar to insertions but they cause",
    "start": "1927520",
    "end": "1932669"
  },
  {
    "text": "node merges not splits and that's pretty",
    "start": "1932669",
    "end": "1939070"
  },
  {
    "text": "much it from the algorithmic perspective in order to summarize the B trees are",
    "start": "1939070",
    "end": "1944710"
  },
  {
    "text": "first and foremost for most mutable so they allow in-place updates but they",
    "start": "1944710",
    "end": "1950679"
  },
  {
    "text": "have to introduce some space overhead and in order to amortize the subsequent",
    "start": "1950679",
    "end": "1956110"
  },
  {
    "text": "rights for the data that sorts closely rights have to locate a point on disk",
    "start": "1956110",
    "end": "1962080"
  },
  {
    "text": "where the data has to be written and unlike unlike LSM trees well unlike LSM",
    "start": "1962080",
    "end": "1969159"
  },
  {
    "text": "trees where the rights are just going to memory and the disks it doesn't have to be performed at all be trees can also be",
    "start": "1969159",
    "end": "1976510"
  },
  {
    "text": "used in LSM storage in order as an index but then you have the B trees with 100%",
    "start": "1976510",
    "end": "1985179"
  },
  {
    "text": "occupancy and B trees are optimized for reads so they do not require reading",
    "start": "1985179",
    "end": "1991960"
  },
  {
    "text": "from and subsequently merging the data from multiple sources in order to satisfy the query rights and deletes",
    "start": "1991960",
    "end": "1998710"
  },
  {
    "text": "might trigger a cascade of splits and merges as we just talked on the last slide making some of the operations but",
    "start": "1998710",
    "end": "2005580"
  },
  {
    "text": "not all more expensive there are more techniques and implementations that people are",
    "start": "2005580",
    "end": "2012210"
  },
  {
    "text": "using in order to optimize it so there is much more to split balancing and merging to in the B tree subject but",
    "start": "2012210",
    "end": "2021690"
  },
  {
    "text": "it's outside of the scope of this talk they're optimized for paged environments",
    "start": "2021690",
    "end": "2026730"
  },
  {
    "text": "and help to manage this ordered data which is located in pages on this over",
    "start": "2026730",
    "end": "2032070"
  },
  {
    "text": "the time after a bunch of subsequent updates and deletes the between notes will get well fragmented this means that",
    "start": "2032070",
    "end": "2039690"
  },
  {
    "text": "there require also requires some maintenance and block rewrites but this cost is usually amortized or",
    "start": "2039690",
    "end": "2047159"
  },
  {
    "text": "absorbed by the fact that b3 implementations are using something called buffer pool which kind of also",
    "start": "2047159",
    "end": "2053460"
  },
  {
    "text": "helps to buffer the update operations before all before the data is actually",
    "start": "2053460",
    "end": "2059839"
  },
  {
    "text": "reaching the disk and lastly the concurrent access to B trees require reader/writer isolation and involves the",
    "start": "2059839",
    "end": "2067648"
  },
  {
    "text": "chains of locks and latches looks like we were on time we still have ten",
    "start": "2067649",
    "end": "2073230"
  },
  {
    "text": "minutes when developing a storage system we're always confronted with the same",
    "start": "2073230",
    "end": "2079108"
  },
  {
    "text": "challenges and have to consider the same factors making a decision about what to",
    "start": "2079109",
    "end": "2086878"
  },
  {
    "text": "optimize for is extremely difficult one can spend time and more time during the",
    "start": "2086879",
    "end": "2093570"
  },
  {
    "text": "write in order to lay out the data structure in more efficient way reserve extra space for in-place updates to",
    "start": "2093570",
    "end": "2100380"
  },
  {
    "text": "facilitate faster writes buffered data in memory in order to ensure sequential",
    "start": "2100380",
    "end": "2105990"
  },
  {
    "text": "access you can do well anything but you cannot do all of these things at the",
    "start": "2105990",
    "end": "2111359"
  },
  {
    "text": "same time an ideal storage system would have the lowest read cost and lowest",
    "start": "2111359",
    "end": "2117839"
  },
  {
    "text": "write cost and have no overhead in practice the data structures compromise",
    "start": "2117839",
    "end": "2122940"
  },
  {
    "text": "between multiple factors and it's important to understand this compromises",
    "start": "2122940",
    "end": "2129270"
  },
  {
    "text": "and the researchers from Hardware DB lab summarize the three key parameters of",
    "start": "2129270",
    "end": "2135599"
  },
  {
    "text": "the database systems are optimized for they say it's optimized for either reads updates or memory overhead",
    "start": "2135599",
    "end": "2143780"
  },
  {
    "text": "understanding which one of these three parameters is most important for your",
    "start": "2143780",
    "end": "2149640"
  },
  {
    "text": "use case can influence the your choice of data structures access methods and",
    "start": "2149640",
    "end": "2155730"
  },
  {
    "text": "even suitability of certain tool for certain workload as some algorithms are",
    "start": "2155730",
    "end": "2161550"
  },
  {
    "text": "tailored specifically having one use case in mind the ROM conjecture that our",
    "start": "2161550",
    "end": "2168930"
  },
  {
    "text": "DB lab folks came up with states that setting up an upper bound for the - of",
    "start": "2168930",
    "end": "2175050"
  },
  {
    "text": "dimension overheads also sets a lower bound for the third one in other words",
    "start": "2175050",
    "end": "2180780"
  },
  {
    "text": "the three parameters form a competing tree triangle and improvement on one",
    "start": "2180780",
    "end": "2186150"
  },
  {
    "text": "side which means compromises on the other - for in for example the B trees",
    "start": "2186150",
    "end": "2192240"
  },
  {
    "text": "are read optimized but they have to reserve empty space for updates right so",
    "start": "2192240",
    "end": "2199050"
  },
  {
    "text": "one against the other therefore resulting in the memory overhead and have higher right cost",
    "start": "2199050",
    "end": "2207320"
  },
  {
    "text": "because they have to locate the target",
    "start": "2207320",
    "end": "2213500"
  },
  {
    "text": "written or place where to write the note on disk during the right time the B tree is",
    "start": "2213500",
    "end": "2218910"
  },
  {
    "text": "optimized for read performance so index is laid out in the way that minimizes",
    "start": "2218910",
    "end": "2223950"
  },
  {
    "text": "disk accesses required to traverse the single index in order to locate the data this is achieved by well keeping the",
    "start": "2223950",
    "end": "2231900"
  },
  {
    "text": "index files mutable but B trees also imply write amplification resulting from",
    "start": "2231900",
    "end": "2237840"
  },
  {
    "text": "node splits and mergers and also subsequent updates of the sale of the",
    "start": "2237840",
    "end": "2243390"
  },
  {
    "text": "data which is written closely together in the same page but written during the",
    "start": "2243390",
    "end": "2249660"
  },
  {
    "text": "different periods in time in order to amortize the update costs between",
    "start": "2249660",
    "end": "2256080"
  },
  {
    "text": "reserved extra space on nodes on all at all the levels and that helps to avoid",
    "start": "2256080",
    "end": "2263070"
  },
  {
    "text": "to change the node size during each right otherwise you would have to resize it constantly and main tree maintain",
    "start": "2263070",
    "end": "2269970"
  },
  {
    "text": "tree in fixed sized pages which simplifies the code and and the layout in short b-trees trade",
    "start": "2269970",
    "end": "2277570"
  },
  {
    "text": "update and memory overhead for better write performance excuse me for better",
    "start": "2277570",
    "end": "2283270"
  },
  {
    "text": "read performance of course in order on the other hand Alice M trees have less",
    "start": "2283270",
    "end": "2288610"
  },
  {
    "text": "space overhead at a cost of read overhead coming from having to access",
    "start": "2288610",
    "end": "2293710"
  },
  {
    "text": "several files several tables on the disk LSM trees optimize for right performance",
    "start": "2293710",
    "end": "2300730"
  },
  {
    "text": "both updates and deletes do not require locating node on the disk or where you",
    "start": "2300730",
    "end": "2306490"
  },
  {
    "text": "have to write on the disk and guarantee sequential writes by buffering everything in memory and only then",
    "start": "2306490",
    "end": "2311860"
  },
  {
    "text": "writing it down this comes at a price higher cost of maintenance which is done",
    "start": "2311860",
    "end": "2320020"
  },
  {
    "text": "by doing like compaction that we just discussed in several several slides before reads are getting more expensive",
    "start": "2320020",
    "end": "2327160"
  },
  {
    "text": "as the data has to be read from multiple sources and merged and compaction is",
    "start": "2327160",
    "end": "2333190"
  },
  {
    "text": "helping us to mitigate this problem and reduce the amount of access tables at",
    "start": "2333190",
    "end": "2338860"
  },
  {
    "text": "the same time Alice M trees eliminate memory overhead by not reserving the",
    "start": "2338860",
    "end": "2344380"
  },
  {
    "text": "empty space and allowing well they can also allow the block compression but I",
    "start": "2344380",
    "end": "2350920"
  },
  {
    "text": "think that's also not super relevant so in short Alice M trees trade read",
    "start": "2350920",
    "end": "2358540"
  },
  {
    "text": "performance and maintenance for better write performance and lower memory overhead so once again the same triangle",
    "start": "2358540",
    "end": "2366940"
  },
  {
    "text": "all over again so there are data structures optimized for each desired",
    "start": "2366940",
    "end": "2373600"
  },
  {
    "text": "properties so you can use LS m trees or B trees but you cannot really put their",
    "start": "2373600",
    "end": "2382090"
  },
  {
    "text": "properties together and have a data structure which will be equally good for every single direction and every single",
    "start": "2382090",
    "end": "2388480"
  },
  {
    "text": "overhead out there there are well pretty much three to nobles that we have discussed today read update and memory",
    "start": "2388480",
    "end": "2394900"
  },
  {
    "text": "overheads and they can help you to evaluate the database and deeper understand the workloads that are best",
    "start": "2394900",
    "end": "2402910"
  },
  {
    "text": "suitable for them and or the other way around to pick the database which",
    "start": "2402910",
    "end": "2408220"
  },
  {
    "text": "is best for the workload that you are having at the moment and of course that there are many other factors to consider",
    "start": "2408220",
    "end": "2414700"
  },
  {
    "text": "in the storage system or especially in the database like there is an maintenance overhead operational",
    "start": "2414700",
    "end": "2421359"
  },
  {
    "text": "simplicity system requirements and so on and so forth but this rum conjecture and",
    "start": "2421359",
    "end": "2428440"
  },
  {
    "text": "understanding well at least the basic algorithms could give you a good rule of",
    "start": "2428440",
    "end": "2433840"
  },
  {
    "text": "thumb in order to develop an intuition and maybe know how to even test the",
    "start": "2433840",
    "end": "2440010"
  },
  {
    "text": "database for the workloads that you would like to test it for and many",
    "start": "2440010",
    "end": "2447160"
  },
  {
    "text": "things vary from implementation to implementation so even the two databases",
    "start": "2447160",
    "end": "2452760"
  },
  {
    "text": "with a similar storage engine which seemingly should implement even the same paper and have the same like design",
    "start": "2452760",
    "end": "2460390"
  },
  {
    "text": "principles may end up performing extremely differently database are complex systems with many moving parts",
    "start": "2460390",
    "end": "2468310"
  },
  {
    "text": "and unfortunately they are or maybe fortunately an integral part of many",
    "start": "2468310",
    "end": "2475540"
  },
  {
    "text": "applications and I hope that this information will help you to peek under",
    "start": "2475540",
    "end": "2481570"
  },
  {
    "text": "the hood of the database and knowing the difference between the underlying data",
    "start": "2481570",
    "end": "2487180"
  },
  {
    "text": "structures and this simple rule of thumb will help you at your work to decide",
    "start": "2487180",
    "end": "2494859"
  },
  {
    "text": "what's best for you and your application I would really advise to take a look at",
    "start": "2494859",
    "end": "2500980"
  },
  {
    "text": "least at these four books because they really help to get you some",
    "start": "2500980",
    "end": "2506230"
  },
  {
    "text": "understanding of basic data structures and what's going on in what was going on",
    "start": "2506230",
    "end": "2514000"
  },
  {
    "text": "in the database world over the last couple of decades for the so to say",
    "start": "2514000",
    "end": "2520869"
  },
  {
    "text": "quote-unquote newer stuff you can just follow me on Twitter and I'm trying to post the papers and soon hopefully maybe",
    "start": "2520869",
    "end": "2530560"
  },
  {
    "text": "sometime next year let's see about that there will be also a book coming out",
    "start": "2530560",
    "end": "2535839"
  },
  {
    "text": "that will help you to develop even further knowledge but generally even",
    "start": "2535839",
    "end": "2540880"
  },
  {
    "text": "like if you hunt for papers you can always hit me up on Twitter it was a pleasure",
    "start": "2540880",
    "end": "2546430"
  },
  {
    "text": "to be here thank you very much and yeah that was it thank you so much Alex do we",
    "start": "2546430",
    "end": "2557740"
  },
  {
    "text": "have any questions that was awesome so much information I need some water yes please",
    "start": "2557740",
    "end": "2567089"
  },
  {
    "text": "hi from the top of your head do you know any metrics like how faster beetrice",
    "start": "2567150",
    "end": "2574480"
  },
  {
    "text": "then binary trees in writing and and well from different perspective B trees",
    "start": "2574480",
    "end": "2581440"
  },
  {
    "text": "are not faster than binary trees and I would say because they're mostly used in absolutely different workloads so B",
    "start": "2581440",
    "end": "2587920"
  },
  {
    "text": "trees are mostly for like in memory or",
    "start": "2587920",
    "end": "2592980"
  },
  {
    "text": "excuse me for on these data structures and binary trees are mostly for the",
    "start": "2592980",
    "end": "2598180"
  },
  {
    "text": "in-memory stuff I would not be able to even so it's like they are so different that I wouldn't even be able to compare",
    "start": "2598180",
    "end": "2604690"
  },
  {
    "text": "them but comparing the LSM trees and B",
    "start": "2604690",
    "end": "2609880"
  },
  {
    "text": "trees I there are plenty of comparisons",
    "start": "2609880",
    "end": "2616090"
  },
  {
    "text": "and probably the most prominent of them that I can name is done by Wired tiger",
    "start": "2616090",
    "end": "2622420"
  },
  {
    "text": "folks this is a storage engine currently underneath MongoDB and wire tiger",
    "start": "2622420",
    "end": "2629110"
  },
  {
    "text": "actually has an implementation of both Ellison trees and B trees in it is solid",
    "start": "2629110",
    "end": "2634810"
  },
  {
    "text": "material really recommended to check out and they also have a big wiki page which",
    "start": "2634810",
    "end": "2640840"
  },
  {
    "text": "compares B trees versus Alice M trees for a bunch of different use cases",
    "start": "2640840",
    "end": "2647140"
  },
  {
    "text": "it's really awesome read very deep analysis so definitely recommended but",
    "start": "2647140",
    "end": "2652360"
  },
  {
    "text": "sorry I cannot really respond to B trees versus binary trees",
    "start": "2652360",
    "end": "2657359"
  },
  {
    "text": "so poke those data structures are created you know in the age where we",
    "start": "2662640",
    "end": "2669190"
  },
  {
    "text": "have only had hard drives and like with the disks and today when we've got you",
    "start": "2669190",
    "end": "2675550"
  },
  {
    "text": "know a SSD disk so is there like how they're applicable right now when random",
    "start": "2675550",
    "end": "2683020"
  },
  {
    "text": "access is not much faster or on SSD so I'm wondering is there is maybe a third",
    "start": "2683020",
    "end": "2689020"
  },
  {
    "text": "way there are there is a third way unfortunately I tried my best to jam it in so there is",
    "start": "2689020",
    "end": "2696130"
  },
  {
    "text": "also something called unordered LSM trees which is basically LSM tree which",
    "start": "2696130",
    "end": "2703000"
  },
  {
    "text": "allows you random access one of the most prominent implementations of such thing it's called bit cask it used to be in",
    "start": "2703000",
    "end": "2709620"
  },
  {
    "text": "used in react by bashful folks open source by bet365 just real year ago or",
    "start": "2709620",
    "end": "2718660"
  },
  {
    "text": "so or maybe it was always open source it just like they continued to keep it alive but yeah bit cask is definitely",
    "start": "2718660",
    "end": "2724900"
  },
  {
    "text": "one of the implementation what they do is basically they have this right ahead log and sequentially right all the data",
    "start": "2724900",
    "end": "2732910"
  },
  {
    "text": "in the log and the keys they keep them well in memory and well they have a hash",
    "start": "2732910",
    "end": "2739300"
  },
  {
    "text": "map that points to well pretty much the last point at in the log where the data",
    "start": "2739300",
    "end": "2748420"
  },
  {
    "text": "is located there is another theoretical or there is a paper so I haven't seen",
    "start": "2748420",
    "end": "2755950"
  },
  {
    "text": "the implementation the paper is called whisk key so I think it's Wisconsin University or something so whiskey",
    "start": "2755950",
    "end": "2763020"
  },
  {
    "text": "database it's a very nice paper from I think it was Sigma very recommended to",
    "start": "2763020",
    "end": "2768190"
  },
  {
    "text": "read as well so they're talking about pretty much the same concept meaning that you write the data into log only",
    "start": "2768190",
    "end": "2775120"
  },
  {
    "text": "right away so without any wealth thing in between but you still maintain the",
    "start": "2775120",
    "end": "2782520"
  },
  {
    "text": "LSM tree but only of the keys so effectively you're compaction costs drop",
    "start": "2782520",
    "end": "2789670"
  },
  {
    "text": "significantly at the expect now I kind of start seeing why everything I was saying it kind of",
    "start": "2789670",
    "end": "2795640"
  },
  {
    "text": "doesn't make sense now I try to put it back so I apologize so why this makes",
    "start": "2795640",
    "end": "2801400"
  },
  {
    "text": "sense for the doesn't didn't make sense previously and now it does because you",
    "start": "2801400",
    "end": "2806650"
  },
  {
    "text": "can search for keys very quickly and",
    "start": "2806650",
    "end": "2811660"
  },
  {
    "text": "like retrieve them in blocks and basically read them out and whisk key for instance even can utilize the SSD",
    "start": "2811660",
    "end": "2820110"
  },
  {
    "text": "parallelism so they can submit like several operations and then get the data",
    "start": "2820110",
    "end": "2825430"
  },
  {
    "text": "quicker like and they claim that they well pretty much can well be absolutely",
    "start": "2825430",
    "end": "2833080"
  },
  {
    "text": "optimized like data structure in terms of like reads and writes and space",
    "start": "2833080",
    "end": "2838180"
  },
  {
    "text": "overhead but with this slight pitfall is that when you're trying to do the range",
    "start": "2838180",
    "end": "2843550"
  },
  {
    "text": "scan you are still going to incur the cost of retrieving the data like for",
    "start": "2843550",
    "end": "2850840"
  },
  {
    "text": "instance you need to have like date like one record from the block over here one record from the block over there and it",
    "start": "2850840",
    "end": "2857470"
  },
  {
    "text": "doesn't matter how quickly you can read out those blocks because like it's not by the dress about storage so if we take",
    "start": "2857470",
    "end": "2864910"
  },
  {
    "text": "all of that and wait three years until something like nvm II like really gets",
    "start": "2864910",
    "end": "2871480"
  },
  {
    "text": "implemented and we have something like byte addressable storage without I mean there is nvme right now but there is no",
    "start": "2871480",
    "end": "2878320"
  },
  {
    "text": "like good way to really like not transfer like blocks I think that the",
    "start": "2878320",
    "end": "2884890"
  },
  {
    "text": "minimum addressable is should be something like a cache line or something",
    "start": "2884890",
    "end": "2889990"
  },
  {
    "text": "but not like entire block but until we get there I would say that we will have",
    "start": "2889990",
    "end": "2895720"
  },
  {
    "text": "to stick probably to one of the two which are proven to work and the third one is a good and great area of research",
    "start": "2895720",
    "end": "2902410"
  },
  {
    "text": "definitely a read up on that so a whiskey and bit cast butt and bit cask",
    "start": "2902410",
    "end": "2907960"
  },
  {
    "text": "is pretty solid but only for like hash map so it's like no range scans whiskey ads range scans but at the cost so hope",
    "start": "2907960",
    "end": "2916510"
  },
  {
    "text": "that's answers I think we have time for just one last quick question you had one no not anymore okay failing all right",
    "start": "2916510",
    "end": "2926410"
  },
  {
    "text": "well you can talk to Alex after the sessions over the next one is going to be at 4:10 p.m. on",
    "start": "2926410",
    "end": "2933789"
  },
  {
    "text": "natural language processing and building voice assistance thank you again Alex awesome thank you very much",
    "start": "2933789",
    "end": "2939650"
  },
  {
    "text": "[Applause]",
    "start": "2939650",
    "end": "2944319"
  }
]