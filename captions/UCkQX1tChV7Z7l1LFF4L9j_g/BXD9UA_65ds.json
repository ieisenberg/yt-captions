[
  {
    "start": "0",
    "end": "55000"
  },
  {
    "text": "foreign [Music]",
    "start": "1380",
    "end": "16580"
  },
  {
    "text": "I'm a partner director at Microsoft hi uh I'm vinod I'm a principal",
    "start": "16580",
    "end": "23640"
  },
  {
    "text": "architect at Azure Cosmos DB today's topic is azure Cosmos DB low",
    "start": "23640",
    "end": "29580"
  },
  {
    "text": "latency and high availability at Planet scale",
    "start": "29580",
    "end": "34520"
  },
  {
    "text": "this is today's agenda first of all we will start with introduction and overview then we know will take us Deep",
    "start": "36840",
    "end": "43559"
  },
  {
    "text": "dive into the storage engine I will be covering API Gateway we'll conclude this talk with learning and takeaway",
    "start": "43559",
    "end": "52039"
  },
  {
    "start": "55000",
    "end": "151000"
  },
  {
    "text": "first of all let's start with a little bit of a Azure Cosmos DB's history Dharma Shukla is a Microsoft technical",
    "start": "55760",
    "end": "63719"
  },
  {
    "text": "fellow in 2010 he had observed the many internal Microsoft applications is",
    "start": "63719",
    "end": "69600"
  },
  {
    "text": "trying to build highly scalable and globally distributed system uh he started a project he hoped to build a",
    "start": "69600",
    "end": "76860"
  },
  {
    "text": "service that is cloud native multi-tenant and share nothing from the",
    "start": "76860",
    "end": "82439"
  },
  {
    "text": "ground up in 2010 why dama Shukla is having a vacation in Florence his proof",
    "start": "82439",
    "end": "88740"
  },
  {
    "text": "of concept started to work guess what is as your customers DB's code name yes",
    "start": "88740",
    "end": "94020"
  },
  {
    "text": "project Florence the brother name was named as documentdb",
    "start": "94020",
    "end": "99840"
  },
  {
    "text": "as we started with document API by 2014 there are plenty assets of critical",
    "start": "99840",
    "end": "106259"
  },
  {
    "text": "Azure services and Microsoft developers using Cosmos DB with continuous",
    "start": "106259",
    "end": "111360"
  },
  {
    "text": "Evolution cast documentdb no longer describe the capability of the product",
    "start": "111360",
    "end": "116600"
  },
  {
    "text": "upon Public's offer in 2017 the product was renamed as Azure Cosmos BB to",
    "start": "116600",
    "end": "123899"
  },
  {
    "text": "reflect the capability more than just document API and also to uh better",
    "start": "123899",
    "end": "130920"
  },
  {
    "text": "describe the aspiration for a scalability Beyond a planet in the initial offering we only have",
    "start": "130920",
    "end": "137520"
  },
  {
    "text": "document apis such as document SQL and mango API by 2018 both grimnam API and",
    "start": "137520",
    "end": "146459"
  },
  {
    "text": "Cassandra API were introduced",
    "start": "146459",
    "end": "150680"
  },
  {
    "start": "151000",
    "end": "272000"
  },
  {
    "text": "managing a single tenant lead is hard it is usually not cost efficient to do",
    "start": "160140",
    "end": "166800"
  },
  {
    "text": "sharding any database is also a complex task what does Azure Cosmos DB offer",
    "start": "166800",
    "end": "173340"
  },
  {
    "text": "this set of features and characteristics are built into Azure customer DB to",
    "start": "173340",
    "end": "178920"
  },
  {
    "text": "support a journey from the box product to the cloud native",
    "start": "178920",
    "end": "184379"
  },
  {
    "text": "on the left hand side it is our core feature sets Azure customer DB is",
    "start": "184379",
    "end": "189420"
  },
  {
    "text": "Microsoft's schema free nosql offering document SQL API is azure customers DBS",
    "start": "189420",
    "end": "196200"
  },
  {
    "text": "native API our query engine supports Rich query semantics such as subquery",
    "start": "196200",
    "end": "203000"
  },
  {
    "text": "aggregation join and more what make that make it unique and also",
    "start": "203000",
    "end": "209040"
  },
  {
    "text": "complex we also support multi-apis um there is a huge base of existing",
    "start": "209040",
    "end": "216180"
  },
  {
    "text": " Cassandra developers and solutions providing this OSS API Smooths out the",
    "start": "216180",
    "end": "221640"
  },
  {
    "text": "migration to Azure customers DB we will see multi-api Choice impacts",
    "start": "221640",
    "end": "227099"
  },
  {
    "text": "throughout our system including storage engine query Gateway and more we have",
    "start": "227099",
    "end": "232500"
  },
  {
    "text": "tunable consistency and conflict resolution this Romantics implementation",
    "start": "232500",
    "end": "238200"
  },
  {
    "text": "will be covered in soil resection by the node for scalability and performance",
    "start": "238200",
    "end": "243720"
  },
  {
    "text": "features we have Geo distribution active active and electricity many of this will",
    "start": "243720",
    "end": "249959"
  },
  {
    "text": "be covered in window section on Enterprise front we are a ring zero Azure services that",
    "start": "249959",
    "end": "257100"
  },
  {
    "text": "means wherever there's a new Azure region built out is happening as your customer DB will be there encryption",
    "start": "257100",
    "end": "263639"
  },
  {
    "text": "Network isolation role-based access are all needed to support enterprise software",
    "start": "263639",
    "end": "271400"
  },
  {
    "text": "so how is your customer DB doing as your customer's DB is one of the fastest",
    "start": "273860",
    "end": "280620"
  },
  {
    "text": "growing Azure Services we have a hundred percent year-over-year growth in",
    "start": "280620",
    "end": "285900"
  },
  {
    "text": "transaction and Storage how big a customer can become uh I'm",
    "start": "285900",
    "end": "291540"
  },
  {
    "text": "just citing one of the single instance customer one of this instance customer is actually uh Support over 100 million",
    "start": "291540",
    "end": "299880"
  },
  {
    "text": "requests per second over petabytes of storage and this customer is globally",
    "start": "299880",
    "end": "306060"
  },
  {
    "text": "distributed in 41 regions 41 regions wow so who uses Azure Cosmos DB we power",
    "start": "306060",
    "end": "313080"
  },
  {
    "text": "many first party uh Partners from teams chat Xbox games LinkedIn",
    "start": "313080",
    "end": "321139"
  },
  {
    "text": "Azure active directory and more we also power many critical external workloads",
    "start": "321139",
    "end": "326340"
  },
  {
    "text": "such as Adobe Creative Cloud Walmart Geico Asus all of our apis are strong",
    "start": "326340",
    "end": "334199"
  },
  {
    "text": "customers we also observed many of our customers have many use several apis and",
    "start": "334199",
    "end": "341460"
  },
  {
    "text": "not just one simply because depends on your data modeling data set in your",
    "start": "341460",
    "end": "346620"
  },
  {
    "text": "query customer tend to pick the right set of API to better support that scenario",
    "start": "346620",
    "end": "353180"
  },
  {
    "start": "353000",
    "end": "427000"
  },
  {
    "text": "this diagram illustrate a high level as your customer's DB architecture",
    "start": "353940",
    "end": "359060"
  },
  {
    "text": "customers creates database account the data is stored in the storage unit",
    "start": "359060",
    "end": "364520"
  },
  {
    "text": "customers send various requests from simple crew it and query operations",
    "start": "364520",
    "end": "370759"
  },
  {
    "text": "what differentiates as your customers DB the most is actually our customer on the",
    "start": "370759",
    "end": "376199"
  },
  {
    "text": "left hand side you can see many of our customer comes but they also they can speak different languages",
    "start": "376199",
    "end": "382560"
  },
  {
    "text": "let's request a right to the right API Gateway requests will be translated and planned subrequest will be routed to",
    "start": "382560",
    "end": "390180"
  },
  {
    "text": "relevant back-end storage result will be processed in the Gateway if needed",
    "start": "390180",
    "end": "395460"
  },
  {
    "text": "before sending it back Azure Cosmos DB is designed with high performing multi-tenancy that means a VM can",
    "start": "395460",
    "end": "403620"
  },
  {
    "text": "contain any storage units and API Gateway process can support multiple customers requests in this talk you will",
    "start": "403620",
    "end": "411300"
  },
  {
    "text": "see multi API and high performing multi-tenancy plays a big part in our",
    "start": "411300",
    "end": "416880"
  },
  {
    "text": "system with that I will head out to be known to dive into storage engine did",
    "start": "416880",
    "end": "422280"
  },
  {
    "text": "not hear you though",
    "start": "422280",
    "end": "425000"
  },
  {
    "start": "427000",
    "end": "456000"
  },
  {
    "text": "so uh as we dive deep into the storage engine uh I wanted to talk about a",
    "start": "429860",
    "end": "435360"
  },
  {
    "text": "couple of themes you'll see in this talk in Cosmos DB We Believe highly in",
    "start": "435360",
    "end": "440699"
  },
  {
    "text": "redundancy if there's one of something you should assume that it fails if there's more than one assume that you",
    "start": "440699",
    "end": "447360"
  },
  {
    "text": "have to prepare to have them swap out at a moment's notice and we will need redundancy at every layer",
    "start": "447360",
    "end": "454879"
  },
  {
    "start": "456000",
    "end": "633000"
  },
  {
    "text": "before we dive deeper uh I wanted to quickly cover some of these simple Concepts in the cosmos DB service",
    "start": "456539",
    "end": "463740"
  },
  {
    "text": "Cosmos DB has a tiered resource hierarchy at a top level you have the database account which is how customers",
    "start": "463740",
    "end": "470039"
  },
  {
    "text": "interact with the database within an account you can have several",
    "start": "470039",
    "end": "475680"
  },
  {
    "text": "databases a database is just a logical grouping of containers",
    "start": "475680",
    "end": "480840"
  },
  {
    "text": "containers are sort of like SQL tables they hold user data and documents that",
    "start": "480840",
    "end": "487740"
  },
  {
    "text": "are stored in physical partitions that are then horizontally scaled",
    "start": "487740",
    "end": "493020"
  },
  {
    "text": "entities such as containers are provisioned with request units which determine the throughput that you can",
    "start": "493020",
    "end": "498780"
  },
  {
    "text": "derive from them so it's kind of like the currency that you use to interact with Cosmos DB",
    "start": "498780",
    "end": "504599"
  },
  {
    "text": "each operation in the container ends up consuming these request units and is restricted to be within the provision",
    "start": "504599",
    "end": "510720"
  },
  {
    "text": "value for the container within any one second every container also comes with a",
    "start": "510720",
    "end": "516899"
  },
  {
    "text": "logical partition key The Logical partition key is basically just a routing key that is used to Route",
    "start": "516899",
    "end": "523140"
  },
  {
    "text": "user data to a given physical partition where the data resides each logical partition key lives in one",
    "start": "523140",
    "end": "530640"
  },
  {
    "text": "physical partition but each physical partition stores many such logical partition keys and that's pretty much",
    "start": "530640",
    "end": "536459"
  },
  {
    "text": "how we end up getting the horizontal scale we talked about for instance consider a container that",
    "start": "536459",
    "end": "542220"
  },
  {
    "text": "has The Logical partition key City documents with the partition key value",
    "start": "542220",
    "end": "547320"
  },
  {
    "text": "Seattle or London may end up in Partition T partition one while Istanbul",
    "start": "547320",
    "end": "553019"
  },
  {
    "text": "ends up in you know the physical partition three we have several types of partitioning",
    "start": "553019",
    "end": "558779"
  },
  {
    "text": "schemes in Cosmos DB they can be a single field path or hierarchical or",
    "start": "558779",
    "end": "564060"
  },
  {
    "text": "composite where you can have multiple Keys contributing to the partitioning strategy",
    "start": "564060",
    "end": "570240"
  },
  {
    "text": "each partitioning scheme is optimized for its own use scenarios for instance when you think about crud operations a",
    "start": "570240",
    "end": "577920"
  },
  {
    "text": "single partition key may be good enough but when you consider complex systems",
    "start": "577920",
    "end": "583620"
  },
  {
    "text": "where you have data skew for instance where you have data skew but you still want to optimize for query usage",
    "start": "583620",
    "end": "589860"
  },
  {
    "text": "patterns hierarchical partition keys can still be useful for instance with Azure",
    "start": "589860",
    "end": "595980"
  },
  {
    "text": "active directory where you have log in tenants that are vastly different such as Microsoft or or adobe but you still",
    "start": "595980",
    "end": "604560"
  },
  {
    "text": "want horizontal scalability because a single tenant may want be larger than what a physical partition can handle but",
    "start": "604560",
    "end": "611519"
  },
  {
    "text": "you want data locality for these queries you can have hierarchical partition keys where you first Partition by the tenant",
    "start": "611519",
    "end": "617940"
  },
  {
    "text": "but also Downstream by an organization or team allowing you to benefit from the",
    "start": "617940",
    "end": "622980"
  },
  {
    "text": "horizontal scalability but also data locality for Fairly complex queries such as order buys or cross joins across the",
    "start": "622980",
    "end": "630000"
  },
  {
    "text": "different partitions the cosmos DB backend is a multi-tenant",
    "start": "630000",
    "end": "636000"
  },
  {
    "start": "633000",
    "end": "717000"
  },
  {
    "text": "share nothing distributed service each region is divided into multiple",
    "start": "636000",
    "end": "641220"
  },
  {
    "text": "logical fault tolerant groups of servers that we call clusters every cluster contains multiple fault",
    "start": "641220",
    "end": "648720"
  },
  {
    "text": "domain boundaries and servers within the cluster are spread across these fault domains to ensure that the cluster as a",
    "start": "648720",
    "end": "655200"
  },
  {
    "text": "whole can survive any failures in a subset of these politomies within each cluster customers physical",
    "start": "655200",
    "end": "662100"
  },
  {
    "text": "partitions are spread across the different servers across all the different fault domains a single partition for a given user",
    "start": "662100",
    "end": "669360"
  },
  {
    "text": "comprises of four replicas that each hold copies of the entire data for that",
    "start": "669360",
    "end": "674760"
  },
  {
    "text": "partition so we have four copies of your data for every partition",
    "start": "674760",
    "end": "679800"
  },
  {
    "text": "different partitions are even within a container are spread across various",
    "start": "679800",
    "end": "684959"
  },
  {
    "text": "machines in a cluster and across partitions we divide them across multiple clusters allowing us to get",
    "start": "684959",
    "end": "691260"
  },
  {
    "text": "elastic and horizontal scale up since we're multi-tenant every server",
    "start": "691260",
    "end": "696540"
  },
  {
    "text": "hosts data from multiple accounts by doing this we ensure that we minimize the cost and get better density and",
    "start": "696540",
    "end": "703380"
  },
  {
    "text": "hacking of customer data per server but to do this we kind of need strong",
    "start": "703380",
    "end": "708720"
  },
  {
    "text": "resource governance which we'll talk about later in this talk now let's take a look at some of these",
    "start": "708720",
    "end": "714600"
  },
  {
    "text": "replicas and partitions in detail each replica of a partition has a",
    "start": "714600",
    "end": "720720"
  },
  {
    "text": "document store which is written to a b tree and inverted index which is an optimized",
    "start": "720720",
    "end": "726480"
  },
  {
    "text": "B plus tree holding index terms used for queries and a replication Cube which logs writes as they happen and is used",
    "start": "726480",
    "end": "734040"
  },
  {
    "text": "to build and catch up slow replicas simple reads and writes are just served",
    "start": "734040",
    "end": "739740"
  },
  {
    "text": "from the document store we can always just look it up by ID and basically replace or read the value",
    "start": "739740",
    "end": "745200"
  },
  {
    "text": "however queries that involve more than simple reads such as order buys or group",
    "start": "745200",
    "end": "750420"
  },
  {
    "text": "buys or Aggregates or even sub queries on any fields are processed in conjunction with the B tree and the",
    "start": "750420",
    "end": "756180"
  },
  {
    "text": "inverted index given that you can index paths or wild cards or just say index everything we",
    "start": "756180",
    "end": "763139"
  },
  {
    "text": "end up building these indexes at runtime as you insert these documents",
    "start": "763139",
    "end": "768240"
  },
  {
    "text": "there is a brilliant paper some of our colleagues wrote on Steam agnostic indexing and how we make it work that is",
    "start": "768240",
    "end": "775200"
  },
  {
    "text": "linked here and honestly that's a talk all by itself now looking at a specific single right",
    "start": "775200",
    "end": "782940"
  },
  {
    "start": "780000",
    "end": "846000"
  },
  {
    "text": "if an issue if a user issues the right request to a given partition the right",
    "start": "782940",
    "end": "788040"
  },
  {
    "text": "is first directed to the primary replica of that partition the primary replica first prepares a",
    "start": "788040",
    "end": "794459"
  },
  {
    "text": "consistent snapshot of the message and dispatches that request to all of the secondaries concurrently",
    "start": "794459",
    "end": "801839"
  },
  {
    "text": "each of the secondaries then commits that prepared message and updates its local state",
    "start": "801839",
    "end": "806940"
  },
  {
    "text": "which involves updating the B tree updating the inverted index and the replication queue",
    "start": "806940",
    "end": "813000"
  },
  {
    "text": "the primary then waits for a quorum of secondaries to respond confirming that they have acknowledged and committed the",
    "start": "813000",
    "end": "819120"
  },
  {
    "text": "right and this means that they've written it fully to disk once a quorum of secondaries act back",
    "start": "819120",
    "end": "825779"
  },
  {
    "text": "the primary then commits the right locally it updates the replication queue its local B tree and the inverted index",
    "start": "825779",
    "end": "832079"
  },
  {
    "text": "before acknowledging the right to the client if there's any replicas that didn't",
    "start": "832079",
    "end": "837180"
  },
  {
    "text": "respond to the Quorum right then the replication queue is used to catch them up in this case the middle replica that",
    "start": "837180",
    "end": "843720"
  },
  {
    "text": "didn't act back that works great if you're in a single region but once again if you have a",
    "start": "843720",
    "end": "850800"
  },
  {
    "start": "846000",
    "end": "925000"
  },
  {
    "text": "single region you only have one of something and we like redundancy at Cosmos DB so we do offer having multiple",
    "start": "850800",
    "end": "857459"
  },
  {
    "text": "regions with multi-homing in the scenario where you have multiple regions a client in region a will first write to",
    "start": "857459",
    "end": "864540"
  },
  {
    "text": "the primary replica in region a the primary then follows the same sequence as before and commits to its",
    "start": "864540",
    "end": "871260"
  },
  {
    "text": "own secondaries one of the secondaries is then elected to be a cross-region replication primary",
    "start": "871260",
    "end": "877740"
  },
  {
    "text": "and replicates the right to region B's primary replica this then follows the same channel as",
    "start": "877740",
    "end": "884579"
  },
  {
    "text": "before and region B's primary commits it to its own secondaries and rights are transparently propagated across various",
    "start": "884579",
    "end": "891480"
  },
  {
    "text": "regions conversely if a user in region B writes to the primary in region B then the",
    "start": "891480",
    "end": "897959"
  },
  {
    "text": "inverse flow happens so first the right is first committed to its primary and secondaries in region B",
    "start": "897959",
    "end": "904079"
  },
  {
    "text": "and one of the secondaries there is elected as the cross region replication primary and rise to region a",
    "start": "904079",
    "end": "911519"
  },
  {
    "text": "which then follows the same sequence as before ensuring that rights are made available transparently across all the",
    "start": "911519",
    "end": "917519"
  },
  {
    "text": "different regions now as both people are writing to both regions one of the things that can",
    "start": "917519",
    "end": "923220"
  },
  {
    "text": "happen is conflicts Cosmos DB has several tunable conflict",
    "start": "923220",
    "end": "928440"
  },
  {
    "start": "925000",
    "end": "969000"
  },
  {
    "text": "resolution policies to help resolve these conflicts the user can configure for instance last Rider wins on the",
    "start": "928440",
    "end": "935100"
  },
  {
    "text": "timestamp of the document or they can provide a custom field that defines how",
    "start": "935100",
    "end": "940199"
  },
  {
    "text": "to resolve conflicts as some path within the document the users can also configure a custom",
    "start": "940199",
    "end": "946139"
  },
  {
    "text": "stored procedure that allows them to customize exactly how config resolution happens or they can have a manual feed",
    "start": "946139",
    "end": "952980"
  },
  {
    "text": "and say I want to resolve these offline additionally there's also API specific policies such as Cassandra which applies",
    "start": "952980",
    "end": "960360"
  },
  {
    "text": "last Rider wins on a timestamp but at a property level all of these are configurable at an account or a",
    "start": "960360",
    "end": "966959"
  },
  {
    "text": "collection basis now when it comes to reads Cosmos DB has",
    "start": "966959",
    "end": "972600"
  },
  {
    "start": "969000",
    "end": "1049000"
  },
  {
    "text": "a set of five well-defined tunable consistencies that have been derived based off of Industry research and user",
    "start": "972600",
    "end": "979079"
  },
  {
    "text": "studies these consistency levels are overrideable on individual reads and",
    "start": "979079",
    "end": "984300"
  },
  {
    "text": "allow for trading off between consistency latency availability and throughput",
    "start": "984300",
    "end": "989339"
  },
  {
    "text": "on one side you have strong which gives you a globally consistent snapshot of",
    "start": "989339",
    "end": "994860"
  },
  {
    "text": "your data next to that we have a regionally consistent consistency called bounded",
    "start": "994860",
    "end": "1000980"
  },
  {
    "text": "stainless which about half of our requests today use in this mode users can also configure an upper bound on the",
    "start": "1000980",
    "end": "1007880"
  },
  {
    "text": "staleness ensuring that reads don't lag behind rights by more than n seconds or k writes across various regions",
    "start": "1007880",
    "end": "1015259"
  },
  {
    "text": "the next consistency we have is session which provides read your own guarantees read your own right guarantees within",
    "start": "1015259",
    "end": "1022519"
  },
  {
    "text": "the client session this gives users the best compromise between consistency and throughput",
    "start": "1022519",
    "end": "1030740"
  },
  {
    "text": "finally we have consistent prefix and eventual which gives you the best latency and throughput but the least",
    "start": "1030740",
    "end": "1036798"
  },
  {
    "text": "guarantees on consistency of course within a single region the right Behavior does not change with the",
    "start": "1036799",
    "end": "1043160"
  },
  {
    "text": "consistency model rights are always durably committed to a quorum of replicas",
    "start": "1043160",
    "end": "1049280"
  },
  {
    "start": "1049000",
    "end": "1073000"
  },
  {
    "text": "now let's look at how some of these are achieved in detail when a client issues a read that is eventual or session these",
    "start": "1049280",
    "end": "1057500"
  },
  {
    "text": "reads are randomly routed to any secondary replica across different reads because of this",
    "start": "1057500",
    "end": "1064280"
  },
  {
    "text": "random selection reads are typically distributed across all the secondaries",
    "start": "1064280",
    "end": "1069919"
  },
  {
    "text": "in the case of a bounded staleness or a strong read the reads are pushed to two random secondaries and we also ensure",
    "start": "1069919",
    "end": "1077120"
  },
  {
    "text": "that Quorum is met across the two different secondaries so given different request types uh but",
    "start": "1077120",
    "end": "1084260"
  },
  {
    "text": "be it rights that go to the primary or reads that go to the secondaries we basically load balance our workloads",
    "start": "1084260",
    "end": "1090679"
  },
  {
    "text": "across all the replicas in our partition making sure that we get better use of the replicas and provision throughput",
    "start": "1090679",
    "end": "1098440"
  },
  {
    "start": "1098000",
    "end": "1148000"
  },
  {
    "text": "coming to availability across various components we continually strive to",
    "start": "1098440",
    "end": "1103820"
  },
  {
    "text": "ensure that we have high availability in all of our backend partitions one of the things we do is load",
    "start": "1103820",
    "end": "1109640"
  },
  {
    "text": "balancing we constantly Monitor and rebalance our replicas and partitions",
    "start": "1109640",
    "end": "1115700"
  },
  {
    "text": "both within and across our clusters to ensure that any one server isn't overloaded as customer workload patterns",
    "start": "1115700",
    "end": "1122720"
  },
  {
    "text": "change over time this load balancing is done on various metrics such as CPU or disk usage or so",
    "start": "1122720",
    "end": "1129620"
  },
  {
    "text": "on additionally things always happen like are machines go down there's upgrades uh the power",
    "start": "1129620",
    "end": "1137419"
  },
  {
    "text": "may go out on part of our data center and we need to react to this if a secondary replica goes down because",
    "start": "1137419",
    "end": "1144140"
  },
  {
    "text": "of any of these things due to maintenance or upgrades or so on new secondaries are automatically",
    "start": "1144140",
    "end": "1150440"
  },
  {
    "start": "1148000",
    "end": "1195000"
  },
  {
    "text": "selected from the remaining servers and rebuilt from the current primary using the existing B tree and replication",
    "start": "1150440",
    "end": "1156799"
  },
  {
    "text": "queue if a primary goes down a new primary is",
    "start": "1156799",
    "end": "1162559"
  },
  {
    "text": "automatically elected from the remaining replicas using paxos",
    "start": "1162559",
    "end": "1168260"
  },
  {
    "text": "this new secondary is chosen to is guaranteed to be chosen so that the latest acknowledged right is available",
    "start": "1168260",
    "end": "1176000"
  },
  {
    "text": "by doing this we ensure that we have high availability within a partition as long as there's enough machines",
    "start": "1176000",
    "end": "1182539"
  },
  {
    "text": "around even as the system evolves and changes",
    "start": "1182539",
    "end": "1187820"
  },
  {
    "text": "finally we have strong admission control and resource governance",
    "start": "1187820",
    "end": "1193160"
  },
  {
    "text": "requests are admission controlled on Entry before any processing occurs based",
    "start": "1193160",
    "end": "1198740"
  },
  {
    "start": "1195000",
    "end": "1272000"
  },
  {
    "text": "on their throughput requirements to ensure that any one customer doesn't overload the machines and there's Fair",
    "start": "1198740",
    "end": "1204320"
  },
  {
    "text": "usage based on throughput guarantees with simpler crowd requests this is relatively easy to do",
    "start": "1204320",
    "end": "1210559"
  },
  {
    "text": "basically when you have a right I know the size of your right and I can figure out the throughput requirements for",
    "start": "1210559",
    "end": "1216140"
  },
  {
    "text": "reads similarly once I look up the document I know how much throughput you're using however when you get to a complex query",
    "start": "1216140",
    "end": "1223700"
  },
  {
    "text": "that has like a join or a group or you're deep in the middle of an index or an order by resource governance gets",
    "start": "1223700",
    "end": "1229940"
  },
  {
    "text": "harder here we have checkpoints in the query VM runtime where we report incremental",
    "start": "1229940",
    "end": "1236419"
  },
  {
    "text": "progress of throughput and we yield execution if you exhaust your budget by",
    "start": "1236419",
    "end": "1242059"
  },
  {
    "text": "doing this we ensure that customers get the ability to make forward progress as they work through a query but also",
    "start": "1242059",
    "end": "1248960"
  },
  {
    "text": "respect resource governance and the bounds of a given replica or partition if however the requests do exceed the",
    "start": "1248960",
    "end": "1256700"
  },
  {
    "text": "budget allocated say you have a large query and there's not enough budget remaining within that replica to handle",
    "start": "1256700",
    "end": "1262640"
  },
  {
    "text": "that query they do get throttled on admission with a delay in the future that allows the partition to operate",
    "start": "1262640",
    "end": "1269299"
  },
  {
    "text": "under safe limits Beyond availability costs",
    "start": "1269299",
    "end": "1274880"
  },
  {
    "start": "1272000",
    "end": "1304000"
  },
  {
    "text": "change over time traffic can be variable with follow the Sun or batch workloads or spiky",
    "start": "1274880",
    "end": "1281539"
  },
  {
    "text": "workloads or analytical and so on their stored data can also change over time",
    "start": "1281539",
    "end": "1287780"
  },
  {
    "text": "Cosmos DB handles this by offering various strategies to help with the elasticity of the service when data",
    "start": "1287780",
    "end": "1293659"
  },
  {
    "text": "needs change first can leverage various strategies like autoscale throughput sharing",
    "start": "1293659",
    "end": "1299360"
  },
  {
    "text": "serverless or elastic scale up for their containers the model on the left shows a simple",
    "start": "1299360",
    "end": "1306260"
  },
  {
    "text": "provision throughput container the container has a fixed guaranteed throughput of 30 000 Rus that is spread",
    "start": "1306260",
    "end": "1313400"
  },
  {
    "text": "evenly across all the partitions the container will always guarantee that",
    "start": "1313400",
    "end": "1319400"
  },
  {
    "text": "thirty thousand request units worth of work is available at any one second and this is divided evenly across all of",
    "start": "1319400",
    "end": "1326780"
  },
  {
    "text": "the different physical partitions this is useful for scenarios where the workload is predictable or constant or",
    "start": "1326780",
    "end": "1333140"
  },
  {
    "text": "well-known apri on the right we have an auto scale container customers provide a range of",
    "start": "1333140",
    "end": "1339620"
  },
  {
    "text": "throughput for the container and each partition gets an equivalent proportion of it",
    "start": "1339620",
    "end": "1344900"
  },
  {
    "text": "in this case the workloads can scale elastically and instantaneously between",
    "start": "1344900",
    "end": "1350419"
  },
  {
    "text": "the minimum and maximum throughput specified but the customer is only Built for the maximum actual throughput",
    "start": "1350419",
    "end": "1356840"
  },
  {
    "text": "consumed which is great for a spiky or bursty workloads now to do this we",
    "start": "1356840",
    "end": "1363740"
  },
  {
    "text": "we rely heavily on our load balancing mechanisms we discussed before because we need to constantly monitor these",
    "start": "1363740",
    "end": "1370820"
  },
  {
    "text": "servers and ensure that any replicas and partitions that get too hot get moved",
    "start": "1370820",
    "end": "1376039"
  },
  {
    "text": "out if any one server gets oversubscribed",
    "start": "1376039",
    "end": "1380799"
  },
  {
    "start": "1381000",
    "end": "1389000"
  },
  {
    "text": "an additional case is if you need to grow your throughput or storage needs beyond what one physical server can",
    "start": "1381799",
    "end": "1388159"
  },
  {
    "text": "handle say a user wants to scale their throughput from 30 to 60 000 request",
    "start": "1388159",
    "end": "1393799"
  },
  {
    "start": "1389000",
    "end": "1401000"
  },
  {
    "text": "units this may require a scale out operation if it does under the cover the first",
    "start": "1393799",
    "end": "1400340"
  },
  {
    "text": "thing we do is allocate new partitions that can handle the desired throughput",
    "start": "1400340",
    "end": "1405380"
  },
  {
    "text": "we then detect the optimal split Point based on consumption or storage on the existing partitions and then migrate the",
    "start": "1405380",
    "end": "1412940"
  },
  {
    "text": "data from the old to the new based on the split point with no downtime to the user",
    "start": "1412940",
    "end": "1418220"
  },
  {
    "text": "new rights arriving to the partition are also copied over so the user doesn't see any impact from this operation",
    "start": "1418220",
    "end": "1424840"
  },
  {
    "text": "finally when the data is caught up there is an atomic swap between the old and",
    "start": "1424840",
    "end": "1430039"
  },
  {
    "text": "new partitions and the newly scaled partitions now take the user traffic note that these are all independent",
    "start": "1430039",
    "end": "1435980"
  },
  {
    "text": "operations so each partition once it's caught up will automatically transition atomically to the new partitions",
    "start": "1435980",
    "end": "1444620"
  },
  {
    "text": "similarly let's say that we detect that your storage is growing beyond what a",
    "start": "1444620",
    "end": "1449780"
  },
  {
    "text": "single partition can handle and it may be better to split we do partition this the same way as a",
    "start": "1449780",
    "end": "1456020"
  },
  {
    "text": "throughput split where we create two child partitions detect a split point and migrate the data to the new child",
    "start": "1456020",
    "end": "1461600"
  },
  {
    "text": "partitions which can elastically grow horizontally we did recently add the ability to merge",
    "start": "1461600",
    "end": "1468380"
  },
  {
    "text": "partitions back as you reduce the throughput means while we didn't need to do this earlier",
    "start": "1468380",
    "end": "1474140"
  },
  {
    "text": "we found out that this became important especially for scenarios around query where data locality is super important",
    "start": "1474140",
    "end": "1481159"
  },
  {
    "text": "and Fanning out the fewer partitions yielded better latencies and performance for the End customer",
    "start": "1481159",
    "end": "1487520"
  },
  {
    "text": "in this case we once again follow the same flow where we provisioned a new Partition copy the data inbound and then",
    "start": "1487520",
    "end": "1494000"
  },
  {
    "text": "do the swap finally Cosmos DB is a service that",
    "start": "1494000",
    "end": "1500360"
  },
  {
    "start": "1497000",
    "end": "1598000"
  },
  {
    "text": "supports multiple apis from SQL Gremlin Cassandra and tables",
    "start": "1500360",
    "end": "1506480"
  },
  {
    "text": "for all of these apis we provide the same availability reliability and elasticity guarantees while ensuring",
    "start": "1506480",
    "end": "1513860"
  },
  {
    "text": "that API semantics are maintained consequently we leverage a common shared",
    "start": "1513860",
    "end": "1519860"
  },
  {
    "text": "infrastructure in the storage engine across all the apis the replication stack the B tree and the storage layer",
    "start": "1519860",
    "end": "1527120"
  },
  {
    "text": "the index and elasticity are all unified this allows us to optimize these",
    "start": "1527120",
    "end": "1532820"
  },
  {
    "text": "scenarios across all the apis and ensure the features in the space benefit all apis equally we can see this because our",
    "start": "1532820",
    "end": "1540380"
  },
  {
    "text": "active active solution is available in Cassandra and SQL and even where we have an active active API",
    "start": "1540380",
    "end": "1548120"
  },
  {
    "text": "however each API can have its own requirements in terms of functionality and in these cases we have extensibility",
    "start": "1548120",
    "end": "1556039"
  },
  {
    "text": "points in the storage engine to support them for instance with patch semantics or",
    "start": "1556039",
    "end": "1561320"
  },
  {
    "text": "index term and query runtime semantics where type coercions and type comparisons and such can vary vastly",
    "start": "1561320",
    "end": "1567919"
  },
  {
    "text": "across apis or even conflict resolution Behavior where resolving conflicts can have API",
    "start": "1567919",
    "end": "1573860"
  },
  {
    "text": "specific behavior like we saw in Cassandra by carefully orchestrating where and",
    "start": "1573860",
    "end": "1580279"
  },
  {
    "text": "when these extensibility points happen we can ensure that we give users the flexibility of the API surface while",
    "start": "1580279",
    "end": "1586460"
  },
  {
    "text": "still guaranteeing availability consistency and elasticity",
    "start": "1586460",
    "end": "1591679"
  },
  {
    "text": "I'll now hand off to make Chen who will talk about the API Gateway",
    "start": "1591679",
    "end": "1597200"
  },
  {
    "text": "well we know that's a very cool database engine but I always joke about it you",
    "start": "1597200",
    "end": "1602419"
  },
  {
    "start": "1598000",
    "end": "1708000"
  },
  {
    "text": "know got an easy problem he got to charge our customer and API Gateway does not",
    "start": "1602419",
    "end": "1607520"
  },
  {
    "text": "so now you have seen this diagram earlier in a talk we are zooming a little bit into the middle box as you",
    "start": "1607520",
    "end": "1614480"
  },
  {
    "text": "can see our API gateways actually is a formed a fleet of microservices",
    "start": "1614480",
    "end": "1621220"
  },
  {
    "text": "each micro Services is specialized in one API and for each API there are many",
    "start": "1621220",
    "end": "1627620"
  },
  {
    "text": "many microservices what is our API Gateway problem space",
    "start": "1627620",
    "end": "1635240"
  },
  {
    "text": "unique our API Gateway is designed for multi API we repeat that many times in",
    "start": "1635240",
    "end": "1642440"
  },
  {
    "text": "this talk already today we support documents equal encode API",
    "start": "1642440",
    "end": "1647539"
  },
  {
    "text": "Cassandra API and grimlim API each API has its own semantics and protocols I",
    "start": "1647539",
    "end": "1653539"
  },
  {
    "text": "will get where you need to be able to understand various protocol and also implement the correct semantics",
    "start": "1653539",
    "end": "1659059"
  },
  {
    "text": "uh just picture key Value Store uh you probably can picture a simple query and request go over the wallet and think",
    "start": "1659059",
    "end": "1666080"
  },
  {
    "text": "about graph graphs usually tend to deal with Traverse and a supernode is a very",
    "start": "1666080",
    "end": "1672559"
  },
  {
    "text": "computation heavy request an hour get await is going up for",
    "start": "1672559",
    "end": "1677900"
  },
  {
    "text": "multi-tenancy know that we do not charge our customer for this giveaway services",
    "start": "1677900",
    "end": "1683000"
  },
  {
    "text": "everything he has clocks for us unlike the backend storage and CPU we",
    "start": "1683000",
    "end": "1688279"
  },
  {
    "text": "delegate to the customer as an idea so this giveaway needs to be performing and fair uh reducing the impact of Noisy",
    "start": "1688279",
    "end": "1695480"
  },
  {
    "text": "Neighbor and also contain the blast radius this giveaway needs to be highly",
    "start": "1695480",
    "end": "1701059"
  },
  {
    "text": "available because that's one of the Azure customers DBS Navi prop",
    "start": "1701059",
    "end": "1707080"
  },
  {
    "text": "foreign design choices",
    "start": "1707779",
    "end": "1714860"
  },
  {
    "start": "1708000",
    "end": "1790000"
  },
  {
    "text": "first our Gateway Federation is separate from our backend Federation that give us",
    "start": "1714860",
    "end": "1721159"
  },
  {
    "text": "the flexibility in scaling independently from the back end for growing or shrinking second to make multi-api",
    "start": "1721159",
    "end": "1728600"
  },
  {
    "text": "implementation effective we abstract our platform host from API specific",
    "start": "1728600",
    "end": "1733820"
  },
  {
    "text": "interrupt this enables us to quickly stand up another protocol or scenario if we",
    "start": "1733820",
    "end": "1739400"
  },
  {
    "text": "desire to and also allow us to optimize the platform layer once and can benefit",
    "start": "1739400",
    "end": "1744500"
  },
  {
    "text": "on all scenarios perform there including how we talk to the back end knowing the",
    "start": "1744500",
    "end": "1750080"
  },
  {
    "text": "petitions and also talking about like memory management we need to be efficient use the nodes to",
    "start": "1750080",
    "end": "1757340"
  },
  {
    "text": "reduce the cost again this is a free services however we also need to balance between maximum use of a node and yet",
    "start": "1757340",
    "end": "1765320"
  },
  {
    "text": "maintain High availability epic traffic a lot of heuristic and tuning has been",
    "start": "1765320",
    "end": "1771080"
  },
  {
    "text": "put in we implemented resource governance to reduce loss radius and to",
    "start": "1771080",
    "end": "1776539"
  },
  {
    "text": "contain Noisy Neighbor our platform is deeply integrated with underlining",
    "start": "1776539",
    "end": "1781940"
  },
  {
    "text": "Hardware such as pneumonoul boundary consideration and core of monetization",
    "start": "1781940",
    "end": "1789398"
  },
  {
    "text": "this diagram illustrates how we leverage a VM our process is our small and big",
    "start": "1790899",
    "end": "1797899"
  },
  {
    "text": "sized as you can tell from this diagram with a process per API we can have",
    "start": "1797899",
    "end": "1803840"
  },
  {
    "text": "better working set and localized locality such as caching we have",
    "start": "1803840",
    "end": "1809659"
  },
  {
    "text": "multiple processes per API on a VM understanding the hardware that you",
    "start": "1809659",
    "end": "1815240"
  },
  {
    "text": "deploying on this is very important we never spend our process across numer",
    "start": "1815240",
    "end": "1820399"
  },
  {
    "text": "node pneuma stands for non-uniform memory access when a process",
    "start": "1820399",
    "end": "1825740"
  },
  {
    "text": "is across Newman nodes memory access latency can increase if your cache",
    "start": "1825740",
    "end": "1830960"
  },
  {
    "text": "misses by by not crossing the Newman node make our performance a lot more",
    "start": "1830960",
    "end": "1836120"
  },
  {
    "text": "predictable we also authenitize each Gateway process to a small set of unique",
    "start": "1836120",
    "end": "1842419"
  },
  {
    "text": "CPU cores this will allow us be free from Os from switching us between",
    "start": "1842419",
    "end": "1847760"
  },
  {
    "text": "process shared by sharing the course and also between the API Gateway process",
    "start": "1847760",
    "end": "1853640"
  },
  {
    "text": "they won't compete second understand the language and framework like it depends",
    "start": "1853640",
    "end": "1859159"
  },
  {
    "text": "on our API Gateway is implemented in the net core we too manage process for",
    "start": "1859159",
    "end": "1865220"
  },
  {
    "text": "performance we put in consideration for potential latency such as GC",
    "start": "1865220",
    "end": "1870380"
  },
  {
    "text": "configuration third poor heuristic we also have been leveraging low allocation",
    "start": "1870380",
    "end": "1875539"
  },
  {
    "text": "API and buffer pooling technique to reduce the GC overhead we are very",
    "start": "1875539",
    "end": "1881299"
  },
  {
    "text": "fortunate that we work closely with download team we are done an early adopter will we provide performance and",
    "start": "1881299",
    "end": "1888559"
  },
  {
    "text": "quality feedback to Danity",
    "start": "1888559",
    "end": "1892778"
  },
  {
    "start": "1893000",
    "end": "1970000"
  },
  {
    "text": "load balancing is a difficult problem if you want to do it well you already seem to be no mentioned about load balancing",
    "start": "1894679",
    "end": "1900919"
  },
  {
    "text": "you know back end same thing is happening in the front end with our API Gateway design low balancing happens in",
    "start": "1900919",
    "end": "1907640"
  },
  {
    "text": "all different levels within a VM there are connection rerouting to balance between process of the same API",
    "start": "1907640",
    "end": "1915440"
  },
  {
    "text": "between VMS account load balancing can happen uh by monitoring system if a VM is getting",
    "start": "1915440",
    "end": "1923059"
  },
  {
    "text": "too hot in CPU or memory account load balancing can happen within a cluster or",
    "start": "1923059",
    "end": "1929240"
  },
  {
    "text": "between classes uh this app has the low balancing because it low balance when a VM may be",
    "start": "1929240",
    "end": "1936440"
  },
  {
    "text": "out of comfort zone what we are working on is active load balancing where we",
    "start": "1936440",
    "end": "1941659"
  },
  {
    "text": "load balance about request arrival we are building a layer 7 customized load",
    "start": "1941659",
    "end": "1947000"
  },
  {
    "text": "balancer this is a work in progress it will require you will leverage VM Health",
    "start": "1947000",
    "end": "1952100"
  },
  {
    "text": "not just vm's liveness it won't have capability to leverage historical",
    "start": "1952100",
    "end": "1957260"
  },
  {
    "text": "Insight in ml for future prediction all these low balancing is done in a",
    "start": "1957260",
    "end": "1963919"
  },
  {
    "text": "non-observable way to a customer and maintain the high availability",
    "start": "1963919",
    "end": "1969700"
  },
  {
    "start": "1970000",
    "end": "2178000"
  },
  {
    "text": "you know we'll cover the uh some of the performance back how we tune the system",
    "start": "1971360",
    "end": "1977059"
  },
  {
    "text": "yep performance is another critical component to manage as the code base",
    "start": "1977059",
    "end": "1982760"
  },
  {
    "text": "evolves it's one of the first things to go if you're not paying close attention to it",
    "start": "1982760",
    "end": "1989120"
  },
  {
    "text": "Cosmos DB applies multiple approaches to ensure that we retain and improve performance over time",
    "start": "1989120",
    "end": "1995539"
  },
  {
    "text": "firstly for every component we have a rigorous validation pipeline that",
    "start": "1995539",
    "end": "2000580"
  },
  {
    "text": "monitors the current throughput and latency continuously we make sure that we never regress this",
    "start": "2000580",
    "end": "2006399"
  },
  {
    "text": "performance and every change that goes into the product has to meet or exceed this performance bar",
    "start": "2006399",
    "end": "2012399"
  },
  {
    "text": "I learned this the hard way when I first joined the team even a one percent regression in latency or throughput is",
    "start": "2012399",
    "end": "2019059"
  },
  {
    "text": "considered unacceptable and that sometimes means that as changes happen developers must reclaim performance in",
    "start": "2019059",
    "end": "2025360"
  },
  {
    "text": "other components so that the overall performance bar is met within the storage engine we have a",
    "start": "2025360",
    "end": "2032200"
  },
  {
    "text": "number of things we do to ensure that we have the performance guarantees needed for the service the first is our inverted index which",
    "start": "2032200",
    "end": "2039399"
  },
  {
    "text": "uses a log-free b-plus tree optimized for indexing which was written in collaboration with Microsoft research",
    "start": "2039399",
    "end": "2046960"
  },
  {
    "text": "this gives us significant benefits over a regular B tree when doing multiple batch updates across different pages",
    "start": "2046960",
    "end": "2053679"
  },
  {
    "text": "which is pretty common in in indexes we also optimize our content",
    "start": "2053679",
    "end": "2060099"
  },
  {
    "text": "serialization format in our storage with our binary format that is length",
    "start": "2060099",
    "end": "2065440"
  },
  {
    "text": "prefixed so that projections as you're doing uh as you're looking at a deeply",
    "start": "2065440",
    "end": "2070780"
  },
  {
    "text": "nested document is highly efficient for queries and we can read and write",
    "start": "2070780",
    "end": "2076118"
  },
  {
    "text": "properties super efficiently we also ensure that we use local disks",
    "start": "2076119",
    "end": "2082240"
  },
  {
    "text": "with ssds instead of remote disks to avoid the network latency when committing changes instead we use the",
    "start": "2082240",
    "end": "2088658"
  },
  {
    "text": "fact that we have four such replicas to achieve High availability and durability",
    "start": "2088659",
    "end": "2093760"
  },
  {
    "text": "we also have custom allocators to optimize for various request patterns for instance a linear allocator for",
    "start": "2093760",
    "end": "2100900"
  },
  {
    "text": "query or we have a custom async scheduler with core routines that ensures that we can",
    "start": "2100900",
    "end": "2106599"
  },
  {
    "text": "optimize for concurrency and resource governance all through the stack",
    "start": "2106599",
    "end": "2111820"
  },
  {
    "text": "within the API Gateway the optimization is sent to be about managing stateless scale out scenarios that focus on",
    "start": "2111820",
    "end": "2117880"
  },
  {
    "text": "proxying data we usually use lowallocation.net apis such as span and memory as Machan",
    "start": "2117880",
    "end": "2124960"
  },
  {
    "text": "mentioned and minimize the transforms parsing or any deserialization needed",
    "start": "2124960",
    "end": "2130420"
  },
  {
    "text": "all to reduce variance in latency and garbage collections additionally to ensure predictability in",
    "start": "2130420",
    "end": "2136480"
  },
  {
    "text": "memory access patterns we have a hard pneuma Affinity within our processes and we also use fixed size processes across",
    "start": "2136480",
    "end": "2143140"
  },
  {
    "text": "all of our apis so that we can optimize for one process size when it comes to core performance characteristics across",
    "start": "2143140",
    "end": "2150520"
  },
  {
    "text": "the entire fleet that we have within Azure rvms in our cluster are",
    "start": "2150520",
    "end": "2157000"
  },
  {
    "text": "placed within a single proximity placement Group which allows us to provide guarantees around internode",
    "start": "2157000",
    "end": "2162880"
  },
  {
    "text": "latencies especially when we're doing things like load balancing finally we partner closely with the.net",
    "start": "2162880",
    "end": "2169420"
  },
  {
    "text": "Team to ensure that we continue to benefit from the latest high performance apis that we can leverage in our",
    "start": "2169420",
    "end": "2175060"
  },
  {
    "text": "software stack and this slide will conclude our talk",
    "start": "2175060",
    "end": "2181660"
  },
  {
    "start": "2178000",
    "end": "2311000"
  },
  {
    "text": "today this is our takeaway from building Azure customers DB reliability and",
    "start": "2181660",
    "end": "2187119"
  },
  {
    "text": "performance are features you must design it in on day one if not Day Zero the",
    "start": "2187119",
    "end": "2193599"
  },
  {
    "text": "architecture has respect that and everything they put in had to respect that reliability and performance is a",
    "start": "2193599",
    "end": "2200500"
  },
  {
    "text": "team culture uh as we know mentioned it could be death by a thousand people cut one percent of a performance regression",
    "start": "2200500",
    "end": "2207760"
  },
  {
    "text": "20 of those you'll be printing professional regression yes so it is very important that continuous",
    "start": "2207760",
    "end": "2214420"
  },
  {
    "text": "amount you must locate the time to continuously",
    "start": "2214420",
    "end": "2220599"
  },
  {
    "text": "improve your system even when you think that you are done with the features",
    "start": "2220599",
    "end": "2226599"
  },
  {
    "text": "third leverage customer Insight this is also super important with all the data",
    "start": "2226599",
    "end": "2231940"
  },
  {
    "text": "in our hand how do you decide what feature to build or that customer can benefit or how what kind of optimization",
    "start": "2231940",
    "end": "2239380"
  },
  {
    "text": "you should optimize uh to prioritize so the most customer can benefit from it one other good example that we have is",
    "start": "2239380",
    "end": "2246400"
  },
  {
    "text": "actually the recent optimization in our query engine even though we Implement  API we know mango support nested",
    "start": "2246400",
    "end": "2253780"
  },
  {
    "text": "arrays we observe many of our customers don't even have nested array so our",
    "start": "2253780",
    "end": "2258820"
  },
  {
    "text": "query engine Implement optimization that the simple co-pass is actually much more",
    "start": "2258820",
    "end": "2264460"
  },
  {
    "text": "performing than a lot of customer benefit from it last stay ahead of the",
    "start": "2264460",
    "end": "2270339"
  },
  {
    "text": "business growth and Trend nobody build the services on day one can power billions of requests and petabytes of",
    "start": "2270339",
    "end": "2278140"
  },
  {
    "text": "data so your continuous monitor to see you know if you're hitting bottleneck",
    "start": "2278140",
    "end": "2283720"
  },
  {
    "text": "um you need to work on removing a bottleneck and sometimes you might need to re-implement if your initial design",
    "start": "2283720",
    "end": "2289540"
  },
  {
    "text": "does not support that scalability and training is also really important to know what is upcoming and how do you",
    "start": "2289540",
    "end": "2296619"
  },
  {
    "text": "prioritize those with that we would like to thank you for listening to the talk and we also would like to acknowledge",
    "start": "2296619",
    "end": "2302800"
  },
  {
    "text": "our teams as your customers DB teams teammate it is their work to make this talk possible so thank you",
    "start": "2302800",
    "end": "2311520"
  },
  {
    "start": "2311000",
    "end": "2897000"
  },
  {
    "text": "hello well thank you very much for your talk vinod um are you the only one joining oh okay",
    "start": "2314339",
    "end": "2320500"
  },
  {
    "text": "I see machine is also here yep okay so uh we have quite a few questions the",
    "start": "2320500",
    "end": "2326740"
  },
  {
    "text": "latest one uh it says could you expand on how you verify that you don't have performance aggressions",
    "start": "2326740",
    "end": "2334380"
  },
  {
    "text": "sure um so there's a number of things that we look at uh so the primary metrics we're",
    "start": "2337599",
    "end": "2343060"
  },
  {
    "text": "looking for are throughput uh latency and CPU and we want to make sure that",
    "start": "2343060",
    "end": "2348579"
  },
  {
    "text": "for our critical workloads at the very least uh throughput latency and CPU are",
    "start": "2348579",
    "end": "2353680"
  },
  {
    "text": "constant and do not degrade over time so uh we basically take a single node we",
    "start": "2353680",
    "end": "2358780"
  },
  {
    "text": "load it up to 90 CPU uh for say a single document read a single document right uh",
    "start": "2358780",
    "end": "2366700"
  },
  {
    "text": "fairly well-known query um and so on and so forth and make sure that the throughput that we derive from",
    "start": "2366700",
    "end": "2372520"
  },
  {
    "text": "a node uh the latency that we see at 90 CPU the memory that we see are all con",
    "start": "2372520",
    "end": "2378400"
  },
  {
    "text": "uh meeting The Benchmark that we want and then we basically add specifically",
    "start": "2378400",
    "end": "2384040"
  },
  {
    "text": "well-known workloads over time as we see scenarios that derive from production but we also have",
    "start": "2384040",
    "end": "2390640"
  },
  {
    "text": "um observability in our production system where we monitor specific queries signatures and so on and make sure that",
    "start": "2390640",
    "end": "2397000"
  },
  {
    "text": "we don't regress that as we do deployments and such yeah I think in summary essentially some",
    "start": "2397000",
    "end": "2403540"
  },
  {
    "text": "layers of uh kind of guardrails right uh there's a like a first like a real more",
    "start": "2403540",
    "end": "2409240"
  },
  {
    "text": "like a benchmark unit testing and then a workload like simulated workload and then real workload and then you will",
    "start": "2409240",
    "end": "2415599"
  },
  {
    "text": "hope that you catch it earlier because those are the places you can catch it will send more of a precise measurement",
    "start": "2415599",
    "end": "2420820"
  },
  {
    "text": "and then when you catch low and and you know further further out then it's just a more like a hey I'm seeing something",
    "start": "2420820",
    "end": "2428320"
  },
  {
    "text": "that is not what I expected does that make sense yeah I think so um",
    "start": "2428320",
    "end": "2434859"
  },
  {
    "text": "so I mean just curious I don't know if this person was asking in just a standalone question or it was related to",
    "start": "2434859",
    "end": "2442000"
  },
  {
    "text": "the I don't think it's related to the replication uh things just query performance I think",
    "start": "2442000",
    "end": "2447280"
  },
  {
    "text": "and I think what I understood is yeah you run your own benchmarks based on your query log a representative query",
    "start": "2447280",
    "end": "2453820"
  },
  {
    "text": "log is that correct uh are you talking about uh for the performance regression stuff or yeah I",
    "start": "2453820",
    "end": "2461320"
  },
  {
    "text": "think that's just a general so basically I think they're asking with variable for variable workloads yes there's a degree",
    "start": "2461320",
    "end": "2466420"
  },
  {
    "text": "of variability which is why our performance runs similar to what you do with like ycsb or any of the major",
    "start": "2466420",
    "end": "2471880"
  },
  {
    "text": "benchmarks so to speak we have a well-known set of uh queries that we execute to measure the performance to",
    "start": "2471880",
    "end": "2478780"
  },
  {
    "text": "ensure that uh we meet the bar for latency CPU and throughput and a one",
    "start": "2478780",
    "end": "2484000"
  },
  {
    "text": "percent regression is literally just any regression in either latency or throughput or CPU uh for any of these",
    "start": "2484000",
    "end": "2491020"
  },
  {
    "text": "well-known workloads okay and one question there is I mean you have a",
    "start": "2491020",
    "end": "2496359"
  },
  {
    "text": "graph workload you could have an analytic workload you could have an oltp workload you can have a document",
    "start": "2496359",
    "end": "2501900"
  },
  {
    "text": "retrieval workloads of a document database you can have a key value workload they're like at least five",
    "start": "2501900",
    "end": "2507040"
  },
  {
    "text": "different I mean yep it's complicated right how do you yep",
    "start": "2507040",
    "end": "2512980"
  },
  {
    "text": "go for it now you know how painful it is when people say you know why don't you adopt a new release of the operating",
    "start": "2512980",
    "end": "2518380"
  },
  {
    "text": "system new hardware all the things that matters that we go in right and I think the question there about like one",
    "start": "2518380",
    "end": "2524740"
  },
  {
    "text": "percent one percent says you're very interesting question because sometimes your performance environment it one",
    "start": "2524740",
    "end": "2531160"
  },
  {
    "text": "percent or less than one percent could be a noise and we don't know so we observe and per change",
    "start": "2531160",
    "end": "2538900"
  },
  {
    "text": "and we also observe the trend so sometimes we can catch a larger regression immediately that kind of",
    "start": "2538900",
    "end": "2546460"
  },
  {
    "text": "stats by thousand paper cuts you hope to catch it after 20 people cut not a",
    "start": "2546460",
    "end": "2552640"
  },
  {
    "text": "thousand right this is like yeah so that's how can we operate",
    "start": "2552640",
    "end": "2558040"
  },
  {
    "text": "um and I think for the last question yes we do use known benchmarks like ycsb uh but",
    "start": "2558040",
    "end": "2565000"
  },
  {
    "text": "we also have our own benchmarks that we've built on top of that especially like uh he said like when you're dealing",
    "start": "2565000",
    "end": "2570579"
  },
  {
    "text": "with a graph workload figuring out the sets of relationships we want to test is also uh a crucial",
    "start": "2570579",
    "end": "2577900"
  },
  {
    "text": "part of it so for instance friends of friends is a common query that most people want to ask for graphs right uh",
    "start": "2577900",
    "end": "2583960"
  },
  {
    "text": "or you know third hop or whatever so we tend to build our performance benchmarks around that okay",
    "start": "2583960",
    "end": "2589900"
  },
  {
    "text": "got it and ycsb is mostly for nosql correct",
    "start": "2589900",
    "end": "2594960"
  },
  {
    "text": "a lot of our apis are nosql right like between or Cassandra or documents",
    "start": "2594960",
    "end": "2600400"
  },
  {
    "text": "API uh and we've extended some of that for our graph database as well okay is there um what about traditional",
    "start": "2600400",
    "end": "2608800"
  },
  {
    "text": "rdbms sort of workloads do you guys support those do you have a storage engine that is like a rdbms type of",
    "start": "2608800",
    "end": "2615520"
  },
  {
    "text": "engine like innodb or something like this and Cosmos DB today I think like we just",
    "start": "2615520",
    "end": "2621520"
  },
  {
    "text": "announced the Azure Cosmos DB for postgres um but at least uh this particular uh",
    "start": "2621520",
    "end": "2628780"
  },
  {
    "text": "engine that we're talking about in this talk is geared towards nosql primarily",
    "start": "2628780",
    "end": "2635680"
  },
  {
    "text": "got it uh great great answers uh stay ahead of business growth and Trend what are current research areas",
    "start": "2635680",
    "end": "2642520"
  },
  {
    "text": "being explored related to current business Trends in Enterprise great broad game tough question a great",
    "start": "2642520",
    "end": "2648400"
  },
  {
    "text": "question so so I think there's a two observation that you kind of like in our bigger",
    "start": "2648400",
    "end": "2655480"
  },
  {
    "text": "organization organization that we observe one is actually more of industrial trained right there's a sequel there's no Sequel and actually uh",
    "start": "2655480",
    "end": "2664000"
  },
  {
    "text": "it's kind of two Spectrum the things we start to observe the new trend more like a distributed SQL uh trying to bring",
    "start": "2664000",
    "end": "2670180"
  },
  {
    "text": "nosql closer to SQL space or like in cap lighter the SQL capability the second Trend that we",
    "start": "2670180",
    "end": "2677260"
  },
  {
    "text": "observe is usually we call it uh like a SAS uh can you remind me to know what's",
    "start": "2677260",
    "end": "2683920"
  },
  {
    "text": "the sauce is so for software service yes yeah yeah that is actually a lot of",
    "start": "2683920",
    "end": "2689920"
  },
  {
    "text": "developer who like to write very little coat and so the chaining",
    "start": "2689920",
    "end": "2696160"
  },
  {
    "text": "things together as experience and actually enable that developers fast productivity is super key",
    "start": "2696160",
    "end": "2703900"
  },
  {
    "text": "and you know is there anything else I would like to add I think uh there's there's one aspect",
    "start": "2703900",
    "end": "2709359"
  },
  {
    "text": "like you said the distributed SQL and uh trying to merge what we think of as",
    "start": "2709359",
    "end": "2714700"
  },
  {
    "text": "traditional relational with nosql is something that's been a direction we've been headed towards for the last at",
    "start": "2714700",
    "end": "2720520"
  },
  {
    "text": "least decade uh and we're pushing further and further on trying to you",
    "start": "2720520",
    "end": "2726880"
  },
  {
    "text": "know become true almost uh you know getting",
    "start": "2726880",
    "end": "2732520"
  },
  {
    "text": "all three of the consistency availability and partitioning with caveats and everyone picks their own so",
    "start": "2732520",
    "end": "2737920"
  },
  {
    "text": "that is one area for sure I think the other part is how do we get lower and lower latency like we keep talking about",
    "start": "2737920",
    "end": "2745240"
  },
  {
    "text": "people building complex systems with caching on top of databases because you know the database takes too long to run",
    "start": "2745240",
    "end": "2750700"
  },
  {
    "text": "your query and uh you know you need to have redis on top of your database or if",
    "start": "2750700",
    "end": "2755980"
  },
  {
    "text": "you have like you know text search you build some other caching layer or another key Value Store you shut the results in how do we integrate these to",
    "start": "2755980",
    "end": "2763000"
  },
  {
    "text": "provide more of a low latency experience for people who really need the high performance and low latency but at the",
    "start": "2763000",
    "end": "2769000"
  },
  {
    "text": "same time give you the flexibility of the query I think is another area that is currently being explored",
    "start": "2769000",
    "end": "2776920"
  },
  {
    "text": "great Point um there's some knowledge or talk about each step right hybrid",
    "start": "2776920",
    "end": "2782859"
  },
  {
    "text": "transactional and analytic processing yeah is that where you'd like to go I mean",
    "start": "2782859",
    "end": "2789460"
  },
  {
    "text": "with these other things graph and nosql combined in that",
    "start": "2789460",
    "end": "2794819"
  },
  {
    "text": "I go for it",
    "start": "2797500",
    "end": "2801599"
  },
  {
    "text": "there were papers like tensing right Google published a while ago which was like this hierarchical storage engines",
    "start": "2802839",
    "end": "2809319"
  },
  {
    "text": "with one API right uh but those are multi for analytic processing it was a",
    "start": "2809319",
    "end": "2814540"
  },
  {
    "text": "paper maybe 10 plus years ago so I mean like htap is definitely",
    "start": "2814540",
    "end": "2820619"
  },
  {
    "text": "for instance things like uh the synapse link where you can ingest with oltp and",
    "start": "2821740",
    "end": "2827380"
  },
  {
    "text": "then run analytical workloads on top um I was just trying to formulate uh the",
    "start": "2827380",
    "end": "2833560"
  },
  {
    "text": "answer on how I see the be the staying ahead part uh and how I can talk about it but at",
    "start": "2833560",
    "end": "2841720"
  },
  {
    "text": "the very least I know that that is a direction we are exploring as well but I think that no there's one time",
    "start": "2841720",
    "end": "2847720"
  },
  {
    "text": "that we are brainstorming and I think he brought very good points that analytic uh query or information some are not",
    "start": "2847720",
    "end": "2855160"
  },
  {
    "text": "needed in real time some are actually needed in real time and differentiate levels and provide different solutions",
    "start": "2855160",
    "end": "2861339"
  },
  {
    "text": "for those certainly it could be a strategy tackling that particular space so so we just need to figure out what is",
    "start": "2861339",
    "end": "2868720"
  },
  {
    "text": "in scope with a customer VP as product as itself and what's in scope as a",
    "start": "2868720",
    "end": "2874060"
  },
  {
    "text": "solution end to end because a lot of times people do not just use a product itself right when uh when our customers",
    "start": "2874060",
    "end": "2882839"
  },
  {
    "text": "architect for this problem space you see many components that kind of chain",
    "start": "2882839",
    "end": "2888280"
  },
  {
    "text": "together and it is about like you know how to use average levels and solve a problem with the best outcome that you",
    "start": "2888280",
    "end": "2894579"
  },
  {
    "text": "would hope for well um I think we're at time but I want to thank you both for this excellent",
    "start": "2894579",
    "end": "2900880"
  },
  {
    "start": "2897000",
    "end": "2963000"
  },
  {
    "text": "talk and I'm sure there are more questions that we didn't get to which you can answer either uh in the stock",
    "start": "2900880",
    "end": "2907240"
  },
  {
    "text": "Channel or you can leave your Twitter handles and it can go on to the Twitter sphere but um I want to thank you for your time",
    "start": "2907240",
    "end": "2913599"
  },
  {
    "text": "thank you so much yeah I want to say thank you for people listen in and feel free to reach out to",
    "start": "2913599",
    "end": "2920980"
  },
  {
    "text": "video or me in any channel you can find Twitter LinkedIn I know the note doesn't",
    "start": "2920980",
    "end": "2926260"
  },
  {
    "text": "have Twitter but it's a little bit sensitive you'll be active in Twitter at this moment but LinkedIn we check and I",
    "start": "2926260",
    "end": "2932680"
  },
  {
    "text": "think Microsoft email we don't like to share as well so please uh reach out we",
    "start": "2932680",
    "end": "2937780"
  },
  {
    "text": "we would love to cross sharing and learning hearing from you guys is very important for us",
    "start": "2937780",
    "end": "2943839"
  },
  {
    "text": "and I'll also be on the slack so if you do have any questions afterwards uh feel free to reach out excellent thank you",
    "start": "2943839",
    "end": "2950680"
  },
  {
    "text": "everybody [Music]",
    "start": "2950680",
    "end": "2961670"
  }
]