[
  {
    "start": "0",
    "end": "206000"
  },
  {
    "text": "all right so the topics we're going to be covering this evening we're going to do a relatively",
    "start": "399",
    "end": "5839"
  },
  {
    "text": "brief overview of spark okay the level set here is you will not walk out of here as a spark expert but you will",
    "start": "5839",
    "end": "12400"
  },
  {
    "text": "hopefully at least understand the architecture we're going to start at about a 30,000 foot level and we might",
    "start": "12400",
    "end": "17800"
  },
  {
    "text": "make it down below the 10,000 ft ceiling maybe okay but I'm basically just going to give you a little bit of background",
    "start": "17800",
    "end": "23359"
  },
  {
    "text": "in how spark Works some very minor comparisons to some aspects of Hadoop um",
    "start": "23359",
    "end": "29119"
  },
  {
    "text": "but I'm going to keep that fair light I could talk about this stuff for a long time I mean you know I've been giving one day and three-day classes in this",
    "start": "29119",
    "end": "35520"
  },
  {
    "text": "and even those don't cover everything so we'll do a little bit of background here we're um we're going to touch on a quick",
    "start": "35520",
    "end": "40760"
  },
  {
    "text": "architectural overview we're going to touch on rdds which are the fundamental building blocks inside spark we're going",
    "start": "40760",
    "end": "46520"
  },
  {
    "text": "to touch on Transformations and actions and tie those back to Scala we're going to touch very briefly on data frames",
    "start": "46520",
    "end": "51920"
  },
  {
    "text": "only because that's the new way that you're supposed to be interacting with spark um and because you're also going",
    "start": "51920",
    "end": "58039"
  },
  {
    "text": "to see them a little bit in some of the demos that I give uh we're going to talk about what spark streaming actually is we're going",
    "start": "58039",
    "end": "65680"
  },
  {
    "text": "to talk about how it works a little bit because it doesn't work like other streaming Solutions um we're going to talk briefly",
    "start": "65680",
    "end": "72840"
  },
  {
    "text": "or you will see in here some of the things you can stream and then we'll talk about other things and how to adapt to those uh I'm going to do Demos in",
    "start": "72840",
    "end": "80720"
  },
  {
    "text": "something called The datab Bricks notebook environment datab bricks is the company that uh that employs most of the",
    "start": "80720",
    "end": "86400"
  },
  {
    "text": "spark committers um and most of those spark Comm mters were in the Berkeley amp lab where Spark was",
    "start": "86400",
    "end": "93000"
  },
  {
    "text": "developed their product is a notebook if you've ever seen the IPython notebook or Jupiter or Zeppelin or any of those",
    "start": "93000",
    "end": "99640"
  },
  {
    "text": "things and there is an open source spark notebook out there it's that kind of thing you know interaction with the",
    "start": "99640",
    "end": "104799"
  },
  {
    "text": "spark cluster from within uh a browser but they're making this into their product and the goal behind this for",
    "start": "104799",
    "end": "110680"
  },
  {
    "text": "them is that if this this thing is not just a playground it's a place where you can actually deploy spark jobs and not",
    "start": "110680",
    "end": "116640"
  },
  {
    "text": "worry about how you know what it takes to spin up all the ec2 Clusters stuff so since I kind of have free access to this",
    "start": "116640",
    "end": "122640"
  },
  {
    "text": "thing as as one of their trainers uh i' I'm going to use that they like this because then it looks like I'm pitching",
    "start": "122640",
    "end": "128479"
  },
  {
    "text": "their product as well okay so um if you if you end up liking the product and you",
    "start": "128479",
    "end": "133920"
  },
  {
    "text": "have information or have questions about it I can get you in touch with the guys who uh who set all the pricing stuff up",
    "start": "133920",
    "end": "140480"
  },
  {
    "text": "um and they have new pricing tiers and stuff coming along and then we are actually going to look at some actual code uh hope modulo",
    "start": "140480",
    "end": "147879"
  },
  {
    "text": "it actually working okay um I need to put this in here first",
    "start": "147879",
    "end": "153200"
  },
  {
    "text": "of all portions of this deck are mine portions of the deck I adapted from databook training material with their",
    "start": "153200",
    "end": "158560"
  },
  {
    "text": "permission most of their training material they release as um under the uh Creative Commons license but I always",
    "start": "158560",
    "end": "164640"
  },
  {
    "text": "ask them permission uh also mentioned this came through today New Circle is actually running a p a public spark",
    "start": "164640",
    "end": "171000"
  },
  {
    "text": "foundations class here in Philly it's currently scheduled for November 3rd through November 5th um I'm slated to",
    "start": "171000",
    "end": "177720"
  },
  {
    "text": "teach it if you're interested in doing that class uh there's a tiny URL up",
    "start": "177720",
    "end": "182800"
  },
  {
    "text": "there these slides will be published okay so you can go find that URL later um I also have a discount code",
    "start": "182800",
    "end": "189720"
  },
  {
    "text": "so if you decide you want to register for this let me know I'll give you the discount code um I don't know what the",
    "start": "189720",
    "end": "194840"
  },
  {
    "text": "discount is I haven't looked at it but but just to keep in mind if you want to go to a training class and not have to",
    "start": "194840",
    "end": "199879"
  },
  {
    "text": "travel there there is a currently public uh one that's scheduled here all right so here's the brief",
    "start": "199879",
    "end": "206959"
  },
  {
    "start": "206000",
    "end": "471000"
  },
  {
    "text": "overview of spark it's a general purpose Computing engine for um for distributed",
    "start": "206959",
    "end": "213280"
  },
  {
    "text": "processing of data that's a fancy term you'll actually see that term out in the uh in the spark documentation but what",
    "start": "213280",
    "end": "220560"
  },
  {
    "text": "most people compare compare it to is Hadoop okay anybody in here used",
    "start": "220560",
    "end": "225680"
  },
  {
    "text": "Hadoop anybody here hate Hadoop as much as I do so spark was intended to a lot of the",
    "start": "225680",
    "end": "233200"
  },
  {
    "text": "guys who developed spark were Hadoop committers actually um but the the intent behind spark was to make it",
    "start": "233200",
    "end": "238799"
  },
  {
    "text": "easier to do um to do this kind of big data processing um easier to program and",
    "start": "238799",
    "end": "245120"
  },
  {
    "text": "faster so one of the things that spark does to make things faster is it keep stuff in memory as much as possible",
    "start": "245120",
    "end": "250280"
  },
  {
    "text": "we'll talk a little bit about that so it it's fast it does as much in memory as it can",
    "start": "250280",
    "end": "257720"
  },
  {
    "text": "unlike Hadoop those of you who've played with Hadoop know how it works right you write a map Produce job read something out of hdfs at the end of that M produce",
    "start": "257720",
    "end": "264720"
  },
  {
    "text": "phase you dump stuff back into hdfs you want to do five or six transformation jobs you're going to use something like",
    "start": "264720",
    "end": "270800"
  },
  {
    "text": "Uzi to do your scheduling and each one of those stages is reading and writing from hdfs right so you're IO bound at",
    "start": "270800",
    "end": "276800"
  },
  {
    "text": "that point um spark is not necessarily IO bound because it does its best to keep things around in memory um there",
    "start": "276800",
    "end": "283680"
  },
  {
    "text": "are benchmarks I did not include in here um but I can find them for you if you want that show how much faster spark",
    "start": "283680",
    "end": "289840"
  },
  {
    "text": "actually is than Hadoop and it's really pretty astonishing um the benchmarks I've seen is not only is it",
    "start": "289840",
    "end": "295440"
  },
  {
    "text": "significantly faster but it tends to use fewer resources um so if if you're paying for your ec2 nodes this is a good",
    "start": "295440",
    "end": "302440"
  },
  {
    "text": "thing it's compact okay I'm going to actually show you an example of this in a minute in general to do the same job",
    "start": "302440",
    "end": "308840"
  },
  {
    "text": "you would do in Hadoop you'll write less code and Spark I have a couple examples of this in a couple of",
    "start": "308840",
    "end": "314759"
  },
  {
    "text": "minutes all right it's also multilanguage you can write your spark applications in Scala which is what it's",
    "start": "314759",
    "end": "321720"
  },
  {
    "text": "mostly implemented in 95% of the code basis Scala um and that's always a good language to choose partly because you're",
    "start": "321720",
    "end": "328520"
  },
  {
    "text": "in this room and you your scholar guys right but partly because if it's written in that language you're you're going to have a lot less of a translation factor",
    "start": "328520",
    "end": "336240"
  },
  {
    "text": "in between you and the apis but you can also write in Java you can ALS you can write in Python and in 14 they added r",
    "start": "336240",
    "end": "344520"
  },
  {
    "text": "support okay so so those data scientists who like to play around with r there's support for R in this language now as",
    "start": "344520",
    "end": "352080"
  },
  {
    "text": "well so here's a I don't know if you can see that I couldn't make this any bigger that's a Hadoop job for computing an",
    "start": "352080",
    "end": "357919"
  },
  {
    "text": "average all you really have to do is see the gray right first of all it's Java which means it's already verbose more",
    "start": "357919",
    "end": "364199"
  },
  {
    "text": "verbose than we scholar programmers like I tweeted something not too long ago I had to help one of the guys on this team",
    "start": "364199",
    "end": "370039"
  },
  {
    "text": "who isn't a scholar programmer doesn't really know python uh so he was building some initial Labs out in Java and I was",
    "start": "370039",
    "end": "376199"
  },
  {
    "text": "helping them debug this and my tweet basically said you know after doing scholar for seven years now I really",
    "start": "376199",
    "end": "383960"
  },
  {
    "text": "really hate writing a Java all right so not only is this really Java verose but",
    "start": "383960",
    "end": "389039"
  },
  {
    "text": "in addition add to that it's um it's there's just a lot of stuff you have to do to set this up this is the rdd",
    "start": "389039",
    "end": "395880"
  },
  {
    "text": "version of that in um in spark now the person who",
    "start": "395880",
    "end": "401240"
  },
  {
    "text": "put this together I cut and pasted this I probably would make this a little cleaner the tupal syntax makes that a little hard to",
    "start": "401240",
    "end": "407199"
  },
  {
    "text": "read but it's less code um in fact it's such a pleasure to work with this for a",
    "start": "407199",
    "end": "413680"
  },
  {
    "text": "couple of reasons first of all those of you who have been playing around with scholar collections will recognize these",
    "start": "413680",
    "end": "418960"
  },
  {
    "text": "things the rdd level stuff it looks a lot like the kind of transforms you're used to doing on a scholar collection what are",
    "start": "418960",
    "end": "426520"
  },
  {
    "text": "the three most common transforms we do on a scholar collection anybody want to besides Michael anybody want to pipe in",
    "start": "426520",
    "end": "432720"
  },
  {
    "text": "what if you have a scholar collection what are the three most common map is the the one of the most common filter",
    "start": "432720",
    "end": "438000"
  },
  {
    "text": "filter what's the third one flat map flat map right those why are those the three most common those are what four",
    "start": "438000",
    "end": "445720"
  },
  {
    "text": "comprehensions dgar into if you have a four yield it always desugar down into map flat map and filter all three of",
    "start": "445720",
    "end": "452840"
  },
  {
    "text": "those Transformations exist here in in the rdd apis so when you're programming",
    "start": "452840",
    "end": "458360"
  },
  {
    "text": "rdds in um in spark mentally you can almost model them like a distributed",
    "start": "458360",
    "end": "464759"
  },
  {
    "text": "collection although it's a little stranger than that we'll we'll see see what I mean by that in a",
    "start": "464759",
    "end": "470759"
  },
  {
    "text": "minute um spark breaks things up into there there there's nomenclature you kind of have to get your head around",
    "start": "470759",
    "end": "476520"
  },
  {
    "start": "471000",
    "end": "981000"
  },
  {
    "text": "your spark program is is called the driver the driver is a specific jvm it",
    "start": "476520",
    "end": "482080"
  },
  {
    "text": "contains your program it runs somewhere out on the cluster unless you're running spark shell okay if you fire up the",
    "start": "482080",
    "end": "489000"
  },
  {
    "text": "spark shell anybody in here not use the spark shell okay anybody in here not use the",
    "start": "489000",
    "end": "494639"
  },
  {
    "text": "scolar reppel ever the Scola reppel is the spark shell basically um the spark",
    "start": "494639",
    "end": "500720"
  },
  {
    "text": "shell fires up the Scala reppel with uh with a bunch of pre-loaded stuff okay",
    "start": "500720",
    "end": "506000"
  },
  {
    "text": "pre-loaded class path and a couple of predefined uh objects specifically a spark context and a SQL context but",
    "start": "506000",
    "end": "512518"
  },
  {
    "text": "other than that it's the standard scholar repple if you're using spark in Python you get the standard python",
    "start": "512519",
    "end": "517640"
  },
  {
    "text": "reppel although there's an option for those of you who like me have done python there's an option to get it to use IPython which all of us always use",
    "start": "517640",
    "end": "525040"
  },
  {
    "text": "if you're going to do python with it okay so that's the driver program the driver is responsible essentially for",
    "start": "525040",
    "end": "531480"
  },
  {
    "text": "kicking everything off I'm going to show you architecturally what that means but it's the driver is not your program it's",
    "start": "531480",
    "end": "537200"
  },
  {
    "text": "one part of your program right your program actually is distributed out across the cluster your program actually",
    "start": "537200",
    "end": "544360"
  },
  {
    "text": "is a multi-node program there are all kinds of complicated pieces to spark",
    "start": "544360",
    "end": "550000"
  },
  {
    "text": "where it does things like gather up the data that you're using and serialize it and send it out across the wire it does",
    "start": "550000",
    "end": "556440"
  },
  {
    "text": "bite code serialization as well oh look you need to do something with this Lambda that you're passing to this map",
    "start": "556440",
    "end": "561800"
  },
  {
    "text": "function it gathers up the bik code for that Lambda and then does a scope analysis figures out what that bik code",
    "start": "561800",
    "end": "567760"
  },
  {
    "text": "is referring to gather up that information and serializes that all and distributes that out all out across the",
    "start": "567760",
    "end": "574000"
  },
  {
    "text": "cluster so literally your application is running on multiple nodes in the cluster",
    "start": "574000",
    "end": "579160"
  },
  {
    "text": "okay your driver is in control of it but it's not all running it's not a single jvm uh uh program anymore it's literally",
    "start": "579160",
    "end": "587680"
  },
  {
    "text": "a distributed program so this is an example um now",
    "start": "587680",
    "end": "594720"
  },
  {
    "text": "before I go into this there are things in spark called cluster managers",
    "start": "594720",
    "end": "600920"
  },
  {
    "text": "there are actually Four I'm going to say four but really it's three there are three cluster managers currently",
    "start": "600920",
    "end": "607079"
  },
  {
    "text": "supported in spark and a fourth one is sort of a pretend cluster manager the Standalone spark cluster manager is what",
    "start": "607079",
    "end": "614560"
  },
  {
    "text": "we're going to be depicting here uh that if you download the data Stacks distribution that you and I were talking",
    "start": "614560",
    "end": "620160"
  },
  {
    "text": "about earlier that comes with Cassandra and a cluster manager which is the spark Standalone cluster manager uh yarn those",
    "start": "620160",
    "end": "626320"
  },
  {
    "text": "of you who've done Hadoop are familiar with yarn right yarn is the cluster manager most people use with Hadoop so",
    "start": "626320",
    "end": "631640"
  },
  {
    "text": "if you're already a Hadoop shop spark will work with yarn you don't have to go use some other cluster manager another",
    "start": "631640",
    "end": "637079"
  },
  {
    "text": "cluster manager it works with that also came out of the Berkeley ampad is maos maos is a general purpose cluster",
    "start": "637079",
    "end": "643040"
  },
  {
    "text": "manager so if you're doing lots and lots of different things across your cluster you might choose to run maos to manage",
    "start": "643040",
    "end": "648760"
  },
  {
    "text": "all that stuff and Spark will work in there so the general yes do you know what's more widely used um which of the",
    "start": "648760",
    "end": "656000"
  },
  {
    "text": "do I know which cluster manager is more widely used spark I don't know the demographics of that um the general",
    "start": "656000",
    "end": "662200"
  },
  {
    "text": "humanistics are if you're going to be a spark only shop probably the simplest choice for you would be to use the spark",
    "start": "662200",
    "end": "668480"
  },
  {
    "text": "Standalone and the simplest way to do that is if you're going to be using Cassandra as well use the data Stacks",
    "start": "668480",
    "end": "674160"
  },
  {
    "text": "distribution because they bundle all that stuff up very nicely if you're already a Hadoop shop and you want to migrate over to spark and you're using",
    "start": "674160",
    "end": "681240"
  },
  {
    "text": "yarn then you would stick with yarn like why introduce additional stuff right like if you're using EMR you might just",
    "start": "681240",
    "end": "686920"
  },
  {
    "text": "stick with yarn right um if you have a lot of other stuff that you're running in your cluster maybe you're going to",
    "start": "686920",
    "end": "692519"
  },
  {
    "text": "migrate toward maos to do common management of all that stuff and then just drop sparking under there it",
    "start": "692519",
    "end": "697680"
  },
  {
    "text": "depends on your use case now I said those are the three right but I actually said there's kind of a fourth one and",
    "start": "697680",
    "end": "703040"
  },
  {
    "text": "there is kind of a fourth one when you bring up the spark shell or pie spark for that matter and you don't specify a",
    "start": "703040",
    "end": "709920"
  },
  {
    "text": "master a particular cluster Master to connect to it comes up in local mode",
    "start": "709920",
    "end": "716760"
  },
  {
    "text": "local mode is basically a pretend cluster inside your jvm essentially it's a thread pool okay so it's all inside",
    "start": "716760",
    "end": "723880"
  },
  {
    "text": "the jvm you have your cluster is the machine you're running on um and the the",
    "start": "723880",
    "end": "729880"
  },
  {
    "text": "remote nodes are simulated with threads inside the jvm obviously you don't want to do this for production but it's a",
    "start": "729880",
    "end": "735240"
  },
  {
    "text": "great way to play around with something on the airplane right where you can't actually spin up a cluster or if you're",
    "start": "735240",
    "end": "740560"
  },
  {
    "text": "just fooling around with spark and you have relatively small data sets Okay so this is intended to depict at kind of a",
    "start": "740560",
    "end": "747320"
  },
  {
    "text": "higher level what goes on with uh the Standalone cluster manager so the cluster manager this would be a cluster",
    "start": "747320",
    "end": "753680"
  },
  {
    "text": "with two worker nodes okay the driver program is depicted like that because it's supposed to look a little bit like",
    "start": "753680",
    "end": "760399"
  },
  {
    "text": "the uh the the scolar shell right there's a master that knows where all the workers",
    "start": "760399",
    "end": "766320"
  },
  {
    "text": "are what I have in in Orange there um is literally a jvm called the worker which",
    "start": "766320",
    "end": "772800"
  },
  {
    "text": "is kind of a misnomer um a worker is really more like a manager so what happens is the driver program starts up",
    "start": "772800",
    "end": "779959"
  },
  {
    "text": "and is told your master is over there so it contacts the master and it says I need these resources to do my",
    "start": "779959",
    "end": "785440"
  },
  {
    "text": "distributed computing the master knows where all the worker nodes are so the master contacts the worker nodes and",
    "start": "785440",
    "end": "792560"
  },
  {
    "text": "says hey you need to allocate me some resources now these workers have been configured so that they have a certain",
    "start": "792560",
    "end": "797959"
  },
  {
    "text": "amount of memory that they're allowed to allocate and a certain number of What spark tends to call slots or cores which",
    "start": "797959",
    "end": "805040"
  },
  {
    "text": "is another misnomer um and and they will they can all at those to what are called",
    "start": "805040",
    "end": "810760"
  },
  {
    "text": "executors so they will allocate an Executor an Executor is another jvm it",
    "start": "810760",
    "end": "815959"
  },
  {
    "text": "is given a certain amount of memory and a certain number of slots indicated here with a t because I didn't change it",
    "start": "815959",
    "end": "822480"
  },
  {
    "text": "these executors are individual jvms running on this node the executors then",
    "start": "822480",
    "end": "827519"
  },
  {
    "text": "contact the driver and they say okay you and me we're working together at which",
    "start": "827519",
    "end": "833800"
  },
  {
    "text": "point the worker can step out of the communications path and the master can step out of the communications path the",
    "start": "833800",
    "end": "839079"
  },
  {
    "text": "workers's job of course is to monitor these executors and restart them if they crash right or or respond to other",
    "start": "839079",
    "end": "846519"
  },
  {
    "text": "requests from other jobs for other executors you know because you can a cluster can run multiple spark jobs at",
    "start": "846519",
    "end": "852199"
  },
  {
    "text": "once but that's the general layout of a spark cluster so what's running your",
    "start": "852199",
    "end": "857320"
  },
  {
    "text": "application here there's code in the driver and there's code that the driver has sent across the wire to the",
    "start": "857320",
    "end": "864639"
  },
  {
    "text": "executors okay so that's the basic 20,000 view of you know a standard spark",
    "start": "864639",
    "end": "870880"
  },
  {
    "text": "cluster architecture yeah that's the safest way to think about it one analogy the question was is it safe to think",
    "start": "870880",
    "end": "876399"
  },
  {
    "text": "about the worker as not really participating in your program but but as managing um yeah I kind of think about",
    "start": "876399",
    "end": "882800"
  },
  {
    "text": "it as analogous to an AA supervisor okay its responsibility is to manage fault tolerance um in one part",
    "start": "882800",
    "end": "890839"
  },
  {
    "text": "and another to allocate resources so it's G given a certain pool of resources it's allowed to allocate and a certain",
    "start": "890839",
    "end": "896920"
  },
  {
    "text": "strategy by which it can allocate those res resources and it takes care of of managing that on your",
    "start": "896920",
    "end": "903160"
  },
  {
    "text": "behalf um in order for the the the code to be Center over the wire I presume it's presumed that the version of of",
    "start": "903160",
    "end": "911240"
  },
  {
    "text": "java running all the machines is to be the same um yeah you have a couple of restrictions you want the same version",
    "start": "911240",
    "end": "916320"
  },
  {
    "text": "of java running across the machines uh your um the things the data that you are",
    "start": "916320",
    "end": "922800"
  },
  {
    "text": "using inside your closures let's say must be serializable right okay now",
    "start": "922800",
    "end": "928240"
  },
  {
    "text": "there are two kinds of serialization right there's the standard Java serialization but what the spark people actually recommend is that you use cryo",
    "start": "928240",
    "end": "935240"
  },
  {
    "text": "which is one of these third party serialization libraries um cryo has a couple of additional requirements like",
    "start": "935240",
    "end": "941199"
  },
  {
    "text": "you need to tell cryo hey these are the classes that I'm going to be serializing but it does a much more efficient",
    "start": "941199",
    "end": "946440"
  },
  {
    "text": "serialization so for a production cluster they recommend that you tell spark hey I don't want to use standard",
    "start": "946440",
    "end": "951680"
  },
  {
    "text": "serialization I want to use cryo um and then the requirements for serialization are just only slightly different um but",
    "start": "951680",
    "end": "958040"
  },
  {
    "text": "one of the common things that that can happen is that you try to start something up and you your your whole program dies with a not serializable",
    "start": "958040",
    "end": "965000"
  },
  {
    "text": "exception right and sometimes it's weird because you've you've confirmed that I mean you look at the code and you say it",
    "start": "965000",
    "end": "971600"
  },
  {
    "text": "this is this should be serializable but guess what it closed over something all the way up there that's not serializable",
    "start": "971600",
    "end": "978480"
  },
  {
    "text": "right so you'd have to be a little careful about that all right so what's an rdd we're going to talk very briefly about what",
    "start": "978480",
    "end": "984600"
  },
  {
    "start": "981000",
    "end": "1625000"
  },
  {
    "text": "this is um because largely because you you need to understand currently you need to understand a",
    "start": "984600",
    "end": "990959"
  },
  {
    "text": "little bit about rdds to understand how to interact with spark even though data frames are the preferred way to interact",
    "start": "990959",
    "end": "996839"
  },
  {
    "text": "with spark you can't really use them effectively if you don't understand at least something about",
    "start": "996839",
    "end": "1002639"
  },
  {
    "text": "rdds It's actually an acronym it stands for res resilient distributed data",
    "start": "1002639",
    "end": "1008199"
  },
  {
    "text": "set okay I'm going to show you a picture in a minute these are just the words right cons think of it like I said think",
    "start": "1008199",
    "end": "1013959"
  },
  {
    "text": "of it as sort of like a distributed collection or a distri like a distributed array or a list um so kind",
    "start": "1013959",
    "end": "1021160"
  },
  {
    "text": "of like an array or list in your program like doing a new array or something except it's all over the cluster okay",
    "start": "1021160",
    "end": "1028120"
  },
  {
    "text": "it's actually it's actually a data set that's been partitioned across the cluster all right so that I kind of",
    "start": "1028120",
    "end": "1035120"
  },
  {
    "text": "jumped ahead there it represents partitioned data so the idea here is that if you're processing data out of a",
    "start": "1035120",
    "end": "1041079"
  },
  {
    "text": "Cassandra database let's say or a parquet file we'll talk about some of these types later or an hdfs file right",
    "start": "1041079",
    "end": "1048760"
  },
  {
    "text": "that data actually gets distributed out across all the nodes in the cluster each each node gets a different partition of",
    "start": "1048760",
    "end": "1055200"
  },
  {
    "text": "that data and all of those partition combined together represent all of your data and those are modeled with this",
    "start": "1055200",
    "end": "1062440"
  },
  {
    "text": "thing called an rdd all right and actually we kind of have two different uses for this term",
    "start": "1062440",
    "end": "1069679"
  },
  {
    "text": "there's the generic use of an rdd meaning we mean this is a distributed data set the resilient part by the way",
    "start": "1069679",
    "end": "1075520"
  },
  {
    "text": "is the Fault tolerant part um but inside your your uh your program inside your driver program remember your program the",
    "start": "1075520",
    "end": "1082960"
  },
  {
    "text": "code that you write is the driver right pieces of it get picked up by spark and serialized out over to the the network",
    "start": "1082960",
    "end": "1089120"
  },
  {
    "text": "but your program actually starts up in the driver and you'll have handles to these objects called",
    "start": "1089120",
    "end": "1094400"
  },
  {
    "text": "rdds the way I like to describe these in classes is that you've got the general concept of an rdd being a distributed",
    "start": "1094400",
    "end": "1101000"
  },
  {
    "text": "data set inside your program this thing that you're holding in your hand that has an rdd type that's kind of like a",
    "start": "1101000",
    "end": "1106280"
  },
  {
    "text": "file handle right so if it one of the common questions that comes up in classes is you just created an rdd does",
    "start": "1106280",
    "end": "1112760"
  },
  {
    "text": "that thing have any data in it well no it's think of it like a file descriptor you create it it's like opening a file",
    "start": "1112760",
    "end": "1119799"
  },
  {
    "text": "if you open a file and you get a file descriptor back is there any data in that thing no it's just a handle that lets you go get the data when you need",
    "start": "1119799",
    "end": "1126159"
  },
  {
    "text": "it so conceptually that's kind of sort of it's an imperfect metaphor but it's kind of the it's kind of the notion you",
    "start": "1126159",
    "end": "1132559"
  },
  {
    "text": "should be striving for it's not really a container of data it's a handle to allow you to manipulate data from the driver",
    "start": "1132559",
    "end": "1139400"
  },
  {
    "text": "even though the data isn't local right so this is an example of of",
    "start": "1139400",
    "end": "1145240"
  },
  {
    "text": "an rdd okay let's suppose I've got this data set with 25 items in it hardly big data but I can't fit bit fit big data on",
    "start": "1145240",
    "end": "1152240"
  },
  {
    "text": "the screen right um and I've I've distributed this data out across the cluster um and my particular",
    "start": "1152240",
    "end": "1159840"
  },
  {
    "text": "cluster um in with a particular partitioner and there are several partitioners in there the most common",
    "start": "1159840",
    "end": "1165159"
  },
  {
    "text": "one is called a hash partitioner and it does what you would expect it uses a code this has divided it up into five",
    "start": "1165159",
    "end": "1172640"
  },
  {
    "text": "partitions okay so what is a partition does anybody have an idea what the part what I mean by partition",
    "start": "1172640",
    "end": "1178200"
  },
  {
    "text": "here those chunks of dat be be on",
    "start": "1178200",
    "end": "1183480"
  },
  {
    "text": "um maybe definitely okay so here's the thing you need to you need to distinguish in your",
    "start": "1183480",
    "end": "1189799"
  },
  {
    "text": "mind between the notion of a partition and the notion of a node it's entirely possible to have data",
    "start": "1189799",
    "end": "1196679"
  },
  {
    "text": "that is partitioned into five partitions running on a two node cluster right um at which point at at",
    "start": "1196679",
    "end": "1205159"
  },
  {
    "text": "least two of the Clusters are going to have two of those partitions they may still be working on them in parallel in",
    "start": "1205159",
    "end": "1210760"
  },
  {
    "text": "a multi-threaded sense right and those threads may still be mapped onto multiple cores within that machine so it",
    "start": "1210760",
    "end": "1217480"
  },
  {
    "text": "may be truly parallel but it's not going to be parallel in the sense that each one is necessarily running on a",
    "start": "1217480",
    "end": "1223400"
  },
  {
    "text": "different machine partitions are are the means by which we parallelize our data but they're not the only thing you have",
    "start": "1223400",
    "end": "1229520"
  },
  {
    "text": "to partition your data first but where they end up on the cluster depends on how big the cluster is how many resources are being allocated that sort",
    "start": "1229520",
    "end": "1236360"
  },
  {
    "text": "of thing migrate from one place to they can um in a sense the partitions will",
    "start": "1236360",
    "end": "1241840"
  },
  {
    "text": "not migrate but what can happen is that certain Transformations on an rdd will cause data to go across a network okay",
    "start": "1241840",
    "end": "1248640"
  },
  {
    "text": "I'll talk about that in just a minute I don't actually have a slide that discusses that I ask one question um who",
    "start": "1248640",
    "end": "1254000"
  },
  {
    "text": "who takes care of the overall health of the Clusters that zookeeper or something under the hood um it doesn't under the",
    "start": "1254000",
    "end": "1261000"
  },
  {
    "text": "hood but if you want if you want all the cluster managers except local can be run under zookeeper keeper to manage that",
    "start": "1261000",
    "end": "1268280"
  },
  {
    "text": "okay so rather than build that in they basically said we will run with zookeeper so if you want that kind of high availability you can run the whole",
    "start": "1268280",
    "end": "1274279"
  },
  {
    "text": "thing under zookeeper all right so this is a three node cluster so we can already tell that",
    "start": "1274279",
    "end": "1279520"
  },
  {
    "text": "that we're not going to get one partition per node right and it might be partitioned like",
    "start": "1279520",
    "end": "1286679"
  },
  {
    "text": "this all right it might be spread out across the and again those arrows don't",
    "start": "1286679",
    "end": "1292080"
  },
  {
    "text": "mean anything other than to indicate this is one possible way that they could be distributed across the",
    "start": "1292080",
    "end": "1297159"
  },
  {
    "text": "cluster right and then that executor has threads in it that are going to be working on those individual",
    "start": "1297159",
    "end": "1304760"
  },
  {
    "text": "rdds now there are several ways that you can create an rdd um I'm actually going to show you",
    "start": "1306159",
    "end": "1312080"
  },
  {
    "text": "some code in a couple of minutes you can parallelize a collection um I'll show you the code for",
    "start": "1312080",
    "end": "1317559"
  },
  {
    "text": "that before we actually run some of it what does this mean um so what this actually means is I",
    "start": "1317559",
    "end": "1325360"
  },
  {
    "text": "start U this is most commonly done within the uh within the spark shell I've got an array of data I know the",
    "start": "1325360",
    "end": "1332159"
  },
  {
    "text": "array will fit in memory because I loaded it in memory but I want to play with some spark algorithms with this",
    "start": "1332159",
    "end": "1337279"
  },
  {
    "text": "data so I call parallelize on it and what that causes to happen is it causes the driver to partition up that array",
    "start": "1337279",
    "end": "1345039"
  },
  {
    "text": "and distribute those partitions out across the network from the driver okay",
    "start": "1345039",
    "end": "1350240"
  },
  {
    "text": "so now you've got that that array that you loaded into the driver pushed out across the network so now you can play",
    "start": "1350240",
    "end": "1355640"
  },
  {
    "text": "with some of your parallel uh algorithms now this is really good for playing with data and doing data",
    "start": "1355640",
    "end": "1361720"
  },
  {
    "text": "analysis prior to doing ETL for instance or prior to doing machine learning because it's very common you know before",
    "start": "1361720",
    "end": "1367400"
  },
  {
    "text": "you do ETL you have to kind of get a sense of what the data looks like before you do machine learning you definitely have to get a sense for for what the",
    "start": "1367400",
    "end": "1373520"
  },
  {
    "text": "data looks like it's very common to say okay I've got to explore this data a little bit I'm going to chunk it down",
    "start": "1373520",
    "end": "1379120"
  },
  {
    "text": "into a small hunk of data and load it into memory and play with it locally uh and that's fine and parallelize is good",
    "start": "1379120",
    "end": "1385200"
  },
  {
    "text": "for working out your algorithms you definitely don't want to do this in production okay if you do this in",
    "start": "1385200",
    "end": "1391080"
  },
  {
    "text": "production that means your driver has become a bottleneck for Io bad move all right so the second way to do it is to",
    "start": "1391080",
    "end": "1397760"
  },
  {
    "text": "read data from an external usually a distributed source so if you read data for instance from",
    "start": "1397760",
    "end": "1403480"
  },
  {
    "text": "hdfs you tell spark I want to initialize this rdd from hdfs",
    "start": "1403480",
    "end": "1408640"
  },
  {
    "text": "um the Assumption of course is that hdfs being a distributed file system is accessible from all the nodes in the",
    "start": "1408640",
    "end": "1414880"
  },
  {
    "text": "cluster right so what will happen is the partitioning strategy is they they're all aware of what that partitioning",
    "start": "1414880",
    "end": "1421159"
  },
  {
    "text": "strategy is so they all choose an appropriate block from that htfs file so each block",
    "start": "1421159",
    "end": "1428120"
  },
  {
    "text": "in the htfs file becomes a partition that's just the way the htfs connector works and then different nodes read",
    "start": "1428120",
    "end": "1434120"
  },
  {
    "text": "different blocks and they all read them themselves so the drivers is no longer a bottleneck right the data is actually",
    "start": "1434120",
    "end": "1440320"
  },
  {
    "text": "being read on the individual nodes so when you do that kind of IO and the cassander connector and the S3 connector",
    "start": "1440320",
    "end": "1445840"
  },
  {
    "text": "they all work the same way right they all do their own IO the the driver's out of the loop then Brian what do you do if",
    "start": "1445840",
    "end": "1453760"
  },
  {
    "text": "um uh if memory is exceeded does it just blow up does it have automated spill over to dis or is that um yes to both",
    "start": "1453760",
    "end": "1461279"
  },
  {
    "text": "okay the answer is that it again it depends on what you're doing and where you're doing it there are ways to spill",
    "start": "1461279",
    "end": "1467120"
  },
  {
    "text": "stuffff out into dis um in general um I assume you're talking about",
    "start": "1467120",
    "end": "1473080"
  },
  {
    "text": "when an rdd is being processed all right so in general what will happen is spark will do its best to put as much of that",
    "start": "1473080",
    "end": "1479679"
  },
  {
    "text": "rdd in memory as possible okay but if it won't fit in memory it it will go out to the io area and get it again okay and",
    "start": "1479679",
    "end": "1488039"
  },
  {
    "text": "and read it as it needs it um there is the capability of doing caching so you can say I'd like to Cache this rdd",
    "start": "1488039",
    "end": "1494799"
  },
  {
    "text": "please um I've read it in I don't want to have to go back to hdf s every time I'm going to be running through this",
    "start": "1494799",
    "end": "1500159"
  },
  {
    "text": "stuff multiple times please cach it caching strategies are a whole topic we could spend hours on right but there is",
    "start": "1500159",
    "end": "1506440"
  },
  {
    "text": "a way to do that and when you get into caching there are ways to say cach it in memory cach it to disk cach it to disk",
    "start": "1506440",
    "end": "1512279"
  },
  {
    "text": "with one level of replication cach it to disk serialize so it takes up less memory um there are a whole bunch of",
    "start": "1512279",
    "end": "1517919"
  },
  {
    "text": "different strategies for caching that you can use all right another um another way",
    "start": "1517919",
    "end": "1523039"
  },
  {
    "text": "that you can create an rdd is by transforming it from another rdd okay now thinking back to scholar collections",
    "start": "1523039",
    "end": "1529440"
  },
  {
    "text": "we do this all the time I start with an array of strings um I'm going to do a map on that and convert those things to",
    "start": "1529440",
    "end": "1535720"
  },
  {
    "text": "an array of ins I'm then going to do another map on that convert that to a sequence um and then maybe I'll do",
    "start": "1535720",
    "end": "1542000"
  },
  {
    "text": "another map on that convert it to a sequence of tupal and finally filter the stuff out I don't want and map it into a",
    "start": "1542000",
    "end": "1548559"
  },
  {
    "text": "map right these are all transformations on arrays um and you can do those on rdds and what you get back is another",
    "start": "1548559",
    "end": "1556200"
  },
  {
    "text": "rdd right and finally um via spark streaming which is the reason we're here",
    "start": "1556200",
    "end": "1561679"
  },
  {
    "text": "right spark streaming produces rdds and I'm going to show you how that works in a little bit and that went too fast so this is um",
    "start": "1561679",
    "end": "1569799"
  },
  {
    "text": "these are two ways for instance to create um I assume you can all read that these are two ways to create",
    "start": "1569799",
    "end": "1576840"
  },
  {
    "text": "um an initial rdd um typically if you're reading the documentation the initial rdd is is typically either called a base",
    "start": "1576840",
    "end": "1584520"
  },
  {
    "text": "rdd or a uh or the input rdd that's the first rdd in the chain of",
    "start": "1584520",
    "end": "1590960"
  },
  {
    "text": "rdds so up here I'm playing around with an an array that just has some strings in it obviously that's not big data but",
    "start": "1590960",
    "end": "1597279"
  },
  {
    "text": "maybe I want to play with it I want to see what the algorithms do with it so I've parallelized an array which will shove that out over the network and give",
    "start": "1597279",
    "end": "1603480"
  },
  {
    "text": "me a handle back to that now distributed data in this case I've decided to read a text called",
    "start": "1603480",
    "end": "1610559"
  },
  {
    "text": "fubar.gr um and that'll be read in a parallel fashion and distributed out",
    "start": "1611399",
    "end": "1616640"
  },
  {
    "text": "across the cluster and of course that could be as big as I'm willing to pay to put it in S3 right and that's actually a",
    "start": "1616640",
    "end": "1622480"
  },
  {
    "text": "fairly common thing to do all right so now we need to talk a little bit about transformation so what",
    "start": "1622480",
    "end": "1628880"
  },
  {
    "start": "1625000",
    "end": "3620000"
  },
  {
    "text": "is a transformation so let's take a look at this example this is a this is an rdd representing data that has been",
    "start": "1628880",
    "end": "1635440"
  },
  {
    "text": "partitioned into four different pieces four partitions it's supposed to indicate sort of a log file at a generic",
    "start": "1635440",
    "end": "1641840"
  },
  {
    "text": "level so you got an error warning and info you've got TS which indicates a time stamp and some message right and",
    "start": "1641840",
    "end": "1648320"
  },
  {
    "text": "that would be our base rdd we're going to create that by reading some source of log data right we're going to analyze",
    "start": "1648320",
    "end": "1654720"
  },
  {
    "text": "that now so maybe you want to put a maybe maybe what you're looking for are just the error messages I need to do",
    "start": "1654720",
    "end": "1660960"
  },
  {
    "text": "some analysis on the error messages I'm trying to determine if my nodes are crashing all the time so I'm going to do",
    "start": "1660960",
    "end": "1666279"
  },
  {
    "text": "some analysis of just the error messages in the log file so you may run a filter",
    "start": "1666279",
    "end": "1671720"
  },
  {
    "text": "on it and what you get out of that then is another rdd called errors rdd and",
    "start": "1671720",
    "end": "1676960"
  },
  {
    "text": "this is the dat that it represents now there's something here that's a bit misleading that I'll get to in a",
    "start": "1676960",
    "end": "1683039"
  },
  {
    "text": "minute but that's essentially a transformation and in fact filter is one of the Transformations just like filter is uh",
    "start": "1683039",
    "end": "1690720"
  },
  {
    "text": "you know one of the Transformations we use in scholar collections API Transformations are",
    "start": "1690720",
    "end": "1698880"
  },
  {
    "text": "lazy okay this is the key Point Behind these things if you go back to this diagram over",
    "start": "1698880",
    "end": "1705039"
  },
  {
    "text": "here the misleading part of this diagram is that I am representing these",
    "start": "1705039",
    "end": "1710200"
  },
  {
    "text": "partitions as having data in them but when you create an rdd such as by",
    "start": "1710200",
    "end": "1715799"
  },
  {
    "text": "reading a text file there's no data in there yet at all nothing has happened",
    "start": "1715799",
    "end": "1722000"
  },
  {
    "text": "you've essentially said hey you know somewhere down the line I'm going to read that file it's kind of like creating a lazy",
    "start": "1722000",
    "end": "1729200"
  },
  {
    "text": "file in in Scala if you've done that say lazy Val Source Dot from file and you",
    "start": "1729200",
    "end": "1735519"
  },
  {
    "text": "give it a pointer to a file that doesn't exist you don't get an error you don't get a runtime error when you get the",
    "start": "1735519",
    "end": "1740919"
  },
  {
    "text": "runtime error is when you actually try to read that when you actually try to use that that lazy operator at which",
    "start": "1740919",
    "end": "1746600"
  },
  {
    "text": "point it it goes and and initializes it finds out the file doesn't exist and blows up same thing will happen in spark",
    "start": "1746600",
    "end": "1752960"
  },
  {
    "text": "if you point at a non-existent file it won't tell you right away that it's not there it'll tell you later when you",
    "start": "1752960",
    "end": "1759080"
  },
  {
    "text": "execute the chain of commands Okay so what happens is these Transformations",
    "start": "1759080",
    "end": "1764919"
  },
  {
    "text": "get built up as a a lazy chain which",
    "start": "1764919",
    "end": "1770159"
  },
  {
    "text": "unfortunately in the documentation is called the dag okay um I I think this is",
    "start": "1770159",
    "end": "1775640"
  },
  {
    "text": "unfortunate in that it's an implementation detail that leaked into the documentation probably by way of the",
    "start": "1775640",
    "end": "1781320"
  },
  {
    "text": "academic PhD level papers these guys were writing what they do is as you apply these Transformations they're",
    "start": "1781320",
    "end": "1787480"
  },
  {
    "text": "building up a directed ayylic graph under the covers okay we all know what that is it's just typical computer science data",
    "start": "1787480",
    "end": "1795120"
  },
  {
    "text": "structure makes sense in this case it's directed it doesn't go in a cycle you're building up a graph right and that graph",
    "start": "1795120",
    "end": "1802640"
  },
  {
    "text": "is just in a sense the execution instructions that you're going to to run",
    "start": "1802640",
    "end": "1807720"
  },
  {
    "text": "later okay and literally in the documentation you'll see this referred to as the dag okay so whether the fact",
    "start": "1807720",
    "end": "1813840"
  },
  {
    "text": "that it's leaked into the documentation might be unfortunate but we're stuck with it so you might as well get used to",
    "start": "1813840",
    "end": "1819039"
  },
  {
    "text": "the term they build this execution plan the dag transformation after",
    "start": "1819039",
    "end": "1824880"
  },
  {
    "text": "transformation after transformation after transformation but nothing happens nothing happens",
    "start": "1824880",
    "end": "1830399"
  },
  {
    "text": "until you run an action which we're going to look at in a minute well but if you think about",
    "start": "1830399",
    "end": "1837640"
  },
  {
    "text": "it it's conceptually similar to this okay anyone in here play with scholar collection views at",
    "start": "1837640",
    "end": "1845000"
  },
  {
    "text": "all so if you create a scholar collection like you say array and you give it some bunch of elements or you",
    "start": "1845000",
    "end": "1851080"
  },
  {
    "text": "say like one to 100. to array you get an array back and if you do a filter on",
    "start": "1851080",
    "end": "1856519"
  },
  {
    "text": "that it trans forms it right then and there and gives you back a new filtered array and then if you do a map on the result of that it transforms it right",
    "start": "1856519",
    "end": "1862960"
  },
  {
    "text": "then and there and gives you back a new mapped array if on the other hand you say take that initial array and I'd like",
    "start": "1862960",
    "end": "1869760"
  },
  {
    "text": "to operate on a view of that array then what happens is that each of those filter Transformations the map",
    "start": "1869760",
    "end": "1875639"
  },
  {
    "text": "Transformations whatever they return what amounts to a lazy iterator okay they return what amounts",
    "start": "1875639",
    "end": "1881440"
  },
  {
    "text": "to something that hasn't executed yet and it's not going to be executed until later you do something like to array on",
    "start": "1881440",
    "end": "1888120"
  },
  {
    "text": "it or for each on it or something that causes traversal to happen at which point the the entire chain on that view",
    "start": "1888120",
    "end": "1895440"
  },
  {
    "text": "will be lazily executed you'll also see this in the new Java 8 collections API with lambdas they have that same notion",
    "start": "1895440",
    "end": "1902360"
  },
  {
    "text": "right because it's very efficient and for those python programmers python 2 if you go from python 2 to Python 3 that's",
    "start": "1902360",
    "end": "1908159"
  },
  {
    "text": "sort of the default behavior in Python 3 now this is like anable it's very similar to an I",
    "start": "1908159",
    "end": "1914919"
  },
  {
    "text": "inumerable um it it behaves aves in the same way it's a lazy iterator okay so a",
    "start": "1914919",
    "end": "1920399"
  },
  {
    "text": "view and Scola is a lazy version of a collection um so Transformations are",
    "start": "1920399",
    "end": "1925840"
  },
  {
    "text": "like calling filter or map on a view um I would recommend uh we're not going to do it here but I would recommend pulling",
    "start": "1925840",
    "end": "1931360"
  },
  {
    "text": "up the scolar reppel sometime create yourself an array and do some maps and filters on that and then go back to that",
    "start": "1931360",
    "end": "1937279"
  },
  {
    "text": "original array and say array. View and do some maps and filters on that and see what happens it's read I mean this is",
    "start": "1937279",
    "end": "1943320"
  },
  {
    "text": "one of those things I can talk about in words but as soon as you throw it up in the repple it's immediately obvious what's going on all right yes the driver",
    "start": "1943320",
    "end": "1951559"
  },
  {
    "text": "program that's building up the",
    "start": "1951559",
    "end": "1954720"
  },
  {
    "text": "dag yes right and in fact the the triggering of the dag the execution of",
    "start": "1957320",
    "end": "1962840"
  },
  {
    "text": "the dag which is triggered by an action that's actually what causes the file read right and here's one of the",
    "start": "1962840",
    "end": "1968480"
  },
  {
    "text": "fundamental problems with that people don't realize about spark I wouldn't say it's a problem here's one of the",
    "start": "1968480",
    "end": "1974039"
  },
  {
    "text": "problems that you can get into with spark if you're not aware about how this works I build up this long chain of",
    "start": "1974039",
    "end": "1979600"
  },
  {
    "text": "Transformations and I'm reading out of a 4 gab htfs file let's say and I I run a",
    "start": "1979600",
    "end": "1985200"
  },
  {
    "text": "transformation I I finally I have all these Transformations on there and I run an action and there's an action like",
    "start": "1985200",
    "end": "1991120"
  },
  {
    "text": "count how many records are left count what does count do it's going to to trigger the execution of that dag it's",
    "start": "1991120",
    "end": "1997840"
  },
  {
    "text": "going to trigger a read of the 4 gigabyte file data will start flowing into the nodes down through the",
    "start": "1997840",
    "end": "2003559"
  },
  {
    "text": "partitions the Transformations will be remotely processed bit by bit over and over again finally they get through the",
    "start": "2003559",
    "end": "2010639"
  },
  {
    "text": "whole chain on all the data and the count comes back to the driver okay good",
    "start": "2010639",
    "end": "2015679"
  },
  {
    "text": "it's it's it's a nice size count all right I'm ready to do my real work so now you run another action guess what",
    "start": "2015679",
    "end": "2021880"
  },
  {
    "text": "everything starts all over again okay that read of the data that data that just contributed to that that executed",
    "start": "2021880",
    "end": "2029320"
  },
  {
    "text": "query plan that worked so hard to get you your account all that data is gone now",
    "start": "2029320",
    "end": "2035760"
  },
  {
    "text": "okay no not by default not always um I'll get let me get back to that in a",
    "start": "2035760",
    "end": "2041240"
  },
  {
    "text": "minute when you create a bunch of Transformations so you have you have your initial rdd and then you do like",
    "start": "2041240",
    "end": "2047200"
  },
  {
    "text": "map filter map map map map like a few times right um does does it efficiently",
    "start": "2047200",
    "end": "2054800"
  },
  {
    "text": "combine those Transformations they get run into they get run in the order you give them that's why there are data",
    "start": "2054800",
    "end": "2060040"
  },
  {
    "text": "frames we'll get to that in a minute the answer to your question is that by default nothing is cached but that's kind of sort of a lie um in that there",
    "start": "2060040",
    "end": "2067480"
  },
  {
    "text": "are several kinds of Transformations so there are transformations where no data",
    "start": "2067480",
    "end": "2072720"
  },
  {
    "text": "needs to go across the network so for instance I start out with an rdd of strings okay um let's say what I've done",
    "start": "2072720",
    "end": "2080398"
  },
  {
    "text": "is I've read a line in and I've broken that line up into the words that are in that line so now that second rdd is an",
    "start": "2080399",
    "end": "2086240"
  },
  {
    "text": "rdd of strings words right and what I do is I transform those words into a tupal",
    "start": "2086240",
    "end": "2092079"
  },
  {
    "text": "word one word one I'm going to do this to to do a count that transformation doesn't cause any network traffic",
    "start": "2092079",
    "end": "2097880"
  },
  {
    "text": "because all I'm doing is a local augmentation of information within each node but now let's say okay I've got a",
    "start": "2097880",
    "end": "2103839"
  },
  {
    "text": "bunch of these words all out there and I want to I want to sum them all up I want to collect them all together and group",
    "start": "2103839",
    "end": "2109599"
  },
  {
    "text": "the group The like words with each other well that's a problem right because what I've got is maybe theth a the him over",
    "start": "2109599",
    "end": "2117880"
  },
  {
    "text": "here and I've got her sheth over here and the th all have to get together so",
    "start": "2117880",
    "end": "2123440"
  },
  {
    "text": "there's a certain amount of network traffic that's involved in that sort of trans transformation that particular",
    "start": "2123440",
    "end": "2128520"
  },
  {
    "text": "kind of transformation is called a shuffle you have to shuffle data across the network under the covers as it turns",
    "start": "2128520",
    "end": "2135119"
  },
  {
    "text": "out in order to do the shuffle files are written and those files will hang out",
    "start": "2135119",
    "end": "2140480"
  },
  {
    "text": "there so spark will do a little bit of an optimization if you later run that same dag again spark will notice if the",
    "start": "2140480",
    "end": "2148160"
  },
  {
    "text": "shuffle files are still there it will say ah you know what I don't have to go back to the beginning because all the shuffle files are still there right so",
    "start": "2148160",
    "end": "2154760"
  },
  {
    "text": "there's a case where there is some caching but you can't count on that which is why what a lot of people do is they'll say once I've got my data",
    "start": "2154760",
    "end": "2161319"
  },
  {
    "text": "winnowed down to an rdd I think is a reasonable size and this is the rdd I'm going to be calling a lot of actions on",
    "start": "2161319",
    "end": "2168400"
  },
  {
    "text": "that's the one I'll cash right and if you cash it of course the next the next action that you run the first action",
    "start": "2168400",
    "end": "2174400"
  },
  {
    "text": "that you run will cause spark to go read the data from the data source but as it drops through that cached rdd it's",
    "start": "2174400",
    "end": "2180079"
  },
  {
    "text": "almost like it's falling into a net right at which point later provided you don't run out of cash memory um later",
    "start": "2180079",
    "end": "2187200"
  },
  {
    "text": "when you execute the second action spark will look up and say oh you know what this guy's cashed I don't have to go",
    "start": "2187200",
    "end": "2192280"
  },
  {
    "text": "back to the beginning right but that's your job to choose when to cach that it's not done by default okay but as I",
    "start": "2192280",
    "end": "2199640"
  },
  {
    "text": "said there is sort of a there's sort of a slight optimization under the covers for that one all right so example of an",
    "start": "2199640",
    "end": "2206319"
  },
  {
    "text": "action might be this one okay and this is a horribly named action and I'm probably going to get some grief for",
    "start": "2206319",
    "end": "2212440"
  },
  {
    "text": "saying that when people see this on YouTube but um who has used the collect function",
    "start": "2212440",
    "end": "2219319"
  },
  {
    "text": "inside the Scola collections API anybody in here besides",
    "start": "2219319",
    "end": "2225400"
  },
  {
    "text": "Michael there is a collect function in the scolar collections API and as it turns out there's an analog in the colle",
    "start": "2226359",
    "end": "2232200"
  },
  {
    "text": "in the rdd API and it's not this collect okay the collect that's in the scholar collections API is sort of a combination",
    "start": "2232200",
    "end": "2238960"
  },
  {
    "text": "of map and filter it's a really cool function I recommend you look at it um this function takes um this this collect",
    "start": "2238960",
    "end": "2246000"
  },
  {
    "text": "function method takes a partial function for those who don't know what a partial function that is that's a function that",
    "start": "2246000",
    "end": "2252200"
  },
  {
    "text": "has the case statements from a match Clause but it doesn't cover all the possible cases okay um and when the scholar",
    "start": "2252200",
    "end": "2260160"
  },
  {
    "text": "compiler compiles that it generates among other things a method called is defined at so once you once collect gets",
    "start": "2260160",
    "end": "2268200"
  },
  {
    "text": "this partial function it can take that partial function that Lambda and it can ask it a question it could say hey I got",
    "start": "2268200",
    "end": "2273680"
  },
  {
    "text": "this piece of input are you defined for that input if if not I won't pass this input to you because I'll get an",
    "start": "2273680",
    "end": "2279760"
  },
  {
    "text": "exception right The Scholar collections API uses this so that you can do the following if you call collect on",
    "start": "2279760",
    "end": "2286640"
  },
  {
    "text": "something and you give it a partial function what will happen is the collections API will walk through",
    "start": "2286640",
    "end": "2291800"
  },
  {
    "text": "everything in the collection and for each one it will say are are you defined for this input if not I'm throwing that",
    "start": "2291800",
    "end": "2297839"
  },
  {
    "text": "away it's a filter operation if so I'm going to pass it to you and you can transform it so that collect function",
    "start": "2297839",
    "end": "2304000"
  },
  {
    "text": "which is wicked cool is sort of like a com combined map and filter that's not what this is although there is one of",
    "start": "2304000",
    "end": "2310240"
  },
  {
    "text": "those in here just to confuse you okay and it's a transformation not an action this action is probably one of the more",
    "start": "2310240",
    "end": "2316960"
  },
  {
    "text": "common spark actions what this action does is it says essentially hey look I",
    "start": "2316960",
    "end": "2322240"
  },
  {
    "text": "think I've gotten this rdd down to a small enough size I want to pull that data back into the",
    "start": "2322240",
    "end": "2327800"
  },
  {
    "text": "driver so collect sucks all the data back into the driver so that you can now",
    "start": "2327800",
    "end": "2333520"
  },
  {
    "text": "play with it in an array comes back in an array partion it goes out to all the",
    "start": "2333520",
    "end": "2339040"
  },
  {
    "text": "partitions and says hey the data you have left give it to me goes out to all the nodes sucks all that data into the",
    "start": "2339040",
    "end": "2345640"
  },
  {
    "text": "driver into one big array and those partitions go away are they tied to the",
    "start": "2345640",
    "end": "2350800"
  },
  {
    "text": "data or the I I'm hesitating because we are",
    "start": "2350800",
    "end": "2358200"
  },
  {
    "text": "having a terminology issue conceptually the rdd is still there you still have a handle to it and if you run another",
    "start": "2358200",
    "end": "2364800"
  },
  {
    "text": "action those partitions will filled up the same way they were filled up before but at the point where you run the",
    "start": "2364800",
    "end": "2370520"
  },
  {
    "text": "action and the execution completes um unless caching is in play there's no data in the rdds anymore so those",
    "start": "2370520",
    "end": "2377640"
  },
  {
    "text": "partitions are conceptually still there but they're empty so rdds are rdds are sort of like containers but",
    "start": "2377640",
    "end": "2385960"
  },
  {
    "text": "they might be empty right I kind of think of them as future containers right they're sort of almost like Futures in a",
    "start": "2385960",
    "end": "2393160"
  },
  {
    "text": "way conceptually but not quite um so what's the problem then with that",
    "start": "2393160",
    "end": "2400440"
  },
  {
    "text": "particular guy like a blow up memory yeah that's exactly the problem um you'll hear guys",
    "start": "2400440",
    "end": "2406440"
  },
  {
    "text": "and I don't actually like this term but you'll hear guys say well you know you could o the driver right um it's very",
    "start": "2406440",
    "end": "2412560"
  },
  {
    "text": "common if you if you do collect and you you haven't winnowed your data set down to a sufficiently small amount of data",
    "start": "2412560",
    "end": "2418400"
  },
  {
    "text": "you'll blow the driver memory your HEAP will just get exhausted and the whole thing will just happily go away um so",
    "start": "2418400",
    "end": "2424240"
  },
  {
    "text": "you do have to be careful about this now there are other actions as well but running this then as indicated here",
    "start": "2424240",
    "end": "2429800"
  },
  {
    "text": "we'll pull it back into the driver all right so some some of the actions again mentioning again actions trigger the dag",
    "start": "2429800",
    "end": "2437240"
  },
  {
    "text": "forces the the data to be read again modulo any caching right if you have",
    "start": "2437240",
    "end": "2442599"
  },
  {
    "text": "caching in place the data might not be reread uh there are other things there to consider okay just to make your your",
    "start": "2442599",
    "end": "2449040"
  },
  {
    "text": "minds blow a little bit let's imagine I've implemented i' I've said I'm going to cach this",
    "start": "2449040",
    "end": "2454560"
  },
  {
    "text": "rdd all right but let's suppose that there's more me more data to be cached",
    "start": "2454560",
    "end": "2460200"
  },
  {
    "text": "than there is memory to cach it and I'm using the default caching strategy of memory",
    "start": "2460200",
    "end": "2465440"
  },
  {
    "text": "only anybody want to guess what",
    "start": "2465440",
    "end": "2469480"
  },
  {
    "text": "happens okay so those are the two answers and your answer is correct one answer was the executor Will Will",
    "start": "2472599",
    "end": "2479079"
  },
  {
    "text": "O um no that's not what happens what happens is it stores as much as it can",
    "start": "2479079",
    "end": "2484839"
  },
  {
    "text": "and the rest of it it tosses out using an lru a least recently used algorithm right so that's fine everything recovers",
    "start": "2484839",
    "end": "2491839"
  },
  {
    "text": "nothing dies but here's your issue what happens then later when you run that",
    "start": "2491839",
    "end": "2498079"
  },
  {
    "text": "that dag again because you executed a second action spark will go up and say oh look",
    "start": "2498079",
    "end": "2503720"
  },
  {
    "text": "I don't have to go any higher than this because I'm cached and then this executor will say yeah but I don't have",
    "start": "2503720",
    "end": "2508800"
  },
  {
    "text": "everything so it will actually reinitiate a read of the block of data that to fill in the gaps of the things",
    "start": "2508800",
    "end": "2515520"
  },
  {
    "text": "that weren't cached okay not necessarily problematic unless your data source has changed between",
    "start": "2515520",
    "end": "2522240"
  },
  {
    "text": "runs okay so this is why you can't just say I'll just cash it you have to think",
    "start": "2522240",
    "end": "2527280"
  },
  {
    "text": "very carefully what am I going to cach how much data is likely to be there and what caching strategy do I want to use",
    "start": "2527280",
    "end": "2533760"
  },
  {
    "text": "you know do I want it to spill to dis do I want to spill it do I want to just keep it all in memory there it's I mean",
    "start": "2533760",
    "end": "2538839"
  },
  {
    "text": "there nothing's free right there's always some consequence under the covers that you have to think",
    "start": "2538839",
    "end": "2544240"
  },
  {
    "text": "about all right um the other thing I didn't mention there are actually two kinds of actions we've only talked about",
    "start": "2544880",
    "end": "2550839"
  },
  {
    "text": "once one kind of action is a driver action pulls the data back to the driver examples of that are collect count there",
    "start": "2550839",
    "end": "2557520"
  },
  {
    "text": "are some others count by key is another one these things actually do some calculations and pull data back into the",
    "start": "2557520",
    "end": "2563680"
  },
  {
    "text": "driver memory okay but there's this would be if all actions were like this your program",
    "start": "2563680",
    "end": "2569200"
  },
  {
    "text": "would be very inefficient here's an action that you would not want to be pulling into the driver okay the very",
    "start": "2569200",
    "end": "2575480"
  },
  {
    "text": "last thing I want want to do is save as a text file I've done all this transformation on this incoming S3 data",
    "start": "2575480",
    "end": "2581200"
  },
  {
    "text": "and now I want to dump it as my last resort into hdfs if that were a driver action I'm",
    "start": "2581200",
    "end": "2587720"
  },
  {
    "text": "right back to the driver being a bottleneck for iio again okay so the save as actions all run",
    "start": "2587720",
    "end": "2595559"
  },
  {
    "text": "distributed because that's more efficient and there's another action called for each again that's a scholar",
    "start": "2595559",
    "end": "2601000"
  },
  {
    "text": "collection action right you can run dot for each on any collection that runs distributed so the dot for each will",
    "start": "2601000",
    "end": "2606920"
  },
  {
    "text": "actually run on each individual node and only on the data that happens to be in that rdd on that",
    "start": "2606920",
    "end": "2613119"
  },
  {
    "text": "node so there are different kinds of actions at the rdd",
    "start": "2613119",
    "end": "2618318"
  },
  {
    "text": "level okay and again just just as a reminder calling an action like collect",
    "start": "2618640",
    "end": "2623920"
  },
  {
    "text": "is conceptually similar to calling two array on a view okay or dot for each on",
    "start": "2623920",
    "end": "2629280"
  },
  {
    "text": "a view right if you've transformed a Scola view over and over and over again finally you call to array which causes",
    "start": "2629280",
    "end": "2635640"
  },
  {
    "text": "that thing to need to be traversed in order to produce your array that's like an action conceptually except that's all",
    "start": "2635640",
    "end": "2641800"
  },
  {
    "text": "local to your VM whereas here it's all distributed all right so that's rdds I'm",
    "start": "2641800",
    "end": "2648520"
  },
  {
    "text": "going actually show you a couple of an example in here real quick but before I do any any questions at all about any of",
    "start": "2648520",
    "end": "2655079"
  },
  {
    "text": "this that you haven't already raised and it's okay you've been interrupting me with questions and that's fantastic so",
    "start": "2655079",
    "end": "2661599"
  },
  {
    "text": "um please continue to do that now I actually have to sit down and crane my neck for this part um um so let me uh",
    "start": "2661599",
    "end": "2668119"
  },
  {
    "text": "let me un un full screen this and let's go out",
    "start": "2668119",
    "end": "2675640"
  },
  {
    "text": "here and I'll try to make I think I can make this full",
    "start": "2675640",
    "end": "2680520"
  },
  {
    "text": "screen okay so where I'm going to go here",
    "start": "2681680",
    "end": "2686880"
  },
  {
    "text": "is um this is the data bricks Cloud I'm going to make this bigger but when you when you log into here this is",
    "start": "2687720",
    "end": "2694200"
  },
  {
    "text": "essentially a series of notebooks all",
    "start": "2694200",
    "end": "2699640"
  },
  {
    "text": "right I've got this stuff over here I can take a look at things like what clusters are currently",
    "start": "2700440",
    "end": "2705920"
  },
  {
    "text": "running right now this one's called trainers this is where we do our development for training so there are a",
    "start": "2705920",
    "end": "2711280"
  },
  {
    "text": "bunch of existing clusters out there and I created a new one right here you know it says Brian temporary for meetup",
    "start": "2711280",
    "end": "2718280"
  },
  {
    "text": "that's to tell all these people don't kill this please I'm using it right but I will kill it afterwards and not chew",
    "start": "2718280",
    "end": "2723400"
  },
  {
    "text": "up all your ec2 resources um if I own this I the the way this would work is they would they would create a host for",
    "start": "2723400",
    "end": "2729720"
  },
  {
    "text": "me and then I would hook up my S3 credentials to here so that whenever I spawn a cluster it would come out of my",
    "start": "2729720",
    "end": "2735680"
  },
  {
    "text": "ec2 or my um AWS account but it would run through their product here and they",
    "start": "2735680",
    "end": "2741520"
  },
  {
    "text": "would charge me I think the current Enterprise tier is like 750 a month uh",
    "start": "2741520",
    "end": "2746800"
  },
  {
    "text": "so obviously my small company isn't paying 750 a month for this thing um but they're coming out with a developer tier",
    "start": "2746800",
    "end": "2752520"
  },
  {
    "text": "which if I weren't actually doing work for them and getting getting it for free I would buy um underneath here I could",
    "start": "2752520",
    "end": "2759319"
  },
  {
    "text": "see any tables that were created um jobs that have been run uh where we're going",
    "start": "2759319",
    "end": "2764839"
  },
  {
    "text": "to go is in this area called workspace um this new version has users it would be under here except that this is an",
    "start": "2764839",
    "end": "2770240"
  },
  {
    "text": "older version so where I have put things here is under a directory for me here's the phase Meetup bunch of stuff under",
    "start": "2770240",
    "end": "2777359"
  },
  {
    "text": "here and we're going to take a look at this one for now it's called rdds This is a notebook all right now the",
    "start": "2777359",
    "end": "2784280"
  },
  {
    "text": "interesting thing here is this looks like a repple but they have some special",
    "start": "2784280",
    "end": "2789440"
  },
  {
    "text": "percent commands in here that I can use like percent MD which is anybody want to guess Bingo this a markdown cell um used",
    "start": "2789440",
    "end": "2797839"
  },
  {
    "text": "to be this actually would go back so so what does this really mean and and this notebook environment supports R Python",
    "start": "2797839",
    "end": "2804640"
  },
  {
    "text": "and uh Scala not Java because there's no real reppel although I did by the way find a Java reppel that some guy wrote",
    "start": "2804640",
    "end": "2811800"
  },
  {
    "text": "that actually works pretty well but they didn't integrate it here um what happens is that there's a jvm in the background",
    "start": "2811800",
    "end": "2818400"
  },
  {
    "text": "that um that these this JavaScript is connecting to whenever I run a command and in fact it used to be the jvm in the",
    "start": "2818400",
    "end": "2824880"
  },
  {
    "text": "background was actually used to render the markdown but now they actually do that up here so it's nice and fast so",
    "start": "2824880",
    "end": "2830040"
  },
  {
    "text": "when you get in here you can type whatever code you want just like in a repple up here it's uh it's shift enter",
    "start": "2830040",
    "end": "2835640"
  },
  {
    "text": "to run the thing all right so what we're going to do is um I've got a copy of I",
    "start": "2835640",
    "end": "2840960"
  },
  {
    "text": "went out to um to blebe and pulled down uh Leaves of Grass and ripped out all",
    "start": "2840960",
    "end": "2846920"
  },
  {
    "text": "the metadata which probably violates the license uh and dumped it into S3 and then um this particular environment",
    "start": "2846920",
    "end": "2853800"
  },
  {
    "text": "you're going to see a weird path name it's going to say dbfs colon that's a datab bricks file system all that really",
    "start": "2853800",
    "end": "2860000"
  },
  {
    "text": "is is their front end it's a little mini file system that makes things in S3",
    "start": "2860000",
    "end": "2865160"
  },
  {
    "text": "buckets behave like they're local files so that you can use use all the standard",
    "start": "2865160",
    "end": "2870319"
  },
  {
    "text": "spark local stuff but it gets resolved out of an S3 bucket and so what I did was I I used their tool set to copy out",
    "start": "2870319",
    "end": "2876440"
  },
  {
    "text": "of my S3 bucket into their S3 bucket so that I could create a dbfs mount point for this data so we're going to read",
    "start": "2876440",
    "end": "2882800"
  },
  {
    "text": "this Leaves of Grass text file so if uh let me make this even bigger so here you can see that I'm",
    "start": "2882800",
    "end": "2889319"
  },
  {
    "text": "opening up this text file and I'm getting back in rdd all right and to prove to you that",
    "start": "2889319",
    "end": "2895680"
  },
  {
    "text": "this is completely lazy um I'll show you what happens if I give you the wrong one that cancel thing",
    "start": "2895680",
    "end": "2901800"
  },
  {
    "text": "says it was going into the background so now there's an rdd but nothing's been read yet yet right so let me open this",
    "start": "2901800",
    "end": "2907680"
  },
  {
    "text": "one up you can see the results from the previous run here I didn't clear them out so that's how many now what is that",
    "start": "2907680",
    "end": "2914880"
  },
  {
    "text": "what does that represent anybody want to guess Bingo text file by default breaks",
    "start": "2914880",
    "end": "2923079"
  },
  {
    "text": "up the file that it's reading in lines so I've got files with lines in them all",
    "start": "2923079",
    "end": "2929319"
  },
  {
    "text": "right um so this count tells me how many lines are in the file and let's go back up here and change change this name",
    "start": "2929319",
    "end": "2936760"
  },
  {
    "text": "rerun that cell and go down here and run this it's at that point that I get my",
    "start": "2936760",
    "end": "2942680"
  },
  {
    "text": "input exception okay and it truly is a complete ugly exception right there's",
    "start": "2942680",
    "end": "2949200"
  },
  {
    "text": "the whole stack Trace so it it literally is lazy it did not detect that this path was incorrect",
    "start": "2949200",
    "end": "2956400"
  },
  {
    "text": "until it actually tried to run that count and execute the dag which had one node in it right so let's fix that",
    "start": "2956400",
    "end": "2964520"
  },
  {
    "text": "up all right so there's um what we're going to",
    "start": "2964520",
    "end": "2972319"
  },
  {
    "text": "do is well that's really ugly this was all on one line let me actually uh it's",
    "start": "2973480",
    "end": "2981280"
  },
  {
    "text": "because I made it bigger make that a tiny bit smaller there can you guys still read",
    "start": "2981280",
    "end": "2986960"
  },
  {
    "text": "that I'm actually going to do this this looked great at a higher",
    "start": "2986960",
    "end": "2992839"
  },
  {
    "text": "resolution okay so this is an rdd two rdd Transformations somebody want to",
    "start": "2992839",
    "end": "2998200"
  },
  {
    "text": "explain to me what's going on there well let's take it let's take it",
    "start": "2998200",
    "end": "3004960"
  },
  {
    "text": "apart look at the first one um I'm trimming the input line and then what am I",
    "start": "3004960",
    "end": "3012079"
  },
  {
    "text": "doing I'm splitting it on actually spaces and non-word characters",
    "start": "3012240",
    "end": "3018400"
  },
  {
    "text": "why did I choose that because I examined the data before I actually wrote this thing and noticed that there were things like um like M dashes and stuff in it",
    "start": "3018400",
    "end": "3025799"
  },
  {
    "text": "and commas and I don't want to count those and this is the ugly regular expression that will do that split for me so I've split this into a series of",
    "start": "3025799",
    "end": "3032640"
  },
  {
    "text": "words why did I use flat map and not map so you get a sequence of words and not a",
    "start": "3032640",
    "end": "3038720"
  },
  {
    "text": "sequence exactly um the answer for the for the camera here is that if I didn't",
    "start": "3038720",
    "end": "3044160"
  },
  {
    "text": "use flat map if I just used map what would happen is that each line would be mapped into an array at which point",
    "start": "3044160",
    "end": "3049640"
  },
  {
    "text": "conceptually I'd have an array of array of words I don't want that I want to flatten out that level so that what I",
    "start": "3049640",
    "end": "3055000"
  },
  {
    "text": "have is an rdd of all just the words okay so that that's why flatmap filter",
    "start": "3055000",
    "end": "3060640"
  },
  {
    "text": "what am I doing in filter um in fact let's actually make this even more",
    "start": "3060640",
    "end": "3066079"
  },
  {
    "text": "readable uh I didn't mean to hit that hang on that's right every time you execute",
    "start": "3066760",
    "end": "3073920"
  },
  {
    "text": "one it goes to the next cell I should know that by now turns out that um that there are",
    "start": "3073920",
    "end": "3080400"
  },
  {
    "text": "some words that end up being empty after you do the split so I wanted to get rid of them all right and then I call map to",
    "start": "3080400",
    "end": "3087040"
  },
  {
    "text": "map them all to lowercase so that Capital the and lowercase the will end up getting counted together now um a",
    "start": "3087040",
    "end": "3093240"
  },
  {
    "text": "point I should mention in here if you look at these three things there are we are now up to numerous rdds right we",
    "start": "3093240",
    "end": "3101040"
  },
  {
    "text": "started with one up top this rdd rd. flatmap returns another",
    "start": "3101040",
    "end": "3107400"
  },
  {
    "text": "rdd which I then promptly call filter on which returns a third rdd which I",
    "start": "3107400",
    "end": "3113000"
  },
  {
    "text": "promptly call map on which returns a four rdd which I capture in words.",
    "start": "3113000",
    "end": "3118280"
  },
  {
    "text": "rdd okay there are actually four rdds in the underlying dag I just don't happen",
    "start": "3118280",
    "end": "3123319"
  },
  {
    "text": "to have references to all four of them they're there okay this is a this this",
    "start": "3123319",
    "end": "3128640"
  },
  {
    "text": "directed a cyclic graph this execution plan contains four nodes right there are",
    "start": "3128640",
    "end": "3134359"
  },
  {
    "text": "four um there are four vertices in this graph at the moment but I happen to have only uh you",
    "start": "3134359",
    "end": "3141880"
  },
  {
    "text": "know handles to two of them now I'm going to create another one I create a fifth rdd in this Chain by",
    "start": "3141880",
    "end": "3149079"
  },
  {
    "text": "mapping the words rdd to a tupal this tupal um and this is the classic word",
    "start": "3149079",
    "end": "3155480"
  },
  {
    "text": "count example this is the pattern you always use to count Words Right basically what I'm doing is I'm transforming each word into that word",
    "start": "3155480",
    "end": "3163240"
  },
  {
    "text": "and and and account of that word right so now I have an rdd that has a key",
    "start": "3163240",
    "end": "3168640"
  },
  {
    "text": "value pair in it and so the next step then is that I want to do",
    "start": "3168640",
    "end": "3175920"
  },
  {
    "text": "a reduce operation on this stuff now there are actually two ways to do this I'm showing you the efficient way one",
    "start": "3175920",
    "end": "3182640"
  },
  {
    "text": "common thing is to do first a group by key okay let me get all the th together",
    "start": "3182640",
    "end": "3188319"
  },
  {
    "text": "and and group all this stuff together right so that would cause data to be shuffled across the network so that all",
    "start": "3188319",
    "end": "3193880"
  },
  {
    "text": "theth are in one partition and all the the hyms are in another partition and then you would do a reduce right to add",
    "start": "3193880",
    "end": "3201040"
  },
  {
    "text": "all those ones together and get the actual count right turns out that's pretty inefficient I shuffled a lot of",
    "start": "3201040",
    "end": "3206160"
  },
  {
    "text": "data across the network just to count it so I'm actually doing a little trick here I'm calling a separate function",
    "start": "3206160",
    "end": "3212160"
  },
  {
    "text": "called Reduce by key which is specifically optimized for that use case what reduced by key does is it says look",
    "start": "3212160",
    "end": "3219160"
  },
  {
    "text": "before we Shuffle data there may be five or six th right here in this partition let me do a local sum of those and then",
    "start": "3219160",
    "end": "3226079"
  },
  {
    "text": "I I can I can coales those just those local sums across the network I'm transferring a lot less data that way Su",
    "start": "3226079",
    "end": "3232640"
  },
  {
    "text": "it locally then transfer it across the network and some sum it up again all right so we do a Reduce by key that is",
    "start": "3232640",
    "end": "3238960"
  },
  {
    "text": "also a transformation okay does anybody out there not understand this goofy",
    "start": "3238960",
    "end": "3244079"
  },
  {
    "text": "syntax the new the new scholar people okay you don't get that syntax yet kind",
    "start": "3244079",
    "end": "3249319"
  },
  {
    "text": "of all right so the easiest way to describe that syntax is as follows and I'll use this",
    "start": "3249319",
    "end": "3256880"
  },
  {
    "text": "reppel just follow it's easy to explain I can show you exactly what it is I'm going to create an array",
    "start": "3257200",
    "end": "3264079"
  },
  {
    "text": "here a equals 1 to 10 to array all right and",
    "start": "3264079",
    "end": "3273240"
  },
  {
    "text": "what I want to do with that array having created that I want to map it to an array of",
    "start": "3273240",
    "end": "3279040"
  },
  {
    "text": "strings okay um actually no let's let's use reduce as an example I want to sum",
    "start": "3279040",
    "end": "3284880"
  },
  {
    "text": "it up I want to do a reduce so I could do this reduce all right so what goes into",
    "start": "3284880",
    "end": "3290760"
  },
  {
    "text": "reduce a reduction function what does that reduction function take two arguments I of type int J of type int i+",
    "start": "3290760",
    "end": "3301319"
  },
  {
    "text": "J okay if I run that I get a sum that is the formal definition of this Lambda",
    "start": "3301319",
    "end": "3307400"
  },
  {
    "text": "however Scala knows what these types are why because it knows what kind of",
    "start": "3307400",
    "end": "3312839"
  },
  {
    "text": "function what kind of array is being passed into reduce the compiler knows that so I'm allowed to do this I can",
    "start": "3312839",
    "end": "3320079"
  },
  {
    "text": "leave the type signature off for these variables and just put them in like that okay with me so far all right still",
    "start": "3320079",
    "end": "3328039"
  },
  {
    "text": "works right that works because it can infer the types as it happens there's another compiler optimization the",
    "start": "3328039",
    "end": "3333760"
  },
  {
    "text": "compiler says you know what unless you're wrapping this stuff up in weird parentheses that confuse me the",
    "start": "3333760",
    "end": "3339960"
  },
  {
    "text": "compiler if you are only referring to these arguments once inside the Lambda",
    "start": "3339960",
    "end": "3346599"
  },
  {
    "text": "then you can you don't even need this part you can just replace them with underscores that's the first argument",
    "start": "3346599",
    "end": "3352440"
  },
  {
    "text": "that's the second argument okay that's that's all it is so that's the deconstruction this looks",
    "start": "3352440",
    "end": "3359520"
  },
  {
    "text": "really really weird to people when they first see this and and what what I've often heard is God why are you scholar",
    "start": "3359520",
    "end": "3365880"
  },
  {
    "text": "people doing this right um and and so the answer is that this is like anything",
    "start": "3365880",
    "end": "3371319"
  },
  {
    "text": "else right it doesn't take really long to understand this it's two arguments",
    "start": "3371319",
    "end": "3377359"
  },
  {
    "text": "it's very clear you're adding them together right now if I had five of them I probably would not use this underscore",
    "start": "3377359",
    "end": "3383599"
  },
  {
    "text": "convention because then I it's ridiculous to try to follow what particularly if I'm doing you know a number of different operations but for",
    "start": "3383599",
    "end": "3389880"
  },
  {
    "text": "two or sometimes even three your your typical experienced scholar programmer will look at that and say of course I",
    "start": "3389880",
    "end": "3396359"
  },
  {
    "text": "understand that that makes sense perfect sense what what value does it give you how much harder is it how much easier is it to type a number Square than well so",
    "start": "3396359",
    "end": "3403119"
  },
  {
    "text": "the answer to that is um in the reppel it's of extraordinary value okay okay um",
    "start": "3403119",
    "end": "3408160"
  },
  {
    "text": "but even in code the SC the standard Scola ethos is generally less code is",
    "start": "3408160",
    "end": "3413359"
  },
  {
    "text": "better until it becomes unreadable but sometimes and the until is often",
    "start": "3413359",
    "end": "3418480"
  },
  {
    "text": "ignored a matter of well so like if you look at Scola Z the until it becomes unreadable is generally ignored right",
    "start": "3418480",
    "end": "3425440"
  },
  {
    "text": "okay um but but in but in general um most and and the the guys with data",
    "start": "3425440",
    "end": "3430680"
  },
  {
    "text": "bricks for instance and on the spark team they have a huge um style guide that actually says you know uh we're",
    "start": "3430680",
    "end": "3436640"
  },
  {
    "text": "going to reject this code if it's unreadable right so they probably won't even go above two I don't find that the",
    "start": "3436640",
    "end": "3442680"
  },
  {
    "text": "least bit unreadable because I've been doing scholar for seven years and you get used to it but again it's like anything else if you've programmed",
    "start": "3442680",
    "end": "3448960"
  },
  {
    "text": "python when do you use a Lambda and when do you don't use a Lambda well when they become unreadable you stop using lambdas",
    "start": "3448960",
    "end": "3455000"
  },
  {
    "text": "and you make defs right Lambda is a particularly ugly syntax but it's okay in places this is the same sort of thing",
    "start": "3455000",
    "end": "3461880"
  },
  {
    "text": "all right so going back up to here hey Brian I saw a picture of od's keyboard actually has 12 underscore keys on yeah",
    "start": "3461880",
    "end": "3468559"
  },
  {
    "text": "I'm sure it does do you get many dates at the bar with that",
    "start": "3468559",
    "end": "3473799"
  },
  {
    "text": "joke try try the ve okay so what's happening here is that",
    "start": "3473799",
    "end": "3480280"
  },
  {
    "text": "I'm I'm basically saying Reduce by key and my reduction algorithm is to take the values what reduced by key does is",
    "start": "3480280",
    "end": "3487039"
  },
  {
    "text": "it collects the keys and then hands the values into the reduction function so I'm adding the values together and then",
    "start": "3487039",
    "end": "3493200"
  },
  {
    "text": "I'm saying I want to do a sort on these values because I want to show them in descending order so there's another transformation called sort by that takes",
    "start": "3493200",
    "end": "3500079"
  },
  {
    "text": "a Lambda as well first argument is a Lambda this is really horrible for those",
    "start": "3500079",
    "end": "3505440"
  },
  {
    "text": "of you who don't understand Scala um this combines two unreadable things the underscore Syntax for parameter passing",
    "start": "3505440",
    "end": "3513119"
  },
  {
    "text": "with the underscore reference to an element of a tupal okay because what comes in to sort by then is a tupal of",
    "start": "3513119",
    "end": "3519280"
  },
  {
    "text": "two elements and and the most compact notation for accessing the second element of a two Tuple is underscore two",
    "start": "3519280",
    "end": "3526680"
  },
  {
    "text": "so what am I doing here I'm saying I want to sort by the second ele element of the tupo namely the count I want to",
    "start": "3526680",
    "end": "3532839"
  },
  {
    "text": "sort in descending order okay again this is a transformation I still haven't executed a dag I'm just",
    "start": "3532839",
    "end": "3540720"
  },
  {
    "text": "I'm building up the Transformations then I say take 100 that's an action give me the top 100",
    "start": "3540720",
    "end": "3548319"
  },
  {
    "text": "elements now I could have pulled all this data back into the driver and done these sorts but why would I want to do",
    "start": "3548319",
    "end": "3554440"
  },
  {
    "text": "that I want to do as much on the cluster as I can right so what I've done here is I've done everything on the cluster up",
    "start": "3554440",
    "end": "3559640"
  },
  {
    "text": "to this take this pulls back the top 10 uh top 100 elements",
    "start": "3559640",
    "end": "3565359"
  },
  {
    "text": "and then I call and then it's just a collection then it's an array okay so this final four each is not a a spark",
    "start": "3565359",
    "end": "3571599"
  },
  {
    "text": "action at all this is the standard collection API for each that's looping over each element all right and I'm",
    "start": "3571599",
    "end": "3578200"
  },
  {
    "text": "passing that into a partial function why am I using a partial function there because that way I don't have to use that underscore Tuple nonsense I can",
    "start": "3578200",
    "end": "3585079"
  },
  {
    "text": "decode it into two nice easy to read variables and I'm just printing them out and now we can see that it's no big",
    "start": "3585079",
    "end": "3591000"
  },
  {
    "text": "surprise that the' is the most popular word in there but if you've ever read any Walt Whitman poetry you'll not be at",
    "start": "3591000",
    "end": "3597640"
  },
  {
    "text": "all surprised to find especially if you've read Leaves of Grass that the fourth one in there is I okay um he he",
    "start": "3597640",
    "end": "3604680"
  },
  {
    "text": "uses the first person a lot in in that particular collection so you know not not terribly interesting as far as big",
    "start": "3604680",
    "end": "3610920"
  },
  {
    "text": "data processing goes but it gives you a flavor for how you use rdds all right",
    "start": "3610920",
    "end": "3616839"
  },
  {
    "text": "with me so far we're still not to streaming yet right so I'm going to do a brief um A Brief Review of data frames",
    "start": "3616839",
    "end": "3624680"
  },
  {
    "start": "3620000",
    "end": "4709000"
  },
  {
    "text": "we can take five minute bathroom break then and then dive into streaming if you want okay this might run a little past",
    "start": "3624680",
    "end": "3630799"
  },
  {
    "text": "the hour and a half I allocated if you if you get bored or have kids at home or whatever you have to leave I won't be",
    "start": "3630799",
    "end": "3635880"
  },
  {
    "text": "offended all right so data frames really brief introduction to data frames they've been there since 1.3 they're",
    "start": "3635880",
    "end": "3642799"
  },
  {
    "text": "pretty much solidified now they're conceptually based on pandas in Python and the data frames in R um what are",
    "start": "3642799",
    "end": "3650200"
  },
  {
    "text": "they they are a higher level abstraction so one of the problems you have with rdds is",
    "start": "3650200",
    "end": "3655440"
  },
  {
    "text": "The Scholar ones are really pretty fast the python ones are really pretty slow",
    "start": "3655440",
    "end": "3660760"
  },
  {
    "text": "why well with the python rdds you can pass lambdas in Python functions but this is python this isn't jython",
    "start": "3660760",
    "end": "3667760"
  },
  {
    "text": "something has to be able to execute those lambdas and access the python data structures so what happens is that spark",
    "start": "3667760",
    "end": "3673599"
  },
  {
    "text": "actually says oh you're using Python and on each one of the nodes it spins up a python JV uh python VM next to the jvm",
    "start": "3673599",
    "end": "3681000"
  },
  {
    "text": "and whenever it has to do python specific stuff it ships that stuff over to the python VM and back okay not",
    "start": "3681000",
    "end": "3687760"
  },
  {
    "text": "exceptionally quick okay um but if all you're doing is examining data Maybe",
    "start": "3687760",
    "end": "3693079"
  },
  {
    "text": "it's okay right oh I'm doing some ETL here or I'm doing some you know dat some data analysis or data sciency stuff um",
    "start": "3693079",
    "end": "3701440"
  },
  {
    "text": "but in general those of you in here are going to be much happier with spark because you're all going to be using it",
    "start": "3701440",
    "end": "3706880"
  },
  {
    "text": "from Scala so what data frames do though is that we don't pass lambdas around and we don't pass data structures around",
    "start": "3706880",
    "end": "3713319"
  },
  {
    "text": "what the API looks more like like I'm going to show you in a minute it looks more like SQL and in fact it's tied to",
    "start": "3713319",
    "end": "3719599"
  },
  {
    "text": "SQL spark SQL and data frames are fundamentally tied together and the data frame um the declarative data frame API",
    "start": "3719599",
    "end": "3726039"
  },
  {
    "text": "looks a lot like SQL calls or at least conceptually sort",
    "start": "3726039",
    "end": "3731079"
  },
  {
    "text": "of and I'm not going to go into this in any detail you can find more detail about this online but I will mention",
    "start": "3731079",
    "end": "3736279"
  },
  {
    "text": "that what happens is that with data frames what the what the calls to these SQL is kinds of of functions are really",
    "start": "3736279",
    "end": "3743960"
  },
  {
    "text": "doing they're essentially building like a SQL a under the covers or a query plan think",
    "start": "3743960",
    "end": "3750440"
  },
  {
    "text": "of it as a query plan right when that query plan is finally realized which is",
    "start": "3750440",
    "end": "3756119"
  },
  {
    "text": "basically you're calling an action there is an Optimizer a query Optimizer that is very similar",
    "start": "3756119",
    "end": "3763039"
  },
  {
    "text": "conceptually to the query optimizers in rdbms's that figures out how to",
    "start": "3763039",
    "end": "3768240"
  },
  {
    "text": "implement those data frame operations in terms of rdds and it turns out there are 60 types of rdds it uses like three of",
    "start": "3768240",
    "end": "3775599"
  },
  {
    "text": "them okay and then it calculates a a couple of physical plans out of that",
    "start": "3775599",
    "end": "3781240"
  },
  {
    "text": "does a cost optimization chooses the appropriate physical plan based on the C cost",
    "start": "3781240",
    "end": "3787359"
  },
  {
    "text": "optimization executes that physical plan in terms of rdds this turns out to have a couple of benefits benefit number one",
    "start": "3787359",
    "end": "3794079"
  },
  {
    "text": "is you don't have to do all that low-level rdd stuff it's sort of like a higher level language benefit number two is it doesn't matter what language",
    "start": "3794079",
    "end": "3799839"
  },
  {
    "text": "you're programming in because this declarative API is just building a query plan the query plan ends up being implemented purely on the",
    "start": "3799839",
    "end": "3806400"
  },
  {
    "text": "jvm okay so now that penalty that the performance penalty I mentioned where you use Python and it's dramatically",
    "start": "3806400",
    "end": "3812640"
  },
  {
    "text": "slower because it's shipping data back and forth to these python VMS it goes away because all you're doing is",
    "start": "3812640",
    "end": "3818160"
  },
  {
    "text": "describing what you want to do and this Catalyst Optimizer under the covers is implementing what you want to do in",
    "start": "3818160",
    "end": "3824119"
  },
  {
    "text": "terms of its chosen rdds turns out it can do other things too like predicate push down it notices for instance that",
    "start": "3824119",
    "end": "3830880"
  },
  {
    "text": "you're you're doing a join up here and then filtering the data for instance and it says you know what that join would be",
    "start": "3830880",
    "end": "3836119"
  },
  {
    "text": "a lot more efficient if I move the filter up here so it will do that right it can even be smart enough to say hey",
    "start": "3836119",
    "end": "3842359"
  },
  {
    "text": "you know what you're um you're doing that filter operation on um on an rdbms",
    "start": "3842359",
    "end": "3848279"
  },
  {
    "text": "I can shove that down into SQL or you're doing the filter operation in parquet on",
    "start": "3848279",
    "end": "3853720"
  },
  {
    "text": "a parquet file I can actually just shove all that operations all those operations right down into the parquet file I don't",
    "start": "3853720",
    "end": "3859640"
  },
  {
    "text": "have to do them at all right so it's it's actually really really smart um and and the the performance characteristics",
    "start": "3859640",
    "end": "3866520"
  },
  {
    "text": "of this are even if you hand code rdds the likelihood is you're not going to get as fast as a catalyst Optimizer I",
    "start": "3866520",
    "end": "3873079"
  },
  {
    "text": "always describe it like this there was a point back when I first started writing C many many years ago where you would sometimes escape to assembler because it",
    "start": "3873079",
    "end": "3879440"
  },
  {
    "text": "made things faster but then somewhere along the line we hit this inflection point where the the compiler optimizers",
    "start": "3879440",
    "end": "3887200"
  },
  {
    "text": "got to be so good that if you wanted to beat the compiler optimized assembler",
    "start": "3887200",
    "end": "3893000"
  },
  {
    "text": "you would have to spend days crafting your assembly code and it just wasn't worth it anymore and so that's sort of",
    "start": "3893000",
    "end": "3898799"
  },
  {
    "text": "where we're heading with this it's you can always still use rdds but it's going to come to a point where this just",
    "start": "3898799",
    "end": "3904240"
  },
  {
    "text": "really like there's no point this thing's going to do a better job than you than you can so why not use",
    "start": "3904240",
    "end": "3909279"
  },
  {
    "text": "it so they they're becoming the preferred interface to spark in fact our teaching generally says this is what you",
    "start": "3909279",
    "end": "3916240"
  },
  {
    "text": "want to use now the reason we still talk about rdds is because sometimes you need to use them to bridge into Data",
    "start": "3916240",
    "end": "3923079"
  },
  {
    "text": "frames so they're schema oriented this is the first thing they have a schema data",
    "start": "3923079",
    "end": "3929720"
  },
  {
    "text": "frames have a schema what is a schema a schema is a set of name columns",
    "start": "3929720",
    "end": "3934960"
  },
  {
    "text": "with types that's what a schema is okay so if your data source has a",
    "start": "3934960",
    "end": "3940880"
  },
  {
    "text": "self-describing schema you can jump right to data frames and not even touch an rdd an example of files that have",
    "start": "3940880",
    "end": "3946920"
  },
  {
    "text": "self-describing schemas par a par is a column oriented um file",
    "start": "3946920",
    "end": "3953400"
  },
  {
    "text": "type Apache par par where um and it's very efficient the column access is very efficient in par but those columns have",
    "start": "3953400",
    "end": "3960039"
  },
  {
    "text": "names and types already already has a schema don't even need to do rdds here",
    "start": "3960039",
    "end": "3965319"
  },
  {
    "text": "rdbms is obviously Hive which is basically just an rdbm sitting on top of",
    "start": "3965319",
    "end": "3970599"
  },
  {
    "text": "htfs again Json it does something very interesting with Json it will infer the schema by sampling the Json data and",
    "start": "3970599",
    "end": "3977920"
  },
  {
    "text": "then looking at each Json record and trying to figure out you know what the name and types are um and CSV it's not",
    "start": "3977920",
    "end": "3984799"
  },
  {
    "text": "native but there's a third party package that you can drop in where it'll use the header and try to infer the schema out",
    "start": "3984799",
    "end": "3991079"
  },
  {
    "text": "of that so yes doesn't it also support orc as as of 1.4 as well um yeah I",
    "start": "3991079",
    "end": "3997240"
  },
  {
    "text": "believe it does yeah these are just the standard list of things that we mentioned there's um there are more and",
    "start": "3997240",
    "end": "4002760"
  },
  {
    "text": "more there are loads of things out there between what it supports natively and what's in spark packages. org there's a",
    "start": "4002760",
    "end": "4008200"
  },
  {
    "text": "lot of stuff that it will read yeah right I know some time ago there was sharp and Spark SQL and like",
    "start": "4008200",
    "end": "4016039"
  },
  {
    "text": "are those kind of those all those things are those names now gone and they're all part of spark SQL and data frames are",
    "start": "4016039",
    "end": "4021440"
  },
  {
    "text": "now um essentially the same thing so um so if you want to think about spark SQL what's the difference between spark SQL",
    "start": "4021440",
    "end": "4027799"
  },
  {
    "text": "and data frames when you create a data frame you have it's you have this thing in your hand that that any calls you",
    "start": "4027799",
    "end": "4033839"
  },
  {
    "text": "make on it are going to contribute to this underlying query plan so what about SQL so with SQL you give a SQL statement",
    "start": "4033839",
    "end": "4039839"
  },
  {
    "text": "it parses that SQL statement into a SQL a which it then translates into that",
    "start": "4039839",
    "end": "4045279"
  },
  {
    "text": "query plan so SQL statements in spark are fundamentally backed by data frames",
    "start": "4045279",
    "end": "4050359"
  },
  {
    "text": "okay they are in fact one of the cool things is is that you can basically say I've got this data frame which is sort",
    "start": "4050359",
    "end": "4056680"
  },
  {
    "text": "of an abstraction on top of some rdds I want to create I want to make a call on that data frame which is called register",
    "start": "4056680",
    "end": "4063000"
  },
  {
    "text": "temp table and it doesn't really register a temporary table all it really does is it says in a sense hey I you",
    "start": "4063000",
    "end": "4069200"
  },
  {
    "text": "know I want to be able to do SQL queries on this data frame so I'd like to give it a name that that is visible in SQL",
    "start": "4069200",
    "end": "4074440"
  },
  {
    "text": "land it doesn't create a table it doesn't persist anything but in effect it gives it a name and now you can do",
    "start": "4074440",
    "end": "4079520"
  },
  {
    "text": "select queries against the thing um and is it really selecting against a table no it's building more stuff on on the",
    "start": "4079520",
    "end": "4085160"
  },
  {
    "text": "data frames right so you can flip back and forth between SQL and the declarative API which is kind of cool",
    "start": "4085160",
    "end": "4090599"
  },
  {
    "text": "because some problems you know you can read them fairly well in the declarative API but then the thing starts to get ugly to read and you say you know if I",
    "start": "4090599",
    "end": "4097000"
  },
  {
    "text": "could do this in SQL it'd be a lot easier to read oh I can do it in SQL so it it's basically choose whichever one",
    "start": "4097000",
    "end": "4103238"
  },
  {
    "text": "makes sense um when I was teaching this class at spark Summit West when the guy there was",
    "start": "4103239",
    "end": "4108278"
  },
  {
    "text": "one group of people this like 150 people um I'm walking around and one guy says I wish you'd show me the squl stuff",
    "start": "4108279",
    "end": "4114199"
  },
  {
    "text": "earlier this is awesome okay so you're a squel guy I get it not a programmer a squel guy real happy to find the SQL",
    "start": "4114199",
    "end": "4121238"
  },
  {
    "text": "interface um totally irrelevant to him that it did the exact same thing as the other one we looked at okay so what it",
    "start": "4121239",
    "end": "4127600"
  },
  {
    "text": "does is it pro it provides functions like this select filter Group by um",
    "start": "4127600",
    "end": "4132679"
  },
  {
    "text": "count things like that and count different kind of count than the one that pulls stuff back so um it it really",
    "start": "4132679",
    "end": "4138798"
  },
  {
    "text": "does look an awful lot like um like a declarative SQL it now this is this even requires",
    "start": "4138799",
    "end": "4146120"
  },
  {
    "text": "less code usually than rdd so if we flip back to the average right so there's our",
    "start": "4146120",
    "end": "4151400"
  },
  {
    "text": "there's our rdd average against right up here that's how you do it in data",
    "start": "4151400",
    "end": "4158199"
  },
  {
    "text": "frames I mean it's it's much smaller and it's if you that's not terribly hard to read if you if you know the apis and if",
    "start": "4158199",
    "end": "4165440"
  },
  {
    "text": "that you find that ugly to read you could just as easily get the initial data frame here which I'm notice I'm",
    "start": "4165440",
    "end": "4171199"
  },
  {
    "text": "reading a park file um map that to a temporary table and convert that to a SQL statement it would it would do the",
    "start": "4171199",
    "end": "4177359"
  },
  {
    "text": "exact same thing right so it it even reduces the amount of code you have to write which",
    "start": "4177359",
    "end": "4182440"
  },
  {
    "text": "as scholar programmers we love that right the less I have to type the happier a guy I am um but that's not",
    "start": "4182440",
    "end": "4189719"
  },
  {
    "text": "just an ethos right I mean I like the fact that when I'm reading scholar code I'm not waiting through all that Java boiler plate that that's there for no",
    "start": "4189719",
    "end": "4196640"
  },
  {
    "text": "other reason than fear right oh my God I better put an accessor there because I might I can't make it public and I got",
    "start": "4196640",
    "end": "4202560"
  },
  {
    "text": "all these weeds to walk through to get to your your business logic right so that's one reason we like less code but",
    "start": "4202560",
    "end": "4208080"
  },
  {
    "text": "the other reason is that it's very common in spark to do what we often do in in Scala hey I got to experiment with",
    "start": "4208080",
    "end": "4214560"
  },
  {
    "text": "this thing what's the fastest way to experiment with this fire up the repple tap around right so data scientists and",
    "start": "4214560",
    "end": "4219880"
  },
  {
    "text": "even developers will often say before I start writing the the chain of transform for this particular piece of data I need",
    "start": "4219880",
    "end": "4226400"
  },
  {
    "text": "to explore the data before I decide what machine learning algorithm to to apply to this thing and and what what",
    "start": "4226400",
    "end": "4232960"
  },
  {
    "text": "hyperparameters to give it I need to look at the data and do some ETL on it you know I'm not writing programs to do",
    "start": "4232960",
    "end": "4238199"
  },
  {
    "text": "that I'm going to do it in the repple and and the less I have to type the faster my job is going to go I notice a",
    "start": "4238199",
    "end": "4243880"
  },
  {
    "text": "distinct lack of underscores in the data frame example I can put them in if you [Laughter]",
    "start": "4243880",
    "end": "4249760"
  },
  {
    "text": "want um there are some things I will show you in there that that they do have",
    "start": "4249760",
    "end": "4255159"
  },
  {
    "text": "like here okay so here's an example where we're reading",
    "start": "4255159",
    "end": "4260960"
  },
  {
    "text": "a parquet file so again as I said par has a self-describing schema so I can go",
    "start": "4260960",
    "end": "4266000"
  },
  {
    "text": "I can go great right to data frames because it can look in the paret meta data and determine what the columns and",
    "start": "4266000",
    "end": "4271480"
  },
  {
    "text": "their types are right so here I've got a data frame that has been built out of this parquet file um I I do a filter on",
    "start": "4271480",
    "end": "4279719"
  },
  {
    "text": "that now you see that right there DF sub AG this is part of the",
    "start": "4279719",
    "end": "4285360"
  },
  {
    "text": "DSL that in in Scala anyway that wraps the data frames so this this actually",
    "start": "4285360",
    "end": "4290760"
  },
  {
    "text": "gets translated into a query if you've used the Slick API it's the same kind of idea you've got these Expressions but",
    "start": "4290760",
    "end": "4296360"
  },
  {
    "text": "those expressions don't actually execute anything they're captured as part of the DSL and and they become part of the",
    "start": "4296360",
    "end": "4301880"
  },
  {
    "text": "query plan this isable link right um I I will not claim",
    "start": "4301880",
    "end": "4308080"
  },
  {
    "text": "to have spent happily I I will not claim to have spent very much time playing with link on Microsoft but my",
    "start": "4308080",
    "end": "4314360"
  },
  {
    "text": "understanding is that it is yes um now to your point I can introduce something",
    "start": "4314360",
    "end": "4319679"
  },
  {
    "text": "here that's more underscore if you'd like okay so where I have DF peren age",
    "start": "4319679",
    "end": "4325920"
  },
  {
    "text": "they've also introduced in in the Scola API a string interpolator we've all seen",
    "start": "4325920",
    "end": "4331040"
  },
  {
    "text": "those right s quote F quote things like that they've introduced one called dollar um why because we don't want to",
    "start": "4331040",
    "end": "4337920"
  },
  {
    "text": "type a lot when we're in the repple so instead of saying DF peren age peren I can just say dollar quote age",
    "start": "4337920",
    "end": "4344920"
  },
  {
    "text": "okay so you can get almost pearish if you really want with",
    "start": "4344920",
    "end": "4350440"
  },
  {
    "text": "this all right and uh there was an extra line in here that I need to remove that Line's that rdd does not belong",
    "start": "4351239",
    "end": "4357920"
  },
  {
    "text": "there now there isn't there is a problem okay let me back up so I'm not giving away the story yet there is a problem",
    "start": "4357920",
    "end": "4364480"
  },
  {
    "text": "what if you're reading a file that does not have a self-describing schema so",
    "start": "4364480",
    "end": "4370320"
  },
  {
    "text": "let's say you're reading a it's sort of like a CSV file it's not a true CSV file it's just comma separated doesn't have",
    "start": "4370320",
    "end": "4376239"
  },
  {
    "text": "quoting and any of that stuff the government produces thousands of these kinds of files right and a lot of times they don't have headers that's not",
    "start": "4376239",
    "end": "4382840"
  },
  {
    "text": "self-describing you can't say to the software hey read that and infer the schema but what you can do is you can",
    "start": "4382840",
    "end": "4388480"
  },
  {
    "text": "say you know look I look at that there's obviously human readable schema in there the software isn't smart enough to pick",
    "start": "4388480",
    "end": "4394800"
  },
  {
    "text": "it out but I can tell that that's a first name and that's an age and and that's a gender so the thing does have a",
    "start": "4394800",
    "end": "4399840"
  },
  {
    "text": "schema it's just not self-describing turns out it's pretty easy to teach the data frame API what the schema is this",
    "start": "4399840",
    "end": "4407000"
  },
  {
    "text": "is a case where why you need to know rdds right because you have to start with an rdd and",
    "start": "4407000",
    "end": "4413159"
  },
  {
    "text": "then here's one of several ways to do it for scholar programmers this turns out to be the easiest way I start out with",
    "start": "4413159",
    "end": "4418920"
  },
  {
    "text": "this rdd I've got an rdd of lines okay I have this case class my I happen to know",
    "start": "4418920",
    "end": "4424719"
  },
  {
    "text": "this particular CSV file has a person's first name gender and age okay so",
    "start": "4424719",
    "end": "4430560"
  },
  {
    "text": "there's my case class that represents that I take that rdd I map each line I",
    "start": "4430560",
    "end": "4435960"
  },
  {
    "text": "split it into columns and I use those columns to initiate a case class so what comes out of here is an rdd of type",
    "start": "4435960",
    "end": "4442159"
  },
  {
    "text": "person as opposed to an rdd of type string lines right at which point I can",
    "start": "4442159",
    "end": "4447239"
  },
  {
    "text": "say hey Spark make that into a data frame and what spark will do is um it",
    "start": "4447239",
    "end": "4453199"
  },
  {
    "text": "doesn't exactly it kind of uses reflection but it doesn't actually reflect on the data um for for the",
    "start": "4453199",
    "end": "4460960"
  },
  {
    "text": "advanced people in the room and I suspect only two or three of you're going to be familiar with this but what it uses is an is a manifest evidence",
    "start": "4460960",
    "end": "4467360"
  },
  {
    "text": "parameter which is a way that um that you may not be aware of this but everybody knows about type eraser in the",
    "start": "4467360",
    "end": "4472760"
  },
  {
    "text": "jvm right you know who's responsible for type eraser ODI yes yeah ODI is actually",
    "start": "4472760",
    "end": "4480000"
  },
  {
    "text": "yeah generics well he wrote pizza because he was and and they said hey we like your compiler and then son took it",
    "start": "4480000",
    "end": "4486760"
  },
  {
    "text": "and ran with it and we're stuck with generics so what the scholar guys have done is they've said we're still stuck with generics but you can add this in",
    "start": "4486760",
    "end": "4493239"
  },
  {
    "text": "licit it's it's kind of like an implicit really what it is is something you add to the type signature for a generic and",
    "start": "4493239",
    "end": "4499360"
  },
  {
    "text": "you can you're basically instructing the compiler capture the generic type information and stash it away in the",
    "start": "4499360",
    "end": "4504920"
  },
  {
    "text": "object somewhere and then you can query it at runtime so what happens is that 2df is defined to have one of those",
    "start": "4504920",
    "end": "4511000"
  },
  {
    "text": "things on it which means that at runtime then the code can actually say Okay pull out that generic stuff that was passed",
    "start": "4511000",
    "end": "4517080"
  },
  {
    "text": "into me um and it can dig out the name of type string the gender of type string",
    "start": "4517080",
    "end": "4523440"
  },
  {
    "text": "and age of type in uh and and infer the schema from the case class so it's a",
    "start": "4523440",
    "end": "4528600"
  },
  {
    "text": "really trivial way to take an rdd that has a schema but want not one spark and discover and in a couple lines of code",
    "start": "4528600",
    "end": "4535360"
  },
  {
    "text": "start with an rdd apply a case class and bootstrap up into Data frames yes is",
    "start": "4535360",
    "end": "4541280"
  },
  {
    "text": "that why there was a struct type in the data frames API as well that's different um I said there were one of several ways",
    "start": "4541280",
    "end": "4547480"
  },
  {
    "text": "to to to explain uh to to teach the data frame API about um about the schema case",
    "start": "4547480",
    "end": "4554159"
  },
  {
    "text": "class is one way but there is a struct type with struct field which is another way okay it's just another way to do it",
    "start": "4554159",
    "end": "4560480"
  },
  {
    "text": "um if it's not for instance possible to create a case class and why wouldn't that be",
    "start": "4560480",
    "end": "4566120"
  },
  {
    "text": "possible you're not using scholar no no let's say even in scholar there's a case where you can't make you can't they",
    "start": "4566120",
    "end": "4572159"
  },
  {
    "text": "can't inherit from no no you can't make a case class with more",
    "start": "4572159",
    "end": "4577440"
  },
  {
    "text": "than 22 parameters right because case class unapply methods decode into options of tup that many AR well but and",
    "start": "4577440",
    "end": "4584880"
  },
  {
    "text": "you're right so the argument is why the heck do you want 22 um fields in a case",
    "start": "4584880",
    "end": "4589920"
  },
  {
    "text": "class but the problem of course is that I can think of a data set right off the box right out of the box there's a",
    "start": "4589920",
    "end": "4595120"
  },
  {
    "text": "government data set that comes from the national plan and provider enumeration system used by Medicare that lists all",
    "start": "4595120",
    "end": "4601400"
  },
  {
    "text": "the the the health care providers doctors in the US you can download this data set I had to ETL this for for a",
    "start": "4601400",
    "end": "4608159"
  },
  {
    "text": "client of mine it's a 4 gigabyte file um with about 1.8 million providers in it",
    "start": "4608159",
    "end": "4615480"
  },
  {
    "text": "that is 300 some columns wide use a yeah there you go um so in that case I would",
    "start": "4615480",
    "end": "4622159"
  },
  {
    "text": "fall back on the struct type because the way you build the struct type is it's a collection of struct fields and I can",
    "start": "4622159",
    "end": "4627560"
  },
  {
    "text": "make a list as big as I want right so there there are then there are a couple other there's another thing that you can use called a row object there are a",
    "start": "4627560",
    "end": "4633880"
  },
  {
    "text": "number of different ways that you can build this but for a data set where you're you're have you have fewer than 22 columns the case class is arguably",
    "start": "4633880",
    "end": "4641480"
  },
  {
    "text": "for a scholar programmer the most Rec it has no support for shap Less that I'm",
    "start": "4641480",
    "end": "4647480"
  },
  {
    "text": "aware of but you don't need shap less for this but you can use a tupal okay so a",
    "start": "4647480",
    "end": "4653400"
  },
  {
    "text": "tupal is another way to do it you can build a tupal because each element of a tupal has its own distinct type and then",
    "start": "4653400",
    "end": "4658560"
  },
  {
    "text": "when you call 2df you pass the column names in okay you give it the column names to fill in the other half so there",
    "start": "4658560",
    "end": "4664639"
  },
  {
    "text": "about five or six different ways to do this if you can use this it's probably the easiest to read okay this is just",
    "start": "4664639",
    "end": "4670199"
  },
  {
    "text": "one example of how you do that okay so that's a super super brief overview of data frames there's a lot more in there",
    "start": "4670199",
    "end": "4676800"
  },
  {
    "text": "I would recommend if you're interested in this and if you're using spark you ought to be there's um there's a a data",
    "start": "4676800",
    "end": "4683560"
  },
  {
    "text": "frame guide in in the spark programming guide um out here and it's worth reading",
    "start": "4683560",
    "end": "4689400"
  },
  {
    "text": "through it explains all this stuff um it's it's it's a useful API um so at",
    "start": "4689400",
    "end": "4695520"
  },
  {
    "text": "that point we can either take a five minute break if everybody's up for that or we can keep going it's up to you guys",
    "start": "4695520",
    "end": "4700880"
  },
  {
    "text": "hands up for a break hands up for no break no break wins so",
    "start": "4700880",
    "end": "4706520"
  },
  {
    "text": "let's keep going all right so let's finally now we're getting to the me of the matter right I went through all of",
    "start": "4706520",
    "end": "4713159"
  },
  {
    "start": "4709000",
    "end": "6145000"
  },
  {
    "text": "that I felt like I had to go through all of that just to give everybody a basic background and Spark so that we could understand streaming for those of you",
    "start": "4713159",
    "end": "4719080"
  },
  {
    "text": "who used spark this was a review and I apologize you're probably bored to death by now um but we're going to get into",
    "start": "4719080",
    "end": "4724480"
  },
  {
    "text": "the real reason for this talk which is streaming so what is streaming provides a way to consume",
    "start": "4724480",
    "end": "4731000"
  },
  {
    "text": "continual streams of data right it's intended to be scalable high throughput",
    "start": "4731000",
    "end": "4736480"
  },
  {
    "text": "and fault tolerant um high throughput but not low latency okay that's an important thing",
    "start": "4736480",
    "end": "4742800"
  },
  {
    "text": "you'll see what I mean by that in a couple of minutes it's built on top of spark",
    "start": "4742800",
    "end": "4749239"
  },
  {
    "text": "core okay so the the the intent here is that the API for for spark streaming",
    "start": "4749239",
    "end": "4755280"
  },
  {
    "text": "looks a lot like the rdd API so if you already know regular spark bootstrapping",
    "start": "4755280",
    "end": "4760880"
  },
  {
    "text": "into streaming is has a a relatively low pain threshold right as opposed to if it",
    "start": "4760880",
    "end": "4767120"
  },
  {
    "text": "weren't there you would have to use something like Twitter storm completely totally different API and different approach so the intent here is to say",
    "start": "4767120",
    "end": "4773800"
  },
  {
    "text": "look we want spark to be the tool that you use to do streaming and we want to make it easy for you to go from regular",
    "start": "4773800",
    "end": "4778960"
  },
  {
    "text": "spark into streaming without a you know this this huge hill to",
    "start": "4778960",
    "end": "4785040"
  },
  {
    "text": "climb so that and that's the point of this bullet so I'll skip ahead here it supports a number of different inputs",
    "start": "4785440",
    "end": "4791360"
  },
  {
    "text": "we're going to see a couple of these um TCP socket we're going to see this I actually have a a little dummy server",
    "start": "4791360",
    "end": "4797480"
  },
  {
    "text": "running out there Kafka and Flume if you want to do buffering okay so there's a Kafka connector there actually two",
    "start": "4797480",
    "end": "4803719"
  },
  {
    "text": "there's a pull and a push um it'll read from htfs or S3 now there's some cool things you can do with that um one of",
    "start": "4803719",
    "end": "4810280"
  },
  {
    "text": "the things you can have it do for instance is you can point it at an hdfs directory and it will monitor that",
    "start": "4810280",
    "end": "4816560"
  },
  {
    "text": "directory and every time a file shows up in there it'll stream it in so you could use that for a log drop or something um",
    "start": "4816560",
    "end": "4824239"
  },
  {
    "text": "Kinesis uh even Twitter which can be cool for demos although we're not going to do that in here um but there's one",
    "start": "4824239",
    "end": "4830000"
  },
  {
    "text": "demo they give it some of the conferences where the guy fires up a streaming app that reads the Twitter stream and then he tells everybody okay",
    "start": "4830000",
    "end": "4836159"
  },
  {
    "text": "start tweeting and then he has it set up so that stuff ends up showing up in the console um which can be interesting but",
    "start": "4836159",
    "end": "4842520"
  },
  {
    "text": "I didn't put that one together for here um it's currently based on rdds now since data frames is the way of",
    "start": "4842520",
    "end": "4849320"
  },
  {
    "text": "the future they're busy trying to get a data frame streaming solution put into the API but at the moment if you want to",
    "start": "4849320",
    "end": "4855400"
  },
  {
    "text": "do streaming you need to know about rdds which is another reason I wanted to go over them briefly because what streaming",
    "start": "4855400",
    "end": "4861159"
  },
  {
    "text": "does is it produces rdds right so as an example if you if",
    "start": "4861159",
    "end": "4866280"
  },
  {
    "text": "you envision and this is yes quick question is there any um relationship between this and the reactive streams",
    "start": "4866280",
    "end": "4873880"
  },
  {
    "text": "spec like like is it is are are they like I know the reactive stream soors it's idea of be able to have a",
    "start": "4873880",
    "end": "4880199"
  },
  {
    "text": "crossplatform streaming thing that everybody could Implement is does the spark streaming stuff is it even consistent with that or I don't I don't",
    "start": "4880199",
    "end": "4887480"
  },
  {
    "text": "I you know actually I should look that up but I don't see any reference in any the streaming documentation to reactive",
    "start": "4887480",
    "end": "4892520"
  },
  {
    "text": "streams so um having not read the that part of the reactive Manifesto I can't",
    "start": "4892520",
    "end": "4898120"
  },
  {
    "text": "answer that authoritatively um but I I don't believe it's like AA streams for example is",
    "start": "4898120",
    "end": "4904639"
  },
  {
    "text": "considered an implementation of reactives you well so you can you can stream from AA as well okay so there is",
    "start": "4904639",
    "end": "4911679"
  },
  {
    "text": "some hookup in there but I don't think they're deliberately trying to make this a reactive stream right um this is a",
    "start": "4911679",
    "end": "4918840"
  },
  {
    "text": "30,000 foot view of this okay just to kind of explain the general view of",
    "start": "4918840",
    "end": "4924280"
  },
  {
    "text": "what's going on here we'll drill down in a second so the idea here is can think of these two things as a box you've got streaming over here and Spark core over",
    "start": "4924280",
    "end": "4931480"
  },
  {
    "text": "here and here's your data this is your your data stream coming in this is a stream of bytes or stream of strings or",
    "start": "4931480",
    "end": "4938239"
  },
  {
    "text": "whatever it's a stream of data coming in um spark core doesn't work on streams of data spark core works on discrete things",
    "start": "4938239",
    "end": "4946239"
  },
  {
    "text": "called rdds and rdds are a fixed size a fixed big size often but they're a fixed",
    "start": "4946239",
    "end": "4952360"
  },
  {
    "text": "size right they're not geared to working on continual data coming in so the way streaming works is that it basically",
    "start": "4952360",
    "end": "4960400"
  },
  {
    "text": "batches these things up into rdds so your incoming stream gets buffered into rdds and then those rdds get handed down",
    "start": "4960400",
    "end": "4967600"
  },
  {
    "text": "to spark core over and over again for processing okay so this is this spark",
    "start": "4967600",
    "end": "4973080"
  },
  {
    "text": "core then becomes the implementation and it emits basically batches of processed data so you're still using spark core",
    "start": "4973080",
    "end": "4979760"
  },
  {
    "text": "you're processing rdds under the covers um so let's dig in a little bit into",
    "start": "4979760",
    "end": "4984840"
  },
  {
    "text": "that to see what that means so Brian does it um does it uh is it",
    "start": "4984840",
    "end": "4991440"
  },
  {
    "text": "fundamentally a pull architecture and th obviate the need for like back pressure um it depends kfka is a good example",
    "start": "4991440",
    "end": "4997960"
  },
  {
    "text": "there the initial connector was a push okay there's a new pull connector pull is more efficient",
    "start": "4997960",
    "end": "5003360"
  },
  {
    "text": "but a TCP push one then you got to deal with not being able to keep up yes that's exactly right that's exactly",
    "start": "5003360",
    "end": "5009120"
  },
  {
    "text": "right you do have to worry about that and we can touch on that a little bit you'll see you'll see the implications of that even better in a minute it uses",
    "start": "5009120",
    "end": "5016520"
  },
  {
    "text": "what this is what's called a micro batch architecture right so each rdd is a batch we're batching the rdd up we're",
    "start": "5016520",
    "end": "5023480"
  },
  {
    "text": "basically collecting this incoming stream and we're saying chop it off here hand that off it's almost you can almost",
    "start": "5023480",
    "end": "5029440"
  },
  {
    "text": "think of it like a conveyor belt of cookies right here come the cookies put them in boxes okay you take a box you",
    "start": "5029440",
    "end": "5035280"
  },
  {
    "text": "take a box right and we process the boxes from that point on cookie metaphor I have to remember that all right so um",
    "start": "5035280",
    "end": "5042320"
  },
  {
    "text": "it the spark streaming uh component continually receives this live input",
    "start": "5042320",
    "end": "5048480"
  },
  {
    "text": "data and it divides this data up into those little boxes of cookies if you will I'm going to keep using that",
    "start": "5048480",
    "end": "5053920"
  },
  {
    "text": "metaphor until it gets really gross um so these new batches are actually um",
    "start": "5053920",
    "end": "5059400"
  },
  {
    "text": "time based not size based all right which makes sense if you think",
    "start": "5059400",
    "end": "5064480"
  },
  {
    "text": "about streaming right um because if you made them size-based and your stream is",
    "start": "5064480",
    "end": "5071159"
  },
  {
    "text": "intermittent then you might wait hours for the next batch to come in right so they've made the decision that it's",
    "start": "5071159",
    "end": "5077440"
  },
  {
    "text": "better to hand you a partially filled batch on a regular interval than to wait a half an hour to get the right number",
    "start": "5077440",
    "end": "5083520"
  },
  {
    "text": "of bites okay so it's all time based then and the batches are created on in batch intervals and you can control that",
    "start": "5083520",
    "end": "5090480"
  },
  {
    "text": "that's actually defined when you create the Stream uh or the streaming context well an rdd should can be of arbitrary",
    "start": "5090480",
    "end": "5097159"
  },
  {
    "text": "size because it's distributed so it wouldn't make sense for it to be size oriented um or be limiting if it were",
    "start": "5097159",
    "end": "5102880"
  },
  {
    "text": "size Orient it that the point is that it would be limiting right but of course the flip side of that is that if it's",
    "start": "5102880",
    "end": "5107920"
  },
  {
    "text": "time based it's entirely possible that you hit the end of a time interval and there's nothing there right so at the",
    "start": "5107920",
    "end": "5114040"
  },
  {
    "text": "beginning of each new time interval a new batch gets created that batch gets filled up and when you hit the end of",
    "start": "5114040",
    "end": "5121040"
  },
  {
    "text": "the time interval the batch is done growing and it's um that time and again",
    "start": "5121040",
    "end": "5129080"
  },
  {
    "text": "that that's called the batch interval it's actually a parameter at the end of the time interval the batch is done growing it becomes an rdd and it's",
    "start": "5129080",
    "end": "5135040"
  },
  {
    "text": "handed down to spark core for processing all right we're going to see what that means in a minute the batch",
    "start": "5135040",
    "end": "5140320"
  },
  {
    "text": "interval um typically is is between 500 milliseconds and several seconds but you can configure this I know one example",
    "start": "5140320",
    "end": "5147719"
  },
  {
    "text": "where the guy actually set it to 3 minutes um in general the recommended starting time for a new application is",
    "start": "5147719",
    "end": "5153880"
  },
  {
    "text": "around a second so that's what I meant when I said it's not low latency right if you got a batch interval of a second",
    "start": "5153880",
    "end": "5160320"
  },
  {
    "text": "you're not processing stuff as it comes in you're always one second behind the actual data right but the general idea",
    "start": "5160320",
    "end": "5166639"
  },
  {
    "text": "is you want to set it at a value that makes sense for you now that to get back to what Joe is saying let's suppose I'm creating batch intervals every second",
    "start": "5166639",
    "end": "5174080"
  },
  {
    "text": "and every time a batch I'm sorry the batch intervals uh one second so I'm creating a batch every second and when I",
    "start": "5174080",
    "end": "5180320"
  },
  {
    "text": "go to process that rdd I'm doing some wild set of Transformations on it and then running a really complicated",
    "start": "5180320",
    "end": "5186800"
  },
  {
    "text": "machine learning algorithm on it and it turns out it takes two and a half seconds to process an rdd you now a",
    "start": "5186800",
    "end": "5192719"
  },
  {
    "text": "problem right and if you're if if the source is pushing to you then you're",
    "start": "5192719",
    "end": "5197840"
  },
  {
    "text": "going to be exhibiting back pressure if the source is a TCP socket then eventually the kernel buffers will fill",
    "start": "5197840",
    "end": "5204360"
  },
  {
    "text": "up and then the back pressure gets translated over to the network in terms of window full knacks right um which is",
    "start": "5204360",
    "end": "5211719"
  },
  {
    "text": "why if you have that kind of a setup a lot of people will look at Kafka particularly looking at Kafka with a",
    "start": "5211719",
    "end": "5217040"
  },
  {
    "text": "pull architecture right look Kafka you just buffer it up I'll come get it when I want it now again that doesn't",
    "start": "5217040",
    "end": "5223320"
  },
  {
    "text": "necessarily alleviate the problem it may be that eventually you fill Kafka up but but Kafka can level out that kind of",
    "start": "5223320",
    "end": "5230040"
  },
  {
    "text": "back pressure if your if your stream is you know not regular right High burst",
    "start": "5230040",
    "end": "5235119"
  },
  {
    "text": "low burst right you could use Kafka to level that out and that b distributed multiple",
    "start": "5235119",
    "end": "5242719"
  },
  {
    "text": "workers batches are by definition distributed they become rdds which means they're",
    "start": "5242719",
    "end": "5247960"
  },
  {
    "text": "partitioned okay so when a batch we're going to see exactly what happens in just a minute but when the batch comes",
    "start": "5247960",
    "end": "5253119"
  },
  {
    "text": "in once that batch is filled it gets turned into an rdd and an rdd by definition is distributed that's what",
    "start": "5253119",
    "end": "5258600"
  },
  {
    "text": "the first d means right so it is it is getting processed across the",
    "start": "5258600",
    "end": "5265400"
  },
  {
    "text": "cluster um an empty batch obviously there's nothing to process right uh you know I have to actually",
    "start": "5266199",
    "end": "5273400"
  },
  {
    "text": "look at that I don't believe so I don't believe it'll actually send it but if it does there's an easy way to test",
    "start": "5273400",
    "end": "5278600"
  },
  {
    "text": "it okay so",
    "start": "5278600",
    "end": "5283000"
  },
  {
    "text": "um now we're going to get into a little bit more terminology then I have some pretty pictures and then I'll show you a couple examples of this all right so the",
    "start": "5283639",
    "end": "5290679"
  },
  {
    "text": "the the thing that we build in spark streaming uh remember I said in data frames we build a data frame and a data frame is implemented in terms of rdds",
    "start": "5290679",
    "end": "5297239"
  },
  {
    "text": "under the covers in um in spark streaming we don't operate on rdds in a",
    "start": "5297239",
    "end": "5303920"
  },
  {
    "text": "not completely what we do is we build up these things called dams which is a short for a discretized stream right",
    "start": "5303920",
    "end": "5311280"
  },
  {
    "text": "you're taking a stream and building in breaking it up into discreete batches hence a discreete I stream right so the",
    "start": "5311280",
    "end": "5318880"
  },
  {
    "text": "the actual data type that you'll see in the API is called a dam um it's a SE it's intended to model",
    "start": "5318880",
    "end": "5325520"
  },
  {
    "text": "a sequence of data arriving um over time internally each dstream is is",
    "start": "5325520",
    "end": "5332000"
  },
  {
    "text": "represented as a sequence of rdds now the interesting thing I'm probably jumping ahead in my own slides here but",
    "start": "5332000",
    "end": "5337639"
  },
  {
    "text": "the interesting thing is that you can apply Transformations and actions to to",
    "start": "5337639",
    "end": "5342719"
  },
  {
    "text": "dams so what does that mean we know what it means in data frames right it contributes to a query plan which and",
    "start": "5342719",
    "end": "5350520"
  },
  {
    "text": "they're not really actions and Transformations there in the same in the same sense but what does an action mean on a",
    "start": "5350520",
    "end": "5357840"
  },
  {
    "text": "dstream or what does a transformation mean on a dstream like a m function and then it gets executed on each of the",
    "start": "5357840",
    "end": "5364080"
  },
  {
    "text": "underlying AR yeah yeah that's exactly right I almost think of it as a template or a die stamp right I like the DI stamp",
    "start": "5364080",
    "end": "5370560"
  },
  {
    "text": "metaphor right you create this dstream with a bunch of Transformations on it and this transformation produces another Dam and then another dstream so it",
    "start": "5370560",
    "end": "5377360"
  },
  {
    "text": "almost looks like a dag it almost looks like an rdd chain but it isn't right but what ends up happening is that when an",
    "start": "5377360",
    "end": "5382760"
  },
  {
    "text": "rdd comes in it's almost as if the the series of actions Transformations and",
    "start": "5382760",
    "end": "5388199"
  },
  {
    "text": "actions that you put on the dams are used as a stamp to stamp the D on that rdd which then gets",
    "start": "5388199",
    "end": "5394520"
  },
  {
    "text": "executed okay and then that rdd is executed and thrown away new rdd comes in we stamp that guy and you know so",
    "start": "5394520",
    "end": "5401679"
  },
  {
    "text": "it's sort of like that kind of process so an rdd comes in it's naked there's nothing on it yet what the templatized",
    "start": "5401679",
    "end": "5407400"
  },
  {
    "text": "operations on the D stream are used to create the corresponding operations on the rdds and run the dag on that rdd and",
    "start": "5407400",
    "end": "5413080"
  },
  {
    "text": "then we throw it away so so bottom line you tend to process dstreams by",
    "start": "5413080",
    "end": "5418199"
  },
  {
    "text": "configuring the dstreams and then just saying go man go and let them let it run and it will each",
    "start": "5418199",
    "end": "5424800"
  },
  {
    "text": "rdd that comes in gets those operations gets processed and gets thrown",
    "start": "5424800",
    "end": "5430040"
  },
  {
    "text": "away all right and this is sort of what it looks like right now I haven't talked about the the",
    "start": "5431280",
    "end": "5437639"
  },
  {
    "text": "other little thing in there yes are these Dam resilient or you lose them",
    "start": "5437639",
    "end": "5445520"
  },
  {
    "text": "um you can lose them they are resilient to a degree um but you can lose them",
    "start": "5445880",
    "end": "5453159"
  },
  {
    "text": "right they're resilient in the sense that U and as we'll see in a minute a backup copy of one of them is made right",
    "start": "5453159",
    "end": "5459719"
  },
  {
    "text": "but but there's no notion with a stream of being able to rewind the stream and start again on well so right so if if if",
    "start": "5459719",
    "end": "5467960"
  },
  {
    "text": "you're in a situation where your executors are crashing all the time you have a problem because you can't go back",
    "start": "5467960",
    "end": "5473159"
  },
  {
    "text": "and fill this stuff in right so so they it's I mean streams of data come in and",
    "start": "5473159",
    "end": "5478600"
  },
  {
    "text": "you throw them away now I don't talk about this in here but but there are window operations on dams so you can say",
    "start": "5478600",
    "end": "5486239"
  },
  {
    "text": "look I'd like to keep a sliding window of the last five you know rdds worth of the stream around so I can do some",
    "start": "5486239",
    "end": "5492480"
  },
  {
    "text": "comparisons and there are window operations and things like that you can do on them so you get the standard trade-offs you get with any winding",
    "start": "5492480",
    "end": "5498000"
  },
  {
    "text": "architecture right how big should I make the window how much memory do I have to hold on to all this stuff um and what",
    "start": "5498000",
    "end": "5505119"
  },
  {
    "text": "happens if I want to look further back in time than that than the largest window I can make right so the window is",
    "start": "5505119",
    "end": "5511360"
  },
  {
    "text": "good for short burst of looking back in time um but if you need to do longer",
    "start": "5511360",
    "end": "5516400"
  },
  {
    "text": "time series analyses then typically what you're going to do is you're going to filter the incoming streaming data and",
    "start": "5516400",
    "end": "5521719"
  },
  {
    "text": "then dump it into a persistent store like Cassandra or something so that you can then do you know later data analysis",
    "start": "5521719",
    "end": "5527159"
  },
  {
    "text": "and that's kind of consistent if you think about it with a data warehousing approach right so um but you your capability to",
    "start": "5527159",
    "end": "5536040"
  },
  {
    "text": "rewind is is probably a function of your the underlying source of your data right like so something like kofka you could",
    "start": "5536040",
    "end": "5541719"
  },
  {
    "text": "potentially rewind potentially whereas if you've got something that's just popping messages in and out underneath the hood if you're connected to a TCP",
    "start": "5541719",
    "end": "5547920"
  },
  {
    "text": "socket good luck rewinding it right um so what the the new part in",
    "start": "5547920",
    "end": "5553560"
  },
  {
    "text": "here is this notion of a block which I'm going to talk about in just a minute okay so notice that this this is a batch",
    "start": "5553560",
    "end": "5560480"
  },
  {
    "text": "right these are my batches so batch interval here is set to 5 seconds and at",
    "start": "5560480",
    "end": "5566239"
  },
  {
    "text": "t equal 5 this batch gets turned into an rdd at t equal 10 whatever I've",
    "start": "5566239",
    "end": "5573040"
  },
  {
    "text": "accumulated in this batch gets turned into the next rdd now we have to talk about these block things that I've shown",
    "start": "5573040",
    "end": "5579400"
  },
  {
    "text": "here but I haven't actually described what they are or why they're there but I'll bet you can",
    "start": "5579400",
    "end": "5585199"
  },
  {
    "text": "guess okay so let's we'll get there in a second first of all deream is Created from something called a streaming",
    "start": "5585199",
    "end": "5591320"
  },
  {
    "text": "context right and you create a streaming context from your spark context now there's a restriction on the streaming",
    "start": "5591320",
    "end": "5597280"
  },
  {
    "text": "context which is that um you can only have one streaming context for jvn",
    "start": "5597280",
    "end": "5602639"
  },
  {
    "text": "that means if you try to create two streaming contexts within your jvm you're going to get an error however",
    "start": "5602639",
    "end": "5608960"
  },
  {
    "text": "that's not the same as saying you're limited to one stream of data with a given streaming context you can create",
    "start": "5608960",
    "end": "5614600"
  },
  {
    "text": "multiple streams you can be reading from multiple pieces of stream data but you",
    "start": "5614600",
    "end": "5620480"
  },
  {
    "text": "only ever have one streaming context once you've created a dam you have as I mentioned earlier",
    "start": "5620480",
    "end": "5628040"
  },
  {
    "text": "Transformations these yield dams and output oper operations they're not so much actions as output operations okay",
    "start": "5628040",
    "end": "5635119"
  },
  {
    "text": "although there is one common action which is called for each rdd that's your escape hatch okay it's an action it runs",
    "start": "5635119",
    "end": "5642760"
  },
  {
    "text": "distributed and what you're saying is look I want to run this block of code for every rdd that comes in um a typical",
    "start": "5642760",
    "end": "5648239"
  },
  {
    "text": "use case there is this is where I want to do some Transformations that I can't do on D streams or this is where I want",
    "start": "5648239",
    "end": "5653560"
  },
  {
    "text": "to bootstrap up into a data frame to do some data framy stuff you know I want to dump this incoming stuff into uh into",
    "start": "5653560",
    "end": "5661080"
  },
  {
    "text": "hive it's just easier to do that from a data frame so I within a 4 each rdd block that's my manhole cover I open",
    "start": "5661080",
    "end": "5667480"
  },
  {
    "text": "that up I get down into the guts and I say okay I've got an rdd here I'm going to bootstrap up into a data frame and immediately write this out to um you",
    "start": "5667480",
    "end": "5675520"
  },
  {
    "text": "know to Hive or whatever all right mentioned this",
    "start": "5675520",
    "end": "5681280"
  },
  {
    "text": "already um so how do we map a dstream to an",
    "start": "5681280",
    "end": "5687119"
  },
  {
    "text": "rdd because what is an rdd let's rewind our discussion of an rdd again we know it stands for resilient distributed data",
    "start": "5687119",
    "end": "5694159"
  },
  {
    "text": "uh data set but what does it consist of louder please partitions partitions",
    "start": "5694159",
    "end": "5700440"
  },
  {
    "text": "so what we haven't talked about yet is how do we divide up the incoming batch remember I said a batch becomes an rdd",
    "start": "5700440",
    "end": "5707800"
  },
  {
    "text": "how does that get partitioned the answer is that there are",
    "start": "5707800",
    "end": "5712960"
  },
  {
    "text": "actually two time intervals there's a batch interval and a block interval all right and a block interval",
    "start": "5712960",
    "end": "5719600"
  },
  {
    "text": "I I believe it defaults to 200 Mill seconds a new rdd is created every batch interval a new block is created every",
    "start": "5719600",
    "end": "5727119"
  },
  {
    "text": "block interval so suppose I have a batch interval of 1 second and I've got a",
    "start": "5727119",
    "end": "5732320"
  },
  {
    "text": "block interval of 200 milliseconds how many partitions will that rdd have five right oh we've hit 200 milliseconds",
    "start": "5732320",
    "end": "5739040"
  },
  {
    "text": "there's a block we've hit 200 milliseconds there's a block and the blocks then get translated in the part into the",
    "start": "5739040",
    "end": "5744639"
  },
  {
    "text": "partitions um that that are used to process the rdd that might sound strange",
    "start": "5744639",
    "end": "5750360"
  },
  {
    "text": "but really um that's just you can just think of it as another data connector for instance um if you look at the",
    "start": "5750360",
    "end": "5755400"
  },
  {
    "text": "Cassandra data connector for rdds how does that stuff get partitioned turns out it gets partitioned I believe",
    "start": "5755400",
    "end": "5761480"
  },
  {
    "text": "currently it's 100,000 rows per partition and so you read a bunch of",
    "start": "5761480",
    "end": "5767960"
  },
  {
    "text": "data from Cassandra each 100,000 rows becomes a partition when you read an hdfs file each hdfs block becomes a",
    "start": "5767960",
    "end": "5775080"
  },
  {
    "text": "partition so you know this is just another data source and each and it blocks things up on time Bal because",
    "start": "5775080",
    "end": "5781880"
  },
  {
    "text": "this whole thing is time based happens",
    "start": "5781880",
    "end": "5787400"
  },
  {
    "text": "the well can yeah right",
    "start": "5787840",
    "end": "5793280"
  },
  {
    "text": "dat okay so my data on cassander is probably a little out of date all",
    "start": "5795119",
    "end": "5800920"
  },
  {
    "text": "right so this is an example of how this happens right this is that same diagram earlier right we've got a batch the",
    "start": "5800920",
    "end": "5807360"
  },
  {
    "text": "batch has blocks that becomes an rdd where the blocks become partitions",
    "start": "5807360",
    "end": "5813280"
  },
  {
    "text": "we're almost at the end here so here's an example of setting one of these guys up um a socket text stream is like it's",
    "start": "5813800",
    "end": "5820360"
  },
  {
    "text": "calling sc. text file you're getting in text lines usually right so there create",
    "start": "5820360",
    "end": "5826159"
  },
  {
    "text": "a create I've got my streaming context um I'm using a duration there seconds one okay so we got a one second batch",
    "start": "5826159",
    "end": "5833840"
  },
  {
    "text": "interval my D stream is that line stream and then I'm filtering that line stream",
    "start": "5833840",
    "end": "5839480"
  },
  {
    "text": "and getting out of that line L that contain the word error and then I'm doing a for each rdd",
    "start": "5839480",
    "end": "5845840"
  },
  {
    "text": "and I'm saving that as a text file okay each rdd that comes in I dump to a text",
    "start": "5845840",
    "end": "5851800"
  },
  {
    "text": "file um now the bottom part is the final bit this stuff all gets executed inside a",
    "start": "5851800",
    "end": "5857360"
  },
  {
    "text": "thread So when you say to get this whole thing going you call start on the streaming context that kicks off a",
    "start": "5857360",
    "end": "5864320"
  },
  {
    "text": "thread that receives the data if you're running inside the shell that's good enough because you're going",
    "start": "5864320",
    "end": "5870560"
  },
  {
    "text": "to get a prompt back the shell is not going to go away if you're running inside a submitted job and you don't",
    "start": "5870560",
    "end": "5875880"
  },
  {
    "text": "call a weight termination congratulations you just set up a you just set up a connection and then you",
    "start": "5875880",
    "end": "5881080"
  },
  {
    "text": "promptly went away and everything went away okay so you need basically to suspend the main thread until until the",
    "start": "5881080",
    "end": "5888719"
  },
  {
    "text": "streaming is done or you kill the job right so that's what the await termination is there for this would be a",
    "start": "5888719",
    "end": "5894520"
  },
  {
    "text": "submitted job right if it were running in the Shell there's really no point to calling a weight termination all that's",
    "start": "5894520",
    "end": "5899639"
  },
  {
    "text": "going to do is prevent me from getting a prompt so I can shut the whole thing down when I'm done playing",
    "start": "5899639",
    "end": "5904840"
  },
  {
    "text": "around okay but it runs in a background thread um which we're going to look at in just a minute there it is so there is a",
    "start": "5904840",
    "end": "5914199"
  },
  {
    "text": "receiver thread the question is where who is responsible for receiving this data chunking it up and distributing it",
    "start": "5914199",
    "end": "5920159"
  },
  {
    "text": "out across the network it turns out that the spark streaming API allocates a thread out of one of the slots that's",
    "start": "5920159",
    "end": "5926920"
  },
  {
    "text": "running on one of the workers so um workers are workers are created with a number of slots they're allowed to give",
    "start": "5926920",
    "end": "5932960"
  },
  {
    "text": "out and a certain amount of memory that they're allowed to give out at least in the in the um the Standalone cluster so",
    "start": "5932960",
    "end": "5938199"
  },
  {
    "text": "those s's represent slots or potential threads they may actually be there in a thread pool they may not be there yet",
    "start": "5938199",
    "end": "5944080"
  },
  {
    "text": "but these are threads that can run to process things that aren't currently being used the T up here indicates that",
    "start": "5944080",
    "end": "5950880"
  },
  {
    "text": "there's one thread running processing an existing rdd one of the slots in one of",
    "start": "5950880",
    "end": "5955920"
  },
  {
    "text": "the workers or one of the executors is taken up by the receiver thread",
    "start": "5955920",
    "end": "5961840"
  },
  {
    "text": "okay so what this means is that if you have a really really tiny cluster with with like one worker node that has one",
    "start": "5961840",
    "end": "5968320"
  },
  {
    "text": "slot in it you're not going to do very much you'll receive these rdds but they're",
    "start": "5968320",
    "end": "5973639"
  },
  {
    "text": "never going to get processed okay because the the only thread in the whole cluster is dedicated to to you know receiving the data and",
    "start": "5973639",
    "end": "5981000"
  },
  {
    "text": "I've actually seen this in misconfigured clusters and it's very irritating all right so what happens is",
    "start": "5981000",
    "end": "5986679"
  },
  {
    "text": "that we open up this stream and that data starts streaming into that receiver thread all right and now it has to start",
    "start": "5986679",
    "end": "5992400"
  },
  {
    "text": "buffering that data up into a batch as it turns out that's what it does okay buffers it into two places one",
    "start": "5992400",
    "end": "6001119"
  },
  {
    "text": "one place is the node that is supposed to process it and then some other node in the cluster is chosen based on some",
    "start": "6001119",
    "end": "6007239"
  },
  {
    "text": "algorithm under the covers it might even be random but that's the backup right so that's your one level of oh my God the",
    "start": "6007239",
    "end": "6013480"
  },
  {
    "text": "executor crashed okay so you could restart that block from there because it's been cached over there right",
    "start": "6013480",
    "end": "6022400"
  },
  {
    "text": "and then in comes another partition and that same thing happens another block comes in right third block comes",
    "start": "6022400",
    "end": "6030560"
  },
  {
    "text": "in right same sort of thing and then finally that whole thing gets turned into an",
    "start": "6030560",
    "end": "6036400"
  },
  {
    "text": "rdd and gets processed a thread gets allocated to process that so this is how",
    "start": "6036400",
    "end": "6041560"
  },
  {
    "text": "essentially how streaming Works under the covers so your bottleneck is that one receiver thread right somebody has",
    "start": "6041560",
    "end": "6047040"
  },
  {
    "text": "to be responsible for getting the data and pushing it out and it's a thread somewhere on your cluster um and as far",
    "start": "6047040",
    "end": "6053199"
  },
  {
    "text": "as I know you do have do not have any control over uh over where that thread ends up it's it's in one of the",
    "start": "6053199",
    "end": "6058880"
  },
  {
    "text": "executors on your cluster and as I said you can do multiple",
    "start": "6058880",
    "end": "6064280"
  },
  {
    "text": "dams on One streaming context and if you do that you get multiple receiver threads also distributed out across the",
    "start": "6064280",
    "end": "6071000"
  },
  {
    "text": "cluster so in this case what we're doing is is we're reading from Kafka so there's a thread over here that's reading from Kafka um and in this case",
    "start": "6071000",
    "end": "6078719"
  },
  {
    "text": "you know this is our original streaming source and we're reading that that's the receiver thread that is responsible for",
    "start": "6078719",
    "end": "6084320"
  },
  {
    "text": "batching that stuff up and distributing it out across the cluster all right is that clear how this",
    "start": "6084320",
    "end": "6091679"
  },
  {
    "text": "works so when you call start on that streaming context it basically runs any D streams you you define yep yep that's",
    "start": "6091679",
    "end": "6098400"
  },
  {
    "text": "exactly right that's exactly right when you s when you call start on that streaming context all the D streams that",
    "start": "6098400",
    "end": "6103760"
  },
  {
    "text": "you've defined on that context begin to run begin to run that's right is the level of redundancy um configurable not",
    "start": "6103760",
    "end": "6111119"
  },
  {
    "text": "to my knowledge but I'd have to look that up so I can imagine if you're running",
    "start": "6111119",
    "end": "6116159"
  },
  {
    "text": "spot cluster yeah that could be a problem you know you actually lose two I don't I don't to be honest I don't",
    "start": "6116159",
    "end": "6122599"
  },
  {
    "text": "remember I'd have to look that up it's it's um the API is pretty easy to see and there are a bunch of configuration parameters it wouldn't surprise me if",
    "start": "6122599",
    "end": "6128960"
  },
  {
    "text": "there is a configuration parameter for to control that level of redundancy of course there are other options you have",
    "start": "6128960",
    "end": "6134360"
  },
  {
    "text": "too right do as little as possible and jam the stuff into Cassandra as fast as you can there's another option the third",
    "start": "6134360",
    "end": "6140599"
  },
  {
    "text": "op is to say look it's streaming data dude I'm going to drop some of it um these are the uh some of the things that",
    "start": "6140599",
    "end": "6147320"
  },
  {
    "start": "6145000",
    "end": "8230000"
  },
  {
    "text": "it supports out of the box it's also easy to write your own and I'm going to show you one of those in a couple of",
    "start": "6147320",
    "end": "6152480"
  },
  {
    "text": "minutes we're going to get to some code right now actually so I'm going to go back into one of these notebooks which",
    "start": "6152480",
    "end": "6159719"
  },
  {
    "text": "now I have to reconnect so that's pretty much done with the slides I can make this thing go away for good",
    "start": "6159719",
    "end": "6165960"
  },
  {
    "text": "now um and let's go back over here and make this let me Igan",
    "start": "6165960",
    "end": "6176118"
  },
  {
    "text": "this all right let me gotta find this",
    "start": "6178199",
    "end": "6183360"
  },
  {
    "text": "again all right so the first one I want to show you is this custom receiver one um all this thing does is read from",
    "start": "6188760",
    "end": "6196800"
  },
  {
    "text": "uh a source of random words I basically went out and downloaded the official Scrabble dictionary and what this thing",
    "start": "6196800",
    "end": "6202560"
  },
  {
    "text": "does is it reads those words in um but it needs some source for them um this is",
    "start": "6202560",
    "end": "6207719"
  },
  {
    "text": "kind of a candy um toy demo but why I like to show it is because of",
    "start": "6207719",
    "end": "6215560"
  },
  {
    "text": "this let me drag this over here all right come",
    "start": "6215560",
    "end": "6222760"
  },
  {
    "text": "on maybe I'll do it the easy",
    "start": "6224119",
    "end": "6228159"
  },
  {
    "text": "way there that's better okay so this uh",
    "start": "6229679",
    "end": "6235320"
  },
  {
    "text": "obviously is intellig make that bigger and uh what's with that what's",
    "start": "6235320",
    "end": "6243280"
  },
  {
    "text": "with that bright theme on intelligent right there dude that's why we have",
    "start": "6243280",
    "end": "6249320"
  },
  {
    "text": "choices okay so um so what is this this is um this is a random word Source this",
    "start": "6250080",
    "end": "6256040"
  },
  {
    "text": "is the class that generates this data okay um this is again this is a toy but",
    "start": "6256040",
    "end": "6262000"
  },
  {
    "text": "I want to show it to you because let's imagine a scenario where you need to process through spark streaming data of",
    "start": "6262000",
    "end": "6268320"
  },
  {
    "text": "a of a type that isn't supported uh by any of the existing connectors a really good example of that is UDP data let's",
    "start": "6268320",
    "end": "6275800"
  },
  {
    "text": "say you're processing audio data audio and video data we tend to do with UDP why well if we did it with TCP then",
    "start": "6275800",
    "end": "6283760"
  },
  {
    "text": "every time you drop the packet you've got back pressure because you have a retransmit right so you typically do",
    "start": "6283760",
    "end": "6289080"
  },
  {
    "text": "that kind of what's called isochronous data with UDP because it's better to drop a packet and get that little VoIP",
    "start": "6289080",
    "end": "6294520"
  },
  {
    "text": "glitch and keep moving than it is to to try to do retransmits and get behind",
    "start": "6294520",
    "end": "6300040"
  },
  {
    "text": "what if you had a source of data that's coming in over UDP and there's no connector for that",
    "start": "6300040",
    "end": "6305159"
  },
  {
    "text": "how would you do that well it's not that terribly hard to write a UDP server I mean it's a bit of a pain in the butt",
    "start": "6305159",
    "end": "6310960"
  },
  {
    "text": "but the Java API support it so you could write a Scala hun a scolar code that easily handles you know making the UDP",
    "start": "6310960",
    "end": "6318679"
  },
  {
    "text": "connection and reading the data but you need to have some way of of telling spark the streaming API look um this use",
    "start": "6318679",
    "end": "6327760"
  },
  {
    "text": "this to read my data so this is what you have to do you create a class that extends this reever this receiver class",
    "start": "6327760",
    "end": "6334760"
  },
  {
    "text": "this is a receiver that produces strings you can produce whatever you want this is the Storage level so there's your",
    "start": "6334760",
    "end": "6340480"
  },
  {
    "text": "level of redundancy Joe okay and you define two methods on",
    "start": "6340480",
    "end": "6345880"
  },
  {
    "text": "start on stop my onart method here creates a thread",
    "start": "6345880",
    "end": "6351040"
  },
  {
    "text": "and the run method of that thread calls another method called receive my receive method which I'm not going to bother to",
    "start": "6351040",
    "end": "6356320"
  },
  {
    "text": "show you here takes this dictionary that was passed in spits out this many words",
    "start": "6356320",
    "end": "6362239"
  },
  {
    "text": "at a time and then sleeps this amount of time this number of milliseconds before waking up and doing it all over again",
    "start": "6362239",
    "end": "6368800"
  },
  {
    "text": "okay so this is kind of a silly receiver Source it's it's a generator of data but you could imagine replacing that receive",
    "start": "6368800",
    "end": "6374920"
  },
  {
    "text": "method with the with the UDP data on start would do the initial connection and call the receive method which sits",
    "start": "6374920",
    "end": "6381080"
  },
  {
    "text": "there reads the UDP data until on stop is called at which point you would shut the connection down can you explain what",
    "start": "6381080",
    "end": "6387360"
  },
  {
    "text": "you mean by Storage level um that's just um I I have to look in the API but I'm pretty sure that's just the degree to",
    "start": "6387360",
    "end": "6393400"
  },
  {
    "text": "which the replication is occurring okay okay so here we're rep we're we're storing it in memory and twice on disk",
    "start": "6393400",
    "end": "6400880"
  },
  {
    "text": "okay what's with that W loop I thought I thought this was Scala um you can use while Loops in",
    "start": "6400880",
    "end": "6406719"
  },
  {
    "text": "Scola you'll not I'm happy to use a while loop there a new keyword too I thought scy heads don't like have to use",
    "start": "6406719",
    "end": "6412159"
  },
  {
    "text": "M um well um that's only if there's a companion object you tell me if there's a companion object For",
    "start": "6412159",
    "end": "6418520"
  },
  {
    "text": "Thread okay um and and I'm happy to use a while loop if it's if it's not using a",
    "start": "6418520",
    "end": "6423920"
  },
  {
    "text": "VAR do you see any vars in there I don't no underscores it is underscore friendly it's Al it's also underscore friendly",
    "start": "6423920",
    "end": "6429520"
  },
  {
    "text": "but that's actually you know kind of a failure of mine I think um so you know",
    "start": "6429520",
    "end": "6435520"
  },
  {
    "text": "again not not terribly difficult to write one of these things so if we flip back to um",
    "start": "6435520",
    "end": "6440840"
  },
  {
    "text": "and again I mean this is a boiler plate you could easily use this to build um to",
    "start": "6440840",
    "end": "6446280"
  },
  {
    "text": "to build something that reads a real data source yes the the type parameter to receive does that have to be sterilizable so that toiz",
    "start": "6446280",
    "end": "6454360"
  },
  {
    "text": "that yep definitely does and that's a good point actually somebody's been paying attention yes it does because the",
    "start": "6454360",
    "end": "6460560"
  },
  {
    "text": "it's going to come in um the data is going to come into one receiver thread which then has to distribute that data out across the network so therefore it",
    "start": "6460560",
    "end": "6466800"
  },
  {
    "text": "must be serializable um in general your stuff needs to be seral realizable for this",
    "start": "6466800",
    "end": "6471920"
  },
  {
    "text": "stuff to work all right back out to trainers and I'll show you the uh the example that uses this and then",
    "start": "6471920",
    "end": "6478840"
  },
  {
    "text": "there's another example I'll show you a server that um that I have running assuming it hasn't crashed at",
    "start": "6478840",
    "end": "6485280"
  },
  {
    "text": "which point I'll restart it if it has okay what the heck is going on here",
    "start": "6485280",
    "end": "6493360"
  },
  {
    "text": "there we go all right workspace home",
    "start": "6493360",
    "end": "6502239"
  },
  {
    "text": "all right here's the custom receiver so that custom receiver is spitting out words it's spitting them out from here",
    "start": "6504880",
    "end": "6513360"
  },
  {
    "text": "um and what I'm going to be doing I'm going to create all this stuff here let me just set all that up helps if I",
    "start": "6513360",
    "end": "6520159"
  },
  {
    "text": "attach it to a cluster okay we'll attach it to my cluster and let's start it",
    "start": "6520159",
    "end": "6528360"
  },
  {
    "text": "up okay here's the configuration for how many words per cycle Etc so we're going",
    "start": "6528360",
    "end": "6533599"
  },
  {
    "text": "to do that I'm not actually using checkpointing in here um this guy creates the random Source this is just",
    "start": "6533599",
    "end": "6538920"
  },
  {
    "text": "so I don't have to type this stuff over and over again so here I'm using rdds just because this is the most convenient way for me to read that file of words in",
    "start": "6538920",
    "end": "6545960"
  },
  {
    "text": "it it doesn't happen to be that big you know it's a Scrabble dictionary how big could it be right so collecting it is",
    "start": "6545960",
    "end": "6551320"
  },
  {
    "text": "not going to O anything I'm just going to collect that into an array and pass it into the word source that that that's",
    "start": "6551320",
    "end": "6558320"
  },
  {
    "text": "the pool of words it's going to use to at me manually all right so there it is there's our list of",
    "start": "6558320",
    "end": "6565520"
  },
  {
    "text": "words all right and here's how I create the stream process now note I wrap this up inside an object um I run into",
    "start": "6565520",
    "end": "6571960"
  },
  {
    "text": "problems if I try to do this line by line in a repple whether it's this reppel or the spark shell because it can",
    "start": "6571960",
    "end": "6578000"
  },
  {
    "text": "get confused about the scope um what this what this What spark has to do when",
    "start": "6578000",
    "end": "6584760"
  },
  {
    "text": "it encounters a Lambda that's going to be sent across the network is it not only has to take the bite code and send",
    "start": "6584760",
    "end": "6590119"
  },
  {
    "text": "send that across the network to all the units but it also has to figure out like what what pieces of data that thing",
    "start": "6590119",
    "end": "6596199"
  },
  {
    "text": "closed over and pick those things up and the repple can be full of data right so",
    "start": "6596199",
    "end": "6602599"
  },
  {
    "text": "not wrapping in an object means that it can sometimes pick up a non-serializable piece of data and it's not obvious and I",
    "start": "6602599",
    "end": "6608480"
  },
  {
    "text": "found that wrapping the stuff up inside an object contains the scope and makes it a little bit easier so what I've done",
    "start": "6608480",
    "end": "6614199"
  },
  {
    "text": "here is I've got two methods there's a stop method and a start method um the stop method basically I'm I'm keeping a",
    "start": "6614199",
    "end": "6621159"
  },
  {
    "text": "VAR up here although as it turns out I really don't need to do that there's one underneath in here that I could do um",
    "start": "6621159",
    "end": "6626800"
  },
  {
    "text": "but there's a there's a um a complicated method here called create streaming context uh this is the Fault tolerant",
    "start": "6626800",
    "end": "6634239"
  },
  {
    "text": "way of handling this you have a function that you give the streaming API and you tell the streaming API look get me the",
    "start": "6634239",
    "end": "6640679"
  },
  {
    "text": "current active streaming context but if there isn't one called this function to create one by doing it that way the",
    "start": "6640679",
    "end": "6646079"
  },
  {
    "text": "streaming Library can basically do fault tolerance if if the streaming context",
    "start": "6646079",
    "end": "6651159"
  },
  {
    "text": "dies it will create a new one but if not it'll just hand you back the one that's currently operating within here I set up",
    "start": "6651159",
    "end": "6657280"
  },
  {
    "text": "my Stream So I create the streaming context",
    "start": "6657280",
    "end": "6662320"
  },
  {
    "text": "um skip this from it I create a receiver stream out of the random word",
    "start": "6662320",
    "end": "6667719"
  },
  {
    "text": "Source okay so this is the call that you make to use a custom",
    "start": "6667719",
    "end": "6674119"
  },
  {
    "text": "receiver okay this code is just saying if I have a check Point directory um Define it to",
    "start": "6675280",
    "end": "6683760"
  },
  {
    "text": "the streaming context this is useful if you're using spark things like accumulator variables which we didn't",
    "start": "6683760",
    "end": "6689320"
  },
  {
    "text": "talk about in here parallelize um actually you know what I think I put the wrong one in here",
    "start": "6689320",
    "end": "6695719"
  },
  {
    "text": "I don't know that this is the one I really want to be",
    "start": "6695719",
    "end": "6699480"
  },
  {
    "text": "using I pulled the wrong one over here so I'm not actually going to run this but the idea here was um what this thing",
    "start": "6701400",
    "end": "6707760"
  },
  {
    "text": "was going to do is dump these words out out and then I would capture them in a map so that we could look at them okay",
    "start": "6707760",
    "end": "6714800"
  },
  {
    "text": "um and the only reason I like to show this as I said is to show you the custom receiver Source they're really easy to",
    "start": "6714800",
    "end": "6720400"
  },
  {
    "text": "write of more interest to me is actually this",
    "start": "6720400",
    "end": "6725040"
  },
  {
    "text": "one okay so this guy is actually going to connect uh he's going to connect to a",
    "start": "6725639",
    "end": "6731280"
  },
  {
    "text": "server and he's going to read the incoming data from that server and dump it into a paret file um since I'm",
    "start": "6731280",
    "end": "6737480"
  },
  {
    "text": "writing into a parquet file I'm going to use a data frame it makes it easier so what do we what does that server look",
    "start": "6737480",
    "end": "6743280"
  },
  {
    "text": "like U let me show you I hope it's still open let me get a command window here",
    "start": "6743280",
    "end": "6750840"
  },
  {
    "text": "all right um I don't know why this thing won't let",
    "start": "6750840",
    "end": "6757599"
  },
  {
    "text": "me drag across you're right it is that's the",
    "start": "6757599",
    "end": "6763840"
  },
  {
    "text": "problem and chrome does not let me un full screen there we press the green the",
    "start": "6764079",
    "end": "6769440"
  },
  {
    "text": "there we go okay I so you can tell I so rarely I use a Mac all the time I rarely put anything",
    "start": "6769440",
    "end": "6775840"
  },
  {
    "text": "into full screen all right so where's This Server running it's running on some source that I happen to",
    "start": "6775840",
    "end": "6782239"
  },
  {
    "text": "have out here on Port 26656 I believe no",
    "start": "6782239",
    "end": "6789400"
  },
  {
    "text": "255 let me double check that 25 this is my my daughter's Minecraft server and that's the only Port that's open so",
    "start": "6789400",
    "end": "6795760"
  },
  {
    "text": "that's the port I'm running on rather than fiddle with the firewall okay so you can see it's dumping log data",
    "start": "6795760",
    "end": "6801920"
  },
  {
    "text": "out this server basically just randomly generates log data and we're going to analyze this",
    "start": "6801920",
    "end": "6808880"
  },
  {
    "text": "stuff right so what I want to be able to do is connect a stream to this thing right and then we're going to take a",
    "start": "6808880",
    "end": "6814599"
  },
  {
    "text": "look at some of these messages using um uh using spark so first things",
    "start": "6814599",
    "end": "6823639"
  },
  {
    "text": "first I've got I've got some imports up here that we need to pull in and then this is where we're going to",
    "start": "6823639",
    "end": "6830280"
  },
  {
    "text": "dump these files as they come in we're going to dump them into a parquet file all right and once again I have an",
    "start": "6830280",
    "end": "6835960"
  },
  {
    "text": "object down here let me um let me first do this I'm making sure that uh that the directory is empty so we're going to",
    "start": "6835960",
    "end": "6843880"
  },
  {
    "text": "remove it and create it again so in here what I'm doing um is using this really",
    "start": "6843880",
    "end": "6849880"
  },
  {
    "text": "horrible funky regular expression to pick apart the pieces of the log line the time stamp the uh the type the error",
    "start": "6849880",
    "end": "6857199"
  },
  {
    "text": "or whatever and the the string and we're going to map them into a log",
    "start": "6857199",
    "end": "6862360"
  },
  {
    "text": "message case class right notice the use of timestamp here turns out if you try",
    "start": "6862360",
    "end": "6867599"
  },
  {
    "text": "to use Java util date you'll get a runtime error okay it wants a SQL time stamp um the reason is that it turns out",
    "start": "6867599",
    "end": "6875280"
  },
  {
    "text": "there are two SQL parsers there's the default one that comes with spark if you just pull spark down or build it and",
    "start": "6875280",
    "end": "6881320"
  },
  {
    "text": "then there's the hive SQL parser so if you P build spark with Hive support which is not the default because their",
    "start": "6881320",
    "end": "6887639"
  },
  {
    "text": "hive has about a several bazillion dependencies but if you build it with Hive support that doesn't necessarily",
    "start": "6887639",
    "end": "6893199"
  },
  {
    "text": "mean that you're going to be reading Hive tables but it will what it will use is the hive ql parser which is much more",
    "start": "6893199",
    "end": "6899719"
  },
  {
    "text": "Rich um but the hiveql parser is what dictates which types are supported so",
    "start": "6899719",
    "end": "6904960"
  },
  {
    "text": "I'm going to convert this to a timestamp here's the stop this is a better stop okay get active returns an option so",
    "start": "6904960",
    "end": "6911560"
  },
  {
    "text": "obviously I'm mapping over the option I'm doing a stop on it and note that I pass false in here for whatever reason",
    "start": "6911560",
    "end": "6917800"
  },
  {
    "text": "this is true so a really good way to destroy your notebook or your your spark shell is to allow stop spark context to",
    "start": "6917800",
    "end": "6925719"
  },
  {
    "text": "default because that means your spark context which comes for free whenever you sp fire up spark shell will go away",
    "start": "6925719",
    "end": "6932040"
  },
  {
    "text": "and then you have to restart the shell which is really irritating um we start it up we create the uh streaming context",
    "start": "6932040",
    "end": "6938280"
  },
  {
    "text": "here's a date parser there's my connection to the socket okay connect to",
    "start": "6938280",
    "end": "6943960"
  },
  {
    "text": "that particular host on that particular Port okay we're going to determine which logs to keep by flat mapping over the",
    "start": "6943960",
    "end": "6950760"
  },
  {
    "text": "the uh the dam okay why am I flat mapping I'm using",
    "start": "6950760",
    "end": "6955960"
  },
  {
    "text": "a trick here I'm say I'm doing a line match if the line matches this regular expression then I'm going to convert the",
    "start": "6955960",
    "end": "6963520"
  },
  {
    "text": "time stamp into a a true Tim stamp and save it inside a",
    "start": "6963520",
    "end": "6970040"
  },
  {
    "text": "sum okay but if the message type is not error or warn I'm going to return a none",
    "start": "6970040",
    "end": "6975560"
  },
  {
    "text": "and if it doesn't match this pattern then it's a corrupt or mouth formed line and I'm going to pass a nun back what happens if I if I create an array",
    "start": "6975560",
    "end": "6983159"
  },
  {
    "text": "and I fill it with options and then I flat map",
    "start": "6983159",
    "end": "6987280"
  },
  {
    "text": "it all the nuns go away why does that happen um so the why why that happens",
    "start": "6990119",
    "end": "6995599"
  },
  {
    "text": "this is Scala not not spark why that happens is what happens if I take an array of arrays and flatmap that I get",
    "start": "6995599",
    "end": "7001760"
  },
  {
    "text": "it it flattens out one level of the array right scholar programmers think of option as a collection of zero or one",
    "start": "7001760",
    "end": "7007880"
  },
  {
    "text": "things if you model an option in your head as an array containing either nothing or one thing then flat mapping",
    "start": "7007880",
    "end": "7015239"
  },
  {
    "text": "options make sense the options the sums get unpacked and the nuns go away by",
    "start": "7015239",
    "end": "7021320"
  },
  {
    "text": "doing this flat map with options I'm essentially turning this flat map into a combination of a map and a",
    "start": "7021320",
    "end": "7027119"
  },
  {
    "text": "filter right anything that returns a none gets thrown away anything that returns a sum gets",
    "start": "7027119",
    "end": "7032920"
  },
  {
    "text": "unpacked so what I've got at the end now is a deream of the logs I care about and",
    "start": "7032920",
    "end": "7038040"
  },
  {
    "text": "so now what I'm going to do is a 4 rdd on them now remember these things this Dam coming out of here is a dam of type",
    "start": "7038040",
    "end": "7045040"
  },
  {
    "text": "log message that case class up here right so now I can just call 2df on that",
    "start": "7045040",
    "end": "7050840"
  },
  {
    "text": "to convert that into a data frame um and the main reason I'm doing that here is because it's easy to write paret files",
    "start": "7050840",
    "end": "7056079"
  },
  {
    "text": "with a data frame but I could do all kinds of other data framy stuff on here if I wanted okay so then I start this and",
    "start": "7056079",
    "end": "7064000"
  },
  {
    "text": "because I'm running it inside um uh inside a notebook I'm not going to bother calling a wait question",
    "start": "7064000",
    "end": "7070280"
  },
  {
    "text": "yeah how would you um other than making your interval time very large prevent",
    "start": "7070280",
    "end": "7075920"
  },
  {
    "text": "writing small files which hdfs kind of fall yeah that's actually a problem with hdfs you don't want to be dumping this",
    "start": "7075920",
    "end": "7082119"
  },
  {
    "text": "stuff to hdfs because you're getting you're and even if it's fronted with Hive you get these tiny tiny files so",
    "start": "7082119",
    "end": "7087480"
  },
  {
    "text": "that's you could set the block size larger I mean the batch interval larger um or you could just choose not to dump",
    "start": "7087480",
    "end": "7094079"
  },
  {
    "text": "them to htfs dump them to something like Cassandra right where that's not likely to be as big a problem um and I you",
    "start": "7094079",
    "end": "7101599"
  },
  {
    "text": "might just choose a different data output for that in this particular case that's probably also true of par parquet",
    "start": "7101599",
    "end": "7106840"
  },
  {
    "text": "files right anything that's small in hdfs Falls over but a parquet file is not hdfs but but in this case I'll get",
    "start": "7106840",
    "end": "7113119"
  },
  {
    "text": "individual slots inside the parquet file that aren't very big which isn't the most efficient storage for paret either",
    "start": "7113119",
    "end": "7118840"
  },
  {
    "text": "but for a demo that's okay in production I probably dump this into cassander or something right so I'm going to Define",
    "start": "7118840",
    "end": "7125639"
  },
  {
    "text": "this Runner here now I always um when I'm doing this stuff inside a notebook um I",
    "start": "7125639",
    "end": "7131000"
  },
  {
    "text": "always do something like this this basically serializes my Runner",
    "start": "7131000",
    "end": "7137000"
  },
  {
    "text": "object to a bite stream that I promptly throw away a bite a bite array output",
    "start": "7137000",
    "end": "7142159"
  },
  {
    "text": "stream um there's only one reason for doing this if this thing isn't serializable because I picked up",
    "start": "7142159",
    "end": "7148040"
  },
  {
    "text": "something that isn't serializable I will get a runtime error and the likelihood of the runtime error being readable is a",
    "start": "7148040",
    "end": "7153639"
  },
  {
    "text": "little higher here than it is if spark generates it so I always like to write a couple of line test here just to verify",
    "start": "7153639",
    "end": "7161119"
  },
  {
    "text": "you know will this thing serialize and so therefore we're good so we should be able to start this so I'm going to stop any existing context and start a new one",
    "start": "7161119",
    "end": "7168920"
  },
  {
    "text": "and then we're going to let this run a little bit um I suspect there's nothing there yet oh no look we're we're getting",
    "start": "7168920",
    "end": "7174840"
  },
  {
    "text": "some stuff so we're building a paret file here of junk right it's getting bigger so data",
    "start": "7174840",
    "end": "7182560"
  },
  {
    "text": "is streaming in I'm going to leave it run for a minute and show you the spark UI that'll pop up in",
    "start": "7182560",
    "end": "7188400"
  },
  {
    "text": "another by the way this is not the datab bricks UI right now I'm inside data bricks but this is the standard spark UI",
    "start": "7188400",
    "end": "7194520"
  },
  {
    "text": "when you bring spark up the workers and the U master um open up a they they listen and",
    "start": "7194520",
    "end": "7201800"
  },
  {
    "text": "produce web traffic so you can connect to the master and ask questions of it and you connect connect to the",
    "start": "7201800",
    "end": "7206960"
  },
  {
    "text": "individual workers and ask them questions and you can do that irrespective of whether you're running inside this environment or not as long",
    "start": "7206960",
    "end": "7213079"
  },
  {
    "text": "as you know what it is you can connect your browser to it so this is the standard stock spark UI that I'm looking at here and you'll note that there a",
    "start": "7213079",
    "end": "7219360"
  },
  {
    "text": "streaming tab so I can take a look at these statistics about streaming I can see that these guys are queued but one",
    "start": "7219360",
    "end": "7225880"
  },
  {
    "text": "guy is actually being processed okay um if all I saw was cued here that would mean I have a problem",
    "start": "7225880",
    "end": "7232719"
  },
  {
    "text": "I'm receiving data but nobody's processing it all right so I'm going to go back over here I think that's run long enough",
    "start": "7232719",
    "end": "7240360"
  },
  {
    "text": "we're going to stop it let me go back up here and stop it",
    "start": "7240360",
    "end": "7247560"
  },
  {
    "text": "and take another quick look okay now it's busy flushing a bunch of stuff okay",
    "start": "7247560",
    "end": "7253679"
  },
  {
    "text": "we got a bunch of crap in",
    "start": "7253679",
    "end": "7256760"
  },
  {
    "text": "here yeah quite a few files right at which point what I can do is some data",
    "start": "7261719",
    "end": "7266760"
  },
  {
    "text": "framy stuff here so what I'm going to do is since that's a parquet file I can jump right into a data frame",
    "start": "7266760",
    "end": "7273079"
  },
  {
    "text": "self-describing schema read that parquet file there it is and then this is the",
    "start": "7273079",
    "end": "7279719"
  },
  {
    "text": "schema that it inferred okay if I want that in a prettier format by the way um there's Al",
    "start": "7279719",
    "end": "7285880"
  },
  {
    "text": "there's also",
    "start": "7285880",
    "end": "7288440"
  },
  {
    "text": "this all right so there's the schema that it inferred from that file all",
    "start": "7291280",
    "end": "7296560"
  },
  {
    "text": "right this display function is not a spark function this display function is particular to this environment it just",
    "start": "7296560",
    "end": "7302760"
  },
  {
    "text": "makes the output look nicer you could do you could do this this is part of um",
    "start": "7302760",
    "end": "7308159"
  },
  {
    "text": "data frames show um and you get this looks a lot like a MySQL table bump right the",
    "start": "7308159",
    "end": "7315520"
  },
  {
    "text": "defaults to the top 20 of them but I find if I'm in this particular environment I'd rather use that display",
    "start": "7315520",
    "end": "7321560"
  },
  {
    "text": "helper I get a nice table output and if the stuff is actually graph I can do data visualization right in here so here",
    "start": "7321560",
    "end": "7328760"
  },
  {
    "text": "I can see I've got errors and warnings right oh",
    "start": "7328760",
    "end": "7334320"
  },
  {
    "text": "look four space aliens are fiddling with the hardware right so Lo yeah that's that's",
    "start": "7334320",
    "end": "7341040"
  },
  {
    "text": "in there for there's",
    "start": "7341040",
    "end": "7346760"
  },
  {
    "text": "a yeah there's a bunch of silly stuff in here for people who are watching um I don't know if I don't see it yet but",
    "start": "7346760",
    "end": "7353000"
  },
  {
    "text": "there's a um right there's no such thing as a free lunch um there is a there is a Monty",
    "start": "7353000",
    "end": "7361000"
  },
  {
    "text": "Python gag in here somewhere the one of the Fay rods one of the and there's um",
    "start": "7361000",
    "end": "7366159"
  },
  {
    "text": "there's another one in here that says your mother called you never call her you know stuff like that so there there",
    "start": "7366159",
    "end": "7372119"
  },
  {
    "text": "it is there's your Monty Python thing okay so we could now use",
    "start": "7372119",
    "end": "7377159"
  },
  {
    "text": "data frames to figure out things like um let's get all the errors out of here so",
    "start": "7377159",
    "end": "7382760"
  },
  {
    "text": "there's another data frame all right and what are the errors I'm not interested in the warnings what",
    "start": "7382760",
    "end": "7388760"
  },
  {
    "text": "are some of the errors okay there are some of the errors and let's actually do this as SQL so I'm going to register",
    "start": "7388760",
    "end": "7395400"
  },
  {
    "text": "this as a temporary table um and now I can do a SQL query on here so I want to see all the ones that",
    "start": "7395400",
    "end": "7401679"
  },
  {
    "text": "have the word device in them okay and there's the word device right and here",
    "start": "7401679",
    "end": "7406960"
  },
  {
    "text": "I'm going to use an R like to look up alien that's a regular expression match",
    "start": "7406960",
    "end": "7412079"
  },
  {
    "text": "anything that has upper or lowercase alien so how many times were aliens screwing around with us during this run",
    "start": "7412079",
    "end": "7418239"
  },
  {
    "text": "and there you go we have a lot of space alien",
    "start": "7418239",
    "end": "7422678"
  },
  {
    "text": "activity right so this is just an example of what you can do with with streaming",
    "start": "7424400",
    "end": "7429679"
  },
  {
    "text": "um and these are you know rather toy examples but uh but they give you the flavor of what you can do yes I assume",
    "start": "7429679",
    "end": "7435679"
  },
  {
    "text": "that's a shortcut for SQL context. SQL yes in fact that's exactly what this is",
    "start": "7435679",
    "end": "7441639"
  },
  {
    "text": "this is um a a I can take this exact same thing and I can choose to do this",
    "start": "7441639",
    "end": "7448119"
  },
  {
    "text": "I'm going to show you the the real data framy way to do it if you don't have this particular notebook environment all",
    "start": "7448119",
    "end": "7454719"
  },
  {
    "text": "um spark shells and these notebooks have something called a SQL context this happens to be a hive context which",
    "start": "7454719",
    "end": "7460920"
  },
  {
    "text": "means this was compiled with Hive support So if I were doing this in a program I wouldn't have access to that",
    "start": "7460920",
    "end": "7466159"
  },
  {
    "text": "percent SQL thing so I would just do this instead now what this is going to",
    "start": "7466159",
    "end": "7472760"
  },
  {
    "text": "return is uh what what do you suppose this type",
    "start": "7474040",
    "end": "7479639"
  },
  {
    "text": "is want SQL yes I do it's hard to type over my neck it's",
    "start": "7479639",
    "end": "7485960"
  },
  {
    "text": "data frame so it is literally another data frame at which point I could do a show",
    "start": "7485960",
    "end": "7492119"
  },
  {
    "text": "on it right or since I happen to be in this notebook environment I could do a display x on it and you know that'll run",
    "start": "7492119",
    "end": "7499119"
  },
  {
    "text": "the whole thing through that under the covers that does a a the equivalent of a",
    "start": "7499119",
    "end": "7504199"
  },
  {
    "text": "the equivalent of a collect one top 10,000 entries right and then it will show me",
    "start": "7504199",
    "end": "7509280"
  },
  {
    "text": "this stuff so this is how you would do this in your own spark shell on your own",
    "start": "7509280",
    "end": "7514719"
  },
  {
    "text": "machine and that's that's what two hours we did that that's pretty much uh that's",
    "start": "7515079",
    "end": "7520559"
  },
  {
    "text": "pretty much the whole thing we lost about half the crowd because I went to I I don't think I could have done this in less than two hours and covered it",
    "start": "7520559",
    "end": "7526440"
  },
  {
    "text": "properly so I apologize for keeping you all out so late but um thanks a l but that's it and if there are any",
    "start": "7526440",
    "end": "7533079"
  },
  {
    "text": "questions any um any any remaining",
    "start": "7533079",
    "end": "7538599"
  },
  {
    "text": "questions okay so one which is um how what use cases like the the power that I",
    "start": "7538599",
    "end": "7546199"
  },
  {
    "text": "see here is if I wanted super low late ly I wouldn't bother with something like this I'd maybe pull something off of cof",
    "start": "7546199",
    "end": "7552079"
  },
  {
    "text": "or whatever but if I can tolerate a little bit of latency like near real time right I can get the declarative",
    "start": "7552079",
    "end": "7557760"
  },
  {
    "text": "processing of spark and so I could do machine learning where where models update themselves based off of new data",
    "start": "7557760",
    "end": "7564079"
  },
  {
    "text": "coming in and like that so what are some example use cases for streaming that we've seen is that um well some of",
    "start": "7564079",
    "end": "7570360"
  },
  {
    "text": "that show up in some of the Decks that we show are imagine you're reading smart meters and reading live weather data and",
    "start": "7570360",
    "end": "7576400"
  },
  {
    "text": "correlating those things for anomaly detection on the fly to do some predictive analysis of like how much how",
    "start": "7576400",
    "end": "7582400"
  },
  {
    "text": "much electrical power am I going to to need over the next 24 hours but there a couple of places a couple of places",
    "start": "7582400",
    "end": "7588199"
  },
  {
    "text": "where I recently taught some spark courses were telecoms and what these guys want to be able to do is um analyze",
    "start": "7588199",
    "end": "7594199"
  },
  {
    "text": "call patterns right um for a whole bunch of different reasons um you know they want",
    "start": "7594199",
    "end": "7599840"
  },
  {
    "text": "to be able to they want to be able to do graph analysis and there is a graph library in here for calling graphs right",
    "start": "7599840",
    "end": "7605400"
  },
  {
    "text": "this would help them decide for example things like what plans to offer um or maybe what what load to be able to",
    "start": "7605400",
    "end": "7611280"
  },
  {
    "text": "handle um you know there are all kinds of of of those kinds of processings that you might want to do on live incoming",
    "start": "7611280",
    "end": "7617639"
  },
  {
    "text": "call data um you know so there's that there's um there's another uh company that wants to use it to",
    "start": "7617639",
    "end": "7623960"
  },
  {
    "text": "do uh credit card uh fraud detection um another example that we saw",
    "start": "7623960",
    "end": "7630199"
  },
  {
    "text": "at a company uh that runs there there a competitor kind of a competitor to square they do point of sale systems",
    "start": "7630199",
    "end": "7637599"
  },
  {
    "text": "right and so want to do streaming and they want to use that to do um predictions on things coming in um",
    "start": "7637599",
    "end": "7644400"
  },
  {
    "text": "there's another company a credit card company that wants to be able to to use this kind of incoming data to be able to",
    "start": "7644400",
    "end": "7651679"
  },
  {
    "text": "predict hey this person's getting near their credit limit this might be a really good time to offer them a deal",
    "start": "7651679",
    "end": "7657840"
  },
  {
    "text": "right or to have uh to have somebody call them and say Hey you know for another $10 we could raise your credit",
    "start": "7657840",
    "end": "7663880"
  },
  {
    "text": "limit or something and they wouldn't be able to do this in In from from from the standpoint point of customer service",
    "start": "7663880",
    "end": "7669559"
  },
  {
    "text": "they want to be able to do that in in a lot closer to real time they want to be able to get the customer an answer in",
    "start": "7669559",
    "end": "7675159"
  },
  {
    "text": "like 10 minutes as opposed to 24 hours on things yeah it's likely I we have a use case coming up where it's like it's",
    "start": "7675159",
    "end": "7681920"
  },
  {
    "text": "going to be a really good fit we have right now we have predictive models that are built used in pmnl but they're",
    "start": "7681920",
    "end": "7689440"
  },
  {
    "text": "static and they have to be refresh manual we're going to need self-healing ones that adjust a new signal in the market um pretty soon so we're going to",
    "start": "7689440",
    "end": "7696520"
  },
  {
    "text": "have a sort of a network of self healing self-adjusting models this would be a great way to kind do that well so prior",
    "start": "7696520",
    "end": "7702760"
  },
  {
    "text": "to going out on my own I worked for a company that did um uh Internet advertising and one of the things that",
    "start": "7702760",
    "end": "7709159"
  },
  {
    "text": "that we had to do was to uh analyze the logs right how's this campaign going",
    "start": "7709159",
    "end": "7714360"
  },
  {
    "text": "well how do you find out you look at the logs you figure out where were the ads delivered who how many people saw them",
    "start": "7714360",
    "end": "7719880"
  },
  {
    "text": "you know what information can we glean from that demographically right this was Hadoop days so what happened is the logs",
    "start": "7719880",
    "end": "7726639"
  },
  {
    "text": "would collect and then you'd run these long Hadoop map produce jobs on them that would reduce these logs and",
    "start": "7726639",
    "end": "7731760"
  },
  {
    "text": "correlate them down to a point where they could be loaded into a data warehouse think about what I do with spark I could do that stuff you know as",
    "start": "7731760",
    "end": "7738159"
  },
  {
    "text": "the log data came in you essentially have the the web server create these log files and whenever you got to a log",
    "start": "7738159",
    "end": "7744520"
  },
  {
    "text": "rolling point you roll the log and dump the rolled log into an hdfs directory",
    "start": "7744520",
    "end": "7750000"
  },
  {
    "text": "right and then you've got a streaming job watching that directory boom new log shows up immediate real-time analysis of",
    "start": "7750000",
    "end": "7756679"
  },
  {
    "text": "that and it doesn't even need to be real time analysis basically it could be something as simple as real time pairing",
    "start": "7756679",
    "end": "7762400"
  },
  {
    "text": "down and correlation so that we can dump it efficiently into the data warehouse which would mean then of course that you",
    "start": "7762400",
    "end": "7768760"
  },
  {
    "text": "could be doing analysis on this stuff all day long out of the data warehouse right um so there I mean to me I can",
    "start": "7768760",
    "end": "7774760"
  },
  {
    "text": "think of a number of different use cases for this well so since it boils down to okay so the question for the camera is",
    "start": "7774760",
    "end": "7781400"
  },
  {
    "text": "you know what if you're pulling in real-time streaming data but you're wanting to combine that with something",
    "start": "7781400",
    "end": "7786599"
  },
  {
    "text": "that's static that's not streaming the the bottom line is that um that that boils down to can I open can I create",
    "start": "7786599",
    "end": "7793719"
  },
  {
    "text": "two rdds and correlate data between them never mind streaming right and the",
    "start": "7793719",
    "end": "7799440"
  },
  {
    "text": "answer is absolutely you can right there are operations on rdds and on data",
    "start": "7799440",
    "end": "7804520"
  },
  {
    "text": "frames for that example like join right or um or Co a co-group is is a a good",
    "start": "7804520",
    "end": "7810559"
  },
  {
    "text": "call in an rdd that's like a join right so if I can take two static rdds and",
    "start": "7810559",
    "end": "7816239"
  },
  {
    "text": "group them together like that and cross correlate them which I can or Bridge those two rdds up into Data frames so",
    "start": "7816239",
    "end": "7822599"
  },
  {
    "text": "that I can actually do SQL style joins which my brain you know understands a little bit better then there's nothing",
    "start": "7822599",
    "end": "7828480"
  },
  {
    "text": "that says I can't create a static data a static data frame let's say of that static data and then um and and find a",
    "start": "7828480",
    "end": "7836320"
  },
  {
    "text": "good point to Cache that right and then as the streams start coming in take each rdd that comes in and do the same join",
    "start": "7836320",
    "end": "7842880"
  },
  {
    "text": "against that same data because at the point at which you get an rdd it's just",
    "start": "7842880",
    "end": "7848320"
  },
  {
    "text": "an rdd right remember what's happening is that you've got this continuous stream of data coming in but it's being",
    "start": "7848320",
    "end": "7853920"
  },
  {
    "text": "broken up into rdds and each rdd is being processed one at a time and it's",
    "start": "7853920",
    "end": "7860320"
  },
  {
    "text": "just another rdd it's just another spark job processing an rdd so there's absolutely nothing that says you can't",
    "start": "7860320",
    "end": "7866079"
  },
  {
    "text": "join that with some other rdd or convert it to a data frame and join it with another existing data frame which could",
    "start": "7866079",
    "end": "7872679"
  },
  {
    "text": "be a data frame for another stream or it could be a data frame that for something coming out of Cassandra",
    "start": "7872679",
    "end": "7879040"
  },
  {
    "text": "I mean it just works the whole point behind this model is it is it is giving you something the underlying rdd that's",
    "start": "7879040",
    "end": "7886040"
  },
  {
    "text": "what spark is built on you know you're operating at the what what's harder though what's harder is to say what if",
    "start": "7886040",
    "end": "7893559"
  },
  {
    "text": "what I want to do for fraud detection is correlate all this streaming data over",
    "start": "7893559",
    "end": "7899800"
  },
  {
    "text": "the last five hours okay now you have a problem right because each rdd that comes in gets",
    "start": "7899800",
    "end": "7906400"
  },
  {
    "text": "processed and thrown away and if you want to do that kind of real-time correlation do you really you",
    "start": "7906400",
    "end": "7912760"
  },
  {
    "text": "could create a sliding window but do you really want to create a sliding window that goes back five hours you you might",
    "start": "7912760",
    "end": "7918559"
  },
  {
    "text": "be reaching the limits of your memory you might have well exceeded it at that point depending on the volume of data that comes in in a batch so now you have",
    "start": "7918559",
    "end": "7925520"
  },
  {
    "text": "to play other games like all right if I'm going to do a correlation um of streaming data with other streaming data",
    "start": "7925520",
    "end": "7931599"
  },
  {
    "text": "over five hours so that I can do a Time series projection over the last 5 hours what I would probably choose to do is",
    "start": "7931599",
    "end": "7937559"
  },
  {
    "text": "dump that into to a persistent store of some kind where I can actually do that kind of analysis offline so you would",
    "start": "7937559",
    "end": "7943719"
  },
  {
    "text": "have a persistent store that has a um well yeah and and so then what",
    "start": "7943719",
    "end": "7949480"
  },
  {
    "text": "would the what would the point of your streaming job be the point of your streaming job would be to take the incoming stream data filter it down",
    "start": "7949480",
    "end": "7957320"
  },
  {
    "text": "convert it to the format that is appropriate throwing away any extraneous data you don't care about and rapidly",
    "start": "7957320",
    "end": "7963599"
  },
  {
    "text": "getting into into Cassandra or whatever your persistent store is if there's a Time series we want that last of data to",
    "start": "7963599",
    "end": "7971440"
  },
  {
    "text": "put right right right so the I mean the point here is is you almost have to think about streaming as um how would I",
    "start": "7971440",
    "end": "7979199"
  },
  {
    "text": "process a continuous stream of TCP data it's the exact same problem right how do",
    "start": "7979199",
    "end": "7984599"
  },
  {
    "text": "I look five hours back in in a stream of TCP data hey well unless I put it",
    "start": "7984599",
    "end": "7989920"
  },
  {
    "text": "somewhere I can't right unless I managed somehow to keep it around for five hours I can't do that so it's really the same",
    "start": "7989920",
    "end": "7996880"
  },
  {
    "text": "problem it's just the spark gives you a couple extra tools you do have window for a limited look back um and you have",
    "start": "7996880",
    "end": "8002880"
  },
  {
    "text": "the ease of dumping into whatever persistent store you want which is what you'd probably be doing with a TCP",
    "start": "8002880",
    "end": "8007960"
  },
  {
    "text": "stream anyway right there's a book um by the guy Nathan something the guy who",
    "start": "8007960",
    "end": "8013199"
  },
  {
    "text": "invented storm yeah yeah the land really good book I God forbid the name of the",
    "start": "8013199",
    "end": "8018800"
  },
  {
    "text": "book is actually Big Data please but but it's actually a very good book and it's about he has this kind of reference",
    "start": "8018800",
    "end": "8024280"
  },
  {
    "text": "architecture that talks about you know the batch layer the real you know what I",
    "start": "8024280",
    "end": "8030840"
  },
  {
    "text": "mean well yeah but I mean but but the problem is I mean these are what it comes down to is that these are all just",
    "start": "8032079",
    "end": "8038559"
  },
  {
    "text": "tools to implement um to implement the solution and they all have many of the same problems when you come right down",
    "start": "8038559",
    "end": "8045280"
  },
  {
    "text": "to it right um they have the same problems of look back they have the same problems of how do I process this this",
    "start": "8045280",
    "end": "8052119"
  },
  {
    "text": "stuff quickly um where do I put this stuff if I'm stuffing it in hdfs how do I get past HD F s's potential slowness",
    "start": "8052119",
    "end": "8060520"
  },
  {
    "text": "you know how do i b buffer up enough data so I don't run into the tiny files problem you know they all have the same",
    "start": "8060520",
    "end": "8065599"
  },
  {
    "text": "problem storm has the same problem in fact one of the reasons storm actually exists is because they were having",
    "start": "8065599",
    "end": "8071320"
  },
  {
    "text": "problems with Hadoop and hdfs you know that guy that's actually one of the things the guy mentioned in the the",
    "start": "8071320",
    "end": "8077280"
  },
  {
    "text": "Philly ET talk that they gave on Twitter storm about three years ago they all have the same problem it's just a matter",
    "start": "8077280",
    "end": "8082880"
  },
  {
    "text": "which vocabulary you're you're using and the nice thing about spark streaming is that the vocabulary I mean you saw it",
    "start": "8082880",
    "end": "8089320"
  },
  {
    "text": "it's not that much different from the rdd apis you know the vocabulary is consistent at least at a conceptual",
    "start": "8089320",
    "end": "8095639"
  },
  {
    "text": "level with everything else in spark so the the um the the the learning overhead",
    "start": "8095639",
    "end": "8101960"
  },
  {
    "text": "if you're already using spark is minimal they're also doing something similar um",
    "start": "8101960",
    "end": "8107360"
  },
  {
    "text": "for graph processing there's this tool called graphx that comes with spark that is doing graph analysis and it's the",
    "start": "8107360",
    "end": "8114559"
  },
  {
    "text": "same sort of idea we're going to build up these graphing algorithms but they're built up in such a way that",
    "start": "8114559",
    "end": "8120079"
  },
  {
    "text": "the graph analysis then can then be distributed across the spark cluster the same way so you can do things like page",
    "start": "8120079",
    "end": "8126000"
  },
  {
    "text": "Rank and and recommendation engines and I'm sorry those are ml libraries but I mean you can do the same kinds of not",
    "start": "8126000",
    "end": "8132559"
  },
  {
    "text": "page rank I was thinking more like the one example we gave was um it's actually called a property graph we were using it",
    "start": "8132559",
    "end": "8139079"
  },
  {
    "text": "um I like to tell this demo as the um the world's worst dating site because there are five people in the in their in",
    "start": "8139079",
    "end": "8146480"
  },
  {
    "text": "the data but we were doing graph analysis to try to figure out like which two should get matched up right and it's",
    "start": "8146480",
    "end": "8151639"
  },
  {
    "text": "the same sort of thing it's it's intended to sit on top of of um of of spark core and distribute this stuff out",
    "start": "8151639",
    "end": "8157960"
  },
  {
    "text": "across the cluster the same with the the ml the machine learning stuff you can certainly do machine learning you know",
    "start": "8157960",
    "end": "8164079"
  },
  {
    "text": "within one jbm but if you can do machine learning across the cluster it's likely to to be faster and there are in fact",
    "start": "8164079",
    "end": "8171199"
  },
  {
    "text": "ways in which to tie uh uh machine learning to incoming streaming data so you could imagine using data coming in",
    "start": "8171199",
    "end": "8178119"
  },
  {
    "text": "to feed your your learning algorithm right or having already run your learning algorithm you could take",
    "start": "8178119",
    "end": "8184679"
  },
  {
    "text": "the existing model and run that against streaming data and do your predictions you know we've got predictions coming in",
    "start": "8184679",
    "end": "8190159"
  },
  {
    "text": "oh my God we're going to run out of in inventory on this item by the end of the day it's time to order something or oh",
    "start": "8190159",
    "end": "8196399"
  },
  {
    "text": "my God you know we're going to run out of electrical capacity bring the wind turbines online or or whatever you know",
    "start": "8196400",
    "end": "8202760"
  },
  {
    "text": "so it's intending to tie all of this stuff together so you can mix and match",
    "start": "8202760",
    "end": "8209678"
  },
  {
    "text": "great St all right well I think we're uh I think we're done we can shut the camera down and um thanks guys I",
    "start": "8209840",
    "end": "8216280"
  },
  {
    "text": "appreciate those of you who hung this late I appreciate",
    "start": "8216281",
    "end": "8220199"
  },
  {
    "text": "it",
    "start": "8228120",
    "end": "8231120"
  }
]