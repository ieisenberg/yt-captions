[
  {
    "text": "all right So we're all in the correct room That's",
    "start": "10160",
    "end": "15519"
  },
  {
    "text": "good Uh we're going to talk today about uh topologies for cost-saving",
    "start": "15519",
    "end": "20960"
  },
  {
    "text": "autoscaling Um and just to get you prepared it's not going to be like I'm showing you this is how you're going to",
    "start": "20960",
    "end": "26720"
  },
  {
    "text": "autoscale your uh environment but rather uh ways to think about autoscaling and",
    "start": "26720",
    "end": "32558"
  },
  {
    "text": "what are the pitfalls in the architecture of open search that limit autoscaling in reality And to kick us",
    "start": "32559",
    "end": "39360"
  },
  {
    "text": "off I'm going to start start talking about um storing objects um actual objects uh icecore",
    "start": "39360",
    "end": "46360"
  },
  {
    "text": "samples If you're unfamiliar icecore samples are drilled or these cylinders drilled from ice sheets or glaciers and",
    "start": "46360",
    "end": "53520"
  },
  {
    "text": "they provide us a record of Earth's climate and um environment Um the",
    "start": "53520",
    "end": "59280"
  },
  {
    "text": "interesting thing I believe and relevant to us uh in this in these ice uh core",
    "start": "59280",
    "end": "65518"
  },
  {
    "text": "samples is that when they arrive at the storage facility they are parsed And if you think about it this is probably the",
    "start": "65519",
    "end": "71760"
  },
  {
    "text": "most columnar data of any columnar data that we have It's a literal column and",
    "start": "71760",
    "end": "77840"
  },
  {
    "text": "it's uh sorted by time stamp right You have the new ice at the top uh and the",
    "start": "77840",
    "end": "82880"
  },
  {
    "text": "old ice at the bottom And the way the scientific community has decided to",
    "start": "82880",
    "end": "87920"
  },
  {
    "text": "parse this data is in the middle of the slide And it's very clear to them uh that this is how they want it parsed And",
    "start": "87920",
    "end": "94960"
  },
  {
    "text": "this person managing the storage facility is going to parse the data that way all of it And because it's because",
    "start": "94960",
    "end": "102320"
  },
  {
    "text": "the scientific community has a very narrow um span of inquiry regarding this type",
    "start": "102320",
    "end": "109600"
  },
  {
    "text": "of data it is easy to store it It is easy to make it very compact And you can",
    "start": "109600",
    "end": "115119"
  },
  {
    "text": "see the storage facility is kind of boring It's uh shelves everything's kind of condensed Um a visitor arriving uh at",
    "start": "115119",
    "end": "123360"
  },
  {
    "text": "the facility has an easy time looking for things It's very well sorted and structured If we take a hypothetical",
    "start": "123360",
    "end": "129599"
  },
  {
    "text": "example of lots of visitors coming and the person here who is managing the storage facility wants to um scale out",
    "start": "129599",
    "end": "137520"
  },
  {
    "text": "he wants to be able to uh accommodate much many more visitors at a time What he'll do is he'll take all those icecore",
    "start": "137520",
    "end": "143440"
  },
  {
    "text": "samples Let's just this is totally hypothetical Please don't be offended if you're in the if you're in this field",
    "start": "143440",
    "end": "148480"
  },
  {
    "text": "and cut them in half Okay so that's just time divided by two That's easy and uh",
    "start": "148480",
    "end": "154480"
  },
  {
    "text": "add a room put everything in the you know all these halves in another room and you can kind of spread out the load",
    "start": "154480",
    "end": "160640"
  },
  {
    "text": "The read load will kind of be spread it out and it really makes things easy It's practically easy to think about how",
    "start": "160640",
    "end": "166080"
  },
  {
    "text": "you'd scale out such a facility Let's talk about a different object storage facility like a museum",
    "start": "166080",
    "end": "172879"
  },
  {
    "text": "where we don't really know what kind of samples are coming in Uh if you have a new sample coming in it could be a",
    "start": "172879",
    "end": "178400"
  },
  {
    "text": "statue it could be an archaeological artifact Uh it could be a postmodern sculpture of the Kraken or a dinosaur",
    "start": "178400",
    "end": "184720"
  },
  {
    "text": "bones Um h how do we index these things in a way that they're easy to search It's very hard Uh one of the things",
    "start": "184720",
    "end": "192480"
  },
  {
    "text": "that's uh interesting is that a visitor at a museum has such a wide span of inquiry Like what are they going to ask",
    "start": "192480",
    "end": "198400"
  },
  {
    "text": "you What what a person managing the museum how does he index things so that they're easily queryable What if someone",
    "start": "198400",
    "end": "204959"
  },
  {
    "text": "wants the top K objects in the museum He'll need these two but they're from completely different fields So when your",
    "start": "204959",
    "end": "212080"
  },
  {
    "text": "objects are unstructured it's very hard to store them in a way that um is",
    "start": "212080",
    "end": "218120"
  },
  {
    "text": "scalable If we wanted to scale our museum for this hypothetical situation",
    "start": "218120",
    "end": "223360"
  },
  {
    "text": "where many visitors are coming it's kind of hard to do Would we have to take half of this dinosaur dinosaur skeleton and",
    "start": "223360",
    "end": "229760"
  },
  {
    "text": "put it in another room uh would we take samples from each exhibit and make a smaller museum on the side How do you do",
    "start": "229760",
    "end": "235200"
  },
  {
    "text": "this And in some real world uh cases uh there's a lot of visitors who want to",
    "start": "235200",
    "end": "240239"
  },
  {
    "text": "see a specific art piece and it's hard How do you scale the Mona Lisa You cannot It's just there and everybody's",
    "start": "240239",
    "end": "246560"
  },
  {
    "text": "going to wait in line and complain about it later Similarly to open",
    "start": "246560",
    "end": "252439"
  },
  {
    "text": "search you can scale it That's adding nodes It's a it's a mechanical thing",
    "start": "252439",
    "end": "257759"
  },
  {
    "text": "you're just going to add some machines But spreading the load when your data is unstructured is difficult It's not a",
    "start": "257759",
    "end": "265759"
  },
  {
    "text": "straightforward answer And this is why in this particular type of system and in elastic search as well you don't have",
    "start": "265759",
    "end": "271440"
  },
  {
    "text": "autoscaling native to the to the uh software So this is me I'm Amit Stern",
    "start": "271440",
    "end": "279360"
  },
  {
    "text": "I'm um a member of the technical steering committee of the open search uh software foundation uh leading the open",
    "start": "279360",
    "end": "285840"
  },
  {
    "text": "search uh software I'm a tech lead at logs.io and I manage the sto the telemetry storage uh team where we",
    "start": "285840",
    "end": "293360"
  },
  {
    "text": "manage um pabytes of logs and traces and um many terabytes of monitoring and",
    "start": "293360",
    "end": "301120"
  },
  {
    "text": "metric data for customers Our metrics are on Thanos clusters and everything",
    "start": "301120",
    "end": "306479"
  },
  {
    "text": "else that I mentioned logs and traces are all going to be stored on open search",
    "start": "306479",
    "end": "311919"
  },
  {
    "text": "clusters What is open search Uh probably most of you know but just to get us",
    "start": "312280",
    "end": "318400"
  },
  {
    "text": "oriented if you're in the room and you're not really familiar open search is a fork of elastic search So it's very",
    "start": "318400",
    "end": "323840"
  },
  {
    "text": "it's very similar Uh it's been a fork for the last three years and the",
    "start": "323840",
    "end": "328880"
  },
  {
    "text": "divergence is not too great So if you're familiar with elastic search this is very much uh relevant as well Um open",
    "start": "328880",
    "end": "336560"
  },
  {
    "text": "search is used to uh make uh bring order to unstructured data at scale It's the",
    "start": "336560",
    "end": "342720"
  },
  {
    "text": "last uh line over here So as I said it is a fork of elastic search Elastic",
    "start": "342720",
    "end": "348240"
  },
  {
    "text": "search used to be open source It would provided an open source uh version and then later they stopped doing that Open",
    "start": "348240",
    "end": "354960"
  },
  {
    "text": "search is a fork that was primarily um driven by AWS and today it's completely",
    "start": "354960",
    "end": "361039"
  },
  {
    "text": "donated to the Linux Foundation It's totally out of their hands at this point Open search clusters um are",
    "start": "361039",
    "end": "369360"
  },
  {
    "text": "monolithic applications You could have it on one node and from here on in the talk this um rounded rectangle will",
    "start": "369360",
    "end": "377039"
  },
  {
    "text": "represent a node A node in open search can be uh can have many roles So you can",
    "start": "377039",
    "end": "382639"
  },
  {
    "text": "have just one and it'll act as a its own little cluster but you could also have many and they'll interact together And",
    "start": "382639",
    "end": "388960"
  },
  {
    "text": "that's what monolithic applications are Usually in the wild we'll see clusters divided into three different",
    "start": "388960",
    "end": "395199"
  },
  {
    "text": "groups of these um of these roles Uh the first one would be a cluster manager Uh",
    "start": "395199",
    "end": "400720"
  },
  {
    "text": "so the cluster manager nodes are managing the state where indexes are uh creating and deleting",
    "start": "400720",
    "end": "406840"
  },
  {
    "text": "indexes There's coordinating nodes and they're in charge of uh the HTTP uh requests So they're kind of like the um",
    "start": "406840",
    "end": "414479"
  },
  {
    "text": "uh the load balancer for the cluster And then there's the data nodes and this is the part that we're going to want to",
    "start": "414479",
    "end": "420840"
  },
  {
    "text": "scale Normally this is where the data is Um the data is stored within a construct",
    "start": "420840",
    "end": "426800"
  },
  {
    "text": "called an index This index contains both the data and the inverted index that",
    "start": "426800",
    "end": "432479"
  },
  {
    "text": "makes search fast and efficient These indexes are split up uh divided between",
    "start": "432479",
    "end": "440400"
  },
  {
    "text": "the data nodes in a construct called a shard Shards go from zero to",
    "start": "440400",
    "end": "445639"
  },
  {
    "text": "n And a shard is in fact um a lucine index just to make things a little bit",
    "start": "445639",
    "end": "451039"
  },
  {
    "text": "confusing So I already used the term index so you don't need to remember that But they're like little lucine",
    "start": "451039",
    "end": "458440"
  },
  {
    "text": "databases On the data nodes are three types of pressure if you're managing one",
    "start": "458440",
    "end": "464400"
  },
  {
    "text": "Okay Okay so you're managing your cluster You have the read pressure all the requests coming in to pull data out as efficiently as possible and quickly",
    "start": "464400",
    "end": "470639"
  },
  {
    "text": "as possible And this right pressure of all these documents coming in There's the third pressure when you're managing",
    "start": "470639",
    "end": "476000"
  },
  {
    "text": "a cluster which is the financial one Since if your read and writes are fairly low you'll get a question from your",
    "start": "476000",
    "end": "482479"
  },
  {
    "text": "management or from uh the CFO like what's going on These things cost a lot of money all this disk space all these",
    "start": "482479",
    "end": "487680"
  },
  {
    "text": "all the memory and CPU cores So it's three types of pressure And let's move on to like why",
    "start": "487680",
    "end": "494000"
  },
  {
    "text": "would we even autoscale So yes financially cluster costs a lot of money We want to reduce the amount of nodes",
    "start": "494000",
    "end": "499919"
  },
  {
    "text": "that we have But what if we just had enough to handle the scale Okay so this",
    "start": "499919",
    "end": "506800"
  },
  {
    "text": "blue line will be the load And let's imagine that and the red line is the uh load that we can accommodate for with",
    "start": "506800",
    "end": "514240"
  },
  {
    "text": "the current um configuration Kind of leave it vague that way",
    "start": "514240",
    "end": "519279"
  },
  {
    "text": "uh overprovisioned is blue and underprovisioned is like uh the red over there If we said the max load is going",
    "start": "519279",
    "end": "527040"
  },
  {
    "text": "to be X and we're just going to say okay that's the that's the we should just provision for there and we'll have that",
    "start": "527040",
    "end": "532800"
  },
  {
    "text": "many nodes The problem would be that we're wasting money and this is good in some cases if you have the money to",
    "start": "532800",
    "end": "538560"
  },
  {
    "text": "spend but normally we're going to try and reduce that So you opt for manual uh",
    "start": "538560",
    "end": "543680"
  },
  {
    "text": "scaling and manual scaling is uh the worst of both worlds uh you you wait too",
    "start": "543680",
    "end": "549200"
  },
  {
    "text": "long to scale up because you know something's happening to the system It's bad bad performance You scale up then",
    "start": "549200",
    "end": "555760"
  },
  {
    "text": "you're afraid to scale down at this point because a second ago people were complaining So you're going to wait too long to scale down It's really the",
    "start": "555760",
    "end": "562440"
  },
  {
    "text": "worst And autoscaling is following that curve uh automatically So that's that's",
    "start": "562440",
    "end": "568480"
  },
  {
    "text": "what we want That's this is the holy grail Some line that sort of follows the uh the",
    "start": "568480",
    "end": "574040"
  },
  {
    "text": "load When we're scaling open search we're scaling hardware So we have to",
    "start": "574040",
    "end": "579920"
  },
  {
    "text": "think about these three things these three elements that we're scaling We're going to scale disk we're going to scale",
    "start": "579920",
    "end": "585200"
  },
  {
    "text": "memory we're going to scale CPU cores These are the three things we want to scale The load splits off into these",
    "start": "585200",
    "end": "591680"
  },
  {
    "text": "three Read load doesn't really affect um the disk So you can have a lot of read",
    "start": "591680",
    "end": "597920"
  },
  {
    "text": "load or less read It doesn't mean you're going to add disk So for the sake of this um talk we're going to focus more",
    "start": "597920",
    "end": "604320"
  },
  {
    "text": "on the right load and its effects on the cluster because it affects all three of these components And if if we're if",
    "start": "604320",
    "end": "610240"
  },
  {
    "text": "there's a lot of writes we might need to add more disk um or we might need more CPU cores because the um the type of",
    "start": "610240",
    "end": "617760"
  },
  {
    "text": "writing is a little more complex or we need more memory I have exactly one slide devoted",
    "start": "617760",
    "end": "625839"
  },
  {
    "text": "to vertical scaling uh because um when I was going over this con over the talk",
    "start": "625839",
    "end": "631279"
  },
  {
    "text": "with other folks um they said what about vertical scaling Amazon behind the scenes when they're running your um",
    "start": "631279",
    "end": "638160"
  },
  {
    "text": "serverless uh platform they're going to vertically scale it first and if you",
    "start": "638160",
    "end": "643440"
  },
  {
    "text": "have your own data center it could be kind of easy to do that relatively Amazon do this because they have the",
    "start": "643440",
    "end": "650160"
  },
  {
    "text": "capability to vertically scale easily if you're using a cloud it's harder So when",
    "start": "650160",
    "end": "655839"
  },
  {
    "text": "you scale up usually you go from one set of machines to the next scale of machines It means you have to stop that",
    "start": "655839",
    "end": "660959"
  },
  {
    "text": "machine and move data And that's not something that's easily done normally So vertically scaling for most intent",
    "start": "660959",
    "end": "668160"
  },
  {
    "text": "purposes most companies is really just the disk That is easy And you can",
    "start": "668160",
    "end": "673760"
  },
  {
    "text": "increase the number of EBS instances and uh you can increase the disk over there",
    "start": "673760",
    "end": "679720"
  },
  {
    "text": "Horizontal scaling is the thing you need to know how to do If you're managing and maintaining clusters you have to know",
    "start": "679720",
    "end": "685440"
  },
  {
    "text": "how to do this and open search You just have to add a node and it gets discovered by the cluster and it's there",
    "start": "685440",
    "end": "691760"
  },
  {
    "text": "So practically it's easy and there's a need to do this because of the load the changing",
    "start": "691760",
    "end": "698040"
  },
  {
    "text": "load Um there's an expectation however that when you add a node the load will",
    "start": "698040",
    "end": "703920"
  },
  {
    "text": "just distribute And this is the case in a lot of different projects Uh here",
    "start": "703920",
    "end": "709760"
  },
  {
    "text": "similar to our um the example with the museum it's not the case You have to learn how the load is spread out you",
    "start": "709760",
    "end": "715600"
  },
  {
    "text": "have to actually change that as well How you're maintaining the data you have to change that as you are adding",
    "start": "715600",
    "end": "723079"
  },
  {
    "text": "nodes If the load is dis disproportionately uh hitting one of the nodes we call that",
    "start": "723079",
    "end": "729920"
  },
  {
    "text": "a hotspot Um any of you familiar with what hotspots are already Okay Yeah many",
    "start": "729920",
    "end": "736480"
  },
  {
    "text": "Um so you get a lot of load on one of those nodes and then rights start to lag",
    "start": "736480",
    "end": "742000"
  },
  {
    "text": "Uh so hotspots are a thing we want to avoid Um which moves us into another",
    "start": "742000",
    "end": "748880"
  },
  {
    "text": "place of like how do we actually distribute this data so it's kind of going to all these nodes um in the same",
    "start": "748880",
    "end": "755120"
  },
  {
    "text": "fashion and we're not getting these hot spots When we index data into open",
    "start": "755120",
    "end": "760560"
  },
  {
    "text": "search it each document gets its own ID And that ID is going to be hashed And",
    "start": "760560",
    "end": "767279"
  },
  {
    "text": "then we're going to do a mod of n N being the number of shards So uh in this",
    "start": "767279",
    "end": "774160"
  },
  {
    "text": "example the mod is uh something that ends with uh 45 and mod 4 because we",
    "start": "774160",
    "end": "779440"
  },
  {
    "text": "have four shards and that would be equal to one So it's going to go to shard number one So if you have thousands of documents coming in each with their own",
    "start": "779440",
    "end": "786639"
  },
  {
    "text": "unique ID then they're going to go to these different shards and it more or less balances out It works in reality",
    "start": "786639",
    "end": "794720"
  },
  {
    "text": "If we wanted to have the capability to just add a shard make the index just",
    "start": "794720",
    "end": "799839"
  },
  {
    "text": "slightly bigger why can't we do that And the reason is this hashmod n If we were to potentially add another shard our",
    "start": "799839",
    "end": "807360"
  },
  {
    "text": "document is now stored in shard number one and we wanted to scale up So we like extended the index just a",
    "start": "807360",
    "end": "813560"
  },
  {
    "text": "bit The next time we want to search for that ID we're going to do hashmod to see",
    "start": "813560",
    "end": "818800"
  },
  {
    "text": "where it is N just changed it's five and not four So we're looking for the",
    "start": "818800",
    "end": "824639"
  },
  {
    "text": "document in a different shard and it is now gone So that's why we have a fixed number of shards in our indices and we",
    "start": "824639",
    "end": "832240"
  },
  {
    "text": "actually can't change that So when you're scaling open search you have to know this You can't just add shards to",
    "start": "832240",
    "end": "838399"
  },
  {
    "text": "the index You have to do something we call rollover So you take the index that you're writing to and you add a new",
    "start": "838399",
    "end": "845120"
  },
  {
    "text": "index with the same aliases So you're still writing you're going to start writing to the new alias uh the new",
    "start": "845120",
    "end": "850480"
  },
  {
    "text": "index atomically and this new index would have more shards That's the only way to really increase",
    "start": "850480",
    "end": "857839"
  },
  {
    "text": "throughput Another thing that's frustrating when you're trying to horizontally scale a cluster is that",
    "start": "857880",
    "end": "863440"
  },
  {
    "text": "there's shared resources Each of our data nodes is getting uh hit with all these requests",
    "start": "863440",
    "end": "869680"
  },
  {
    "text": "to pull data out and at the same time to put data in So if you have a really",
    "start": "869680",
    "end": "875760"
  },
  {
    "text": "heavy query with a leading wild card reax something like that hitting your one of your one or two of your nodes the",
    "start": "875760",
    "end": "882959"
  },
  {
    "text": "right throughput's going to be impacted and you're going to start lagging in your rights And why is this important to note",
    "start": "882959",
    "end": "890000"
  },
  {
    "text": "Because autoscaling often we look at the CPU and you say oh CPU high add nodes but that could be because of one of",
    "start": "890000",
    "end": "896880"
  },
  {
    "text": "these two pressures It could be the right pressure or the read If it's the read it could be momentary and you just",
    "start": "896880",
    "end": "902720"
  },
  {
    "text": "added you just wasted money by adding a lot of nodes Uh so on the one hand we shouldn't",
    "start": "902720",
    "end": "909760"
  },
  {
    "text": "look at the CPU and you might want to look at the right load and the readload On the other hand right load and reload",
    "start": "909760",
    "end": "915519"
  },
  {
    "text": "could be fine but you have so many shards packed in each one of your nodes because you've been doing all these",
    "start": "915519",
    "end": "920800"
  },
  {
    "text": "rollover tasks that you get out of memory Uh so these kind of this this is",
    "start": "920800",
    "end": "927440"
  },
  {
    "text": "I'm just trying to give you the the feeling of why it's actually very hard to do this thing where you're kind of saying this metric says scale",
    "start": "927440",
    "end": "934680"
  },
  {
    "text": "up The good news is it's sometimes really simple and",
    "start": "934680",
    "end": "940800"
  },
  {
    "text": "um it does come at a cost similarly to eating cake Uh but it is still simple If",
    "start": "940800",
    "end": "947040"
  },
  {
    "text": "the load is um imbalanced on one of our on one of those three um different types",
    "start": "947040",
    "end": "954000"
  },
  {
    "text": "disk memory or CPU we could add extra nodes and it will balance out especially",
    "start": "954000",
    "end": "960079"
  },
  {
    "text": "if it's disk and it and similarly if the load is low on all three it can't be",
    "start": "960079",
    "end": "965360"
  },
  {
    "text": "just one on all three of those So low memory low CPU low disk We can remove",
    "start": "965360",
    "end": "971320"
  },
  {
    "text": "nodes and that's when it is simple When you can clearly see the picture is overprovisioned or",
    "start": "971320",
    "end": "979120"
  },
  {
    "text": "underprovisioned and I want to um devote the rest of the talk to when it's actually complicated because the easy is",
    "start": "980519",
    "end": "987120"
  },
  {
    "text": "really easy Let's assume that we're looking at one of those spikes right the load goes up and down And let's say we",
    "start": "987120",
    "end": "993440"
  },
  {
    "text": "want to um say that when we see a lot of rights coming in we want to roll over",
    "start": "993440",
    "end": "999360"
  },
  {
    "text": "Okay And when they go down we want to roll over again because we don't want to waste money So the red is going to say",
    "start": "999360",
    "end": "1005279"
  },
  {
    "text": "that the rights are too high So we're going to roll over And then um we add",
    "start": "1005279",
    "end": "1011360"
  },
  {
    "text": "this extra node and so everything's okay Uh then the rights start to go down",
    "start": "1011360",
    "end": "1018079"
  },
  {
    "text": "We're wasting money at this point There's 20% load on each of these nodes",
    "start": "1018079",
    "end": "1023519"
  },
  {
    "text": "If we remove a node we get a hot spot because now we just created a situation where 40% is hitting one node A",
    "start": "1023519",
    "end": "1030079"
  },
  {
    "text": "disproportionate amount of pressure on one And that's bad So what do we do Do",
    "start": "1030079",
    "end": "1036000"
  },
  {
    "text": "another rollover task And now it's 25% to each node Uh and we could do this again and again on each of these If it's",
    "start": "1036000",
    "end": "1043120"
  },
  {
    "text": "like a dayight load you'd have way too many shards already in your cluster and you'd start hitting out of",
    "start": "1043120",
    "end": "1049880"
  },
  {
    "text": "memory Getting rid of those extra shards is is uh practically hard You have to",
    "start": "1049880",
    "end": "1056080"
  },
  {
    "text": "find a way to either do it slowly by changing to size-based retention or you",
    "start": "1056080",
    "end": "1061520"
  },
  {
    "text": "can do merging of indexes which you can do but it's very slow and it takes a lot",
    "start": "1061520",
    "end": "1068160"
  },
  {
    "text": "of CPU There is a way uh rather simple way",
    "start": "1068160",
    "end": "1076400"
  },
  {
    "text": "to overcome this problem and that is to overshard So rather than have uh shards",
    "start": "1076400",
    "end": "1082240"
  },
  {
    "text": "spread out one per node I could have uh three shards per node and then when I",
    "start": "1082240",
    "end": "1087760"
  },
  {
    "text": "want to scale I'll add nodes and let those shards spread out The shard's going to take up as many compute power",
    "start": "1087760",
    "end": "1093120"
  },
  {
    "text": "as it can as much compute power as it can from those new nodes So kind of like hulking out Uh um that's the concept",
    "start": "1093120",
    "end": "1100960"
  },
  {
    "text": "However finding the sweet spot between overshing and underssharting becomes",
    "start": "1100960",
    "end": "1106679"
  },
  {
    "text": "hard and it's difficult to calculate and in many cases you'd want to sort of roll",
    "start": "1106679",
    "end": "1113600"
  },
  {
    "text": "over again into an even bigger index So I'm going to suggest a few topologies",
    "start": "1113600",
    "end": "1119280"
  },
  {
    "text": "for scaling in a way that um allows us to really maintain this kind of um sweet",
    "start": "1119280",
    "end": "1126080"
  },
  {
    "text": "spot between way too many way too few uh shards The first uh the first kind is uh",
    "start": "1126080",
    "end": "1134559"
  },
  {
    "text": "what I'd call a burst index So as I mentioned earlier each index has uh a",
    "start": "1134559",
    "end": "1141200"
  },
  {
    "text": "right alias and that's where you're going to atomically be writing you can change this alias and it'll switch over",
    "start": "1141200",
    "end": "1146799"
  },
  {
    "text": "to whatever index you point to So it's an important uh concept to be familiar with if you're managing a",
    "start": "1146799",
    "end": "1153400"
  },
  {
    "text": "cluster What we're suggesting is to have these burst indices um prepared in case",
    "start": "1153400",
    "end": "1160000"
  },
  {
    "text": "you want to scale out So you don't have to you can they can be maintained for weeks um and they will be that place",
    "start": "1160000",
    "end": "1168320"
  },
  {
    "text": "where you direct traffic when you need to have a lot of it directed there",
    "start": "1168320",
    "end": "1174320"
  },
  {
    "text": "So that's what we would do We just change the right alias to the right data stream And that would look something",
    "start": "1174320",
    "end": "1179520"
  },
  {
    "text": "like this Um there's an arbitrary tag a label we can give uh nodes called box",
    "start": "1179520",
    "end": "1185799"
  },
  {
    "text": "type You could tell an index to allocate its shards on a specific uh box type or",
    "start": "1185799",
    "end": "1192720"
  },
  {
    "text": "a few different box types So the concept is you have burst type the box type",
    "start": "1192720",
    "end": "1199919"
  },
  {
    "text": "burst and you have box type low And as long as you have low throughput in your",
    "start": "1199919",
    "end": "1205760"
  },
  {
    "text": "writes and again that is probably the best indicator of I need more nodes is",
    "start": "1205760",
    "end": "1211280"
  },
  {
    "text": "the right throughput So if you have a lot of writes come uh a low throughput on the rights we don't need our extra",
    "start": "1211280",
    "end": "1217880"
  },
  {
    "text": "nodes and the low right throughput index",
    "start": "1217880",
    "end": "1223360"
  },
  {
    "text": "is allocated to indexes that have the low box type",
    "start": "1223360",
    "end": "1229039"
  },
  {
    "text": "If throughout the day the th the throughput is not so low and we anticipate that there we're going to have a spike and this again it's so",
    "start": "1229039",
    "end": "1237120"
  },
  {
    "text": "tailored to your use case that I can't tell you exactly what that is but if you see in many case it is it is that the",
    "start": "1237120",
    "end": "1244480"
  },
  {
    "text": "right throughput is growing on a trend then what you do is you add these extra",
    "start": "1244480",
    "end": "1249520"
  },
  {
    "text": "nodes and you don't need to add nodes that are as expensive as the other ones",
    "start": "1249520",
    "end": "1255360"
  },
  {
    "text": "why because you don't intend to have that amount of disk space used on them They're temporary You could have a real",
    "start": "1255360",
    "end": "1262080"
  },
  {
    "text": "small and efficient disc there uh on these new box types So you create the",
    "start": "1262080",
    "end": "1267360"
  },
  {
    "text": "new ones The allocation of our burst index says it can be on either low or",
    "start": "1267360",
    "end": "1275679"
  },
  {
    "text": "burst or both And all you have to do is tell that index that you're allowed to have total",
    "start": "1275679",
    "end": "1282480"
  },
  {
    "text": "shards per node one And then it automatically will spread out to all of these uh all of these",
    "start": "1282480",
    "end": "1291000"
  },
  {
    "text": "nodes At this point you're prepared for the higher throughput and the right alias you you switch the right alias to",
    "start": "1291000",
    "end": "1298320"
  },
  {
    "text": "be the high throughput index So this is the uh burst index type As it goes down",
    "start": "1298320",
    "end": "1304320"
  },
  {
    "text": "you can move the shards back by doing something called uh exclude of the nodes",
    "start": "1304320",
    "end": "1310080"
  },
  {
    "text": "you just exclude these nodes and shards just fly off of it uh to other one other",
    "start": "1310080",
    "end": "1315320"
  },
  {
    "text": "nodes and then you remove them So this is the first form of",
    "start": "1315320",
    "end": "1321320"
  },
  {
    "text": "autoscaling It works when um you don't have many tenants on your cluster So if",
    "start": "1321320",
    "end": "1330159"
  },
  {
    "text": "you have one big index and it may grow or shrink this makes sense",
    "start": "1330159",
    "end": "1336159"
  },
  {
    "text": "However in some cases we have many many tenants and they're doing many different",
    "start": "1336159",
    "end": "1341360"
  },
  {
    "text": "things all at the same time some throughputs spike and others will when",
    "start": "1341360",
    "end": "1346400"
  },
  {
    "text": "others will kind of go down and you don't want to be in a situation where you're uh having your cluster tailored",
    "start": "1346400",
    "end": "1351840"
  },
  {
    "text": "just for the highest uh tenant the highest throughput tenant because then again you are wasting uh",
    "start": "1351840",
    "end": "1359240"
  },
  {
    "text": "resources which brings me to the uh second and last topology that I want to discuss here which is called the burst",
    "start": "1359240",
    "end": "1365600"
  },
  {
    "text": "cluster It is very similar uh to the previous one and uh but the difference",
    "start": "1365600",
    "end": "1371440"
  },
  {
    "text": "is is kind of big We're not just changing the index that we're going to within the cluster We're changing the",
    "start": "1371440",
    "end": "1376799"
  },
  {
    "text": "direction to a completely different cluster So we wouldn't be using the right alias We would be diverting",
    "start": "1376799",
    "end": "1384520"
  },
  {
    "text": "traffic And it would look something like this If each of these circles is a cluster and each of them have that many",
    "start": "1384520",
    "end": "1392919"
  },
  {
    "text": "nodes why would we have a 10 and a five and a 60 Um the reason is we'd want to",
    "start": "1392919",
    "end": "1399200"
  },
  {
    "text": "avoid hotspots So you should fine-tune your clusters initially for your average load",
    "start": "1399200",
    "end": "1406080"
  },
  {
    "text": "The average load for a low throughput might be five nodes So you want only five shards And for a higher throughput",
    "start": "1406080",
    "end": "1413280"
  },
  {
    "text": "you want a 10 node cluster So you have 10 shards each If you're suffering from hotspots all you have to do to fix that",
    "start": "1413280",
    "end": "1420480"
  },
  {
    "text": "is spread the shards perfectly on the cluster And that's that means zero",
    "start": "1420480",
    "end": "1425600"
  },
  {
    "text": "hotspots So in this situation we've tailored our system so that on these green clusters uh the smaller",
    "start": "1425600",
    "end": "1433000"
  },
  {
    "text": "circles they're fine-tuned for the exact amount of writes that we're getting But then one of our tenants spikes while the",
    "start": "1433000",
    "end": "1440799"
  },
  {
    "text": "others don't We move that only that tenant to send all their data we divert it to the 60 node cluster capable of",
    "start": "1440799",
    "end": "1447919"
  },
  {
    "text": "handling very high throughputs but not capable of handing handling a lot of disk space So it's not as expensive as a",
    "start": "1447919",
    "end": "1454960"
  },
  {
    "text": "six times these 10 node clusters but it is still more expensive So data is being",
    "start": "1454960",
    "end": "1461039"
  },
  {
    "text": "uh diverted to a totally different environment Um we use something called crosscluster search in order to search",
    "start": "1461039",
    "end": "1468240"
  },
  {
    "text": "on both So from the perspective of the person uh running the search nothing has",
    "start": "1468240",
    "end": "1474400"
  },
  {
    "text": "changed at any point it's completely transparent for them Uh and in terms of",
    "start": "1474400",
    "end": "1480080"
  },
  {
    "text": "the throughput nothing has changed like they're sending much more data but they don't get any lag whoever is sending",
    "start": "1480080",
    "end": "1487080"
  },
  {
    "text": "it Um and all the other tenants don't feel it There are many more tenants on this 10 node cluster and they're just",
    "start": "1487080",
    "end": "1494799"
  },
  {
    "text": "living you know their best life over there You could also have a few tenants",
    "start": "1494799",
    "end": "1501120"
  },
  {
    "text": "sending to this 60 node cluster You just have to manage how much disk you're expecting to fill at that time of the",
    "start": "1501120",
    "end": "1506960"
  },
  {
    "text": "burst A way to make this a little more economical is to have um one of your",
    "start": "1506960",
    "end": "1512559"
  },
  {
    "text": "highest throughput tenants always on the 60 node cluster So you still maintain um",
    "start": "1512559",
    "end": "1518159"
  },
  {
    "text": "a reason to have them up when there's no high throughput tenants on these other clusters And this is a way to have auto",
    "start": "1518159",
    "end": "1525039"
  },
  {
    "text": "well this is a way to think of autoscaling in a way that is a bit outside of the box and not just adding",
    "start": "1525039",
    "end": "1531679"
  },
  {
    "text": "nodes to a single cluster It is very useful by the way if you are running um",
    "start": "1531679",
    "end": "1537120"
  },
  {
    "text": "a feature that is not very used in open search but is is u up and coming called",
    "start": "1537120",
    "end": "1543200"
  },
  {
    "text": "searchable snapshots So if you're running searchable snapshots all your data is going to be",
    "start": "1543200",
    "end": "1548720"
  },
  {
    "text": "on S3 uh and you're only going to have metadata on your",
    "start": "1548720",
    "end": "1554279"
  },
  {
    "text": "cluster If you the more nodes you have that are searching S3 the better So they",
    "start": "1554279",
    "end": "1560080"
  },
  {
    "text": "can be small nodes with very small disk and they could be searching many terabytes on S3 But if you have one node",
    "start": "1560080",
    "end": "1567600"
  },
  {
    "text": "with a lot of disk trying to do that the throughput's going to be too low and your search is going to be too slow So",
    "start": "1567600",
    "end": "1572720"
  },
  {
    "text": "if you want to utilize these kind of features where your data is remote you have to have many many nodes So that's",
    "start": "1572720",
    "end": "1579600"
  },
  {
    "text": "another reason to have such a cluster just up and running all the time You could use it to search audit data that",
    "start": "1579600",
    "end": "1585200"
  },
  {
    "text": "spans back many years Of course we don't want to keep it",
    "start": "1585200",
    "end": "1591120"
  },
  {
    "text": "there forever Uh a way to do that is just snapshot it to S3 Snapshots in Open",
    "start": "1591120",
    "end": "1597039"
  },
  {
    "text": "Search are a really powerful tool They're not the same as they are in other databases It takes the index kind",
    "start": "1597039",
    "end": "1602799"
  },
  {
    "text": "of as it is Doesn't do any additional compression but it stores it in a very special way So it's easy to um extract",
    "start": "1602799",
    "end": "1610640"
  },
  {
    "text": "it and uh restore cluster in case of a disaster So we would move the data to S3",
    "start": "1610640",
    "end": "1616799"
  },
  {
    "text": "and then restore it back into these original clusters that we had previously",
    "start": "1616799",
    "end": "1622320"
  },
  {
    "text": "uh been running our tenants on And then we could do a merge task down the line when the load is low We could merge that",
    "start": "1622320",
    "end": "1628480"
  },
  {
    "text": "data into smaller and smaller indexes if we like Another thing that happens usually in these kind of uh situations",
    "start": "1628480",
    "end": "1635600"
  },
  {
    "text": "is that you have retention and once the retention is gone just delete the data which is great especially if you're in",
    "start": "1635600",
    "end": "1641760"
  },
  {
    "text": "Europe you you have to delete it like right on time Uh so this is the uh burst",
    "start": "1641760",
    "end": "1646960"
  },
  {
    "text": "cluster topology like to summarize it's going to be a bit long that's why it's happening at this point in time So just",
    "start": "1646960",
    "end": "1654240"
  },
  {
    "text": "to summarize um there are three different resources that we want to be scaling Okay you have to be mindful when",
    "start": "1654240",
    "end": "1661919"
  },
  {
    "text": "you're maintaining your cluster which one is the one that causes the pressure Okay So if you have very long",
    "start": "1661919",
    "end": "1669080"
  },
  {
    "text": "retention then disk space and you have to start considering things like um",
    "start": "1669080",
    "end": "1675360"
  },
  {
    "text": "searchable snapshots or maintaining maybe a crosscluster search where you have just data sitting on a separate",
    "start": "1675360",
    "end": "1682480"
  },
  {
    "text": "cluster that's just accumulating you know in very large discs whereas your right load is on a smaller cluster So",
    "start": "1682480",
    "end": "1689840"
  },
  {
    "text": "that's one possibility If it's um memory or CPU then you would definitely have to",
    "start": "1689840",
    "end": "1694960"
  },
  {
    "text": "add stronger machines Um so you have to think about these things ahead of time Some of them are a one-way door So if",
    "start": "1694960",
    "end": "1701279"
  },
  {
    "text": "you're using AWS and and you add um uh to your uh disk space uh in some cases",
    "start": "1701279",
    "end": "1709360"
  },
  {
    "text": "you may find it difficult to reduce the disk space again Okay this is a common",
    "start": "1709360",
    "end": "1714720"
  },
  {
    "text": "problem Um and when when I say that the main reason it is is because when you",
    "start": "1714720",
    "end": "1720720"
  },
  {
    "text": "want to reduce a node uh you have to shift the data to the other nodes and in",
    "start": "1720720",
    "end": "1726320"
  },
  {
    "text": "in certain cases especially after you've added a lot of disk that can take a lot of time So some of them are sort of a",
    "start": "1726320",
    "end": "1733360"
  },
  {
    "text": "one-way door and many of them require a restart of a node which is potential",
    "start": "1733360",
    "end": "1738480"
  },
  {
    "text": "downtime We talked about these two topologies I'll remind you the burst index and the",
    "start": "1738480",
    "end": "1745440"
  },
  {
    "text": "burst cluster which are very important to think about as completely different options And I like to highlight that",
    "start": "1745440",
    "end": "1752159"
  },
  {
    "text": "that first option that I gave sort of uh the hulking out like the overindex the overshing uh proposition is also viable",
    "start": "1752159",
    "end": "1760720"
  },
  {
    "text": "for many use cases If you have a really easy trend that you can follow your data is just going up and down and it's the",
    "start": "1760720",
    "end": "1766799"
  },
  {
    "text": "same you know twon people are sending you know 2x midnight it goes down to",
    "start": "1766799",
    "end": "1772799"
  },
  {
    "text": "half of that and it keeps going up and down By all means have a cluster that has you know 10 nodes with 20 shards on",
    "start": "1772799",
    "end": "1780880"
  },
  {
    "text": "it And when you hit that afternoon peak just scale out and let it spread out And",
    "start": "1780880",
    "end": "1786080"
  },
  {
    "text": "then when it gets to the evening then scale down again If that's your use case you shouldn't be implementing things",
    "start": "1786080",
    "end": "1791679"
  },
  {
    "text": "that are this complex and you should definitely use the the concept of overshing which is well",
    "start": "1791679",
    "end": "1797399"
  },
  {
    "text": "known Um I'd like to talk about some upcoming key features which is different than uh when I started writing this uh",
    "start": "1797399",
    "end": "1804399"
  },
  {
    "text": "presentation These things changed like in the last uh month or so Uh the open search software foundation which",
    "start": "1804399",
    "end": "1810080"
  },
  {
    "text": "supports open search Uh one of the things that that's really neat is that uh from it being very AWS centric has",
    "start": "1810080",
    "end": "1816640"
  },
  {
    "text": "become um much more widespread So there's a lot of people from Uber and",
    "start": "1816640",
    "end": "1822080"
  },
  {
    "text": "Slack Intel Airbnb um developers starting to take an interest and",
    "start": "1822080",
    "end": "1827840"
  },
  {
    "text": "developing things within the ecosystem They're changing it in ways that will benefit their business And if that",
    "start": "1827840",
    "end": "1834240"
  },
  {
    "text": "business is as big as Uber then the changes are profound Um one of the",
    "start": "1834240",
    "end": "1840080"
  },
  {
    "text": "changes that really affects autoscaling is readwrite separation And that's going",
    "start": "1840080",
    "end": "1845440"
  },
  {
    "text": "to come in the next uh few versions It's going to I think it's nearly beta but uh",
    "start": "1845440",
    "end": "1850559"
  },
  {
    "text": "a lot of the code's already there I think this was in August when I took this screenshot and it was five out of",
    "start": "1850559",
    "end": "1855760"
  },
  {
    "text": "out of 11 tasks So they're pretty much there by now Uh this will allow you to have nodes that are tailored for write",
    "start": "1855760",
    "end": "1862559"
  },
  {
    "text": "and nodes that are tailored for read and then you're scaling the right and you're scaling the read separately which makes",
    "start": "1862559",
    "end": "1868480"
  },
  {
    "text": "life so much more simple Yeah All right And the other it's not me",
    "start": "1868480",
    "end": "1875600"
  },
  {
    "text": "right now it's okay Um the other one uh which is really cool is streaming",
    "start": "1875600",
    "end": "1881039"
  },
  {
    "text": "ingestion So one of the things that really makes that makes it difficult to ingest a lot of data all at once is that",
    "start": "1881039",
    "end": "1889120"
  },
  {
    "text": "uh today in both elastic search and open search we're pushing it in and the index",
    "start": "1889120",
    "end": "1894559"
  },
  {
    "text": "uh is trying to do that trying to push the data ingest it but the node might be overloaded in which case the shard will",
    "start": "1894559",
    "end": "1900720"
  },
  {
    "text": "just say I'm sorry CPU up to here and you get what is called a write Q and",
    "start": "1900720",
    "end": "1906559"
  },
  {
    "text": "once that right Q starts to build someone's going to be woken up normally if you're running let's the uh",
    "start": "1906559",
    "end": "1912320"
  },
  {
    "text": "observability data That's a wakeup call In pullbased what you get is the",
    "start": "1912320",
    "end": "1918480"
  },
  {
    "text": "shard is hard-coded uh to listen and and retrieve documents from a particular",
    "start": "1918480",
    "end": "1925600"
  },
  {
    "text": "partition in um for example Kafka It could be it would be pluggable So it's",
    "start": "1925600",
    "end": "1931120"
  },
  {
    "text": "not only Kafka but let's just since it's very common let's talk and use Kafka as",
    "start": "1931120",
    "end": "1936240"
  },
  {
    "text": "an example So each shard will read from a particular partition A topic would represent let's say a um a tenant You",
    "start": "1936240",
    "end": "1944159"
  },
  {
    "text": "could have a shard reading reading from different partitions from different",
    "start": "1944159",
    "end": "1950080"
  },
  {
    "text": "topics but per topic it would be one So shard zero from partition zero etc",
    "start": "1950080",
    "end": "1957519"
  },
  {
    "text": "What this gives us is the capability for the shard to read as fast as it can",
    "start": "1957519",
    "end": "1963279"
  },
  {
    "text": "which means that you don't get the situation of a right Q because it's reading as just as fast as it possibly",
    "start": "1963279",
    "end": "1969039"
  },
  {
    "text": "can based on the machine wherever you put it So if you want to scale in this case it's easy you you look at the at",
    "start": "1969039",
    "end": "1976799"
  },
  {
    "text": "the lag in Kafka you don't look at how at these metrics in terms of like the",
    "start": "1976799",
    "end": "1981919"
  },
  {
    "text": "cluster uh the metrics here are much easier is there lag in Kafka yes I need",
    "start": "1981919",
    "end": "1987440"
  },
  {
    "text": "to scale much easier uh let's look at CPU let's look at memory let's see if the",
    "start": "1987440",
    "end": "1992720"
  },
  {
    "text": "shards are balanced it's much harder to do and in this case it'll make life much much easier",
    "start": "1992720",
    "end": "1999159"
  },
  {
    "text": "um that is my talk um open for Q&A if uh anybody has any",
    "start": "1999159",
    "end": "2006320"
  },
  {
    "text": "questions",
    "start": "2008279",
    "end": "2011279"
  },
  {
    "text": "Thanks Yeah Hi um had a question about that last thing",
    "start": "2015960",
    "end": "2023200"
  },
  {
    "text": "you talked about about streaming ingestion beyond just looking at a metric um at the lag in CFKA is does",
    "start": "2023200",
    "end": "2031519"
  },
  {
    "text": "that expose a way to know precisely up to which point in the stream this got in",
    "start": "2031519",
    "end": "2037519"
  },
  {
    "text": "the document we use open search in a bunch of places where we need to know exactly what's available in the index so",
    "start": "2037519",
    "end": "2042880"
  },
  {
    "text": "that we can propagate things to other systems um",
    "start": "2042880",
    "end": "2048000"
  },
  {
    "text": "uh so at this I I don't know if you saw it is an RFC It's a request a request for comments Okay There's not a feature",
    "start": "2048000",
    "end": "2055200"
  },
  {
    "text": "yet Right now it's like it's in the phase of what we call like a feature",
    "start": "2055200",
    "end": "2060240"
  },
  {
    "text": "branch where it's being implemented in a way that it's like totally breakable and we're not going to put that in",
    "start": "2060240",
    "end": "2065839"
  },
  {
    "text": "production But it's if you have any comments like that please do comment in the uh in the GitHub uh so you can come",
    "start": "2065839",
    "end": "2072079"
  },
  {
    "text": "up and share a link Uh that would be welcome Yeah It's in the exact phase",
    "start": "2072079",
    "end": "2077520"
  },
  {
    "text": "where we need those comments Yeah Uh so might be slightly off-topic question Um",
    "start": "2077520",
    "end": "2083118"
  },
  {
    "text": "so this is time series data right Do you also roll your indexes by time like quarterly or monthly or h how do you",
    "start": "2083119",
    "end": "2090638"
  },
  {
    "text": "combine this approach with uh burst indexes with situation where you have",
    "start": "2090639",
    "end": "2097480"
  },
  {
    "text": "indexes by if it's retention yeah along the time axis So one of the things you",
    "start": "2097480",
    "end": "2102720"
  },
  {
    "text": "can do is you have um the burst index you don't want it to be there for too long right that's that's the question",
    "start": "2102720",
    "end": "2109520"
  },
  {
    "text": "like the burst index you want it to live longer than the retention if I understood correctly it's not just the",
    "start": "2109520",
    "end": "2114880"
  },
  {
    "text": "burst indexes your normal indexes are oh so",
    "start": "2114880",
    "end": "2120079"
  },
  {
    "text": "in some cases uh if your if your indexes are time",
    "start": "2120079",
    "end": "2126079"
  },
  {
    "text": "based and you're rolling over every day then you're going to have a problem of too many shards if you don't fill them",
    "start": "2126079",
    "end": "2133200"
  },
  {
    "text": "up enough You'll have basically like shards that have two megabytes inside",
    "start": "2133200",
    "end": "2138520"
  },
  {
    "text": "them It just inflates too much If you have 365 days or two years of data that",
    "start": "2138520",
    "end": "2144320"
  },
  {
    "text": "becomes too many shards Um so I do recommend moving to size based like a hybrid solution of size based as long as",
    "start": "2144320",
    "end": "2150800"
  },
  {
    "text": "it's less than x amount of days so that you're not exactly on the date but better Um having said that the idea is",
    "start": "2150800",
    "end": "2158880"
  },
  {
    "text": "that you have your right alias pointed at the head Okay And then after a",
    "start": "2158880",
    "end": "2164240"
  },
  {
    "text": "certain amount of time you do a rollover task Um the burst index you don't roll",
    "start": "2164240",
    "end": "2170160"
  },
  {
    "text": "over necessarily And that one what you do instead of rolling over you merge or you do a reindex of that data into the",
    "start": "2170160",
    "end": "2177200"
  },
  {
    "text": "other one And you can do that It's not it just takes a lot of time to do but you can do that in the background Yeah I",
    "start": "2177200",
    "end": "2183760"
  },
  {
    "text": "didn't go into like there's nitty-gritty here but we didn't go into that Yeah",
    "start": "2183760",
    "end": "2189200"
  },
  {
    "text": "sure I have two questions Uh number one I think you mentioned uh uh reader and",
    "start": "2189200",
    "end": "2196640"
  },
  {
    "text": "separation of reading and writing Uh it's already supported in open search",
    "start": "2196640",
    "end": "2202680"
  },
  {
    "text": "serverless in AWS So am I missing something the one that you are talking about is is going to come for the",
    "start": "2202680",
    "end": "2209920"
  },
  {
    "text": "regular open search and it's not yet implemented I I'm I don't work at AWS I",
    "start": "2209920",
    "end": "2215040"
  },
  {
    "text": "I'm representing open source Both of these are going to be completely in open source They're not they're not being but",
    "start": "2215040",
    "end": "2220640"
  },
  {
    "text": "that's what I'm saying like it seems like it's already there in uh I I forget the version maybe 2.134 something like",
    "start": "2220640",
    "end": "2227599"
  },
  {
    "text": "that Uh so you mentioned it is a version that is coming Yeah But I I have",
    "start": "2227599",
    "end": "2234720"
  },
  {
    "text": "practically observed that it's already there So in the you mean in the Amazon Amazon serverless Yes So Amazon",
    "start": "2234720",
    "end": "2241000"
  },
  {
    "text": "serverless is a fork of open search and uh it took a great deal of u a nice",
    "start": "2241000",
    "end": "2249280"
  },
  {
    "text": "amount of engineers uh more than a year to kind of separate these two things these concepts of like",
    "start": "2249280",
    "end": "2255200"
  },
  {
    "text": "open search as a monolithic application and having the read write Um I can tell you that a lot of these improvements",
    "start": "2255200",
    "end": "2261920"
  },
  {
    "text": "they're kind of working upstream They like to add these special capabilities like rewrite separation and then they",
    "start": "2261920",
    "end": "2268960"
  },
  {
    "text": "contribute a lot of the stuff back into the open source So you'll have it in some cases you'll have features already",
    "start": "2268960",
    "end": "2274800"
  },
  {
    "text": "available in in the Amazon open search offering and then later it'll get um I",
    "start": "2274800",
    "end": "2279920"
  },
  {
    "text": "see introduced into the open search uh open source Okay and the strategies that",
    "start": "2279920",
    "end": "2285440"
  },
  {
    "text": "you explained just now and they are coming especially the second one one",
    "start": "2285440",
    "end": "2290640"
  },
  {
    "text": "with the Kafka thing uh when is it what version is there a plan oh it again this",
    "start": "2290640",
    "end": "2297040"
  },
  {
    "text": "is very early stage the the pollbased indexing yes that one is at a stage",
    "start": "2297040",
    "end": "2302400"
  },
  {
    "text": "where we presented the API that we imagine would be useful for this we",
    "start": "2302400",
    "end": "2308960"
  },
  {
    "text": "developed the concept of like it'll be pluggable like which you know which streaming uh service you use and it's at",
    "start": "2308960",
    "end": "2315920"
  },
  {
    "text": "a request for comments stage So um I present it because I am I am happy to present these things and ask for",
    "start": "2315920",
    "end": "2322160"
  },
  {
    "text": "comments So if you have anything that's relevant just go on GitHub and say look we're using it for this and I don't know",
    "start": "2322160",
    "end": "2328599"
  },
  {
    "text": "onetoone doesn't make sense to us Okay If that's the case then so it it can",
    "start": "2328599",
    "end": "2334400"
  },
  {
    "text": "take about a six months to a year I we're trying to put we're particular one",
    "start": "2334400",
    "end": "2339440"
  },
  {
    "text": "we're trying to get in under a year I don't think it's possible in six months to be honest That's that's a stretch About a year Yeah Okay I",
    "start": "2339440",
    "end": "2347760"
  },
  {
    "text": "guess I I think this question pertains to both uh the burst index and the burst cluster um solution But if you have a a",
    "start": "2358119",
    "end": "2367200"
  },
  {
    "text": "document I I think I understand how this this helps for writing new documents But if you have like an update or a delete",
    "start": "2367200",
    "end": "2373280"
  },
  {
    "text": "operation where you're searching across your old index or your kind of your",
    "start": "2373280",
    "end": "2378480"
  },
  {
    "text": "normal index and then either the burst index or the burst cluster and that update or that delete is reflected in",
    "start": "2378480",
    "end": "2384240"
  },
  {
    "text": "the burst cluster like what how does that get rectified between those two One",
    "start": "2384240",
    "end": "2389520"
  },
  {
    "text": "of the things you have to do if you're maintaining um these types of indexes",
    "start": "2389520",
    "end": "2394640"
  },
  {
    "text": "like a burst index and a is is you would want to have a prefix that signifies",
    "start": "2394640",
    "end": "2400960"
  },
  {
    "text": "that tenant so that any action you do like a deletion You'd say delete um based on",
    "start": "2400960",
    "end": "2408160"
  },
  {
    "text": "these aliases So you have the capability of specifying either the prefix with a",
    "start": "2408160",
    "end": "2413440"
  },
  {
    "text": "star in the end like a a wild card You could also give indexes and it's very",
    "start": "2413440",
    "end": "2418560"
  },
  {
    "text": "common to do this if especially if it's um time series data by the way is to give a read alias per day So you have um",
    "start": "2418560",
    "end": "2426560"
  },
  {
    "text": "an index and it contains different dates with the tenant ID connected to them So",
    "start": "2426560",
    "end": "2432560"
  },
  {
    "text": "when you perform a search um that tenant ID plus you know November um what is",
    "start": "2432560",
    "end": "2438640"
  },
  {
    "text": "today the 17th there 18th uh then it'll search that index is then made available",
    "start": "2438640",
    "end": "2444240"
  },
  {
    "text": "for search you can do the same thing when you're doing operations like get for a delete you can say these aliases I",
    "start": "2444240",
    "end": "2451040"
  },
  {
    "text": "want to delete them or I want to delete documents from them and it can go either to the burst cluster or you could go to",
    "start": "2451040",
    "end": "2458240"
  },
  {
    "text": "the um the indexes that have completely different names as long as the alias points to the",
    "start": "2458240",
    "end": "2463680"
  },
  {
    "text": "correct one The cluster means you have to really manage it You have to have some place where you're saying this",
    "start": "2463680",
    "end": "2469280"
  },
  {
    "text": "tenant has data here and there and the date that I changed the the tenant to be",
    "start": "2469280",
    "end": "2474960"
  },
  {
    "text": "over there and the date that I changed them back It's very important to keep track of those things I wouldn't do it within open search It's a common mistake",
    "start": "2474960",
    "end": "2481760"
  },
  {
    "text": "when you're managing open search is to say I have open search so I'm going to just store lots of information in it not just the you know the data that I built",
    "start": "2481760",
    "end": "2488960"
  },
  {
    "text": "the cluster for Mhm You really should just it should be a cluster for a thing and not for other things So audit data",
    "start": "2488960",
    "end": "2495200"
  },
  {
    "text": "should be separated from let's say your observability data You don't want to put them in the same place Thank you Sure",
    "start": "2495200",
    "end": "2504119"
  },
  {
    "text": "Um great talk by the way Um thank yeah so a question regarding the burst clusters as well as the the burst uh",
    "start": "2505200",
    "end": "2512480"
  },
  {
    "text": "nodes that you have with clusters How do you redirect uh the reads read load",
    "start": "2512480",
    "end": "2517760"
  },
  {
    "text": "directly Like are you uh is the assumption that we do like crosscluster search where you have uh the the the the",
    "start": "2517760",
    "end": "2526640"
  },
  {
    "text": "with open search dashboards in particular when you have all your alerts and all that and with observability data",
    "start": "2526640",
    "end": "2532599"
  },
  {
    "text": "um you are querying a particular set of indexes So when you move the data around",
    "start": "2532599",
    "end": "2537920"
  },
  {
    "text": "clusters how do you manage the search would be the question Okay So a few things for the for alerting Yeah it is",
    "start": "2537920",
    "end": "2545599"
  },
  {
    "text": "very difficult to do this if you're managing alerting using uh just the index If you use like a prefix it could",
    "start": "2545599",
    "end": "2552000"
  },
  {
    "text": "work Yeah when you search for if you're searching if you're doing crosscluster search the way that that feature works",
    "start": "2552000",
    "end": "2558720"
  },
  {
    "text": "is that you provide um in the cluster settings you provide the clusters that",
    "start": "2558720",
    "end": "2564079"
  },
  {
    "text": "it can also search on Yes Okay And then when you run a search um if you're doing",
    "start": "2564079",
    "end": "2569599"
  },
  {
    "text": "it through Amazon service it should be seamless If you're running it on your own you do have to specify instead of",
    "start": "2569599",
    "end": "2576000"
  },
  {
    "text": "just search this index it doesn't know that it has to go to the other cluster You have to say within this cluster and",
    "start": "2576000",
    "end": "2581920"
  },
  {
    "text": "that cluster and the other cluster search for this index So there is a multiplication of like you kind of have",
    "start": "2581920",
    "end": "2588000"
  },
  {
    "text": "to add these extra indexes to your search Understood Yeah there's a colon kind of mechanism where you put in So",
    "start": "2588000",
    "end": "2593599"
  },
  {
    "text": "basically what you're expecting here is an addition to write with read We have to keep that in mind before spinning up",
    "start": "2593599",
    "end": "2600319"
  },
  {
    "text": "a bus cluster Yeah you have to keep track where your data is when you're moving it Got it Yeah So the second part",
    "start": "2600319",
    "end": "2606079"
  },
  {
    "text": "of the question with burst nodes is um so I'm assuming you're amotizing the",
    "start": "2606079",
    "end": "2612319"
  },
  {
    "text": "cost of rebalancing because whenever the node goes up and down so your cluster uh",
    "start": "2612319",
    "end": "2618359"
  },
  {
    "text": "capacity or the CPU because shots are moving around and the and that requires",
    "start": "2618359",
    "end": "2623520"
  },
  {
    "text": "CPU network storage the transport actions are happening So you're assuming as part of your capacity planning you",
    "start": "2623520",
    "end": "2629119"
  },
  {
    "text": "have to amortize that cost as well Yes Moving a shard while it's being written",
    "start": "2629119",
    "end": "2634800"
  },
  {
    "text": "to Yeah And it has already like I don't know 100 gigs on it Moving that shard is a task that is just going to take time",
    "start": "2634800",
    "end": "2642480"
  },
  {
    "text": "You need high throughput now So it's kind of amvertised but it's very common",
    "start": "2642480",
    "end": "2647760"
  },
  {
    "text": "to do a rollover task with more shards when your throughput is big So it's it's kind of the same like you'd anyway be",
    "start": "2647760",
    "end": "2654000"
  },
  {
    "text": "doing this you'd anyway be adding a new you'd anyway anyway be rolling over to an index that has more shards and more",
    "start": "2654000",
    "end": "2660640"
  },
  {
    "text": "capability of writing on more nodes So it's sort of amvertised but Got it I",
    "start": "2660640",
    "end": "2666800"
  },
  {
    "text": "mean with the rollover you're not moving the data though right Like it's new shots getting created We don't want to move data when we're doing the spread",
    "start": "2666800",
    "end": "2673680"
  },
  {
    "text": "out That really slows things down Got it Yes Thank you",
    "start": "2673680",
    "end": "2679400"
  },
  {
    "text": "All right Thank you very much for coming and uh have a good one",
    "start": "2685680",
    "end": "2692200"
  }
]