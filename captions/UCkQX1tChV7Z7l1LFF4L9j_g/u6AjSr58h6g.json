[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "text": "foreign [Music]",
    "start": "1380",
    "end": "15680"
  },
  {
    "text": "my name is Alan Wang I've been building real-time data infrastructure for the past seven years first and Netflix for",
    "start": "15680",
    "end": "23400"
  },
  {
    "text": "the Keystone data Pipeline and currently at doordash building a real-time event processing system called Iguazu and the",
    "start": "23400",
    "end": "31199"
  },
  {
    "text": "journey of building those systems gave me an opportunity to find some common patterns in successful real-time data",
    "start": "31199",
    "end": "37559"
  },
  {
    "text": "architectures which I want to share with you in this talk",
    "start": "37559",
    "end": "42200"
  },
  {
    "start": "42000",
    "end": "42000"
  },
  {
    "text": "in my mind there are four principles that have passed the test of time",
    "start": "43079",
    "end": "48719"
  },
  {
    "text": "first decoupling stages we'll talk about what those cities are and why and how to",
    "start": "48719",
    "end": "54719"
  },
  {
    "text": "make that decoupling happen second leveraging stream processing",
    "start": "54719",
    "end": "60239"
  },
  {
    "text": "framework will cover what stream processing framework can do for you and how you can pick one",
    "start": "60239",
    "end": "67799"
  },
  {
    "text": "third creating abstractions for that I will List the common abstractions that",
    "start": "67799",
    "end": "72840"
  },
  {
    "text": "you can create to facilitate the adoption of your system and finally fine-grained failure",
    "start": "72840",
    "end": "80400"
  },
  {
    "text": "isolation and scalability failures will happen how do you isolate the failures and avoid bottleneck in scalability",
    "start": "80400",
    "end": "89640"
  },
  {
    "text": "I know those Concepts may sound a bit abstract at this point so I'm going to",
    "start": "89640",
    "end": "94860"
  },
  {
    "text": "use real-time event plus in system we created at doordash as an example to",
    "start": "94860",
    "end": "100320"
  },
  {
    "text": "give you some concrete ideas first let's talk about use cases of",
    "start": "100320",
    "end": "107939"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "real-time events at doordash real-time events are an important data source to",
    "start": "107939",
    "end": "113280"
  },
  {
    "text": "gain insight into our business and help us make data-driven decisions just to",
    "start": "113280",
    "end": "118619"
  },
  {
    "text": "give you a few examples how real-time events are used at doordash",
    "start": "118619",
    "end": "124079"
  },
  {
    "text": "first almost all events need to be reliably transported to our daily Warehouse snowflake or all or other",
    "start": "124079",
    "end": "131160"
  },
  {
    "text": "online analytical data stores with low latency for business analysis for",
    "start": "131160",
    "end": "136440"
  },
  {
    "text": "example the Dasher assignment team relies on the assignment events to detect any bugs in their algorithm at",
    "start": "136440",
    "end": "143580"
  },
  {
    "text": "near real time and the Second Use case is mobile application Health monitoring",
    "start": "143580",
    "end": "150599"
  },
  {
    "text": "some mobile events will be integrated with our time series metric backend for monitoring and alerting so that the",
    "start": "150599",
    "end": "157980"
  },
  {
    "text": "teams can quickly identify issues in the latest mobile application releases",
    "start": "157980",
    "end": "164599"
  },
  {
    "text": "the third use case is sessionalization we would like to group user events into sessions and generate session attributes",
    "start": "164819",
    "end": "172920"
  },
  {
    "text": "at real time so that we can better analyze the user behaviors and push real-time recommendations",
    "start": "172920",
    "end": "180980"
  },
  {
    "text": "historically doordash has a few data pipelines that get data from our Legacy",
    "start": "181400",
    "end": "186780"
  },
  {
    "text": "monolithic replication and ingest the data into snowflake each pipeline is",
    "start": "186780",
    "end": "192360"
  },
  {
    "text": "built differently and can only handle one kind of event they involve mixed data transports",
    "start": "192360",
    "end": "199260"
  },
  {
    "text": "multiple queuing systems and multiple third-party data services we should make",
    "start": "199260",
    "end": "204840"
  },
  {
    "text": "it really difficult to control the latency maintain the cost and identify singularity bottlenecks in operations",
    "start": "204840",
    "end": "214159"
  },
  {
    "start": "213000",
    "end": "213000"
  },
  {
    "text": "so we started to rethink this approach and decided to build a new system to replace those Legacy pipelines and",
    "start": "214680",
    "end": "222420"
  },
  {
    "text": "address the future event process needs that we anticipated first it should support heterogeneous",
    "start": "222420",
    "end": "229799"
  },
  {
    "text": "data sources including microservices and mobile or web applications can be able",
    "start": "229799",
    "end": "235799"
  },
  {
    "text": "to deliver the events to different destinations second it should provide low latency and",
    "start": "235799",
    "end": "242879"
  },
  {
    "text": "reliable data ingest into our data warehouse with a reasonable cost",
    "start": "242879",
    "end": "249180"
  },
  {
    "text": "third real-time events should be easily accessible for data consumers we want to",
    "start": "249180",
    "end": "254580"
  },
  {
    "text": "empower all users or teams to create their own processing logic and tap into",
    "start": "254580",
    "end": "260639"
  },
  {
    "text": "the streams of real-time data and finally to improve data quality we",
    "start": "260639",
    "end": "267479"
  },
  {
    "text": "want to have end-to-end skin reinforcement and schema evolution",
    "start": "267479",
    "end": "273259"
  },
  {
    "text": "so two years ago we started the Journey of creating from scratch a real-time",
    "start": "273960",
    "end": "279600"
  },
  {
    "text": "event processing system named Iguazu any important design decisions we made is to",
    "start": "279600",
    "end": "286080"
  },
  {
    "text": "shift the strategy from having rely on third-party data services to leveraging open source Frameworks that can be",
    "start": "286080",
    "end": "293040"
  },
  {
    "text": "customized and better integrated with the doordash infrastructure and the fast forward to today we scale",
    "start": "293040",
    "end": "299699"
  },
  {
    "text": "Legos from processing just a few beating events to hundreds of billions of events per day with four lines of delivery rate",
    "start": "299699",
    "end": "307080"
  },
  {
    "text": "and compared to the Legacy pipelines the end-to-end latency to snowflake is reduced from the day to just a few",
    "start": "307080",
    "end": "313800"
  },
  {
    "text": "minutes so this is the architecture overview of",
    "start": "313800",
    "end": "318919"
  },
  {
    "start": "315000",
    "end": "315000"
  },
  {
    "text": "the data ingestion from microservices and mobile clients are enabled through",
    "start": "318919",
    "end": "324240"
  },
  {
    "text": "our Iguazu Kafka proxy before it lands into Kafka as you can see we use Apache",
    "start": "324240",
    "end": "330479"
  },
  {
    "text": "Cloud for Pub sub and decoupling the producers from consumers once the data",
    "start": "330479",
    "end": "336180"
  },
  {
    "text": "lends into Kafka we use stream processing applications built on top of a party Frank for data transformation",
    "start": "336180",
    "end": "342560"
  },
  {
    "text": "and that is achieved through flink's data stream apis and Flink SQL",
    "start": "342560",
    "end": "348419"
  },
  {
    "text": "after the stream processing is done the data is sent to different gas stations including S3 for data warehouse and data",
    "start": "348419",
    "end": "355500"
  },
  {
    "text": "Lake Integrations radius or real-time features and chronosphere for operational Matrix",
    "start": "355500",
    "end": "361979"
  },
  {
    "text": "in the following slides I'll discuss the details of each those major components",
    "start": "361979",
    "end": "368360"
  },
  {
    "start": "368000",
    "end": "368000"
  },
  {
    "text": "so let's first talk about producing events the main focus is to make event",
    "start": "368580",
    "end": "374759"
  },
  {
    "text": "producing as easy as possible and optimize for the workload of analytical data",
    "start": "374759",
    "end": "380100"
  },
  {
    "text": "and to give more context let me first explain the requirements for processing analytical data",
    "start": "380100",
    "end": "386520"
  },
  {
    "text": "first analytical data usually comes with high volume and growth rate as an example in the process of making",
    "start": "386520",
    "end": "393600"
  },
  {
    "text": "restaurant order on doordash tens of thousands of analytical events are",
    "start": "393600",
    "end": "398940"
  },
  {
    "text": "published and as we strive to improve user experience and delivery quality",
    "start": "398940",
    "end": "404819"
  },
  {
    "text": "more and more user actions are tracked at the fine granularity and these lead",
    "start": "404819",
    "end": "410639"
  },
  {
    "text": "to the higher growth rate of analytical data volume compared with other volume",
    "start": "410639",
    "end": "417060"
  },
  {
    "text": "and therefore compare with the transactional data analytical data requires higher scalability and cost",
    "start": "417060",
    "end": "424139"
  },
  {
    "text": "efficiency on the other hand because analytical data are almost always processed and",
    "start": "424139",
    "end": "430680"
  },
  {
    "text": "analyzed in aggregated fashion minor data laws would not affect the quality of analysis for example if you collect",
    "start": "430680",
    "end": "438900"
  },
  {
    "text": "One Million results in an ad test and randomly drop 100 events from the",
    "start": "438900",
    "end": "444360"
  },
  {
    "text": "results most likely it will not affect the conclusion of experiment typically",
    "start": "444360",
    "end": "449699"
  },
  {
    "text": "we found the data loss of less than 0.1 percent will be accessible for for",
    "start": "449699",
    "end": "455099"
  },
  {
    "text": "analytical events so let's look at the measures that we",
    "start": "455099",
    "end": "462479"
  },
  {
    "start": "458000",
    "end": "458000"
  },
  {
    "text": "have taken to ensure the efficiency and the scalability in analytical event publishing",
    "start": "462479",
    "end": "468840"
  },
  {
    "text": "as I mentioned in the overview of the browser architecture we choose copper as",
    "start": "468840",
    "end": "474000"
  },
  {
    "text": "the central pub sub system but one challenge we face is how we can",
    "start": "474000",
    "end": "480020"
  },
  {
    "text": "enable every door Dev service to easily produce events through Kafka",
    "start": "480020",
    "end": "485340"
  },
  {
    "text": "and even though Kafka has been out there for quite some time there are still teams struggling with the producer",
    "start": "485340",
    "end": "491580"
  },
  {
    "text": "internals tuning the producer properties and making the Kafka connection and the solution we created is to",
    "start": "491580",
    "end": "499080"
  },
  {
    "text": "Leverage The Confluence calculus proxy the proxy provides a central place where",
    "start": "499080",
    "end": "504599"
  },
  {
    "text": "we can enhance and optimize event-producing functionalities and it provides abstractions over Kaka",
    "start": "504599",
    "end": "511740"
  },
  {
    "text": "with HP interface eliminating the need to config copper connections from all",
    "start": "511740",
    "end": "517260"
  },
  {
    "text": "services and making event publishing much easier",
    "start": "517260",
    "end": "522440"
  },
  {
    "start": "522000",
    "end": "522000"
  },
  {
    "text": "the Kafka West proxy provides all the basic features we need of the box one",
    "start": "523200",
    "end": "528540"
  },
  {
    "text": "critical feature is that it supports event patching you probably know that patching is a key factor in improving",
    "start": "528540",
    "end": "534959"
  },
  {
    "text": "efficiencies in data processing but why would it specifically help improving efficiency in event publishing",
    "start": "534959",
    "end": "542700"
  },
  {
    "text": "to Kafka what we have found out is is that the Kaka Brokers workload is highly",
    "start": "542700",
    "end": "549060"
  },
  {
    "text": "dependent on the rate of the producing requests the higher the rate of producing requests the higher the CPU utilization",
    "start": "549060",
    "end": "555839"
  },
  {
    "text": "on the broker and more Brokers will likely be needed so how can you produce your Kaka with",
    "start": "555839",
    "end": "562800"
  },
  {
    "text": "high data volume but at a low rate or produce requests as a formula in the slide suggests you",
    "start": "562800",
    "end": "571140"
  },
  {
    "text": "need to increase the number of events per request which essentially means increasing the batch size",
    "start": "571140",
    "end": "578160"
  },
  {
    "text": "batching comes in with trade-off of higher latency in event publishing",
    "start": "578160",
    "end": "584160"
  },
  {
    "text": "however produce processing of analytical events are typically done in an",
    "start": "584160",
    "end": "589680"
  },
  {
    "text": "asynchronous fashion and does not require sub second latency in getting back the final results",
    "start": "589680",
    "end": "596459"
  },
  {
    "text": "so small increase in the event population latency is an acceptable",
    "start": "596459",
    "end": "601500"
  },
  {
    "text": "trade-off with calcareous proxy you can patch events in each request from client-side",
    "start": "601500",
    "end": "608459"
  },
  {
    "text": "and rely on the Kaka finally the proxy to further bash them before producing to",
    "start": "608459",
    "end": "613860"
  },
  {
    "text": "the broker the result is that you will get a nice effect of event patching across client instances and applications",
    "start": "613860",
    "end": "621720"
  },
  {
    "text": "let's say you have an application that publishes events at very low rate which",
    "start": "621720",
    "end": "626880"
  },
  {
    "text": "would lead to inefficient batching but the proxy will be able to mix this",
    "start": "626880",
    "end": "632940"
  },
  {
    "text": "low volume events with other high volume events in one batch so it makes event",
    "start": "632940",
    "end": "639180"
  },
  {
    "text": "publishing um very efficient and greatly reduce the workload for the Kafka brokers",
    "start": "639180",
    "end": "647540"
  },
  {
    "start": "647000",
    "end": "647000"
  },
  {
    "text": "the calcareous proxy provides all the basic features we need out of the box but to further improve its performance",
    "start": "647640",
    "end": "655140"
  },
  {
    "text": "and scalability we added our own feature like multi-class reproducing and",
    "start": "655140",
    "end": "660480"
  },
  {
    "text": "asynchronous request processing multicaster producing means the same",
    "start": "660480",
    "end": "666120"
  },
  {
    "text": "proxy can produce through multiple Kafka clusters each topic will be mapped to a",
    "start": "666120",
    "end": "671519"
  },
  {
    "text": "cluster and this ensures that we can scale Beyond one Kaka cluster and it",
    "start": "671519",
    "end": "676860"
  },
  {
    "text": "also enables us to migrate topics from one copper cluster to another which",
    "start": "676860",
    "end": "682260"
  },
  {
    "text": "helps to balance the workload and improve the cost efficiency of cop car clusters",
    "start": "682260",
    "end": "688519"
  },
  {
    "text": "asynchronous request process it means the proxy will respond to a produced request as soon as it puts the copper",
    "start": "688519",
    "end": "695100"
  },
  {
    "text": "records into the copper client's producer buffer without waiting for the broker's acknowledgment and this has and",
    "start": "695100",
    "end": "702839"
  },
  {
    "text": "this has a few advantages first it significantly improves the",
    "start": "702839",
    "end": "708300"
  },
  {
    "text": "performance of the proxy and helps reduce the back pressure on the proxies",
    "start": "708300",
    "end": "713399"
  },
  {
    "text": "clients second asynchronous request process means the proxy spend less time block",
    "start": "713399",
    "end": "719760"
  },
  {
    "text": "reading for the broker's response and more time for the proxy to process",
    "start": "719760",
    "end": "726360"
  },
  {
    "text": "requests which lead to better batching and throughput finally we understand that this",
    "start": "726360",
    "end": "732899"
  },
  {
    "text": "asynchronous mode means clients may not get the actual Fair response from the",
    "start": "732899",
    "end": "738720"
  },
  {
    "text": "broker and may lead to date loss to mitigate this issue we added",
    "start": "738720",
    "end": "744120"
  },
  {
    "text": "automated reach files on the proxy side on behalf of the client the number of",
    "start": "744120",
    "end": "749220"
  },
  {
    "text": "retries is configurable and each subsequent retry will be done on a randomly picked partition to maximize",
    "start": "749220",
    "end": "756240"
  },
  {
    "text": "the chance of success the result is minimal data loss of less than",
    "start": "756240",
    "end": "762079"
  },
  {
    "text": "0.001 percent which is well in range of accept acceptable data loss level for",
    "start": "762079",
    "end": "769200"
  },
  {
    "text": "analytical events now that I have covered events producing",
    "start": "769200",
    "end": "775440"
  },
  {
    "start": "772000",
    "end": "772000"
  },
  {
    "text": "let's focus on what we have done on facilitated event consuming one important objective for Iguazu is to",
    "start": "775440",
    "end": "784019"
  },
  {
    "text": "create a platform for easy data processing Apache flings later API architecture",
    "start": "784019",
    "end": "790740"
  },
  {
    "text": "fits perfectly with this objective we choose Apache flank also because it's of",
    "start": "790740",
    "end": "797459"
  },
  {
    "text": "because of its low latency processing native support of processing based on",
    "start": "797459",
    "end": "803279"
  },
  {
    "text": "event time fault tolerance and built-in Integrations with the right wide range",
    "start": "803279",
    "end": "809100"
  },
  {
    "text": "of sources and things including Kaka radish elasticsearch and S3",
    "start": "809100",
    "end": "817339"
  },
  {
    "start": "816000",
    "end": "816000"
  },
  {
    "text": "But ultimately we need to understand what stream processing framework can do for you",
    "start": "817680",
    "end": "823260"
  },
  {
    "text": "we will demonstrate that by looking at a simple cargo consumer so this is a typical Cloud consumer",
    "start": "823260",
    "end": "830700"
  },
  {
    "text": "first it gets records from Cargo in a loop then it will update a local state",
    "start": "830700",
    "end": "836700"
  },
  {
    "text": "using the records it just retrieves and produce a result from the state finally",
    "start": "836700",
    "end": "842880"
  },
  {
    "text": "it will push on the result to a downstream process perhaps over the network",
    "start": "842880",
    "end": "849959"
  },
  {
    "text": "on the first look the code is really simple and does the job however",
    "start": "849959",
    "end": "855899"
  },
  {
    "text": "questions will arise over the time first note that the code does not commit",
    "start": "855899",
    "end": "862620"
  },
  {
    "text": "Kaka offset and this will likely lead to failures to provide any delivery",
    "start": "862620",
    "end": "868980"
  },
  {
    "text": "guarantee when failures occur but where should the offset commit be",
    "start": "868980",
    "end": "875160"
  },
  {
    "text": "added to the code to provide the desired delivery guarantee be it at least once and most ones or exactly ones",
    "start": "875160",
    "end": "883800"
  },
  {
    "text": "second the local state object is stored in memory and it will be lost when the",
    "start": "883800",
    "end": "889620"
  },
  {
    "text": "consumer crashes it should be persisted along with the Kafka offset so that the state can be",
    "start": "889620",
    "end": "896220"
  },
  {
    "text": "accurately restored upon failure recovery so how can we persist and restore the",
    "start": "896220",
    "end": "902699"
  },
  {
    "text": "state where necessary and finally the parallelism of Kappa",
    "start": "902699",
    "end": "908579"
  },
  {
    "text": "consumer is limited by the number of partitions of the topic being consumed but what if the bottleneck of the",
    "start": "908579",
    "end": "915720"
  },
  {
    "text": "application is in processing of the records or pushing the results to Downstream and we need higher",
    "start": "915720",
    "end": "922139"
  },
  {
    "text": "parallelism than the number of partitions apparently it takes more than a simple",
    "start": "922139",
    "end": "928800"
  },
  {
    "text": "cargo consumer to create a scalable and for torrent application this is where stream processing",
    "start": "928800",
    "end": "935339"
  },
  {
    "text": "framework like Flink shines as shown in this code example ring helps you to",
    "start": "935339",
    "end": "940800"
  },
  {
    "text": "achieve delivery guarantees automatically persists and restores application State through checkpointing",
    "start": "940800",
    "end": "948899"
  },
  {
    "text": "and provides a flexible way to assign compute resources to different data",
    "start": "948899",
    "end": "954300"
  },
  {
    "text": "operators and these are just a few examples that stream processing framework can offer",
    "start": "954300",
    "end": "960800"
  },
  {
    "start": "960000",
    "end": "960000"
  },
  {
    "text": "one of the most important features from Freak Flink is a layered apis",
    "start": "961139",
    "end": "967980"
  },
  {
    "text": "bottom and process function allows Engineers to create highly customized code and have precise control on",
    "start": "967980",
    "end": "975120"
  },
  {
    "text": "handling events State and time the next level up Flink offers data",
    "start": "975120",
    "end": "981600"
  },
  {
    "text": "stream apis with a built-in high level functions to support different aggregations and windowing so that",
    "start": "981600",
    "end": "988079"
  },
  {
    "text": "Engineers can create a stream processing solution with just a few lines of code",
    "start": "988079",
    "end": "994620"
  },
  {
    "text": "and on the top we have SQL and table apis which offer casual data users the",
    "start": "994620",
    "end": "999899"
  },
  {
    "text": "opportunity to write bling applications in a decorative way using SQL instead of",
    "start": "999899",
    "end": "1005540"
  },
  {
    "text": "code so to help people at door national average think we have created a stream",
    "start": "1005540",
    "end": "1011899"
  },
  {
    "start": "1007000",
    "end": "1007000"
  },
  {
    "text": "processing platform our platform provides a base frame Docker image with all the necessary",
    "start": "1011899",
    "end": "1017779"
  },
  {
    "text": "configurations that are well integrated with the rest of doordash infrastructure",
    "start": "1017779",
    "end": "1023240"
  },
  {
    "text": "blinks High availability setup and the flank internal metrics will be available",
    "start": "1023240",
    "end": "1028938"
  },
  {
    "text": "out of the box for better failure isolation and ability to scale independently each flame job is",
    "start": "1028939",
    "end": "1036500"
  },
  {
    "text": "deployed in a standalone node as a separate kubernetes service and we support two abstractions in our",
    "start": "1036500",
    "end": "1042798"
  },
  {
    "text": "platform data stream apis for engineers and the flame SQL for casual data users",
    "start": "1042799",
    "end": "1049160"
  },
  {
    "start": "1049000",
    "end": "1049000"
  },
  {
    "text": "you may be wondering why flank SQL is an important abstraction we want to leverage and here's a concrete example",
    "start": "1049160",
    "end": "1056660"
  },
  {
    "text": "real-time features are an important component in machine learning for data",
    "start": "1056660",
    "end": "1062720"
  },
  {
    "text": "modeling and data for for model model training and prediction",
    "start": "1062720",
    "end": "1068000"
  },
  {
    "text": "for example to predict an ETA of a doordash delivery order requires",
    "start": "1068000",
    "end": "1073520"
  },
  {
    "text": "up-to-date store order account for each restaurant which we call a real-time",
    "start": "1073520",
    "end": "1078740"
  },
  {
    "text": "feature traditionally creating a real-time feature requires coding a stream",
    "start": "1078740",
    "end": "1084380"
  },
  {
    "text": "processing application and transforms and algories the events into real-time",
    "start": "1084380",
    "end": "1090380"
  },
  {
    "text": "features creating a blink application requires a",
    "start": "1090380",
    "end": "1095780"
  },
  {
    "text": "big learning curve for machine learning engineers and becomes a bottleneck when",
    "start": "1095780",
    "end": "1101260"
  },
  {
    "text": "tens or hundreds of features needed to be created and the application created often have a",
    "start": "1101260",
    "end": "1108980"
  },
  {
    "text": "lot of boiler play code that are replicated across multiple applications Engineers also take the shortcut to",
    "start": "1108980",
    "end": "1116360"
  },
  {
    "text": "bundle the calculation of multiple features in one application which lacks failure isolation or the",
    "start": "1116360",
    "end": "1123559"
  },
  {
    "text": "ability to allocate more resources to specific feature calculation so to meet those challenges we decided",
    "start": "1123559",
    "end": "1130940"
  },
  {
    "text": "to create a SQL based DSL framework called video era where all the necessary processing logic and wiring are captured",
    "start": "1130940",
    "end": "1138320"
  },
  {
    "text": "in a yaml file the yamla file creates a lot of high-level abstractions for example",
    "start": "1138320",
    "end": "1143539"
  },
  {
    "text": "connecting to Cargo source and producing two certain things and to create a real-time feature",
    "start": "1143539",
    "end": "1150740"
  },
  {
    "text": "Engineers only need to create one yaml file so read the era achieved great results",
    "start": "1150740",
    "end": "1158020"
  },
  {
    "text": "of the time needed to develop a new feature is reduced from weeks to hours",
    "start": "1158020",
    "end": "1163539"
  },
  {
    "text": "and a feature engineering code base is reduced by 70 after migrating to Riviera",
    "start": "1163539",
    "end": "1170419"
  },
  {
    "start": "1170000",
    "end": "1170000"
  },
  {
    "text": "and here's a real DSL example uh to show how Flink SQL is used to calculate store",
    "start": "1170419",
    "end": "1177860"
  },
  {
    "text": "order count for each restaurant which is an important real-time feature used in production for model prediction",
    "start": "1177860",
    "end": "1185360"
  },
  {
    "text": "first you need to specify the source sync and a schema used in the application and in this case the source",
    "start": "1185360",
    "end": "1193100"
  },
  {
    "text": "is a Kafka topic and the sync is a radius cluster the process logic is",
    "start": "1193100",
    "end": "1198140"
  },
  {
    "text": "expressed as a SQL query you can see that we used the built-in count function",
    "start": "1198140",
    "end": "1203660"
  },
  {
    "text": "to aggregate the order count and we also used hot window",
    "start": "1203660",
    "end": "1208940"
  },
  {
    "text": "the hot window indicates that you want to process the data that are that are",
    "start": "1208940",
    "end": "1215059"
  },
  {
    "text": "received in the last 20 minutes and refresh the results every 20 seconds",
    "start": "1215059",
    "end": "1221740"
  },
  {
    "text": "so in the above two sections we covered event producing and consuming in Iguazu",
    "start": "1222140",
    "end": "1227660"
  },
  {
    "text": "however without a unified given format it's still difficult for producers and",
    "start": "1227660",
    "end": "1233059"
  },
  {
    "text": "consumers to understand each other so here we'll discuss the event format and",
    "start": "1233059",
    "end": "1238460"
  },
  {
    "text": "schemas which serve as a protocol between producers and consumers",
    "start": "1238460",
    "end": "1244580"
  },
  {
    "start": "1244000",
    "end": "1244000"
  },
  {
    "text": "from the very beginning we defined a unified event format the unified event format includes an",
    "start": "1244580",
    "end": "1251240"
  },
  {
    "text": "envelope and payload the payload contains the schema encoded event properties the envelope contains context",
    "start": "1251240",
    "end": "1258620"
  },
  {
    "text": "of the event for example event creation time metadata including encoding methods and",
    "start": "1258620",
    "end": "1266179"
  },
  {
    "text": "the references to the schema it also includes a non-schematized Json",
    "start": "1266179",
    "end": "1271580"
  },
  {
    "text": "blog called custom attributes this Json section in the envelope gives user a",
    "start": "1271580",
    "end": "1276799"
  },
  {
    "text": "choice where they can store certain data and evolve them freely without going through the formal schema Evolution and",
    "start": "1276799",
    "end": "1284780"
  },
  {
    "text": "this flexibility proves to be useful at early stage of event creation where",
    "start": "1284780",
    "end": "1290360"
  },
  {
    "text": "frequent adjustments of schema definition are expected",
    "start": "1290360",
    "end": "1296559"
  },
  {
    "text": "then we created a serialization libraries for both event producers and",
    "start": "1296780",
    "end": "1302419"
  },
  {
    "text": "consumers to interact with this standard event format in Kafka the event envelope is stored as",
    "start": "1302419",
    "end": "1310400"
  },
  {
    "text": "a cop car record header and the schema encoded payload is stored as a record value our serialization library takes",
    "start": "1310400",
    "end": "1316820"
  },
  {
    "text": "the responsibility of converting back and forth between the event API and",
    "start": "1316820",
    "end": "1322100"
  },
  {
    "text": "under properly encoded Kafka record so that the applications can focus on their",
    "start": "1322100",
    "end": "1327200"
  },
  {
    "text": "main logic and we happily leveraged Confluence",
    "start": "1327200",
    "end": "1332360"
  },
  {
    "start": "1329000",
    "end": "1329000"
  },
  {
    "text": "schema register 1400 data processing but first let me briefly introduce",
    "start": "1332360",
    "end": "1337640"
  },
  {
    "text": "schema registry as we know schema is a contract between",
    "start": "1337640",
    "end": "1342919"
  },
  {
    "text": "producers and consumers on data or events that they both interact with",
    "start": "1342919",
    "end": "1348559"
  },
  {
    "text": "to make sure producers and consumers agree on you on the schema one simple",
    "start": "1348559",
    "end": "1353900"
  },
  {
    "text": "way is to present the schema as a Tojo class which is available for both producers",
    "start": "1353900",
    "end": "1359179"
  },
  {
    "text": "and consumers however there's no guarantee that a producers and consumers will have the",
    "start": "1359179",
    "end": "1365299"
  },
  {
    "text": "same version of the podium and to ensure that a change of the",
    "start": "1365299",
    "end": "1370880"
  },
  {
    "text": "schema is propagated from the producer to the consumer they must coordinate on",
    "start": "1370880",
    "end": "1377419"
  },
  {
    "text": "when the new project class will be made available for each party",
    "start": "1377419",
    "end": "1382640"
  },
  {
    "text": "Sigma registry helps to avoid this manual coordination between producers and consumers on schema changes",
    "start": "1382640",
    "end": "1390320"
  },
  {
    "text": "it is a standalone best service to store schemas and serve the schema lookup",
    "start": "1390320",
    "end": "1395480"
  },
  {
    "text": "requests from both producers and consumers using schema IDs",
    "start": "1395480",
    "end": "1400880"
  },
  {
    "text": "when schema changes are registered with schema registry it will enforce compatibility rules and reject",
    "start": "1400880",
    "end": "1407539"
  },
  {
    "text": "incompatible schema changes so to leverage this given registry of",
    "start": "1407539",
    "end": "1413419"
  },
  {
    "text": "1400 data processing schema ID is embedded in the event panel so that the downstream consumers can look it up from",
    "start": "1413419",
    "end": "1420740"
  },
  {
    "text": "schema registry without relying on the object classes on the runtime class path",
    "start": "1420740",
    "end": "1426440"
  },
  {
    "text": "both protobuf and arrow schemas are supported in our cr9 vision library and schema registry we support Pro Golf",
    "start": "1426440",
    "end": "1434120"
  },
  {
    "text": "schema because almost all of our microservices are based on grpc and",
    "start": "1434120",
    "end": "1439159"
  },
  {
    "text": "photo buff supported by a central portable gate Repository",
    "start": "1439159",
    "end": "1444200"
  },
  {
    "text": "so to enforce this single source of Truth avoid duplicate schema definition and to ensure the smooth adoption of",
    "start": "1444200",
    "end": "1451760"
  },
  {
    "text": "Iguazu we decided to use the protobuf as the primary schema type",
    "start": "1451760",
    "end": "1459020"
  },
  {
    "text": "but on the other hand the address FEMA is still better supported than put above",
    "start": "1459020",
    "end": "1464120"
  },
  {
    "text": "in most of the data Frameworks so when necessary our serialization library takes the responsibility of seamlessly",
    "start": "1464120",
    "end": "1471320"
  },
  {
    "text": "converting the product message to Avro format and vice versa",
    "start": "1471320",
    "end": "1477460"
  },
  {
    "start": "1476000",
    "end": "1476000"
  },
  {
    "text": "one decision we have to make is when we should allow schema update to happen and there are two choices build time or",
    "start": "1478039",
    "end": "1485240"
  },
  {
    "text": "producers runtime it is usually tempting to let producers",
    "start": "1485240",
    "end": "1490280"
  },
  {
    "text": "freely update the schema at the time of event publishing and there are some risks associated with",
    "start": "1490280",
    "end": "1497299"
  },
  {
    "text": "that it may lead to data loss because any incompatible schema change will fail",
    "start": "1497299",
    "end": "1503299"
  },
  {
    "text": "the schema registry update and cause runtime failures in event publishing",
    "start": "1503299",
    "end": "1509600"
  },
  {
    "text": "it would also lead to spikes of schema registry update requests causing",
    "start": "1509600",
    "end": "1516280"
  },
  {
    "text": "potential security issues for the schema registry so instead it will be ideal to register",
    "start": "1516280",
    "end": "1523340"
  },
  {
    "text": "and update the schema at build time to catch incompatible schema changes early in the development cycle and reduce the",
    "start": "1523340",
    "end": "1531080"
  },
  {
    "text": "update API call volume to the schema registry but one challenge we faced is how we can",
    "start": "1531080",
    "end": "1539059"
  },
  {
    "start": "1535000",
    "end": "1535000"
  },
  {
    "text": "centrally automate on the schema update at build time the solution we created is to leverage",
    "start": "1539059",
    "end": "1545240"
  },
  {
    "text": "the central repulsory that manages all of our product of schemas and integrate",
    "start": "1545240",
    "end": "1550520"
  },
  {
    "text": "the schema registry update as part of its CI CD process",
    "start": "1550520",
    "end": "1555980"
  },
  {
    "text": "when a product of definition is updated in the pull request the CI process will validate the change with the schema",
    "start": "1555980",
    "end": "1562580"
  },
  {
    "text": "registry and it will fail if the change is incompatible and after the CI passes and the pull",
    "start": "1562580",
    "end": "1569480"
  },
  {
    "text": "request emerged the CD process and will actually register our updated schema",
    "start": "1569480",
    "end": "1574640"
  },
  {
    "text": "registry and publish the compiled import of Jar files",
    "start": "1574640",
    "end": "1580940"
  },
  {
    "text": "the CI CD process not only eliminates the overhead of manual schema",
    "start": "1580940",
    "end": "1586159"
  },
  {
    "text": "registration but also guarantees the early detection of incompatible schema",
    "start": "1586159",
    "end": "1591200"
  },
  {
    "text": "changes and the consistency between the 3D support above cost binaries and the",
    "start": "1591200",
    "end": "1597260"
  },
  {
    "text": "schemas in the schema registry and ultimately on this automation avoids",
    "start": "1597260",
    "end": "1603140"
  },
  {
    "text": "schema update at runtime and the possible data loss due to incompatible",
    "start": "1603140",
    "end": "1608299"
  },
  {
    "text": "schema changes so far we haven't talked about them",
    "start": "1608299",
    "end": "1613940"
  },
  {
    "start": "1610000",
    "end": "1610000"
  },
  {
    "text": "producing consuming an event format and in this slide I'll give some details on",
    "start": "1613940",
    "end": "1619340"
  },
  {
    "text": "our data warehouse integration data warehouse integration is one of the key goals of egoism snowflake is our",
    "start": "1619340",
    "end": "1627140"
  },
  {
    "text": "main data warehouse and we expect events to be delivered to a snowflake with",
    "start": "1627140",
    "end": "1632840"
  },
  {
    "text": "strong consistency and low latency the data warehouse integration is",
    "start": "1632840",
    "end": "1638240"
  },
  {
    "text": "implemented as a two-step process in the first step data is consumed by a",
    "start": "1638240",
    "end": "1645020"
  },
  {
    "text": "Flink application from a Kafka topic and upload it to S3 in the Pro key format",
    "start": "1645020",
    "end": "1650960"
  },
  {
    "text": "and this step leverages S3 to decouple the stream processing from Snowflake so that a snowflake",
    "start": "1650960",
    "end": "1658880"
  },
  {
    "text": "ingredient failures will not impact stream processing it will also provide a backfield",
    "start": "1658880",
    "end": "1665179"
  },
  {
    "text": "mechanism from S3 given kafka's limited retention and finally having the proper files on",
    "start": "1665179",
    "end": "1672500"
  },
  {
    "text": "S3 enables daily Integrations as the pro key data can be easily converted to any",
    "start": "1672500",
    "end": "1678799"
  },
  {
    "text": "desired table format and the implementation of uploading data",
    "start": "1678799",
    "end": "1684260"
  },
  {
    "text": "to S3 is done through flink's streaming file sync when completing an upload as part of",
    "start": "1684260",
    "end": "1691039"
  },
  {
    "text": "Link's checkpoint streaming file signal guarantees strong consistency and exactly once delivery",
    "start": "1691039",
    "end": "1697880"
  },
  {
    "text": "stream housing also allows customized marketing on S3 which means you can",
    "start": "1697880",
    "end": "1702919"
  },
  {
    "text": "flexibly partition the data using any fields and this optimization greatly",
    "start": "1702919",
    "end": "1708620"
  },
  {
    "text": "facilitates the downstream consumers as a second step and data is copied from",
    "start": "1708620",
    "end": "1716419"
  },
  {
    "text": "S3 to snowflake tables via snow pipe and snowpipe is a snowflake service to load",
    "start": "1716419",
    "end": "1723679"
  },
  {
    "text": "it from external storage at near real time triggered by sqs messages snow pipe",
    "start": "1723679",
    "end": "1729559"
  },
  {
    "text": "copies data from S3 as soon as they become available",
    "start": "1729559",
    "end": "1734600"
  },
  {
    "text": "snowpipe also allows simple data transformation during the copy process and given that the soap pipe is",
    "start": "1734600",
    "end": "1741320"
  },
  {
    "text": "decorative and easy to use it's a good alternative compared to doing data transformation in stream processing",
    "start": "1741320",
    "end": "1748640"
  },
  {
    "text": "and why important thing to know is that each event has its own stream processing",
    "start": "1748640",
    "end": "1754400"
  },
  {
    "text": "application for S3 upload and its own Stone pipe as a result we can scale",
    "start": "1754400",
    "end": "1760460"
  },
  {
    "text": "pipelines for each event individually and I say the failures",
    "start": "1760460",
    "end": "1766779"
  },
  {
    "start": "1766000",
    "end": "1766000"
  },
  {
    "text": "so far we covered end-to-end data flows from clients to data warehouse and here",
    "start": "1767480",
    "end": "1774200"
  },
  {
    "text": "we want to discuss the operational aspect of Iguazu and see how we are making it a self-serve to reduce",
    "start": "1774200",
    "end": "1781340"
  },
  {
    "text": "operational burdens as you recall to achieve failure",
    "start": "1781340",
    "end": "1787279"
  },
  {
    "text": "isolation each event in Wazoo has its own pipeline from Flint job to store pipe",
    "start": "1787279",
    "end": "1793520"
  },
  {
    "text": "and this requires a lot more setup work and makes your operation a challenge",
    "start": "1793520",
    "end": "1798860"
  },
  {
    "text": "and this diagram shows the complicated steps required to onboard a new event",
    "start": "1798860",
    "end": "1804159"
  },
  {
    "text": "including create creation of the Kappa topic schema registration creation of",
    "start": "1804159",
    "end": "1809779"
  },
  {
    "text": "the of Link stream processing job and creation of this snowflake objects",
    "start": "1809779",
    "end": "1816260"
  },
  {
    "text": "so we really want to automate all these manual steps to improve operational",
    "start": "1816260",
    "end": "1821299"
  },
  {
    "text": "efficiency but one challenge we face is how we can accomplish it under the infrastructure",
    "start": "1821299",
    "end": "1827720"
  },
  {
    "text": "as co-principal at doordash most of most of operations",
    "start": "1827720",
    "end": "1833299"
  },
  {
    "text": "from creating copper topic to setting up service configurations involves some",
    "start": "1833299",
    "end": "1838399"
  },
  {
    "text": "kind of pull requests to different terraform repositories and requires code review",
    "start": "1838399",
    "end": "1844220"
  },
  {
    "text": "so how can we automate all these to solve the issue we first worked with",
    "start": "1844220",
    "end": "1850700"
  },
  {
    "text": "our infrastructure team to set up the right pre-approval process and then automate the pull request using",
    "start": "1850700",
    "end": "1858200"
  },
  {
    "text": "GitHub automation essentially a GitHub app is created where we can programmatically create and",
    "start": "1858200",
    "end": "1867140"
  },
  {
    "text": "merge pull requests and we also Leverage The Cadence workflow engine and implemented the",
    "start": "1867140",
    "end": "1873620"
  },
  {
    "text": "process as a reliable workflow and this home automation reduce the",
    "start": "1873620",
    "end": "1879620"
  },
  {
    "text": "event onboarding time from days to minutes and we get the best of both worlds where",
    "start": "1879620",
    "end": "1886520"
  },
  {
    "text": "we achieve affirmations but also get versioning all the necessary code reviews and consistent",
    "start": "1886520",
    "end": "1893480"
  },
  {
    "text": "State between our code and the infrastructure and to get us one step closer to",
    "start": "1893480",
    "end": "1899960"
  },
  {
    "text": "self-serve we created high-level uis for users to onboard the event",
    "start": "1899960",
    "end": "1905899"
  },
  {
    "text": "and this screenshot shows the schema exploration UI where users can search",
    "start": "1905899",
    "end": "1911539"
  },
  {
    "text": "for a schema using rackets pick the right um schema subject and version and then",
    "start": "1911539",
    "end": "1918740"
  },
  {
    "text": "onboard the event from there and this screenshot shows the snowpipe",
    "start": "1918740",
    "end": "1924679"
  },
  {
    "start": "1921000",
    "end": "1921000"
  },
  {
    "text": "integration UI where users can review and create snowflake table schemas and",
    "start": "1924679",
    "end": "1930679"
  },
  {
    "text": "snow pipe schemas most importantly we created a service called Minions that does the",
    "start": "1930679",
    "end": "1937399"
  },
  {
    "text": "orchestration and have have it integrated with slack so that we get",
    "start": "1937399",
    "end": "1942919"
  },
  {
    "text": "modification notifications on each step it carries out on this screenshot you can see about 17",
    "start": "1942919",
    "end": "1950380"
  },
  {
    "text": "actions it has taken in order to onboard an event including all the pull requests",
    "start": "1950380",
    "end": "1956179"
  },
  {
    "text": "launching the Flint job and creating snowflake objects",
    "start": "1956179",
    "end": "1962860"
  },
  {
    "text": "so now that we covered um the architecture and designs of all the major components in ibazu I would like",
    "start": "1963320",
    "end": "1970760"
  },
  {
    "text": "to once again review the four principles that I have emphasized throughout my talk",
    "start": "1970760",
    "end": "1977559"
  },
  {
    "text": "so the first um principle is to decouple different stages",
    "start": "1978260",
    "end": "1984220"
  },
  {
    "text": "you want to use a pop sub or messaging system to decouple data producers from consumers",
    "start": "1984860",
    "end": "1990980"
  },
  {
    "text": "this not only relieves the producers from back pressure but also increases the data durability for Downstream",
    "start": "1990980",
    "end": "1998480"
  },
  {
    "text": "consumers and there are a couple of considerations you can use when choosing the right pops",
    "start": "1998480",
    "end": "2004720"
  },
  {
    "text": "up or messaging system first it should be simple to use I tend",
    "start": "2004720",
    "end": "2009820"
  },
  {
    "text": "to choose the one that is built with a single purpose and do it well secondly each if you have high",
    "start": "2009820",
    "end": "2016059"
  },
  {
    "text": "durability and throughput guarantees and it should be highly scalable even when",
    "start": "2016059",
    "end": "2021580"
  },
  {
    "text": "there's a lot of consumers and high data fan art",
    "start": "2021580",
    "end": "2026580"
  },
  {
    "text": "similarly stream processing should be decoupled from any down Downstream",
    "start": "2027399",
    "end": "2032620"
  },
  {
    "text": "processes including the data warehouse using a cost-effective cloud storage or",
    "start": "2032620",
    "end": "2039340"
  },
  {
    "text": "data Lake and this guarantees that in case there's any failures in the data warehouse you don't have to stop the",
    "start": "2039340",
    "end": "2046000"
  },
  {
    "text": "stream processing job and you can always use the cheaper storage for backfill",
    "start": "2046000",
    "end": "2052800"
  },
  {
    "start": "2052000",
    "end": "2052000"
  },
  {
    "text": "as a second principle it pays to invest on the right stream processing framework",
    "start": "2053320",
    "end": "2060158"
  },
  {
    "text": "three reasons you can use when choosing the right stream processing framework",
    "start": "2060159",
    "end": "2065560"
  },
  {
    "text": "first issues support multiple abstractions so that you can create different solutions according to the",
    "start": "2065560",
    "end": "2071618"
  },
  {
    "text": "type of the audience and it should have Integrations with",
    "start": "2071619",
    "end": "2077138"
  },
  {
    "text": "sources and things you need and support a variety of data formats like Avro Json",
    "start": "2077139",
    "end": "2084040"
  },
  {
    "text": "Port above or parquet so that you can choose the right data format in the",
    "start": "2084040",
    "end": "2089560"
  },
  {
    "text": "right stage of stream processing and the third principle is to create",
    "start": "2089560",
    "end": "2095858"
  },
  {
    "start": "2092000",
    "end": "2092000"
  },
  {
    "text": "abstractions in my opinion obstruction is an",
    "start": "2095859",
    "end": "2101020"
  },
  {
    "text": "important factor to facilitate the adoption of any complicated system when it comes to the real-time data",
    "start": "2101020",
    "end": "2107980"
  },
  {
    "text": "infrastructure infrastructure there are a few common abstractions worth considering for example leveraging",
    "start": "2107980",
    "end": "2114880"
  },
  {
    "text": "Copperas proxy creating event API and serialization Library",
    "start": "2114880",
    "end": "2120700"
  },
  {
    "text": "providing a DSL framework by Frank siku and providing higher high level uis with",
    "start": "2120700",
    "end": "2127599"
  },
  {
    "text": "orchestrations finally you need to have fine-grained",
    "start": "2127599",
    "end": "2133480"
  },
  {
    "start": "2130000",
    "end": "2130000"
  },
  {
    "text": "failure isolation and scalability for that purpose you should aim to",
    "start": "2133480",
    "end": "2139540"
  },
  {
    "text": "create independent stream processing jobs or pipelines for each event this",
    "start": "2139540",
    "end": "2145000"
  },
  {
    "text": "avoids resource contention isolates failures and makes it easy to scale each pipeline independently",
    "start": "2145000",
    "end": "2152140"
  },
  {
    "text": "you also get the added benefits of being able to provide different slas for",
    "start": "2152140",
    "end": "2158320"
  },
  {
    "text": "different events and easily attribute your costs at event level and because you are creating a lot of",
    "start": "2158320",
    "end": "2164560"
  },
  {
    "text": "independent services and pipeline to achieve feeder isolation it is crucial to create orchestration service to",
    "start": "2164560",
    "end": "2172720"
  },
  {
    "text": "automate things and reduce the operational overhead",
    "start": "2172720",
    "end": "2177960"
  },
  {
    "start": "2177000",
    "end": "2177000"
  },
  {
    "text": "and here are some final thoughts that go beyond the architecture first I find it important to build",
    "start": "2178000",
    "end": "2184180"
  },
  {
    "text": "products using a platform mindset ad hoc Solutions are not only",
    "start": "2184180",
    "end": "2190000"
  },
  {
    "text": "inefficient but also difficult to skill and operate and secondly picking the right framework",
    "start": "2190000",
    "end": "2197020"
  },
  {
    "text": "and creating the right building blocks is crucial to ensure success once you",
    "start": "2197020",
    "end": "2202839"
  },
  {
    "text": "have research and find the sweet spots of those Frameworks you can easily create new products by combining a few",
    "start": "2202839",
    "end": "2210040"
  },
  {
    "text": "building blocks and this dramatically reduces the time to build new products and the effort to maintain the platform",
    "start": "2210040",
    "end": "2217300"
  },
  {
    "text": "I would like to use the calculus proxy as an example we use it initially for event publishing",
    "start": "2217300",
    "end": "2224320"
  },
  {
    "text": "from internal Microsoft microservices but when the time comes for us to",
    "start": "2224320",
    "end": "2229420"
  },
  {
    "text": "develop a solution for mobile events we found the proxy is also good fit because",
    "start": "2229420",
    "end": "2234820"
  },
  {
    "text": "it supports patching and Json payload which are important for mobile events so",
    "start": "2234820",
    "end": "2240880"
  },
  {
    "text": "with a little enhancement we made it work as an edge service which save us a",
    "start": "2240880",
    "end": "2246220"
  },
  {
    "text": "lot of development time that's all I have today thank you for",
    "start": "2246220",
    "end": "2251859"
  },
  {
    "text": "attending my talk and please follow us on our tech box for more details and the",
    "start": "2251859",
    "end": "2257800"
  },
  {
    "text": "latest developments",
    "start": "2257800",
    "end": "2260700"
  },
  {
    "text": "hello hi sir how are you doing okay you've been on top of all of these",
    "start": "2264099",
    "end": "2269680"
  },
  {
    "text": "questions like a huge number of questions to come in um were there any that you wanted to",
    "start": "2269680",
    "end": "2274720"
  },
  {
    "text": "provide more context on I'm sorry the context of uh of this talk",
    "start": "2274720",
    "end": "2281099"
  },
  {
    "text": "no no the questions uh so uh you wrote some for example there was one about",
    "start": "2281099",
    "end": "2286540"
  },
  {
    "text": "would you use the best proxy of data option if lost is not an option yeah yeah so that's a good question uh",
    "start": "2286540",
    "end": "2293079"
  },
  {
    "text": "remember in the talk I mentioned uh that we added the uh the actual capability to",
    "start": "2293079",
    "end": "2298720"
  },
  {
    "text": "do asynchronous request processing so that's uh is designed purely for",
    "start": "2298720",
    "end": "2303880"
  },
  {
    "text": "analytical data because it would introduce maybe like very minor data loss because we are not giving back the",
    "start": "2303880",
    "end": "2311440"
  },
  {
    "text": "actual broker accounting back to the client but you can also use the rest",
    "start": "2311440",
    "end": "2317200"
  },
  {
    "text": "proxy in a different way where it will give precisely what you know what the uh",
    "start": "2317200",
    "end": "2322780"
  },
  {
    "text": "broker announcements is so you are you're basically you are adding you are adding a date hop in the middle but",
    "start": "2322780",
    "end": "2329859"
  },
  {
    "text": "client would know definitely whether this you know uh you know this this producing request is success or not or",
    "start": "2329859",
    "end": "2337180"
  },
  {
    "text": "it can timeout sometimes you can time or let's say uh you you send something to the proxy and the proxy crash right and",
    "start": "2337180",
    "end": "2343900"
  },
  {
    "text": "you don't hear anything back from the proxy but on the on the client side you will know it's time though then you",
    "start": "2343900",
    "end": "2349180"
  },
  {
    "text": "you'll try to do sometimes we try yeah um another question there like because",
    "start": "2349180",
    "end": "2355000"
  },
  {
    "text": "you were mentioning about throughput um you so I'm assuming like rest",
    "start": "2355000",
    "end": "2360280"
  },
  {
    "text": "requests coming to the rest proxy you accumulated in some buffer but you're responding back uh to the rest clients",
    "start": "2360280",
    "end": "2366820"
  },
  {
    "text": "if they're mobile or whatever that you receive the data but it's uncommitted and so therefore you can do larger",
    "start": "2366820",
    "end": "2372460"
  },
  {
    "text": "rights and get higher throughput sense uh correct would you also to change that uh so that yes of course that that also",
    "start": "2372460",
    "end": "2380980"
  },
  {
    "text": "has to be changed if you are really looking for a lossless uh data",
    "start": "2380980",
    "end": "2387339"
  },
  {
    "text": "publishing yeah that that you you it's uh it's not desirable to have that kind",
    "start": "2387339",
    "end": "2392920"
  },
  {
    "text": "of behavior okay there's another question uh also curious about alternative to Flink there",
    "start": "2392920",
    "end": "2399220"
  },
  {
    "text": "are two people who asked about your fling Alternatives uh one person is using akka",
    "start": "2399220",
    "end": "2404859"
  },
  {
    "text": "and uh they they're moving off AKA towards link and they were wondering what your alternative is",
    "start": "2404859",
    "end": "2410560"
  },
  {
    "text": "I think you said uh I think you haven't responded to that one no I",
    "start": "2410560",
    "end": "2416880"
  },
  {
    "text": "have to have not used akka before um yes flank is the the primary one that",
    "start": "2416880",
    "end": "2423640"
  },
  {
    "text": "I use uh I I think um fling probably provides more data",
    "start": "2423640",
    "end": "2429760"
  },
  {
    "text": "processing capability than occur including all the abstractions it",
    "start": "2429760",
    "end": "2435400"
  },
  {
    "text": "provide the sequel support uh and the uh",
    "start": "2435400",
    "end": "2441060"
  },
  {
    "text": "and the fault tolerance built on top of the the Flink internal State uh so I I",
    "start": "2441060",
    "end": "2448720"
  },
  {
    "text": "would definitely recommend using either you know these kind of Frameworks that provide these capabilities",
    "start": "2448720",
    "end": "2456339"
  },
  {
    "text": "another person asked also curious about alternatives to Flink we used to use airflow plus Beam on data flow to build",
    "start": "2456339",
    "end": "2464200"
  },
  {
    "text": "a similar framework I'm not sure how airflow Works in that but uh did your team evaluate beam as well as and why",
    "start": "2464200",
    "end": "2472060"
  },
  {
    "text": "just Flink uh we usually hear about beam and Flink right because correct",
    "start": "2472060",
    "end": "2479560"
  },
  {
    "text": "um so uh I I I think beam does not add a",
    "start": "2480579",
    "end": "2487720"
  },
  {
    "text": "lot of advantages in streaming that's an API kind of layer right and it can be used with smart streaming or with right",
    "start": "2487720",
    "end": "2494619"
  },
  {
    "text": "fling so so unless you want to have the capability of being able to migrate your",
    "start": "2494619",
    "end": "2500079"
  },
  {
    "text": "workload to a completely different stream processing engine uh being is is",
    "start": "2500079",
    "end": "2505839"
  },
  {
    "text": "one of those choices that you can use but I I also know that some of the",
    "start": "2505839",
    "end": "2510940"
  },
  {
    "text": "built-in functionalities in those stream stream constant edges are not transferable to to another",
    "start": "2510940",
    "end": "2518440"
  },
  {
    "text": "um so I think most likely than not than not you are using certain you know built-in functionalities of those stream",
    "start": "2518440",
    "end": "2525520"
  },
  {
    "text": "processing framework which is not exposed with the beam uh and and the one and so it's actually hard to migrate to",
    "start": "2525520",
    "end": "2533020"
  },
  {
    "text": "it to a different one um so I think beam is a is a very good concept but in",
    "start": "2533020",
    "end": "2538240"
  },
  {
    "text": "reality um I feel it's it's actually more efficient to to stick to to the native",
    "start": "2538240",
    "end": "2544420"
  },
  {
    "text": "stream processing framework got it um because because I guess it's a single",
    "start": "2544420",
    "end": "2549579"
  },
  {
    "text": "dialect if the two engines Below have the same functionality but the two fun engines are diverging and adding their",
    "start": "2549579",
    "end": "2555820"
  },
  {
    "text": "own so having a single dialect is tough how do you handle retries in the event processing platform are there any",
    "start": "2555820",
    "end": "2561640"
  },
  {
    "text": "persistence are there any persistence to the problematic events with an eventual",
    "start": "2561640",
    "end": "2566800"
  },
  {
    "text": "reprocessing yeah so so the uh retries",
    "start": "2566800",
    "end": "2572440"
  },
  {
    "text": "um I'm not sure I'm sure what is that context so in in both uh the event publishing stage uh and the event",
    "start": "2572440",
    "end": "2579339"
  },
  {
    "text": "process of state or consuming stage you can do retries right so let's say in publishing stage of course when you when",
    "start": "2579339",
    "end": "2586420"
  },
  {
    "text": "you try to publish something the broker you know give you a an active response you can always retry",
    "start": "2586420",
    "end": "2593200"
  },
  {
    "text": "um and on the processing side when you consume the data um",
    "start": "2593200",
    "end": "2600119"
  },
  {
    "text": "I think I understand what they're saying so what they're saying is how do you handle retries and event processing platform they're asking that by the way",
    "start": "2600280",
    "end": "2605560"
  },
  {
    "text": "they're actually the next one is if you have a bad message do you persist these away like in a dead",
    "start": "2605560",
    "end": "2611560"
  },
  {
    "text": "letter queue with an eventual reprocessing of it once it's patched right so so",
    "start": "2611560",
    "end": "2617619"
  },
  {
    "text": "um so in in the uh um in in the context of data processing or consuming the data uh you when you if you push something to",
    "start": "2617619",
    "end": "2625420"
  },
  {
    "text": "the downstream and that that failed or you are processing something in that field you can you can still do retry but",
    "start": "2625420",
    "end": "2630700"
  },
  {
    "text": "I think that Latitude is uh probably the ultimate answer um that you can you know push the the",
    "start": "2630700",
    "end": "2637599"
  },
  {
    "text": "data back to a different Kappa topic for example and have uh the the application",
    "start": "2637599",
    "end": "2643300"
  },
  {
    "text": "consumed again from that that other queue uh and uh you can you can add a",
    "start": "2643300",
    "end": "2649060"
  },
  {
    "text": "lot of controls on like how how many times you want to retry how long you want to keep retrying uh but yeah I",
    "start": "2649060",
    "end": "2655240"
  },
  {
    "text": "think that attitude is essential if you really want uh minimize your data",
    "start": "2655240",
    "end": "2662099"
  },
  {
    "text": "okay so practical question Apache Kafka is a popular choice especially for real-time event stream processing I'm",
    "start": "2662260",
    "end": "2668500"
  },
  {
    "text": "curious if presenter's opinion on simpler Event Services such as Amazon SNS how does one decide if you truly",
    "start": "2668500",
    "end": "2674800"
  },
  {
    "text": "need something as powerful as Kafka or perhaps combination of SNS and sqs for added message durability",
    "start": "2674800",
    "end": "2681339"
  },
  {
    "text": "could they be enough for a use case yeah uh I think SNS as a QRS have different",
    "start": "2681339",
    "end": "2686859"
  },
  {
    "text": "kind of use cases they are more like point to point uh messaging system but",
    "start": "2686859",
    "end": "2692380"
  },
  {
    "text": "while the but kafta on the other side it's really have emphasized on streaming for example uh in Kafka you always",
    "start": "2692380",
    "end": "2700480"
  },
  {
    "text": "consume a batch of messages and your offset commit is is on the basal batch",
    "start": "2700480",
    "end": "2705640"
  },
  {
    "text": "not individual uh messages and Kafka has another great Advantage where you can",
    "start": "2705640",
    "end": "2711099"
  },
  {
    "text": "have different consumers uh consumed from the same queue or or same topic and",
    "start": "2711099",
    "end": "2716740"
  },
  {
    "text": "these and it would not affect each other so data fan out is uh is a very good use",
    "start": "2716740",
    "end": "2722859"
  },
  {
    "text": "case of of Kafka if you need but I think data fan out is a little bit difficult",
    "start": "2722859",
    "end": "2727900"
  },
  {
    "text": "to do with with SNS and sqs I've used SNS sqs together",
    "start": "2727900",
    "end": "2734560"
  },
  {
    "text": "um I mean it's basically you combine sns's topic based but if you if a",
    "start": "2734560",
    "end": "2740079"
  },
  {
    "text": "consumer joins late they don't get historical data then you you basically",
    "start": "2740079",
    "end": "2745480"
  },
  {
    "text": "have like multiple consumer groups as sqs topics and then each consumer group has to like",
    "start": "2745480",
    "end": "2751359"
  },
  {
    "text": "link to that sqs topic but the benefit I mean the what it what it didn't work for for me is Kafka when the producer writes",
    "start": "2751359",
    "end": "2758680"
  },
  {
    "text": "once it's durable it doesn't matter how many consumer groups you have later right right different apps but with SNS sqs you have",
    "start": "2758680",
    "end": "2766300"
  },
  {
    "text": "you end up adding the sqs topic or the sqs queue as another for another",
    "start": "2766300",
    "end": "2771640"
  },
  {
    "text": "consumer group but there's no historical data I think that's the difference Kafka supports that SNS sqs doesn't actually",
    "start": "2771640",
    "end": "2778780"
  },
  {
    "text": "solve that problem um are there any plans in exploring Pulsar to replace Kafka what are your",
    "start": "2778780",
    "end": "2785079"
  },
  {
    "text": "thoughts on Pulsar if you already explored it yeah so we actually export Pulsa in my previous job at Netflix",
    "start": "2785079",
    "end": "2792880"
  },
  {
    "text": "um at that time also was not super mature uh it it still has some stability",
    "start": "2792880",
    "end": "2798579"
  },
  {
    "text": "issues um but um the",
    "start": "2798579",
    "end": "2803680"
  },
  {
    "text": "but in the end I think uh Kafka is uh is",
    "start": "2803680",
    "end": "2808720"
  },
  {
    "text": "still simpler to use I think Paul says try to do a lot of things it's trying to accomplish both uh you know supporting",
    "start": "2808720",
    "end": "2815740"
  },
  {
    "text": "stream processing and also supporting this you know point to point-to-point uh",
    "start": "2815740",
    "end": "2821079"
  },
  {
    "text": "messaging you try to accommodate the boat and um it becomes a little bit more",
    "start": "2821079",
    "end": "2826540"
  },
  {
    "text": "complicated uh a complex uh architecture uh than uh than Kafka for example I",
    "start": "2826540",
    "end": "2833800"
  },
  {
    "text": "think Paul said relies on another service or or open source framework called uh bookkeeper right Apache",
    "start": "2833800",
    "end": "2840460"
  },
  {
    "text": "bookkeepers yeah so it there's a multiple layers of of services in between",
    "start": "2840460",
    "end": "2846700"
  },
  {
    "text": "um and it you know deploy I we think that deployment um you know installation operation could",
    "start": "2846700",
    "end": "2852579"
  },
  {
    "text": "be a headache so we so we choose Kafka",
    "start": "2852579",
    "end": "2857619"
  },
  {
    "text": "we want to stick with Kafka for its you know simplicity okay",
    "start": "2857619",
    "end": "2862780"
  },
  {
    "text": "um someone asked I don't know if you answered this one about they were using um uh spring boot with abro",
    "start": "2862780",
    "end": "2869380"
  },
  {
    "text": "ad yeah I think I answered one about it yeah",
    "start": "2869380",
    "end": "2876339"
  },
  {
    "text": "okay okay so you have the option of doing build time versus directly hooked",
    "start": "2877260",
    "end": "2883480"
  },
  {
    "text": "in at I think that option is only a published type like producer time right",
    "start": "2883480",
    "end": "2888880"
  },
  {
    "text": "got it that's correct um I think we're almost gonna have maybe one or two questions left were there",
    "start": "2888880",
    "end": "2895240"
  },
  {
    "text": "anything you wanted to share if someone if people had questions for you how would they reach you is Twitter a good",
    "start": "2895240",
    "end": "2900880"
  },
  {
    "text": "option yeah calendar uh LinkedIn um does email we can do all we can do",
    "start": "2900880",
    "end": "2908140"
  },
  {
    "text": "all of those thank you well thank you very much for your time Alan uh lots of amazing questions lots of interest in",
    "start": "2908140",
    "end": "2914380"
  },
  {
    "text": "your talk excellent talk I loved it the second time I saw it first time was it you can SF really appreciate it thank",
    "start": "2914380",
    "end": "2920380"
  },
  {
    "text": "you for your time thank you sis",
    "start": "2920380",
    "end": "2924300"
  },
  {
    "text": "[Music]",
    "start": "2926740",
    "end": "2932189"
  }
]