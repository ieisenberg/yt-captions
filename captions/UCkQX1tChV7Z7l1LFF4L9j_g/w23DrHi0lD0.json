[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "thanks for coming everybody my name is Kevin Kemper I worked for consistent",
    "start": "9380",
    "end": "15089"
  },
  {
    "text": "state and today we're going to talk about order PG I'll do a little bit of",
    "start": "15089",
    "end": "20640"
  },
  {
    "text": "an overview we'll talk about where you can download it there are some prerequisites we'll go through a high",
    "start": "20640",
    "end": "27269"
  },
  {
    "text": "level look at the install we'll spend most of our time looking at the order PG",
    "start": "27269",
    "end": "33059"
  },
  {
    "text": "config file and then I'll throw out some some ways that we've used it in the past",
    "start": "33059",
    "end": "38610"
  },
  {
    "text": "so order PG is based on Pearl DB I underneath the covers you need a working",
    "start": "38610",
    "end": "46379"
  },
  {
    "start": "39000",
    "end": "152000"
  },
  {
    "text": "connection obviously to both Postgres and Oracle you can actually run it in a",
    "start": "46379",
    "end": "54449"
  },
  {
    "text": "way where you only use the DVI to connect Oracle and then it dumps flat files for you and you use P SQL or",
    "start": "54449",
    "end": "61079"
  },
  {
    "text": "what-have-you to load your data into Postgres order PG will dump the full",
    "start": "61079",
    "end": "68070"
  },
  {
    "text": "database schema you can identify particular schemas or you can do them all from Oracle it will dump tables",
    "start": "68070",
    "end": "73860"
  },
  {
    "text": "views sequences indexes it'll even do stored procedures and functions however",
    "start": "73860",
    "end": "81950"
  },
  {
    "text": "the stored procedure and function migration is a to use their words a",
    "start": "81950",
    "end": "89189"
  },
  {
    "text": "work-in-progress so it's not perfect but it's actually not bad it's it's pretty",
    "start": "89189",
    "end": "94920"
  },
  {
    "text": "good we've used it a couple times and it gets you 80 to 90% there in fact the",
    "start": "94920",
    "end": "100530"
  },
  {
    "text": "last time we used it we had to make one change to the code and it was across the",
    "start": "100530",
    "end": "107100"
  },
  {
    "text": "board but it was in the way that the functions called other functions we had to make a single change and everything",
    "start": "107100",
    "end": "113070"
  },
  {
    "text": "works out of the box which was nice likewise you can export the full data",
    "start": "113070",
    "end": "119939"
  },
  {
    "text": "sense for the tables you specify you can specify individual tables you can specify a where clause and only dump",
    "start": "119939",
    "end": "127200"
  },
  {
    "text": "part of the tables on and on and on there's this particular tool has more options than",
    "start": "127200",
    "end": "134610"
  },
  {
    "text": "then you could believe we we probably use 10% of the options at best most of",
    "start": "134610",
    "end": "141180"
  },
  {
    "text": "it a default values works really well we haven't tested a lot of the you know you",
    "start": "141180",
    "end": "146910"
  },
  {
    "text": "can do this and that in certain scenarios but in general it's it's pretty solid tool so if you wanted to",
    "start": "146910",
    "end": "154290"
  },
  {
    "start": "152000",
    "end": "182000"
  },
  {
    "text": "download it you can get it from SourceForge likewise if you go to the",
    "start": "154290",
    "end": "160830"
  },
  {
    "text": "second link here the hora de 2 PG dot Darryl D net that's basically the order",
    "start": "160830",
    "end": "167550"
  },
  {
    "text": "P G homepage and there's a lot of good information on that page it talks that the documentation is out there the",
    "start": "167550",
    "end": "173630"
  },
  {
    "text": "general approach to usage is out there and obviously the links to to jump over",
    "start": "173630",
    "end": "179340"
  },
  {
    "text": "to SourceForge and download it are out there in terms of prerequisites you have",
    "start": "179340",
    "end": "184500"
  },
  {
    "start": "182000",
    "end": "315000"
  },
  {
    "text": "you need a valid Postgres install a valid Oracle install and at least",
    "start": "184500",
    "end": "190500"
  },
  {
    "text": "version 5-6 of Perl likewise just as a",
    "start": "190500",
    "end": "196410"
  },
  {
    "text": "side note the oracle install and the Postgres install do not have to live on",
    "start": "196410",
    "end": "201990"
  },
  {
    "text": "the same boxes where you install order PG so you can have Oracle on box a and",
    "start": "201990",
    "end": "207750"
  },
  {
    "text": "Postgres on box B and you can even put order PG on say box T and use it to do",
    "start": "207750",
    "end": "214350"
  },
  {
    "text": "all your conversions from the same box which is the last customer we helped",
    "start": "214350",
    "end": "220290"
  },
  {
    "text": "with this that's exactly their plan because they've got 70 some database servers",
    "start": "220290",
    "end": "225930"
  },
  {
    "text": "they're going to migrate and they don't want to go through the the process of installing or 2 PG and getting it",
    "start": "225930",
    "end": "232260"
  },
  {
    "text": "working on every single box do it once and do the migrations from there",
    "start": "232260",
    "end": "237270"
  },
  {
    "text": "likewise perl dbi needs to be installed and working the DVD Oracle package needs",
    "start": "237270",
    "end": "243360"
  },
  {
    "text": "to be installed and working the DVD Oracle package is a little tricky",
    "start": "243360",
    "end": "250530"
  },
  {
    "text": "meaning it generally works but you have to fiddle with the the parameters a",
    "start": "250530",
    "end": "256650"
  },
  {
    "text": "little bit so making sure that the obvious of the Oracle home is that in the Oracle said it's at and so forth all",
    "start": "256650",
    "end": "262530"
  },
  {
    "text": "the main Oracle Prem there's on top of that you need you have to point it to the Oracle client",
    "start": "262530",
    "end": "269260"
  },
  {
    "text": "libraries you cannot point it to the Oracle the server-side libraries plus",
    "start": "269260",
    "end": "275830"
  },
  {
    "text": "you probably will have to change the permissions on the Oracle client libraries so if we in our case we were",
    "start": "275830",
    "end": "283090"
  },
  {
    "text": "running the function or running the the tool as the Postgres user on the server",
    "start": "283090",
    "end": "288490"
  },
  {
    "text": "and of course the Postgres user doesn't even have read access to the Oracle",
    "start": "288490",
    "end": "293520"
  },
  {
    "text": "client library directory because those are owned by Oracle so we had that we",
    "start": "293520",
    "end": "299170"
  },
  {
    "text": "had to do some permission resets not a big deal we did a change mod and added",
    "start": "299170",
    "end": "304960"
  },
  {
    "text": "our forever just read for everybody from the Oracle client library down and",
    "start": "304960",
    "end": "310500"
  },
  {
    "text": "everything worked from that point forward in terms of installation that",
    "start": "310500",
    "end": "317080"
  },
  {
    "start": "315000",
    "end": "403000"
  },
  {
    "text": "the download is a tarball so uncompress it you CD into that",
    "start": "317080",
    "end": "323260"
  },
  {
    "text": "directory and like any any other perl module you execute the capital m-make",
    "start": "323260",
    "end": "330340"
  },
  {
    "text": "file dot PL as as perl and then run your make and your make install there is a",
    "start": "330340",
    "end": "337120"
  },
  {
    "text": "make test that make test is pretty involved and it's fairly difficult to get it to be successful all the way",
    "start": "337120",
    "end": "343900"
  },
  {
    "text": "through because of all the different environment variables you need to set for the tasks so we actually skipped the",
    "start": "343900",
    "end": "351190"
  },
  {
    "text": "make tasks we did the make and make install and wrote a quick little test script to make sure that that the DVD",
    "start": "351190",
    "end": "360580"
  },
  {
    "text": "oracle was working to connect to Oracle and that DB d PG was working to connect to Postgres and we were good to go after",
    "start": "360580",
    "end": "366520"
  },
  {
    "text": "that so likewise once you do the install it's actually going to give you an order",
    "start": "366520",
    "end": "372460"
  },
  {
    "text": "PG perl module in your your perl site repository in user local bin it's going",
    "start": "372460",
    "end": "380110"
  },
  {
    "text": "to give you to order and Ora to PG binary and it it creates a Etsy",
    "start": "380110",
    "end": "387030"
  },
  {
    "text": "directory in the directory you uncompress your tarball and it puts an",
    "start": "387030",
    "end": "392950"
  },
  {
    "text": "order PG config file in there the order PG config file can live anywhere because",
    "start": "392950",
    "end": "398710"
  },
  {
    "text": "you use you specify it on the command line when you execute the tool so the",
    "start": "398710",
    "end": "406419"
  },
  {
    "text": "config file itself is significant in size we'll go through some of the key",
    "start": "406419",
    "end": "411760"
  },
  {
    "text": "variables but I haven't covered all of them so the one of the most obvious sections is the oracle variables we need",
    "start": "411760",
    "end": "420010"
  },
  {
    "text": "oracle home we need the oracle DSN the username and the password the oracle DSN",
    "start": "420010",
    "end": "427330"
  },
  {
    "text": "if you specify it like we have here DB i : oracle host equals host cid equals sid",
    "start": "427330",
    "end": "435550"
  },
  {
    "text": "name that works about 50 percent of the time it depends this is completely",
    "start": "435550",
    "end": "441280"
  },
  {
    "text": "dependent on how they've configured Oracle in the Oracle side so for example",
    "start": "441280",
    "end": "446289"
  },
  {
    "text": "in a number of the servers we converted we had to specify it as leave off the",
    "start": "446289",
    "end": "453669"
  },
  {
    "text": "host and specify the SID as host name at sid dot schema sign it sort of thing",
    "start": "453669",
    "end": "461860"
  },
  {
    "text": "most oracle DBA s are pretty familiar with the different syntaxes for connections the different syntax you",
    "start": "461860",
    "end": "468669"
  },
  {
    "text": "would use for a DSN an oracle DSN connection string and if if the Oracle",
    "start": "468669",
    "end": "475330"
  },
  {
    "text": "DBA s that you're working with assuming there are some if they can give you a",
    "start": "475330",
    "end": "480550"
  },
  {
    "text": "command line version of how to connect to the Oracle database you can basically plug that in as your connection string",
    "start": "480550",
    "end": "486610"
  },
  {
    "text": "and it'll work obviously the Oracle user and password there is a user grants",
    "start": "486610",
    "end": "493229"
  },
  {
    "text": "variable by default it's set to false if you set it to true it tells order PG that you're not",
    "start": "493229",
    "end": "501669"
  },
  {
    "text": "connecting as as sis DBA or somebody with the equivalent of super user which",
    "start": "501669",
    "end": "508330"
  },
  {
    "text": "means that it'll hit different system catalogs to try to pull the list of of",
    "start": "508330",
    "end": "513630"
  },
  {
    "text": "information that it needs to build to generate the DDL for you and to extract the data so we obviously ran as an",
    "start": "513630",
    "end": "522729"
  },
  {
    "text": "Oracle super user as this DBA so we didn't have to worry about it but in the case that you don't have that access you",
    "start": "522729",
    "end": "529340"
  },
  {
    "text": "can still leverage this user grants variable set it to true and then you can",
    "start": "529340",
    "end": "535640"
  },
  {
    "text": "log in to Oracle as an enough as an authenticated normal user as opposed to",
    "start": "535640",
    "end": "542810"
  },
  {
    "text": "like sis off or sis TVA and it will still work for most functions the areas",
    "start": "542810",
    "end": "550040"
  },
  {
    "text": "where you're going to run into trouble is if you actually want to pull some of the system catalogs which in our case",
    "start": "550040",
    "end": "555890"
  },
  {
    "text": "with one client in particular that was pretty important to him not sure why but they wanted to pull the system catalog",
    "start": "555890",
    "end": "562250"
  },
  {
    "text": "data as well export schema is again it's off by",
    "start": "562250",
    "end": "568730"
  },
  {
    "text": "default which basically says export the create schema command there's a schema",
    "start": "568730",
    "end": "577810"
  },
  {
    "text": "schema name variable schema equals blah and if you set that to a schema name or",
    "start": "577810",
    "end": "584380"
  },
  {
    "text": "a number of schemas separated by spaces those are the schemas that are going to",
    "start": "584380",
    "end": "590270"
  },
  {
    "text": "be exported when you run the order PG tool likewise there's a Postgres search",
    "start": "590270",
    "end": "596120"
  },
  {
    "text": "path that you can set i we did not need to tweak this everything we pushed when",
    "start": "596120",
    "end": "601580"
  },
  {
    "text": "either into the schema the originating schema which already had the path or",
    "start": "601580",
    "end": "607750"
  },
  {
    "text": "tables went into the public schema which is default likewise this this type",
    "start": "607750",
    "end": "615830"
  },
  {
    "start": "613000",
    "end": "714000"
  },
  {
    "text": "variable is probably the the primary variable you would tweak so order PG",
    "start": "615830",
    "end": "621110"
  },
  {
    "text": "works by it looks at this type variable and based on what you have in here that's sort of the the operation that",
    "start": "621110",
    "end": "628220"
  },
  {
    "text": "it's going to perform for example if we have table as our type variable it's",
    "start": "628220",
    "end": "635000"
  },
  {
    "text": "going to effectively dump DDL it'll dump all the create table statements likewise",
    "start": "635000",
    "end": "640760"
  },
  {
    "text": "it'll dump the alter table statements but it won't dump the indexes there's a separate actually my bad it does dump",
    "start": "640760",
    "end": "649040"
  },
  {
    "text": "indexes likewise if you set it to data it will dump the data but it'll dump as",
    "start": "649040",
    "end": "655160"
  },
  {
    "text": "it as insert statements if you set it to copy it'll dump the data as copy statements",
    "start": "655160",
    "end": "661240"
  },
  {
    "text": "so on and so forth you can dump views when you do grant it dumps all the",
    "start": "661240",
    "end": "666740"
  },
  {
    "text": "permissions sequences triggers functions so on and so forth",
    "start": "666740",
    "end": "673000"
  },
  {
    "text": "likewise there's a variable called tables by default it's commented out if",
    "start": "673000",
    "end": "679579"
  },
  {
    "text": "you don't specify anything it dumps all the tables in the specified schema or schemas or if you're dumping data it",
    "start": "679579",
    "end": "688459"
  },
  {
    "text": "will dump data for all the tables in that schema or schemas however if you put a table here or a list of tables",
    "start": "688459",
    "end": "695180"
  },
  {
    "text": "separated by spaces those are the only tables that order PG will dump",
    "start": "695180",
    "end": "701440"
  },
  {
    "text": "there's annex the opposite of that is the exclude variable so you can say I want to dump everything except for the",
    "start": "701440",
    "end": "709040"
  },
  {
    "text": "list of tables that I give it in the exclude variable there's a a skip",
    "start": "709040",
    "end": "716420"
  },
  {
    "start": "714000",
    "end": "853000"
  },
  {
    "text": "function or variable that basically says I want to skip foreign keys primary keys",
    "start": "716420",
    "end": "722019"
  },
  {
    "text": "unique keys so on and so forth all this will do is say effectively don't don't",
    "start": "722019",
    "end": "729199"
  },
  {
    "text": "dump these constructs for me dump everything else but what I specify the",
    "start": "729199",
    "end": "734990"
  },
  {
    "text": "data limit is by default set to 10,000 is that ten thousand a hundred thousand",
    "start": "734990",
    "end": "741010"
  },
  {
    "text": "which means that the each copy statement is going to dump a hundred thousand rows",
    "start": "741010",
    "end": "746870"
  },
  {
    "text": "and then it will add multiple copy statements to the file you can tweak",
    "start": "746870",
    "end": "752060"
  },
  {
    "text": "this obviously just be careful about tweaking it too high because you would",
    "start": "752060",
    "end": "757940"
  },
  {
    "text": "really want to force your machine to get into a swapping scenario it's not so much of an issue if you dump to a flat",
    "start": "757940",
    "end": "763910"
  },
  {
    "text": "file and then you want to push that over to say a Postgres database server that's",
    "start": "763910",
    "end": "769040"
  },
  {
    "text": "dedicated box lots of RAM and run your copy statement from the command line",
    "start": "769040",
    "end": "774290"
  },
  {
    "text": "it's more of an issue if you have order PG connect directly both to Oracle and",
    "start": "774290",
    "end": "780079"
  },
  {
    "text": "Postgres and do a memory to memory migration if you out",
    "start": "780079",
    "end": "785860"
  },
  {
    "text": "there's a modified struct parameter basically this one says if I specify a",
    "start": "787070",
    "end": "794360"
  },
  {
    "text": "table I can give it a list of columns and those are the only columns for that",
    "start": "794360",
    "end": "800029"
  },
  {
    "text": "table that it will dump so if you've got a table with 47 fields but for the",
    "start": "800029",
    "end": "805339"
  },
  {
    "text": "migration you only care about two of them you don't have to dump them all you can just dump the two columns you care",
    "start": "805339",
    "end": "810829"
  },
  {
    "text": "about there's a replace tables this one it's interesting",
    "start": "810829",
    "end": "817360"
  },
  {
    "text": "thus I kept it in the presentation but we never actually used it you can",
    "start": "817360",
    "end": "823880"
  },
  {
    "text": "specify the original table name and a new table name effectively all it does is a rename for you I can see where this",
    "start": "823880",
    "end": "831199"
  },
  {
    "text": "would be valuable but again we didn't use it replace columns does the same thing you",
    "start": "831199",
    "end": "836540"
  },
  {
    "text": "specify for a table you specify in the per in the parenthesis a series of",
    "start": "836540",
    "end": "845019"
  },
  {
    "text": "original name : new name original name : new name and it'll rename the columns",
    "start": "845019",
    "end": "850130"
  },
  {
    "text": "within the table for you if you specify the Postgres connection parameters then",
    "start": "850130",
    "end": "857329"
  },
  {
    "start": "853000",
    "end": "1005000"
  },
  {
    "text": "order PG by nature well go ahead and connect to Oracle and pull the data and",
    "start": "857329",
    "end": "863779"
  },
  {
    "text": "automatically connect to Postgres memory to memory and push it straight into your Postgres database if these are not",
    "start": "863779",
    "end": "870769"
  },
  {
    "text": "specified or if they're commented out the default behavior is for order PG to",
    "start": "870769",
    "end": "875839"
  },
  {
    "text": "dump two flat files or a flat file and you can control we'll get into that in a",
    "start": "875839",
    "end": "881540"
  },
  {
    "text": "few slides here whether it dumps to a single file or dumps multiple files for",
    "start": "881540",
    "end": "887990"
  },
  {
    "text": "you the PG DSN PG user PG password",
    "start": "887990",
    "end": "893889"
  },
  {
    "text": "standard pearl dbi connection strings likewise you can have a ware parameter",
    "start": "893889",
    "end": "901370"
  },
  {
    "text": "so in the examples we have here the top one where one equals one or if we put",
    "start": "901370",
    "end": "906949"
  },
  {
    "text": "some of the where clause in there we could specify a where clause that applied to all tables or you could say",
    "start": "906949",
    "end": "913370"
  },
  {
    "text": "where and give it a table name and then in in square brackets give the actual where",
    "start": "913370",
    "end": "919779"
  },
  {
    "text": "clause and you can have multiple table names and where clauses separated by spaces so it'll apply those where",
    "start": "919779",
    "end": "927459"
  },
  {
    "text": "clauses only to those individual tables there's an output parameter you can set",
    "start": "927459",
    "end": "935769"
  },
  {
    "text": "the by default the output is set to outputs equal and that's the output file",
    "start": "935769",
    "end": "941620"
  },
  {
    "text": "where it's going to want to dump all its data if you don't have the Postgres connection string variables set if you",
    "start": "941620",
    "end": "948399"
  },
  {
    "text": "set it to outputs equal dot GZ it'll automatically do a tart GZ for I'm sorry",
    "start": "948399",
    "end": "954790"
  },
  {
    "text": "it'll do a gzip and likewise if you set it to be z2 it'll do a be unzip however",
    "start": "954790",
    "end": "961029"
  },
  {
    "text": "you have to make sure that you have the Perl compression the B zip to",
    "start": "961029",
    "end": "969029"
  },
  {
    "text": "compression package installed by default Perl knows how to do gzip but it doesn't",
    "start": "969029",
    "end": "975009"
  },
  {
    "text": "know how to do B be zip - if you decide to specify B zip - then you also need to",
    "start": "975009",
    "end": "983319"
  },
  {
    "text": "specify the path to where B sub 2 lives and the B zip parameter likewise the",
    "start": "983319",
    "end": "991149"
  },
  {
    "text": "output directory by default is going to be your local directory wherever you run it from however if you uncomment the",
    "start": "991149",
    "end": "998380"
  },
  {
    "text": "output door and set it to some directory that's where all your output files will go there's a number of parameters that",
    "start": "998380",
    "end": "1007589"
  },
  {
    "start": "1005000",
    "end": "1240000"
  },
  {
    "text": "allow or 2pg to try to defer constraint checking so there's an F key deferrable",
    "start": "1007589",
    "end": "1014279"
  },
  {
    "text": "which will defer all foreign key constraint checking and there's a defer",
    "start": "1014279",
    "end": "1019980"
  },
  {
    "text": "F key option that defers all foreign key constraints during a data dump I'm not",
    "start": "1019980",
    "end": "1028020"
  },
  {
    "text": "exactly sure what the difference is but it's nice that they have the ability to",
    "start": "1028020",
    "end": "1034470"
  },
  {
    "text": "defer the constraints because if not you get into some odd situations likewise",
    "start": "1034470",
    "end": "1041548"
  },
  {
    "text": "and we'll cover this a little bit later - there's ways that you can tell it to separate everything what we did just to",
    "start": "1041549",
    "end": "1048809"
  },
  {
    "text": "jump ahead a little is be dumped all our DeeDee Alda desk and we dumped our indexes and our",
    "start": "1048809",
    "end": "1054690"
  },
  {
    "text": "constraints into separate files then we did all the create table statements and we would do a manual review of the DDL",
    "start": "1054690",
    "end": "1063240"
  },
  {
    "text": "to make sure it looked okay and then we run it against the target database and then when we did the data migration we",
    "start": "1063240",
    "end": "1069059"
  },
  {
    "text": "would do a direct memory to memory straight from Oracle into Postgres and that seemed to work pretty well and it",
    "start": "1069059",
    "end": "1077610"
  },
  {
    "text": "to be honest gave the client a really high level of confidence that you know",
    "start": "1077610",
    "end": "1083460"
  },
  {
    "text": "they have the ability to take a look at the DDL and make sure that everything looks kosher there's a drop F key and a",
    "start": "1083460",
    "end": "1091770"
  },
  {
    "text": "drop indexes parameter that says drop all the foreign keys before the data import starts and then replace them",
    "start": "1091770",
    "end": "1100110"
  },
  {
    "text": "after the import is done and likewise for the indexes drop all the indexes when you're doing a copy or a data",
    "start": "1100110",
    "end": "1107539"
  },
  {
    "text": "migration and then replace them after it's finished which pretty powerful parameters because of course we can get",
    "start": "1107539",
    "end": "1114480"
  },
  {
    "text": "better performance by dropping all the constraints and indexes shoving the data into the database and then recreate in",
    "start": "1114480",
    "end": "1121470"
  },
  {
    "text": "all the indexes there's a PG numeric type variable by default it's set to 1",
    "start": "1121470",
    "end": "1128279"
  },
  {
    "text": "which says it'll replace the the numeric Oracle type with either a big int or an",
    "start": "1128279",
    "end": "1134909"
  },
  {
    "text": "integer depending on how big the number actually is if in there are certain",
    "start": "1134909",
    "end": "1141450"
  },
  {
    "text": "cases where you want to set that to 0 but in most cases if it's strictly",
    "start": "1141450",
    "end": "1146820"
  },
  {
    "text": "numbers then it's a good idea just to let Postgres change it to an integer or to a big int we found that in 99% of the",
    "start": "1146820",
    "end": "1156090"
  },
  {
    "text": "cases there I think there were three instances out of about 1,500 tables where we had to come back and modify",
    "start": "1156090",
    "end": "1163010"
  },
  {
    "text": "what order PG had selected based on that criteria in our case there were a couple",
    "start": "1163010",
    "end": "1169049"
  },
  {
    "text": "of numbers that should have been begins that it actually converted to integers",
    "start": "1169049",
    "end": "1174179"
  },
  {
    "text": "we ran into some issues with that",
    "start": "1174179",
    "end": "1177740"
  },
  {
    "text": "this parameter the datatype parameter is basically a way for you to specify the",
    "start": "1182539",
    "end": "1190080"
  },
  {
    "text": "mapping yourself instead of Latin or to PG do the mapping for you so you can say",
    "start": "1190080",
    "end": "1195270"
  },
  {
    "text": "I want a date to be specifically a timestamp or a timestamp without timezone or whatever you want it to look",
    "start": "1195270",
    "end": "1201750"
  },
  {
    "text": "like I want along to come across as a text and so on and so forth so if you",
    "start": "1201750",
    "end": "1207030"
  },
  {
    "text": "specify the data type variable you can specify the mapping yourself one note is",
    "start": "1207030",
    "end": "1212909"
  },
  {
    "text": "that mapping will be applied to all tables you can't specify just a",
    "start": "1212909",
    "end": "1217919"
  },
  {
    "text": "particular table unless you go into the tables variable and set it just for a",
    "start": "1217919",
    "end": "1223200"
  },
  {
    "text": "couple of tables and do a run and then go back in and pull those tables out and",
    "start": "1223200",
    "end": "1228720"
  },
  {
    "text": "put them in the exclude tables list and then do another run with one with the",
    "start": "1228720",
    "end": "1234539"
  },
  {
    "text": "mapping on your own and one with the order PG mapping there you can set the",
    "start": "1234539",
    "end": "1241590"
  },
  {
    "start": "1240000",
    "end": "1518000"
  },
  {
    "text": "the default language likewise there you",
    "start": "1241590",
    "end": "1246840"
  },
  {
    "text": "can enable the PL PG seek welcome conversion convention in this case we",
    "start": "1246840",
    "end": "1256950"
  },
  {
    "text": "left it at the default and we didn't have any issues with it but in again if",
    "start": "1256950",
    "end": "1263970"
  },
  {
    "text": "you look into the documentation it talks about if you're having issues and your your your data migration flat isn't",
    "start": "1263970",
    "end": "1271200"
  },
  {
    "text": "working right and your number running into a number of problems where the the data won't load on the Postgres side",
    "start": "1271200",
    "end": "1276990"
  },
  {
    "text": "then this is one of the options to try is turn this off by default this is trying to make sure that the the oracle",
    "start": "1276990",
    "end": "1284220"
  },
  {
    "text": "dumps become as compatible as possible with not only Postgres but with PL PG",
    "start": "1284220",
    "end": "1289950"
  },
  {
    "text": "sequel in terms of the stored procedures in particular there's a files per",
    "start": "1289950",
    "end": "1298590"
  },
  {
    "text": "constraint and files per index variable which says by default they're 0 so if we",
    "start": "1298590",
    "end": "1304559"
  },
  {
    "text": "were going to do a dump and we're going to dump the flat files everything would obviously go to the output sequel file",
    "start": "1304559",
    "end": "1310440"
  },
  {
    "text": "however if I said file our index and set that to true then",
    "start": "1310440",
    "end": "1315600"
  },
  {
    "text": "every individual index I'm sorry every individual table would get a new file with all of its indexes",
    "start": "1315600",
    "end": "1323190"
  },
  {
    "text": "in that file and likewise if you set the file per constraint for any constraint",
    "start": "1323190",
    "end": "1328620"
  },
  {
    "text": "so effectively you're going to get a table and all of its alter statements in",
    "start": "1328620",
    "end": "1334260"
  },
  {
    "text": "a single file and then all of its indexes in a single file so you'll have a propagation of output dots equal files",
    "start": "1334260",
    "end": "1344659"
  },
  {
    "text": "named per table name but it is beneficial in terms of it gives you the",
    "start": "1344659",
    "end": "1350700"
  },
  {
    "text": "ability to have a lot of control over how you do the migration and what order",
    "start": "1350700",
    "end": "1356940"
  },
  {
    "text": "you do the migration and so on and so forth we found that the tool works really well",
    "start": "1356940",
    "end": "1362340"
  },
  {
    "text": "but it's not necessarily a good idea to point or to PG at an Oracle database and",
    "start": "1362340",
    "end": "1368250"
  },
  {
    "text": "just say convert the whole thing top to bottom because there are small hiccups along the way sometimes you need to",
    "start": "1368250",
    "end": "1375059"
  },
  {
    "text": "tweak the DD all a little bit and the other thing we ran into is if we tried",
    "start": "1375059",
    "end": "1381929"
  },
  {
    "text": "even just for the data migration if we said do copy statements 100,000 rows at",
    "start": "1381929",
    "end": "1387270"
  },
  {
    "text": "a time and do the entire database table wise top to bottom that we ran into",
    "start": "1387270",
    "end": "1392850"
  },
  {
    "text": "memory issues so we and we'll get into this in a couple slides here we actually",
    "start": "1392850",
    "end": "1398309"
  },
  {
    "text": "set up a script and we would pull the list of tables from Oracle and we would",
    "start": "1398309",
    "end": "1403320"
  },
  {
    "text": "loop through all those tables and we would effectively run the data migration one table at a time",
    "start": "1403320",
    "end": "1409710"
  },
  {
    "text": "it was automated so it was still a single function that the user had to run however it was much more effective",
    "start": "1409710",
    "end": "1417330"
  },
  {
    "text": "because it would run a table and then basically exit the loop and come back and run another table and we all the",
    "start": "1417330",
    "end": "1424890"
  },
  {
    "text": "memory issues went away there's a file per table which again I like the other",
    "start": "1424890",
    "end": "1431010"
  },
  {
    "text": "parameters we just talked about the file per constraint and file per index this one would say when we're dumping either",
    "start": "1431010",
    "end": "1437340"
  },
  {
    "text": "data or when we're dumping DDL we will get a single file per table likewise",
    "start": "1437340",
    "end": "1443190"
  },
  {
    "text": "there's a file per function which same thing every function if",
    "start": "1443190",
    "end": "1448320"
  },
  {
    "text": "you're going to try to migrate stored procedures and functions will go into a separate file we dumped all our DDL in",
    "start": "1448320",
    "end": "1455660"
  },
  {
    "text": "terms of functions and tables into single files so we dumped all the functions into one file we dumped all",
    "start": "1455660",
    "end": "1462030"
  },
  {
    "text": "the tables into a single file but we dumped our our constraints and our indexes into a separate file so we could",
    "start": "1462030",
    "end": "1470100"
  },
  {
    "text": "sort of manually control dumping the DDL without any indexes or constraints do a",
    "start": "1470100",
    "end": "1476310"
  },
  {
    "text": "review and apply that and then migrate the data and then come back after we've done a review and apply all the",
    "start": "1476310",
    "end": "1482010"
  },
  {
    "text": "constraints and indexes and it works pretty well truncate table is just that",
    "start": "1482010",
    "end": "1487560"
  },
  {
    "text": "it will truncate all the tables before the data migration you can set the",
    "start": "1487560",
    "end": "1493290"
  },
  {
    "text": "client encoding if you're having issues which we actually set this because the",
    "start": "1493290",
    "end": "1498720"
  },
  {
    "text": "last client we did this for was using some off-the-wall encoding that is",
    "start": "1498720",
    "end": "1504660"
  },
  {
    "text": "pretty non-standard so we had issues even though their database was set without encoding the migration was",
    "start": "1504660",
    "end": "1511800"
  },
  {
    "text": "trying to force it back to a Latin one and it just wasn't working out a couple",
    "start": "1511800",
    "end": "1518790"
  },
  {
    "start": "1518000",
    "end": "1951000"
  },
  {
    "text": "examples I'll walk through so again like I mentioned we would migrate the DDL first and we would do that to flat files",
    "start": "1518790",
    "end": "1525650"
  },
  {
    "text": "then we would migrate the data and then we would come back and migrate the",
    "start": "1525650",
    "end": "1531420"
  },
  {
    "text": "functions so in terms of just a walkthrough of what we did first we",
    "start": "1531420",
    "end": "1538440"
  },
  {
    "text": "wrote a script that would generate a table list for us and the script was nothing more than a wrapper script",
    "start": "1538440",
    "end": "1543600"
  },
  {
    "text": "around order PG so we would set that the oracle variables we set the schema variable we left the tables to blank and",
    "start": "1543600",
    "end": "1552300"
  },
  {
    "text": "we set the type variable to show tables which actually isn't in the list in the comments but if you read through the",
    "start": "1552300",
    "end": "1559200"
  },
  {
    "text": "documentation it talks about you can set it to show tables and it will show you just a list of tables likewise there's a",
    "start": "1559200",
    "end": "1567090"
  },
  {
    "text": "show schema variable or value that you can set it to which will simply dump a",
    "start": "1567090",
    "end": "1573150"
  },
  {
    "text": "list of all the schemas for the Oracle instance you're connecting to which is",
    "start": "1573150",
    "end": "1578620"
  },
  {
    "text": "whole nother kind of because the schema and Oracle isn't really a real schema as",
    "start": "1578620",
    "end": "1584080"
  },
  {
    "text": "far as this Postgres guys think however it will give you a list of names that",
    "start": "1584080",
    "end": "1590200"
  },
  {
    "text": "means something to the Oracle DBA s so in any case we had this rapper script and we would run it we would execute",
    "start": "1590200",
    "end": "1597580"
  },
  {
    "text": "order PG and give it the config file and basically redirect that to a file that",
    "start": "1597580",
    "end": "1602860"
  },
  {
    "text": "contained it contained only the list of tables and then we would migrate the DDL",
    "start": "1602860",
    "end": "1608590"
  },
  {
    "text": "based on that table list so effectively we would set all the oracle variables",
    "start": "1608590",
    "end": "1614850"
  },
  {
    "text": "set the schema would leave the tables blank and set the type to table and in",
    "start": "1614850",
    "end": "1620260"
  },
  {
    "text": "this case we would set the file per constraint and file per index both to one so what we would end up with is an",
    "start": "1620260",
    "end": "1627760"
  },
  {
    "text": "output sequel file with all the create table statements and then per table we",
    "start": "1627760",
    "end": "1633220"
  },
  {
    "text": "would end up with a file with all of the alter statements and a file with all the create index statements which made it",
    "start": "1633220",
    "end": "1640960"
  },
  {
    "text": "pretty easy to walk through do a quick review of the DDL make sure that it looks appropriate and then execute the",
    "start": "1640960",
    "end": "1648130"
  },
  {
    "text": "create table portion of it against the Postgres database so again we wrapped a",
    "start": "1648130",
    "end": "1653620"
  },
  {
    "text": "script around this it would execute order PG and it would give it the proper config file we would send the output to",
    "start": "1653620",
    "end": "1661750"
  },
  {
    "text": "a log in case there's errors and then at that point we would go in and manually",
    "start": "1661750",
    "end": "1666910"
  },
  {
    "text": "run the DDL against the Postgres database likewise and we didn't do this",
    "start": "1666910",
    "end": "1673510"
  },
  {
    "text": "we left the views and the grants and the sequences and so forth as part of the main DDL file but you can split all",
    "start": "1673510",
    "end": "1679600"
  },
  {
    "text": "those out as well and then we came back and we would migrate the data so again",
    "start": "1679600",
    "end": "1686140"
  },
  {
    "text": "we would go back to that original script we wrote to generate the table list and",
    "start": "1686140",
    "end": "1693040"
  },
  {
    "text": "we gave it the show tables and we redirected that in this case to a a file",
    "start": "1693040",
    "end": "1698200"
  },
  {
    "text": "name called tab last so after we applied the DDL to Postgres we would set in",
    "start": "1698200",
    "end": "1703990"
  },
  {
    "text": "addition to set in the oracle config settings and setting the schema in our",
    "start": "1703990",
    "end": "1709390"
  },
  {
    "text": "case we did have a particular schema we were leveraged and we would set the type to copy and we",
    "start": "1709390",
    "end": "1715820"
  },
  {
    "text": "would assign the proper variables to the Postgres connection settings what the",
    "start": "1715820",
    "end": "1722480"
  },
  {
    "text": "script would do is it would walk through the tab list file and it would effectively on the fly plugged the the",
    "start": "1722480",
    "end": "1729080"
  },
  {
    "text": "tables variable with the table name one at a time and then it would execute",
    "start": "1729080",
    "end": "1734659"
  },
  {
    "text": "order PG for us and in this case we effectively got to do the copy memory to memory we didn't have to dump the data",
    "start": "1734659",
    "end": "1741140"
  },
  {
    "text": "to disk but at the same time we're minimizing our use of memory if you well",
    "start": "1741140",
    "end": "1746659"
  },
  {
    "text": "and we found that by doing one table two time in this way we didn't run into",
    "start": "1746659",
    "end": "1751669"
  },
  {
    "text": "memory issues and at the same time the the old the DBAs that were going to take",
    "start": "1751669",
    "end": "1756950"
  },
  {
    "text": "this and migrate the rest of the servers they only had to run the one command they didn't have to go manually tweak",
    "start": "1756950",
    "end": "1763789"
  },
  {
    "text": "the config file and change the table name and manually execute it for all",
    "start": "1763789",
    "end": "1769340"
  },
  {
    "text": "1,500 tables in the database they just had to run the one script it would loop through the tables and migrate the data",
    "start": "1769340",
    "end": "1776659"
  },
  {
    "text": "one at a time and then likewise we would execute the shell script that would",
    "start": "1776659",
    "end": "1783140"
  },
  {
    "text": "migrate the the remaining DDL ie the",
    "start": "1783140",
    "end": "1788590"
  },
  {
    "text": "indexes the alter taemin's alter table statements and so forth and then as a",
    "start": "1788590",
    "end": "1794000"
  },
  {
    "text": "last step we came back and did the function so we went into the config file and created a separate config that had",
    "start": "1794000",
    "end": "1800809"
  },
  {
    "text": "the type set to function we set the file for function to one so we got a separate file for each function and it gave us an",
    "start": "1800809",
    "end": "1808520"
  },
  {
    "text": "opportunity to review those functions before we executed them against the Postgres database and we didn't dig too",
    "start": "1808520",
    "end": "1817940"
  },
  {
    "text": "much deeper than that however there are variables that allow us to dump",
    "start": "1817940",
    "end": "1823490"
  },
  {
    "text": "procedures and triggers separately we dumped the procedures and triggers as part of dumping the the initial DDL ie",
    "start": "1823490",
    "end": "1832610"
  },
  {
    "text": "we dumped the triggers as part of dumping the table statements or we appended it to that file and then we",
    "start": "1832610",
    "end": "1839480"
  },
  {
    "text": "dumped the procedures as part of when we were dumping the functions in order pg of course",
    "start": "1839480",
    "end": "1844610"
  },
  {
    "text": "converts those two functions because Postgres native generic Postgres of",
    "start": "1844610",
    "end": "1850640"
  },
  {
    "text": "course doesn't support procedures it supports functions in our case it worked",
    "start": "1850640",
    "end": "1856850"
  },
  {
    "text": "out great we there were no there are no particular issues in their application that",
    "start": "1856850",
    "end": "1863210"
  },
  {
    "text": "required the or that caused the fact that we no longer have a procedure or a",
    "start": "1863210",
    "end": "1870190"
  },
  {
    "text": "package we didn't have any issues with those now being functions not really",
    "start": "1870190",
    "end": "1876020"
  },
  {
    "text": "working for the app anymore I could see where there might be some scenarios where code would have to",
    "start": "1876020",
    "end": "1881840"
  },
  {
    "text": "change if you are leveraging the true Oracle package syntax in this shop in",
    "start": "1881840",
    "end": "1888380"
  },
  {
    "text": "particular impact both times we've used this we didn't run into that so apparently I don't know how widespread",
    "start": "1888380",
    "end": "1895880"
  },
  {
    "text": "the use of that syntax is and the it apparently it doesn't seem to be all",
    "start": "1895880",
    "end": "1901190"
  },
  {
    "text": "that wide so if if the call of the procedures and the packages and the",
    "start": "1901190",
    "end": "1907760"
  },
  {
    "text": "functions are basically what a typical DBA would do sort of an C syntax style where you call the individual functions",
    "start": "1907760",
    "end": "1915320"
  },
  {
    "text": "within that package in the application then this should work out of the gate if not there will be some application code",
    "start": "1915320",
    "end": "1921260"
  },
  {
    "text": "that needs to change and that's kind of it for Oracle to order PG it is a big",
    "start": "1921260",
    "end": "1927020"
  },
  {
    "text": "tool I would recommend you take a good look at the config file read through all the comments in the config read through",
    "start": "1927020",
    "end": "1933530"
  },
  {
    "text": "the readme file there's a lot of information this tool has more flexibility than you could imagine and",
    "start": "1933530",
    "end": "1940280"
  },
  {
    "text": "you'll probably only use 10% of it like we did but it does work",
    "start": "1940280",
    "end": "1945280"
  }
]