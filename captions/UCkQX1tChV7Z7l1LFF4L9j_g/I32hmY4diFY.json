[
  {
    "text": "hi everyone I'm super excited to speak uh at the keynote session in qon um I",
    "start": "4520",
    "end": "9960"
  },
  {
    "text": "keep telling where you know qon is one of my favorite conferences to be at I feel pretty much at home here um you",
    "start": "9960",
    "end": "16920"
  },
  {
    "text": "know this talk is about um a big change that I experience um companies going",
    "start": "16920",
    "end": "23320"
  },
  {
    "text": "through in my day-to-day job this change is about how companies manage their data",
    "start": "23320",
    "end": "29039"
  },
  {
    "text": "the resulting ETL technology that they use and how the rise of realtime data",
    "start": "29039",
    "end": "34960"
  },
  {
    "text": "and stream processing is driving that change uh in this talk I'm going to you",
    "start": "34960",
    "end": "41520"
  },
  {
    "text": "know go through how things work today uh what are the drawbacks you know what is",
    "start": "41520",
    "end": "47039"
  },
  {
    "text": "this new shiny future for ETL look like and then what role does Kafka play in",
    "start": "47039",
    "end": "53800"
  },
  {
    "text": "this picture so before I start can I get a quick show of hands of people who've",
    "start": "53800",
    "end": "58960"
  },
  {
    "text": "either heard of or have used Kafka in any form of manner okay about 90% I'm super excited",
    "start": "58960",
    "end": "66920"
  },
  {
    "text": "all right so let's get started you know um data and Data Systems I've observed",
    "start": "66920",
    "end": "72640"
  },
  {
    "text": "have really changed in the past decade if you think about how things worked",
    "start": "72640",
    "end": "78119"
  },
  {
    "text": "about roughly a decade ago data really resided in two popular locations the",
    "start": "78119",
    "end": "83640"
  },
  {
    "text": "operational databases and the warehouse most of your reporting ran on",
    "start": "83640",
    "end": "89320"
  },
  {
    "text": "the warehouse about once a day sometimes several times a day and so data didn't",
    "start": "89320",
    "end": "94799"
  },
  {
    "text": "really need to move between these two locations for any faster than several times a day this in turn influenced the",
    "start": "94799",
    "end": "103240"
  },
  {
    "text": "architecture of the tool stack or the technology to move data between places called ETL and also the process of",
    "start": "103240",
    "end": "111399"
  },
  {
    "text": "integrating data between sources and destinations which broadly came to be known as data",
    "start": "111399",
    "end": "117680"
  },
  {
    "text": "integration but now several recent data Trends they're driving a really dramatic",
    "start": "117680",
    "end": "123759"
  },
  {
    "text": "change in the ETL architecture first off these single server databases are",
    "start": "123759",
    "end": "129840"
  },
  {
    "text": "rapidly being replaced or augmented by a whole bunch of distributed data",
    "start": "129840",
    "end": "135200"
  },
  {
    "text": "platforms these can operate at companywide scale you know be it no SQL systems like Cassandra or mongodb or or",
    "start": "135200",
    "end": "143040"
  },
  {
    "text": "uh elastic or no SQL you know other systems SAS applications and so on so",
    "start": "143040",
    "end": "148879"
  },
  {
    "text": "your ETL tools um you know have to handle more than just databases and the",
    "start": "148879",
    "end": "154160"
  },
  {
    "text": "warehouse there are many more types of data sources that companies are interested in collecting beyond the",
    "start": "154160",
    "end": "159640"
  },
  {
    "text": "transactional data sources these include logs sensors metrics so now your ETL",
    "start": "159640",
    "end": "165440"
  },
  {
    "text": "tools need to you know go beyond handling just the relational data model and be able to support a pretty large",
    "start": "165440",
    "end": "171800"
  },
  {
    "text": "variety of data models and last but not the least you know the the stream data and the rise of",
    "start": "171800",
    "end": "177879"
  },
  {
    "text": "stream data it is uh becoming increasingly ubiquitous there is a need",
    "start": "177879",
    "end": "183000"
  },
  {
    "text": "for processing data quickly as it arrives instead of in",
    "start": "183000",
    "end": "188200"
  },
  {
    "text": "batches now these Trends are playing out but the technology hadn't caught up so",
    "start": "188200",
    "end": "193840"
  },
  {
    "text": "if you were wondering what data pipelines in companies actually look like you know this is what I've seen in",
    "start": "193840",
    "end": "199239"
  },
  {
    "text": "practice there are applications that talk to each other using some kind of Enterprise messaging cues there are Uh",
    "start": "199239",
    "end": "207319"
  },
  {
    "text": "custom ETL scripts that are written to move data between sources and destinations you know this ad hoc manner",
    "start": "207319",
    "end": "214280"
  },
  {
    "text": "of connecting sources and destinations in a one-off fashion as they arrive is",
    "start": "214280",
    "end": "219720"
  },
  {
    "text": "pretty chaotic it is unmanageable it is lossy in this talk I want to explore how",
    "start": "219720",
    "end": "227640"
  },
  {
    "text": "transitioning to streams cleans up this mess and it works towards a much more",
    "start": "227640",
    "end": "233159"
  },
  {
    "text": "you know scalable manageable ETL architecture this is the idea of having",
    "start": "233159",
    "end": "238959"
  },
  {
    "text": "a streaming platform that serves as your you know central nervous system it",
    "start": "238959",
    "end": "244280"
  },
  {
    "text": "allows applications to talk to each other it allows you to stream change logs from databases and make it",
    "start": "244280",
    "end": "250079"
  },
  {
    "text": "available to other systems and more importantly or interestingly it allows",
    "start": "250079",
    "end": "255400"
  },
  {
    "text": "you know stream processing applications to thrive that can transform this data in a much more incremental",
    "start": "255400",
    "end": "262040"
  },
  {
    "text": "fashion now before I take a look at the solution why don't we you know take a quick look at a short history of how",
    "start": "262040",
    "end": "268600"
  },
  {
    "text": "these tools evolved you know data integration had surfaced in the 1990s when retail organizations",
    "start": "268600",
    "end": "275639"
  },
  {
    "text": "wanted to analyze buyer Trends the way they did that is by extracting data from operational databases transforming that",
    "start": "275639",
    "end": "283600"
  },
  {
    "text": "data into a single Global schema that matches the warehouse and then loading that data into the warehouse by the",
    "start": "283600",
    "end": "290800"
  },
  {
    "text": "early 2000s a lot of companies and industries followed this trend and a new",
    "start": "290800",
    "end": "295880"
  },
  {
    "text": "you know class of Technology emerged to essentially facilitate this data",
    "start": "295880",
    "end": "301039"
  },
  {
    "text": "movement between operational databases and Warehouse called extract transform and land uh uh load which is ETL in its",
    "start": "301039",
    "end": "308880"
  },
  {
    "text": "simplest form just means copying data between different locations now ETL tools have been around",
    "start": "308880",
    "end": "315680"
  },
  {
    "text": "for a while but the data coverage in warehouses is still pretty low you know",
    "start": "315680",
    "end": "320759"
  },
  {
    "text": "what I've seen is out of thousands of operational databases maybe a couple hundreds are really available in the",
    "start": "320759",
    "end": "326400"
  },
  {
    "text": "warehouse and you might think why well it's because um you know ETL tools the",
    "start": "326400",
    "end": "332039"
  },
  {
    "text": "traditional ones they have drawbacks that show up when you try to use them in practice right here are a couple of",
    "start": "332039",
    "end": "337680"
  },
  {
    "text": "those drawbacks one is the need for a global schema you know data modeling is",
    "start": "337680",
    "end": "342800"
  },
  {
    "text": "a really hard problem in its own right but modeling you know one Global schema for a very large domain is even harder",
    "start": "342800",
    "end": "350319"
  },
  {
    "text": "and this is really something that limits the plausible scope of either warehouses or ETL",
    "start": "350319",
    "end": "356080"
  },
  {
    "text": "tools the second is that you know uh ETL the T really stands for data cleansing",
    "start": "356080",
    "end": "361960"
  },
  {
    "text": "which is uh you know transforming data into its cleanest form and defining what",
    "start": "361960",
    "end": "367039"
  },
  {
    "text": "it means and that process is aerop prone it is manual and as you can imagine from the",
    "start": "367039",
    "end": "373919"
  },
  {
    "text": "previous two drawbacks the operational cost of ETL is really high now it is slow it is time and resource",
    "start": "373919",
    "end": "383120"
  },
  {
    "text": "intensive most importantly you know ETL tools were built for a niche problem",
    "start": "383120",
    "end": "388560"
  },
  {
    "text": "right narrowly focus on connecting databases and the warehouse and do that in a batch",
    "start": "388560",
    "end": "396120"
  },
  {
    "text": "fashion now because ETL was narrowly focused on that problem a new class of",
    "start": "396120",
    "end": "401919"
  },
  {
    "text": "Technology emerged to connect applications in real time and that came to be known as Enterprise application",
    "start": "401919",
    "end": "408919"
  },
  {
    "text": "integration you know it is just a bunch of tools that were built to facilitate",
    "start": "408919",
    "end": "414160"
  },
  {
    "text": "exchange of business transactions messages between applications unsurprisingly they used Enterprise",
    "start": "414160",
    "end": "421199"
  },
  {
    "text": "service buses underneath the covers or Enterprise messaging cues underneath the covers and the problem was that these",
    "start": "421199",
    "end": "428400"
  },
  {
    "text": "mqs worked for small scale data they were just not designed to handle the scale of data that is required for",
    "start": "428400",
    "end": "434840"
  },
  {
    "text": "modern data sets like logs and sensors and so we can't really use this for any",
    "start": "434840",
    "end": "440400"
  },
  {
    "text": "kind of companywide large scale real-time data integration the summary is that you know",
    "start": "440400",
    "end": "447759"
  },
  {
    "text": "the class of Technology available for solving this whole data integration problem whether it's ETL or eii they're",
    "start": "447759",
    "end": "455720"
  },
  {
    "text": "really outdated you know on one hand you have Enterprise application integration",
    "start": "455720",
    "end": "461759"
  },
  {
    "text": "it is real time but not scalable on the other hand you have ETL which is",
    "start": "461759",
    "end": "467159"
  },
  {
    "text": "scalable but batch and so organizations you know face a tough choice when you adopt one of these you get either real",
    "start": "467159",
    "end": "474319"
  },
  {
    "text": "time um or you get scale but batch but you have to pick one",
    "start": "474319",
    "end": "481240"
  },
  {
    "text": "now these Trends are pretty big you know they demand not just small change small",
    "start": "481240",
    "end": "486440"
  },
  {
    "text": "changes to these tools but they actually require a complete revamp this complete revamp is required",
    "start": "486440",
    "end": "494039"
  },
  {
    "text": "to create a technology that is actually suitable to a modern streaming world where real time and scale they're not",
    "start": "494039",
    "end": "501599"
  },
  {
    "text": "the exception they're pretty much the rule and so as a result of this you know",
    "start": "501599",
    "end": "507520"
  },
  {
    "text": "modern streaming World actually has a new set of requirements for data integration first is you have to have",
    "start": "507520",
    "end": "513839"
  },
  {
    "text": "the ability to process not just high volume but High diversity",
    "start": "513839",
    "end": "518919"
  },
  {
    "text": "data it needs to be real time from the grounds up you know and you might think",
    "start": "518919",
    "end": "524839"
  },
  {
    "text": "what does that entail one is we need to have the technology to support that but the other",
    "start": "524839",
    "end": "530880"
  },
  {
    "text": "is which is equally important is that we need to make a pretty much fundamental transition to what I'm going to call",
    "start": "530880",
    "end": "536640"
  },
  {
    "text": "event Centric thinking so trying to explain explain that a little bit more with the help of this example let's",
    "start": "536640",
    "end": "541920"
  },
  {
    "text": "assume this magical streaming platform can support uh high volume High diversity data as well as do that in",
    "start": "541920",
    "end": "548480"
  },
  {
    "text": "real time uh now this is an example of a retail web app it uh logs product page",
    "start": "548480",
    "end": "555399"
  },
  {
    "text": "View events we want to analyze that in Hadoop so on one hand we stream those",
    "start": "555399",
    "end": "560760"
  },
  {
    "text": "events into a streaming platform and on the other side Hadoop subscribes to the streaming platform and loads that data",
    "start": "560760",
    "end": "567600"
  },
  {
    "text": "that works but at this point it may not be clear what problem this streaming platform is",
    "start": "567600",
    "end": "573360"
  },
  {
    "text": "solving so if you go one step further you know you'll realize that over time you've um added more ways of producing",
    "start": "573360",
    "end": "580279"
  },
  {
    "text": "these product View events you launch a mobile app you create external apis when that happens you will notice in this",
    "start": "580279",
    "end": "587640"
  },
  {
    "text": "picture the Hadoop side of the load doesn't need to change at all this decoupling that is introduced",
    "start": "587640",
    "end": "594560"
  },
  {
    "text": "by the pub sub model in the streaming platform it allows things that produce data and things that consume data to",
    "start": "594560",
    "end": "602320"
  },
  {
    "text": "evolve on their own because they don't have to know about each other going one step further as you add",
    "start": "602320",
    "end": "609760"
  },
  {
    "text": "more Downstream applications that need to consume the same product View events but process them differently you will",
    "start": "609760",
    "end": "616560"
  },
  {
    "text": "notice that as you add those you don't have to add point-to-point connections with everything that produces the same",
    "start": "616560",
    "end": "623480"
  },
  {
    "text": "data you merely subscribe to the central streaming platform so the end result is",
    "start": "623480",
    "end": "629000"
  },
  {
    "text": "that you know what I've observed is event Centric thinking when applied at a",
    "start": "629000",
    "end": "634279"
  },
  {
    "text": "companywide scale is actually the core reason for you know ending up with a",
    "start": "634279",
    "end": "639880"
  },
  {
    "text": "much more cleaner streaming platform and a much more cleaner ETL",
    "start": "639880",
    "end": "645680"
  },
  {
    "text": "architecture the third is um you know the third requirement is of forward compatibility so this event Centric",
    "start": "645680",
    "end": "652240"
  },
  {
    "text": "thinking actually has another Advantage it allows you to create a forward compatible data architecture what I mean",
    "start": "652240",
    "end": "658920"
  },
  {
    "text": "by that is the ability to add more applications that might need to process",
    "start": "658920",
    "end": "664000"
  },
  {
    "text": "data differently so if You observe in the previous example right um every new",
    "start": "664000",
    "end": "671000"
  },
  {
    "text": "application was added to serve a business need what that was SP at that time there was no way to predict that",
    "start": "671000",
    "end": "677839"
  },
  {
    "text": "ahead of time so when we added a retail web app we didn't know we might create a mobile web app or mobile app in the",
    "start": "677839",
    "end": "684560"
  },
  {
    "text": "future when we had just Hadoop we didn't know that we might have to add four other ways of processing that same data",
    "start": "684560",
    "end": "692040"
  },
  {
    "text": "differently and this is really important you know your ETL architecture it needs to allow new data sources and new Data",
    "start": "692040",
    "end": "699440"
  },
  {
    "text": "Systems to emerge over time to use the same data differently so to explore this",
    "start": "699440",
    "end": "705600"
  },
  {
    "text": "you know requirement or this need a little bit more to enable forward compatibility that is multiple",
    "start": "705600",
    "end": "711600"
  },
  {
    "text": "destinations for the same data it has a really important implication on what T stands in ETA which is traditionally for",
    "start": "711600",
    "end": "718880"
  },
  {
    "text": "you know data cleansing but we need to move from data cleansing to data Transformations let me explain that with",
    "start": "718880",
    "end": "725519"
  },
  {
    "text": "an example let's say this is the same example we have the logs we need to now move them to the warehouse traditionally",
    "start": "725519",
    "end": "732560"
  },
  {
    "text": "you might extract that as unstructured text your T actually stands for data cleansing which is really defining what",
    "start": "732560",
    "end": "740040"
  },
  {
    "text": "this product view means all the different fields and then your load involves loading it into a specific",
    "start": "740040",
    "end": "746440"
  },
  {
    "text": "system which is the data warehouse and let's say you might need to you know drop some personally identifiable",
    "start": "746440",
    "end": "753680"
  },
  {
    "text": "information fields from this data to really make it usable for the users so that custom transformation runs in your",
    "start": "753680",
    "end": "759680"
  },
  {
    "text": "warehouse now what happens if you add another destination for this data let's say Cassandra you will notice that you",
    "start": "759680",
    "end": "767720"
  },
  {
    "text": "repeat the business Logic for extracting that data you also repeat the transformation which is actually",
    "start": "767720",
    "end": "773880"
  },
  {
    "text": "cleansing that data and turning it into a real product view then you load it into Cassandra and then you run the same",
    "start": "773880",
    "end": "780360"
  },
  {
    "text": "transformation but this is pretty wasteful you know in order to allow more destinations if we repeat the business",
    "start": "780360",
    "end": "787399"
  },
  {
    "text": "logic of cleansing that data it is um you know not only inefficient but it can also be lossy if one of these scripts",
    "start": "787399",
    "end": "795519"
  },
  {
    "text": "fails What If instead we make clean data available UPF front which is we extract",
    "start": "795519",
    "end": "802199"
  },
  {
    "text": "and we load just clean producty events into the streaming platform the transform runs on the streaming platform",
    "start": "802199",
    "end": "808880"
  },
  {
    "text": "to create another stream which drops the pii fields and now you have two choices for loading the data in either the",
    "start": "808880",
    "end": "815279"
  },
  {
    "text": "warehouse or Cassandra or anything that might arrive in the future now the second implication on",
    "start": "815279",
    "end": "822760"
  },
  {
    "text": "this T is you might wonder well if clean data is available up front then does TL completely go away not really it now",
    "start": "822760",
    "end": "830600"
  },
  {
    "text": "turns and stands for actual data transformations to make data ready for Destination systems so for example if",
    "start": "830600",
    "end": "837480"
  },
  {
    "text": "you were to run a query like find me the top and popularly viewed products in a certain price segment we now have to",
    "start": "837480",
    "end": "844560"
  },
  {
    "text": "enrich this data that previously only had you know all the product View events",
    "start": "844560",
    "end": "849720"
  },
  {
    "text": "but filtered on Pi Fields when it is done on top of this streaming platform",
    "start": "849720",
    "end": "855360"
  },
  {
    "text": "which has the clean product for you events upgrading your transform you know becomes simpler you upgrade it to don't",
    "start": "855360",
    "end": "861759"
  },
  {
    "text": "just drop events but now enrich it with product metadata which might itself be a",
    "start": "861759",
    "end": "867560"
  },
  {
    "text": "change lock stream from another Source database that is available in the streaming platform and this is great",
    "start": "867560",
    "end": "872839"
  },
  {
    "text": "because your your extraction and transformation is done once but now your load can be done several times in",
    "start": "872839",
    "end": "879279"
  },
  {
    "text": "different systems they don't have to all repeat that transformation so to summarize you know",
    "start": "879279",
    "end": "885440"
  },
  {
    "text": "this point is important forward compatibility it actually stands for let's extract clean data once let's then",
    "start": "885440",
    "end": "892199"
  },
  {
    "text": "make it available to be transformed in several different ways to load it into the respective destinations but then do",
    "start": "892199",
    "end": "898920"
  },
  {
    "text": "that that as in when required so to summarize you know what are the needs of a modern streaming data",
    "start": "898920",
    "end": "906160"
  },
  {
    "text": "integration solution we need scale we need diversity latency and more",
    "start": "906160",
    "end": "911320"
  },
  {
    "text": "importantly forward compatibility I summarize these needs because you know they drive the",
    "start": "911320",
    "end": "917399"
  },
  {
    "text": "requirements for this solution right we need fall tolerance and parallelism so we can deploy lots and lots of these ETL",
    "start": "917399",
    "end": "923839"
  },
  {
    "text": "processes to handle large data sources we need it to support low latency see",
    "start": "923839",
    "end": "929880"
  },
  {
    "text": "delivery semantics what about ordering that is important operations and monitoring to be able to you know View",
    "start": "929880",
    "end": "936240"
  },
  {
    "text": "and monitor all your ETL copier processes centrally and then schema Management on how schemas can evolve as",
    "start": "936240",
    "end": "943319"
  },
  {
    "text": "you copy data but these are all hard problems in their own right instead of",
    "start": "943319",
    "end": "948800"
  },
  {
    "text": "solving them in a one-off way in custom ETL tools that are meant for specific",
    "start": "948800",
    "end": "954040"
  },
  {
    "text": "systems I'm advocating for an approach that is you know much more practical and efficient which which is let's solve all",
    "start": "954040",
    "end": "960000"
  },
  {
    "text": "these problems in a common platform which is reusable for many different use",
    "start": "960000",
    "end": "967279"
  },
  {
    "text": "cases so now I want to present what a new and shiny future for ETL looks like",
    "start": "967279",
    "end": "973399"
  },
  {
    "text": "this is in this future all your data is represented as streams the central",
    "start": "973399",
    "end": "978480"
  },
  {
    "text": "streaming platform it serves as a storage layer for your stream data",
    "start": "978480",
    "end": "984720"
  },
  {
    "text": "extract and load involves moving streams between external system systems and this",
    "start": "984720",
    "end": "990040"
  },
  {
    "text": "Central streaming platform and Transformations actually takes the uh shape and form of stream",
    "start": "990040",
    "end": "997279"
  },
  {
    "text": "processing now the streaming platform it serves as you know almost like a central nervous system for your company's data",
    "start": "997279",
    "end": "1004160"
  },
  {
    "text": "it serves as the realtime messaging bus so your applications can exchange",
    "start": "1004160",
    "end": "1009639"
  },
  {
    "text": "messages it serves as the source of Truth pipeline for feeding any and all",
    "start": "1009639",
    "end": "1014839"
  },
  {
    "text": "data processing systems be tooop or Warehouse or no SQL systems or or several",
    "start": "1014839",
    "end": "1020399"
  },
  {
    "text": "more and it actually serves as the you know building block for stateful stream",
    "start": "1020399",
    "end": "1026038"
  },
  {
    "text": "processing microservices or applications which all represent your company's",
    "start": "1026039",
    "end": "1031079"
  },
  {
    "text": "business logic as stream processing in this future you know",
    "start": "1031079",
    "end": "1036280"
  },
  {
    "text": "companies still have the data integration problem the solution just looks very different in a streaming",
    "start": "1036280",
    "end": "1042038"
  },
  {
    "text": "first world we still do ETL but in a streaming",
    "start": "1042039",
    "end": "1047160"
  },
  {
    "text": "Fashion on top of a Central platform redefining what T stands for which is",
    "start": "1047160",
    "end": "1053320"
  },
  {
    "text": "essentially stream processing so let me summarize you know",
    "start": "1053320",
    "end": "1058559"
  },
  {
    "text": "what we've gone through so far before diving into um the streaming platform we went through a short history of what uh",
    "start": "1058559",
    "end": "1065320"
  },
  {
    "text": "data integration is what are the drawbacks of the ETL tools what are the needs and requirements for a streaming",
    "start": "1065320",
    "end": "1071960"
  },
  {
    "text": "platform what is this new and shiny future for ETL look like and in the",
    "start": "1071960",
    "end": "1077360"
  },
  {
    "text": "latter part of this talk what I want to go through is you know what does a streaming platform look like how does it",
    "start": "1077360",
    "end": "1083799"
  },
  {
    "text": "enable this streaming ETL and that Journey starts with Apache",
    "start": "1083799",
    "end": "1089559"
  },
  {
    "text": "Kafka you know it is an open-source distributed streaming platform we",
    "start": "1089559",
    "end": "1095400"
  },
  {
    "text": "created Kafka to essentially make this event Centric thinking available at a",
    "start": "1095400",
    "end": "1100960"
  },
  {
    "text": "companywide scale we had a very particular vision for what a company should look like if it had reimagined",
    "start": "1100960",
    "end": "1107720"
  },
  {
    "text": "its use of data around streams of events we started Kafka roughly 6 years",
    "start": "1107720",
    "end": "1114159"
  },
  {
    "text": "ago at LinkedIn today it is deployed at a pretty large scale at LinkedIn it serves",
    "start": "1114159",
    "end": "1120880"
  },
  {
    "text": "more than 1.4 trillion messages per day across several data",
    "start": "1120880",
    "end": "1127159"
  },
  {
    "text": "centers and uh Kafka is now adopted across thousands of companies worldwide from you know retail to webtech and",
    "start": "1127159",
    "end": "1134559"
  },
  {
    "text": "fintech and so on roughly about 35% of Fortune 500 Kafka",
    "start": "1134559",
    "end": "1140039"
  },
  {
    "text": "today so what I want to look into is you know what role does Kafka play in this",
    "start": "1140039",
    "end": "1146240"
  },
  {
    "text": "new future for data integration well first off Kafka is the",
    "start": "1146240",
    "end": "1151880"
  },
  {
    "text": "de facto storage of choice for stream data so most of you are familiar with",
    "start": "1151880",
    "end": "1157640"
  },
  {
    "text": "Kafka some of you might also be familiar with this log abstraction this is what",
    "start": "1157640",
    "end": "1163720"
  },
  {
    "text": "uh the storage back end of Kafka is based on which is this concept of of a",
    "start": "1163720",
    "end": "1169480"
  },
  {
    "text": "persistent replicated WR ahead and a pendon log where every record is",
    "start": "1169480",
    "end": "1176159"
  },
  {
    "text": "identified using a unique index called an offset wrs are only in the form of a",
    "start": "1176159",
    "end": "1182120"
  },
  {
    "text": "pens readers can use that offset and index into the log and then read",
    "start": "1182120",
    "end": "1187559"
  },
  {
    "text": "messages in order now the key Insight at the heart of Kafka is that this abstraction is a",
    "start": "1187559",
    "end": "1193840"
  },
  {
    "text": "great primitive for building scalable pup sub uh messaging so you can imagine that your publisher",
    "start": "1193840",
    "end": "1200760"
  },
  {
    "text": "is the one that append data to the end of the log and your subscribers in kafal",
    "start": "1200760",
    "end": "1206159"
  },
  {
    "text": "land they maintain their offset they can index into this log and then scan",
    "start": "1206159",
    "end": "1211320"
  },
  {
    "text": "sequentially from there onwards the key point is that the sequential nature of reads and writs it",
    "start": "1211320",
    "end": "1216840"
  },
  {
    "text": "allows this abstraction to support pretty impressive through Port so Kafka supports about you know hundreds of",
    "start": "1216840",
    "end": "1222039"
  },
  {
    "text": "thousands of messages per second per server the first is you know Kafka is a",
    "start": "1222039",
    "end": "1228240"
  },
  {
    "text": "storage back end but Kafka offers a scalable messaging backbone for application integration so this is what",
    "start": "1228240",
    "end": "1235080"
  },
  {
    "text": "we all know Kafka for uh one of the core apis of Kafka is the messaging API which",
    "start": "1235080",
    "end": "1241480"
  },
  {
    "text": "allows you to produce and consume messages so applications embed these",
    "start": "1241480",
    "end": "1246600"
  },
  {
    "text": "libraries and talk to each other using Kafka but the second you know the Third",
    "start": "1246600",
    "end": "1252000"
  },
  {
    "text": "Way is like Kafka actually enables building streaming data pipelines in the 09 release of Apache",
    "start": "1252000",
    "end": "1258600"
  },
  {
    "text": "afka the community added a connect API the core Focus for connect API is to",
    "start": "1258600",
    "end": "1264600"
  },
  {
    "text": "make building these streaming data pipelines from external systems into Kafka really easy in an off-the-shelf",
    "start": "1264600",
    "end": "1272520"
  },
  {
    "text": "Manner and last but not the least you know the way Kafka completes this picture for data integration is is that",
    "start": "1272520",
    "end": "1278679"
  },
  {
    "text": "now it's the basis for stream processing and transformations in fact the o10",
    "start": "1278679",
    "end": "1283919"
  },
  {
    "text": "release of Apache Kafka added the streams API which allows you to",
    "start": "1283919",
    "end": "1289440"
  },
  {
    "text": "essentially write stream processing uh operators or stream processing programs very easily on top of Kafka you can",
    "start": "1289440",
    "end": "1296960"
  },
  {
    "text": "embed the streams API and uh write stream processing in your application so in the remainder of the",
    "start": "1296960",
    "end": "1304360"
  },
  {
    "text": "stock I want to dive a little bit deeper into both the connect and streams API because they really complete the vision",
    "start": "1304360",
    "end": "1310880"
  },
  {
    "text": "for streaming ETL now let's start with kafas connect API which is really the E",
    "start": "1310880",
    "end": "1316679"
  },
  {
    "text": "and the L in streaming at you know building streaming data pipelines using Kafka is all about these",
    "start": "1316679",
    "end": "1324640"
  },
  {
    "text": "connectors in this picture it seems simple to just draw the line and have data flow between any external system",
    "start": "1324640",
    "end": "1331039"
  },
  {
    "text": "and Kafka but there's a lot that goes underneath the covers to do this",
    "start": "1331039",
    "end": "1336080"
  },
  {
    "text": "correctly and let's take a step back and look at the system level view of things that was a logical picture but streaming",
    "start": "1336080",
    "end": "1342400"
  },
  {
    "text": "etail today it might involve moving data between data centers companies have",
    "start": "1342400",
    "end": "1347720"
  },
  {
    "text": "multiple data Cent centers for several reasons know you might be migrating from an on-prem to a public Cloud uh for",
    "start": "1347720",
    "end": "1355159"
  },
  {
    "text": "Disaster Recovery merges and Acquisitions so any problem that",
    "start": "1355159",
    "end": "1360480"
  },
  {
    "text": "involves moving data between sources and destinations today it involves moving",
    "start": "1360480",
    "end": "1365720"
  },
  {
    "text": "data also between data centers in a streaming fashion cfas connect API there are two",
    "start": "1365720",
    "end": "1373679"
  },
  {
    "text": "abstractions pretty easy to understand there are sources that pull data from exter internal systems into Kafka and",
    "start": "1373679",
    "end": "1381240"
  },
  {
    "text": "then there are syncs that push data from Kafka into external syncs and some of these sources and syns",
    "start": "1381240",
    "end": "1388440"
  },
  {
    "text": "can be written in both streaming as well as batch fashion if needed this is the most interesting you",
    "start": "1388440",
    "end": "1394559"
  },
  {
    "text": "know ETL problem as we talked about how do we make data from databases available",
    "start": "1394559",
    "end": "1399679"
  },
  {
    "text": "to not just the warehouse but any other application that needs to use it if you",
    "start": "1399679",
    "end": "1405080"
  },
  {
    "text": "think about it for a moment in order to do this in a streaming fashion one way is you can set up triggers and you can",
    "start": "1405080",
    "end": "1411240"
  },
  {
    "text": "scan the database and that works but that is inefficient another way is to stream the",
    "start": "1411240",
    "end": "1418600"
  },
  {
    "text": "change log of a database now some of you might be familiar with this you know databases are designed like this",
    "start": "1418600",
    "end": "1424640"
  },
  {
    "text": "underneath the covers where they rely on a commit log as the source of Truth and tables are merely views of that commit",
    "start": "1424640",
    "end": "1431200"
  },
  {
    "text": "log and The way database replication works for the most part is by shipping these change logs around",
    "start": "1431200",
    "end": "1438679"
  },
  {
    "text": "this change log is you know it's an abstraction where every message is",
    "start": "1438679",
    "end": "1443760"
  },
  {
    "text": "essentially an update or a change or a mutation to the database so if you were",
    "start": "1443760",
    "end": "1449840"
  },
  {
    "text": "to scan this change lock from the very beginning and apply it to an empty database you can essentially recreate the",
    "start": "1449840",
    "end": "1455400"
  },
  {
    "text": "database you will notice that this abstraction looks very similar to the log abstraction in Kafka in fact Kafka",
    "start": "1455400",
    "end": "1462559"
  },
  {
    "text": "has special support for supporting change logs and these database",
    "start": "1462559",
    "end": "1468799"
  },
  {
    "text": "connectors are in fact the most popular ones written on top of cfas connect API",
    "start": "1468799",
    "end": "1473840"
  },
  {
    "text": "it has actually another cool Advantage which is by making these change logs available in",
    "start": "1473840",
    "end": "1479240"
  },
  {
    "text": "Kafka now Transformations become much easier and they're much more scalable so",
    "start": "1479240",
    "end": "1486000"
  },
  {
    "text": "instead of applying Transformations on either the source or the destination databases it can be made available on",
    "start": "1486000",
    "end": "1492360"
  },
  {
    "text": "this you know replicated log which is a lot faster so instead of moving data",
    "start": "1492360",
    "end": "1497880"
  },
  {
    "text": "just blindly between the source and the destination if move through Kafka you can transform it and then move it into",
    "start": "1497880",
    "end": "1503640"
  },
  {
    "text": "destination applications so the core focus of kafka's connect API is to really make",
    "start": "1503640",
    "end": "1511159"
  },
  {
    "text": "writing these connectors super simple you know to make it available in a",
    "start": "1511159",
    "end": "1516360"
  },
  {
    "text": "really off-the-shelf Manner and do all the hard work underneath the covers so cfas connect API it leverages and it",
    "start": "1516360",
    "end": "1522799"
  },
  {
    "text": "builds on top of kafka's scalability and fall tolerance model it allows you one",
    "start": "1522799",
    "end": "1529360"
  },
  {
    "text": "way of monitoring all your connectors and most importantly it offers the option of carrying the source",
    "start": "1529360",
    "end": "1535720"
  },
  {
    "text": "schema into all the destination systems so what you can do is if you add a",
    "start": "1535720",
    "end": "1540840"
  },
  {
    "text": "column in your Source database what would have previously broken data pipelines now would carry that extra",
    "start": "1540840",
    "end": "1547720"
  },
  {
    "text": "column seeming seamlessly throughout the data Pipeline and you know apply it to",
    "start": "1547720",
    "end": "1553039"
  },
  {
    "text": "either the elastic index or a hive table and do that you know without user",
    "start": "1553039",
    "end": "1558559"
  },
  {
    "text": "knowing about it in a pretty you know transparent",
    "start": "1558559",
    "end": "1563600"
  },
  {
    "text": "manner today um you know there are lots and lots of these sources and syncs all",
    "start": "1563600",
    "end": "1569120"
  },
  {
    "text": "open source available for use so you can connect a pretty large set of sources and syns in a off-the-shelf manner using",
    "start": "1569120",
    "end": "1576960"
  },
  {
    "text": "Kus connect API so then if connect is e and l let's",
    "start": "1576960",
    "end": "1583600"
  },
  {
    "text": "now look at you know the streams API which is really stands for the T in stream",
    "start": "1583600",
    "end": "1589760"
  },
  {
    "text": "ATL stream processing is all about applying Transformations on stream data",
    "start": "1589760",
    "end": "1597159"
  },
  {
    "text": "Transformations can take many forms you know filters Maps window joints and",
    "start": "1597159",
    "end": "1602760"
  },
  {
    "text": "aggregations and so on before I take a look at the streams",
    "start": "1602760",
    "end": "1608679"
  },
  {
    "text": "API let me take a step back and talk about these broad two visions for stream",
    "start": "1608679",
    "end": "1614320"
  },
  {
    "text": "processing I got a chance to experience both the first vision is let's make map",
    "start": "1614320",
    "end": "1619520"
  },
  {
    "text": "produce go faster so let's build a real-time map produce layer uh that used to work really well so let's make it go",
    "start": "1619520",
    "end": "1626360"
  },
  {
    "text": "faster the other vision is very different it says you know let's look at all your business logic all your",
    "start": "1626360",
    "end": "1632799"
  },
  {
    "text": "applications and think of those applications as event driven you know stream",
    "start": "1632799",
    "end": "1638399"
  },
  {
    "text": "processors I mentioned these two visions because they really influence what the",
    "start": "1638399",
    "end": "1643640"
  },
  {
    "text": "solution looks like so if you think about building stream process as a realtime map produce layer now you have",
    "start": "1643640",
    "end": "1651480"
  },
  {
    "text": "a central cluster that runs a whole bunch of processes you have to express",
    "start": "1651480",
    "end": "1656559"
  },
  {
    "text": "your stream processing code as a job that is packaged in a custom manner just",
    "start": "1656559",
    "end": "1662360"
  },
  {
    "text": "like Hadoop is it is deployed and monitored in a custom manner you probably have yarn or measur for fall",
    "start": "1662360",
    "end": "1669080"
  },
  {
    "text": "tolerance and so on this model probably works well for you know long running",
    "start": "1669080",
    "end": "1674120"
  },
  {
    "text": "analytical type of queries where you want to run a large multi-tenant cluster",
    "start": "1674120",
    "end": "1679720"
  },
  {
    "text": "now event driven microservices you know the focus is very different it's saying that let's think of applications as",
    "start": "1679720",
    "end": "1685600"
  },
  {
    "text": "things that take input streams business logic that is really stream processing",
    "start": "1685600",
    "end": "1690679"
  },
  {
    "text": "and and then it produces output streams if you think of that and making that easier then now your stream processing",
    "start": "1690679",
    "end": "1697519"
  },
  {
    "text": "engine really has to be a library that application developers can just embed and start using so in this world you have a kafa",
    "start": "1697519",
    "end": "1705120"
  },
  {
    "text": "cluster and then you have your app and then you don't deploy anything else and the main focus for this vision",
    "start": "1705120",
    "end": "1712120"
  },
  {
    "text": "is really to make stream processing available as a general purpose programming Paradigm it is not a niche",
    "start": "1712120",
    "end": "1719080"
  },
  {
    "text": "thing but it is available across the company here are some examples of",
    "start": "1719080",
    "end": "1724799"
  },
  {
    "text": "systems that subscribe to the realtime map produce Vision in fact um I had a chance to work on Apache samza which is",
    "start": "1724799",
    "end": "1732799"
  },
  {
    "text": "also um you know similar to some of these systems and while putting it into practice at LinkedIn we learned that you",
    "start": "1732799",
    "end": "1740640"
  },
  {
    "text": "know what developers wanted was they wanted to continue developing their Java apps what we asked them to do is to take",
    "start": "1740640",
    "end": "1747279"
  },
  {
    "text": "some part of their Java app and express it as a job and then talk to the stream processing people to get it deployed on",
    "start": "1747279",
    "end": "1753519"
  },
  {
    "text": "their cluster so that created sufficient um you know friction in order uh to",
    "start": "1753519",
    "end": "1758559"
  },
  {
    "text": "adopt stream processing at a LinkedIn wide scale so then we introduced you know we looked at this problem a",
    "start": "1758559",
    "end": "1764640"
  },
  {
    "text": "different way we we came at it from the event and microservices vision and created streams so streams is you know",
    "start": "1764640",
    "end": "1772080"
  },
  {
    "text": "the streams is just a library it is an API that you embed in your application and then you can do stream",
    "start": "1772080",
    "end": "1778799"
  },
  {
    "text": "processing so really you know the core Focus that we had when we created streams was that you know let's make it",
    "start": "1778799",
    "end": "1785880"
  },
  {
    "text": "the easiest way to do stream processing on top of Kafka you know people love the producers and consumer libraries let's",
    "start": "1785880",
    "end": "1791799"
  },
  {
    "text": "give them a stream processor Library so as a result of that you know this is what it looks like it is a it's",
    "start": "1791799",
    "end": "1798080"
  },
  {
    "text": "a powerful but lightweight Java Library so all you need is if you have a Kafka",
    "start": "1798080",
    "end": "1803120"
  },
  {
    "text": "cluster then you have a streams API that can be used to transform cfot",
    "start": "1803120",
    "end": "1808679"
  },
  {
    "text": "toopics it is um it has a convenient DSL with all sorts of operators still",
    "start": "1808679",
    "end": "1814240"
  },
  {
    "text": "evolving but joints map filter window Aggregates and so on so this is a you know code you might write just as an",
    "start": "1814240",
    "end": "1821200"
  },
  {
    "text": "example the cliched you know word count example you create a builder you write your code to count the word words and",
    "start": "1821200",
    "end": "1828720"
  },
  {
    "text": "then in this example the output is just another Kafka topic but you could send it to any other external system and then",
    "start": "1828720",
    "end": "1835440"
  },
  {
    "text": "you say start the cool thing is you know you might realize that you can take this code and run it on one instance and it",
    "start": "1835440",
    "end": "1842039"
  },
  {
    "text": "will run just fine you can package it in a Docker container and deploy it on mesos and your code doesn't have to",
    "start": "1842039",
    "end": "1849120"
  },
  {
    "text": "change essentially all the hard work which is how do you load balance and assign partitions to all your different",
    "start": "1849120",
    "end": "1855679"
  },
  {
    "text": "application instances is handled transparently by the streams library because it uh Builds on top of Kafka",
    "start": "1855679",
    "end": "1863919"
  },
  {
    "text": "Primitives streams is a event at a time stream processing engine so it doesn't",
    "start": "1863919",
    "end": "1869519"
  },
  {
    "text": "do micro batching it can process events as they arrive we've been pretty uh inspired by",
    "start": "1869519",
    "end": "1877240"
  },
  {
    "text": "all the insights that Google's data flow team has shared which all revolve around",
    "start": "1877240",
    "end": "1882960"
  },
  {
    "text": "you know how do we handle late arriving data you know without going into too many Det details the core Insight is",
    "start": "1882960",
    "end": "1890320"
  },
  {
    "text": "that we need to accept that there will be late arriving data in when you're processing data in a streaming fashion",
    "start": "1890320",
    "end": "1896799"
  },
  {
    "text": "we just need to be able to handle it correctly so the Insight is that let's differentiate between event time which",
    "start": "1896799",
    "end": "1903080"
  },
  {
    "text": "is when an event takes place in the real world from processing time which is when it actually gets processed and if we",
    "start": "1903080",
    "end": "1910000"
  },
  {
    "text": "handle these two things differently then we might get correct results even as you get late arriving data this is a pretty",
    "start": "1910000",
    "end": "1916440"
  },
  {
    "text": "deep Topic in its own right I know Tyler um and Francis are giving a talk on this",
    "start": "1916440",
    "end": "1921720"
  },
  {
    "text": "I would highly recommend going and uh you know learning from those uh two great",
    "start": "1921720",
    "end": "1927200"
  },
  {
    "text": "Engineers the fifth point is um Kaka streams API it does this cool thing it",
    "start": "1927200",
    "end": "1933039"
  },
  {
    "text": "has outof thee box support for local States so this is one of the things we learned from samza that worked well and",
    "start": "1933039",
    "end": "1938440"
  },
  {
    "text": "we adopted it in KFA streams API and it is essential because it really allows you to build these uh High throughput",
    "start": "1938440",
    "end": "1945480"
  },
  {
    "text": "fast stream processor apps so to to get into a little more detail you know if",
    "start": "1945480",
    "end": "1950679"
  },
  {
    "text": "you think about State and how do you manage State and apps the traditional way is you know let's let's stick that",
    "start": "1950679",
    "end": "1957159"
  },
  {
    "text": "state in some kind of external database and get done with it now this works",
    "start": "1957159",
    "end": "1962200"
  },
  {
    "text": "right you can trust this external database so um there are some inefficiencies at the same time one",
    "start": "1962200",
    "end": "1969399"
  },
  {
    "text": "inefficiency is that there's less isolation so any app could overwhelm the",
    "start": "1969399",
    "end": "1974720"
  },
  {
    "text": "database you have less choices for how to pick different databases depending on what your app is doing and more",
    "start": "1974720",
    "end": "1981960"
  },
  {
    "text": "importantly um you know in order to write a stateful stream processor app you also have to manage this external",
    "start": "1981960",
    "end": "1989360"
  },
  {
    "text": "database what streams does is it it pushes this external database and Divi",
    "start": "1989360",
    "end": "1994960"
  },
  {
    "text": "divis it up into shards and makes it available as local embedded",
    "start": "1994960",
    "end": "2000440"
  },
  {
    "text": "State now this local state could be a rock DB engine it could be an inmemory",
    "start": "2000440",
    "end": "2005519"
  },
  {
    "text": "hashmap and there will be more that come uh you know as we make more progress on streams but the concept is that this is",
    "start": "2005519",
    "end": "2013000"
  },
  {
    "text": "highly efficient because your data is sharded the same way as your input",
    "start": "2013000",
    "end": "2018080"
  },
  {
    "text": "streams so all your processing can happen locally with the data available you don't have to make external RPC",
    "start": "2018080",
    "end": "2024279"
  },
  {
    "text": "calls and hence that is super effic",
    "start": "2024279",
    "end": "2028840"
  },
  {
    "text": "efficient more than that I think this local state is also fall tolerant because it's a local embedded engine",
    "start": "2029600",
    "end": "2035159"
  },
  {
    "text": "what if your app dies does your state go away not really the Kafka you know Kafka and",
    "start": "2035159",
    "end": "2040960"
  },
  {
    "text": "Kafka streams API it knows how to do load balancing just how it load balances your partitions so if an app instance",
    "start": "2040960",
    "end": "2047919"
  },
  {
    "text": "dies it automatically load balances and moves that local state embedded database",
    "start": "2047919",
    "end": "2053520"
  },
  {
    "text": "into the remaining instances that are still alive and all of that happens without users finding out so it can",
    "start": "2053520",
    "end": "2059520"
  },
  {
    "text": "self-heal automatically another important point",
    "start": "2059520",
    "end": "2064679"
  },
  {
    "text": "about kafa streams API is that it allows reprocessing so if you think about you know what we do with apps we upgrade",
    "start": "2064679",
    "end": "2070960"
  },
  {
    "text": "apps we do AB testing and when that happens we need to reprocess you know go",
    "start": "2070960",
    "end": "2077520"
  },
  {
    "text": "back and reprocess either a small window of data or perhaps sometimes the entire",
    "start": "2077520",
    "end": "2083440"
  },
  {
    "text": "history so let's take an example right you deploy an app and a day later you find a bug so all the results that were",
    "start": "2083440",
    "end": "2091158"
  },
  {
    "text": "processed using the last 24-hour window perhaps are incorrect so as you deploy",
    "start": "2091159",
    "end": "2096638"
  },
  {
    "text": "the new version of your app you actually want to go back and reprocess the 24 hours in order to reflect correct",
    "start": "2096639",
    "end": "2102599"
  },
  {
    "text": "results now this again is one of the uh you know deeper topics to get stream processing right but it is one that is",
    "start": "2102599",
    "end": "2109359"
  },
  {
    "text": "really important now let's uh let me conclude",
    "start": "2109359",
    "end": "2114760"
  },
  {
    "text": "with an example uh to show how these two visions can uh really influence how your",
    "start": "2114760",
    "end": "2120000"
  },
  {
    "text": "solution looks like you know in this example you're building a realtime dashboard for security monitoring so",
    "start": "2120000",
    "end": "2125240"
  },
  {
    "text": "you're monitoring user activity across the Globe you're aggregating by GE region and then in this particular app",
    "start": "2125240",
    "end": "2133000"
  },
  {
    "text": "you're highlighting regions that have irregular user activity now here's your solution right",
    "start": "2133000",
    "end": "2139160"
  },
  {
    "text": "if you were using Vision one you have perhaps kafa clusters that are uh hosting all these user activity streams",
    "start": "2139160",
    "end": "2146800"
  },
  {
    "text": "you have to deploy this stream processing cluster you write your code",
    "start": "2146800",
    "end": "2152400"
  },
  {
    "text": "that is the aggregation Geo agregation code you deployed as a job on this cluster",
    "start": "2152400",
    "end": "2158560"
  },
  {
    "text": "you might have counts and you want to serve it through the dashboard app so now you use your external database and",
    "start": "2158560",
    "end": "2165200"
  },
  {
    "text": "then you finally have your dashboard app that reads the counts and then highlights it on that",
    "start": "2165200",
    "end": "2170520"
  },
  {
    "text": "UI now contrast this with vision two you have your Kafka cluster that hosts all",
    "start": "2170520",
    "end": "2176359"
  },
  {
    "text": "those activity events you just have your dashboard app what it does is it embeds the streams Library it processes it",
    "start": "2176359",
    "end": "2183319"
  },
  {
    "text": "counts those and it stores the counts in the local embedded engine the local",
    "start": "2183319",
    "end": "2188680"
  },
  {
    "text": "state engines and CCS API allows the local engines to be quer queriable so in",
    "start": "2188680",
    "end": "2194680"
  },
  {
    "text": "your dashboard app not only are you querying or not only are you storing all those uh State or the aggregated numbers",
    "start": "2194680",
    "end": "2202200"
  },
  {
    "text": "you're also able to query it and display it on your dashboard so in this picture I think it probably summarizes all of",
    "start": "2202200",
    "end": "2208960"
  },
  {
    "text": "what kafka's streams API is about is essentially",
    "start": "2208960",
    "end": "2214160"
  },
  {
    "text": "Simplicity now what I'd like to you know conclude with is a lot of these ideas",
    "start": "2214520",
    "end": "2220520"
  },
  {
    "text": "this um this observation about what batch processing really is you know it is it is a style of processing where you",
    "start": "2220520",
    "end": "2227920"
  },
  {
    "text": "can take a window of data and you process it and after you're done with that window you shut down then you wake",
    "start": "2227920",
    "end": "2233119"
  },
  {
    "text": "up at a particular future point in time you process the next window and you keep doing that so let's assume that a you",
    "start": "2233119",
    "end": "2240800"
  },
  {
    "text": "know streams app can do that you know it processes window and then it shuts down now contrast that with um you know",
    "start": "2240800",
    "end": "2248839"
  },
  {
    "text": "traditional streams app which is that when it finishes processing that Windows it doesn't shut down it keeps going on",
    "start": "2248839",
    "end": "2255119"
  },
  {
    "text": "and it keeps processing the next window as it arrives so you'll realize that you",
    "start": "2255119",
    "end": "2260760"
  },
  {
    "text": "know this is nothing but a different style of processing on the same abstraction essentially logs actually",
    "start": "2260760",
    "end": "2266920"
  },
  {
    "text": "help unify batch and stream processing on a single reusable",
    "start": "2266920",
    "end": "2272359"
  },
  {
    "text": "layer so to conclude you know this is um the streams in the connect API with",
    "start": "2272359",
    "end": "2277800"
  },
  {
    "text": "Kafka it really encapsulates everything about what streaming ETL means and looks",
    "start": "2277800",
    "end": "2285000"
  },
  {
    "text": "like and remember this messy picture it helps you actually clean that up and end",
    "start": "2285000",
    "end": "2291760"
  },
  {
    "text": "up with a much more you know scalable and um manageable ETL",
    "start": "2291760",
    "end": "2297880"
  },
  {
    "text": "architecture this is the vision that we started Kafka with this is the vision that some of us have at confluent is",
    "start": "2297880",
    "end": "2303800"
  },
  {
    "text": "really to make all your data available everywhere and and",
    "start": "2303800",
    "end": "2309280"
  },
  {
    "text": "immediately if you were intrigued about some of these ideas if it was uh you know too much uh too much too soon then",
    "start": "2309280",
    "end": "2316359"
  },
  {
    "text": "we have a tutorial on Friday if you were uh to dive deeper into connect and streams one of the conflent members is",
    "start": "2316359",
    "end": "2322680"
  },
  {
    "text": "going to walk you through that we blog a lot about these ideas so if you hit",
    "start": "2322680",
    "end": "2327720"
  },
  {
    "text": "confluent doio blog you will be able to catch up uh to the latest of what we are all up to and with that I'd like to",
    "start": "2327720",
    "end": "2334920"
  },
  {
    "text": "conclude this talk thank you very much for listening",
    "start": "2334920",
    "end": "2339440"
  }
]