[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "[Applause] so hey everybody thank you so much for",
    "start": "11230",
    "end": "16520"
  },
  {
    "text": "coming to this talk the EDG has become a very popular application of distributed systems but",
    "start": "16520",
    "end": "23359"
  },
  {
    "text": "what exactly is The Edge The Edge is simply something that runs very close to clients",
    "start": "23359",
    "end": "29000"
  },
  {
    "text": "geographically so when we think of services such as DNS resolution content",
    "start": "29000",
    "end": "34520"
  },
  {
    "text": "delivery and IP firewalling which are close to the client these are things that we consider near the edge and",
    "start": "34520",
    "end": "41120"
  },
  {
    "text": "traditionally we have considered things like relational databases which have lived closer in centralized uh data",
    "start": "41120",
    "end": "47360"
  },
  {
    "text": "centers which are further away from clients so we can see that the edge is really just a relative metric where",
    "start": "47360",
    "end": "54039"
  },
  {
    "text": "services that run closer to the clients are considered more edgy location based",
    "start": "54039",
    "end": "59320"
  },
  {
    "text": "Network lat and costs are massively reduced when we can serve requests from clients at the edge of the network",
    "start": "59320",
    "end": "65880"
  },
  {
    "text": "closer to where they are so hi everybody my name is Justin and I'm a software engineering intern at",
    "start": "65880",
    "end": "72520"
  },
  {
    "text": "Apple building out services for icloud's Edge Network previously I worked at cloudflare as a system software engineer",
    "start": "72520",
    "end": "79759"
  },
  {
    "text": "focusing on storage infrastructure multi-tenant resource isolation and even databased internals where VES was my",
    "start": "79759",
    "end": "85960"
  },
  {
    "text": "super amazing engineering manager and then VES is a VES has been an",
    "start": "85960",
    "end": "92840"
  },
  {
    "text": "engineering manager at Cloud Flare from 2019 up until now he's been running the entire uh software storage",
    "start": "92840",
    "end": "99520"
  },
  {
    "text": "infrastructure org as well as the database team and he's also worked on the a startup that he's working on",
    "start": "99520",
    "end": "105320"
  },
  {
    "text": "called spin up. host he also is a member of pgus and from 2015 2016 to 2019 VES",
    "start": "105320",
    "end": "113640"
  },
  {
    "text": "gained a lot of database engineering experience uh at the ground level as a databased engineer at Ticket Master",
    "start": "113640",
    "end": "121159"
  },
  {
    "text": "so today I'm excited to share with you guys how we design and operate our entire High availability database",
    "start": "122039",
    "end": "128000"
  },
  {
    "text": "architecture at the edge together we'll break down in high detail the high",
    "start": "128000",
    "end": "133160"
  },
  {
    "text": "availability setup and consider the trade-offs that we had to make around each part of the system then we'll explore some of the",
    "start": "133160",
    "end": "139760"
  },
  {
    "text": "interesting Performance challenges that we faced when bringing our database infrastructure closer than ever to the",
    "start": "139760",
    "end": "145080"
  },
  {
    "text": "edge and dive deep into the solutions that we've implemented we'll finally take a sneak",
    "start": "145080",
    "end": "150200"
  },
  {
    "text": "peek at some of the really interesting recurring patterns that we see emerging in relational data storage collocated at",
    "start": "150200",
    "end": "155640"
  },
  {
    "text": "the edge so today just some quick numbers cloudfare is the market leader in",
    "start": "155640",
    "end": "161400"
  },
  {
    "text": "network security performance and Edge Computing so for some cool numbers let's give you guys an idea of what the scale we're dealing with we have over 27",
    "start": "161400",
    "end": "168599"
  },
  {
    "text": "million internet properties that depend on our protection which means that our Network needs to handle over 46 million",
    "start": "168599",
    "end": "174280"
  },
  {
    "text": "HTTP requests per second on average at any given time we leverage our Network point of presence locations spread",
    "start": "174280",
    "end": "180879"
  },
  {
    "text": "across 250 locations globally so this level of traffic translates into over 55",
    "start": "180879",
    "end": "187599"
  },
  {
    "text": "million row operations per second on average against our busiest postgress database cluster and that stores over 50",
    "start": "187599",
    "end": "193879"
  },
  {
    "text": "terabytes of data across all of our clusters now Cloud control plane is an",
    "start": "193879",
    "end": "200680"
  },
  {
    "text": "orchestration of many microservices that power our API our dashboard our control",
    "start": "200680",
    "end": "206599"
  },
  {
    "text": "plane manages the rules and configurations for critical Network Services along the data path so our",
    "start": "206599",
    "end": "212239"
  },
  {
    "text": "database clusters store things like DNS record changes firewall and DDOS mitigation rules API Gateway routes and",
    "start": "212239",
    "end": "218840"
  },
  {
    "text": "even data for internal services like filling entitlements and user off so we've commonly seen application",
    "start": "218840",
    "end": "225439"
  },
  {
    "text": "teams frequently use postgress in some pretty interesting ways since cloudfare began store procedures have been",
    "start": "225439",
    "end": "231400"
  },
  {
    "text": "commonly used to execute business logic as units of work the robust psql language enables intricate branching",
    "start": "231400",
    "end": "237799"
  },
  {
    "text": "logic which we've used for enabling disabling domain zones with transactional",
    "start": "237799",
    "end": "243560"
  },
  {
    "text": "consistency we've also commonly seen applications where service teams will use postc as an outbox queue for",
    "start": "243560",
    "end": "250000"
  },
  {
    "text": "capturing domain events one example is when DDOS rules are generated from traffic analysis in a",
    "start": "250000",
    "end": "256479"
  },
  {
    "text": "centralized data center they're first ritten into a postc table a separate Damon will typically",
    "start": "256479",
    "end": "262680"
  },
  {
    "text": "pull these events uh from post and pipe them through CFA uh to services at the",
    "start": "262680",
    "end": "268120"
  },
  {
    "text": "edge this means that latency critical Services can avoid directly hitting the database while enjoying the durability",
    "start": "268120",
    "end": "273880"
  },
  {
    "text": "that postgress and CFA gives so keep in mind that at Cloud",
    "start": "273880",
    "end": "280600"
  },
  {
    "text": "flurt we're running our entire software and service stack on our own bare metal hardware so unlike many other major",
    "start": "280600",
    "end": "287280"
  },
  {
    "text": "service providers we spent a great deal of time considering things like rack mounted servers network cards that power",
    "start": "287280",
    "end": "292800"
  },
  {
    "text": "our high bandwidth backbone and the operational maintenance required on premise data storage offers",
    "start": "292800",
    "end": "298840"
  },
  {
    "text": "us the highest degree of flexibility across the entire stack we can meticulous meticulously fine-tune",
    "start": "298840",
    "end": "304560"
  },
  {
    "text": "elements such as our solid state raid configuration and we've even implemented features in the open source cluster",
    "start": "304560",
    "end": "309919"
  },
  {
    "text": "management system and even have applied Custom Performance patches against postl",
    "start": "309919",
    "end": "315160"
  },
  {
    "text": "itself this level of transparency and control over our system would otherwise be impossible had we used any manage",
    "start": "315160",
    "end": "321560"
  },
  {
    "text": "vendors such as AWS RDS but unfortunately that also means",
    "start": "321560",
    "end": "326600"
  },
  {
    "text": "that there's no magical Auto scaling button that I can just push and immediately increase capacity when we start experiencing load and so I would",
    "start": "326600",
    "end": "333479"
  },
  {
    "text": "say this is one of the most challenging aspects of running Cloud fl's entire stack on premise",
    "start": "333479",
    "end": "340639"
  },
  {
    "text": "right so let's dive into Cloud flar database architecture that sits near the",
    "start": "343000",
    "end": "348560"
  },
  {
    "text": "edge we need to process transactions on the order of millions per second when designing and selecting the components",
    "start": "348560",
    "end": "354280"
  },
  {
    "text": "of our system we tried to consider what was most needed from an ideal architecture and so the most important",
    "start": "354280",
    "end": "359759"
  },
  {
    "text": "thing is high availability the control plan for our critical Services uh need to remain available 24/7 to protect our",
    "start": "359759",
    "end": "366280"
  },
  {
    "text": "customers initially we aim for we aim for an SLO of 59 of availability across all of our services and that means that",
    "start": "366280",
    "end": "373000"
  },
  {
    "text": "we get 5 minutes and a half of downtime per year across the entire software stack this gives our databases even less",
    "start": "373000",
    "end": "380240"
  },
  {
    "text": "leeway achieving a high level of availability requires our system to operate at immense scale and so our",
    "start": "380240",
    "end": "386039"
  },
  {
    "text": "transactional workloads typically skew on the read heavy side so our structure has to handle a high rate of read RPS",
    "start": "386039",
    "end": "391759"
  },
  {
    "text": "and write RPS with minimal latency as well as maintaining fault tolerance of course we want to talk we",
    "start": "391759",
    "end": "398319"
  },
  {
    "text": "also care about observability which is important in any distributive system but for the purpose of this talk we're leaving it out of",
    "start": "398319",
    "end": "405400"
  },
  {
    "text": "scope so we internally leverage our anycast Network which means that clients are naturally load balanced across RPG",
    "start": "405560",
    "end": "412039"
  },
  {
    "text": "bouncer proxy instances this is the first step of our journey in pushing the databases to the edge since queries",
    "start": "412039",
    "end": "417960"
  },
  {
    "text": "could be processed faster and close proximity to the clients while bgp ncast allows us to",
    "start": "417960",
    "end": "424360"
  },
  {
    "text": "proxy queries to the optimal region closest to the clients where they're deployed right queries still need to be",
    "start": "424360",
    "end": "430080"
  },
  {
    "text": "forwarded all the way back to the primary region where the primary database instance resides while read",
    "start": "430080",
    "end": "435160"
  },
  {
    "text": "queries can be locally served from the closest nearby region so here's a picture of our entire",
    "start": "435160",
    "end": "442599"
  },
  {
    "text": "massive architecture but let's just start from the very top at the proxy layer so at the top we have PG bouncer",
    "start": "442599",
    "end": "448800"
  },
  {
    "text": "this manages uh this manages database Server Connection pools used by our application clients queries are then",
    "start": "448800",
    "end": "454840"
  },
  {
    "text": "passed through ha proxy which are load balanced across several database cluster instances to prevent any single",
    "start": "454840",
    "end": "460800"
  },
  {
    "text": "postgress database from becoming overloaded with a disproportionate number of queries so in the middle a typical",
    "start": "460800",
    "end": "468199"
  },
  {
    "text": "postgress deployment contains a single primary instance which replicates data to multiple replicas and this is done to",
    "start": "468199",
    "end": "474599"
  },
  {
    "text": "support a high read of rate uh like read queries our primary database handles all of the rights and our asynchronous and",
    "start": "474599",
    "end": "481000"
  },
  {
    "text": "synchronous replicas handle reads our postgress cluster topologies",
    "start": "481000",
    "end": "486400"
  },
  {
    "text": "are managed by uh the high availability High availability cluster management tool called stolon it's backed by the",
    "start": "486400",
    "end": "492759"
  },
  {
    "text": "etcd distributed key value store which underlying it uses raft protocol for leadership consensus and election in the",
    "start": "492759",
    "end": "498879"
  },
  {
    "text": "case of primary failovers you'll notice that we have adopted an active standby model to",
    "start": "498879",
    "end": "504759"
  },
  {
    "text": "ensure that we have cross region data redundancy our primary region in Portland serves all inbound queries and",
    "start": "504759",
    "end": "511479"
  },
  {
    "text": "at the same time our standby region in Luxembourg is ready to handle all incoming traffic if we ever need to",
    "start": "511479",
    "end": "517200"
  },
  {
    "text": "evacuate the region or fail out this means that our primary postgress cluster",
    "start": "517200",
    "end": "522640"
  },
  {
    "text": "replicates data across regions to our standby as well so there's a lot going on in this highle diagram we'll dive",
    "start": "522640",
    "end": "529480"
  },
  {
    "text": "into each layer and disect each of the components and the trade-offs that we considered so I'm going to hand it off",
    "start": "529480",
    "end": "535519"
  },
  {
    "text": "to VES all right uh thanks Justin for the for",
    "start": "535519",
    "end": "542200"
  },
  {
    "text": "the for the nice introduction I appreciate that so we decided hey we have to build",
    "start": "542200",
    "end": "548720"
  },
  {
    "text": "this highly distributed and relational database using Edge so when we wanted to set on to do this we wanted to look at",
    "start": "548720",
    "end": "555959"
  },
  {
    "text": "what are the fundamental things that's going to basically Define the way we how how we Define the the architecture so",
    "start": "555959",
    "end": "563000"
  },
  {
    "text": "the as the first principles we looked at cap theorem since it's a distributed application which we know is going to be",
    "start": "563000",
    "end": "568839"
  },
  {
    "text": "uh distri geographically and especially using open source software postris and also",
    "start": "568839",
    "end": "575440"
  },
  {
    "text": "everything is built on our own data centers so just a quick recap on uh on",
    "start": "575440",
    "end": "582399"
  },
  {
    "text": "cap theorem you know for any distributed applications you can have only one one of the two either you can have",
    "start": "582399",
    "end": "588440"
  },
  {
    "text": "consistency or high availability and a lot of the times we pick availability",
    "start": "588440",
    "end": "593959"
  },
  {
    "text": "over consistency let's go over the cases where we pick the trade-offs and what are those trade-offs",
    "start": "593959",
    "end": "601320"
  },
  {
    "text": "so this is a typical architecture of how we how we deployed uh in a single data center there is a primary database and",
    "start": "602240",
    "end": "609000"
  },
  {
    "text": "then there is a synchronous replica and then if you look at it at least we have a couple of asynchronous replica the",
    "start": "609000",
    "end": "615000"
  },
  {
    "text": "same kind of topology is replicated across multiple data centers we have at least right now three core data centers",
    "start": "615000",
    "end": "621640"
  },
  {
    "text": "where where we have replicated at a real time so why do we pick this kind of a",
    "start": "621640",
    "end": "629320"
  },
  {
    "text": "semisynchronous replication topology where we have the primary database and at least one synchronous replica and a",
    "start": "629320",
    "end": "635040"
  },
  {
    "text": "bunch of asynchronous replica so in a typical situation where",
    "start": "635040",
    "end": "640399"
  },
  {
    "text": "we have like an asynchronous replica which goes down we don't have any problem with respect to the applications",
    "start": "640399",
    "end": "646399"
  },
  {
    "text": "the app applications can still continue working without any impact because it's just an asynchronous replica similar",
    "start": "646399",
    "end": "654760"
  },
  {
    "text": "similar tradeoffs be made for cross region replication let's say we have a application you know region in Europe as",
    "start": "654760",
    "end": "661800"
  },
  {
    "text": "well as in Asia if either of those two Goes Down Still our applications in the US can continue working",
    "start": "661800",
    "end": "667800"
  },
  {
    "text": "well however if the synchronous replica goes down we basically bring the entire",
    "start": "667800",
    "end": "673560"
  },
  {
    "text": "application to Halt because we didn't want her to uh take any any kind of reads and rights so especially rights",
    "start": "673560",
    "end": "680560"
  },
  {
    "text": "without making any tradeoffs pretty much every time when there is a failover that happens on the",
    "start": "680560",
    "end": "686120"
  },
  {
    "text": "primary database we pick the synchronous replica to be the new primary and that's the reason why we have this kind of a",
    "start": "686120",
    "end": "692639"
  },
  {
    "text": "semisynchronous uh replication topology that we bent with so as I mentioned earlier the there",
    "start": "692639",
    "end": "701120"
  },
  {
    "text": "are two two fundamental things one it's going to be based on top of postris and to everything on based on onr uh postr",
    "start": "701120",
    "end": "709120"
  },
  {
    "text": "by itself is not distributed database you know it's started in '90s in Berkeley and uh it's been designed with",
    "start": "709120",
    "end": "715800"
  },
  {
    "text": "monolithic architecture at mind so how are we going to take this postest into a",
    "start": "715800",
    "end": "720959"
  },
  {
    "text": "distributed post two things uh one we we rely",
    "start": "720959",
    "end": "726519"
  },
  {
    "text": "heavily on the replication aspects one is The Logical replication and the other one is uh streaming",
    "start": "726519",
    "end": "732160"
  },
  {
    "text": "replication there is a third derivative what I call the cascading replication which is basically built on top of the",
    "start": "732160",
    "end": "738920"
  },
  {
    "text": "the above two ones either logical or streaming so before we get into the",
    "start": "738920",
    "end": "745120"
  },
  {
    "text": "replication and how how it's useful for us to build the build the distributor post I wanted to have a you know quick",
    "start": "745120",
    "end": "752279"
  },
  {
    "text": "primer on right lcks how many of you aware of right logs that's pretty good uh for for some",
    "start": "752279",
    "end": "759040"
  },
  {
    "text": "of them you know just just a quick overview so I mean the in general like you know",
    "start": "759040",
    "end": "764959"
  },
  {
    "text": "in any any database not just in postris whether you pick uh my SQL or Oracle all",
    "start": "764959",
    "end": "771760"
  },
  {
    "text": "this fundamentally relational databased Management Systems achieves the durability in assd using right locks in",
    "start": "771760",
    "end": "779240"
  },
  {
    "text": "a simple terms whenever you make any change to your database it doesn't need to be synced directly on your file",
    "start": "779240",
    "end": "785560"
  },
  {
    "text": "system or the database files rather you can apply capture those changes sequentially in a in a in a in a",
    "start": "785560",
    "end": "791720"
  },
  {
    "text": "basically a lock file and then those lock files are synced to your database files you know in in a in an",
    "start": "791720",
    "end": "797680"
  },
  {
    "text": "asynchronous fashion so this provides the durability options that we pretty much need in any database systems and",
    "start": "797680",
    "end": "805720"
  },
  {
    "text": "that's pretty much turned out to be a really good feature for us to to build a replication Mission or replication uh",
    "start": "805720",
    "end": "811839"
  },
  {
    "text": "systems where we capture these changes and then replicate out to another",
    "start": "811839",
    "end": "817680"
  },
  {
    "text": "replica all right so going into the streaming replication mode so in in streaming replication mode it's pretty",
    "start": "818320",
    "end": "824160"
  },
  {
    "text": "simple um replica basically creates a TCP connection and streams pretty much every log entry from operations within a",
    "start": "824160",
    "end": "831600"
  },
  {
    "text": "transaction to another replica and and the interesting part with this uh streaming replication is",
    "start": "831600",
    "end": "837680"
  },
  {
    "text": "that it's pretty performance in the sense that we can pretty much capture any changes that happens at pretty much",
    "start": "837680",
    "end": "843759"
  },
  {
    "text": "like 1 terab per second changes even to the to the replicas in the on the other other",
    "start": "843759",
    "end": "850160"
  },
  {
    "text": "end uh minimal delay so pretty much I mean well there are delays we have seen",
    "start": "850160",
    "end": "855839"
  },
  {
    "text": "um just we go over some of the delays and how you can create those delays but in general that that it's uh it's pretty",
    "start": "855839",
    "end": "862920"
  },
  {
    "text": "performant um the other other caveat with streaming application is it's It's All or Nothing so if you set up a",
    "start": "862920",
    "end": "870560"
  },
  {
    "text": "replica then you have to pretty much replicate every single data because it's based on the file system level Block",
    "start": "870560",
    "end": "875920"
  },
  {
    "text": "Level there is no way for us to you know at least so far optimize which data is being",
    "start": "875920",
    "end": "882639"
  },
  {
    "text": "replicated the more newer version of replication is logical replication uh",
    "start": "884279",
    "end": "890440"
  },
  {
    "text": "you know as as the name suggests it's more at The Logical level instead of the Block Level or the foundation level",
    "start": "890440",
    "end": "896000"
  },
  {
    "text": "where you are replicating data by block by blocks rather in logical replication you are replicating data at the SQL",
    "start": "896000",
    "end": "902680"
  },
  {
    "text": "level where each SQL statements that are captured at the end of the transaction and they are like replicated on the",
    "start": "902680",
    "end": "908440"
  },
  {
    "text": "other hand more like if you can think about like a publisher and subscriber option uh definitely more flexible",
    "start": "908440",
    "end": "916160"
  },
  {
    "text": "compared to the streaming replication here you can create uh Publishers and subscribers even at a table level and",
    "start": "916160",
    "end": "923360"
  },
  {
    "text": "even more granular at a row level and on a column level so you have like a much more flexib than let's say a streamer",
    "start": "923360",
    "end": "930319"
  },
  {
    "text": "application provides however it has two caveats that that really you know that that's a",
    "start": "930319",
    "end": "937639"
  },
  {
    "text": "hindrance for us at least as of now to adopt logical replication the biggest one is the schema changes no ddl changes",
    "start": "937639",
    "end": "944680"
  },
  {
    "text": "are replicated so you have to kind of figure out some way build like custom tools where when you have to make some",
    "start": "944680",
    "end": "950240"
  },
  {
    "text": "changes replicate those changes to all the other replicas that's just not easy",
    "start": "950240",
    "end": "955800"
  },
  {
    "text": "the the other bigger problem is that logical replication is still not performant at",
    "start": "955800",
    "end": "960920"
  },
  {
    "text": "scale for a cluster probably you know um maybe gigabytes of size maybe it's good",
    "start": "960920",
    "end": "967040"
  },
  {
    "text": "but not at least at the terabyte multiple terabyte scale logical replication is not yet",
    "start": "967040",
    "end": "973680"
  },
  {
    "text": "there right um cluster management",
    "start": "973880",
    "end": "978920"
  },
  {
    "text": "so yeah pretty much you know uh one of the biggest reasons we would have a cluster management as we all know",
    "start": "983639",
    "end": "990480"
  },
  {
    "text": "database failures right whether we like it or not failures do happen for for",
    "start": "990480",
    "end": "995720"
  },
  {
    "text": "multitude of reasons right like as simple as that a we have a logical failure like a data corruption happened",
    "start": "995720",
    "end": "1001440"
  },
  {
    "text": "on one of the primary databases and what do we do or or even more like severe",
    "start": "1001440",
    "end": "1006759"
  },
  {
    "text": "things like natural disasters and and and whatnot so in any of those situations we have to trigger a failover",
    "start": "1006759",
    "end": "1013360"
  },
  {
    "text": "and doing them manually is not fun and another fact you know uh at least 5% of",
    "start": "1013360",
    "end": "1019279"
  },
  {
    "text": "hardware at any given time is kind of faulty pretty much if you think about it on a fleet of like multiple thousands of",
    "start": "1019279",
    "end": "1025438"
  },
  {
    "text": "servers across multiple data centers at pretty much any point in time you have like some kind of failures that is",
    "start": "1025439",
    "end": "1031558"
  },
  {
    "text": "always going on it's could be as simple as like a raid failure on a dis all the way up to like availability Zone going",
    "start": "1031559",
    "end": "1037959"
  },
  {
    "text": "down so how are you going to be prepared for these failures without having a good cluster",
    "start": "1037959",
    "end": "1044559"
  },
  {
    "text": "management so we picked stolon which is a pretty thin layer running on top of the post clusters itself it's open",
    "start": "1044720",
    "end": "1051400"
  },
  {
    "text": "source software written in go started in probably I guess 201 18 or 19 it started",
    "start": "1051400",
    "end": "1056600"
  },
  {
    "text": "picking up um the the the features that really brought us to use cholon is one",
    "start": "1056600",
    "end": "1062919"
  },
  {
    "text": "it's post native right you can you can it's it speaks pois and and also it",
    "start": "1062919",
    "end": "1069280"
  },
  {
    "text": "supports like multiple side redundancy like in the sense that you can deploy a single stolon cluster which is",
    "start": "1069280",
    "end": "1075480"
  },
  {
    "text": "distributed across multiple like post clusters those clusters can be located",
    "start": "1075480",
    "end": "1080679"
  },
  {
    "text": "in single region or even they can be distributed across multiple regions um there are a few more you know",
    "start": "1080679",
    "end": "1086840"
  },
  {
    "text": "interesting aspects one it pro it provides like a failover um obviously that's what you would expect from your",
    "start": "1086840",
    "end": "1093240"
  },
  {
    "text": "cluster management Tool uh but but the good thing is it's more stable in the sense we have seen very less like you",
    "start": "1093240",
    "end": "1099919"
  },
  {
    "text": "know false false",
    "start": "1099919",
    "end": "1103080"
  },
  {
    "text": "positives so digging deep onto our cluster man agement the first component is keeper keepers are basically the the",
    "start": "1105400",
    "end": "1112679"
  },
  {
    "text": "the the parent process which manages the the postest itself any changes that has to happen has to go through this keeper",
    "start": "1112679",
    "end": "1119640"
  },
  {
    "text": "so you can you can think about like keepers as basically the the postest",
    "start": "1119640",
    "end": "1125159"
  },
  {
    "text": "process then there is the Sentinels which are basically the I I would consider them as as the as the",
    "start": "1125159",
    "end": "1132200"
  },
  {
    "text": "orchestrator which looks at the health checks of each components of postr and then makes decisions such as like hey is",
    "start": "1132200",
    "end": "1138440"
  },
  {
    "text": "the is the primary is healthy or should we like you know should we elect should we start an election and figure out who",
    "start": "1138440",
    "end": "1145000"
  },
  {
    "text": "the new primary should be and finally the proxy layer where all",
    "start": "1145000",
    "end": "1151559"
  },
  {
    "text": "the clients connect to it and then proxy layers make sure that you know where is the right primary is you know we should",
    "start": "1151559",
    "end": "1157360"
  },
  {
    "text": "avoid any kind of let's say like multimaster situation",
    "start": "1157360",
    "end": "1162360"
  },
  {
    "text": "Etc so over to Justin where he's going to dig down into the connection pooling and the ha aspect",
    "start": "1162720",
    "end": "1170720"
  },
  {
    "text": "nice so finally database connections are a finite resource which means that we need to manage them",
    "start": "1174039",
    "end": "1181280"
  },
  {
    "text": "efficiently now we all know that there's overhead when you go and open a new TCP connection or postgress connection but",
    "start": "1181280",
    "end": "1187240"
  },
  {
    "text": "what's actually happening under the hood well first postgress connections are built on top of the TCP protocol so",
    "start": "1187240",
    "end": "1192679"
  },
  {
    "text": "requires that three-way handshake following this we also have an SSL handshake to secure that communication",
    "start": "1192679",
    "end": "1198840"
  },
  {
    "text": "also since postrest dedicates a separate operating system level process for each proc for each connection the main",
    "start": "1198840",
    "end": "1205080"
  },
  {
    "text": "postmaster process has to Fork itself to execute queries for the connection this forking as we all know involves a new",
    "start": "1205080",
    "end": "1210880"
  },
  {
    "text": "page allocations and copying memory from the parent process to the new child address bace so in other words each",
    "start": "1210880",
    "end": "1216760"
  },
  {
    "text": "connection requires a non-zero amount of time of CPU time and this consumes finite resources such as",
    "start": "1216760",
    "end": "1223440"
  },
  {
    "text": "memory now when we have thousands of these connections open concurrently these three steps start eating up a lot",
    "start": "1223440",
    "end": "1228760"
  },
  {
    "text": "of CPU time that should otherwise be used for transaction processing in the execution",
    "start": "1228760",
    "end": "1234000"
  },
  {
    "text": "engine so a server side connection Pooler will open a finite number of connections against the server data the",
    "start": "1234000",
    "end": "1240240"
  },
  {
    "text": "database server itself while exposing a blackbox interface that matches the same wire protocol that postgress",
    "start": "1240240",
    "end": "1247280"
  },
  {
    "text": "supports so clients can connect to the same connect to the Pooler the same way they would connect to the database for",
    "start": "1247280",
    "end": "1252919"
  },
  {
    "text": "queries to be sent managing pools in a separate server allows us to recycle previously open",
    "start": "1252919",
    "end": "1258880"
  },
  {
    "text": "connections while minimizing the total number of open connections uh in the database server but it also allows us to",
    "start": "1258880",
    "end": "1265320"
  },
  {
    "text": "centrally control tenant resources for each client such as the number of allocated",
    "start": "1265320",
    "end": "1270840"
  },
  {
    "text": "connections and so internally we've actually done a lot of work in foring post PG Bouncer and adding a lot of",
    "start": "1270840",
    "end": "1276919"
  },
  {
    "text": "interesting features that take inspiration from Vegas uh congestion control algorithms to actually give us",
    "start": "1276919",
    "end": "1282679"
  },
  {
    "text": "stricter multitenant resource isolation but that's also a little bit out of the scope for this talk",
    "start": "1282679",
    "end": "1288919"
  },
  {
    "text": "so in our specific setup we opted for PG bouncer as our connection Pooler like we said PG bouncer has the same wire",
    "start": "1288919",
    "end": "1295120"
  },
  {
    "text": "protocol as postgress clients just connect to it and submit their queries like they normally would we particularly",
    "start": "1295120",
    "end": "1300559"
  },
  {
    "text": "chose PG bouncer because it uh to Shield clients from the complexity of database switches and fail overs so instead of",
    "start": "1300559",
    "end": "1307120"
  },
  {
    "text": "clients having to think about Hey where's the new database when it failed over where should I connect to instead",
    "start": "1307120",
    "end": "1312559"
  },
  {
    "text": "PG bouncer manages this and abstracts it away PG bouncer operates as a very lightweight single process server which",
    "start": "1312559",
    "end": "1319480"
  },
  {
    "text": "handles Network IO asynchronously so this allows it to handle a much higher uh number of",
    "start": "1319480",
    "end": "1324760"
  },
  {
    "text": "concurrent client connections as opposed to postgress PG bouncer introduces an",
    "start": "1324760",
    "end": "1331200"
  },
  {
    "text": "abstract concept known as client side connections now this isn't the exact same as a this is not the same as a",
    "start": "1331200",
    "end": "1336760"
  },
  {
    "text": "direct Server postgress Connection when a client establishes a new connection it obtains one of these client side",
    "start": "1336760",
    "end": "1343000"
  },
  {
    "text": "connections from PG bouncer PG bouncer then multiplexes these queries in an NTM fashion where queries originating from",
    "start": "1343000",
    "end": "1349360"
  },
  {
    "text": "various client side connections are relayed against across actual postgress server side connections and they go",
    "start": "1349360",
    "end": "1355000"
  },
  {
    "text": "through a proxy so why is PG bouncer so",
    "start": "1355000",
    "end": "1360159"
  },
  {
    "text": "lightweight well PG bouncer employs a non-blocking model to handle networ IO efficiently so unlike the traditional",
    "start": "1360159",
    "end": "1366799"
  },
  {
    "text": "approach of allocating a separate thread or process for each TCP connection such that uh that postest follows for compute",
    "start": "1366799",
    "end": "1374120"
  },
  {
    "text": "B requests PG bouncer utilizes a single thread that just executes a single Loop meaning that only we we only need one",
    "start": "1374120",
    "end": "1380799"
  },
  {
    "text": "thread stack space for all the required connections there's however a bit more complexity in managing the state for all",
    "start": "1380799",
    "end": "1386320"
  },
  {
    "text": "of these clients the loop monitors sockets for read and write events using the eull",
    "start": "1386320",
    "end": "1391840"
  },
  {
    "text": "mechanism so instead of pulling actively and spawning a new thread on each new connection in user space we just need to",
    "start": "1391840",
    "end": "1398400"
  },
  {
    "text": "tell the colel hey these are the file socket descriptors that I want monitored and let the colonel do the rest we then",
    "start": "1398400",
    "end": "1404559"
  },
  {
    "text": "call eole weight in PG bouncer which then just tells the operating system put me this S I don't need to do anything until you let me know that the uh put me",
    "start": "1404559",
    "end": "1412159"
  },
  {
    "text": "to sleep and until you let me know that TCP packets have arrived to this file descriptor and tell me uh and notify me",
    "start": "1412159",
    "end": "1418360"
  },
  {
    "text": "and then the colonel just simply raises a hardware interrupt and provides PG bouncer with an Associated file",
    "start": "1418360",
    "end": "1424039"
  },
  {
    "text": "descriptor so we can process logic in user space such as making an authentication call to postgress uh PG",
    "start": "1424039",
    "end": "1430039"
  },
  {
    "text": "Shadow before forwarding off the query making it a memory efficient solution for managing and relaying between a",
    "start": "1430039",
    "end": "1435799"
  },
  {
    "text": "large number of client to server connections one of the interesting challenges we've",
    "start": "1435799",
    "end": "1441039"
  },
  {
    "text": "had with setting up uh PG bouncer is having multiple single-threaded PG bouncer processes uh utilize all the CPU",
    "start": "1441039",
    "end": "1447400"
  },
  {
    "text": "cores on a single machine because it is a single process uh it's a single process program however we wanted them",
    "start": "1447400",
    "end": "1453960"
  },
  {
    "text": "to all listen on the same port so how could we reduce complexity for application teams where they don't have",
    "start": "1453960",
    "end": "1459559"
  },
  {
    "text": "to think about oh which Port should I go to for a specific PG bouncer process luckily we looked to the operating",
    "start": "1459559",
    "end": "1466279"
  },
  {
    "text": "system again for a solution and found that you can buy multiple TCP sockets from different processes to the same",
    "start": "1466279",
    "end": "1471679"
  },
  {
    "text": "port so we've actually patched PG bouncer in open source to use the ReUse Port socket option when opening a new",
    "start": "1471679",
    "end": "1478159"
  },
  {
    "text": "TCP socket to listen on and finally let's just chat about",
    "start": "1478159",
    "end": "1483720"
  },
  {
    "text": "load balancing so we use load balancers which front our postgress instances to distribute incoming database queries",
    "start": "1483720",
    "end": "1489919"
  },
  {
    "text": "across multiple postgress servers specifically we've chosen ha proxy as our load balancer using round",
    "start": "1489919",
    "end": "1497279"
  },
  {
    "text": "robin load balancing has an effective way to distribute queries evenly across postgress instances while preventing any",
    "start": "1497279",
    "end": "1503440"
  },
  {
    "text": "single instance from becoming overwhelmed similar to PG bouncer a load balancer such as ha proxy also provides",
    "start": "1503440",
    "end": "1510480"
  },
  {
    "text": "High availability and fault tolerance by automating automatically routing traffic away from failed or unresponsive",
    "start": "1510480",
    "end": "1516559"
  },
  {
    "text": "postgress servers to healthy ones which any downtime from degraded in uh which avoids any downtime from degraded",
    "start": "1516559",
    "end": "1522880"
  },
  {
    "text": "database instances we use ha proxy to Rel TCP connections at layer 4 for incing",
    "start": "1522880",
    "end": "1529039"
  },
  {
    "text": "minimal overhead and fun fact it uses the kernel splice system call which just attaches an inbound and outbound TCP",
    "start": "1529039",
    "end": "1535039"
  },
  {
    "text": "stream in the kernel and this means that data received from the inbound tcv socket is immediately transferred to",
    "start": "1535039",
    "end": "1540799"
  },
  {
    "text": "that outbound socket and forwarded without having to be copied into user space so now we get to the fun part",
    "start": "1540799",
    "end": "1548080"
  },
  {
    "text": "we're going to dive into some of the interesting problems and challenges across our database infrastructure and here some of the cool performance uh",
    "start": "1548080",
    "end": "1554159"
  },
  {
    "text": "tips and tricks that helped us in our journey to high availability at the edge by the way I think that's an actual",
    "start": "1554159",
    "end": "1560399"
  },
  {
    "text": "image of VES when we see one of these problems so first off replication lag we",
    "start": "1560399",
    "end": "1568120"
  },
  {
    "text": "found that our postgress replication lag becomes extremely pronounced under heavy traffic especially for applications who",
    "start": "1568120",
    "end": "1574279"
  },
  {
    "text": "have many auxiliary or redundant data structures and schemas such as bloated indices in these cases right operations",
    "start": "1574279",
    "end": "1580640"
  },
  {
    "text": "become Amplified because we're updating them in the primary rebalancing our index B plus trees on dis and we also",
    "start": "1580640",
    "end": "1585919"
  },
  {
    "text": "need to replicate that as well other bulk heavy read oper WR operations such as ETL jobs data migrations and even",
    "start": "1585919",
    "end": "1593679"
  },
  {
    "text": "Mass solutions for gdpr compliance are also common offenders so another interesting cause",
    "start": "1593679",
    "end": "1599840"
  },
  {
    "text": "of replication like is automatic storage compaction otherwise known as autovacuum in postgress this just executes on",
    "start": "1599840",
    "end": "1606039"
  },
  {
    "text": "interval so when the data is deleted fix list Tuple slots become fragmented on this so we have this process that goes",
    "start": "1606039",
    "end": "1611919"
  },
  {
    "text": "ahead and cleans those uh and cleans up those fixed width data data slots and so that avoids any sort of fragmentation",
    "start": "1611919",
    "end": "1620240"
  },
  {
    "text": "now replication lag is an unavoidable problem in any replicated distributed System since data needs to cross the",
    "start": "1621760",
    "end": "1627360"
  },
  {
    "text": "network path internally we in we target an SLO of 60 seconds of replication lag",
    "start": "1627360",
    "end": "1633159"
  },
  {
    "text": "and we till this to the Upstream application teams which we work with so to minimize the replication leg",
    "start": "1633159",
    "end": "1640880"
  },
  {
    "text": "we number one try to batch our SQL query rights into smaller chunks to avoid replicating large blocks of data all at",
    "start": "1640880",
    "end": "1647200"
  },
  {
    "text": "once one way we also sidestep replication leg and maintain read after write uh consistency is by caching or",
    "start": "1647200",
    "end": "1654159"
  },
  {
    "text": "reading directly after writing to the primary or the synchronous replica finally we can avoid replication",
    "start": "1654159",
    "end": "1660480"
  },
  {
    "text": "lag by simply ejecting all replicas from the cluster leaving only the primary no replicas means no replication lag right",
    "start": "1660480",
    "end": "1668320"
  },
  {
    "text": "now while this might sound insane this approach requires a deep understanding of tendance query workloads and",
    "start": "1668320",
    "end": "1673559"
  },
  {
    "text": "potential changes in volatility and so that might significantly change the system resilience so you can think of",
    "start": "1673559",
    "end": "1679960"
  },
  {
    "text": "this as kind of the unsafe keyword in Rust so another interesting incident one",
    "start": "1679960",
    "end": "1688039"
  },
  {
    "text": "of our largest public facing incidents happened back in 2020 where a series of cascading failures severely impacted our",
    "start": "1688039",
    "end": "1694679"
  },
  {
    "text": "databases performance and availability our public API Services experienced a drastic drop in",
    "start": "1694679",
    "end": "1701120"
  },
  {
    "text": "availability plummeting all the way down to 75% while our dashboards for other critical Network Services were becoming",
    "start": "1701120",
    "end": "1707159"
  },
  {
    "text": "80 times slower so from what we saw both regions primary",
    "start": "1707159",
    "end": "1713440"
  },
  {
    "text": "databases had executed a failover and promoted their synchronous replicas cleanly however the primary database in",
    "start": "1713440",
    "end": "1720519"
  },
  {
    "text": "the primary region soon started to crumble under the massive load of RPS traffic and so as a result this led to a",
    "start": "1720519",
    "end": "1727840"
  },
  {
    "text": "second failure of the new elected primary database and that left us with no more synchronous replicas to",
    "start": "1727840",
    "end": "1733799"
  },
  {
    "text": "promote now this is starting to sound like the beginning of a high availability nightmare",
    "start": "1733799",
    "end": "1738960"
  },
  {
    "text": "we found ourselves facing two choices we either promote an asynchronous replica and we risk potential data loss or we",
    "start": "1738960",
    "end": "1745519"
  },
  {
    "text": "suffer additional downtime by manually initiating a failover to our standby cluster in our standby region so for us",
    "start": "1745519",
    "end": "1751919"
  },
  {
    "text": "of course data loss is an unacceptable option and so we chose the latter",
    "start": "1751919",
    "end": "1757278"
  },
  {
    "text": "approach so what the heck just happened here well after further investigation we found that a network switch had",
    "start": "1757600",
    "end": "1763480"
  },
  {
    "text": "partially failed and was operating in degraded States the rack with the misbehaving switch included one server",
    "start": "1763480",
    "end": "1768840"
  },
  {
    "text": "in our etcd cluster which handled the leadership election so when that cluster leader when that cluster when a cluster",
    "start": "1768840",
    "end": "1774840"
  },
  {
    "text": "leader fails etcd us utilizes the raap protocol to maintain consistency and select a new leader for",
    "start": "1774840",
    "end": "1781360"
  },
  {
    "text": "promotion however there's a simplification of the raap protocol which all members just need to State whether they're available or unavailable",
    "start": "1781360",
    "end": "1788399"
  },
  {
    "text": "expecting them to provide accurate information or not at all so this works well for full failures like machine",
    "start": "1788399",
    "end": "1793880"
  },
  {
    "text": "crashes but you run into cases of undefined behavior when different nodes in your raft cluster start telling each other different things so in this case",
    "start": "1793880",
    "end": "1801000"
  },
  {
    "text": "we have node one and node two and node three which had degraded network switch so the network link is uh degraded",
    "start": "1801000",
    "end": "1806799"
  },
  {
    "text": "between them and so node one didn't think that node 3 was any more a leader but node one and node two still had a link and node two and node three still",
    "start": "1806799",
    "end": "1813080"
  },
  {
    "text": "had a link and node two acknowledged that node 3 was still a leader so in this in this uh in this",
    "start": "1813080",
    "end": "1820440"
  },
  {
    "text": "situation node ones tried to initiate many uh leadership elections and so it",
    "start": "1820440",
    "end": "1825480"
  },
  {
    "text": "would continuously vote for itself while no two still saw node 3 existed and would vote for node 3 so what happened",
    "start": "1825480",
    "end": "1831120"
  },
  {
    "text": "here well we get into a deadlock state where no leader is elected and when we get into a deadlock State then the",
    "start": "1831120",
    "end": "1837720"
  },
  {
    "text": "cluster becomes readon now because the cluster is readon clusters in both regions are no longer",
    "start": "1837720",
    "end": "1844440"
  },
  {
    "text": "able to communicate with each other and this initiated the both clusters to actually fail over to the primary",
    "start": "1844440",
    "end": "1850600"
  },
  {
    "text": "replicas that we saw now typically when we fail over the",
    "start": "1850600",
    "end": "1856039"
  },
  {
    "text": "synchronous replica is promoted but what happens the old primary well when we fail over the S",
    "start": "1856039",
    "end": "1861600"
  },
  {
    "text": "replic has promoted the old primary uh needs to actually begin undoing transactions that it has committed",
    "start": "1861600",
    "end": "1867760"
  },
  {
    "text": "because there could be some transaction history that could have diverged between the new primary and the old one while we",
    "start": "1867760",
    "end": "1873120"
  },
  {
    "text": "kept it available under a network partition after it unwinds all the way",
    "start": "1873120",
    "end": "1878559"
  },
  {
    "text": "back the diverge history is uh unwinding our diverge history kind of like a git Branch being undone the synchronous",
    "start": "1878559",
    "end": "1884639"
  },
  {
    "text": "replica needs to then send over uh and and basically receive and replay all the new and correct transactions that are",
    "start": "1884639",
    "end": "1890559"
  },
  {
    "text": "happening on the new and correct primary that it missed our primary failed once again",
    "start": "1890559",
    "end": "1896399"
  },
  {
    "text": "because we had no more synchronous replicas to absorb the RPS and so once the primary failed there was no longer a",
    "start": "1896399",
    "end": "1901679"
  },
  {
    "text": "synchronous replica meaning that we had downtime but also we need to figure out",
    "start": "1901679",
    "end": "1907039"
  },
  {
    "text": "why was it taking so long for the synchronous replica to try to replay all of those transaction logs from the new",
    "start": "1907039",
    "end": "1914559"
  },
  {
    "text": "primary so there's three things going on here I mean this is a pretty large scale issue right number one we have a",
    "start": "1914559",
    "end": "1920320"
  },
  {
    "text": "hardware failure that kind of is just an unfortunate situation that happens number two we had a Byzantine fault with",
    "start": "1920320",
    "end": "1927880"
  },
  {
    "text": "raft now this is a really rare event and it's known with the raft consensus protocol that this is a a faulty",
    "start": "1927880",
    "end": "1933440"
  },
  {
    "text": "situation but most implementations of the raap cluster or the raap consensus protocol will choose to use a simpler",
    "start": "1933440",
    "end": "1940679"
  },
  {
    "text": "algorithm versus something that us as cryptographic hashing but it's harder to understand and our third problem here",
    "start": "1940679",
    "end": "1947080"
  },
  {
    "text": "was that post was taking a very long time to resynchronize when the primary was sending back the uh the right ahead",
    "start": "1947080",
    "end": "1953399"
  },
  {
    "text": "logs for the new synchronous replica to replay so for us we decided to pick the",
    "start": "1953399",
    "end": "1959360"
  },
  {
    "text": "third option to solve because this was under the most control actually imp uh optimizing postest",
    "start": "1959360",
    "end": "1965880"
  },
  {
    "text": "internally so typically when you have the right ah headlock this is kind of what it looks like you have the original timeline with uh the new the original",
    "start": "1965880",
    "end": "1972919"
  },
  {
    "text": "history you have the old timeline which kind of diverges on its own path because it's no longer communicating with the",
    "start": "1972919",
    "end": "1979320"
  },
  {
    "text": "original one and receiving replica right ahead logs so what we actually found when",
    "start": "1979320",
    "end": "1984399"
  },
  {
    "text": "digging into our logs was that most of the time postcards was spent in our sync just copying over 1.5 terabytes of log",
    "start": "1984399",
    "end": "1991720"
  },
  {
    "text": "files from the from the primary to the old synchronous replica that was",
    "start": "1991720",
    "end": "1997840"
  },
  {
    "text": "resynchronizing so what the uh solution here well we optimiz postgress by instead of reading all the way back in",
    "start": "1997960",
    "end": "2004600"
  },
  {
    "text": "history of time and copying those files we just went back to the last of merg point where the timeline forks and then",
    "start": "2004600",
    "end": "2010039"
  },
  {
    "text": "we just need to copy off files from there that's like 5% of all the data that was originally",
    "start": "2010039",
    "end": "2016799"
  },
  {
    "text": "copied so doing this our replica rebuild time is reduced from 2 plus hours to",
    "start": "2016919",
    "end": "2022279"
  },
  {
    "text": "just 5 minutes so that's like a 95% speed up we also we also submitted our",
    "start": "2022279",
    "end": "2027799"
  },
  {
    "text": "patch for open source contribution and as another cool feature now we're able to resync and do this whole failover",
    "start": "2027799",
    "end": "2034360"
  },
  {
    "text": "resync in our cross cluster uh failover and resynchronization across multiple",
    "start": "2034360",
    "end": "2041039"
  },
  {
    "text": "regions so what are the key lessons of this very confusing but large scale incident well number one we need to",
    "start": "2041039",
    "end": "2047600"
  },
  {
    "text": "anticipate the effects of degraded State and not just fully failed State and so for this we actually have some internal",
    "start": "2047600",
    "end": "2053760"
  },
  {
    "text": "tools that do chaos experimentation randomly on a cluster uh which is kind of a little bit out of scope for this",
    "start": "2053760",
    "end": "2059079"
  },
  {
    "text": "talk but also another thing we found is that it pays uh it's quite a good",
    "start": "2059079",
    "end": "2064480"
  },
  {
    "text": "investment to go and build and fix software and open Source yourself you build the inhouse expertise that",
    "start": "2064480",
    "end": "2070599"
  },
  {
    "text": "otherwise would be very hard to find and it's better than just trying to co figure out oh we should just use this at separate tool because we're having",
    "start": "2070599",
    "end": "2076960"
  },
  {
    "text": "problems with this one you don't know if it's going to be any better and so that's our open source uh",
    "start": "2076960",
    "end": "2082839"
  },
  {
    "text": "commit that we're we sending off for postc commit Fest and I'm going to hand it back to",
    "start": "2082839",
    "end": "2089838"
  },
  {
    "text": "VES yep not fun right just maintaining one faster in one region you can think",
    "start": "2092720",
    "end": "2099640"
  },
  {
    "text": "about all the different failures like Hardware failures even some of the failures if they fail cleanly that's so",
    "start": "2099640",
    "end": "2105800"
  },
  {
    "text": "much better there are cases like these bison and Hardware failure where",
    "start": "2105800",
    "end": "2111119"
  },
  {
    "text": "components are you know not fully failed but they are actually operating in degraded state which is much more",
    "start": "2111119",
    "end": "2117800"
  },
  {
    "text": "challenging to figure out than actually you know building your systems and archite architecting for full failures",
    "start": "2117800",
    "end": "2125520"
  },
  {
    "text": "so now we take this monolithic monolithic like posterous clusters in in",
    "start": "2125520",
    "end": "2131160"
  },
  {
    "text": "couple of regions and distribute them across four regions or three regions and they will pretty much look",
    "start": "2131160",
    "end": "2138480"
  },
  {
    "text": "like from from an edge application let's take you know CLW workers standpoint view it will look like a cluster of",
    "start": "2138480",
    "end": "2146559"
  },
  {
    "text": "clusters so this is where we are currently heading down to so we had like primary region based out of pretty much",
    "start": "2148160",
    "end": "2154599"
  },
  {
    "text": "most of the time in the US we spread it to EU within you we have like couple of regions and then now we are branching to",
    "start": "2154599",
    "end": "2161599"
  },
  {
    "text": "Asia and pretty much all of them uses postrest streaming",
    "start": "2161599",
    "end": "2166720"
  },
  {
    "text": "replication especially using the The Hub and spoke model that we have the primary primary region which replicates to all",
    "start": "2166720",
    "end": "2173880"
  },
  {
    "text": "the regions there is no like a cascading replication involved here uh we have to watch out for all the",
    "start": "2173880",
    "end": "2180839"
  },
  {
    "text": "fun things that Justin mentioned about like replication lag keeping all of them in sync is important and also when there",
    "start": "2180839",
    "end": "2187400"
  },
  {
    "text": "is a very high replication lag we have to somehow make sure that uh we address",
    "start": "2187400",
    "end": "2193359"
  },
  {
    "text": "the the slas that we have provided to the application teams sometimes which means we have to divert the traffic to",
    "start": "2193359",
    "end": "2199680"
  },
  {
    "text": "all the way back to the primary region itself and uh having this kind of",
    "start": "2199680",
    "end": "2205920"
  },
  {
    "text": "distributed post is cool but what is what is the actual benefit like where can we you know kind of use the",
    "start": "2205920",
    "end": "2212280"
  },
  {
    "text": "strengths of of a distributed postgress so this is when we introduced",
    "start": "2212280",
    "end": "2219400"
  },
  {
    "text": "smart failovers look at look at this new topology and figure out which region",
    "start": "2219400",
    "end": "2225040"
  },
  {
    "text": "could be could be the primary one of the first attributes is like pretty much as",
    "start": "2225040",
    "end": "2230400"
  },
  {
    "text": "simple as like follow the sun hey keep your primary region keep moving as it",
    "start": "2230400",
    "end": "2235720"
  },
  {
    "text": "progresses for example like start with us and then when the day ends move your",
    "start": "2235720",
    "end": "2241119"
  },
  {
    "text": "primary region back to the Asia which pretty much solves the latency problem because you are right",
    "start": "2241119",
    "end": "2247920"
  },
  {
    "text": "are going to be much faster since your primary database is closer to the vast majority of population or client",
    "start": "2247920",
    "end": "2256119"
  },
  {
    "text": "applications I mean all we are trying to do is still we are working around the the fundamental limitation that post is",
    "start": "2256119",
    "end": "2262720"
  },
  {
    "text": "pretty much a single primary system if posters can be made like a multiple primary without a lot of the downsides",
    "start": "2262720",
    "end": "2269599"
  },
  {
    "text": "we don't actually need to do a lot of these dance uh the second one is sometimes we",
    "start": "2269599",
    "end": "2275359"
  },
  {
    "text": "have done with based on the capacity certain regions have more capacity than than other regions for example like",
    "start": "2275359",
    "end": "2281560"
  },
  {
    "text": "shipping Hardware to to a particular continent has become challenge especially during covid for example we",
    "start": "2281560",
    "end": "2286800"
  },
  {
    "text": "couldn't get Hardware to Asia in in time so we had to pretty much keep most of the time our primaries away from away",
    "start": "2286800",
    "end": "2293720"
  },
  {
    "text": "from Asia uh yeah the traffic which I already touched upon and the last one is the",
    "start": "2293720",
    "end": "2299440"
  },
  {
    "text": "complaints based uh where we we are getting into more and more challenges that a uh for regulatory reasons we have",
    "start": "2299440",
    "end": "2306599"
  },
  {
    "text": "to have certain kind of data in certain region and how do we do them and that's once again we can do that using using",
    "start": "2306599",
    "end": "2313119"
  },
  {
    "text": "smart ways so if you think okay well it's not",
    "start": "2313119",
    "end": "2319040"
  },
  {
    "text": "that simple seems like you have Post in one region and then put them out in five regions and and and whatnot well then",
    "start": "2319040",
    "end": "2325560"
  },
  {
    "text": "keep you know failing over and some of the optimization especially what we talked about earlier about PG rewind",
    "start": "2325560",
    "end": "2331640"
  },
  {
    "text": "using that we can rebuild the Clusters pretty much in less than one minute so you you have your primary region in in",
    "start": "2331640",
    "end": "2338880"
  },
  {
    "text": "US fail over to Asia what happens to your your your primary region in used to",
    "start": "2338880",
    "end": "2344520"
  },
  {
    "text": "be a primary region in US you can rebuild that within a minute which is good so that seems like we can be keep",
    "start": "2344520",
    "end": "2351640"
  },
  {
    "text": "doing this all day well it's to be honest it's not that",
    "start": "2351640",
    "end": "2357839"
  },
  {
    "text": "simple one of the biggest challenge that we ran into is the is the dependency graph right if you think about it well",
    "start": "2357839",
    "end": "2364640"
  },
  {
    "text": "just failing over your databases is not that hard at least fairly hard but still at seems like a solvable problem but the",
    "start": "2364640",
    "end": "2371640"
  },
  {
    "text": "biggest challenge comes in when you bring in your applications inside this how do we do just your databases without",
    "start": "2371640",
    "end": "2378880"
  },
  {
    "text": "failing over your entire applications especially nowadays applications are duct taped like with multiple components",
    "start": "2378880",
    "end": "2385599"
  },
  {
    "text": "like message cues and Kafka and and you know anal analytical databases it's just not easy so anyone who is here thinking",
    "start": "2385599",
    "end": "2392760"
  },
  {
    "text": "about like hey I'm just going to do this cross region and and I'm going to like you know fail over just look at your",
    "start": "2392760",
    "end": "2397880"
  },
  {
    "text": "application dependency graph even not just like first layer of application dependency you have to like go all the",
    "start": "2397880",
    "end": "2404359"
  },
  {
    "text": "way down like track down all the dependency layer because even the the last one of the dependencies is still",
    "start": "2404359",
    "end": "2410400"
  },
  {
    "text": "going to like you know suffer from a network",
    "start": "2410400",
    "end": "2414240"
  },
  {
    "text": "call yeah the other big aspect is it requires a lot of coordination at least we haven't still completely automated so",
    "start": "2416760",
    "end": "2423440"
  },
  {
    "text": "we have like jump on a bridge call across 20 different team members from different different teams figuring out",
    "start": "2423440",
    "end": "2430440"
  },
  {
    "text": "how to do this yeah in our case it's something",
    "start": "2430440",
    "end": "2435960"
  },
  {
    "text": "like this a dependency graph for example like you have databases that you know pretty much the first layer of or like",
    "start": "2435960",
    "end": "2441720"
  },
  {
    "text": "you know level zero of dependency then you have like fundamental systems like for example entitlements like building",
    "start": "2441720",
    "end": "2447920"
  },
  {
    "text": "and authentication and configuration which pretty much uh you know being used to buy R of let's say you know um Like A",
    "start": "2447920",
    "end": "2455079"
  },
  {
    "text": "Primitive applications like R2 and SSL and",
    "start": "2455079",
    "end": "2460480"
  },
  {
    "text": "whatnot finally I just want to you know quickly touch upon the database strengths you know especially as a",
    "start": "2460560",
    "end": "2465599"
  },
  {
    "text": "practitioner that I've been looking at and have some kind of knowledge one uh",
    "start": "2465599",
    "end": "2471200"
  },
  {
    "text": "we are looking at more of providing embed data The Edge where we are still keeping your monolithic postas at at",
    "start": "2471200",
    "end": "2477640"
  },
  {
    "text": "least like three to four regions and then bring a lot of those uh key data to",
    "start": "2477640",
    "end": "2483640"
  },
  {
    "text": "uh embeded database for example SQL light so we can keep your pro as",
    "start": "2483640",
    "end": "2489319"
  },
  {
    "text": "somewhat monolithic as or as edgy and then keep real Edge using your SQL light",
    "start": "2489319",
    "end": "2496160"
  },
  {
    "text": "and and especially when you are using SQL light at the edge make sure that it is still like postc crust wire",
    "start": "2496160",
    "end": "2501560"
  },
  {
    "text": "compatible so there are open source projects where they take the the postr wire wire protocol and then uh replace",
    "start": "2501560",
    "end": "2508640"
  },
  {
    "text": "or underneath put SQL light as the storage engine so in that way that applications feels uh like they are",
    "start": "2508640",
    "end": "2514400"
  },
  {
    "text": "still talking to poas uh the other other other idea is that",
    "start": "2514400",
    "end": "2521319"
  },
  {
    "text": "you know persistence at the edge where you know obviously CL workers and and what not others are definitely looking",
    "start": "2521319",
    "end": "2527400"
  },
  {
    "text": "at bringing more more like you know client side data uh the other interesting one which",
    "start": "2527400",
    "end": "2534319"
  },
  {
    "text": "you folks might have noticed is bringing more of uh you know separated storage and compute for example the the typical",
    "start": "2534319",
    "end": "2540880"
  },
  {
    "text": "typical architecture where we have like CL workers running in Europe whereas your databases in North America",
    "start": "2540880",
    "end": "2547839"
  },
  {
    "text": "however that's that's changing a lot of the newest features are where we are bringing or collocating both your",
    "start": "2547839",
    "end": "2554680"
  },
  {
    "text": "storage and the compute even for example Clare workers has a new feature called smart placement uh and pretty much what",
    "start": "2554680",
    "end": "2561800"
  },
  {
    "text": "it does is it moves the cloudfare workers close to the where the databas is earlier we pushed the code more close",
    "start": "2561800",
    "end": "2569400"
  },
  {
    "text": "to the clients but what we noticed is that lot of those client applications are actually spending a lot of time",
    "start": "2569400",
    "end": "2576240"
  },
  {
    "text": "talking to their dat database and pretty much any business application is like chatty right they",
    "start": "2576240",
    "end": "2582079"
  },
  {
    "text": "want to like talk to the database five times to even respond to a single request back to the clients so we pretty",
    "start": "2582079",
    "end": "2588000"
  },
  {
    "text": "much you know like a full circle we moved out the compute close to the end users and now we are slowly bringing",
    "start": "2588000",
    "end": "2593960"
  },
  {
    "text": "back those compute back to where your databases and lastly uh this is another",
    "start": "2593960",
    "end": "2600680"
  },
  {
    "text": "one where we are running into a lot of uh new challenges is how do we do this uh data localization",
    "start": "2600680",
    "end": "2607599"
  },
  {
    "text": "especially especially for you know EUR Europe region and and still using postris and this is where we are betting",
    "start": "2607599",
    "end": "2614640"
  },
  {
    "text": "heavily on logical replication so logical replication provides the flexibilities where we can even",
    "start": "2614640",
    "end": "2620680"
  },
  {
    "text": "replicate on a on a ro level on a specific column level so still this is little bit in I would say in a more of",
    "start": "2620680",
    "end": "2627040"
  },
  {
    "text": "an exploratory phase we haven't yet made anything production using logical replication but I I have a strong",
    "start": "2627040",
    "end": "2633520"
  },
  {
    "text": "feeling that it's going to be this way so the areas we didn't cover a little",
    "start": "2633520",
    "end": "2639000"
  },
  {
    "text": "bit uh you know happy to syn offline is one of the multi-tenant resource isolation that that's Al together a",
    "start": "2639000",
    "end": "2645800"
  },
  {
    "text": "separate kind of problem like how do we keep you know every one of them as a good citizens right like Noisy Neighbor",
    "start": "2645800",
    "end": "2651079"
  },
  {
    "text": "problems Etc so we haven't touched upon that with this I am wrapping up thank",
    "start": "2651079",
    "end": "2658160"
  },
  {
    "text": "you very much and happy to take any questions",
    "start": "2658160",
    "end": "2662920"
  },
  {
    "text": "[Music]",
    "start": "2664040",
    "end": "2670340"
  }
]