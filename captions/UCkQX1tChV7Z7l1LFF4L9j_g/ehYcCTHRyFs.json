[
  {
    "text": "foreign [Music]",
    "start": "0",
    "end": "17650"
  },
  {
    "text": "there was a time when you could have any database as long as it was my sequel or postgres these systems took 30 years to",
    "start": "17880",
    "end": "25199"
  },
  {
    "text": "develop they were tried and tested and people thought twice before replacing",
    "start": "25199",
    "end": "30359"
  },
  {
    "text": "them and then something happened a wave of discoveries in the research around durability efficiency and testing grew",
    "start": "30359",
    "end": "37260"
  },
  {
    "text": "more and more difficult for existing database designs to retrofit at least this was our experience of the impact of",
    "start": "37260",
    "end": "44579"
  },
  {
    "text": "This research on our design decisions for tiger beetle and why we decided to",
    "start": "44579",
    "end": "49800"
  },
  {
    "text": "start fresh tiger beetle is a new open source distributed database where some",
    "start": "49800",
    "end": "55680"
  },
  {
    "text": "databases are designed for analytics others for streaming and still others for time series Target bill is designed",
    "start": "55680",
    "end": "62340"
  },
  {
    "text": "from the ground up for tracking balances to track the movement of value from one person or place to another for example",
    "start": "62340",
    "end": "69540"
  },
  {
    "text": "to track Financial transactions in-app purchases economies or to switch",
    "start": "69540",
    "end": "74939"
  },
  {
    "text": "payments record trades or Arbitrage Commodities like energy and to do this",
    "start": "74939",
    "end": "80280"
  },
  {
    "text": "with Mission critical safety and performance you can use balance tracking to model any kind of business event and",
    "start": "80280",
    "end": "87840"
  },
  {
    "text": "that's because the way you track balances is really Double Entry accounting which is the schema tiger",
    "start": "87840",
    "end": "93720"
  },
  {
    "text": "beetle provides as a first class primitive out of the box you can spin up",
    "start": "93720",
    "end": "98939"
  },
  {
    "text": "the replicas of a tiger beetle cluster with a single binary and then use a tiger beetle client to connect to the",
    "start": "98939",
    "end": "104579"
  },
  {
    "text": "cluster to create accounts and execute Double Entry transactions between accounts with strict serializability",
    "start": "104579",
    "end": "111180"
  },
  {
    "text": "tiger beetle is designed for high availability with automated failover if the leader of the cluster fails so that",
    "start": "111180",
    "end": "118200"
  },
  {
    "text": "everything just works we wanted to make it easy for others to build and operate the next generation of financial",
    "start": "118200",
    "end": "123780"
  },
  {
    "text": "services and applications without having to Cobble together in Ledger database from scratch",
    "start": "123780",
    "end": "129899"
  },
  {
    "text": "or to execute manual database failover at 2 am with a tightly scope domain we've gone",
    "start": "129899",
    "end": "136260"
  },
  {
    "text": "deep on the technology to do new things with the whole design of tiger beetle our Global consensus protocol local",
    "start": "136260",
    "end": "143520"
  },
  {
    "text": "storage engine the way we work with the network disk in memory the testing techniques we use and the guarantees",
    "start": "143520",
    "end": "149940"
  },
  {
    "text": "that tiger beetle gives the operator first and foremost of which is durability what is durability durability",
    "start": "149940",
    "end": "157379"
  },
  {
    "text": "means that once a database transaction has been acknowledged as committed to the user",
    "start": "157379",
    "end": "163260"
  },
  {
    "text": "it will remain committed even in the event of a crash it's fine to lose a transaction if the transaction has not",
    "start": "163260",
    "end": "169739"
  },
  {
    "text": "yet been acknowledged to the user but once it's been committed it can't be lost to achieve durability a database",
    "start": "169739",
    "end": "176160"
  },
  {
    "text": "must first write the transaction to stable storage or disk before acknowledging the transactions that",
    "start": "176160",
    "end": "181680"
  },
  {
    "text": "after a crash the transaction is still there however writing data to disk is an",
    "start": "181680",
    "end": "187680"
  },
  {
    "text": "art or a Science Blog person papers have been written about how hard it is to get right it's tempting to use a file system",
    "start": "187680",
    "end": "194940"
  },
  {
    "text": "and not a database but writing to a file from an application when the plug can be pulled at any time is simply too hard at",
    "start": "194940",
    "end": "202319"
  },
  {
    "text": "least if you want consistency so we might think to ourselves surely we know by now you know how to use renam to do",
    "start": "202319",
    "end": "208680"
  },
  {
    "text": "atomic file updates and then you hear this fresh report from three weeks ago that green M8 is not linearizable on",
    "start": "208680",
    "end": "215640"
  },
  {
    "text": "Windows subsystem for linux's xt4 and this is why our applications trust the database with your ability to get this",
    "start": "215640",
    "end": "222299"
  },
  {
    "text": "right at least in the face of sun and Powerless not to mention gradual disc corruption so there are at least three",
    "start": "222299",
    "end": "229019"
  },
  {
    "text": "ways that a database can be designed to write the editor disk first of these is",
    "start": "229019",
    "end": "234420"
  },
  {
    "text": "mmap where you map the contents of a file into your program's address space to give the illusion that you're working",
    "start": "234420",
    "end": "241260"
  },
  {
    "text": "with pure memory instead of disk however any Pavlov any students at Carnegie Mellon wrote an excellent warning on the",
    "start": "241260",
    "end": "249000"
  },
  {
    "text": "pitfalls of their map to motivate while a map is not acceptable for database durability some databases do still use",
    "start": "249000",
    "end": "255360"
  },
  {
    "text": "mmap but since we're going to cover many of the same pitfalls as we go along as",
    "start": "255360",
    "end": "261479"
  },
  {
    "text": "we look at other designs we won't go into in map further except to shine up a spotlight on the paper next after M Map",
    "start": "261479",
    "end": "268860"
  },
  {
    "text": "There's o direct or direct i o and this is where the database takes responsibility for working with the disk",
    "start": "268860",
    "end": "276120"
  },
  {
    "text": "directly so that writes and reads go directly from user memory to disk and back again and they just bypass the",
    "start": "276120",
    "end": "282419"
  },
  {
    "text": "Kernel's page cache it's much more work but the database has full control over durability as well as caching however",
    "start": "282419",
    "end": "289740"
  },
  {
    "text": "this is what Linus has to say so the right way is to just not use a direct",
    "start": "289740",
    "end": "294960"
  },
  {
    "text": "there is no valid reason for ever using o direct you need a buffer whatever I O you do and it might as well be the page",
    "start": "294960",
    "end": "301800"
  },
  {
    "text": "cache so don't use a direct pretty clear right so if we go with any Pavlo and we",
    "start": "301800",
    "end": "307080"
  },
  {
    "text": "stick clear of M map and if we don't rebel against the PDF file on a direct what else is left and here we come to",
    "start": "307080",
    "end": "314340"
  },
  {
    "text": "what many databases do such as postgres and this is to Outsource durability to",
    "start": "314340",
    "end": "319500"
  },
  {
    "text": "the kernel with buffered IO um so you write from the databases memory to the Kernel's page cache and",
    "start": "319500",
    "end": "325800"
  },
  {
    "text": "then you rely on the Kernel to flush or sync the page cache to disk whenever the",
    "start": "325800",
    "end": "330840"
  },
  {
    "text": "database wants to write to disk IT issues a right system call to the kernel passing the buffer to be written way",
    "start": "330840",
    "end": "337800"
  },
  {
    "text": "undiscuit should be written to and the interesting thing is that this write system call does nothing at least",
    "start": "337800",
    "end": "343800"
  },
  {
    "text": "nothing durable at this point and that's because when the kernel receives the right system call from the database the",
    "start": "343800",
    "end": "349560"
  },
  {
    "text": "kernel simply copies the buffer across to some pages in the page cache marks",
    "start": "349560",
    "end": "354900"
  },
  {
    "text": "the pages as dirty and in other words that you know they're on route to disk but they're not there yet so finally",
    "start": "354900",
    "end": "361500"
  },
  {
    "text": "because there's no real discard we're involved these write system calls don't typically fail the kernel usually",
    "start": "361500",
    "end": "367740"
  },
  {
    "text": "returns success almost immediately back to the database and then at some point the kernel is going to start writing the",
    "start": "367740",
    "end": "373740"
  },
  {
    "text": "data out to disk um and as each page is written it's marked clean if the database does",
    "start": "373740",
    "end": "379919"
  },
  {
    "text": "nothing more than issue rights to the kernel in this way then it can't know when these rights are safely on disk but",
    "start": "379919",
    "end": "386160"
  },
  {
    "text": "that's not a problem so far because if the machine were to crash right now then the data would be lost uh it wouldn't",
    "start": "386160",
    "end": "392580"
  },
  {
    "text": "matter because the database would not yet have acknowledged the transaction to the user right so when the database does",
    "start": "392580",
    "end": "399419"
  },
  {
    "text": "want to commit the transaction when it wants all the dirty Pages relating to the transaction to be flushed to disk",
    "start": "399419",
    "end": "406020"
  },
  {
    "text": "again for durability then it issues another system call this time fsync",
    "start": "406020",
    "end": "411740"
  },
  {
    "text": "fsync tells the kernel to finish all rights to disk before returning so this",
    "start": "411740",
    "end": "417360"
  },
  {
    "text": "ensures that all the data can be retrieved even if the system crashes or restarts in other words where a database",
    "start": "417360",
    "end": "424560"
  },
  {
    "text": "does make the big design decision to Outsource your ability to the kernel then ifsync is crucial this is because",
    "start": "424560",
    "end": "432000"
  },
  {
    "text": "most database update protocols such as right ahead logging or copy on right designs they rely on forcing data to",
    "start": "432000",
    "end": "438479"
  },
  {
    "text": "disk in the right order for correctness um however ifsync is not easy to get",
    "start": "438479",
    "end": "444180"
  },
  {
    "text": "right because under the buffered IO design ifsync is where the rubber hits the road and the dead actually hits the",
    "start": "444180",
    "end": "450240"
  },
  {
    "text": "desk so because discs are physical they can fail in all kinds of ways either",
    "start": "450240",
    "end": "455580"
  },
  {
    "text": "permanently or temporarily Sun sectors might fail others might not if you're lucky the disk will tell you when an i o",
    "start": "455580",
    "end": "462419"
  },
  {
    "text": "fails if you're unlucky it won't so we'll come back to this the takeaway for",
    "start": "462419",
    "end": "467940"
  },
  {
    "text": "now is that since the kernel may know that some of the Buffett Wright Smith hit Iowa errors on their way to the physical disk when the database does",
    "start": "467940",
    "end": "475259"
  },
  {
    "text": "eventually call fsync then it might receive an error back from the kernel indicating that some of the rights in",
    "start": "475259",
    "end": "481440"
  },
  {
    "text": "the batch didn't make it if fsync does return an i o error there are three",
    "start": "481440",
    "end": "486539"
  },
  {
    "text": "choices that the database can make so option one is just ignore any error from fsync pretend that",
    "start": "486539",
    "end": "493800"
  },
  {
    "text": "the rights didn't fail which is what some databases used to do in the past option two retry the ifsync in the hope",
    "start": "493800",
    "end": "500520"
  },
  {
    "text": "that the kernel will retry all the buffered rights to disk and keep retrying until you're durable",
    "start": "500520",
    "end": "507300"
  },
  {
    "text": "um and until you don't get an error back from fsync option three is just crash the database and then you restart and",
    "start": "507300",
    "end": "514500"
  },
  {
    "text": "you recover from a checkpoint so if you were in my sequel or postgres issues what would you choose how confident",
    "start": "514500",
    "end": "521700"
  },
  {
    "text": "would you be that your choice guarantees durability so if you've heard the story before and you know the answer then I",
    "start": "521700",
    "end": "528540"
  },
  {
    "text": "promise there's a Twist in the tail there's something new which I think you will find surprising but before we get",
    "start": "528540",
    "end": "534959"
  },
  {
    "text": "to the answer I want to warm up with a look at some of the cracks in buffered IO none of these",
    "start": "534959",
    "end": "541560"
  },
  {
    "text": "cracks on their own are enough to to end an era of database design or begin",
    "start": "541560",
    "end": "546899"
  },
  {
    "text": "another but I think they point to something shaky in the foundations first writing to cash instead of disk means",
    "start": "546899",
    "end": "553680"
  },
  {
    "text": "that you lose congestion control over the disc with no mechanism for back pressure this can result in significant",
    "start": "553680",
    "end": "561000"
  },
  {
    "text": "latency spikes you know when the system has to write out gigabytes of dirty Pages second it's difficult to",
    "start": "561000",
    "end": "568860"
  },
  {
    "text": "prioritize foreground and background IO you can't schedule your fsyncs apart",
    "start": "568860",
    "end": "574560"
  },
  {
    "text": "from other application data in the page cache you know everything is just sharing this one page cache so there are",
    "start": "574560",
    "end": "581459"
  },
  {
    "text": "ways around this like sync file range but the man page for that has this friendly warning this system call is",
    "start": "581459",
    "end": "588000"
  },
  {
    "text": "extremely dangerous and should not be used in portable programs now what does that mean uh who knows uh third",
    "start": "588000",
    "end": "595800"
  },
  {
    "text": "um buffered IO is All or Nothing so you can't handle errors related to specific",
    "start": "595800",
    "end": "601019"
  },
  {
    "text": "rights if something goes wrong you don't know what it was or where finally disks",
    "start": "601019",
    "end": "606300"
  },
  {
    "text": "have now become so fast on the order of three gigabytes a second they're starting to approach per core memory",
    "start": "606300",
    "end": "611820"
  },
  {
    "text": "bandwidth on the order of you know 6 to 20 Gigabytes per second maybe if you've got an M1 but this means that if you're",
    "start": "611820",
    "end": "618480"
  },
  {
    "text": "still using buffered IO and you're doing mem copies to the kernel page cache for every right then you're not only",
    "start": "618480",
    "end": "624899"
  },
  {
    "text": "thrashing your L1 through L3 CPU caches using up CPU to do the copies but you",
    "start": "624899",
    "end": "631140"
  },
  {
    "text": "also potentially halving memory bandwidth so this is assuming that the copy to the page cache is the only copy",
    "start": "631140",
    "end": "637380"
  },
  {
    "text": "in your data plan if you need a second copy maybe for networking or deserialization then that can be almost",
    "start": "637380",
    "end": "644220"
  },
  {
    "text": "all your memory bandwidth gone so when it comes to buffered IO there's a crack in everything but that's how the light",
    "start": "644220",
    "end": "651060"
  },
  {
    "text": "gets in let's return to our foundational fsync question and again the question is if",
    "start": "651060",
    "end": "657480"
  },
  {
    "text": "fsync fails with an i o error what should the database do how do you handle f-sync failure do you ignore the error",
    "start": "657480",
    "end": "664200"
  },
  {
    "text": "and pretend that your buffered rights are durable do you keep retrying the f-sync in the hope that the kernel will",
    "start": "664200",
    "end": "669480"
  },
  {
    "text": "retry all the buffered rights that failed do you crash and restart to recover from a checkpoint so of course",
    "start": "669480",
    "end": "676019"
  },
  {
    "text": "ignoring the error is not correct it's not an option what about retrying and",
    "start": "676019",
    "end": "681060"
  },
  {
    "text": "indeed for many years most databases would you try for example postgrades would keep retrying a failed",
    "start": "681060",
    "end": "687420"
  },
  {
    "text": "fsync until it succeeded under the assumption that the kernel would retry any failed buffered rights at least this",
    "start": "687420",
    "end": "694380"
  },
  {
    "text": "was the Assumption for 20 years and then something happened so five years ago",
    "start": "694380",
    "end": "699620"
  },
  {
    "text": "Craig ringer posted this post on the postgres mailing list to report that he",
    "start": "699620",
    "end": "706860"
  },
  {
    "text": "had run into real data loss with postgres so the critical database",
    "start": "706860",
    "end": "712440"
  },
  {
    "text": "guarantee of durability the D in in asset had been violated but what was",
    "start": "712440",
    "end": "718440"
  },
  {
    "text": "more stunning I think and perhaps the reason that the incident became known as Epson gate is that this wasn't due to a",
    "start": "718440",
    "end": "724800"
  },
  {
    "text": "bug in postgres per se uh postgres had followed Linus's advice and relied on",
    "start": "724800",
    "end": "730860"
  },
  {
    "text": "the design of the page cache and even the postgres was careful to retry after an IO error it was still using data so",
    "start": "730860",
    "end": "738480"
  },
  {
    "text": "this was because in the kernel when a buffered right failed due to disk error the kernel was in fact simply marking",
    "start": "738480",
    "end": "744959"
  },
  {
    "text": "the 11 pages as clean even though the dirty pages had not been",
    "start": "744959",
    "end": "750540"
  },
  {
    "text": "written properly to disk so this means that postgres might get the first ifsc",
    "start": "750540",
    "end": "756300"
  },
  {
    "text": "or the first time round assuming that another process didn't consume at first but then the second time around when",
    "start": "756300",
    "end": "763079"
  },
  {
    "text": "postgres retries the fsync the ifsync would succeed and postgres",
    "start": "763079",
    "end": "768959"
  },
  {
    "text": "would proceed as if the data was committed to disk despite the relevant Pages still not",
    "start": "768959",
    "end": "775560"
  },
  {
    "text": "having to be made durable so again the daily Pages would simply be more clean in the kernel and the kernel developers",
    "start": "775560",
    "end": "782160"
  },
  {
    "text": "maintained that this mock clean Behavior was necessary for example to avoid out of memory situations if a USB stick was",
    "start": "782160",
    "end": "790139"
  },
  {
    "text": "pulled out so if the page cache wouldn't fill up with dirty pages that could never be flushed but it rocked the",
    "start": "790139",
    "end": "797399"
  },
  {
    "text": "database world after the discovery of epson get postgres MySQL other affected",
    "start": "797399",
    "end": "803279"
  },
  {
    "text": "databases they decided to fix the issue by just changing their answer to the",
    "start": "803279",
    "end": "809040"
  },
  {
    "text": "foundational Epson question so instead of attempting to retry fsync off to failure as before they would crash and",
    "start": "809040",
    "end": "816180"
  },
  {
    "text": "then recover from the data file on disk Jonathan Corbett wrote about postgres's",
    "start": "816180",
    "end": "821880"
  },
  {
    "text": "fsync surprise and then a year later in 2019 with the fix in place Thomas vandra",
    "start": "821880",
    "end": "828300"
  },
  {
    "text": "gave a fascinating talk about this quote how is it that postgres used ifsync incorrectly for 20 years and what we'll",
    "start": "828300",
    "end": "836399"
  },
  {
    "text": "do about it up to now if I've told the story properly then I think you'll agree that this was the correct fix for fsync",
    "start": "836399",
    "end": "842880"
  },
  {
    "text": "gate this is where the story ends right perhaps you're still wondering where all",
    "start": "842880",
    "end": "849360"
  },
  {
    "text": "the seismic shifts in database design that you promised at the beginning because it looks like the fix for FC",
    "start": "849360",
    "end": "855540"
  },
  {
    "text": "gate was not a major design change not even a minor design change but just a",
    "start": "855540",
    "end": "860820"
  },
  {
    "text": "simple Panic to restart and recover so it's a clever fix for sure but a simple",
    "start": "860820",
    "end": "866940"
  },
  {
    "text": "fix well here we come to the part of the story that I find is less often told",
    "start": "866940",
    "end": "873959"
  },
  {
    "text": "uh the story picks up tears later in 2020 when University of Wisconsin-Madison asked the question can",
    "start": "873959",
    "end": "880980"
  },
  {
    "text": "applications recover from ifsc failures and you can probably guess what the answer is when some of the leading",
    "start": "880980",
    "end": "886740"
  },
  {
    "text": "storage researchers Ramsey and Andrea Apache dessert and the students when",
    "start": "886740",
    "end": "892019"
  },
  {
    "text": "they decide to ask the question in this way so if what the first Epson gate had",
    "start": "892019",
    "end": "897060"
  },
  {
    "text": "found was stunning then I think that what this paper found was even more so because while database is such a SQL",
    "start": "897060",
    "end": "903180"
  },
  {
    "text": "item postgres would now crash often ifsync eio error after restarting their",
    "start": "903180",
    "end": "908940"
  },
  {
    "text": "recovery with still their recovery would still read from the page cache not the",
    "start": "908940",
    "end": "915180"
  },
  {
    "text": "actual on disk set so they're potentially making recovery decisions",
    "start": "915180",
    "end": "920220"
  },
  {
    "text": "based on non-jurable pages that were marked clean through the f-sync failure so the paper also raised other ways that",
    "start": "920220",
    "end": "926880"
  },
  {
    "text": "ifsc failure was not being handled correctly by these databases for example where xt4 in data mode uh is it would",
    "start": "926880",
    "end": "935160"
  },
  {
    "text": "suppress an eio error and only return it to the next fsync call so you want to",
    "start": "935160",
    "end": "942240"
  },
  {
    "text": "get an f-sync area you have to call fsync twice uh so users were still at risk of data",
    "start": "942240",
    "end": "947399"
  },
  {
    "text": "loss and Corruption but the most important finding at least for me was just this little sentence at the end of",
    "start": "947399",
    "end": "954420"
  },
  {
    "text": "the abstract you know sometimes in the paper there's just these these little you you read a careful and you there's",
    "start": "954420",
    "end": "959940"
  },
  {
    "text": "these little things that you almost gloss over and you eat it again um and and there's the sentence that",
    "start": "959940",
    "end": "965220"
  },
  {
    "text": "just changes your understanding of things and for me this sentence at the end of the abstract was one of those so",
    "start": "965220",
    "end": "971639"
  },
  {
    "text": "our findings have strong implications for the design of file systems and applications I.E databases that intend",
    "start": "971639",
    "end": "979680"
  },
  {
    "text": "to provide strong durability guarantees strong implementation implications for",
    "start": "979680",
    "end": "986040"
  },
  {
    "text": "the design of databases so UW medicine had shown that the classic buffered IO design for",
    "start": "986040",
    "end": "992639"
  },
  {
    "text": "databases of the past 30 years was now fundamentally broken there was in fact",
    "start": "992639",
    "end": "997740"
  },
  {
    "text": "no correct answer to the question of how to handle fsync failure under the buffered IO design so the answers were",
    "start": "997740",
    "end": "1004759"
  },
  {
    "text": "all wrong and not actually sufficient for a database to guarantee durability",
    "start": "1004759",
    "end": "1010120"
  },
  {
    "text": "instead database design would need to move from buffered IO to direct IO",
    "start": "1010120",
    "end": "1016339"
  },
  {
    "text": "databases would need to take direct responsibility for durability they would",
    "start": "1016339",
    "end": "1021500"
  },
  {
    "text": "need to be able to read and write to the disk directly to be sure that they always made decisions and",
    "start": "1021500",
    "end": "1027199"
  },
  {
    "text": "acknowledgments of durable data instead of you know basing decisions only on the",
    "start": "1027199",
    "end": "1032959"
  },
  {
    "text": "contents of the kernel page cache so I think the implications of this design change are enormous it's one thing to",
    "start": "1032959",
    "end": "1040160"
  },
  {
    "text": "design a database from scratch like we did with tiger beetle to take advantage of directio is something else entirely",
    "start": "1040160",
    "end": "1047298"
  },
  {
    "text": "to try and you know retro retrofit an existing design for directio the reason",
    "start": "1047299",
    "end": "1053360"
  },
  {
    "text": "is you need to align all your memory allocations and eye operations to Advanced format for kilobyte sector size",
    "start": "1053360",
    "end": "1060140"
  },
  {
    "text": "you need a new buffer pool a new user Space page cache and because the",
    "start": "1060140",
    "end": "1066200"
  },
  {
    "text": "latencies of your rights to disk are now realistic I.E disk speed rather than memory speed you can't afford to to",
    "start": "1066200",
    "end": "1073100"
  },
  {
    "text": "block on synchronous IO anymore you could have taken those shortcuts in your design now you actually need to",
    "start": "1073100",
    "end": "1080059"
  },
  {
    "text": "implement proper asynchronous i o in your right path so if you can do this there's some",
    "start": "1080059",
    "end": "1086299"
  },
  {
    "text": "incredible performance gains but it's a complete overhaul of your your whole database design or as Andreas freund on",
    "start": "1086299",
    "end": "1094760"
  },
  {
    "text": "the postgres team said back in 2018 when this happened so five years ago efficient Dio usage is a metric ton of",
    "start": "1094760",
    "end": "1103100"
  },
  {
    "text": "work so under assistance done a phenomenal amount of work around direct IO for postgres we were in touch last",
    "start": "1103100",
    "end": "1110480"
  },
  {
    "text": "week and Andres shared this with me that Thomas Monroe is planning to merge some",
    "start": "1110480",
    "end": "1116299"
  },
  {
    "text": "actual Dio support soon at the same time it's exactly five years tomorrow since",
    "start": "1116299",
    "end": "1123500"
  },
  {
    "text": "the 28th of March 2018 and the first discovery of epson get so for all these",
    "start": "1123500",
    "end": "1128960"
  },
  {
    "text": "reasons for all this history I believe that this event in 2018",
    "start": "1128960",
    "end": "1134360"
  },
  {
    "text": "drew the first line in the second this is you know what marked the end of an",
    "start": "1134360",
    "end": "1140660"
  },
  {
    "text": "era for database Design This was the first line I think and it almost passed us by",
    "start": "1140660",
    "end": "1147200"
  },
  {
    "text": "um but the design of new databases that intend to provide strong durability guarantees would have to change and not",
    "start": "1147200",
    "end": "1154820"
  },
  {
    "text": "only because of epson get so something else was about to happen in 2018 and",
    "start": "1154820",
    "end": "1160640"
  },
  {
    "text": "researchers at DW medicine were again about to discover something just as momentous this time not in the Buffett",
    "start": "1160640",
    "end": "1167120"
  },
  {
    "text": "IO design but in the right-to-head log design of almost every database you know",
    "start": "1167120",
    "end": "1172480"
  },
  {
    "text": "so we started out by saying that for a database to guarantee durability",
    "start": "1172480",
    "end": "1178120"
  },
  {
    "text": "it must protect committed transactions through power loss",
    "start": "1178120",
    "end": "1183200"
  },
  {
    "text": "one aspect of this is the right path which we've looked at to compare buffered I O with direct i o you know in",
    "start": "1183200",
    "end": "1191000"
  },
  {
    "text": "how a database writes transactions to disk and then reads them back in at",
    "start": "1191000",
    "end": "1196940"
  },
  {
    "text": "recovery but how does a database actually recover these transactions after a crash so the",
    "start": "1196940",
    "end": "1204200"
  },
  {
    "text": "idea is pretty simple and common to most databases it's called the write ahead log and I first learned the technique",
    "start": "1204200",
    "end": "1211340"
  },
  {
    "text": "when I was diving into redness a decade ago so redis has what is called the aof",
    "start": "1211340",
    "end": "1216640"
  },
  {
    "text": "which stands for append only file this is a logon disk when redis wants to",
    "start": "1216640",
    "end": "1222559"
  },
  {
    "text": "commit a transaction it first appends the transaction to the end of the slog and then it calls ifsync for example if",
    "start": "1222559",
    "end": "1229640"
  },
  {
    "text": "you want to execute a transaction in redis to set the qcon key to London then",
    "start": "1229640",
    "end": "1234980"
  },
  {
    "text": "redis would append this log append this to the log so I've simplified it a little here but this is an elegant",
    "start": "1234980",
    "end": "1241880"
  },
  {
    "text": "format so first you've got the number of arguments in this case three then you've got the number of bytes in each argument",
    "start": "1241880",
    "end": "1248299"
  },
  {
    "text": "so three bytes for the command which is set four bytes for qcon and then finally",
    "start": "1248299",
    "end": "1253580"
  },
  {
    "text": "you've got six bytes for London right so if we then want to update qcom to New York we would append another transaction",
    "start": "1253580",
    "end": "1260480"
  },
  {
    "text": "to the log like this so the trick is that redis never updates anything in place in the log it always appends so",
    "start": "1260480",
    "end": "1267620"
  },
  {
    "text": "that data is never overwritten this means that if the power goes then existing data is not destroyed so there",
    "start": "1267620",
    "end": "1273500"
  },
  {
    "text": "might be some garbage debt at the end of the log if a partial transaction was being appended and then the power went",
    "start": "1273500",
    "end": "1279200"
  },
  {
    "text": "for example if we try to set cuecon to San Francisco then the power goes might",
    "start": "1279200",
    "end": "1284419"
  },
  {
    "text": "end up with this like partial transaction but it's startup redis can figure this out and discard the partial",
    "start": "1284419",
    "end": "1291080"
  },
  {
    "text": "transaction this is safe to do because redis would not yet have acknowledged the San Francisco transaction until it",
    "start": "1291080",
    "end": "1297620"
  },
  {
    "text": "was durably on the log so this writer head log design is the foundation for most databases it's the crucial building",
    "start": "1297620",
    "end": "1304700"
  },
  {
    "text": "block for ensuring Atomic changes to database State it's also the foundation",
    "start": "1304700",
    "end": "1309980"
  },
  {
    "text": "for distributed databases so this is where each node in the cluster will keep its own writer head log which is then",
    "start": "1309980",
    "end": "1317059"
  },
  {
    "text": "appended to by the global census protocols such as view stamp replication raft or paxos however for distributed",
    "start": "1317059",
    "end": "1324980"
  },
  {
    "text": "databases the write ahead log is doubly critical",
    "start": "1324980",
    "end": "1330080"
  },
  {
    "text": "um because if anything goes wrong if you have a bug that undermines durability uh",
    "start": "1330080",
    "end": "1335659"
  },
  {
    "text": "so that you lose a transaction that you've acknowledged to the rest of the cluster then you've not only lost a copy",
    "start": "1335659",
    "end": "1341059"
  },
  {
    "text": "you know of user data which you have but you've also undermined the Quorum voting",
    "start": "1341059",
    "end": "1347299"
  },
  {
    "text": "that's going on in the consensus protocol so you mess with the Quorum votes and this can easily lead to like",
    "start": "1347299",
    "end": "1353780"
  },
  {
    "text": "split brain which can in turn cause global cluster data loss so the writer",
    "start": "1353780",
    "end": "1359360"
  },
  {
    "text": "head log is foundational to the design of a database with a single node or distributed",
    "start": "1359360",
    "end": "1365179"
  },
  {
    "text": "and there are many variants on this design so for example I've shown you you",
    "start": "1365179",
    "end": "1370340"
  },
  {
    "text": "know a simplified version of red assist text format to give you the big idea but",
    "start": "1370340",
    "end": "1375500"
  },
  {
    "text": "most databases use a binary format then they prefix each transaction in the",
    "start": "1375500",
    "end": "1380840"
  },
  {
    "text": "log with a transaction header there's also a checksum so if at startup the database sees that the checksum doesn't",
    "start": "1380840",
    "end": "1387380"
  },
  {
    "text": "match then the database truncates the log from that point to discard the partial transaction however it's",
    "start": "1387380",
    "end": "1394039"
  },
  {
    "text": "critical that only the last partial transaction a transaction that was being appended as the power went out is",
    "start": "1394039",
    "end": "1400700"
  },
  {
    "text": "truncated the database must never truncate any other transactions in the",
    "start": "1400700",
    "end": "1405980"
  },
  {
    "text": "right to hit log because obviously these would have been acknowledged to the users committed so to to truncate them",
    "start": "1405980",
    "end": "1412760"
  },
  {
    "text": "would violate durability so at a high level this is how most right-to-head log",
    "start": "1412760",
    "end": "1418760"
  },
  {
    "text": "designs all work today but if we apply what we've learned from ifscincate how",
    "start": "1418760",
    "end": "1424640"
  },
  {
    "text": "does this design interact with real storage Hardware so can you spot the",
    "start": "1424640",
    "end": "1429980"
  },
  {
    "text": "problem we've seen that qcon London is already committed as a transaction through a writer head log but what if",
    "start": "1429980",
    "end": "1436280"
  },
  {
    "text": "while the machine is off the disk sector containing our qcon London transaction",
    "start": "1436280",
    "end": "1441980"
  },
  {
    "text": "in the right ahead log what if it if that disk sector is corrupted and experience is just a single flip bit so",
    "start": "1441980",
    "end": "1450320"
  },
  {
    "text": "the length prefix for London changes from six to four",
    "start": "1450320",
    "end": "1456320"
  },
  {
    "text": "um so when the database starts up and reads the log the qcon key now is going to look like it was set to land but then",
    "start": "1456320",
    "end": "1463340"
  },
  {
    "text": "the New York transaction after that which was committed now that's going to look like it was being appended when the",
    "start": "1463340",
    "end": "1469340"
  },
  {
    "text": "power went out because the log is now no longer going to have the proper you know format it's going to have garbage data",
    "start": "1469340",
    "end": "1475940"
  },
  {
    "text": "at the end after you know from o and n onwards so as far as I'm aware in this situation most databases would truncate",
    "start": "1475940",
    "end": "1483679"
  },
  {
    "text": "the log before New York and the committed New York transaction as well as every transaction after it would be",
    "start": "1483679",
    "end": "1490280"
  },
  {
    "text": "lost violating durability and for databases that checks on the transaction and they write ahead log designs the",
    "start": "1490280",
    "end": "1497419"
  },
  {
    "text": "London Key would also be truncated because of the checksum mismatch even though it was in fact committed so in",
    "start": "1497419",
    "end": "1504020"
  },
  {
    "text": "other words a single disk sector failure is enough to break the right ahead log design of most databases so they",
    "start": "1504020",
    "end": "1511100"
  },
  {
    "text": "incorrectly truncate the committed log potentially truncating tens to 100 of committed transactions you can see",
    "start": "1511100",
    "end": "1518600"
  },
  {
    "text": "the big problem you know they're all trying to solve the problem of power loss and torn rights you know after a",
    "start": "1518600",
    "end": "1525740"
  },
  {
    "text": "crash and they're being fooled because now you get a little bit of bitrot in",
    "start": "1525740",
    "end": "1531679"
  },
  {
    "text": "the middle of the committed log and they they you know they conflate that with the system crash and just rewind",
    "start": "1531679",
    "end": "1537380"
  },
  {
    "text": "everything which is not correct uh and I as far as I know that's every database",
    "start": "1537380",
    "end": "1542720"
  },
  {
    "text": "out there with a writer head log you know if if yours handles this please let me know",
    "start": "1542720",
    "end": "1548539"
  },
  {
    "text": "um but you know so with these with these designs uh users will experience silent",
    "start": "1548539",
    "end": "1553820"
  },
  {
    "text": "data loss um whereas for a single mode database the correct Behavior would be to fail fast uh then you notify the user",
    "start": "1553820",
    "end": "1562039"
  },
  {
    "text": "of the corruption uh because at least then they've got a chance to restore from backup but for distributed",
    "start": "1562039",
    "end": "1567679"
  },
  {
    "text": "databases it's worse so this single disk sector fault would also have the potential to",
    "start": "1567679",
    "end": "1575299"
  },
  {
    "text": "do more damage again like we said you know it can mess with the Quorum votes of course split brain and that can",
    "start": "1575299",
    "end": "1582320"
  },
  {
    "text": "Cascade into Global cluster data loss so you know there you're not just losing",
    "start": "1582320",
    "end": "1587720"
  },
  {
    "text": "one piece of user data in the transactions you're actually corrupting everything the whole the whole data file",
    "start": "1587720",
    "end": "1594020"
  },
  {
    "text": "of of every replica just gets messed up so split brain brain Cascades into cluster data loss and at least this was",
    "start": "1594020",
    "end": "1601520"
  },
  {
    "text": "the finding of the research team that discovered this as they analyzed the right ahead logged you know designs of",
    "start": "1601520",
    "end": "1608539"
  },
  {
    "text": "distributed systems using consensus protocols such as raft um so they wanted to see you know if",
    "start": "1608539",
    "end": "1615559"
  },
  {
    "text": "they can just put in one single District default on one node",
    "start": "1615559",
    "end": "1621080"
  },
  {
    "text": "um on a single machine you know what could that do to the cluster um and you basically it could do the",
    "start": "1621080",
    "end": "1627200"
  },
  {
    "text": "worst so can you guess who the team was so again Ramsay Andrea Apache de so",
    "start": "1627200",
    "end": "1632360"
  },
  {
    "text": "students at UW-Madison as well this time uh the students were Ram Nathan alagapan",
    "start": "1632360",
    "end": "1638020"
  },
  {
    "text": "ashwari Afghanistan who won best paper at fast for their research on this which",
    "start": "1638020",
    "end": "1644419"
  },
  {
    "text": "was called protocol aware recovery for consensus-based storage so can you guess the year again 2018 and this was in fact",
    "start": "1644419",
    "end": "1652760"
  },
  {
    "text": "a month before fcget and it totally passed us by so we we heard about fsyncet I don't think many of us have",
    "start": "1652760",
    "end": "1659840"
  },
  {
    "text": "heard about this but like fsyncet um again it's something we still haven't",
    "start": "1659840",
    "end": "1666080"
  },
  {
    "text": "dealt with fully as an industry I think so some Cloud providers I know you know they've patched their proprietary",
    "start": "1666080",
    "end": "1672020"
  },
  {
    "text": "databases for these findings but I'm not aware of other open source databases",
    "start": "1672020",
    "end": "1677240"
  },
  {
    "text": "that handle this correctly so in the case of fsyncet it took users to experience data loss then they had to be",
    "start": "1677240",
    "end": "1684440"
  },
  {
    "text": "dedicated enough to report you know the data loss and to figure out what had happened that this was not just this",
    "start": "1684440",
    "end": "1691340"
  },
  {
    "text": "corruption but a design flaw so the database could have handled if sincere but it didn't so instead it accelerated",
    "start": "1691340",
    "end": "1699620"
  },
  {
    "text": "the storage fault into unnecessary data loss in the case of you know the right-hand log design floor you know",
    "start": "1699620",
    "end": "1706340"
  },
  {
    "text": "protocol aware recovery so here the research Community they've done the failure you know they've discovered this",
    "start": "1706340",
    "end": "1711740"
  },
  {
    "text": "proactively but it's yet to trickle down to inform new designs so how long until",
    "start": "1711740",
    "end": "1717260"
  },
  {
    "text": "the right ahead log powering a major kubernetes deployment is truncated in a way that leads to split brain law also",
    "start": "1717260",
    "end": "1724640"
  },
  {
    "text": "the control plane and public outage so I think this is the kind of paper you need to read like three times",
    "start": "1724640",
    "end": "1731419"
  },
  {
    "text": "to let the impact sink in so how do you fix a design flow like this how do you distinguish between power loss and",
    "start": "1731419",
    "end": "1737720"
  },
  {
    "text": "bitrot to know whether to truncate a torn right power loss or to raise an error audio",
    "start": "1737720",
    "end": "1745340"
  },
  {
    "text": "distributed recovery in the case of bitrot so the strategy given by the paper what we do in tiger beetle is to",
    "start": "1745340",
    "end": "1752179"
  },
  {
    "text": "supplement the right ahead log with the second writer headlock so we've actually got two writer hit logs in Target all",
    "start": "1752179",
    "end": "1758059"
  },
  {
    "text": "right we love we love them so much the second right ahead log it's very small",
    "start": "1758059",
    "end": "1764179"
  },
  {
    "text": "it's lightweight it just contains a copy of the headers from the first so first right to hit log has got your messages",
    "start": "1764179",
    "end": "1770720"
  },
  {
    "text": "and your headers or your you know your transactions and transaction headers the second writer log right hand log just",
    "start": "1770720",
    "end": "1777860"
  },
  {
    "text": "has the transaction headers so this turns the right to head log append",
    "start": "1777860",
    "end": "1782899"
  },
  {
    "text": "process into a two-step process so first you append to the log as you normally would",
    "start": "1782899",
    "end": "1787940"
  },
  {
    "text": "but then after you append the transaction you also pin the small transaction header to a second log so",
    "start": "1787940",
    "end": "1794120"
  },
  {
    "text": "you've got another copy on it and this enables you to distinguish between corruption in the middle of the committed log caused by bitrut from",
    "start": "1794120",
    "end": "1801260"
  },
  {
    "text": "corruption at the end of the log course by paulus but again like the fix for epsyncet it's a major design change for",
    "start": "1801260",
    "end": "1808399"
  },
  {
    "text": "a database right to have two writer headlogs um you know not just one so it's not",
    "start": "1808399",
    "end": "1815120"
  },
  {
    "text": "always easy to retrofit so there were also other findings in the protocol where uh recovery paper from UW medicine",
    "start": "1815120",
    "end": "1822200"
  },
  {
    "text": "they impact how we design distributed databases showing that for protocols like raft if that operator is lucky in a",
    "start": "1822200",
    "end": "1829700"
  },
  {
    "text": "single set default doesn't lead to Global cluster data loss then the fault can still cause the cluster to become",
    "start": "1829700",
    "end": "1836740"
  },
  {
    "text": "prematurely unavailable because of how storage Faults Are handled in the right",
    "start": "1836740",
    "end": "1842720"
  },
  {
    "text": "analog so you're paying for this durability but you're not getting the availability so fixing these is also",
    "start": "1842720",
    "end": "1848960"
  },
  {
    "text": "going to require design changes for example in the past Global consensus protocol local storage engine would be",
    "start": "1848960",
    "end": "1855440"
  },
  {
    "text": "separate modules or components you know that we used to think of these as completely decoupled or not integrated",
    "start": "1855440",
    "end": "1861860"
  },
  {
    "text": "but if you want your distributed database to maximize availability",
    "start": "1861860",
    "end": "1868220"
  },
  {
    "text": "um how your local storage engine recovers from Storage faults in the writer head log needs to be properly",
    "start": "1868220",
    "end": "1874640"
  },
  {
    "text": "integrated you know with the global consensus protocol so if you want to optimize for high availability you want",
    "start": "1874640",
    "end": "1880700"
  },
  {
    "text": "to tolerate up to the theoretical limit of faults you know that your consensus protocol should be able to tolerate then",
    "start": "1880700",
    "end": "1886700"
  },
  {
    "text": "it's no longer enough you know just to Cobble together off-the-shelf raft with",
    "start": "1886700",
    "end": "1891740"
  },
  {
    "text": "off the shelf and the same tree like you know rocks or level DB instead you need",
    "start": "1891740",
    "end": "1896960"
  },
  {
    "text": "to take both of these you know according to the paper and they show you you know how to make them storage fault away how",
    "start": "1896960",
    "end": "1903620"
  },
  {
    "text": "to talk to each other um so they can both recover it sounds complicated but it's really simple it's",
    "start": "1903620",
    "end": "1909380"
  },
  {
    "text": "an obvious idea right use your replicated redundancy to heal your right",
    "start": "1909380",
    "end": "1914480"
  },
  {
    "text": "ahead log your local writer headlong but this is like a design change that's it's",
    "start": "1914480",
    "end": "1920059"
  },
  {
    "text": "fundamental enough I think to Signal a new era for how we design distributed databases you have to you know use a",
    "start": "1920059",
    "end": "1926179"
  },
  {
    "text": "consensus protocol that in implements this and the storage engine that implements it and both of them must talk",
    "start": "1926179",
    "end": "1932480"
  },
  {
    "text": "to each other you know with these new interfaces so it's a big design change to summarize what both Epson gate and",
    "start": "1932480",
    "end": "1938840"
  },
  {
    "text": "protocol aware recovery from UW Madison have in common is that I think storage",
    "start": "1938840",
    "end": "1943880"
  },
  {
    "text": "faults Force us to reconsider how we design our databases to write to disk or",
    "start": "1943880",
    "end": "1949940"
  },
  {
    "text": "even just to append to write a head log and how they recovered startup so if we",
    "start": "1949940",
    "end": "1955220"
  },
  {
    "text": "can extract a principle from this then the principle is that if we want to guarantee durability we need to move",
    "start": "1955220",
    "end": "1961820"
  },
  {
    "text": "Beyond a crash safety model you know this is the model that that we've had up until now let's just survive you know",
    "start": "1961820",
    "end": "1967940"
  },
  {
    "text": "durability through power loss we need to move beyond that um this idea that the plug can be pulled",
    "start": "1967940",
    "end": "1974419"
  },
  {
    "text": "any second um if we need yeah sure we need to support that but we need more we need to",
    "start": "1974419",
    "end": "1981140"
  },
  {
    "text": "actually adopt an explicit storage fault model so that we can test our designs",
    "start": "1981140",
    "end": "1986299"
  },
  {
    "text": "you know against the full range of storage faults that we expect not only crash safety but you know storage fault",
    "start": "1986299",
    "end": "1992539"
  },
  {
    "text": "safety in the security world this would be the equivalent of a threat model so and just",
    "start": "1992539",
    "end": "1998419"
  },
  {
    "text": "as it can make for better security design to be upfront about your threat model so I think it can make for better",
    "start": "1998419",
    "end": "2004779"
  },
  {
    "text": "durability or just durability right if we expect the disk to be not perfectly",
    "start": "2004779",
    "end": "2011559"
  },
  {
    "text": "pristine you know this is formal proofs for paxos or raft",
    "start": "2011559",
    "end": "2017080"
  },
  {
    "text": "um this is what they assume right in your perfect storage but rather you know what I think we need",
    "start": "2017080",
    "end": "2023620"
  },
  {
    "text": "to do is think of disks as near Byzantine that's what we call it a tiger",
    "start": "2023620",
    "end": "2029080"
  },
  {
    "text": "beetle it's our own made-up term right it's somewhere between non-byzentine fault tolerance and Byzantine photons",
    "start": "2029080",
    "end": "2035019"
  },
  {
    "text": "discs are somewhere in between they're near Byzantine um you can actually you do well you know",
    "start": "2035019",
    "end": "2040960"
  },
  {
    "text": "if you expect the disc to be almost an active adversary",
    "start": "2040960",
    "end": "2046840"
  },
  {
    "text": "um so for tiger beetle we saw both these events of 2018 as an opportunity we",
    "start": "2046840",
    "end": "2051940"
  },
  {
    "text": "could take the growing body of storage fault research there's so much out there you know all the other papers coming",
    "start": "2051940",
    "end": "2057220"
  },
  {
    "text": "also from UW-Madison with Vanguard we could take that designed tiger beetle to",
    "start": "2057220",
    "end": "2062618"
  },
  {
    "text": "not only be crash tolerant but also storage fault tolerant to start to see disks as distributed systems you know",
    "start": "2062619",
    "end": "2069940"
  },
  {
    "text": "one disc there's so much stuff going on there in terms of failures it's like the network fault model you know like",
    "start": "2069940",
    "end": "2075460"
  },
  {
    "text": "Network partitions you can get cut off from disk sectors so you start to see just a single disk as a distributed",
    "start": "2075460",
    "end": "2081460"
  },
  {
    "text": "system it's a whole faulty microcosm uh you know where you can have bugs in the disk firmware and the device drivers",
    "start": "2081460",
    "end": "2087878"
  },
  {
    "text": "even in the file systems you can have latent sector errors at least when you get an explicit eio from the kernel as",
    "start": "2087879",
    "end": "2094780"
  },
  {
    "text": "we've seen but then you can also get silent corruption where you don't uh you know this could be caused by bit right",
    "start": "2094780",
    "end": "2100800"
  },
  {
    "text": "even lost or misdirected reads or writes and this is just where the disk sends the i o to the wrong side sector for",
    "start": "2100800",
    "end": "2107200"
  },
  {
    "text": "some reason so for example here is how we design Target Beetle",
    "start": "2107200",
    "end": "2112780"
  },
  {
    "text": "um to you know read our write ahead log at startup we've got these two right-handed logs so even in the",
    "start": "2112780",
    "end": "2118420"
  },
  {
    "text": "presence of storage faults so what was what we really liked about this was we enumerated all the possible faults we",
    "start": "2118420",
    "end": "2125500"
  },
  {
    "text": "expected according to the fault model you know while we're recovering from both writer head logs you've got the",
    "start": "2125500",
    "end": "2131680"
  },
  {
    "text": "transaction log the header log and then we enumerated all these combinations in a matrix so",
    "start": "2131680",
    "end": "2137380"
  },
  {
    "text": "my co-founder on DJ myself we worked these up together you know at a countless course work them up by hand",
    "start": "2137380",
    "end": "2143680"
  },
  {
    "text": "and then we generated all the failure handing handling branches in the code",
    "start": "2143680",
    "end": "2148980"
  },
  {
    "text": "dynamically and you know this way we can ensure that we wouldn't miss a fault combination we just enumerated them all",
    "start": "2148980",
    "end": "2155859"
  },
  {
    "text": "worked out what the proper response should be and then we we generated it in the code so that all the branches are",
    "start": "2155859",
    "end": "2162160"
  },
  {
    "text": "there so of course you know these storage faults are rare compared to whole machine failures but in",
    "start": "2162160",
    "end": "2168700"
  },
  {
    "text": "large-scale deployments um even rare failures become prevalent so at the same time while we had the",
    "start": "2168700",
    "end": "2174940"
  },
  {
    "text": "opportunity to focus on safety to design for an explicit storage spot model and for a director from scratch we also took",
    "start": "2174940",
    "end": "2181599"
  },
  {
    "text": "the opportunity to ask well you know how could we take advantage of direct i o from a performance perspective in the",
    "start": "2181599",
    "end": "2187359"
  },
  {
    "text": "past you know when working with directio you would use asynchronous IO um like we've said in this that your",
    "start": "2187359",
    "end": "2193660"
  },
  {
    "text": "system calls don't block your main process however historically on Linux because the i o API for async Io Is Not",
    "start": "2193660",
    "end": "2200859"
  },
  {
    "text": "Great it wasn't great you'd have to implement this illusion of asynchronous",
    "start": "2200859",
    "end": "2206020"
  },
  {
    "text": "IO by means of a user space thread pool so you know think of libuv right so your",
    "start": "2206020",
    "end": "2211720"
  },
  {
    "text": "database control plane would submit IO to a user space thread pool which would then use a blocking syscall to the",
    "start": "2211720",
    "end": "2217540"
  },
  {
    "text": "kernel to do the i o problem with this is that your control plane is first Concepts context switch to your thread",
    "start": "2217540",
    "end": "2223720"
  },
  {
    "text": "pool then your thread pool must pay the cost of The Blocking Cisco to the kernel and then you must context switch back to",
    "start": "2223720",
    "end": "2230380"
  },
  {
    "text": "your control plan so in July 2020 as you know as we designing tiger beetle we're asking the performance questions you",
    "start": "2230380",
    "end": "2236440"
  },
  {
    "text": "know what if we could have first class asynchronous IO but without the",
    "start": "2236440",
    "end": "2242020"
  },
  {
    "text": "complexity you know of a user space thread pool um so with the cost of a context switch",
    "start": "2242020",
    "end": "2247359"
  },
  {
    "text": "in the range of a microsecond it's starting you know context switches approaching the same order as just doing the i o itself what if you also didn't",
    "start": "2247359",
    "end": "2254740"
  },
  {
    "text": "need a context switch we'd love to ask these questions you know let's just get rid of the thread pool just get rid of",
    "start": "2254740",
    "end": "2260440"
  },
  {
    "text": "the context switch so even better you know with the mitigations of the Specter and meltdown you know this was making",
    "start": "2260440",
    "end": "2266079"
  },
  {
    "text": "Cisco slower so we asked you know what if you just didn't need to syscall into",
    "start": "2266079",
    "end": "2271119"
  },
  {
    "text": "the kernel so we wanted to embrace direct our almost to the point of bypassing the canal completely so we're",
    "start": "2271119",
    "end": "2278320"
  },
  {
    "text": "really bypassing the page cache and I'll be bypassing the context switches and you know the user space thread pool and",
    "start": "2278320",
    "end": "2284680"
  },
  {
    "text": "the syscalls but except obviously we didn't want the complexity of bypassing",
    "start": "2284680",
    "end": "2290200"
  },
  {
    "text": "the kennel completely so for example we didn't want to use kernel bypass techniques like you know spdk or dpdk",
    "start": "2290200",
    "end": "2297880"
  },
  {
    "text": "um because well firstly we just not that good to do that you know that's what red panda do which is amazing one of my",
    "start": "2297880",
    "end": "2304660"
  },
  {
    "text": "favorite databases um and so that's high performance right but also more complex I think so finally",
    "start": "2304660",
    "end": "2311740"
  },
  {
    "text": "we wanted a unified asynchronous i o interface with this simple API for",
    "start": "2311740",
    "end": "2317380"
  },
  {
    "text": "networking and Storage so here again we're lucky you know with the timing because just as we were",
    "start": "2317380",
    "end": "2323079"
  },
  {
    "text": "thinking about these things uh Jens axbor comes along you know with his new IO urine uh IO you know IO urine API for",
    "start": "2323079",
    "end": "2330520"
  },
  {
    "text": "Linux and this is arriving on the scene and starting to make waves and the reason is that like yeah it's just",
    "start": "2330520",
    "end": "2336640"
  },
  {
    "text": "single-handedly fixed linux's asynchronous IO problem he landed a series of magnificent patches to the",
    "start": "2336640",
    "end": "2343060"
  },
  {
    "text": "kernel um he actually almost started the end of 2018 just you know touching AIO a little",
    "start": "2343060",
    "end": "2350440"
  },
  {
    "text": "bit and then Jan you know that there comes the first IO urine patch early",
    "start": "2350440",
    "end": "2356140"
  },
  {
    "text": "2019 um so and here he gives you know user space a ring buffer to submit IO to the",
    "start": "2356140",
    "end": "2363040"
  },
  {
    "text": "kernel without blocking and then he gives the colonel another ring buffer to send i o completion callbacks back to",
    "start": "2363040",
    "end": "2369760"
  },
  {
    "text": "user space so without blocking so no assist calls or it could be amortized no need for user space thread pool you can",
    "start": "2369760",
    "end": "2376540"
  },
  {
    "text": "now have a single threaded event Loop control plane and then and you can use the Colonel's own thread pool as your data plan right no more no more thread",
    "start": "2376540",
    "end": "2383740"
  },
  {
    "text": "pool but get this like you get to use the Kernel's own thread pool as your data plan so it's much more efficient",
    "start": "2383740",
    "end": "2389980"
  },
  {
    "text": "also significantly simple which is simpler which I really love so if you've",
    "start": "2389980",
    "end": "2395020"
  },
  {
    "text": "enjoyed Martin Thompson's brilliant talks with UConn you know over the years as much as I have then you'll recognize",
    "start": "2395020",
    "end": "2402099"
  },
  {
    "text": "the design of IO urine has what Martin calls mechanical sympathy and if you",
    "start": "2402099",
    "end": "2407980"
  },
  {
    "text": "don't know this word I know you didn't watch Martin's talks but please go watch them they're fantastic they've they've",
    "start": "2407980",
    "end": "2413020"
  },
  {
    "text": "made a huge impact on tiger Beetle's design but I think you know again IO urine is a beautiful design what I love",
    "start": "2413020",
    "end": "2420339"
  },
  {
    "text": "most of all about eye earring is how it not only gives the perfect interface for file and disk IO but also for Network IO",
    "start": "2420339",
    "end": "2428020"
  },
  {
    "text": "you know so historically storage and network with different apis now with our",
    "start": "2428020",
    "end": "2434020"
  },
  {
    "text": "earring you've got a unified interface for both so we really couldn't ask for a better I API in fact it's so good I",
    "start": "2434020",
    "end": "2441460"
  },
  {
    "text": "think IO hearing alone is a perfect excuse you know to redesign the way that our databases do i o in 2023 so a",
    "start": "2441460",
    "end": "2449500"
  },
  {
    "text": "round of applause please uh given these major design changes",
    "start": "2449500",
    "end": "2455200"
  },
  {
    "text": "already at what point do we start to consider the next Studios of databases so if we're going to write new databases",
    "start": "2455200",
    "end": "2461740"
  },
  {
    "text": "for the future we've we've made that you know that decision we're going to do this um if we're going to write them with new",
    "start": "2461740",
    "end": "2467740"
  },
  {
    "text": "designs what languages are we going to write them in are we going to use the systems languages of the last 30 years",
    "start": "2467740",
    "end": "2474460"
  },
  {
    "text": "CEC plus plus are we going to use the systems languages of the next three years so these questions were on our",
    "start": "2474460",
    "end": "2481359"
  },
  {
    "text": "mind when we were deciding whether to write tiger beetle in C or else in a new systems language called Zig that we've",
    "start": "2481359",
    "end": "2488020"
  },
  {
    "text": "been following for two years you know by this point so Barbara liskov said that",
    "start": "2488020",
    "end": "2493660"
  },
  {
    "text": "if you want to teach programmers new ideas you need to give them new languages to think those ideas in",
    "start": "2493660",
    "end": "2501040"
  },
  {
    "text": "what we came to appreciate with Zig is that it fixed all the issues we had with C also made it easier to write correct",
    "start": "2501040",
    "end": "2508660"
  },
  {
    "text": "code but it also resonated with our thinking on how you work with memory efficiently",
    "start": "2508660",
    "end": "2515079"
  },
  {
    "text": "as you do systems programming memory is probably the most important aspect of systems programming how you",
    "start": "2515079",
    "end": "2522640"
  },
  {
    "text": "how good are you you know working with memory how sharp are your tools to do that so for example it was refreshing",
    "start": "2522640",
    "end": "2529839"
  },
  {
    "text": "how easily we could Implement direct IO in Zig where all the memory that you pass to the disk is sector aligned where",
    "start": "2529839",
    "end": "2537160"
  },
  {
    "text": "you can enforce this in the type system even the allocators are just they give you you don't need special",
    "start": "2537160",
    "end": "2543540"
  },
  {
    "text": "calls anymore you know just to do an aligned allocation with Zig there's also",
    "start": "2543540",
    "end": "2549160"
  },
  {
    "text": "no second class macro language instead you simply program in Zig at compile",
    "start": "2549160",
    "end": "2554500"
  },
  {
    "text": "time as your binary is being compiled so your meter language is also a zig so we",
    "start": "2554500",
    "end": "2560920"
  },
  {
    "text": "considered rust but IO urine and the ability to use the kernel thread pool without context switches meant we had",
    "start": "2560920",
    "end": "2569260"
  },
  {
    "text": "less need or desire for Fearless multi-threading in user space like we",
    "start": "2569260",
    "end": "2574480"
  },
  {
    "text": "didn't actually want to do that we were trying to get away from that so we you know we the biotech would still have",
    "start": "2574480",
    "end": "2580119"
  },
  {
    "text": "been useful for for a single threaded eventually but you know for logical concurrency bugs but no we didn't want",
    "start": "2580119",
    "end": "2586180"
  },
  {
    "text": "to do multi-threading in the first place so we also wanted tiger beetle to follow NASA's power of 10 rules for safety",
    "start": "2586180",
    "end": "2592599"
  },
  {
    "text": "critical code um you have to handle memory allocation failure for your database",
    "start": "2592599",
    "end": "2597839"
  },
  {
    "text": "we also wanted to enforce explicit limits on all research resource usage in",
    "start": "2597839",
    "end": "2604540"
  },
  {
    "text": "the design so even for Loops you know there's no wild true every Loop must terminate and there's an expected you",
    "start": "2604540",
    "end": "2611500"
  },
  {
    "text": "know balance on how long that Loop can possibly run for but another you know",
    "start": "2611500",
    "end": "2617440"
  },
  {
    "text": "big example of this is just tiger Beetle's use of static memory allocation so this means that all the memory that",
    "start": "2617440",
    "end": "2623859"
  },
  {
    "text": "tiger beetle needs is calculated and allocated at startup after that there's",
    "start": "2623859",
    "end": "2629260"
  },
  {
    "text": "no Dynamic allocation at runtime so this is almost a lost art but we wanted to",
    "start": "2629260",
    "end": "2634960"
  },
  {
    "text": "bring it back to enable Target Beetle to decouple database performance from memory allocation you know for extreme",
    "start": "2634960",
    "end": "2641980"
  },
  {
    "text": "memory efficiency and predictable operating experience this means that",
    "start": "2641980",
    "end": "2647619"
  },
  {
    "text": "after you start Target Beetle again you know there's no more malloc or free no risk of fragmentation it's just pure",
    "start": "2647619",
    "end": "2654520"
  },
  {
    "text": "predictable performance so we've made resource usage explicit like this",
    "start": "2654520",
    "end": "2659740"
  },
  {
    "text": "throughout the design with memory with other resources as well it's like c",
    "start": "2659740",
    "end": "2664780"
  },
  {
    "text": "groups in your database and this is striking I think because you know compared to designs where the limits",
    "start": "2664780",
    "end": "2670839"
  },
  {
    "text": "have not been thought through or made explicit I really we really enjoy this",
    "start": "2670839",
    "end": "2676480"
  },
  {
    "text": "model of of you know building the database and because tiger beetle makes",
    "start": "2676480",
    "end": "2682359"
  },
  {
    "text": "everything explicit from Storage fault model to Resource usage this means that we can test everything and then actually",
    "start": "2682359",
    "end": "2688960"
  },
  {
    "text": "like test it see the limit we've got all the limits we can we can now test them so for example to simulate this",
    "start": "2688960",
    "end": "2695560"
  },
  {
    "text": "corruption on the read or write path in the double digit percentages on every node in the cluster even the primary we",
    "start": "2695560",
    "end": "2702700"
  },
  {
    "text": "inject faults you know up to the Limit the theoretical limit and then we check that tiger beetle doesn't truncate",
    "start": "2702700",
    "end": "2709480"
  },
  {
    "text": "committed transactions you know is our right to hit log design working can it heal can the writer head log",
    "start": "2709480",
    "end": "2717819"
  },
  {
    "text": "um you know use replicated redundancy to heal itself can we preserve durability",
    "start": "2717819",
    "end": "2722980"
  },
  {
    "text": "can we remain available so what's powerful about this testing also is that everyone on our team can run this",
    "start": "2722980",
    "end": "2730720"
  },
  {
    "text": "simulator on their local laptop and every bug that the simulator finds can be reproduced deterministically replayed",
    "start": "2730720",
    "end": "2738220"
  },
  {
    "text": "again and again you know for incredible developer velocity as you you know you should be building a new feature you",
    "start": "2738220",
    "end": "2743980"
  },
  {
    "text": "just run the simulator and this is the final major design decision in Target Beetle because the whole database has",
    "start": "2743980",
    "end": "2751720"
  },
  {
    "text": "been designed from the ground up as a deterministic distributed database so",
    "start": "2751720",
    "end": "2757780"
  },
  {
    "text": "this means that all the abstractions are deterministic um for example again the control plane",
    "start": "2757780",
    "end": "2763480"
  },
  {
    "text": "is single threaded so there's no non-determinism from the operating systems thread scheduler you know that",
    "start": "2763480",
    "end": "2769420"
  },
  {
    "text": "could be a source of Randomness um even the source of time you know the",
    "start": "2769420",
    "end": "2774819"
  },
  {
    "text": "clock in each node in the database is deterministic so it's an abstraction that you can tick it's got a tick method",
    "start": "2774819",
    "end": "2781900"
  },
  {
    "text": "just like you would tick the second hand of a clock and then we can shim the source of time or we can shim the disc",
    "start": "2781900",
    "end": "2788680"
  },
  {
    "text": "you know or the message bus we can give a network to a message bus that's actually backed by a packet simulator",
    "start": "2788680",
    "end": "2795220"
  },
  {
    "text": "and then we can run a whole cluster of tiger beetle replicas in a single process so this is the second order",
    "start": "2795220",
    "end": "2801819"
  },
  {
    "text": "effect but if we want because we control the whole simulation we can literally",
    "start": "2801819",
    "end": "2807099"
  },
  {
    "text": "speed up time I kid you not you know so instead of ticking time every 10 milliseconds as we would normally do you",
    "start": "2807099",
    "end": "2814119"
  },
  {
    "text": "know if we want 10 millisecond granularity in our in our timeouts instead of doing that we can tick time",
    "start": "2814119",
    "end": "2819760"
  },
  {
    "text": "you know in a tight all true Loop so that every iteration of the wild tulip",
    "start": "2819760",
    "end": "2824940"
  },
  {
    "text": "is just a hot Loop you know we've simulated 10 milliseconds of real world",
    "start": "2824940",
    "end": "2830200"
  },
  {
    "text": "time in a fraction of the time so we actually worked out the time dilation numbers for this last week and an",
    "start": "2830200",
    "end": "2837400"
  },
  {
    "text": "average run of the tiger beetle simulator it takes just 3.3 seconds to run pretty interesting simulation",
    "start": "2837400",
    "end": "2843940"
  },
  {
    "text": "you know with like 64 committed operations all kinds of stuff just 3.3 seconds",
    "start": "2843940",
    "end": "2850119"
  },
  {
    "text": "um that executes 235 000 you know clock ticks",
    "start": "2850119",
    "end": "2855579"
  },
  {
    "text": "um each of those represent 10 milliseconds of time in the real world um in other words 39 minutes in total so",
    "start": "2855579",
    "end": "2862480"
  },
  {
    "text": "you can run the simulator for 3.3 seconds on your laptop and you've achieved the equivalent of 39 minutes of",
    "start": "2862480",
    "end": "2869859"
  },
  {
    "text": "simulated test Time full of all kinds of network latencies packet drops partitions crashes disorruptions disk",
    "start": "2869859",
    "end": "2877720"
  },
  {
    "text": "slowdowns every possible fault right and if you want to debug something that would normally take 39 minutes to",
    "start": "2877720",
    "end": "2884740"
  },
  {
    "text": "manifest you can now do that in just 3.3 seconds so it's a speed up factor of 712 times",
    "start": "2884740",
    "end": "2893140"
  },
  {
    "text": "you know whereas with existing test harnesses like Jepsen they're fantastic but they're not deterministic uh so you",
    "start": "2893140",
    "end": "2899920"
  },
  {
    "text": "might you know if you find a bug you might not find it again um but also they run in real time so if",
    "start": "2899920",
    "end": "2905380"
  },
  {
    "text": "you want 39 minutes of test time you have you know you have to give up 39 minutes of real world time on your local",
    "start": "2905380",
    "end": "2912040"
  },
  {
    "text": "laptop you don't get the same developer velocity so being able to speed up time like this feels like magic like a silver",
    "start": "2912040",
    "end": "2918640"
  },
  {
    "text": "bullet and what I'm most excited about is that because everything in Target is abstracted you know even the state",
    "start": "2918640",
    "end": "2924760"
  },
  {
    "text": "machine logic you can just take this accounting State machine out put another state machine in and you've got a whole",
    "start": "2924760",
    "end": "2930760"
  },
  {
    "text": "new distributed database you you know but you benefit from all the fault tolerance testing of Target so for",
    "start": "2930760",
    "end": "2936579"
  },
  {
    "text": "example we've done this internally to create our own control plan database in a week before might have taken years and",
    "start": "2936579",
    "end": "2942700"
  },
  {
    "text": "that's why I believe we're really in a new era the rate at which new databases can be created is going to just",
    "start": "2942700",
    "end": "2948700"
  },
  {
    "text": "accelerate and they're going to operate and be tested at you know much tighter tolerances anything we've seen before so",
    "start": "2948700",
    "end": "2956140"
  },
  {
    "text": "each of these advances direct IR political recovery for consensus-based storage explicit storage fault model IO",
    "start": "2956140",
    "end": "2962980"
  },
  {
    "text": "urine an efficient replacement for C and Zig deterministic simulation testing each of these on their own I think makes",
    "start": "2962980",
    "end": "2970240"
  },
  {
    "text": "for a whole new dimension in database design hard to retrofit but taking it",
    "start": "2970240",
    "end": "2975460"
  },
  {
    "text": "together you know these advances in database design are going to unlock an abundance of new correct and high",
    "start": "2975460",
    "end": "2981880"
  },
  {
    "text": "performance open source database Management Systems tailored to The Domain it's a new era for database",
    "start": "2981880",
    "end": "2988660"
  },
  {
    "text": "design and I think it's a good day to do this",
    "start": "2988660",
    "end": "2993420"
  },
  {
    "text": "[Music]",
    "start": "2996080",
    "end": "3002370"
  }
]