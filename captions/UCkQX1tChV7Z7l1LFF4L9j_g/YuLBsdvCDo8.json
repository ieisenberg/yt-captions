[
  {
    "start": "0",
    "end": "171000"
  },
  {
    "text": "[Music]",
    "start": "2520",
    "end": "19179"
  },
  {
    "text": "um I'm going to talk about the Disco map ruce framework so I Travis did a great lead in this morning saying that Hadoop",
    "start": "20080",
    "end": "25519"
  },
  {
    "text": "kind of is all is the end all in um in the map reduce world and um I found",
    "start": "25519",
    "end": "31400"
  },
  {
    "text": "another one disco I I don't work in the Disco project but I do use it from time to time so um when I Peter asked me to",
    "start": "31400",
    "end": "36879"
  },
  {
    "text": "do a talk and map reduce I figured I would go this way because disco is a python map reduce framework um it's",
    "start": "36879",
    "end": "42079"
  },
  {
    "text": "actually Python and earling we'll talk a lot about that in a minute and kind of how it all works but um it is a it is a kind of a nice alternative to Hadoop and",
    "start": "42079",
    "end": "49120"
  },
  {
    "text": "um has a lot of the same features so um I'm just gonna I've got a bunch of slides and some examples that we'll walk",
    "start": "49120",
    "end": "54680"
  },
  {
    "text": "through um it's this isn't going to be interactive um I tried to get some AWS stuff set up and just out of time to",
    "start": "54680",
    "end": "60519"
  },
  {
    "text": "make it really interactive so people can log in and play with it but um but it's pretty easy to set up so we'll show you",
    "start": "60519",
    "end": "65760"
  },
  {
    "text": "that um so to get started um map reduce if you're has anyone here first of all used",
    "start": "65760",
    "end": "71400"
  },
  {
    "text": "disco Hadoop okay so we got some Hadoop people and so so yeah so we'll talk a little",
    "start": "71400",
    "end": "78360"
  },
  {
    "text": "about map reduce then to get started um as as you probably heard map reduce is you know the OS for distributed",
    "start": "78360",
    "end": "83640"
  },
  {
    "text": "computing um you know Google actually has a patent on it which is always fun fun to remind yourself of when you start",
    "start": "83640",
    "end": "90240"
  },
  {
    "text": "using this that any day now you might you might have to stop or pay for it but I doubt that'll happen but um it is sold",
    "start": "90240",
    "end": "96479"
  },
  {
    "text": "as something that you know works on large amounts of of of data distributed across thousands of commodity computers",
    "start": "96479",
    "end": "101640"
  },
  {
    "text": "and um I'm kind of throw that in there at the beginning to mention this because one thing I've actually learned and you'll see kind of at as we get to the",
    "start": "101640",
    "end": "107960"
  },
  {
    "text": "end of this too is that um that thousands of commodity computers part is kind of important for really getting map ruce um getting the most out of it um",
    "start": "107960",
    "end": "116000"
  },
  {
    "text": "but the the overall um idea of map reduce is pretty straightforward um you have a a data stream coming in with a",
    "start": "116000",
    "end": "121600"
  },
  {
    "text": "number of Records in it um often times it's just a line from lines from a log file or something um you take the",
    "start": "121600",
    "end": "126880"
  },
  {
    "text": "mapping the mapping step takes those and returns a list of key value pairs that are derived from that from each record",
    "start": "126880",
    "end": "133400"
  },
  {
    "text": "um there's a potential a possible sort or partitioning step that happens next um Hadoop does a big sort in the middle",
    "start": "133400",
    "end": "139519"
  },
  {
    "text": "um as we'll see with um with disco it actually uses partitioning and then an optional optional sort but anyway you're",
    "start": "139519",
    "end": "145480"
  },
  {
    "text": "you take the you take the um the key value pairs from your math step and and bucket them somehow and then feed all of those buckets onto",
    "start": "145480",
    "end": "151760"
  },
  {
    "text": "a reduced phase which um can go through and either um you know sum up all the numbers and give you a single number or",
    "start": "151760",
    "end": "156959"
  },
  {
    "text": "it can actually give you a list back and then you can feed those lists back into a new map job and chain them together forever and um have something that looks",
    "start": "156959",
    "end": "163840"
  },
  {
    "text": "a lot like list um that is um distributed in big um so it's um that's",
    "start": "163840",
    "end": "169599"
  },
  {
    "text": "that's the basic Paradigm it's actually really simple um if you kind of look at the data flow here um one of the big things with map reduce is you really are",
    "start": "169599",
    "end": "175879"
  },
  {
    "start": "171000",
    "end": "171000"
  },
  {
    "text": "trying to do this on in a distributed environment across um a fairly large across clusters so so looking at looking",
    "start": "175879",
    "end": "181800"
  },
  {
    "text": "at the details here a little more you can kind of see I'm breaking this up into a number of different mapping jobs um the idea here is all the mapping jobs",
    "start": "181800",
    "end": "188640"
  },
  {
    "text": "are embarrassingly parallel jobs you can operate on the input stream um completely in parallel so you have a",
    "start": "188640",
    "end": "193799"
  },
  {
    "text": "bunch of Records fanned out to different mapping jobs which then spit back the key value pairs partition them and then",
    "start": "193799",
    "end": "200000"
  },
  {
    "text": "feed them into the reduce phase and give you each reduced phase gives you a result and and most map redu engines um",
    "start": "200000",
    "end": "205400"
  },
  {
    "text": "kind of give you the results of separate files from each reduced phase and then you know use python or pearler something",
    "start": "205400",
    "end": "210640"
  },
  {
    "text": "to put them back together and um this goes no different than that you end up with results from each reducer that you work with at the end",
    "start": "210640",
    "end": "218720"
  },
  {
    "text": "um kind of looking at it from the job job distribution perspective um it's it's kind of what you would",
    "start": "218720",
    "end": "225080"
  },
  {
    "start": "222000",
    "end": "222000"
  },
  {
    "text": "expect you have mapping jobs on a number of nodes um what's interesting is a lot of the M Hadoop and disco both um don't",
    "start": "225080",
    "end": "231640"
  },
  {
    "text": "really rely on don't require you to have the map and the reduce jobs in the same node so there's a lot of network traffic that actually happens in between these",
    "start": "231640",
    "end": "237360"
  },
  {
    "text": "there are ways to kind of force things to stay locally but um when you're when you're doing this it's it's assumed that um the mapping jobs and the reducing",
    "start": "237360",
    "end": "243640"
  },
  {
    "text": "jobs may be on different nodes so the streams are kind of merged um at the network level and passed out to",
    "start": "243640",
    "end": "248879"
  },
  {
    "text": "different nodes um and a lot of the intermediate results might be stored in the local file system um or the you know",
    "start": "248879",
    "end": "254400"
  },
  {
    "text": "the distributed file system so hdfs or ddfs we'll talk about that in a second but the notion of a distributed file",
    "start": "254400",
    "end": "260120"
  },
  {
    "text": "system is actually underlies all of these and I and that's kind of the foundation and I think Travis actually",
    "start": "260120",
    "end": "265280"
  },
  {
    "text": "mentioned that hdfs is what most people are actually using to dup for is the distributed file system system rather",
    "start": "265280",
    "end": "270320"
  },
  {
    "text": "than the actual map reduce part um and the whole idea of the distributed file systems on on both both ones for disco",
    "start": "270320",
    "end": "276280"
  },
  {
    "text": "and and Hadoop are you basically take your input files chunk them up across a bunch of clusters have a few redundant",
    "start": "276280",
    "end": "282120"
  },
  {
    "text": "copies so you might have three copies of each file split up across the across the Clusters and then it can move your",
    "start": "282120",
    "end": "287240"
  },
  {
    "text": "mapping jobs to where the files are so you usually have your mapping jobs as fairly large or or your files in Fairly",
    "start": "287240",
    "end": "292360"
  },
  {
    "text": "large chunks so I think last time I used to dup it was 64 megabytes is kind of the smallest recommended chunk size for",
    "start": "292360",
    "end": "298960"
  },
  {
    "text": "for hdfs and often times um when I work with this I have fairly large files in my domain and I'll just use those whole",
    "start": "298960",
    "end": "305639"
  },
  {
    "text": "files um they're you know hundreds of gigabytes and you'll just kind of put those across the cluster and then move the computations over to them but um the",
    "start": "305639",
    "end": "312120"
  },
  {
    "text": "whole idea of that distributed file system is kind of integral to how this how this works is that you have the idea",
    "start": "312120",
    "end": "317479"
  },
  {
    "text": "is is that you have um fast no or or faster dis F faster access on your local",
    "start": "317479",
    "end": "322520"
  },
  {
    "text": "nodes than you would if you were doing a big um sand or you know kind of the the other storage Alternatives and um there",
    "start": "322520",
    "end": "328639"
  },
  {
    "text": "are some caveats there kind of touch at on the end to that we've learned from doing this in a number of different environments but um but overall it's",
    "start": "328639",
    "end": "335280"
  },
  {
    "text": "that's that's that's the basic idea and it's pretty straightforward um it's easy to easy to deploy these on a cluster easy to deploy them on Amazon um and",
    "start": "335280",
    "end": "342520"
  },
  {
    "text": "they're pretty easy to work with um so disco is the one we're going to I'm going to talk about for the next hour or",
    "start": "342520",
    "end": "348479"
  },
  {
    "text": "so and disco is a map reduce engine built in Python and earling so it's actually not pure python the um",
    "start": "348479",
    "end": "355319"
  },
  {
    "text": "distributed processing part is all handled by llang so they actually um use the llang cord to fire off um jobs and",
    "start": "355319",
    "end": "361600"
  },
  {
    "text": "all the nodes and then all the client code is actually done in Python so you write your mappers and your reducers in",
    "start": "361600",
    "end": "366680"
  },
  {
    "text": "Python and those get distributed out to out using the Earl um distributed programming environments and then they",
    "start": "366680",
    "end": "373319"
  },
  {
    "text": "fire up local python instances on the nodes and run everything um in Python",
    "start": "373319",
    "end": "378639"
  },
  {
    "text": "but the actual distribution is all handled with the ear laying engine and it's actually um I've been through the code a number of times um trying to",
    "start": "378639",
    "end": "385520"
  },
  {
    "text": "track down various installation issues at different times and the earlan code is is pretty small I think only about 6,000 lines of code it's pretty easy to",
    "start": "385520",
    "end": "391639"
  },
  {
    "text": "track through and the python code is about the same too there's not a lot of um it's it's not it's not a fairly large",
    "start": "391639",
    "end": "396919"
  },
  {
    "text": "code base um to do all this which is kind of what you would expect I mean if you kind of think about map reduce it's not that complicated and python working",
    "start": "396919",
    "end": "403639"
  },
  {
    "text": "on um you know kind of with the lists as a lists and tles as kind of main data",
    "start": "403639",
    "end": "409120"
  },
  {
    "text": "types there's not a lot of you know you don't have to write a lot of infrastructure to move data around using those um those yeah those paradigms so",
    "start": "409120",
    "end": "416639"
  },
  {
    "text": "disco actually the whole python idea works really well um in here and when you look through the code it's implemented a lot kind of the way you",
    "start": "416639",
    "end": "423000"
  },
  {
    "text": "would expect it to when you start writing mappers and reducers um which is which is always kind of neat to see um",
    "start": "423000",
    "end": "430080"
  },
  {
    "text": "it does have a distributed file system called ddfs which is um Disco distributed file system um and ddfs is a",
    "start": "430080",
    "end": "437720"
  },
  {
    "text": "tag based distributed file system so there's no file hierarchy um it stores",
    "start": "437720",
    "end": "443000"
  },
  {
    "text": "everything as either files or chunks and you basically when you when you load up a file the first time you give it the",
    "start": "443000",
    "end": "448520"
  },
  {
    "text": "default tag for that file file and then afterwards you can have um more tags pointing to those files or the chunks",
    "start": "448520",
    "end": "454199"
  },
  {
    "text": "that you're doing so um the example I have here is um one one that we do we have um um I'll talk a little bit about",
    "start": "454199",
    "end": "460360"
  },
  {
    "text": "our domain in a second but we do um Gene sequencing is what we do so we have these files with um a lot of sequencing",
    "start": "460360",
    "end": "465479"
  },
  {
    "text": "reads inside of them so um in this case you know I have a bunch of sequencing reads that are are stored and the and",
    "start": "465479",
    "end": "470960"
  },
  {
    "text": "the main tag really is just the the instrument run name so where that came from and that's the easy the canonical way to get back to any set of reads but",
    "start": "470960",
    "end": "477680"
  },
  {
    "text": "then on top of that you know we might have different experim ments we might actually want to um you know tag them by instrument tag them by the user the",
    "start": "477680",
    "end": "483520"
  },
  {
    "text": "person who put them in so we can build these nice tag hierarchies group those by projects so then when you want to",
    "start": "483520",
    "end": "488960"
  },
  {
    "text": "when you want a collection of files out of this you just give it one of those tags and it'll go through and resolve all the files associated with it so if",
    "start": "488960",
    "end": "494960"
  },
  {
    "text": "you want to perform a map operation across you know all the sequencing runs in you know Project X you just say",
    "start": "494960",
    "end": "500199"
  },
  {
    "text": "Project X as your input and then it'll take care of finding all the files for you and feed those to the mapping jobs",
    "start": "500199",
    "end": "505879"
  },
  {
    "text": "um and it's it's it's actually nice I if youve used and the other kind of um column store type databases where using",
    "start": "505879",
    "end": "512279"
  },
  {
    "text": "tags it's similar you end up building these schemas for your keys you know that often times have lots of um colons",
    "start": "512279",
    "end": "517560"
  },
  {
    "text": "inside of them so you can you know Trace down and and use that but it's um it's a",
    "start": "517560",
    "end": "522839"
  },
  {
    "text": "nice way of doing this um and it's pretty pretty easy once you start putting your data in there um so really this is a simple ddfs",
    "start": "522839",
    "end": "530560"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "example right here um I have a bunch of code examples in here I I'm not going to run any of them but I try to get as much",
    "start": "530560",
    "end": "535920"
  },
  {
    "text": "in here um and this is loading it that's just the ddfs S command and um you can chunk it up so in this case we're",
    "start": "535920",
    "end": "541519"
  },
  {
    "text": "loading the Holy Grail script um just into it um then you can give it a command the blobs command shows you",
    "start": "541519",
    "end": "547880"
  },
  {
    "text": "where what what um what what objects are actually loaded into your data data store and then you can use um they have",
    "start": "547880",
    "end": "553920"
  },
  {
    "text": "a cat xcat so you can actually go and stream the files back to your console and there's um a whole lot of other",
    "start": "553920",
    "end": "559519"
  },
  {
    "text": "features you can do on here to manage your files um it does use gzip um if you're chunking data um if you're not",
    "start": "559519",
    "end": "565200"
  },
  {
    "text": "chunking data it stores it in its native format and that's useful to know if you're um dealing with especially binary data we actually deal",
    "start": "565200",
    "end": "571440"
  },
  {
    "text": "with a lot of binary data in these setups so we don't tend to chunk it um but it is stored and it's native format",
    "start": "571440",
    "end": "577200"
  },
  {
    "text": "when you're doing that way so you don't have to worry about lines getting Rewritten or anything",
    "start": "577200",
    "end": "582399"
  },
  {
    "text": "weird um so a simple example um word counting is hello world of map redu so",
    "start": "582399",
    "end": "587800"
  },
  {
    "start": "585000",
    "end": "585000"
  },
  {
    "text": "if you've ever tried Hadoop or anything else you've probably seen this example this is um this is word counting in",
    "start": "587800",
    "end": "592959"
  },
  {
    "text": "Disco right here so you can see um the pointer yeah I do have a pointer good um so so you can see it's it's",
    "start": "592959",
    "end": "600360"
  },
  {
    "text": "pretty straightforward you define your mapper and in this case it's just a simple generator function so we have the map it takes a line in and um there are",
    "start": "600360",
    "end": "607120"
  },
  {
    "text": "some parameters some runtime parameters you can pass around um and um in in this",
    "start": "607120",
    "end": "612920"
  },
  {
    "text": "case we're just going to go and take each word in the line so we're going to partition the split up the line into a bunch of words and then for each word",
    "start": "612920",
    "end": "618560"
  },
  {
    "text": "we're just going to return a count of one so that's the mapping step so what you get out of the mapping step is a stream of words with the the um number",
    "start": "618560",
    "end": "625200"
  },
  {
    "text": "one next to them and so what the reduce step is going to do is it's going to take those words and the count",
    "start": "625200",
    "end": "630320"
  },
  {
    "text": "associated with it and sum those counts so in the middle here is where you actually um so you're using actually",
    "start": "630320",
    "end": "636480"
  },
  {
    "text": "we're doing the sort inside here so this um this KV group sorted it is actually going to be sorting the um the words for",
    "start": "636480",
    "end": "642040"
  },
  {
    "text": "us and then we're going to get one word with the counts out you can sum the counts out some of the counts and um",
    "start": "642040",
    "end": "647120"
  },
  {
    "text": "that'll give you the total word counts in the document you were looking at um so really really nice and simple um it's",
    "start": "647120",
    "end": "652279"
  },
  {
    "text": "python so it's super easy to read um you can just write it the way you would all of this lives in one file so it's it's",
    "start": "652279",
    "end": "657839"
  },
  {
    "text": "pretty self-contained um when you run it you just create a new a new job here um give it a name um give it some input",
    "start": "657839",
    "end": "665320"
  },
  {
    "text": "what one kind of nice thing is they have a lot of different input types um that you can use so you can put um HTTP in",
    "start": "665320",
    "end": "671000"
  },
  {
    "text": "front of something you can put ddfs in front of it to kind of tell it where to get the data from and so it know it knows a couple different protocols so in",
    "start": "671000",
    "end": "676880"
  },
  {
    "text": "this case um this is a ddfs load right here the one that's commented out so I'm just giving it the python Grail script",
    "start": "676880",
    "end": "683440"
  },
  {
    "text": "tag so that is the key key value pair I used for that one or I can just tell it to go get it from the source and in this",
    "start": "683440",
    "end": "688760"
  },
  {
    "text": "case it'll just pull it off the web um right right as that for the mapping job it'll just load it stream it straight",
    "start": "688760",
    "end": "694160"
  },
  {
    "text": "from the web into the mapping job and you don't have to use ddfs at all um and",
    "start": "694160",
    "end": "699399"
  },
  {
    "text": "then um you set up the map job the reduced job um tell it to save some results and then just wait on it and",
    "start": "699399",
    "end": "705320"
  },
  {
    "text": "when it's done you have a result iterator that you get out and just print them out so um these are the results that came out of out of this run so you",
    "start": "705320",
    "end": "711839"
  },
  {
    "text": "can see you just look at the top ones there's all the args from the script which actually I I thought they said ARG",
    "start": "711839",
    "end": "717920"
  },
  {
    "text": "more in the movie but they did say it a lot of different ways so so that was um that was an",
    "start": "717920",
    "end": "724440"
  },
  {
    "text": "interesting thing in there um and then you can see the s the word they actually sorted by count you know the is almost",
    "start": "724440",
    "end": "729639"
  },
  {
    "text": "always the most common thing in when you do word counting and then Arthur you know the actual label they used for the",
    "start": "729639",
    "end": "735839"
  },
  {
    "text": "script lines was the next biggest one um so that's the that's a really simple",
    "start": "735839",
    "end": "740959"
  },
  {
    "text": "really simple example um of using disco um and really that's actually all you need to know to do most disco",
    "start": "740959",
    "end": "747040"
  },
  {
    "text": "applications right there is just write a map or write a redu or um think python when you're doing it and the whole",
    "start": "747040",
    "end": "752680"
  },
  {
    "text": "system handles everything else magically for you which is really nice um I'm going to do a little aside here and just",
    "start": "752680",
    "end": "759360"
  },
  {
    "text": "kind of catch you up to the domain I come from because the rest of the examples are all going to use stuff that I work on daily it was for me it was a",
    "start": "759360",
    "end": "766120"
  },
  {
    "text": "lot easier to just take a lot of stuff we're already doing um so I work in biotech um Next Generation sequencing is",
    "start": "766120",
    "end": "773040"
  },
  {
    "text": "what we do a lot of is anyone here in biotech or sequencing um okay so one",
    "start": "773040",
    "end": "778360"
  },
  {
    "text": "person so um so so really quick the stump speech for this is um really simple um we did the Human Genome",
    "start": "778360",
    "end": "784160"
  },
  {
    "text": "Project um kind of n you know throughout the 90s about 10 to 12 Years A Time thousands of sequencers billions of",
    "start": "784160",
    "end": "790360"
  },
  {
    "text": "dollars generated coverage of one human um and only about only about 7x coverage",
    "start": "790360",
    "end": "795440"
  },
  {
    "text": "of one person um so about 21 billion bases were generated in that project um today we can um it's it's a little",
    "start": "795440",
    "end": "802560"
  },
  {
    "text": "different so we have um High throughput sequencing instruments came on the scene around um 2006 and the original ones",
    "start": "802560",
    "end": "809240"
  },
  {
    "text": "kind of took about two weeks um generated a little generated about the same as the Human Genome Project today",
    "start": "809240",
    "end": "814279"
  },
  {
    "text": "they're doing um about 100 to 200 all the way up to 600 gigabases of data per single run about5 to $6,000 um depending",
    "start": "814279",
    "end": "821959"
  },
  {
    "text": "on the discounts you're getting um there's benchtop ones too now that basically can do similar things actually",
    "start": "821959",
    "end": "827560"
  },
  {
    "text": "do way more than the Human Genome Project in about four hours turnaround time single instrument for about 500 bucks so this has really changed",
    "start": "827560",
    "end": "834240"
  },
  {
    "text": "everything um but what's interesting about this is this it's just a sheer amount of data so if you can imagine 100 G bases of data is 800 gabes of a g C's",
    "start": "834240",
    "end": "843360"
  },
  {
    "text": "and T's right there so you're getting very large data files off the instruments um you know we we see a cou",
    "start": "843360",
    "end": "849079"
  },
  {
    "text": "you know when we're running the big instruments you know we can run those four times a day and you know we can generate you know four or five terabytes",
    "start": "849079",
    "end": "854320"
  },
  {
    "text": "a day off these instruments just off of a single instrument um so it's a lot of data um kind of give you an idea of what",
    "start": "854320",
    "end": "861720"
  },
  {
    "text": "what the data actually is it's it's over here on the edge so um if you kind of think about what the the way sequencing",
    "start": "861720",
    "end": "867040"
  },
  {
    "start": "864000",
    "end": "864000"
  },
  {
    "text": "actually works is you um take your DNA and in your cell from your sample you CH fragment it up into a bunch of little",
    "start": "867040",
    "end": "872880"
  },
  {
    "text": "pieces um you a couple hundred bases long and then those are what you actually sequence and then you have to put all those back together so what you",
    "start": "872880",
    "end": "879480"
  },
  {
    "text": "get off the instrument is actually a file that looks exactly like this that has um a little label for each sequence",
    "start": "879480",
    "end": "885079"
  },
  {
    "text": "that it found along with the actual sequence that it read so um you get",
    "start": "885079",
    "end": "890120"
  },
  {
    "text": "basically all of these really these um fairly short strings you know usually 50 to a couple hundred bases long is what you get for each read off the instrument",
    "start": "890120",
    "end": "896959"
  },
  {
    "text": "um and then all the algorithms that you do afterwards go and put those back together and tell you how they actually map back to the human genome reference",
    "start": "896959",
    "end": "903360"
  },
  {
    "text": "or whatever species you're working on um so it's it's a real big data problem um",
    "start": "903360",
    "end": "909600"
  },
  {
    "text": "and I say big with a capital B because um it it really is you know we we deal with terabytes on a daily basis um so",
    "start": "909600",
    "end": "917800"
  },
  {
    "start": "910000",
    "end": "910000"
  },
  {
    "text": "it's a lot of data and it's it's one of those ones that um there's no way around it either the the amount of data we have",
    "start": "917800",
    "end": "923000"
  },
  {
    "text": "to generates driven by the biology it's not driven by what the instruments can do or anything else if you want to ask a specific question um on a human on a",
    "start": "923000",
    "end": "930199"
  },
  {
    "text": "human so say you want to find all the variations between you and the reference you have to generate about 15 to 30X",
    "start": "930199",
    "end": "936079"
  },
  {
    "text": "coverage of your genome so you're right away looking at you know kind of that 40 45 to 90 gigabases of data that you have",
    "start": "936079",
    "end": "942079"
  },
  {
    "text": "to generate which when you're actually working with it you it explodes a little bit so you've got a couple hundred gigabytes of data per run that you're",
    "start": "942079",
    "end": "948000"
  },
  {
    "text": "working with um and we can generate that fairly quickly and this is kind of a slide from showing people what our",
    "start": "948000",
    "end": "954720"
  },
  {
    "text": "instruments can do um which is actually getting better and better every every every month um and that's kind of my",
    "start": "954720",
    "end": "961600"
  },
  {
    "text": "point here is that it's probably better just to run away now and forget about this problem because there's just too much data but um but but it's we do get",
    "start": "961600",
    "end": "968440"
  },
  {
    "text": "a lot of data and it's kind of fun to work with it um so um so I'm going to use from for",
    "start": "968440",
    "end": "974639"
  },
  {
    "text": "the rest of this I'm going to use some sequencing data just as kind of examples of how we can do do different things in Disco so this is going to show some",
    "start": "974639",
    "end": "980040"
  },
  {
    "text": "techniques in Disco um but also you know kind of show it's not going to go really into biology at all but um we'll talk",
    "start": "980040",
    "end": "986360"
  },
  {
    "text": "about some of the some of the some of the data you'll see some of the data in here um so so",
    "start": "986360",
    "end": "992680"
  },
  {
    "text": "the first thing that um you'll kind of notice is if you look at you know the files I showed you before the sequencing data comes off the instrument it's not",
    "start": "992680",
    "end": "998759"
  },
  {
    "text": "single line based and a lot of um you know most the default for map reduce engines is just to feed you one line at",
    "start": "998759",
    "end": "1003839"
  },
  {
    "text": "a time and since each step in a map phase is is essentially has to be an atomic step you can't really remember",
    "start": "1003839",
    "end": "1009920"
  },
  {
    "start": "1004000",
    "end": "1004000"
  },
  {
    "text": "what the line you saw before was um a lot of our the data that comes off the instruments is actually spans um two to",
    "start": "1009920",
    "end": "1015880"
  },
  {
    "text": "four lines depending on the format we're using so the first line is the ID then in this case you get the sequence information um and you have a plus um",
    "start": "1015880",
    "end": "1023800"
  },
  {
    "text": "and then you have the um Quality information so you actually have four lines here that is really what a single",
    "start": "1023800",
    "end": "1028880"
  },
  {
    "text": "record is so if you want to work on this using disco you'll have to write you you can write a new input stream um reader",
    "start": "1028880",
    "end": "1034798"
  },
  {
    "text": "that will actually go through and take this pull out every four lines and then return it as a tuple to the mapping",
    "start": "1034799",
    "end": "1040400"
  },
  {
    "text": "phase so now the mapping phase instead of getting a line is getting a tuple that has the um the sequencing data",
    "start": "1040400",
    "end": "1045558"
  },
  {
    "text": "reformatted in a way that's pretty that's easy to work with so taking in the four lines up here and then",
    "start": "1045559",
    "end": "1051559"
  },
  {
    "text": "returning the tupple right here and then you end up with a stream of single single tuples that have all the information that you want to work in so",
    "start": "1051559",
    "end": "1058960"
  },
  {
    "text": "using the um custom input streams is kind of the first thing that you'll find yourself doing often if you're using",
    "start": "1058960",
    "end": "1064360"
  },
  {
    "text": "disco um and have anything other than like really straightforward log data that you're parsing if you have complicated data um that works in that",
    "start": "1064360",
    "end": "1071080"
  },
  {
    "text": "and I'll have an example in a little bit that shows um how to do this with binary data too so there's you can you can work",
    "start": "1071080",
    "end": "1076559"
  },
  {
    "text": "with pretty much anything you want um and it's it's really just how clever you are on writing your input streams",
    "start": "1076559",
    "end": "1083640"
  },
  {
    "text": "um so the the next thing um is that sort sorting step that happens in the middle",
    "start": "1083640",
    "end": "1088919"
  },
  {
    "text": "and this is kind of where disco differs um a lot from Hadoop so Hadoop by default does um its mapping phase then",
    "start": "1088919",
    "end": "1095159"
  },
  {
    "text": "all of the all of the data is pushed into a sort phase um sorts all the data and then gives it back to and then feeds",
    "start": "1095159",
    "end": "1100679"
  },
  {
    "text": "the sort of data into the into the reducers disco decided to do it a little differently um they don't actually sort",
    "start": "1100679",
    "end": "1106120"
  },
  {
    "text": "in the middle um sorting is completely optional and by default they don't um so they give you all the keys unsorted so you saw in that first",
    "start": "1106120",
    "end": "1112640"
  },
  {
    "text": "example with the word the word count we actually I was actually sorting them kind of as I got them in the reducer um",
    "start": "1112640",
    "end": "1118280"
  },
  {
    "text": "and of course if you've got a very large data set you probably don't want to do an in memory sort um so so that's not",
    "start": "1118280",
    "end": "1125159"
  },
  {
    "text": "not always an option but the the other thing thing that's that's worth noting is when you're working with the mappers",
    "start": "1125159",
    "end": "1130880"
  },
  {
    "text": "normally everything gets fed to a single R reduction step unless you have a way of partitioning those streams coming out",
    "start": "1130880",
    "end": "1136360"
  },
  {
    "start": "1134000",
    "end": "1134000"
  },
  {
    "text": "so um and this is what um the way the way that disco does it is it has a partition step in the middle so as soon",
    "start": "1136360",
    "end": "1142559"
  },
  {
    "text": "as the map step finishes so I'll I'll walk through this in a second um you can you can basically feed feed the key from",
    "start": "1142559",
    "end": "1148440"
  },
  {
    "text": "the mapping step into our partition function and then return an integer and that tells you what partition you should be in so what it'll what it'll do then",
    "start": "1148440",
    "end": "1154919"
  },
  {
    "text": "is it'll create um the number of partitions you have it'll create a reduced job for each one of those",
    "start": "1154919",
    "end": "1159960"
  },
  {
    "text": "partitions and then so you can basically do some do data parallelism on your reduce phase too as long as you um",
    "start": "1159960",
    "end": "1166159"
  },
  {
    "text": "understand how to partition your data so um so so in this case right here um what I'm doing is I'm just taking all the",
    "start": "1166159",
    "end": "1172840"
  },
  {
    "text": "reads off of the instrument and we're going to find the frequencies of of bases at each at each position in the",
    "start": "1172840",
    "end": "1178039"
  },
  {
    "text": "read so in this case the reads um were off of an instrument that only produces 50 base pair reads so we want to see you",
    "start": "1178039",
    "end": "1183280"
  },
  {
    "text": "know did you see you know do see how often do you see an A in position one how often do you see a c in position one",
    "start": "1183280",
    "end": "1188840"
  },
  {
    "text": "and then a g and a t in position one um and you can actually there's the results right there um but it's pretty",
    "start": "1188840",
    "end": "1194520"
  },
  {
    "text": "straightforward so so the really simple way of doing this is for the position I I I wrote a map function that goes through and it just takes the",
    "start": "1194520",
    "end": "1201080"
  },
  {
    "text": "record so it takes the sequence coming in um that's that's the entry one in the um in the tle that I fed is the actual",
    "start": "1201080",
    "end": "1208039"
  },
  {
    "text": "sequence and it just enumerates the position and the base and returns position and base um into the stream so",
    "start": "1208039",
    "end": "1213799"
  },
  {
    "text": "this is kind of like the word counting except in except we feeding um the position along with with the base and",
    "start": "1213799",
    "end": "1219760"
  },
  {
    "text": "then what what I do is um figure out how many partitions we're going to do um that's what the NP is and then the key",
    "start": "1219760",
    "end": "1225960"
  },
  {
    "text": "it gives you is the first thing here is the first element in the tle that we returned from the um the mapping the",
    "start": "1225960",
    "end": "1232000"
  },
  {
    "text": "mapping routine so in this case it's going to be somewhere between 1 and 50 so just um kind of modding that by the",
    "start": "1232000",
    "end": "1238240"
  },
  {
    "text": "number so if I give it 50 partitions you know we'll actually have 50 reduced jobs that'll that'll be be fed into here and",
    "start": "1238240",
    "end": "1243600"
  },
  {
    "text": "then every every Position will have its own reduced job to go and figure out the base frequencies at those positions and",
    "start": "1243600",
    "end": "1249200"
  },
  {
    "text": "each of those reduced steps then will get um will'll get a base out of it and the position and if they're all if it's",
    "start": "1249200",
    "end": "1255880"
  },
  {
    "text": "actually sorted that way you'll actually see all of them um in this case I think I have it set up to um handle seeing any",
    "start": "1255880",
    "end": "1261360"
  },
  {
    "text": "any number of any number of positions in here um so this is kind of the more General case right here but um but then",
    "start": "1261360",
    "end": "1267960"
  },
  {
    "text": "it just goes and it counts for each base that it sees you know every time it sees one at a POS at a position it just adds",
    "start": "1267960",
    "end": "1273039"
  },
  {
    "text": "a count to it and and you end up with the the nice summary plot here and if you've ever done human genomics you'll",
    "start": "1273039",
    "end": "1279000"
  },
  {
    "text": "you'll kind of know that this is pretty much the the way that bases are distributed um across the genome and and",
    "start": "1279000",
    "end": "1284960"
  },
  {
    "text": "what you can see here this is actually you know more for internal QC at the beginning of the reads you can see some",
    "start": "1284960",
    "end": "1290200"
  },
  {
    "text": "bias in specific directions and and this is just kind of an artifact of the way that the sequencing actually works that you um you preferentially um sequ when",
    "start": "1290200",
    "end": "1298640"
  },
  {
    "text": "you go and split up the fragment things you kind of preferentially fragment in certain areas and you can see that a",
    "start": "1298640",
    "end": "1303880"
  },
  {
    "text": "little bit here um what's going on um and sometimes at the end too you'll you'll see things happen",
    "start": "1303880",
    "end": "1311000"
  },
  {
    "text": "there so so that's basically partitioning and how and and an example of how you would use it um in here um so",
    "start": "1311000",
    "end": "1318799"
  },
  {
    "text": "so for this talk um I I wanted to go a little further than what we do so we usually when we don't use disco all the time but when we do we have it running",
    "start": "1318799",
    "end": "1324840"
  },
  {
    "text": "on our internal clusters um and I wanted to see how can we move this into the cloud can we actually set up something",
    "start": "1324840",
    "end": "1330480"
  },
  {
    "text": "like um Amazon's elastic map reduce using disco um you know if you if you kind of Google disco and Amazon you know",
    "start": "1330480",
    "end": "1337039"
  },
  {
    "text": "you find one or two hits of people who tried it but never posted anything else and on the Disco page it says sure you",
    "start": "1337039",
    "end": "1342559"
  },
  {
    "text": "can do it but doesn't give you any examples of how to do it so I wanted to figure out how to actually set up disco on AWS and be able ble to create",
    "start": "1342559",
    "end": "1349000"
  },
  {
    "text": "clusters and use um you use um ec2 to um set up disco jobs and then pull data",
    "start": "1349000",
    "end": "1355679"
  },
  {
    "text": "from different sources and actually process them all on the cloud so um so what I did um have has anyone here",
    "start": "1355679",
    "end": "1362520"
  },
  {
    "text": "who's used ec2 before okay and um star cluster guys use",
    "start": "1362520",
    "end": "1369279"
  },
  {
    "text": "that so so so this is what I ended up doing um just want to see who else knows because I was learning all of this as I",
    "start": "1369279",
    "end": "1375159"
  },
  {
    "text": "was doing this so I always looking for more feedback on the ways but this this was actually really easy so star cluster",
    "start": "1375159",
    "end": "1381279"
  },
  {
    "start": "1377000",
    "end": "1377000"
  },
  {
    "text": "is um basically it's it's out of MIT and it's a tool for setting up clusters on ec2 so what you what it'll let you do um",
    "start": "1381279",
    "end": "1388679"
  },
  {
    "text": "is you create your machine you can create a custom ma machine image and then you can basically tell it to create a cluster for you and it'll go off and",
    "start": "1388679",
    "end": "1395240"
  },
  {
    "text": "you tell it how many nodes you want it just allocates everything for you um it's it's it's actually a nice tool and",
    "start": "1395240",
    "end": "1400360"
  },
  {
    "text": "one of the other reasons I use it here is star cluster is all written in Python 2 so we're actually showing how to do an entire stack entire map reduce stack",
    "start": "1400360",
    "end": "1407039"
  },
  {
    "text": "using pretty much entirely python tools um but when you're working with um so if you worked with machine images on Amazon",
    "start": "1407039",
    "end": "1413880"
  },
  {
    "text": "um Amazon has a bunch of default machine images that kind of have some basic things on it star cluster provides a",
    "start": "1413880",
    "end": "1419240"
  },
  {
    "text": "couple machine images that have a few more scientific tools so IPython is actually available on there as numpy scipi um and and there's there's some",
    "start": "1419240",
    "end": "1427320"
  },
  {
    "text": "other tools that are common scientific Computing tools that that people use a lot are on their images but disco wasn't",
    "start": "1427320",
    "end": "1433279"
  },
  {
    "text": "on their image so the first thing I did here is actually created an image that had disco and a few other Bion from",
    "start": "1433279",
    "end": "1438559"
  },
  {
    "text": "matics libraries that I needed to use on here um and it was pretty straightforward they actually have just a little command line tools where you go",
    "start": "1438559",
    "end": "1444480"
  },
  {
    "text": "fire off a single node instance so that little S1 is telling us to do one instance tell it what you're using as",
    "start": "1444480",
    "end": "1450320"
  },
  {
    "text": "your starting instance um so I just used one of their default ones as that or is their base Ami and then um the instance",
    "start": "1450320",
    "end": "1456520"
  },
  {
    "text": "type is actually the size of the um the the type of instance on the um of",
    "start": "1456520",
    "end": "1461559"
  },
  {
    "text": "compute virtual compute resource you're using so they have like the small ones and the large ones and the extra large and the high memory so that's Amazon",
    "start": "1461559",
    "end": "1467600"
  },
  {
    "text": "things right there um so you can kind of pick what what type of machines you're going to Target on here and in this one",
    "start": "1467600",
    "end": "1472919"
  },
  {
    "text": "I just Ed the small ones I I was trying to keep the price down on all of this too",
    "start": "1472919",
    "end": "1478399"
  },
  {
    "text": "um so so but it's it's pretty straightforward so you create you create a single node um then you just log in",
    "start": "1478399",
    "end": "1484240"
  },
  {
    "text": "and then you log into that node and configure it however you want so in this case I logged in installed disco installed the other things um then you",
    "start": "1484240",
    "end": "1490760"
  },
  {
    "text": "basically tell it to create an image um and you could Amazon has um two different data stores or two different",
    "start": "1490760",
    "end": "1496240"
  },
  {
    "text": "ways of storing these you can store it on S3 which is is kind of the the standard way of storing a lot of data on",
    "start": "1496240",
    "end": "1501840"
  },
  {
    "text": "Amazon or you can use these EBS images um and those are a little faster um so I",
    "start": "1501840",
    "end": "1507000"
  },
  {
    "text": "used one of those for this and so I have an image now created for the Disco for disco that um has all of disco installed",
    "start": "1507000",
    "end": "1513039"
  },
  {
    "text": "and now I can go create disco clusters out of it so um at the bottom here you can kind of see the example for starting a cluster using star cluster um and",
    "start": "1513039",
    "end": "1519520"
  },
  {
    "text": "again it's really really easy you just give it the start command tell it how many nodes you want tell it which um",
    "start": "1519520",
    "end": "1525080"
  },
  {
    "text": "instance type to use and it goes and does everything else for you and then you um Can SSH into it and and have fun",
    "start": "1525080",
    "end": "1531559"
  },
  {
    "text": "with it so um once you're on the once you've got the cluster up and running um you want",
    "start": "1531559",
    "end": "1537520"
  },
  {
    "text": "to go go ahead and get disco up and running on it so you're going to there's going to be a few more steps that we'll do to set up set up disco um the first",
    "start": "1537520",
    "end": "1545279"
  },
  {
    "start": "1541000",
    "end": "1541000"
  },
  {
    "text": "one is I I do here is I actually um so and this is kind of showing some of the configuration and how you can configure",
    "start": "1545279",
    "end": "1550640"
  },
  {
    "text": "so disco disco has a bunch of configuration options that you can go through you can um tell it how many nodes to use for redundency so if you're",
    "start": "1550640",
    "end": "1556880"
  },
  {
    "text": "copying files out to ddfs you can tell it to do three copies or one copy or 10 copies um you can tell it",
    "start": "1556880",
    "end": "1564559"
  },
  {
    "text": "yeah there's a lot of different different commands in there um but one of the ones is where all the default paths are so I am actually just go and",
    "start": "1564559",
    "end": "1569919"
  },
  {
    "text": "change the paths in here to use um one of so there when you when you have an Amazon image you have a couple",
    "start": "1569919",
    "end": "1575880"
  },
  {
    "text": "different default Mount points one of them is the actual image that you mounted and then you've also got kind of",
    "start": "1575880",
    "end": "1581320"
  },
  {
    "text": "your default scratch Pages which is most likely the local drives on the Node that you're running so you get a with a small",
    "start": "1581320",
    "end": "1587640"
  },
  {
    "text": "instance you get a 160 gigabytes that you can work with so I um tell ddfs to use those um the one key thing when",
    "start": "1587640",
    "end": "1594279"
  },
  {
    "text": "you're when you're doing Amazon clusters is those are transient so um if you're trying to use this as a um as a",
    "start": "1594279",
    "end": "1599559"
  },
  {
    "text": "permanent file system you'd want to set up a slightly different strategy than this so this is just using the transient drive so every time I ran this I would",
    "start": "1599559",
    "end": "1605520"
  },
  {
    "text": "have to you know reload all my data over to it um um and then you create a mount point",
    "start": "1605520",
    "end": "1612440"
  },
  {
    "text": "so so create the mount point for the ddfs this is so disco obviously since you know not everyone knows about it is",
    "start": "1612440",
    "end": "1618440"
  },
  {
    "text": "not near does not nearly get the developer resources that um the dup does so there are a few little bugs that I ran into every time I've installed it um",
    "start": "1618440",
    "end": "1625440"
  },
  {
    "text": "and I keep meaning to post the mailing list and help them out on that but this this one gets me every time is that as soon as you point your data to somewhere",
    "start": "1625440",
    "end": "1632360"
  },
  {
    "text": "else you actually have to go through and create these ddfs directories by hand um and then everything works fine after",
    "start": "1632360",
    "end": "1638159"
  },
  {
    "text": "that um and then you get Amazon working um so the control panel for disco is a",
    "start": "1638159",
    "end": "1643919"
  },
  {
    "text": "web app so you have to be able to point your web browser through through to it and they use port 8 989 is the default",
    "start": "1643919",
    "end": "1649360"
  },
  {
    "text": "one so on Amazon you have to update your Security Group to allow in inbound traffic from that Port um and that's a",
    "start": "1649360",
    "end": "1656159"
  },
  {
    "text": "pretty simple step there and then once you've got that set up you can go ahead and fire off disco and um it's really",
    "start": "1656159",
    "end": "1662720"
  },
  {
    "start": "1662000",
    "end": "1662000"
  },
  {
    "text": "simple it's just like kind of like Star cluster it's the Disco command and then you do start and as long as things were set up right it it will just start and",
    "start": "1662720",
    "end": "1669480"
  },
  {
    "text": "run and then you can hit the web app and go see and and right here you can see when you start it up the first time what",
    "start": "1669480",
    "end": "1675000"
  },
  {
    "text": "you end up with is a disco instance with a single node node on here you can see",
    "start": "1675000",
    "end": "1680399"
  },
  {
    "text": "um it tells you how many it gives you some some statistics here's um this this",
    "start": "1680399",
    "end": "1685480"
  },
  {
    "text": "the numbers there will tell you how many jobs are running um in different map map and reduced jobs and I think failed jobs",
    "start": "1685480",
    "end": "1690720"
  },
  {
    "text": "is the third one um and it'll also tell tell you how much storage you have available on your ddfs volume so that",
    "start": "1690720",
    "end": "1696399"
  },
  {
    "text": "little green bar will start filling up as you start adding stuff to each node um so you can use that for monitoring um",
    "start": "1696399",
    "end": "1703200"
  },
  {
    "text": "but one node disco cluster is not that interesting so um you want to go and you'll want to go to add some nodes right away so in Disco the way you do",
    "start": "1703200",
    "end": "1710480"
  },
  {
    "start": "1710000",
    "end": "1710000"
  },
  {
    "text": "that is you just go to the web brows the web app and over to the configuration panel and you just add some nodes so in",
    "start": "1710480",
    "end": "1716640"
  },
  {
    "text": "this case um the naming convention on the star clusters is node and then three three um three numbers so node 0 0 one",
    "start": "1716640",
    "end": "1724440"
  },
  {
    "text": "all the way up to however many nodes you've allocated and disco lets you use kind of a a range notation there so you",
    "start": "1724440",
    "end": "1730240"
  },
  {
    "text": "can go node zero node one to node three and it'll automatically it guesses the naming convention and fills in all the",
    "start": "1730240",
    "end": "1735679"
  },
  {
    "text": "nodes for you and when you do that hit the save table button um which I've",
    "start": "1735679",
    "end": "1740880"
  },
  {
    "text": "forgotten to do and never for some reason that that didn't make sense I",
    "start": "1740880",
    "end": "1747519"
  },
  {
    "text": "lost that the first time like where are all my noes but um their their user interface isn't the most thought out one it's um again it's simple it's nice and",
    "start": "1747519",
    "end": "1753919"
  },
  {
    "text": "once you once you once you've done it once or twice everything starts to make sense um but it also supports",
    "start": "1753919",
    "end": "1759440"
  },
  {
    "text": "blacklisting um for disco and for ddfs so if you want like just dedicated compute nodes you can tell it with you",
    "start": "1759440",
    "end": "1764679"
  },
  {
    "text": "basically Blackness list a bunch of nodes and tell them not to store files on those you can do the opposite if you want dedicated storage nodes um for that",
    "start": "1764679",
    "end": "1771600"
  },
  {
    "text": "so there's there's some things you can do for configuring your cluster but if everything works you end up with um on your on your status page now you'll see",
    "start": "1771600",
    "end": "1778120"
  },
  {
    "text": "you'll see all the nodes that you have allocated so that's basically it once you have it up and running now you've got a we've got a fully functional disco",
    "start": "1778120",
    "end": "1784840"
  },
  {
    "text": "installation running on ec2 so um what can we do with it um let's go ahead and",
    "start": "1784840",
    "end": "1790640"
  },
  {
    "text": "actually I'm going to walk through an example now processing some um some more big data so one of the um one of the",
    "start": "1790640",
    "end": "1796960"
  },
  {
    "text": "neat neat public datas data sets on EC or on S3 is um some data from the Thousand genomes project so the Thousand",
    "start": "1796960",
    "end": "1803840"
  },
  {
    "text": "genomes project is um a big research project where they're going to go fully sequence a thousand individuals and just",
    "start": "1803840",
    "end": "1810000"
  },
  {
    "text": "have have all of the all of their sequence data available for research so it's looking for variation DET variations are the big one they're",
    "start": "1810000",
    "end": "1815760"
  },
  {
    "text": "looking for um but there's a lot of a lot of a lot of studies that you can imagine being done on that on that amount of data um it's a lot of data",
    "start": "1815760",
    "end": "1822840"
  },
  {
    "text": "though so um they've posted um 7.3 terabytes of data on S3 um anyone can go",
    "start": "1822840",
    "end": "1828559"
  },
  {
    "text": "and play with and as I learned very quickly um you really have to understand sequencing and what the data is to",
    "start": "1828559",
    "end": "1834559"
  },
  {
    "text": "actually work with any of the data so it made a great press release when they did it but you you actually look at the data it's it's kind of a low-level sequencing",
    "start": "1834559",
    "end": "1841039"
  },
  {
    "text": "data format that's hard to work with unless you've used it before um they it's actually all posted as binary files so these these binary alignment files",
    "start": "1841039",
    "end": "1847880"
  },
  {
    "text": "which what what so what they've actually posted on here is they've taken all the all the data for the individuals map",
    "start": "1847880",
    "end": "1853080"
  },
  {
    "text": "them back to the human reference and they tell you where all of the reads from the sequencing instruments mapped the human reference so you actually have",
    "start": "1853080",
    "end": "1859279"
  },
  {
    "text": "the results of of the first passive analysis is what they've actually made available to us um to work with and it",
    "start": "1859279",
    "end": "1865360"
  },
  {
    "text": "is a binary format so it's binary data so we're GNA have to figure out how to work with binary data on Disco in this",
    "start": "1865360",
    "end": "1870960"
  },
  {
    "text": "case um so what so so as a really simple example what I wanted to do is just basically take all of the all the genome",
    "start": "1870960",
    "end": "1876919"
  },
  {
    "start": "1874000",
    "end": "1874000"
  },
  {
    "text": "data theyve push put on here and just see what what's the coverage look like for all the chromosomes um so how would",
    "start": "1876919",
    "end": "1882120"
  },
  {
    "text": "we go ahead how do we go through and take take the mapping data do a simple a simple map reduce pipeline streaming all",
    "start": "1882120",
    "end": "1888320"
  },
  {
    "text": "of the BAM files that they have and then create a coverage map of the genome to see if we're actually you know covering all all of what we would expect to see",
    "start": "1888320",
    "end": "1895320"
  },
  {
    "text": "in a genome um so and actually it's not as I changed",
    "start": "1895320",
    "end": "1901279"
  },
  {
    "text": "it from snip to coverage so the slide's a little bit wrong or the title's wrong um but it's pretty straightforward again",
    "start": "1901279",
    "end": "1906519"
  },
  {
    "text": "this is this is um looks a lot like the word counting example um and and all the other ones the the coverage map is",
    "start": "1906519",
    "end": "1912399"
  },
  {
    "text": "really simple um assuming I'm getting a record out of the binary file and we'll talk about that in the next slide how",
    "start": "1912399",
    "end": "1917440"
  },
  {
    "text": "how we actually did that um basically I get the reference and the read passed to me those are the two the two things that",
    "start": "1917440",
    "end": "1923440"
  },
  {
    "text": "are passed in into the map map thing so the references what chromosome it hits so that would be you know chromosome one",
    "start": "1923440",
    "end": "1928960"
  },
  {
    "text": "2 or three or X Y or M or Mt the way it was annotated in here um and then all",
    "start": "1928960",
    "end": "1935679"
  },
  {
    "text": "and then for and then each of those read records is going to have a position where that mapped to the chromosome so",
    "start": "1935679",
    "end": "1941360"
  },
  {
    "text": "the first thing I so what I'm pulling so what I'm returning as my um as my key is I'm returning the chromosome name and",
    "start": "1941360",
    "end": "1946840"
  },
  {
    "text": "then the actual coordinate on that chromosome that the read mapped to and then I'm returning how long that read",
    "start": "1946840",
    "end": "1951880"
  },
  {
    "text": "actually was so depending on the instrument platform you're using you might get 50 Base PA reads but you also",
    "start": "1951880",
    "end": "1956919"
  },
  {
    "text": "a lot of them have variable length reads that you're getting out of there too so so you could have you know up to 800 bases possibly in a read um so that",
    "start": "1956919",
    "end": "1963600"
  },
  {
    "text": "gives you an idea of what the overall coverage would be so you can you can use the start position and the read length to figure out the depth of coverage at",
    "start": "1963600",
    "end": "1969360"
  },
  {
    "text": "that in that area on the genome from that particular read um so so for um the",
    "start": "1969360",
    "end": "1974519"
  },
  {
    "text": "partitioning function I actually just partitioned it kind of logically um using the domain the domain model here",
    "start": "1974519",
    "end": "1980760"
  },
  {
    "text": "and just partition it based on chromosomes so if it's chromosome I I just use simp you know numbering x 24 y",
    "start": "1980760",
    "end": "1987880"
  },
  {
    "text": "25 mitochondrial was zero and then for the 1 to 23 just used ins on there so",
    "start": "1987880",
    "end": "1995638"
  },
  {
    "text": "that um that comes from the cutting and pacing I did to make this fit on the [Laughter]",
    "start": "1997720",
    "end": "2004720"
  },
  {
    "text": "slide it's coming from key so in here it's key so it actually is key um in here so good catch see someone's reading",
    "start": "2004720",
    "end": "2012519"
  },
  {
    "text": "actually reading reading the reading the code here but yeah that's so that's sorry about that that the the key is the",
    "start": "2012519",
    "end": "2017760"
  },
  {
    "text": "chrom so if you see where I build the key up here I put the chromosome name which is coming from reference in the position so right here I'm splitting it",
    "start": "2017760",
    "end": "2025039"
  },
  {
    "text": "um and that right there should actually be CHR but this was reform copying and pasting and reformatting to make it work",
    "start": "2025039",
    "end": "2031600"
  },
  {
    "text": "on here um but yeah so that's that's where that's going so this will create um 26 buckets that um",
    "start": "2031600",
    "end": "2038919"
  },
  {
    "text": "the various chromosome data will will go into so in the end I'll have 26 reduced steps here and 26 um sets of results",
    "start": "2038919",
    "end": "2044799"
  },
  {
    "text": "that I get out of that I get out at the end that that will show me the maps um and then the reduction function",
    "start": "2044799",
    "end": "2051320"
  },
  {
    "text": "is is pretty simple um this actually hits an important thing in um in Disco",
    "start": "2051320",
    "end": "2057000"
  },
  {
    "text": "that's worth talking about a little bit so you'll notice I'm importing numpy inside the reduction function so one of",
    "start": "2057000",
    "end": "2063878"
  },
  {
    "text": "the requirements in Disco is that the map and reduce function should be essentially kind of pure functions um",
    "start": "2063879",
    "end": "2069599"
  },
  {
    "text": "you you really don't have any access to any external data so there's no globals you don't know what modules can be",
    "start": "2069599",
    "end": "2074800"
  },
  {
    "text": "loaded so if you're going to use a module you have to load it inside the function um when you're working on this",
    "start": "2074800",
    "end": "2080118"
  },
  {
    "text": "so so you'll see in here I'm importing numpy directly to work with the data um and create and do it so um that that's",
    "start": "2080119",
    "end": "2087878"
  },
  {
    "text": "just one of the one of the ways that that it's set up and and it actually is the way under the hood the way it's implemented is it actually takes it you",
    "start": "2087879",
    "end": "2095520"
  },
  {
    "text": "know takes the client data essentially um packages it up and ships it all over",
    "start": "2095520",
    "end": "2100839"
  },
  {
    "text": "and then fires it off in in a um so it doesn't have python interpreters running it just takes the running State from",
    "start": "2100839",
    "end": "2106520"
  },
  {
    "text": "here and moves it over there I think is how they're doing it so kind of like what IPython does in a way um but they",
    "start": "2106520",
    "end": "2112280"
  },
  {
    "text": "they've been really trying to make it so you can have very minimal python installations on the nodes or none at all and then be able to move dat move",
    "start": "2112280",
    "end": "2118160"
  },
  {
    "text": "move the move the actual code over there execute it um move selfcontain code from the client side over to to all the nodes",
    "start": "2118160",
    "end": "2124400"
  },
  {
    "text": "and do it um I wish I understood a little more about how it's doing but um that's one of the is one of the important things to know in here um and",
    "start": "2124400",
    "end": "2133040"
  },
  {
    "text": "and and kind of another caveat on that is the map functions you know really get called once for each line so they do",
    "start": "2133040",
    "end": "2138480"
  },
  {
    "text": "give you this params thing and you are allowed to stick some some persistent data in those but there there are some",
    "start": "2138480",
    "end": "2144280"
  },
  {
    "text": "um some some constraints on that too so the parameters data is really persistent to the actual node that you're working",
    "start": "2144280",
    "end": "2150440"
  },
  {
    "text": "on so you can't you can't it's not globally persistent data so you can't use that to like do communication back and forth between map jobs but again",
    "start": "2150440",
    "end": "2157280"
  },
  {
    "text": "that goes with the whole Spirit of map and reduce is that all the map jobs should be independent jobs um the",
    "start": "2157280",
    "end": "2162880"
  },
  {
    "text": "reduction jobs are actually fairly long lived um these usually don't get restarted you feed the stream coming into it and these will these will live",
    "start": "2162880",
    "end": "2168839"
  },
  {
    "text": "the whole time so the way um the way I the way I actually create the chromosome Maps here um is I have a an an just a",
    "start": "2168839",
    "end": "2177240"
  },
  {
    "text": "dictionary that gives me the sizes or rough roughly the sizes of all the chromosomes and then I go ahead and",
    "start": "2177240",
    "end": "2182640"
  },
  {
    "text": "create an array for those that just has a use zeros create a an array that I have just count at every position in the",
    "start": "2182640",
    "end": "2188440"
  },
  {
    "text": "chromosome so these are can be fairly large the largest chromosome is 250 Megs so",
    "start": "2188440",
    "end": "2194119"
  },
  {
    "text": "um that can end up being about two gigs of data that you that you end up allocating there um and actually on the",
    "start": "2194119",
    "end": "2200319"
  },
  {
    "text": "small memory nodes I had to CH drop these off by three orders of magnitude to make it fit in the RAM on the nodes",
    "start": "2200319",
    "end": "2206760"
  },
  {
    "text": "but um but that's the basic idea is create an array here and then stream in all the all the position data that comes",
    "start": "2206760",
    "end": "2211960"
  },
  {
    "text": "out of the um the mapper so I pull out the chromosome find the you know I have one one per chromosome in here so what",
    "start": "2211960",
    "end": "2217920"
  },
  {
    "text": "I'm doing is each reducer will know what chromosome they're working on so they'll only get data from the same chromosome so then when I'm going through here I",
    "start": "2217920",
    "end": "2223880"
  },
  {
    "text": "pull out the position information and just go ahead and um using just slice",
    "start": "2223880",
    "end": "2229240"
  },
  {
    "text": "slice notation start at the position and position plus length and add a count to there and now we kind of know what the",
    "start": "2229240",
    "end": "2234839"
  },
  {
    "text": "coverage at that position on on the chromosome was from this and then I when I'm when I'm done I'll I just return",
    "start": "2234839",
    "end": "2240119"
  },
  {
    "text": "that as a string so it kind of works with the asking formats that they like",
    "start": "2240119",
    "end": "2245240"
  },
  {
    "text": "to ship around from these um so the so the big thing I kind of left out here",
    "start": "2245240",
    "end": "2250440"
  },
  {
    "text": "which goes into the next slide is um this is all binary data and and this is one of the fun things I think with the",
    "start": "2250440",
    "end": "2256040"
  },
  {
    "start": "2256000",
    "end": "2256000"
  },
  {
    "text": "map reduce systems is trying to figure out how how could you work this with any type of data and so usually you can",
    "start": "2256040",
    "end": "2261119"
  },
  {
    "text": "think of how if I have my data in the record format I'm used to working I you can think of a mapper that you'll do and you can think of a reducer but often",
    "start": "2261119",
    "end": "2266960"
  },
  {
    "text": "times if you're working with scientific data or data from you know arbitrary sources you know they're usually is a record format that you're dealing with",
    "start": "2266960",
    "end": "2273119"
  },
  {
    "text": "and if it is binary you got to figure out a way to actually work with that and in this case um it was also data stored",
    "start": "2273119",
    "end": "2279079"
  },
  {
    "text": "on S3 and I didn't really want to copy 7 terabytes of data over to my local nodes because that would have cost me a lot of",
    "start": "2279079",
    "end": "2284400"
  },
  {
    "text": "money um instead I wanted to kind of move the the BAM files over one at a time cach them stream them as binary",
    "start": "2284400",
    "end": "2291520"
  },
  {
    "text": "files and then um get rid of them right away so so the way the reader that I ended up writing for this um it looks",
    "start": "2291520",
    "end": "2298480"
  },
  {
    "text": "like this and and again you can see the importing inside the reader to um get the temp file and the pyam which is the",
    "start": "2298480",
    "end": "2303960"
  },
  {
    "text": "library the domain specific Library I'm using um one kind of NE thing with um with disco is all of the mappers",
    "start": "2303960",
    "end": "2311440"
  },
  {
    "text": "reducers um readers a lot of these can be kind of chained chained together and and readers are really nice to chain",
    "start": "2311440",
    "end": "2316960"
  },
  {
    "text": "because what you can do is you can take advantage of all of their infrastructure um and um so so so in here the S3 files",
    "start": "2316960",
    "end": "2327760"
  },
  {
    "text": "are just a um just just web address addresses so HTTP strings that um tell me where the file is so the first thing",
    "start": "2327760",
    "end": "2334640"
  },
  {
    "text": "that they'll actually do is they'll they'll use their http reader and then I get a stream from that HTTP reader that",
    "start": "2334640",
    "end": "2341240"
  },
  {
    "text": "I can work with right here um so that's so they're so they're taking care of getting the data from the web or from S3",
    "start": "2341240",
    "end": "2347000"
  },
  {
    "text": "for me already so what I do is I um to make this work work with the binary data I just create a temp file take the data",
    "start": "2347000",
    "end": "2353400"
  },
  {
    "text": "from the stream that they're giving me read it dump it to the disk and then open up using the the Sam the Sam tools",
    "start": "2353400",
    "end": "2360000"
  },
  {
    "text": "Library the pyam library I open it up and this takes care of all the binary for um unpacking the binary data for me",
    "start": "2360000",
    "end": "2366200"
  },
  {
    "text": "and now I just have a nice iter of every single read in the file and I just yield these so now my reader is is",
    "start": "2366200",
    "end": "2371880"
  },
  {
    "text": "almost almost a map step in a way but it's instead of you know doing a key value pair I'm just I'm just returning",
    "start": "2371880",
    "end": "2377280"
  },
  {
    "text": "the single lines that um and they're not even lines in this case it's really just a a tle that contains the reference name",
    "start": "2377280",
    "end": "2384319"
  },
  {
    "text": "and the and the read so you disco makes it kind of easy to blur those lines between you know",
    "start": "2384319",
    "end": "2390720"
  },
  {
    "text": "mapping and reading and reducing but it also gives you a lot of flexibility and if you're kind of used to you know building pipelines like this for data",
    "start": "2390720",
    "end": "2397680"
  },
  {
    "text": "you you can you can kind of see a lot of different ways in Disco where you can you can chain a lot of a lot of different pipeline steps together and",
    "start": "2397680",
    "end": "2403720"
  },
  {
    "text": "you know kind of massage your data kind of independently at each one and and get around some of the you know either",
    "start": "2403720",
    "end": "2409160"
  },
  {
    "text": "different problems that you run into or just work with different data types um and internally it actually does disco",
    "start": "2409160",
    "end": "2415520"
  },
  {
    "text": "does pass everything around as as as these lists that you return from here so they don't those don't get changed too",
    "start": "2415520",
    "end": "2420760"
  },
  {
    "text": "much as they're as they're going through the system so um that was actually one of one of those undocumented features I",
    "start": "2420760",
    "end": "2426839"
  },
  {
    "text": "did find is if you don't return topples in your yield statements it actually crashes and doesn't explain why so they",
    "start": "2426839",
    "end": "2432400"
  },
  {
    "text": "actually do do assume that the readers after the first one aren't working on line data instead they're working on tles which",
    "start": "2432400",
    "end": "2438400"
  },
  {
    "text": "was um an interesting one but anyway so so with this basically that's what I do and then what I'm done I just you know",
    "start": "2438400",
    "end": "2444040"
  },
  {
    "text": "close the close my my my reference two references to the um the file I created",
    "start": "2444040",
    "end": "2449599"
  },
  {
    "text": "and then temp file will go ahead and delete those for me so I never fill up my local space at all with the 7 terabytes of data instead I only have",
    "start": "2449599",
    "end": "2455920"
  },
  {
    "text": "the working data that I've got um and you'll notice too that I I skipped ddfs entirely um for this",
    "start": "2455920",
    "end": "2461560"
  },
  {
    "text": "particular example um and that was in large part because there was seven terabytes of data on the other end that I didn't want to have to push in and and",
    "start": "2461560",
    "end": "2468839"
  },
  {
    "text": "pay and pay the store um so so the final thing for this",
    "start": "2468839",
    "end": "2474319"
  },
  {
    "start": "2474000",
    "end": "2474000"
  },
  {
    "text": "one basically um you saw the map the mapper the reducer and the reader and what we um to put it all",
    "start": "2474319",
    "end": "2481480"
  },
  {
    "text": "together here's the way here's what the job looks like um in this case I give it the list of all the all the all the locations in S3 so the um there were 500",
    "start": "2481480",
    "end": "2488960"
  },
  {
    "text": "bam files in in the final example that I used here and give it a give it the reader tell it the reader tell it how to",
    "start": "2488960",
    "end": "2495079"
  },
  {
    "text": "partition tell it how many partitions um give it the mapper and give it the reducer and then fire it off and runs",
    "start": "2495079",
    "end": "2501400"
  },
  {
    "text": "for long time and then um I get back the results for each chromosome and just write those out to a text file and",
    "start": "2501400",
    "end": "2509160"
  },
  {
    "text": "um I was able so I'll talk a little bit about some of the things that can go wrong in here but",
    "start": "2509160",
    "end": "2514599"
  },
  {
    "text": "um let's see here's here's one one one of them them from chromosome 12 for one of the data sets so you can actually see",
    "start": "2514599",
    "end": "2521200"
  },
  {
    "text": "this is um there's a big spike and coverage in one area but um if you know much about kind of genomics what you can",
    "start": "2521200",
    "end": "2526680"
  },
  {
    "text": "actually see here is that you know this is there's always um a region in the middle um where the what's the name you",
    "start": "2526680",
    "end": "2535800"
  },
  {
    "text": "I'm pointing to one other guy who raised his hand for biotech the cir thank you so the centrair is is at the beginning",
    "start": "2535800",
    "end": "2542599"
  },
  {
    "text": "is in the middle of the chromosome so yeah if you picture those two um carou type things that meet in the middle in",
    "start": "2542599",
    "end": "2548280"
  },
  {
    "text": "the pictures that you see um there's this centromere area in here and it's actually an area of really low complexity um and it's also you know",
    "start": "2548280",
    "end": "2555440"
  },
  {
    "text": "what in in this case it's an area that actually picked up a lot of reads um it's probably just a priming bias but overall you you see you know nice nice",
    "start": "2555440",
    "end": "2563240"
  },
  {
    "text": "coverage across the whole across this whole chromosome right here um a little bit more at the end again the characteristics of the ends are kind of",
    "start": "2563240",
    "end": "2569400"
  },
  {
    "text": "like the characteristics in here but um if you were to see that for all of them you'd see the similar pattern where you have you know the two the two sides the",
    "start": "2569400",
    "end": "2575760"
  },
  {
    "text": "two arms on the chromosome and then the Gap in the middle and and fairly fairly good coverage all the way",
    "start": "2575760",
    "end": "2582040"
  },
  {
    "text": "across oh yeah so so so the x-axis here is um is chromosome position so this one",
    "start": "2583119",
    "end": "2590040"
  },
  {
    "text": "is going up to add 120 million about 130 million so these are actually I bended a",
    "start": "2590040",
    "end": "2596280"
  },
  {
    "text": "little smaller to to actually get it to work on the small nodes um so this is 130 million or so long for the",
    "start": "2596280",
    "end": "2602640"
  },
  {
    "text": "chromosome and then this is the number of counts that were observed at that position so you it goes basically you",
    "start": "2602640",
    "end": "2608040"
  },
  {
    "text": "know from zero counts in some areas all the way up to you know kind of around 200 counts for for this particular data",
    "start": "2608040",
    "end": "2613400"
  },
  {
    "text": "set so that's that's the amount of depth that they were able to get out of that sequencing run on on this",
    "start": "2613400",
    "end": "2620280"
  },
  {
    "text": "chromosome so so a count is basically when you um when you have a so so if you picture with the sequencing when you",
    "start": "2620359",
    "end": "2626359"
  },
  {
    "text": "fragment up the source material those fragments are all fairly short so each time one of those one of",
    "start": "2626359",
    "end": "2631880"
  },
  {
    "text": "those frag fragments Maps back to the chromosome you get a count and it's just I do it based on the length of the frag",
    "start": "2631880",
    "end": "2637079"
  },
  {
    "text": "magment that mapped back so you you get a that's that's how you end up with a with a with the Peaks there and the",
    "start": "2637079",
    "end": "2645440"
  },
  {
    "text": "depth um so so it's a simple example but it's you know that that type of workflow",
    "start": "2645440",
    "end": "2650559"
  },
  {
    "text": "is fairly common in genomics is just going through all the reads Computing something simple and and looking at some",
    "start": "2650559",
    "end": "2655800"
  },
  {
    "text": "results but a few things um that so so I I I use map reduce from time to",
    "start": "2655800",
    "end": "2662160"
  },
  {
    "text": "time and you know I kind of wanted to take advantage of this to talk a little bit about that too um you know kind of what I found it's for and what it may",
    "start": "2662160",
    "end": "2668079"
  },
  {
    "start": "2663000",
    "end": "2663000"
  },
  {
    "text": "not be but disco first of all is a great alternative to Hadoop um I don't know if",
    "start": "2668079",
    "end": "2673160"
  },
  {
    "text": "you've ever tried to install Hadoop you know it actually takes time um and there's a lot of configuration stuff you've got to do to make it work Disco",
    "start": "2673160",
    "end": "2679599"
  },
  {
    "text": "generally installs a lot cleaner assuming you don't run into some of the few bugs that seem to get fixed um every",
    "start": "2679599",
    "end": "2684960"
  },
  {
    "text": "now and then there's a weird bug and I I that that is one thing I know because I've been through the code a lot the first time I did this because it didn't",
    "start": "2684960",
    "end": "2691400"
  },
  {
    "text": "install very well the first time um and there were a bunch of little issues that they had but at the same time I got to see all the code for it in doing that",
    "start": "2691400",
    "end": "2697559"
  },
  {
    "text": "and they actually have done it's it's a fairly clean implementation it makes a lot of sense if you're trying to write um a distributed processing engine what",
    "start": "2697559",
    "end": "2704680"
  },
  {
    "text": "they've done here is actually really clever um it's a really interesting way they did especially the way that they use earling and python together so the",
    "start": "2704680",
    "end": "2711200"
  },
  {
    "text": "the reason they use llang because by default lling is really easy to distribute across a lot of nodes and it",
    "start": "2711200",
    "end": "2716760"
  },
  {
    "text": "has the whole distributed computing infrastructure built in so you kind of have your master node that no as long as it knows all the nodes it can fire off",
    "start": "2716760",
    "end": "2722920"
  },
  {
    "text": "the infrastructure um and they've done a really good job of making sure python plays very well on top of the earling",
    "start": "2722920",
    "end": "2728119"
  },
  {
    "text": "infrastructure so I think the way they've set it up is is actually pretty clever and I'm kind of pointing at you guys out there",
    "start": "2728119",
    "end": "2733839"
  },
  {
    "text": "because it's worth looking at the architecture they did for for for some of it for for for doing that and you",
    "start": "2733839",
    "end": "2740119"
  },
  {
    "text": "know it's not too far removed from how I python does some things either so it's kind of it's kind of a NE a neat little system um I don't know how many people",
    "start": "2740119",
    "end": "2746440"
  },
  {
    "text": "actually work on it but I as far as I can tell it's only a couple guys at Nokia that do this and um so it actually was developed at Nokia and they um use",
    "start": "2746440",
    "end": "2753480"
  },
  {
    "text": "it internally um another interesting thing i' I've learned doing it so we have a number of",
    "start": "2753480",
    "end": "2759079"
  },
  {
    "text": "different clusters um some of them are kind of the traditional commodity clusters where you have you know three terabytes on each node and you have",
    "start": "2759079",
    "end": "2765839"
  },
  {
    "text": "either 10 you know giggy or infin band between the nodes um and that works well um just fine but we also use this on",
    "start": "2765839",
    "end": "2772280"
  },
  {
    "text": "just traditional HPC storage so you know the sand system so we have um some a um",
    "start": "2772280",
    "end": "2778000"
  },
  {
    "text": "HP systems and then some some data direct Network systems that we've we've run this on and it actually as long as",
    "start": "2778000",
    "end": "2783480"
  },
  {
    "text": "as long as the discs can feed all the nodes fast enough it works really well too so you're not really um I actually",
    "start": "2783480",
    "end": "2789160"
  },
  {
    "text": "found um on the real high performance storage systems disco runs a lot faster than it does on commodity clusters when",
    "start": "2789160",
    "end": "2794559"
  },
  {
    "text": "you have the same number of nodes um and a lot of that comes from just HPC storage is really really fast um so our",
    "start": "2794559",
    "end": "2800400"
  },
  {
    "text": "ddn systems give us about three gigabytes per second and that's bytes not bits three gigabytes per second throughput to the nodes versus if you",
    "start": "2800400",
    "end": "2807520"
  },
  {
    "text": "even if you even have a good raid setup on a commodity node you know at most you're going to get kind of 100 to 200",
    "start": "2807520",
    "end": "2812880"
  },
  {
    "text": "megabytes per second so um we're able to get really high throughput on the on the storage systems but then still you know",
    "start": "2812880",
    "end": "2818640"
  },
  {
    "text": "use map reduce as kind of more the processing so then you don't even have to worry about ddfs for the storage um",
    "start": "2818640",
    "end": "2824440"
  },
  {
    "text": "you just use your HPC storage system and then if your if your problem fits well in the map reduce framework you can just",
    "start": "2824440",
    "end": "2830000"
  },
  {
    "text": "use that um on top of it so we I've done it in both settings and um I tend to actually use the HPC storage use case",
    "start": "2830000",
    "end": "2836040"
  },
  {
    "text": "more often now than I use the ddfs um when I'm doing this um but on that you",
    "start": "2836040",
    "end": "2841440"
  },
  {
    "text": "know I I was kind of new to the whole tag based file access and I like the way they do that that that actually has been",
    "start": "2841440",
    "end": "2846599"
  },
  {
    "text": "a great way to organize data and kind of do those ad hoc databases um which if you're playing with any of the other",
    "start": "2846599",
    "end": "2851800"
  },
  {
    "text": "column stores you know you're kind of probably getting used to used to naming all of your data using those conventions but it's really nice that the way that",
    "start": "2851800",
    "end": "2858040"
  },
  {
    "text": "they allow you to have a t tag hierarchies they so you have the tags pointing to diff to each other um that",
    "start": "2858040",
    "end": "2863160"
  },
  {
    "text": "can ultimately resolve down to the file so you can group things nicely and disco also has um this thing called disco DB",
    "start": "2863160",
    "end": "2870000"
  },
  {
    "text": "and I didn't talk about it in here but it basically is a tag based data store so kind of to compete with all the other",
    "start": "2870000",
    "end": "2875480"
  },
  {
    "text": "column Stores um and they claim it's it's really fast but I've never set it up so I I can't say",
    "start": "2875480",
    "end": "2881359"
  },
  {
    "text": "anything other than it does exist and and and it looks looks promising um on",
    "start": "2881359",
    "end": "2886440"
  },
  {
    "text": "the map reduce side um you know I know this is a python and map reduce talk but um it's definitely you know a good and a",
    "start": "2886440",
    "end": "2893000"
  },
  {
    "text": "bad thing I I I found um it definitely certain problems it's kind I always like in map redu to gpus you know if your",
    "start": "2893000",
    "end": "2899319"
  },
  {
    "text": "problem looks like a um image or you know a graphics pipeline a GPU is a great option for you um if it doesn't",
    "start": "2899319",
    "end": "2904359"
  },
  {
    "text": "look like a graphics pipeline maybe it's not the best option for par ISM um and map reduce is very similar to that if",
    "start": "2904359",
    "end": "2909559"
  },
  {
    "text": "you're if your if your problem looks like a streaming data problem where you have a clear map step and a clear reduce",
    "start": "2909559",
    "end": "2915280"
  },
  {
    "text": "step it's a really easy way I mean you saw the mappers and reducers I was able to write they're really simple especially you know using something like",
    "start": "2915280",
    "end": "2921240"
  },
  {
    "text": "disco where you can just keep everything in Python in a single file it's very easy to work with of course python is is",
    "start": "2921240",
    "end": "2926400"
  },
  {
    "text": "pretty straightforward too so oftentimes your pure python code that doesn't use map reduce looks a lot like your just",
    "start": "2926400",
    "end": "2932319"
  },
  {
    "text": "straight map reduce code so you can I I um and that's kind of the next point there is that um I I when I when we",
    "start": "2932319",
    "end": "2938839"
  },
  {
    "text": "first started using this I did a lot of performance tests to kind of figure out you know are we actually gaining anything with map reduce because my goal",
    "start": "2938839",
    "end": "2945000"
  },
  {
    "text": "with map reduce in our environment was to make it easier for biologists to work with the really large data they were um",
    "start": "2945000",
    "end": "2950160"
  },
  {
    "text": "struggling with um you know kind of loading all of these reads into a pearl hash and watching their you know program",
    "start": "2950160",
    "end": "2955640"
  },
  {
    "text": "crash after about three hours of trying to load it happens every time someone starts doing this that that happens so",
    "start": "2955640",
    "end": "2962720"
  },
  {
    "text": "so so my goal with this was to see is can can we give them an abstraction that's a little easier to work with that hides them from a lot of the common",
    "start": "2962720",
    "end": "2968119"
  },
  {
    "text": "problems you run into when you're working with these you know data files that are a couple hundred gigabytes um in size and so it definitely it does",
    "start": "2968119",
    "end": "2975680"
  },
  {
    "text": "make it easier if you can get everyone to Learn Python um and as we learned a lot of analysts actually learn r instead so but um I've been working on trans",
    "start": "2975680",
    "end": "2983000"
  },
  {
    "text": "transitioning them over to play with this um but on the flip side when when we did the performance test we ended up",
    "start": "2983000",
    "end": "2988599"
  },
  {
    "text": "finding that the simple simple batch processing was for for almost every problem was significantly faster um so",
    "start": "2988599",
    "end": "2994920"
  },
  {
    "text": "if you have a job scheduler um if you have python installed all the nodes and and you don't mind doing a little bit of",
    "start": "2994920",
    "end": "3000000"
  },
  {
    "text": "data you know data parsing on on your on your own end um I found that you know it",
    "start": "3000000",
    "end": "3005079"
  },
  {
    "text": "was usually 20x um faster to do it just but with pure python than it was to use the map reduce Frameworks and we tested",
    "start": "3005079",
    "end": "3011000"
  },
  {
    "text": "this back and forth with um Hadoop also and found that that generally to be true um and doing some deeper analysis so I",
    "start": "3011000",
    "end": "3017040"
  },
  {
    "text": "did some profiling on there to find out what was really going on and it's it's just it's really simple it's just the abstraction penalty of those map steps",
    "start": "3017040",
    "end": "3024240"
  },
  {
    "text": "um so if you look at getting a single record in each time um and if you actually go and you know",
    "start": "3024240",
    "end": "3029680"
  },
  {
    "text": "just do a pro do a trace on um on the python stack you've got you know 20 or so function calls leading into that that",
    "start": "3029680",
    "end": "3035079"
  },
  {
    "text": "have to get executed every time for for you to get every single data element and nup has the same type of stack on the",
    "start": "3035079",
    "end": "3040160"
  },
  {
    "text": "Java side um in a lot of cases whereas if you're just working on the raw file streams um off of the operating system",
    "start": "3040160",
    "end": "3046200"
  },
  {
    "text": "you know you basically have the io buffers and that's it and then they're giving into python so you have no abstract your abstraction penalty is",
    "start": "3046200",
    "end": "3052200"
  },
  {
    "text": "significantly less when you're just writing simple batch jobs um so",
    "start": "3052200",
    "end": "3057359"
  },
  {
    "text": "you know while while I think that you know while while these are this is a neat programming Paradigm and it works well on commodity clusters um I think",
    "start": "3057359",
    "end": "3063000"
  },
  {
    "text": "it's it's always worth looking looking at what resources you actually have available and seeing if the overhead of something like this is actually worth it",
    "start": "3063000",
    "end": "3069359"
  },
  {
    "text": "um or if you're getting the benefit some it's kind of a a good and a bad thing because I the one thing I have learned",
    "start": "3069359",
    "end": "3075160"
  },
  {
    "text": "is every time I play with this I you know you get sucked into it it's like when you pull out pull out scheme Or List you know you get kind of sucked",
    "start": "3075160",
    "end": "3080839"
  },
  {
    "text": "back into that world and you have a lot of fun with it for a while and then you come back to reality um and I find that this is just as intoxicating is that",
    "start": "3080839",
    "end": "3087359"
  },
  {
    "text": "it's really fun to start thinking of everything in terms of maap and reduce so it's a neat Paradigm and and I think you know as more people use it we might",
    "start": "3087359",
    "end": "3093160"
  },
  {
    "text": "see more improvements in making those Stacks run a lot faster um but every now and then you know you do have cases",
    "start": "3093160",
    "end": "3099079"
  },
  {
    "text": "where you know if you really have pedis scale data and you want to distribute it across you know a warehouse of computers this is not a bad way to process it um",
    "start": "3099079",
    "end": "3106359"
  },
  {
    "text": "and I think I I don't have any context into how how it's been used at um you know outside of what papers publish at",
    "start": "3106359",
    "end": "3111599"
  },
  {
    "text": "Google or places like that but but that seems to be where the use case where it actually is really useful is when you do",
    "start": "3111599",
    "end": "3117280"
  },
  {
    "text": "have 2,000 computers and you actually have a couple pedabytes of data spread across those and there's no other way to get at it easily um when you have even",
    "start": "3117280",
    "end": "3124559"
  },
  {
    "text": "even with what we're doing when you have you know 10 or 20 terabytes of data um if you have access to it on a decent",
    "start": "3124559",
    "end": "3129680"
  },
  {
    "text": "storage system you know you can you can operate on that just fine in python as long as you're kind of smart about it um",
    "start": "3129680",
    "end": "3134960"
  },
  {
    "text": "and that's where you where we were seeing the big penalties for it but I can see if you've got you know 2,000 nodes and you really want to do a pedis",
    "start": "3134960",
    "end": "3140240"
  },
  {
    "text": "scale question you know this is probably the easiest way to do it um so so so",
    "start": "3140240",
    "end": "3146640"
  },
  {
    "text": "that's kind of what I what I figured on it I got some references in here but disco I I definitely encourage if you want to play with map ruce and like",
    "start": "3146640",
    "end": "3151920"
  },
  {
    "start": "3148000",
    "end": "3148000"
  },
  {
    "text": "python it's a lot like I say it is intoxicating when you get it set up and you fire off 20 nodes and um especially",
    "start": "3151920",
    "end": "3157240"
  },
  {
    "text": "when it's working on Amazon and or places like that it's kind of fun to play with it um the map redu start",
    "start": "3157240",
    "end": "3162559"
  },
  {
    "start": "3162000",
    "end": "3162000"
  },
  {
    "text": "cluster um and I was kicking around ideas too because there were some some things I so I I started the whole Amazon",
    "start": "3162559",
    "end": "3168799"
  },
  {
    "text": "part of this um on Tuesday thinking that I would actually have a lot more running by now and I have some open open things",
    "start": "3168799",
    "end": "3174839"
  },
  {
    "text": "that I think would be a lot of fun because when I started the Amazon one I went to see has someone done this already and of course no one had yet so",
    "start": "3174839",
    "end": "3181400"
  },
  {
    "text": "um it'd be fun to take take what I've got started and and just clean it up a little bit and make a public Ami out of it so anyone else who wants to run disco",
    "start": "3181400",
    "end": "3187520"
  },
  {
    "text": "on Amazon can do it um almost as easily as they can Hadoop so if you've seen",
    "start": "3187520",
    "end": "3192880"
  },
  {
    "text": "Amazon has their elastic elastic map reduce that you know is just a kind of a front end for Hadoop so you you don't",
    "start": "3192880",
    "end": "3198240"
  },
  {
    "text": "even have to do any work to set it up there um we can't quite get that far with disco but we can make it a lot easier for people to try it so I think",
    "start": "3198240",
    "end": "3204880"
  },
  {
    "text": "um you know for hackathon toight if anyone's interested um you know I I think it' be fun to go go and and I",
    "start": "3204880",
    "end": "3210680"
  },
  {
    "text": "might just do this with my time there um and get this thing made public so I we can put it out there put a little documentation as as to how to do it um",
    "start": "3210680",
    "end": "3218119"
  },
  {
    "text": "I've never done star cluster plugins either but that would be an alternative for doing this some is it might actually be a little little easier with star",
    "start": "3218119",
    "end": "3224319"
  },
  {
    "text": "cluster you can actually install a bunch of stuff as the cluster is being set up so that would be another way to do this um and another one too I don't know if",
    "start": "3224319",
    "end": "3231240"
  },
  {
    "text": "anyone's used EBS volumes but um it'd be fun to try that's their kind of their",
    "start": "3231240",
    "end": "3236680"
  },
  {
    "text": "you you can have up to a terabyte of kind of Block Level data storage in Amazon and if and I I'd like to figure",
    "start": "3236680",
    "end": "3242200"
  },
  {
    "text": "out how to set that up on here so you can actually have your ddfs clusters persistent so the way I I did it it used",
    "start": "3242200",
    "end": "3247480"
  },
  {
    "text": "the transient storage so we lost every time I shut down the Clusters I would lose the data um but I'd like to figure out how to do it so you have one EBS",
    "start": "3247480",
    "end": "3253960"
  },
  {
    "text": "volume per node so you can actually leverage the whole ddfs file system um for more long-term data storage so those",
    "start": "3253960",
    "end": "3260839"
  },
  {
    "text": "were some kind of next steps that I think would be fun to go go on this thread but um all right well thank you",
    "start": "3260839",
    "end": "3266119"
  },
  {
    "text": "everyone one [Applause]",
    "start": "3266119",
    "end": "3271920"
  },
  {
    "text": "[Music]",
    "start": "3271920",
    "end": "3280260"
  }
]