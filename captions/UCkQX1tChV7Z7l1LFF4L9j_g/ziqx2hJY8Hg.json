[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "hello my name is Samir Farooqi and I'm a freelance trainer and consultant for",
    "start": "7440",
    "end": "13090"
  },
  {
    "text": "Hadoop I'm also the creator of the 2-day Hadoop fundamentals class for Maracana and in this demo I'm going to walk you",
    "start": "13090",
    "end": "20140"
  },
  {
    "text": "through the HDFS layer of Hadoop which is a subset of the to date class the",
    "start": "20140",
    "end": "28269"
  },
  {
    "start": "27000",
    "end": "110000"
  },
  {
    "text": "first thing you should know about Hadoop is that its origins come from Hadoop from Google white papers there's three",
    "start": "28269",
    "end": "35230"
  },
  {
    "text": "white papers Google file system Google MapReduce and Google BigTable which",
    "start": "35230",
    "end": "40810"
  },
  {
    "text": "become Apache HDFS Apache MapReduce and",
    "start": "40810",
    "end": "45940"
  },
  {
    "text": "Apache HBase about 95 percent of the",
    "start": "45940",
    "end": "51190"
  },
  {
    "text": "architecture described in these white papers is faithfully implemented in the",
    "start": "51190",
    "end": "56470"
  },
  {
    "text": "Apache projects so if best place to",
    "start": "56470",
    "end": "63700"
  },
  {
    "text": "start learning about Google file system and HDFS is really this white paper there's some differences where in GFS a",
    "start": "63700",
    "end": "71140"
  },
  {
    "text": "file gets broken down into chunks which get distributed across the distributed",
    "start": "71140",
    "end": "76810"
  },
  {
    "text": "file system in HDFS a file gets broken up into blocks which get distributed so",
    "start": "76810",
    "end": "83470"
  },
  {
    "text": "there's some terminology difference there but the architecture is very very similar between GFS and HDFS Google",
    "start": "83470",
    "end": "91210"
  },
  {
    "text": "released the white papers with the architecture of how these projects works but no code was released so it's up to",
    "start": "91210",
    "end": "98350"
  },
  {
    "text": "up to a lot of the engineers at Yahoo and now many other companies who created",
    "start": "98350",
    "end": "105400"
  },
  {
    "text": "these Java based Apache projects so the",
    "start": "105400",
    "end": "112240"
  },
  {
    "text": "first thing you should know about Hadoop is understand how a file breaks up into",
    "start": "112240",
    "end": "117610"
  },
  {
    "text": "the blocks and what kind of what that gives you so let's say if you want to",
    "start": "117610",
    "end": "123820"
  },
  {
    "text": "read one terabyte of data on the Left we have the traditional ways way of doing things where we have one machine with",
    "start": "123820",
    "end": "131590"
  },
  {
    "text": "let's say four hard drives go to the machine and each one of these hard drives has a 100 megabyte per second",
    "start": "131590",
    "end": "140290"
  },
  {
    "text": "capability of feeding files off disk so we take the first terabyte of the file",
    "start": "140290",
    "end": "145900"
  },
  {
    "text": "and we scatter it evenly across these four hard drives so we have 250",
    "start": "145900",
    "end": "152590"
  },
  {
    "text": "gigabytes of data on each hard drive all right so if you wanted to read one terabyte from this machine it would take",
    "start": "152590",
    "end": "161430"
  },
  {
    "text": "45 minutes to read that this is just a theoretical calculation of how long it",
    "start": "161430",
    "end": "167260"
  },
  {
    "text": "would take to read from a 250 gigabyte drive at 100 megabytes per second so the",
    "start": "167260",
    "end": "174550"
  },
  {
    "text": "power of Hadoop is you take that same terabyte and you will scatter it for",
    "start": "174550",
    "end": "180400"
  },
  {
    "text": "example across a 10 node cluster with 10 worker machines and you put one tenth of",
    "start": "180400",
    "end": "189220"
  },
  {
    "text": "a terabyte on each one of the machines furthermore let's say that each machine also has four hard drives so you're",
    "start": "189220",
    "end": "195850"
  },
  {
    "text": "putting about 25 gigabytes of data on each one of the hard drives so across the 10 machines there are 40 hard drives",
    "start": "195850",
    "end": "203170"
  },
  {
    "text": "and each hard drive is now holding about 25 gigabytes of data and to read one",
    "start": "203170",
    "end": "209890"
  },
  {
    "text": "terabyte in parallel from the 10 machines it would only take 4 and a half",
    "start": "209890",
    "end": "215019"
  },
  {
    "text": "minutes so by harnessing the power of multiple machines together you can achieve your",
    "start": "215019",
    "end": "224440"
  },
  {
    "text": "reads or writes significantly faster",
    "start": "224440",
    "end": "229080"
  },
  {
    "start": "230000",
    "end": "454000"
  },
  {
    "text": "since Hadoop comes from Google white papers it subscribes to a master/slave",
    "start": "231120",
    "end": "238090"
  },
  {
    "text": "architecture you know the competing art paradigm here in the world of no sequel",
    "start": "238090",
    "end": "244120"
  },
  {
    "text": "is the Amazon Dynamo architecture which is more of a peer-to-peer architecture and that's kind of how Cassandra works",
    "start": "244120",
    "end": "250000"
  },
  {
    "text": "but Hadoop is a master/slave architecture and that's the first thing you should really know about Hadoop that",
    "start": "250000",
    "end": "256000"
  },
  {
    "text": "the different processes or services running in Hadoop are going to be classified as either a master or slave",
    "start": "256000",
    "end": "261970"
  },
  {
    "text": "component so the master components of Hadoop are the name node and the job tracker and the",
    "start": "261970",
    "end": "268360"
  },
  {
    "text": "Laith components of hadoop are the data node and the task tracker we're going to",
    "start": "268360",
    "end": "273909"
  },
  {
    "text": "be focused on the file system in this tutorial so we're going to be more looking at the name node in the data",
    "start": "273909",
    "end": "279879"
  },
  {
    "text": "node the name that is a master of the file system the data node is a slave component of the file system there is",
    "start": "279879",
    "end": "286150"
  },
  {
    "text": "only one name node running in a Hadoop cluster using Gen 1 Hadoop or Hadoop 1.0",
    "start": "286150",
    "end": "293319"
  },
  {
    "text": "and there are going to be multiple data nodes running in the cluster so every slave machine will run a data node",
    "start": "293319",
    "end": "300550"
  },
  {
    "text": "daemon it will also run a task tracker daemon for MapReduce but we're more focused with the data node daemon here",
    "start": "300550",
    "end": "307120"
  },
  {
    "text": "so if you have a 10 node cluster with 10 worker machines you would have 10 data node demons and the 11th machine would",
    "start": "307120",
    "end": "315279"
  },
  {
    "text": "be the name node which is the master for the file system um you should know that",
    "start": "315279",
    "end": "321009"
  },
  {
    "text": "the name node is a single point of availability failure so if the name node",
    "start": "321009",
    "end": "326650"
  },
  {
    "text": "goes down the data nodes won't know how to make sense of the blocks that are on there",
    "start": "326650",
    "end": "332110"
  },
  {
    "text": "so you want to make sure that the name node is running on a dual or triple redundant Hardware a machine you want to",
    "start": "332110",
    "end": "338319"
  },
  {
    "text": "use something like a raid 1+0 there on the data nodes there is redundancy in",
    "start": "338319",
    "end": "346449"
  },
  {
    "text": "the fact that every file that gets stored gets 3 replicas of it and if some",
    "start": "346449",
    "end": "351490"
  },
  {
    "text": "of the nodes crash there should be other notes that have a copy of the file also",
    "start": "351490",
    "end": "357219"
  },
  {
    "text": "in Gen 2 Hadoop there is an active/passive architecture for the name node so you",
    "start": "357219",
    "end": "363520"
  },
  {
    "text": "can run a active name node and if that goes down then within a few seconds",
    "start": "363520",
    "end": "369250"
  },
  {
    "text": "a passive name hood will come up to take or those responsibilities however in Gen",
    "start": "369250",
    "end": "375490"
  },
  {
    "text": "1 Hadoop there is really only one name node if that goes down well your cluster",
    "start": "375490",
    "end": "380889"
  },
  {
    "text": "will not be able to read or write any files to or from the cluster there is a secondary name node daemon as well in",
    "start": "380889",
    "end": "387610"
  },
  {
    "text": "Hadoop which is probably the worst name demon in computer science because the name secondary named it implies that if",
    "start": "387610",
    "end": "394810"
  },
  {
    "text": "the name node goes down the secondary name it will come up but that's not what the secondary name node does it does",
    "start": "394810",
    "end": "401199"
  },
  {
    "text": "more like how keeping for the name node the name node which keeps all the file system metadata",
    "start": "401199",
    "end": "406360"
  },
  {
    "text": "in RAM has no capability to actually persist that metadata onto disk right so",
    "start": "406360",
    "end": "413259"
  },
  {
    "text": "if the name node crashes you lose I lose everything in RAM and therefore you have no file system backup so what the",
    "start": "413259",
    "end": "420729"
  },
  {
    "text": "secondary name node does is it contacts the name node like every hour pulls that metadata out of the name node and kind",
    "start": "420729",
    "end": "428409"
  },
  {
    "text": "of reshuffles and merges it into a clean file called a checkpoint that gets",
    "start": "428409",
    "end": "433779"
  },
  {
    "text": "written on the secondary name node it also gets sent to the name node for persistence so that's how the metadata",
    "start": "433779",
    "end": "440110"
  },
  {
    "text": "in RAM on the name node actually gets persisted onto a file through the help",
    "start": "440110",
    "end": "446289"
  },
  {
    "text": "of the secondary name node but don't think of the second your name node like high availability for the for the name",
    "start": "446289",
    "end": "452020"
  },
  {
    "text": "node so here's the basic architecture of",
    "start": "452020",
    "end": "457839"
  },
  {
    "start": "454000",
    "end": "659000"
  },
  {
    "text": "how HDFS works we have a name node here with the file system metadata that I",
    "start": "457839",
    "end": "462999"
  },
  {
    "text": "mentioned the name is currently keeping track of two files data one dot text and",
    "start": "462999",
    "end": "468430"
  },
  {
    "text": "data to text you can see that data one dot text is actually broken up into",
    "start": "468430",
    "end": "473949"
  },
  {
    "text": "three different blocks blocks one two and three and data two dot text is",
    "start": "473949",
    "end": "479379"
  },
  {
    "text": "broken up into two blocks blocks four and five now the files themselves are",
    "start": "479379",
    "end": "485259"
  },
  {
    "text": "not stored in the name node this is just the file system metadata that kind of",
    "start": "485259",
    "end": "490569"
  },
  {
    "text": "points the files towards the blocks that gets broken up into the other metadata",
    "start": "490569",
    "end": "495849"
  },
  {
    "text": "and the name notice things like the file system permissions liquid what your users have access to a file also last",
    "start": "495849",
    "end": "502629"
  },
  {
    "text": "access time for a file is kept track of in the name node and also disk space",
    "start": "502629",
    "end": "507789"
  },
  {
    "text": "quotas and file system quotas are kept track of in the name node alright so in",
    "start": "507789",
    "end": "514479"
  },
  {
    "text": "this diagram we also have four data nodes which are the slave components to Hadoop and we are setting the",
    "start": "514479",
    "end": "523078"
  },
  {
    "text": "replication factor for this cluster to three that's that in a file called HDFS",
    "start": "523079",
    "end": "529050"
  },
  {
    "text": "I XML and there is a parameter called DFS replication that is set to three in",
    "start": "529050",
    "end": "535089"
  },
  {
    "text": "this cluster so let's take a look at how the files actually gets stored in the name in the",
    "start": "535089",
    "end": "542380"
  },
  {
    "text": "data nodes all right so file one dot txt which is data 1 dot txt is broken up with into",
    "start": "542380",
    "end": "550060"
  },
  {
    "text": "three blocks and here's kind of how those blocks are stored in the data",
    "start": "550060",
    "end": "555490"
  },
  {
    "text": "nodes you can see that for each one of those blocks like block 1 for example there's 3 replicas made of it so the",
    "start": "555490",
    "end": "562870"
  },
  {
    "text": "first replicas on this on the second data node and there's another replicas on the third data node and there's yet",
    "start": "562870",
    "end": "568870"
  },
  {
    "text": "another replicas on the fourth data node same thing with the second block you can see that there's a replica of it here",
    "start": "568870",
    "end": "574779"
  },
  {
    "text": "and a replica of it on another data node and a replica of it on the first data node so this leads to HDFS being a",
    "start": "574779",
    "end": "582570"
  },
  {
    "text": "highly available filesystem and it's also self-healing it's highly available because if the",
    "start": "582570",
    "end": "589180"
  },
  {
    "text": "second data node here crashes with a copy of block 1 and block 3 that's fine",
    "start": "589180",
    "end": "595630"
  },
  {
    "text": "because you have two other copies of those blocks in the cluster and this is also a self-healing filesystem because",
    "start": "595630",
    "end": "602080"
  },
  {
    "text": "if one of these data nodes goes down then the heartbeat from that data node",
    "start": "602080",
    "end": "607210"
  },
  {
    "text": "to the name node will cease to heartbeat and after 10 minutes the name node will consider that data node to be dead and",
    "start": "607210",
    "end": "614740"
  },
  {
    "text": "then what our blocks were on that data node will get respond so that the",
    "start": "614740",
    "end": "620500"
  },
  {
    "text": "replicas count 3 is achieved the other blocks will get respond from the other",
    "start": "620500",
    "end": "625990"
  },
  {
    "text": "two remaining data nodes that should have a replica of that all right so",
    "start": "625990",
    "end": "631000"
  },
  {
    "text": "that's the first file and here's the second file data to text you can see how",
    "start": "631000",
    "end": "638290"
  },
  {
    "text": "once again with the second file you were taking the two blocks block four and five and replicating it three times and",
    "start": "638290",
    "end": "645790"
  },
  {
    "text": "then scattering it across the data nodes now there is some intelligence to this to how the filesystem actually stores",
    "start": "645790",
    "end": "652900"
  },
  {
    "text": "the blocks across the different data nodes so we'll talk about that in in one of the future slides the name node also",
    "start": "652900",
    "end": "661000"
  },
  {
    "start": "659000",
    "end": "709000"
  },
  {
    "text": "has an embedded web server that hosts a kind of basic website showing statistics",
    "start": "661000",
    "end": "666459"
  },
  {
    "text": "about the file system so you can see and the name node was started we can see",
    "start": "666459",
    "end": "673059"
  },
  {
    "text": "the configured capacity of the name node you can see how many data nodes make up",
    "start": "673059",
    "end": "678129"
  },
  {
    "text": "this HDFS cluster and we can see statistics about how what percentage of",
    "start": "678129",
    "end": "683829"
  },
  {
    "text": "the distributed file system we're actually using this is just a one node cluster running in Amazon",
    "start": "683829",
    "end": "689199"
  },
  {
    "text": "so the configured capacity is only in the gigabytes range but the largest",
    "start": "689199",
    "end": "694359"
  },
  {
    "text": "cluster is with HDFS and the world can serve up to 20 30 petabytes of data",
    "start": "694359",
    "end": "700739"
  },
  {
    "text": "across something like 4,500 machines for generation 1 Hadoop the first I guess",
    "start": "700739",
    "end": "712089"
  },
  {
    "text": "confusion that some people have with HDFS is that it's a very atypical file",
    "start": "712089",
    "end": "717220"
  },
  {
    "text": "system so you don't actually format the hard drives into cluster with HDFS right",
    "start": "717220",
    "end": "723639"
  },
  {
    "text": "you actually format the underlying hard drives in the slave machines with EHD 3",
    "start": "723639",
    "end": "729639"
  },
  {
    "text": "or ext4 you can also use XFS so say you have a",
    "start": "729639",
    "end": "736299"
  },
  {
    "text": "10 node cluster 10 worker machines and one master machine running the name node in that case on every one of the slave",
    "start": "736299",
    "end": "743169"
  },
  {
    "text": "machines you would put maybe 5 or 10 hard drives of size 1 terabyte to 2",
    "start": "743169",
    "end": "750100"
  },
  {
    "text": "terabytes each and you'd have to format those slave hard drives with exg 3 or",
    "start": "750100",
    "end": "757959"
  },
  {
    "text": "ext4 because to some extent HDFS is an abstract filesystem so like the blocks",
    "start": "757959",
    "end": "764079"
  },
  {
    "text": "come into HDFS and then they get committed ultimately to ext3 or ext4 or",
    "start": "764079",
    "end": "770739"
  },
  {
    "text": "XFS ext3 and ext4 are the most common underlying file systems I see I would",
    "start": "770739",
    "end": "777189"
  },
  {
    "text": "recommend going with ext4 if you're starting a new cluster and there are some configuration parameters for ext3",
    "start": "777189",
    "end": "784660"
  },
  {
    "text": "and ext4 that I also recommend setting like know a time and turning on the",
    "start": "784660",
    "end": "789699"
  },
  {
    "text": "reserved blocks XFS can also be used for the underlying file system for HDFS but",
    "start": "789699",
    "end": "796359"
  },
  {
    "text": "it's far less commonly seen in my in my opinion",
    "start": "796359",
    "end": "802920"
  },
  {
    "start": "802000",
    "end": "858000"
  },
  {
    "text": "these are the common HDFS shell commands so if you're familiar with Linux then",
    "start": "802920",
    "end": "809819"
  },
  {
    "text": "this should look pretty familiar so you can run the LS command to list the",
    "start": "809819",
    "end": "815160"
  },
  {
    "text": "directories in HDFS you can make a new directory copy from your local file",
    "start": "815160",
    "end": "820199"
  },
  {
    "text": "system into HDFS you can delete a file in HDFS you can tail the end of a file",
    "start": "820199",
    "end": "827189"
  },
  {
    "text": "you can change permissions in a file and this longer command at the end is actually changing the replication factor",
    "start": "827189",
    "end": "833879"
  },
  {
    "text": "to four recursively for everything within this subdirectory so let's say",
    "start": "833879",
    "end": "840299"
  },
  {
    "text": "you previously stored something in that subdirectory with replication factor three and you want to add your more",
    "start": "840299",
    "end": "845669"
  },
  {
    "text": "resiliency to the filesystem and you can you can update replication factor to four or you can bring it down to two or",
    "start": "845669",
    "end": "851850"
  },
  {
    "text": "one after you've actually loaded the files into HDFS this is a diagram that",
    "start": "851850",
    "end": "860759"
  },
  {
    "start": "858000",
    "end": "984000"
  },
  {
    "text": "shows the best practices approach for how to architect HDFS cluster so this is",
    "start": "860759",
    "end": "867660"
  },
  {
    "text": "a how Yahoo sets up their 4500 node cluster so we have three different racks",
    "start": "867660",
    "end": "874769"
  },
  {
    "text": "here each rack has 40 slave machines and",
    "start": "874769",
    "end": "880399"
  },
  {
    "text": "every rack has a top of the racks which in a Yahoo cluster with 4500 machines",
    "start": "880399",
    "end": "887429"
  },
  {
    "text": "you have something like 110 racks and eight core switches I'm showing to coerce which is here and three racks so",
    "start": "887429",
    "end": "894449"
  },
  {
    "text": "every one of the slave machines will have two cat5 cables coming out of it",
    "start": "894449",
    "end": "899939"
  },
  {
    "text": "going into the top of the rack switch so that means the top of the racks which has at least 80 ports on each rack and",
    "start": "899939",
    "end": "909739"
  },
  {
    "text": "that within the rack it's a one gigabit network so the connectivity within the",
    "start": "909739",
    "end": "917699"
  },
  {
    "text": "rack is one gigabit however the course which layer is a 10 gigabit network of",
    "start": "917699",
    "end": "923309"
  },
  {
    "text": "which eight Giga bits are dedicated to HDFS and the other two gigabits are kind",
    "start": "923309",
    "end": "929039"
  },
  {
    "text": "of open for MapReduce administration work and just a user traffic on",
    "start": "929039",
    "end": "934860"
  },
  {
    "text": "the network this should be a dedicated network though at their tour at the top",
    "start": "934860",
    "end": "939930"
  },
  {
    "text": "of the racks which layer and at the course which I'm dedicated for HDFS and MapReduce you'll see that some of these",
    "start": "939930",
    "end": "947520"
  },
  {
    "text": "racks also have you know master nodes like the name node or the job tracker or",
    "start": "947520",
    "end": "952860"
  },
  {
    "text": "the HBase master but you know if you have a hundred racks there was a lot of",
    "start": "952860",
    "end": "957900"
  },
  {
    "text": "the racks will have no master machines on them you'll also notice that the master machines are beefier they're dual",
    "start": "957900",
    "end": "963480"
  },
  {
    "text": "or triple redundant hardware machines as opposed to the slave machines which are",
    "start": "963480",
    "end": "968520"
  },
  {
    "text": "usually just one power supply and there are single points of failure Zhan the",
    "start": "968520",
    "end": "974910"
  },
  {
    "text": "slave machines but that's not as critical because if a slave machine crashes there are other machines to have",
    "start": "974910",
    "end": "980790"
  },
  {
    "text": "a copy of that of that data another",
    "start": "980790",
    "end": "985980"
  },
  {
    "start": "984000",
    "end": "1112000"
  },
  {
    "text": "feature in HDFS is that the name node is rack aware so in this slide we're seeing",
    "start": "985980",
    "end": "991560"
  },
  {
    "text": "the same three racks as before each rack has you know five data nodes the second",
    "start": "991560",
    "end": "997800"
  },
  {
    "text": "rack has data node six seven eight nine and ten and you can see that there are three replicas of block a they're on",
    "start": "997800",
    "end": "1004490"
  },
  {
    "text": "data nodes one seven and eight and that information is as you know kept in the name nodes metadata so the name node",
    "start": "1004490",
    "end": "1011780"
  },
  {
    "text": "knows that this file dot text breaks up into two blocks block a and B and block",
    "start": "1011780",
    "end": "1017240"
  },
  {
    "text": "a is made ax is on three different data nodes and Block B is on these three different data nodes another thing is",
    "start": "1017240",
    "end": "1023930"
  },
  {
    "text": "that the name node is rack aware that means that the name node knows what rack",
    "start": "1023930",
    "end": "1029240"
  },
  {
    "text": "each data node is on you have to actually manually configure that on the name node viola a bash or Python script",
    "start": "1029240",
    "end": "1036230"
  },
  {
    "text": "it's pretty simple to do and now this name node knows that rack one has data",
    "start": "1036230",
    "end": "1041870"
  },
  {
    "text": "nodes 1 2 3 4 and 5 and rack 2 has an X 5 data nodes that rack awareness is",
    "start": "1041870",
    "end": "1049250"
  },
  {
    "text": "going to come into play when you store a file in the cluster because the name",
    "start": "1049250",
    "end": "1055040"
  },
  {
    "text": "node won't Minnis stores its 3 replicas for each block in the file the name node wants to scatter those replicas across",
    "start": "1055040",
    "end": "1062150"
  },
  {
    "text": "at least two different racks so if an entire top of the rack switch failure happens and",
    "start": "1062150",
    "end": "1068160"
  },
  {
    "text": "Tyrod goes out of commission there should still be at least one other rack that has a copy of your data if there",
    "start": "1068160",
    "end": "1075000"
  },
  {
    "text": "was no rack awareness in Hadoop then there is a chance that one rack would",
    "start": "1075000",
    "end": "1080100"
  },
  {
    "text": "have gone all three replicas and if that rack goes down because of a failure then you will lose those replicas for that",
    "start": "1080100",
    "end": "1087600"
  },
  {
    "text": "point in time that the failure exists in a smaller Hadoop node though with under",
    "start": "1087600",
    "end": "1093120"
  },
  {
    "text": "20 nodes you don't need to configure a rack awareness you can you can configure all 20 nodes to the same switch and just",
    "start": "1093120",
    "end": "1100500"
  },
  {
    "text": "you act like they're all in the same rack which is called the default rack however as you start to scale out your",
    "start": "1100500",
    "end": "1107730"
  },
  {
    "text": "cluster you'll want to use a rack awareness topology script so with that",
    "start": "1107730",
    "end": "1113280"
  },
  {
    "start": "1112000",
    "end": "1235000"
  },
  {
    "text": "introduction to HDFS let's take a little bit deeper dive on how HDFS actually",
    "start": "1113280",
    "end": "1119220"
  },
  {
    "text": "does the writes on the left here if you have a client so that's like you know your laptop that is outside of the",
    "start": "1119220",
    "end": "1125610"
  },
  {
    "text": "cluster that has a file that you want to load into your Hadoop cluster so the",
    "start": "1125610",
    "end": "1130680"
  },
  {
    "text": "file you you'll want to load here actually breaks up into three different blocks block a B and C and in the",
    "start": "1130680",
    "end": "1136800"
  },
  {
    "text": "cluster I've kind of removed some of the complexity from the previous slides so we're only seeing three data nodes and",
    "start": "1136800",
    "end": "1142230"
  },
  {
    "text": "two racks however keep in mind that each rack does have more data nodes and this",
    "start": "1142230",
    "end": "1147900"
  },
  {
    "text": "is a bigger cluster but I've removed all the actors that will not play a role in the right we're going to do for this",
    "start": "1147900",
    "end": "1153480"
  },
  {
    "text": "file and we also I've also moved the name node out of the cluster keep in mind the name node is RAC aware it knows",
    "start": "1153480",
    "end": "1160050"
  },
  {
    "text": "that data node 1 is on rack 1 and data node 7 and 9 are on rack 5 all right so",
    "start": "1160050",
    "end": "1167190"
  },
  {
    "text": "when you actually want to store this file with 3 blocks into the cluster you",
    "start": "1167190",
    "end": "1173010"
  },
  {
    "text": "will basically just run one command called a copy from local command or you'll write like five or six lines of",
    "start": "1173010",
    "end": "1180090"
  },
  {
    "text": "Java MapReduce which will then instruct the client API for Hadoop on your laptop",
    "start": "1180090",
    "end": "1187980"
  },
  {
    "text": "to go through the following steps to actually write the file into the cluster so you don't have to actually do this",
    "start": "1187980",
    "end": "1193830"
  },
  {
    "text": "yourself or you don't have to actually break the file up into three blocks and",
    "start": "1193830",
    "end": "1198900"
  },
  {
    "text": "then contact the name node to ask you know where am I going to store the this block this is done for you by the",
    "start": "1198900",
    "end": "1205630"
  },
  {
    "text": "Hadoop client API like I said all you have to do is write some simple Java code or run a command line copy from",
    "start": "1205630",
    "end": "1212799"
  },
  {
    "text": "local command but what happens is the first step is when you submit a file to",
    "start": "1212799",
    "end": "1218260"
  },
  {
    "text": "the client API on your laptop the client will contact the name going to say I want to write file dot text block a into",
    "start": "1218260",
    "end": "1225880"
  },
  {
    "text": "the Hadoop cluster the name node will respond saying okay write that block a",
    "start": "1225880",
    "end": "1231760"
  },
  {
    "text": "two data nodes one seven and nine and then the client will directly contact",
    "start": "1231760",
    "end": "1239289"
  },
  {
    "start": "1235000",
    "end": "1287000"
  },
  {
    "text": "data node one and over TCP 510 send a",
    "start": "1239289",
    "end": "1245200"
  },
  {
    "text": "ready command to data node one set basically saying get yourself ready I'm about to send a block down to you but",
    "start": "1245200",
    "end": "1251830"
  },
  {
    "text": "also get get data node seven and nine ready so then data node one will forward on",
    "start": "1251830",
    "end": "1258220"
  },
  {
    "text": "that ready command by hopping through the top of the rack switch the core switch and the next top of the rack",
    "start": "1258220",
    "end": "1263590"
  },
  {
    "text": "switch or two data node seven and send the ready command and ask seven to get",
    "start": "1263590",
    "end": "1269350"
  },
  {
    "text": "nine ready as well and then finally data node 9 will let seven know that it's",
    "start": "1269350",
    "end": "1275919"
  },
  {
    "text": "ready and sound will let one know that it's ready and one will let the client know that they are all ready and the TCP",
    "start": "1275919",
    "end": "1282429"
  },
  {
    "text": "pipeline is ready to take down that first block so this is where the",
    "start": "1282429",
    "end": "1288820"
  },
  {
    "start": "1287000",
    "end": "1466000"
  },
  {
    "text": "pipelined right actually begins you'll notice that when the client is going to start streaming the block down to the",
    "start": "1288820",
    "end": "1294820"
  },
  {
    "text": "clients it is not having to go through the name node anymore the name node is really only used to figure out where in",
    "start": "1294820",
    "end": "1303159"
  },
  {
    "text": "the cluster you will write that block but then after that the client contacts one of the data nodes directly to start",
    "start": "1303159",
    "end": "1309820"
  },
  {
    "text": "streaming the block in so the client will send block a down to data node one",
    "start": "1309820",
    "end": "1316000"
  },
  {
    "text": "which will then forward it to data node",
    "start": "1316000",
    "end": "1321279"
  },
  {
    "text": "seven which will then forward it down to data node nine and then each one of the",
    "start": "1321279",
    "end": "1327909"
  },
  {
    "text": "data nodes will let the name node know that they receive the block successfully and the first data node will let the",
    "start": "1327909",
    "end": "1335110"
  },
  {
    "text": "claw I know that they all received that block successfully and then the Klein will move on we did in with the Block B and",
    "start": "1335110",
    "end": "1342790"
  },
  {
    "text": "Block B will be stored in perhaps three entirely different data nodes right it's",
    "start": "1342790",
    "end": "1348580"
  },
  {
    "text": "not like the file will be stored on only only these three data nodes for the next block the client will basically",
    "start": "1348580",
    "end": "1355770"
  },
  {
    "text": "renegotiate with the name node the location for Block B which might be entirely different set of racks and data",
    "start": "1355770",
    "end": "1362080"
  },
  {
    "text": "nodes I want to point out just a few other things here you'll notice that",
    "start": "1362080",
    "end": "1367900"
  },
  {
    "text": "we're spanning two racks here this is part of the algorithm for HDFS basically",
    "start": "1367900",
    "end": "1374770"
  },
  {
    "text": "when you write a block into HDFS it will put the first replicas on a rack by",
    "start": "1374770",
    "end": "1380440"
  },
  {
    "text": "itself in a data node and that first data notice kind of randomly chosen and",
    "start": "1380440",
    "end": "1385480"
  },
  {
    "text": "that rack is basically kind of randomly chosen and then the HDFS name node will",
    "start": "1385480",
    "end": "1391570"
  },
  {
    "text": "choose a different rack in this case it shows rack v and on that rack it will",
    "start": "1391570",
    "end": "1397930"
  },
  {
    "text": "kind of randomly choose two different data nodes to put the second and third replicas together on a rack",
    "start": "1397930",
    "end": "1404560"
  },
  {
    "text": "so the first replica goes by itself on Iraq and replicas two and three will be",
    "start": "1404560",
    "end": "1409990"
  },
  {
    "text": "more tightly coupled on Iraq the reason for that is because when you write from",
    "start": "1409990",
    "end": "1416020"
  },
  {
    "text": "data node one to seven you have to go through three different switches but to",
    "start": "1416020",
    "end": "1421480"
  },
  {
    "text": "go from seven to nine you just have to go through one top of the rock switch back down to nine so that keeps the",
    "start": "1421480",
    "end": "1428370"
  },
  {
    "text": "latency for the write a little bit lower however this is asynchronous right so",
    "start": "1428370",
    "end": "1434500"
  },
  {
    "text": "the client does have to write to one seven and nine before the write is considered successful if you reduce your",
    "start": "1434500",
    "end": "1441370"
  },
  {
    "text": "replication factor in HDFS to just one then the client only has to write to data node one and then a write",
    "start": "1441370",
    "end": "1448120"
  },
  {
    "text": "successful ascend back to the client and the name node that kind of write is going to be many times faster five ten",
    "start": "1448120",
    "end": "1455380"
  },
  {
    "text": "twenty times faster than doing a replica three right which is a synchronous write across multiple switches and multiple",
    "start": "1455380",
    "end": "1462280"
  },
  {
    "text": "data nodes when a client is reading",
    "start": "1462280",
    "end": "1469270"
  },
  {
    "start": "1466000",
    "end": "1630000"
  },
  {
    "text": "a file the same type of architecture comes into play the client has to contact the name note first to figure",
    "start": "1469270",
    "end": "1475960"
  },
  {
    "text": "out where the actual blocks are stored for a file so in this case the client is",
    "start": "1475960",
    "end": "1481330"
  },
  {
    "text": "going to ask the name node what are the block locations for a file called results dot txt and as you can see the",
    "start": "1481330",
    "end": "1488230"
  },
  {
    "text": "name node has in its metadata the block locations for block a and block B for",
    "start": "1488230",
    "end": "1493720"
  },
  {
    "text": "that file so the name hood will respond back saying there's two blocks in this file block a is on data nodes one four",
    "start": "1493720",
    "end": "1501760"
  },
  {
    "text": "and five and block B is on data nodes nine one and two and then once again",
    "start": "1501760",
    "end": "1508150"
  },
  {
    "text": "like before the client will directly connect to the data nodes to read the",
    "start": "1508150",
    "end": "1513670"
  },
  {
    "text": "blocks now this list that the name node sends with block a being on one four and",
    "start": "1513670",
    "end": "1519220"
  },
  {
    "text": "five is a ordered list and this is um kind of recommending the data the client",
    "start": "1519220",
    "end": "1526060"
  },
  {
    "text": "to contact data node one first to read the first block if the client cannot",
    "start": "1526060",
    "end": "1531730"
  },
  {
    "text": "contact data node one for whatever reason then it will move on to data node four to read the other replicas of the",
    "start": "1531730",
    "end": "1539440"
  },
  {
    "text": "block the name node is um intelligently ordering the list of data nodes based on",
    "start": "1539440",
    "end": "1545740"
  },
  {
    "text": "how much network traffic each data node is under but the other important thing",
    "start": "1545740",
    "end": "1552340"
  },
  {
    "text": "to once again reiterate here is that the client has to contact the name no just to figure out where to read the files",
    "start": "1552340",
    "end": "1559720"
  },
  {
    "text": "from or where to write the files to however the relationship is direct",
    "start": "1559720",
    "end": "1564820"
  },
  {
    "text": "between the client and the actual data nodes for where the write takes place it's not like the read or the write is",
    "start": "1564820",
    "end": "1571120"
  },
  {
    "text": "having to flow through the name node to the data nodes that would make the name node a single point of failure in a",
    "start": "1571120",
    "end": "1576610"
  },
  {
    "text": "large cluster you know you can think of the name node kind of like a bookkeeper in a library so you know in a library if",
    "start": "1576610",
    "end": "1585310"
  },
  {
    "text": "you want to find a book you're not going to go randomly from shelf to shelf looking for the book that would take too",
    "start": "1585310",
    "end": "1591670"
  },
  {
    "text": "long instead you you'd prefer to go you know to a cupboard and you open up a",
    "start": "1591670",
    "end": "1598150"
  },
  {
    "text": "drawer in which there are a bunch of index cards which have you know all the books in the library organized by",
    "start": "1598150",
    "end": "1605650"
  },
  {
    "text": "like author's last name or the title in descending order for example right so",
    "start": "1605650",
    "end": "1611350"
  },
  {
    "text": "you can go open up those index cards and very quickly figure out where your book actually is and then that so the index",
    "start": "1611350",
    "end": "1617860"
  },
  {
    "text": "card kind of points you towards that shelf right that's kind of what the name note is doing here the name of it is",
    "start": "1617860",
    "end": "1623890"
  },
  {
    "text": "kind of like that cupboard with all the index cards which make the file system metadata on the name node so now I'm",
    "start": "1623890",
    "end": "1631720"
  },
  {
    "start": "1630000",
    "end": "2015000"
  },
  {
    "text": "going to show you a live demo of how HDFS works we're running a one node virtual machine",
    "start": "1631720",
    "end": "1637870"
  },
  {
    "text": "here I'm currently in Linux in this directory home Cloudera Maracana demo",
    "start": "1637870",
    "end": "1644290"
  },
  {
    "text": "and i'm logged in to this linux machine as a user named cloud era and in this",
    "start": "1644290",
    "end": "1653920"
  },
  {
    "text": "directory marcano demo there are two files Shakespeare txt and Tom Sawyer txt",
    "start": "1653920",
    "end": "1661650"
  },
  {
    "text": "these are just a files from Project Gutenberg so let's take a look at one of",
    "start": "1661650",
    "end": "1667090"
  },
  {
    "text": "these let's take a look at Shakespeare text it's just a you know multi megabyte",
    "start": "1667090",
    "end": "1675190"
  },
  {
    "text": "file that has the entire works of Shakespeare it's just a text file and",
    "start": "1675190",
    "end": "1681600"
  },
  {
    "text": "similarly does it there's a Tom Sawyer file that has the Adventures of Tom",
    "start": "1681600",
    "end": "1687670"
  },
  {
    "text": "Sawyer by Mark Twain so our goal here is to write these two files into HDFS now",
    "start": "1687670",
    "end": "1694570"
  },
  {
    "text": "since this is a pseudo distributed OneNote cluster all of the demons for HDFS and MapReduce are running on this",
    "start": "1694570",
    "end": "1701800"
  },
  {
    "text": "one machine so the first thing we'll do is just kind of run this Hadoop command",
    "start": "1701800",
    "end": "1707460"
  },
  {
    "text": "which shows all of the options for Hadoop we are going to be more focused",
    "start": "1707460",
    "end": "1714550"
  },
  {
    "text": "on this file system option so let's run that and we'll see all the options for",
    "start": "1714550",
    "end": "1723520"
  },
  {
    "text": "the file system so we can do LS we can",
    "start": "1723520",
    "end": "1729940"
  },
  {
    "text": "run addy you command a move command a copy command so let's let's actually run",
    "start": "1729940",
    "end": "1735130"
  },
  {
    "text": "the command too to list what is in the file system for",
    "start": "1735130",
    "end": "1742330"
  },
  {
    "text": "so it's Hadoop FS - LS and will list what's at root of HDFS so we'll we can",
    "start": "1742330",
    "end": "1749080"
  },
  {
    "text": "see that there's three folders at root and these are all three direct one of",
    "start": "1749080",
    "end": "1755080"
  },
  {
    "text": "them is a directory don't be confused between that and the Linux LS command which is showing me what I'm what is in",
    "start": "1755080",
    "end": "1762490"
  },
  {
    "text": "my mark on a demo folder so let's drill a little bit deeper into the filesystem",
    "start": "1762490",
    "end": "1769270"
  },
  {
    "text": "let's do Hadoop FS LS and let's take a look at what's in the user directory so",
    "start": "1769270",
    "end": "1775870"
  },
  {
    "text": "there's two subdirectories here one of them is named Cloud area and one of them is named hive remember I'm logged in as",
    "start": "1775870",
    "end": "1781899"
  },
  {
    "text": "the user cloud era so my home folder is user cloud era and if I do LS and there",
    "start": "1781899",
    "end": "1788620"
  },
  {
    "text": "I don't see any files so there's nothing in there right now let's make a directory first so we'll do M Kader and",
    "start": "1788620",
    "end": "1796140"
  },
  {
    "text": "we'll make a new directory in there called new der and in here we'll store",
    "start": "1796140",
    "end": "1802149"
  },
  {
    "text": "the two ebook files so if I run LS now I should see a directory here called new",
    "start": "1802149",
    "end": "1810130"
  },
  {
    "text": "der and if I go into the new directory I'll see I'll see no files before I",
    "start": "1810130",
    "end": "1818590"
  },
  {
    "text": "actually load the file into HDFS let me show you the name node GUI this GUI is",
    "start": "1818590",
    "end": "1825789"
  },
  {
    "text": "not just showing me when the name mode was started it shows me that there's about 137 files and directories in HDFS",
    "start": "1825789",
    "end": "1833980"
  },
  {
    "text": "and 59 blocks total there's just one data node right",
    "start": "1833980",
    "end": "1841539"
  },
  {
    "text": "now and there's no dead nodes in the cluster this cluster is configured",
    "start": "1841539",
    "end": "1848140"
  },
  {
    "text": "capacity is about four and a half gigs so when I load the files into the",
    "start": "1848140",
    "end": "1853720"
  },
  {
    "text": "cluster this number should increment from 137 to two more it should go up to",
    "start": "1853720",
    "end": "1859480"
  },
  {
    "text": "139 once you load the files in so let's do that we do that using the Hadoop copy",
    "start": "1859480",
    "end": "1866649"
  },
  {
    "text": "from local command so this is a case sensitive and we'll do copy from local and the",
    "start": "1866649",
    "end": "1873850"
  },
  {
    "text": "file was called Shakespeare text and I'm going to give it the path in hdfs where",
    "start": "1873850",
    "end": "1879130"
  },
  {
    "text": "I want it to be stored so user cloudera new door and hit enter and let me do the",
    "start": "1879130",
    "end": "1888160"
  },
  {
    "text": "same thing with the Tom Sawyer file and",
    "start": "1888160",
    "end": "1895680"
  },
  {
    "text": "hit enter and then while I'm here let me just do an LS command and check what's",
    "start": "1895680",
    "end": "1902470"
  },
  {
    "text": "in user cloudera new door and there you can see the two",
    "start": "1902470",
    "end": "1911380"
  },
  {
    "text": "new files I just added in there is Shakespeare text and there's Tom Surrey text you can see the user group and",
    "start": "1911380",
    "end": "1919450"
  },
  {
    "text": "other permissions for that file this number one means that this file is currently replicated once in a one-note",
    "start": "1919450",
    "end": "1926140"
  },
  {
    "text": "cluster this is fine by in a production cluster you should usually use replication factor 3 and this is the",
    "start": "1926140",
    "end": "1933550"
  },
  {
    "text": "size of the file in HDFS now if you go back to the name node GUI and if i refresh this page if i refresh this page",
    "start": "1933550",
    "end": "1944500"
  },
  {
    "text": "we should see the file counting go up from 137 to may be 139 there you go and",
    "start": "1944500",
    "end": "1952690"
  },
  {
    "text": "it's gone up just a bit one more thing in the name no that's kind of cool is if",
    "start": "1952690",
    "end": "1958450"
  },
  {
    "text": "you go into the Browse the file system link you can also look at what's in the file system through here so I'm going",
    "start": "1958450",
    "end": "1964150"
  },
  {
    "text": "under user cloud era new der and there we go I should see my two files there and if I click on one of these files I",
    "start": "1964150",
    "end": "1971160"
  },
  {
    "text": "can see the raw contents of the file as well I hope you enjoyed this short",
    "start": "1971160",
    "end": "1978580"
  },
  {
    "text": "introduction to HDFS if you'd like to learn more about HDFS please check out Maracana comm for upcoming dates for a",
    "start": "1978580",
    "end": "1988600"
  },
  {
    "text": "public class for Hadoop we also do private classes for companies so if you'd like to contract us out for a",
    "start": "1988600",
    "end": "1995530"
  },
  {
    "text": "2-day private class we can do that as well and this is my email address if you want to send me any questions I hope you",
    "start": "1995530",
    "end": "2002400"
  },
  {
    "text": "enjoyed the course and I look for to seeing you in one of my classes in the future",
    "start": "2002400",
    "end": "2008179"
  }
]