[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "I am aware that I'm standing between you and the coffee and 15 minutes time so I shall try and keep you awake in the",
    "start": "3920",
    "end": "9650"
  },
  {
    "text": "meantime and with any luck keep myself awake as well so I will take questions",
    "start": "9650",
    "end": "15050"
  },
  {
    "text": "if we run out of time towards questions at the end but quite happy to sit down and have questions down in this",
    "start": "15050",
    "end": "20750"
  },
  {
    "text": "particular area so if you want to come up and ask me questions then feel free don't don't don't worry about if we run",
    "start": "20750",
    "end": "26630"
  },
  {
    "text": "out of time and if you're watching on the video and you want to ask me a question you can send me a tweet down at the bottom and I'll follow up at some",
    "start": "26630",
    "end": "32750"
  },
  {
    "text": "point I'm not gonna promise instantaneous responses but I will try and follow up with those things so",
    "start": "32750",
    "end": "38960"
  },
  {
    "text": "welcome to the talk this is the talk on understanding CPU microarchitecture for maximum performance and what we're going",
    "start": "38960",
    "end": "45710"
  },
  {
    "text": "to be talking about is really what happens inside a CPU what goes on in the bits and bobs in there how does that",
    "start": "45710",
    "end": "50809"
  },
  {
    "start": "46000",
    "end": "46000"
  },
  {
    "text": "hook in with the rest of the system how does the memory subsystem work how does caching work and so on and we're going",
    "start": "50809",
    "end": "57559"
  },
  {
    "text": "to look at the kind of tools that we have available for being able to do analysis of how can we make our CPU run",
    "start": "57559",
    "end": "63830"
  },
  {
    "text": "faster so we're going to be focusing on this performance pyramid we're going to be talking about the instructions we use",
    "start": "63830",
    "end": "69350"
  },
  {
    "start": "66000",
    "end": "66000"
  },
  {
    "text": "the way the memory works the way the CPU works really at the top part of it here so this is what this talk covers the",
    "start": "69350",
    "end": "75680"
  },
  {
    "text": "presentations by the way will be available afterwards or send out a tweet they'll also be on the coupon website and there are other things that you can",
    "start": "75680",
    "end": "82429"
  },
  {
    "text": "do for performance specifically if you're looking at the performance of a distributed system fix your distributed system first fix",
    "start": "82429",
    "end": "89539"
  },
  {
    "text": "your system architecture first fix the algorithms first really the top level is with the last couple of percent of that",
    "start": "89539",
    "end": "95509"
  },
  {
    "text": "particular process so other coupon talks are available computers have been",
    "start": "95509",
    "end": "101810"
  },
  {
    "start": "101000",
    "end": "101000"
  },
  {
    "text": "getting really quite complicated back when we were just talking about 6502 in the BBC are Apple and other such systems",
    "start": "101810",
    "end": "109579"
  },
  {
    "text": "Commodore 64 was mine you have a single processor it just did one thing and it did it very well these days server",
    "start": "109579",
    "end": "115549"
  },
  {
    "text": "processors come in multi socket configurations and these multiple sockets are connected to multiple memory",
    "start": "115549",
    "end": "120889"
  },
  {
    "text": "chips and there's a communication bandwidth path between them this talk I'm going to be focusing really on Intel",
    "start": "120889",
    "end": "127310"
  },
  {
    "text": "specifics and on Linux as far as operating systems concerned some of them",
    "start": "127310",
    "end": "132530"
  },
  {
    "text": "will be Linux and Intel specific but the ideas will apply to other operating systems and other platforms as well I'm",
    "start": "132530",
    "end": "139460"
  },
  {
    "text": "staking out dual socket communications in this case you've got two sockets talking to each other you can get four socket configurations",
    "start": "139460",
    "end": "145250"
  },
  {
    "text": "and you can get a socket configurations each one of these sockets is connected to a bunch of RAM chips that then is",
    "start": "145250",
    "end": "150950"
  },
  {
    "text": "local to that socket and the other Ram chips whilst accessible are further away and therefore slightly slower and this",
    "start": "150950",
    "end": "156950"
  },
  {
    "text": "is known as non-uniform memory architecture pretty much any serious server-side system is a non-uniform",
    "start": "156950",
    "end": "162860"
  },
  {
    "text": "memory architecture these days well what happens inside the chip well it turns out you go down another level and you",
    "start": "162860",
    "end": "168620"
  },
  {
    "text": "see the same sort of pattern this is what the board well chips look like with a ring bus around for communicating",
    "start": "168620",
    "end": "174800"
  },
  {
    "text": "backwards and forwards across the cause and each one of these little squares for those of you standing at the back are the individual cause themselves with",
    "start": "174800",
    "end": "181850"
  },
  {
    "text": "cache and processing associated with it the 18 cords I looks a little bit weird",
    "start": "181850",
    "end": "187490"
  },
  {
    "text": "because it's got a bi-directional pump between the left-hand side and the right-hand side so if you're going from a core down at the bottom to one over",
    "start": "187490",
    "end": "193640"
  },
  {
    "text": "the far side you then have an increased traffic to be able to get over there and therefore slow a delay as well and the",
    "start": "193640",
    "end": "198830"
  },
  {
    "text": "same thing is true of the 24 core so these sockets in these cores are really getting quite complicated and",
    "start": "198830",
    "end": "204880"
  },
  {
    "text": "importantly although we think of machines as being a von Neumann architecture that you can just reference any memory and get the result back the",
    "start": "204880",
    "end": "211790"
  },
  {
    "text": "time it takes to get it out back can vary dramatically depending on where that data is being loaded for the",
    "start": "211790",
    "end": "217450"
  },
  {
    "text": "current generation of Intel's have moved over to a mesh-like architecture so for",
    "start": "217450",
    "end": "222650"
  },
  {
    "text": "things like the Cascade and skylake systems they have a mesh for being able",
    "start": "222650",
    "end": "227930"
  },
  {
    "text": "to move sideways and this gives more paths to be able to get from one place to another and you can theoretically do",
    "start": "227930",
    "end": "233900"
  },
  {
    "text": "it in less time and they come in a 10 an 18 and a 28 core variety for being able",
    "start": "233900",
    "end": "239630"
  },
  {
    "text": "to move forwards each one of these chips has got bi-directional memory ports going out either side and so you can",
    "start": "239630",
    "end": "246260"
  },
  {
    "text": "actually partition these things into a kind of left and right sub Numa clusters for being able to say I want things to",
    "start": "246260",
    "end": "251570"
  },
  {
    "text": "run on this half and talk to this Bank of memory whilst you have another set of processes the one on this side and talk",
    "start": "251570",
    "end": "256700"
  },
  {
    "text": "to that Bank of memory and in fact there's been a recent release of the cascade 56 core well actually it's a",
    "start": "256700",
    "end": "263180"
  },
  {
    "text": "package rather than a die because all that done is they've taken two of the existing dies slap them next to each",
    "start": "263180",
    "end": "268220"
  },
  {
    "text": "other and actually put in the pipeline on the same piece of socket that goes in",
    "start": "268220",
    "end": "273940"
  },
  {
    "text": "so things are getting complicated and if we drill down further into the cause then we see why because we've got",
    "start": "273940",
    "end": "280550"
  },
  {
    "text": "different levels of cache inside each one of these processes well they use a dollar sign because it's a play on cache",
    "start": "280550",
    "end": "285790"
  },
  {
    "text": "for styling systems you've got a register file which is the kind of number of in-flight variables if you",
    "start": "285790",
    "end": "292310"
  },
  {
    "text": "like registers that are used to hold data as it flies and for cascade and skylake systems at 180 integers in 168",
    "start": "292310",
    "end": "300650"
  },
  {
    "text": "floating points these things can usually be accessed in one clock cycle so half a nanosecond if you're running at two",
    "start": "300650",
    "end": "306500"
  },
  {
    "text": "gigahertz or slightly less if you're running a little bit faster those in turn delegate to a level one cache which",
    "start": "306500",
    "end": "312590"
  },
  {
    "text": "is split into an instruction and a day to half and the idea for splitting it is so that when you're processing large amounts of data your data isn't pushing",
    "start": "312590",
    "end": "319220"
  },
  {
    "text": "your program out of the space and in particular most of the time you're just reading from the instruction cache whereas the data is a two-way street",
    "start": "319220",
    "end": "326110"
  },
  {
    "text": "that access time is about four cycles but that then delegates back to a level",
    "start": "326110",
    "end": "331640"
  },
  {
    "text": "two cache which is shared between them which is typically 12 15 cycles something along those lines depending on",
    "start": "331640",
    "end": "337430"
  },
  {
    "text": "the architecture and of course these have different sizes in the case of sky Lakes and cascade systems it's about a",
    "start": "337430",
    "end": "344120"
  },
  {
    "text": "megabyte at the level 2 cache that's specific to that particular core but if you want to talk to external memory",
    "start": "344120",
    "end": "350210"
  },
  {
    "text": "there's a level 3 cache as well and that's shared across all cores on the same die that you're implemented that",
    "start": "350210",
    "end": "355670"
  },
  {
    "text": "you're loading so there's usually a 16 megabytes or something of that size that is stored on the core itself and each",
    "start": "355670",
    "end": "363770"
  },
  {
    "text": "core can access memory from there but of course the time it takes practice is really a function of how local it is to",
    "start": "363770",
    "end": "369230"
  },
  {
    "text": "that data now one thing I'll to point out the level 3 cache on the Intel chips is non-inclusive at the l3 layer with",
    "start": "369230",
    "end": "376070"
  },
  {
    "text": "inclusive of the second level 2 layer what that means is if you've got some data in the l1 it will also be in the l2",
    "start": "376070",
    "end": "381680"
  },
  {
    "text": "but it doesn't actually have to be in the l3 AMD have just launched a new chip there's got an absolutely massive l3",
    "start": "381680",
    "end": "388220"
  },
  {
    "text": "cache inside there that has a number of performance advantages and we'll probably see Intel coming with bigger l3",
    "start": "388220",
    "end": "395150"
  },
  {
    "text": "caches in the future as well but of course these delegate out and load data from Ram",
    "start": "395150",
    "end": "400770"
  },
  {
    "text": "in this case D um depending on how far away it is can be anywhere from 150 to 300 cycles it gets a little bit",
    "start": "400770",
    "end": "406770"
  },
  {
    "text": "imprecise talking about cycles there because it's a function of both the processor cycle speed and also the memory speed as well and in fact if you",
    "start": "406770",
    "end": "414150"
  },
  {
    "text": "use a program called LS topo it will show you how your computer looks this was taken on my laptop it's a single",
    "start": "414150",
    "end": "420690"
  },
  {
    "text": "core system with a bunch of cash split over various different levels inside there and it's actually reporting a",
    "start": "420690",
    "end": "426090"
  },
  {
    "text": "level for cash now this level for cash isn't really a cash as such it's memories shared between the GPU and the",
    "start": "426090",
    "end": "431520"
  },
  {
    "text": "CPU on my particular machine that's because it happens to be a laptop but actually we're seeing level for cash",
    "start": "431520",
    "end": "437009"
  },
  {
    "text": "turning up and in particular we're seeing non-volatile van coming online at some of the level for caches as well and",
    "start": "437009",
    "end": "443400"
  },
  {
    "text": "they'll be interesting to see how that evolves the cores are shown down at the bottom we've got four core processor with hyper-threading of available but",
    "start": "443400",
    "end": "452009"
  },
  {
    "start": "449000",
    "end": "449000"
  },
  {
    "text": "there's more than just the memory cache when people talk about memory caches they usually think of this level 1 level 2 level 3 combination but there's a",
    "start": "452009",
    "end": "458099"
  },
  {
    "text": "bunch of others that are inside there one of them which is very important it's called the translation lookaside buffer or TLB and the TLB is used to map the",
    "start": "458099",
    "end": "465900"
  },
  {
    "text": "physical the virtual addresses to where the physical addresses are on the system the reason why this is important is",
    "start": "465900",
    "end": "472500"
  },
  {
    "text": "because every time you do a process change or potentially every time you go into the kernel and back out again you",
    "start": "472500",
    "end": "477990"
  },
  {
    "text": "need to update what those tables are and so those tables will have a listing essentially that says if you see an",
    "start": "477990",
    "end": "483360"
  },
  {
    "text": "address that begins with 8000 then actually map it to this particular Ram chip which is going to be somewhere in",
    "start": "483360",
    "end": "488970"
  },
  {
    "text": "the system or if you see somebody begin with ffff then map it somewhere else this happens for every address you look",
    "start": "488970",
    "end": "494820"
  },
  {
    "text": "up and so therefore it needs to be quite fast specifically if you have something in that cache great you can access memory",
    "start": "494820",
    "end": "500880"
  },
  {
    "text": "quickly if it's not in that cache it's going to take you a while and that's because it has to be what's called a page table walk each process in your",
    "start": "500880",
    "end": "507599"
  },
  {
    "start": "504000",
    "end": "504000"
  },
  {
    "text": "operating system has a page table and that page table is stored and see are you register but each time you do a",
    "start": "507599",
    "end": "512909"
  },
  {
    "text": "process which it's changed over to something else essentially this is the map that your your process is running on",
    "start": "512909",
    "end": "518550"
  },
  {
    "text": "that particular machine and it's a tree so I've demonstrated this as a two level hierarchy here for being a upstair",
    "start": "518550",
    "end": "524579"
  },
  {
    "text": "between them but actually it on modern processes it's four levels deep and on ice Lake which is Intel's next",
    "start": "524579",
    "end": "530130"
  },
  {
    "text": "generation it will be a five level deep page drop and in particular this level of page structure can give you a certain amount",
    "start": "530130",
    "end": "536630"
  },
  {
    "text": "of memory space so at the moment for level page tables we'll take 47 bits 48",
    "start": "536630",
    "end": "542570"
  },
  {
    "text": "bits worth of space for the virtual addresses five levels we'll bring that up to 57 bits which means you can",
    "start": "542570",
    "end": "548660"
  },
  {
    "text": "address far more virtual memory then we need 64 terabytes should be enough for",
    "start": "548660",
    "end": "553970"
  },
  {
    "text": "anyone but these memory pages are split into 4k sizes now this 4k size made",
    "start": "553970",
    "end": "559250"
  },
  {
    "text": "sense back in the 386 days when virtual memory came along but it's not really great for systems that are taking",
    "start": "559250",
    "end": "565070"
  },
  {
    "text": "hundreds of megabytes or hundreds of gigabytes worth of space and so you can change the level of granularity that",
    "start": "565070",
    "end": "571700"
  },
  {
    "text": "this mapping happens from a 4k size to a huge page and a huge page basically",
    "start": "571700",
    "end": "576860"
  },
  {
    "text": "means something that isn't a 4k size most Intel systems will have two different sizes for this they'll have a",
    "start": "576860",
    "end": "582500"
  },
  {
    "text": "two megabytes and one gigabyte support it's under operating system control as to which one of those it uses and each",
    "start": "582500",
    "end": "588920"
  },
  {
    "text": "CPU will have flags to say which one it is different architectures have the same idea of large pages as well but they all",
    "start": "588920",
    "end": "595460"
  },
  {
    "text": "work in slightly different ways or have slightly different sizes by default the purpose of using huge pages is so that",
    "start": "595460",
    "end": "601790"
  },
  {
    "text": "the TLB doesn't need to store as many pointers if you've just got one giant page for your process then all of your",
    "start": "601790",
    "end": "607010"
  },
  {
    "text": "lookups go through that one entry in the table and you can load this fairly quickly so it's good from that point of",
    "start": "607010",
    "end": "613070"
  },
  {
    "text": "view it does have some downsides it might be slightly more complex to set up and use and if you're using huge table",
    "start": "613070",
    "end": "618710"
  },
  {
    "text": "FS then you need to configure it and huge table of s is a bit of a pain this was the first thing that came out in",
    "start": "618710",
    "end": "623900"
  },
  {
    "text": "Linux to be able to support large pages what you'd have to do was to be able to specify ahead of time how many launch",
    "start": "623900",
    "end": "629780"
  },
  {
    "text": "pages you wanted in which the size was going to be and then you had to be in a certain permission group and then your",
    "start": "629780",
    "end": "634790"
  },
  {
    "text": "application would then decide to use these things and generally speaking people tried it people didn't like it people stopped using it so there was",
    "start": "634790",
    "end": "641960"
  },
  {
    "text": "something called transparent huge pages instead now this has been slightly more successful but not without its pain points transparent huge pages says when",
    "start": "641960",
    "end": "649280"
  },
  {
    "start": "642000",
    "end": "642000"
  },
  {
    "text": "you ask for a page instead of giving back a default for k1 then give them back a one megabyte a two megabyte one",
    "start": "649280",
    "end": "654830"
  },
  {
    "text": "or 1 gigabyte one depending on how its configured however most applications were written to assume that when you did an",
    "start": "654830",
    "end": "661670"
  },
  {
    "text": "allocation of a page you just get a 4k sized back and so there right for Kate's worth of data and you've ended up allocating two megabytes",
    "start": "661670",
    "end": "668230"
  },
  {
    "text": "worth contiguous physical space and you're only using a small fraction of it so transparent huge pages when it first",
    "start": "668230",
    "end": "674290"
  },
  {
    "text": "came out and just giving everyone large pages by default didn't really work and so there are several configuration",
    "start": "674290",
    "end": "680380"
  },
  {
    "text": "options you can do one of them is in the huge pages enabled is to specify something that will work if you use em",
    "start": "680380",
    "end": "686350"
  },
  {
    "text": "advise and M advisors are called that you can say yes I'd like to use huge pages please and you can specify that in",
    "start": "686350",
    "end": "692440"
  },
  {
    "text": "your code if you don't do that you get small page if you do you get a big cat page but one of the problems that happen",
    "start": "692440",
    "end": "698260"
  },
  {
    "text": "for high-performing systems was that you would ask for a large page and the operating system will hey I don't have a",
    "start": "698260",
    "end": "704050"
  },
  {
    "text": "large page yet stand by while I go and get one and it would then assemble a whole bunch of little pages and it would take some time which is not good if you",
    "start": "704050",
    "end": "710380"
  },
  {
    "text": "have a low latency system so a relatively new option that was added to this like within the last few releases",
    "start": "710380",
    "end": "715690"
  },
  {
    "text": "of Linux is a defer option and what the defer option will do is it will say okay I'm going to ask for something I would",
    "start": "715690",
    "end": "721780"
  },
  {
    "text": "prefer a large page but if you don't have one then that's fine I'll just take a bunch of small pages and you can fix it again afterwards and that has on the",
    "start": "721780",
    "end": "729190"
  },
  {
    "text": "whole reduce the issue of the blocking that you would see so you'll still see a bunch of blog posts and Stack Overflow",
    "start": "729190",
    "end": "735580"
  },
  {
    "text": "answers that say don't use viewed pages well give this a try with the M advice",
    "start": "735580",
    "end": "740590"
  },
  {
    "text": "and with the defer and see what happens but while the operating system deals with memory in the unit of a page size",
    "start": "740590",
    "end": "747010"
  },
  {
    "text": "whether that's 4k not 2 megabytes the actual processor is dealing with memory at a cache line size and the cache line",
    "start": "747010",
    "end": "753580"
  },
  {
    "start": "750000",
    "end": "750000"
  },
  {
    "text": "size is at the moment about 64 bytes now I say about 64 bytes it's exactly 64",
    "start": "753580",
    "end": "759010"
  },
  {
    "text": "bytes with the Intel processors you're using on your laptops but it may well be 128 bytes in the future so don't assume",
    "start": "759010",
    "end": "765580"
  },
  {
    "text": "that it's going to be 64 bytes and particularly if you're working with mobile devices or arm architectures arm",
    "start": "765580",
    "end": "771640"
  },
  {
    "text": "has got something called big little and you end up with processes with different size cache lines inside them so be aware that different ones exist for Intel",
    "start": "771640",
    "end": "778540"
  },
  {
    "text": "servers mostly you're looking at 64 bytes by the time you're watching this in two or three years time on info queue",
    "start": "778540",
    "end": "783880"
  },
  {
    "text": "it'll probably be 128 bytes you heard it here first so when you load memory from",
    "start": "783880",
    "end": "789760"
  },
  {
    "text": "the process and iterate from the processor and iterate through it what will happen is the CPU will notice that",
    "start": "789760",
    "end": "795760"
  },
  {
    "start": "791000",
    "end": "791000"
  },
  {
    "text": "you're reaching out to memory and then start getting the day so when we're sliding through a memory processor and memory subsystem will",
    "start": "795760",
    "end": "802180"
  },
  {
    "text": "automatically start fetching memory for you if you can arrange your processes to iterate through memory in a linear form",
    "start": "802180",
    "end": "808690"
  },
  {
    "text": "great you're going to be able to go through them bouncing around randomly like when you're traversing an object",
    "start": "808690",
    "end": "814060"
  },
  {
    "text": "heap not so good for the object of the memory system it does notice when you're doing other striding information as well",
    "start": "814060",
    "end": "820959"
  },
  {
    "text": "so if you're striding through every 32 bytes or something the memory Prefecture will notice that and just load every other line for you there is something",
    "start": "820959",
    "end": "828699"
  },
  {
    "text": "that you can use in compilers built-in prefetch which ends up being a prefetch instruction under the covers that can",
    "start": "828699",
    "end": "834519"
  },
  {
    "text": "request that you're going to be looking at some memory soon so please kind of make it available only use this if",
    "start": "834519",
    "end": "840129"
  },
  {
    "text": "you've got the data to show that it makes sense that you're mostly going to make the wrong decisions about it not",
    "start": "840129",
    "end": "845740"
  },
  {
    "text": "because you can't make the right decisions but because you live and request it too early and it will push out stuff that you were using or you'll",
    "start": "845740",
    "end": "852579"
  },
  {
    "text": "request it too late and you'll have all to use the data by the time that you need it so it's something that you can use for tweaking but not something I'd",
    "start": "852579",
    "end": "858490"
  },
  {
    "text": "recommend jumping to as a first point trying to organize your memory structure so that you can process it through",
    "start": "858490",
    "end": "863560"
  },
  {
    "text": "linearly is going to be the way that you can improve performance of that layer you can also have something in cache",
    "start": "863560",
    "end": "869290"
  },
  {
    "text": "lines called full sharing and full sharing is when you've got something of a cache line size and you've got two threads being able to read and write",
    "start": "869290",
    "end": "875050"
  },
  {
    "text": "inside it although they're writing to different locations different variables in computer language speak if they're",
    "start": "875050",
    "end": "880899"
  },
  {
    "text": "reading and writing into the same cache line then there's going to be a bit of tug of war between two cords and if you've got those two cores on the other",
    "start": "880899",
    "end": "886720"
  },
  {
    "text": "sides of a 56 core package or in another socket then you're going to get contention inside here and you're either",
    "start": "886720",
    "end": "893199"
  },
  {
    "text": "going to get data loss if you haven't used synchronization primitives or you're going to get a bit of bad performance as they're fighting over the",
    "start": "893199",
    "end": "899350"
  },
  {
    "text": "exclusive ownership for that particular cache line so to avoid this if you",
    "start": "899350",
    "end": "904660"
  },
  {
    "text": "aren't doing processing with multiple threads and you're going to be reading and writing a lot of data then have them",
    "start": "904660",
    "end": "909759"
  },
  {
    "text": "separated by a couple of cache lines I say a couple of cache lines because the line field buffa when it loads things it",
    "start": "909759",
    "end": "915040"
  },
  {
    "text": "will load in a couple of cache lines at a time and so although you're not reading and writing it at that section you might find that you're still",
    "start": "915040",
    "end": "921189"
  },
  {
    "text": "treading on each other's toes and the sun hotspot so the oracle hotspot",
    "start": "921189",
    "end": "926230"
  },
  {
    "text": "compiler has a Sun miss contender which is waxed in 128 bytes of blank space when you're trying to read and",
    "start": "926230",
    "end": "933070"
  },
  {
    "text": "write something so that it avoids this particular problem of course that number will change as the cache line size",
    "start": "933070",
    "end": "938320"
  },
  {
    "text": "changes as well so in order to take the best advantage out of the memory",
    "start": "938320",
    "end": "943660"
  },
  {
    "start": "941000",
    "end": "941000"
  },
  {
    "text": "subsystem try and design your data layout and your data structures so that they fit in within an appropriate amount",
    "start": "943660",
    "end": "949810"
  },
  {
    "text": "of space in other words if your hot data set that you're processing and using a lot can fit inside the l1 cache great",
    "start": "949810",
    "end": "955300"
  },
  {
    "text": "that's fine you'll be able to process it very quickly indeed or you can buy a nice lake jet which has got 48 cage with",
    "start": "955300",
    "end": "960670"
  },
  {
    "text": "level 1 instead of 32 K but if it doesn't see if you can fit in the level 2 and see if it can fit into the level 3",
    "start": "960670",
    "end": "966519"
  },
  {
    "text": "these kind of things are visible from JavaScript or Java or Python or whatever you're dealing with and you can see if",
    "start": "966519",
    "end": "972670"
  },
  {
    "text": "you measure performance as you ramp the sizes up that you get these step changes as you get out of one layer of cash into",
    "start": "972670",
    "end": "979029"
  },
  {
    "text": "the next one consider how you structure your data as well so if you've got data in a set of arrays it may be easier to",
    "start": "979029",
    "end": "986620"
  },
  {
    "text": "pivot and think of it as a set of arrays each with one filled in because often if you're processing through I say you're",
    "start": "986620",
    "end": "993070"
  },
  {
    "text": "processing images you may not need to process the Alpha value but you might want to process the red green and blue and if you have them as been a way of",
    "start": "993070",
    "end": "1000060"
  },
  {
    "text": "reds array of blues and a way of greens then you'll be able to get a better performance than if you iterate through",
    "start": "1000060",
    "end": "1005490"
  },
  {
    "text": "red green blue red green blue red green blue in memory and also consider using thread local or call local data",
    "start": "1005490",
    "end": "1011910"
  },
  {
    "text": "structures as well if you're doing some kind of MapReduce operation over a whole bunch of data treat it is a distributed",
    "start": "1011910",
    "end": "1017220"
  },
  {
    "text": "system in a MapReduce job you'd fire attack you get a load of results and then you'd merge them back in at the last step treat that the same with your",
    "start": "1017220",
    "end": "1023160"
  },
  {
    "text": "calls as well get one call to calculate and do something another call to calculate do something else and then bring those results in and add them up",
    "start": "1023160",
    "end": "1029188"
  },
  {
    "text": "rather than trying to fight over some sort of shared variable in memory and consider compressing data this happened",
    "start": "1029189",
    "end": "1035220"
  },
  {
    "text": "in the JVM recently when we had compressed strings and instead of having each character taking up two bytes worth",
    "start": "1035220",
    "end": "1042480"
  },
  {
    "text": "of space it was compressed down to only taking one bite I'm out of space and that was a performance win because",
    "start": "1042480",
    "end": "1047790"
  },
  {
    "text": "although it costs a little bit of extra instructions to expand and contract on demand as it's loaded in actually the",
    "start": "1047790",
    "end": "1054450"
  },
  {
    "text": "fact that you're shifting less data both in and out of the caches and also for the garbage collector to process meant",
    "start": "1054450",
    "end": "1059880"
  },
  {
    "text": "that it was a performance winner and you could use other kind of compressions oh geez histogram HDR histogram is a",
    "start": "1059880",
    "end": "1066330"
  },
  {
    "text": "good way of compressing a lot of data down in a logarithmic form depending on your application you'll be able to think",
    "start": "1066330",
    "end": "1072000"
  },
  {
    "text": "of something as well you can also pin where memory and feds live so if you're",
    "start": "1072000",
    "end": "1077070"
  },
  {
    "start": "1074000",
    "end": "1074000"
  },
  {
    "text": "dealing with a massively scalable system and you've got a massively scalable problem to solve then try and pin the",
    "start": "1077070",
    "end": "1083040"
  },
  {
    "text": "feds to put certain places to be able to process them if you're dealing with something like a neti benchmark where",
    "start": "1083040",
    "end": "1089400"
  },
  {
    "text": "you're consuming events over network socket then pin your worker thread so that one worker lives on this court and",
    "start": "1089400",
    "end": "1094950"
  },
  {
    "text": "other work and lives on that core another worker lives on a different core and that way you'll get the best performance because you'll never get",
    "start": "1094950",
    "end": "1100500"
  },
  {
    "text": "them processed or swapped around the place if you are doing that you might need to tell the Linux kernel to stay",
    "start": "1100500",
    "end": "1105840"
  },
  {
    "text": "away from the subsection of those cores using ISO CPUs which is a boot time option that you can say you want the Linux kernel to do its housekeeping just",
    "start": "1105840",
    "end": "1113400"
  },
  {
    "text": "on a subset of your memory and you can also use tasks set to say when you start off a program like a logging demon or",
    "start": "1113400",
    "end": "1119370"
  },
  {
    "text": "something that you want to create a CPU set that your application is going to use in other applications are going to be one elsewhere these kind of things",
    "start": "1119370",
    "end": "1125910"
  },
  {
    "text": "you can specify as a kind of Linux s admin to be able to do that there's also",
    "start": "1125910",
    "end": "1131070"
  },
  {
    "text": "Nemo control and Lib NEMA which you can specify in order to be able to control",
    "start": "1131070",
    "end": "1136440"
  },
  {
    "text": "programmatically where you're going to allocate large chunks of memory and again you can use the sub numr clusters",
    "start": "1136440",
    "end": "1141600"
  },
  {
    "text": "or the processes to be able to decide where that goes so we talked a lot about memory what about the actual brains of",
    "start": "1141600",
    "end": "1147150"
  },
  {
    "text": "the operation what about the CPU well the CPU or the core is split into two parts the front end and the back end and",
    "start": "1147150",
    "end": "1154020"
  },
  {
    "start": "1148000",
    "end": "1148000"
  },
  {
    "text": "this isn't like front-end development it doesn't want Java Script in there the front ends job is to take a bunch of instructions in x86 format decode them",
    "start": "1154020",
    "end": "1162210"
  },
  {
    "text": "figure out what they are and then spit out micro operations or you ops because U is easy to type than me on the",
    "start": "1162210",
    "end": "1167790"
  },
  {
    "text": "keyboard for the back end process so we get a bunch of date bytes that come in off the memory system it goes into the",
    "start": "1167790",
    "end": "1174720"
  },
  {
    "text": "pre decode which says this is where this instruction ends this is where this instruction ends this is where this instruction ends goes into an",
    "start": "1174720",
    "end": "1180690"
  },
  {
    "text": "instruction decode and then converts that into micro ops we're going to look at just this increment one here because",
    "start": "1180690",
    "end": "1186540"
  },
  {
    "text": "when you increment something in memory what it really means is you're going to load from memory going to do an addition and then you're going to write back to",
    "start": "1186540",
    "end": "1192570"
  },
  {
    "text": "memory and so that increment responds to free microbes generally speaking you'll have a one-to-one type",
    "start": "1192570",
    "end": "1198410"
  },
  {
    "text": "relationship between the loads and the microbes in the backend if you're using complex complex addressing modes where",
    "start": "1198410",
    "end": "1204800"
  },
  {
    "text": "complex means you're adding an offset or you're multiplying some value then there might be a few other microbes they get",
    "start": "1204800",
    "end": "1210020"
  },
  {
    "text": "generated as well but ultimately the point of this is to spit out a bunch of microbes for the backend to do at this",
    "start": "1210020",
    "end": "1216560"
  },
  {
    "text": "point we're all operating in order so the instructions come in in order the Yorke's come out in order that we want",
    "start": "1216560",
    "end": "1222110"
  },
  {
    "text": "them to execute as well but there's a few things that go on the front end one of them is the UOC cache and so as you",
    "start": "1222110",
    "end": "1228290"
  },
  {
    "text": "do the decoding from Intel instructions which are fairly complicated into a set of these micro operations which are",
    "start": "1228290",
    "end": "1235130"
  },
  {
    "text": "internal but slightly easier they can be cached so the next time you see it it'll spit out the same things and importantly",
    "start": "1235130",
    "end": "1242090"
  },
  {
    "text": "we've got a loop stream decoder which uses loops so that when you're running through a very tight loop it will just",
    "start": "1242090",
    "end": "1248480"
  },
  {
    "text": "serve the decoded you ops rather than going through this parsing process and it will shave off a few elements in the",
    "start": "1248480",
    "end": "1254210"
  },
  {
    "text": "pipeline most pipelines on Intel processors are between 14 and 19 deep",
    "start": "1254210",
    "end": "1259640"
  },
  {
    "text": "depending on what it's doing and one of the things that impacts it more than anything else is the branch predictor so",
    "start": "1259640",
    "end": "1265280"
  },
  {
    "text": "the branch predictors job is to figure out where we are going next it's kind of like the Sat Nav for the CPU it's right",
    "start": "1265280",
    "end": "1272150"
  },
  {
    "start": "1267000",
    "end": "1267000"
  },
  {
    "text": "most of the time probably has a better rating to me when I'm trying to do the driving navigations and but it figures",
    "start": "1272150",
    "end": "1277820"
  },
  {
    "text": "out where you're trying to go it assumes whether or not the branch is taken and then it starts serving the instructions so that by the time the pipeline flows",
    "start": "1277820",
    "end": "1284300"
  },
  {
    "text": "through the instructions are all ready and waiting to go sometimes that will fail and in that case the parsing and",
    "start": "1284300",
    "end": "1291800"
  },
  {
    "text": "the decoding is thrown away it resets to where it should be and then you have what's called a pipeline stall or bubble",
    "start": "1291800",
    "end": "1296960"
  },
  {
    "text": "where it waits for the instructions to go through and then catch up again if you can reduce band speculation on the",
    "start": "1296960",
    "end": "1302570"
  },
  {
    "text": "branches you're going to get better performance in your application so the branch predictor dynamically adapts to",
    "start": "1302570",
    "end": "1309200"
  },
  {
    "text": "what your code is actually doing and this is also something that's visible from the high level as well if you've",
    "start": "1309200",
    "end": "1314720"
  },
  {
    "text": "got a Java program or JavaScript program that's iterating through a bunch of arrays and maybe adding up say positive",
    "start": "1314720",
    "end": "1322160"
  },
  {
    "text": "numbers into one counter and negative numbers into another counter the branch predictor figuring out which of the",
    "start": "1322160",
    "end": "1327830"
  },
  {
    "text": "elements go down is going to be confused on random data and you get reasonable performance but the branch predictor",
    "start": "1327830",
    "end": "1333549"
  },
  {
    "text": "won't be able to help you because it will be right about 50% of the time if you're dealing with a sorted data set so",
    "start": "1333549",
    "end": "1339910"
  },
  {
    "text": "in other words you see all the negative numbers first and then all the positive numbers afterwards the branch predictor is going to be on top of the game and it",
    "start": "1339910",
    "end": "1346480"
  },
  {
    "text": "will give you as best performance because it will assume once it's seen the first few negative numbers that they're all going to be negative until",
    "start": "1346480",
    "end": "1351580"
  },
  {
    "text": "it changes over when it will vert you get a bit of slow performance and then the positive numbers will go on from there the branch predictor which decides",
    "start": "1351580",
    "end": "1359980"
  },
  {
    "text": "whether you go down something is only half of the story there's also a branch target predictor as well and this target",
    "start": "1359980",
    "end": "1365440"
  },
  {
    "start": "1363000",
    "end": "1363000"
  },
  {
    "text": "predictor says where it is that you're going to go to now in a lot of cases the branch predictor is going to know",
    "start": "1365440",
    "end": "1371440"
  },
  {
    "text": "exactly because you're going to jump to a specific memory location you're jumping to the system exit function or",
    "start": "1371440",
    "end": "1376660"
  },
  {
    "text": "you're jumping to this particular routine at the start of the loop again sometimes though you're jumping to the",
    "start": "1376660",
    "end": "1382929"
  },
  {
    "text": "value of a register and that register may not have been computed yet so we'll",
    "start": "1382929",
    "end": "1388030"
  },
  {
    "text": "take a punt and think actually what it's trying to do is that it will load this data value in but sometimes that might",
    "start": "1388030",
    "end": "1394480"
  },
  {
    "text": "be wrong we need to rewind and do things again you'll see the branch target predictor being confused a lot when you",
    "start": "1394480",
    "end": "1399730"
  },
  {
    "text": "iterate through object-oriented code whether that c++ whether that's your own object orientation whether it's the JVM",
    "start": "1399730",
    "end": "1405100"
  },
  {
    "text": "or something and that's because in order to figure out where to go it has to be able to load what the class is look in",
    "start": "1405100",
    "end": "1412210"
  },
  {
    "text": "the class word and then figure out from the class word where the vtable is and then jump to the implementation in the v table and those few jumps are going to",
    "start": "1412210",
    "end": "1419200"
  },
  {
    "text": "confuse branch predictors the branch target predictor one of the things that you can do to speed this up is that you",
    "start": "1419200",
    "end": "1426340"
  },
  {
    "text": "can have a check to say if this looks like an x class jump to the x class",
    "start": "1426340",
    "end": "1431710"
  },
  {
    "text": "implementation otherwise fall back to dynamic dispatch and that's something that you can implement very cheaply and",
    "start": "1431710",
    "end": "1436900"
  },
  {
    "text": "very quickly and in fact the JIT and the JVM does this by using monomorphic and biomorphic dispatch for the common cases",
    "start": "1436900",
    "end": "1442809"
  },
  {
    "text": "and some of the non oracle ones do more than just two but that works in c++ as",
    "start": "1442809",
    "end": "1447970"
  },
  {
    "text": "well so if you've got a dynamic dispatch call like if you're implementing a VFS I saw something a while ago that said a",
    "start": "1447970",
    "end": "1454990"
  },
  {
    "text": "linux VFS operation had been sped up with a factor of three or something simply because they said if you're using",
    "start": "1454990",
    "end": "1460240"
  },
  {
    "text": "like the x2 arming system then delegate this to the x/2 implementation instead and some of",
    "start": "1460240",
    "end": "1466340"
  },
  {
    "text": "the recent mitigations that have been put in the linux kernel to avoid things like spectre and meltdown and so on have",
    "start": "1466340",
    "end": "1472940"
  },
  {
    "text": "actually decreased performance of linux over time so by putting that kind of your own monomorphic dispatch you can",
    "start": "1472940",
    "end": "1480020"
  },
  {
    "text": "then gain some of that speed-up back again and here I think Martin is highlighted this a few times in past",
    "start": "1480020",
    "end": "1486080"
  },
  {
    "text": "inlining is the master optimization because when you do inlining you suddenly know much more about where it",
    "start": "1486080",
    "end": "1492200"
  },
  {
    "text": "is that you're going but you also often lose an indirect call as well actually losing those function calls is a good",
    "start": "1492200",
    "end": "1499130"
  },
  {
    "text": "way of optimizing performance because you then don't have to worry about the branch predictor or the target predictor",
    "start": "1499130",
    "end": "1504260"
  },
  {
    "text": "at the cpu from being else do things so we figure out where we've going we've got a whole bunch of you ops we've got",
    "start": "1504260",
    "end": "1510020"
  },
  {
    "text": "this nice stream with them coming through what happens next well then it goes over to the backend and so the backends job is to take all of these you",
    "start": "1510020",
    "end": "1516320"
  },
  {
    "text": "ops do the calculations and then spit out any side effects to memory so in this particular case we're loading from",
    "start": "1516320",
    "end": "1522290"
  },
  {
    "text": "a location in memory we're incrementing that and then we are writing that value",
    "start": "1522290",
    "end": "1527480"
  },
  {
    "text": "back again now at this point we don't have to worry about things like EAX ESI",
    "start": "1527480",
    "end": "1533060"
  },
  {
    "text": "RSI and so on because we've all changed them to temporary registers the x86 visor has these registers inside the",
    "start": "1533060",
    "end": "1540350"
  },
  {
    "text": "core has a lot more of these things and so it says for this particular instruction we're gonna stick the",
    "start": "1540350",
    "end": "1545690"
  },
  {
    "text": "temporary variable which was EA X here in to say r99 it's you're going to pick",
    "start": "1545690",
    "end": "1551180"
  },
  {
    "text": "one of the ones that's free inside this and so as processes evolve as this register file gets larger we can deal",
    "start": "1551180",
    "end": "1557660"
  },
  {
    "text": "with more and more in-flight data and that's important because inside the back-end once we've done the allocation",
    "start": "1557660",
    "end": "1563090"
  },
  {
    "text": "we are now after the races all of these new ops are competing for availability on the kind of internal processor at the",
    "start": "1563090",
    "end": "1569210"
  },
  {
    "text": "backend to actually do their work and as soon as their data dependencies are there they'll then be scheduled executed",
    "start": "1569210",
    "end": "1575000"
  },
  {
    "text": "and the results will go in Intel processes and most other server-side processes these days are out of order",
    "start": "1575000",
    "end": "1581030"
  },
  {
    "text": "because at this point any of those microbes can happen at a time and importantly as far as the performance is",
    "start": "1581030",
    "end": "1587240"
  },
  {
    "text": "concerned this back-end is capable of running multiple UROP simultaneously so the front-end can dispatch to the",
    "start": "1587240",
    "end": "1593990"
  },
  {
    "text": "back-end for you ops per cycle for Intel and cascade lake systems for isolate systems it can",
    "start": "1593990",
    "end": "1600450"
  },
  {
    "text": "issue five you ops per cycle and if they aren't competing for allocations are for",
    "start": "1600450",
    "end": "1605850"
  },
  {
    "text": "slots that they need to run on then you can have you know a number of those Yorke's running in parallel as well so",
    "start": "1605850",
    "end": "1611700"
  },
  {
    "text": "in this particular case our internal processor has got a number of ports and those ports are the execution units",
    "start": "1611700",
    "end": "1617670"
  },
  {
    "text": "they'll all be able to do with arithmetic they will be able to do logical ones some of them are going to be able to do divide some of them are",
    "start": "1617670",
    "end": "1623070"
  },
  {
    "text": "going to be able to do multiply there are different implementations for the integer and floating-point sides and all",
    "start": "1623070",
    "end": "1628470"
  },
  {
    "text": "of these will be able to take up 256 by 256 bits values inside them port 5 will",
    "start": "1628470",
    "end": "1635910"
  },
  {
    "text": "be able to handle 512 bits on its own and you can combine ports 0 & 1 to have a 512 bit operation so if you are",
    "start": "1635910",
    "end": "1643710"
  },
  {
    "text": "dealing with 512 bits you've essentially got two parts either port 5 or port 0 and one you don't get to decide what",
    "start": "1643710",
    "end": "1649980"
  },
  {
    "text": "happens this is the CPU doing it for you but you can at least execute those two things in parallel and there's a bunch",
    "start": "1649980",
    "end": "1654990"
  },
  {
    "text": "of other ones that you need to use as well for address generation and loading and storing and so on so in this",
    "start": "1654990",
    "end": "1660030"
  },
  {
    "text": "particular case they get allocated to their appropriate ports we figure out what the value is that comes in from the",
    "start": "1660030",
    "end": "1666000"
  },
  {
    "text": "load we figure out that the register now is value to a the increment is now ready",
    "start": "1666000",
    "end": "1671280"
  },
  {
    "text": "to execute because it's dependency has been met and so that does the update and then once that's there the byte then",
    "start": "1671280",
    "end": "1676440"
  },
  {
    "text": "flows out at the end at this point when it goes out the door we're back to in order again lots of stuff can have",
    "start": "1676440",
    "end": "1681960"
  },
  {
    "text": "happened we may have made lots of mistakes we may have knackered our cash up and we can see that externally",
    "start": "1681960",
    "end": "1687330"
  },
  {
    "text": "through spectrin meltdown but between the York's going into the backend and the York's coming out of it all the",
    "start": "1687330",
    "end": "1693060"
  },
  {
    "text": "writes going out to it we've been in out of order through that whole time so how",
    "start": "1693060",
    "end": "1698220"
  },
  {
    "text": "do we know what's going on inside to be able to change things well I'm sure you all away with Perth perf is a general",
    "start": "1698220",
    "end": "1703290"
  },
  {
    "text": "then it's performed tool that can interrogate counters that are kept inside the back end and the front end of",
    "start": "1703290",
    "end": "1710310"
  },
  {
    "text": "the core itself there's tools like record which will allow you to trace the execution of a program annotate and report to tell you what the",
    "start": "1710310",
    "end": "1717150"
  },
  {
    "text": "results actually mean from the binary file and a stack for being able to interrogate performance counters themselves so perfect cord when you run",
    "start": "1717150",
    "end": "1725160"
  },
  {
    "start": "1724000",
    "end": "1724000"
  },
  {
    "text": "it will take an application will then generate a perf data file the results you can then slap that file",
    "start": "1725160",
    "end": "1731100"
  },
  {
    "text": "off to another machine for analysis if you want to but all you can do data processing on the host depending on what",
    "start": "1731100",
    "end": "1736799"
  },
  {
    "text": "you're doing when you do the recordings in Perth it will capture the stack trace",
    "start": "1736799",
    "end": "1742380"
  },
  {
    "text": "at each point and the stack trace is then going to be where you are so you can imagine a tit and get the results back again like profilers that you're",
    "start": "1742380",
    "end": "1748919"
  },
  {
    "text": "used to at the high end will do however at this level profile the profiler is subject to what's called skid and",
    "start": "1748919",
    "end": "1755279"
  },
  {
    "text": "skitty's you say I want to start recording here but actually the process is still doing things and by the time in",
    "start": "1755279",
    "end": "1760740"
  },
  {
    "text": "notices that you want to do a change it's actually got to back up and then",
    "start": "1760740",
    "end": "1765840"
  },
  {
    "text": "say well we're here now so I guess that's where you meant there are some precision Flags the colon Peas that you",
    "start": "1765840",
    "end": "1770970"
  },
  {
    "text": "can add to it for more precise values and they will add more and more overhead but we'll get you more and more closer",
    "start": "1770970",
    "end": "1777299"
  },
  {
    "text": "to the precise value of what's happened generally speaking if you're trying to identify bottlenecks in your system then",
    "start": "1777299",
    "end": "1783059"
  },
  {
    "text": "you'll be able to get a rough overview first of all by running perf without it and then as you hone down and narrow",
    "start": "1783059",
    "end": "1790080"
  },
  {
    "text": "down where the issues are you may then want to start turning on the precision to get exact values of it when you work",
    "start": "1790080",
    "end": "1796259"
  },
  {
    "text": "or branches what will happen is the perfect program will try and figure out what the back-trace is by walking back",
    "start": "1796259",
    "end": "1801899"
  },
  {
    "text": "the stack if you've got code that's dealing with the branch pointer inside",
    "start": "1801899",
    "end": "1806970"
  },
  {
    "text": "there it will be able to follow that back from the stack but quite a lot of programs are compiled without the branch",
    "start": "1806970",
    "end": "1811980"
  },
  {
    "text": "point of support because back in the 32-bit days we didn't have many registers and so it was useful to be",
    "start": "1811980",
    "end": "1817320"
  },
  {
    "text": "able to use the frame pointer for something else these days probably less of a reason not to have it in there but",
    "start": "1817320",
    "end": "1824460"
  },
  {
    "text": "if you're dealing with a debug server with debug symbols it can use the dwarf",
    "start": "1824460",
    "end": "1829620"
  },
  {
    "text": "format to be able to figure out how to walk the stack back again if you ever see things about incomplete stack traces",
    "start": "1829620",
    "end": "1834870"
  },
  {
    "text": "it's probably because you don't have the buffer pointers in there and you don't have the debug symbols to be able to track it back however for running code",
    "start": "1834870",
    "end": "1842639"
  },
  {
    "text": "the perf command supports something called cool graph and what the cool graph option will do is",
    "start": "1842639",
    "end": "1848129"
  },
  {
    "text": "say which flavor of back tracing that you want to use lbr is Intel's last branch record and so what happens is as",
    "start": "1848129",
    "end": "1854190"
  },
  {
    "text": "the Intel processor is jumping around it records where it's been and if you take a snapshot of that every",
    "start": "1854190",
    "end": "1859420"
  },
  {
    "text": "few call stacks then you can build up a complete picture of where you've gone even if you don't have the debugging",
    "start": "1859420",
    "end": "1864910"
  },
  {
    "text": "symbols or the frame pointers inside there so that will give you accurate values now there's also something on",
    "start": "1864910",
    "end": "1870430"
  },
  {
    "text": "even newer Intel processors called Intel processor trace an Intel processor trace does the same thing but with a much",
    "start": "1870430",
    "end": "1876250"
  },
  {
    "text": "lower overhead and there's a couple of Linux weekly articles down at the bottom which you can find when you look at the slides later as well as perf record",
    "start": "1876250",
    "end": "1883810"
  },
  {
    "text": "there's also perf stat perf stat will give you an idea of how much work your program is actually doing and it will",
    "start": "1883810",
    "end": "1889690"
  },
  {
    "text": "read the counters from the processor to be able to tell you that and tell you how many instructions there are how many",
    "start": "1889690",
    "end": "1895750"
  },
  {
    "text": "branches you've taken in this case the branch misses we've got about a 5% branch misses so means the branch predictor is working about 95% of the",
    "start": "1895750",
    "end": "1902200"
  },
  {
    "text": "time which is nice but those are the kind of things which you could expect one thing to look for is the",
    "start": "1902200",
    "end": "1907960"
  },
  {
    "text": "instructions per cycle or IPC and the IPC says how efficiently you're doing work you can have a program that's",
    "start": "1907960",
    "end": "1914020"
  },
  {
    "text": "running at 100% CPU and has an IPC of 9.5 that will run dog slow and you can",
    "start": "1914020",
    "end": "1919420"
  },
  {
    "text": "have the same program running a hundred percent CPU and get an IPC of 2.5 and be running five times faster so a hundred",
    "start": "1919420",
    "end": "1925870"
  },
  {
    "text": "percent times 5 is 100% right it's a great speed up if you can find it so depending on which processor you're",
    "start": "1925870",
    "end": "1932320"
  },
  {
    "text": "running the IPC is kind of going to be in a region of below one which means that we have potentially other issues",
    "start": "1932320",
    "end": "1938890"
  },
  {
    "text": "that we need to investigate or something closer to four which is the sort of maximum type number that you'll see out",
    "start": "1938890",
    "end": "1944350"
  },
  {
    "text": "of these kind of systems I think isolate we might start to Beach into the five IPC but you know larger numbers are",
    "start": "1944350",
    "end": "1951250"
  },
  {
    "text": "better in that particular case and less than one means that you probably need to look in to find out what's going on and these things weed from what are called",
    "start": "1951250",
    "end": "1957700"
  },
  {
    "text": "performance counters performance counters are model specific registers that each CPU has and as it goes through",
    "start": "1957700",
    "end": "1963910"
  },
  {
    "start": "1958000",
    "end": "1958000"
  },
  {
    "text": "and executes instructions it will increment this counter you get some for free a number of branches branch misses",
    "start": "1963910",
    "end": "1969960"
  },
  {
    "text": "a number of instructions executed and so on but there are programmatic ones as well if you want to count how many TLB",
    "start": "1969960",
    "end": "1976630"
  },
  {
    "text": "caches you're doing how many page walks you're doing how many executions you're running on port 5 on the process for",
    "start": "1976630",
    "end": "1984010"
  },
  {
    "text": "itself you can actually configure these counters to be able to give the answers to those values and they've got names",
    "start": "1984010",
    "end": "1989620"
  },
  {
    "text": "perf - list will tell you what they all are if you don't know what the name is that you know what the code is you can put in something random and that will tell you",
    "start": "1989620",
    "end": "1995650"
  },
  {
    "text": "something as well of course I don't know what that is I picked it from somewhere but I can't remember what it is and",
    "start": "1995650",
    "end": "2001320"
  },
  {
    "start": "2001000",
    "end": "2001000"
  },
  {
    "text": "that's because usually when you're doing analysis you need to have a process to follow there's something called the top down and our top down microarchitecture",
    "start": "2001320",
    "end": "2008370"
  },
  {
    "text": "analysis method or team amp which was created by Yassen and what it says is",
    "start": "2008370",
    "end": "2014549"
  },
  {
    "text": "let's take the performance of our application and figure out where the bottlenecks are so in the big diagram I had before with the front in the back",
    "start": "2014549",
    "end": "2020280"
  },
  {
    "text": "end that's the kind of separation that we have are we failing to serve the number of you ops in the front end are",
    "start": "2020280",
    "end": "2025919"
  },
  {
    "text": "we blocked on the back end for doing some kind of processing are we retiring them I doesn't mean going off to live",
    "start": "2025919",
    "end": "2030990"
  },
  {
    "text": "summer in the country that means actually doing the work or is it bad speculation because we've simply no idea where we're going and each one of these",
    "start": "2030990",
    "end": "2037080"
  },
  {
    "text": "has got various different perf counters that you can enable to be able to say where you are now again this is great if",
    "start": "2037080",
    "end": "2044700"
  },
  {
    "text": "you've got it and it mentions it in the Intel software optimization menu but what it really boils down to is saying",
    "start": "2044700",
    "end": "2050520"
  },
  {
    "start": "2046000",
    "end": "2046000"
  },
  {
    "text": "have we ever allocated the you up if we have allocated you up does it retire if it retires then great that's useful work",
    "start": "2050520",
    "end": "2057030"
  },
  {
    "text": "done if not it falls down to the bad speculation route and if the Europe isn't allocated is that because we're",
    "start": "2057030",
    "end": "2062730"
  },
  {
    "text": "stalling on the front end or on the back end now it all sounds very complicated but as long as you can spell top-down",
    "start": "2062730",
    "end": "2068158"
  },
  {
    "start": "2068000",
    "end": "2068000"
  },
  {
    "text": "great because perfect as it for you if you want perf - top-down it will run on",
    "start": "2068159",
    "end": "2073200"
  },
  {
    "text": "your system-wide and it will give you an overview in which buckets your system is running in in this case we're not timing",
    "start": "2073200",
    "end": "2079378"
  },
  {
    "text": "sleep we're just using that as a holding point for being able to specify what's going on in this particular case I've",
    "start": "2079379",
    "end": "2086940"
  },
  {
    "text": "got a six a six core system running on a single socket it's probably three calls and three high profits and the retiring",
    "start": "2086940",
    "end": "2093510"
  },
  {
    "text": "is somewhere between 15% and 35% in other words we've got between three and four times that we could optimize this",
    "start": "2093510",
    "end": "2100619"
  },
  {
    "text": "to go faster if this retiring is like 100% job done it's beer o'clock but most",
    "start": "2100619",
    "end": "2105660"
  },
  {
    "text": "of the time you'll find this is a smaller number and you can optimize this number by figuring out why your programs running slower and then taking steps to",
    "start": "2105660",
    "end": "2112230"
  },
  {
    "text": "be able to optimize it in this particular case it's highlighting in red that we may be front-end or back-end bound across a whole bunch of processes",
    "start": "2112230",
    "end": "2119100"
  },
  {
    "text": "but to find out something more useful than that this will give you a snapshot of your system you really want to find",
    "start": "2119100",
    "end": "2124380"
  },
  {
    "text": "what your process is doing so auntie cleaners written something called top Lev and that top left all will go through the process and give you",
    "start": "2124380",
    "end": "2131200"
  },
  {
    "start": "2127000",
    "end": "2127000"
  },
  {
    "text": "ideas of where you can start looking when you first run it it downloads some configuration files from Intel's website",
    "start": "2131200",
    "end": "2137200"
  },
  {
    "text": "download zero one dog for the processor that you're running at so that it knows what performance counters it has",
    "start": "2137200",
    "end": "2143290"
  },
  {
    "text": "available to it if you're deploying this on a air-gap server you can download these configuration files ahead of time",
    "start": "2143290",
    "end": "2148630"
  },
  {
    "text": "and then deploy them whether there's documentation that shows you how to do that now essentially this is a very",
    "start": "2148630",
    "end": "2154000"
  },
  {
    "text": "fancy front-end to perf so that cpu o 0 3 mask equals zero I don't know what",
    "start": "2154000",
    "end": "2159490"
  },
  {
    "text": "that is but top-load does and that will then be able to generate a perfect command there's an option that you can use to even say what perf come on would",
    "start": "2159490",
    "end": "2166180"
  },
  {
    "text": "you run so that you can then copy and paste it and run it somewhere else but this will do the work for you the other",
    "start": "2166180",
    "end": "2171970"
  },
  {
    "text": "nice thing that top-level will do is if you have a repeatable workload you're repeatedly running a particular process",
    "start": "2171970",
    "end": "2177400"
  },
  {
    "text": "then you can use this in no multiplex mode because we talked about multiplexing before if you've got two",
    "start": "2177400",
    "end": "2183550"
  },
  {
    "text": "symbols and you don't have a count of them it'll record one that it will record the other pin it will record one",
    "start": "2183550",
    "end": "2188920"
  },
  {
    "text": "then it will record the other and then essentially use double account afterwards in no multiplex mode it runs",
    "start": "2188920",
    "end": "2194140"
  },
  {
    "text": "the entire program counting this counter that it runs it again counting the other counter and that will give you a much",
    "start": "2194140",
    "end": "2199240"
  },
  {
    "text": "more accurate view of what's happening in your system assuming that it's repeatable so you can then run it with -",
    "start": "2199240",
    "end": "2204940"
  },
  {
    "text": "l1 it will show you the same sort of thing as perfect optic but for that particular process only and here we're",
    "start": "2204940",
    "end": "2210640"
  },
  {
    "start": "2210000",
    "end": "2210000"
  },
  {
    "text": "creating a 16 megabyte random data file and inside there we're looking at a single-threaded no multiplexing level",
    "start": "2210640",
    "end": "2217180"
  },
  {
    "text": "one for base64 encoding this just because base64 is something pretty much everyone has and you can play around",
    "start": "2217180",
    "end": "2223150"
  },
  {
    "text": "with this example at home afterwards and when we run it it'll do some calculations and one of the things that",
    "start": "2223150",
    "end": "2229030"
  },
  {
    "text": "will tell us is that by the way did you know this program is back in bound in other words we're getting the stuff in the memory we're doing the Europe",
    "start": "2229030",
    "end": "2235530"
  },
  {
    "text": "allocations branch predictors not failing dismally we're passing it to the back end and then the back end isn't",
    "start": "2235530",
    "end": "2241570"
  },
  {
    "text": "going through it as quickly as we would like if you run it with l2 it will say that it's Core bound and if you run it",
    "start": "2241570",
    "end": "2247090"
  },
  {
    "text": "again it will say its ports utilization well this particular thing means is that we're generating a whole bunch of",
    "start": "2247090",
    "end": "2252220"
  },
  {
    "text": "instructions they are running on one of the ports inside that diagram that I showed you several slides ago",
    "start": "2252220",
    "end": "2258950"
  },
  {
    "text": "but because there aren't any other ports available to do that it's all being bottled next on this port so for example",
    "start": "2258950",
    "end": "2264770"
  },
  {
    "text": "if we were doing a bunch of divisions we would expect everything just to be running on port one because that's the only one that's doing into division",
    "start": "2264770",
    "end": "2270589"
  },
  {
    "text": "flags different processes will give you different performance primarily because Intel keep adding execution units so",
    "start": "2270589",
    "end": "2276829"
  },
  {
    "text": "cascade Lake in it and sky Lake are pretty much the same thing one of them is just a slightly optimized way of",
    "start": "2276829",
    "end": "2282950"
  },
  {
    "text": "laying down the silicon ice lake is then adding new ports inside there and new functionality so you'd expect this",
    "start": "2282950",
    "end": "2288829"
  },
  {
    "text": "program to run faster on ice Lake simply because they've changed the internals of the CPU um so there are of course",
    "start": "2288829",
    "end": "2296150"
  },
  {
    "text": "different options that you can come out from there you have to deal further to find out where the processing is happening if it's front-end bound then",
    "start": "2296150",
    "end": "2301970"
  },
  {
    "text": "you need to look into the memory and be able to pull things out if it's back in bound maybe there's something you can do with your algorithm I'm not really",
    "start": "2301970",
    "end": "2308059"
  },
  {
    "text": "proposing to jump in and fix base 64 live on stage but my guess is that it's doing a bunch of multiplies and those",
    "start": "2308059",
    "end": "2313490"
  },
  {
    "text": "multipliers are being executed on a particular port and if we found another way of doing it then perhaps it would be",
    "start": "2313490",
    "end": "2319099"
  },
  {
    "text": "slightly faster if we did it that way one of those ways is vectorization and I'll mention that a little bit later but",
    "start": "2319099",
    "end": "2326569"
  },
  {
    "text": "one of the other things that can affect performance in your program is actually the code layout itself so if you've got",
    "start": "2326569",
    "end": "2331910"
  },
  {
    "start": "2330000",
    "end": "2330000"
  },
  {
    "text": "a program and you've got some sort of if logic you know if an error has occurred if there's a nullpointerexception or",
    "start": "2331910",
    "end": "2337549"
  },
  {
    "text": "whatever there's two different ways of laying down that code in the instructions in memory you can either have an error test that then jumps to",
    "start": "2337549",
    "end": "2344599"
  },
  {
    "text": "you know the good case afterwards and otherwise effectively just follow through into the error case on which you",
    "start": "2344599",
    "end": "2350000"
  },
  {
    "text": "jumped something else or you can do it the other way around where the normal case follows through and the bad case jumps down as far as the branch",
    "start": "2350000",
    "end": "2356210"
  },
  {
    "text": "conductor is concerned as long as you don't error out it's always going to guess the right bunch and it's always going to do the right thing however if",
    "start": "2356210",
    "end": "2363230"
  },
  {
    "text": "you've got cache lines involved in this and you've loaded the instructions in one particular cache line then it's",
    "start": "2363230",
    "end": "2369049"
  },
  {
    "text": "going to be or several cache lines it's going to be good if we've already loaded the code that we want to be able to run so if you can punch your error code so",
    "start": "2369049",
    "end": "2376099"
  },
  {
    "text": "that it lives somewhere else instead of running inside you'll get better performance just because the memory",
    "start": "2376099",
    "end": "2381559"
  },
  {
    "text": "subsystem is going to have less work to do one way of doing this is to use a built in expect function and this is",
    "start": "2381559",
    "end": "2387920"
  },
  {
    "text": "used inside the Linux kernel if they've got macros of likely and unlikely to be able to use built-in expect but if",
    "start": "2387920",
    "end": "2393990"
  },
  {
    "text": "you say ----builtin expect this error condition one in other words we expect that's the default case of doing it the",
    "start": "2393990",
    "end": "2399180"
  },
  {
    "text": "code will be laid out like that back in the old days this used to emitter an instruction to tell the branch process",
    "start": "2399180",
    "end": "2404310"
  },
  {
    "text": "of which way to go that hasn't been used for well since my hair wasn't like that",
    "start": "2404310",
    "end": "2410220"
  },
  {
    "text": "if you wanted to specify the good case you can then specify a built in expect no and it will come and lay it out like",
    "start": "2410220",
    "end": "2416280"
  },
  {
    "text": "that instead now if you're using profile guided optimization this is the kind of thing the profile will learn for you and",
    "start": "2416280",
    "end": "2422730"
  },
  {
    "text": "if you've got a CI pipeline that's doing performance tests gathering profile information and then applying that you",
    "start": "2422730",
    "end": "2428460"
  },
  {
    "text": "know this is already job done it's doing things in this particular case but if you aren't this can be something that",
    "start": "2428460",
    "end": "2433830"
  },
  {
    "text": "will enable your good path code or hot path code to run slightly faster the",
    "start": "2433830",
    "end": "2440040"
  },
  {
    "start": "2440000",
    "end": "2440000"
  },
  {
    "text": "other thing and I mentioned it a little bit before was something called the loop stream decoder and in fact about an hour ago I saw a tweet going past of someone",
    "start": "2440040",
    "end": "2446730"
  },
  {
    "text": "saying why does this one on a particular alignment and there's a thread on reddit that they pointed to Intel has got",
    "start": "2446730",
    "end": "2453180"
  },
  {
    "text": "something called the loop stream detector so when you get to the end of a loop it'll jump back to the top and then go around the next loop and that's",
    "start": "2453180",
    "end": "2458820"
  },
  {
    "text": "basically all it all looks like in code the only difference between these two loops is that it just so happens that",
    "start": "2458820",
    "end": "2465360"
  },
  {
    "text": "there are in different places in memory and that's all okay if you've got a loop and it starts on a 32 byte boundary then",
    "start": "2465360",
    "end": "2472380"
  },
  {
    "text": "the Intel Extreme decoder will serve the instructions from the Europe cache that we talked about earlier and the your",
    "start": "2472380",
    "end": "2477960"
  },
  {
    "text": "cache can hold something like 1.5 thousand new ops along those lines so as long as your loop isn't too big if",
    "start": "2477960",
    "end": "2484140"
  },
  {
    "text": "you're serving things straight from the Europe cache then you will be executing that loop faster without having to do",
    "start": "2484140",
    "end": "2489420"
  },
  {
    "text": "any particular work yourself and if it's slightly allotted or less aligned then you'll go through the ordinary process",
    "start": "2489420",
    "end": "2496560"
  },
  {
    "text": "which is to read the instructions convert them into you ops dispatch those europes and so on and it will be a",
    "start": "2496560",
    "end": "2501690"
  },
  {
    "text": "little bit slower but over large amounts of data or large numbers of times of processing it it can make a difference",
    "start": "2501690",
    "end": "2507740"
  },
  {
    "text": "there is a flag in LLVM that you can specify to say okay for the targets of",
    "start": "2507740",
    "end": "2514020"
  },
  {
    "text": "where we are jumping to align all of them to 32 byte boundaries because hey you're then going to not have this",
    "start": "2514020",
    "end": "2519600"
  },
  {
    "text": "problem and the align all function says whenever you're creating a function make the function start on earth",
    "start": "2519600",
    "end": "2525040"
  },
  {
    "text": "to boundary the align all no fall through blocks is just a very complicated way of saying all of the",
    "start": "2525040",
    "end": "2531730"
  },
  {
    "text": "branch directions to the start of the loop need to be on a 32 byte boundary there was another option which is align",
    "start": "2531730",
    "end": "2538090"
  },
  {
    "text": "all blocks and that says no throw everything on the 32 byte round it in that's overkill because it's only the",
    "start": "2538090",
    "end": "2543130"
  },
  {
    "text": "ones where you're not worried about it when it's only the ones where you need to jump to the start of the loop where",
    "start": "2543130",
    "end": "2549100"
  },
  {
    "text": "they need to be 32 byte aligned I'm this is probably not going to help you it may",
    "start": "2549100",
    "end": "2554260"
  },
  {
    "text": "be able to help you and it's useful to know about but it's almost certainly not something that's going to give you for free I believe that the JIT will",
    "start": "2554260",
    "end": "2560440"
  },
  {
    "text": "automatically align functions on the 32 byte boundary as well for this particular reason now the important",
    "start": "2560440",
    "end": "2567190"
  },
  {
    "text": "thing about joining this is a code layout perspective is because we compiling your code can give different",
    "start": "2567190",
    "end": "2572260"
  },
  {
    "text": "performance profiles there was a blog poked by Dennis Battle of I pronounce",
    "start": "2572260",
    "end": "2577300"
  },
  {
    "text": "your name wrong I apologize when he was talking about having a function and just by adding another function in the code",
    "start": "2577300",
    "end": "2582940"
  },
  {
    "text": "completely uncalled but just the presence of that function affected the alignment and then saw a drop in",
    "start": "2582940",
    "end": "2588490"
  },
  {
    "text": "performance and so when you're doing performance measurement you need to be aware that just recompiling your code",
    "start": "2588490",
    "end": "2593890"
  },
  {
    "text": "can shuffle the code layout which can give different performance profiles to do with things like this and in that",
    "start": "2593890",
    "end": "2599560"
  },
  {
    "text": "particular case these functions very useful because they're not releasing new consistent performance as far as the leap screen decoder is concerned",
    "start": "2599560",
    "end": "2606930"
  },
  {
    "text": "Facebook have written a tool called bolt which I think stands for the binary optimization layout transformer what it",
    "start": "2606930",
    "end": "2612550"
  },
  {
    "start": "2608000",
    "end": "2608000"
  },
  {
    "text": "will do is it will load in your program code parse it essentially reconstitute",
    "start": "2612550",
    "end": "2618040"
  },
  {
    "text": "the basic box that it started with and then defrag them anyone remember defragging disks that's",
    "start": "2618040",
    "end": "2624370"
  },
  {
    "text": "maybe a last millennium thing but basically what it will do is it will run your code figure out where the hotspots",
    "start": "2624370",
    "end": "2630700"
  },
  {
    "text": "are so the same kind of thing that a profile guided optimizer will tell you as well but figure out where those",
    "start": "2630700",
    "end": "2636400"
  },
  {
    "text": "hotspots are and then reorder all of the blocks so that all of the hotspots live in a smaller amount of memory this",
    "start": "2636400",
    "end": "2642910"
  },
  {
    "text": "doesn't change the instructions themselves they're not running any faster is just saying all of the hot code where before maybe it sat in say 10",
    "start": "2642910",
    "end": "2651460"
  },
  {
    "text": "pages of memory now fits into one or two pages of memory and therefore you get much better utilization of the cache",
    "start": "2651460",
    "end": "2657339"
  },
  {
    "text": "and the pages that are bought in art there's an active paper I'd you pronounced aleksef never figured that",
    "start": "2657339",
    "end": "2663700"
  },
  {
    "text": "out that there's a paper that they published in as a repository on the bottom of there which you can follow from the slides another thing that I",
    "start": "2663700",
    "end": "2671440"
  },
  {
    "start": "2670000",
    "end": "2670000"
  },
  {
    "text": "mentioned is the parser Sindhi parser for processing for vectorization and",
    "start": "2671440",
    "end": "2678190"
  },
  {
    "text": "this is something that Daniel Lamar has written and a few other people as well being able to do a vectorized processing",
    "start": "2678190",
    "end": "2686499"
  },
  {
    "text": "of Cindy instructions in other words vectorized instructions for parsing JSON files so typically when you pass Jason",
    "start": "2686499",
    "end": "2693640"
  },
  {
    "text": "it ends up being in a switch loop is this character an open base if it is then go down this particular path",
    "start": "2693640",
    "end": "2699670"
  },
  {
    "text": "because we're doing an object if it's Prout we're doing a string and so on it turns out that if instead of weeding of",
    "start": "2699670",
    "end": "2705460"
  },
  {
    "text": "things byte at a time you read 64 bytes at a time then you can get much better throughput for being",
    "start": "2705460",
    "end": "2711069"
  },
  {
    "text": "able to parse your JSON object and generating the object as a result and so he and a few others worked on this",
    "start": "2711069",
    "end": "2717729"
  },
  {
    "text": "approach for generating instructions generating at the JSON parser and",
    "start": "2717729",
    "end": "2723160"
  },
  {
    "text": "compared it with the kind of known existing best ones in the open source and they saw a speed-up of around 2 to 3",
    "start": "2723160",
    "end": "2729609"
  },
  {
    "text": "times depending on which benchmark it was that they were using there's a couple of reasons why it's faster one is",
    "start": "2729609",
    "end": "2734979"
  },
  {
    "text": "that instead of processing one character at a time we're processing four characters at a time and the second",
    "start": "2734979",
    "end": "2740319"
  },
  {
    "text": "thing is that they've got a they've got a mechanism for avoiding branches and so",
    "start": "2740319",
    "end": "2747190"
  },
  {
    "text": "one of the things that vector operations will give you is a way of doing masking",
    "start": "2747190",
    "end": "2753759"
  },
  {
    "text": "operations and then being able to combine things together because as we talked about previously the branch",
    "start": "2753759",
    "end": "2759219"
  },
  {
    "text": "predictors job guessing whether you went down a branch or not is made significantly easier when there isn't",
    "start": "2759219",
    "end": "2765279"
  },
  {
    "text": "too much to go down and so this particular case was branch free and therefore it was a lot faster forgoing",
    "start": "2765279",
    "end": "2772539"
  },
  {
    "text": "processing so we've talked about a lot of different ways that we can do",
    "start": "2772539",
    "end": "2777940"
  },
  {
    "start": "2776000",
    "end": "2776000"
  },
  {
    "text": "analysis of programs and some of the techniques and some of the ideas that you can use to get programs faster if",
    "start": "2777940",
    "end": "2784210"
  },
  {
    "text": "you're dealing with memory you want to try and use cache aligns memory or cache aware data structures",
    "start": "2784210",
    "end": "2789700"
  },
  {
    "text": "that things are based on 64 byte chunks because that's realistic me what's happening under the covers or page size",
    "start": "2789700",
    "end": "2796570"
  },
  {
    "text": "chunks data structures like bee trees which have historically been used a lot in databases and on disk formats are",
    "start": "2796570",
    "end": "2803260"
  },
  {
    "text": "something which work relatively well for the way memory is laid out and so you can consider that to compress data to",
    "start": "2803260",
    "end": "2810730"
  },
  {
    "text": "make sure that you can compress and decompress data on the fly inside the processor because the cost of schlepping",
    "start": "2810730",
    "end": "2816190"
  },
  {
    "text": "the memory around is usually where the performance problems lie and if you can",
    "start": "2816190",
    "end": "2821410"
  },
  {
    "text": "avoid random memory access random memory access is the thing that will kill memory lookup performance at one time",
    "start": "2821410",
    "end": "2828990"
  },
  {
    "text": "have a look at whether huge pages can help certainly you can get 10 or 15%",
    "start": "2828990",
    "end": "2834220"
  },
  {
    "text": "speed-up on some types of applications simply by enabling huge pages databases",
    "start": "2834220",
    "end": "2839590"
  },
  {
    "text": "tend to be very sensitive to this and you should follow those instructions to say whether or not this is a good thing or a bad thing",
    "start": "2839590",
    "end": "2845830"
  },
  {
    "text": "and can configure how you source your memory by looking at the Lib Numa and",
    "start": "2845830",
    "end": "2852360"
  },
  {
    "text": "the other controls that allow you to specify where your process is about because the more local you can get your",
    "start": "2852360",
    "end": "2858550"
  },
  {
    "text": "data to the program the better it's going to be as far as the CPU is concerned each CPU is its own network",
    "start": "2858550",
    "end": "2866320"
  },
  {
    "start": "2863000",
    "end": "2863000"
  },
  {
    "text": "data center so in the same way that you've been thinking about distributed processing systems being able to move",
    "start": "2866320",
    "end": "2871600"
  },
  {
    "text": "data and summarize them across multiple Network machines think of that as what's",
    "start": "2871600",
    "end": "2877030"
  },
  {
    "text": "happening inside your server as well if you can get it so that your data is closer then it will be processed much",
    "start": "2877030",
    "end": "2885160"
  },
  {
    "text": "faster than if the data has to come from somewhere else and in particular the branch speculation and memory cache",
    "start": "2885160",
    "end": "2892000"
  },
  {
    "text": "misses are pretty costly earlier on there was something someone who mentioned that when you're thinking of",
    "start": "2892000",
    "end": "2899350"
  },
  {
    "text": "complexity of programs don't think of it as the number of instructions that you want but how long you can run without a",
    "start": "2899350",
    "end": "2904990"
  },
  {
    "text": "cache miss and really count complexity of algorithms in the number of cache misses that you hit look at branch v-loc",
    "start": "2904990",
    "end": "2911890"
  },
  {
    "text": "three algorithms we didn't really talk about lock three because when you have locks you generally don't have something that CPU intensive because it's waiting",
    "start": "2911890",
    "end": "2918700"
  },
  {
    "text": "on something else but if you can use block lock free and branch free programs then you're",
    "start": "2918700",
    "end": "2925369"
  },
  {
    "text": "going to get better performance out of it because you're not going to be contending and the branch predictors not going to get in your way and then use",
    "start": "2925369",
    "end": "2931670"
  },
  {
    "text": "perf counters with the help of perf sorry performance counters I should say with the help of perf or top Lev to be",
    "start": "2931670",
    "end": "2938540"
  },
  {
    "text": "able to indicate exactly where your program slowing down is it the pulling in memory is that the cache misses that",
    "start": "2938540",
    "end": "2944119"
  },
  {
    "text": "we're seeing is it the fact that we're waiting for this particular dispatch report and use vectorization where you",
    "start": "2944119",
    "end": "2950930"
  },
  {
    "text": "can most compilers will give you auto vectorization for free with things like loop unrolling to be able to give you",
    "start": "2950930",
    "end": "2958310"
  },
  {
    "text": "the best possible performance but it may be the case that dropping out into your own vector virus code or vectorized",
    "start": "2958310",
    "end": "2964880"
  },
  {
    "text": "assembly makes sense for particular types of operations and things like the JVM will use vector code for doing",
    "start": "2964880",
    "end": "2973359"
  },
  {
    "text": "object array copy for example when it knows it can just copy whole Suede's of data with vector instructions instead of",
    "start": "2973359",
    "end": "2980089"
  },
  {
    "text": "as a byte by byte loop now I've compiled all of the references that I used into the presentation on this slide so if you",
    "start": "2980089",
    "end": "2988040"
  },
  {
    "start": "2984000",
    "end": "2984000"
  },
  {
    "text": "don't want to download the slide and be able to use it you can take a photo of this and you'll be able to find it from",
    "start": "2988040",
    "end": "2993650"
  },
  {
    "text": "there and I've also created a section of links to other people's blogs and other items",
    "start": "2993650",
    "end": "3001150"
  },
  {
    "text": "that you might be able to use as well including some of the ones that I've mentioned within this presentation and",
    "start": "3001150",
    "end": "3006910"
  },
  {
    "text": "so those are good things for following up and with that I will say thank you my",
    "start": "3006910",
    "end": "3012010"
  },
  {
    "text": "links to presentations are down here at the bottom that's my blog on the top and my Twitter feed where I shall post a link to the",
    "start": "3012010",
    "end": "3019450"
  },
  {
    "text": "slide if you're impatient you can go to speaker deck now I've already published it and I'll send out the link as soon as",
    "start": "3019450",
    "end": "3024940"
  },
  {
    "text": "I finish speaking and there's some other links to things like my github repo and other narrated videos that I have done",
    "start": "3024940",
    "end": "3032230"
  },
  {
    "text": "in the past and with that thank you very much you",
    "start": "3032230",
    "end": "3038270"
  }
]