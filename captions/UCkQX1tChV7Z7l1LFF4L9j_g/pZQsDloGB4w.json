[
  {
    "text": "so this talk is on um RDDS data frames and data sets in Apache Spark Uh it's",
    "start": "80",
    "end": "7520"
  },
  {
    "text": "not going to be as uh codeheavy as rune I don't think I used the word monad once",
    "start": "7520",
    "end": "13440"
  },
  {
    "text": "Um but hopefully it'll be enjoyable Anyway the general idea here is that uh RDDs are what most people are used to",
    "start": "13440",
    "end": "19199"
  },
  {
    "text": "using up until recently when everybody was saying no no you want to you want to use data frames Um and then now people",
    "start": "19199",
    "end": "25840"
  },
  {
    "text": "are saying well you know maybe there's this new thing called data sets that we'd like you to migrate to instead And",
    "start": "25840",
    "end": "31039"
  },
  {
    "text": "what we're going to do is compare and contrast the three of them a little bit to give you an idea of what data sets",
    "start": "31039",
    "end": "36079"
  },
  {
    "text": "are going to start looking like um moving forward So the endgame here is a little bit of background on RDDs a",
    "start": "36079",
    "end": "41600"
  },
  {
    "text": "little bit of background on data frames For those of you with Spark knowledge that's going to be a little boring right",
    "start": "41600",
    "end": "46640"
  },
  {
    "text": "but for those of you without Spark knowledge you'll get an idea what the APIs look like And then we're finally going to see data sets and that's sort",
    "start": "46640",
    "end": "52399"
  },
  {
    "text": "of the way forward So that's sort of the intent behind this Um I will tell you upfront I am not a Spark committer Um I",
    "start": "52399",
    "end": "59039"
  },
  {
    "text": "do a lot of Spark training but I'm not a Spark committer So if you ask me under the covers how is that implemented i'm very likely just to say UTSL because",
    "start": "59039",
    "end": "66000"
  },
  {
    "text": "that's exactly what I'm going to have to do if if uh if I try to figure out you know is this using this particular",
    "start": "66000",
    "end": "71600"
  },
  {
    "text": "capability under the covers All the code is out there There's a significant amount of it Um for those who are not",
    "start": "71600",
    "end": "76799"
  },
  {
    "text": "familiar with Spark the Spark um the Spark codebase is largely written in Scola Um so it's a and all of the Spark",
    "start": "76799",
    "end": "84400"
  },
  {
    "text": "committers particularly the one the guy behind data sets is actually absolutely a fan of Scala When I told him I run the",
    "start": "84400",
    "end": "91680"
  },
  {
    "text": "Philly Scala meetup he he looked like he was about to start applauding So you know I think he's had to fight to get",
    "start": "91680",
    "end": "97680"
  },
  {
    "text": "people to use Scola So he's very happy about that All right So the three things that we're going to be discussing here are um",
    "start": "97680",
    "end": "105119"
  },
  {
    "text": "are RDDs which is just short for resilient distributed data set um data frames and",
    "start": "105119",
    "end": "112399"
  },
  {
    "text": "uh data sets which kind of has an unfortunate name I think um but we'll get to that Um so what's an RDD um",
    "start": "112399",
    "end": "120320"
  },
  {
    "text": "basically an RDD is the is the building block of Spark It's what you see under the covers It's what it was the original",
    "start": "120320",
    "end": "126320"
  },
  {
    "text": "API that Spark exposed and pretty much all the higher level APIs decomposed down to RDDs under the covers whether",
    "start": "126320",
    "end": "132720"
  },
  {
    "text": "they expose them to you or not There are still plenty of people out there who who um who code to the RDD API U they have a",
    "start": "132720",
    "end": "140160"
  },
  {
    "text": "couple of characteristics They are compile time type safe They're lazy Okay so they in that",
    "start": "140160",
    "end": "146640"
  },
  {
    "text": "sense they kind of work like streams So when you do things like map and filter over an RDD you get back another RDD but",
    "start": "146640",
    "end": "152800"
  },
  {
    "text": "you're not actually processing data yet right the data doesn't get processed until you call something that says \"Hey I want something out of you.\" At which",
    "start": "152800",
    "end": "159440"
  },
  {
    "text": "point the whole thing gets triggered So they're lazy in that sense Um they're kind of based on the scholar collections",
    "start": "159440",
    "end": "164560"
  },
  {
    "text": "API If you look at them if you're familiar with the scholar collections API you'll look at them and say \"Oh I",
    "start": "164560",
    "end": "169920"
  },
  {
    "text": "recognize half these functions because they make sense.\" They were deliberately built around the model of the uh of the",
    "start": "169920",
    "end": "175280"
  },
  {
    "text": "scholar collections API to the point where it can actually be somewhat confusing If you look in the RDD API",
    "start": "175280",
    "end": "181360"
  },
  {
    "text": "this is still the case There are two collect functions in classes we've done Sometimes people get confused by this",
    "start": "181360",
    "end": "187040"
  },
  {
    "text": "The first collect function looks exactly like the collect function in the scholar collect API The one that we commonly use",
    "start": "187040",
    "end": "193040"
  },
  {
    "text": "to sort of combine map and filter right the second one is the RDD uh collect",
    "start": "193040",
    "end": "198640"
  },
  {
    "text": "action which drags data back uh into your application from the nodes Uh and",
    "start": "198640",
    "end": "203680"
  },
  {
    "text": "so people look at this and they say why are there two collects i don't understand And the reason is it's based on the uh the collections",
    "start": "203680",
    "end": "210440"
  },
  {
    "text": "API So here's an example of of some code Um it's kind of made up but not that",
    "start": "210440",
    "end": "215680"
  },
  {
    "text": "much made up Um that that file the demo I'm going to be giving actually we'll be",
    "start": "215680",
    "end": "220879"
  },
  {
    "text": "looking at a file very similar to this Um the idea is that there's a text file out there We're going to read it in Um",
    "start": "220879",
    "end": "226560"
  },
  {
    "text": "then we're going to uh parse it Okay So it's got some tokens in it So we're going to use flat map so that I can",
    "start": "226560",
    "end": "232400"
  },
  {
    "text": "discard um note that I'm not complaining but I'm going to discard the ones that don't have four fields in them Right so",
    "start": "232400",
    "end": "238319"
  },
  {
    "text": "I only want to process the ones that have four fields in them And then at the bottom bottom I'm going to extract a key",
    "start": "238319",
    "end": "243840"
  },
  {
    "text": "and a value Right um filter out those where the key doesn't match en that in",
    "start": "243840",
    "end": "249519"
  },
  {
    "text": "this particular case is the uh is the code for the the English Wikipedia Uh",
    "start": "249519",
    "end": "254959"
  },
  {
    "text": "and again the demo I'm going to talk about this in in more detail We're going to do a reduce by key In other words I'm",
    "start": "254959",
    "end": "260079"
  },
  {
    "text": "going to combine and add up all of the values Uh then I'm going to take the top 100 of them and loop over them and print",
    "start": "260079",
    "end": "266400"
  },
  {
    "text": "them Okay pretty straightforward if you look at it but you kind of have to look to a little bit to figure out what's",
    "start": "266400",
    "end": "272080"
  },
  {
    "text": "going on What are we trying to accomplish here what we're trying to accomplish is to count up the number of",
    "start": "272080",
    "end": "277199"
  },
  {
    "text": "hits um on the project right and in a distributed fashion Okay So one of the",
    "start": "277199",
    "end": "283280"
  },
  {
    "text": "problems with the RDD library that people complain about is this is a little bit opaque It's a little bit",
    "start": "283280",
    "end": "289280"
  },
  {
    "text": "difficult to see what you're trying to accomplish because you're looking primarily at the how right they're type safe which is the",
    "start": "289280",
    "end": "297040"
  },
  {
    "text": "positive thing They're compile time type safe RDDs have types They can be an RDD of an array of string They can be an RDD",
    "start": "297040",
    "end": "303600"
  },
  {
    "text": "of a case class Um they're compile time type safe This is a real win But the downside is as I said they express the",
    "start": "303600",
    "end": "311039"
  },
  {
    "text": "how more than the what which means it can be hard to look at something and say all of this gibberish is computing an",
    "start": "311039",
    "end": "317600"
  },
  {
    "text": "average right it's a little difficult to read it and understand what's going on Um they cannot be optimized by Spark",
    "start": "317600",
    "end": "324400"
  },
  {
    "text": "They are entirely lambda driven Spark can't look inside the lambdas They're opaque So it can't do any optimization",
    "start": "324400",
    "end": "330479"
  },
  {
    "text": "on your behalf I'll show you an example of what I mean by that Uh they are slow on nonJVM languages Now in here we",
    "start": "330479",
    "end": "336960"
  },
  {
    "text": "probably don't care right um but but an awful lot of people are using uh Spark with Python for instance And we're",
    "start": "336960",
    "end": "343759"
  },
  {
    "text": "talking you know significant slowdowns if you try to use RDDs from Python for reasons I'm not going to bore you guys",
    "start": "343759",
    "end": "349840"
  },
  {
    "text": "with uh in here since we don't care We get to use the JVM right so it's it's",
    "start": "349840",
    "end": "355520"
  },
  {
    "text": "too easy to build an inefficient set of transformations So here's an example of an inefficient transformation right",
    "start": "355520",
    "end": "361440"
  },
  {
    "text": "we're doing the reduce before we do the filter So we're reducing the whole data set counting the whole data set and then",
    "start": "361440",
    "end": "368000"
  },
  {
    "text": "filtering some parts of it out Okay that is not the right way to build this That is very inefficient You want to do the",
    "start": "368000",
    "end": "373360"
  },
  {
    "text": "filter first So you're you're doing less work As it turns out for reasons um having to do with the way Spark works",
    "start": "373360",
    "end": "379759"
  },
  {
    "text": "filter is more efficient than reduce by key Anyway there's no network traffic involved in a filter There is network",
    "start": "379759",
    "end": "386160"
  },
  {
    "text": "traffic involved in a keybased reduction like that So you want to do the filter before you do something more expensive",
    "start": "386160",
    "end": "392240"
  },
  {
    "text": "like a reduce Here's somebody has coded this I have coded this intentionally badly right but Spark can't help you",
    "start": "392240",
    "end": "398800"
  },
  {
    "text": "here It does exactly and only what you tell it So that's a problem because it's very easy to write something that",
    "start": "398800",
    "end": "404560"
  },
  {
    "text": "doesn't perform very well So this is what led to the dataf frames API Uh the dataf frames API was",
    "start": "404560",
    "end": "411280"
  },
  {
    "text": "intended to provide a higher level API something that is a little bit closer to expressing what you're trying to",
    "start": "411280",
    "end": "417280"
  },
  {
    "text": "accomplish rather than how you're trying to do it Uh in a way that can also be optimized better by Spark right so it",
    "start": "417280",
    "end": "423840"
  },
  {
    "text": "gives you a query language If you look at the data frames API it's really a DSL in in both Python and in uh Scola it's a",
    "start": "423840",
    "end": "431800"
  },
  {
    "text": "DSL and it builds a query plan under the covers Um interestingly enough that same",
    "start": "431800",
    "end": "437680"
  },
  {
    "text": "query plan can be built with a SQL statement They work hand inand so that you can say I'd like to build my query",
    "start": "437680",
    "end": "443599"
  },
  {
    "text": "using this this uh DSL this type safe DSL type safeish Um I'd like to build it",
    "start": "443599",
    "end": "450080"
  },
  {
    "text": "with SQL And the net result is the same thing All right I'll show you what I mean by that in a minute So this is the",
    "start": "450080",
    "end": "457360"
  },
  {
    "text": "exact same piece of code or exact same work that we're trying to accomplish in",
    "start": "457360",
    "end": "463120"
  },
  {
    "text": "the dataf frame API So again what we're doing is we're we're loading up our RDD and converting it into a dataf frame",
    "start": "463120",
    "end": "470160"
  },
  {
    "text": "Okay we're giving the data frame column names The uh the characteristic of dataf frames in Spark is that they have",
    "start": "470160",
    "end": "476160"
  },
  {
    "text": "schemas and a schema in Spark basically means it is something that could we can view as a column and columns have names",
    "start": "476160",
    "end": "483280"
  },
  {
    "text": "and types and that's basically it Whether it's really a column or store is kind of irrelevant You can for instance",
    "start": "483280",
    "end": "489599"
  },
  {
    "text": "read a data frame from a JSON file Well that's certainly not a column or store but if you can pretend that it has",
    "start": "489599",
    "end": "496479"
  },
  {
    "text": "columns with names and types then it can be represented in a dataf frame one way or another So that's what we're doing here We're converting our uh RDD into",
    "start": "496479",
    "end": "503840"
  },
  {
    "text": "what conceptually looks like a table with a project uh column and a num",
    "start": "503840",
    "end": "509039"
  },
  {
    "text": "requests column Then we're doing a group by a sum an aggregation of a sum uh a limit on",
    "start": "509039",
    "end": "516719"
  },
  {
    "text": "that and then we're pulling show basically pulls a hundred of the elements back So show is the action Show",
    "start": "516719",
    "end": "522399"
  },
  {
    "text": "kicks off the execution pulls a hundred records back and displays it in a table that looks like it was ripped right out",
    "start": "522399",
    "end": "527920"
  },
  {
    "text": "of the MySQL um ripple Okay Um I can show you an example of that as well And",
    "start": "527920",
    "end": "533200"
  },
  {
    "text": "again you can do the same thing in SQL This does the exact same",
    "start": "533200",
    "end": "538440"
  },
  {
    "text": "thing And I will contend that this is easier to read than the RDD version",
    "start": "538440",
    "end": "544080"
  },
  {
    "text": "right i mean you look at this if you have any idea what SQL does this makes sense um this is not that much harder to",
    "start": "544080",
    "end": "551120"
  },
  {
    "text": "read So the argument behind using one argument behind using dataf frames is that you're closer to what you're trying",
    "start": "551120",
    "end": "557440"
  },
  {
    "text": "to do rather than how you're trying to do it Kind of the same way as if you're talking to an RDBMS engine Um unless",
    "start": "557440",
    "end": "563680"
  },
  {
    "text": "you're a DBA with loads of query hinting you know foo What you typically do is you give the SQL statement and you're",
    "start": "563680",
    "end": "569760"
  },
  {
    "text": "not worried about how the RDBMS goes in and figures out whether to do a table scan or which indexes to use All you",
    "start": "569760",
    "end": "575680"
  },
  {
    "text": "want is the result and you express what you want and you let the RDB message engine under the covers figure out the most efficient way to do that right and",
    "start": "575680",
    "end": "582560"
  },
  {
    "text": "that's the general idea behind the data frames API Let spark figure out how to do it for you It turns out they are in",
    "start": "582560",
    "end": "589200"
  },
  {
    "text": "fact optimized I stole this uh diagram from from data bricks but the general idea here is that whether you start out",
    "start": "589200",
    "end": "595600"
  },
  {
    "text": "with a SQL as a a parsed or a dataf frame what ends up happening is that you're building an",
    "start": "595600",
    "end": "601600"
  },
  {
    "text": "unresolved query under the covers Right you're at the end of the day when you're ready to execute an action you've built",
    "start": "601600",
    "end": "607440"
  },
  {
    "text": "up this query plan Well these guys have enough of a database background that",
    "start": "607440",
    "end": "612800"
  },
  {
    "text": "they optimize their query plans the way an RDBMS does What'll happen is that they use this catalog down here which",
    "start": "612800",
    "end": "619120"
  },
  {
    "text": "basically is just a list of the available userdefined functions Okay so I put that in there so we can ignore",
    "start": "619120",
    "end": "625200"
  },
  {
    "text": "that Now everybody asks what the catalog is um it takes that logical unresolved logical plan and it figures out the",
    "start": "625200",
    "end": "631600"
  },
  {
    "text": "actual logical plan that it wants to use I'm going to show you a diagram about what I mean by that But it does some",
    "start": "631600",
    "end": "636959"
  },
  {
    "text": "optimizations to produce a logical plan and then some further optimizations to produce an optimized logical plan at",
    "start": "636959",
    "end": "643920"
  },
  {
    "text": "which point it starts to construct physical plans How do I want to execute this how do I want to express this",
    "start": "643920",
    "end": "650000"
  },
  {
    "text": "abstract notion in terms of the RDDs that everything ends up being built on under the covers okay so having done",
    "start": "650000",
    "end": "656880"
  },
  {
    "text": "that then it can run a thick cost model and choose the most uh efficient physical plan which then dictates how to",
    "start": "656880",
    "end": "664320"
  },
  {
    "text": "build out the RDD chain to implement this query Right so what we get out of using data frames is this sort of",
    "start": "664320",
    "end": "670800"
  },
  {
    "text": "optimization that goes on under the covers and in classes that we give I always sort of describe it like this You",
    "start": "670800",
    "end": "676000"
  },
  {
    "text": "know I'm obviously old enough to have plenty of gray hair And I remember doing C back in the 80s when they told you",
    "start": "676000",
    "end": "681360"
  },
  {
    "text": "things like well you know if it's not fast enough you could use a pound ASM keyword and throw some assembler in",
    "start": "681360",
    "end": "686720"
  },
  {
    "text": "there which nobody really liked to do but you could right but then there came this inflection point where the",
    "start": "686720",
    "end": "692240"
  },
  {
    "text": "optimizers of the compilers got so good that nobody thought that that was a good idea anymore right why do I want to waste my time making my code on portable",
    "start": "692240",
    "end": "698959"
  },
  {
    "text": "writing assembler when the optimizer is so good that I'd have to work really hard at it and to me that's sort of what",
    "start": "698959",
    "end": "704720"
  },
  {
    "text": "the catalyst optimizer is It's going to keep getting better And so therefore if I don't have to spend time tuning my",
    "start": "704720",
    "end": "709839"
  },
  {
    "text": "RDDs I can focus on the business problem right so that's that's the general idea Um so here are some of the uh this is an",
    "start": "709839",
    "end": "716399"
  },
  {
    "text": "example optimization We use this one a lot So we're doing a join here Um but there's something wrong with",
    "start": "716399",
    "end": "723760"
  },
  {
    "text": "this join Anybody see what's wrong with this join",
    "start": "723760",
    "end": "728519"
  },
  {
    "text": "um well it's not it's not what you're joining it's the fact that you're doing the join before you're doing the filter If you look carefully at the filter the",
    "start": "732480",
    "end": "739040"
  },
  {
    "text": "filter is against the events I want to do the filtering first because that reduces the size of the join and a join",
    "start": "739040",
    "end": "746160"
  },
  {
    "text": "in a distributed computing environment like Spark can can mean network traffic",
    "start": "746160",
    "end": "751360"
  },
  {
    "text": "right so the less data that I have to join the more efficient my join is going to be So if this were RDDDs that would",
    "start": "751360",
    "end": "757120"
  },
  {
    "text": "be bad right that would be inefficient So what um so what happens under the covers is that there's the logical plan",
    "start": "757120",
    "end": "763680"
  },
  {
    "text": "In other words this is essentially your query plan This is what you typed I could execute this under the covers if I",
    "start": "763680",
    "end": "769360"
  },
  {
    "text": "wanted But instead Spark uses some heristics and says \"Hey you know what that's inefficient That join is not",
    "start": "769360",
    "end": "774800"
  },
  {
    "text": "efficient The filter should come first.\" It automatically reorders things and pushes the filter down below the join We",
    "start": "774800",
    "end": "781839"
  },
  {
    "text": "read these bottom up Okay so the the data that we're reading is at the bottom in this in this diagram right so it it",
    "start": "781839",
    "end": "788240"
  },
  {
    "text": "decides that this this needs to be moved down automatically Another thing that it",
    "start": "788240",
    "end": "793279"
  },
  {
    "text": "will do that I'm not showing here is if you put multiple filters in a row it c it based on certain heristics it can",
    "start": "793279",
    "end": "799279"
  },
  {
    "text": "coales them into one filter operation And then it it may also do something",
    "start": "799279",
    "end": "804320"
  },
  {
    "text": "like this Okay it might notice that the events uh uh data uh data source is",
    "start": "804320",
    "end": "811920"
  },
  {
    "text": "intelligent In other words in this particular case maybe it's a Postgress table at which point I can shove the",
    "start": "811920",
    "end": "818000"
  },
  {
    "text": "filter all the way down into the database It just becomes a wear clause right so I never even manifest the data",
    "start": "818000",
    "end": "823680"
  },
  {
    "text": "in memory The database takes care of doing the filtering for me Other examples of that kind of intelligent filtering are uh parquet files right a",
    "start": "823680",
    "end": "831519"
  },
  {
    "text": "parquet file is a a very efficient column-based storage Makes columner access extremely extremely uh efficient",
    "start": "831519",
    "end": "839199"
  },
  {
    "text": "So what Spark will do with a parquet file is if you have a select if you have a parquet file that has say 300 columns",
    "start": "839199",
    "end": "845920"
  },
  {
    "text": "but it determines that at the end of the day you're only using three of those columns It will push effectively push",
    "start": "845920",
    "end": "851279"
  },
  {
    "text": "the selected down into the parquet and say I don't need to manifest all 300 columns just to throw away 297 of them",
    "start": "851279",
    "end": "858399"
  },
  {
    "text": "I'm only going to pull those three columns out of the data source So it has some built-in intelligence that it that",
    "start": "858399",
    "end": "863600"
  },
  {
    "text": "it can make decisions like this And my contention is that um this is not the",
    "start": "863600",
    "end": "868639"
  },
  {
    "text": "kind of work you really want to be doing yourself right so if you work at the RDD level this is kind of the pain It's a",
    "start": "868639",
    "end": "874959"
  },
  {
    "text": "pain to get right It's a lot of work Why not let the optimizer do this for you and then you stay closer to the problem",
    "start": "874959",
    "end": "880320"
  },
  {
    "text": "you're trying to solve and you don't have to worry about the implementation detail The bits become sort of",
    "start": "880320",
    "end": "885560"
  },
  {
    "text": "irrelevant They are faster Another diagram I stole from data bricks Um so you'll note that um that Python as I",
    "start": "885560",
    "end": "893440"
  },
  {
    "text": "said is much much slower um than Scola The reason is that the lambdas are really written in Python in RDDDs and",
    "start": "893440",
    "end": "900800"
  },
  {
    "text": "they're CP Python It's not Jython because you can't use Python packages like numa numpy and scikitlearn if you",
    "start": "900800",
    "end": "908079"
  },
  {
    "text": "do that So that that spark actually has to shove that stuff over to a running Python VM which does the work and then",
    "start": "908079",
    "end": "914000"
  },
  {
    "text": "shoves it back So every lambda that you use in RDD land in Python does that Okay",
    "start": "914000",
    "end": "919279"
  },
  {
    "text": "And it's really slow But but you'll note that data frames are even faster in general Scola to Scola from RDDs and",
    "start": "919279",
    "end": "926800"
  },
  {
    "text": "that's the optimization going on in general It's able to make more intelligent decisions and speed things",
    "start": "926800",
    "end": "932240"
  },
  {
    "text": "up as well as level out the performance across languages This is part of the pitch right this is the pitch to use",
    "start": "932240",
    "end": "937440"
  },
  {
    "text": "dataf frames Um but here's the problem We've lost type safety Okay so it this",
    "start": "937440",
    "end": "944079"
  },
  {
    "text": "is really horrible for a lot of people You'll note that the type if I do a collect on the data frame which says to",
    "start": "944079",
    "end": "949519"
  },
  {
    "text": "spark head I've done all the work I want to do on this thing run the job and pull the data back that's left I want to I",
    "start": "949519",
    "end": "954800"
  },
  {
    "text": "want to look at it in my application note that the type comes back as an array of something called row and rows",
    "start": "954800",
    "end": "961600"
  },
  {
    "text": "are not type discriminated each column has its own type but that that's not exposed to you as part of the API so to",
    "start": "961600",
    "end": "968399"
  },
  {
    "text": "you it's an any I mean you can't see it as anything else other than an any so you've lost compile time type type",
    "start": "968399",
    "end": "974000"
  },
  {
    "text": "safety you have asserted that look my project column is a string My page title",
    "start": "974000",
    "end": "979120"
  },
  {
    "text": "column is a string My num request column is a long but you don't see any of that in the code All right and and you can",
    "start": "979120",
    "end": "985600"
  },
  {
    "text": "see an example of that further down here When you do the collect if you need to map this back to real types to play with",
    "start": "985600",
    "end": "991839"
  },
  {
    "text": "it now you're doing stuff like as instance of which none of us likes to see right that's pretty horrible Um and",
    "start": "991839",
    "end": "998800"
  },
  {
    "text": "extremely errorrone Like what if you get it wrong right well you've run this long job that takes 30 minutes to to chew",
    "start": "998800",
    "end": "1004880"
  },
  {
    "text": "through a terabyte of data and then hits the collect and blows up with a cast a class cast exception right so who wants",
    "start": "1004880",
    "end": "1011759"
  },
  {
    "text": "that so this is part of the reason that that they've moved more toward this thing called data sets Now that they",
    "start": "1011759",
    "end": "1018079"
  },
  {
    "text": "have the infrastructure in place to do the optimizations isn't there a way maybe that we could drop something in",
    "start": "1018079",
    "end": "1023920"
  },
  {
    "text": "place that gets back some of the type safety without losing all of those optimizations because even now even",
    "start": "1023920",
    "end": "1030480"
  },
  {
    "text": "before data sets what you can do is do a lot of work in dataf frames and then drop down to RDDs to do the rest of your",
    "start": "1030480",
    "end": "1036000"
  },
  {
    "text": "work You can always convert a data frame back down to an RDD to go back to type safety but you immediately lose the",
    "start": "1036000",
    "end": "1042880"
  },
  {
    "text": "optimization that's done by the catalyst optimizer You're back on your own right so isn't there some sort of middle",
    "start": "1042880",
    "end": "1048960"
  },
  {
    "text": "ground and that's sort of where uh the impetus my understanding the impetus of data sets came from right because we",
    "start": "1048960",
    "end": "1055039"
  },
  {
    "text": "want to get back our compile time type type safety but we'd also like",
    "start": "1055039",
    "end": "1060320"
  },
  {
    "text": "uh not to lose all the optimizations that this optimizer this catalyst optimizer can provide us under the",
    "start": "1060320",
    "end": "1065559"
  },
  {
    "text": "covers So this is what data sets are attempting to at least start providing So what are they first of all they're an",
    "start": "1065559",
    "end": "1071600"
  },
  {
    "text": "extension to the dataf frames API which means that they operate on data through the SQL context Okay when you when you",
    "start": "1071600",
    "end": "1078480"
  },
  {
    "text": "create one of these things you create it through the SQL context the same way you create a data frame",
    "start": "1078480",
    "end": "1084400"
  },
  {
    "text": "All right The second thing that they do is that they are conceptually similar to RDDs at some level In other words you",
    "start": "1084400",
    "end": "1090480"
  },
  {
    "text": "get back you get back your lambdas and types Okay With data sets you are now",
    "start": "1090480",
    "end": "1096000"
  },
  {
    "text": "operating on domain objects that are uh Scala or if you prefer Java types uh",
    "start": "1096000",
    "end": "1102799"
  },
  {
    "text": "that are manifest in the compiler and you get some compile time protection back Now there's this thing that they've",
    "start": "1102799",
    "end": "1108960"
  },
  {
    "text": "been introducing into Spark called Tungsten One of the things that they're trying to do is make memory management",
    "start": "1108960",
    "end": "1114160"
  },
  {
    "text": "in Spark much more efficient So instead of storing stuff for instance on the JVM heap which immediately runs into all",
    "start": "1114160",
    "end": "1120960"
  },
  {
    "text": "kinds kinds of garbage collection problems they're starting to move memory management of Spark data off heap into",
    "start": "1120960",
    "end": "1126480"
  },
  {
    "text": "this into this unsafe area And they're doing it with this thing called Tungsten Tungsten provides not only off-he memory",
    "start": "1126480",
    "end": "1133280"
  },
  {
    "text": "but it has this compact columner based storage that allows them to operate",
    "start": "1133280",
    "end": "1138400"
  },
  {
    "text": "directly on this stuff very efficiently in memory So tungsten's fast in-memory",
    "start": "1138400",
    "end": "1144720"
  },
  {
    "text": "encoding is used in data sets So instead of RDDs where everything is stored as",
    "start": "1144720",
    "end": "1150400"
  },
  {
    "text": "Java objects this thing actually uses tungsten Now how does it do that it has to know something about your data in",
    "start": "1150400",
    "end": "1156960"
  },
  {
    "text": "order to do that And it turns out that what it does um is it uses these um these encoders",
    "start": "1156960",
    "end": "1164000"
  },
  {
    "text": "We'll get to that in a second Okay But it basically uses these this special",
    "start": "1164000",
    "end": "1169440"
  },
  {
    "text": "kind of code generation to take your domain objects your case classes or whatever and generate code on the fly",
    "start": "1169440",
    "end": "1176080"
  },
  {
    "text": "literally under the covers bite code that can um that can access these things",
    "start": "1176080",
    "end": "1181120"
  },
  {
    "text": "um and map them from domain objects into this tungsten store and back again by",
    "start": "1181120",
    "end": "1186240"
  },
  {
    "text": "analyzing what it is that you're actually working with and generating code on the fly for",
    "start": "1186240",
    "end": "1191880"
  },
  {
    "text": "this right and it also provides easy interoperability with the dataf frame API which is not something RDDs provide",
    "start": "1191880",
    "end": "1199520"
  },
  {
    "text": "Um so what do all these bullets really mean um these bullets really mean that um that you get back some of what you",
    "start": "1199520",
    "end": "1206320"
  },
  {
    "text": "had with RDDs but you're operating in the dataf frame world Okay you're not dropping entirely back down into pure",
    "start": "1206320",
    "end": "1212400"
  },
  {
    "text": "RDD level This stuff all boils down to RDDs under the covers but that's all being managed for you You're operating",
    "start": "1212400",
    "end": "1219200"
  },
  {
    "text": "at a higher level you're still able to think of things in terms of your data frames your columns and and their types",
    "start": "1219200",
    "end": "1225280"
  },
  {
    "text": "but you can switch back into something that has stronger compile time types Now this is an experimental API at the",
    "start": "1225280",
    "end": "1232720"
  },
  {
    "text": "moment It showed up in uh Spark 16 It's becoming their development focus for the",
    "start": "1232720",
    "end": "1238400"
  },
  {
    "text": "next um the next several Spark versions They're starting to do things like take Spark streaming and move it toward a",
    "start": "1238400",
    "end": "1243760"
  },
  {
    "text": "data set uh or data frame model as well Spark streaming currently is their streaming model and it produces RDDDs",
    "start": "1243760",
    "end": "1250480"
  },
  {
    "text": "over time So it takes a stream of continuous data and chunks it up into RDDs as it comes in They're moving that",
    "start": "1250480",
    "end": "1256559"
  },
  {
    "text": "up a level to give it access to data frames and data sets as well So this is becoming the forward focus for",
    "start": "1256559",
    "end": "1264240"
  },
  {
    "text": "Spark Okay So like like an RDD a data set will have a type So this is an example of creating a",
    "start": "1264600",
    "end": "1271600"
  },
  {
    "text": "data set from a JSON file Presumably this JSON file has at least two fields",
    "start": "1271600",
    "end": "1277039"
  },
  {
    "text": "name and age one of which is a string one of which is a long Okay Spark can actually do some analysis of that file",
    "start": "1277039",
    "end": "1284080"
  },
  {
    "text": "and determine you know do some sampling and figure out what the common fields are in in the records and then generate",
    "start": "1284080",
    "end": "1289919"
  },
  {
    "text": "a schema on the fly That's called schema inference And it happens at the dataf frame level already You can do that But",
    "start": "1289919",
    "end": "1296240"
  },
  {
    "text": "in this case what what I've done is I've said read this JSON file into a dataf frame And then I' I'd like to view that",
    "start": "1296240",
    "end": "1302640"
  },
  {
    "text": "data frame as a person object And when I do that when I call that as I get back a",
    "start": "1302640",
    "end": "1307840"
  },
  {
    "text": "data set So it's basically mapped the data frame columns in this case um name",
    "start": "1307840",
    "end": "1313360"
  },
  {
    "text": "and age into my case class So one obvious question is how does it do that",
    "start": "1313360",
    "end": "1318919"
  },
  {
    "text": "it it does that by name All right So the names kind of have to match At this point I have a data set",
    "start": "1318919",
    "end": "1326480"
  },
  {
    "text": "that is based off of a data frame and I can start doing um some fun stuff with it And they've also changed the notion",
    "start": "1326480",
    "end": "1332640"
  },
  {
    "text": "of a data frame A dataf frame is now just a data set of type row In other words it's a data set but it has a type",
    "start": "1332640",
    "end": "1338480"
  },
  {
    "text": "that's not all that useful right so they've they've they've basically taken the whole idea and extended it all the",
    "start": "1338480",
    "end": "1344480"
  },
  {
    "text": "way across the board Everything is now a data set This is an example of RDDs versus",
    "start": "1344480",
    "end": "1350400"
  },
  {
    "text": "data sets You'll note that they look very very similar The first lines are slightly different Um in the first case",
    "start": "1350400",
    "end": "1356400"
  },
  {
    "text": "with the RDD we use the Spark context to read a text file In the second case we",
    "start": "1356400",
    "end": "1362799"
  },
  {
    "text": "use the SQL context to read a text file Um which will bring us back a um a data",
    "start": "1362799",
    "end": "1368960"
  },
  {
    "text": "frame And then we convert that data frame to a data set So we're saying this is a SQL file of type string because I",
    "start": "1368960",
    "end": "1375440"
  },
  {
    "text": "haven't broken it apart yet Right the second line looks exactly the same",
    "start": "1375440",
    "end": "1380480"
  },
  {
    "text": "except that one is RDDs and one is data sets And so the second line is actually going to use the more efficient encoding",
    "start": "1380480",
    "end": "1386559"
  },
  {
    "text": "It's actually going to generate an encoder for our particular type to be able to map this to and from that",
    "start": "1386559",
    "end": "1392440"
  },
  {
    "text": "internal highly efficient tungstenbased column store whereas the RDD one won't",
    "start": "1392440",
    "end": "1397760"
  },
  {
    "text": "It's going to be dealing purely with JVM objects And then final lines are almost the same but the the second line is",
    "start": "1397760",
    "end": "1403760"
  },
  {
    "text": "somewhat shorter And I've called those out specifically here So you can see that they're both",
    "start": "1403760",
    "end": "1409760"
  },
  {
    "text": "doing a group by that looks identical But in the RDD case in order for me to uh count them up I have to do that via",
    "start": "1409760",
    "end": "1416000"
  },
  {
    "text": "my own map Whereas in the second case in the data set case I can use a count",
    "start": "1416000",
    "end": "1422080"
  },
  {
    "text": "aggregator that looks an awful lot like the SQL count star aggregator or the count aggregator uh in the similar data",
    "start": "1422080",
    "end": "1429679"
  },
  {
    "text": "frames API Yes",
    "start": "1429679",
    "end": "1434840"
  },
  {
    "text": "I'm sorry You'll get an error You'll get a runtime",
    "start": "1442080",
    "end": "1448640"
  },
  {
    "text": "error Yeah you'll get a runtime error In fact I would encourage you I'm going to show I'm going to give you the demo that",
    "start": "1448640",
    "end": "1454400"
  },
  {
    "text": "I'm going to do I'm doing a notebook demo but the source will be available I would encourage you to pull the source for that and do exactly that in like the",
    "start": "1454400",
    "end": "1461679"
  },
  {
    "text": "Spark shell and see what happens",
    "start": "1461679",
    "end": "1465720"
  },
  {
    "text": "You Yeah Not it's not perfect right you But I mean think about it this way What you're saying is you're still going to",
    "start": "1469200",
    "end": "1475279"
  },
  {
    "text": "The question was this gives you the same problem You still get a runtime error instead of a compiler time error but",
    "start": "1475279",
    "end": "1480480"
  },
  {
    "text": "you're always going to run into a little bit of that when you're reading an external file right because you can't",
    "start": "1480480",
    "end": "1485840"
  },
  {
    "text": "compile that external data file right so it's giving you it's not perfect but it's giving you more than you get with",
    "start": "1485840",
    "end": "1492320"
  },
  {
    "text": "an RDD Um even with a data even with a well",
    "start": "1492320",
    "end": "1497840"
  },
  {
    "text": "with a data frame what it's going to do is infer the column name from the it",
    "start": "1497840",
    "end": "1503360"
  },
  {
    "text": "will make the column names up right so yes you're absolutely right You have a problem here But with dataf frames let's",
    "start": "1503360",
    "end": "1508960"
  },
  {
    "text": "contrast this So in this case what you have is if you read the file in and it doesn't have a name field it in fact has",
    "start": "1508960",
    "end": "1515279"
  },
  {
    "text": "a first name field right you're going to get a runtime error So contrast that with dataf frames You read the dataf",
    "start": "1515279",
    "end": "1521360"
  },
  {
    "text": "frame in you get a firstname column in the dataf frame but what if your code then says I want the name column It's",
    "start": "1521360",
    "end": "1527600"
  },
  {
    "text": "the same problem right you're still going to get a runtime error So the problem is just moved into a different place Um so it's not really dramatically",
    "start": "1527600",
    "end": "1536679"
  },
  {
    "text": "different Okay Um back here the the only other point I wanted to make here is that the data set code is more visually",
    "start": "1536679",
    "end": "1543960"
  },
  {
    "text": "compact Um I think it's a little easier to read It it again it's it's closer to the what I'm trying to do rather than",
    "start": "1543960",
    "end": "1550720"
  },
  {
    "text": "the how I'm trying to accomplish it right um I I might be heretical and not",
    "start": "1550720",
    "end": "1555840"
  },
  {
    "text": "even comment this but I probably would anyway it Um whereas if I don't comment the RDD code it's very unlikely that my",
    "start": "1555840",
    "end": "1562400"
  },
  {
    "text": "own self 3 months later I'm will understand at a glance what it is I was trying to accomplish right um it will",
    "start": "1562400",
    "end": "1569039"
  },
  {
    "text": "also likely execute faster than its um than its RDD counterpart Okay because of",
    "start": "1569039",
    "end": "1574400"
  },
  {
    "text": "the encoders and because of the tungsten uh format Um here are some of the",
    "start": "1574400",
    "end": "1580640"
  },
  {
    "text": "advantages that data sets are providing Um and there again this is this is ongoing work So they tend to use less",
    "start": "1580640",
    "end": "1586840"
  },
  {
    "text": "memory because Spark builds these encoders does this on the-fly codegen it has a greater understanding of your",
    "start": "1586840",
    "end": "1593360"
  },
  {
    "text": "domain objects than it would within the RDD world where they're just more or less opaque and on the heap right so u",
    "start": "1593360",
    "end": "1599600"
  },
  {
    "text": "because it understands that um it can figure out how to store them more efficiently",
    "start": "1599600",
    "end": "1604799"
  },
  {
    "text": "Um so you know back again we it generates these encoders to to translate your in-memory objects back and forth to",
    "start": "1604799",
    "end": "1611919"
  },
  {
    "text": "this compact in-memory encoding And it can actually work right out of the encoding It doesn't have to",
    "start": "1611919",
    "end": "1618000"
  },
  {
    "text": "serialize to that format and then deserialize it back out into a Java object to operate on it It can actually",
    "start": "1618000",
    "end": "1624240"
  },
  {
    "text": "say okay you want these things of type person but I'm going to store them in this other format this other compact",
    "start": "1624240",
    "end": "1629360"
  },
  {
    "text": "format whenever you need them back as a type person I've got this efficient encoder that I generated to pull this",
    "start": "1629360",
    "end": "1635279"
  },
  {
    "text": "stuff out but I'm going to leave them in that compact format and I'm going to generate these encoders so that I can",
    "start": "1635279",
    "end": "1640320"
  },
  {
    "text": "actually pull the data right out of that compact format and operating on it directly in that format over the chain",
    "start": "1640320",
    "end": "1646720"
  },
  {
    "text": "of operations I'm going to minimize the amount of work I need to do in in the JVM here And I'm going to maximize the",
    "start": "1646720",
    "end": "1652720"
  },
  {
    "text": "amount of data I operate directly in this compact offheap stored",
    "start": "1652720",
    "end": "1657960"
  },
  {
    "text": "format Right right so this way memory is conserved because the format the tungsten format is more compact than the",
    "start": "1657960",
    "end": "1664559"
  },
  {
    "text": "on heap format Turns out it's even more compact than if you serialized it and stored it on heap Um and the speed is",
    "start": "1664559",
    "end": "1671200"
  },
  {
    "text": "improved by the custom code generation right you're not paying the penalty of serializing and deserializing You're",
    "start": "1671200",
    "end": "1676640"
  },
  {
    "text": "operating right out of this compact format So this is an example of the space efficiency at least in some cases",
    "start": "1676640",
    "end": "1683600"
  },
  {
    "text": "that they're observing So if you implement one thing in RDDs and then turn around and implement it in data uh",
    "start": "1683600",
    "end": "1689120"
  },
  {
    "text": "data sets you can see that for this particular data set that they did it's a significant difference Okay I believe",
    "start": "1689120",
    "end": "1695520"
  },
  {
    "text": "that's data size in gigabytes Um same exact kind of job",
    "start": "1695520",
    "end": "1701360"
  },
  {
    "text": "um less than about a quarter of the size consumed in memory So this is um this is",
    "start": "1701360",
    "end": "1706640"
  },
  {
    "text": "really useful for those of us who struggle with running out of memory when we're running these big jobs right",
    "start": "1706640",
    "end": "1712399"
  },
  {
    "text": "um again serialization if you think about it Spark has to serialize data and it has to do it a lot right um it has to",
    "start": "1712399",
    "end": "1719039"
  },
  {
    "text": "send data across the network whenever you get to a situation where um hey I'm grouping things by this key right well",
    "start": "1719039",
    "end": "1725520"
  },
  {
    "text": "that key may be the values for that key might be distributed across your cluster so by definition you're looking at",
    "start": "1725520",
    "end": "1732159"
  },
  {
    "text": "shuffling data across the network to get that stuff aggregated in one place and in order to do that that data has to be",
    "start": "1732159",
    "end": "1737559"
  },
  {
    "text": "serialized right um there are other cases where Spark has to dump data out to disk It turns out it dumps it out to",
    "start": "1737559",
    "end": "1744000"
  },
  {
    "text": "disk when it does those network shuffles It also dumps it out or can dump it out to disk in certain forms of persistence",
    "start": "1744000",
    "end": "1750919"
  },
  {
    "text": "caching Anytime you have to do this it has to serialize it and you're going to pay that serialization and deserialization penalty So the faster",
    "start": "1750919",
    "end": "1758080"
  },
  {
    "text": "that you can make this the better off you are right so um so what typically is",
    "start": "1758080",
    "end": "1763360"
  },
  {
    "text": "done in spark 15 and before is they tell people look you can use the standard Java serialization library for this but",
    "start": "1763360",
    "end": "1769679"
  },
  {
    "text": "we recommend in your production jobs that you use this thing called cryo right because cryo is much faster and",
    "start": "1769679",
    "end": "1775279"
  },
  {
    "text": "much more compact right so um this is faster even than",
    "start": "1775279",
    "end": "1780440"
  },
  {
    "text": "that right the serialized data in this tungsten format is often two times",
    "start": "1780440",
    "end": "1786039"
  },
  {
    "text": "smaller which reduces both your network and disc use right this is really good",
    "start": "1786039",
    "end": "1791279"
  },
  {
    "text": "if you're moving a lot of data across the network Another stolen diagram but this gives you an example of of speed of",
    "start": "1791279",
    "end": "1798559"
  },
  {
    "text": "serialization and deserialization So it's relatively slow with Java You can see why in um5 and",
    "start": "1798559",
    "end": "1806159"
  },
  {
    "text": "prior they're saying use cryo because it's significantly faster But you go to the data sets encoders and I mean we're",
    "start": "1806159",
    "end": "1813360"
  },
  {
    "text": "talking huge difference in speed for serialization and deserialization So again this is the pitch for data sets",
    "start": "1813360",
    "end": "1819840"
  },
  {
    "text": "right you definitely want to be using this as much as you can moving forward because you're going to get this uh benefit as",
    "start": "1819840",
    "end": "1826679"
  },
  {
    "text": "well but there are some limitations Okay one limitation is",
    "start": "1826679",
    "end": "1832159"
  },
  {
    "text": "they're experimental They are marked experimental If you look in the API docs it says experimental Everybody asks",
    "start": "1832159",
    "end": "1838720"
  },
  {
    "text": "what's experimental mean what experimental means is look we think this API is right but we may decide that",
    "start": "1838720",
    "end": "1844880"
  },
  {
    "text": "we're wrong and we may change it This happened with the dataf frames API between I think 13 and 14 They changed",
    "start": "1844880",
    "end": "1850720"
  },
  {
    "text": "everything They changed a few things right so your code breaks or at least you had deprecation warnings So you have",
    "start": "1850720",
    "end": "1856000"
  },
  {
    "text": "to you have to be aware of that going in that this is still experimental It's also not done Um the APIs are complete",
    "start": "1856000",
    "end": "1862559"
  },
  {
    "text": "as I found out when I was hacking my demo up again last night There are some things that are missing that are available elsewhere For instance with a",
    "start": "1862559",
    "end": "1869679"
  },
  {
    "text": "data frame you can do um you can do a an order by to sort the the data With an",
    "start": "1869679",
    "end": "1875840"
  },
  {
    "text": "RDD you can do a sort by to sort the data with um those are missing in the",
    "start": "1875840",
    "end": "1882080"
  },
  {
    "text": "data set API right now as are some of the aggregators So there these general purpose aggregation functions and",
    "start": "1882080",
    "end": "1888000"
  },
  {
    "text": "general purpose aggregation capabilities that you can build on but the common ones that you would hope would be there like oh do sum they're still missing So",
    "start": "1888000",
    "end": "1896799"
  },
  {
    "text": "this this is still a work in progress and that's why they're calling this a preview So you kind of have to use it",
    "start": "1896799",
    "end": "1902000"
  },
  {
    "text": "with a little bit of understanding that you may have to change your jobs down the road when uh Spark 2 comes along Um",
    "start": "1902000",
    "end": "1909279"
  },
  {
    "text": "and then I find this term really hard to to Google Okay Um within the training",
    "start": "1909279",
    "end": "1914720"
  },
  {
    "text": "team we we have been kind of complaining that data sets is too confusing You can't Google it And what do they mean do",
    "start": "1914720",
    "end": "1920799"
  },
  {
    "text": "they mean data we're reading how's this different from a resilient distributed data set but this is the name So I just",
    "start": "1920799",
    "end": "1928320"
  },
  {
    "text": "Google for Apache Spark data sets and you know resign myself to all the extra typing Um so I do have a a code example",
    "start": "1928320",
    "end": "1937120"
  },
  {
    "text": "I'm going to have to try to mirror the display to do that Um I will point out that",
    "start": "1937120",
    "end": "1942919"
  },
  {
    "text": "um that I do have a more resources piece of this slide and the source to this uh",
    "start": "1942919",
    "end": "1948320"
  },
  {
    "text": "slide deck will be out there So I'll tweet you know where you can look at this slide deck in live form sometime",
    "start": "1948320",
    "end": "1954559"
  },
  {
    "text": "later today And then those who want access to the the kind of crusty revealjs source will have access to that",
    "start": "1954559",
    "end": "1960480"
  },
  {
    "text": "as well But in the meantime um I'm going to mirror my display really quickly",
    "start": "1960480",
    "end": "1966640"
  },
  {
    "text": "here All right So where I'm going to go next is to a data bricks notebook environment",
    "start": "1967640",
    "end": "1975039"
  },
  {
    "text": "This is one of the perks of doing some training through them is I have access to this thing Um there are a number of",
    "start": "1975039",
    "end": "1980320"
  },
  {
    "text": "notebook environments out there Some of you probably use the Scola notebook There are Sparkbased notebooks as well Jupiter and Zeppelin are probably two of",
    "start": "1980320",
    "end": "1987760"
  },
  {
    "text": "the most popular Sparkbased ones Uh the data bricks people that's their that's their product So in addition to paying",
    "start": "1987760",
    "end": "1993600"
  },
  {
    "text": "people to maintain um to maintain Spark they have these uh they have this notebook",
    "start": "1993600",
    "end": "2000880"
  },
  {
    "text": "product The resolution is really small here Okay Make sure everybody can read",
    "start": "2001080",
    "end": "2007919"
  },
  {
    "text": "this Okay Is that readable to everybody all right So um this first",
    "start": "2007919",
    "end": "2015320"
  },
  {
    "text": "line I'm attached to a cluster that I spun up a while ago This first line is basically um to make sure that my file",
    "start": "2015320",
    "end": "2022240"
  },
  {
    "text": "is out there Data bicks has this Usually when you do Spark you're reading from some sort of a distributed store like S3",
    "start": "2022240",
    "end": "2027519"
  },
  {
    "text": "or HDFS Um what they've written is their own sort of local file system they call DBFS and it really is supposed to look",
    "start": "2027519",
    "end": "2034559"
  },
  {
    "text": "like HDFS but it's really backed by S3 And they have these uh special escapes that allow me to do things like this to",
    "start": "2034559",
    "end": "2040720"
  },
  {
    "text": "see uh is my file in fact still out",
    "start": "2040720",
    "end": "2045200"
  },
  {
    "text": "there And let's hope that uh I've got till 10",
    "start": "2046519",
    "end": "2053398"
  },
  {
    "text": "right all right So we can see that the file is actually out there What this file is is a um is a file full of edits",
    "start": "2053399",
    "end": "2061760"
  },
  {
    "text": "from um from Wikipedia Um I pulled it down last night You can see in the file time stamp that is from 2200 last night",
    "start": "2061760",
    "end": "2068800"
  },
  {
    "text": "I believe that's UTC right about the time that um uh that the debate was going on So we",
    "start": "2068800",
    "end": "2075839"
  },
  {
    "text": "should see some interesting stuff at Wikipedia during that time So the first step is um how big is",
    "start": "2075839",
    "end": "2082878"
  },
  {
    "text": "this all right So it's about 91 megabytes right it's not huge This is not what I would call big data but if we",
    "start": "2085480",
    "end": "2091919"
  },
  {
    "text": "did big data we'd be standing here forever Okay so we're going to create an RDD Um now the interesting thing about",
    "start": "2091919",
    "end": "2097680"
  },
  {
    "text": "this is that because it's reading a gzipped file um there's only going to be one partition because you can't really",
    "start": "2097680",
    "end": "2103920"
  },
  {
    "text": "split up a gzip file and unpack it It's not built that way You can't say you take half the gzip file you take the",
    "start": "2103920",
    "end": "2109440"
  },
  {
    "text": "other half unpack those two pieces and we'll put them back together So we only get one partition out of this which",
    "start": "2109440",
    "end": "2114480"
  },
  {
    "text": "means that we don't um we don't have very much parallelism So I'm going to beef up the parallelism a little bit Um",
    "start": "2114480",
    "end": "2121040"
  },
  {
    "text": "and then I'm going to see let's make sure see how many records are in",
    "start": "2121040",
    "end": "2126520"
  },
  {
    "text": "there Now at this level we're at the RDD level right i'm doing pure RDD old style",
    "start": "2126520",
    "end": "2132520"
  },
  {
    "text": "Spark Hopefully this thing will",
    "start": "2132520",
    "end": "2136480"
  },
  {
    "text": "return We're getting there Okay so about seven and a half million rows of of edit",
    "start": "2144119",
    "end": "2149200"
  },
  {
    "text": "data in there Um so what do they look like uh they're basically uh files full",
    "start": "2149200",
    "end": "2157520"
  },
  {
    "text": "of four fields right and the first field um indicates the Wikip media product",
    "start": "2157520",
    "end": "2163680"
  },
  {
    "text": "that's being that where the edit has occurred So this is consolidated across not just Wikipedia but all Wikipedias",
    "start": "2163680",
    "end": "2170160"
  },
  {
    "text": "all languages as well as things like Wictionary and Wiki Quote and all of the Wikipedia products So the first the",
    "start": "2170160",
    "end": "2176800"
  },
  {
    "text": "first field will distinguish what it is that we're looking at The second field is the page that's being edited The third field is the number of I'm sorry",
    "start": "2176800",
    "end": "2184000"
  },
  {
    "text": "not edited this is accesses The third field is the number of accesses during",
    "start": "2184000",
    "end": "2189359"
  },
  {
    "text": "this uh hour And the fourth field is how many bytes were actually sent over the network Okay Okay And this is actually",
    "start": "2189359",
    "end": "2196000"
  },
  {
    "text": "the list of of what these things mean So these are the descriptors if you want to go back Um I've got an HTML and a source",
    "start": "2196000",
    "end": "2202240"
  },
  {
    "text": "runnable version of this that are going to be available in the GitHub repository So you can actually read this as HTML",
    "start": "2202240",
    "end": "2207760"
  },
  {
    "text": "and then pull the source into the Spark shell if you have the data file Um so so",
    "start": "2207760",
    "end": "2212960"
  },
  {
    "text": "that we're not um one of the things that Spark does if you run a Spark",
    "start": "2212960",
    "end": "2218440"
  },
  {
    "text": "job and you read the file top to bottom and you get you go all the way down to the bottom and you say okay pull back",
    "start": "2218440",
    "end": "2224320"
  },
  {
    "text": "the data I want and it's finished and it's pulled all this data back to your application the your um the nodes the",
    "start": "2224320",
    "end": "2231040"
  },
  {
    "text": "executors they call them the processes out running on your behalf on the cluster all promptly forget about all",
    "start": "2231040",
    "end": "2236640"
  },
  {
    "text": "the data Okay Okay So then you do if then you do another action against this RDD you want to pull back the count It",
    "start": "2236640",
    "end": "2242640"
  },
  {
    "text": "has to reopen the file in this case reunip process it fire all the lambdas",
    "start": "2242640",
    "end": "2249920"
  },
  {
    "text": "again to pull that second thing back This is why we have caching Caching there are a number of forms of caching",
    "start": "2249920",
    "end": "2255680"
  },
  {
    "text": "But the default is to try to cram the whole thing into memory And so that's what we're going to do here But caching is lazy So when I say",
    "start": "2255680",
    "end": "2262800"
  },
  {
    "text": "cache on an RDD it basically says okay uh whenever you run that next action I'll I'll save the as much of the data",
    "start": "2262800",
    "end": "2268560"
  },
  {
    "text": "as I can Hence my need to run the count Now at this point I'm going to make use of something here to pull up the UI This",
    "start": "2268560",
    "end": "2275520"
  },
  {
    "text": "is just the standard um Spark UI Uh but data bricks notebooks make us make it",
    "start": "2275520",
    "end": "2280960"
  },
  {
    "text": "easy for us to get there So you can see that cached in memory Now we have this thing called page counts RDD2 This is",
    "start": "2280960",
    "end": "2286640"
  },
  {
    "text": "what I just cached and it's storing about a gigabyte of data on the heap across the entire cluster Right so",
    "start": "2286640",
    "end": "2294000"
  },
  {
    "text": "across the cluster when you add up all the data being cached it adds up to about a gig Um save that thought because",
    "start": "2294000",
    "end": "2299760"
  },
  {
    "text": "that's going to be more important as we get further down So this is a variation of um slide five where what I'm trying",
    "start": "2299760",
    "end": "2307119"
  },
  {
    "text": "to do in this case is calculate requests per page So there's my flat map right is",
    "start": "2307119",
    "end": "2312240"
  },
  {
    "text": "my way of uh both parsing the data and tossing out the the bad records You know if it's good data return a sum If it's",
    "start": "2312240",
    "end": "2318720"
  },
  {
    "text": "bad data return a none And then flatmapped unpack it right and then I'm going to filter it I only want to be",
    "start": "2318720",
    "end": "2324400"
  },
  {
    "text": "looking at English Wikipedia So I only want the EN project Um and then once I've got that that I don't need a",
    "start": "2324400",
    "end": "2331280"
  },
  {
    "text": "project anymore so I want to map it down to a a two tupil which is a special kind",
    "start": "2331280",
    "end": "2336320"
  },
  {
    "text": "of RDD as it turns out a pair RDD which allows me to do key value operations At which point I'm going to do a reduce by",
    "start": "2336320",
    "end": "2342720"
  },
  {
    "text": "key to sum up all of the num requests per page Then I want to do a sort by I want them in ascending order because I",
    "start": "2342720",
    "end": "2348880"
  },
  {
    "text": "want the top 100 of them The take is the action that's going to fire the action pull this stuff back And then this final",
    "start": "2348880",
    "end": "2355200"
  },
  {
    "text": "for each is actually just running on um a scholar collection And this will take",
    "start": "2355200",
    "end": "2360800"
  },
  {
    "text": "probably on the order of 30 seconds or so And I think I'm running out of",
    "start": "2360800",
    "end": "2367960"
  },
  {
    "text": "time I'm sorry nine seconds So there you can see this is the list of stuff going on but I've got some weird stuff in here",
    "start": "2367960",
    "end": "2374160"
  },
  {
    "text": "like special search and special blank Turns out there are a whole bunch of special pages Pages where people can talk about pages pages where people can",
    "start": "2374160",
    "end": "2381760"
  },
  {
    "text": "have back channel conversations user pages and I don't want those So I'm going to run another version of this",
    "start": "2381760",
    "end": "2386880"
  },
  {
    "text": "that removes that stuff This should take another nine seconds or",
    "start": "2386880",
    "end": "2393359"
  },
  {
    "text": "so All right And now we can see the topics This one cracks me up in",
    "start": "2395079",
    "end": "2403039"
  },
  {
    "text": "particular Okay so apparently this is what happens when Mitt Romney comes out and says Donald Trump is a jerk",
    "start": "2404200",
    "end": "2409680"
  },
  {
    "text": "Everybody wants to know about RIP now Mitt Romney's dog on on the roof in the carrier on the roof of his car all over",
    "start": "2409680",
    "end": "2415440"
  },
  {
    "text": "again Right Um so you this is I I this is an interesting data set just because I get a laugh out of it",
    "start": "2415440",
    "end": "2423000"
  },
  {
    "text": "Um all right So there uh again this is just emphasizing that I could get this wrong and Spark wouldn't be able to help",
    "start": "2423000",
    "end": "2429520"
  },
  {
    "text": "So um I'm not going to bother going through this This is just a parsing class Here's how I do the same thing with data",
    "start": "2429520",
    "end": "2435480"
  },
  {
    "text": "frames Okay So notice I have three filters here If I were to show you the query plan you would notice that they're",
    "start": "2435480",
    "end": "2441240"
  },
  {
    "text": "consolidated Um and you can do that easily I can take one of these things and do for instance top",
    "start": "2441240",
    "end": "2447720"
  },
  {
    "text": "100df.explain and it will show me the final query plan Uh and I can say explain true and it will show me each",
    "start": "2447720",
    "end": "2453440"
  },
  {
    "text": "stage of optimization which is kind of fun I'm not going to do that here because I'm kind of out of time Um but",
    "start": "2453440",
    "end": "2459480"
  },
  {
    "text": "again not type safe right if I do a take on this you can see that what I get",
    "start": "2459480",
    "end": "2466040"
  },
  {
    "text": "back eventually is a row a row of things And",
    "start": "2466040",
    "end": "2471839"
  },
  {
    "text": "if I want to do if I want to get back to something meaningful to the compiler I've got to use as instance of which I",
    "start": "2471839",
    "end": "2477440"
  },
  {
    "text": "don't want to do right because I could get that wrong So here's the the final bit that I want",
    "start": "2477440",
    "end": "2482480"
  },
  {
    "text": "to go over and that's data sets So now I'm going to read this There are two ways to read this in Here's one I can",
    "start": "2482480",
    "end": "2488240"
  },
  {
    "text": "read the data set directly into a a data frame and then do an as string on it and",
    "start": "2488240",
    "end": "2494800"
  },
  {
    "text": "then do the parsing at the data set level Or I can just take the existing df",
    "start": "2494800",
    "end": "2500400"
  },
  {
    "text": "data frame and do the as on that So here I'm returning a a triplet right and there's my data again Now it's in a a",
    "start": "2500400",
    "end": "2507920"
  },
  {
    "text": "strongly typed string string tupil a string string long tupal 3 right and then I can I can do this as",
    "start": "2507920",
    "end": "2516040"
  },
  {
    "text": "well I think I'd much rather have it in a case class than a tupal 3 for access and readability Um do I still have my 7",
    "start": "2516040",
    "end": "2524880"
  },
  {
    "text": "whatever million rows i do Um if I do a take on it you can see",
    "start": "2524880",
    "end": "2530720"
  },
  {
    "text": "that I get these edit objects back and it came back fairly quickly And then here's my filter Here's here's",
    "start": "2530720",
    "end": "2537680"
  },
  {
    "text": "one operation I'm sorry for the wrapping Um if I get rid of the wrapping you won't be able to read it but you can see",
    "start": "2537680",
    "end": "2542800"
  },
  {
    "text": "that I' I've put my filter in there This is um this is a standard filter lambda Toss out anything that isn't the EN",
    "start": "2542800",
    "end": "2549520"
  },
  {
    "text": "project and and uh contains these weird characters that I'm not interested in Do a group by on",
    "start": "2549520",
    "end": "2555680"
  },
  {
    "text": "the page title What the group by returns is this special thing called a grouped data set that has aggregations on it",
    "start": "2555680",
    "end": "2561760"
  },
  {
    "text": "This is exactly analogous to something called grouped data in the dataf frame API and I mentioned that I imagine that",
    "start": "2561760",
    "end": "2567760"
  },
  {
    "text": "those objects will or those classes will have very similar APIs going forward Um",
    "start": "2567760",
    "end": "2573119"
  },
  {
    "text": "map it so that I extract only the DS entry out of this because that's all I want And now because there's no order by",
    "start": "2573119",
    "end": "2579599"
  },
  {
    "text": "currently no sorting I map it back to the data frame So I can do an order by and then map it back to the data set Not",
    "start": "2579599",
    "end": "2586160"
  },
  {
    "text": "something I'd really like to do but I can do it and it does work uh and then pull the the 100 out And this should",
    "start": "2586160",
    "end": "2592640"
  },
  {
    "text": "give me the same",
    "start": "2592640",
    "end": "2595279"
  },
  {
    "text": "results And it does And finally and I'll do this really fast because I am officially out of time I'm going to",
    "start": "2603319",
    "end": "2609119"
  },
  {
    "text": "cache a couple of these and just show you what they look like in memory So I'm going to take the data set that I was operating on I'm going to cache that",
    "start": "2609119",
    "end": "2616720"
  },
  {
    "text": "Um and then I'm going to create a um an RDD that doesn't contain arrays of",
    "start": "2616720",
    "end": "2621839"
  },
  {
    "text": "strings but instead contains arrays of uh of a case class because that's going to result in a different kind of memory",
    "start": "2621839",
    "end": "2629359"
  },
  {
    "text": "storage right so we're going to do that and cache that Um and now I want to um I",
    "start": "2629359",
    "end": "2634640"
  },
  {
    "text": "I want to cache my data set that also has those edit objects in it And as soon as these three things are done uh I will",
    "start": "2634640",
    "end": "2641440"
  },
  {
    "text": "we'll take a look at how how they're stored in memory I can look at the storage tab in that UI and I can",
    "start": "2641440",
    "end": "2647119"
  },
  {
    "text": "determine uh you know what is the memory efficiency of these guys This is assuming they get",
    "start": "2647119",
    "end": "2653599"
  },
  {
    "text": "done So here we have three of them in memory I think the fourth one is underway now Yep there's the last one So",
    "start": "2655560",
    "end": "2663040"
  },
  {
    "text": "let me refresh this And you can see that the RDD the the the unpared RDD the original",
    "start": "2663040",
    "end": "2670960"
  },
  {
    "text": "one at the bottom takes up about a gig As soon as we add the case class that's the tech the top one page counts parsed",
    "start": "2670960",
    "end": "2677680"
  },
  {
    "text": "RDD So now this is an RDD that doesn't contain just strings It contains these case classes Now the inmemory size has",
    "start": "2677680",
    "end": "2683839"
  },
  {
    "text": "blown up to about one just under 1.4 gig But look at the data sets The data set",
    "start": "2683839",
    "end": "2688960"
  },
  {
    "text": "the first one here um this is the direct string one the second from the bottom that's only 311",
    "start": "2688960",
    "end": "2696720"
  },
  {
    "text": "meg in memory And even when we add the uh the data set containing the edit objects",
    "start": "2696720",
    "end": "2705680"
  },
  {
    "text": "it's still only 323 Why because it's not actually caching the edit objects It's",
    "start": "2705680",
    "end": "2711280"
  },
  {
    "text": "using these fast encoders to translate them into this compact tungsten format And that's what's being stored So this",
    "start": "2711280",
    "end": "2717200"
  },
  {
    "text": "is a fairly dramatic representation of how much memory you can save just by using data sets And that's it We're done",
    "start": "2717200",
    "end": "2724240"
  },
  {
    "text": "So any I probably have time for one question",
    "start": "2724240",
    "end": "2729640"
  },
  {
    "text": "Okay we got one question in the back and then we're going to have to have to bowl Yeah so that's a good question I really",
    "start": "2731040",
    "end": "2736720"
  },
  {
    "text": "don't know the answer The question was so what's going to happen with the machine learning libraries and what",
    "start": "2736720",
    "end": "2741760"
  },
  {
    "text": "predicates that question is the fact that the original MLA was based purely on RDDs and then they said \"Oh no no no",
    "start": "2741760",
    "end": "2747440"
  },
  {
    "text": "no Let's do ML and base that on this pipeline model with their data frames.\" And so the question is will they be",
    "start": "2747440",
    "end": "2753280"
  },
  {
    "text": "folding data sets into the machine learning library um off the top of my head I don't know the answer to that Um",
    "start": "2753280",
    "end": "2759280"
  },
  {
    "text": "but I can get the answer Um luckily because I'm training with these guys I can get back channel info to the",
    "start": "2759280",
    "end": "2764640"
  },
  {
    "text": "development team So I'll ask that and I'll try to tweak the answer to that as soon as I find it All right Um okay one",
    "start": "2764640",
    "end": "2771040"
  },
  {
    "text": "more That's it Then I'm holding somebody else up So so how does it work with different data formats like different",
    "start": "2771040",
    "end": "2777280"
  },
  {
    "text": "file types json right so so it's the same as with the It works the same way as with data frames So with dataf frames",
    "start": "2777280",
    "end": "2784319"
  },
  {
    "text": "what happens is that there are these adapters that allow you to read um certain file types Uh the the classic",
    "start": "2784319",
    "end": "2790000"
  },
  {
    "text": "example that I like to give is built in is support for text files parquet files",
    "start": "2790000",
    "end": "2795520"
  },
  {
    "text": "um and um and JSON files and as well as you know RDBMS's So anything that has a",
    "start": "2795520",
    "end": "2801839"
  },
  {
    "text": "self-describing schema like a parquet file or an RDBMS is trivial to read into a data frame because you can you can",
    "start": "2801839",
    "end": "2808240"
  },
  {
    "text": "figure out the schema because the file actually has a schema For something like JSON they built an adapter and they",
    "start": "2808240",
    "end": "2813680"
  },
  {
    "text": "included it in Spark and by default it samples your JSON file and it says you know what are the fields in there",
    "start": "2813680",
    "end": "2819520"
  },
  {
    "text": "because if you think about JSON you can always determine the field names right and there are a limited number of types",
    "start": "2819520",
    "end": "2825359"
  },
  {
    "text": "that can mostly be inferred from how the literals are expressed And so what it will do is infer a schema out of that",
    "start": "2825359",
    "end": "2830960"
  },
  {
    "text": "And what if you have other ones that you need to be able to do and the answer typically there is either look out on",
    "start": "2830960",
    "end": "2836560"
  },
  {
    "text": "the net There's this place called sparkpackages.org that has a bunch of adapters people wrote So data bicks wrote one for CSV files for instance",
    "start": "2836560",
    "end": "2844400"
  },
  {
    "text": "that will do that or you have to start with an RDD read it in parse the RDD",
    "start": "2844400",
    "end": "2849440"
  },
  {
    "text": "into pieces construct your own schema by a number of different means um case classes being the obvious one and then",
    "start": "2849440",
    "end": "2856720"
  },
  {
    "text": "move on Yeah I think we need to take that offline Um the question is how the as",
    "start": "2856720",
    "end": "2863520"
  },
  {
    "text": "function works and I think uh we could be here for 20 minutes talking about that So why don't we take that one offline um all right So I think um if I",
    "start": "2863520",
    "end": "2870240"
  },
  {
    "text": "read the schedule properly we have another break at this point Um and Brendan McAdams is up next And thanks for your attention I appreciate it",
    "start": "2870240",
    "end": "2878920"
  }
]