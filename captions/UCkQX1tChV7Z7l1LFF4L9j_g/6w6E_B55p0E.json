[
  {
    "text": "thank you for being here imagine it's",
    "start": "4050",
    "end": "10710"
  },
  {
    "text": "Friday night of this week the conference is over you're back at your home sitting",
    "start": "10710",
    "end": "17370"
  },
  {
    "text": "on your favorite couch finally ready to relax and you start Netflix I hope this",
    "start": "17370",
    "end": "25949"
  },
  {
    "text": "is the first screen that you see when you launch Netflix the interesting thing",
    "start": "25949",
    "end": "31830"
  },
  {
    "text": "about this screen is it's not static or universal it's customized to your taste",
    "start": "31830",
    "end": "39260"
  },
  {
    "text": "there are 125 million versions of this tree one for each of our 125 million",
    "start": "39260",
    "end": "47100"
  },
  {
    "text": "customers but this one is my personalized to my taste which I just",
    "start": "47100",
    "end": "54090"
  },
  {
    "text": "realize is varied with crime shows let's not read anything specific into that moving down so please raise your hand if",
    "start": "54090",
    "end": "63000"
  },
  {
    "text": "you start actually watching something anything within say a minute or two after you land in this screen yeah not",
    "start": "63000",
    "end": "73500"
  },
  {
    "text": "me either most of us spend a considerable amount of time browsing",
    "start": "73500",
    "end": "79560"
  },
  {
    "text": "this screen scrolling trend to pitch pick something to watch and that",
    "start": "79560",
    "end": "84799"
  },
  {
    "text": "behavior is actually relevant to our discussion today let's say it's 10 or 20",
    "start": "84799",
    "end": "92850"
  },
  {
    "text": "minutes later and you're still browsing the same screen meanwhile our",
    "start": "92850",
    "end": "98369"
  },
  {
    "text": "personalization algorithms are continuously running in cloud so we could have generated a new better",
    "start": "98369",
    "end": "104240"
  },
  {
    "text": "recommendations for you in those 20 minutes and if that does happen how do",
    "start": "104240",
    "end": "110340"
  },
  {
    "text": "we get that new list in front of you as soon as it's ready how do we tell our application how do we let it know that",
    "start": "110340",
    "end": "118409"
  },
  {
    "text": "there is a better list better recommendations ready for it to download in the cloud push messaging is perfect",
    "start": "118409",
    "end": "126270"
  },
  {
    "text": "solution for situations like this earlier our old application used to poll",
    "start": "126270",
    "end": "131700"
  },
  {
    "text": "our servers periodically for new recommendation which kind of worked",
    "start": "131700",
    "end": "137460"
  },
  {
    "text": "but it's not great it's faithful a it's not great latency wise either what's",
    "start": "137460",
    "end": "146310"
  },
  {
    "text": "worse is the twin goals of UI freshness versus serve efficiency are in direct",
    "start": "146310",
    "end": "153030"
  },
  {
    "text": "contradiction with each other with polling if you increase polling frequency to get the best possible UI",
    "start": "153030",
    "end": "161010"
  },
  {
    "text": "freshness you have an unload your servers and if you decrease it to give you a service a breathing room you're",
    "start": "161010",
    "end": "167520"
  },
  {
    "text": "gonna your UI freshness is gonna suffer now our client our server just sends a",
    "start": "167520",
    "end": "175860"
  },
  {
    "text": "push message to our client as soon as it generates a new list for our client just",
    "start": "175860",
    "end": "181650"
  },
  {
    "text": "as a one stat we reduced our request to our push our website cluster back 12%",
    "start": "181650",
    "end": "187890"
  },
  {
    "text": "when we moved our in browser player from polling to push at 1 million requests",
    "start": "187890",
    "end": "195840"
  },
  {
    "text": "per second those 12% add up really fast so please ignore all push messages on",
    "start": "195840",
    "end": "203160"
  },
  {
    "text": "your smartphones for next 40 minutes because we're going to talk about push messaging push notifications are",
    "start": "203160",
    "end": "209130"
  },
  {
    "text": "terrible for conference speakers like us but background push messages to applications are awesome in specific we",
    "start": "209130",
    "end": "219450"
  },
  {
    "text": "are going to talk about what is push how you can build it how you can operate it",
    "start": "219450",
    "end": "225780"
  },
  {
    "text": "in production and what can you do with it my name is sue she'll or Oskar and",
    "start": "225780",
    "end": "233250"
  },
  {
    "text": "I'm a software engineer at in cloud gateway team at Netflix I have been in",
    "start": "233250",
    "end": "239040"
  },
  {
    "text": "Netflix for 8 years have worked in three different teams in those eight years and somehow it still feels like I'm still",
    "start": "239040",
    "end": "246030"
  },
  {
    "text": "just browsing the list the real show is about to start so let's start with",
    "start": "246030",
    "end": "251900"
  },
  {
    "text": "defining push how is it different than the requests normal request response",
    "start": "251900",
    "end": "258720"
  },
  {
    "text": "paradigm that we all know and love believe it or not this is actually from",
    "start": "258720",
    "end": "265350"
  },
  {
    "text": "a motivational poster from my local gym that's why I don't go there anymore but",
    "start": "265350",
    "end": "271849"
  },
  {
    "text": "it is a surprisingly accurate definition for our purpose today push is really",
    "start": "271849",
    "end": "278819"
  },
  {
    "text": "different in two ways there is a eight there is a persistent connection between a client and a server for the entirety",
    "start": "278819",
    "end": "286409"
  },
  {
    "text": "of the clients lifetime and B there is",
    "start": "286409",
    "end": "291749"
  },
  {
    "text": "it's a server that initiates a data transfer something does really happen on server and then the server pushes the",
    "start": "291749",
    "end": "297930"
  },
  {
    "text": "data to the client instead of client asking for it which would be the normal request response way we built our push",
    "start": "297930",
    "end": "306240"
  },
  {
    "text": "messaging service we named it dual push to send this push messages from our servers to are applicable to our",
    "start": "306240",
    "end": "312629"
  },
  {
    "text": "application dual push messages are very similar to the push messages that you get on your mobile except they work",
    "start": "312629",
    "end": "320099"
  },
  {
    "text": "across a wide variety of devices they work anywhere where Netflix application",
    "start": "320099",
    "end": "325680"
  },
  {
    "text": "runs which means they work on laptops on game consoles on smart TVs and on the",
    "start": "325680",
    "end": "333479"
  },
  {
    "text": "wife to get this cross-platform capability jool push uses standard open",
    "start": "333479",
    "end": "342870"
  },
  {
    "text": "web protocols like WebSockets or server sent events so is our SSE as will push",
    "start": "342870",
    "end": "350669"
  },
  {
    "text": "server itself is open source - and is available on github today so let's get a",
    "start": "350669",
    "end": "358349"
  },
  {
    "text": "little bit into detail about will push architecture Zul push is not a single server or a single service it's a",
    "start": "358349",
    "end": "365339"
  },
  {
    "text": "complete push messaging infrastructure made up of multiple components there are",
    "start": "365339",
    "end": "371370"
  },
  {
    "text": "dual push servers to start with they sit on a network edge and accept incoming",
    "start": "371370",
    "end": "376439"
  },
  {
    "text": "client connections clients connect to these will push servers over the",
    "start": "376439",
    "end": "382560"
  },
  {
    "text": "internet using either web sockets or sse as a protocol and once a client is",
    "start": "382560",
    "end": "388710"
  },
  {
    "text": "connected to a particular she will push server it keeps that connection open for the entirety of its life lifetime so",
    "start": "388710",
    "end": "394919"
  },
  {
    "text": "these are persistent connections that distinction is important now because we have multiple dual push",
    "start": "394919",
    "end": "402509"
  },
  {
    "text": "servers and multiple clients connected to those multiples will push servers we",
    "start": "402509",
    "end": "407520"
  },
  {
    "text": "need to keep track of which client is connected to which push server and that's the job of push registry on the",
    "start": "407520",
    "end": "418560"
  },
  {
    "text": "back end or push message senders which are typically are back in micro services",
    "start": "418560",
    "end": "424639"
  },
  {
    "text": "near a simple robust but high throughput way to send push messages to our clients",
    "start": "424639",
    "end": "432469"
  },
  {
    "text": "but those push message send us don't really know about all the infrastructural details that I'm",
    "start": "432469",
    "end": "437580"
  },
  {
    "text": "explaining to you now what they ideally want is a simple single one-liner call",
    "start": "437580",
    "end": "443009"
  },
  {
    "text": "that lets them push a message to a client given a client ID or Azul push",
    "start": "443009",
    "end": "448919"
  },
  {
    "text": "library provides them that interface by hiding all our infrastructure already",
    "start": "448919",
    "end": "453990"
  },
  {
    "text": "tails behind a single a synchronous send message call behind us in the send",
    "start": "453990",
    "end": "462930"
  },
  {
    "text": "message called takes the push message from the sender and dubs it in the push",
    "start": "462930",
    "end": "468300"
  },
  {
    "text": "message queue by introducing message queues between our senders and our",
    "start": "468300",
    "end": "473699"
  },
  {
    "text": "receivers we effectively decoupled them making it easier for us to run them",
    "start": "473699",
    "end": "480569"
  },
  {
    "text": "independently the also the message queues also let us withstand wide",
    "start": "480569",
    "end": "486930"
  },
  {
    "text": "variations in number of incoming message the act as buffer absorbing big and high",
    "start": "486930",
    "end": "494430"
  },
  {
    "text": "spikes of incoming push messages finally",
    "start": "494430",
    "end": "499889"
  },
  {
    "text": "message processor is the component that ties all these other components together",
    "start": "499889",
    "end": "505110"
  },
  {
    "text": "to do the actual push message delivery it reads messages of the message queue",
    "start": "505110",
    "end": "512250"
  },
  {
    "text": "each push message is addressed to a particular client by client ID or device",
    "start": "512250",
    "end": "519240"
  },
  {
    "text": "ID it then looks up that client in a pusher history to figure out to which",
    "start": "519240",
    "end": "526050"
  },
  {
    "text": "which server that client is connected if it finds a push server",
    "start": "526050",
    "end": "532260"
  },
  {
    "text": "for that client in the push registry it will then directly connect to that push server and hand over that push message",
    "start": "532260",
    "end": "538620"
  },
  {
    "text": "to that server for sending it to that client sorry on the other hand if it",
    "start": "538620",
    "end": "547320"
  },
  {
    "text": "doesn't find a record for that client in the push registry it means the client is not connected at",
    "start": "547320",
    "end": "553530"
  },
  {
    "text": "this time or it's not online in that case it just drops it on the floor now",
    "start": "553530",
    "end": "559890"
  },
  {
    "text": "that we have seen how what are the different components that make up will",
    "start": "559890",
    "end": "566460"
  },
  {
    "text": "push infrastructure and how they operate together we can actually dig a little deeper into some of them this will push",
    "start": "566460",
    "end": "574590"
  },
  {
    "text": "throw is probably the biggest piece of the whole infrastructure or you'll push cluster which comprises of multiples in",
    "start": "574590",
    "end": "581160"
  },
  {
    "text": "June which servers in aggregate handles 10 million persistent always-on",
    "start": "581160",
    "end": "586830"
  },
  {
    "text": "concurrent connection today at peak and it's rapidly growing will push server is",
    "start": "586830",
    "end": "593070"
  },
  {
    "text": "based on dual Cloud gateway and that's why its shares its name Zuul Zul cloud",
    "start": "593070",
    "end": "599640"
  },
  {
    "text": "gateways the API gateway that my team owns and we operate it and it fronts all",
    "start": "599640",
    "end": "605010"
  },
  {
    "text": "the HTTP traffic that comes into Netflix ecosystem at peak jewel push it's",
    "start": "605010",
    "end": "611520"
  },
  {
    "text": "actually PE gateway so it handles more than 1 million requests per second and it was recently rewritten to use",
    "start": "611520",
    "end": "618680"
  },
  {
    "text": "non-blocking a sink IO so it provided a perfect foundation for us on which to build our",
    "start": "618680",
    "end": "624860"
  },
  {
    "text": "massively scalable this will push server but you may ask why do you need",
    "start": "624860",
    "end": "631920"
  },
  {
    "text": "non-blocking or a sink IO in this case you many of you are probably familiar",
    "start": "631920",
    "end": "638100"
  },
  {
    "text": "with C 10k challenge the term C 10 T was first coined in 1999 I believe and the",
    "start": "638100",
    "end": "645000"
  },
  {
    "text": "challenge simply States you have to support 10000 concurrent connections on",
    "start": "645000",
    "end": "650310"
  },
  {
    "text": "a single server we have by the way long since blown past that 10,000 number",
    "start": "650310",
    "end": "656130"
  },
  {
    "text": "initial 10,000 number but the name kind of stuck this capability to support",
    "start": "656130",
    "end": "661740"
  },
  {
    "text": "thousands and thousands of open connections on a single server is fundamental to a server like Zul push",
    "start": "661740",
    "end": "668360"
  },
  {
    "text": "that as an aggregate cluster has to handle millions of always on open current connections which are mostly",
    "start": "668360",
    "end": "675210"
  },
  {
    "text": "idle by the way the traditional way of network programming cannot easily be",
    "start": "675210",
    "end": "681720"
  },
  {
    "text": "scaled to meet the SI 10k challenge the traditional way is to spawn a new thread",
    "start": "681720",
    "end": "687270"
  },
  {
    "text": "per incoming connection and then let that thread do blocking read right on",
    "start": "687270",
    "end": "693120"
  },
  {
    "text": "that connection this doesn't case scale because mainly",
    "start": "693120",
    "end": "698640"
  },
  {
    "text": "because you will quickly exhaust your service memory allocating 10,000 stacks",
    "start": "698640",
    "end": "705000"
  },
  {
    "text": "for those 10,000 threads you'll also most probably pin down your service CPUs by doing constant context",
    "start": "705000",
    "end": "713400"
  },
  {
    "text": "switches between those 10,000 threads so it's not efficient to support large number of open connections a sink i/o",
    "start": "713400",
    "end": "722790"
  },
  {
    "text": "follows a different programming model it uses operating system provided",
    "start": "722790",
    "end": "728029"
  },
  {
    "text": "multiplexing IO primitives like K Q or e pool or IO completion ports on if you",
    "start": "728029",
    "end": "734280"
  },
  {
    "text": "are on Windows to register read and write callbacks for all those open",
    "start": "734280",
    "end": "739860"
  },
  {
    "text": "connections on a single thread from then on when any of that connection is ready",
    "start": "739860",
    "end": "745230"
  },
  {
    "text": "to do a read or write operation its corresponding callback gets invoked on",
    "start": "745230",
    "end": "751260"
  },
  {
    "text": "the same thread so you no longer need as many threads as you have open",
    "start": "751260",
    "end": "756510"
  },
  {
    "text": "connections and it that way it scales much better the trade-off here is your program your",
    "start": "756510",
    "end": "764310"
  },
  {
    "text": "application is somewhat more complicated now because now you as a developer are",
    "start": "764310",
    "end": "769920"
  },
  {
    "text": "responsible for keeping track of all the state of all those connections inside",
    "start": "769920",
    "end": "775320"
  },
  {
    "text": "your code you cannot rely on thread stack to do so because thread stack is shared between all those connections you",
    "start": "775320",
    "end": "781440"
  },
  {
    "text": "typically do that by using some kind of event or a state machine inside your code we use nettie to do this a",
    "start": "781440",
    "end": "791279"
  },
  {
    "text": "synchronous non-blocking i/o net is this great open source library written in",
    "start": "791279",
    "end": "798300"
  },
  {
    "text": "Java and it's used by many many popular open-source Java projects like Cassandra",
    "start": "798300",
    "end": "803790"
  },
  {
    "text": "and Hadoop so it's really very tested and it's battle-proven we're not going",
    "start": "803790",
    "end": "809820"
  },
  {
    "text": "to go into details of nettie programming in this stock it's a subject in itself but this is just to give you an idea",
    "start": "809820",
    "end": "815759"
  },
  {
    "text": "from ten thousand feet how an abstract native program structure looks like the",
    "start": "815759",
    "end": "821579"
  },
  {
    "text": "channel inbound and outbound handlers that you see here are analogous to read and write callbacks that we just",
    "start": "821579",
    "end": "829079"
  },
  {
    "text": "discussed a slide ago so this is our",
    "start": "829079",
    "end": "838610"
  },
  {
    "text": "simplified depiction of how a push nettie pipeline looks like there are a",
    "start": "838610",
    "end": "844500"
  },
  {
    "text": "lot of things going on in here but I really want to draw your attention to just two of the highlighted methods get",
    "start": "844500",
    "end": "851579"
  },
  {
    "text": "push off Handler and get push registration handler you can override these methods to plug",
    "start": "851579",
    "end": "858269"
  },
  {
    "text": "in your own custom authentication and push registration mechanism inside",
    "start": "858269",
    "end": "863670"
  },
  {
    "text": "you'll push rest of the stuff that you see here like HTTP server code ake WebSocket server protocol handler all of",
    "start": "863670",
    "end": "871500"
  },
  {
    "text": "these are standard protocol parsers provided by a native of the shell which",
    "start": "871500",
    "end": "876630"
  },
  {
    "text": "is great because that means nettie is doing most of the heavy lifting here like passing low-level HTTP and",
    "start": "876630",
    "end": "883019"
  },
  {
    "text": "WebSocket protocols each client that connects to a dual push server for the",
    "start": "883019",
    "end": "889649"
  },
  {
    "text": "first time has to authenticate and identify itself before it can start",
    "start": "889649",
    "end": "895139"
  },
  {
    "text": "receiving push messages from server as I said you can plug-in your own custom",
    "start": "895139",
    "end": "901050"
  },
  {
    "text": "authentication and the way to do that is to override the class that we provide push off Handler and override its do",
    "start": "901050",
    "end": "909120"
  },
  {
    "text": "auth method do what method gets the original WebSocket connect request as a",
    "start": "909120",
    "end": "915810"
  },
  {
    "text": "parameter passed into it so you inside that blow up method you have a full access to its request body headers and",
    "start": "915810",
    "end": "922829"
  },
  {
    "text": "cookies which you can then use to implement your custom authentication",
    "start": "922829",
    "end": "929060"
  },
  {
    "text": "moving on push registry is the component as we saw that keeps track or keeps the",
    "start": "930140",
    "end": "936060"
  },
  {
    "text": "mapping of push client and to the server to which that to which server the push",
    "start": "936060",
    "end": "941550"
  },
  {
    "text": "client is connected just like your custom authentication we allow you to",
    "start": "941550",
    "end": "947790"
  },
  {
    "text": "plug in your custom push registration mechanism inside ours will push server the way to do that is again extend out",
    "start": "947790",
    "end": "955649"
  },
  {
    "text": "push registration handler class and implement its or override its register client method the example over here just",
    "start": "955649",
    "end": "963510"
  },
  {
    "text": "shows that mapping inside a Redis store in radius would be the method that you would implement to serialize it that are",
    "start": "963510",
    "end": "970790"
  },
  {
    "text": "mapping into your suffragists tree whichever way you see fit so you can use",
    "start": "970790",
    "end": "976890"
  },
  {
    "text": "any data store of your choice as a push registry but that data store for the",
    "start": "976890",
    "end": "982950"
  },
  {
    "text": "best results should have following characteristics it should have low read",
    "start": "982950",
    "end": "990209"
  },
  {
    "text": "latency and this is important because you only write a record into that push",
    "start": "990209",
    "end": "995610"
  },
  {
    "text": "registry once for every client when it first connects but you look it up or read it multiple times every single time",
    "start": "995610",
    "end": "1002570"
  },
  {
    "text": "someone is trying to send a message for that client so low read latency is important you can somewhat compromised",
    "start": "1002570",
    "end": "1009290"
  },
  {
    "text": "on write late and see if you have to your data store should also support per",
    "start": "1009290",
    "end": "1017060"
  },
  {
    "text": "record record expiry or a TTL of some sort time to live this is necessary",
    "start": "1017060",
    "end": "1022970"
  },
  {
    "text": "because when a client disconnects at the end of its life cycle from push server if it does that cleanly which will",
    "start": "1022970",
    "end": "1029959"
  },
  {
    "text": "happen 99% of the time or higher push server will take care of cleaning up its",
    "start": "1029959",
    "end": "1035449"
  },
  {
    "text": "record from the push registry so it's no longer found but in real life you cannot",
    "start": "1035449",
    "end": "1040670"
  },
  {
    "text": "rely on every single client disconnecting cleanly every single time",
    "start": "1040670",
    "end": "1046990"
  },
  {
    "text": "sometimes service crash sometimes plant crash any of this happening will result",
    "start": "1046990",
    "end": "1053630"
  },
  {
    "text": "in what we called living behind a phantom registration record in your pusher history it's the record that says",
    "start": "1053630",
    "end": "1060080"
  },
  {
    "text": "this client is connected to this particular server but it's no longer accurate because I the server has gone away at this client",
    "start": "1060080",
    "end": "1066620"
  },
  {
    "text": "has gone away Zul push drill relies on TTL to clean such phantom registration",
    "start": "1066620",
    "end": "1074720"
  },
  {
    "text": "records after certain time out besides",
    "start": "1074720",
    "end": "1079760"
  },
  {
    "text": "these two desirable feature then you have a laundry list of usual suspects",
    "start": "1079760",
    "end": "1085190"
  },
  {
    "text": "like sharding for high availability and replication for poor tolerance given",
    "start": "1085190",
    "end": "1094640"
  },
  {
    "text": "these features any of these will be a great choice for your push edges to data store they are probably several more",
    "start": "1094640",
    "end": "1103420"
  },
  {
    "text": "what we use internally is dynamite it's yet another open source project from",
    "start": "1103420",
    "end": "1110380"
  },
  {
    "text": "Netflix it takes readies wraps it and augments it with features like auto",
    "start": "1110380",
    "end": "1118430"
  },
  {
    "text": "sharding cross region replication and readwrite chorim's it's another great",
    "start": "1118430",
    "end": "1125000"
  },
  {
    "text": "choice for your push registry finally message processing is the component that",
    "start": "1125000",
    "end": "1132430"
  },
  {
    "text": "does message queue or message routing queuing and delivery on behalf of our",
    "start": "1132430",
    "end": "1139220"
  },
  {
    "text": "senders we use Kafka for our messaging infrastructure those queues decouple our",
    "start": "1139220",
    "end": "1146870"
  },
  {
    "text": "senders and receivers most of our push senders plus mesh push message senders",
    "start": "1146870",
    "end": "1153620"
  },
  {
    "text": "will take a fire-and-forget approach to message delivery the user push library",
    "start": "1153620",
    "end": "1159410"
  },
  {
    "text": "drop a push message in the queue and carry on with their work few of them might care about the actual end result",
    "start": "1159410",
    "end": "1167090"
  },
  {
    "text": "of status of the push message delivery and those can get to it to the final status either by subscribing to the push",
    "start": "1167090",
    "end": "1174760"
  },
  {
    "text": "delivery status queue or they can rid it of the high table in a batch mode where",
    "start": "1174760",
    "end": "1180890"
  },
  {
    "text": "we log every push message delivery Netflix runs in three different regions",
    "start": "1180890",
    "end": "1189590"
  },
  {
    "text": "of Emerald Amazon Cloud back and push message sender trying to send",
    "start": "1189590",
    "end": "1196039"
  },
  {
    "text": "to message to a client a particular client typically has no idea to which region that client might be connected so",
    "start": "1196039",
    "end": "1203989"
  },
  {
    "text": "our push messaging infrastructure takes care of routing that message to the correct region on behalf of our",
    "start": "1203989",
    "end": "1210019"
  },
  {
    "text": "cylinders at the base level we rely on kafka message cure application to",
    "start": "1210019",
    "end": "1216289"
  },
  {
    "text": "replicate these messages in all the three regions so that we can actually deliver them across the region in",
    "start": "1216289",
    "end": "1225499"
  },
  {
    "text": "practice we have found we can use a single push message queue and to deliver all sorts of push messages and be still",
    "start": "1225499",
    "end": "1233450"
  },
  {
    "text": "below or delivery latency SLA but if you are worried about something we call",
    "start": "1233450",
    "end": "1239599"
  },
  {
    "text": "priority inversion or design allows you to use different message queues for",
    "start": "1239599",
    "end": "1245419"
  },
  {
    "text": "different priority priority inversion happens when your message of a higher",
    "start": "1245419",
    "end": "1250970"
  },
  {
    "text": "priority is made to wait behind bunch of messages of lower priority because you",
    "start": "1250970",
    "end": "1256759"
  },
  {
    "text": "are using a single cue to cue them all up having different message queues for",
    "start": "1256759",
    "end": "1263090"
  },
  {
    "text": "different priorities guarantees priority inversion will never happen we run",
    "start": "1263090",
    "end": "1270049"
  },
  {
    "text": "multiple instances of our message processors to scale up our message processing throughput message processors",
    "start": "1270049",
    "end": "1277909"
  },
  {
    "text": "are based on Montes mantises are internal stream scaleable stream",
    "start": "1277909",
    "end": "1284539"
  },
  {
    "text": "processing engine kind of somewhat similar to apache flink it uses mezzos",
    "start": "1284539",
    "end": "1290779"
  },
  {
    "text": "container management system which makes it easy for us to quickly start have",
    "start": "1290779",
    "end": "1295970"
  },
  {
    "text": "spin up a bunch of message processor instances if you are falling behind in",
    "start": "1295970",
    "end": "1301399"
  },
  {
    "text": "processing our message to backlog critically it comes with out of box",
    "start": "1301399",
    "end": "1307609"
  },
  {
    "text": "support for scaling message processor instance number or number of message processors based on how many messages",
    "start": "1307609",
    "end": "1315499"
  },
  {
    "text": "are waiting in the push message queue this feature alone makes it very easy for us to meet our delivery latency SLA",
    "start": "1315499",
    "end": "1323509"
  },
  {
    "text": "under a wide variety of load while still staying resource efficient",
    "start": "1323509",
    "end": "1329679"
  },
  {
    "text": "so at this point I would like to switch gears a little bit and go over some of",
    "start": "1329779",
    "end": "1335399"
  },
  {
    "text": "the operational lessons or tactics that we learn when we first started operating",
    "start": "1335399",
    "end": "1340470"
  },
  {
    "text": "will push in production zoom push is somewhat different than the normal",
    "start": "1340470",
    "end": "1345809"
  },
  {
    "text": "stateless rest services that we were used to till then so it required a",
    "start": "1345809",
    "end": "1350879"
  },
  {
    "text": "little TLC or tender love and care when the first started operating in production the biggest difference is",
    "start": "1350879",
    "end": "1358639"
  },
  {
    "text": "long glute stable connections they make as will push servers somewhat stateful",
    "start": "1358639",
    "end": "1365869"
  },
  {
    "text": "long low stable connections are great from client point of view right because they they improve clients efficiency",
    "start": "1365869",
    "end": "1374039"
  },
  {
    "text": "dramatically they've no wrong plans no longer have to continuously make and break connections like they would have",
    "start": "1374039",
    "end": "1379889"
  },
  {
    "text": "to in simple I should EB word that's why we all rejoiced when WebSockets were finally widely supported and they",
    "start": "1379889",
    "end": "1386429"
  },
  {
    "text": "replace the hats like comet or long pole but they're quite terrible for somebody",
    "start": "1386429",
    "end": "1392340"
  },
  {
    "text": "who's trying from the point of view of somebody who's trying to operate a server mostly because they make quick",
    "start": "1392340",
    "end": "1399539"
  },
  {
    "text": "deploys and quickly rollbacks problematic they make they complicate it let's give an example let's say you have",
    "start": "1399539",
    "end": "1406590"
  },
  {
    "text": "to push a certain urgent fix right you have to push on it or deploy a new build",
    "start": "1406590",
    "end": "1413789"
  },
  {
    "text": "so you do that now you have a new cluster with a new build in production but all your plants are still happily",
    "start": "1413789",
    "end": "1420570"
  },
  {
    "text": "connected to your old cluster because remember they open that connection only once when they start up and they hang on",
    "start": "1420570",
    "end": "1426299"
  },
  {
    "text": "to that connection for the entirety of their lifetime so they are not automatically migrated to the new cluster just because you deployed it you",
    "start": "1426299",
    "end": "1433529"
  },
  {
    "text": "will have to post fully migrate them back killing the old cluster but then if you do that they are gonna all swamped",
    "start": "1433529",
    "end": "1440639"
  },
  {
    "text": "that your new cluster at the exact same time giving rise to something we called a thunder encoder so this is a lose-lose",
    "start": "1440639",
    "end": "1448409"
  },
  {
    "text": "scenario thundering herd is basically large number of plants all trying to",
    "start": "1448409",
    "end": "1454710"
  },
  {
    "text": "connect to the same service at the same time this gives rise to a big spike in",
    "start": "1454710",
    "end": "1462150"
  },
  {
    "text": "connections or traffic which is orders of magnitude higher than your steadies",
    "start": "1462150",
    "end": "1467340"
  },
  {
    "text": "normal steady-state traffic it's one of the things that you have to watch out for when you're trying to build a",
    "start": "1467340",
    "end": "1473100"
  },
  {
    "text": "resilient and robust system so the way we found our way out of this pickle was",
    "start": "1473100",
    "end": "1479700"
  },
  {
    "text": "to limit a client connection lifetime all our plants are coded in such a way",
    "start": "1479700",
    "end": "1486030"
  },
  {
    "text": "that they know to try and reconnect back whenever they lose a connection to the server we took advantage of that fact",
    "start": "1486030",
    "end": "1493260"
  },
  {
    "text": "and we auto close connections from server side after certain period so when",
    "start": "1493260",
    "end": "1500550"
  },
  {
    "text": "the this client loses the connections is going to try and connect back and when it does so it's going to land on most",
    "start": "1500550",
    "end": "1507480"
  },
  {
    "text": "likely on some different server because of the way load balancers work right so this takes care of single clients",
    "start": "1507480",
    "end": "1515430"
  },
  {
    "text": "stickiness to a single push server which is at the root of all our deployment and",
    "start": "1515430",
    "end": "1520830"
  },
  {
    "text": "rollback news we have tuned this connection life time period carefully to",
    "start": "1520830",
    "end": "1528810"
  },
  {
    "text": "strike a good balance between client efficiency that we desire and plant",
    "start": "1528810",
    "end": "1535890"
  },
  {
    "text": "stickiness that's we are trying to avoid not only we limit a single clients",
    "start": "1535890",
    "end": "1543150"
  },
  {
    "text": "connection lifetime we also randomize it from time to time or from connection to connection and this is important to",
    "start": "1543150",
    "end": "1551270"
  },
  {
    "text": "dampen any thundering code that you may still get in spite of best of your",
    "start": "1551270",
    "end": "1556290"
  },
  {
    "text": "design and intentions as the time progresses I'll give you an example let's say there was some network blip",
    "start": "1556290",
    "end": "1563130"
  },
  {
    "text": "right and many of your clients drop the connections and they are all going to be connected back so you're going to get a",
    "start": "1563130",
    "end": "1568830"
  },
  {
    "text": "thundering code even if you provide accounted for it now the problem is they all connect at around the same time T is",
    "start": "1568830",
    "end": "1576210"
  },
  {
    "text": "equal to zero let's say now if all of them get the exact same connection",
    "start": "1576210",
    "end": "1582000"
  },
  {
    "text": "lifetimes let's say 30 minutes then they all go not disconnect again right at the",
    "start": "1582000",
    "end": "1587100"
  },
  {
    "text": "30 minutes boundary from now and they are all gonna reconnect back at that same boundary and this is will go on on",
    "start": "1587100",
    "end": "1593840"
  },
  {
    "text": "perpetuity so any flip will cause this and now you have a guard the only thing that's worse than a",
    "start": "1593840",
    "end": "1600990"
  },
  {
    "text": "thundering heard a recurring thundering hood but consider now instead of giving",
    "start": "1600990",
    "end": "1607770"
  },
  {
    "text": "them each one of them just 30 minutes you randomize the connection lifetime by plus minus two minutes right so they're",
    "start": "1607770",
    "end": "1614910"
  },
  {
    "text": "gonna get a connection life term of something between 28 to 32 minutes for example when that happens for the next",
    "start": "1614910",
    "end": "1621540"
  },
  {
    "text": "subsequent reconnect attempt they are gonna get a little bit dispersed on the timeline some of them will drop the connection",
    "start": "1621540",
    "end": "1627960"
  },
  {
    "text": "when they connect a 28-minute 29 minute 30 31 and 32 and all the seconds in between so this has the effect of",
    "start": "1627960",
    "end": "1634800"
  },
  {
    "text": "spreading out that initial peak over four minutes and when that happens when",
    "start": "1634800",
    "end": "1640470"
  },
  {
    "text": "they recommend they are again wanna get another randomize period so as the time progresses it will automatically dampen",
    "start": "1640470",
    "end": "1646860"
  },
  {
    "text": "like the curve shows you it's a very simple trick but it's very valuable to",
    "start": "1646860",
    "end": "1652130"
  },
  {
    "text": "correctly tame any thundering hood that you will eventually get one day or the",
    "start": "1652130",
    "end": "1657480"
  },
  {
    "text": "other this is really a nice optimization",
    "start": "1657480",
    "end": "1663210"
  },
  {
    "text": "extra optimization I know just a couple of slides ago I said we ought to close",
    "start": "1663210",
    "end": "1668340"
  },
  {
    "text": "the connection from server side but it's no longer entirely accurate it used to be the case but we flipped it around in",
    "start": "1668340",
    "end": "1675780"
  },
  {
    "text": "our latest version such that now our server sends a message to the client",
    "start": "1675780",
    "end": "1681510"
  },
  {
    "text": "asking the client to close its connection and I know it sounds like a roundabout way of doing the same thing",
    "start": "1681510",
    "end": "1687420"
  },
  {
    "text": "but we do that mainly because how disappea works according to the TCP spec",
    "start": "1687420",
    "end": "1693440"
  },
  {
    "text": "any the party that initiates closing of the connection ends up in that if time",
    "start": "1693440",
    "end": "1700530"
  },
  {
    "text": "wait state time rate state is the last state in the disappear down flow the",
    "start": "1700530",
    "end": "1707580"
  },
  {
    "text": "problem with time word state is on Linux that can consume that connections file",
    "start": "1707580",
    "end": "1715170"
  },
  {
    "text": "descriptor for up to two minutes now our server is the one that is handling",
    "start": "1715170",
    "end": "1720870"
  },
  {
    "text": "thousands and thousands of concurrent connections so our service file descriptors are much more valuable than",
    "start": "1720870",
    "end": "1728070"
  },
  {
    "text": "our client descriptives so by having this roundabout way by having the client",
    "start": "1728070",
    "end": "1733210"
  },
  {
    "text": "close the connection we make sure that service file descriptors are comes out",
    "start": "1733210",
    "end": "1739110"
  },
  {
    "text": "there is a flip side to this optimization though because now you have to be prepared to hand out in",
    "start": "1739110",
    "end": "1745080"
  },
  {
    "text": "misbehaving plants that won't follow service lead and close the connection when they are told to do so to handle",
    "start": "1745080",
    "end": "1752649"
  },
  {
    "text": "such client we start a timer when we send a close connection message",
    "start": "1752649",
    "end": "1757659"
  },
  {
    "text": "downstream and then forcefully close it from the server side if the client doesn't comply within a set time limit",
    "start": "1757659",
    "end": "1766559"
  },
  {
    "text": "so with all these tweaks we more or less took care of the sticky stateful connection problem and next we focused",
    "start": "1766559",
    "end": "1774010"
  },
  {
    "text": "our attention in optimizing our push cluster size on reducing the number of push servers that we need to support our",
    "start": "1774010",
    "end": "1781000"
  },
  {
    "text": "traffic our big epiphany here was most of our connections were idle most of the",
    "start": "1781000",
    "end": "1787539"
  },
  {
    "text": "time which meant even with large number of connections open to a single server",
    "start": "1787539",
    "end": "1792750"
  },
  {
    "text": "the CPU or memory on that server was not under particularly under any particular pressure armed with this knowledge we",
    "start": "1792750",
    "end": "1801630"
  },
  {
    "text": "chose a really big meaty Amazon instance type for our push server we carefully",
    "start": "1801630",
    "end": "1808179"
  },
  {
    "text": "tuned its TCP kernel parameters JVM startup options and things like that and",
    "start": "1808179",
    "end": "1813549"
  },
  {
    "text": "we crammed it with as many connections as we possibly could then just one of",
    "start": "1813549",
    "end": "1820510"
  },
  {
    "text": "those servers crashed on us in production and of course we got again",
    "start": "1820510",
    "end": "1826750"
  },
  {
    "text": "the visit from our dear old friend the thundering God all those thousands and",
    "start": "1826750",
    "end": "1832720"
  },
  {
    "text": "thousands of connections that we crammed so efficiently on the single server the all came roaring right back with",
    "start": "1832720",
    "end": "1838899"
  },
  {
    "text": "reconnects you know you have a problem when a single server going down in",
    "start": "1838899",
    "end": "1844149"
  },
  {
    "text": "production can start a stampede in your system so we looked our wounds we learn",
    "start": "1844149",
    "end": "1850899"
  },
  {
    "text": "from our mistake and for the second round we went with a Goldilocks strategy now we know that you don't want to learn",
    "start": "1850899",
    "end": "1858220"
  },
  {
    "text": "run your service either too hot or too cold so we found the server Amazon instance type that was",
    "start": "1858220",
    "end": "1865929"
  },
  {
    "text": "just right for us which happens to be m4 large in Amazon terminology which is",
    "start": "1865929",
    "end": "1873220"
  },
  {
    "text": "basically a server with eight gigs of ram and two virtual CPUs and with our",
    "start": "1873220",
    "end": "1878920"
  },
  {
    "text": "load testing and switch squeeze testing steps we figured out that on that particular server configuration we can",
    "start": "1878920",
    "end": "1885429"
  },
  {
    "text": "comfortably handle up to eighty four thousand concurrent connections at one",
    "start": "1885429",
    "end": "1890590"
  },
  {
    "text": "time and if that server goes down a couple of servers like that goes down that instan type goes down handling",
    "start": "1890590",
    "end": "1898540"
  },
  {
    "text": "those many connections is we are comfortable with that that's the size of thundering code we can handle given our",
    "start": "1898540",
    "end": "1904330"
  },
  {
    "text": "traffic so real lesson here is you",
    "start": "1904330",
    "end": "1909490"
  },
  {
    "text": "should really optimize for the total cost of your server form operation and not optimized for low server count I",
    "start": "1909490",
    "end": "1917500"
  },
  {
    "text": "know when stated like that it sounds obvious it should have been obvious but it wasn't initially for us mainly",
    "start": "1917500",
    "end": "1924040"
  },
  {
    "text": "because we I think we conflated efficient operation with low number of servers where in reality more number of",
    "start": "1924040",
    "end": "1932500"
  },
  {
    "text": "cheaper servers are actually preferable to fewer number of large servers as long",
    "start": "1932500",
    "end": "1938080"
  },
  {
    "text": "as your cost stays the same next problem",
    "start": "1938080",
    "end": "1943270"
  },
  {
    "text": "we had to address was how to auto scale how to increase and decrease the size of our push firm number of our push servers",
    "start": "1943270",
    "end": "1949870"
  },
  {
    "text": "as our traffic goes and comes down",
    "start": "1949870",
    "end": "1955049"
  },
  {
    "text": "our two main go-to strategies for auto scaling were either requests per second",
    "start": "1957900",
    "end": "1964720"
  },
  {
    "text": "RPS or average CPU load both of them are ineffective for push cluster because",
    "start": "1964720",
    "end": "1970630"
  },
  {
    "text": "there is no RPS as we said because these are long a persistent connection there",
    "start": "1970630",
    "end": "1975760"
  },
  {
    "text": "are no continuous requests and CPU was low as we said even with a large number",
    "start": "1975760",
    "end": "1983200"
  },
  {
    "text": "of open connections so how do you auto scale the only limiting factor for push",
    "start": "1983200",
    "end": "1988960"
  },
  {
    "text": "server is the number of open connections is handling at any instance of time so",
    "start": "1988960",
    "end": "1994630"
  },
  {
    "text": "it makes perfect sense to auto scale push cluster based on average number of",
    "start": "1994630",
    "end": "2000360"
  },
  {
    "text": "open connections thankfully Amazon makes it very easy to scale on any metric you",
    "start": "2000360",
    "end": "2006420"
  },
  {
    "text": "want or Auto scale on any metric you want as long as you can export it as a custom cloud watch matrix and that's",
    "start": "2006420",
    "end": "2012900"
  },
  {
    "text": "what we ended up doing we export number of open connections from a server process and we hook up our auto scaling",
    "start": "2012900",
    "end": "2020280"
  },
  {
    "text": "policies to that metric the last hurdle we had to solve was Amazon Elastic load",
    "start": "2020280",
    "end": "2027810"
  },
  {
    "text": "balancers cannot proxy WebSockets our cluster is in Amazon Cloud of course and",
    "start": "2027810",
    "end": "2036300"
  },
  {
    "text": "they sit behind these Amazon Elastic to load balancers or lbs for short",
    "start": "2036300",
    "end": "2041840"
  },
  {
    "text": "unfortunately elby's do not understand the initial request that WebSocket",
    "start": "2041840",
    "end": "2047940"
  },
  {
    "text": "client makes which is called a WebSocket upgrade request it's a it should be request but it's a special request and",
    "start": "2047940",
    "end": "2054179"
  },
  {
    "text": "they do not understand it so they handle it as any other HTTP request which means",
    "start": "2054180",
    "end": "2059940"
  },
  {
    "text": "as soon as the server returns the response definition is broken which is not what we want we want a persistent",
    "start": "2059940",
    "end": "2064980"
  },
  {
    "text": "connection but you cannot have a persistent WebSocket connection through ELB by the way this is not specific to",
    "start": "2064980",
    "end": "2073710"
  },
  {
    "text": "just Amazon elby's any load balance hardware or software that doesn't",
    "start": "2073710",
    "end": "2078990"
  },
  {
    "text": "understand WebSocket protocol is gonna run into the same issues that includes older versions of H a proxy and nginx as",
    "start": "2078990",
    "end": "2086159"
  },
  {
    "text": "well before they started supporting WebSocket protocol the way we found our way around this",
    "start": "2086160",
    "end": "2092639"
  },
  {
    "text": "problem is to make Amazon real bees run as disappear load balancers by default",
    "start": "2092640",
    "end": "2099450"
  },
  {
    "text": "Amazon yell bees run as HTTP load balancer doing load balancing at layer 7",
    "start": "2099450",
    "end": "2105770"
  },
  {
    "text": "but there is a configuration there's a switch that you can flip that makes them",
    "start": "2105770",
    "end": "2111290"
  },
  {
    "text": "run as a TCP load balancer which means they do a load balancing at layer 4 at",
    "start": "2111290",
    "end": "2117330"
  },
  {
    "text": "that point they just proxy the TCP packets back and forth without trying to parse or interpret any of the layer 7",
    "start": "2117330",
    "end": "2125220"
  },
  {
    "text": "application protocol which would be it should be in our case this keeps them from mangling the WebSocket upgrade",
    "start": "2125220",
    "end": "2132600"
  },
  {
    "text": "request that did not understand at this point I just want to note however that",
    "start": "2132600",
    "end": "2138510"
  },
  {
    "text": "Amazon has come up with a new load balancing offering called LB or application load balancer which is",
    "start": "2138510",
    "end": "2145020"
  },
  {
    "text": "supposed to support WebSockets natively unfortunately it came too late for us by",
    "start": "2145020",
    "end": "2151110"
  },
  {
    "text": "that time we had already figured out all these tweaks to make elby's do what we want but if you are starting out today",
    "start": "2151110",
    "end": "2157170"
  },
  {
    "text": "you may want to give lb a try so let's do a quick recap of how you effectively",
    "start": "2157170",
    "end": "2165150"
  },
  {
    "text": "manage a push cluster in production you want to recycle connections after tens of minutes ideally between 25 to 30",
    "start": "2165150",
    "end": "2171720"
  },
  {
    "text": "minutes that's switch spot that we found you want to randomize each connections",
    "start": "2171720",
    "end": "2177660"
  },
  {
    "text": "lifetime to tame the thundering herd as the time progresses you should prefer",
    "start": "2177660",
    "end": "2184370"
  },
  {
    "text": "smaller number more the number of smaller services servers over fewer but",
    "start": "2184370",
    "end": "2190950"
  },
  {
    "text": "bigger expensive servers that will withstand much better in thundering herd",
    "start": "2190950",
    "end": "2196350"
  },
  {
    "text": "scenario you should Auto scale on number of open sockets rather than CPU or RPS",
    "start": "2196350",
    "end": "2204450"
  },
  {
    "text": "which are more con commonplace and finally if you are going to put your push cross push cluster behind the load",
    "start": "2204450",
    "end": "2211080"
  },
  {
    "text": "balancer either use a WebSocket of where load balancer or run your load balance",
    "start": "2211080",
    "end": "2216120"
  },
  {
    "text": "and the TCP mode any of which would work so let's say if you did all that and now",
    "start": "2216120",
    "end": "2223840"
  },
  {
    "text": "you have a push cluster in production running flawlessly what can you do with it now that we have our push hammer in",
    "start": "2223840",
    "end": "2231430"
  },
  {
    "text": "Netflix we are seeing a lot of nails we plan to use it on for on-demand",
    "start": "2231430",
    "end": "2238360"
  },
  {
    "text": "diagnostic we can detect misbehaving devices or clients in the field which",
    "start": "2238360",
    "end": "2245140"
  },
  {
    "text": "are generating lots of errors from their telemetry and then send a specific special push message to those devices",
    "start": "2245140",
    "end": "2253650"
  },
  {
    "text": "asking them to upload their state and any other relevant diagnostic detail to cloud so that we can triage them we can",
    "start": "2253650",
    "end": "2260470"
  },
  {
    "text": "debug and see why they are causing these errors and if all of that extra debug",
    "start": "2260470",
    "end": "2267790"
  },
  {
    "text": "data still Intel we can always reach to reach out for the most trusted tool in",
    "start": "2267790",
    "end": "2273790"
  },
  {
    "text": "any software developer's toolbox and restart the application we can now do it",
    "start": "2273790",
    "end": "2278980"
  },
  {
    "text": "remotely what could go wrong but if something does go wrong now we can send",
    "start": "2278980",
    "end": "2286090"
  },
  {
    "text": "you a message saying we are sorry because we have push messaging capability so hopefully you have some",
    "start": "2286090",
    "end": "2297220"
  },
  {
    "text": "good ideas where you can use push messages too so I've been talking about",
    "start": "2297220",
    "end": "2303010"
  },
  {
    "text": "more than 40 minutes for around push now pleading the case for push and at this",
    "start": "2303010",
    "end": "2308830"
  },
  {
    "text": "point I have only one single last request for you for me to make to all of",
    "start": "2308830",
    "end": "2314230"
  },
  {
    "text": "you pull",
    "start": "2314230",
    "end": "2318119"
  },
  {
    "text": "all of this we discussed everything we discussed so far is open source you can",
    "start": "2321860",
    "end": "2327390"
  },
  {
    "text": "find it inside a Zul project in our Netflix or SSRI PO on github it even",
    "start": "2327390",
    "end": "2333810"
  },
  {
    "text": "comes with a toy sample will push server that you can fire up immediately and start playing with so give it a spin",
    "start": "2333810",
    "end": "2341160"
  },
  {
    "text": "file bugs and it would be so kind maybe send they're gonna send us a wonderful request or two so in",
    "start": "2341160",
    "end": "2349590"
  },
  {
    "text": "conclusion push can make you rich tin",
    "start": "2349590",
    "end": "2356550"
  },
  {
    "text": "and happy thank you",
    "start": "2356550",
    "end": "2363770"
  },
  {
    "text": "I'd be glad to take any questions about Zulu push architecture or Zulu",
    "start": "2369520",
    "end": "2374690"
  },
  {
    "text": "operations at this point hello",
    "start": "2374690",
    "end": "2384260"
  },
  {
    "text": "so Amazon Netflix have 100 million customer right 125 so how do you test",
    "start": "2384260",
    "end": "2390710"
  },
  {
    "text": "this if you want to test some something how do you taste I mean it goes to",
    "start": "2390710",
    "end": "2396050"
  },
  {
    "text": "mobile and different devices right how do you create QA this message meaning so",
    "start": "2396050",
    "end": "2403150"
  },
  {
    "text": "these are different from normal push messages and since these are background push messages these are not mainly the",
    "start": "2403150",
    "end": "2409970"
  },
  {
    "text": "main use case is not to show something to the user so for example the test the",
    "start": "2409970",
    "end": "2416630"
  },
  {
    "text": "use case I just explained where you have a new list in the cloud and you send a",
    "start": "2416630",
    "end": "2421820"
  },
  {
    "text": "push message for the client to the client to download that list so you can track if I send it my push message did",
    "start": "2421820",
    "end": "2428480"
  },
  {
    "text": "that client actually come back and downloaded that list so all this background push messages how some",
    "start": "2428480",
    "end": "2434930"
  },
  {
    "text": "actions coupled with them that the client takes and we track those actions",
    "start": "2434930",
    "end": "2439940"
  },
  {
    "text": "and I'm not sure that that's the right answer and the second part of that question is when rolling out Netflix",
    "start": "2439940",
    "end": "2447290"
  },
  {
    "text": "lives and dies with a be test so we put a few like a small percent of current in",
    "start": "2447290",
    "end": "2453349"
  },
  {
    "text": "the a/b test enabled with push that cell was enabled for push messaging and we",
    "start": "2453349",
    "end": "2458839"
  },
  {
    "text": "tested with them and then we rolled it out 200 percent of the users I mean how does this architecture",
    "start": "2458839",
    "end": "2467180"
  },
  {
    "text": "compare with apples ApS notification service that's a great question",
    "start": "2467180",
    "end": "2474890"
  },
  {
    "text": "Apple being Apple I don't have a lot of insight into their haka tech chure the",
    "start": "2474890",
    "end": "2480230"
  },
  {
    "text": "last I heard is they use some variant of XMPP it's not really APNs is really XMPP",
    "start": "2480230",
    "end": "2486980"
  },
  {
    "text": "and I think they use our line but you",
    "start": "2486980",
    "end": "2493339"
  },
  {
    "text": "know at a conceptual level they are very similar plants open persistent connections and then you have to do all",
    "start": "2493339",
    "end": "2501140"
  },
  {
    "text": "this stuff I'm sure they are doing this stuff in some other flavor or some other version and you push messages it's",
    "start": "2501140",
    "end": "2508080"
  },
  {
    "text": "just the messing protocol we went with WebSocket and SSE because these are open web protocols and anybody can build",
    "start": "2508080",
    "end": "2515010"
  },
  {
    "text": "those clients I have a question regarding the protocol so do you use",
    "start": "2515010",
    "end": "2521490"
  },
  {
    "text": "like JSON or like a binary protocol communicating so all of the thanks for",
    "start": "2521490",
    "end": "2527550"
  },
  {
    "text": "your question all of the current use cases use JSON but there is nothing in",
    "start": "2527550",
    "end": "2533310"
  },
  {
    "text": "our push message architecture that mandates JSON so we basically establish",
    "start": "2533310",
    "end": "2540870"
  },
  {
    "text": "a WebSocket connection so we support both text WebSocket frames and binary WebSocket frames so if you wanted today",
    "start": "2540870",
    "end": "2547350"
  },
  {
    "text": "tomorrow we could anyone send like something like a protocol for a binary message having said that yes today",
    "start": "2547350",
    "end": "2553230"
  },
  {
    "text": "mostly our message is RJ sorry two",
    "start": "2553230",
    "end": "2560550"
  },
  {
    "text": "questions one is how many connections per server using how much memory you",
    "start": "2560550",
    "end": "2567060"
  },
  {
    "text": "might have addressed that okay and this and the second question is is this only used for push or can the client",
    "start": "2567060",
    "end": "2574140"
  },
  {
    "text": "communicate like request information from you right thanks good question so",
    "start": "2574140",
    "end": "2580680"
  },
  {
    "text": "our current server is 8 GB RAM and we",
    "start": "2580680",
    "end": "2586680"
  },
  {
    "text": "are comfortable pushing that server to 84,000 concurrent connections on a single server 84,000 yeah it for we do",
    "start": "2586680",
    "end": "2596520"
  },
  {
    "text": "operate at a level below that to give us some Headroom so mostly around 72 K",
    "start": "2596520",
    "end": "2601550"
  },
  {
    "text": "connections and the second question yes in theory the client can send something",
    "start": "2601550",
    "end": "2608790"
  },
  {
    "text": "to the server and there are very few corner cases with client does that but",
    "start": "2608790",
    "end": "2613830"
  },
  {
    "text": "as I I just went over this WebSocket connection it's sticky so it has all",
    "start": "2613830",
    "end": "2618990"
  },
  {
    "text": "these problems so we try to keep all the upstream which API traffic on HTTP",
    "start": "2618990",
    "end": "2625320"
  },
  {
    "text": "because it comes with all the stateless benefits so mostly this is used for",
    "start": "2625320",
    "end": "2631050"
  },
  {
    "text": "communication from server to the client but there is nothing in design that precludes sending something",
    "start": "2631050",
    "end": "2638380"
  },
  {
    "text": "up from client to server we just try not to for architectural illusions hi I'm",
    "start": "2638380",
    "end": "2648370"
  },
  {
    "text": "sorry you've mentioned that you've you're disconnecting the clients in your ass in the client feels the disconnect",
    "start": "2648370",
    "end": "2654460"
  },
  {
    "text": "themselves how do you cope with the fact that some messages are going to be lost",
    "start": "2654460",
    "end": "2659770"
  },
  {
    "text": "because the client it is at that moment not specifically connected because it's",
    "start": "2659770",
    "end": "2664870"
  },
  {
    "text": "between that stage right so that really depends on client the client yes so most",
    "start": "2664870",
    "end": "2672970"
  },
  {
    "text": "of the clients for example the perfect example is again the recommendation screen right whenever plant starts up",
    "start": "2672970",
    "end": "2681070"
  },
  {
    "text": "it's going to do a first page of all the recommendations so if your client was offline I sent you that a new",
    "start": "2681070",
    "end": "2687340"
  },
  {
    "text": "recommendation is there for you and if you did not receive that message we can safely ignore that because when the",
    "start": "2687340",
    "end": "2693340"
  },
  {
    "text": "client startup starts up is going to do that first page so in those cases it's not a problem in some other use cases",
    "start": "2693340",
    "end": "2700420"
  },
  {
    "text": "when you need a guarantee that you can never lose a message what we advise our",
    "start": "2700420",
    "end": "2705730"
  },
  {
    "text": "client to do is something we call hand over hand transfer of WebSocket connections so if they have a WebSocket",
    "start": "2705730",
    "end": "2711610"
  },
  {
    "text": "connection and they get a close connection message we ask them to open another WebSocket connection so they",
    "start": "2711610",
    "end": "2717580"
  },
  {
    "text": "have now - over a socket connections with our push server and then the close the old one the way our push our history",
    "start": "2717580",
    "end": "2724150"
  },
  {
    "text": "structure is the last one wins so when you open a new push WebSocket connection",
    "start": "2724150",
    "end": "2729700"
  },
  {
    "text": "that record wins for your old record and then you can easily safely shut down",
    "start": "2729700",
    "end": "2736570"
  },
  {
    "text": "your old connection so now you always have one connection upon it push sir so you will not lose any messages but",
    "start": "2736570",
    "end": "2742900"
  },
  {
    "text": "again having said that it's just like push notifications right like push notifications work 99.99 percent of the",
    "start": "2742900",
    "end": "2749110"
  },
  {
    "text": "time on mobile but if you read APNs documentation they say it's a best effort delivery it's not guaranteed",
    "start": "2749110",
    "end": "2755860"
  },
  {
    "text": "delivery same is true with dual push messages hi sorry I can't see over here",
    "start": "2755860",
    "end": "2766340"
  },
  {
    "text": "and right here in front of you on the question is what kind of information do",
    "start": "2766340",
    "end": "2774290"
  },
  {
    "text": "you save what what is this state that you're saving on the back end and if you",
    "start": "2774290",
    "end": "2779330"
  },
  {
    "text": "do have an upgrades in error where we need to change that type of state what information are saving how do you handle",
    "start": "2779330",
    "end": "2785690"
  },
  {
    "text": "that switch to the new format so you mean what state we save on the push",
    "start": "2785690",
    "end": "2791840"
  },
  {
    "text": "registry it's simply simplified it just to two",
    "start": "2791840",
    "end": "2799520"
  },
  {
    "text": "pieces of information so your plant is identified either by a customer ID or a device ID or a combination of two so",
    "start": "2799520",
    "end": "2806270"
  },
  {
    "text": "that as a key versus the internal 10/20 for address of the Zul pushed server that client is connected to we don't",
    "start": "2806270",
    "end": "2813530"
  },
  {
    "text": "store any other state and so if you store just that state it's practically imitable the only thing",
    "start": "2813530",
    "end": "2819620"
  },
  {
    "text": "that can change that state is your client drops the connection or gets like crashed or something and just goes away",
    "start": "2819620",
    "end": "2826550"
  },
  {
    "text": "in the first case it will clear the record because client cleanly terminated in the second place the TTL we take care",
    "start": "2826550",
    "end": "2833360"
  },
  {
    "text": "of taking the record so for some amount of time that record is going to be inaccurate but that's not a problem",
    "start": "2833360",
    "end": "2839330"
  },
  {
    "text": "because the worst that can happen is you you look up that record and say this server has this client connection and",
    "start": "2839330",
    "end": "2846470"
  },
  {
    "text": "then you connect to this server and send that server to send a push message downstream and the server is gonna",
    "start": "2846470",
    "end": "2852650"
  },
  {
    "text": "respond I don't have that information because the server itself maintains all the client connections against their",
    "start": "2852650",
    "end": "2858200"
  },
  {
    "text": "client IDs in its memory and server is always more authoritative so server is gonna say this is no longer",
    "start": "2858200",
    "end": "2864410"
  },
  {
    "text": "correct and then the message processor will clear the registry this is",
    "start": "2864410",
    "end": "2869600"
  },
  {
    "text": "somebody's",
    "start": "2869600",
    "end": "2871990"
  },
  {
    "text": "hi thanks is all of the deduplication handled on a client or there's something",
    "start": "2880740",
    "end": "2886870"
  },
  {
    "text": "that's done on the servers as well human deduplication of messages yes it's",
    "start": "2886870",
    "end": "2892330"
  },
  {
    "text": "mostly planned yeah we don't we could but then you get into the application",
    "start": "2892330",
    "end": "2897340"
  },
  {
    "text": "specific logic because you have to understand what are duplicate messages and some of our messages are actually",
    "start": "2897340",
    "end": "2902710"
  },
  {
    "text": "batch so if I send a message it does not get it there the second message will have this message plus another part to",
    "start": "2902710",
    "end": "2909970"
  },
  {
    "text": "it so the deduplication on server side is not really that easy you have to",
    "start": "2909970",
    "end": "2915910"
  },
  {
    "text": "understand the application semantics so we let clients handle it basically we do give each message a unique good and",
    "start": "2915910",
    "end": "2923700"
  },
  {
    "text": "application message type and that makes it easy for clients to say whether they got it first or not I think we are being",
    "start": "2923700",
    "end": "2933310"
  },
  {
    "text": "asked to we are running out of time thank you thank you so much I want you",
    "start": "2933310",
    "end": "2943330"
  },
  {
    "text": "to remember push is green and easy thank you",
    "start": "2943330",
    "end": "2948240"
  }
]