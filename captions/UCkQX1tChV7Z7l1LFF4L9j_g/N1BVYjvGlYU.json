[
  {
    "text": "hey it's Roland M here before we start today's podcast I wanted to tell you",
    "start": "1079",
    "end": "7040"
  },
  {
    "text": "about cuon London 2024 it is quon's Flagship International software development conference that takes place",
    "start": "7040",
    "end": "13960"
  },
  {
    "text": "in the heart of London next April 8 to 10 I will be there learning about senior",
    "start": "13960",
    "end": "19400"
  },
  {
    "text": "practitioners experiences and exploring their points of view on emerging Trends and best practices across topics like",
    "start": "19400",
    "end": "26880"
  },
  {
    "text": "software architecture generative AI plat Form Engineering observability and",
    "start": "26880",
    "end": "32599"
  },
  {
    "text": "secure software Supply chains discover what your pairs have learned explore the techniques they are using and learn",
    "start": "32599",
    "end": "39719"
  },
  {
    "text": "about or the pitfalls to avoid learn more at cucon l.com and we really hope",
    "start": "39719",
    "end": "45480"
  },
  {
    "text": "to see you there please say hi to me when you",
    "start": "45480",
    "end": "49840"
  },
  {
    "text": "are welcome everybody to the infoq podcast my name is Rand merens and I",
    "start": "51600",
    "end": "57600"
  },
  {
    "text": "your host for today today I'm interview in Sam party with a principal applied AI",
    "start": "57600",
    "end": "63320"
  },
  {
    "text": "engineer at RIS we are talking to each other in person at the cuon S Francisco",
    "start": "63320",
    "end": "68400"
  },
  {
    "text": "Conference just after he gave the presentation called generative search practical advice for retrieval augmented",
    "start": "68400",
    "end": "76040"
  },
  {
    "text": "generation keep an eye on info.com for his presentation as it contains many",
    "start": "76040",
    "end": "81200"
  },
  {
    "text": "insights into how one can enhance large language models by adding a search component the retriever augmented",
    "start": "81200",
    "end": "87799"
  },
  {
    "text": "generation during today's interview we will dive deeper into how you can do this and I hope you enjoy it and I hope",
    "start": "87799",
    "end": "94479"
  },
  {
    "text": "you can learn from the conversation welcome Sam to the infq podcast we're recording this live at cucon in San",
    "start": "94479",
    "end": "101079"
  },
  {
    "text": "Francisco how do you like the conference so far it's awesome you know I'm really glad he invited me this and we've had a",
    "start": "101079",
    "end": "107520"
  },
  {
    "text": "really good time I've met some really interesting people I was talking to the source graph guys earlier and really loved their demo and you know there's a",
    "start": "107520",
    "end": "114399"
  },
  {
    "text": "lot of tech right now in the scene that even someone like me who every single day I wake up I went to a Meetup for",
    "start": "114399",
    "end": "120920"
  },
  {
    "text": "web8 last night I still see new things and it's one of the coolest things about",
    "start": "120920",
    "end": "126640"
  },
  {
    "text": "living here and being in this space and cucon is a great example of that yeah it's so cool that here everybody is",
    "start": "126640",
    "end": "132000"
  },
  {
    "text": "working on the state of the-art things I think your presentation was also very much toward State of the art and one of",
    "start": "132000",
    "end": "138920"
  },
  {
    "text": "the first things people should look at if they want to set up a system with embeddings and want to set up a system",
    "start": "138920",
    "end": "145440"
  },
  {
    "text": "with large language models I think uh can you maybe give a summary of your talk yeah absolutely so about 2 years",
    "start": "145440",
    "end": "151519"
  },
  {
    "text": "ago redus introduced its Vector offering essentially Vector database offering it",
    "start": "151519",
    "end": "157000"
  },
  {
    "text": "turns redus into a vector database so I started at reddis around that time and",
    "start": "157000",
    "end": "163239"
  },
  {
    "text": "my job was not necessarily to embed you know the hnsw into the database itself",
    "start": "163239",
    "end": "170280"
  },
  {
    "text": "there was an awesome set of Engineers D duckin and those guys who are exceptional Engineers there was a gap",
    "start": "170280",
    "end": "176440"
  },
  {
    "text": "that other Vector databases had that you couldn't use redison Lang chain or llama",
    "start": "176440",
    "end": "181480"
  },
  {
    "text": "index or what have you so my job is to actually do those Integrations and on",
    "start": "181480",
    "end": "186640"
  },
  {
    "text": "top of that to work with customers and so over the past two or so years I've been working with those integration",
    "start": "186640",
    "end": "192400"
  },
  {
    "text": "Frameworks with those customers with those you know users in open source and",
    "start": "192400",
    "end": "198280"
  },
  {
    "text": "one of the things that you kind of learn through doing all that are a lot of like the best practices and that's really",
    "start": "198280",
    "end": "204959"
  },
  {
    "text": "what the talk is is just a lot of stuff that I've learned by building and and",
    "start": "204959",
    "end": "210200"
  },
  {
    "text": "that's essentially the content okay but so if you say that you're working on Vector databases why can't you just",
    "start": "210200",
    "end": "217159"
  },
  {
    "text": "simply store a vector in any database yeah so it kind of matters what your use",
    "start": "217159",
    "end": "223720"
  },
  {
    "text": "case is so for redis for instance let's take that it's an incredible realtime",
    "start": "223720",
    "end": "230879"
  },
  {
    "text": "platform but if your vectors never change if you have a static data set of a billion embeddings you're way better",
    "start": "230879",
    "end": "238360"
  },
  {
    "text": "off using something something like feice and storing in an S3 bucket loading it into a Lambda function and calling it",
    "start": "238360",
    "end": "245120"
  },
  {
    "text": "every once in a while just like programming languages there's not a one-size fits-all programming language",
    "start": "245120",
    "end": "251360"
  },
  {
    "text": "you might think python is but that's just because it's awesome but it's the truth that there's no tool that fits",
    "start": "251360",
    "end": "258400"
  },
  {
    "text": "every use case and there's certainly no vendor that fits every use case reddis is really good at what it does and so",
    "start": "258400",
    "end": "265120"
  },
  {
    "text": "are a lot of other vendors and so you really just got to be able to evaluate and know your use case and evaluate",
    "start": "265120",
    "end": "270680"
  },
  {
    "text": "based on that so what use cases have you carved out for what people would you",
    "start": "270680",
    "end": "276080"
  },
  {
    "text": "recommend to maybe rewat your talk yeah so specifically use cases one thing",
    "start": "276080",
    "end": "282160"
  },
  {
    "text": "that's been really big that we've done a lot of our chat conversations so long-term memory for large language",
    "start": "282160",
    "end": "288240"
  },
  {
    "text": "models is this concept where the context window of even in the largest case what",
    "start": "288240",
    "end": "294120"
  },
  {
    "text": "is it 16k no 32k something like that for I think that's GPT 4 even then you could",
    "start": "294120",
    "end": "300720"
  },
  {
    "text": "have a chat history that is over that 32k token limit and in that case you",
    "start": "300720",
    "end": "307160"
  },
  {
    "text": "need other data structures than just a vector index and you need the ability to",
    "start": "307160",
    "end": "314080"
  },
  {
    "text": "sort like zets and redus and so there are other data structures that come into",
    "start": "314080",
    "end": "319639"
  },
  {
    "text": "play that you know play as memory buffers or things like that that for those kind of chat conversations end up",
    "start": "319639",
    "end": "325479"
  },
  {
    "text": "really mattering and they're actually integrated into L chain another one's semantic caching is this concept of it's",
    "start": "325479",
    "end": "332000"
  },
  {
    "text": "what redis has been best at for its decadal career I don't know something like that ever since salvator wrote it",
    "start": "332000",
    "end": "338960"
  },
  {
    "text": "and semantic caching is simply the addition it's like almost the next evolution of caching where instead of",
    "start": "338960",
    "end": "345880"
  },
  {
    "text": "just a perfect onetoone match like you would expect a hash would be it's more of a one to many in the sense that you",
    "start": "345880",
    "end": "353800"
  },
  {
    "text": "can have a threshold for how similar a cached item should be and what what that",
    "start": "353800",
    "end": "359759"
  },
  {
    "text": "cash item should return is based on that specific percentage of threshold and so",
    "start": "359759",
    "end": "365000"
  },
  {
    "text": "it allows these types of things like say the chat conversation where if you want to return oh what's the last time that I",
    "start": "365000",
    "end": "371880"
  },
  {
    "text": "said something like X you can now do that and have the same thing that is not",
    "start": "371880",
    "end": "378400"
  },
  {
    "text": "only returning that conversational memory but also have that cashed and",
    "start": "378400",
    "end": "384319"
  },
  {
    "text": "with redus you get that all at a really high speed and so for those use cases it",
    "start": "384319",
    "end": "390000"
  },
  {
    "text": "ends up being really great and so there's obviously a lot of others but I'll talk about some that it's not right",
    "start": "390000",
    "end": "396000"
  },
  {
    "text": "so some that it's not that we've seen that we are not necessarily the best for is it's in a memory database right and",
    "start": "396000",
    "end": "402560"
  },
  {
    "text": "we have tearing and auto teering which allows you to go to mvme and you can have an mvme drive whatever and go from",
    "start": "402560",
    "end": "408720"
  },
  {
    "text": "memory to mvme and you can actually do that automatically now which is quite fascinating to me but even then and even",
    "start": "408720",
    "end": "416639"
  },
  {
    "text": "if you have those kinds of things enabled there are cases like I mentioned where you have a product catalog that",
    "start": "416639",
    "end": "423240"
  },
  {
    "text": "changes once every six months and it's not a demanding QPS use case you don't",
    "start": "423240",
    "end": "429680"
  },
  {
    "text": "need the latencies of fedus you call this thing once every month to set user",
    "start": "429680",
    "end": "435360"
  },
  {
    "text": "based recommendations that are relatively static or something like that those use cases it's kind of an",
    "start": "435360",
    "end": "441599"
  },
  {
    "text": "impractical expense and you know it's not like I'm trying to downtalk the place I work right now it's really just",
    "start": "441599",
    "end": "448280"
  },
  {
    "text": "so that people understand like why it is so good for those use cases and why it",
    "start": "448280",
    "end": "454759"
  },
  {
    "text": "justifies and even in that case of something like a recommendation system that is live or online it even justifies",
    "start": "454759",
    "end": "462000"
  },
  {
    "text": "itself in terms of return on investment and so those types of use cases it's",
    "start": "462000",
    "end": "467479"
  },
  {
    "text": "really good for but the types that are static that don't change it really isn't",
    "start": "467479",
    "end": "473039"
  },
  {
    "text": "one of the tools that you're going to want to have in your stack unless you're going to be doing something more traditional like caching or using it for",
    "start": "473039",
    "end": "479560"
  },
  {
    "text": "one of its other data structures which is also a nice side benefit that it has so many other things it's used for I",
    "start": "479560",
    "end": "486840"
  },
  {
    "text": "mean it's Zone streaming platform yeah so maybe let's break down some of this use case you mentioned extracting the",
    "start": "486840",
    "end": "493680"
  },
  {
    "text": "vectors from documents and you also mentioned that oh if a vector is close enough then you use it for caching maybe",
    "start": "493680",
    "end": "500400"
  },
  {
    "text": "let's first dive into the second part because that's what we just talked about so you say oh if a vector is close",
    "start": "500400",
    "end": "505960"
  },
  {
    "text": "enough does redis then internally build up a tree to do this fast nearest neighbor search sure yeah so we have two",
    "start": "505960",
    "end": "513240"
  },
  {
    "text": "algorithms we have K&N K nearest neighbors brute force and you can think",
    "start": "513240",
    "end": "518640"
  },
  {
    "text": "of this like an exhaustive search right it's obviously a little bit better than that but imagine just going down a list",
    "start": "518640",
    "end": "524720"
  },
  {
    "text": "and doing that comparison that simplified view of it but that's called our flat index and then we have hnsw",
    "start": "524720",
    "end": "531560"
  },
  {
    "text": "which is our approximate nearest neighbors index and so both of those are integrated that we vendored hnsw lib and",
    "start": "531560",
    "end": "538480"
  },
  {
    "text": "that's what's including uded inside of redus it's modified for making it work with things like crud operations inside",
    "start": "538480",
    "end": "544959"
  },
  {
    "text": "of redus but that's what happens when you have those vectors and they're index inside of redus you choose if you're",
    "start": "544959",
    "end": "551399"
  },
  {
    "text": "using something like redv you can pass in a dictionary configuration or yel file or what have you and that chooses",
    "start": "551399",
    "end": "558560"
  },
  {
    "text": "what index you end up using for search and so for the people out there that are wondering which index do I use because",
    "start": "558560",
    "end": "564600"
  },
  {
    "text": "that's always a follow-up question if you have under a million embeddings the in search is often better because if you",
    "start": "564600",
    "end": "571839"
  },
  {
    "text": "think about it like in the list example appending to a list is very fast and recreating that list is very fast doing",
    "start": "571839",
    "end": "579120"
  },
  {
    "text": "so for a complex tree or graph based structure you know that is more computationally complex and so if you",
    "start": "579120",
    "end": "586560"
  },
  {
    "text": "don't need the latencies of something like hnsw if you don't have that many",
    "start": "586560",
    "end": "591720"
  },
  {
    "text": "documents if you're not at that scale then you should use the KNN index in the other case if you're above that",
    "start": "591720",
    "end": "598000"
  },
  {
    "text": "threshold and you do need those latencies then hnsw provides those benefits which is why we have both and",
    "start": "598000",
    "end": "604560"
  },
  {
    "text": "we have tons of customers using either one yeah so basically then we're just down to if you have anything stored in",
    "start": "604560",
    "end": "611800"
  },
  {
    "text": "your database it's basically like a dictionary with nearest neighbor do you then get multiple neighbors back or do",
    "start": "611800",
    "end": "618160"
  },
  {
    "text": "you just return what you stored in your database at this dictionary location",
    "start": "618160",
    "end": "623200"
  },
  {
    "text": "there are two specific data structures hashes and Json documents hashes and reddis are like at a key you store a",
    "start": "623200",
    "end": "629839"
  },
  {
    "text": "value Json documents you have at a key you store a Json document when you are",
    "start": "629839",
    "end": "635320"
  },
  {
    "text": "doing a vector similarity search within reddis whatever client you use whether it's go or Java or what have you or",
    "start": "635320",
    "end": "642160"
  },
  {
    "text": "python what you get back in terms of a vector search is defined by the syntax",
    "start": "642160",
    "end": "648880"
  },
  {
    "text": "of that query and there are two major ones to know about first are vector searches just plain Vector searches",
    "start": "648880",
    "end": "656399"
  },
  {
    "text": "which are I want a specific number of results that are semantically similar to",
    "start": "656399",
    "end": "663399"
  },
  {
    "text": "this query embedding and then you have range queries which are you can return",
    "start": "663399",
    "end": "668600"
  },
  {
    "text": "as many results as you want but they have to be this specific range in this",
    "start": "668600",
    "end": "674680"
  },
  {
    "text": "range of vector distance away from this specific vector and whether I said",
    "start": "674680",
    "end": "679880"
  },
  {
    "text": "semantically earlier it could be visual embeddings it could be semantic embeddings it doesn't matter and so",
    "start": "679880",
    "end": "685519"
  },
  {
    "text": "Vector searches range searches queries etc those those are the two major",
    "start": "685519",
    "end": "690560"
  },
  {
    "text": "methodologies it's important to note that also redus just supports straight up just text search and other types of",
    "start": "690560",
    "end": "695680"
  },
  {
    "text": "search features which you can use combinator so all of those are available when you run that and it's really",
    "start": "695680",
    "end": "701760"
  },
  {
    "text": "defined by how you use it so like if you are particular about like let's say it's",
    "start": "701760",
    "end": "707279"
  },
  {
    "text": "a recommendation system or a product catalog again to use that example you might say I only want to recommend",
    "start": "707279",
    "end": "713399"
  },
  {
    "text": "things to the user there's probably a case for this if they're this similar if",
    "start": "713399",
    "end": "718760"
  },
  {
    "text": "the they this similar to what is in the user's cart or this basket you might want to use something like a range query",
    "start": "718760",
    "end": "725320"
  },
  {
    "text": "right yeah yeah makes sense yeah if you're searching for I don't know your cookbooks on Amazon you don't want to",
    "start": "725320",
    "end": "731959"
  },
  {
    "text": "get the nearest instruction manual for cars whatever even though it's there yeah sure at some point there has a cut",
    "start": "731959",
    "end": "737800"
  },
  {
    "text": "off that might be a semetic similarity or let's say a score rather than a vector distance you know one minus the",
    "start": "737800",
    "end": "743639"
  },
  {
    "text": "distance that might be a score of like let's say 6 right but that's not",
    "start": "743639",
    "end": "748959"
  },
  {
    "text": "relevant enough to be a recommendation that's worthwhile and so if there's 700",
    "start": "748959",
    "end": "755320"
  },
  {
    "text": "of them that are worthwhile you might want 700 of them but if there's only two",
    "start": "755320",
    "end": "760760"
  },
  {
    "text": "you might only want two that's what range queries are really good for yeah is because you might not know ahead of",
    "start": "760760",
    "end": "766000"
  },
  {
    "text": "time how many results you want back but you might want to say they can only be",
    "start": "766000",
    "end": "772040"
  },
  {
    "text": "this far away and that's a concept that's been around in Vector search libraries for quite some time but it is",
    "start": "772040",
    "end": "778519"
  },
  {
    "text": "now you know you can get back in milliseconds when you're using redus which is pretty cool yeah nice sounds",
    "start": "778519",
    "end": "783760"
  },
  {
    "text": "pretty interesting you also mentioned that you can combine this with other queries yeah so we often call this",
    "start": "783760",
    "end": "790279"
  },
  {
    "text": "hybrid search really hybrid search is weighted search so I'm going to start saying filtered search for the purposes",
    "start": "790279",
    "end": "795320"
  },
  {
    "text": "of this podcast if you have what I'm going to call a recall set which is what you get back after you do a vector",
    "start": "795320",
    "end": "801000"
  },
  {
    "text": "search you can have a pre or post filter this is particular to reddis but there",
    "start": "801000",
    "end": "806519"
  },
  {
    "text": "are tons of other Vector databases that support this and you can do a pre- or post filter the pre-filter in a lot of",
    "start": "806519",
    "end": "812079"
  },
  {
    "text": "cases is more important think about this example let's say I'm using as a",
    "start": "812079",
    "end": "817959"
  },
  {
    "text": "conversational memory buffer okay yeah and this could be in L chain it's implemented there too and I only want",
    "start": "817959",
    "end": "824040"
  },
  {
    "text": "the conversation with this user well then I would use a tag filter where the",
    "start": "824040",
    "end": "829800"
  },
  {
    "text": "tag it's basically like exact text search you think about like that or categorical search where some piece of",
    "start": "829800",
    "end": "835120"
  },
  {
    "text": "that metadata in my hash or Json document in reddis is going to be a user's username and then I can use that",
    "start": "835120",
    "end": "841839"
  },
  {
    "text": "tag to filter all of the records that I have that are specific to that user and",
    "start": "841839",
    "end": "848160"
  },
  {
    "text": "then I can do a vector search so it allows you to almost have it's like a schema in a way of like think about it",
    "start": "848160",
    "end": "855199"
  },
  {
    "text": "it's like a SQL database it allows you to Define kind of how you're going to use it but the benefits here are that if",
    "start": "855199",
    "end": "861120"
  },
  {
    "text": "you don't do something in the beginning you can then add it later and still alter the schema of the index adjust and",
    "start": "861120",
    "end": "868720"
  },
  {
    "text": "grow your platform which is a really cool thing so the hybrid searches are really interesting in red you can do it",
    "start": "868720",
    "end": "873839"
  },
  {
    "text": "with text full text search like bm25 you can do it with tags Geographic by you",
    "start": "873839",
    "end": "880720"
  },
  {
    "text": "can do polygon search now which is really interesting literally just draw a polygon of coordinates and if they're",
    "start": "880720",
    "end": "886880"
  },
  {
    "text": "within that polygon of coordinates then that is where you do your vector search pretty good for any mapping application",
    "start": "886880",
    "end": "892959"
  },
  {
    "text": "I sh or like say food delivery yeah I actually think I gave that example in the talk I gave that example cuz he was",
    "start": "892959",
    "end": "899240"
  },
  {
    "text": "in the front he's you know obviously door Dash guy they're power users of Open Source and it's always fun to see",
    "start": "899240",
    "end": "905199"
  },
  {
    "text": "how people use it but so in terms of performance your embeddings are represented in a certain way to make it",
    "start": "905199",
    "end": "911560"
  },
  {
    "text": "fast to search through them yeah and then but filters are a completely different game right so is there any",
    "start": "911560",
    "end": "919360"
  },
  {
    "text": "performance benefits to pre-filtering over post filtering or the other way around I always hate when I hear this",
    "start": "919360",
    "end": "925680"
  },
  {
    "text": "answer but it depends if you have a pre filter that filters it down to a really",
    "start": "925680",
    "end": "930880"
  },
  {
    "text": "small set then yes but if you have a pre filter you know you can combine them",
    "start": "930880",
    "end": "936880"
  },
  {
    "text": "with Boolean operators if you have a pre-filter that's really complicated and does a lot of operations on each record",
    "start": "936880",
    "end": "944199"
  },
  {
    "text": "to see whether it belongs to that set then you can shoot yourself in the foot trying to achieve that performance",
    "start": "944199",
    "end": "950279"
  },
  {
    "text": "benefit and so it really depends on your query structure and your schema",
    "start": "950279",
    "end": "955480"
  },
  {
    "text": "structure and so that's not always obvious I've seen an old to say an",
    "start": "955480",
    "end": "961079"
  },
  {
    "text": "e-commerce company that had about aund and something combined filters in their",
    "start": "961079",
    "end": "967759"
  },
  {
    "text": "pre-filter actually no it was post filter for them because they wanted to do VOR search over all the records and then do a post filter but it was like",
    "start": "967759",
    "end": "974600"
  },
  {
    "text": "140 different filters right that's a very dedicated they want something very",
    "start": "974600",
    "end": "979839"
  },
  {
    "text": "specific well it made sense for the platform which I can't talk about but we found a much better way to do it and I",
    "start": "979839",
    "end": "986079"
  },
  {
    "text": "can talk about that which is that ahead of time you can just combine a lot of those fields and so you know you have",
    "start": "986079",
    "end": "993120"
  },
  {
    "text": "extra fields in your schema you're storing more your memory consumption goes up but your runtime complexity the",
    "start": "993120",
    "end": "999360"
  },
  {
    "text": "latency of the system goes down because it's almost like your pre-computing which is like an age-old computer",
    "start": "999360",
    "end": "1004680"
  },
  {
    "text": "science technique right so you know increase space complexity decrease",
    "start": "1004680",
    "end": "1010800"
  },
  {
    "text": "runtime complexity and that really helped yeah perfect right of going back to the other thing you mentioned about",
    "start": "1010800",
    "end": "1017480"
  },
  {
    "text": "the documents I think you mentioned two different ways that you can represent your documents in this Ming space yeah",
    "start": "1017480",
    "end": "1025199"
  },
  {
    "text": "can you maybe elaborate on what the two different ways are and when you would choose one over the other yeah so what I",
    "start": "1025199",
    "end": "1031120"
  },
  {
    "text": "was talking about here was a lot of people just it's funny I was talking to a great guy I ate lunch with him and he",
    "start": "1031120",
    "end": "1037760"
  },
  {
    "text": "was talking about Rag and how people just take Lang chain or llama index or one of these Frameworks and they use a",
    "start": "1037760",
    "end": "1045079"
  },
  {
    "text": "recursive character text splitter or something and split their documents up not caring about overlap not caring",
    "start": "1045079",
    "end": "1051320"
  },
  {
    "text": "about how many tokens they have and chunk it up and use those randomly as",
    "start": "1051320",
    "end": "1058440"
  },
  {
    "text": "the text raw text basically for the embeddings and then they run their rag system and wonder why it's bad and it's",
    "start": "1058440",
    "end": "1065679"
  },
  {
    "text": "because you have filler text you have text that isn't relevant you possibly have the wrong size and your embeddings",
    "start": "1065679",
    "end": "1072520"
  },
  {
    "text": "possibly aren't even relevant so what I'm suggesting in this talk is a couple ways and actually a quick shout shout",
    "start": "1072520",
    "end": "1078520"
  },
  {
    "text": "out to Jerry Lou for the diagram there he runs llama index great guy what I'm suggesting is there's two approaches I",
    "start": "1078520",
    "end": "1085280"
  },
  {
    "text": "talk about first is you take that raw text and ask an llm to summarize it this",
    "start": "1085280",
    "end": "1091200"
  },
  {
    "text": "approach allows you to have a whole document summary and then the chunks of",
    "start": "1091200",
    "end": "1096440"
  },
  {
    "text": "that document associated with that summary so first you go and do a vector search over the summaries of the",
    "start": "1096440",
    "end": "1103320"
  },
  {
    "text": "documents which are often semantically more like rich in terms of context which",
    "start": "1103320",
    "end": "1108440"
  },
  {
    "text": "helps that Vector search out and then you can return all of the document junks",
    "start": "1108440",
    "end": "1113559"
  },
  {
    "text": "and even then sometimes on the client side do either a database local Vector search on the chunks that you return",
    "start": "1113559",
    "end": "1120799"
  },
  {
    "text": "after that first Vector search and with rdus you can also combine those two operations triggers functions are",
    "start": "1120799",
    "end": "1127039"
  },
  {
    "text": "awesome people should check that out 72 release is awesome but then the second approach is also really interesting and",
    "start": "1127039",
    "end": "1134039"
  },
  {
    "text": "it involves cases where you would like the surrounding context to be included",
    "start": "1134039",
    "end": "1141679"
  },
  {
    "text": "but your user query is often something that is found in maybe one or two",
    "start": "1141679",
    "end": "1147600"
  },
  {
    "text": "sentences and includes things like maybe names or specific numbers or phrases or",
    "start": "1147600",
    "end": "1155240"
  },
  {
    "text": "you know to use this Finance example we worked on it's like the name of this mutual bond in this paragraph or",
    "start": "1155240",
    "end": "1162080"
  },
  {
    "text": "whatever it was what we did there was instead we split it sentence by sentence",
    "start": "1162080",
    "end": "1167600"
  },
  {
    "text": "and so that when the user entered a query it found that particular sentence through Vector search semantic search",
    "start": "1167600",
    "end": "1174080"
  },
  {
    "text": "but the context the text that was retrieved was a larger window around",
    "start": "1174080",
    "end": "1180799"
  },
  {
    "text": "that sentence and so it had more information when you retrieved that context and so the first thing that",
    "start": "1180799",
    "end": "1188039"
  },
  {
    "text": "people should know about this approach is that it absolutely blows up the size of your database ites you get embedding",
    "start": "1188039",
    "end": "1193280"
  },
  {
    "text": "per sentence yeah and you spend way more on your vector database because think",
    "start": "1193280",
    "end": "1198520"
  },
  {
    "text": "about it you're not only storing more text you're storing more vectors and it",
    "start": "1198520",
    "end": "1203840"
  },
  {
    "text": "works well for those use cases but you have to make sure that that's worth it and that's why I'm advocating for people",
    "start": "1203840",
    "end": "1209720"
  },
  {
    "text": "this is why I made it my first slide in that section is just go try a bunch I talk about like using traditional",
    "start": "1209720",
    "end": "1214960"
  },
  {
    "text": "machine learning techniques so weird that we call it traditional now but do like a kfold try five different things",
    "start": "1214960",
    "end": "1221480"
  },
  {
    "text": "and then have an eval set try it against an eval set you know just like we would have with XG boost when it was 5 years",
    "start": "1221480",
    "end": "1227960"
  },
  {
    "text": "ago It Feels Like Everything has changed but yeah that's what I was talking about but so if you are doing the sentence by",
    "start": "1227960",
    "end": "1234520"
  },
  {
    "text": "sentence do the embeddings and you have the larger context around it is there still enough uniqueness for every",
    "start": "1234520",
    "end": "1241200"
  },
  {
    "text": "sentence or do these large language models then just kind of make the same Vector of everything if you have a",
    "start": "1241200",
    "end": "1248640"
  },
  {
    "text": "situation where the query or whatever's being used as the query Vector is a lot",
    "start": "1248640",
    "end": "1254960"
  },
  {
    "text": "of text is a lot of semantic information this is not the approach to use but if",
    "start": "1254960",
    "end": "1260360"
  },
  {
    "text": "it's something like a one or two liner question or one or two sentence question it does work well what you're I think",
    "start": "1260360",
    "end": "1267480"
  },
  {
    "text": "getting at too is that imagine the sentences that people write especially in some PDFs that just don't matter they",
    "start": "1267480",
    "end": "1273360"
  },
  {
    "text": "don't need to be there yeah and you're paying for not only that embedding but the storage space and so this approach",
    "start": "1273360",
    "end": "1279279"
  },
  {
    "text": "has drawbacks but who's going to go through all I forget how many PDFs there were in that use case but like 40,000",
    "start": "1279279",
    "end": "1285600"
  },
  {
    "text": "PDFs which ended up creating it was like 180 million embeddings or something yeah",
    "start": "1285600",
    "end": "1291039"
  },
  {
    "text": "I can imagine if you use this approach on the entire archive database of of scientific papers then doc search. RIS",
    "start": "1291039",
    "end": "1298279"
  },
  {
    "text": "ventures.com you can look at a semantic search app that does on the abstracts",
    "start": "1298279",
    "end": "1303799"
  },
  {
    "text": "which is essentially the first approach right yeah but it just doesn't have the second layer right it doesn't have that",
    "start": "1303799",
    "end": "1310080"
  },
  {
    "text": "mostly because we haven't posted that it would be more expensive to host but it does it on the summaries which this",
    "start": "1310080",
    "end": "1316039"
  },
  {
    "text": "thing about the paper summary that's actually a great example example thank you for bringing that up is that the paper summary think about how more",
    "start": "1316039",
    "end": "1322440"
  },
  {
    "text": "information is packed into that than random sections of a paper right and so",
    "start": "1322440",
    "end": "1329640"
  },
  {
    "text": "that's why sometimes using an llm to essentially create what seems like a paper abstract is actually a really good",
    "start": "1329640",
    "end": "1335679"
  },
  {
    "text": "way of handling this yeah and cheaper usually I think the other thing you mentioned during your talk which I",
    "start": "1335679",
    "end": "1342200"
  },
  {
    "text": "thought was a really interesting trick is if you are having a question and answer Tre system that you let the large",
    "start": "1342200",
    "end": "1351120"
  },
  {
    "text": "language model create a possible answer and then search for that answer in your database yeah how do you quot this how",
    "start": "1351120",
    "end": "1357360"
  },
  {
    "text": "does this work again maybe you can explain this better than I just did oh no actually it's great I wish I remember the author's name of that paper right",
    "start": "1357360",
    "end": "1363559"
  },
  {
    "text": "now cuz he or she or whoever it is deserves an award and essentially the",
    "start": "1363559",
    "end": "1368880"
  },
  {
    "text": "hide approach it's called hypothetical document embeddings so Hyde Hyde like jackqueline Hyde you use a people use",
    "start": "1368880",
    "end": "1376840"
  },
  {
    "text": "the term hallucinations with llms when they make stuff up so I'm going to use that term here even though I don't",
    "start": "1376840",
    "end": "1382000"
  },
  {
    "text": "really like it I mentioned that in the talk it's just wrong information but I'll get off that high horse when you",
    "start": "1382000",
    "end": "1387360"
  },
  {
    "text": "use a hallucinated answer to a question to look up the right answer or at least",
    "start": "1387360",
    "end": "1394240"
  },
  {
    "text": "I should say the right context and so why does this work well you have a",
    "start": "1394240",
    "end": "1399720"
  },
  {
    "text": "question and that question let's say it's something like in the talk what I",
    "start": "1399720",
    "end": "1405400"
  },
  {
    "text": "say I said like what is reddis right think about how different that question is than the actual answer which is like",
    "start": "1405400",
    "end": "1413000"
  },
  {
    "text": "an in memory database but a fake answer even if it's something like it's a tool for doing",
    "start": "1413000",
    "end": "1419520"
  },
  {
    "text": "yada yada yada it's still semantically more similar in both sentence structure",
    "start": "1419520",
    "end": "1426600"
  },
  {
    "text": "and most often it's actual semantics that it returns a greater amount of",
    "start": "1426600",
    "end": "1433960"
  },
  {
    "text": "relevant information because of the way that that the semantic representation of",
    "start": "1433960",
    "end": "1440720"
  },
  {
    "text": "an answer is different from the semantic representation of a query kind of like",
    "start": "1440720",
    "end": "1447120"
  },
  {
    "text": "you dress for a job you want instead of for a job you have you search you search for the job you want you search for the",
    "start": "1447120",
    "end": "1453640"
  },
  {
    "text": "data you want for the data you have couldn't agree more and that's also what's interesting about I gave that",
    "start": "1453640",
    "end": "1459400"
  },
  {
    "text": "hotel example that was me messing around like I just created that app for fun but I realized how good of an example of",
    "start": "1459400",
    "end": "1465919"
  },
  {
    "text": "like a high example it is is because it's showing you that searching for a",
    "start": "1465919",
    "end": "1471559"
  },
  {
    "text": "review with a fake generated review is so much more likely to return reviews",
    "start": "1471559",
    "end": "1477960"
  },
  {
    "text": "that you want to see than saying this is what I want in a hotel because that",
    "start": "1477960",
    "end": "1484399"
  },
  {
    "text": "structurally and semantically is far different from a",
    "start": "1484399",
    "end": "1489720"
  },
  {
    "text": "review than some English Professor is probably crying right now with the way",
    "start": "1489720",
    "end": "1494799"
  },
  {
    "text": "I'm describing the English language I guess not just English but but you get the point it's so much more similar to",
    "start": "1494799",
    "end": "1501279"
  },
  {
    "text": "the actual reviews that you want the the query often doesn't really represent the",
    "start": "1501279",
    "end": "1506440"
  },
  {
    "text": "context you want I really liked it as an example also with hotels because on any Hotel website you can't search for",
    "start": "1506440",
    "end": "1514640"
  },
  {
    "text": "reviews but you oh of course not yeah but it kind of makes sense to start searching for the holiday you want or",
    "start": "1514640",
    "end": "1520799"
  },
  {
    "text": "others have instead of searching for the normal things you normally search for like location etc etc it was funny I",
    "start": "1520799",
    "end": "1527240"
  },
  {
    "text": "think I said that I started doing it because I actually did get mad that day at this travel website because I just",
    "start": "1527240",
    "end": "1534000"
  },
  {
    "text": "couldn't find things I was looking for and I was like why can't I do this and I realize I'm a little bit further ahead",
    "start": "1534000",
    "end": "1539880"
  },
  {
    "text": "in the field I guess than you know like some Enterprise companies are and thinking about these things because I",
    "start": "1539880",
    "end": "1545120"
  },
  {
    "text": "work on it all the time I guess but I just imagine the next few years it's just going to completely change the user",
    "start": "1545120",
    "end": "1550640"
  },
  {
    "text": "experience of so many things I've seen so many demos lately and obviously just hanging around ASF you talk to so many",
    "start": "1550640",
    "end": "1557360"
  },
  {
    "text": "people that are creating their own company or something and I've seen so many demos where they're you know using me for essentially validation of ideas",
    "start": "1557360",
    "end": "1563760"
  },
  {
    "text": "or something where my mind's just blown at how good it is and I really do think it's going to completely change user",
    "start": "1563760",
    "end": "1569919"
  },
  {
    "text": "experience going forward do you have more applications where you think it should be used for this this should exist interesting a review data is",
    "start": "1569919",
    "end": "1577640"
  },
  {
    "text": "certainly good so look right now we're really good at text representations at",
    "start": "1577640",
    "end": "1583480"
  },
  {
    "text": "semantics and the reason for that is cuz we have a lot of that data the next Frontier is definitely multimodal open",
    "start": "1583480",
    "end": "1589760"
  },
  {
    "text": "AI I think has already started on this in some of their models but one thing I was thinking about in honestly was in",
    "start": "1589760",
    "end": "1595279"
  },
  {
    "text": "creating this talk was why can't I talk to a slide and change the way it looks",
    "start": "1595279",
    "end": "1601640"
  },
  {
    "text": "and I can basically do that with stable diffusion it's on my newsletter head or you know the top of my newsletter is",
    "start": "1601640",
    "end": "1608480"
  },
  {
    "text": "this cool scene where I said the prompt is something like the evolution of tech through time because that's what I'm",
    "start": "1608480",
    "end": "1614399"
  },
  {
    "text": "curious about but you still can't interact with also with stable diffusion you can give a prompt but you can't say",
    "start": "1614399",
    "end": "1621240"
  },
  {
    "text": "oh I want this but that make it a bit brighter or replace it and you can do like you know you can",
    "start": "1621240",
    "end": "1627440"
  },
  {
    "text": "optimize it make it look a little better but you're right it's not an interaction the difference with rag in a lot of",
    "start": "1627440",
    "end": "1632559"
  },
  {
    "text": "these systems like the chat experience see a chat pretty recently made by an Enterprise company using redus that is",
    "start": "1632559",
    "end": "1638480"
  },
  {
    "text": "absolutely fantastic and the reason is is because it's interactive it's an",
    "start": "1638480",
    "end": "1644000"
  },
  {
    "text": "experience that is different and I'd imagine that in a few years you're",
    "start": "1644000",
    "end": "1649200"
  },
  {
    "text": "literally never going to call an agent on a cell phone again you're actually never going to pick up the phone and",
    "start": "1649200",
    "end": "1655200"
  },
  {
    "text": "call a customer service line because there will be a time and place and maybe",
    "start": "1655200",
    "end": "1660600"
  },
  {
    "text": "it's 10 years two years I don't know I'm not asron but it will be to the point",
    "start": "1660600",
    "end": "1666039"
  },
  {
    "text": "where it's so good it knows you personally it knows your information and",
    "start": "1666039",
    "end": "1671080"
  },
  {
    "text": "it's not because it's been trained on it it's because it's injected at runtime and it knows the last thing you ordered",
    "start": "1671080",
    "end": "1677799"
  },
  {
    "text": "it knows what the previous complaints you've had are it can solve them for you by looking up company documentation and",
    "start": "1677799",
    "end": "1685840"
  },
  {
    "text": "it can address them internally by saying hey product team we should think about",
    "start": "1685840",
    "end": "1691279"
  },
  {
    "text": "doing this like that is where we're headed to the point where they're so helpful and it's not because they",
    "start": "1691279",
    "end": "1696679"
  },
  {
    "text": "actually know all this stuff it's because that combined with really careful prompt engineering and injection",
    "start": "1696679",
    "end": "1703919"
  },
  {
    "text": "of accurate relevant data Mak systems that are seemingly incredibly",
    "start": "1703919",
    "end": "1710519"
  },
  {
    "text": "intelligent and I say seemingly because I'm not yet completely convinced that",
    "start": "1710519",
    "end": "1716200"
  },
  {
    "text": "it's anything more than a tool so anybody that personifies that's why I don't like the word hallucinations but",
    "start": "1716200",
    "end": "1722640"
  },
  {
    "text": "it is just a tool but this tool happens to be really really good the future is",
    "start": "1722640",
    "end": "1729039"
  },
  {
    "text": "bright if it can finally solve the issues that you have whenever you have to call your phone company God I hope I",
    "start": "1729039",
    "end": "1734640"
  },
  {
    "text": "never have to call another agent again in any case for the last question the",
    "start": "1734640",
    "end": "1739840"
  },
  {
    "text": "thing we like you discussed with another participant here at the cucon conference",
    "start": "1739840",
    "end": "1745000"
  },
  {
    "text": "was if you want to run these large language models is there any way to do it or do you have any recommendations",
    "start": "1745000",
    "end": "1750480"
  },
  {
    "text": "for doing this on Prem rather than having to send everything good question partner there's a cool company I think",
    "start": "1750480",
    "end": "1755760"
  },
  {
    "text": "it's out of Italy called Prem literally that has a lot of these and so shout out to them they're great but in general the",
    "start": "1755760",
    "end": "1763919"
  },
  {
    "text": "best way that I've seen companies do it is NVIDIA Triton is a really great tool",
    "start": "1763919",
    "end": "1769240"
  },
  {
    "text": "the pipelining and like how being able to feed a python model's result to a C++",
    "start": "1769240",
    "end": "1775519"
  },
  {
    "text": "quantized pytorch model and whatnot if you're really going to go down the route of like doing it custom and whatnot",
    "start": "1775519",
    "end": "1781399"
  },
  {
    "text": "going and talking to Nvidia is never a bad idea they're probably going to love that but one of the biggest things I've",
    "start": "1781399",
    "end": "1786960"
  },
  {
    "text": "seen is that people that are doing it custom that are actually making their own models aren't talking about it a",
    "start": "1786960",
    "end": "1793440"
  },
  {
    "text": "whole lot and I think that's because it's a big source of IP a lot of these platforms and that's why people so",
    "start": "1793440",
    "end": "1799559"
  },
  {
    "text": "commonly have questions about on-prem and I do think it's a huge open market but personally if you're training models",
    "start": "1799559",
    "end": "1807880"
  },
  {
    "text": "you can use things like determined shout out Evans Sparks and HP but there's a",
    "start": "1807880",
    "end": "1813799"
  },
  {
    "text": "lot of ways to train models there's really not a lot right now of ways to",
    "start": "1813799",
    "end": "1819360"
  },
  {
    "text": "use those models in the same way that you would use open ai's API there's not",
    "start": "1819360",
    "end": "1825840"
  },
  {
    "text": "a lot of ways to say like even Triton has an HPS API but the way that you form",
    "start": "1825840",
    "end": "1832080"
  },
  {
    "text": "the thing that you send to Triton versus what you do for open AI the barrier to",
    "start": "1832080",
    "end": "1837320"
  },
  {
    "text": "entry of those two things yeah grpc and HP just so far apart yeah and so the",
    "start": "1837320",
    "end": "1844640"
  },
  {
    "text": "barrier to adoption for the API level tools is so low and the barrier to",
    "start": "1844640",
    "end": "1850720"
  },
  {
    "text": "adoption for on Prem is unbelievably high and let alone you can probably not",
    "start": "1850720",
    "end": "1855799"
  },
  {
    "text": "even get a data center GPU right now I actually saw a company recently that's actually doing this on some AMD chips I",
    "start": "1855799",
    "end": "1862559"
  },
  {
    "text": "love AMD but Cuda runs the world in AI right now and if you want to run a model",
    "start": "1862559",
    "end": "1869320"
  },
  {
    "text": "on Prim you got to have a Cuda enable GPU and they're tough to get so it's it's a hard game right now on prise I",
    "start": "1869320",
    "end": "1875600"
  },
  {
    "text": "got to say they are all sold out everywhere also on the Google Cloud platform they're sold out even even on",
    "start": "1875600",
    "end": "1881919"
  },
  {
    "text": "hugging face is sometimes hard to get one Lambda is another good place I really liked their cloud UI Robert",
    "start": "1881919",
    "end": "1888360"
  },
  {
    "text": "Brooks and Co over there at Lambda are awesome so that's another good one all right thanks for your tips s andk you",
    "start": "1888360",
    "end": "1894320"
  },
  {
    "text": "very much for joining podcast of course thank you very much for listening to this podcast I hope you enjoyed the",
    "start": "1894320",
    "end": "1901200"
  },
  {
    "text": "conversation as I mentioned we will upload the talk or sarty on info.com",
    "start": "1901200",
    "end": "1906760"
  },
  {
    "text": "sometime in the future so keep an eye on that thank you again for listening and thanks again to Sam for being a",
    "start": "1906760",
    "end": "1913560"
  },
  {
    "text": "[Music] guest",
    "start": "1913560",
    "end": "1918980"
  },
  {
    "text": "[Music]",
    "start": "1918980",
    "end": "1934990"
  }
]