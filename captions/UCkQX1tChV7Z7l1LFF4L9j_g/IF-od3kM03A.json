[
  {
    "start": "0",
    "end": "135000"
  },
  {
    "text": "[Music]",
    "start": "3360",
    "end": "6550"
  },
  {
    "text": "hello everyone welcome to qcon plus 2020. i hope everyone is healthy and",
    "start": "9760",
    "end": "15599"
  },
  {
    "text": "comfortable today in my session i will introduce you to some cool data infra challenges that you only see in",
    "start": "15599",
    "end": "22160"
  },
  {
    "text": "iot space let's start with a small problem that you might relate to tesla designed a supercharger network to",
    "start": "22160",
    "end": "28960"
  },
  {
    "text": "enable a seamless enjoyable road trip experience it can be quite frustrating to drive to a supercharger",
    "start": "28960",
    "end": "35520"
  },
  {
    "text": "only to notice that it's down so we provide real-time visibility into superchargers",
    "start": "35520",
    "end": "40960"
  },
  {
    "text": "health status we receive telemetry from our superchargers that let us know",
    "start": "40960",
    "end": "46239"
  },
  {
    "text": "when a post is occupied and various other stats using this data we can assess the health",
    "start": "46239",
    "end": "53039"
  },
  {
    "text": "of a supercharger now on the surface it's a tractable problem but there are",
    "start": "53039",
    "end": "58800"
  },
  {
    "text": "many edge cases which make it noble some issues such as a broken connector",
    "start": "58800",
    "end": "64478"
  },
  {
    "text": "or an obstructed parking stall will prevent the vehicle from charging through that supercharger post",
    "start": "64479",
    "end": "71280"
  },
  {
    "text": "and in some cases the supercharger may be perfectly healthy but due to extreme weather or some other",
    "start": "71280",
    "end": "77360"
  },
  {
    "text": "reason it might lose connectivity with the cloud and stop sending telemetry so the down",
    "start": "77360",
    "end": "83759"
  },
  {
    "text": "detection algorithm the algorithm which figures out where to route a user to has to employ many heuristics",
    "start": "83759",
    "end": "91920"
  },
  {
    "text": "it has to not only be ready for the superchargers which are sending data in",
    "start": "91920",
    "end": "97439"
  },
  {
    "text": "a perfect way but it has to also account for the fact that data can be missing many times while any algorithm which",
    "start": "97439",
    "end": "104479"
  },
  {
    "text": "depends on data has to be prepared for missing data but in case of iot it's not a rare event it's actually",
    "start": "104479",
    "end": "111040"
  },
  {
    "text": "quite common it's problem like these which make iot",
    "start": "111040",
    "end": "116640"
  },
  {
    "text": "data space very unique to work with my name is srijeet i am part of core data platform team at",
    "start": "116640",
    "end": "122719"
  },
  {
    "text": "tesla my team is responsible for handling all mission critical data systems at tesla",
    "start": "122719",
    "end": "128879"
  },
  {
    "text": "and today i am here to tell you more about these unique challenges and some unique solutions",
    "start": "128879",
    "end": "135599"
  },
  {
    "start": "135000",
    "end": "135000"
  },
  {
    "text": "let's start with a 10 000 feet view of iot data pipeline",
    "start": "135599",
    "end": "142239"
  },
  {
    "text": "uh most of the devices at least in case of tesla run a linux operating system and",
    "start": "142239",
    "end": "148879"
  },
  {
    "text": "provide local data storage computation and control while maintaining bi-directional communication with the",
    "start": "148879",
    "end": "155840"
  },
  {
    "text": "cloud the communication flows through an edge gateway that handles",
    "start": "155840",
    "end": "161200"
  },
  {
    "text": "connectivity as well as security the edge gateway integrates with the cloud",
    "start": "161200",
    "end": "166400"
  },
  {
    "text": "which has a kafka cluster behind it to ingest large volumes of telemetry the kafka",
    "start": "166400",
    "end": "172560"
  },
  {
    "text": "based pep sub provides message durability it decouples producers of data from the",
    "start": "172560",
    "end": "178239"
  },
  {
    "text": "consumers of data and allows sharing this telemetry across many downstream services",
    "start": "178239",
    "end": "185840"
  },
  {
    "text": "at first glance the picture that we just saw it seems very similar to any other data pipeline we can imagine and the",
    "start": "186560",
    "end": "193760"
  },
  {
    "text": "very first comparison that comes to mind is click stream uh",
    "start": "193760",
    "end": "198959"
  },
  {
    "text": "the there that we collect from ad networks the googles and the facebooks of the world",
    "start": "198959",
    "end": "204159"
  },
  {
    "text": "and we have come a long way in defining elegant design patterns for dealing with massive scale click stream data",
    "start": "204159",
    "end": "211440"
  },
  {
    "text": "that we observe at conventional internet systems these design patterns work really well",
    "start": "211440",
    "end": "217120"
  },
  {
    "text": "because the underlying data producer is stable and it is built with a common",
    "start": "217120",
    "end": "222959"
  },
  {
    "text": "standard set of assumptions you expect the network to be always reliable you",
    "start": "222959",
    "end": "228319"
  },
  {
    "text": "expect the traffic patterns to be more or less the same and even if they change even if there is",
    "start": "228319",
    "end": "233840"
  },
  {
    "text": "a seasonality after a period of time even that can be predicted",
    "start": "233840",
    "end": "238959"
  },
  {
    "text": "essentially you trust the upstream to always behave well and the way you make sure that they",
    "start": "238959",
    "end": "245519"
  },
  {
    "text": "behave well is putting multiple layers of redundancy a conventional software service is",
    "start": "245519",
    "end": "252000"
  },
  {
    "text": "deployed in data center which is responsible to make sure this redundancy is",
    "start": "252000",
    "end": "257359"
  },
  {
    "text": "there in contrast for the most part iot devices are",
    "start": "257359",
    "end": "263520"
  },
  {
    "text": "installed in a very hostile environment in iot data acquisition can go partially",
    "start": "263520",
    "end": "269600"
  },
  {
    "text": "dark for legitimate reasons and thus knowing whether it's a real problem",
    "start": "269600",
    "end": "275040"
  },
  {
    "text": "is very subjective iot devices generally suffer from lack of resources power storage etc",
    "start": "275040",
    "end": "283440"
  },
  {
    "text": "their computational and storage capabilities do not allow complex operations such as sanitization",
    "start": "283440",
    "end": "290080"
  },
  {
    "text": "putting a standard format deduplication and things like that and finally like",
    "start": "290080",
    "end": "295840"
  },
  {
    "text": "i've been repeating intermittent loss of connection in case of iot with the cloud is relatively frequent",
    "start": "295840",
    "end": "305199"
  },
  {
    "start": "304000",
    "end": "304000"
  },
  {
    "text": "iot devices are also incredibly heterogeneous regarding basic communication protocols",
    "start": "305600",
    "end": "311759"
  },
  {
    "text": "data formats and technologies furthermore in case of iot if we divide data into",
    "start": "311759",
    "end": "318320"
  },
  {
    "text": "categories one is diagnostic data and the other category is something that is critical for the",
    "start": "318320",
    "end": "324000"
  },
  {
    "text": "functionality of the device the diagnostic data is always collected in",
    "start": "324000",
    "end": "329759"
  },
  {
    "text": "best effort mode and devices will choose to stop sending data",
    "start": "329759",
    "end": "336720"
  },
  {
    "text": "if it comes in the way of providing critical functionality for obvious reasons",
    "start": "336720",
    "end": "343199"
  },
  {
    "text": "with its unique challenges iot data engineering has modest overlap with conventional approaches",
    "start": "343199",
    "end": "349360"
  },
  {
    "text": "over the years we have developed a few targeted patterns to undertake these challenges",
    "start": "349360",
    "end": "354479"
  },
  {
    "text": "let's look into some of those patterns the very first design pattern we use is",
    "start": "354479",
    "end": "360080"
  },
  {
    "start": "358000",
    "end": "358000"
  },
  {
    "text": "isolation we can find data pipelines along multiple dimensions the obvious dimensions are business",
    "start": "360080",
    "end": "366400"
  },
  {
    "text": "units in case of tesla we have end-to-end data flow for every",
    "start": "366400",
    "end": "371759"
  },
  {
    "text": "uh business unit uh the underlying infrastructure on which these data flows run that might",
    "start": "371759",
    "end": "378319"
  },
  {
    "text": "be shared for example you might share kaffir cluster we might share kubernetes setup we might even share some databases",
    "start": "378319",
    "end": "386240"
  },
  {
    "text": "at infrastructure level but at application level they are completely isolated the next",
    "start": "386240",
    "end": "393120"
  },
  {
    "text": "dimension the obvious dimension to uh divide after business using unit is uh products",
    "start": "393120",
    "end": "398720"
  },
  {
    "text": "if we have multiple uh product models for example if you have multiple models of vehicles",
    "start": "398720",
    "end": "404080"
  },
  {
    "text": "they all will have their own end-to-end data flows will not mix data from model a with model b the next",
    "start": "404080",
    "end": "412160"
  },
  {
    "text": "dimension at which we isolate is geography and those were some of the obvious dimensions and then there are some",
    "start": "412160",
    "end": "418240"
  },
  {
    "text": "interesting dimensions such as transmission modes devices collect data periodically",
    "start": "418240",
    "end": "423440"
  },
  {
    "text": "and that's an obvious transmission mode where they're periodically sending some payload to the cloud and",
    "start": "423440",
    "end": "429440"
  },
  {
    "text": "there are cases when you would want out-of-band collection of data for example a service technician wanting to know more about your device",
    "start": "429440",
    "end": "435840"
  },
  {
    "text": "and they can request an ad hoc pull which can be more verbose so that's another place that we would not",
    "start": "435840",
    "end": "441599"
  },
  {
    "text": "want periodic collection to interfere with out of band collection while this strategy of isolating",
    "start": "441599",
    "end": "448479"
  },
  {
    "text": "pipelines and data flows across multiple dimensions allows us to limit the noise",
    "start": "448479",
    "end": "454160"
  },
  {
    "text": "and allows us to reduce the problems of noisy neighbors",
    "start": "454160",
    "end": "459199"
  },
  {
    "text": "and subsequently improve the data quality the downside is that we have too many",
    "start": "459199",
    "end": "465039"
  },
  {
    "text": "data sets and too many data pipelines to manage the next commonly used pattern is",
    "start": "465039",
    "end": "470639"
  },
  {
    "start": "469000",
    "end": "469000"
  },
  {
    "text": "deferred complexity as noted due to constrained resources the devices cannot handle",
    "start": "470639",
    "end": "476720"
  },
  {
    "text": "operations such as standardization putting some standard format i keep saying standard format because as",
    "start": "476720",
    "end": "483759"
  },
  {
    "text": "noted the data in many cases is proprietary deduplication is another thing that",
    "start": "483759",
    "end": "490319"
  },
  {
    "text": "devices will never perform they will always differ all these complex operations to the cloud",
    "start": "490319",
    "end": "496879"
  },
  {
    "text": "and the device is always operating in a mode where it can push as much data as possible as fast as",
    "start": "496879",
    "end": "503120"
  },
  {
    "text": "it can and in the tiniest possible payload that it can manage to send that data",
    "start": "503120",
    "end": "508560"
  },
  {
    "text": "now on the cloud side on the cloud platform side to restrain to contain the the heterogeneity and bustiness and",
    "start": "508560",
    "end": "515440"
  },
  {
    "text": "noisy noise of data we try to uh explode",
    "start": "515440",
    "end": "520800"
  },
  {
    "text": "the payloads that we receive which are typically batch payloads we get telemetry in batches we try to",
    "start": "520800",
    "end": "527519"
  },
  {
    "text": "explode regardless of which business unit or product this data is coming from we try to employ some",
    "start": "527519",
    "end": "533040"
  },
  {
    "text": "common strategies so that we can put sanitization",
    "start": "533040",
    "end": "537920"
  },
  {
    "text": "common standard formats and some other standard cleaning steps in the first",
    "start": "538160",
    "end": "545440"
  },
  {
    "text": "step of data collection itself and this this slide is a demonstration of that as you zoom in",
    "start": "545440",
    "end": "551200"
  },
  {
    "text": "into uh the kubernetes uh magical black box that you're seeing there",
    "start": "551200",
    "end": "556480"
  },
  {
    "text": "that is running some real-time data pipelines that we have developed over a framework",
    "start": "556480",
    "end": "561519"
  },
  {
    "text": "called akka uh we call this whole product kafka channels and in there we",
    "start": "561519",
    "end": "566720"
  },
  {
    "text": "uh apply these complex operations as the first thing as we receive these payloads",
    "start": "566720",
    "end": "571920"
  },
  {
    "text": "when you zoom in further in in any given uh sanitization or uh cleaning uh data flow",
    "start": "571920",
    "end": "581120"
  },
  {
    "text": "subflow you will find more sub flows and this pattern results in data processing broken down into",
    "start": "581120",
    "end": "586399"
  },
  {
    "text": "multiple intermediate data sets and sub pipelines on the cloud platform and so we already had",
    "start": "586399",
    "end": "592959"
  },
  {
    "text": "uh too many datasets and too many pipelines to manage because of the isolation that we were doing and now",
    "start": "592959",
    "end": "598560"
  },
  {
    "text": "due to uh this uh breaking down data flow into subflows is",
    "start": "598560",
    "end": "604160"
  },
  {
    "text": "increasing this uh data sets and data pipelines even further",
    "start": "604160",
    "end": "610399"
  },
  {
    "text": "our design choices and tooling has enabled us to digest the explosive growth that we have seen",
    "start": "610399",
    "end": "616720"
  },
  {
    "text": "and the patterns that i just described were were instrumental in making sure that we are able to keep up with the",
    "start": "616720",
    "end": "622240"
  },
  {
    "text": "growth but the ease with which we can set up a new pipeline today",
    "start": "622240",
    "end": "627760"
  },
  {
    "text": "was biting at the back by making making it next to impossible to keep a clean model in our head about",
    "start": "627760",
    "end": "634320"
  },
  {
    "text": "data pipelines and we had a fresh set of challenges around operating",
    "start": "634320",
    "end": "639360"
  },
  {
    "text": "these pipelines and how visible the health of these pipelines",
    "start": "639360",
    "end": "644640"
  },
  {
    "text": "were so let's look into those fresh set of challenges as next thing",
    "start": "644640",
    "end": "650560"
  },
  {
    "text": "uh with the with the explosive growth uh data sets under management kept growing like i said uh and the",
    "start": "650959",
    "end": "656720"
  },
  {
    "text": "common way to organize that information is documentation but the kind of growth we were seeing",
    "start": "656720",
    "end": "662880"
  },
  {
    "text": "documentation was not keeping up the other common thing that you see in any data platform in any company",
    "start": "662880",
    "end": "670240"
  },
  {
    "text": "is any data org is you you have data heroes there are champions which are your go-to guys for uh",
    "start": "670240",
    "end": "677600"
  },
  {
    "text": "questions when you cannot figure out where the data is or uh where a particular piece of information is and",
    "start": "677600",
    "end": "683519"
  },
  {
    "text": "we also had them but obviously that's bad that doesn't scale beyond the number of data sets and and",
    "start": "683519",
    "end": "690640"
  },
  {
    "text": "the isolation pattern that we described it was actually worsening the problem further",
    "start": "690640",
    "end": "697279"
  },
  {
    "text": "for instance a device has many sensors and some of those sensors may be perfectly fine and there's uh",
    "start": "697279",
    "end": "703680"
  },
  {
    "text": "sending data we are collecting data end-to-end and without any problem but there could be some uh sensors or",
    "start": "703680",
    "end": "709920"
  },
  {
    "text": "there's there could be a subset of sensors which have stopped sending data and that is also not a real scenario",
    "start": "709920",
    "end": "717279"
  },
  {
    "text": "so in the context of data discovery a user cannot just ask where is the data for",
    "start": "717279",
    "end": "723680"
  },
  {
    "text": "device x or where is the data for a certain category of devices",
    "start": "723680",
    "end": "729360"
  },
  {
    "text": "a more pointed question a more common question is where is the data for device x of",
    "start": "729360",
    "end": "736320"
  },
  {
    "text": "kind y and then you throw a dimension of region for instance where is the data",
    "start": "736320",
    "end": "742720"
  },
  {
    "text": "for device x of kind y from reason z and then you through more dimensions and",
    "start": "742720",
    "end": "748880"
  },
  {
    "text": "you can see how data discovery become gets more and more complicated",
    "start": "748880",
    "end": "754160"
  },
  {
    "text": "the next problem we were seeing was uh governance we required since day one uh",
    "start": "754160",
    "end": "760480"
  },
  {
    "text": "developers to apply data sets and data pipeline changes through pull requests which was great",
    "start": "760480",
    "end": "765839"
  },
  {
    "text": "and the build process could uh apply some elementary checks where the syntax is correct whether you're putting configuration at the right place or not",
    "start": "765839",
    "end": "772720"
  },
  {
    "text": "however we relied on the human the person who is looking at the code reviewing the code to spot data",
    "start": "772720",
    "end": "779040"
  },
  {
    "text": "governance gaps such as regulatory requirements data security concerns",
    "start": "779040",
    "end": "784480"
  },
  {
    "text": "uh naming conventions etc over time such processes at toil and they reduced",
    "start": "784480",
    "end": "790480"
  },
  {
    "text": "to male formality what we wanted was we wanted the bill to do all these semantic checks as well",
    "start": "790480",
    "end": "798480"
  },
  {
    "start": "798000",
    "end": "798000"
  },
  {
    "text": "uh next uncertainty erroneousness and noisiness is given with iot data",
    "start": "799360",
    "end": "805519"
  },
  {
    "text": "we never expect hundred percent coverage at any given point of time like i said um a few few seconds ago",
    "start": "805519",
    "end": "813600"
  },
  {
    "text": "a device sensor for the same device some sensors could be sending data while others might fail",
    "start": "813600",
    "end": "819040"
  },
  {
    "text": "so the coverage tells you that you might have some data available",
    "start": "819040",
    "end": "824160"
  },
  {
    "text": "but is it 100 or not is what you're trying to get from the coverage question and we had some uh handcrafted uh",
    "start": "824160",
    "end": "831680"
  },
  {
    "text": "metrics that would give you a sense of data quality data coverage for some products but we didn't",
    "start": "831680",
    "end": "837199"
  },
  {
    "text": "have anything that generalizes well across all business unit business units and products",
    "start": "837199",
    "end": "842959"
  },
  {
    "text": "and that was very important over period of time that we can confidently uh give a assessment of what",
    "start": "842959",
    "end": "850399"
  },
  {
    "text": "is the coverage of our data sets for a real-time data",
    "start": "850399",
    "end": "856880"
  },
  {
    "start": "854000",
    "end": "854000"
  },
  {
    "text": "pipeline or even batch pipeline data freshness represents uh",
    "start": "856880",
    "end": "862399"
  },
  {
    "text": "the most important service quality metric that you can have and for as long as i can remember",
    "start": "862399",
    "end": "867839"
  },
  {
    "text": "freshness was the single metric that we alerted on",
    "start": "867839",
    "end": "873279"
  },
  {
    "text": "most accurately it is simple yet effective for all parties involved data producers understand it data consumers",
    "start": "873279",
    "end": "880000"
  },
  {
    "text": "and understand it data scientists understand it everyone understands it and it's it's after period flying once you",
    "start": "880000",
    "end": "886240"
  },
  {
    "text": "figure out the nuances it's actually pretty easy to calculate and we had some great tooling to calculate latency for individual steps",
    "start": "886240",
    "end": "894160"
  },
  {
    "text": "however for someone at the far end of the spectrum like a data scientist for them the data flow starts at the",
    "start": "894160",
    "end": "901040"
  },
  {
    "text": "device and it ends at some data set that they want to query anything in between is",
    "start": "901040",
    "end": "906240"
  },
  {
    "text": "implementation detail for them they do not care how long it took for any sub step what they care about is what was the end",
    "start": "906240",
    "end": "912480"
  },
  {
    "text": "to end latency and we didn't have a way to expose that so uh something that we were",
    "start": "912480",
    "end": "918000"
  },
  {
    "text": "really proud of we thought our service quality indicator in terms of freshness are great they were actually not that great",
    "start": "918000",
    "end": "924959"
  },
  {
    "text": "so we covered some problems we covered uh problems related to coverage uh we could we covered the problem",
    "start": "925519",
    "end": "931759"
  },
  {
    "text": "related to freshness we didn't have a good story for governance we didn't have a good story for data discovery",
    "start": "931759",
    "end": "938320"
  },
  {
    "text": "we decided to solve this problem through a tool chain that we now call data flow and my next uh section is going to cover",
    "start": "938320",
    "end": "945519"
  },
  {
    "text": "our solution data discovery is not a new problem you might have that in your team",
    "start": "945519",
    "end": "951920"
  },
  {
    "text": "and you might be using some existing uh tool or infrastructure to solve data discovery problem",
    "start": "951920",
    "end": "958240"
  },
  {
    "text": "um the way you solve like the most basic solution for this data discovery problem",
    "start": "958240",
    "end": "963680"
  },
  {
    "text": "is to have catalog where data sets are tagged with some kind of labels and when you think about tag you have",
    "start": "963680",
    "end": "970480"
  },
  {
    "text": "two choices you can have free text tags where a user can go to some ui or through some",
    "start": "970480",
    "end": "976399"
  },
  {
    "text": "configuration and they can apply some any any possible any logical tag or any",
    "start": "976399",
    "end": "982079"
  },
  {
    "text": "combination of tags that you can imagine that is free text text and the second choice that you",
    "start": "982079",
    "end": "987440"
  },
  {
    "text": "have is you restrict the tags to a taxonomy we chose b we restricted the",
    "start": "987440",
    "end": "992560"
  },
  {
    "text": "allowed tags to a taxonomy that we maintain in source control and the reason we did that",
    "start": "992560",
    "end": "997680"
  },
  {
    "text": "is that that gave gave us some consistency now as a platform engineer we had little",
    "start": "997680",
    "end": "1004240"
  },
  {
    "text": "control over how owners curated these data sets beyond laying best practices we chose",
    "start": "1004240",
    "end": "1010240"
  },
  {
    "text": "not to dictate their names schemas etc as that would restrict the expression noise expressiveness of the data owner",
    "start": "1010240",
    "end": "1016800"
  },
  {
    "text": "which is not great we have an example here imagine you have",
    "start": "1016800",
    "end": "1022399"
  },
  {
    "text": "a company which is which has two business domains one is dealing with cameras there is a",
    "start": "1022399",
    "end": "1027678"
  },
  {
    "text": "thermostat business domain and there are few regions in which they operate each business unit has some",
    "start": "1027679",
    "end": "1034240"
  },
  {
    "text": "product that there are some product models that they're selling then typically in an iot uh infrastructure",
    "start": "1034240",
    "end": "1041600"
  },
  {
    "text": "you devise you divide the deployed devices into a few categories you can have customer category of",
    "start": "1041600",
    "end": "1047678"
  },
  {
    "text": "devices you can have prototype or engineering category so you can test your devices and then i",
    "start": "1047679",
    "end": "1053600"
  },
  {
    "text": "explain the concept of publishing mode or trigger mode periodic or order band that can be under",
    "start": "1053600",
    "end": "1061120"
  },
  {
    "text": "dimension at which you want to uh tag data sets at and finally you can have data types",
    "start": "1061120",
    "end": "1066400"
  },
  {
    "text": "uh so this this although this is sort of a made up example of taxonomy but the structure is pretty real that is what we",
    "start": "1066400",
    "end": "1072080"
  },
  {
    "text": "do at tesla we uh we allow business units to to have flexibility in the",
    "start": "1072080",
    "end": "1080160"
  },
  {
    "text": "the set of values that allow in certain dimension you can take you",
    "start": "1080160",
    "end": "1085600"
  },
  {
    "text": "can have any number of products you want and you choose to call the products whatever you want to call them obviously but each business",
    "start": "1085600",
    "end": "1092400"
  },
  {
    "text": "unit has to have all these dimensions they are common everyone has to have region product fleet triggers",
    "start": "1092400",
    "end": "1098240"
  },
  {
    "text": "and data types and combination of these this is naturally hierarchical if you take this uh",
    "start": "1098240",
    "end": "1103760"
  },
  {
    "text": "these files and put them into a single data structure you get a tree what is a path entry",
    "start": "1103760",
    "end": "1110400"
  },
  {
    "text": "a path in this taxonomy hierarchical taxonomy represents a label so you can take any path in the tree as",
    "start": "1110799",
    "end": "1116559"
  },
  {
    "text": "long as that's a valid path in a tree we allow that to be a valid label imagine",
    "start": "1116559",
    "end": "1121600"
  },
  {
    "text": "that there is uh there is a data set which represents data for um a certain model of camera",
    "start": "1121600",
    "end": "1129039"
  },
  {
    "text": "in the engineering fleet and data is being pulled manually in that data set and the data type in the data that is of",
    "start": "1129039",
    "end": "1135679"
  },
  {
    "text": "telemetry kind then we can use that example taxonomy that i just showed you and we can slap this label in",
    "start": "1135679",
    "end": "1141600"
  },
  {
    "text": "it can live this data set can physically rest in multiple data sets you can have some kafka topics in which this dataset",
    "start": "1141600",
    "end": "1148160"
  },
  {
    "text": "is present you can have some uh you can have a random access database in which dataset is present you can have some",
    "start": "1148160",
    "end": "1154160"
  },
  {
    "text": "batch database in which dataset is present logically they are all same and so you can slap single label uh and",
    "start": "1154160",
    "end": "1161600"
  },
  {
    "text": "that would be your logical dataset um let's take a pause here so we are",
    "start": "1161600",
    "end": "1168320"
  },
  {
    "text": "we have defined the concept of taxonomy we have defined the concept of logical data set and we are getting",
    "start": "1168320",
    "end": "1173600"
  },
  {
    "text": "towards the solution for data catalog which is great all we need is some way to",
    "start": "1173600",
    "end": "1178799"
  },
  {
    "text": "given a label we need to figure out what all data sets are covered in the dataset which is easy",
    "start": "1178799",
    "end": "1184160"
  },
  {
    "text": "right but the other problem we had was around operability how we operability and visibility how do",
    "start": "1184160",
    "end": "1193120"
  },
  {
    "text": "we uh deal with the number of pipelines that we had how do we figure out what is the end-to-end latency so we needed some",
    "start": "1193120",
    "end": "1199360"
  },
  {
    "text": "data lineage abstraction we wanted to capture the flow as data is moving from one data set to",
    "start": "1199360",
    "end": "1205200"
  },
  {
    "text": "other data sets all the steps involved in some abstraction and that is data lineage abstraction and we decided it would be",
    "start": "1205200",
    "end": "1213520"
  },
  {
    "text": "it would be prudent if you can combine data catalog and data lineage into one single abstraction and",
    "start": "1213520",
    "end": "1220000"
  },
  {
    "text": "that abstraction was graph so we came up with the concept of data flow graph",
    "start": "1220000",
    "end": "1225679"
  },
  {
    "text": "data flow for a device sensor begins at that sensor as the origin and ends at a data set in one or more",
    "start": "1225679",
    "end": "1231760"
  },
  {
    "text": "data stores as noted earlier we break down the flow into multiple intermediate data sets and",
    "start": "1231760",
    "end": "1237360"
  },
  {
    "text": "sub lows data lineage captures this flow from its source to its destination via various changes",
    "start": "1237360",
    "end": "1245039"
  },
  {
    "text": "and hops on its way how the data gets transformed along the way and how it splits converges after a",
    "start": "1245039",
    "end": "1252159"
  },
  {
    "text": "stage naturally it is best represented as a graph if it is a graph what are the vertices and edges",
    "start": "1252159",
    "end": "1259600"
  },
  {
    "start": "1259000",
    "end": "1259000"
  },
  {
    "text": "each data set a physical data set for example a kafka topic in some proprietary format or a kafka",
    "start": "1259600",
    "end": "1267440"
  },
  {
    "text": "topic for the same proprietary format in canonical format like a standard format in our case that is",
    "start": "1267440",
    "end": "1273120"
  },
  {
    "text": "abro then same data in um in let's say a time series data base and",
    "start": "1273120",
    "end": "1279679"
  },
  {
    "text": "same data set in a batch database they all become data set the vertices in this",
    "start": "1279679",
    "end": "1285679"
  },
  {
    "text": "graph and they're all labeled with uh same taxonomy label",
    "start": "1285679",
    "end": "1292080"
  },
  {
    "text": "uh i have the structure of data set in this slide it has a name it has a data store in which it lives",
    "start": "1292080",
    "end": "1297760"
  },
  {
    "text": "then it has a collection of labels and finally it has a tier the concept of tier is used in our case",
    "start": "1297760",
    "end": "1304000"
  },
  {
    "text": "to to organize our alerts uh so we have the cons we have defined",
    "start": "1304000",
    "end": "1311120"
  },
  {
    "start": "1308000",
    "end": "1308000"
  },
  {
    "text": "how we uh capture vertices let's talk about edges we represent intermediate steps",
    "start": "1311120",
    "end": "1317120"
  },
  {
    "text": "as data flow steps these steps are kafka streams or bad jobs powered by spark or",
    "start": "1317120",
    "end": "1323280"
  },
  {
    "text": "mapreduce a single data flow step can consume multiple data sets",
    "start": "1323280",
    "end": "1328880"
  },
  {
    "text": "and produce multiple data sets hence it can translate into multiple edges in the graph we",
    "start": "1328880",
    "end": "1334720"
  },
  {
    "text": "represent edge as data flow transformation so data flow transformation has the step",
    "start": "1334720",
    "end": "1341360"
  },
  {
    "text": "which which produced this transformation and then it has the tuple the source and",
    "start": "1341360",
    "end": "1346720"
  },
  {
    "text": "target data sets which are in for involving this data flow transformation",
    "start": "1346720",
    "end": "1352159"
  },
  {
    "text": "so we have defined uh what it means for vertex to be in the graph and what is an edge in this graph but how do we put",
    "start": "1352159",
    "end": "1359280"
  },
  {
    "text": "the graph together in any uh mature data platform",
    "start": "1359280",
    "end": "1365679"
  },
  {
    "text": "after a period of time you will find there are multiple frameworks involved you might have some",
    "start": "1365679",
    "end": "1371120"
  },
  {
    "text": "streaming technology you might have some batch frameworks involved and you will always also have multiple storage technologies",
    "start": "1371120",
    "end": "1377840"
  },
  {
    "text": "involved in our case we have kafka we have hbase we have hdfs and the configuration how do you",
    "start": "1377840",
    "end": "1385679"
  },
  {
    "text": "create a new topic how do you create a new table in time series database how do you create a new table in hbase",
    "start": "1385679",
    "end": "1391280"
  },
  {
    "text": "how do you create a new table in hdfs it's entirely possible for those things to live in",
    "start": "1391280",
    "end": "1397360"
  },
  {
    "text": "in various different ways you can have you can have a team which is completely",
    "start": "1397360",
    "end": "1402640"
  },
  {
    "text": "operating in cowboy mode where they they go to a terminal and just create the uh create the table there or you can",
    "start": "1402640",
    "end": "1409600"
  },
  {
    "text": "have more discipline team which has which is writing tools which can read configuration and create this",
    "start": "1409600",
    "end": "1416159"
  },
  {
    "text": "these data sets and data pipelines for you even if you have everything in the configuration file then",
    "start": "1416159",
    "end": "1422480"
  },
  {
    "text": "this information can be spread into multiple repositories and the problem with having the staggered information is that it's very",
    "start": "1422480",
    "end": "1429039"
  },
  {
    "text": "hard to create what i just described because you need to pull information from multiple sources and",
    "start": "1429039",
    "end": "1435200"
  },
  {
    "text": "put it into one place luckily in our case uh",
    "start": "1435200",
    "end": "1440880"
  },
  {
    "text": "even before we started with data flow i've said this before one of the good things that we have been",
    "start": "1440880",
    "end": "1446080"
  },
  {
    "text": "doing is every change goes through source control so we didn't have a problem of someone doing something random",
    "start": "1446080",
    "end": "1452240"
  },
  {
    "text": "directly in production and we just had one repository we use them on a repository that we",
    "start": "1452240",
    "end": "1457840"
  },
  {
    "text": "build using a build called bazel and bazel organizes build rules in a",
    "start": "1457840",
    "end": "1463840"
  },
  {
    "text": "simple configuration where you can feed in the sources uh the co the source as in the code source",
    "start": "1463840",
    "end": "1469520"
  },
  {
    "text": "and then you can kind of change the dependency graph so this example uh is is a very uh high level example of",
    "start": "1469520",
    "end": "1477120"
  },
  {
    "text": "how we actually do it so we define a file group called dataset in which we feed all the configuration files which",
    "start": "1477120",
    "end": "1484000"
  },
  {
    "text": "configure dataset into all different kind of databases that we have kafka topics hbase tables",
    "start": "1484000",
    "end": "1490320"
  },
  {
    "text": "hdfs tables and then we have a file group which captures all",
    "start": "1490320",
    "end": "1496559"
  },
  {
    "text": "the data pipelines we have the real-time data pipelines that we built that we run operate through a framework that we",
    "start": "1496559",
    "end": "1502320"
  },
  {
    "text": "built called kaffa channels then you can also have map reduce jobs and your spark jobs in this file group and then",
    "start": "1502320",
    "end": "1509039"
  },
  {
    "text": "both those are fed into the final rule which is the graph generating rule",
    "start": "1509039",
    "end": "1514880"
  },
  {
    "text": "as dependencies so if anything changes the the final rule will re-trigger and it will re-generate the graph and",
    "start": "1514880",
    "end": "1521279"
  },
  {
    "text": "bazel captures this dependency and change detection uh very cleanly",
    "start": "1521279",
    "end": "1527600"
  },
  {
    "text": "another thing to notice is we are not storing our catalog and lineage into any fancy database",
    "start": "1527600",
    "end": "1533120"
  },
  {
    "text": "even with thousands of data sets and thousands of pipelines this information when serialized as data",
    "start": "1533120",
    "end": "1539600"
  },
  {
    "text": "structures can be stored in kilobytes so what we have is a binary representation of a",
    "start": "1539600",
    "end": "1544799"
  },
  {
    "text": "catalog and data lineage that we can supply to any downstream",
    "start": "1544799",
    "end": "1550080"
  },
  {
    "text": "tool or any downstream service that wants it and it regenerates regenerates for every",
    "start": "1550080",
    "end": "1555520"
  },
  {
    "text": "pull request so that we can now apply strong semantic checks as well which was one of the problems that we",
    "start": "1555520",
    "end": "1561360"
  },
  {
    "text": "had we can verify that there is a data set a and they'll set b and they should",
    "start": "1561360",
    "end": "1566400"
  },
  {
    "text": "never be connected in any scenarios because a data set a can have some private data that we do not want to",
    "start": "1566400",
    "end": "1572960"
  },
  {
    "text": "make it to an openly accessible data set b and that check can be put into the build build stage",
    "start": "1572960",
    "end": "1580640"
  },
  {
    "text": "now taxonomy and graph give us a foundation for operating the telemetry pipelines next look at a building block which",
    "start": "1580640",
    "end": "1586559"
  },
  {
    "text": "gives us some hold on visibility and the common",
    "start": "1586559",
    "end": "1592640"
  },
  {
    "text": "gist of visibility is that you should be able to [Music] monitor the health of a complex system",
    "start": "1592640",
    "end": "1598960"
  },
  {
    "text": "by just looking into inputs and outputs of the system we described that freshness is a very",
    "start": "1598960",
    "end": "1604559"
  },
  {
    "text": "strong indicator for uh figuring out the service quality of data pipelines but their problems such as coverage and",
    "start": "1604559",
    "end": "1611600"
  },
  {
    "text": "completeness which go beyond freshness and how do we solve them [Music]",
    "start": "1611600",
    "end": "1618380"
  },
  {
    "start": "1617000",
    "end": "1617000"
  },
  {
    "text": "measuring data quality requires looking at data in aggregate and identifying patterns rather than individual events",
    "start": "1618799",
    "end": "1625120"
  },
  {
    "text": "the rate of individual events that we see is about 30 million uh events per second that that is the",
    "start": "1625120",
    "end": "1631440"
  },
  {
    "text": "peak rate that translates to trillions of events per day and if we had to go and infer data",
    "start": "1631440",
    "end": "1636799"
  },
  {
    "text": "quality it's perfectly doable but that is very expensive in terms of cost competition cost",
    "start": "1636799",
    "end": "1642720"
  },
  {
    "text": "and it is prone to lag now if we take a step back and think from first principle",
    "start": "1642720",
    "end": "1648240"
  },
  {
    "text": "what is needed to to uh to for example derive coverage for every",
    "start": "1648240",
    "end": "1654080"
  },
  {
    "text": "payload that we receive from the device if we can record the fact that we receive the payload at certain time",
    "start": "1654080",
    "end": "1660080"
  },
  {
    "text": "next if you can record if you can associate that payload with a device id and finally if you can associate and",
    "start": "1660080",
    "end": "1665760"
  },
  {
    "text": "record where we actually receive that payload with this these three pieces of information",
    "start": "1665760",
    "end": "1671760"
  },
  {
    "text": "uh we can actually do very interesting things let's let's take all of uh these things and capture that as a",
    "start": "1671760",
    "end": "1678799"
  },
  {
    "text": "matter record called trace record and that's what we do we have a concept of data flow trace",
    "start": "1678799",
    "end": "1684960"
  },
  {
    "text": "and uh a blessing in disguise is in in case of iot data always comes",
    "start": "1684960",
    "end": "1691919"
  },
  {
    "text": "in payload and the payload rate is actually not a lot maybe a billion per day when we",
    "start": "1691919",
    "end": "1698960"
  },
  {
    "text": "explode those payloads when you process those payloads that's what that's when we get into 30 million per",
    "start": "1698960",
    "end": "1704159"
  },
  {
    "text": "second kind of throughput so we record traces at raw payload level",
    "start": "1704159",
    "end": "1710000"
  },
  {
    "text": "and we store those traces every payload that moves the very first entry point",
    "start": "1710000",
    "end": "1715679"
  },
  {
    "text": "records that we receive something in this data set this time for this device and we take that and store that as a",
    "start": "1715679",
    "end": "1722559"
  },
  {
    "text": "separate meta table and by analyzing that meta table we can do very interesting things",
    "start": "1722559",
    "end": "1727600"
  },
  {
    "text": "for example coverage this is a dashboard or real dashboard which gives us a sense of coverage for",
    "start": "1727600",
    "end": "1734000"
  },
  {
    "start": "1728000",
    "end": "1728000"
  },
  {
    "text": "any given product and transmission mode for across all business unit it's a very general",
    "start": "1734000",
    "end": "1739440"
  },
  {
    "text": "solution i'll explain this dashboard pretty quickly so for every transmission mode whether",
    "start": "1739440",
    "end": "1746080"
  },
  {
    "text": "it's pseudonymous urgent or manual in our case we know the number of devices which are",
    "start": "1746080",
    "end": "1752559"
  },
  {
    "text": "expected to send data there are about 790k devices which should be sending us data then for for for a fixed period of time",
    "start": "1752559",
    "end": "1760799"
  },
  {
    "text": "daily weekly and monthly we know what devices are actually sending us data through tracing and then we once we have",
    "start": "1760799",
    "end": "1767520"
  },
  {
    "text": "these two pieces of information we can we can calculate coverage and we can set up monitoring on that",
    "start": "1767520",
    "end": "1772720"
  },
  {
    "text": "in certain cases we want coverage to be very high in some cases we want coverage to be low for example we do not want urgent",
    "start": "1772720",
    "end": "1779760"
  },
  {
    "text": "data flow to see too much activity because that means something is wrong uh next end-to-end freshness uh",
    "start": "1779760",
    "end": "1787919"
  },
  {
    "start": "1784000",
    "end": "1784000"
  },
  {
    "text": "we always had very good tools for capturing latency of individual steps in data flow",
    "start": "1787919",
    "end": "1794399"
  },
  {
    "text": "by putting all those data sets and data pipelines as vertices and edges in the graph it's",
    "start": "1794399",
    "end": "1800640"
  },
  {
    "text": "very easy to calculate end to end freshness because that reduces to a graph problem",
    "start": "1800640",
    "end": "1806159"
  },
  {
    "text": "given a daily set as of today we can figure out all the data stores in which this data lives and all the",
    "start": "1806159",
    "end": "1812480"
  },
  {
    "text": "steps which are involved in making sure that data is making there we calculate the latencies of that we",
    "start": "1812480",
    "end": "1817600"
  },
  {
    "text": "produce this neat dashboard and also output end-to-end latency so that users don't have to worry about but",
    "start": "1817600",
    "end": "1823679"
  },
  {
    "text": "in case they do they also have this [Music] that completes my talk my final message",
    "start": "1823679",
    "end": "1830480"
  },
  {
    "text": "is that iot data is quite unique in many ways and",
    "start": "1830480",
    "end": "1837919"
  },
  {
    "text": "although you can apply the patterns that we apply in conventional systems but there will be some edge cases which",
    "start": "1837919",
    "end": "1845679"
  },
  {
    "text": "require first principle based thinking and once you apply that once you apply",
    "start": "1845679",
    "end": "1851360"
  },
  {
    "text": "those once you sit back and look at the problem holistically you can you can",
    "start": "1851360",
    "end": "1857200"
  },
  {
    "text": "come up with very simple solutions which give you a very good uh operational and observability hold on these data",
    "start": "1857200",
    "end": "1864799"
  },
  {
    "text": "pipelines i hope you were able to learn something new or interesting today if you have any",
    "start": "1864799",
    "end": "1870080"
  },
  {
    "text": "questions i'd be happy to answer them if you meet thank you so much",
    "start": "1870080",
    "end": "1878320"
  }
]