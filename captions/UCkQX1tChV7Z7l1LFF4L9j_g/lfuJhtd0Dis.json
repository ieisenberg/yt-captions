[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "alright welcome looks like we have a full house who would have thought the sponsored solutions track could pack a",
    "start": "0",
    "end": "6210"
  },
  {
    "text": "room this is excellent so our goal today is spark a coding joyride and what does",
    "start": "6210",
    "end": "14160"
  },
  {
    "text": "that mean well our goal today is number one to showcase sparks ability to",
    "start": "14160",
    "end": "20820"
  },
  {
    "start": "15000",
    "end": "88000"
  },
  {
    "text": "process big data so what we're going to do is we're going to take a straightforward example we're going to",
    "start": "20820",
    "end": "27090"
  },
  {
    "text": "extract information using an RDD will just read a file and do standard rtd",
    "start": "27090",
    "end": "32790"
  },
  {
    "text": "style programming we'll talk about what an RDD is as well well then demonstrate",
    "start": "32790",
    "end": "37860"
  },
  {
    "text": "some of the higher level api's for example the data frames API which in spark 16 has substantial the performance",
    "start": "37860",
    "end": "45570"
  },
  {
    "text": "improvements over what we used to be able to do with our dd's we'll talk a little bit about that well then do some",
    "start": "45570",
    "end": "52770"
  },
  {
    "text": "visualization so we can start extracting patterns and doing exploratory data analysis will even create a simple",
    "start": "52770",
    "end": "60510"
  },
  {
    "text": "machine learning pipeline to create a model that can do predictive analytics based on our data so that we can then",
    "start": "60510",
    "end": "66570"
  },
  {
    "text": "begin predicting future outcomes based on past data points and along the way I",
    "start": "66570",
    "end": "76560"
  },
  {
    "text": "really hope to give you a sense under the hood of some of the things that makes Park ten to a hundred times faster",
    "start": "76560",
    "end": "84030"
  },
  {
    "text": "than hadoop mapreduce so that's your",
    "start": "84030",
    "end": "89310"
  },
  {
    "text": "goal for today before we get started I thought I'd introduce myself so my name is Doug Bateman I'm the director of",
    "start": "89310",
    "end": "94710"
  },
  {
    "text": "training at new circle new circle is a data bricks partner we have partnered up with data bricks to help provide their",
    "start": "94710",
    "end": "101070"
  },
  {
    "text": "entire training curriculum and catalog been working with Java back since 1995",
    "start": "101070",
    "end": "107520"
  },
  {
    "text": "so that was Java 1 point 0 if anybody remembers I always thought it was funny back then you'd see job postings for",
    "start": "107520",
    "end": "113280"
  },
  {
    "text": "five years experience with Java and of course this is Java one point 0 but I've been working as a software architect as",
    "start": "113280",
    "end": "120329"
  },
  {
    "text": "well as trainer during this period of time so that's a little bit about me",
    "start": "120329",
    "end": "125430"
  },
  {
    "text": "these are some of the projects I worked on including Microsoft Azure been working on various Android applications",
    "start": "125430",
    "end": "130800"
  },
  {
    "text": "and development and we also do training new circle on HTML spark Java and Python",
    "start": "130800",
    "end": "136440"
  },
  {
    "text": "and I work in all these areas as well but I'm more than just a professional guy I also like to have fun so given",
    "start": "136440",
    "end": "144390"
  },
  {
    "text": "that we're on the west coast here I always like to break the ice a little bit so here's picture of me sailing this",
    "start": "144390",
    "end": "149580"
  },
  {
    "text": "was up in Vancouver Canada I recently moved down here to San Francisco so one of my hobbies is sailing I also enjoy a",
    "start": "149580",
    "end": "156240"
  },
  {
    "text": "little bit of rock climbing i picked that up for my wife we're actually expecting our first child in March thank",
    "start": "156240",
    "end": "163770"
  },
  {
    "text": "you but before we go any further I wanted to just quickly pull the audience",
    "start": "163770",
    "end": "168900"
  },
  {
    "text": "because one of the things you just don't know coming into a presentation is what type of audience to expect so first of",
    "start": "168900",
    "end": "175230"
  },
  {
    "text": "all there should be how many of you guys are new to spark excellent how many of",
    "start": "175230",
    "end": "181230"
  },
  {
    "text": "you guys have used spark hands on at least little okay and how many of you",
    "start": "181230",
    "end": "186480"
  },
  {
    "text": "guys have more than one year of experience on spark excellent okay so for the audio that was almost the entire",
    "start": "186480",
    "end": "192870"
  },
  {
    "text": "room was new to spark so this is perfect and I will aim the presentation towards that so when we talk about spark the",
    "start": "192870",
    "end": "201600"
  },
  {
    "start": "198000",
    "end": "623000"
  },
  {
    "text": "first thing we want to understand is its goal a unified engine across data",
    "start": "201600",
    "end": "207630"
  },
  {
    "text": "sources workloads and environments so",
    "start": "207630",
    "end": "214950"
  },
  {
    "text": "let's take a look at that in a little bit more detail here so first of all we have a unified engine so spark or is",
    "start": "214950",
    "end": "223290"
  },
  {
    "text": "really the hub of spark and it uses this ingenious in memory of abstraction known",
    "start": "223290",
    "end": "228690"
  },
  {
    "text": "as a resilient distributed data set so though how many of you guys are familiar with to do all right so we've got a lot",
    "start": "228690",
    "end": "235980"
  },
  {
    "text": "of Hadoop guys in here excellent so in Hadoop you guys are very familiar we do a map operation we write back to disk we",
    "start": "235980",
    "end": "244440"
  },
  {
    "text": "do a reduce operation we write back to disk and typically we're using the Hadoop file system HDFS to do these",
    "start": "244440",
    "end": "251519"
  },
  {
    "text": "operations and one of the big reasons we write to disk is because you're doing a big job and you don't want to lose your",
    "start": "251519",
    "end": "258419"
  },
  {
    "text": "entire job in the event that a single machine were to fail now with the",
    "start": "258419",
    "end": "264270"
  },
  {
    "text": "resilient distributed data set sparkle or allows us to do the same type of thing but in memory and so it's a very",
    "start": "264270",
    "end": "271840"
  },
  {
    "text": "different runtime architecture and we'll look at it in just a little bit later but thats parkour now dealing with spark",
    "start": "271840",
    "end": "279970"
  },
  {
    "text": "or is very much functional programming you're doing MapReduce type operations we'd like to have a much higher level",
    "start": "279970",
    "end": "287260"
  },
  {
    "text": "API and so built on top of spark or are four primary engines first of all spark",
    "start": "287260",
    "end": "296050"
  },
  {
    "text": "sequel which has both a programming API where you express your queries using what are known as data frames or a raw",
    "start": "296050",
    "end": "302860"
  },
  {
    "text": "sequel API and this actually uses an underlying query optimizer to rewrite",
    "start": "302860",
    "end": "309610"
  },
  {
    "text": "your optimization your query to run on top of our dd's but much much faster",
    "start": "309610",
    "end": "315520"
  },
  {
    "text": "it's capable of pushing down parts of the query down to the database so that the filters are done on the underlying",
    "start": "315520",
    "end": "321700"
  },
  {
    "text": "data source or Park a file rather than in the MapReduce operation it's also",
    "start": "321700",
    "end": "327010"
  },
  {
    "text": "capable of offloading work into native memory to achieve far more memory",
    "start": "327010",
    "end": "332110"
  },
  {
    "text": "density than you get with ordinary Java objects just to give you an idea a string in Java is encoded as a Unicode",
    "start": "332110",
    "end": "340000"
  },
  {
    "text": "value utf-16 plus it's got a length field and a hash code field and overall",
    "start": "340000",
    "end": "346660"
  },
  {
    "text": "the very short string may be up to 64 bytes or longer 64 words I should say",
    "start": "346660",
    "end": "352530"
  },
  {
    "text": "whereas if you compare that to what you can do with natively in memory I could fit quite a bit information with just",
    "start": "352530",
    "end": "359530"
  },
  {
    "text": "one byte per character much more efficiently and when you're dealing with in memory operations that matters so",
    "start": "359530",
    "end": "365229"
  },
  {
    "text": "this gets optimized with native memory and performs faster we then have things",
    "start": "365229",
    "end": "371200"
  },
  {
    "text": "like spark streaming which allows us to do micro batch jobs on top of spark that",
    "start": "371200",
    "end": "376900"
  },
  {
    "text": "would allow us to actually process streaming data very much the way you might process it on something like Apache storm but with the compute power",
    "start": "376900",
    "end": "383830"
  },
  {
    "text": "of the spark engine we've got ml lib Emma lid is typically used by data",
    "start": "383830",
    "end": "389680"
  },
  {
    "text": "scientists and what it's capable of doing is training mathematical models based on very large data sets how many",
    "start": "389680",
    "end": "396580"
  },
  {
    "text": "people here have a data science background a few of you guys in here very good so this is ml lib we can do linear",
    "start": "396580",
    "end": "403420"
  },
  {
    "text": "regression we can do k-means algorithms classification machine learning is really the future of computation in many",
    "start": "403420",
    "end": "410290"
  },
  {
    "text": "ways given enough information we can learn about anything Google learns which emails you consider important we can",
    "start": "410290",
    "end": "419590"
  },
  {
    "text": "learn which websites typically associated with spam which web pages are more likely to contain the information",
    "start": "419590",
    "end": "425980"
  },
  {
    "text": "you need Facebook learns which friends are more likely to be set trendsetters",
    "start": "425980",
    "end": "432420"
  },
  {
    "text": "you can analyze networks to figure out which people are really leading the",
    "start": "432420",
    "end": "439510"
  },
  {
    "text": "industry in terms of new announcements that everybody else follows and retweets and then lastly well I just kind of",
    "start": "439510",
    "end": "446950"
  },
  {
    "text": "spoiled graphics this last one where I talked about analyzing these types of networks is the engine graphics which is",
    "start": "446950",
    "end": "453100"
  },
  {
    "text": "really vertice edge databases so these are the four engines on top of spark that are built on top of spark or which",
    "start": "453100",
    "end": "460150"
  },
  {
    "text": "is basically a much faster compute model than what you get with spark MapReduce those of you how many of you have worked",
    "start": "460150",
    "end": "465580"
  },
  {
    "text": "with hive so you may be slightly familiar with the capabilities of sparks equal sparks equal in fact uses the hive",
    "start": "465580",
    "end": "472450"
  },
  {
    "text": "query language but it uses the spark runtime and it's capable of connecting to your existing hype tables so built on",
    "start": "472450",
    "end": "481240"
  },
  {
    "text": "top of spark or we have both the rdd API that's that resilient distributed data",
    "start": "481240",
    "end": "486760"
  },
  {
    "text": "set that dysfunctional programming and what it's known as the data frames API",
    "start": "486760",
    "end": "492040"
  },
  {
    "text": "which is a much easier and simple API for processing big data and I'm going to",
    "start": "492040",
    "end": "497620"
  },
  {
    "text": "showcase both of those today and then I talk about spar expanding many data",
    "start": "497620",
    "end": "503440"
  },
  {
    "text": "sources so spark can pull its data from postgres SQL or mysql are really any",
    "start": "503440",
    "end": "509500"
  },
  {
    "text": "data source that offers a JDBC connector we can also pull our data in for",
    "start": "509500",
    "end": "515140"
  },
  {
    "text": "Cassandra or HBase Hadoop HDFS amazon",
    "start": "515140",
    "end": "520930"
  },
  {
    "text": "ec2 we can read in JSON files we can use elasta search parquet files which is a",
    "start": "520930",
    "end": "527410"
  },
  {
    "text": "very efficient Hadoop based file format for reading database tables",
    "start": "527410",
    "end": "534769"
  },
  {
    "text": "so we can connect to a whole ecosystem of different data sources process it on spark or and the beauty here is that we",
    "start": "534769",
    "end": "542269"
  },
  {
    "text": "could then deploy this into many different types of environments so at data bricks we like to use Amazon ec2 so",
    "start": "542269",
    "end": "549170"
  },
  {
    "text": "that you can quickly launch your application and say you know what I need a hundred computers but i only need them",
    "start": "549170",
    "end": "555559"
  },
  {
    "text": "right now and you know what i want to run on spark 16 I don't want to wait six",
    "start": "555559",
    "end": "561499"
  },
  {
    "text": "to seven months for my vendor to update to 16 and then have my IT team install",
    "start": "561499",
    "end": "566869"
  },
  {
    "text": "it because 16 has all this off memory heap stuff so basically by running",
    "start": "566869",
    "end": "572959"
  },
  {
    "text": "inside of the cloud the idea data bricks is that you can always be using the latest and greatest spark and still",
    "start": "572959",
    "end": "578089"
  },
  {
    "text": "connect to any of your data sources but the engine runs in the cloud you quickly bring up the machines when you need them",
    "start": "578089",
    "end": "584299"
  },
  {
    "text": "you shut them down when you don't none of this I think I need 100 machines and I need to order them and then have IG",
    "start": "584299",
    "end": "590660"
  },
  {
    "text": "set them up before I've done any type of benchmarking now just to give you an",
    "start": "590660",
    "end": "596509"
  },
  {
    "text": "idea of the success of spark spark is a hundred percent open-source Apache project and it's used in production",
    "start": "596509",
    "end": "602329"
  },
  {
    "text": "today in over 500 organizations from fig fortune 100 really small shops and this",
    "start": "602329",
    "end": "609019"
  },
  {
    "text": "is just a list of some of the companies that are using apache spark today and the list is grogs if you're wondering is",
    "start": "609019",
    "end": "614899"
  },
  {
    "text": "spark real is it going to go the distance the answer is absolutely it already is so just to give you an idea",
    "start": "614899",
    "end": "624889"
  },
  {
    "start": "623000",
    "end": "711000"
  },
  {
    "text": "about the activity going on commits in the past year hadoop mapreduce is fairly",
    "start": "624889",
    "end": "629929"
  },
  {
    "text": "mature not that much change to core hadoop mapreduce yarn which is a big",
    "start": "629929",
    "end": "636170"
  },
  {
    "text": "scheduling application that allows us to basically manage resources in a cluster quite a bit of work done hdfs sparked",
    "start": "636170",
    "end": "643939"
  },
  {
    "text": "very much embraces a embraces HDFS HDFS is not going away anytime soon let me",
    "start": "643939",
    "end": "650389"
  },
  {
    "text": "have a patchy storm which is a big streaming engine we look on top of all of that the activity going on spark is",
    "start": "650389",
    "end": "657379"
  },
  {
    "text": "very much an active project to give you an idea of scale the largest publicly",
    "start": "657379",
    "end": "664339"
  },
  {
    "text": "acknowledged cluster in the world is 10 in China with over 8,000 notes the",
    "start": "664339",
    "end": "670820"
  },
  {
    "text": "largest publicly acknowledged single job is alibaba which it processes a petabyte",
    "start": "670820",
    "end": "676670"
  },
  {
    "text": "of data and as you see later on spark off data bricks running spark awesome",
    "start": "676670",
    "end": "682580"
  },
  {
    "text": "one the terabyte sort competition and after they finish clobbering Hadoop and the terabytes or competition went on to",
    "start": "682580",
    "end": "689060"
  },
  {
    "text": "process a petabyte of data just for fun",
    "start": "689060",
    "end": "693010"
  },
  {
    "text": "it's also got very high streaming throughput so a terabyte of data per",
    "start": "694720",
    "end": "699770"
  },
  {
    "text": "hour at Gina our janila farm and as I",
    "start": "699770",
    "end": "705260"
  },
  {
    "text": "mentioned in 2004 it got the world record for sorting a hundred terabytes of data so let's look at that world",
    "start": "705260",
    "end": "712820"
  },
  {
    "start": "711000",
    "end": "769000"
  },
  {
    "text": "record here so 2013 Hadoop has the world record for sorting 100 terabytes of data",
    "start": "712820",
    "end": "719170"
  },
  {
    "text": "they create a custom cluster of over 2,100 machines and they're able to very",
    "start": "719170",
    "end": "726260"
  },
  {
    "text": "impressive sort 100 terabytes of data in 72 minutes 2014 spark comes along with",
    "start": "726260",
    "end": "737270"
  },
  {
    "text": "only 200 machines running inside of Amazon ec2 spark sorts the same amount",
    "start": "737270",
    "end": "744800"
  },
  {
    "text": "of data in 23 minutes and then goes on to sort a terabyte of data in 10 times",
    "start": "744800",
    "end": "751850"
  },
  {
    "text": "at two hundred and thirty minutes for a petabyte of data I'm sorry a petabyte of data in two hundred and thirty minutes",
    "start": "751850",
    "end": "758899"
  },
  {
    "text": "same resources truly impressive results in terms of scalability and throughput",
    "start": "758899",
    "end": "764240"
  },
  {
    "text": "spark is saturating the capabilities of the underlying hardware now how does",
    "start": "764240",
    "end": "770209"
  },
  {
    "start": "769000",
    "end": "848000"
  },
  {
    "text": "spark really work let's look at some of the terminology here so when you start getting used to spark and you start",
    "start": "770209",
    "end": "775760"
  },
  {
    "text": "working with spark the first thing you want to be aware of is when you launch your application you're launching a driver can either be a shell which will",
    "start": "775760",
    "end": "782600"
  },
  {
    "text": "be using today or it can actually be a standalone program and that driver will",
    "start": "782600",
    "end": "788089"
  },
  {
    "text": "reach out to whatever resource manager you're using it can be yarn to acquire",
    "start": "788089",
    "end": "793100"
  },
  {
    "text": "several machines and they can be executives and this or these will become our executives what's going to happen in",
    "start": "793100",
    "end": "800070"
  },
  {
    "text": "the drivers going to break up our work and farm it out to these executor machines now this is very different than",
    "start": "800070",
    "end": "807030"
  },
  {
    "text": "what you saw on Hadoop because these executives are going to be living for the lifetime of our application which",
    "start": "807030",
    "end": "812910"
  },
  {
    "text": "allows us to cash our data in memory so",
    "start": "812910",
    "end": "819510"
  },
  {
    "text": "we'll farm out these individual tasks that'll be processing what we call partitions of data on each of these",
    "start": "819510",
    "end": "825990"
  },
  {
    "text": "outlier machines so the executor zuv the workhorse the driver is the director it",
    "start": "825990",
    "end": "831960"
  },
  {
    "text": "drives the process it is our application and for the duration of the application",
    "start": "831960",
    "end": "837570"
  },
  {
    "text": "which may be many jobs these executives will remain up which is very different",
    "start": "837570",
    "end": "843120"
  },
  {
    "text": "than what you would see with a dupe where the processes or short lid for an individual operation now to get us going",
    "start": "843120",
    "end": "850530"
  },
  {
    "text": "here let me make sure Oh before I get into my demo and one of the energies one",
    "start": "850530",
    "end": "856020"
  },
  {
    "text": "of the key abstractions so I'm going to pull up just some of our course we're from our spark training new circle and",
    "start": "856020",
    "end": "862710"
  },
  {
    "text": "data breaks so we talked about this idea of spark or really being our DD zor",
    "start": "862710",
    "end": "870060"
  },
  {
    "text": "resilient distributed data sets so let's look at how those compute model works and why it beats Hadoop inter- MapReduce",
    "start": "870060",
    "end": "877920"
  },
  {
    "text": "civically in terms of performance so let's say I've got my data in HDFS in",
    "start": "877920",
    "end": "883880"
  },
  {
    "text": "spark what I'm going to do is first of all read that data into what we call a",
    "start": "883880",
    "end": "889920"
  },
  {
    "text": "resilient distributed data set these will be partitioned out different executives will page in the partitions",
    "start": "889920",
    "end": "896070"
  },
  {
    "text": "that they need to do their job but initially when I define this our DD",
    "start": "896070",
    "end": "901400"
  },
  {
    "text": "nothing gets loaded immediately it's done lazily so you'll see this dotted line nothing's in memory just yet I then",
    "start": "901400",
    "end": "909690"
  },
  {
    "text": "start defining my computing processing pipeline so for example I might do a",
    "start": "909690",
    "end": "915960"
  },
  {
    "text": "filter transformation so here we have a log file log data and I just want to",
    "start": "915960",
    "end": "921090"
  },
  {
    "text": "filter out the errors those of you who have worked with a dupe are going to be familiar with this typical map operation",
    "start": "921090",
    "end": "926790"
  },
  {
    "text": "or in this case it's the filter operation I'm just filtering out only error messages now",
    "start": "926790",
    "end": "933130"
  },
  {
    "text": "you see this little Optimus Prime that's a transformer we're doing a transformation okay once we've done a",
    "start": "933130",
    "end": "945100"
  },
  {
    "text": "filter operation notice that we have a lot less data we may wish to coalesce",
    "start": "945100",
    "end": "950230"
  },
  {
    "text": "our data into smaller partitions so that",
    "start": "950230",
    "end": "955360"
  },
  {
    "text": "our jobs don't finish so far our individual tasks don't finish so fast the more partitions the more tasks this",
    "start": "955360",
    "end": "962980"
  },
  {
    "text": "is a good thing it's more parallelism but if you have too many partitions and your individual tasks are finishing in a",
    "start": "962980",
    "end": "969940"
  },
  {
    "text": "matter of milliseconds at that point your scheduling delay can become significant so generally speaking more",
    "start": "969940",
    "end": "975790"
  },
  {
    "text": "partitions as more parallelism as better until your tasks are so fast that the",
    "start": "975790",
    "end": "982650"
  },
  {
    "text": "overhead starts to become significant so you want your tasks to finish half a",
    "start": "982650",
    "end": "988480"
  },
  {
    "text": "second rather than a few milliseconds so we might reduce the total number of partitions with a coalesce operation and",
    "start": "988480",
    "end": "995370"
  },
  {
    "text": "then at some point we call what's known as an action how many of you guys are",
    "start": "995370",
    "end": "1000480"
  },
  {
    "text": "familiar with the streaming API in Java 8 so you guys are familiar you're building up a pipeline but nothing runs",
    "start": "1000480",
    "end": "1007860"
  },
  {
    "text": "until you actually call an action a terminal operation and those Java 8 pipelines we're doing the same thing",
    "start": "1007860",
    "end": "1015090"
  },
  {
    "text": "here in spark we've really created a stream of operations but nothing is going to run until we call this terminal",
    "start": "1015090",
    "end": "1021210"
  },
  {
    "text": "operation here called an action and what the collect operation says is take those",
    "start": "1021210",
    "end": "1027720"
  },
  {
    "text": "results bring them back into memory in the driver this is one possible action",
    "start": "1027720",
    "end": "1035280"
  },
  {
    "text": "other actions might be save it to disk in HDFS but bring it back in the memory",
    "start": "1035280",
    "end": "1041640"
  },
  {
    "text": "in the driver now let me ask you if I had six terabytes of data in memory my",
    "start": "1041640",
    "end": "1047550"
  },
  {
    "text": "drivers got three gigs of RAM what's going to happen I get a nice big out of",
    "start": "1047550",
    "end": "1054330"
  },
  {
    "text": "memory error this is because with collect I'm saying bring this back in as an array in memory so typically you're",
    "start": "1054330",
    "end": "1063840"
  },
  {
    "text": "only going to call collect if the results are small otherwise you want to to a file somewhere or continue to do",
    "start": "1063840",
    "end": "1069780"
  },
  {
    "text": "other operations sum up the values do some type of reduce operation et cetera",
    "start": "1069780",
    "end": "1075770"
  },
  {
    "text": "now let's look at how this thing runs when I call an action only then does",
    "start": "1075770",
    "end": "1083220"
  },
  {
    "text": "anything run and if you start reading the documentation for spark you'll hear",
    "start": "1083220",
    "end": "1088440"
  },
  {
    "text": "this phrase lineage or dag you execute the dag or materialize the rtd what they",
    "start": "1088440",
    "end": "1096660"
  },
  {
    "text": "mean is this pipeline that we've been defining which forms a directed acyclic graph in other words everything flows",
    "start": "1096660",
    "end": "1104130"
  },
  {
    "text": "downstream there no circles and you run",
    "start": "1104130",
    "end": "1109650"
  },
  {
    "text": "it to get the result so it's going to execute the dag when we call an action",
    "start": "1109650",
    "end": "1116190"
  },
  {
    "text": "and not before so at that point it's going to go out read the file do the",
    "start": "1116190",
    "end": "1123720"
  },
  {
    "text": "filter do the coalesce so here we go we're going to read the file do the",
    "start": "1123720",
    "end": "1129390"
  },
  {
    "text": "filter do the coalesce bring the data back into the driver and when we're done",
    "start": "1129390",
    "end": "1135360"
  },
  {
    "text": "it releases all that memory it's just",
    "start": "1135360",
    "end": "1142530"
  },
  {
    "text": "run a job it brought it into memory here's what's different than what to do",
    "start": "1142530",
    "end": "1149090"
  },
  {
    "text": "all of the communication between these stages is in memory nothing gets written",
    "start": "1149090",
    "end": "1157170"
  },
  {
    "text": "out to disk until I call it an action that says write to disk in this case I didn't even bother writing to disk I",
    "start": "1157170",
    "end": "1163679"
  },
  {
    "text": "said bring it back to the driver but I could say save his text file and write",
    "start": "1163679",
    "end": "1168840"
  },
  {
    "text": "it out to HDFS so all of these intermediate operations are happening in",
    "start": "1168840",
    "end": "1173970"
  },
  {
    "text": "memory and then you say but Doug what happens if the computer fails do I lose",
    "start": "1173970",
    "end": "1180210"
  },
  {
    "text": "my work well with spark what will happen is let's say the computer that's",
    "start": "1180210",
    "end": "1186030"
  },
  {
    "text": "executing this part of the pipeline here let me run a little arrow program if the",
    "start": "1186030",
    "end": "1191070"
  },
  {
    "text": "computer running this part of the pipeline we're to suddenly crash and",
    "start": "1191070",
    "end": "1197250"
  },
  {
    "text": "actually in this particular case because of the coalesce that whole thing there is going to be one task in my pipeline and the computer",
    "start": "1197250",
    "end": "1206100"
  },
  {
    "text": "running this task were to crash no big deal if the computer running this task",
    "start": "1206100",
    "end": "1212370"
  },
  {
    "text": "crashes the driver knows well this task was running on this input data set let",
    "start": "1212370",
    "end": "1219120"
  },
  {
    "text": "me go ahead and start this up on another executor in my cluster when one becomes available or maybe one executor is just",
    "start": "1219120",
    "end": "1226950"
  },
  {
    "text": "taking a really long time and spark has no idea if it's hung or not it'll speculatively re-execute that task on",
    "start": "1226950",
    "end": "1234330"
  },
  {
    "text": "another executor and wait for the first one to finish and in this way we get",
    "start": "1234330",
    "end": "1242910"
  },
  {
    "text": "resilience while still getting the high performance benefit of being in memory",
    "start": "1242910",
    "end": "1249630"
  },
  {
    "text": "and what we're done the memory is",
    "start": "1249630",
    "end": "1257700"
  },
  {
    "text": "released now you say wait a minute what if I didn't want that memory released",
    "start": "1257700",
    "end": "1263400"
  },
  {
    "text": "what if I'm going to call a count action but i also want to save it as a text",
    "start": "1263400",
    "end": "1268530"
  },
  {
    "text": "file and maybe i want to do a filter and collect that result well if I don't tell",
    "start": "1268530",
    "end": "1277500"
  },
  {
    "text": "it to cash what's going to happen is it's going to load this data and do the count release the memory load this data",
    "start": "1277500",
    "end": "1285690"
  },
  {
    "text": "save it to a text file release the memory load this data do a collect and",
    "start": "1285690",
    "end": "1292760"
  },
  {
    "text": "release the memory well clearly this is",
    "start": "1292760",
    "end": "1300300"
  },
  {
    "text": "undesirable we'd like to get a long-lived benefit in memory so we have",
    "start": "1300300",
    "end": "1306870"
  },
  {
    "text": "the option of telling it to cash by calling the cash operation to cash this",
    "start": "1306870",
    "end": "1313620"
  },
  {
    "text": "result long-term in memory so if I cash this in memory by calling the cash",
    "start": "1313620",
    "end": "1319440"
  },
  {
    "text": "operation then they get the benefit of",
    "start": "1319440",
    "end": "1324500"
  },
  {
    "text": "not even having to go back to the original data source now you want to be",
    "start": "1324500",
    "end": "1329790"
  },
  {
    "text": "careful when you cash things when you cash things in spark very very important don't cash the",
    "start": "1329790",
    "end": "1336480"
  },
  {
    "text": "original data set cash is far downstream as possible if you cash too much you",
    "start": "1336480",
    "end": "1348270"
  },
  {
    "text": "lose the benefit of cashing you really want to cash a small data set so that it",
    "start": "1348270",
    "end": "1354210"
  },
  {
    "text": "fits mostly in memory and so one of the biggest things we see when we do audits",
    "start": "1354210",
    "end": "1360930"
  },
  {
    "text": "of production systems is that they didn't cash intelligently they either cash too much or not at all and of",
    "start": "1360930",
    "end": "1369000"
  },
  {
    "text": "course it really comes down to what's the series of jobs your application wants to send and spark has no way of",
    "start": "1369000",
    "end": "1375330"
  },
  {
    "text": "knowing what future jobs are about to run so this is why you as the programmer provide that insight by calling cash the",
    "start": "1375330",
    "end": "1383160"
  },
  {
    "text": "other thing you need to remember is to uncashed it and cash is really a shortcut to persist a memory so the",
    "start": "1383160",
    "end": "1390780"
  },
  {
    "text": "opposite of cash in the API is called unpurchased gets everybody the first",
    "start": "1390780",
    "end": "1398580"
  },
  {
    "text": "time they go how do I own cash the answer is you call unprocessed now I",
    "start": "1398580",
    "end": "1404310"
  },
  {
    "text": "want to check in with my audience here how are we doing in terms of hitting the sweet spot of your interest good once",
    "start": "1404310",
    "end": "1410370"
  },
  {
    "text": "i'm coding we want to see that coding excellent so this is the picture that i",
    "start": "1410370",
    "end": "1415500"
  },
  {
    "text": "wanted to show you now that we've done that let's go ahead and do some coding",
    "start": "1415500",
    "end": "1420510"
  },
  {
    "text": "and see spark in action so we're going to do first of all hourly straightforward example based on the",
    "start": "1420510",
    "end": "1426750"
  },
  {
    "text": "audience and then later on i'll point you to a more complex example you could explore on your own but let's take a",
    "start": "1426750",
    "end": "1431760"
  },
  {
    "text": "look at this power plant demo so how many of you guys are familiar with what are called peaking power plants all",
    "start": "1431760",
    "end": "1440280"
  },
  {
    "text": "right so California in the middle of the day it's really hot and they need to",
    "start": "1440280",
    "end": "1446130"
  },
  {
    "text": "turn on some additional power plants to meet peak demand based on the temperature humidity etc they need to be",
    "start": "1446130",
    "end": "1455160"
  },
  {
    "text": "able to anticipate how many of these plans to turn on and more importantly for each plan how efficient are they",
    "start": "1455160",
    "end": "1460920"
  },
  {
    "text": "going to be because outside temperature that's really hot out the plants don't run as efficiently because they're",
    "start": "1460920",
    "end": "1466110"
  },
  {
    "text": "exhausting sure sire second law of thermodynamics so what we have is a data set we've got",
    "start": "1466110",
    "end": "1473660"
  },
  {
    "start": "1470000",
    "end": "1513000"
  },
  {
    "text": "atmosphere temperature in centigrade exhaust vacuum atmospheric pressure and",
    "start": "1473660",
    "end": "1479440"
  },
  {
    "text": "relative humidity and we've measured the output of all of these power plants",
    "start": "1479440",
    "end": "1484810"
  },
  {
    "text": "based on today we have these readings and we got this output next hour we had",
    "start": "1484810",
    "end": "1490190"
  },
  {
    "text": "these readings we got this output and what we'd like to do is to predict the",
    "start": "1490190",
    "end": "1496330"
  },
  {
    "text": "power efficiency tomorrow given our prediction for the temperature and",
    "start": "1496330",
    "end": "1501470"
  },
  {
    "text": "pressure and so forth so step one we do what's known as an extract transform and load operation step 2 we're going to",
    "start": "1501470",
    "end": "1508190"
  },
  {
    "text": "explore and visualize the data and step three we're going to do some machine learning so let's jump in now to our",
    "start": "1508190",
    "end": "1516500"
  },
  {
    "start": "1513000",
    "end": "1605000"
  },
  {
    "text": "demo so right now I am firing up data bricks oops data Brooks runs entirely in",
    "start": "1516500",
    "end": "1525020"
  },
  {
    "text": "the cloud on Amazon ec2 I come into data bricks and the first thing I want to do",
    "start": "1525020",
    "end": "1530810"
  },
  {
    "text": "is I come here to clusters and I say you know what I need a cluster to be able to",
    "start": "1530810",
    "end": "1536780"
  },
  {
    "text": "run this job so let's see here q con demo and you know what I'd like to have",
    "start": "1536780",
    "end": "1543020"
  },
  {
    "text": "eight machines and I'd like to be running spark one dot five and i'm going",
    "start": "1543020",
    "end": "1548870"
  },
  {
    "text": "to use on demand instances so that nothing gets killed in the middle of my presentation by amazon click confirm and",
    "start": "1548870",
    "end": "1559600"
  },
  {
    "text": "this is going to start launching inside of amazon ec2 i could bring up a hundred two hundred node cluster now at this",
    "start": "1559600",
    "end": "1567440"
  },
  {
    "text": "point what I'd like to do is go into my",
    "start": "1567440",
    "end": "1572600"
  },
  {
    "text": "workspace q con san francisco 2015 and what i'm going to do is just make a",
    "start": "1572600",
    "end": "1578810"
  },
  {
    "text": "clone so that you guys can still keep the original wait for that to finish",
    "start": "1578810",
    "end": "1587240"
  },
  {
    "text": "cloning there we go I'll go into the power plant I've got an example here",
    "start": "1587240",
    "end": "1593120"
  },
  {
    "text": "that I think is right for you guys with power plant there's a more elaborate one where we're processing a much larger data set in Wikipedia there click stir",
    "start": "1593120",
    "end": "1599789"
  },
  {
    "text": "data trying to predict how many clicks they're going to get and detect anomalies like big traffic on a day",
    "start": "1599789",
    "end": "1605340"
  },
  {
    "start": "1605000",
    "end": "1960000"
  },
  {
    "text": "let's go into power plan for now so step one I need to do an extract transform",
    "start": "1605340",
    "end": "1611429"
  },
  {
    "text": "and load operation so let's see here very first thing I need to do is to open",
    "start": "1611429",
    "end": "1617039"
  },
  {
    "text": "up our text file and read it so I go spark context i'd like to read a text",
    "start": "1617039",
    "end": "1622889"
  },
  {
    "text": "file please in which text file will all copy paste this path oops copy paste",
    "start": "1622889",
    "end": "1631559"
  },
  {
    "text": "this path here oh I do have text field",
    "start": "1631559",
    "end": "1639799"
  },
  {
    "text": "alright text file and we'll read in the",
    "start": "1639919",
    "end": "1647940"
  },
  {
    "text": "text file let's see how our clusters doing all right it's still launching it'll let us know when our clusters",
    "start": "1647940",
    "end": "1653729"
  },
  {
    "text": "launching its bringing up an Amazon ec2 instance and once we've read that text file the very next thing we'd like to do",
    "start": "1653729",
    "end": "1660539"
  },
  {
    "text": "is actually take a look at what's in this text file so I'm going to go raw data a raw text rdd dot take and we'll",
    "start": "1660539",
    "end": "1669899"
  },
  {
    "text": "read in just the first five rows of the file the Amazons begin a little bit slower today so we're going to keep",
    "start": "1669899",
    "end": "1675960"
  },
  {
    "text": "going and we'll come back and run it when we're ready so to give you the spoiler of what will see the first line",
    "start": "1675960",
    "end": "1682619"
  },
  {
    "text": "of the file is comments about the headers the next line of the file is a comma separated values file so what I'd",
    "start": "1682619",
    "end": "1690929"
  },
  {
    "text": "like to do is using our DD programming I would like to extract all that",
    "start": "1690929",
    "end": "1696749"
  },
  {
    "text": "information from the file into nice little Python objects that represent",
    "start": "1696749",
    "end": "1701820"
  },
  {
    "text": "each data point so what I'm going to do is I'll start with my raw text our DD",
    "start": "1701820",
    "end": "1708259"
  },
  {
    "text": "and I'd like to filter in this case I'm using the Python API we could just as",
    "start": "1708259",
    "end": "1714809"
  },
  {
    "text": "easily use the Scala API I find that for people who don't know Scala they prefer Python for the demos the wiki ml I've",
    "start": "1714809",
    "end": "1723149"
  },
  {
    "text": "got solutions in both Python and Scala for anybody would like to see them and what would I like to do well I'd like to",
    "start": "1723149",
    "end": "1730019"
  },
  {
    "text": "filter out give it a line if the line starts with a 80 atmospheric",
    "start": "1730019",
    "end": "1742059"
  },
  {
    "text": "temperature that's the header line of my file that defines the columns let's see",
    "start": "1742059",
    "end": "1747130"
  },
  {
    "text": "it still bring it up my Amazon ec2 okay so what we're going to do we're going to",
    "start": "1747130",
    "end": "1754120"
  },
  {
    "text": "filter out lines that start with atmospheric temperature and then what I'd like to do is for each line split it",
    "start": "1754120",
    "end": "1760390"
  },
  {
    "text": "up by commas turn them into numbers and place them in my Python object so I'm going to want to do a map operation",
    "start": "1760390",
    "end": "1767370"
  },
  {
    "text": "convert line to row or line to power plant row and finally that gives me back",
    "start": "1767370",
    "end": "1777850"
  },
  {
    "text": "my rtd so what I'm missing at this moment and let's go ahead and print that",
    "start": "1777850",
    "end": "1783340"
  },
  {
    "text": "raw data rdd take five now we need to make this function convert line to our",
    "start": "1783340",
    "end": "1789580"
  },
  {
    "text": "DD this is standard functional programming I'll go ahead and define it here in Python convert line to row so",
    "start": "1789580",
    "end": "1796179"
  },
  {
    "text": "give it a text line I want to return a new power plant row so lying dot split",
    "start": "1796179",
    "end": "1804210"
  },
  {
    "text": "based in this case they actually not comma separated values their tab-separated values so we're going to",
    "start": "1804210",
    "end": "1810970"
  },
  {
    "text": "split it up so this is gives me my cells and then let's see here for each cell I",
    "start": "1810970",
    "end": "1820710"
  },
  {
    "text": "want to create a power plant row object so power plant row cells 0 and I might",
    "start": "1820710",
    "end": "1828700"
  },
  {
    "text": "want to let's actually read this out happens for York temperature is as a floating point number cell 0 and so",
    "start": "1828700",
    "end": "1837400"
  },
  {
    "text": "forth where do we got vacuum pressure floating-point cells one what else",
    "start": "1837400",
    "end": "1844210"
  },
  {
    "text": "atmospheric pressure floating-point cells to relative humidity",
    "start": "1844210",
    "end": "1851290"
  },
  {
    "text": "floating-point cells three and finally the power efficiency floating-point",
    "start": "1851290",
    "end": "1856630"
  },
  {
    "text": "cells for and then I want to return a new row object return power plant row",
    "start": "1856630",
    "end": "1862330"
  },
  {
    "text": "which I define right up above atmospheric temperature vacuum pressure atmospheric pressure relative humidity",
    "start": "1862330",
    "end": "1869350"
  },
  {
    "text": "and power efficiency boy amazon is not being very friendly today not good for",
    "start": "1869350",
    "end": "1874990"
  },
  {
    "text": "live demo I should have fired this up five minutes before class I thought how cool would it be for you to see me fire",
    "start": "1874990",
    "end": "1880539"
  },
  {
    "text": "it up we're going to keep going just in the interest of time and then come back in and actually run it so what we're",
    "start": "1880539",
    "end": "1889630"
  },
  {
    "text": "doing is we're taking line to text filtering out any line that starts with",
    "start": "1889630",
    "end": "1895780"
  },
  {
    "text": "atmospheric temperature because it's a header label we're then going to take the next line split it up based on white",
    "start": "1895780",
    "end": "1902830"
  },
  {
    "text": "space convert them to numbers and throw them into a Python object standard",
    "start": "1902830",
    "end": "1909610"
  },
  {
    "text": "MapReduce functional programming for all the Hadoop guys in here right nothing unusual about this and then we might",
    "start": "1909610",
    "end": "1917500"
  },
  {
    "text": "take back the first lines and see how they look ah here we go my cluster is up",
    "start": "1917500",
    "end": "1925530"
  },
  {
    "text": "run it",
    "start": "1925530",
    "end": "1928650"
  },
  {
    "text": "first time it's got to bring up some processes nothing like a live demo this",
    "start": "1935730",
    "end": "1944830"
  },
  {
    "text": "particular file is not too huge the the wiki ml though is leave half a terabyte",
    "start": "1944830",
    "end": "1951429"
  },
  {
    "text": "and of course the entire wikipedia which we do do in our labs in our training is",
    "start": "1951429",
    "end": "1956860"
  },
  {
    "text": "much much larger so here we go notice what I'm getting is a header separated",
    "start": "1956860",
    "end": "1964330"
  },
  {
    "start": "1960000",
    "end": "2083000"
  },
  {
    "text": "by tabs followed by a line that's a string of tab-separated values but this",
    "start": "1964330",
    "end": "1970000"
  },
  {
    "text": "is not very pretty but I'm running in Python so i can go lines and then go for",
    "start": "1970000",
    "end": "1975429"
  },
  {
    "text": "line in lines print line there we go",
    "start": "1975429",
    "end": "1982320"
  },
  {
    "text": "notice it formatted a little better because I'm still running and I got the full power of the Python programming",
    "start": "1982320",
    "end": "1987519"
  },
  {
    "text": "language available to me running in my shell can you guys see this okay I can",
    "start": "1987519",
    "end": "1993130"
  },
  {
    "text": "make my font a little bit bigger so at this point we've said we want to split",
    "start": "1993130",
    "end": "1998169"
  },
  {
    "text": "this up and turn them into power plant row object so let's run this guy again",
    "start": "1998169",
    "end": "2004278"
  },
  {
    "text": "and oh we have a job failure does anybody see any spelling errors I don't",
    "start": "2007039",
    "end": "2014250"
  },
  {
    "text": "see one so let's look at our error message come down here in the middle of",
    "start": "2014250",
    "end": "2019500"
  },
  {
    "text": "our nice stack trace that's one of the skills we have to teach and training is how to read these stack traces and it",
    "start": "2019500",
    "end": "2025230"
  },
  {
    "text": "says error occurred while calling run",
    "start": "2025230",
    "end": "2031350"
  },
  {
    "text": "job could not convert string to a floating point number atmospheric",
    "start": "2031350",
    "end": "2036570"
  },
  {
    "text": "temperature so here's what I did wrong somebody spot it i filtered out lines",
    "start": "2036570",
    "end": "2042510"
  },
  {
    "text": "that start with atmospheric temperature don't I want to filter lines that don't",
    "start": "2042510",
    "end": "2048060"
  },
  {
    "text": "keep lines only that don't start with atmospheric temperature so I did the",
    "start": "2048060",
    "end": "2053429"
  },
  {
    "text": "exact opposite to see that let's just get rid of this map operation run it again and now the first 5 lines on my",
    "start": "2053429",
    "end": "2061319"
  },
  {
    "text": "already do you were the headers alright well we'll go back and fix that we're",
    "start": "2061319",
    "end": "2067319"
  },
  {
    "text": "going to come here and we want lines that do not start without mysterio temperature we run it again there we go",
    "start": "2067319",
    "end": "2075480"
  },
  {
    "text": "now we've got in memory Python objects that we could begin to process now the",
    "start": "2075480",
    "end": "2083909"
  },
  {
    "start": "2083000",
    "end": "2388000"
  },
  {
    "text": "trouble with Python objects of course is that we're stuck doing this MapReduce style of programming so what we'd like",
    "start": "2083910",
    "end": "2090450"
  },
  {
    "text": "to do I go into the second stage here is move beyond that style of programming",
    "start": "2090450",
    "end": "2096300"
  },
  {
    "text": "and begin to use what are known as data frames so let me reconnect my cluster",
    "start": "2096300",
    "end": "2102680"
  },
  {
    "text": "rerun the work we just did and this time",
    "start": "2102680",
    "end": "2110750"
  },
  {
    "text": "I'd like to actually take a look at it so I'm going to convert that our DD we",
    "start": "2110750",
    "end": "2115920"
  },
  {
    "text": "just made called what was it called raw data our DD raw data our DD and I'll",
    "start": "2115920",
    "end": "2122850"
  },
  {
    "text": "convert it to a data frame",
    "start": "2122850",
    "end": "2126050"
  },
  {
    "text": "and now what I have in my disposal is the full power of a query engine that",
    "start": "2140250",
    "end": "2145980"
  },
  {
    "text": "can run either sequel or programmatically written queries so i",
    "start": "2145980",
    "end": "2150990"
  },
  {
    "text": "can start saying only give me things that were on days where the temperature was greater than 20 degrees let's try it",
    "start": "2150990",
    "end": "2160700"
  },
  {
    "text": "so I now have a power plant data frame now the first thing I'll do is I'll want",
    "start": "2160700",
    "end": "2167250"
  },
  {
    "text": "to register it here as a table so I'll come down here and i'll say power plant dot register as temp table and i'll call",
    "start": "2167250",
    "end": "2178800"
  },
  {
    "text": "a power underscore plant that registers it to my sequel binding and at this",
    "start": "2178800",
    "end": "2185280"
  },
  {
    "text": "point i could start running queries like select star from power plant where",
    "start": "2185280",
    "end": "2192500"
  },
  {
    "text": "atmospheric temperature is greater than 20 degrees i could of course do an RDD",
    "start": "2192500",
    "end": "2200190"
  },
  {
    "text": "filter operation but this is much easier and more concise now for those of you",
    "start": "2200190",
    "end": "2212490"
  },
  {
    "text": "who don't like sequel i can do the very same thing in Python power plant dot",
    "start": "2212490",
    "end": "2220460"
  },
  {
    "text": "filter where the power plant atmospheric",
    "start": "2220460",
    "end": "2226590"
  },
  {
    "text": "temperature is greater than 20 degrees this is similar to something called pandas or sequel alchemy I'm basically",
    "start": "2226590",
    "end": "2234510"
  },
  {
    "text": "expressing my query is still a declarative query executed by the same",
    "start": "2234510",
    "end": "2239700"
  },
  {
    "text": "query engine but this time I'm expressing my query in Python and it",
    "start": "2239700",
    "end": "2246810"
  },
  {
    "text": "still builds the same abstract syntax tree and runs it but let's go ahead and",
    "start": "2246810",
    "end": "2252120"
  },
  {
    "text": "display that and again you see that I",
    "start": "2252120",
    "end": "2259110"
  },
  {
    "text": "filtered out only jobs that have a high atmospheric temperature let's roll down a little further and get to even more",
    "start": "2259110",
    "end": "2265290"
  },
  {
    "text": "interesting stuff how am I on time by the way I've got about 10 minutes 10 minutes alright so I'll fast forward",
    "start": "2265290",
    "end": "2271440"
  },
  {
    "text": "just a little bit I'll do one plot not so yes I could describe the power",
    "start": "2271440",
    "end": "2276779"
  },
  {
    "text": "plant table for example I want to get into the machine learning aspect so",
    "start": "2276779",
    "end": "2282569"
  },
  {
    "text": "we'll jump down a little bit let's say I wanted to visualize my data I could do a",
    "start": "2282569",
    "end": "2287970"
  },
  {
    "text": "scatterplot so i could say select PE as",
    "start": "2287970",
    "end": "2294420"
  },
  {
    "text": "power in 80 as temperature from power",
    "start": "2294420",
    "end": "2300750"
  },
  {
    "text": "underscore plant and i could actually change it to be a plot you notice I've",
    "start": "2300750",
    "end": "2310440"
  },
  {
    "text": "got a very strong linear relationship between the outside temperature and the",
    "start": "2310440",
    "end": "2316559"
  },
  {
    "text": "power efficiency of our power plant let's go a little bit further I'm going",
    "start": "2316559",
    "end": "2323640"
  },
  {
    "text": "to jump ahead just a little bit in the interest of time",
    "start": "2323640",
    "end": "2329480"
  },
  {
    "text": "so you'll notice that atmospheric temperature let me zoom out a little bit very nice line exhaust vacuum a little",
    "start": "2345770",
    "end": "2356120"
  },
  {
    "text": "bit less clear there's something going on here it's kind of linear maybe not this is the kind of analysis a really",
    "start": "2356120",
    "end": "2362870"
  },
  {
    "text": "good data scientists would do be to try to convert this into a line by deploying transformations then we look at",
    "start": "2362870",
    "end": "2369170"
  },
  {
    "text": "something like atmospheric pressure there's a semblance of a line but a much weaker correlation and then you get to",
    "start": "2369170",
    "end": "2375320"
  },
  {
    "text": "something like relative humidity and I have no idea what that's supposed to be it's just a cloud so relative humidity",
    "start": "2375320",
    "end": "2382580"
  },
  {
    "text": "evidently has very little impact on the overall power efficiency of our power plant so at this point we're ready to",
    "start": "2382580",
    "end": "2391250"
  },
  {
    "start": "2388000",
    "end": "2432000"
  },
  {
    "text": "actually start doing some machine learning and in the interest of time I'm",
    "start": "2391250",
    "end": "2396560"
  },
  {
    "text": "going to go ahead and jump straight to my machine learning solution notebook instead of live coding how many data",
    "start": "2396560",
    "end": "2402620"
  },
  {
    "text": "scientists in here again just a small number of you guys okay so just kind of",
    "start": "2402620",
    "end": "2408410"
  },
  {
    "text": "show you what you can do to get a real application out of this so we finished visualizing our data let me do run all",
    "start": "2408410",
    "end": "2415490"
  },
  {
    "text": "again notice the older version of the",
    "start": "2415490",
    "end": "2420830"
  },
  {
    "text": "notebook had showed way fewer data points they've now improved that to show a lot more data points which is really neat go ahead and hit run all and while",
    "start": "2420830",
    "end": "2428900"
  },
  {
    "text": "we're running everything I'm going to scroll down to linear regression so what",
    "start": "2428900",
    "end": "2435830"
  },
  {
    "start": "2432000",
    "end": "2594000"
  },
  {
    "text": "I'd like to do is to build a linear regression model of our data so the",
    "start": "2435830",
    "end": "2442220"
  },
  {
    "text": "first thing I'm going to do is I'm going to take my original data set here and I'm going to split it up with an 8020",
    "start": "2442220",
    "end": "2448880"
  },
  {
    "text": "split i'm going to use eighty percent of those data points to do training twenty percent then to do predictions and then",
    "start": "2448880",
    "end": "2455270"
  },
  {
    "text": "i'm going to evaluate how good are my predictions it's a very common machine learning use case and if i want my job",
    "start": "2455270",
    "end": "2463370"
  },
  {
    "text": "to be reproducible let's say an RDD partition goes down and i do this random",
    "start": "2463370",
    "end": "2469160"
  },
  {
    "text": "split over again I would ideally like that partition to have the same data it",
    "start": "2469160",
    "end": "2474950"
  },
  {
    "text": "had before so one of the best practices is to give it a random seed any seat at all but a random seed so that",
    "start": "2474950",
    "end": "2481869"
  },
  {
    "text": "it's deterministic in the event that something needed to rerun so here I've given it a random seed you'll also",
    "start": "2481869",
    "end": "2488259"
  },
  {
    "text": "notice that I've asked it to cache the data frame in memory to allow my iterative algorithm to run faster now",
    "start": "2488259",
    "end": "2496779"
  },
  {
    "text": "one of the really amazing things about machine learning is that it benefits enormously from having our data in",
    "start": "2496779",
    "end": "2503680"
  },
  {
    "text": "memory rather than on disk because what machine learning is doing is iterating on this data over and over and over",
    "start": "2503680",
    "end": "2509140"
  },
  {
    "text": "again basically trying to find the perfect linear regression model in order",
    "start": "2509140",
    "end": "2515829"
  },
  {
    "text": "to scale it's doing this using what's known as gradient descent rather than an ordinary matrix multiplication in order",
    "start": "2515829",
    "end": "2522609"
  },
  {
    "text": "to be able to scale to enormous amounts of data so here we go I'm going to say I",
    "start": "2522609",
    "end": "2528609"
  },
  {
    "text": "want to make my predictions go in the predicted PE column when I do my",
    "start": "2528609",
    "end": "2534700"
  },
  {
    "text": "evaluations i'm going to do training based on the original PE column i'm going to allow my algorithm to iterate",
    "start": "2534700",
    "end": "2541210"
  },
  {
    "text": "no more than a hundred times and the first thing i'm going to do is take my",
    "start": "2541210",
    "end": "2546789"
  },
  {
    "text": "data frame and turn it into a mathematical vector and then i'm going to run linear regression and then i'm",
    "start": "2546789",
    "end": "2557739"
  },
  {
    "text": "going to give it my training data and out pops a model let's run this",
    "start": "2557739",
    "end": "2565380"
  },
  {
    "text": "notice how quickly 15 jobs just ran in spark because all that date is now",
    "start": "2570050",
    "end": "2576660"
  },
  {
    "text": "cashed in memory you guys were all thinking why spark going so slow when I was running stuff up above weren't you",
    "start": "2576660",
    "end": "2582270"
  },
  {
    "text": "tell me the truth how many of you guys have gone why was it so slow answer I hadn't cashed it was going back to HDFS",
    "start": "2582270",
    "end": "2589050"
  },
  {
    "text": "each time are actually in that case amazon ec2 s3 storage this time it's",
    "start": "2589050",
    "end": "2596910"
  },
  {
    "start": "2594000",
    "end": "2679000"
  },
  {
    "text": "cached in memory it ran 15 jobs like lightning to 15 total iterations I then",
    "start": "2596910",
    "end": "2604890"
  },
  {
    "text": "could take my test data and get the predictions and let's take a look at how",
    "start": "2604890",
    "end": "2610500"
  },
  {
    "text": "we did should I take my test data I run predictions and take a look some of them",
    "start": "2610500",
    "end": "2621510"
  },
  {
    "text": "are quite close others that's pretty close to here we go this one's five away",
    "start": "2621510",
    "end": "2629220"
  },
  {
    "text": "this one's seven away not too bad so a good data scientist would look to tweak",
    "start": "2629220",
    "end": "2636060"
  },
  {
    "text": "their model to improve performance but notice how fast and how easy it is to",
    "start": "2636060",
    "end": "2641610"
  },
  {
    "text": "program on sparc using either Python or Scala and then I could go even further I",
    "start": "2641610",
    "end": "2647030"
  },
  {
    "text": "can actually print out the equation that it learned why would be our power",
    "start": "2647030",
    "end": "2653100"
  },
  {
    "text": "efficiency and so it learned i just multiply the atmospheric temperature by this the vacuum pressure by this notice",
    "start": "2653100",
    "end": "2659700"
  },
  {
    "text": "how little contribution vacuum pressure and other things have compared to atmospheric temperature we come over to",
    "start": "2659700",
    "end": "2665730"
  },
  {
    "text": "relative humidity and it's got almost no contribution no here we go what's this one as atmospheric printer has almost no",
    "start": "2665730",
    "end": "2673260"
  },
  {
    "text": "contribution at all now part of that's because of its units and then I could",
    "start": "2673260",
    "end": "2680670"
  },
  {
    "start": "2679000",
    "end": "2710000"
  },
  {
    "text": "actually do some evaluations I can do some nice visualizations scroll down a little bit I can see for example that I",
    "start": "2680670",
    "end": "2688500"
  },
  {
    "text": "get a nice bell curve on my error this is excellent data result and I could go even further and see how much of my data",
    "start": "2688500",
    "end": "2695460"
  },
  {
    "text": "is one standard deviation two standard deviations or three standard deviations away from its prediction so as a data",
    "start": "2695460",
    "end": "2701760"
  },
  {
    "text": "science I can very rapidly build and develop models and they get much better performance and throughput than I would",
    "start": "2701760",
    "end": "2708370"
  },
  {
    "text": "expect in Hadoop so with that said I'm going to wrap up and then take questions",
    "start": "2708370",
    "end": "2714420"
  },
  {
    "start": "2710000",
    "end": "3203000"
  },
  {
    "text": "so who are we again the presentation is a joint sponsorship between data bricks",
    "start": "2714420",
    "end": "2720820"
  },
  {
    "text": "and new circle data bricks has done about 75% of the commits on spark",
    "start": "2720820",
    "end": "2725980"
  },
  {
    "text": "because it's basically the original team at Berkeley that made spark so spark is an enormous ecosystem but a lot of the",
    "start": "2725980",
    "end": "2734020"
  },
  {
    "text": "core talent driving spark is at data bridge new circle is training partners with data bricks we do training in spark",
    "start": "2734020",
    "end": "2742240"
  },
  {
    "text": "big data android java python and a whole host of other technologies if you liked",
    "start": "2742240",
    "end": "2749470"
  },
  {
    "text": "this style of learning through hands-on demonstration really getting a sense of a mixture of the power spark what's",
    "start": "2749470",
    "end": "2756160"
  },
  {
    "text": "happening under the hood and actual live coding this is what we do in our training it's three days of very intense",
    "start": "2756160",
    "end": "2762850"
  },
  {
    "text": "hands-on but hopefully this has been a valuable experience for everybody here thank you guys very very much for",
    "start": "2762850",
    "end": "2769810"
  },
  {
    "text": "attending I can take other questions support for neural networks and deep machine learning so there's a website",
    "start": "2769810",
    "end": "2776320"
  },
  {
    "text": "called spark dash packages org where some people have tried to do deep machine learning with neural networks",
    "start": "2776320",
    "end": "2782260"
  },
  {
    "text": "and of course she'll realize a simple neural network is just a well a",
    "start": "2782260",
    "end": "2787860"
  },
  {
    "text": "level-one neural network is really just a simple linear regression but once you add deep neural network chaining that's",
    "start": "2787860",
    "end": "2794680"
  },
  {
    "text": "when it becomes more advanced go to spark dash packages org anybody else yes",
    "start": "2794680",
    "end": "2801150"
  },
  {
    "text": "what happens if the driver crashes so this is a great question and the answer to that depends on how you've launched",
    "start": "2801150",
    "end": "2809170"
  },
  {
    "text": "the driver if it's a nightly job for example you can run the driver in what's known as supervised mode where the yarn",
    "start": "2809170",
    "end": "2816310"
  },
  {
    "text": "master or the standalone cluster master would detect that the driver failed and relaunch the driver and the entire",
    "start": "2816310",
    "end": "2822310"
  },
  {
    "text": "application on the other hand you know if it's an interactive shell you launch",
    "start": "2822310",
    "end": "2827890"
  },
  {
    "text": "it but yes when the driver crashes you lose your work unfortunately at this time there is a technique known as checkpointing where you actually",
    "start": "2827890",
    "end": "2834190"
  },
  {
    "text": "checkpoint your work so even if the driver were to crash you can recover where you left off and this is",
    "start": "2834190",
    "end": "2839300"
  },
  {
    "text": "very commonly used in streaming scenarios question here not very",
    "start": "2839300",
    "end": "2845270"
  },
  {
    "text": "difficult at all so if you were to set up your own cluster you can do that on what's known as spark standalone mode",
    "start": "2845270",
    "end": "2851390"
  },
  {
    "text": "which comes it's called standalone because it comes with spark and you can quickly set up a cluster like right out",
    "start": "2851390",
    "end": "2857060"
  },
  {
    "text": "of the box if you just have a bunch of machine just launched the executor 'he's you'd launch the driver and tell it you",
    "start": "2857060",
    "end": "2862700"
  },
  {
    "text": "know where the actually the executives register back to the drivers so you launch the driver than you lost the exact lesion tell them to register with",
    "start": "2862700",
    "end": "2868100"
  },
  {
    "text": "the driver very easy though you can also actually run spark on yarn in which case",
    "start": "2868100",
    "end": "2873170"
  },
  {
    "text": "you're then using the yarn master to do all of us work that takes a bit more work so the nice thing about standalone",
    "start": "2873170",
    "end": "2878300"
  },
  {
    "text": "is that it comes out without having to configure a third-party product like yarn question here Peter and then Matt",
    "start": "2878300",
    "end": "2884920"
  },
  {
    "text": "so what we didn't get to go into as much detail so the question was what happens",
    "start": "2884920",
    "end": "2890690"
  },
  {
    "text": "when spark doesn't necessarily have reused data from the purposes of caching so remember this picture that I showed",
    "start": "2890690",
    "end": "2897350"
  },
  {
    "text": "much earlier let me bring it back up again it's the rdd slides this picture",
    "start": "2897350",
    "end": "2905530"
  },
  {
    "text": "now one thing that's actually happening in this picture that we didn't get to go into because of time constraints today",
    "start": "2905530",
    "end": "2911600"
  },
  {
    "text": "is something known as pipelining so what happens is he in Hadoop each of these",
    "start": "2911600",
    "end": "2917810"
  },
  {
    "text": "individual stages would be their own process running and dumping to HDFS which spark is going to do is realize",
    "start": "2917810",
    "end": "2924680"
  },
  {
    "text": "that all of these things could actually be done locally between each of these",
    "start": "2924680",
    "end": "2929900"
  },
  {
    "text": "individual transformations so it's going to read in a partition in this case let's say I had one executor how would",
    "start": "2929900",
    "end": "2936320"
  },
  {
    "text": "this run it's going to read in in this case two blocks from HDFS do the filters",
    "start": "2936320",
    "end": "2944530"
  },
  {
    "text": "do the coalesce and then of course do the collect operation and it's going to",
    "start": "2944530",
    "end": "2950330"
  },
  {
    "text": "do that all in memory on one partition then it might fire off another task to",
    "start": "2950330",
    "end": "2956060"
  },
  {
    "text": "work on the next two partitions and so forth so what actually happens is even",
    "start": "2956060",
    "end": "2961490"
  },
  {
    "text": "if it does in cash in memory you're getting the benefit of what's known as pipelining through this analytical model you're also getting what they're called",
    "start": "2961490",
    "end": "2968870"
  },
  {
    "text": "shuffle so if you have a more elaborate operation where it's actually what's called a multi-stage operation between",
    "start": "2968870",
    "end": "2975140"
  },
  {
    "text": "each stage it actually dumps out intermediate results to a file which creates a second level of caching Matt I",
    "start": "2975140",
    "end": "2982009"
  },
  {
    "text": "think I said first yeah ah so spark supports Python Scala are and Java and",
    "start": "2982009",
    "end": "2988539"
  },
  {
    "text": "starting 15 job at eight so what you really want is to choose the language of",
    "start": "2988539",
    "end": "2995299"
  },
  {
    "text": "your choice Scala is actually your first choice if you guys are good with Scala",
    "start": "2995299",
    "end": "3001509"
  },
  {
    "text": "you understand scala where you take our quick new circle class on Scala the majority all the API is available",
    "start": "3001509",
    "end": "3008529"
  },
  {
    "text": "Scarlett's Park is written on Scala with that said maybe you guys are a Java shop you can use Java now I discourage you",
    "start": "3008529",
    "end": "3015130"
  },
  {
    "text": "using Java 7 because it's very verbose with inner classes you really want lambdas and then we find the machine",
    "start": "3015130",
    "end": "3021069"
  },
  {
    "text": "learning community is more comfortable with r and python so we teach it in our in Python for that type of audience does",
    "start": "3021069",
    "end": "3027069"
  },
  {
    "text": "that answer your question Eric and how is optimized is it from pulling from a",
    "start": "3027069",
    "end": "3033069"
  },
  {
    "text": "sequel source so that's a great question and there's two optimizations that you want to be aware of one of them is if",
    "start": "3033069",
    "end": "3038200"
  },
  {
    "text": "I'm doing a dataframe query and let's say I do a filter with my data frame and it looks and goes you know what I could",
    "start": "3038200",
    "end": "3044200"
  },
  {
    "text": "do that filter in my in Oracle itself or maybe i'm doing a join but it looks and says i'm joining two tables from oracle",
    "start": "3044200",
    "end": "3050980"
  },
  {
    "text": "it pushes the joint and filter into oracle first second it can actually",
    "start": "3050980",
    "end": "3056980"
  },
  {
    "text": "create parallel jdbc readers so if it's a sorted data set you can actually say",
    "start": "3056980",
    "end": "3063069"
  },
  {
    "text": "give me the first thousand to this partition the second thousand to this partition and then the individual",
    "start": "3063069",
    "end": "3068980"
  },
  {
    "text": "executor czar running in parallel but of course your Oracle database under the hood needs to be able to support that level of parallelism so usually we only",
    "start": "3068980",
    "end": "3076029"
  },
  {
    "text": "do that when we're writing is something like Amazon redshift where you can handle concurrent queries so what do you",
    "start": "3076029",
    "end": "3082779"
  },
  {
    "text": "have to do with spark from a management perspective and then you specifically mentioned business intelligence so if you're a team that's doing exploratory",
    "start": "3082779",
    "end": "3089470"
  },
  {
    "text": "data analysis and business intelligence skip our dd's out right go straight to data frames are td's are going to be",
    "start": "3089470",
    "end": "3096369"
  },
  {
    "text": "your fallback data frames are the preferred way to go if you can use data frames use them are dd's are for only",
    "start": "3096369",
    "end": "3102279"
  },
  {
    "text": "things you can't do what they two frames and then it's going to be as straightforward as using sequel now from",
    "start": "3102279",
    "end": "3108130"
  },
  {
    "text": "a management perspective what do you need well you want to make sure that a you know how to set up your spark",
    "start": "3108130",
    "end": "3113620"
  },
  {
    "text": "cluster so there's a skill set there be your team are they familiar with what",
    "start": "3113620",
    "end": "3120070"
  },
  {
    "text": "language are or Python or what a Python",
    "start": "3120070",
    "end": "3125740"
  },
  {
    "text": "Java and C sharp so you would choose whether you want to use Java 8 I don't recommend Java 7 or preferably Python",
    "start": "3125740",
    "end": "3132070"
  },
  {
    "text": "for that team if they're very fluent in that and then just sign up we have a one-day spark overview and we have a",
    "start": "3132070",
    "end": "3138640"
  },
  {
    "text": "three-day spark developer we're also starting out to write a class specifically for exploratory data",
    "start": "3138640",
    "end": "3144400"
  },
  {
    "text": "analysis and bi which would go into lesson to the programming side and more",
    "start": "3144400",
    "end": "3149410"
  },
  {
    "text": "into doing visualizations and sequel queries they bi guys can do it as long",
    "start": "3149410",
    "end": "3156370"
  },
  {
    "text": "as they can write sequel they have to be able to write sequel that's the one caveat now you can also make spark a",
    "start": "3156370",
    "end": "3164430"
  },
  {
    "text": "JDBC provider so your existing tableau tools could connect to spark as a JDBC",
    "start": "3164430",
    "end": "3171760"
  },
  {
    "text": "provider in which case spark is writing the JDBC but not now you're actually",
    "start": "3171760",
    "end": "3176950"
  },
  {
    "text": "running your familiar tools and still connecting to spark oh absolutely would",
    "start": "3176950",
    "end": "3183130"
  },
  {
    "text": "take advantage of parallelism the sparks equal execution engine will take care of the parallelism thanks guys very much",
    "start": "3183130",
    "end": "3188470"
  },
  {
    "text": "for coming I know there's food out there I'll take a few more questions you're welcome to come forward and ask",
    "start": "3188470",
    "end": "3195210"
  },
  {
    "text": "you",
    "start": "3201609",
    "end": "3203670"
  }
]