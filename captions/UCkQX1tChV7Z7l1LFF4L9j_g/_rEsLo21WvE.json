[
  {
    "text": "[Music]",
    "start": "3320",
    "end": "8769"
  },
  {
    "text": "thank you all right thanks Len for introduction um so morning everyone I'm",
    "start": "10080",
    "end": "15759"
  },
  {
    "text": "Cody I'm staff soft engineer and skill today I'm going to talk about um how to scale our Bach inference with",
    "start": "15759",
    "end": "21760"
  },
  {
    "text": "r so first of all for myself uh I'm a s engineer and Tech leader for lar model",
    "start": "21760",
    "end": "27599"
  },
  {
    "text": "performance at any scale and I'm also a VN sgn TVN which are famous open source",
    "start": "27599",
    "end": "33440"
  },
  {
    "text": "project commuters um before that I was also the expon engineer at B also this",
    "start": "33440",
    "end": "39120"
  },
  {
    "text": "senior PR scientist and AWS so before we dive into the bach",
    "start": "39120",
    "end": "44800"
  },
  {
    "text": "inference um for lar model within Ray I want to first start with um the",
    "start": "44800",
    "end": "50199"
  },
  {
    "text": "statement that we are in the Gen in gen AI era um for example you can see today",
    "start": "50199",
    "end": "56760"
  },
  {
    "text": "A lot of people are talking to the chat Bots and some companies have already used um chot or lar length model for",
    "start": "56760",
    "end": "64400"
  },
  {
    "text": "their online customer services and a more complicated situation or the use cases could be using the logic language",
    "start": "64400",
    "end": "71159"
  },
  {
    "text": "model as the MTI agent to process a complicate Tas and we don't know we know",
    "start": "71159",
    "end": "76320"
  },
  {
    "text": "that the model today can actually generate not only the text but also images or even videos even the images",
    "start": "76320",
    "end": "82840"
  },
  {
    "text": "from slides were generated by opening edbd 4 so this illustrates how useful",
    "start": "82840",
    "end": "88079"
  },
  {
    "text": "and importance of the the applications in the Str area at the same time uh the demand of",
    "start": "88079",
    "end": "95880"
  },
  {
    "text": "the bat inference is also getting higher and higher uh this is mainly because we now have a multimodality data sources",
    "start": "95880",
    "end": "102479"
  },
  {
    "text": "you have cameras um mic and sensors and PDF files and then by processing this",
    "start": "102479",
    "end": "109079"
  },
  {
    "text": "files you'll get different kind of R datas in different kinds of format which",
    "start": "109079",
    "end": "114240"
  },
  {
    "text": "is unstructured or structured for example you can have the images extract from the camera or",
    "start": "114240",
    "end": "120560"
  },
  {
    "text": "you can have the audio tablet data or video or even a road Tex and in order to",
    "start": "120560",
    "end": "127079"
  },
  {
    "text": "make those data pretty useful for your real application and services and production you will need either the",
    "start": "127079",
    "end": "132879"
  },
  {
    "text": "embedding models or the large language models to help process data and then you can save the results back to the vector",
    "start": "132879",
    "end": "139879"
  },
  {
    "text": "database used for model training classification or extract the knowledge from data for the various of",
    "start": "139879",
    "end": "147640"
  },
  {
    "text": "applications so in short um while lar Dage model becomes on",
    "start": "147640",
    "end": "153000"
  },
  {
    "text": "becomes High On Demand the demand of the bash inference will go even higher",
    "start": "153000",
    "end": "158239"
  },
  {
    "text": "that's why we want to dive into the bat inference and make it more cost effective and high thus but now scaling out batch inference",
    "start": "158239",
    "end": "166519"
  },
  {
    "text": "actually will bring up certain challenges for example the first thing you want to deal with is the scalability",
    "start": "166519",
    "end": "172000"
  },
  {
    "text": "because you can imagine those rad data can easily go up with hundreds of thousands of gigabytes or terabytes or",
    "start": "172000",
    "end": "178000"
  },
  {
    "text": "even more so scalability becomes the biggest challenge come with the high scalability you have the reliability",
    "start": "178000",
    "end": "185040"
  },
  {
    "text": "issue because in order to save the money usually we will prefer to use the spot instances on the public Club providers",
    "start": "185040",
    "end": "192720"
  },
  {
    "text": "um for the cost saving the spot instances will actually offer you the instances with for example just 10% of",
    "start": "192720",
    "end": "200400"
  },
  {
    "text": "the original price but it can be preempted any time by the cloud provider",
    "start": "200400",
    "end": "205599"
  },
  {
    "text": "in the high PE hours so in order to build a system on spotting instances we",
    "start": "205599",
    "end": "210879"
  },
  {
    "text": "have to have a good reliability that means you can deal with the failures due to the preemption and you'll be able to",
    "start": "210879",
    "end": "217560"
  },
  {
    "text": "resume the failure jobs without any human in Loop and of course Computing is",
    "start": "217560",
    "end": "223480"
  },
  {
    "text": "one of the most important factors because different um stage in your data pipeline may require different Hardwares",
    "start": "223480",
    "end": "229360"
  },
  {
    "text": "which can be CPUs or gpus or different kinds of a number of gpus so dealing",
    "start": "229360",
    "end": "234680"
  },
  {
    "text": "with multiple stages in your data Pipeline and hetrogeneous Computing also becomes a big challenge",
    "start": "234680",
    "end": "240799"
  },
  {
    "text": "and also the flexibility because you will need different models to process different data for simple models you may",
    "start": "240799",
    "end": "246560"
  },
  {
    "text": "need just simple like object detection model uh and for complicated data you may need large language model to analyze",
    "start": "246560",
    "end": "252920"
  },
  {
    "text": "semantic or extract information so the system the bat inference pipeline also need to be deal with the flexibility of",
    "start": "252920",
    "end": "259840"
  },
  {
    "text": "dealing with different OS models or the custom models Trend by yourself finally SAS when you're doing the pro uh B",
    "start": "259840",
    "end": "267440"
  },
  {
    "text": "inference processing production you cannot just just through all the data and get the result without considering",
    "start": "267440",
    "end": "272680"
  },
  {
    "text": "the cost so either you will set the high throughput cost or latency as your SLA",
    "start": "272680",
    "end": "278240"
  },
  {
    "text": "before you get into the production to make sure you won't bankrupt by um processing those kind of",
    "start": "278240",
    "end": "284800"
  },
  {
    "text": "datas so in any scale we solve this problem by introducing a multi-layer approach and this is also agenda of this",
    "start": "284800",
    "end": "291639"
  },
  {
    "text": "talk um we we basically have three layers approach from the bottom up we have the r core which is a scalable",
    "start": "291639",
    "end": "298720"
  },
  {
    "text": "general purpose AI Computing engine to deal with the scalability and reliability and on top of that we build",
    "start": "298720",
    "end": "305520"
  },
  {
    "text": "a rate data which is a efficient and scalable data processing Pipeline on Ray",
    "start": "305520",
    "end": "310960"
  },
  {
    "text": "um because Ray core already deals with um the scalability and reliability um so",
    "start": "310960",
    "end": "316280"
  },
  {
    "text": "we can be we can build R data on top of that easily so I will show how we build it and how do we Implement that later on",
    "start": "316280",
    "end": "323319"
  },
  {
    "text": "and on top of that we need a powerful large language model inference engine uh",
    "start": "323319",
    "end": "328400"
  },
  {
    "text": "we use that we use open source VM which is the most popular open source uh large",
    "start": "328400",
    "end": "333479"
  },
  {
    "text": "language model inference engine at the moment um and based on that to construct our batch inference",
    "start": "333479",
    "end": "339840"
  },
  {
    "text": "pipeline so let's start with Ray um there's a ray overview Ray is basically",
    "start": "339840",
    "end": "345720"
  },
  {
    "text": "a distributed library to deal with the large scale cluster and resource management and tax allocation on top of",
    "start": "345720",
    "end": "352840"
  },
  {
    "text": "Ray we already have a very planful ecosystem which are the application Level libraries buil on top of Ray for",
    "start": "352840",
    "end": "359639"
  },
  {
    "text": "for example we have R and R Trend to deal with the training jobs we have the race serve to deal with the online model",
    "start": "359639",
    "end": "365720"
  },
  {
    "text": "serving our AR Le which is a reinforcement learning library buil on",
    "start": "365720",
    "end": "370840"
  },
  {
    "text": "Ray and also the ray data which I will be introduced uh very soon is for data processing all of those applications are",
    "start": "370840",
    "end": "379000"
  },
  {
    "text": "implemented for different functionalities but the common part of them is to deal with the large",
    "start": "379000",
    "end": "384080"
  },
  {
    "text": "scaleability so at the bottom we have the remote functions and classes we call that TX",
    "start": "384080",
    "end": "390759"
  },
  {
    "text": "and actors and then the r core which is a general purpose distributed execution",
    "start": "390759",
    "end": "396039"
  },
  {
    "text": "layer to deal with all the tax allocation scheduling and actor",
    "start": "396039",
    "end": "402680"
  },
  {
    "text": "manipulations for example for now N scale has been using Ray to push the scalability to thousands of instances",
    "start": "402680",
    "end": "410840"
  },
  {
    "text": "for running our workloads for our customers um in short you can see we have the Hat noes and the worker noes",
    "start": "410840",
    "end": "417199"
  },
  {
    "text": "and the worker no can be a thousand in each node we have the driver and worker and on the line we have two important",
    "start": "417199",
    "end": "423599"
  },
  {
    "text": "components uh which is GCS and the dashboard and the dash server for GCS",
    "start": "423599",
    "end": "429879"
  },
  {
    "text": "this is used for actor scheduling placement group scheduling no resource views etc etc and for dashboard server",
    "start": "429879",
    "end": "437039"
  },
  {
    "text": "um this basically shows the dashboard metric system logs job apis Etc um we",
    "start": "437039",
    "end": "442319"
  },
  {
    "text": "highlight those two components because we realized that when you're scaling up to thousand notes um this is not only",
    "start": "442319",
    "end": "450520"
  },
  {
    "text": "like not uh scalability reliability are the important factors but other than",
    "start": "450520",
    "end": "456800"
  },
  {
    "text": "that how do you make sure all the metrics and logs and the current status can be easily monitored and interpreted",
    "start": "456800",
    "end": "464879"
  },
  {
    "text": "by users are also becoming important factors so that's why in the recent development of Ray we also focus on um",
    "start": "464879",
    "end": "471919"
  },
  {
    "text": "those GCS and dashboard server developments and we want to highlight that those two components are actually",
    "start": "471919",
    "end": "478479"
  },
  {
    "text": "become much more Rob bu on actual large clusters and this is be uh basically because we have optimized the event Loop",
    "start": "478479",
    "end": "486120"
  },
  {
    "text": "very carefully for example we have the dedicated threats for high value and simple rpcs and you also move the",
    "start": "486120",
    "end": "492280"
  },
  {
    "text": "expensive data fetching for the background threats and the break up the expensive handlers etc etc um I won't",
    "start": "492280",
    "end": "499400"
  },
  {
    "text": "dive into too much details for the record but if you're interested in the details of Ray welcome to check out the",
    "start": "499400",
    "end": "505440"
  },
  {
    "text": "N scale documentation or contact us for more details",
    "start": "505440",
    "end": "510639"
  },
  {
    "text": "okay so once we have the ray to deal with the scalability next uh we can",
    "start": "510639",
    "end": "516279"
  },
  {
    "text": "build Ray data which could be a scalable data processing for distributed machine learning",
    "start": "516279",
    "end": "521839"
  },
  {
    "text": "processing so let's start with the key challenges that R data has softed first",
    "start": "521839",
    "end": "527240"
  },
  {
    "text": "is the hetrogeneous Computing like I mentioned in the beginning in the different stage of your data pipeline",
    "start": "527240",
    "end": "532640"
  },
  {
    "text": "you may need a different Hardwares in terms of CPUs and gpus so in array data",
    "start": "532640",
    "end": "537839"
  },
  {
    "text": "we enable streaming execution that means we basically chunk the data into multiple blocks and then we can process",
    "start": "537839",
    "end": "544279"
  },
  {
    "text": "all the blocks in a streaming way so in this way we can make sure the resource",
    "start": "544279",
    "end": "549480"
  },
  {
    "text": "on each stage can be maximized all the time so in this example you can see we",
    "start": "549480",
    "end": "554800"
  },
  {
    "text": "load and preprocess data on CPU and do the inference on GPU so we want to maximize the GP uh we want to minimize",
    "start": "554800",
    "end": "561519"
  },
  {
    "text": "the GPU idal time because the GPU cost is usually like 4X of the CPU cost or",
    "start": "561519",
    "end": "566640"
  },
  {
    "text": "even more and the second challenge is like I mentioned reliability is an issue if we want to build the data Pipeline on",
    "start": "566640",
    "end": "573279"
  },
  {
    "text": "the very cheap spot instances so rate data along with the record also make",
    "start": "573279",
    "end": "579200"
  },
  {
    "text": "sure the reliability is there um specifically we have the fall tolerance and checkpointing to um make sure all",
    "start": "579200",
    "end": "586240"
  },
  {
    "text": "the jobs can be done uh with the correct functionality and statements and also uh",
    "start": "586240",
    "end": "592320"
  },
  {
    "text": "we have the job reasonability so that whatever the instances are preempted by the cloud providers we can make sure all",
    "start": "592320",
    "end": "598959"
  },
  {
    "text": "the jobs will get executed only once and the third challenge is to deal with a complicated ecosystem now you",
    "start": "598959",
    "end": "606320"
  },
  {
    "text": "have the thousands of ways to process your data using various of the existing and mature um systems like hdfs F3 those",
    "start": "606320",
    "end": "614320"
  },
  {
    "text": "are objective systems and also spark desk those are the Computing PL uh Computing Frameworks um and that's for",
    "start": "614320",
    "end": "621480"
  },
  {
    "text": "data data part and then for the machine learning AI of course people usually use tens flow or P torch to do the inference",
    "start": "621480",
    "end": "629320"
  },
  {
    "text": "for your model so Ray data has already integrated all of those popular user Frameworks so that you can easily use",
    "start": "629320",
    "end": "636440"
  },
  {
    "text": "Ray data to integrate all of those components together to build your scalable data",
    "start": "636440",
    "end": "642360"
  },
  {
    "text": "pipeline so let me DI into a bit about Ray data implementation first is the ray",
    "start": "642360",
    "end": "648200"
  },
  {
    "text": "data engine in the example on the slides we show a ray data battery inference example in this example we first read",
    "start": "648200",
    "end": "655600"
  },
  {
    "text": "the image from S3 buckets and apply the custom transform function to process the",
    "start": "655600",
    "end": "661079"
  },
  {
    "text": "data in any way you like and after that we will send the process the data to",
    "start": "661079",
    "end": "666560"
  },
  {
    "text": "inference so this inference is a class and it will be hosting a model on gpus",
    "start": "666560",
    "end": "672800"
  },
  {
    "text": "after the model has been processed we will just after data has been processed we'll basically upload the result back",
    "start": "672800",
    "end": "678839"
  },
  {
    "text": "to the F3 and all this happens in a streaming Way by writing this simple code you already construct a high",
    "start": "678839",
    "end": "686760"
  },
  {
    "text": "reliable High scalable data pipeline using rate data what's happening under the hood is we will first construct a",
    "start": "686760",
    "end": "693880"
  },
  {
    "text": "logical plan this basically designs what to do with the data pipeline so you can see by this post snip we defined the",
    "start": "693880",
    "end": "700880"
  },
  {
    "text": "four stages in the C pipeline first read the data transform data on CPU perform inference on GPU and then upload the",
    "start": "700880",
    "end": "707680"
  },
  {
    "text": "result on CPU however you can imagine although this data pipeline looks pretty simple",
    "start": "707680",
    "end": "713760"
  },
  {
    "text": "and straightforward it could be inefficient during the execution um so f",
    "start": "713760",
    "end": "719240"
  },
  {
    "text": "after this rate data will actually compile this logical plan to generate the physical plan the physical plan",
    "start": "719240",
    "end": "725240"
  },
  {
    "text": "basically defines what to how to do with the data pipeline in this simple example we can see the read and transform are",
    "start": "725240",
    "end": "732760"
  },
  {
    "text": "actually fused to a single map operator that means we can save the data",
    "start": "732760",
    "end": "737920"
  },
  {
    "text": "communication data rewrite dis iio by streaming the read and transform",
    "start": "737920",
    "end": "745680"
  },
  {
    "text": "together so once we have the logical plan during the actual execution the",
    "start": "745680",
    "end": "750720"
  },
  {
    "text": "data actually will be processed by rate Tex or actors with different resource requirements like we mentioned we can",
    "start": "750720",
    "end": "756440"
  },
  {
    "text": "actually specify how many CPU cores you need to process the input data or how many gpus you need to influence the data",
    "start": "756440",
    "end": "764720"
  },
  {
    "text": "so by specifying different requests what's happening under the hood is the rate data will do the scheduling for",
    "start": "764720",
    "end": "770680"
  },
  {
    "text": "different taxs on different Hardware in a dynamic way so in this example we have",
    "start": "770680",
    "end": "776199"
  },
  {
    "text": "three nodes two CPU nodes and one GPU nodes and even for the GPU notes we have the CPU course so rata will basically",
    "start": "776199",
    "end": "783320"
  },
  {
    "text": "just schedule the right test on the CPU or GPU based on the requirement and of course we'll try to minimize the data",
    "start": "783320",
    "end": "789839"
  },
  {
    "text": "Movement by scheduling uh the similar data to the similar nodes and like I mentioned again and",
    "start": "789839",
    "end": "796680"
  },
  {
    "text": "again there's important to have the streaming ex executor so we will schedule test dynamically and the M",
    "start": "796680",
    "end": "802199"
  },
  {
    "text": "resources and handle the uh back pressure and all of this is to ensure",
    "start": "802199",
    "end": "808040"
  },
  {
    "text": "the GPU is Con consistently saturated this is becomes a very important metrix when we're evaluating the efficiency of",
    "start": "808040",
    "end": "815040"
  },
  {
    "text": "the data Pipeline with gpus and talking about streaming",
    "start": "815040",
    "end": "820160"
  },
  {
    "text": "execution I do want to highlight one thing uh uh one thing that plays the important rule in the efficiency um it's",
    "start": "820160",
    "end": "827360"
  },
  {
    "text": "about how to communicate data between text and notes um as you can imagine",
    "start": "827360",
    "end": "832800"
  },
  {
    "text": "because we can probably schedule the tasks on different um stages on different notes so you will need",
    "start": "832800",
    "end": "839120"
  },
  {
    "text": "efficient way to communicate the data between nodes um so we introduced an abstraction called the Ray Object Store",
    "start": "839120",
    "end": "846680"
  },
  {
    "text": "which is basically a storage or like a file system so that uh the map operator",
    "start": "846680",
    "end": "852720"
  },
  {
    "text": "can basically upload your temporary result to this object storage and the next stage can consume that in order to",
    "start": "852720",
    "end": "859079"
  },
  {
    "text": "achieve the best efficiency this abstraction can can be implemented in the different ways for example between",
    "start": "859079",
    "end": "865120"
  },
  {
    "text": "noes and noes you can use share file system or a three object storage um if",
    "start": "865120",
    "end": "871360"
  },
  {
    "text": "with the same node this can even be in for example the share memory or local dis a so by optimizing and elegantly",
    "start": "871360",
    "end": "879480"
  },
  {
    "text": "select the most suitable file system under the hood we can optimize the data transfer between",
    "start": "879480",
    "end": "887040"
  },
  {
    "text": "Texs so here is a case study uh using Ray data this example is the batch",
    "start": "887040",
    "end": "893560"
  },
  {
    "text": "embedding Generation Um in this simple pipeline we first load PDFs uh CPU and",
    "start": "893560",
    "end": "899800"
  },
  {
    "text": "extract and clean text on CPU and suppose this is uh streaming easy in this example the most difficult part is",
    "start": "899800",
    "end": "907000"
  },
  {
    "text": "to um extract embeddings for those text using the sentence Transformer model so",
    "start": "907000",
    "end": "913360"
  },
  {
    "text": "after this pipeline we will have embedding factors for each PDF file and this embedding can be used for data",
    "start": "913360",
    "end": "919680"
  },
  {
    "text": "retrieval or the PDF search so in order to generate such embeddings um we use R",
    "start": "919680",
    "end": "926199"
  },
  {
    "text": "data with 20 gpus with 100% GPU utilization so this 2K uh 2K PDFs which",
    "start": "926199",
    "end": "932319"
  },
  {
    "text": "include 30k pages and would like to generate 140k encodings um we use 20 gpus and this",
    "start": "932319",
    "end": "940199"
  },
  {
    "text": "will take about just four minutes round time to finish all the process uh all the processing and by the trace on the",
    "start": "940199",
    "end": "946759"
  },
  {
    "text": "right hand side we can see once the TX is kick off all the gpus are basically",
    "start": "946759",
    "end": "952120"
  },
  {
    "text": "saturated basically all the time for the entire four minutes and by doing so you",
    "start": "952120",
    "end": "957800"
  },
  {
    "text": "only need to pay less than $1 to finish the 2K PDF processing and of course we have some",
    "start": "957800",
    "end": "964519"
  },
  {
    "text": "more examples um which are available on N scale blog post you're free free to",
    "start": "964519",
    "end": "969560"
  },
  {
    "text": "check out um we do have more example case studies with the higher a much higher scalability so I do encourage you",
    "start": "969560",
    "end": "976759"
  },
  {
    "text": "to um check out and see what other use cases so now um I want to dive into the",
    "start": "976759",
    "end": "984199"
  },
  {
    "text": "large language model part the previous example I show you can use Ray data to construct Pipeline and basically",
    "start": "984199",
    "end": "990880"
  },
  {
    "text": "leverage Leverage The Machine learning model to process the PDF file however if",
    "start": "990880",
    "end": "996480"
  },
  {
    "text": "we get into the large language model you'll bring up other more challenges what's the so first of all what's the",
    "start": "996480",
    "end": "1002880"
  },
  {
    "text": "motivation because we know that large language models are actually much more powerful to process the various text you",
    "start": "1002880",
    "end": "1009240"
  },
  {
    "text": "can that do the summarization tagging or sematic analysis um keyword extraction",
    "start": "1009240",
    "end": "1014920"
  },
  {
    "text": "or onra data processing etc etc you can do much more things than the traditional machine learning",
    "start": "1014920",
    "end": "1020720"
  },
  {
    "text": "models however it also brings challenges the most important challenges bring up",
    "start": "1020720",
    "end": "1025880"
  },
  {
    "text": "is large because it's called large L model so it's basically too large to achieve the high throughput in a very",
    "start": "1025880",
    "end": "1032438"
  },
  {
    "text": "naive way um so here is one possibility of your data Pipeline with large Lang",
    "start": "1032439",
    "end": "1038520"
  },
  {
    "text": "model now in the data pre-processing stage you not just do the data processing in the way you like you also",
    "start": "1038520",
    "end": "1044839"
  },
  {
    "text": "need to to tokenize your input data and you probably also need to code your image if your data has an image input um",
    "start": "1044839",
    "end": "1052960"
  },
  {
    "text": "this is because a large language model doesn't really take the text and image directly as the input instead it take",
    "start": "1052960",
    "end": "1060200"
  },
  {
    "text": "tokens tokens are basically integer representation of the inputs for example",
    "start": "1060200",
    "end": "1066480"
  },
  {
    "text": "a English word can be positioned to one or multiple tokens and the large engage",
    "start": "1066480",
    "end": "1072679"
  },
  {
    "text": "model will s tokens and do the mathematic uh computation like me modication and so on to calculate the",
    "start": "1072679",
    "end": "1079159"
  },
  {
    "text": "features and hidden States and then determine what's going to response so you have to do the tokenization to",
    "start": "1079159",
    "end": "1086440"
  },
  {
    "text": "transform the text and images to the tokens and send it to the large langage model and after you got the response the",
    "start": "1086440",
    "end": "1093200"
  },
  {
    "text": "response is also in the token format so you have to do DET tokenization to convert those tokens back to the text so",
    "start": "1093200",
    "end": "1100320"
  },
  {
    "text": "we can now see that we have a more complicated Pipeline and also this large langage model has you oftenly has to be",
    "start": "1100320",
    "end": "1107880"
  },
  {
    "text": "executed on multiple gpus instead of one so those are the challenges we need to deal",
    "start": "1107880",
    "end": "1113240"
  },
  {
    "text": "with and in the summary by the end of this talk I want to introduce raym batch",
    "start": "1113240",
    "end": "1119840"
  },
  {
    "text": "this is the solution we have by combining VM and Ray data so to achieve",
    "start": "1119840",
    "end": "1125400"
  },
  {
    "text": "the scalable Ln batch processing but before that I want to introduce uh vrn a",
    "start": "1125400",
    "end": "1131559"
  },
  {
    "text": "bit so VN itself is the most popular all",
    "start": "1131559",
    "end": "1136640"
  },
  {
    "text": "model inference framework um is fully open sourc and it's also part of the",
    "start": "1136640",
    "end": "1141799"
  },
  {
    "text": "Linux Foundation it now has more than 30k star on the GitHub and we merge more",
    "start": "1141799",
    "end": "1146840"
  },
  {
    "text": "than 200 um PRS every month and there are notable contributors including N",
    "start": "1146840",
    "end": "1152760"
  },
  {
    "text": "scale a21 Alibaba AWS and S SRA you can see a lot of companies are actively contributing to uh VM to make it better",
    "start": "1152760",
    "end": "1160919"
  },
  {
    "text": "and have more features for the log model inference um there also a lot of adapter",
    "start": "1160919",
    "end": "1168080"
  },
  {
    "text": "um that use vrm in their product they're including some open source projects and the",
    "start": "1168080",
    "end": "1174480"
  },
  {
    "text": "companies so I want to talk a bit about what's the key features um we use the in",
    "start": "1174480",
    "end": "1180640"
  },
  {
    "text": "end and what we have already contribute but before that I want to take a step back to briefly introduce how large D",
    "start": "1180640",
    "end": "1187559"
  },
  {
    "text": "model do the inference different from the traditional machine learning model that you simply send input and do the",
    "start": "1187559",
    "end": "1193799"
  },
  {
    "text": "model forwarding and get output log model uses the way so-called Auto",
    "start": "1193799",
    "end": "1199039"
  },
  {
    "text": "regressive decoding to in to do the inference this is basic basically",
    "start": "1199039",
    "end": "1204280"
  },
  {
    "text": "because um if you just run the model once you will just generate one token and like I mentioned if you want X model",
    "start": "1204280",
    "end": "1211480"
  },
  {
    "text": "to response you with the text or the paragraph you will need a tons of tokens and those tokens have to be generated",
    "start": "1211480",
    "end": "1218600"
  },
  {
    "text": "one by one in the iterative fashion so that's why we call that auto regressive because for an example we can see that",
    "start": "1218600",
    "end": "1225600"
  },
  {
    "text": "we have the prompt and then do the tokenization it will be converted to the number of tokens and then doing the one",
    "start": "1225600",
    "end": "1232360"
  },
  {
    "text": "forward we just predict one next uh one next token and one next word and then we will send this back to the model so this",
    "start": "1232360",
    "end": "1240080"
  },
  {
    "text": "is called Auto regressive so you will send the output become the input of the next iteration and generate next token",
    "start": "1240080",
    "end": "1246360"
  },
  {
    "text": "you have to do that iteratively um to generate the complete response um so you can see this is",
    "start": "1246360",
    "end": "1253000"
  },
  {
    "text": "actually a very long dist long latency process if you use the chat GPT or other uh chap providers you can see the",
    "start": "1253000",
    "end": "1259200"
  },
  {
    "text": "response you always output in streaming way that's not because that's not only just because the ux perspective it's",
    "start": "1259200",
    "end": "1266000"
  },
  {
    "text": "because the model generates the output in exactly in that way now from the",
    "start": "1266000",
    "end": "1271320"
  },
  {
    "text": "production or the vendor point of view we want to maximize the throughput in the server side to reduce the cost so we",
    "start": "1271320",
    "end": "1277600"
  },
  {
    "text": "cannot just generate one response by fully utilizing the GPU resource at a",
    "start": "1277600",
    "end": "1282919"
  },
  {
    "text": "time we have to batch all the requests and generate request together to minimize our cost and maximize the",
    "start": "1282919",
    "end": "1289279"
  },
  {
    "text": "system throughput so one important technology being introduced is called continuous batching basically you want",
    "start": "1289279",
    "end": "1295559"
  },
  {
    "text": "to batch the request coming from the users and um decode them together in this example we have the first request",
    "start": "1295559",
    "end": "1302360"
  },
  {
    "text": "coming in and we first do the prefill to generate the KV cach for the input",
    "start": "1302360",
    "end": "1307679"
  },
  {
    "text": "prompt and after that we get into the decode stage to generate the response uh",
    "start": "1307679",
    "end": "1312760"
  },
  {
    "text": "response token one by one so let's say in this time period you got the second request coming in",
    "start": "1312760",
    "end": "1319559"
  },
  {
    "text": "um because we first have the prefill first policy so we will pass the decode",
    "start": "1319559",
    "end": "1324720"
  },
  {
    "text": "process and first do the prefill for the second request and after that we will",
    "start": "1324720",
    "end": "1330279"
  },
  {
    "text": "let this pref this second request join the decode group to batch decoding",
    "start": "1330279",
    "end": "1335880"
  },
  {
    "text": "together so you can see at this moment our batch size becomes two because we're decoding two requests together so this",
    "start": "1335880",
    "end": "1341279"
  },
  {
    "text": "is called continuous batching now we have um two more requests coming in so we pause the decode process again and",
    "start": "1341279",
    "end": "1348520"
  },
  {
    "text": "and do this prefill and then let all the requests join together and decode together so this is a normal um batch",
    "start": "1348520",
    "end": "1356480"
  },
  {
    "text": "processing uh normal batch processing with continuous batching now we can see an obvious",
    "start": "1356480",
    "end": "1362600"
  },
  {
    "text": "challenge for this algorithm to do the Contin batching first you can see that",
    "start": "1362600",
    "end": "1367760"
  },
  {
    "text": "the decode process has to be interrupt when the new request coming in so from the user point of view you may see the",
    "start": "1367760",
    "end": "1374279"
  },
  {
    "text": "model is generating some test and then pause for a while and then continue generating that um this is because of",
    "start": "1374279",
    "end": "1381320"
  },
  {
    "text": "this Behavior and the second part is you can see if there's a long prompt let's",
    "start": "1381320",
    "end": "1386480"
  },
  {
    "text": "say the request three has the input token 1.5k and this will slow down the entire",
    "start": "1386480",
    "end": "1392720"
  },
  {
    "text": "batching of the request so you can see the latency between this token and this token is actually much longer than this",
    "start": "1392720",
    "end": "1398919"
  },
  {
    "text": "token and this token so in order to solve this problem an important",
    "start": "1398919",
    "end": "1404600"
  },
  {
    "text": "technology or the feature that's already introduced and integrated into the V called a trunk prefill the basic idea is",
    "start": "1404600",
    "end": "1411120"
  },
  {
    "text": "we want to split the long prompts to multiple Trunks and then we want to batch uh we want to bad chunk prompts",
    "start": "1411120",
    "end": "1417640"
  },
  {
    "text": "and decod tokens together so again in this example when the new request coming",
    "start": "1417640",
    "end": "1422679"
  },
  {
    "text": "in instead of passing the current decode uh we actually let current decode join",
    "start": "1422679",
    "end": "1428679"
  },
  {
    "text": "the prefill batch so we can do the prefill to get decode together in the same batch and let's continue doing",
    "start": "1428679",
    "end": "1436559"
  },
  {
    "text": "that now the request three is coming in again we don't have to pause the decoding process of the existing two",
    "start": "1436559",
    "end": "1443000"
  },
  {
    "text": "requests we can directly introduce the new request coming in and then we do the trun prefill for the request three",
    "start": "1443000",
    "end": "1450080"
  },
  {
    "text": "followed by request four and now we have all the decoding process together so",
    "start": "1450080",
    "end": "1456440"
  },
  {
    "text": "this becomes an ideal situ ideal case that we can balance all the batch sizes",
    "start": "1456440",
    "end": "1461480"
  },
  {
    "text": "between all the time frames and then we we also we also make sure the code process won't be interrupt and this",
    "start": "1461480",
    "end": "1468480"
  },
  {
    "text": "feature is basically contribut by aay which uh he is the Chun pre author and",
    "start": "1468480",
    "end": "1473520"
  },
  {
    "text": "PhD student and also sing from Nale so here is a experimental result",
    "start": "1473520",
    "end": "1480600"
  },
  {
    "text": "for enabling trun Prill in your model serving you can see that we basically have 1.4x itl Improvement itl stand for inter",
    "start": "1480600",
    "end": "1488600"
  },
  {
    "text": "token latency so from the user point of view you will see the requests coming out much faster than before but we're",
    "start": "1488600",
    "end": "1496520"
  },
  {
    "text": "also suffering from 30% ttft slowdown ttft stands for time to the first token",
    "start": "1496520",
    "end": "1502360"
  },
  {
    "text": "this is easy to imagine because now for the long prompts you need multiple batches to process them all so this is",
    "start": "1502360",
    "end": "1508559"
  },
  {
    "text": "being sacrificed by this technology but because we have the much higher it Improvement the overall n latency is",
    "start": "1508559",
    "end": "1516000"
  },
  {
    "text": "also reduced so the overall system throughput is also improved by enabling TR",
    "start": "1516000",
    "end": "1522000"
  },
  {
    "text": "prefill so the next uh feature I want to introduce is the previous caching um",
    "start": "1522000",
    "end": "1527440"
  },
  {
    "text": "this is basically very very straightforward we want to reuse the KV cach from another request with the same",
    "start": "1527440",
    "end": "1532559"
  },
  {
    "text": "bat so that we don't have to compute the same um pden State for the same tokens again and again there are two obvious",
    "start": "1532559",
    "end": "1540000"
  },
  {
    "text": "examples um that pre cach can help a lot the first one is called uh shared system",
    "start": "1540000",
    "end": "1545080"
  },
  {
    "text": "prompt you can imagine if you're hosting a service a trar or something like that you probably have the system",
    "start": "1545080",
    "end": "1550480"
  },
  {
    "text": "instructions across all the requests from the for example the open a point of view they will have a very long system",
    "start": "1550480",
    "end": "1556760"
  },
  {
    "text": "instruction to tell the model you use for as since your response should follow this rule 123 and those are",
    "start": "1556760",
    "end": "1564159"
  },
  {
    "text": "consistent across all the requests so we want to share that and avoid computation",
    "start": "1564159",
    "end": "1569559"
  },
  {
    "text": "of those instruction prompts and the second example is a Mone conversation um",
    "start": "1569559",
    "end": "1576559"
  },
  {
    "text": "you can imagine when you're talking to the chatbot uh when you response the model with the new question the previous",
    "start": "1576559",
    "end": "1583120"
  },
  {
    "text": "chatting history is actually reused from your previous round so this is also a share pref",
    "start": "1583120",
    "end": "1589360"
  },
  {
    "text": "um yeah so because of that we want to need use pref caching and this in the",
    "start": "1589360",
    "end": "1594720"
  },
  {
    "text": "vrm we have the hash based automatic pref caching the idea is pretty straightforward we basically chunk your",
    "start": "1594720",
    "end": "1600799"
  },
  {
    "text": "prompts into multiple blocks and then we use the token ID as the hash key and",
    "start": "1600799",
    "end": "1606679"
  },
  {
    "text": "when the new request coming in we will we'll use the token ID to see if we have already computed uh the blocks in the",
    "start": "1606679",
    "end": "1613760"
  },
  {
    "text": "system if so we just directly reuse that block without recomputing that that and",
    "start": "1613760",
    "end": "1619360"
  },
  {
    "text": "this feature uh is basically lead by Johan from Berkeley and me from any",
    "start": "1619360",
    "end": "1624760"
  },
  {
    "text": "scale and another one is called Spectre decoding um basically the idea is also",
    "start": "1624760",
    "end": "1630440"
  },
  {
    "text": "very straightforward when you're serving the large model like 70 model the latency is usually pretty high but um",
    "start": "1630440",
    "end": "1637279"
  },
  {
    "text": "you may imagine that this model like sometimes the model will response a very",
    "start": "1637279",
    "end": "1642919"
  },
  {
    "text": "simple output and in this case use a small model can maybe fulfill your requirement over already so specular",
    "start": "1642919",
    "end": "1649600"
  },
  {
    "text": "decoding is basically a case that we deploy both small model and large model at the same time and when the request is",
    "start": "1649600",
    "end": "1655840"
  },
  {
    "text": "coming in we use a small model to generate the speculative tokens and the",
    "start": "1655840",
    "end": "1661240"
  },
  {
    "text": "let the large model to verify that because the verification cost is much lower than generating the token so if",
    "start": "1661240",
    "end": "1668559"
  },
  {
    "text": "the small model can guess the token in a precise way we can sign significantly",
    "start": "1668559",
    "end": "1674720"
  },
  {
    "text": "reduce uh the latency for generating the response and this is also an important feature",
    "start": "1674720",
    "end": "1681159"
  },
  {
    "text": "contribut by Kate and L from inscale last but not least um I",
    "start": "1681159",
    "end": "1687320"
  },
  {
    "text": "introduce the last feature I want to introduce today is the pipeline parm um",
    "start": "1687320",
    "end": "1692679"
  },
  {
    "text": "like I mentioned in the beginning the large language model may not be fitting into the single GPU due to the number of",
    "start": "1692679",
    "end": "1699440"
  },
  {
    "text": "its uh parameters um so usually we will need more than one gpus to host one single",
    "start": "1699440",
    "end": "1705880"
  },
  {
    "text": "replica for the model um that means you have to paralyze the model execution in",
    "start": "1705880",
    "end": "1710960"
  },
  {
    "text": "some ways there are two famous model parm strategies you can do tensor parm or pipeline parm tens parm is a",
    "start": "1710960",
    "end": "1719559"
  },
  {
    "text": "straightforward approach basically you just paralyze the execution of the MRI modifcation and then aggregate results",
    "start": "1719559",
    "end": "1726399"
  },
  {
    "text": "after each layers so this implementation is extremely simple but you to",
    "start": "1726399",
    "end": "1731840"
  },
  {
    "text": "introduces the very high communication overheads so it is good for latency uh",
    "start": "1731840",
    "end": "1737880"
  },
  {
    "text": "good for improving latency on the single node and all of your gpus have a very",
    "start": "1737880",
    "end": "1743080"
  },
  {
    "text": "high Bend with inter GPU communication such as EnV link on the other hand the pipeline parm is a low communication",
    "start": "1743080",
    "end": "1751000"
  },
  {
    "text": "overhead solution that basically partion different decoding layers to different gpus and pipeline the execution together",
    "start": "1751000",
    "end": "1759240"
  },
  {
    "text": "um this is very good for throughput because you can saturate all the gpus um",
    "start": "1759240",
    "end": "1764720"
  },
  {
    "text": "on different uh pipeline stages um so this often very useful for offline batching on the community gpus which",
    "start": "1764720",
    "end": "1771799"
  },
  {
    "text": "doesn't have the MV link so the inter GPU communication has to go through the",
    "start": "1771799",
    "end": "1777120"
  },
  {
    "text": "PCI and we have already actively using the pipeline parm in the VN for the bat",
    "start": "1777120",
    "end": "1782640"
  },
  {
    "text": "inference and this feature was contributed by moral from CML and sing FR",
    "start": "1782640",
    "end": "1788480"
  },
  {
    "text": "scale so here is an example of the pipeline parm uh one thing I do want to",
    "start": "1788480",
    "end": "1793640"
  },
  {
    "text": "highlight is although pip parm is a intuitive solution to achieve the high throughput for the bach inference it's",
    "start": "1793640",
    "end": "1800159"
  },
  {
    "text": "not straightforward to achieve the best throughput out of the box for example in",
    "start": "1800159",
    "end": "1805559"
  },
  {
    "text": "this case we show that we use a gpus a L4 gpus to serve a single Lama 3.17 B",
    "start": "1805559",
    "end": "1812880"
  },
  {
    "text": "billion model and we can see TPA stand for tensor parm with hpus and PPA stand",
    "start": "1812880",
    "end": "1818720"
  },
  {
    "text": "for pipeline parm on hpus we can see that although pipeline paron should give",
    "start": "1818720",
    "end": "1824200"
  },
  {
    "text": "us a better third put the third put shown here is actually worse than parm and this is because we don't really",
    "start": "1824200",
    "end": "1832080"
  },
  {
    "text": "optimize the pipeline efficiency in this setting why first of all we have to",
    "start": "1832080",
    "end": "1838440"
  },
  {
    "text": "unbalance the pipeline stage execution time because in a large engage model the basic architecture you can imagine is",
    "start": "1838440",
    "end": "1844480"
  },
  {
    "text": "the repeated decoding layers but actually in the beginning and the end of your model you have the embedding layer",
    "start": "1844480",
    "end": "1851880"
  },
  {
    "text": "uh the first layer a first embedding layer will convert the tokens to the embedding factors and the last layer",
    "start": "1851880",
    "end": "1858039"
  },
  {
    "text": "will also uh convert the hidden States back to the embedding factors for your",
    "start": "1858039",
    "end": "1863279"
  },
  {
    "text": "vocabularies so that means the first stage and the second stage in your pipeline must have higher execution time",
    "start": "1863279",
    "end": "1870559"
  },
  {
    "text": "than other stages if you partion the decoding layers evenly so we have to",
    "start": "1870559",
    "end": "1877960"
  },
  {
    "text": "consider this factors when we petition the decoder decoding layers specifically in this uh in this evaluation we just",
    "start": "1877960",
    "end": "1885159"
  },
  {
    "text": "allocate last decoding layers in the first and last stages to balance all to balance the execution time of all the",
    "start": "1885159",
    "end": "1891240"
  },
  {
    "text": "pipeline stages and you can see by doing so we have already achieved the similar throughput as the T parison but uh same",
    "start": "1891240",
    "end": "1900279"
  },
  {
    "text": "performance is not good as well it's not good either because we expect a much higher throughput so another bottom neck",
    "start": "1900279",
    "end": "1908399"
  },
  {
    "text": "is the unbalanced batch sizes between peline stages imagine you have the",
    "start": "1908399",
    "end": "1913519"
  },
  {
    "text": "pipeline stag with a you have the pipeline with a stages in this example and in the first Tim stamp you're",
    "start": "1913519",
    "end": "1919519"
  },
  {
    "text": "processing a prefill with 2K tokens and followed by a decode with just 10 tokens",
    "start": "1919519",
    "end": "1925519"
  },
  {
    "text": "although the badge with 10 tokens can be processed much faster um there's a prefill um block uh in front of you to",
    "start": "1925519",
    "end": "1933639"
  },
  {
    "text": "block your execution so this imbalance will create the pipeline Bubbles and reduce the pipeline efficiency so our",
    "start": "1933639",
    "end": "1940760"
  },
  {
    "text": "solution is to enable the trunk prefill I introduced uh in the beginning of the session so we can make sure every pipe",
    "start": "1940760",
    "end": "1948240"
  },
  {
    "text": "at every batch has a similar number of tokens so the execution time of each batch can be balanced across all the",
    "start": "1948240",
    "end": "1954440"
  },
  {
    "text": "pipeline stages and you can see the results pretty amazing we basically get almost 2x for improvement by simply",
    "start": "1954440",
    "end": "1961519"
  },
  {
    "text": "enable the trunk prefill but this is not the end we actually can achieve the Much Higher",
    "start": "1961519",
    "end": "1967559"
  },
  {
    "text": "throughput by tweaking another configuration in this example if we change the configuration to use two gpus",
    "start": "1967559",
    "end": "1975480"
  },
  {
    "text": "to do the T parm and use four gpus to to do the pipeline parm we can actually achieve a much higher throughput and",
    "start": "1975480",
    "end": "1981720"
  },
  {
    "text": "this is because um if we use the pipeline stage we four pipeline stages",
    "start": "1981720",
    "end": "1986840"
  },
  {
    "text": "we can actually balance the execution time in a better way uh by given the number of decoding layers in this 70b",
    "start": "1986840",
    "end": "1993720"
  },
  {
    "text": "model so you can imagine this Bas uh this configuration tuning is basically Case by case and there's no optimal",
    "start": "1993720",
    "end": "2000799"
  },
  {
    "text": "solution across all the workflows and models okay finally uh let me get you to",
    "start": "2000799",
    "end": "2008760"
  },
  {
    "text": "the our my final goal to combine VM this is a powerful LM inference Engine with",
    "start": "2008760",
    "end": "2015760"
  },
  {
    "text": "rate data to construct a high scalable uh data uh bat inference pipeline so Ray",
    "start": "2015760",
    "end": "2023320"
  },
  {
    "text": "earn bat is N scale library buil on buil for large scale cost optimized batch inference the key features is we can you",
    "start": "2023320",
    "end": "2029799"
  },
  {
    "text": "can bring any open source model and it's dealing with the for tolerance so we can",
    "start": "2029799",
    "end": "2035159"
  },
  {
    "text": "uh use the spot instances at scale to save the money and we also have the optimizer for the custom workloads and",
    "start": "2035159",
    "end": "2042080"
  },
  {
    "text": "there is a simp simple Cod snip as being used by our customers firstly you just need to Define your workload for example",
    "start": "2042080",
    "end": "2048079"
  },
  {
    "text": "how you read a data how you want to process a data and then you just need to specify how many gpus you have and you",
    "start": "2048079",
    "end": "2053679"
  },
  {
    "text": "want to use that for the batch inference and then the rest of the thing will be deal will be deal with the um R",
    "start": "2053679",
    "end": "2061638"
  },
  {
    "text": "batch other than that we have the inference engine Optimizer uh like I mentioned in the pipeline compon example",
    "start": "2061639",
    "end": "2069280"
  },
  {
    "text": "um you have to optimize the engine configuration in terms of number of gpus",
    "start": "2069280",
    "end": "2074638"
  },
  {
    "text": "uh number of pipeline stages or even the batch sizes whether enable chunk prefill or something like that um this actually",
    "start": "2074639",
    "end": "2082158"
  },
  {
    "text": "pretty complicated in the right hand side there's a very scary usage example dumped from the vrm open source there's",
    "start": "2082159",
    "end": "2089960"
  },
  {
    "text": "basically thousands of different configurations you can play with to achieve the Optimal Performance um this",
    "start": "2089960",
    "end": "2096480"
  },
  {
    "text": "actually pretty painful so in N scale uh we have the inference engine Optimizer",
    "start": "2096480",
    "end": "2102160"
  },
  {
    "text": "which is pretty kind of straightforward we have the autot tuner to explore those possible configurations and then",
    "start": "2102160",
    "end": "2108720"
  },
  {
    "text": "evaluate those configurations um on rgp clusters and because of Ray which is",
    "start": "2108720",
    "end": "2114359"
  },
  {
    "text": "already very scalable and reliable we simply build this Optimizer on top of Ray so that we can tune um lot of",
    "start": "2114359",
    "end": "2122079"
  },
  {
    "text": "configurations in a very short period of time okay so find is a case study um we",
    "start": "2122079",
    "end": "2129680"
  },
  {
    "text": "use a synthetic data set with uh 320 million tokens and then we enable TR",
    "start": "2129680",
    "end": "2134920"
  },
  {
    "text": "prefill pre cashing and pipeline parm like I introduced before um and also so",
    "start": "2134920",
    "end": "2141000"
  },
  {
    "text": "from this result we can see the engine Optimizer can help reduce the total processing time by up to 34% it's",
    "start": "2141000",
    "end": "2146880"
  },
  {
    "text": "showing the right hand side and also the previous caching can help reduce the total process Time by up to up to 64%",
    "start": "2146880",
    "end": "2154240"
  },
  {
    "text": "with the 80% share prefix and we can dive into this figure in the left hand",
    "start": "2154240",
    "end": "2159319"
  },
  {
    "text": "side we have the Lama 3.1 AB on uh l4s gpus and the right hand side is the 70b",
    "start": "2159319",
    "end": "2165760"
  },
  {
    "text": "so we can see that by enabling the engine Optimizer and the previous caching we can reduce the call by almost",
    "start": "2165760",
    "end": "2172119"
  },
  {
    "text": "an order of magnitude and then this also again illustrates that different workflows and different models will",
    "start": "2172119",
    "end": "2178800"
  },
  {
    "text": "require different optimizations okay finally uh takeways",
    "start": "2178800",
    "end": "2185280"
  },
  {
    "text": "uh in this talk we mentioned that a large scale bat inference actually becomes high demand workloads in the AI",
    "start": "2185280",
    "end": "2190920"
  },
  {
    "text": "era and the r data can actually push the scalability of the bat inference to",
    "start": "2190920",
    "end": "2195960"
  },
  {
    "text": "thousand of notes and larage model actually make bat inference more even",
    "start": "2195960",
    "end": "2201000"
  },
  {
    "text": "more practical and vrm has enabled most features for high throughput a inference",
    "start": "2201000",
    "end": "2207599"
  },
  {
    "text": "and it works out of box but if you combine vrm with Ray data to become raym",
    "start": "2207599",
    "end": "2214720"
  },
  {
    "text": "batch that would be an ideal solution for language uh large scale bch",
    "start": "2214720",
    "end": "2220240"
  },
  {
    "text": "inference that's all my talk and thank you for attention and happy to take questions",
    "start": "2220240",
    "end": "2226670"
  },
  {
    "text": "[Applause]",
    "start": "2226670",
    "end": "2230449"
  },
  {
    "text": "all right I'll I'll ask a question um are there any examples of what um you've",
    "start": "2248000",
    "end": "2254079"
  },
  {
    "text": "used Ray llm badge to build for example uh yeah we do have lot of like customer",
    "start": "2254079",
    "end": "2261079"
  },
  {
    "text": "examples I cannot reveal all the details here um but some examples I can probably",
    "start": "2261079",
    "end": "2266359"
  },
  {
    "text": "share in high level is we have a customer to use R and bad to process their uh customer data and generating",
    "start": "2266359",
    "end": "2273280"
  },
  {
    "text": "the uh possible next RIS like next action item like how do we deal with this customer where we should uh give",
    "start": "2273280",
    "end": "2279920"
  },
  {
    "text": "them the discount or we should like green them more frequently or something like that um so they process like",
    "start": "2279920",
    "end": "2287000"
  },
  {
    "text": "thousands of millions of customer data using Ray and Bash another example is probably uh there's a there's a our",
    "start": "2287000",
    "end": "2294119"
  },
  {
    "text": "customer they wants to they have the like customer purchase history there's like grocery the kind of customers um",
    "start": "2294119",
    "end": "2301760"
  },
  {
    "text": "they have the customer purchase history they want to have some insight from the processing history so they th the",
    "start": "2301760",
    "end": "2307880"
  },
  {
    "text": "process uh the purchase history with some system prompt and ask the model to analyze those purchase history and do",
    "start": "2307880",
    "end": "2315040"
  },
  {
    "text": "probably the recommendation and probably just get a sense about what items are being are getting popular across all the",
    "start": "2315040",
    "end": "2322240"
  },
  {
    "text": "customers and those are the obvious examples uh that's been already actively",
    "start": "2322240",
    "end": "2327440"
  },
  {
    "text": "used uh re B at the moment um but we believe there are a lot of more applications and I'm looking forward to",
    "start": "2327440",
    "end": "2334319"
  },
  {
    "text": "see uh what else people can build",
    "start": "2334319",
    "end": "2339280"
  },
  {
    "text": "with v LM you went over several different inference optimization techniques that are being used um which",
    "start": "2352720",
    "end": "2360160"
  },
  {
    "text": "one do you think might contribute more towards inference optimization that's a",
    "start": "2360160",
    "end": "2366520"
  },
  {
    "text": "good question um like I mentioned in order to question you have to make a lot",
    "start": "2366520",
    "end": "2371880"
  },
  {
    "text": "of assumptions about your workflows and models for example if your model is uh for example a bil and parameters um it",
    "start": "2371880",
    "end": "2379319"
  },
  {
    "text": "can be usually fited into single GPU then the most important feature could be",
    "start": "2379319",
    "end": "2384400"
  },
  {
    "text": "trunk fill improv scatching and probably floating points ex FPA execution",
    "start": "2384400",
    "end": "2389920"
  },
  {
    "text": "quantization but if you want to serve a much larger model like 70 billion or even 45 billions uh from the Lama 3",
    "start": "2389920",
    "end": "2398000"
  },
  {
    "text": "then you need multiple gpus uh in this case the most important feature I would say could be the pipeline parm so it",
    "start": "2398000",
    "end": "2404079"
  },
  {
    "text": "really depends on um your use case and worklow and for example if your workload doesn't have any previous sharing every",
    "start": "2404079",
    "end": "2411079"
  },
  {
    "text": "of your texts are having different prompts or even the prompt is actually pretty short then preious caching may",
    "start": "2411079",
    "end": "2417880"
  },
  {
    "text": "not be the one you want but on the other hand if you have 80% of the share prompt then previous caching plays the most",
    "start": "2417880",
    "end": "2424280"
  },
  {
    "text": "important role",
    "start": "2424280",
    "end": "2427599"
  },
  {
    "text": "yeah thanks really great uh talk I I have a question on the on the tuner you are using is it a more of a grid search",
    "start": "2442599",
    "end": "2450240"
  },
  {
    "text": "or do you use some you know size of activation size of weight and do just uh",
    "start": "2450240",
    "end": "2456760"
  },
  {
    "text": "you know as ated guest first and they start from there yeah this is a good question um so for our tuner this now is",
    "start": "2456760",
    "end": "2463640"
  },
  {
    "text": "uh is config in a flexible way so we would use we use different search algorism based on the r cases for",
    "start": "2463640",
    "end": "2469680"
  },
  {
    "text": "example for the certain cases that we already manually shrink the tuning space to a very small size like 10 or 20",
    "start": "2469680",
    "end": "2476800"
  },
  {
    "text": "configurations in this case we'll just use the GRE search or exha search um but for a more broad use cases which we may",
    "start": "2476800",
    "end": "2484119"
  },
  {
    "text": "have thousands of configuration points in this case we're experimenting different such algorithms currently",
    "start": "2484119",
    "end": "2490079"
  },
  {
    "text": "under our in our radar could be the basent optimization or uh the tree based",
    "start": "2490079",
    "end": "2495960"
  },
  {
    "text": "structure search or even the reinforcement B reinforcement learning based search algorithms um there are a",
    "start": "2495960",
    "end": "2502280"
  },
  {
    "text": "lot of um tuning libraries over the shelf so we're basically just understand we're not inventing the new such",
    "start": "2502280",
    "end": "2508200"
  },
  {
    "text": "algorithm but basically just see which algorithm is the most suitable one for our use cases and we want to make it",
    "start": "2508200",
    "end": "2514880"
  },
  {
    "text": "more efficient right right so s on the same line for for a model that",
    "start": "2514880",
    "end": "2520880"
  },
  {
    "text": "either fits in a GPU or or let's say 2 GPU is there any point on going you know",
    "start": "2520880",
    "end": "2527599"
  },
  {
    "text": "more in term of tensor parallel because uh it's it's just going to add the overhead right unless you are looking",
    "start": "2527599",
    "end": "2534319"
  },
  {
    "text": "for like lower latency is there any point on going on the tensor parallel route um Can the question so the",
    "start": "2534319",
    "end": "2541599"
  },
  {
    "text": "question is whether to use T perm or not yeah yeah so let's say my model fits in 2 GPU and I happy with latency is there",
    "start": "2541599",
    "end": "2548119"
  },
  {
    "text": "any point in going Tor parallel on 4 GPU yeah so like you mentioned very uh this",
    "start": "2548119",
    "end": "2555160"
  },
  {
    "text": "is very correct so T parison has two benefits one is uh you should lower the latency given that you paralyze the meas",
    "start": "2555160",
    "end": "2562079"
  },
  {
    "text": "application on Two gpus And the second um is basically you balance the workloads because like I mentioned in",
    "start": "2562079",
    "end": "2568000"
  },
  {
    "text": "order to make the pipeline par efficient you have to deal with the pipeline Bubbles and you want to make sure the extion time of each pipeline stage is",
    "start": "2568000",
    "end": "2574200"
  },
  {
    "text": "balanced but for tens per we don't have this problem because you naturally have the balance the workflow across all gpus",
    "start": "2574200",
    "end": "2580520"
  },
  {
    "text": "so that's why we call that simple and easy to um easy to optimize yeah thanks",
    "start": "2580520",
    "end": "2585920"
  },
  {
    "text": "I I have one more question so on the on the chonk prefilling what happens let's say uh let's say you have a continuous",
    "start": "2585920",
    "end": "2593160"
  },
  {
    "text": "batching Max batch size is 128 and then after some time let's say 16 of my uh",
    "start": "2593160",
    "end": "2600760"
  },
  {
    "text": "prompts are done right so they they have reached the stopping condition but rest of the prompts are still active",
    "start": "2600760",
    "end": "2607920"
  },
  {
    "text": "does the bats have to you know waste computation for those 16 queries that are already done yeah so this is",
    "start": "2607920",
    "end": "2615119"
  },
  {
    "text": "naturally handled by continuous batching uh with or without the trunk prefill so we can see that in each time stamp we",
    "start": "2615119",
    "end": "2621599"
  },
  {
    "text": "actually reconstruct a batch on the fly so if for example the request one is down in a certain time you will just be",
    "start": "2621599",
    "end": "2627960"
  },
  {
    "text": "kick off kick out to this batch and then directly return to the user so it don't",
    "start": "2627960",
    "end": "2633319"
  },
  {
    "text": "it doesn't have to be staying here until other requests are done so this is the",
    "start": "2633319",
    "end": "2638599"
  },
  {
    "text": "flexibility of the conven batching okay that's awesome thanks thank",
    "start": "2638599",
    "end": "2644160"
  },
  {
    "text": "you hi so for I guess companies or organizations that are getting started",
    "start": "2658480",
    "end": "2664119"
  },
  {
    "text": "with this like you mentioned some overhead trade off so is it um easy for",
    "start": "2664119",
    "end": "2669960"
  },
  {
    "text": "let's say like a like a startup to uh get started what are the main maybe",
    "start": "2669960",
    "end": "2675680"
  },
  {
    "text": "challenges if they're doing that um I would say based on all the",
    "start": "2675680",
    "end": "2681319"
  },
  {
    "text": "customers we have dealing with and people were collaborating in open source Community it's actually kind of a",
    "start": "2681319",
    "end": "2687160"
  },
  {
    "text": "straightforward if you want to start with open source Solutions and tweak that by yourself to face those",
    "start": "2687160",
    "end": "2692359"
  },
  {
    "text": "trade-offs um but usually things get more challenging when you scaling out uh",
    "start": "2692359",
    "end": "2697400"
  },
  {
    "text": "with the like thousands hundreds of thousands notes um so usually uh our our",
    "start": "2697400",
    "end": "2703079"
  },
  {
    "text": "experience see from our customer is they have the prototyping with a small scale like five noes or so and once they want",
    "start": "2703079",
    "end": "2710240"
  },
  {
    "text": "to get into a higher scale ability it probably just seek for the help like us so that's the way they're dealing with",
    "start": "2710240",
    "end": "2716680"
  },
  {
    "text": "that but um yeah to answer the question shortly I would say tweaking the performance um for the small scaleable",
    "start": "2716680",
    "end": "2724359"
  },
  {
    "text": "for the small scale problems um could be kind of straightforward if you spend time on",
    "start": "2724359",
    "end": "2731440"
  },
  {
    "text": "that I'd be interested to hear the challenges you have with this uh prefix batching um to me to me it kind of flies",
    "start": "2740400",
    "end": "2748559"
  },
  {
    "text": "in the face of like does it lead to hallucinations have you have you seen accuracy problems once you start",
    "start": "2748559",
    "end": "2754359"
  },
  {
    "text": "batching or caching the pre section yeah yeah um so in terms of the output the",
    "start": "2754359",
    "end": "2761839"
  },
  {
    "text": "previous caching was will guarantee the exactly same output as before because we just avoid recomputation we are not",
    "start": "2761839",
    "end": "2768559"
  },
  {
    "text": "using approximation or anything like that so as long as you proves cash in",
    "start": "2768559",
    "end": "2774200"
  },
  {
    "text": "the right way the output will guaranteed to be the same of course there are some like more",
    "start": "2774200",
    "end": "2779760"
  },
  {
    "text": "Advanced Technologies like approximate pre cach like I only have a slightly difference compared to the previous PR",
    "start": "2779760",
    "end": "2785520"
  },
  {
    "text": "can I directly use uh his results in this way uh you may",
    "start": "2785520",
    "end": "2791160"
  },
  {
    "text": "still get some reasonable response but the accuracy may be changed yeah because I know proximity",
    "start": "2791160",
    "end": "2797839"
  },
  {
    "text": "matters and it's longtail as it goes off it matters less and less how how is the",
    "start": "2797839",
    "end": "2803880"
  },
  {
    "text": "how is that determined in the pre is it automatically determined or are you manually defining that yeah so in the v",
    "start": "2803880",
    "end": "2810640"
  },
  {
    "text": "we have automatically Pro caching like I mentioned we chunk the prompts in several blocks and then basically we",
    "start": "2810640",
    "end": "2816400"
  },
  {
    "text": "just see how many blocks you can match from the beginning so if you can match 50% of your blocks then you have the 50%",
    "start": "2816400",
    "end": "2823440"
  },
  {
    "text": "Computing reduction if you can only merge like 30% you can save only 30% so",
    "start": "2823440",
    "end": "2828640"
  },
  {
    "text": "it depends on two things one is whether your PR is actually shared with the previous request and a second whether",
    "start": "2828640",
    "end": "2834839"
  },
  {
    "text": "the compute uh results is still in the cache it may be evict if you have so many requests with different",
    "start": "2834839",
    "end": "2841359"
  },
  {
    "text": "prompts thank you thank you [Music]",
    "start": "2841359",
    "end": "2847289"
  },
  {
    "text": "thank you thank you",
    "start": "2855200",
    "end": "2859359"
  },
  {
    "text": "[Music]",
    "start": "2863320",
    "end": "2868770"
  }
]