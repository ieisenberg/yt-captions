[
  {
    "start": "0",
    "end": "87000"
  },
  {
    "text": "[Music]",
    "start": "610",
    "end": "11429"
  },
  {
    "text": "[Music]",
    "start": "13550",
    "end": "16739"
  },
  {
    "text": "um so let's say you want to interface with Twitter Facebook LinkedIn whatever social network um but you have a problem",
    "start": "18640",
    "end": "25680"
  },
  {
    "text": "because third party interfaces in general are hard uh lots of reasons for this just to few uh are that uh they are",
    "start": "25680",
    "end": "32439"
  },
  {
    "text": "slow it is so much slower to call an external service than anything you have internally uh even the uh hard dis that",
    "start": "32439",
    "end": "38520"
  },
  {
    "text": "Eric was showing last presentation uh but users don't understand this right they want results right now um they can",
    "start": "38520",
    "end": "44280"
  },
  {
    "text": "see their friends instantly on Facebook why can't they see them on your site why are you slowing down what they're trying to do uh another problem is that uh",
    "start": "44280",
    "end": "51800"
  },
  {
    "text": "third parties always have rate limits these vary widely from one service to another there are all kinds of weird",
    "start": "51800",
    "end": "57640"
  },
  {
    "text": "Corner cases and rules and some don't even publish their rate limits like Facebook so you have to deal with them",
    "start": "57640",
    "end": "63320"
  },
  {
    "text": "reactively rather than proactively uh this causes a lot of problems uh instability um yes even Facebook goes",
    "start": "63320",
    "end": "70960"
  },
  {
    "text": "down so you have to deal with outages uh you have to deal with random failures 500s 503s fail whales uh all the kinds",
    "start": "70960",
    "end": "78040"
  },
  {
    "text": "of stuff um that in your internal system you work really hard to work around uh you are confronted with them headon in",
    "start": "78040",
    "end": "85240"
  },
  {
    "text": "third- party uh interfaces uh so luckily um there is is a really great tool um built for the",
    "start": "85240",
    "end": "91960"
  },
  {
    "start": "87000",
    "end": "136000"
  },
  {
    "text": "python Community First integrated in ajango then split off called celery um it is a uh um distributed task Q",
    "start": "91960",
    "end": "100000"
  },
  {
    "text": "essentially uh and so you have chosen to use celery because it is asynchronous",
    "start": "100000",
    "end": "105320"
  },
  {
    "text": "which means that you don't have to do all of your third party interactions within the request response cycle uh it's distributed so you can spin up",
    "start": "105320",
    "end": "111479"
  },
  {
    "text": "pools of workers as you need them shut them down when you don't need them anymore uh it's fault tolerant has um",
    "start": "111479",
    "end": "116759"
  },
  {
    "text": "retri built in as a feature um you can fail safely and still uh maintain your",
    "start": "116759",
    "end": "122600"
  },
  {
    "text": "normal workflow but now you have two problems uh actually much more than two",
    "start": "122600",
    "end": "128360"
  },
  {
    "text": "up there but these are all design problems that come with dealing with distributed systems uh and celery is no",
    "start": "128360",
    "end": "134440"
  },
  {
    "text": "stranger to these kinds of problems before we dive into um each of these design issues uh you'll you may notice",
    "start": "134440",
    "end": "141720"
  },
  {
    "start": "136000",
    "end": "203000"
  },
  {
    "text": "throughout the course of this presentation that I have pretty strong opinions uh about what you should and should not be doing uh with celery and",
    "start": "141720",
    "end": "148360"
  },
  {
    "text": "how you should do it um you can feel free to trust me at my word or you can you know grow me later but one thing I",
    "start": "148360",
    "end": "155760"
  },
  {
    "text": "will say upfront always use rabid mq as your broker never use rabid mq as your",
    "start": "155760",
    "end": "161760"
  },
  {
    "text": "result store uh you will be so much happier if you just follow these rules um rabid mq has uh AC uh support um it",
    "start": "161760",
    "end": "170879"
  },
  {
    "text": "has you know built-in um exchanges and CU bindings and all kinds of routing features in it it is meant to be used as",
    "start": "170879",
    "end": "178319"
  },
  {
    "text": "a message queue un like reddis and all those others other backends that SEL supports uh rabbit mq however is a",
    "start": "178319",
    "end": "184920"
  },
  {
    "text": "terrible result store because as soon as you ask for the result once it's gone and you can never ask for it again uh so",
    "start": "184920",
    "end": "191040"
  },
  {
    "text": "if you have a distributed um system that's checking for when things are done one place asks for it it says uh yeah",
    "start": "191040",
    "end": "197480"
  },
  {
    "text": "I'm done or no I'm not done yet you ask for it again somewhere else and what task I don't have a task that is not",
    "start": "197480",
    "end": "204000"
  },
  {
    "start": "203000",
    "end": "257000"
  },
  {
    "text": "what you want from a result store it should store the result okay so let's get on to organization",
    "start": "204000",
    "end": "210439"
  },
  {
    "text": "you want to try to make your tasks small and as atomic as possible um the idea here is that workers are ephemeral units",
    "start": "210439",
    "end": "217920"
  },
  {
    "text": "uh you should be able to kill them at will spining them up at will uh and you get much better distribution from small",
    "start": "217920",
    "end": "224080"
  },
  {
    "text": "tasks um you can spread them out better parallelize them uh and you should",
    "start": "224080",
    "end": "229360"
  },
  {
    "text": "preferably only make one third party call per task if you have more than one API call um say you burned through the",
    "start": "229360",
    "end": "236239"
  },
  {
    "text": "first two you fail in the third one you now have essential lost two calls from your rate limit with",
    "start": "236239",
    "end": "242400"
  },
  {
    "text": "no effective work done provided that you need that third to complete your task if you separate them all and use a",
    "start": "242400",
    "end": "248319"
  },
  {
    "text": "dispatcher to call each one uh either in series or parallel then you keep each stage of your work as you go along and",
    "start": "248319",
    "end": "255079"
  },
  {
    "text": "you get like I said better distribution you want to try to keep task arguments uh to a minimal State um",
    "start": "255079",
    "end": "263199"
  },
  {
    "start": "257000",
    "end": "318000"
  },
  {
    "text": "a lot of uh reasons for this but uh for instance um model instances you should",
    "start": "263199",
    "end": "268800"
  },
  {
    "text": "pass the PK of the model model rather than the model instance um seler is actually serializing that full model",
    "start": "268800",
    "end": "274600"
  },
  {
    "text": "instance sending it up to your broker and back down to the workers uh you don't know how long that's going to take",
    "start": "274600",
    "end": "280039"
  },
  {
    "text": "and in the meantime you could have deployed new code that has a migration say so now your model uh instant state",
    "start": "280039",
    "end": "286479"
  },
  {
    "text": "is out of sync uh with the actual uh model definition in your code base lots of bad things can happen so you will",
    "start": "286479",
    "end": "293840"
  },
  {
    "text": "avoid all of this and you will minimize the size of your messages which means less memory overhead and faster message",
    "start": "293840",
    "end": "299280"
  },
  {
    "text": "passing if you only use uh minimal state in your message definitions um you",
    "start": "299280",
    "end": "304360"
  },
  {
    "text": "should defer data access to the task itself um again for the same reason because uh you don't want a lot of",
    "start": "304360",
    "end": "310960"
  },
  {
    "text": "information going over the queue um like I said it increases performance and it prevents serialization synchronization",
    "start": "310960",
    "end": "317840"
  },
  {
    "text": "issues so let's look at an example task uh this is a bad example uh it passes in",
    "start": "317840",
    "end": "323199"
  },
  {
    "start": "318000",
    "end": "344000"
  },
  {
    "text": "a model instance as I said not to do it makes several API calls in series in the same task and when it tries to save the",
    "start": "323199",
    "end": "329919"
  },
  {
    "text": "instance at the end of the task uh you could blow up here um as I've mentioned",
    "start": "329919",
    "end": "335080"
  },
  {
    "text": "um because you are now working with a d serialized instance of a model whose state is completely undefined in",
    "start": "335080",
    "end": "342080"
  },
  {
    "text": "relation to the database that you're working with instead if you pass in the model PK",
    "start": "342080",
    "end": "347880"
  },
  {
    "text": "you can avoid uh any race conditions by retrying if you get a do not or does not exist exception um this is actually good",
    "start": "347880",
    "end": "354800"
  },
  {
    "text": "it helps you maintain uh reality with your database um and then you can use",
    "start": "354800",
    "end": "360440"
  },
  {
    "text": "your task as a dispatcher for several API calls and get them all done in parallel so um as much as sary tries to",
    "start": "360440",
    "end": "369039"
  },
  {
    "start": "365000",
    "end": "391000"
  },
  {
    "text": "hide it from you on the surface tasks are classes you may Define a function but as soon as you decorate that",
    "start": "369039",
    "end": "374240"
  },
  {
    "text": "function you've made it into an instance of uh a subass of task um so uh tasks",
    "start": "374240",
    "end": "381720"
  },
  {
    "text": "are just classes don't be afraid to subass them yourself and make abstract parent task classes to encapsulate",
    "start": "381720",
    "end": "388479"
  },
  {
    "text": "common functionality that you need for a lot of your tasks uh so if you were to take uh say this task as an example uh",
    "start": "388479",
    "end": "396440"
  },
  {
    "text": "it does a lot of boiler plate stuff that you'll find that you're going to do over and over again between different API calls uh for this uh in this instance to",
    "start": "396440",
    "end": "403319"
  },
  {
    "text": "Twitter instead if you make an abstract uh task based class and you wrap all",
    "start": "403319",
    "end": "409080"
  },
  {
    "text": "that function up uh in you know say an API call function and then you just return the result of whatever your",
    "start": "409080",
    "end": "414680"
  },
  {
    "text": "client were to give you then you can make a subass of that take away a lot of that boiler plate and just use that API",
    "start": "414680",
    "end": "421479"
  },
  {
    "text": "call function over and over again um that has other benefits that we'll cover later the final uh recommendation for",
    "start": "421479",
    "end": "429599"
  },
  {
    "text": "organization is to make tasks at imp poent when possible uh tasks will fail",
    "start": "429599",
    "end": "434720"
  },
  {
    "text": "um you cannot stop tasks from failing they will fail in creative ways that you didn't expect it's a very easy fix to",
    "start": "434720",
    "end": "441360"
  },
  {
    "text": "say just run that task over again rather than to say oh well we were on in some",
    "start": "441360",
    "end": "446960"
  },
  {
    "text": "unknown State halfway through the task so we have to patch that up try to manually you know get it to the end",
    "start": "446960",
    "end": "452360"
  },
  {
    "text": "result um it's not always possible of course because you may have database uh rights that are not replicable you may",
    "start": "452360",
    "end": "458960"
  },
  {
    "text": "have rights to your third party services but when you can item poent will save",
    "start": "458960",
    "end": "464039"
  },
  {
    "text": "you grief so let's talk about task distribution uh every third party",
    "start": "464039",
    "end": "469720"
  },
  {
    "start": "465000",
    "end": "503000"
  },
  {
    "text": "service you're going to work with is going to have pagination of some kind when you're retrieving a list of resources and pages are very logical",
    "start": "469720",
    "end": "477319"
  },
  {
    "text": "places to break up your tasks because they're essentially uh instances of for Loop you're doing the same thing over and over again that",
    "start": "477319",
    "end": "483919"
  },
  {
    "text": "says to me distribution so the strategy is different dependent on whether you have access to limit offset uh through",
    "start": "483919",
    "end": "490840"
  },
  {
    "text": "the third party or whether you're dealing with a cursor that you get you know a value of a cursor for each page",
    "start": "490840",
    "end": "495879"
  },
  {
    "text": "and you call it with an x value of the cursor um sometimes you don't even know the total size of the set you're working",
    "start": "495879",
    "end": "501039"
  },
  {
    "text": "with so that can change your strategy as well so provided that you have the best case where you have limit offset and you",
    "start": "501039",
    "end": "507800"
  },
  {
    "start": "503000",
    "end": "668000"
  },
  {
    "text": "know your set size then the easiest way to handle pagination is to have a dispatcher task that instantly",
    "start": "507800",
    "end": "514760"
  },
  {
    "text": "dispatches every single page that it's going to need um to you know however big",
    "start": "514760",
    "end": "520120"
  },
  {
    "text": "your set size is uh and so you get 100 a th000 10,000 whatever tasks dumped on",
    "start": "520120",
    "end": "526360"
  },
  {
    "text": "the Queue at the same time the workers turn through them furiously fast and provided that you don't hit rate limits",
    "start": "526360",
    "end": "531399"
  },
  {
    "text": "you get really fast uh import of large number of objects from a third party",
    "start": "531399",
    "end": "537839"
  },
  {
    "text": "that very little code code this is the dispatcher that just goes through the range of offsets and launches",
    "start": "537839",
    "end": "547160"
  },
  {
    "text": "pages so let's say that you do not have limit offset um support in which case uh",
    "start": "547160",
    "end": "553800"
  },
  {
    "text": "whether you know the set size is irrelevant because you can't launch page two until you've done page one because",
    "start": "553800",
    "end": "559040"
  },
  {
    "text": "page one includes the cursor value you use to launch page two so it still makes sense to uh do this in separate tasks",
    "start": "559040",
    "end": "566320"
  },
  {
    "text": "because you get to save the bit of work you know uh from page to page but you can't run them in",
    "start": "566320",
    "end": "573440"
  },
  {
    "text": "parallel that would look something like this where there is no dispatcher the task itself dispatches the next page and",
    "start": "573440",
    "end": "580040"
  },
  {
    "text": "you start out with the Cur default cursor value of uh you know whatever zero um whatever the first one is and",
    "start": "580040",
    "end": "586160"
  },
  {
    "text": "you look for a sentinal cursor value that tells you you're done you launch uh one page after another um and uh yeah",
    "start": "586160",
    "end": "593640"
  },
  {
    "text": "until you get to the end so let's say you do have limit offset but you don't know your set size",
    "start": "593640",
    "end": "600760"
  },
  {
    "text": "um this is the case for I believe LinkedIn actually lies about the number of friends over a certain amount uh and",
    "start": "600760",
    "end": "606680"
  },
  {
    "text": "things like that so um you create a dispatcher task you determine how many concurrent Pages you want to launch at",
    "start": "606680",
    "end": "613200"
  },
  {
    "text": "the same time uh and then you have one page Leap Frog its other concurrent pages and request the next in the set so",
    "start": "613200",
    "end": "620880"
  },
  {
    "text": "page one will request page four page two page five on and on until you get to page six over there which doesn't have",
    "start": "620880",
    "end": "627040"
  },
  {
    "text": "any results and so it says there are no more pages I'm going to stop requesting more pages um the downside to this is",
    "start": "627040",
    "end": "632720"
  },
  {
    "text": "you make API calls you don't have to because Pages seven and eight could have been skipped if you ran them all uh as",
    "start": "632720",
    "end": "638680"
  },
  {
    "text": "the previous example one page launching another the upside is you get the parallelization in your distribution uh",
    "start": "638680",
    "end": "644920"
  },
  {
    "text": "multiple Pages at once um so the dispatcher for that method uh looks something like this you set a number of",
    "start": "644920",
    "end": "650720"
  },
  {
    "text": "concurrent pages and uh for each of those in the range you uh launch a",
    "start": "650720",
    "end": "655760"
  },
  {
    "text": "single page uh and then from within that page task it uh looks for results if",
    "start": "655760",
    "end": "662240"
  },
  {
    "text": "there are results in that page it then increments Itself by the number of concurrent pages and launches that next",
    "start": "662240",
    "end": "668760"
  },
  {
    "start": "668000",
    "end": "839000"
  },
  {
    "text": "page pagination is not a science it is an art you will find um based on uh uh",
    "start": "668760",
    "end": "676800"
  },
  {
    "text": "varying degrees of variance and response times from social networks um that",
    "start": "676800",
    "end": "682279"
  },
  {
    "text": "sometimes a social network will say like Facebook will say I can support 5,000 of these per page and they'll time out if",
    "start": "682279",
    "end": "688639"
  },
  {
    "text": "you ask for any more than 3,000 for instance sometimes you're bottlenecked on your database rights and so you can't",
    "start": "688639",
    "end": "694560"
  },
  {
    "text": "actually finish uh a page of results in a good amount of time um your goal here",
    "start": "694560",
    "end": "700040"
  },
  {
    "text": "is to minimize the number of API calls over the set of pages but you may have to make your page size smaller than the",
    "start": "700040",
    "end": "705720"
  },
  {
    "text": "maximum in order to get tasks done in a reasonable amount of time um and uh",
    "start": "705720",
    "end": "712160"
  },
  {
    "text": "remember um you need to minimize a state in your task definitions so it's a very bad idea to say um request all all of uh",
    "start": "712160",
    "end": "719920"
  },
  {
    "text": "person's friends uh in in a single task and then just pass off thousands of",
    "start": "719920",
    "end": "725079"
  },
  {
    "text": "friends in messages to other workers because you'll get megabytes of uh",
    "start": "725079",
    "end": "730279"
  },
  {
    "text": "message sizes that have to live in memory in rabbit and you'll eat up your me memory real fast rabbit's a little",
    "start": "730279",
    "end": "735959"
  },
  {
    "text": "bit better about that these days but there was a time not too long ago when rabbit would just flat out like hard",
    "start": "735959",
    "end": "741560"
  },
  {
    "text": "crash if he ran a memory no warning no nothing just everything's gone that's",
    "start": "741560",
    "end": "747120"
  },
  {
    "text": "bad um you want set a timeout uh on your task selary gives you a way to set a",
    "start": "747120",
    "end": "752760"
  },
  {
    "text": "soft timeout and a hard timeout um so that uh you can uh deploy code easily um",
    "start": "752760",
    "end": "759440"
  },
  {
    "text": "so that you make sure that you know when you have long running tasks and you can go and fix that and it's it's a problem",
    "start": "759440",
    "end": "765040"
  },
  {
    "text": "that you know you know about you so you can solve uh so um another problem with",
    "start": "765040",
    "end": "770320"
  },
  {
    "text": "uh task distribution is dependency when you have tasks launching tasks launching tasks it's very hard to answer the",
    "start": "770320",
    "end": "776560"
  },
  {
    "text": "question am I done with this set of work um because you have this dependency graph and it could be a very complex",
    "start": "776560",
    "end": "782680"
  },
  {
    "text": "graph depending on how your tasks are structured um so luckily celery 3 if you",
    "start": "782680",
    "end": "787959"
  },
  {
    "text": "haven't checked out celery 3 yet a major version released not too long ago it has a lot of good features one feature it has built in uh is this dependency graph",
    "start": "787959",
    "end": "795160"
  },
  {
    "text": "tracking when a task launches a task um that child task says uh okay this is my",
    "start": "795160",
    "end": "800320"
  },
  {
    "text": "parent task that gets serialized into the result store uh and um so you can",
    "start": "800320",
    "end": "806000"
  },
  {
    "text": "ask for the full graph of tasks if they're if they've been built and if not it'll raise an exception and you know",
    "start": "806000",
    "end": "812240"
  },
  {
    "text": "you haven't built out your full graph yet uh and then you can go through and check the result state of each of those",
    "start": "812240",
    "end": "817920"
  },
  {
    "text": "tasks to get an answer to am I done yet um so this requires that you don't",
    "start": "817920",
    "end": "823199"
  },
  {
    "text": "ignore the results because otherwise there won't be anything in the result store and um reemphasizing do not use",
    "start": "823199",
    "end": "829560"
  },
  {
    "text": "rabid mq as your result backend uh if any of you take anything from this presentation it's do not use Rapid mq as",
    "start": "829560",
    "end": "837040"
  },
  {
    "text": "your results door back end please don't uh so now let's talk a bit about rate limiting um problems with rate limiting",
    "start": "837040",
    "end": "844560"
  },
  {
    "start": "839000",
    "end": "866000"
  },
  {
    "text": "well the first problem is that seler's rate limit feature probably doesn't do what you think it does unless you've",
    "start": "844560",
    "end": "850240"
  },
  {
    "text": "actually you know looked at it and and used it directly um we'll get to that in",
    "start": "850240",
    "end": "855759"
  },
  {
    "text": "a minute uh third party rate limits depend on many different factors they they can be unpredictable they can be",
    "start": "855759",
    "end": "861120"
  },
  {
    "text": "complex uh you have to be able to deal with all of those factors if you want to respect third party rate limits so we've",
    "start": "861120",
    "end": "867360"
  },
  {
    "start": "866000",
    "end": "1017000"
  },
  {
    "text": "got a little sample task here it's a simp simple as I could possibly make it and it's rate limited to uh run once per",
    "start": "867360",
    "end": "872880"
  },
  {
    "text": "hour uh and it's important that it only run once per hour um and so uh I'm going",
    "start": "872880",
    "end": "877920"
  },
  {
    "text": "to now do a live demonstration to show you how celery will handle this uh rate",
    "start": "877920",
    "end": "884800"
  },
  {
    "text": "limit so the same things that I just mentioned uh it doesn't work uh with multiple worker demons um it doesn't uh",
    "start": "884800",
    "end": "893279"
  },
  {
    "text": "maintain it State between Dam and restarts um luckily our rate limits we're wanting to deal with are actually",
    "start": "893279",
    "end": "899680"
  },
  {
    "text": "enforced by the third party so we only have to make sure we're respecting their rate limits we don't have to enforce our own uh and the net net is you want to",
    "start": "899680",
    "end": "908399"
  },
  {
    "text": "use your own external centralized store for rate limiting when doing tasks and not celeries built-in support um redus",
    "start": "908399",
    "end": "915360"
  },
  {
    "text": "is really good at this it has a lot of data types that we can use um the factors can vary based on who's asking",
    "start": "915360",
    "end": "923199"
  },
  {
    "text": "uh what feature they're asking for whether it's public or private information all kinds of other unknowns",
    "start": "923199",
    "end": "928600"
  },
  {
    "text": "um the best case scenario is that all these are published the reality is even when",
    "start": "928600",
    "end": "934000"
  },
  {
    "text": "they're published they're usually not consistent um so here's just an example of two calls to the Twitter API uh the",
    "start": "934000",
    "end": "940639"
  },
  {
    "text": "first one is just getting your profile settings you can see it gives you a rate limit class these are all HTTP headers",
    "start": "940639",
    "end": "946600"
  },
  {
    "text": "by the way um it gives you a class of API identified a certain rate limit and a time when that rate gets reset you",
    "start": "946600",
    "end": "952720"
  },
  {
    "text": "call uh the same API but a different endpoint the search endpoint and you see these uh feature rate class for user",
    "start": "952720",
    "end": "960360"
  },
  {
    "text": "search has a completely different rate limit and a completely different reset time um and uh Twitter is actually going",
    "start": "960360",
    "end": "966560"
  },
  {
    "text": "to be rolling out more of this with 1.1 uh and so you need to be able to react dyn dynamically to these rate limits",
    "start": "966560",
    "end": "972759"
  },
  {
    "text": "based on the factors that they uh depend on uh so there are a few strategies here",
    "start": "972759",
    "end": "977800"
  },
  {
    "text": "when you have known rate limits with a fixed time window like the one I just showed you which means that you have 350",
    "start": "977800",
    "end": "983319"
  },
  {
    "text": "calls per hour and at this Unix timestamp you get your new set of calls um there is a simple way to deal with it",
    "start": "983319",
    "end": "989120"
  },
  {
    "text": "which is you just keep calling things until you hit the rate limit uh once you hit the rate limit you inspect the headers to say when do I get more calls",
    "start": "989120",
    "end": "995680"
  },
  {
    "text": "you stick that time stamp into redus everybody looks for it uh and they delay themselves until you get your new batch",
    "start": "995680",
    "end": "1002120"
  },
  {
    "text": "of calls uh if you need to answer to the user how many calls do I have left this hour that's a little bit harder and you",
    "start": "1002120",
    "end": "1008040"
  },
  {
    "text": "have to use a counter and either increment uh from zero or a decrement from your initial rate limit uh until",
    "start": "1008040",
    "end": "1014480"
  },
  {
    "text": "that time stamp passes and then you can reset it so we're going to show you the simple solution which is just see if the",
    "start": "1014480",
    "end": "1020959"
  },
  {
    "start": "1017000",
    "end": "1137000"
  },
  {
    "text": "reddest key exists um if it does pull the time stamp out subtract from now and",
    "start": "1020959",
    "end": "1026079"
  },
  {
    "text": "retry with a countown equal to the difference so that you won't retry until you get your new batch of tasks um if uh",
    "start": "1026079",
    "end": "1034120"
  },
  {
    "text": "yeah if you don't see the key then you try the call if you get a rate limit exception then um presumably your client",
    "start": "1034120",
    "end": "1040839"
  },
  {
    "text": "is nice enough to tell you when you are getting uh more calls but if not you need a better client uh and then you uh",
    "start": "1040839",
    "end": "1048400"
  },
  {
    "text": "same thing you just figure out how long it is in the future and do a retry with a countdown until that happens uh if you",
    "start": "1048400",
    "end": "1054320"
  },
  {
    "text": "know your rate limit but you have a rolling time window for instance like 25 calls every two hours um which",
    "start": "1054320",
    "end": "1060160"
  },
  {
    "text": "incidentally is Facebook's unpublished rate limit for wall toall posts uh you",
    "start": "1060160",
    "end": "1065679"
  },
  {
    "text": "uh can use a red sorted set of timestamps uh and remove any stale ones from the end uh or from the beginning",
    "start": "1065679",
    "end": "1072640"
  },
  {
    "text": "that is uh check the length and if the length is less than your max number of requests within that window you're free to make another request other otherwise",
    "start": "1072640",
    "end": "1079640"
  },
  {
    "text": "uh you have to wait until the oldest one would have dropped off the window in order to make more requests so that",
    "start": "1079640",
    "end": "1086280"
  },
  {
    "text": "essentially looks like there we go like this uh where you",
    "start": "1086280",
    "end": "1094280"
  },
  {
    "text": "um have your your window uh you have your expiration time based on the current time minus that window um uh",
    "start": "1094280",
    "end": "1101240"
  },
  {
    "text": "this all Z range whatever um that's all Redd sorted set functions um they're",
    "start": "1101240",
    "end": "1107360"
  },
  {
    "text": "really useful so you just chop off all the instances of the set whose score is lower than the expires time and then you",
    "start": "1107360",
    "end": "1114000"
  },
  {
    "text": "do a z-card to find out how many there are in the Set uh if there are less than your max you're free to make another",
    "start": "1114000",
    "end": "1119440"
  },
  {
    "text": "call and you add yourself to the end um with the current uh time value otherwise",
    "start": "1119440",
    "end": "1125039"
  },
  {
    "text": "you determine what time uh stamp for the first one has uh you determine how long that is uh in the future for it to roll",
    "start": "1125039",
    "end": "1132080"
  },
  {
    "text": "off the expires window and you retry with that uh countdown value so if you",
    "start": "1132080",
    "end": "1137880"
  },
  {
    "start": "1137000",
    "end": "1184000"
  },
  {
    "text": "don't know the limit rate limit at all you have to be reactive instead of proactive um using something Google",
    "start": "1137880",
    "end": "1144360"
  },
  {
    "text": "calls exponential back off uh which means that once you hit the rate limit then um you start incrementing a counter",
    "start": "1144360",
    "end": "1150880"
  },
  {
    "text": "that acts as an exponent to the number of seconds that you should wait to retry uh so you're hammering Google or whoever",
    "start": "1150880",
    "end": "1157440"
  },
  {
    "text": "really hard at first they start to rate limit you and you back off by 2 4 8 16 32 seconds um until you reach a maximum",
    "start": "1157440",
    "end": "1164720"
  },
  {
    "text": "ceiling uh and then once you start to get through again you can either chop off your uh your counter and go back to",
    "start": "1164720",
    "end": "1172200"
  },
  {
    "text": "zero and open up the pipe instantly or you can gradually decrement your exponent counter and you know kind of",
    "start": "1172200",
    "end": "1177440"
  },
  {
    "text": "soft go back into it which you employ it depends on well how nice you want to be and um what they'll let you get away",
    "start": "1177440",
    "end": "1183400"
  },
  {
    "text": "with honestly so an implementation of that uh is uh here you get your exponent",
    "start": "1183400",
    "end": "1190320"
  },
  {
    "text": "uh if there's not an exponent or if it's zero you go ahead and make your call otherwise you retry with two to the back",
    "start": "1190320",
    "end": "1195840"
  },
  {
    "text": "off exponent seconds uh if you try to make your call and you get rate limited then you increment this key and uh you",
    "start": "1195840",
    "end": "1202559"
  },
  {
    "text": "do the exact same retry um you'll see a common pattern here is if you have already been rate limited you implement",
    "start": "1202559",
    "end": "1208880"
  },
  {
    "text": "your strategy otherwise you try to meet the call if you get rate limited there you implement the same exact",
    "start": "1208880",
    "end": "1214480"
  },
  {
    "text": "strategy so let's talk about failover um unfortunately we've been",
    "start": "1214480",
    "end": "1220280"
  },
  {
    "start": "1218000",
    "end": "1238000"
  },
  {
    "text": "using countdown a lot here for failover with rares but even countdown doesn't do uh exactly what you would expect with",
    "start": "1220280",
    "end": "1227039"
  },
  {
    "text": "celery uh and thir parties can again fail in lots of interesting ways so you have to make sure your failover strategy",
    "start": "1227039",
    "end": "1233840"
  },
  {
    "text": "is really solid really tight um gosh I keep losing connection here all",
    "start": "1233840",
    "end": "1240880"
  },
  {
    "text": "right so I would love to give you another code demonstration here but my presentation is already really long so",
    "start": "1240880",
    "end": "1246159"
  },
  {
    "text": "I'm going to skip that uh suffice to say um tasks are immediately dispatched to",
    "start": "1246159",
    "end": "1251640"
  },
  {
    "text": "uh a worker Damon on retry um they're serialized with an ETA of when it's safe",
    "start": "1251640",
    "end": "1256919"
  },
  {
    "text": "to actually try that task again the worker uh Damon hangs on to it in memory until that ETA is passed it does some",
    "start": "1256919",
    "end": "1264240"
  },
  {
    "text": "internal prioritization to determine when it should retry the task and then it retries the task um you would think",
    "start": "1264240",
    "end": "1271799"
  },
  {
    "text": "that you could lose the work this way except that celery tries really hard to make sure you don't on a soft shut down",
    "start": "1271799",
    "end": "1277400"
  },
  {
    "text": "and rabbit uh has an acknowledgement feature so the worker doesn't acknowledge um that it's working the",
    "start": "1277400",
    "end": "1283640"
  },
  {
    "text": "task until it is actually working the task which saves you from losing work even if you were to kill das9",
    "start": "1283640",
    "end": "1289159"
  },
  {
    "text": "uh your celery worker Damon um but this is still a very suboptimal solution um",
    "start": "1289159",
    "end": "1296039"
  },
  {
    "text": "because you have workers hanging on to tasks that should be in the queue uh and so your centralization your",
    "start": "1296039",
    "end": "1301640"
  },
  {
    "text": "decentralization strategy your distribution strategy is totally thrown out of whack because you could have one worker Damon that has a whole bunch that",
    "start": "1301640",
    "end": "1308080"
  },
  {
    "text": "is waiting to work and another that's starved for tasks at the same time and uh you just this is not a good solution",
    "start": "1308080",
    "end": "1314960"
  },
  {
    "start": "1314000",
    "end": "1333000"
  },
  {
    "text": "so the solution to this that I've uh come up with uh um inspired by um uh there's a a guy who wrote a blog",
    "start": "1314960",
    "end": "1321919"
  },
  {
    "text": "post on this uh is dead letter exchange is an extension of amqp that you can",
    "start": "1321919",
    "end": "1327320"
  },
  {
    "text": "employ to fake this kind of ETA support that rabbit itself is missing um so uh",
    "start": "1327320",
    "end": "1335480"
  },
  {
    "start": "1333000",
    "end": "1414000"
  },
  {
    "text": "you have uh exchanges in amqp that um qes bind to with routing keys and so you",
    "start": "1335480",
    "end": "1342640"
  },
  {
    "text": "have a message that that reaches an exchange uh and then it has a routing key in the message the exchange says",
    "start": "1342640",
    "end": "1349200"
  },
  {
    "text": "what cues do I have that match this routing key uh I'm going to send the message to all of those cues and then",
    "start": "1349200",
    "end": "1354559"
  },
  {
    "text": "the que can do what it wants to as far as consumers pulling off of it in this case we Define an ad hoc Q um with our",
    "start": "1354559",
    "end": "1361320"
  },
  {
    "text": "default routing key but in a separate exchange that Q has a TTL with the amount of time that we want to wait for",
    "start": "1361320",
    "end": "1367679"
  },
  {
    "text": "that task before it gets actually put into the main queue um in just an example 60 seconds and then we set this",
    "start": "1367679",
    "end": "1374400"
  },
  {
    "text": "property on the Queue called dead letter exchange to tell it what exchange to dump the message into after that tth uh",
    "start": "1374400",
    "end": "1380880"
  },
  {
    "text": "has passed we don't hook any workers up to this queue there are no consumers so all the messages stay in the queue for",
    "start": "1380880",
    "end": "1386080"
  },
  {
    "text": "the full TTL they expire and when they expire they get dumped into this other exchange here uh which is our normal",
    "start": "1386080",
    "end": "1392320"
  },
  {
    "text": "celery exchange um has the same routing key bounds to our default queue with workers uh consuming from it and so at",
    "start": "1392320",
    "end": "1399840"
  },
  {
    "text": "the end of the TTL it goes through the normal process gets consumed by a worker just as if it were newly sent but all",
    "start": "1399840",
    "end": "1405400"
  },
  {
    "text": "this time the message exists on your rabit mq uh server uh not in your",
    "start": "1405400",
    "end": "1411320"
  },
  {
    "text": "workers uh and so um this is a snippet from uh an overridden uh apply async",
    "start": "1411320",
    "end": "1418799"
  },
  {
    "start": "1414000",
    "end": "1471000"
  },
  {
    "text": "function from a task subass uh that dynamically uh tells celery about a que",
    "start": "1418799",
    "end": "1424679"
  },
  {
    "text": "that it's creating with these Q arguments of message TTL and dead letter exchange you also see this x expires",
    "start": "1424679",
    "end": "1430480"
  },
  {
    "text": "here um that basically ensures that the que um disappears after uh that number",
    "start": "1430480",
    "end": "1436000"
  },
  {
    "text": "of seconds because we don't actually want to keep all these qes around we just want them temporarily for the purpose of dead letter exchange so then",
    "start": "1436000",
    "end": "1442360"
  },
  {
    "text": "you update your routing options by telling it uh route it to my countdown exchange with this new queue I've just",
    "start": "1442360",
    "end": "1448919"
  },
  {
    "text": "created uh and then celery will dump it into the countdown exchange uh it'll get",
    "start": "1448919",
    "end": "1454120"
  },
  {
    "text": "routed to your queue it'll sit there for your TTL which is your countdown value and then get dumped into the normal",
    "start": "1454120",
    "end": "1460159"
  },
  {
    "text": "celery process uh so that is the alternative to uh using celeries built-in countdown um I hope to conin",
    "start": "1460159",
    "end": "1468720"
  },
  {
    "text": "ask that this is worthy of putting in uh celery itself um so uh again third",
    "start": "1468720",
    "end": "1474520"
  },
  {
    "start": "1471000",
    "end": "1512000"
  },
  {
    "text": "parties can fill in lots of interesting ways um so you want to um figure out all",
    "start": "1474520",
    "end": "1479600"
  },
  {
    "text": "of your edge cases and weird things that only happen once in a blue moon uh wrap all of those up into a centralized",
    "start": "1479600",
    "end": "1485159"
  },
  {
    "text": "function uh stick it in a task based class and use that base class uh for all",
    "start": "1485159",
    "end": "1490679"
  },
  {
    "text": "of your subclasses that use that same social network or third party um to make those calls uh this can include uh retri",
    "start": "1490679",
    "end": "1498399"
  },
  {
    "text": "functionality it can include you know exception handling all the things that make it really hard to uh wrap up your",
    "start": "1498399",
    "end": "1504960"
  },
  {
    "text": "task nicely and neatly and make sure that it gets done or retried um you want to handle within a",
    "start": "1504960",
    "end": "1510919"
  },
  {
    "text": "single function all right so dealing with multiple cues uh why would you want multiple cues well um you get better",
    "start": "1510919",
    "end": "1518279"
  },
  {
    "start": "1512000",
    "end": "1573000"
  },
  {
    "text": "control over the priority of tasks if you segment your queue by task type or by you know priority that sort of thing",
    "start": "1518279",
    "end": "1525240"
  },
  {
    "text": "um you can allow for spikes much better because you can distribute your resources however you want to uh you can",
    "start": "1525240",
    "end": "1531480"
  },
  {
    "text": "launch worker Damon pools that only consume from certain cues rather than all the cues uh and so you have many",
    "start": "1531480",
    "end": "1539080"
  },
  {
    "text": "many more routing options available to you if you use multiple cues for different purposes um another",
    "start": "1539080",
    "end": "1544600"
  },
  {
    "text": "interesting use case is what I call a trickle Q which is when you have really low priority work but maybe a large",
    "start": "1544600",
    "end": "1551600"
  },
  {
    "text": "volume of it needs to be done over a long period of time uh and we'll talk about that more in just a second it's",
    "start": "1551600",
    "end": "1556880"
  },
  {
    "text": "really easy to implement most multiple cues in celery uh it's just a setting in your settings.py uh you define the Q",
    "start": "1556880",
    "end": "1563080"
  },
  {
    "text": "name and The Binding key uh you can get deeper into this if you actually want to use the comu Q object um under the",
    "start": "1563080",
    "end": "1569640"
  },
  {
    "text": "covers in the amqp uh implementation but you don't have to uh and then you just",
    "start": "1569640",
    "end": "1574679"
  },
  {
    "text": "launch your celery worker Damon uh with your Q's argument um I think by default it uses all the qes but it's always",
    "start": "1574679",
    "end": "1581279"
  },
  {
    "text": "better to be explicit uh so you can tell this worker pool use this que this worker pool use all the cues and you",
    "start": "1581279",
    "end": "1588000"
  },
  {
    "text": "know manage it however you want to so back to the trickle Q um so let's say you want to do something like um keep",
    "start": "1588000",
    "end": "1593880"
  },
  {
    "text": "the avatars fresh for all of your you know users that have Twitter accounts connected uh you want to do this for say",
    "start": "1593880",
    "end": "1600720"
  },
  {
    "text": "hundreds of thousands of users um but you don't want to starve the other things in your que so you set up a KRON",
    "start": "1600720",
    "end": "1606320"
  },
  {
    "text": "job um you have a cursor persisted somewhere like redis or in your database or something that says this is where I last left off um it's been a minute you",
    "start": "1606320",
    "end": "1613440"
  },
  {
    "text": "know just put five or 10 more in the queue and work them it's very low priority but if you dump all of them at",
    "start": "1613440",
    "end": "1619120"
  },
  {
    "text": "once into your queue then you're going to starve uh everything else that wants to use that same priority queue uh and",
    "start": "1619120",
    "end": "1625480"
  },
  {
    "text": "um so let's say you had Facebook people who uh avatars you wanted to update as well you dump 100,000 Twitter jobs in",
    "start": "1625480",
    "end": "1632000"
  },
  {
    "text": "this queue it takes days and days and days to work and now your Facebook users have days uh old stale avatars because",
    "start": "1632000",
    "end": "1638960"
  },
  {
    "text": "you starved one task in favor of another making implicit priority where you should have been uh having equal",
    "start": "1638960",
    "end": "1645919"
  },
  {
    "text": "priority uh by trickling them into the cube so I mentioned a KRON job just now to uh",
    "start": "1645919",
    "end": "1653640"
  },
  {
    "start": "1649000",
    "end": "1720000"
  },
  {
    "text": "implement this tricle Q um this is another one of my strong opinions uh I think celery be is uh a reimplementation",
    "start": "1653640",
    "end": "1661360"
  },
  {
    "text": "of the cron wheel that doesn't need to exist uh so um don't don't use it it has",
    "start": "1661360",
    "end": "1667120"
  },
  {
    "text": "problems um one of those problems is that uh it uses a persistence layer",
    "start": "1667120",
    "end": "1672240"
  },
  {
    "text": "either the file system or the database to keep track of your periodic tasks that can get out of sync with your tasks especially if you delete tasks and the",
    "start": "1672240",
    "end": "1679159"
  },
  {
    "text": "record is still in the database um celer be will try to launch tasks that don't exist and then we'll yell at you um this",
    "start": "1679159",
    "end": "1684960"
  },
  {
    "text": "is bad uh it's just one more process to manage who wants to manage another process unless you're an Ops guy and",
    "start": "1684960",
    "end": "1690880"
  },
  {
    "text": "your job is managing processes uh but I'm not I don't like managing processes so I like the fact that everyone in the",
    "start": "1690880",
    "end": "1696840"
  },
  {
    "text": "world uses KRON and it's really just not that hard um there's no need to uh make",
    "start": "1696840",
    "end": "1701919"
  },
  {
    "text": "a reinvention of it there's no need to reimplement it uh in a way that you know could have bugs um less codee is good",
    "start": "1701919",
    "end": "1709640"
  },
  {
    "text": "right so just use Kon don't use celery beat um there's probably not actually time left but I'm going to pretend that",
    "start": "1709640",
    "end": "1716240"
  },
  {
    "text": "there is for the sake of this Meetup and go right into debugging um just a few tips um when you're debugging locally go",
    "start": "1716240",
    "end": "1723039"
  },
  {
    "start": "1720000",
    "end": "1975000"
  },
  {
    "text": "through the effort to set up rabbit um redus uh set up your worker Damons all",
    "start": "1723039",
    "end": "1728559"
  },
  {
    "text": "this stuff um I know it's antithetical to the Run server way of doing things in Jango local development but you really",
    "start": "1728559",
    "end": "1735720"
  },
  {
    "text": "don't want to use always eager um to write your tasks synchronously when you're developing locally uh a lot of",
    "start": "1735720",
    "end": "1740799"
  },
  {
    "text": "these strategies have tasks launching tasks dependency tree that I mentioned uh if you have like a big Network that",
    "start": "1740799",
    "end": "1746840"
  },
  {
    "text": "you're importing locally you're going to exceed your maximum recursion depth um you're going to do other nasty things",
    "start": "1746840",
    "end": "1751919"
  },
  {
    "text": "like um incidentally launch a task that takes a long time to get all worked out",
    "start": "1751919",
    "end": "1757200"
  },
  {
    "text": "uh and you're waiting for your local web page to load for minutes and minutes and minutes while it's running in the background doing these things",
    "start": "1757200",
    "end": "1762559"
  },
  {
    "text": "synchronously that's not really designed to be done synchronously and it's always better to be as close to production as",
    "start": "1762559",
    "end": "1767799"
  },
  {
    "text": "possible possible when you're working your local environment because things work more like production that way um so",
    "start": "1767799",
    "end": "1774840"
  },
  {
    "text": "given that you have given up synchronous running of tasks uh you now have debugging issues because problems are",
    "start": "1774840",
    "end": "1780720"
  },
  {
    "text": "harder to track down you can't easily just set a break point although you you you can set a breakpoint in s workers",
    "start": "1780720",
    "end": "1786320"
  },
  {
    "text": "but that's another problem um so logging is the answer to these issues and it's good for production as well as uh your",
    "start": "1786320",
    "end": "1793000"
  },
  {
    "text": "local environment um add more logging info level logging doesn't get caught by",
    "start": "1793000",
    "end": "1798720"
  },
  {
    "text": "default so you can specify log level info on your worker Damon pools um but logging is really the is uh the central",
    "start": "1798720",
    "end": "1805720"
  },
  {
    "text": "tool that you have to debugging a um distributed system and third when you're",
    "start": "1805720",
    "end": "1811240"
  },
  {
    "text": "working with third party dependencies unit tests are good but you really want a suite of integration tests as well um",
    "start": "1811240",
    "end": "1819000"
  },
  {
    "text": "there's an idea that you should never be mocking anything that is outside of your control that you don't control the API",
    "start": "1819000",
    "end": "1824960"
  },
  {
    "text": "for uh and you don't control the API for third party but you mock them anyway and I do too because it saves time in your",
    "start": "1824960",
    "end": "1831360"
  },
  {
    "text": "test suite and you don't actually want to make those calls to live servers I understand uh it's completely reasonable",
    "start": "1831360",
    "end": "1836679"
  },
  {
    "text": "but you're not actually testing the full loop until you have integration tests that actually go out and hit those live",
    "start": "1836679",
    "end": "1842960"
  },
  {
    "text": "servers and third parties and make sure that the API that you coded against is actually the API that still exists um or",
    "start": "1842960",
    "end": "1850200"
  },
  {
    "text": "you know any number of other things that might have changed because you're mocking out these third party calls a few gotas um this is just a",
    "start": "1850200",
    "end": "1858519"
  },
  {
    "text": "basic python thing but if you're using c-l blocking like sockets uh then",
    "start": "1858519",
    "end": "1863760"
  },
  {
    "text": "nothing else is going to be able to interrupt your code uh the soft timeout that seler provides as a feature uh",
    "start": "1863760",
    "end": "1869919"
  },
  {
    "text": "won't actually um won't actually fire you'll get to the hard timeout which is",
    "start": "1869919",
    "end": "1875039"
  },
  {
    "text": "you know just like a hard process kill and you'll lose workers and bad things will happen and you won't know why so",
    "start": "1875039",
    "end": "1881159"
  },
  {
    "text": "the easy solution for thirdparty interfaces in particular is just to set a Timeout on all of your socket calls",
    "start": "1881159",
    "end": "1887159"
  },
  {
    "text": "make sure um when you're using client libraries most of them support timeout again if they don't then you should",
    "start": "1887159",
    "end": "1892200"
  },
  {
    "text": "probably consider using a different Library um but make sure that you implement that always uh soft timeout",
    "start": "1892200",
    "end": "1898960"
  },
  {
    "text": "doesn't automatically retry a task uh it doesn't know whether it should or not so you really need to be um trying and",
    "start": "1898960",
    "end": "1905559"
  },
  {
    "text": "catching that soft timeout if you are implementing a soft timeout which I recommend uh to make sure that um you're",
    "start": "1905559",
    "end": "1912000"
  },
  {
    "text": "retrying if you hit it uh because you know most of the time it's due to just uh a network being slow your database",
    "start": "1912000",
    "end": "1919000"
  },
  {
    "text": "being slow something being slow that's ephemeral and it's going to go away so you might as well retry that task um",
    "start": "1919000",
    "end": "1924880"
  },
  {
    "text": "another thing and this is an odd design choice on seler's part is the default task result state is pending so um even",
    "start": "1924880",
    "end": "1933679"
  },
  {
    "text": "if selery has no record of that result whatsoever uh either it's expired from cash or you've deleted it from the",
    "start": "1933679",
    "end": "1939880"
  },
  {
    "text": "database or whatever um seler will tell you it's pending uh this is not very",
    "start": "1939880",
    "end": "1945840"
  },
  {
    "text": "useful but if you know about it you know it's it's it's okay um just know that you know if you set a cash expire time",
    "start": "1945840",
    "end": "1953000"
  },
  {
    "text": "on your results and you get pending back as an answer it's not because it didn't complete it's probably because it either",
    "start": "1953000",
    "end": "1958240"
  },
  {
    "text": "completed or failed and then got deleted from your results store and celer is just being uh helpful and telling you",
    "start": "1958240",
    "end": "1964039"
  },
  {
    "text": "pending so that is about it",
    "start": "1964039",
    "end": "1970080"
  }
]