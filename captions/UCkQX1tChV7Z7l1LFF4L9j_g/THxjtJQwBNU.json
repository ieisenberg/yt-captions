[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "text": "foreign [Music]",
    "start": "1380",
    "end": "16400"
  },
  {
    "start": "16000",
    "end": "16000"
  },
  {
    "text": "I'm a software engineer Dropbox and today I'm going to be talking about magic pocket which is an exabyte scale",
    "start": "16400",
    "end": "23100"
  },
  {
    "text": "blob storage system okay let's get right into it so",
    "start": "23100",
    "end": "30900"
  },
  {
    "start": "28000",
    "end": "28000"
  },
  {
    "text": "magic pocket is used to store all of dropbox's customer data and we also have",
    "start": "30900",
    "end": "36120"
  },
  {
    "text": "many internal use cases at its core magic pocket is a very large key Value",
    "start": "36120",
    "end": "42660"
  },
  {
    "text": "Store where the value can be arbitrarily sized blobs",
    "start": "42660",
    "end": "47760"
  },
  {
    "text": "our system has over four nines of availability we operate across three geographical",
    "start": "47760",
    "end": "54840"
  },
  {
    "text": "regions in North America our rights are optimized for four",
    "start": "54840",
    "end": "60300"
  },
  {
    "text": "megabyte blobs rights are immutable and we also",
    "start": "60300",
    "end": "65518"
  },
  {
    "text": "optimize for cold data our system overall manages over tens of",
    "start": "65519",
    "end": "70860"
  },
  {
    "text": "millions of requests per second a lot of the traffic behind the scenes",
    "start": "70860",
    "end": "76220"
  },
  {
    "text": "comes from verifiers and background migrations and currently deployed we have more than",
    "start": "76220",
    "end": "84240"
  },
  {
    "text": "600 000 storage drives and all of this also utilizes thousands",
    "start": "84240",
    "end": "90119"
  },
  {
    "text": "and thousands of compute machines okay let's talk about the most important",
    "start": "90119",
    "end": "98220"
  },
  {
    "start": "94000",
    "end": "94000"
  },
  {
    "text": "component of a magic bucket which are the optic storage devices which we call",
    "start": "98220",
    "end": "104100"
  },
  {
    "text": "osds so this is an actual OSD in one of our data centers you can see all the drives",
    "start": "104100",
    "end": "110700"
  },
  {
    "text": "in here and typically per storage machine um we have about 100 disks per OSD",
    "start": "110700",
    "end": "119340"
  },
  {
    "text": "we utilize SMR technology and each storage device has over two",
    "start": "119340",
    "end": "125640"
  },
  {
    "text": "petabytes of capacity let's",
    "start": "125640",
    "end": "131819"
  },
  {
    "text": "um let's talk about why we use SMR technology so SMR stands for shingled magnetic recording",
    "start": "131819",
    "end": "138239"
  },
  {
    "text": "and it's different from the conventional perpendicular magnetic recording or PMI",
    "start": "138239",
    "end": "143340"
  },
  {
    "text": "drives that allow for random rights across the whole disk with SMR what your",
    "start": "143340",
    "end": "150420"
  },
  {
    "text": "the trade-offs are that you offer increased density by doing sequential",
    "start": "150420",
    "end": "156000"
  },
  {
    "text": "rights instead of random rights so as you can see here you're squeezing",
    "start": "156000",
    "end": "161700"
  },
  {
    "text": "tracks of SMR Drives together which causes the head to erase the next",
    "start": "161700",
    "end": "168780"
  },
  {
    "text": "track when you walk over it but in this case you can still read from it it just",
    "start": "168780",
    "end": "174620"
  },
  {
    "text": "that you can't just randomly write in any place that you want",
    "start": "174620",
    "end": "180319"
  },
  {
    "text": "and this is actually perfect for cases based on the workload patterns that I just told you about so this actually",
    "start": "180540",
    "end": "186239"
  },
  {
    "text": "ends up working really well for us SMR drives also have a conventional Zone",
    "start": "186239",
    "end": "191640"
  },
  {
    "text": "outside the diameter that allows for caching of random rights if you need to",
    "start": "191640",
    "end": "198360"
  },
  {
    "text": "and this conventional conventional Zone it's typically less than about one",
    "start": "198360",
    "end": "204420"
  },
  {
    "text": "percent of the total capacity of the drive",
    "start": "204420",
    "end": "208159"
  },
  {
    "text": "okay let's talk about the architecture of magic pocket now so",
    "start": "210900",
    "end": "217200"
  },
  {
    "text": "at a very high level view of our system we operate out of three zones so we",
    "start": "217200",
    "end": "222599"
  },
  {
    "text": "operate out of a West Coast Central Zone and East Coast and the first subtraction here is a",
    "start": "222599",
    "end": "230040"
  },
  {
    "text": "pocket so a pocket is a way to represent a logical version of everything that makes",
    "start": "230040",
    "end": "235560"
  },
  {
    "text": "up our system so we can have many different instances of magic pocket and some versions of",
    "start": "235560",
    "end": "243060"
  },
  {
    "text": "that could be a test pocket so something in like a developer desktop you can run",
    "start": "243060",
    "end": "248700"
  },
  {
    "text": "an instance of magic pocket we also have a stage pocket which is before our production",
    "start": "248700",
    "end": "256739"
  },
  {
    "text": "um a production Zone and",
    "start": "256739",
    "end": "261979"
  },
  {
    "text": "databases and compute are actually not shared between between Pockets so they",
    "start": "261979",
    "end": "267720"
  },
  {
    "text": "operate completely independently of each other okay so now that we have a high level",
    "start": "267720",
    "end": "274320"
  },
  {
    "text": "view of what a pocket is let's talk about the different components so let's get into what a zone",
    "start": "274320",
    "end": "279360"
  },
  {
    "text": "is So within a Zone we have the first service here which is the",
    "start": "279360",
    "end": "287759"
  },
  {
    "text": "front-end service and this is the service that we expose to our clients so all of our clients interact with us",
    "start": "287759",
    "end": "293639"
  },
  {
    "text": "through this front-end service um this is what they call to make any",
    "start": "293639",
    "end": "299280"
  },
  {
    "text": "kind of requests and the types of requests our clients typically make are a put request with the key and a blob a",
    "start": "299280",
    "end": "307680"
  },
  {
    "text": "get request given some key uh they might make a delete call they",
    "start": "307680",
    "end": "314639"
  },
  {
    "text": "might scan for what hashes are available in the system um they might want to update some",
    "start": "314639",
    "end": "319919"
  },
  {
    "text": "metadata on a specific key as well and when",
    "start": "319919",
    "end": "326039"
  },
  {
    "text": "um a get request comes in uh will we first first have to consult is the hash",
    "start": "326039",
    "end": "332759"
  },
  {
    "text": "index so the hash index is a bunch of shorted uh my SQL databases",
    "start": "332759",
    "end": "341280"
  },
  {
    "text": "and everything is sharded by this hash so a hash is basically the key for a",
    "start": "341280",
    "end": "348539"
  },
  {
    "text": "blob and we simply just take the shot 356 of that that blob internally we call",
    "start": "348539",
    "end": "354720"
  },
  {
    "text": "those those blocks which are parts of a file pieces of a file typically no more than",
    "start": "354720",
    "end": "361979"
  },
  {
    "text": "four megabyte chunks and in the index what you'll find is that a hash is mapped to a cell which",
    "start": "361979",
    "end": "369660"
  },
  {
    "text": "we'll talk about in a minute a bucket and we also have a checksum as well for that specific cache",
    "start": "369660",
    "end": "377100"
  },
  {
    "text": "and a cell is another isolation unit so this is where all of the storage devices",
    "start": "377100",
    "end": "382860"
  },
  {
    "text": "actually live um and so the index table just points to the specific cell and a bucket so",
    "start": "382860",
    "end": "389460"
  },
  {
    "text": "another level and Direction Where to go to in the cell",
    "start": "389460",
    "end": "394340"
  },
  {
    "text": "and the cells can be quite large so they can be over 100 petabytes",
    "start": "395280",
    "end": "401100"
  },
  {
    "text": "um in size of customer data they do have some limitations of how",
    "start": "401100",
    "end": "407699"
  },
  {
    "text": "much they can grow but as the system as a whole if we are low on capacity we",
    "start": "407699",
    "end": "415139"
  },
  {
    "text": "just simply open up a new cell um so let's say so 10.",
    "start": "415139",
    "end": "421800"
  },
  {
    "text": "and that's how our system is able to horizontally scale forever",
    "start": "421800",
    "end": "428280"
  },
  {
    "text": "with some limitations of course all right let's talk about what else we have in the zone so another component",
    "start": "428280",
    "end": "435479"
  },
  {
    "text": "that we have in the zone is the cross Zone replicator so the idea behind the crosstown",
    "start": "435479",
    "end": "442259"
  },
  {
    "text": "replicator is that within magic pocket we do cross Zone replication",
    "start": "442259",
    "end": "447919"
  },
  {
    "text": "so that means we store your data in multiple regions and this is done asynchronously in the",
    "start": "447919",
    "end": "454380"
  },
  {
    "text": "background once a commit happens and you know where your data is we automatically queue it up into the crosstown",
    "start": "454380",
    "end": "461220"
  },
  {
    "text": "replicator so it can be moved over to the other Zone somewhere else",
    "start": "461220",
    "end": "468319"
  },
  {
    "text": "we also have a control plane so the control plane is basically manages",
    "start": "468360",
    "end": "475520"
  },
  {
    "text": "traffic coordination within its own um in order to for example if we have",
    "start": "475520",
    "end": "481080"
  },
  {
    "text": "migrations in the background it generates a plan in order to",
    "start": "481080",
    "end": "486360"
  },
  {
    "text": "um not impede with live traffic um and we also use it to manage let's say",
    "start": "486360",
    "end": "494639"
  },
  {
    "text": "reinstallations of specific storage machines so for example we might want to do a kernel upgrade or an operating",
    "start": "494639",
    "end": "501360"
  },
  {
    "text": "system upgrade the control plane will take care of that for us",
    "start": "501360",
    "end": "507180"
  },
  {
    "text": "and it also manages uh uh cell State information so the cells have certain",
    "start": "507180",
    "end": "514440"
  },
  {
    "text": "attributes as well about maybe let's say type of Hardware that's involved in there and so on that's that's what it's",
    "start": "514440",
    "end": "522659"
  },
  {
    "text": "doing Okay so we talked about a hash is mapped to a",
    "start": "522659",
    "end": "529680"
  },
  {
    "text": "cell and a bucket so let's get into what a cell and a bucket is",
    "start": "529680",
    "end": "538639"
  },
  {
    "text": "okay so within the cell we have a",
    "start": "541380",
    "end": "546959"
  },
  {
    "text": "um a bucket and now we already went into the cell so the first thing we have to do if we want to fetch our blob back is",
    "start": "546959",
    "end": "553980"
  },
  {
    "text": "that we have to consult this component called the bucket service so the bucket service knows about a few",
    "start": "553980",
    "end": "560399"
  },
  {
    "text": "things so it knows about buckets and volumes",
    "start": "560399",
    "end": "566760"
  },
  {
    "text": "um and a anytime you want to let's say as we're doing a fetch here we",
    "start": "566760",
    "end": "575519"
  },
  {
    "text": "first find the bucket the bucket is actually mapped to a volume and the volume is mapped to a set of osds",
    "start": "575519",
    "end": "584000"
  },
  {
    "text": "and the volume can be and can be open or closed",
    "start": "584000",
    "end": "589519"
  },
  {
    "text": "there's a generation associated with it and also volumes are of different types",
    "start": "589519",
    "end": "595800"
  },
  {
    "text": "so volume could be and some replicated or Erasure coded state",
    "start": "595800",
    "end": "601740"
  },
  {
    "text": "so when we ask for a bucket that volume will",
    "start": "601740",
    "end": "607380"
  },
  {
    "text": "tell us which specific OSD has our blob so let's say",
    "start": "607380",
    "end": "613220"
  },
  {
    "text": "we go to bucket1 map to volume 10 and then within the",
    "start": "613220",
    "end": "620459"
  },
  {
    "text": "volume we find this OSD here so we will simply just need to ask this",
    "start": "620459",
    "end": "627779"
  },
  {
    "text": "OSD about our specific blob and it will hand it back to us and that basically",
    "start": "627779",
    "end": "634200"
  },
  {
    "text": "completes the way that we retrieve um a blob from Magic bucket",
    "start": "634200",
    "end": "639779"
  },
  {
    "text": "if you want to do a write it's a little bit different but essentially the same it's just that the buckets are",
    "start": "639779",
    "end": "647060"
  },
  {
    "text": "pre-created for us so simply the front-end service needs to figure out what buckets are open for",
    "start": "647060",
    "end": "656100"
  },
  {
    "text": "um for it to write to so it will write to buckets that are open and ready to be",
    "start": "656100",
    "end": "663360"
  },
  {
    "text": "written so the in this case here if it's an open volume ready to be written into",
    "start": "663360",
    "end": "671000"
  },
  {
    "text": "what we do is we may rights to this set of osds So within a volume you it will",
    "start": "671000",
    "end": "678899"
  },
  {
    "text": "be mapped to these four and that's where your your data will be stored for that specific volume",
    "start": "678899",
    "end": "685140"
  },
  {
    "text": "okay let's talk about a couple other things uh within the cell so another component that's very important is this",
    "start": "685140",
    "end": "692700"
  },
  {
    "text": "coordinator so the coordinator is not in the path of a request for a put or get",
    "start": "692700",
    "end": "699600"
  },
  {
    "text": "call it's actually works in the background and what it does is it's",
    "start": "699600",
    "end": "705540"
  },
  {
    "text": "managing all of the buckets and volumes as well as all of the storage machines",
    "start": "705540",
    "end": "710820"
  },
  {
    "text": "themselves so it's constantly Health checking storage machines it's asking",
    "start": "710820",
    "end": "716880"
  },
  {
    "text": "them what information do they know um and reconciling information from the",
    "start": "716880",
    "end": "723720"
  },
  {
    "text": "storage machines with uh the bucket um",
    "start": "723720",
    "end": "728820"
  },
  {
    "text": "service in the bucket database and other things that it does is that it",
    "start": "728820",
    "end": "735779"
  },
  {
    "text": "will do Erasure coding it will do repairs so if for example if this storage machine goes out and you'll need",
    "start": "735779",
    "end": "742440"
  },
  {
    "text": "to move data to another machine so it's in charge of doing that it'll optimize data by moving things",
    "start": "742440",
    "end": "750060"
  },
  {
    "text": "around within the cell or if there's a machine that needs to go offline it will also take care of that",
    "start": "750060",
    "end": "757380"
  },
  {
    "text": "too by moving data around to some other machine and the way that it moves data around it",
    "start": "757380",
    "end": "764160"
  },
  {
    "text": "doesn't actually copy data itself it's actually all done by this component here called the volume manager and the volume",
    "start": "764160",
    "end": "770940"
  },
  {
    "text": "manager um basically is in charge of moving data around or when we need to move data",
    "start": "770940",
    "end": "779459"
  },
  {
    "text": "how to recreate the new volume when to recreate the new volumes and so on",
    "start": "779459",
    "end": "785700"
  },
  {
    "text": "and um I talked a little bit about some of the background traffic that we have a lot of",
    "start": "785700",
    "end": "792120"
  },
  {
    "text": "verification steps also happen within the cell as well as outside of the cell and we'll talk about that as well",
    "start": "792120",
    "end": "799820"
  },
  {
    "text": "okay let's talk about a little bit more about buckets and volumes and what those",
    "start": "799980",
    "end": "806040"
  },
  {
    "text": "are in more detail",
    "start": "806040",
    "end": "808940"
  },
  {
    "start": "809000",
    "end": "809000"
  },
  {
    "text": "so we have the concept of these three components we have buckets volumes and",
    "start": "811320",
    "end": "818760"
  },
  {
    "text": "extents and you can think about a bucket as a",
    "start": "818760",
    "end": "823860"
  },
  {
    "text": "logical storage unit so I mentioned if we wanted to do a write we write into a bucket which is",
    "start": "823860",
    "end": "829740"
  },
  {
    "text": "associated with the volume and then finally this other concept called an extent and the extent is",
    "start": "829740",
    "end": "837540"
  },
  {
    "text": "um the actual it's just about one or two gigabytes of",
    "start": "837540",
    "end": "846180"
  },
  {
    "text": "data found on the disk and so if we want to do a write we",
    "start": "846180",
    "end": "853139"
  },
  {
    "text": "simply figure out what open buckets are so assuming we found you know let's bucket one",
    "start": "853139",
    "end": "858660"
  },
  {
    "text": "we um we have to get the set of osds associated with those and then when we",
    "start": "858660",
    "end": "865680"
  },
  {
    "text": "do the the rights we simply make a right within the extents themselves and the",
    "start": "865680",
    "end": "872820"
  },
  {
    "text": "extent information and volume information in the buckets are all managed by the coordinator I talked",
    "start": "872820",
    "end": "878820"
  },
  {
    "text": "about before so if any data is missing or things like that you",
    "start": "878820",
    "end": "884699"
  },
  {
    "text": "can always get it from the other remaining storage machines um and the coordinator will be in charge",
    "start": "884699",
    "end": "892380"
  },
  {
    "text": "of finding new placement for that extent that was deleted so buckets think about as logical",
    "start": "892380",
    "end": "899459"
  },
  {
    "text": "storage typically one to two gigs um volume",
    "start": "899459",
    "end": "905360"
  },
  {
    "text": "composed of one or more buckets depending on the type a set of osds",
    "start": "905360",
    "end": "914000"
  },
  {
    "text": "um type again whether it's replicator razor coded um and whether it's open or not",
    "start": "914160",
    "end": "922680"
  },
  {
    "text": "and once we close the volume it is never opened up again",
    "start": "922680",
    "end": "928220"
  },
  {
    "text": "okay let's talk about how we find the actual volume in the storage machine or",
    "start": "931740",
    "end": "937079"
  },
  {
    "text": "the actual uh blob in a storage machine",
    "start": "937079",
    "end": "942560"
  },
  {
    "start": "942000",
    "end": "942000"
  },
  {
    "text": "okay so we have the blob we want to fetch the blob we know now the volume we know that the",
    "start": "943740",
    "end": "951360"
  },
  {
    "text": "volume is associated these osds are supposed to have our blob",
    "start": "951360",
    "end": "957600"
  },
  {
    "text": "so simply what we do is that we store the address of these different osds",
    "start": "957600",
    "end": "965100"
  },
  {
    "text": "and um we talk directly to those osds",
    "start": "965100",
    "end": "972180"
  },
  {
    "text": "over here so we talk directly to those osds and the the osds when they load up they",
    "start": "972180",
    "end": "981959"
  },
  {
    "text": "actually load all the extent information um all and create an in-memory index of",
    "start": "981959",
    "end": "989339"
  },
  {
    "text": "which hashes they have uh to the disk offset so for example if you had some",
    "start": "989339",
    "end": "994680"
  },
  {
    "text": "hash of blob Foo uh it'll be located in disk offset uh 19292.",
    "start": "994680",
    "end": "1003019"
  },
  {
    "text": "and in this case uh this volume is of type replicated so it's actually not",
    "start": "1005000",
    "end": "1010759"
  },
  {
    "text": "Erasure coded so we have the full copy available and all the four OCS mentioned here",
    "start": "1010759",
    "end": "1017959"
  },
  {
    "text": "and uh this is for fetching the block if you want to do the put it'll be of the",
    "start": "1017959",
    "end": "1024380"
  },
  {
    "text": "same type it'll be 4X replicated and you'll simply do it right to every single OSD",
    "start": "1024380",
    "end": "1031520"
  },
  {
    "text": "itself and we do the request in parallel and we don't act back until the write",
    "start": "1031520",
    "end": "1037040"
  },
  {
    "text": "has been completed on all of the storage machines",
    "start": "1037040",
    "end": "1041798"
  },
  {
    "text": "okay so let's talk about the difference between a replicated volume",
    "start": "1043640",
    "end": "1050480"
  },
  {
    "text": "and Erasure code volume and how we handle that",
    "start": "1050480",
    "end": "1054520"
  },
  {
    "start": "1053000",
    "end": "1053000"
  },
  {
    "text": "so obviously Forex replication and we have",
    "start": "1055700",
    "end": "1061280"
  },
  {
    "text": "two zone two zones that we replicate the volume to um that can be quite expensive so",
    "start": "1061280",
    "end": "1067460"
  },
  {
    "text": "overall this would be 8X replication uh and that's very costly so",
    "start": "1067460",
    "end": "1074419"
  },
  {
    "text": "um we want to be able to lower our replication factor to make our systems",
    "start": "1074419",
    "end": "1079460"
  },
  {
    "text": "more efficient so this is where Erasure codes come in um",
    "start": "1079460",
    "end": "1085940"
  },
  {
    "text": "so when a volume is in a replicated state after a few hours the volume will be or",
    "start": "1085940",
    "end": "1093380"
  },
  {
    "text": "shortly after the volume was almost full it will be closed and it will be",
    "start": "1093380",
    "end": "1099799"
  },
  {
    "text": "eligible to be Erasure coded so",
    "start": "1099799",
    "end": "1104620"
  },
  {
    "text": "um Erasure codes are able to help us reduce the replication costs with similar",
    "start": "1104960",
    "end": "1111740"
  },
  {
    "text": "durability as straight up replication so in this case we have on razor code let's",
    "start": "1111740",
    "end": "1117559"
  },
  {
    "text": "say this is read Solomon six plus three so we have six osds and we have three",
    "start": "1117559",
    "end": "1125660"
  },
  {
    "text": "parodies over here and the idea is that um we call this a volume group or",
    "start": "1125660",
    "end": "1132620"
  },
  {
    "text": "grouping and you'll have the blob a single blob in one of the data extents",
    "start": "1132620",
    "end": "1138260"
  },
  {
    "text": "and then if you lose any one of these osds you can reconstruct it from the",
    "start": "1138260",
    "end": "1144799"
  },
  {
    "text": "remaining parities and the other data extents",
    "start": "1144799",
    "end": "1149200"
  },
  {
    "text": "let's go into a little bit more detail here on Erasure coating",
    "start": "1151100",
    "end": "1158120"
  },
  {
    "text": "um continue this conversation so yeah in this case as I mentioned you can read",
    "start": "1158120",
    "end": "1163520"
  },
  {
    "text": "from other extents in parity for the reconstruction and as you can imagine this area becomes",
    "start": "1163520",
    "end": "1169220"
  },
  {
    "text": "really interesting there's going to be a lot of variations of ratio codes out there with many different trade-offs",
    "start": "1169220",
    "end": "1175520"
  },
  {
    "text": "around overhead so let's talk briefly about that",
    "start": "1175520",
    "end": "1181059"
  },
  {
    "start": "1180000",
    "end": "1180000"
  },
  {
    "text": "so you for Erasure codes you know they can",
    "start": "1182360",
    "end": "1187460"
  },
  {
    "text": "be quite simple you can use something like xor where you can reconstruct from the xor",
    "start": "1187460",
    "end": "1194539"
  },
  {
    "text": "of other other things or you can use very custom Erasure codes",
    "start": "1194539",
    "end": "1201320"
  },
  {
    "text": "and there's a lot of trade-offs for example if you want to do less reads you might have higher overhead",
    "start": "1201320",
    "end": "1208340"
  },
  {
    "text": "um if you want to tolerate more failures the overhead so your replication factor is likely to increase if you want to",
    "start": "1208340",
    "end": "1215600"
  },
  {
    "text": "tolerate multiple failures within that volume group um",
    "start": "1215600",
    "end": "1220820"
  },
  {
    "text": "and this is a very interesting paper um by Microsoft called Erasure coding and",
    "start": "1220820",
    "end": "1227720"
  },
  {
    "text": "windows Azure storage by hung and others and this came out a few years ago but",
    "start": "1227720",
    "end": "1233120"
  },
  {
    "text": "it's super interesting and it's something very similar that we actually do within magic pocket as well",
    "start": "1233120",
    "end": "1239900"
  },
  {
    "text": "uh so I mentioned the example before around",
    "start": "1239900",
    "end": "1245780"
  },
  {
    "text": "um with the example with Reed Salomon 63 so six data extends and three parities",
    "start": "1245780",
    "end": "1252679"
  },
  {
    "text": "so with the codes that they came up with called least reconstruction codes",
    "start": "1252679",
    "end": "1260480"
  },
  {
    "text": "they have a concept of optimizing for read costs so in read settlement six",
    "start": "1260480",
    "end": "1266960"
  },
  {
    "text": "three the read costs if you have any failures is going to be the full set of",
    "start": "1266960",
    "end": "1272960"
  },
  {
    "text": "all the extents here so your read um read penalty will be let's say six",
    "start": "1272960",
    "end": "1278240"
  },
  {
    "text": "here well they came up with equivalent codes such that for one type of",
    "start": "1278240",
    "end": "1284720"
  },
  {
    "text": "data failures you can have the same read costs but a lower storage overhead",
    "start": "1284720",
    "end": "1292280"
  },
  {
    "text": "so in this example here there's storage overhead the replication",
    "start": "1292280",
    "end": "1297919"
  },
  {
    "text": "factor is roughly 1.33 X where the same replication factor with reads element is",
    "start": "1297919",
    "end": "1305000"
  },
  {
    "text": "going to be 1.5 so it may not seem like a big savings but at a very large scale this ends up being",
    "start": "1305000",
    "end": "1312919"
  },
  {
    "text": "quite a lot that you end up saving of course this optimizes for",
    "start": "1312919",
    "end": "1318559"
  },
  {
    "text": "one failure within the group which is typically what you'll see in production",
    "start": "1318559",
    "end": "1324260"
  },
  {
    "text": "um if you can repair quickly enough you will hardly see more than one type of",
    "start": "1324260",
    "end": "1329559"
  },
  {
    "text": "failure within a volume group those types of failures are a lot a lot rarer",
    "start": "1329559",
    "end": "1335539"
  },
  {
    "text": "and they typically don't happen that often so it's okay to to make the trade-off here with that",
    "start": "1335539",
    "end": "1344980"
  },
  {
    "text": "and yeah it's just to iterate 1.33 X overhead for the same as a read Solomon",
    "start": "1348140",
    "end": "1355640"
  },
  {
    "text": "code um so super interesting paper um you can even continue to lower your",
    "start": "1355640",
    "end": "1361159"
  },
  {
    "text": "replication factor and you can see here at far outpaces uh read settlement codes for",
    "start": "1361159",
    "end": "1368799"
  },
  {
    "text": "similar overhead um or sorry lower overhead but somewhere",
    "start": "1368799",
    "end": "1375200"
  },
  {
    "text": "reconstruction read costs",
    "start": "1375200",
    "end": "1378820"
  },
  {
    "text": "um yeah and typically for lrc 12 to 2 you can tolerate any one any three",
    "start": "1380240",
    "end": "1386720"
  },
  {
    "text": "failures within the group but you can't actually tolerate any four failures only",
    "start": "1386720",
    "end": "1394280"
  },
  {
    "text": "some of them can be actually reconstructed it's just not simply not possible",
    "start": "1394280",
    "end": "1399820"
  },
  {
    "text": "okay so can we do better than this",
    "start": "1401960",
    "end": "1407000"
  },
  {
    "start": "1402000",
    "end": "1402000"
  },
  {
    "text": "um notice that you know we have a 2X overhead for cross Zone replication so",
    "start": "1407000",
    "end": "1413539"
  },
  {
    "text": "even though our internal Zone replication factor is quite good",
    "start": "1413539",
    "end": "1419299"
  },
  {
    "text": "um and ideal we still have this multi-region replication that we do so",
    "start": "1419299",
    "end": "1424940"
  },
  {
    "text": "what else can we do",
    "start": "1424940",
    "end": "1428019"
  },
  {
    "start": "1431000",
    "end": "1431000"
  },
  {
    "text": "so a while ago the team made some really good observations about the type of data that we have in our systems",
    "start": "1432080",
    "end": "1438740"
  },
  {
    "text": "so the observation was that retrievals are for 90 percent of data uploaded in",
    "start": "1438740",
    "end": "1444980"
  },
  {
    "text": "the last year so even as you go here through the graph you can see that",
    "start": "1444980",
    "end": "1450799"
  },
  {
    "text": "80 percent of the retrievals happen within the first 100 days",
    "start": "1450799",
    "end": "1456980"
  },
  {
    "text": "and this is quite interesting which means that we have a bunch of data that's essentially cold and not accessed",
    "start": "1456980",
    "end": "1463880"
  },
  {
    "text": "very much so we want to actually optimize for this",
    "start": "1463880",
    "end": "1470360"
  },
  {
    "start": "1466000",
    "end": "1466000"
  },
  {
    "text": "workload so we have a workload with low reads um we want similar latency to what we",
    "start": "1470360",
    "end": "1477260"
  },
  {
    "text": "have today for the rest of magic pocket and",
    "start": "1477260",
    "end": "1482840"
  },
  {
    "text": "the requests don't have to be in the hot part of regress meaning we don't have to do live rights into the cold storage it",
    "start": "1482840",
    "end": "1491000"
  },
  {
    "text": "can happen at some point later and data",
    "start": "1491000",
    "end": "1496179"
  },
  {
    "text": "and we want to keep the same similar durability and availability guarantees but again Lower that replication factor",
    "start": "1496179",
    "end": "1502940"
  },
  {
    "text": "from 2x further down and another thing that we can do here is",
    "start": "1502940",
    "end": "1508280"
  },
  {
    "text": "we can make use of more than one of our regions",
    "start": "1508280",
    "end": "1514240"
  },
  {
    "text": "okay so now I'm going to talk about our cold storage system and how that works at a",
    "start": "1514460",
    "end": "1520100"
  },
  {
    "text": "very high level so the inspiration came from",
    "start": "1520100",
    "end": "1526700"
  },
  {
    "text": "um Facebook's warm blob storage system there was a paper that was written a few years ago and had a very interesting",
    "start": "1526700",
    "end": "1533120"
  },
  {
    "text": "idea there and the idea is as follows so let's say you have a Blog and you split it in half",
    "start": "1533120",
    "end": "1539600"
  },
  {
    "text": "so the first half is blob one and the second half is blob two then you take the third Parts which is",
    "start": "1539600",
    "end": "1546500"
  },
  {
    "text": "going to be the xor of blob 1 and block two and we call those fragments those",
    "start": "1546500",
    "end": "1551840"
  },
  {
    "text": "fragments will be individually stored in different zones so you'll block one blob two",
    "start": "1551840",
    "end": "1557240"
  },
  {
    "text": "and blob one xorb blob tube so now if you need to get the full blob",
    "start": "1557240",
    "end": "1566900"
  },
  {
    "text": "you simply need any one combination of blob 1 and blob 2 or blob 1 xor blob two",
    "start": "1566900",
    "end": "1575179"
  },
  {
    "text": "um or blood one uh and the xor over here and",
    "start": "1575179",
    "end": "1580580"
  },
  {
    "text": "you need to have any of the two regions to be available to do the Fetch and if",
    "start": "1580580",
    "end": "1587539"
  },
  {
    "text": "you want to do it right you have to have all regions fully available and again this is fine because the migrations are",
    "start": "1587539",
    "end": "1594620"
  },
  {
    "text": "happening in the background so we're not doing them live",
    "start": "1594620",
    "end": "1600340"
  },
  {
    "text": "so let's talk about some of the wins that we get from our Cold Storage",
    "start": "1601880",
    "end": "1607279"
  },
  {
    "start": "1604000",
    "end": "1604000"
  },
  {
    "text": "so we went from 2x replication to 1.5 replication and this is a huge amount of",
    "start": "1607279",
    "end": "1614360"
  },
  {
    "text": "savings so this is nearly 25 savings from a 2X replication that we had",
    "start": "1614360",
    "end": "1622460"
  },
  {
    "text": "um another win around durability is that the fragments themselves they're still",
    "start": "1622460",
    "end": "1627679"
  },
  {
    "text": "internally Erasure coded and the migration as I said is done in",
    "start": "1627679",
    "end": "1633620"
  },
  {
    "text": "the background and when you do the fetch from multiple",
    "start": "1633620",
    "end": "1639200"
  },
  {
    "text": "zones that actually endures a lot of overhead on the backbone bandwidth so",
    "start": "1639200",
    "end": "1644360"
  },
  {
    "text": "what we do is we hatch requests such that we send the request to the two",
    "start": "1644360",
    "end": "1650240"
  },
  {
    "text": "closest Zone from where the originating service is is at",
    "start": "1650240",
    "end": "1656120"
  },
  {
    "text": "um and then if we don't hear response from any one of the um from the two zones uh over some period of a few",
    "start": "1656120",
    "end": "1664940"
  },
  {
    "text": "hundred milliseconds we actually fetch from the remaining Zone and that",
    "start": "1664940",
    "end": "1670700"
  },
  {
    "text": "um is able to save quite a lot of uh on the backbone itself",
    "start": "1670700",
    "end": "1676000"
  },
  {
    "text": "okay let's move over to some operational discussions on Magic pocket",
    "start": "1676940",
    "end": "1685299"
  },
  {
    "start": "1687000",
    "end": "1687000"
  },
  {
    "text": "so the first component that I want to talk about here is how we do releases",
    "start": "1688340",
    "end": "1694279"
  },
  {
    "text": "so a release cycle is around four weeks end to end across all our zones",
    "start": "1694279",
    "end": "1701260"
  },
  {
    "text": "we first start off with a series of unit or integration tests before the changes",
    "start": "1701260",
    "end": "1706400"
  },
  {
    "text": "can be committed and unit and integration tests typically run a full version of a magic bucket",
    "start": "1706400",
    "end": "1713659"
  },
  {
    "text": "with all of our dependencies and you can run this in a developer box",
    "start": "1713659",
    "end": "1719139"
  },
  {
    "text": "um yeah fully the we also have a durability stage so the",
    "start": "1719240",
    "end": "1727400"
  },
  {
    "text": "durability tester itself runs a series a longer Suite of tests with um with full",
    "start": "1727400",
    "end": "1733220"
  },
  {
    "text": "verification of all the data so we'll do a bunch of writes and we'll make sure that the data is all fully covered",
    "start": "1733220",
    "end": "1741020"
  },
  {
    "text": "and it's about one week per stage here uh and this is to allow",
    "start": "1741020",
    "end": "1747559"
  },
  {
    "text": "the verifications to happen in each individual zone so typically we can do",
    "start": "1747559",
    "end": "1752720"
  },
  {
    "text": "all of verifications for metadata checks um within one week",
    "start": "1752720",
    "end": "1758960"
  },
  {
    "text": "and the release cycle is basically automated end to end we have",
    "start": "1758960",
    "end": "1764120"
  },
  {
    "text": "um [Music] checks that we do as we push code",
    "start": "1764120",
    "end": "1772220"
  },
  {
    "text": "changes forward and they'll automatically get aborted or they will not proceed if there's any let's say",
    "start": "1772220",
    "end": "1777559"
  },
  {
    "text": "alerts and things like that um and just for some exceptional cases",
    "start": "1777559",
    "end": "1783080"
  },
  {
    "text": "do we do we have to take control all right let's get into verifications",
    "start": "1783080",
    "end": "1789200"
  },
  {
    "start": "1786000",
    "end": "1786000"
  },
  {
    "text": "so we have a lot of verifications that happen within our system so the first",
    "start": "1789200",
    "end": "1794299"
  },
  {
    "text": "one is called the cross Zone verifier so the idea behind this is that we have",
    "start": "1794299",
    "end": "1799580"
  },
  {
    "text": "clients Upstream from us that know about data so files and how that maps to",
    "start": "1799580",
    "end": "1806659"
  },
  {
    "text": "specific hashes in our system so the crosstown verifier is essentially",
    "start": "1806659",
    "end": "1811779"
  },
  {
    "text": "making sure that these two systems are in sync all the time and then we have an index verifier and",
    "start": "1811779",
    "end": "1819740"
  },
  {
    "text": "the index verifier is scanning through our index table and it's going to ask",
    "start": "1819740",
    "end": "1825679"
  },
  {
    "text": "every single storage machine if they know about that specific blob we won't actually fetch the blob from disk we'll",
    "start": "1825679",
    "end": "1831679"
  },
  {
    "text": "just simply ask hey do you have it based on what it recently loaded from its",
    "start": "1831679",
    "end": "1837020"
  },
  {
    "text": "extend it extends that story then we have the Watcher and The Watcher",
    "start": "1837020",
    "end": "1843020"
  },
  {
    "text": "is a full validation of the actual blobs themselves and we validate the we do",
    "start": "1843020",
    "end": "1850100"
  },
  {
    "text": "sampling here we don't actually do this for all of our data and we validate this after one minute an hour a day and a",
    "start": "1850100",
    "end": "1857059"
  },
  {
    "text": "week and then we have a component called the trash inspector and this is making sure",
    "start": "1857059",
    "end": "1864500"
  },
  {
    "text": "that once an extent is deleted that all the the hashes in the extents have",
    "start": "1864500",
    "end": "1870380"
  },
  {
    "text": "actually been deleted so it's a last minute verification that we do and then we also",
    "start": "1870380",
    "end": "1876320"
  },
  {
    "text": "um scrub or scan through the extent information checking a checksum that's",
    "start": "1876320",
    "end": "1882740"
  },
  {
    "text": "on the extent themselves okay let's go to more operations so we",
    "start": "1882740",
    "end": "1891080"
  },
  {
    "text": "deal with lots and lots of migrations so we operate out of multiple data centers so there's migrations to move out of",
    "start": "1891080",
    "end": "1897919"
  },
  {
    "text": "different data centers all the time um we have a very large Fleet of storage",
    "start": "1897919",
    "end": "1904100"
  },
  {
    "text": "machines that we manage we have to um",
    "start": "1904100",
    "end": "1909620"
  },
  {
    "text": "you have to know what is happening all the time and there's lots and lots of automated",
    "start": "1909620",
    "end": "1915980"
  },
  {
    "text": "chaos going on so we have tons of Disaster Recovery events that are happening to test the reliability of our",
    "start": "1915980",
    "end": "1921440"
  },
  {
    "text": "system and operating magic pocket at this scale is just as difficult as the system",
    "start": "1921440",
    "end": "1928460"
  },
  {
    "text": "itself so it's just not counting um magic pocket so",
    "start": "1928460",
    "end": "1936340"
  },
  {
    "start": "1936000",
    "end": "1936000"
  },
  {
    "text": "Okay so some of the operations that we do is around managing background traffic so",
    "start": "1936559",
    "end": "1942919"
  },
  {
    "text": "background traffic accounts for most most of our traffic and this guy UPS so let's say the the disk scrubber is",
    "start": "1942919",
    "end": "1950720"
  },
  {
    "text": "constantly scanning through all this and checking that",
    "start": "1950720",
    "end": "1956299"
  },
  {
    "text": "um that checksum on the extents um we do a couple things so traffic by",
    "start": "1956299",
    "end": "1963679"
  },
  {
    "text": "service can be categorized into different traffic tiers live traffic is prioritized by the network",
    "start": "1963679",
    "end": "1969740"
  },
  {
    "text": "so we're okay with dropping let's say background traffic and I talked about the control plane but",
    "start": "1969740",
    "end": "1975200"
  },
  {
    "text": "it generates plans for a lot of the background traffic based on um any forecasts that we have about",
    "start": "1975200",
    "end": "1981980"
  },
  {
    "text": "let's say a data data center migration that we need to do the type of migration that we're doing maybe it's for cold",
    "start": "1981980",
    "end": "1987860"
  },
  {
    "text": "storage and so on",
    "start": "1987860",
    "end": "1991120"
  },
  {
    "start": "1991000",
    "end": "1991000"
  },
  {
    "text": "um around failures a couple interesting notes uh we repair about let's say four extents are",
    "start": "1993980",
    "end": "2002320"
  },
  {
    "text": "repaired every second uh the extents can be anywhere from one to two gigs in size",
    "start": "2002320",
    "end": "2007799"
  },
  {
    "text": "we have a pretty strict SLA on repairs less than 48 hours",
    "start": "2007799",
    "end": "2014620"
  },
  {
    "text": "um it's fine if we go over this but typically because the 48 hours is baked into our durability model we want to",
    "start": "2014620",
    "end": "2022120"
  },
  {
    "text": "keep this as low as possible and uh osds get allocated into our system",
    "start": "2022120",
    "end": "2029260"
  },
  {
    "text": "automatically based on the the size of the cell the current utilization if there's any free",
    "start": "2029260",
    "end": "2036820"
  },
  {
    "text": "pool within the uh the data center they're operating in",
    "start": "2036820",
    "end": "2043380"
  },
  {
    "text": "um and we have lots of stories around fighting Unknown single points of failures like ssds",
    "start": "2043480",
    "end": "2050560"
  },
  {
    "text": "um of certain variety filling all around the same time uh we have to manage all those things",
    "start": "2050560",
    "end": "2057898"
  },
  {
    "start": "2057000",
    "end": "2057000"
  },
  {
    "text": "another note on migrations uh we have lots of them uh two years ago we",
    "start": "2058720",
    "end": "2063878"
  },
  {
    "text": "migrated out of the SJC region there was a ton of planning that went behind the",
    "start": "2063879",
    "end": "2069520"
  },
  {
    "text": "scenes for something like this to happen I'll show you a quick graph of a plan for our migration out of",
    "start": "2069520",
    "end": "2076658"
  },
  {
    "text": "SJC so this is the amount of essentially data that we had to migrate",
    "start": "2076659",
    "end": "2083560"
  },
  {
    "text": "out of SGC over the period of time the um the light red line is the trend line",
    "start": "2083560",
    "end": "2091919"
  },
  {
    "text": "and the blue line is about the uh what we were expecting so",
    "start": "2091919",
    "end": "2097839"
  },
  {
    "text": "initially when we started the migration it was going really badly over here",
    "start": "2097839",
    "end": "2104859"
  },
  {
    "text": "and then over time we got really good and then we had this like really long tail end that we didn't really know how to",
    "start": "2104859",
    "end": "2111640"
  },
  {
    "text": "address so for these migrations that are very large hundreds of petabytes in size",
    "start": "2111640",
    "end": "2117060"
  },
  {
    "text": "there's a lot of planning that goes on behind the scenes we have to give ourselves extra time to make sure that",
    "start": "2117060",
    "end": "2122619"
  },
  {
    "text": "we can finish it in time okay and",
    "start": "2122619",
    "end": "2129700"
  },
  {
    "start": "2127000",
    "end": "2127000"
  },
  {
    "text": "um forecasting is another very important of managing a storage system of the scale",
    "start": "2129700",
    "end": "2134920"
  },
  {
    "text": "so storage is growing constantly sometimes we have unexpected growth we",
    "start": "2134920",
    "end": "2140020"
  },
  {
    "text": "need to account for and absorb it to our system uh we may have capacity crunch issues",
    "start": "2140020",
    "end": "2145420"
  },
  {
    "text": "due to supply chain um going bad let's say like covid disruptions",
    "start": "2145420",
    "end": "2150940"
  },
  {
    "text": "so we always need to have a a backup plan as soon as we kind of figure out",
    "start": "2150940",
    "end": "2156160"
  },
  {
    "text": "that there's problems up ahead because it takes so long to actually get combat new capacity ordered and delivered to a",
    "start": "2156160",
    "end": "2162880"
  },
  {
    "text": "data center it's not instantaneous and finally we do try and our forecasts",
    "start": "2162880",
    "end": "2170200"
  },
  {
    "text": "are actually directly fed into the control plane to perform these migrations based on what",
    "start": "2170200",
    "end": "2176680"
  },
  {
    "text": "um or capacity teams tell us",
    "start": "2176680",
    "end": "2181500"
  },
  {
    "text": "okay so in conclusion a couple of notes that have helped us",
    "start": "2183339",
    "end": "2188560"
  },
  {
    "start": "2184000",
    "end": "2184000"
  },
  {
    "text": "manage magic pocket are protect and verify your system so always be verifying your system this has a very",
    "start": "2188560",
    "end": "2196839"
  },
  {
    "text": "large overhead um but it's worth having this end-to-end verification",
    "start": "2196839",
    "end": "2202660"
  },
  {
    "text": "um it's incredibly empowering for engineers to know that the system is always verifying itself",
    "start": "2202660",
    "end": "2209160"
  },
  {
    "text": "for any inconsistencies and at this scale it's actually very",
    "start": "2209160",
    "end": "2216640"
  },
  {
    "text": "much and preferred to move slow um uh durability is one of the most",
    "start": "2216640",
    "end": "2222280"
  },
  {
    "text": "important things that we care about within our system so moving slow is totally okay waiting for those",
    "start": "2222280",
    "end": "2228400"
  },
  {
    "text": "verifications to happen before you deploy anything um always thinking about risk and what",
    "start": "2228400",
    "end": "2235480"
  },
  {
    "text": "can happen is very important to mindset that we have to keep in mind",
    "start": "2235480",
    "end": "2240820"
  },
  {
    "text": "um keeping things simple very important as we have let's say a very large scale migrations",
    "start": "2240820",
    "end": "2246720"
  },
  {
    "text": "if you have too many optimizations there's too much that you have to keep in mind for as for a mental model that",
    "start": "2246720",
    "end": "2254380"
  },
  {
    "text": "makes things difficult when you have to plan ahead or debug issues",
    "start": "2254380",
    "end": "2259420"
  },
  {
    "text": "and we always try and prepare for the worst we always have a a backup plan especially for migrations or if there's",
    "start": "2259420",
    "end": "2267280"
  },
  {
    "text": "any single points of failures within our system or when we're deploying changes make sure that things are not a one-way",
    "start": "2267280",
    "end": "2274119"
  },
  {
    "text": "door okay thank you that concludes my talk",
    "start": "2274119",
    "end": "2282240"
  },
  {
    "text": "okay so the first question um that I see here is does Dropbox",
    "start": "2285220",
    "end": "2290260"
  },
  {
    "text": "maintain its own data center or do you partner with hyperscalers like AWS for co-location uh so the answer to that is",
    "start": "2290260",
    "end": "2296920"
  },
  {
    "text": "that we do a little bit of both actually so um and North America regions uh we",
    "start": "2296920",
    "end": "2303099"
  },
  {
    "text": "actually lease our data centers um and in other regions where it doesn't",
    "start": "2303099",
    "end": "2309339"
  },
  {
    "text": "make sense we actually utilize for example S3 and",
    "start": "2309339",
    "end": "2314680"
  },
  {
    "text": "AWS for for compute as well so a bit of a mix there",
    "start": "2314680",
    "end": "2320320"
  },
  {
    "text": "oh so you leverage S3 yeah so um for example",
    "start": "2320320",
    "end": "2326200"
  },
  {
    "text": "um in some European regions uh some customers there want their data actually to exist uh locally in the region for",
    "start": "2326200",
    "end": "2335619"
  },
  {
    "text": "various compliance reasons so in those cases we actually utilize the street because we don't have data center",
    "start": "2335619",
    "end": "2342220"
  },
  {
    "text": "presence there okay yep and then do you use any type of",
    "start": "2342220",
    "end": "2347800"
  },
  {
    "text": "compression of the actual data before storing uh we we don't compress the data before",
    "start": "2347800",
    "end": "2353440"
  },
  {
    "text": "storing it so once the data is actually uploaded uh and in our servers that's",
    "start": "2353440",
    "end": "2359020"
  },
  {
    "text": "when we we do um uh compression and uh encryption uh so the data is encrypted",
    "start": "2359020",
    "end": "2367480"
  },
  {
    "text": "and compressed at rest and there's obviously trade-offs with that so if you were to compress data on",
    "start": "2367480",
    "end": "2375160"
  },
  {
    "text": "the clients um you know we have many different types of clients from uh desktops to mobile",
    "start": "2375160",
    "end": "2382180"
  },
  {
    "text": "Etc so it could actually be quite expensive for for the user to do that so",
    "start": "2382180",
    "end": "2388119"
  },
  {
    "text": "okay there's another question here just curious for SMR disk how does it cost compared to others",
    "start": "2388119",
    "end": "2395260"
  },
  {
    "text": "to other options I guess yeah so um we work very closely with uh",
    "start": "2395260",
    "end": "2401020"
  },
  {
    "text": "different Hardware vendors to basically adopt always latest hard drive",
    "start": "2401020",
    "end": "2406780"
  },
  {
    "text": "technology and a few years ago probably maybe five six years ago now we started",
    "start": "2406780",
    "end": "2413260"
  },
  {
    "text": "utilizing SMR technology very heavily so I talked about the trade-offs in the talk",
    "start": "2413260",
    "end": "2419020"
  },
  {
    "text": "um and yeah compared to [Music] um uh to PMR so traditional drives SMR is a",
    "start": "2419020",
    "end": "2429640"
  },
  {
    "text": "lot cheaper uh given given the density so um I can't talk about exactly the the",
    "start": "2429640",
    "end": "2435520"
  },
  {
    "text": "costs on the per gigabyte basis but um it is significantly cheaper and hence",
    "start": "2435520",
    "end": "2441160"
  },
  {
    "text": "why we're able to it works so well for us and and we save a ton of money by adopting a smart technology",
    "start": "2441160",
    "end": "2448420"
  },
  {
    "text": "and then yeah so we actually published some um information about the read speed",
    "start": "2448420",
    "end": "2454420"
  },
  {
    "text": "and IELTS performance on SMR versus PMR so in the end for us because we have",
    "start": "2454420",
    "end": "2460060"
  },
  {
    "text": "sequential reads and writes um it didn't make too much of a difference so there's some latency",
    "start": "2460060",
    "end": "2466660"
  },
  {
    "text": "differences with with SMR but um given our our workloads",
    "start": "2466660",
    "end": "2473260"
  },
  {
    "text": "um it's actually pretty comparable to PMR because again we don't do kind of these",
    "start": "2473260",
    "end": "2480240"
  },
  {
    "text": "random rates and so on so works very well for us",
    "start": "2480240",
    "end": "2486359"
  },
  {
    "text": "um any other Disk type other than SMR you may also consider in the future",
    "start": "2486579",
    "end": "2492220"
  },
  {
    "text": "um again so the industry is moving to um various Technologies in the future so",
    "start": "2492220",
    "end": "2498160"
  },
  {
    "text": "SMR is sort of hitting um capacity limits over the next like few",
    "start": "2498160",
    "end": "2504099"
  },
  {
    "text": "years so uh to give you some idea uh the latest drives today with SMR are coming",
    "start": "2504099",
    "end": "2509740"
  },
  {
    "text": "out this year um at about 26 terabytes per Drive which is huge but uh different vendors",
    "start": "2509740",
    "end": "2517060"
  },
  {
    "text": "obviously looking out three four five years from now uh it looks like laser assisted technology is going to be the",
    "start": "2517060",
    "end": "2523780"
  },
  {
    "text": "next big thing uh so look up look up for for Hammer For example uh technology and",
    "start": "2523780",
    "end": "2529839"
  },
  {
    "text": "so on and those drives are expected to increase density uh to 40 terabytes",
    "start": "2529839",
    "end": "2535839"
  },
  {
    "text": "within the next few years so that looks like that's the next big thing that's coming for for new Drive technology",
    "start": "2535839",
    "end": "2544060"
  },
  {
    "text": "I didn't know if you uh if you covered this but do you have like tiered storage just less recently accessed storage you",
    "start": "2544060",
    "end": "2552040"
  },
  {
    "text": "do that behind the scenes uh yeah so we we don't do",
    "start": "2552040",
    "end": "2557859"
  },
  {
    "text": "um tier storage so what we used to have is we used to utilize SSD drives um as a",
    "start": "2557859",
    "end": "2563800"
  },
  {
    "text": "cache for writing um uh on a per OSD device basis so",
    "start": "2563800",
    "end": "2571780"
  },
  {
    "text": "the problem that ends up happening is that that became our limiting bottleneck",
    "start": "2571780",
    "end": "2577960"
  },
  {
    "text": "um for rights and so we actually ended up getting rid of that recently um and we're able to get a lot more",
    "start": "2577960",
    "end": "2584440"
  },
  {
    "text": "right throughput across the fleet uh in order to to save from that so",
    "start": "2584440",
    "end": "2590980"
  },
  {
    "text": "um the other option the other thing that I talked about is colder rights which we utilize",
    "start": "2590980",
    "end": "2596500"
  },
  {
    "text": "um our Cold Storage tier but it's still on SMR Drive backed by SMR",
    "start": "2596500",
    "end": "2603940"
  },
  {
    "text": "and then we have explored um uh for example using ssds more heavily and",
    "start": "2603940",
    "end": "2611560"
  },
  {
    "text": "so on so that's actually my something that we might leverage um it just actually incredibly hard to",
    "start": "2611560",
    "end": "2617400"
  },
  {
    "text": "uh to do caching well uh at the scale for various reasons",
    "start": "2617400",
    "end": "2623920"
  },
  {
    "text": "um protections and verifications inconsistencies all that stuff is is",
    "start": "2623920",
    "end": "2629200"
  },
  {
    "text": "really hard once you um once you go through the the motion so",
    "start": "2629200",
    "end": "2634540"
  },
  {
    "text": "um that's one of the the limiting factors but for a cold storage tier because we built it on top of existing",
    "start": "2634540",
    "end": "2640300"
  },
  {
    "text": "infrastructure it was quite straightforward to to build on top of that so",
    "start": "2640300",
    "end": "2646200"
  },
  {
    "text": "um are there any uh I mean you've seen this system I guess in uh evolve and uh",
    "start": "2646420",
    "end": "2652359"
  },
  {
    "text": "for a few years now what do you have on the horizon that you'd like to see uh like built or you'd like to build or",
    "start": "2652359",
    "end": "2659920"
  },
  {
    "text": "design into the system yeah I think uh for us there's always a",
    "start": "2659920",
    "end": "2666099"
  },
  {
    "text": "few different things so the steady state kind of status quo that we're kind of looking into is uh continue to scale the",
    "start": "2666099",
    "end": "2673060"
  },
  {
    "text": "system um for continuous growth right so uh we're we're growing at",
    "start": "2673060",
    "end": "2678640"
  },
  {
    "text": "um double digits per year right so uh that typically means that in in three years",
    "start": "2678640",
    "end": "2684819"
  },
  {
    "text": "three four years we essentially have to double the overall capacity of our system right",
    "start": "2684819",
    "end": "2690160"
  },
  {
    "text": "um and so when that happens that has a lot of implications for our system so a lot of things that you thought would",
    "start": "2690160",
    "end": "2695859"
  },
  {
    "text": "be able to continually scale uh you actually run into various types of limited limited um uh vertical scaling",
    "start": "2695859",
    "end": "2704440"
  },
  {
    "text": "limits and so that's something that we're actively looking into for next year",
    "start": "2704440",
    "end": "2710020"
  },
  {
    "text": "and then the other ones around having more flexibility around our metadata so",
    "start": "2710020",
    "end": "2716940"
  },
  {
    "text": "we use MySQL behind the scenes shout out my sequel but that has its own set of",
    "start": "2716940",
    "end": "2722500"
  },
  {
    "text": "problems around being able to manage MySQL at that scale um you know if you want to easily add new",
    "start": "2722500",
    "end": "2729640"
  },
  {
    "text": "columns and so on that's also a huge pain if you want to um",
    "start": "2729640",
    "end": "2735040"
  },
  {
    "text": "uh continue to kind of scale that up you also have to completely do a split and",
    "start": "2735040",
    "end": "2741280"
  },
  {
    "text": "that's costly so um most likely our metadata stack will change next year and that's kind of what",
    "start": "2741280",
    "end": "2747579"
  },
  {
    "text": "we're looking at and then again the last one is um Hardware advancements and supporting",
    "start": "2747579",
    "end": "2754900"
  },
  {
    "text": "for example Hammer once it comes out and being able to get ahead of the curve on that is something that we're always",
    "start": "2754900",
    "end": "2761260"
  },
  {
    "text": "continually supporting as well um do you have any War stories that come",
    "start": "2761260",
    "end": "2767500"
  },
  {
    "text": "to your mind that uh you know parts of the system that kind of woke you up late at night burned you and you guys had to",
    "start": "2767500",
    "end": "2774460"
  },
  {
    "text": "jump on it and come up with maybe a short-term solution long-term solution are there like any interesting work stories that you might be sure that you",
    "start": "2774460",
    "end": "2779980"
  },
  {
    "text": "feel you can actually um yeah there's a lot of interesting stories that we've had to deal with over",
    "start": "2779980",
    "end": "2785800"
  },
  {
    "text": "the years around um um memory corruption and so on and",
    "start": "2785800",
    "end": "2792280"
  },
  {
    "text": "um finding areas within our system that haven't had proper protection so",
    "start": "2792280",
    "end": "2797560"
  },
  {
    "text": "um we we do have for example Corruptions are happening all the time but because of these protections verifications that",
    "start": "2797560",
    "end": "2803980"
  },
  {
    "text": "are going on behind the scenes uh we don't really notice them and um you know the system fails gracefully uh but once",
    "start": "2803980",
    "end": "2810520"
  },
  {
    "text": "in a while we'll get woken up in the middle of the night uh for some new random corruption that we didn't know",
    "start": "2810520",
    "end": "2816520"
  },
  {
    "text": "about um and one of them actually happened recently data is fine because it has",
    "start": "2816520",
    "end": "2821980"
  },
  {
    "text": "replicated many many regions and um even if the data is corrupted",
    "start": "2821980",
    "end": "2827740"
  },
  {
    "text": "um the the data still continues to live to live in what we call trash which is a",
    "start": "2827740",
    "end": "2833020"
  },
  {
    "text": "another protection mechanism where the data is deleted but it's kind of self-deleted you can still recover it",
    "start": "2833020",
    "end": "2839200"
  },
  {
    "text": "anyways uh yeah we had kind of like a all hands on",
    "start": "2839200",
    "end": "2844300"
  },
  {
    "text": "deck situation a few weeks ago where kind of this happened and uh took",
    "start": "2844300",
    "end": "2850720"
  },
  {
    "text": "many kind of late nights to kind of figure out where exactly things might be obviously we don't log everything so uh",
    "start": "2850720",
    "end": "2857500"
  },
  {
    "text": "being able to um figure out where the problem came from is very difficult but I think we we",
    "start": "2857500",
    "end": "2864700"
  },
  {
    "text": "kind of tracked it down to one or two places now it's like trash was filling up faster",
    "start": "2864700",
    "end": "2870220"
  },
  {
    "text": "than you could sort of get rid of it because because the soft Elites were not real delete so you're running out of storage uh we always keep data around in",
    "start": "2870220",
    "end": "2877900"
  },
  {
    "text": "trash for seven days um that's part of the problem um it's the the memory corruption so we",
    "start": "2877900",
    "end": "2884380"
  },
  {
    "text": "found a a just a single hash single piece of data uh that ended up we found",
    "start": "2884380",
    "end": "2890440"
  },
  {
    "text": "that it was corrupted um and yeah I was just kind of mentioning like being able to track that stuff down um",
    "start": "2890440",
    "end": "2897460"
  },
  {
    "text": "is very difficult um uh you know even though you may have many different places where we do",
    "start": "2897460",
    "end": "2903579"
  },
  {
    "text": "various tracks and so on along the way but yeah I I see that we're out of time but I",
    "start": "2903579",
    "end": "2908980"
  },
  {
    "text": "want to thank you again fukundo for sharing your experience with us and if people have a question for you how can",
    "start": "2908980",
    "end": "2914980"
  },
  {
    "text": "they best reach you yeah I'll be on slack uh today um if you have any other questions you",
    "start": "2914980",
    "end": "2920140"
  },
  {
    "text": "think about I'm also on LinkedIn I'm happy to chat with you there it's up to your request for uh for any connections",
    "start": "2920140",
    "end": "2927880"
  },
  {
    "text": "um looks like there's one more question I'll copy that and answer in Slack oh yeah sure all right thank you very",
    "start": "2927880",
    "end": "2933520"
  },
  {
    "text": "much all right thank you guys bye",
    "start": "2933520",
    "end": "2937740"
  },
  {
    "text": "foreign [Music]",
    "start": "2938619",
    "end": "2946380"
  }
]