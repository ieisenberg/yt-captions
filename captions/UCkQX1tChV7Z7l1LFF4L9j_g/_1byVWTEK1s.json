[
  {
    "text": "hi I'm Adam brelle I do a lot of work with Apache spark and I teach a lot of classes on spark with data bricks and",
    "start": "6839",
    "end": "13240"
  },
  {
    "text": "New Circle and in a lot of classes there are questions that come up uh about",
    "start": "13240",
    "end": "18640"
  },
  {
    "text": "certain apis data frames and data sets how they work and how to use them so I wanted to talk a little bit uh about how",
    "start": "18640",
    "end": "25119"
  },
  {
    "text": "those work uh when they make sense and go a little bit deeper technically uh into the advantages and mechanisms here",
    "start": "25119",
    "end": "32680"
  },
  {
    "text": "uh everything we're doing today is on the internet and you'll be able to play around with spark 2.0 through data",
    "start": "32680",
    "end": "38239"
  },
  {
    "text": "bricks Community Edition um so this is called modern spark data frame and data set frequent questions answers and a",
    "start": "38239",
    "end": "45000"
  },
  {
    "text": "look behind the scenes so what do we mean by modern spark well about a year ago we were really excited about the",
    "start": "45000",
    "end": "50640"
  },
  {
    "text": "data frame API and until relatively recently it was marked experimental because of the way the uh documentation",
    "start": "50640",
    "end": "56760"
  },
  {
    "text": "is generated but really by this point data frames are the established way to do things in spark what I'm going to",
    "start": "56760",
    "end": "63680"
  },
  {
    "text": "argue is that uh not only they perform better and they're easier to use but at this point there isn't really any reason",
    "start": "63680",
    "end": "70880"
  },
  {
    "text": "not to be using this API uh so that's kind of the tldr but what we want to do is say okay how does this actually work",
    "start": "70880",
    "end": "78400"
  },
  {
    "text": "not just how do you use it uh but when I say it works better what do I actually mean so let's go and take a look at some",
    "start": "78400",
    "end": "84840"
  },
  {
    "text": "code uh what we're going to play with here is a small data set that's actually hosted by mongodb uh just has zip code",
    "start": "84840",
    "end": "91000"
  },
  {
    "text": "data for the United States so this is not big data uh but it makes it really it's got a little bit of structure to it makes it easy to uh take a look and",
    "start": "91000",
    "end": "98200"
  },
  {
    "text": "explore with datab bricks Community Edition so the very first thing I'm going to do here is just retrieve the",
    "start": "98200",
    "end": "103240"
  },
  {
    "text": "data set uh using W get uh for Big Data is probably not the best way to do it you'd probably load data into spark from",
    "start": "103240",
    "end": "109560"
  },
  {
    "text": "hdfs or S3 or something like that um but we're going to retrieve it here since",
    "start": "109560",
    "end": "114680"
  },
  {
    "text": "it's a small data set over the web and we'll just wait for this to download and it looks like like here it",
    "start": "114680",
    "end": "121520"
  },
  {
    "text": "is and I'm going to scroll down so where did this file go zip. Json we'll do a",
    "start": "121520",
    "end": "126560"
  },
  {
    "text": "PWD here looks like we're in SL datab brick SL driver",
    "start": "126560",
    "end": "132200"
  },
  {
    "text": "so how can I get started with this and data frames I'm going to go SQL context",
    "start": "132200",
    "end": "137519"
  },
  {
    "text": "read Json and point it at the file and I'm going to say register temp table uh",
    "start": "137519",
    "end": "142800"
  },
  {
    "text": "which creates a view that I can use to talk about that table in SQL so I'm",
    "start": "142800",
    "end": "147959"
  },
  {
    "text": "going to go ahead and run that and notice the deprecation warning that came up uh that's new because of spark",
    "start": "147959",
    "end": "155760"
  },
  {
    "text": "2.0 um in spark 2.0 we call this create a replace Temp View uh because register",
    "start": "155760",
    "end": "161920"
  },
  {
    "text": "temp table uh caused a little bit of confusion so I can use that call but it generates this deprecation warning so",
    "start": "161920",
    "end": "168319"
  },
  {
    "text": "we'll come down here and try the uh spark 2.0 version notice also that I use",
    "start": "168319",
    "end": "173519"
  },
  {
    "text": "the spark session object at the beginning of the line uh so I say spark instead of SQL context in spark 2.0",
    "start": "173519",
    "end": "180800"
  },
  {
    "text": "because of the proliferation of context objects spark context SQL context streaming context uh there's a new spark",
    "start": "180800",
    "end": "187920"
  },
  {
    "text": "session object that provides an easy entry point for our code so I'm going to try this spark. read. Json and the",
    "start": "187920",
    "end": "194799"
  },
  {
    "text": "create or replace Temp View so we'll run this and we get the same thing but this shows that we're using the latest API",
    "start": "194799",
    "end": "202239"
  },
  {
    "text": "and then I can hop over here into SQL and just take a look at this table uh",
    "start": "202239",
    "end": "208439"
  },
  {
    "text": "not the data this is the SC scha of the table I do a describe on it and we can see that there's a uh City location",
    "start": "208439",
    "end": "215920"
  },
  {
    "text": "population state the underscore ID field is actually where the ZIP code lives uh",
    "start": "215920",
    "end": "221239"
  },
  {
    "text": "this is sort of a mongoism it'll be familiar to you if you've used but we're going to go ahead and change that",
    "start": "221239",
    "end": "226280"
  },
  {
    "text": "to be called zip so in this next line I'm just going to grab the table and say",
    "start": "226280",
    "end": "231319"
  },
  {
    "text": "with column renamed and then I'll create or replace the Temp View so I'm going to throw away the old one and bring a newer",
    "start": "231319",
    "end": "237640"
  },
  {
    "text": "one in here so that's all we had to do to get our",
    "start": "237640",
    "end": "243879"
  },
  {
    "text": "data uh into a situation where we can work with it using SQL so let's do a very simple analytic query on this thing",
    "start": "243879",
    "end": "251200"
  },
  {
    "text": "what are the most popul cities in Illinois and how many post codes do they have right this is a pretty reasonable thing to do with SQL uh we've got a",
    "start": "251200",
    "end": "258560"
  },
  {
    "text": "select count of zip here sum of population and City from the zip table",
    "start": "258560",
    "end": "265199"
  },
  {
    "text": "we made where the state is Illinois we're going to group by City we're going to order by the population total",
    "start": "265199",
    "end": "271840"
  },
  {
    "text": "descending and we're going to limit to the top 10 and if we go ahead and run this just straightforward SQL against",
    "start": "271840",
    "end": "278440"
  },
  {
    "text": "this data set uh you know we see Chicago at the top which isn't too surprising uh and a handful of other cities that have",
    "start": "278440",
    "end": "285400"
  },
  {
    "text": "100,000 or 50,000 or more people here in Illinois uh over on the left you can see the number of zip codes so we're not",
    "start": "285400",
    "end": "292160"
  },
  {
    "text": "going to do a whole lot of analytics work here but this just is a straightforward query and what I want to",
    "start": "292160",
    "end": "298400"
  },
  {
    "text": "do is talk about you know why would we do do it this way and not with the rdd API so there a couple of",
    "start": "298400",
    "end": "306759"
  },
  {
    "text": "reasons um first I I prepared a little example so we can see what it looks like to code this with the rdd API so to",
    "start": "306759",
    "end": "314360"
  },
  {
    "text": "rewind a second remember this is a simple query top most populous cities in",
    "start": "314360",
    "end": "319800"
  },
  {
    "text": "Illinois uh ordered uh grouped together and when we uh go to do this using the",
    "start": "319800",
    "end": "327120"
  },
  {
    "text": "rdd API first I'm going to run this cell that prepares the the file we're going to look at and this is how we're going",
    "start": "327120",
    "end": "333840"
  },
  {
    "text": "to do it uh we're going to read the data in we're going to parse it on the delimiters here we're going to create",
    "start": "333840",
    "end": "340240"
  },
  {
    "text": "these Scala tupal that represent rows and then to do the query we're going to filter for Illinois over here we're",
    "start": "340240",
    "end": "346840"
  },
  {
    "text": "going to uh project out um the city and",
    "start": "346840",
    "end": "353199"
  },
  {
    "text": "the population we're going to do a Reduce by key so for each City we roll up all the population data",
    "start": "353199",
    "end": "360759"
  },
  {
    "text": "data then we're going to sort this thing uh in a descending order and we'll do a",
    "start": "360759",
    "end": "366599"
  },
  {
    "text": "take 10 that we can get the results back more efficiently and we'll print them all out so I'm going to run this and I I",
    "start": "366599",
    "end": "374560"
  },
  {
    "text": "just want you to see that we get the same data but unless you uh really kind",
    "start": "374560",
    "end": "379680"
  },
  {
    "text": "of En en enjoy doing things the hard way this is not easy uh to read or to write",
    "start": "379680",
    "end": "386280"
  },
  {
    "text": "right once we know what we're doing if I already explained what the goal is this kind of makes sense but uh it takes a it",
    "start": "386280",
    "end": "392440"
  },
  {
    "text": "takes a couple minutes to think about how to structure this you can imagine it's pretty easy to get it wrong and in",
    "start": "392440",
    "end": "398400"
  },
  {
    "text": "six months when I come back to it or my cooworker needs to look at it it's really not obvious that this is a simple analytics query so just from a human",
    "start": "398400",
    "end": "404880"
  },
  {
    "text": "usability point of view uh the the sql's a whole lot whole lot nicer and what I'm",
    "start": "404880",
    "end": "410000"
  },
  {
    "text": "going to be showing is that not only is that the case but the SQL version will run faster and it will uh execute a",
    "start": "410000",
    "end": "417080"
  },
  {
    "text": "better version of the query than the one that we're trying to do by hand anyway so now that we've got that comparison",
    "start": "417080",
    "end": "424759"
  },
  {
    "text": "down let's come down here and ask a simple question is this the best way to",
    "start": "424759",
    "end": "430240"
  },
  {
    "text": "write this query and I just want to demonstrate something here um so what what I wrote is reasonable uh but would",
    "start": "430240",
    "end": "436599"
  },
  {
    "text": "spark fix it if we wrote a worse version so in this next cell here's a slightly worse version of that same query it",
    "start": "436599",
    "end": "443120"
  },
  {
    "text": "produces the same answer but why is it worse uh well I'm doing all of my processing before I filter for Illinois",
    "start": "443120",
    "end": "449360"
  },
  {
    "text": "right right so I'm doing a lot more work than throwing most of it away if I filtered first I'd have less data this",
    "start": "449360",
    "end": "454520"
  },
  {
    "text": "is obvious and most uh traditional database uh systems will do these optimizations for you but the rdd API",
    "start": "454520",
    "end": "461319"
  },
  {
    "text": "isn't going to and to prove that let's go and take a look at the job UI and see what just",
    "start": "461319",
    "end": "466440"
  },
  {
    "text": "happened so I'm going to switch over to this tab uh this shows my spark 2.0",
    "start": "466440",
    "end": "472120"
  },
  {
    "text": "cluster that's running I'm going to click on this link that says spark UI I'm going to come over to jobs",
    "start": "472120",
    "end": "480400"
  },
  {
    "text": "and what I'm going to do first is look at the job I just ran that's this one over here job number",
    "start": "480400",
    "end": "487759"
  },
  {
    "text": "seven and what I want to do is take a look down where it says Shuffle read and",
    "start": "488120",
    "end": "493360"
  },
  {
    "text": "Shuffle write so in the efficient version of the query or at least the the one that I wrote pretty well we're",
    "start": "493360",
    "end": "500319"
  },
  {
    "text": "moving about 15K well 14k through this uh Shuffle between the stages now if I",
    "start": "500319",
    "end": "507280"
  },
  {
    "text": "flip back and run this less efficient version of the same query right we get",
    "start": "507280",
    "end": "513200"
  },
  {
    "text": "you know nice answer here but if we come back to the jobs UI and if I click on",
    "start": "513200",
    "end": "519240"
  },
  {
    "text": "job nine and scroll down notice that we're moving several 100K through the shuffle",
    "start": "519240",
    "end": "526959"
  },
  {
    "text": "so we're up by you know somewhere between one and two orders of magnitude in terms of the quantity of data that",
    "start": "526959",
    "end": "533200"
  },
  {
    "text": "we're making spark process and that means that as our job scales up things are going to take a lot more resources",
    "start": "533200",
    "end": "538760"
  },
  {
    "text": "and a lot longer longer to do the exact same job so in other words with the rdd API the burden is on you as a developer",
    "start": "538760",
    "end": "545600"
  },
  {
    "text": "to understand this and to get it right and if you get it wrong even if things work they may work a lot slower okay you",
    "start": "545600",
    "end": "552560"
  },
  {
    "text": "can see I just keyed in a little label here that says onto data frame and data",
    "start": "552560",
    "end": "557920"
  },
  {
    "text": "set details so we'll switch over uh first let's pick up something small let's cach this table we don't really",
    "start": "557920",
    "end": "564480"
  },
  {
    "text": "need to cat it uh but it's always handy to have it in memory since it doesn't take up much space so notice that when I",
    "start": "564480",
    "end": "570720"
  },
  {
    "text": "run a SQL cache table command that it actually cached right away it actually did some work there uh this is",
    "start": "570720",
    "end": "577160"
  },
  {
    "text": "interesting because typically caching in spark when you go through the programmatic apis is lazy your data",
    "start": "577160",
    "end": "582480"
  },
  {
    "text": "doesn't actually get cached until it gets touched by some subsequent action but when I use SQL and I say cach table",
    "start": "582480",
    "end": "588720"
  },
  {
    "text": "zip it's cached right away how can we tell let's flip over to the storage UI",
    "start": "588720",
    "end": "594680"
  },
  {
    "text": "and see if it's there and here we can see it in memory table zip and we can",
    "start": "594680",
    "end": "600680"
  },
  {
    "text": "see that it's actually pretty small it fits in memory and it's only about two 2 megabytes here so uh nothing too",
    "start": "600680",
    "end": "606480"
  },
  {
    "text": "exciting uh but it's all cached in memory uh from doing that cach table",
    "start": "606480",
    "end": "612600"
  },
  {
    "text": "directive so once we're agreed that we don't want to be using rdds which are",
    "start": "612600",
    "end": "617640"
  },
  {
    "text": "really kind of an internal API going forward um what about data sets and data",
    "start": "617640",
    "end": "622880"
  },
  {
    "text": "frames well what I want to explain here is they're really the same thing they've always been intended to be the same",
    "start": "622880",
    "end": "628640"
  },
  {
    "text": "thing and now that we have spark 2.0 they really are so how can we tell well",
    "start": "628640",
    "end": "634560"
  },
  {
    "text": "right here we can do a little check in Scala um is class of data frame the same as the class of data set so if I run",
    "start": "634560",
    "end": "641680"
  },
  {
    "text": "this you can see that it comes back true and if you want to see where that lives in the code I'm going to flip over to",
    "start": "641680",
    "end": "647440"
  },
  {
    "text": "this tab and I'm going to scroll down this is from the SQL package in the spark code",
    "start": "647440",
    "end": "655480"
  },
  {
    "text": "base on GitHub as of today right down here online 9 45 we Define data frame to",
    "start": "655480",
    "end": "662040"
  },
  {
    "text": "be data set of row uh and that allows backward compatibility we can talk about data frames uh but they really are the",
    "start": "662040",
    "end": "668480"
  },
  {
    "text": "same thing as data set and the reason why that's important is that we want to think about data sets and data frames",
    "start": "668480",
    "end": "675120"
  },
  {
    "text": "being very similar so the the differences are much less important than the similarities we want to program them",
    "start": "675120",
    "end": "681760"
  },
  {
    "text": "the same way as much as we can so data sets offer this strong typing instead of",
    "start": "681760",
    "end": "687440"
  },
  {
    "text": "just having rows we can have uh strongly typed Scala objects in there but the first thing to realize is this typing is",
    "start": "687440",
    "end": "693959"
  },
  {
    "text": "virtual it's like a view it's applied when we ask for it but the underlying data is always stored in a binary",
    "start": "693959",
    "end": "701040"
  },
  {
    "text": "language neutral encoding we'll demonstrate that later so when we talk about having a data set and making it be",
    "start": "701040",
    "end": "707560"
  },
  {
    "text": "one type Or Another We're not actually doing any processing and we're not actually converting anything uh we're",
    "start": "707560",
    "end": "713399"
  },
  {
    "text": "just treating it as a certain type when we want to use that in our code so here's a really quick demo",
    "start": "713399",
    "end": "719839"
  },
  {
    "text": "the traditional SQL context. range call that used to produce a data frame if I run it now it produces a data set notice",
    "start": "719839",
    "end": "727240"
  },
  {
    "text": "that I get a data set of long the schema has a column called ID which is a long",
    "start": "727240",
    "end": "732360"
  },
  {
    "text": "just like we used to have with data frame um and notice that over here it says ID big in right so that's the",
    "start": "732360",
    "end": "739600"
  },
  {
    "text": "actual data type underneath I can take that data set and I can say I'd like to treat this as a data set of strings so",
    "start": "739600",
    "end": "746959"
  },
  {
    "text": "if I run this cell I get a data set of string and again on the right it's still big int so note the Scola type changes but",
    "start": "746959",
    "end": "754800"
  },
  {
    "text": "the underlying schema stays the same uh what is this big int well the underlying types here are language neutral database",
    "start": "754800",
    "end": "762079"
  },
  {
    "text": "style types so think about the kind of types that you have in a SQL database or in Hive uh things like big in and string",
    "start": "762079",
    "end": "769320"
  },
  {
    "text": "uh those are the universal underlying types and they're encoded in a binary",
    "start": "769320",
    "end": "775399"
  },
  {
    "text": "format if we wanted to go back to a data frame we can call 2df on a data set and",
    "start": "775680",
    "end": "781519"
  },
  {
    "text": "that just gets us rows so instead of having a data a data set of the original type we'll get a data set of row and you",
    "start": "781519",
    "end": "787480"
  },
  {
    "text": "can see that here so uh here I've got when I do a collect I get an array of row objects that have these numbers in",
    "start": "787480",
    "end": "795320"
  },
  {
    "text": "them so language neutral what does that mean well I'm not going to demonstrate",
    "start": "795320",
    "end": "800880"
  },
  {
    "text": "this but with an rdd if I cach it in Scala and then come in say through Python and use the spark context to find",
    "start": "800880",
    "end": "807519"
  },
  {
    "text": "the persistent rdds pull pull back some data that will actually work but the data I'm looking at is going to be uh",
    "start": "807519",
    "end": "813920"
  },
  {
    "text": "encoded objects from whatever language created the rdd so that would mean it's either going to be very difficult to use",
    "start": "813920",
    "end": "820199"
  },
  {
    "text": "or I'll need to somehow decode this myself uh I'm sure it's possible but it's certainly not easy uh in this case",
    "start": "820199",
    "end": "826199"
  },
  {
    "text": "we don't have to do any of that um if I take this little list of integers here and I say create or replace Temp View",
    "start": "826199",
    "end": "833320"
  },
  {
    "text": "numbers so I'm making a little table called numbers and now I'm going to come into python so here's python code and I'm",
    "start": "833320",
    "end": "840399"
  },
  {
    "text": "going to look up this table by name and I'm going to do a print schema on it so",
    "start": "840399",
    "end": "845720"
  },
  {
    "text": "notice that it does show up the same way over here in Python and I can actually read the contents out in Python so I'm",
    "start": "845720",
    "end": "851560"
  },
  {
    "text": "going to go SQL context. numbers I'm going to collect that data back to the driver in Python and we're going to",
    "start": "851560",
    "end": "857120"
  },
  {
    "text": "print it out using regular python code and here are my numbers right so Scala python are SQL it doesn't matter uh the",
    "start": "857120",
    "end": "865240"
  },
  {
    "text": "data is all language neutral and we can access it in whatever ways most convenient for whichever business group",
    "start": "865240",
    "end": "870720"
  },
  {
    "text": "needs to do that so okay so how would this work with",
    "start": "870720",
    "end": "877440"
  },
  {
    "text": "a more complicated type so there's you know we can have a data set of strings or or Longs or something uh but how does",
    "start": "877440",
    "end": "883440"
  },
  {
    "text": "this work with a more complex class so here's an example remember the class",
    "start": "883440",
    "end": "889199"
  },
  {
    "text": "that we're using is purely on The View side right it gets mapped nicely against the data but neither one depends on the",
    "start": "889199",
    "end": "895639"
  },
  {
    "text": "other so recall that we loaded this data from it was just uh some Json",
    "start": "895639",
    "end": "901440"
  },
  {
    "text": "documents relatively flat schema um it's got a little bit of nesting where the latitude and longitude are if I Define a",
    "start": "901440",
    "end": "907680"
  },
  {
    "text": "case class so here's a Scola case class called zip uh it's got the fields with",
    "start": "907680",
    "end": "913759"
  },
  {
    "text": "the same names zip city uh location which is an array of doubles pop and",
    "start": "913759",
    "end": "920279"
  },
  {
    "text": "state and notice that inside this case class I've gone ahead and defined some pretend complex business logic I've",
    "start": "920279",
    "end": "927800"
  },
  {
    "text": "specified a Latitude long longitude for Chicago and I've defined a business method called near Chicago uh which uses",
    "start": "927800",
    "end": "934839"
  },
  {
    "text": "uh some math to uh basically find things that are in kind of an ellipsoid near the center of Chicago so this isn't",
    "start": "934839",
    "end": "941240"
  },
  {
    "text": "terribly complicated but in a lot of business domains you may have methods that you want to use that are either",
    "start": "941240",
    "end": "946839"
  },
  {
    "text": "very complicated uh they've been you know QA and you know built and approved",
    "start": "946839",
    "end": "952000"
  },
  {
    "text": "for a long time you want to reuse those uh in some businesses there's regulatory restrictions you may have code that you",
    "start": "952000",
    "end": "959040"
  },
  {
    "text": "know needs to be approved by you know the FAA or the FDA or something like that and you can't just go and write a",
    "start": "959040",
    "end": "965160"
  },
  {
    "text": "similar uh operation in spark and you know put it into production so if you",
    "start": "965160",
    "end": "970240"
  },
  {
    "text": "have code that you need to use Asis or you'd like to use Asis uh that's one of the places where data sets really shine",
    "start": "970240",
    "end": "976560"
  },
  {
    "text": "because if I have a data set of zip objects I can use the methods on zip uh",
    "start": "976560",
    "end": "982360"
  },
  {
    "text": "just like any other method so let's take a look at an example uh first I'm going to evaluate this cell that defin the zip",
    "start": "982360",
    "end": "989680"
  },
  {
    "text": "case class and then I'm going to take my table called zip and I'm going to go as",
    "start": "989680",
    "end": "996279"
  },
  {
    "text": "and then the type zip and I'll pull back the first record so we can just see what that looks like and you can see that what we get",
    "start": "996279",
    "end": "1003480"
  },
  {
    "text": "back is a Scala zip class instance with the data inside so here's the ZIP code",
    "start": "1003480",
    "end": "1009199"
  },
  {
    "text": "here's the name of the city uh here's the population here is the state what is this mess in the middle um that's",
    "start": "1009199",
    "end": "1016399"
  },
  {
    "text": "actually a stringified version of the array of double Precision floats uh we",
    "start": "1016399",
    "end": "1021959"
  },
  {
    "text": "can look at that if we want in this next cell so I'm going to do the same thing I'm going to pull out the first record",
    "start": "1021959",
    "end": "1027038"
  },
  {
    "text": "and I'm just going to pull out the Lo member and here you can see it's an array of double and there are the actual",
    "start": "1027039",
    "end": "1032678"
  },
  {
    "text": "coordinates in there so uh okay so how can we use this data set object to do",
    "start": "1032679",
    "end": "1039918"
  },
  {
    "text": "some analytics well remember I said the data sets are really the same as data frames so don't think of them as being",
    "start": "1039919",
    "end": "1046558"
  },
  {
    "text": "object collections like an rdd think of them as being just another way of dealing with a table although you know",
    "start": "1046559",
    "end": "1051880"
  },
  {
    "text": "maybe it will accommodate more complex tables or documents so here I'm taking the data set I'm saying Group by state",
    "start": "1051880",
    "end": "1058559"
  },
  {
    "text": "count and then order by state so I'm just looking to see which states have",
    "start": "1058559",
    "end": "1064360"
  },
  {
    "text": "um uh which states have how many records for different ZIP codes in there and I'm",
    "start": "1064360",
    "end": "1069640"
  },
  {
    "text": "calling doow so what we can see is that um Alaska has 195 Alabama has 56 6 7 and",
    "start": "1069640",
    "end": "1079440"
  },
  {
    "text": "so on uh might be more interesting if we ordered by count we could see which states have the most zip codes or the",
    "start": "1079440",
    "end": "1084760"
  },
  {
    "text": "fewest but this is showing how we can use a data set so we're going to use this just like a data",
    "start": "1084760",
    "end": "1091520"
  },
  {
    "text": "frame now if I do want to use my complex business logic and I put complex and",
    "start": "1091520",
    "end": "1096799"
  },
  {
    "text": "quotes here uh but you may have more sophisticated code in your company you need to use here I'm going to read in",
    "start": "1096799",
    "end": "1102840"
  },
  {
    "text": "that same file and I'm going to say filter and I can filter on a column",
    "start": "1102840",
    "end": "1109360"
  },
  {
    "text": "expression just like I can with a data frame but with data set I can also filter on a function a Lambda and I can",
    "start": "1109360",
    "end": "1115080"
  },
  {
    "text": "just use the method that's defined in this ZIP object so here is my data set",
    "start": "1115080",
    "end": "1120960"
  },
  {
    "text": "filtered for zip codes near Chicago and here's kind of a just the first 20 that",
    "start": "1120960",
    "end": "1126400"
  },
  {
    "text": "come up in the list so using existing domain logic that you may have invested years uh in",
    "start": "1126400",
    "end": "1133440"
  },
  {
    "text": "building is as simple as uh plugging them right into certain apis in the data set just like this highlighted section",
    "start": "1133440",
    "end": "1139799"
  },
  {
    "text": "right here so we just looked at how we can use data sets uh together with some custom",
    "start": "1139799",
    "end": "1145919"
  },
  {
    "text": "business logic and I want to talk for a second about the query optimization um if you want to know a",
    "start": "1145919",
    "end": "1152000"
  },
  {
    "text": "lot of details about the query optimization pipeline in spark which is called Catalyst uh there are some",
    "start": "1152000",
    "end": "1157360"
  },
  {
    "text": "fantastic Talks by some of the folks who have built it including Michael armrest uh from spark Summit so and uh in our",
    "start": "1157360",
    "end": "1165760"
  },
  {
    "text": "three-day spark classes we actually go into detail about how this optimization",
    "start": "1165760",
    "end": "1171080"
  },
  {
    "text": "pipeline Works what I'm going to do today though is just give you a quick demo if you wanted to actually see with",
    "start": "1171080",
    "end": "1177200"
  },
  {
    "text": "your own hands and eyes how this thing uh that it actually does optimization um I've got a query or two",
    "start": "1177200",
    "end": "1184520"
  },
  {
    "text": "that we can take a look at so down here we're going to take the same data set",
    "start": "1184520",
    "end": "1189559"
  },
  {
    "text": "we're going to find uh population greater than 10,000 we're going to lowercase the city just as an example of",
    "start": "1189559",
    "end": "1196880"
  },
  {
    "text": "some operation you might be doing uh in your logic so this lower case is a built-in function you might be using",
    "start": "1196880",
    "end": "1203120"
  },
  {
    "text": "some other function here we're going to project that out uh and then we're going to filter for State equals Rhode Island",
    "start": "1203120",
    "end": "1209159"
  },
  {
    "text": "so recall that when we did the rdd version we said that pretty much what you write is What spark is going to",
    "start": "1209159",
    "end": "1214960"
  },
  {
    "text": "execute so the pipelined operations ran in the same order that we coded them in and that turned out to be a bad order in",
    "start": "1214960",
    "end": "1221960"
  },
  {
    "text": "one of the queries and a good order in the other and we could see that spark ran what I wrote here we want to see",
    "start": "1221960",
    "end": "1228120"
  },
  {
    "text": "whether spark will do something a little bit different so if I do a collect here we can see that the bigger cities in uh",
    "start": "1228120",
    "end": "1234559"
  },
  {
    "text": "Rhode Island we've got maybe you know 20 or 30 of these this is what the data looks like let's go and see how spark is going",
    "start": "1234559",
    "end": "1243039"
  },
  {
    "text": "to execute this and we can do that by calling explain so this is kind of like asking",
    "start": "1243039",
    "end": "1249360"
  },
  {
    "text": "for a plan on a SQL database we can say explain true and that'll get us the detailed execution",
    "start": "1249360",
    "end": "1257600"
  },
  {
    "text": "plan so I'm going to scroll down here and we'll take a look see if I can expand this so that we",
    "start": "1257600",
    "end": "1265280"
  },
  {
    "text": "can see the whole thing so here are the four stages of",
    "start": "1265280",
    "end": "1272120"
  },
  {
    "text": "plan we've got the parse logical plan that's pretty much what I typed in uh",
    "start": "1272120",
    "end": "1277320"
  },
  {
    "text": "it's just spark saying okay I understand what you keyed in here then we have analyze logical plan uh that's the parse",
    "start": "1277320",
    "end": "1283600"
  },
  {
    "text": "logical plan combined with catalog information so combined with the names of the dat sets and the types uh and the",
    "start": "1283600",
    "end": "1290960"
  },
  {
    "text": "columns then we have optimized logical uh plan so notice the difference between",
    "start": "1290960",
    "end": "1296559"
  },
  {
    "text": "the analyzed logical plan and the optimized logical plan so in the analyzed logical",
    "start": "1296559",
    "end": "1303760"
  },
  {
    "text": "plan we start out with this uh project and we Alias",
    "start": "1303760",
    "end": "1309640"
  },
  {
    "text": "it and then we're filtering on the population and then we're projecting the",
    "start": "1309640",
    "end": "1315520"
  },
  {
    "text": "city and the lowercase version of the city and then filtering on the state and",
    "start": "1315520",
    "end": "1320919"
  },
  {
    "text": "then we're uh pulling out the lowercase version of the city because that's the only field that we asked for at the end",
    "start": "1320919",
    "end": "1327000"
  },
  {
    "text": "so we have a lot of operators here and they're just in the order that I keyed them in they're not in the optimal order",
    "start": "1327000",
    "end": "1332880"
  },
  {
    "text": "down here under optimized logical plan notice that spark has applied a number",
    "start": "1332880",
    "end": "1337960"
  },
  {
    "text": "of rules to simplify this query so first thing that we're doing is we've got a",
    "start": "1337960",
    "end": "1344880"
  },
  {
    "text": "filter operation here and we've moved the filter before any of the",
    "start": "1344880",
    "end": "1350960"
  },
  {
    "text": "projections and we've also combined the two filters so take a look in here we have state Rhode Island and population",
    "start": "1350960",
    "end": "1358520"
  },
  {
    "text": "greater than 10,000 so we've moved the filters earlier in the query and we've combined them together and then after",
    "start": "1358520",
    "end": "1365880"
  },
  {
    "text": "that uh all we have to do is pull out the lowercase version of the city so you can see here that uh it's",
    "start": "1365880",
    "end": "1374360"
  },
  {
    "text": "easy to ask spark what the plan is actually going to run like and you can see the difference between",
    "start": "1374360",
    "end": "1381080"
  },
  {
    "text": "the query that you punched in and the final one that spark has optimized and so unlike with rdds I don't have to",
    "start": "1381080",
    "end": "1388120"
  },
  {
    "text": "write the perfect query every time and in fact a lot of different queries I can write in whatever order is most natural",
    "start": "1388120",
    "end": "1394600"
  },
  {
    "text": "or most readable for my team and it'll end up uh being produced in an optimal",
    "start": "1394600",
    "end": "1399799"
  },
  {
    "text": "way by the spark Catalyst",
    "start": "1399799",
    "end": "1403480"
  },
  {
    "text": "engine so so notice these Stars over here in the physical plan so these are",
    "start": "1407200",
    "end": "1412880"
  },
  {
    "text": "connected to a brand new feature in spark 2.0 called whol stage Cod gen so",
    "start": "1412880",
    "end": "1418360"
  },
  {
    "text": "you'll be hearing a lot about whol stage code gen over the next few weeks and the idea is to take all of the pipelined",
    "start": "1418360",
    "end": "1425559"
  },
  {
    "text": "operations between shuffles and that's what makes up a stage and Spark and actually instead of uh composing those",
    "start": "1425559",
    "end": "1432760"
  },
  {
    "text": "operators and running data through using an iterator it's like a cursor type approach uh instead to to generate a",
    "start": "1432760",
    "end": "1439200"
  },
  {
    "text": "block of code that represents that computation as if that was the entire thing that you were trying to do and",
    "start": "1439200",
    "end": "1445880"
  },
  {
    "text": "then compile that code and execute that code against the source data so to take",
    "start": "1445880",
    "end": "1451159"
  },
  {
    "text": "a look at whole stage code gen uh first thing I'm going to do is take our query",
    "start": "1451159",
    "end": "1457039"
  },
  {
    "text": "where we converted the data set to zip codes uh and filtered for New York Chicago and I'm going to ask spark to do",
    "start": "1457039",
    "end": "1463400"
  },
  {
    "text": "an explain on that and if we scroll down and take a look at this plan",
    "start": "1463400",
    "end": "1470360"
  },
  {
    "text": "so down here near the bottom we can see that whole stage code gen is turned on",
    "start": "1470480",
    "end": "1476000"
  },
  {
    "text": "and the function that we wrote Our near Chicago function that shows up right",
    "start": "1476000",
    "end": "1481159"
  },
  {
    "text": "here where it says filter function 1. apply so our custom code has actually",
    "start": "1481159",
    "end": "1486520"
  },
  {
    "text": "been integrated right into the pipeline of code that's going to get whole stage",
    "start": "1486520",
    "end": "1491679"
  },
  {
    "text": "generated and then compiled uh and this helps a lot because it means that spark isn't doing one set of work from",
    "start": "1491679",
    "end": "1497840"
  },
  {
    "text": "compiled code then separating out our work to process near Chicago and then",
    "start": "1497840",
    "end": "1503840"
  },
  {
    "text": "going back to some other optimized code it's all going to be inline and if you want to take a peek",
    "start": "1503840",
    "end": "1510520"
  },
  {
    "text": "behind the curtain and really see what this code looks like you can do that by",
    "start": "1510520",
    "end": "1515880"
  },
  {
    "text": "importing this particular uh code right here so if we bring in orap Pache spark SQL execution.",
    "start": "1515880",
    "end": "1526159"
  },
  {
    "text": "debug and then we take the exact ex same query and at the end where we had",
    "start": "1526159",
    "end": "1531200"
  },
  {
    "text": "explained we say debug code gen if we run this cell",
    "start": "1531200",
    "end": "1536840"
  },
  {
    "text": "here this shows the actual code that's been generated for this query so it's",
    "start": "1536840",
    "end": "1543679"
  },
  {
    "text": "Java code and it's machine generated so I don't expect this to make a lot of sense the first time you see it you know",
    "start": "1543679",
    "end": "1550000"
  },
  {
    "text": "like all machine generated code it has two interesting properties one is uh",
    "start": "1550000",
    "end": "1555039"
  },
  {
    "text": "it's very consistent and two is uh it can be pretty hard to read for a human the first time you look at it uh but if",
    "start": "1555039",
    "end": "1563039"
  },
  {
    "text": "you are interested and you spend a little bit of time with this code you can actually see how our function",
    "start": "1563039",
    "end": "1569559"
  },
  {
    "text": "application is integrated directly into the rest of the code that represents this",
    "start": "1569559",
    "end": "1576158"
  },
  {
    "text": "query uh if we come all the way down to uh line",
    "start": "1576640",
    "end": "1582360"
  },
  {
    "text": "83 uh we can see that that's where we're getting our filter so filter value one.",
    "start": "1582360",
    "end": "1587919"
  },
  {
    "text": "a plus and if we scroll back up we can see what that",
    "start": "1587919",
    "end": "1594480"
  },
  {
    "text": "is here we can see on line 55 that filter value one is a Scala",
    "start": "1594480",
    "end": "1600919"
  },
  {
    "text": "function and it's a cast applied to this filter object that we fed in so our code",
    "start": "1600919",
    "end": "1608000"
  },
  {
    "text": "that was originally our business domain code about uh interesting customers or conditions near Chicago uh is actually",
    "start": "1608000",
    "end": "1615159"
  },
  {
    "text": "getting compiled uh directly into the code code that spark is executing for",
    "start": "1615159",
    "end": "1620559"
  },
  {
    "text": "this stage so that's uh all that we're going to talk about as far as whol stage code",
    "start": "1620559",
    "end": "1627279"
  },
  {
    "text": "gen uh there'll be a lot of other materials online from spark Summit that talk about whol stage code gen in terms",
    "start": "1627279",
    "end": "1633240"
  },
  {
    "text": "of performance and other details but I want to give you a little flavor of how you can see that that is actually",
    "start": "1633240",
    "end": "1638320"
  },
  {
    "text": "happening and how you can even take a look at the code that spark is emitting couple more things that we want to uh",
    "start": "1638320",
    "end": "1644200"
  },
  {
    "text": "make sure that we talk about why you would want to be using data frame and data set uh so you get Hive integration",
    "start": "1644200",
    "end": "1649799"
  },
  {
    "text": "for free a lot of companies have a big investment in Hive and related tools and",
    "start": "1649799",
    "end": "1655240"
  },
  {
    "text": "Spark will automatically work with your um Hive meta store so your hive tables",
    "start": "1655240",
    "end": "1661240"
  },
  {
    "text": "are available in spark and you can take data in spark and make them available as Hive tables really easily uh so just to",
    "start": "1661240",
    "end": "1667880"
  },
  {
    "text": "give a quick example we can take our ZIP table right here and say wr. saave as",
    "start": "1667880",
    "end": "1673200"
  },
  {
    "text": "table hior zip and that",
    "start": "1673200",
    "end": "1679279"
  },
  {
    "text": "succeeds and then the question is well where did it go and how do I know that it's available to Hive well if I look",
    "start": "1679279",
    "end": "1685799"
  },
  {
    "text": "under the tables tab here in data bricks I can see the hive zip table uh that's",
    "start": "1685799",
    "end": "1691480"
  },
  {
    "text": "showing me the hive metast stor but what if you're not on data bricks or you don't believe me uh let's take a look a",
    "start": "1691480",
    "end": "1697360"
  },
  {
    "text": "little bit further down here's the access to the zip table using the spark session that we mentioned earlier so in",
    "start": "1697360",
    "end": "1704240"
  },
  {
    "text": "spark 2.0 we can say spark. table so given that spark session we can ask for",
    "start": "1704240",
    "end": "1710120"
  },
  {
    "text": "the catalog and the databases that spark knows about and we can see where the uh",
    "start": "1710120",
    "end": "1715519"
  },
  {
    "text": "Hive data actually lives so I'm going to do a spark. catalog. list",
    "start": "1715519",
    "end": "1720960"
  },
  {
    "text": "databases and you can see we get back a data set that says okay here's the default Hive database and here is the",
    "start": "1720960",
    "end": "1727720"
  },
  {
    "text": "hive Warehouse uh so the default warehouse location in data bricks uh in this version of datab bricks is under",
    "start": "1727720",
    "end": "1734559"
  },
  {
    "text": "user hiive Warehouse uh in your installation of spark you just configure this to match wherever your hive",
    "start": "1734559",
    "end": "1740440"
  },
  {
    "text": "installation is and if we want to see that our data actually lives under there we can do an",
    "start": "1740440",
    "end": "1746120"
  },
  {
    "text": "LS and look under user hiive Warehouse hior zip which is the name I gave to",
    "start": "1746120",
    "end": "1751440"
  },
  {
    "text": "that table that we just uh wrote out and here you can see there's parquet data",
    "start": "1751440",
    "end": "1757240"
  },
  {
    "text": "underneath that uh folder so the data actually has been moved into the hive",
    "start": "1757240",
    "end": "1763240"
  },
  {
    "text": "Warehouse so that almost concludes our tour of data frame and data set so kind",
    "start": "1763240",
    "end": "1769240"
  },
  {
    "text": "of wrapping up and looking to the Future all of the new development or nearly all the new development in terms of uh new",
    "start": "1769240",
    "end": "1776480"
  },
  {
    "text": "apis and optimizations on spark are focused on data frame and data set so",
    "start": "1776480",
    "end": "1781799"
  },
  {
    "text": "you get the benefit of all of that research and implementation when you're using these apis um if you're using Mach",
    "start": "1781799",
    "end": "1788360"
  },
  {
    "text": "if you're doing machine learning there's spark ml or the ml pipelines API uh that you've probably heard quite a lot about",
    "start": "1788360",
    "end": "1794080"
  },
  {
    "text": "over the past year uh here's a link to the docs more recent recently graph frames so there used to be an API called",
    "start": "1794080",
    "end": "1801039"
  },
  {
    "text": "graphx there still is that's the uh traditional API for graph processing over rdds uh well if we want the",
    "start": "1801039",
    "end": "1807760"
  },
  {
    "text": "performance and storage advantages of data frames and data set what we're going to do is use graph frames which is",
    "start": "1807760",
    "end": "1814320"
  },
  {
    "text": "an implementation of graph algorithms over data frames uh here is a link to a really cool demo uh that features uh",
    "start": "1814320",
    "end": "1822399"
  },
  {
    "text": "analysis of passing among the Golden State Warriors uh where they use uh",
    "start": "1822399",
    "end": "1827480"
  },
  {
    "text": "something like Google page ranks algorithm to uh look at who are the key players on the Warriors you can probably",
    "start": "1827480",
    "end": "1833960"
  },
  {
    "text": "guess who's getting the ball the most and the last thing I want to show",
    "start": "1833960",
    "end": "1839159"
  },
  {
    "text": "so we've said we've we're going to use data frames as the underlying infrastructure for moving around our data we're going to do sparkml to do",
    "start": "1839159",
    "end": "1846120"
  },
  {
    "text": "machine langu uh machine learning on top of that we're going to use graph frames to do graph analysis on top of data",
    "start": "1846120",
    "end": "1853480"
  },
  {
    "text": "frames and the newest thing is structured streaming uh so you may have seen some talks about building streaming",
    "start": "1853480",
    "end": "1860120"
  },
  {
    "text": "directly on top of data frames and the early iteration of that is out in spark 2.0 and I put together a really quick",
    "start": "1860120",
    "end": "1867279"
  },
  {
    "text": "demo of just kind of a Hello World of structured streaming so you can actually see what this looks like um so instead",
    "start": "1867279",
    "end": "1873360"
  },
  {
    "text": "of using the uh traditional streaming API that's focused around rdds and dams",
    "start": "1873360",
    "end": "1879240"
  },
  {
    "text": "what we're going to do is we're going to create something called a continuous query uh that reads data from some",
    "start": "1879240",
    "end": "1885639"
  },
  {
    "text": "streaming source does some processing on on it and that data is going to flow out somewhere else and we can do whatever",
    "start": "1885639",
    "end": "1892039"
  },
  {
    "text": "queries we'd like on the live stream so let's just take a look at a a fun demo",
    "start": "1892039",
    "end": "1897960"
  },
  {
    "text": "that we can use to start with uh what I'm going to do here is prepare some folders I'm going to use file sources",
    "start": "1897960",
    "end": "1903799"
  },
  {
    "text": "and syns for this demonstration so I've got a folder stream in where I'm going to put some data into my stream these might be",
    "start": "1903799",
    "end": "1910120"
  },
  {
    "text": "transactions or something like that coming in then I've got stream out this is where I'm going to write my data too",
    "start": "1910120",
    "end": "1916200"
  },
  {
    "text": "and in order to support uh fault recovery we're going to set up checkpointing under a folder called",
    "start": "1916200",
    "end": "1921799"
  },
  {
    "text": "stream CK I'm also going to define a schema so that spark knows the shape of the data",
    "start": "1921799",
    "end": "1928600"
  },
  {
    "text": "before the records actually start coming in and each one's going to have a last name which is a string an email which is",
    "start": "1928600",
    "end": "1933960"
  },
  {
    "text": "a string and a number of hits like in a web analytic scenario for example number of hits which is an integer so I'll go",
    "start": "1933960",
    "end": "1940360"
  },
  {
    "text": "ahead and run this okay so now we get down to the fun",
    "start": "1940360",
    "end": "1946880"
  },
  {
    "text": "part uh let's start our stream up first we're going to set up the source spark.",
    "start": "1946880",
    "end": "1952639"
  },
  {
    "text": "read. format json. schema and we pass the schema we created and we're going to say do stream and I'm going to point at",
    "start": "1952639",
    "end": "1959760"
  },
  {
    "text": "this input Stream So this particular path is dbfs that's data that actually",
    "start": "1959760",
    "end": "1964880"
  },
  {
    "text": "lives on S3 that's attached here through data bricks so I'm going to run",
    "start": "1964880",
    "end": "1970360"
  },
  {
    "text": "that and notice that I get back a data frame object and now I'm going to define the",
    "start": "1970360",
    "end": "1977960"
  },
  {
    "text": "processing we're going to do on this stream and where we're going to write it to so the modified version of this data",
    "start": "1977960",
    "end": "1984440"
  },
  {
    "text": "uh is defined right here I'm going to take the data that comes through I'm going to pull out the last name I'm",
    "start": "1984440",
    "end": "1990080"
  },
  {
    "text": "going to lowercase the email it's kind of a normalization maybe uh and assign that to have the name email I'm going to",
    "start": "1990080",
    "end": "1997720"
  },
  {
    "text": "get the current timestamp so the time that we're processing the event um I'm going to call that now and I'm going to",
    "start": "1997720",
    "end": "2004320"
  },
  {
    "text": "grab the hits the number of hits that came in then I'm going to take is modified data and I'm going to say right",
    "start": "2004320",
    "end": "2010760"
  },
  {
    "text": "and I pass the checkpoint location and then I say start stream and I point to",
    "start": "2010760",
    "end": "2016039"
  },
  {
    "text": "the output folder so let's go ahead and run that and you can see right here in",
    "start": "2016039",
    "end": "2021200"
  },
  {
    "text": "data bricks we can visually see that something is running it says stream query zero status active if I open this",
    "start": "2021200",
    "end": "2028880"
  },
  {
    "text": "disclosure triangle here it says awaiting data so we'll see if we can feed some data in here and we'll see",
    "start": "2028880",
    "end": "2034120"
  },
  {
    "text": "what happens so in this next cell I'm using echo in a shell command to actually take",
    "start": "2034120",
    "end": "2040559"
  },
  {
    "text": "two Json records and drop them into a file in the uh temp directory so we'll",
    "start": "2040559",
    "end": "2047760"
  },
  {
    "text": "do that that's the temp directory not the actual streaming directory and then I'm going to move this file into the",
    "start": "2047760",
    "end": "2054520"
  },
  {
    "text": "streaming input directory think about this as being like when you do log rotation right you you finished creating",
    "start": "2054520",
    "end": "2060760"
  },
  {
    "text": "a file that contains some records or events and then you rotate over to a new file name or maybe move the the existing",
    "start": "2060760",
    "end": "2067839"
  },
  {
    "text": "log file into another location so I'm going to toss this file right into the",
    "start": "2067839",
    "end": "2073720"
  },
  {
    "text": "stream and we should see some data showing up in fact you can see right",
    "start": "2077520",
    "end": "2082679"
  },
  {
    "text": "here it just changed the UI under stream says sources one it shows some details",
    "start": "2082679",
    "end": "2089800"
  },
  {
    "text": "about the source and the last offset and it shows the sync and we can actually",
    "start": "2089800",
    "end": "2095079"
  },
  {
    "text": "query the output which is a part K file here so here are the records that came through so so far we just have one set",
    "start": "2095079",
    "end": "2102400"
  },
  {
    "text": "of Records so what happens if we wanted to do some analysis on here or send in",
    "start": "2102400",
    "end": "2107839"
  },
  {
    "text": "some more data so um if I wanted to say uh roll up the total number of hits by",
    "start": "2107839",
    "end": "2114599"
  },
  {
    "text": "email so far we just have three for Jones and five for Smith so what happens",
    "start": "2114599",
    "end": "2120119"
  },
  {
    "text": "when this next event happens where Smith sends in another 12 hits so we'll do the",
    "start": "2120119",
    "end": "2125160"
  },
  {
    "text": "same thing we Echo to a file we're going to rotate that file here into dbfs stream",
    "start": "2125160",
    "end": "2134359"
  },
  {
    "text": "in and now we're going to run the query the same one we did",
    "start": "2135599",
    "end": "2141240"
  },
  {
    "text": "before and you can see that Smith has 17 hits right so we've gotten another record we did the same query we did",
    "start": "2141280",
    "end": "2147839"
  },
  {
    "text": "before and now we're seeing live updating data if we scroll back up we can see that the offset has moved in the",
    "start": "2147839",
    "end": "2156440"
  },
  {
    "text": "Stream So now it says last offset number",
    "start": "2156440",
    "end": "2160480"
  },
  {
    "text": "one and if I want to shut the stream down I'm going to come down here and go stream. stop so this is just a very",
    "start": "2166560",
    "end": "2173800"
  },
  {
    "text": "simple hello world of structured streaming but this is the new streaming API that you can use uh directly with",
    "start": "2173800",
    "end": "2181119"
  },
  {
    "text": "data frames uh it's shipping in spark 2.0 right now probably the easiest way to try out spark 2.0 uh is through datab",
    "start": "2181119",
    "end": "2188319"
  },
  {
    "text": "bricks Community Edition so you can sign up for a free account and run this exact code that we've been playing with uh",
    "start": "2188319",
    "end": "2193880"
  },
  {
    "text": "we'll go publish this online so you can imp import the same notebook and try it uh or you can download the uh open",
    "start": "2193880",
    "end": "2200000"
  },
  {
    "text": "source spark project uh code and build it yourself uh from spark. apache.org",
    "start": "2200000",
    "end": "2207200"
  },
  {
    "text": "hopefully this helps clarify why data frame and data set are the future of programming with Apache spark you get a",
    "start": "2207200",
    "end": "2214400"
  },
  {
    "text": "little look under the covers here if you want to learn more there's great material online from spark Summit and if",
    "start": "2214400",
    "end": "2220560"
  },
  {
    "text": "you want to learn to do spark programming yourself New Circle has public classes and for teams of five or",
    "start": "2220560",
    "end": "2226720"
  },
  {
    "text": "more we can come on site and do an in-depth spark programming class thanks for watching",
    "start": "2226720",
    "end": "2234839"
  }
]