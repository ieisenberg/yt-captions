[
  {
    "text": "good afternoon has what are you doing awake cool",
    "start": "4010",
    "end": "12330"
  },
  {
    "text": "my name is Kavya and I'm here today to talk to you about analyzing real world",
    "start": "12330",
    "end": "19200"
  },
  {
    "text": "systems using performance modeling which is theory now why in the world might we",
    "start": "19200",
    "end": "24869"
  },
  {
    "text": "want to do this well ideally we want to give our users a good user experience so we care about things like response time",
    "start": "24869",
    "end": "31650"
  },
  {
    "text": "and ideally wants do so without breaking the bank so we care about things like server",
    "start": "31650",
    "end": "37050"
  },
  {
    "text": "utilization capacity planning and so as systems designers and builders as",
    "start": "37050",
    "end": "42390"
  },
  {
    "text": "performance engineers we spend many a night trying to answer questions like these how much load can my server handle",
    "start": "42390",
    "end": "51540"
  },
  {
    "text": "do we need more servers are we already over provisioned does this sound familiar how many people",
    "start": "51540",
    "end": "59039"
  },
  {
    "text": "here are intimately familiar we're trying to answer questions like these alright just about everybody right so",
    "start": "59039",
    "end": "67259"
  },
  {
    "text": "you know there are a few standard approaches to go about trying to answer these questions the first is what I like",
    "start": "67259",
    "end": "73350"
  },
  {
    "text": "to call the hashtag Yolo method where you try something and hope it works",
    "start": "73350",
    "end": "79050"
  },
  {
    "text": "we've all been here the second method is load simulation where you gradually",
    "start": "79050",
    "end": "84869"
  },
  {
    "text": "apply a load to your production system with the goal of stressing the system to",
    "start": "84869",
    "end": "90060"
  },
  {
    "text": "try and determine the actual performance characteristics and bottlenecks of your",
    "start": "90060",
    "end": "95160"
  },
  {
    "text": "system now I'm a big believer of load simulations so we'll come back to this briefly throughout the talk but the",
    "start": "95160",
    "end": "102360"
  },
  {
    "text": "focus of today's talk is going to be the third approach namely performance modeling now performance modeling speaks",
    "start": "102360",
    "end": "109259"
  },
  {
    "text": "to this idea of taking your real world system representing it by a theoretical",
    "start": "109259",
    "end": "116789"
  },
  {
    "text": "model analyzing the model to get results and translating those results back to",
    "start": "116789",
    "end": "122700"
  },
  {
    "text": "your actual system now the crucial thing to remember about performance modeling is that modeling by definition makes",
    "start": "122700",
    "end": "129720"
  },
  {
    "text": "assumptions right in the case of queueing theory and queueing theory theory models which is what we'll be",
    "start": "129720",
    "end": "135329"
  },
  {
    "text": "looking at today it makes assumptions about the arrival rate of your requests it makes",
    "start": "135329",
    "end": "141040"
  },
  {
    "text": "assumptions about what order requests are processed in the service time distribution of your requests and so in",
    "start": "141040",
    "end": "150610"
  },
  {
    "text": "order to use these models in order to use the results of these models your system needs to satisfy the underlying",
    "start": "150610",
    "end": "157540"
  },
  {
    "text": "assumptions if they don't you might still be able to leverage the model but you can't apply it blindly alright so",
    "start": "157540",
    "end": "164410"
  },
  {
    "text": "we'll be looking at performance modeling today to answer questions about our system's performance and its capacity",
    "start": "164410",
    "end": "170380"
  },
  {
    "text": "needs specifically this is how the talk will be structured I will first talk about analyzing a",
    "start": "170380",
    "end": "176200"
  },
  {
    "text": "single server system will then talk about analyzing a cluster of servers and then finally we'll take a step back and",
    "start": "176200",
    "end": "182980"
  },
  {
    "text": "see what we can take away alright then let's get started",
    "start": "182980",
    "end": "189310"
  },
  {
    "text": "so a single server system we are going to assume so say for example our model",
    "start": "189310",
    "end": "196600"
  },
  {
    "text": "is a simple web server it receives requests from a client it processes them",
    "start": "196600",
    "end": "201910"
  },
  {
    "text": "pretty standard now the questions we typically have about such a system are",
    "start": "201910",
    "end": "207480"
  },
  {
    "text": "what's the maximum throughput the server can support with that falling over that",
    "start": "207480",
    "end": "213160"
  },
  {
    "text": "is given we have an SLA a response time relate and C threshold that we don't want to cross what's the maximum",
    "start": "213160",
    "end": "219190"
  },
  {
    "text": "requests per second we can serve so in this graph for example at 4,000 4,500",
    "start": "219190",
    "end": "224980"
  },
  {
    "text": "requests per second we're still nice our response time is still nice and low below the threshold our question is what",
    "start": "224980",
    "end": "231280"
  },
  {
    "text": "about can we serve 5,000 requests per second what about 7,000 what about 10,000 and the second question we might",
    "start": "231280",
    "end": "238870"
  },
  {
    "text": "care about in such a system is well how can we improve our mean response time to give our users a better experience so",
    "start": "238870",
    "end": "246940"
  },
  {
    "text": "we're going to use performance modeling specifically we're going to model this server as a queueing system the way this",
    "start": "246940",
    "end": "254829"
  },
  {
    "text": "works is requests come in at some arrival rate if the server is busy the",
    "start": "254829",
    "end": "261370"
  },
  {
    "text": "request is immediately processed if the server is the server is free if the server is busy processing other requests",
    "start": "261370",
    "end": "267970"
  },
  {
    "text": "the requests queue now the time they spend in the queue is called the queuing delay or the waiting",
    "start": "267970",
    "end": "273830"
  },
  {
    "text": "time we'll be using the term queuing delay today finally when the server is free it picks off a request from the",
    "start": "273830",
    "end": "280070"
  },
  {
    "text": "queue and processes it the time the server takes to do this is the service time and finally a response is sent now",
    "start": "280070",
    "end": "289280"
  },
  {
    "text": "we're going to make some simplifying assumptions about our system the first is that requests are independent and",
    "start": "289280",
    "end": "296810"
  },
  {
    "text": "random that is to say the requests they're not correlated with each other",
    "start": "296810",
    "end": "301970"
  },
  {
    "text": "they're not correlated with the response time of the system no they're independent and random it's a random variable the second assumption is",
    "start": "301970",
    "end": "309530"
  },
  {
    "text": "that requests are processed one at a time and in first-in first-out order so you can imagine this is a single core",
    "start": "309530",
    "end": "315260"
  },
  {
    "text": "single threaded server and finally we're going to assume that the service time",
    "start": "315260",
    "end": "320660"
  },
  {
    "text": "the time it actually takes to process the each request is constant what this",
    "start": "320660",
    "end": "326240"
  },
  {
    "text": "means is we're assuming that requests are the same size they take the same",
    "start": "326240",
    "end": "331760"
  },
  {
    "text": "time to be processed and we're also assuming there's no downstream saturation the database the network nothing gets saturated this allows us to",
    "start": "331760",
    "end": "338720"
  },
  {
    "text": "treat the service time as a constant all right so our first question what's the",
    "start": "338720",
    "end": "344900"
  },
  {
    "text": "maximum throughput of the server well using our QE theory model let's just",
    "start": "344900",
    "end": "350480"
  },
  {
    "text": "reason about it intuitively say we crank up the arrival rate at our server what",
    "start": "350480",
    "end": "356540"
  },
  {
    "text": "happens well our servers utilization its busyness increases now there's nothing",
    "start": "356540",
    "end": "362540"
  },
  {
    "text": "revolutionary about this it's like your boss shows up and gives you more tasks",
    "start": "362540",
    "end": "368390"
  },
  {
    "text": "to do you're going to spend a larger fraction of your time being busy same",
    "start": "368390",
    "end": "373430"
  },
  {
    "text": "idea but our question is what we care about is how does the utilization",
    "start": "373430",
    "end": "378560"
  },
  {
    "text": "increase with arrival rate and queuing Theory has a formula called the utilization law which tells us that the",
    "start": "378560",
    "end": "385940"
  },
  {
    "text": "utilization is equal to the arrival rate of requests time the services times the service time if you think about it for a",
    "start": "385940",
    "end": "392690"
  },
  {
    "text": "second it makes sense and since we're assuming service time is constant what this means is that the",
    "start": "392690",
    "end": "399860"
  },
  {
    "text": "utilization arrival rate graph is Alayne it's a line all right so now our servers",
    "start": "399860",
    "end": "407659"
  },
  {
    "text": "gotten busier what does this mean well it means that the probability that an",
    "start": "407659",
    "end": "413960"
  },
  {
    "text": "incoming request comes in and finds the server busy and therefore has to queue",
    "start": "413960",
    "end": "419270"
  },
  {
    "text": "that probability goes up so the probability of requests needing to Hue goes up which means that the mean queue",
    "start": "419270",
    "end": "426710"
  },
  {
    "text": "length the average queue length increases which in turn means that the",
    "start": "426710",
    "end": "431840"
  },
  {
    "text": "average queuing delay increases does that make sense cool so get our question is well how",
    "start": "431840",
    "end": "439039"
  },
  {
    "text": "does how does it increase what is the relation between utilisation and chemo delay and for this queueing theory gives",
    "start": "439039",
    "end": "445280"
  },
  {
    "text": "us another formula called the PK formula now the PK formula has a full name that",
    "start": "445280",
    "end": "452870"
  },
  {
    "text": "is somewhat hard to pronounce I think it's the Zack kin shine formula but we're gonna call the PK formula and",
    "start": "452870",
    "end": "460400"
  },
  {
    "text": "the PK formula tells us that the mean queuing delay is has a dependency that",
    "start": "460400",
    "end": "466669"
  },
  {
    "text": "dependency on the utilization of the server and there are two other terms that we'll come back to later but those",
    "start": "466669",
    "end": "474319"
  },
  {
    "text": "two terms depend on the distribution of service times now the reason we're going",
    "start": "474319",
    "end": "479810"
  },
  {
    "text": "to come back to this later is because we assumed that service time is constant so we don't need to worry about those two",
    "start": "479810",
    "end": "485479"
  },
  {
    "text": "terms right now so we can say that the mean queuing delay is directly",
    "start": "485479",
    "end": "491210"
  },
  {
    "text": "proportional to that term the server's of the service utilization u over 1",
    "start": "491210",
    "end": "497629"
  },
  {
    "text": "minus u what does that look like in practice does anybody know what that",
    "start": "497629",
    "end": "503029"
  },
  {
    "text": "function looks like it looks like hockey stick what this graph tells us is as",
    "start": "503029",
    "end": "510169"
  },
  {
    "text": "utilization increases the queuing delay is going to increase in that manner and",
    "start": "510169",
    "end": "517099"
  },
  {
    "text": "that makes sense right when u is large the denominator approaches zero which means the entire fraction approaches",
    "start": "517099",
    "end": "523969"
  },
  {
    "text": "infinity now because the response time is the queuing delay plus the service",
    "start": "523969",
    "end": "530959"
  },
  {
    "text": "time service time is constant in our model response time is directly proportional to queuing delay",
    "start": "530959",
    "end": "536740"
  },
  {
    "text": "so what this means is that the response time utilization graph is the infamous",
    "start": "536740",
    "end": "542180"
  },
  {
    "text": "hockey stick that we're all probably familiar with all right so back to our",
    "start": "542180",
    "end": "548150"
  },
  {
    "text": "question what's the maximum throughput of the server well what happens when we",
    "start": "548150",
    "end": "553880"
  },
  {
    "text": "increase the arrival rate we know that initially in the low utilization regime",
    "start": "553880",
    "end": "559930"
  },
  {
    "text": "response time is going to remain approximately the same but as soon as",
    "start": "559930",
    "end": "565520"
  },
  {
    "text": "we're in the high utilization regime the response time is going to starts to hockey-stick so when we're in the high",
    "start": "565520",
    "end": "575300"
  },
  {
    "text": "utilization regime increasing the arrival rate it's not going to improve our systems throughput at that point the",
    "start": "575300",
    "end": "581810"
  },
  {
    "text": "system your server is busy it's as busy as it can be it can't do any more work so at that point increasing arrival rate",
    "start": "581810",
    "end": "589280"
  },
  {
    "text": "is simply going to increase the response time of your requests so what's the",
    "start": "589280",
    "end": "594560"
  },
  {
    "text": "maximum throughput anywhere well the maximum throughput is right around 5500",
    "start": "594560",
    "end": "600350"
  },
  {
    "text": "requests per second so that's that's around when the HUD or our response time",
    "start": "600350",
    "end": "607130"
  },
  {
    "text": "threshold is crossed right because response time hockey sticks it doesn't stay constant cool so this answer is our",
    "start": "607130",
    "end": "617870"
  },
  {
    "text": "first question moving on to our second question how can we improve the mean",
    "start": "617870",
    "end": "623750"
  },
  {
    "text": "response time well we just saw that response time depended on queuing delay",
    "start": "623750",
    "end": "631490"
  },
  {
    "text": "right and we just saw that response times start to get bad when queuing delay increase so here's an idea",
    "start": "631490",
    "end": "639530"
  },
  {
    "text": "what if we just prevent the queue from getting too long that should take care of it right that's in fact an excellent",
    "start": "639530",
    "end": "648050"
  },
  {
    "text": "idea and a pretty common way to address this problem a few common strategies",
    "start": "648050",
    "end": "653360"
  },
  {
    "text": "that you're probably aware of are you said a max queue length and if incoming",
    "start": "653360",
    "end": "659450"
  },
  {
    "text": "requests finds the queue full those requests are dropped another common strategy is client-side concurrency",
    "start": "659450",
    "end": "666500"
  },
  {
    "text": "control so on the client you time how long your request to the server takes and if that if the service",
    "start": "666500",
    "end": "673189"
  },
  {
    "text": "starting to take too long it means the server is overloaded your requests are queuing so you back off you",
    "start": "673189",
    "end": "679610"
  },
  {
    "text": "stop trying red this is a form of circuit breaking but there are a couple of novel or uncommon approved approaches",
    "start": "679610",
    "end": "686899"
  },
  {
    "text": "that I'd like to talk about the first is this idea of controlled delay this and",
    "start": "686899",
    "end": "695179"
  },
  {
    "text": "this is an algorithm that Facebook implements in their thrift and their RPC framework and the key insight of",
    "start": "695179",
    "end": "702230"
  },
  {
    "text": "controlled delay is the queues are normally empty you still want to have a",
    "start": "702230",
    "end": "709040"
  },
  {
    "text": "queue for reliability to absorb burstiness in your traffic but even so",
    "start": "709040",
    "end": "714889"
  },
  {
    "text": "you accused normally empty so if you queue has not been emptied for a long",
    "start": "714889",
    "end": "722389"
  },
  {
    "text": "time so your queue has not been emptied in the recent past it means your server",
    "start": "722389",
    "end": "727639"
  },
  {
    "text": "is overloaded and so you start timing out requests sooner than you would",
    "start": "727639",
    "end": "733399"
  },
  {
    "text": "otherwise so this is what the algorithm looks like it basically checks to see when the",
    "start": "733399",
    "end": "739309"
  },
  {
    "text": "queue was last emptied if it was greater than some thresholds time ago you in",
    "start": "739309",
    "end": "744740"
  },
  {
    "text": "queue the request but you said a short timeout right so that's Codell a second",
    "start": "744740",
    "end": "752740"
  },
  {
    "text": "uncommon and interesting approach is you process requests in life phone order and",
    "start": "752740",
    "end": "759199"
  },
  {
    "text": "last in first out order now Facebook",
    "start": "759199",
    "end": "765009"
  },
  {
    "text": "implements this as well in their PHP runtime and they use adaptive LIFO so",
    "start": "765009",
    "end": "770509"
  },
  {
    "text": "they start off their queue start off as first in first out queues and they switch to last in first add when they",
    "start": "770509",
    "end": "776869"
  },
  {
    "text": "notice that the server is getting overloaded but Dropbox recently released an article talking about their reverse",
    "start": "776869",
    "end": "784549"
  },
  {
    "text": "proxy band-aid and band-aid always processes requests in last in first order now why do they do this well the",
    "start": "784549",
    "end": "793929"
  },
  {
    "text": "the insight here is that when the server is not overloaded it doesn't really",
    "start": "793929",
    "end": "800360"
  },
  {
    "text": "matter what order you process requests in however when the server is overloaded it makes sense to",
    "start": "800360",
    "end": "807260"
  },
  {
    "text": "process newest requests first because you're old you first your oldest requests have been sitting in the queue",
    "start": "807260",
    "end": "814010"
  },
  {
    "text": "for the longest amount of time so they're likely to time out so there's no point in wasting resources on them okay",
    "start": "814010",
    "end": "822769"
  },
  {
    "text": "so this talks is so so far we've seen methods to control the queue length these are somewhat reactive methods",
    "start": "822769",
    "end": "829399"
  },
  {
    "text": "right they intercept the queue length can we be more proactive about it well",
    "start": "829399",
    "end": "835029"
  },
  {
    "text": "let's go back to the PK formula because that tells us a few things we can do",
    "start": "835029",
    "end": "840320"
  },
  {
    "text": "organically to prevent the problem so Rumble PK formula now we're looking at",
    "start": "840320",
    "end": "845720"
  },
  {
    "text": "those last two terms of the PK formula it tells us that queuing delay has a",
    "start": "845720",
    "end": "852500"
  },
  {
    "text": "linear dependency on the average service time what does that mean well we can",
    "start": "852500",
    "end": "858950"
  },
  {
    "text": "spend some time decreasing service time we can optimize our application code our",
    "start": "858950",
    "end": "864709"
  },
  {
    "text": "request handling code and that you see that that's the graph on the green graph",
    "start": "864709",
    "end": "869870"
  },
  {
    "text": "you see that at hockey sticks much later than the original graph then the which",
    "start": "869870",
    "end": "875029"
  },
  {
    "text": "is the blue line the PK formula also tells us that queuing delay has a",
    "start": "875029",
    "end": "880430"
  },
  {
    "text": "quadratic dependency on the service time variability right which means that if",
    "start": "880430",
    "end": "887029"
  },
  {
    "text": "your requests the time taken to process each request if there's a lot of variance than that that's bad because",
    "start": "887029",
    "end": "893420"
  },
  {
    "text": "that causes more queuing and so we can try and decrees decrees the variance for",
    "start": "893420",
    "end": "903680"
  },
  {
    "text": "example by batching requests in order to make sure that they all take approximately the same amount of time",
    "start": "903680",
    "end": "909350"
  },
  {
    "text": "and this is what the graph looks like the blue line here corresponds to the blue distribution where you have many",
    "start": "909350",
    "end": "915649"
  },
  {
    "text": "service times and the green graph here which hockey sticks much later shows us",
    "start": "915649",
    "end": "922190"
  },
  {
    "text": "that if your tasks all have the same size then then your queuing delay is",
    "start": "922190",
    "end": "929810"
  },
  {
    "text": "less cool all right so at this point",
    "start": "929810",
    "end": "934860"
  },
  {
    "text": "answered both are both our questions but throughput and improving response time for this single server question so",
    "start": "934860",
    "end": "943410"
  },
  {
    "text": "here's a question do all single server systems look like a web server nope",
    "start": "943410",
    "end": "949920"
  },
  {
    "text": "here's another single server system in",
    "start": "949920",
    "end": "955380"
  },
  {
    "text": "this case imagine that you are an IMT",
    "start": "955380",
    "end": "960450"
  },
  {
    "text": "company so you have a number of sensors out in the field these sensors say their temperature",
    "start": "960450",
    "end": "966570"
  },
  {
    "text": "sensors so they measure temperature and it like period they periodically upload data to your back-end server which is",
    "start": "966570",
    "end": "973320"
  },
  {
    "text": "sitting in the cloud and your back-end server processes data from these sensors",
    "start": "973320",
    "end": "979220"
  },
  {
    "text": "now the actual code on the sensors might look like this the sensor synchronously",
    "start": "979220",
    "end": "985710"
  },
  {
    "text": "so it sends a blocking request to the backend with some data it says upload this batch of data it waits for the",
    "start": "985710",
    "end": "992160"
  },
  {
    "text": "server to ACK it and then it goes to sleep this is - this is to preserve",
    "start": "992160",
    "end": "997500"
  },
  {
    "text": "power on the on the devices and then say five seconds later or sumsy seconds",
    "start": "997500",
    "end": "1003830"
  },
  {
    "text": "later it wakes up and it sends the new data to the server so the question is",
    "start": "1003830",
    "end": "1011810"
  },
  {
    "text": "can we can we use the model we used to analyze the web server can we use that",
    "start": "1011810",
    "end": "1017120"
  },
  {
    "text": "model to analyze this system nah-ah this is a very different system in two",
    "start": "1017120",
    "end": "1025339"
  },
  {
    "text": "important ways the first is that requests are synchronized each clients",
    "start": "1025339",
    "end": "1032600"
  },
  {
    "text": "request is synchronized what do we mean by this remember the sensor sends data it waits",
    "start": "1032600",
    "end": "1039170"
  },
  {
    "text": "for it it waits for the response and then it sends the next request right so",
    "start": "1039170",
    "end": "1046189"
  },
  {
    "text": "there's almost a feedback loop and secondly there's we're assuming that",
    "start": "1046190",
    "end": "1052070"
  },
  {
    "text": "there's a fixed number of clients so the queueing theory model for such a system",
    "start": "1052070",
    "end": "1058460"
  },
  {
    "text": "looks like this right this model we need a model that captures those two",
    "start": "1058460",
    "end": "1064280"
  },
  {
    "text": "properties so there's that feedback loop between the response and the Westside and we have a fixed number of",
    "start": "1064280",
    "end": "1071269"
  },
  {
    "text": "clients why do these differences matter well these differences mean two",
    "start": "1071269",
    "end": "1079879"
  },
  {
    "text": "important things the first is that throughput depends on the response time",
    "start": "1079879",
    "end": "1085580"
  },
  {
    "text": "right there's a negative feedback loop in this built into the system because if",
    "start": "1085580",
    "end": "1091700"
  },
  {
    "text": "you're if your clients send some number of requests and those requests your response time is taking longer and",
    "start": "1091700",
    "end": "1097909"
  },
  {
    "text": "longer those clients will be waiting for the responses before they send out another request right so they naturally",
    "start": "1097909",
    "end": "1104690"
  },
  {
    "text": "will not send out any more requests so you're through but decreases as your response time goes up it's an inverse",
    "start": "1104690",
    "end": "1110899"
  },
  {
    "text": "relation and the second important way in which the system is different is in this",
    "start": "1110899",
    "end": "1117320"
  },
  {
    "text": "case the queue length the maximum queue length is bounded we have a fixed number",
    "start": "1117320",
    "end": "1122960"
  },
  {
    "text": "of clients N and each client can have at most one outstanding request to the",
    "start": "1122960",
    "end": "1128119"
  },
  {
    "text": "server which means your maximum queue length can be at most n make sense cool",
    "start": "1128119",
    "end": "1135529"
  },
  {
    "text": "so this this such a system is called a closed system in theory and as you can",
    "start": "1135529",
    "end": "1143509"
  },
  {
    "text": "see it's super different than the open system that we previously saw okay so",
    "start": "1143509",
    "end": "1151489"
  },
  {
    "text": "our question again is what does response",
    "start": "1151489",
    "end": "1156919"
  },
  {
    "text": "time versus load look like for this system first things first the",
    "start": "1156919",
    "end": "1162889"
  },
  {
    "text": "assumptions we need to make the first is we're going to assume that the sleep time is constant so every every device",
    "start": "1162889",
    "end": "1170929"
  },
  {
    "text": "sleeps for a fixed Z seconds as before",
    "start": "1170929",
    "end": "1176090"
  },
  {
    "text": "we're going to assume that requests are processed one at a time and in first-in first-out order and the actual processing the",
    "start": "1176090",
    "end": "1182389"
  },
  {
    "text": "actual service time is constant as well okay so let's see in this case to",
    "start": "1182389",
    "end": "1190820"
  },
  {
    "text": "increase the load on the system we have to increase the number of clients right each client has one outstanding requests",
    "start": "1190820",
    "end": "1197090"
  },
  {
    "text": "so if you increase the number of clients you have more more requests okay so as we increase the number of",
    "start": "1197090",
    "end": "1205130"
  },
  {
    "text": "clients like we saw before throughput originally increases until utilization",
    "start": "1205130",
    "end": "1213320"
  },
  {
    "text": "is high until the server is as busy as it can get and beyond that point",
    "start": "1213320",
    "end": "1219429"
  },
  {
    "text": "increasing the number of clients simply means your requests are starting to Q so",
    "start": "1219429",
    "end": "1224840"
  },
  {
    "text": "queuing increases right which is why throughput looks like that and the low realization regime grows and then it",
    "start": "1224840",
    "end": "1232039"
  },
  {
    "text": "flatlines and what we're interested in is what happens to response time in the",
    "start": "1232039",
    "end": "1239150"
  },
  {
    "text": "high utilization regime in this case to answer that question again we turn to",
    "start": "1239150",
    "end": "1245120"
  },
  {
    "text": "the theory which tells us to use littles law now before we talk about littles law",
    "start": "1245120",
    "end": "1253840"
  },
  {
    "text": "I'd like to clarify how the model works so we're all on the same page about it",
    "start": "1253840",
    "end": "1258970"
  },
  {
    "text": "now the system in this case is the entire loop okay so we can take a",
    "start": "1258970",
    "end": "1266919"
  },
  {
    "text": "request as being in three states in the system it can be in the sleeping State",
    "start": "1266919",
    "end": "1272600"
  },
  {
    "text": "on the devices it can be waiting in the server's queue or it can be or it might",
    "start": "1272600",
    "end": "1279919"
  },
  {
    "text": "be being processed so in the server itself and so the total number of",
    "start": "1279919",
    "end": "1285140"
  },
  {
    "text": "requests in the system is the includes the request across all three states so",
    "start": "1285140",
    "end": "1290720"
  },
  {
    "text": "it includes the requests on the sensors on the devices and the requests being",
    "start": "1290720",
    "end": "1296240"
  },
  {
    "text": "processed on the server or in the server cube so littles log gives us a formula",
    "start": "1296240",
    "end": "1301669"
  },
  {
    "text": "for the total number of requests and littles law tells us that this number is",
    "start": "1301669",
    "end": "1308270"
  },
  {
    "text": "equal to the throughput times the round-trip time of a request the total",
    "start": "1308270",
    "end": "1314720"
  },
  {
    "text": "timer request spends in the system which is the sleep time plus the response time",
    "start": "1314720",
    "end": "1321730"
  },
  {
    "text": "okay so since we're in the high utilization regime we said throughput is",
    "start": "1321940",
    "end": "1327020"
  },
  {
    "text": "constant we're also assuming that the sleep time is constant which means this formula simply",
    "start": "1327020",
    "end": "1334380"
  },
  {
    "text": "becomes the number of requests in the system this has to be equal to n like we",
    "start": "1334380",
    "end": "1341880"
  },
  {
    "text": "said before because we have n clients and this is simply equal to a constant",
    "start": "1341880",
    "end": "1347490"
  },
  {
    "text": "times the response time does this make sense ok so what this is telling us is",
    "start": "1347490",
    "end": "1356940"
  },
  {
    "text": "that the response time in this case it only grows linearly with n it doesn't",
    "start": "1356940",
    "end": "1363960"
  },
  {
    "text": "grow unbounded so going back to our question when we're in the low",
    "start": "1363960",
    "end": "1374250"
  },
  {
    "text": "utilization regime response time stays about the same but we're in the high",
    "start": "1374250",
    "end": "1379440"
  },
  {
    "text": "when we're in the high utilization regime it grows linearly with n putting",
    "start": "1379440",
    "end": "1384810"
  },
  {
    "text": "those two pieces together this is what our graph our response time graph looks",
    "start": "1384810",
    "end": "1389910"
  },
  {
    "text": "like for a closed system it's initially pretty it stays pretty flat you can't",
    "start": "1389910",
    "end": "1396960"
  },
  {
    "text": "see that portion very well in this picture but then it becomes a line and",
    "start": "1396960",
    "end": "1402180"
  },
  {
    "text": "this is way different than the similar the corresponding graph for an open",
    "start": "1402180",
    "end": "1408600"
  },
  {
    "text": "system which looked like this and the high utilization regime if we allowed",
    "start": "1408600",
    "end": "1414120"
  },
  {
    "text": "the queue to grow unbounded response time would tend to infinity ok so at",
    "start": "1414120",
    "end": "1423240"
  },
  {
    "text": "this point we talked about open systems and closed systems and we've talked",
    "start": "1423240",
    "end": "1428910"
  },
  {
    "text": "about how they're different what this implies is that we probably shouldn't",
    "start": "1428910",
    "end": "1435120"
  },
  {
    "text": "model an open system using a closed system model and vice versa right",
    "start": "1435120",
    "end": "1441560"
  },
  {
    "text": "there's only one problem and the problem is does the closed system remind you of",
    "start": "1441560",
    "end": "1448890"
  },
  {
    "text": "anything you might use load simulators most standard load simulators typically",
    "start": "1448890",
    "end": "1459000"
  },
  {
    "text": "mimic closed systems right you you have a number of virtual",
    "start": "1459000",
    "end": "1464730"
  },
  {
    "text": "virtual users each each client sends a request to your production system it",
    "start": "1464730",
    "end": "1471150"
  },
  {
    "text": "waits for a response that might be sleep built-in and then it repeats this is a closed system but your actual production",
    "start": "1471150",
    "end": "1479580"
  },
  {
    "text": "system with real users might not be a closed system right it might be an open",
    "start": "1479580",
    "end": "1485430"
  },
  {
    "text": "system or there's research that shows that most real systems lie somewhere in",
    "start": "1485430",
    "end": "1491730"
  },
  {
    "text": "between the spectrum between closed and open systems they're called partly open systems though the big takeaway here is",
    "start": "1491730",
    "end": "1499710"
  },
  {
    "text": "that if you're using a closed system type simulator you need to be aware and",
    "start": "1499710",
    "end": "1505770"
  },
  {
    "text": "wary of the results you get specifically your load simulation might predict lower",
    "start": "1505770",
    "end": "1512100"
  },
  {
    "text": "response times than you'd actually see in your in your production system there",
    "start": "1512100",
    "end": "1518910"
  },
  {
    "text": "are other differences closed systems tend to show a better tolerance to variability in request size than open",
    "start": "1518910",
    "end": "1527220"
  },
  {
    "text": "systems and scheduling algorithms produce different results for what's",
    "start": "1527220",
    "end": "1533340"
  },
  {
    "text": "good in closed systems versus open systems are there workarounds is this",
    "start": "1533340",
    "end": "1541230"
  },
  {
    "text": "something we can do if the answer is yes there is the workarounds are somewhat",
    "start": "1541230",
    "end": "1547290"
  },
  {
    "text": "involved so I we won't go into the details right now but if you're curious",
    "start": "1547290",
    "end": "1552960"
  },
  {
    "text": "ask me later I do encourage checking out these two neat neat papers on the topic",
    "start": "1552960",
    "end": "1558710"
  },
  {
    "text": "this open versus closed and how to emulate web traffic the links are on the slides which answer the question in",
    "start": "1558710",
    "end": "1566700"
  },
  {
    "text": "detail okay so that's single server",
    "start": "1566700",
    "end": "1573150"
  },
  {
    "text": "systems now let's move on to a cluster of many servers because really our",
    "start": "1573150",
    "end": "1579630"
  },
  {
    "text": "production systems look like this what are questions we might be",
    "start": "1579630",
    "end": "1585720"
  },
  {
    "text": "interested in for these systems well questions of capacity planning how many",
    "start": "1585720",
    "end": "1592320"
  },
  {
    "text": "servers do we need and questions of scalability how can we improve how the system scales right how",
    "start": "1592320",
    "end": "1599280"
  },
  {
    "text": "can we get higher throughput from our by increasing the number of servers okay so",
    "start": "1599280",
    "end": "1606560"
  },
  {
    "text": "let's start with the first question how many servers do we need well we know",
    "start": "1606560",
    "end": "1617490"
  },
  {
    "text": "that the maximum throughput a single server can support right that's what we just looked at if we have a cluster of n",
    "start": "1617490",
    "end": "1624630"
  },
  {
    "text": "servers is the throughput of the cluster simply n times the throughput of the",
    "start": "1624630",
    "end": "1630090"
  },
  {
    "text": "single server I wish systems don't scale linearly",
    "start": "1630090",
    "end": "1636780"
  },
  {
    "text": "right so this is not the case and systems don't scale linearly typically",
    "start": "1636780",
    "end": "1643200"
  },
  {
    "text": "for two reasons the first is contention right where the idea is there's some",
    "start": "1643200",
    "end": "1650280"
  },
  {
    "text": "fraction of your workload that cannot be paralyzed there's a serial fraction to your",
    "start": "1650280",
    "end": "1655740"
  },
  {
    "text": "workload and for that fraction when you",
    "start": "1655740",
    "end": "1661020"
  },
  {
    "text": "increase the number of servers you're increasing the contention penalty right",
    "start": "1661020",
    "end": "1666450"
  },
  {
    "text": "this is also captured in our dolls law a classic example is serialization",
    "start": "1666450",
    "end": "1673100"
  },
  {
    "text": "contention for shared resources for example database contention lock",
    "start": "1673100",
    "end": "1678240"
  },
  {
    "text": "contention an example that I like to use is if you have a scatter gather a type",
    "start": "1678240",
    "end": "1683450"
  },
  {
    "text": "type workload where there's a scatter phase and I and then you have the",
    "start": "1683450",
    "end": "1689100"
  },
  {
    "text": "gathering you can by increasing the number of servers that are performing",
    "start": "1689100",
    "end": "1694560"
  },
  {
    "text": "the scatter phase you can increase parallelization to some degree but then",
    "start": "1694560",
    "end": "1700560"
  },
  {
    "text": "at the gather phase contention is going to increase right so those a contention penalty there and this contention",
    "start": "1700560",
    "end": "1708630"
  },
  {
    "text": "penalty can be expressed as alpha times and a constant times n and all this",
    "start": "1708630",
    "end": "1714240"
  },
  {
    "text": "captures is that contention the contention penalty increases linearly with the number of servers which is what",
    "start": "1714240",
    "end": "1721890"
  },
  {
    "text": "you'd expect the second reason why systems don't",
    "start": "1721890",
    "end": "1727040"
  },
  {
    "text": "scale linearly is cross stock penalty or coordination penalty consistency penalty",
    "start": "1727040",
    "end": "1734710"
  },
  {
    "text": "the idea here is if your systems need to coordinate typically they need to",
    "start": "1734710",
    "end": "1740960"
  },
  {
    "text": "coordinate to synchronize mutable state but if this coordination in your system",
    "start": "1740960",
    "end": "1746020"
  },
  {
    "text": "then the worst case penalty is when each server has to talk to every other server",
    "start": "1746020",
    "end": "1751760"
  },
  {
    "text": "has to coordinated with every other server so this penalty skills is a",
    "start": "1751760",
    "end": "1757120"
  },
  {
    "text": "constant times N squared right so the penalty here is quadratic with the number of servers okay so we know our",
    "start": "1757120",
    "end": "1767480"
  },
  {
    "text": "cluster might have to pay these two penalties again our question is how does",
    "start": "1767480",
    "end": "1773330"
  },
  {
    "text": "this affect the throughput and the answer to this is given to us by the",
    "start": "1773330",
    "end": "1778820"
  },
  {
    "text": "universal scalability law now don't let the formula scare you it makes intuitive",
    "start": "1778820",
    "end": "1785390"
  },
  {
    "text": "sense if we work through it the idea here is if there is no contention in our",
    "start": "1785390",
    "end": "1792080"
  },
  {
    "text": "system and if there is no coordination and we're not paying those penalties then we'd get the much-sought-after a",
    "start": "1792080",
    "end": "1798440"
  },
  {
    "text": "linear scaling right which is the line over there now if we have only contention and not cross stock then this",
    "start": "1798440",
    "end": "1806690"
  },
  {
    "text": "is what the graph looks like you see through put this concurrency on the",
    "start": "1806690",
    "end": "1813170"
  },
  {
    "text": "x-axis and throughput on the y-axis so you see throughput initially increase",
    "start": "1813170",
    "end": "1819110"
  },
  {
    "text": "and then somewhat flatline and if there's both contention and cross stock",
    "start": "1819110",
    "end": "1824960"
  },
  {
    "text": "in our system then bad luck because the theory says you start to actually see",
    "start": "1824960",
    "end": "1831830"
  },
  {
    "text": "retrograde scalability where throughput actually starts to decrease there are examples of this in online in Baran",
    "start": "1831830",
    "end": "1839690"
  },
  {
    "text": "Schwartz's blogs and Neil Gunther's blog posts I personally have not seen this",
    "start": "1839690",
    "end": "1846790"
  },
  {
    "text": "all right so that's universal scalability and that helps us answer the",
    "start": "1846790",
    "end": "1853100"
  },
  {
    "text": "question what is the throughput of my cluster no let's ask our second question how can",
    "start": "1853100",
    "end": "1861509"
  },
  {
    "text": "we improve how the system scales well the universal scalability law tells us",
    "start": "1861509",
    "end": "1867029"
  },
  {
    "text": "where to look it tells us to look for sources of contention and cross stock",
    "start": "1867029",
    "end": "1873059"
  },
  {
    "text": "and to eliminate or reduce those right because that is what is preventing our",
    "start": "1873059",
    "end": "1878490"
  },
  {
    "text": "system from scaling linearly some real-life examples of real-world",
    "start": "1878490",
    "end": "1885389"
  },
  {
    "text": "examples of doing this Facebook Facebook",
    "start": "1885389",
    "end": "1890820"
  },
  {
    "text": "has a caching service called Tao and they use smarter data partitioning to",
    "start": "1890820",
    "end": "1899100"
  },
  {
    "text": "help their clusters scale better so they had an old partitioning algorithm which made it so that a greater fraction of",
    "start": "1899100",
    "end": "1908759"
  },
  {
    "text": "the popular data ended up on just a few servers and as a results that was",
    "start": "1908759",
    "end": "1914309"
  },
  {
    "text": "contention for those caching servers what they ended up doing is use a different a smarter partitioning",
    "start": "1914309",
    "end": "1921059"
  },
  {
    "text": "algorithm so the popular partitions were more balanced across servers and that",
    "start": "1921059",
    "end": "1926580"
  },
  {
    "text": "helped get throughput get higher their cache caching service throughput get hot",
    "start": "1926580",
    "end": "1932129"
  },
  {
    "text": "cluster get higher throughput another example is by using smarter aggregation",
    "start": "1932129",
    "end": "1939240"
  },
  {
    "text": "another example of this is in Facebook's kuba time-series data store this is sort",
    "start": "1939240",
    "end": "1946919"
  },
  {
    "text": "of a scatter gather system in that the data is partitioned across a number of",
    "start": "1946919",
    "end": "1952590"
  },
  {
    "text": "leaf nodes and you have an aggregator node that when you make a query for a",
    "start": "1952590",
    "end": "1958259"
  },
  {
    "text": "time series data it performs aggregation across the data nodes so rather than",
    "start": "1958259",
    "end": "1963269"
  },
  {
    "text": "having a flat hierarchy where you have one aggregation node and you just scale",
    "start": "1963269",
    "end": "1968429"
  },
  {
    "text": "up the data nodes to accommodate more data they structure Scooba as an",
    "start": "1968429",
    "end": "1973649"
  },
  {
    "text": "aggregation tree so you have several levels of aggregation and the thinking here is you can paralyze the aggregation",
    "start": "1973649",
    "end": "1980159"
  },
  {
    "text": "step better and funnily other things you",
    "start": "1980159",
    "end": "1986909"
  },
  {
    "text": "can do is you can use better load balancing strategies again so that your loads there's less",
    "start": "1986909",
    "end": "1992850"
  },
  {
    "text": "contention for a few servers best of two random choices is what is used in most",
    "start": "1992850",
    "end": "2000529"
  },
  {
    "text": "systems at companies like Facebook and Google you can use fine grained locking to reduce contention you can use MVCC",
    "start": "2000529",
    "end": "2008080"
  },
  {
    "text": "most databases today support MVCC and so on and so forth all right so all right",
    "start": "2008080",
    "end": "2022090"
  },
  {
    "text": "it is now time to take a step back and consider what we can take away for our",
    "start": "2022090",
    "end": "2030409"
  },
  {
    "text": "systems right today we looked at analyzing single server systems open",
    "start": "2030409",
    "end": "2037700"
  },
  {
    "text": "versus closed we looked at analyzing a cluster of servers but we'll barely",
    "start": "2037700",
    "end": "2044059"
  },
  {
    "text": "scratch the surface of performance modeling right however we do know enough",
    "start": "2044059",
    "end": "2050290"
  },
  {
    "text": "to try and take this and attempt to apply it to our systems which raises the",
    "start": "2050290",
    "end": "2057290"
  },
  {
    "text": "big question which is where does performance modeling fit in our toolkit",
    "start": "2057290",
    "end": "2065559"
  },
  {
    "text": "in my opinion performance modeling is the most useful and the most powerful",
    "start": "2065770",
    "end": "2071960"
  },
  {
    "text": "when used in conjunction with more empirical methods if performance",
    "start": "2071960",
    "end": "2077059"
  },
  {
    "text": "analysis like load simulation and performance experiments right because",
    "start": "2077059",
    "end": "2083690"
  },
  {
    "text": "performance modeling makes assumptions that might be difficult to validate in",
    "start": "2083690",
    "end": "2089179"
  },
  {
    "text": "practice for example the queuing Theory models we looked at require you to know",
    "start": "2089179",
    "end": "2095378"
  },
  {
    "text": "your service time distribution or at least its mean and variance and in practice this is difficult to get right",
    "start": "2095379",
    "end": "2101660"
  },
  {
    "text": "because even before the request hits your application you can instrument that but even before it hits your application",
    "start": "2101660",
    "end": "2107599"
  },
  {
    "text": "it's going to spend time and TCP cues and what-have-you but performance",
    "start": "2107599",
    "end": "2115460"
  },
  {
    "text": "modeling it does give us a bigger us frame work to do a few things the first",
    "start": "2115460",
    "end": "2121940"
  },
  {
    "text": "is it gives us a frame where to determine what experiments to run right you run the experiments that you",
    "start": "2121940",
    "end": "2129800"
  },
  {
    "text": "need to get the data in order to fit the USL the universal scalability Locker where your response time graphs it gives",
    "start": "2129800",
    "end": "2140270"
  },
  {
    "text": "you a framework in which you can to interpret and evaluate the results of your experiments right for example you",
    "start": "2140270",
    "end": "2146869"
  },
  {
    "text": "know that load simulations the results you got it predicted blow your load",
    "start": "2146869",
    "end": "2152000"
  },
  {
    "text": "simulation predicted better results than your actual production system now you know why right or here's another example",
    "start": "2152000",
    "end": "2158990"
  },
  {
    "text": "I like to use so you run a load simulation with end clients where you",
    "start": "2158990",
    "end": "2165320"
  },
  {
    "text": "increase the number of virtual users and you get a response time you get a",
    "start": "2165320",
    "end": "2171770"
  },
  {
    "text": "response time graph like the one on the right well you know that that graph is the wrong shape response time doesn't",
    "start": "2171770",
    "end": "2178010"
  },
  {
    "text": "behave like that response time behaves like one of these graphs on the left right this and we know that from",
    "start": "2178010",
    "end": "2185000"
  },
  {
    "text": "performance modeling from the theory and finally it gives us a framework to help",
    "start": "2185000",
    "end": "2192380"
  },
  {
    "text": "us determine and decide what improvements would give us the biggest wins right where we need to focus our",
    "start": "2192380",
    "end": "2198590"
  },
  {
    "text": "efforts to help our systems performance and scalability all right and with that",
    "start": "2198590",
    "end": "2206359"
  },
  {
    "text": "it's time to say goodbye thank you all [Applause]",
    "start": "2206359",
    "end": "2213179"
  }
]