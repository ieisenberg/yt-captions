[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "good morning to Khan come on yeah that's better yeah so",
    "start": "3939",
    "end": "14620"
  },
  {
    "text": "welcome to the bare-knuckles performance track and to to my talk about high",
    "start": "14620",
    "end": "19690"
  },
  {
    "text": "resolution performance telemetry at scale first a little bit about me start",
    "start": "19690",
    "end": "29200"
  },
  {
    "text": "my timer there me I'm Brian I work at Twitter I'm a staff",
    "start": "29200",
    "end": "38890"
  },
  {
    "start": "33000",
    "end": "83000"
  },
  {
    "text": "site reliability engineer I've been at Twitter for over five years now I work",
    "start": "38890",
    "end": "44230"
  },
  {
    "text": "on a team called IO which is infrastructure optimization and performance my background is on",
    "start": "44230",
    "end": "52089"
  },
  {
    "text": "telemetry performance tuning benchmarking that kind of stuff I really",
    "start": "52089",
    "end": "57999"
  },
  {
    "text": "like nerding out about making things go faster I'm also heavily involved in our open",
    "start": "57999",
    "end": "66280"
  },
  {
    "text": "source program at Twitter I have two open source projects currently hopefully",
    "start": "66280",
    "end": "73750"
  },
  {
    "text": "more in the future but I also work on our process of helping engineers",
    "start": "73750",
    "end": "78940"
  },
  {
    "text": "actually publish open source software at Twitter so what are we going to talk",
    "start": "78940",
    "end": "85300"
  },
  {
    "start": "83000",
    "end": "89000"
  },
  {
    "text": "about today obviously high resolution performance telemetry but specifically",
    "start": "85300",
    "end": "90310"
  },
  {
    "start": "89000",
    "end": "118000"
  },
  {
    "text": "we're going to talk about various sources of telemetry sampling resolution",
    "start": "90310",
    "end": "96610"
  },
  {
    "text": "and sort of the issues that come along with it we're going to talk about how to",
    "start": "96610",
    "end": "101650"
  },
  {
    "text": "achieve low cost and high resolution at the same time and some of the practical",
    "start": "101650",
    "end": "107050"
  },
  {
    "text": "benefits that we've seen at Twitter from having high resolution telemetry so",
    "start": "107050",
    "end": "115630"
  },
  {
    "text": "let's begin by talking about various telemetry sources but what do we even mean by telemetry well telemetry is the",
    "start": "115630",
    "end": "123160"
  },
  {
    "start": "118000",
    "end": "159000"
  },
  {
    "text": "process of recording and transmitting readings of an instrument with systems",
    "start": "123160",
    "end": "128920"
  },
  {
    "text": "performance telemetry we look to add instrumentation that helps us to quantify runtime characteristics of our",
    "start": "128920",
    "end": "135880"
  },
  {
    "text": "workload and the amount of resource that it takes to execute",
    "start": "135880",
    "end": "140980"
  },
  {
    "text": "essentially we're trying to quantify what our systems are doing and how well they are doing it by understanding these",
    "start": "140980",
    "end": "148420"
  },
  {
    "text": "aspects of performance were then able to optimize for increased performance and efficiency if you can't measure it you",
    "start": "148420",
    "end": "156190"
  },
  {
    "text": "can't improve it so what do we want to",
    "start": "156190",
    "end": "162160"
  },
  {
    "start": "159000",
    "end": "305000"
  },
  {
    "text": "know when we're talking about systems performance and Brendan Gregg systems performance book he talks about the used",
    "start": "162160",
    "end": "169239"
  },
  {
    "text": "method essentially the used method is used to quickly identify resource",
    "start": "169239",
    "end": "176200"
  },
  {
    "text": "bottlenecks or errors use focuses on the closely related concepts of utilization",
    "start": "176200",
    "end": "182799"
  },
  {
    "text": "and saturation and also errors when we're talking about utilization and",
    "start": "182799",
    "end": "189040"
  },
  {
    "text": "saturation there they're sort of different sides of the same coin",
    "start": "189040",
    "end": "194280"
  },
  {
    "text": "utilization directly correlates to making the most use of the amount of resource that our infrastructure has we",
    "start": "194280",
    "end": "202450"
  },
  {
    "text": "actually want to maximize this within the constraint of saturation though once",
    "start": "202450",
    "end": "208450"
  },
  {
    "text": "we've hit saturation we've hit a bottleneck in the system and our system is going to start have degraded",
    "start": "208450",
    "end": "214599"
  },
  {
    "text": "performance as load increases so we want to try and avoid that because that leads",
    "start": "214599",
    "end": "221079"
  },
  {
    "text": "to bad user experience errors is maybe",
    "start": "221079",
    "end": "226959"
  },
  {
    "text": "somewhat self-explanatory but things like packet drops retransmits and other",
    "start": "226959",
    "end": "232720"
  },
  {
    "text": "errors can have a huge impact on distributed systems so we want to be able to quantify those as well beyond",
    "start": "232720",
    "end": "240970"
  },
  {
    "text": "use we begin to look at things like efficiency this is a measure of how well",
    "start": "240970",
    "end": "247060"
  },
  {
    "text": "our workload is running on a given platform or resource we can think of",
    "start": "247060",
    "end": "254099"
  },
  {
    "text": "efficiency in probably a variety of ways it might be the amount of QPS per dollar",
    "start": "254099",
    "end": "261549"
  },
  {
    "text": "that we're getting or we might be trying to optimize a target like power efficiency so trying to get the most",
    "start": "261549",
    "end": "268150"
  },
  {
    "text": "workload for the least amount of power expense [Music]",
    "start": "268150",
    "end": "273750"
  },
  {
    "text": "the other way to look at efficiency is how well we're utilizing a specific hardware resource so we might look at",
    "start": "273750",
    "end": "280050"
  },
  {
    "text": "things like our CPU cache hit rate our branch predictor performance etc another",
    "start": "280050",
    "end": "289320"
  },
  {
    "text": "really important aspect of systems performance particularly in you know distributed micro service architecture",
    "start": "289320",
    "end": "296070"
  },
  {
    "text": "stuff is latency though which is how long it takes to perform a given",
    "start": "296070",
    "end": "302130"
  },
  {
    "text": "operation so there are a bunch of",
    "start": "302130",
    "end": "307410"
  },
  {
    "start": "305000",
    "end": "403000"
  },
  {
    "text": "traditional telemetry sources that are",
    "start": "307410",
    "end": "313610"
  },
  {
    "text": "generally exposed through command-line tools and common monitoring agents these",
    "start": "313610",
    "end": "320340"
  },
  {
    "text": "help us to essentially get a high-level understanding of utilization saturation and errors you can things like top like",
    "start": "320340",
    "end": "330120"
  },
  {
    "text": "the basic sort of stuff that you would start to look at when you're looking at",
    "start": "330120",
    "end": "335450"
  },
  {
    "text": "systems performance brendan actually has a really good rundown of like systems",
    "start": "335450",
    "end": "340950"
  },
  {
    "text": "performance Diagnostics in 60 seconds or something like that so these are the types of things that you might use for",
    "start": "340950",
    "end": "349110"
  },
  {
    "text": "that so you're looking at you know things like the CPU time running which",
    "start": "349110",
    "end": "355200"
  },
  {
    "text": "can tell us both utilization and saturation because we know that the CPU",
    "start": "355200",
    "end": "360540"
  },
  {
    "text": "can only run for the amount of nanoseconds within that second for that",
    "start": "360540",
    "end": "365640"
  },
  {
    "text": "number of cores that you have we also know that disks have a maximum bandwidth",
    "start": "365640",
    "end": "371100"
  },
  {
    "text": "and a maximum number of IAP so that they're able to perform and network also",
    "start": "371100",
    "end": "377760"
  },
  {
    "text": "has sort of limits their network is sort of where it becomes obvious when there",
    "start": "377760",
    "end": "385560"
  },
  {
    "text": "are errors because you have these nice little like protocol statistics and package drops and all sorts of wonderful",
    "start": "385560",
    "end": "393090"
  },
  {
    "text": "things so these are sort of the traditional things that a lot of",
    "start": "393090",
    "end": "398750"
  },
  {
    "text": "telemetry agents expose [Music] the next evolution is starting to",
    "start": "398750",
    "end": "405539"
  },
  {
    "start": "403000",
    "end": "488000"
  },
  {
    "text": "understand more about the actual hardware behaviors we can do this by",
    "start": "405539",
    "end": "411030"
  },
  {
    "text": "using performance counters which allow us to instrument the CPU and some of the",
    "start": "411030",
    "end": "417120"
  },
  {
    "text": "software behaviors performance events help us measure really granular things",
    "start": "417120",
    "end": "425340"
  },
  {
    "text": "like we can actually count the number of cycles that the CPU has run we can count",
    "start": "425340",
    "end": "431490"
  },
  {
    "text": "the number of instructions retired we can count the number of cache accesses at all these different cache levels",
    "start": "431490",
    "end": "437759"
  },
  {
    "text": "within within the CPU this telemetry is",
    "start": "437759",
    "end": "443520"
  },
  {
    "text": "more typically exposed by profiling tools things like intel vtune and the",
    "start": "443520",
    "end": "450240"
  },
  {
    "text": "Linux tool perf however some telemetry agents actually do start to expose metrics from these",
    "start": "450240",
    "end": "457680"
  },
  {
    "text": "performance counters and they become really interesting for starting to look",
    "start": "457680",
    "end": "462870"
  },
  {
    "text": "at how the workload is is running on the hardware and you can start to identify",
    "start": "462870",
    "end": "468860"
  },
  {
    "text": "areas where tuning might make a difference like you might realize that",
    "start": "468860",
    "end": "475199"
  },
  {
    "text": "you have a bunch of cache misses and that by changing your code a little bit you can start to actually improve your",
    "start": "475199",
    "end": "482310"
  },
  {
    "text": "codes performance dramatically yeah so",
    "start": "482310",
    "end": "489570"
  },
  {
    "start": "488000",
    "end": "580000"
  },
  {
    "text": "performance events for efficiency we can start to look at things like power we",
    "start": "489570",
    "end": "496949"
  },
  {
    "text": "can look at cycles per instruction which is how many clock cycles it actually",
    "start": "496949",
    "end": "503370"
  },
  {
    "text": "takes for each assembly instruction to execute CPI actually is a pretty common",
    "start": "503370",
    "end": "510680"
  },
  {
    "text": "metric to start to look at how well your workload is running on a CPU you can",
    "start": "510680",
    "end": "517560"
  },
  {
    "text": "look at as I as I said like cache hit rates you can also look at how well the branch predictor is performing in the",
    "start": "517560",
    "end": "524250"
  },
  {
    "text": "cpu modern CPUs are like crazy complicated and they do all sorts of",
    "start": "524250",
    "end": "533880"
  },
  {
    "text": "really interesting stuff behind the scenes to sort of make your code run fast a modern hyper",
    "start": "533880",
    "end": "542100"
  },
  {
    "text": "scalar processors we should actually expect less than one cycle per",
    "start": "542100",
    "end": "548069"
  },
  {
    "text": "instruction because they're able to retire multiple instructions in parallel even on the same core but kind of in",
    "start": "548069",
    "end": "557429"
  },
  {
    "text": "reality that's rare to see on anything but like a very compute heavy workload",
    "start": "557429",
    "end": "564679"
  },
  {
    "text": "anytime you're accessing things like memory it's it's more likely that you're going to see multiple cycles per",
    "start": "564679",
    "end": "571170"
  },
  {
    "text": "instruction as the CPU winds up essentially waiting for that data to",
    "start": "571170",
    "end": "576269"
  },
  {
    "text": "come in and then we have EBP F which is",
    "start": "576269",
    "end": "582689"
  },
  {
    "start": "580000",
    "end": "836000"
  },
  {
    "text": "really cool and it gives us super powers it's arguably one of the most powerful",
    "start": "582689",
    "end": "589139"
  },
  {
    "text": "tools that we have for understanding systems performance and really I think a",
    "start": "589139",
    "end": "594179"
  },
  {
    "text": "lot of us are just starting to really tap into it to help get better telemetry",
    "start": "594179",
    "end": "600619"
  },
  {
    "text": "EBP F is the enhanced Berkeley packet filter as you might guess it sort of",
    "start": "600619",
    "end": "607739"
  },
  {
    "text": "evolved from tooling to filter packets but it's turned into a very powerful",
    "start": "607739",
    "end": "612989"
  },
  {
    "text": "tracing tool it gives us the ability to trace things that are occurring both in",
    "start": "612989",
    "end": "620189"
  },
  {
    "text": "kernel space and user space and actually execute custom code in the kernel to",
    "start": "620189",
    "end": "628529"
  },
  {
    "text": "provide summaries which we can then pull into user space while traditional",
    "start": "628529",
    "end": "637699"
  },
  {
    "text": "sources can measure like the total number of packets transferred or total",
    "start": "637699",
    "end": "643049"
  },
  {
    "text": "bytes with EBP F we can actually start to do things like get histograms of our",
    "start": "643049",
    "end": "650339"
  },
  {
    "text": "packet size distribution and that's really really exciting similarly we're",
    "start": "650339",
    "end": "657600"
  },
  {
    "text": "able to get things like block i/o size distribution block device Layton sees",
    "start": "657600",
    "end": "663059"
  },
  {
    "text": "all sorts of like really really cool stuff that help us understand down to",
    "start": "663059",
    "end": "670350"
  },
  {
    "text": "down to individual events right we're doing actual tracing with EB PF and it gives",
    "start": "670350",
    "end": "677100"
  },
  {
    "text": "us a really interesting view about how our systems are performing so II BPF",
    "start": "677100",
    "end": "685769"
  },
  {
    "text": "very powerful for understanding latency we can start to understand how long",
    "start": "685769",
    "end": "691009"
  },
  {
    "text": "runnable tasks are waiting in the run queue before they get scheduled to run",
    "start": "691009",
    "end": "696660"
  },
  {
    "text": "on a cpu we can actually actually measure the distribution of file system",
    "start": "696660",
    "end": "703040"
  },
  {
    "text": "operation latencies like read write open F sync all that stuff we can measure how",
    "start": "703040",
    "end": "710699"
  },
  {
    "text": "long individual requests are waiting waiting on the block device queue and",
    "start": "710699",
    "end": "716630"
  },
  {
    "text": "that helps us to evaluate how different queuing algorithms actually wind up",
    "start": "716630",
    "end": "724259"
  },
  {
    "text": "performing for our workload we can also measure things like outbound Network",
    "start": "724259",
    "end": "730050"
  },
  {
    "text": "connect Layton sees the the Delta in time between San and getting a syn ACK back so all very very cool stuff",
    "start": "730050",
    "end": "739040"
  },
  {
    "text": "eb PF is also insanely powerful for workload characterization which is sort",
    "start": "739040",
    "end": "746279"
  },
  {
    "text": "of an often neglected aspect of systems performance telemetry it becomes very",
    "start": "746279",
    "end": "754290"
  },
  {
    "text": "difficult to work with a vendor if they pose the question of like oh you know what are your you know block i/o sizes",
    "start": "754290",
    "end": "761579"
  },
  {
    "text": "and you're just like I don't know it just it's this type of workload I guess",
    "start": "761579",
    "end": "766670"
  },
  {
    "text": "you actually need to be able to give him numbers so they can help you with eb PF",
    "start": "766670",
    "end": "772769"
  },
  {
    "text": "you can start to get things like the block i/o size distribution for for read",
    "start": "772769",
    "end": "778050"
  },
  {
    "text": "and write operations so now you can actually be like hey you know it's not",
    "start": "778050",
    "end": "783540"
  },
  {
    "text": "it's not 4k it's actually you know this size and you can give them the full",
    "start": "783540",
    "end": "789509"
  },
  {
    "text": "distribution and that really starts to help things you can also look at your",
    "start": "789509",
    "end": "794910"
  },
  {
    "text": "packet size distribution so you can understand you know actually if you're",
    "start": "794910",
    "end": "799949"
  },
  {
    "text": "you know approaching full MTU packets or you know if jumbo dreams would help or you know if you're",
    "start": "799949",
    "end": "806940"
  },
  {
    "text": "just sending a lot of very small packets a lot of my background is from",
    "start": "806940",
    "end": "812160"
  },
  {
    "text": "optimizing cash services like memcache and register type stuff where we have",
    "start": "812160",
    "end": "819750"
  },
  {
    "text": "very small packet sizes generally and yeah it can be interesting that you can",
    "start": "819750",
    "end": "826230"
  },
  {
    "text": "start to hit actual packet rate limits within the kernel way before you hit",
    "start": "826230",
    "end": "832290"
  },
  {
    "text": "actual bandwidth saturation so one of",
    "start": "832290",
    "end": "837960"
  },
  {
    "text": "the most critical aspects about measuring systems performance is how you're sampling it and how often you're",
    "start": "837960",
    "end": "844500"
  },
  {
    "text": "sampling it it all comes down to sampling resolution the Nyquist shannon",
    "start": "844500",
    "end": "852450"
  },
  {
    "start": "850000",
    "end": "873000"
  },
  {
    "text": "theorem basically states that when we're sampling a fear signal there's an",
    "start": "852450",
    "end": "859830"
  },
  {
    "text": "inherent bandwidth limit or a filter that's defined by our sampling rate this",
    "start": "859830",
    "end": "865860"
  },
  {
    "text": "imposes an upper bound on the highest frequency component that we can capture in our measured signal more practically",
    "start": "865860",
    "end": "874680"
  },
  {
    "start": "873000",
    "end": "947000"
  },
  {
    "text": "if we want to capture bursts that are happening we need to sample at least twice within the duration of the",
    "start": "874680",
    "end": "880920"
  },
  {
    "text": "shortest burst we wish to capture when we're talking about like you know",
    "start": "880920",
    "end": "887300"
  },
  {
    "text": "web-scale distributed systems type things you know 200 milliseconds is very",
    "start": "887300",
    "end": "896310"
  },
  {
    "text": "very long time actually a lot of users will not think your site is responsive",
    "start": "896310",
    "end": "902010"
  },
  {
    "text": "if it takes more than 200 milliseconds when we're talking about things like",
    "start": "902010",
    "end": "907589"
  },
  {
    "text": "caches 50 milliseconds is a very long time so you start to get into this",
    "start": "907589",
    "end": "913890"
  },
  {
    "text": "interesting problem where now things that take a very long time on a computer",
    "start": "913890",
    "end": "920430"
  },
  {
    "text": "scale are actually very short on a human scale and so you actually need to sample",
    "start": "920430",
    "end": "926550"
  },
  {
    "text": "very very frequently otherwise you're going to kind of just miss these things that are happening in order to",
    "start": "926550",
    "end": "934440"
  },
  {
    "text": "demonstrate the effect that sampling rate has on telemetry we're going to take a look at CPU utilization",
    "start": "934440",
    "end": "941280"
  },
  {
    "text": "on random machine i sampled for an hour",
    "start": "941280",
    "end": "946490"
  },
  {
    "start": "947000",
    "end": "1089000"
  },
  {
    "text": "there it is this is sampled on a minute ly basis and",
    "start": "948230",
    "end": "955100"
  },
  {
    "text": "is just the CPU utilization the actual scale doesn't matter for the purpose of",
    "start": "955100",
    "end": "962190"
  },
  {
    "text": "our talk here but we can tell from this that hey you know our utilization is",
    "start": "962190",
    "end": "967980"
  },
  {
    "text": "relatively constant at the end there's like this brief period where it's you",
    "start": "967980",
    "end": "973320"
  },
  {
    "text": "know a little bit higher and then comes back down to normal but I would say that other than that it's pretty even well",
    "start": "973320",
    "end": "982830"
  },
  {
    "text": "that's not actually how it is here we're seeing secondly data in the blue with",
    "start": "982830",
    "end": "990690"
  },
  {
    "text": "the minute ly data in the red and we now see a much different picture we can now",
    "start": "990690",
    "end": "996510"
  },
  {
    "text": "see that there are regular spikes and dips below this minute ly average that we are otherwise capturing time series",
    "start": "996510",
    "end": "1005690"
  },
  {
    "text": "is also a lot fuzzier just sort of in general and arguably a lot harder to",
    "start": "1005690",
    "end": "1011570"
  },
  {
    "text": "read like I wouldn't want to look at this from like thousands of servers at",
    "start": "1011570",
    "end": "1017210"
  },
  {
    "text": "that sort of resolution it just becomes too much too much cognitive burden for me as a human and",
    "start": "1017210",
    "end": "1026530"
  },
  {
    "text": "here we're actually looking at a histogram distribution of those values",
    "start": "1026530",
    "end": "1033010"
  },
  {
    "text": "normalized into utilization nanoseconds per second sort of thing so they're",
    "start": "1033010",
    "end": "1040040"
  },
  {
    "text": "comparable but really the most interesting thing is about the the skew",
    "start": "1040040",
    "end": "1047870"
  },
  {
    "text": "of these distributions the minute ly gives you sort of this false impression",
    "start": "1047870",
    "end": "1053900"
  },
  {
    "text": "that it's all basically the same and the reason why the chart looks very very",
    "start": "1053900",
    "end": "1061040"
  },
  {
    "text": "boring to the right hand side is because that secondly data has a very very long tail off to the right that just doesn't",
    "start": "1061040",
    "end": "1068270"
  },
  {
    "text": "show with this particular scaling log scale might have been better here but this technically data also has more",
    "start": "1068270",
    "end": "1075799"
  },
  {
    "text": "of a even though it's it's right skewed it's more of you know a normal distribution whereas the the minute late",
    "start": "1075799",
    "end": "1083600"
  },
  {
    "text": "data is kind of weird-looking so this",
    "start": "1083600",
    "end": "1091880"
  },
  {
    "start": "1089000",
    "end": "1195000"
  },
  {
    "text": "problem isn't restricted to just sampling gauges and counters it can also",
    "start": "1091880",
    "end": "1098240"
  },
  {
    "text": "appear in things like latency histograms",
    "start": "1098240",
    "end": "1103120"
  },
  {
    "text": "RPC perf which is my cache benchmarking tool has the ability to generate these waterfall plots essentially you have",
    "start": "1104379",
    "end": "1112580"
  },
  {
    "text": "time running downwards and increased latency to the right-hand side of the chart and the color intensity kind of",
    "start": "1112580",
    "end": "1121549"
  },
  {
    "text": "sweeps from black through all the shades of blue and then from blue all the way to red to indicate the number of events",
    "start": "1121549",
    "end": "1130009"
  },
  {
    "text": "that fell into that latency bucket so here we're looking at some synthetic",
    "start": "1130009",
    "end": "1135919"
  },
  {
    "text": "testing with RPC perf against a cache instance in this particular case when I",
    "start": "1135919",
    "end": "1144049"
  },
  {
    "text": "was looking at minute lis data my p39 and p4 9 was higher than I really",
    "start": "1144049",
    "end": "1152330"
  },
  {
    "text": "expected it to be and when I took a look at this latency waterfall you can see",
    "start": "1152330",
    "end": "1159980"
  },
  {
    "text": "that there are these periodic spikes of higher latency that are actually skewing",
    "start": "1159980",
    "end": "1165110"
  },
  {
    "text": "my minute li tail latencies and to me",
    "start": "1165110",
    "end": "1170299"
  },
  {
    "text": "this indicated that there was something we needed to dig in and and see what was going on like it's pretty obvious that",
    "start": "1170299",
    "end": "1178250"
  },
  {
    "text": "these things are occurring on a minute ly basis and always at the same offset",
    "start": "1178250",
    "end": "1183259"
  },
  {
    "text": "in a minute and these types of anomalies or deviations and performance can",
    "start": "1183259",
    "end": "1189919"
  },
  {
    "text": "actually have like a very huge impact on your systems so how can we capture",
    "start": "1189919",
    "end": "1198350"
  },
  {
    "start": "1195000",
    "end": "1277000"
  },
  {
    "text": "bursts oh we just increase our sampling resolution right but this comes at a",
    "start": "1198350",
    "end": "1206359"
  },
  {
    "text": "cost you have the overhead of just collecting the data and that's pretty hard to",
    "start": "1206359",
    "end": "1213049"
  },
  {
    "text": "mitigate but then you also need to store it and analyze it and these are areas",
    "start": "1213049",
    "end": "1220849"
  },
  {
    "text": "where I think we can improve so how can we get the best of both worlds right we",
    "start": "1220849",
    "end": "1227989"
  },
  {
    "text": "want to be able to capture these very short bursts which necessitate a very high sampling rate but we don't want to",
    "start": "1227989",
    "end": "1235249"
  },
  {
    "text": "pay for it we need to think about what we're really trying to achieve in our",
    "start": "1235249",
    "end": "1242119"
  },
  {
    "text": "telemetry we need to sort of go back to the beginning and think about what we're trying to capture and what we're trying",
    "start": "1242119",
    "end": "1247909"
  },
  {
    "text": "to measure we want to capture these bursts and we know that reporting",
    "start": "1247909",
    "end": "1255009"
  },
  {
    "text": "percentiles for latency distributions is a pretty common way of being able to",
    "start": "1255009",
    "end": "1261079"
  },
  {
    "text": "understand what's happening at rates of thousands and millions you know of events per second like very many so",
    "start": "1261079",
    "end": "1268779"
  },
  {
    "text": "let's see what we might be able to do by using distributions to help us",
    "start": "1268779",
    "end": "1274219"
  },
  {
    "text": "understand our data so instead of a",
    "start": "1274219",
    "end": "1279589"
  },
  {
    "start": "1277000",
    "end": "1517000"
  },
  {
    "text": "minute lis average as the red line here we actually have a p 50 or the 50th",
    "start": "1279589",
    "end": "1286549"
  },
  {
    "text": "percentile across each of these minutes so this tells us that half the time",
    "start": "1286549",
    "end": "1293779"
  },
  {
    "text": "within the minute the utilization is this red line value or less but also",
    "start": "1293779",
    "end": "1299869"
  },
  {
    "text": "that it's higher than this the other half of the time it actually looks pretty similar to the minute lis average",
    "start": "1299869",
    "end": "1305629"
  },
  {
    "text": "just because of how this data is distributed within each minute if we",
    "start": "1305629",
    "end": "1314089"
  },
  {
    "text": "shift that up to the 90th percentile we're now capturing everything except those spikes this could be a much better",
    "start": "1314089",
    "end": "1323379"
  },
  {
    "text": "time series to look at if we're trying to do something like capacity planning",
    "start": "1323379",
    "end": "1329319"
  },
  {
    "text": "particularly when you're looking at like network link utilization you might have",
    "start": "1329319",
    "end": "1336340"
  },
  {
    "text": "a target that like you know you want I don't know eighty eighty percent utilization is is",
    "start": "1336340",
    "end": "1342429"
  },
  {
    "text": "your target for the network so you can like actually handle bursts that are happening so looking at like the p90 of",
    "start": "1342429",
    "end": "1351340"
  },
  {
    "text": "you know a higher resolution time series might might give you a better idea of what you're actually using while still",
    "start": "1351340",
    "end": "1359650"
  },
  {
    "text": "allowing for some bursts that are happening outside of that and here we've",
    "start": "1359650",
    "end": "1365559"
  },
  {
    "text": "abandoned our secondly hidden entirely but we're reporting out multiple percentiles we have the min the P 1050",
    "start": "1365559",
    "end": "1374380"
  },
  {
    "text": "ninety and the max and we can really get a sense of how the CPU utilization",
    "start": "1374380",
    "end": "1383230"
  },
  {
    "text": "looked within each minute you can tell that we still have this like period of",
    "start": "1383230",
    "end": "1389919"
  },
  {
    "text": "increased CPU utilization at the end but now we're actually capturing these",
    "start": "1389919",
    "end": "1395230"
  },
  {
    "text": "spikes right our max is now reflecting sort of those peaks in utilization but",
    "start": "1395230",
    "end": "1406780"
  },
  {
    "text": "we can also tell that generally it's a lot lower than that right the the distance between the max and the p90 is",
    "start": "1406780",
    "end": "1412150"
  },
  {
    "text": "actually pretty intense so we can actually start telling that like hey our workload is spiky just based on the",
    "start": "1412150",
    "end": "1418270"
  },
  {
    "text": "ratio between these two time series particularly bursty workloads might be",
    "start": "1418270",
    "end": "1425409"
  },
  {
    "text": "an indicator that there's something to go in and look at and areas were like",
    "start": "1425409",
    "end": "1432340"
  },
  {
    "text": "you could you could benefit from from performance analysis and tuning the other really cool thing about this is",
    "start": "1432340",
    "end": "1439539"
  },
  {
    "text": "that instead of needing sixty times the amount of data to get secondly instead",
    "start": "1439539",
    "end": "1444640"
  },
  {
    "text": "of minute ly now we only need five times the amount of data to be stored and",
    "start": "1444640",
    "end": "1449830"
  },
  {
    "text": "aggregated and we have still a pretty good indicator of what the submitted Li",
    "start": "1449830",
    "end": "1456100"
  },
  {
    "text": "behaviors are also this is a lot easier to look at as a person I can kind of",
    "start": "1456100",
    "end": "1462580"
  },
  {
    "text": "look at this and make sense of it the fuzzy secondly time series is very very hard",
    "start": "1462580",
    "end": "1468970"
  },
  {
    "text": "especially with you know thousands of computers so to me this winds up being a",
    "start": "1468970",
    "end": "1476080"
  },
  {
    "text": "very interesting tool for for understanding our distribution without burning your eyes out looking at charts",
    "start": "1476080",
    "end": "1484680"
  },
  {
    "text": "so how can we make use of this we want to be able to sample at a high",
    "start": "1484680",
    "end": "1490930"
  },
  {
    "text": "resolution and we want to be able to produce these histograms of what the data looks like within a given window",
    "start": "1490930",
    "end": "1500290"
  },
  {
    "text": "essentially so you're doing serves like a moving histogram across time shoving",
    "start": "1500290",
    "end": "1506860"
  },
  {
    "text": "your values in there and then exporting these these summaries about what the",
    "start": "1506860",
    "end": "1512830"
  },
  {
    "text": "percentiles look like what the distribution of your data looks like so",
    "start": "1512830",
    "end": "1518470"
  },
  {
    "text": "this leads me to Rizla's so I had to write a tool to do this",
    "start": "1518470",
    "end": "1524740"
  },
  {
    "text": "resolute resinous does high-resolution",
    "start": "1524740",
    "end": "1529960"
  },
  {
    "text": "sampling of our underlying sources there's a metrics library that's shared",
    "start": "1529960",
    "end": "1535840"
  },
  {
    "text": "between restless and my benchmarking tool RPC perf and it's able to produce",
    "start": "1535840",
    "end": "1542230"
  },
  {
    "text": "these these summaries based on data that's inserted into it Restless gives",
    "start": "1542230",
    "end": "1549490"
  },
  {
    "text": "us the ability to sample from all those sources that we talked about earlier our",
    "start": "1549490",
    "end": "1555340"
  },
  {
    "text": "traditional counters about you know CPU utilization disk bandwidth stuff like that it can also sample performance",
    "start": "1555340",
    "end": "1563170"
  },
  {
    "text": "counters which gives us the ability to start to look at how efficiently our",
    "start": "1563170",
    "end": "1568780"
  },
  {
    "text": "code is running on our compute platform and it has eb PF support which is very",
    "start": "1568780",
    "end": "1576010"
  },
  {
    "text": "cool for being able to look at our workload and latency of these very",
    "start": "1576010",
    "end": "1581860"
  },
  {
    "text": "granular events resinous is able to",
    "start": "1581860",
    "end": "1587050"
  },
  {
    "text": "produce insightful metrics even if we're only externally aggregating on a minute",
    "start": "1587050",
    "end": "1593590"
  },
  {
    "text": "ly basis we get those hints about what our submitted lis",
    "start": "1593590",
    "end": "1598669"
  },
  {
    "text": "distribution looks like the eb PF support has been very cool for helping",
    "start": "1598669",
    "end": "1605840"
  },
  {
    "text": "us to actually measure what our workloads look like and be able to",
    "start": "1605840",
    "end": "1612169"
  },
  {
    "text": "capture the performance characteristics these types of things would be",
    "start": "1612169",
    "end": "1617509"
  },
  {
    "text": "unavailable to us otherwise there's there's just no way to do it except to trace these things eb PF is also very",
    "start": "1617509",
    "end": "1626480"
  },
  {
    "text": "low overhead just because it's it's running in the kernel and then you're just pulling the summary data over so it",
    "start": "1626480",
    "end": "1634639"
  },
  {
    "text": "winds up being fairly cheap to instrument these things that are happening all the time so we're able to",
    "start": "1634639",
    "end": "1643999"
  },
  {
    "text": "measure like our scheduler latency what our packet size distribution is our block i/o size distribution very very",
    "start": "1643999",
    "end": "1651139"
  },
  {
    "text": "important stuff when you're talking to a hardware vendor to you know try to optimize your workload on a given",
    "start": "1651139",
    "end": "1660049"
  },
  {
    "text": "platform or the platform for the hardware Rizla's is open source yay it's",
    "start": "1660049",
    "end": "1669950"
  },
  {
    "start": "1662000",
    "end": "1764000"
  },
  {
    "text": "available on github issues and pull requests are welcome I think that it's",
    "start": "1669950",
    "end": "1675950"
  },
  {
    "text": "been able to provide us a lot of interesting insight into how our systems",
    "start": "1675950",
    "end": "1681799"
  },
  {
    "text": "are running it's helped us to capture some bursts that we wouldn't have seen otherwise",
    "start": "1681799",
    "end": "1687129"
  },
  {
    "text": "and I think it's useful even for smaller environments right a lot a lot of times",
    "start": "1687129",
    "end": "1697909"
  },
  {
    "text": "had small as I've worked in small shops before I worked at Twitter and I really",
    "start": "1697909",
    "end": "1703429"
  },
  {
    "text": "wish I had a tool like this before at small shops like off and you don't have",
    "start": "1703429",
    "end": "1710059"
  },
  {
    "text": "the time to go write something like this so it's really great that Twitter",
    "start": "1710059",
    "end": "1715070"
  },
  {
    "text": "allowed me to open-source something like this so it can sort of become a community thing and oftentimes it's",
    "start": "1715070",
    "end": "1722960"
  },
  {
    "text": "smaller shops like your performance actually matters a lot because you don't necessarily have a huge budget to just",
    "start": "1722960",
    "end": "1728899"
  },
  {
    "text": "throw money at the problem so understanding how your system is performing and being able to",
    "start": "1728899",
    "end": "1735200"
  },
  {
    "text": "diagnose runtime performance issues is actually very critical at Twitter scale",
    "start": "1735200",
    "end": "1744160"
  },
  {
    "text": "even just like a percent is a very large number of dollars so we want to be able",
    "start": "1744160",
    "end": "1752300"
  },
  {
    "text": "to to squeeze the most out of our systems so in practice how has Rizla's",
    "start": "1752300",
    "end": "1760100"
  },
  {
    "text": "helped us going back to this it's",
    "start": "1760100",
    "end": "1765980"
  },
  {
    "start": "1764000",
    "end": "1862000"
  },
  {
    "text": "terrible terrible latency waterfall so we actually saw something like this in",
    "start": "1765980",
    "end": "1772730"
  },
  {
    "text": "production we were seeing these peaks in",
    "start": "1772730",
    "end": "1778120"
  },
  {
    "text": "actually we measured it as CPU utilization what we're looking at I've been production and periodically the CPU",
    "start": "1778120",
    "end": "1786770"
  },
  {
    "text": "utilization of the cache instances was was bursting Restless in addition to",
    "start": "1786770",
    "end": "1794180"
  },
  {
    "text": "providing this histogram distribution sort of thing also looks for the peak",
    "start": "1794180",
    "end": "1801970"
  },
  {
    "text": "within each rolling interval and will report out the offset from the top of a",
    "start": "1801970",
    "end": "1809540"
  },
  {
    "text": "second that the peak occurred and that can be very useful to start correlating",
    "start": "1809540",
    "end": "1815690"
  },
  {
    "text": "with your logs and your other like tracing data within within your environment and help to really sort of",
    "start": "1815690",
    "end": "1822770"
  },
  {
    "text": "narrow down what you're looking at so Rizla's was able to identify this peak",
    "start": "1822770",
    "end": "1829580"
  },
  {
    "text": "in CPU utilization which was twice the",
    "start": "1829580",
    "end": "1834740"
  },
  {
    "text": "base law that twice what the baseline was and it had a fixed offset in a",
    "start": "1834740",
    "end": "1839960"
  },
  {
    "text": "minute we narrowed it down to a background task that was running and",
    "start": "1839960",
    "end": "1845990"
  },
  {
    "text": "causing causing this sort of impact and for the particular case of cache we were",
    "start": "1845990",
    "end": "1852740"
  },
  {
    "text": "able to just eliminate this background task and that made our latency go back",
    "start": "1852740",
    "end": "1860480"
  },
  {
    "text": "down to normal Reza's also helped us detect CPU",
    "start": "1860480",
    "end": "1866600"
  },
  {
    "start": "1862000",
    "end": "1915000"
  },
  {
    "text": "saturation that happened on a sub Manila basis so this is something where if you",
    "start": "1866600",
    "end": "1872990"
  },
  {
    "text": "have minute ly data you're just not going to see this at all because it warns up getting smoothed out by this",
    "start": "1872990",
    "end": "1879220"
  },
  {
    "text": "sort of low-pass filter that the Nyquist Shannon theorem talks about and Restless",
    "start": "1879220",
    "end": "1889040"
  },
  {
    "text": "was able to detect this in production and capture and reflect the CPU",
    "start": "1889040",
    "end": "1894470"
  },
  {
    "text": "saturation that was happening and that helped the backend team actually go in",
    "start": "1894470",
    "end": "1900950"
  },
  {
    "text": "and identify that oh hey yeah you know our upstream is sending us this you know",
    "start": "1900950",
    "end": "1906710"
  },
  {
    "text": "bursts of traffic at that period of time and they were able to work with the the upstream service to smooth that out and",
    "start": "1906710",
    "end": "1913190"
  },
  {
    "text": "have it not be so spiky so in summary",
    "start": "1913190",
    "end": "1920380"
  },
  {
    "start": "1915000",
    "end": "2012000"
  },
  {
    "text": "there are many many sources of telemetry to understand our systems performance",
    "start": "1920380",
    "end": "1928450"
  },
  {
    "text": "sampling resolution is very important otherwise you're going to miss these small things that actually do matter in",
    "start": "1928450",
    "end": "1936890"
  },
  {
    "text": "process summarization can reduce your cost of aggregation and storage you know",
    "start": "1936890",
    "end": "1943520"
  },
  {
    "text": "instead of taking you know 60 times the amount of data to store a secondly time",
    "start": "1943520",
    "end": "1949970"
  },
  {
    "text": "series you can just export like maybe five percentiles you can actually even",
    "start": "1949970",
    "end": "1958460"
  },
  {
    "text": "start the savings becomes even more apparent when you're sampling at even higher rates like you might want to",
    "start": "1958460",
    "end": "1964010"
  },
  {
    "text": "sample every you know 100 millisecond or ten times per second or something like that you might want to say I believe in",
    "start": "1964010",
    "end": "1971150"
  },
  {
    "text": "faster than that for certain things really it all goes back to what is the",
    "start": "1971150",
    "end": "1976670"
  },
  {
    "text": "smallest thing I want to be able to capture and as you start multiplying how",
    "start": "1976670",
    "end": "1984110"
  },
  {
    "text": "much data you need within a second the the savings just becomes even more and",
    "start": "1984110",
    "end": "1991870"
  },
  {
    "text": "having high-resolution telemetry can help us diagnose run time for formants issues as well as steer",
    "start": "1991870",
    "end": "1999130"
  },
  {
    "text": "optimization and performance tuning efforts and res lists has helped us at",
    "start": "1999130",
    "end": "2006490"
  },
  {
    "text": "twitter to address these needs and it's again available on github and now i have",
    "start": "2006490",
    "end": "2015100"
  },
  {
    "start": "2012000",
    "end": "2345000"
  },
  {
    "text": "some time for Q&A there is a mic on a stand here in the center and on each end",
    "start": "2015100",
    "end": "2022810"
  },
  {
    "text": "by the projector tables there are mics so if anyone has questions please PLEASE",
    "start": "2022810",
    "end": "2030730"
  },
  {
    "text": "ask",
    "start": "2030730",
    "end": "2032970"
  },
  {
    "text": "hello are you able to hear me okay so my question is you mentioned this",
    "start": "2037290",
    "end": "2044020"
  },
  {
    "text": "[Music] sub-second sampling so what are the",
    "start": "2044020",
    "end": "2052270"
  },
  {
    "text": "tools you use to sample set CPU utilization in microseconds or you know",
    "start": "2052270",
    "end": "2059919"
  },
  {
    "text": "disk usage in microseconds what are those basic tools that you are using so",
    "start": "2059919",
    "end": "2066240"
  },
  {
    "text": "those those basic your traditional telemetry sources are exposed by Paco",
    "start": "2066240",
    "end": "2073090"
  },
  {
    "text": "fest and Sisyphus so you're able to just kind of open open those files and read",
    "start": "2073090",
    "end": "2079540"
  },
  {
    "text": "out of them periodically okay so broke FS okay I but then you",
    "start": "2079540",
    "end": "2088090"
  },
  {
    "text": "need root access maybe or is something that any person can so that kind of",
    "start": "2088090",
    "end": "2094629"
  },
  {
    "text": "stuff actually does not need root access when you start looking at things like",
    "start": "2094630",
    "end": "2099840"
  },
  {
    "text": "perfo dents you do need sysadmin",
    "start": "2099840",
    "end": "2106780"
  },
  {
    "text": "privileges essentially you need root access or for the task for the binary to",
    "start": "2106780",
    "end": "2113200"
  },
  {
    "text": "have caps sysadmin would be the capability and EBP F also requires",
    "start": "2113200",
    "end": "2122070"
  },
  {
    "text": "higher level access because you're injecting code into the kernel and actually there's something in",
    "start": "2122070",
    "end": "2130240"
  },
  {
    "text": "I forget what recent curl version but they're starting to lock down things like that and by default there's still",
    "start": "2130240",
    "end": "2138040"
  },
  {
    "text": "it seems like they're still trying to work out how to deal with that exactly okay another question is like how how",
    "start": "2138040",
    "end": "2146680"
  },
  {
    "text": "you do it like I don't think own productions you can do it I do have to come up with a system that is very",
    "start": "2146680",
    "end": "2153640"
  },
  {
    "text": "similar to production and where you run these tests to make sure that or are you running on the production itself we run",
    "start": "2153640",
    "end": "2160630"
  },
  {
    "text": "Restless on our production fleet okay having having the tooling so we're able",
    "start": "2160630",
    "end": "2167080"
  },
  {
    "text": "to rapidly identify runtime performance issues and give teams insights to help",
    "start": "2167080",
    "end": "2175780"
  },
  {
    "text": "diagnose those issues and root cause them and resolve them is definitely",
    "start": "2175780",
    "end": "2183340"
  },
  {
    "text": "worthwhile restless winds up being not",
    "start": "2183340",
    "end": "2189400"
  },
  {
    "text": "not super expensive to to run actually I'm trying to think of it takes about 5%",
    "start": "2189400",
    "end": "2200950"
  },
  {
    "text": "of a single-core and maybe 50 megabytes of resident memory to sample at I",
    "start": "2200950",
    "end": "2208360"
  },
  {
    "text": "believe that's for 10 times per second with all the default samplers so none of",
    "start": "2208360",
    "end": "2213730"
  },
  {
    "text": "the eb PF functionality but all of the perf and all of the traditional systems",
    "start": "2213730",
    "end": "2220090"
  },
  {
    "text": "performance telemetry and what i i think is is a very small footprint and",
    "start": "2220090",
    "end": "2226410"
  },
  {
    "text": "definitely the insight is worth whatever cost that has to us thank you",
    "start": "2226410",
    "end": "2235590"
  },
  {
    "text": "yeah come up and ask your example the",
    "start": "2239470",
    "end": "2247010"
  },
  {
    "text": "superfluous opossums something is in",
    "start": "2247010",
    "end": "2255560"
  },
  {
    "text": "terms of statistics and if it does so",
    "start": "2255560",
    "end": "2264640"
  },
  {
    "text": "that's a that's a very interesting question about whether Rizla's records",
    "start": "2264640",
    "end": "2272540"
  },
  {
    "text": "as it as it detects anomalies and that's something that we've been talking about",
    "start": "2272540",
    "end": "2279380"
  },
  {
    "text": "internally and I think would be a very cool feature to add one could imagine",
    "start": "2279380",
    "end": "2288010"
  },
  {
    "text": "brazilís being able to use that sort of like metrics library to easily detect",
    "start": "2288010",
    "end": "2293210"
  },
  {
    "text": "that something abnormal is happening and either increase its sampling resolution",
    "start": "2293210",
    "end": "2298580"
  },
  {
    "text": "or dump a trace out res list winds up in a very interesting position in terms of",
    "start": "2298580",
    "end": "2306640"
  },
  {
    "text": "observability in telemetry and stuff like that where it could pretty easily",
    "start": "2306640",
    "end": "2312820"
  },
  {
    "text": "have like an on disk buffer of this",
    "start": "2312820",
    "end": "2319460"
  },
  {
    "text": "telemetry available at very high resolution this hasn't been implemented yet but is definitely a direction that I",
    "start": "2319460",
    "end": "2326720"
  },
  {
    "text": "want to be able to take the project in yeah so great question okay",
    "start": "2326720",
    "end": "2337300"
  },
  {
    "text": "[Music] yeah so is it possible to extend Rizla's",
    "start": "2343550",
    "end": "2349130"
  },
  {
    "start": "2345000",
    "end": "2830000"
  },
  {
    "text": "to provide telemetry on business KPIs as opposed to just systems KPIs",
    "start": "2349130",
    "end": "2357430"
  },
  {
    "text": "the answer is yes so res lists also has this mode that we",
    "start": "2357430",
    "end": "2365630"
  },
  {
    "text": "use on some of our production caches so the memcache compatible servers so we",
    "start": "2365630",
    "end": "2375319"
  },
  {
    "text": "use memcache at Twitter which is our fork exposes a there's a stats thing",
    "start": "2375319",
    "end": "2384289"
  },
  {
    "text": "built into the protocol and the Restless can actually sort of sit in between and",
    "start": "2384289",
    "end": "2390730"
  },
  {
    "text": "sample at higher resolution the underlying stat source and then expose",
    "start": "2390730",
    "end": "2396770"
  },
  {
    "text": "these percentiles about that so we actually use that on some of our larger",
    "start": "2396770",
    "end": "2403789"
  },
  {
    "text": "production caches to help us capture peaks in QPS and what the offset is into",
    "start": "2403789",
    "end": "2410599"
  },
  {
    "text": "the minute or yeah so yeah one could",
    "start": "2410599",
    "end": "2416329"
  },
  {
    "text": "imagine extending that to capture data from any sort of standard metrics x",
    "start": "2416329",
    "end": "2424190"
  },
  {
    "text": "position endpoint and ingest that into restless and then exposed those those",
    "start": "2424190",
    "end": "2431510"
  },
  {
    "text": "percentiles so there would be probably",
    "start": "2431510",
    "end": "2439339"
  },
  {
    "text": "some development work needed that would actually be I would call that almost",
    "start": "2439339",
    "end": "2445190"
  },
  {
    "text": "trivial if one we're familiar with the code base it would definitely be trivial",
    "start": "2445190",
    "end": "2450380"
  },
  {
    "text": "[Music] but yeah there's already sort of a little bit of that that framework in",
    "start": "2450380",
    "end": "2456170"
  },
  {
    "text": "there just from being able to ingest stats from from twelve cache but it would need to be extended to pull from",
    "start": "2456170",
    "end": "2462829"
  },
  {
    "text": "you know HTTP and parse JSON or something like that you know whatever the the metrics x position format is",
    "start": "2462829",
    "end": "2469950"
  },
  {
    "text": "this is actually another thing that we've been talking about internally because Twitter uses finagle for a lot",
    "start": "2469950",
    "end": "2479099"
  },
  {
    "text": "of things and our kind of standard stack exposes metrics being HTTP endpoint and",
    "start": "2479099",
    "end": "2486240"
  },
  {
    "text": "the ability to sort of sit in between our traditional collector and the",
    "start": "2486240",
    "end": "2491700"
  },
  {
    "text": "application and provide this sort of increased insight into things without",
    "start": "2491700",
    "end": "2497579"
  },
  {
    "text": "that high cost of aggregating is very",
    "start": "2497579",
    "end": "2502770"
  },
  {
    "text": "interesting to us so that I think is work that is likely to happen although I",
    "start": "2502770",
    "end": "2512030"
  },
  {
    "text": "there are there some competing priorities right now but yeah if that",
    "start": "2512030",
    "end": "2519540"
  },
  {
    "text": "would be the the kind of stuff that I would definitely welcome a pull request on",
    "start": "2519540",
    "end": "2526160"
  },
  {
    "text": "so the the question was about the resource footprint of restless and sort",
    "start": "2560840",
    "end": "2567270"
  },
  {
    "text": "of the the trade-off between running it all the time and getting that telemetry",
    "start": "2567270",
    "end": "2572420"
  },
  {
    "text": "from production systems first sort of the cost and you know do we do sampling or run it everywhere all the time my",
    "start": "2572420",
    "end": "2581850"
  },
  {
    "text": "goal with the footprint that it has is that it could be run everywhere all the",
    "start": "2581850",
    "end": "2588510"
  },
  {
    "text": "time in production I think that we",
    "start": "2588510",
    "end": "2596310"
  },
  {
    "text": "always have you know telemetry agents running Frank and I think that Restless",
    "start": "2596310",
    "end": "2602910"
  },
  {
    "text": "can actually take over a lot of what our current agent is doing and fit within",
    "start": "2602910",
    "end": "2609480"
  },
  {
    "text": "that footprint and still give us this increased resolution and yeah I'm trying",
    "start": "2609480",
    "end": "2620340"
  },
  {
    "text": "to think of what the what the current percentage of rollout is but the the",
    "start": "2620340",
    "end": "2626820"
  },
  {
    "text": "goal is to run it everywhere all the time in some sort of resource",
    "start": "2626820",
    "end": "2632370"
  },
  {
    "text": "constrained areas so if you think about like containerization environments where",
    "start": "2632370",
    "end": "2638670"
  },
  {
    "text": "you have you know basically like a small",
    "start": "2638670",
    "end": "2645930"
  },
  {
    "text": "system slice dedicated that isn't isn't running containers that's where things",
    "start": "2645930",
    "end": "2651180"
  },
  {
    "text": "become a little more resource constrained but again I believe that",
    "start": "2651180",
    "end": "2656450"
  },
  {
    "text": "Restless can help take away some of that work that's being done and we'll be able",
    "start": "2656450",
    "end": "2662790"
  },
  {
    "text": "to actually get rid of existing agents",
    "start": "2662790",
    "end": "2668510"
  },
  {
    "text": "things like you know the the resource footprint of things like you know Python",
    "start": "2668510",
    "end": "2674910"
  },
  {
    "text": "telemetry agents and stuff like that it just gets really unpredictable brazilís",
    "start": "2674910",
    "end": "2680580"
  },
  {
    "text": "is in in rusts so the the memory footprints actually vary",
    "start": "2680580",
    "end": "2686220"
  },
  {
    "text": "easy to predict the CPU utilization is pretty constant doesn't have any you",
    "start": "2686220",
    "end": "2691470"
  },
  {
    "text": "know GC overheads or stuff like that so I think that we'll be able to shift",
    "start": "2691470",
    "end": "2698420"
  },
  {
    "text": "where we're spending resource and be able to run the res lists everywhere all the time",
    "start": "2698420",
    "end": "2703560"
  },
  {
    "text": "I think that we will be looking into things like dynamically increasing the",
    "start": "2703560",
    "end": "2709430"
  },
  {
    "text": "sampling resolution having an on disk buffer being able to capture tracing",
    "start": "2709430",
    "end": "2714660"
  },
  {
    "text": "information possibly integrated into our distributed tracing tool Zipkin stuff",
    "start": "2714660",
    "end": "2719790"
  },
  {
    "text": "like that and be able to start tying all these pieces together but yeah",
    "start": "2719790",
    "end": "2725160"
  },
  {
    "text": "everywhere all the time on yeah it is on",
    "start": "2725160",
    "end": "2731490"
  },
  {
    "text": "ok can you go into a little more detail about ok you use rez list you find a",
    "start": "2731490",
    "end": "2738330"
  },
  {
    "text": "spike in performance and then how do we magically map that back to what's going",
    "start": "2738330",
    "end": "2743460"
  },
  {
    "text": "on what applications causing the performance yeah that is where things",
    "start": "2743460",
    "end": "2751890"
  },
  {
    "text": "become more art than science so typically I've had to go in and trace",
    "start": "2751890",
    "end": "2762570"
  },
  {
    "text": "on the individual system and really look to understand what's causing that we we",
    "start": "2762570",
    "end": "2772230"
  },
  {
    "text": "can look at things like you know which container is using the resource at that point of time but I think at some point",
    "start": "2772230",
    "end": "2782280"
  },
  {
    "text": "you wind up falling back onto other tools while while you're doing root",
    "start": "2782280",
    "end": "2789060"
  },
  {
    "text": "cause analysis and those might be things like getting perf record data that might",
    "start": "2789060",
    "end": "2797160"
  },
  {
    "text": "be things like looking at your application logs and just kind of seeing what sort of requests were coming in at",
    "start": "2797160",
    "end": "2803460"
  },
  {
    "text": "that point in time or doing a more analysis of the log file",
    "start": "2803460",
    "end": "2810020"
  },
  {
    "text": "cool well thank you all for coming to the talk and you'll find me floating",
    "start": "2816900",
    "end": "2823170"
  },
  {
    "text": "around and can always come up and ask me questions later - thank you [Applause]",
    "start": "2823170",
    "end": "2831419"
  }
]