[
  {
    "start": "0",
    "end": "465000"
  },
  {
    "text": "foreign [Music]",
    "start": "1380",
    "end": "16100"
  },
  {
    "text": "how many of you ever wanted a database that provides predictable performance higher availability and is fully managed",
    "start": "16100",
    "end": "25980"
  },
  {
    "text": "awesome I was expecting 100 of you will say yes so we are in the",
    "start": "25980",
    "end": "32398"
  },
  {
    "text": "right room and today what I'm going to do in this talk is talk about evolution",
    "start": "32399",
    "end": "39000"
  },
  {
    "text": "of a hyperscale cloud database service which is dynamodb",
    "start": "39000",
    "end": "44300"
  },
  {
    "text": "talk through the lessons that we have learned over the years while building this hyperscale database",
    "start": "44300",
    "end": "51660"
  },
  {
    "text": "and I am akshatwig I'm a principal engineer in Amazon danmodb team",
    "start": "51660",
    "end": "58620"
  },
  {
    "text": "and I've been with dynamodb right from its Inception so with that let's jump in",
    "start": "58620",
    "end": "65338"
  },
  {
    "text": "AWS offers 15 plus purpose built database engines to support diverse data",
    "start": "65339",
    "end": "72780"
  },
  {
    "text": "models including relational in-memory document graph time series so the idea",
    "start": "72780",
    "end": "79140"
  },
  {
    "text": "is that you as a customer can choose the right tool for the use case that you are",
    "start": "79140",
    "end": "84960"
  },
  {
    "text": "trying to solve and in this talk we are zooming in into dynamodb which is a key value database",
    "start": "84960",
    "end": "93240"
  },
  {
    "text": "so first question that comes to mind is why Dynamo leaving and you know let's go",
    "start": "93240",
    "end": "98280"
  },
  {
    "text": "back to history during 2004 2005 time frame amazon.com",
    "start": "98280",
    "end": "103799"
  },
  {
    "text": "was facing scaling challenges caused by the relational database that the website",
    "start": "103799",
    "end": "109740"
  },
  {
    "text": "was using and at Amazon whenever we have these service disruptions one thing we do as a",
    "start": "109740",
    "end": "116700"
  },
  {
    "text": "habit as a culture is you know we do Coes which are basically correction of errors and in that Coe we ask questions",
    "start": "116700",
    "end": "124320"
  },
  {
    "text": "how can we make sure that the issue that happened does not happen again and you",
    "start": "124320",
    "end": "129660"
  },
  {
    "text": "know the use case that for which we were with that particular outage happened was",
    "start": "129660",
    "end": "135900"
  },
  {
    "text": "related to a shopping cart so one of the questions that we asked in the cue was why are we using the SQL database for",
    "start": "135900",
    "end": "142620"
  },
  {
    "text": "this specific use case what are the SQL capabilities that are actually needed and turns out you know not many and",
    "start": "142620",
    "end": "148940"
  },
  {
    "text": "choosing the right database technology is the key to build a system for scale",
    "start": "148940",
    "end": "155580"
  },
  {
    "text": "and predictable performance and at that time when we asked this question if not an SQL database what exactly would we do",
    "start": "155580",
    "end": "162300"
  },
  {
    "text": "at that time no other database technology existed that met the requirements that we had for the",
    "start": "162300",
    "end": "168840"
  },
  {
    "text": "shopping cart use case so Amazon created Dynamo and it was",
    "start": "168840",
    "end": "174540"
  },
  {
    "text": "between 2004 to 2007 where Dynamo was created and finally in 2007 we published",
    "start": "174540",
    "end": "180660"
  },
  {
    "text": "the Dynamo paper after letting it run uh in production and used by not just the",
    "start": "180660",
    "end": "187140"
  },
  {
    "text": "shopping cart use case but multiple amazon.com services so Dynamo was created in response to the",
    "start": "187140",
    "end": "195060"
  },
  {
    "text": "need for a highly available scalable and you know durable key value data database",
    "start": "195060",
    "end": "201000"
  },
  {
    "text": "for the shopping cart and then more and more team started using it so Dynamo was a software system that you",
    "start": "201000",
    "end": "210300"
  },
  {
    "text": "know teams had to take run the installations on resources that were",
    "start": "210300",
    "end": "215400"
  },
  {
    "text": "owned by them and it became really really popular in inside Amazon",
    "start": "215400",
    "end": "222720"
  },
  {
    "text": "um within like multiple teams and you know hearing this one thing we heard",
    "start": "222720",
    "end": "227879"
  },
  {
    "text": "from all these teams is that hey Dynamo is amazing but what if you make that as a service so that a lot of teams who are",
    "start": "227879",
    "end": "236940"
  },
  {
    "text": "trying to become experts in you know running these Dynamo installations it becomes easier that led to the launch of",
    "start": "236940",
    "end": "243120"
  },
  {
    "text": "Nano living and Dynamo and dynamodb like",
    "start": "243120",
    "end": "248340"
  },
  {
    "text": "they are different so Amazon dynamodb is a result of everything we have learned about",
    "start": "248340",
    "end": "254879"
  },
  {
    "text": "building scalable large scale um you know databases at Amazon and you",
    "start": "254879",
    "end": "260699"
  },
  {
    "text": "know it has evolved based on the experiences that we have learned while",
    "start": "260699",
    "end": "266040"
  },
  {
    "text": "building these services and there are differences between Dynamo and dynamodb for example Dynamo as I was saying it it",
    "start": "266040",
    "end": "273120"
  },
  {
    "text": "was single tenant as a team you would run an installation you would own the resources that are used to run that",
    "start": "273120",
    "end": "279419"
  },
  {
    "text": "service dynamodb is multi-tenant you it's it's basically serverless",
    "start": "279419",
    "end": "284960"
  },
  {
    "text": "Dynamo provides tunable consistency dynamodb is opinionated about it and",
    "start": "284960",
    "end": "290400"
  },
  {
    "text": "provides strong and eventual consistency Dynamo it prefers availability over",
    "start": "290400",
    "end": "297180"
  },
  {
    "text": "consistency versus dynamodb prefers consistency over availability in Dynamo",
    "start": "297180",
    "end": "303180"
  },
  {
    "text": "routing and storage are coupled and we'll see in the stock routing and",
    "start": "303180",
    "end": "308820"
  },
  {
    "text": "storage in dynamolib are decoupled last custom conflict resolution was something",
    "start": "308820",
    "end": "315479"
  },
  {
    "text": "that was supported in lanimo in dynamodb we have last fighter wins so there are",
    "start": "315479",
    "end": "321720"
  },
  {
    "text": "differences between Dynamo and dynamodb so coming back to the question of like",
    "start": "321720",
    "end": "327300"
  },
  {
    "text": "why um why dynamodb and if you ask this question today 10 Years Later customer",
    "start": "327300",
    "end": "332699"
  },
  {
    "text": "still will say they want consistent performance they want better performance they want a fully managed serverless",
    "start": "332699",
    "end": "339960"
  },
  {
    "text": "experience they want higher availability for for on their service and you know we",
    "start": "339960",
    "end": "346259"
  },
  {
    "text": "are seeing that like consistent performance at scale Dynamo this is one of the key durable tenets of dynamooky",
    "start": "346259",
    "end": "353639"
  },
  {
    "text": "properties that Dynamo provides that as Dynamo is being adopted by hundreds of",
    "start": "353639",
    "end": "359340"
  },
  {
    "text": "thousands of customers and as the requests are increasing even though request rates are increasing customers",
    "start": "359340",
    "end": "366060"
  },
  {
    "text": "who are running Mission critical workloads on dynamodb the performance they are seeing is you",
    "start": "366060",
    "end": "373320"
  },
  {
    "text": "know consistent they are getting consistent performance at scale and proof is in the pudding like one of the",
    "start": "373320",
    "end": "379020"
  },
  {
    "text": "customers zoom in early 2020 when they saw like unprecedented usage That Grew",
    "start": "379020",
    "end": "385979"
  },
  {
    "text": "From 10 million to 300 million daily meeting participants Dynamo DB was able",
    "start": "385979",
    "end": "392340"
  },
  {
    "text": "to scale with just a click of a button and still provide predictable",
    "start": "392340",
    "end": "397440"
  },
  {
    "text": "performance to them quite amazing right now",
    "start": "397440",
    "end": "403440"
  },
  {
    "text": "Dynamo DB is fully managed what does it mean like dynamodb I would say was",
    "start": "403440",
    "end": "408600"
  },
  {
    "text": "serverless even before the term serverless was coined um you pay for whatever you use in",
    "start": "408600",
    "end": "416460"
  },
  {
    "text": "dynamod EB um you can scale down to zero essentially if you're not sending any requests you don't get charged for you",
    "start": "416460",
    "end": "422759"
  },
  {
    "text": "know whatever you're doing um it is built with separation of storage and compute as a customer",
    "start": "422759",
    "end": "430440"
  },
  {
    "text": "in case you run into like logical Corruptions where you accidentally deleted some of the items or deleted",
    "start": "430440",
    "end": "437460"
  },
  {
    "text": "your table you can do a restore um and Dynamo also provides you know Global active active replication where",
    "start": "437460",
    "end": "445319"
  },
  {
    "text": "you have use cases where you want the data closer to the user so you can run",
    "start": "445319",
    "end": "450500"
  },
  {
    "text": "dynamodb table as a global table on availability Dynamo offers an SLA of",
    "start": "450500",
    "end": "457020"
  },
  {
    "text": "four lines of availability for a single Regional setup and if you have a global table then you get five nines of",
    "start": "457020",
    "end": "463440"
  },
  {
    "text": "availability and just talking about like magnitude of scale to understand that like",
    "start": "463440",
    "end": "471740"
  },
  {
    "text": "amazon.com being one of the customers of dynamodb",
    "start": "471740",
    "end": "477319"
  },
  {
    "text": "2022 Prime day they generated like amazon.com and all",
    "start": "477319",
    "end": "483780"
  },
  {
    "text": "the different websites 105.2 million requests per second and",
    "start": "483780",
    "end": "489720"
  },
  {
    "text": "this is just one customer this can help you understand the magnitude at which dynamodb runs and throughout all this",
    "start": "489720",
    "end": "498120"
  },
  {
    "text": "they saw you know predictable single digit millisecond performance",
    "start": "498120",
    "end": "503460"
  },
  {
    "text": "and it's not just amazon.com hundreds of thousands of customers have chosen Nano",
    "start": "503460",
    "end": "509460"
  },
  {
    "text": "DB to run their mission critical workloads so introduction of Dynamo DB how is it",
    "start": "509460",
    "end": "517800"
  },
  {
    "text": "different from Dynamo what properties are kind of the durable tenants of the",
    "start": "517800",
    "end": "523020"
  },
  {
    "text": "service now let's look at how it has evolved over the years right so Dynamo DB over",
    "start": "523020",
    "end": "529500"
  },
  {
    "text": "the years it was launched in 2012 working backward from customers that's",
    "start": "529500",
    "end": "534540"
  },
  {
    "text": "how Amazon operates it started as a key values too right we",
    "start": "534540",
    "end": "539940"
  },
  {
    "text": "first launched 2012 dynamodb where you as a customer can do put gets and it",
    "start": "539940",
    "end": "545339"
  },
  {
    "text": "scales foundationally very strong then we started hearing from customers hey we",
    "start": "545339",
    "end": "551040"
  },
  {
    "text": "want more you know query capabilities search capabilities in dynamodb and we added indexing",
    "start": "551040",
    "end": "558360"
  },
  {
    "text": "then customers started asking about Json documents we added that so that they can now preserve complex complex and",
    "start": "558360",
    "end": "565380"
  },
  {
    "text": "possibly nested structures um inside inside danmodb items and then",
    "start": "565380",
    "end": "571100"
  },
  {
    "text": "2015 a lot of customers were asking us hey can you provide us materialized",
    "start": "571100",
    "end": "576420"
  },
  {
    "text": "views can you provide us backup restore can you provide us you know Global replication so we said okay let's take a",
    "start": "576420",
    "end": "582839"
  },
  {
    "text": "step back figure out what common building block we need to build all these different things that customers",
    "start": "582839",
    "end": "588779"
  },
  {
    "text": "are asking and we launched dynamodb stream so that you know by the time we",
    "start": "588779",
    "end": "594899"
  },
  {
    "text": "build all these native features inside Dynamo customers can innovate on their own and a lot of customers actually used",
    "start": "594899",
    "end": "601980"
  },
  {
    "text": "um like the basic scan operation and streams to innovate on their own um",
    "start": "601980",
    "end": "607680"
  },
  {
    "text": "and most recently we launched like um easier ingestion of data into Dynamo",
    "start": "607680",
    "end": "612720"
  },
  {
    "text": "or easier export of data from Dynamo so over the years the ask from customers",
    "start": "612720",
    "end": "618420"
  },
  {
    "text": "around features predictable performance availability durability that has been like constant",
    "start": "618420",
    "end": "625920"
  },
  {
    "start": "625000",
    "end": "1105000"
  },
  {
    "text": "so how does you must be wondering how does dynamodb scale and provide predictable performance let's you know",
    "start": "625920",
    "end": "632040"
  },
  {
    "text": "try to understand this particular aspect of Dynamo by understanding how exactly a",
    "start": "632040",
    "end": "638519"
  },
  {
    "text": "put item request works so as a client you send a request you might be either",
    "start": "638519",
    "end": "645000"
  },
  {
    "text": "in Amazon ec2 Network or somewhere on the internet it doesn't matter your request as soon as you make a request to",
    "start": "645000",
    "end": "651959"
  },
  {
    "text": "do a put item it lands on the request router now request router",
    "start": "651959",
    "end": "657620"
  },
  {
    "text": "is the first service that you hit and as",
    "start": "657620",
    "end": "663120"
  },
  {
    "text": "every AWS call this call is authenticated and authorized using IM",
    "start": "663120",
    "end": "668180"
  },
  {
    "text": "once we once the request is authenticated and authorized then we",
    "start": "668180",
    "end": "673560"
  },
  {
    "text": "look at the metadata we try to figure out where exactly do we need to Route the request right because the address of",
    "start": "673560",
    "end": "680339"
  },
  {
    "text": "like where exactly the data this particular item is has to finally land",
    "start": "680339",
    "end": "685880"
  },
  {
    "text": "its store it is stored in a metadata service which is what the request router",
    "start": "685880",
    "end": "691440"
  },
  {
    "text": "concerns once it knows the answer where to Route the request next thing it does",
    "start": "691440",
    "end": "696779"
  },
  {
    "text": "is it basically verifies whether the table that the customer is",
    "start": "696779",
    "end": "702120"
  },
  {
    "text": "trying to use whether it has enough capacity if it has enough capacity the request is admitted in case the capacity",
    "start": "702120",
    "end": "709019"
  },
  {
    "text": "is not there request is rejected so this is basically admission control done at the request router layer",
    "start": "709019",
    "end": "715380"
  },
  {
    "text": "once all that goes through request is sent to the storage node and for every",
    "start": "715380",
    "end": "720860"
  },
  {
    "text": "item in Dynamo we maintain multiple copies of that data and dynamodb storage",
    "start": "720860",
    "end": "727620"
  },
  {
    "text": "nodes one of the storage node is the leader storage node and the other two",
    "start": "727620",
    "end": "733500"
  },
  {
    "text": "storage nodes are you know follower storage nodes so whenever you make a write request it goes to the leader",
    "start": "733500",
    "end": "740640"
  },
  {
    "text": "gets written on at least one more follower before the write is actually acknowledged back to the client",
    "start": "740640",
    "end": "748200"
  },
  {
    "text": "and we don't have like not just a single request router and not",
    "start": "748200",
    "end": "753300"
  },
  {
    "text": "just like three storage nodes the service consists of many thousand of these components",
    "start": "753300",
    "end": "761399"
  },
  {
    "text": "so whenever a client like makes a request",
    "start": "761399",
    "end": "766620"
  },
  {
    "text": "the request is routed to a specific storage node and sent to the request router and then send to the storage node and AWS",
    "start": "766620",
    "end": "773399"
  },
  {
    "text": "like just like a Well architected Service dynamodb is also designed to be fault",
    "start": "773399",
    "end": "780480"
  },
  {
    "text": "tolerant across multiple availability Zone and in each region there are basically",
    "start": "780480",
    "end": "787200"
  },
  {
    "text": "request router and storage nodes which are in three different availability zones and we maintain three different",
    "start": "787200",
    "end": "794220"
  },
  {
    "text": "copies of data for every item that you store in the dynamology table now request router essentially does a",
    "start": "794220",
    "end": "802500"
  },
  {
    "text": "metadata lookup to find out where exactly to Route the request and it",
    "start": "802500",
    "end": "808620"
  },
  {
    "text": "takes away the burden from the clients to do the routing that's what you know it means by when I said storage and",
    "start": "808620",
    "end": "817560"
  },
  {
    "text": "routing is decoupled that's what I meant that the clients now don't have to know about where to Route the request it is",
    "start": "817560",
    "end": "823440"
  },
  {
    "text": "all abstracted away in the request router so whenever the request router gets a",
    "start": "823440",
    "end": "829260"
  },
  {
    "text": "request it finds out the storage node that are hosting the data it will connect to the leader storage node the leader storage node submits the request",
    "start": "829260",
    "end": "835740"
  },
  {
    "text": "acknowledges and finally once it get an acknowledgment from one more replica it acknowledges it back to the client data",
    "start": "835740",
    "end": "842279"
  },
  {
    "text": "is duplicated at least two availability zones before it is acknowledged now dynamodb uses multi-pack source to",
    "start": "842279",
    "end": "849959"
  },
  {
    "text": "elect a leader and leader continuously heartbeats with its peers",
    "start": "849959",
    "end": "855480"
  },
  {
    "text": "and the reason it is doing it is that so that if a peer fails to hear heartbeats from a leader a",
    "start": "855480",
    "end": "863160"
  },
  {
    "text": "new leader can be elected so that availability is you know not impacted so",
    "start": "863160",
    "end": "868200"
  },
  {
    "text": "the goal is to reduce the failure detection and elect a new leader as soon as possible in case of failures",
    "start": "868200",
    "end": "875279"
  },
  {
    "text": "awesome so now we understand the scale at which Dynamo operates we understand",
    "start": "875279",
    "end": "880380"
  },
  {
    "text": "how the request routing logic works let's look at the logical construct the",
    "start": "880380",
    "end": "887820"
  },
  {
    "text": "table and how exactly dynamodb automatically scales as your traffic increases as your",
    "start": "887820",
    "end": "894720"
  },
  {
    "text": "data and data size increases in the dynamodb table so as a customer",
    "start": "894720",
    "end": "900079"
  },
  {
    "text": "dynamodb you create a table and each table you specify a partition key",
    "start": "900079",
    "end": "907980"
  },
  {
    "text": "now in this particular example each customer has a unique customer identifier and we are storing customer",
    "start": "907980",
    "end": "915120"
  },
  {
    "text": "information in this table so customer ID is your partition key and then you also",
    "start": "915120",
    "end": "920820"
  },
  {
    "text": "store other customer information like name City Etc in the item as other",
    "start": "920820",
    "end": "927240"
  },
  {
    "text": "attributes now dynamodb",
    "start": "927240",
    "end": "933000"
  },
  {
    "text": "scales by doing partitioning and how it exactly that happens is behind the",
    "start": "933000",
    "end": "938040"
  },
  {
    "text": "scenes whenever you make a call to dynamodb with the customer ID or",
    "start": "938040",
    "end": "943800"
  },
  {
    "text": "whatever is your partition key Dynamo runs a one-way hash and what the reason",
    "start": "943800",
    "end": "949139"
  },
  {
    "text": "for doing that one-way hash is that we can result it results in random distribution across the total hash page",
    "start": "949139",
    "end": "955860"
  },
  {
    "text": "associated with that table now one way hash it cannot be reversed it's possible it's not possible to essentially",
    "start": "955860",
    "end": "962360"
  },
  {
    "text": "determine the input from the hash output the hashing algorithm it results in",
    "start": "962360",
    "end": "967860"
  },
  {
    "text": "essentially highly randomized hash values even for inputs that are very",
    "start": "967860",
    "end": "973980"
  },
  {
    "text": "similar now a table is partitioned into smaller",
    "start": "973980",
    "end": "980519"
  },
  {
    "text": "segments based on the overall capacity that you have asked or the size",
    "start": "980519",
    "end": "986760"
  },
  {
    "text": "of the table and each partition it can it contains a contiguous range of key",
    "start": "986760",
    "end": "991800"
  },
  {
    "text": "value pairs for example in this case we have a green partition that has values roughly from zero to six similarly we",
    "start": "991800",
    "end": "999060"
  },
  {
    "text": "have the orange partition which has values from 9 to B and then you have the",
    "start": "999060",
    "end": "1006139"
  },
  {
    "text": "pink partition which has values from E to F right so essentially given a hashed",
    "start": "1006139",
    "end": "1011839"
  },
  {
    "text": "value of an item partition key a request router can determine which",
    "start": "1011839",
    "end": "1017540"
  },
  {
    "text": "hash segment that particular item falls into and from the partition metadata",
    "start": "1017540",
    "end": "1023000"
  },
  {
    "text": "service and you can find out the three storage nodes which are holding the copy of that particular item",
    "start": "1023000",
    "end": "1028699"
  },
  {
    "text": "and then send the request to that particular set of nodes",
    "start": "1028699",
    "end": "1034280"
  },
  {
    "text": "now as I explained previously we have three copies of data in three different",
    "start": "1034280",
    "end": "1039558"
  },
  {
    "text": "availability zones so if we have these three partitions essentially we have three green partitions in three",
    "start": "1039559",
    "end": "1045438"
  },
  {
    "text": "different zones three orange partitions and then three um pink partitions and all these all",
    "start": "1045439",
    "end": "1053419"
  },
  {
    "text": "these partitions the metadata about where exactly they these partitions are is stored in a metadata service and that",
    "start": "1053419",
    "end": "1060500"
  },
  {
    "text": "particular Marita is called as partition map and what a partition map look like looks",
    "start": "1060500",
    "end": "1067100"
  },
  {
    "text": "like it essentially is the key ranges that that partition store supports and then green one green to Green three",
    "start": "1067100",
    "end": "1073760"
  },
  {
    "text": "these are essentially the addresses of the three storage nodes where that data is actually where that partition is",
    "start": "1073760",
    "end": "1081500"
  },
  {
    "text": "actually hosted now think about it when Zoom comes and asks for 10 million read",
    "start": "1081500",
    "end": "1086900"
  },
  {
    "text": "capacity unit table we would essentially add more partitions if suddenly they",
    "start": "1086900",
    "end": "1092240"
  },
  {
    "text": "increase their throughput 200 million corresponding to that we would add more partitions update the metadata and",
    "start": "1092240",
    "end": "1099440"
  },
  {
    "text": "that's how nanomodb um scales now",
    "start": "1099440",
    "end": "1105919"
  },
  {
    "start": "1105000",
    "end": "1213000"
  },
  {
    "text": "what challenges are there I said dynamodb is a multi-tenant system",
    "start": "1105919",
    "end": "1113000"
  },
  {
    "text": "so what are the different challenges that come into picture that we have to solve what are the lessons that we have",
    "start": "1113000",
    "end": "1120080"
  },
  {
    "text": "learned to provide predictable performance and one of the common challenges in a multi-tenant system is",
    "start": "1120080",
    "end": "1127820"
  },
  {
    "text": "workload isolation because it's not just one customer that we have we have multiple customers and these customers",
    "start": "1127820",
    "end": "1135080"
  },
  {
    "text": "they are running they are their partitions are installed on these storage nodes which are multi-tenant",
    "start": "1135080",
    "end": "1141679"
  },
  {
    "text": "right if if not then right if isolation is not done right it can cause you know performance impact to these customers so",
    "start": "1141679",
    "end": "1148640"
  },
  {
    "text": "let's jump into how exactly we solve that so in the original version of dynamodb",
    "start": "1148640",
    "end": "1154460"
  },
  {
    "text": "that was released in 2012 customers explicitly specified the throughput that",
    "start": "1154460",
    "end": "1160700"
  },
  {
    "text": "the table required in terms of read capacity units and right capacity units",
    "start": "1160700",
    "end": "1166039"
  },
  {
    "text": "and that is you know combined that is what is called as provisioned throughput of the table",
    "start": "1166039",
    "end": "1172520"
  },
  {
    "text": "now if a customer is essentially reading an",
    "start": "1172520",
    "end": "1178039"
  },
  {
    "text": "item which is up to 4kb that means it has consumed one rate capacity units similarly if a customer is doing a right",
    "start": "1178039",
    "end": "1184580"
  },
  {
    "text": "of a 1kb item that would mean a the right capacity unit is consumed now",
    "start": "1184580",
    "end": "1191120"
  },
  {
    "text": "recall from the previous example for the customers table we had three partitions",
    "start": "1191120",
    "end": "1196340"
  },
  {
    "text": "right and if a customer asks for 300 read capacity",
    "start": "1196340",
    "end": "1201919"
  },
  {
    "text": "units in the original version of Dynamo what we would do is we would assign 100 rcus to each of the partitions",
    "start": "1201919",
    "end": "1209720"
  },
  {
    "text": "um so you have basically 300 rcus in total for your table and if your workload is uniform assuming",
    "start": "1209720",
    "end": "1218000"
  },
  {
    "start": "1213000",
    "end": "1321000"
  },
  {
    "text": "that your workload is uniform essentially your traffic is going to three different partitions at a uniform",
    "start": "1218000",
    "end": "1223700"
  },
  {
    "text": "rate to provide workload isolation dynamodb uses",
    "start": "1223700",
    "end": "1230120"
  },
  {
    "text": "token bucket algorithm and token bucket is to track the consumption of tokens of",
    "start": "1230120",
    "end": "1236960"
  },
  {
    "text": "the capacity that that that particular table has and a partition has an enforce basically",
    "start": "1236960",
    "end": "1245780"
  },
  {
    "text": "a ceiling for that now looking at one of these partitions we had token buckets at",
    "start": "1245780",
    "end": "1250880"
  },
  {
    "text": "a partition level in the original version of Dynamo and each second essentially we are refilling tokens at",
    "start": "1250880",
    "end": "1258440"
  },
  {
    "text": "the rate of the capacity assigned to the partition which is the bucket in this",
    "start": "1258440",
    "end": "1263720"
  },
  {
    "text": "particular case so when rcu's are used for read requests we are continuously",
    "start": "1263720",
    "end": "1269559"
  },
  {
    "text": "deducting them based on the consumption so if you do one request and we",
    "start": "1269559",
    "end": "1274580"
  },
  {
    "text": "basically consume one token from this bucket the bucket is getting refilled at a constant rate if the bucket is empty",
    "start": "1274580",
    "end": "1281059"
  },
  {
    "text": "obviously we cannot accept the request and we ask customers to try again",
    "start": "1281059",
    "end": "1287440"
  },
  {
    "text": "so overall let's say that the customer is doing like sending request and if",
    "start": "1287660",
    "end": "1293780"
  },
  {
    "text": "there are 100 tokens the request will get accepted for the green partition as soon as the token go as soon as the",
    "start": "1293780",
    "end": "1300799"
  },
  {
    "text": "consumed rate goes above 100 rcus in this particular example as soon as it reaches 101 RCU your request will get",
    "start": "1300799",
    "end": "1308600"
  },
  {
    "text": "rejected because there are no tokens that are left in that token bucket",
    "start": "1308600",
    "end": "1316460"
  },
  {
    "text": "so this is you know a high level idea of how token buckets could work now",
    "start": "1316460",
    "end": "1322280"
  },
  {
    "start": "1321000",
    "end": "1400000"
  },
  {
    "text": "what we found out when we launched dynamodb is that uniform distribution",
    "start": "1322280",
    "end": "1328240"
  },
  {
    "text": "for the workloads is very hard and it like uniform essentially getting uniform",
    "start": "1328240",
    "end": "1334940"
  },
  {
    "text": "workloads across for the full duration of when your application is running your",
    "start": "1334940",
    "end": "1340400"
  },
  {
    "text": "table exists it's very hard for customers to achieve that and that because the traffic tend to come in",
    "start": "1340400",
    "end": "1347120"
  },
  {
    "text": "waves or spikes for example let's take an example let's say you have you have an application which is for serving",
    "start": "1347120",
    "end": "1354679"
  },
  {
    "text": "coffee right you are you suddenly you will see that Spike happening early in",
    "start": "1354679",
    "end": "1360200"
  },
  {
    "text": "the morning right and then suddenly your traffic will increase as most of the customers get the coffee they go to",
    "start": "1360200",
    "end": "1366559"
  },
  {
    "text": "their office what you'll see is the traffic suddenly drops so traffic is not uniform it is",
    "start": "1366559",
    "end": "1373360"
  },
  {
    "text": "it basically changes with time right sometimes it is spiky sometimes there is",
    "start": "1373360",
    "end": "1378500"
  },
  {
    "text": "not much traffic in the system now if you create a table with 100 rcus",
    "start": "1378500",
    "end": "1383900"
  },
  {
    "text": "and you see a spike of traffic greater than 100 rcus then whatever is above 100",
    "start": "1383900",
    "end": "1390860"
  },
  {
    "text": "rcos will get rejected right so that's what it means that's what I mean by non-uniform traffic over time so",
    "start": "1390860",
    "end": "1398000"
  },
  {
    "text": "essentially what's happening is maybe your traffic is getting distributed across all the partitions or",
    "start": "1398000",
    "end": "1404299"
  },
  {
    "start": "1400000",
    "end": "1470000"
  },
  {
    "text": "maybe it's getting to a bunch of partitions but it is not uniform across",
    "start": "1404299",
    "end": "1409640"
  },
  {
    "text": "time which means if you have provisioned the table at 100 rcus any request that",
    "start": "1409640",
    "end": "1414799"
  },
  {
    "text": "is being sent above the 100 RCU limit it will all get rejected",
    "start": "1414799",
    "end": "1420799"
  },
  {
    "text": "another challenge what we saw was now customers to solve this problem of uh",
    "start": "1420799",
    "end": "1429200"
  },
  {
    "text": "you know seeing that they are they are getting",
    "start": "1429200",
    "end": "1434720"
  },
  {
    "text": "throttle to solve this problem of getting throttled what they did was they started provisioning for the peak and",
    "start": "1434720",
    "end": "1440419"
  },
  {
    "text": "instead of doing 100 rcus they would ask for 500 rcus which means that",
    "start": "1440419",
    "end": "1446480"
  },
  {
    "text": "it is able to handle the peak workload that they'll see in the day but at the same time for the rest of the day you",
    "start": "1446480",
    "end": "1453500"
  },
  {
    "text": "are seeing a lot of waste in the system right this meant a lot of capacity",
    "start": "1453500",
    "end": "1459080"
  },
  {
    "text": "unused a lot of capacity wasted which incurred cost to the customers and you",
    "start": "1459080",
    "end": "1465559"
  },
  {
    "text": "know customers um asked to ask to us hey can you solve this problem",
    "start": "1465559",
    "end": "1471080"
  },
  {
    "start": "1470000",
    "end": "1549000"
  },
  {
    "text": "so we said okay what if this like what if we let the customers burst so what is",
    "start": "1471080",
    "end": "1478940"
  },
  {
    "text": "the capacity of the bucket to help accommodate the spike in the consumption",
    "start": "1478940",
    "end": "1485120"
  },
  {
    "text": "we launched bursting where we allow customers to carry over",
    "start": "1485120",
    "end": "1491720"
  },
  {
    "text": "their unused throughput in a rolling five minute window it's very similar to how you think about like unused minutes",
    "start": "1491720",
    "end": "1499460"
  },
  {
    "text": "in a cellular plan right you're capped but if you don't use minutes in the last",
    "start": "1499460",
    "end": "1506659"
  },
  {
    "text": "cycle you can move them to the next one and that's what we called as the burst",
    "start": "1506659",
    "end": "1511700"
  },
  {
    "text": "bucket now effectively The increased capacity of the bucket was able to help customers",
    "start": "1511700",
    "end": "1520700"
  },
  {
    "text": "absorb their spikes so you have 2013 this is 2013 time frame where you",
    "start": "1520700",
    "end": "1527419"
  },
  {
    "text": "when we introduced bursting unused provision capacity was banned to be used",
    "start": "1527419",
    "end": "1534140"
  },
  {
    "text": "later and when you exercised those tokens your tokens will be spent and",
    "start": "1534140",
    "end": "1541640"
  },
  {
    "text": "finally you know that particular problem of non-uniform workload over time we were able to solve it",
    "start": "1541640",
    "end": "1549860"
  },
  {
    "start": "1549000",
    "end": "1643000"
  },
  {
    "text": "now we talked about the non -fiction over time let",
    "start": "1549860",
    "end": "1556179"
  },
  {
    "text": "me form distribution over so let's say that you're running a census application for Canada and the",
    "start": "1556220",
    "end": "1562340"
  },
  {
    "text": "data of the table is partitioned based on zip codes right now you can see in this map",
    "start": "1562340",
    "end": "1570260"
  },
  {
    "text": "50 of Canadians live below that line and 50 of Canadians live north of that line",
    "start": "1570260",
    "end": "1575720"
  },
  {
    "text": "so which means what you'll see is that as the most of your data is essentially",
    "start": "1575720",
    "end": "1583220"
  },
  {
    "text": "going in a bunch of partitions right and which means your traffic on those",
    "start": "1583220",
    "end": "1589940"
  },
  {
    "text": "partitions will be higher as compared to your traffic uh on some some partition so in this example we have 250 rcu's",
    "start": "1589940",
    "end": "1596900"
  },
  {
    "text": "going to the green partition but 10 and 10 RCU is going to orange and pink",
    "start": "1596900",
    "end": "1602480"
  },
  {
    "text": "partition so your overall the the partitions they're not seeing uniform",
    "start": "1602480",
    "end": "1609380"
  },
  {
    "text": "traffic and this the takeaway that we had from bursting",
    "start": "1609380",
    "end": "1615260"
  },
  {
    "text": "and non-uniform distribution over over space was that we had essentially tightly coupled how much capacity",
    "start": "1615260",
    "end": "1623779"
  },
  {
    "text": "a partition will get to how the physically we are we are",
    "start": "1623779",
    "end": "1630679"
  },
  {
    "text": "basically Landing these partitions so we had essentially um coupled partition level capacity to",
    "start": "1630679",
    "end": "1637520"
  },
  {
    "text": "admission control and then Mission Control was distributed and performed at a partition level what that resulted in",
    "start": "1637520",
    "end": "1645080"
  },
  {
    "start": "1643000",
    "end": "1771000"
  },
  {
    "text": "just a pictorial picture of that is you would see all the traffic going to a single partition and then since there is",
    "start": "1645080",
    "end": "1651320"
  },
  {
    "text": "not enough capacity on that partition the request will start getting rejected the key point to note here is that even",
    "start": "1651320",
    "end": "1658460"
  },
  {
    "text": "though a customer table has enough capacity for example in this case 300",
    "start": "1658460",
    "end": "1663500"
  },
  {
    "text": "rcus but that particular partition got only assigned 100 rcus so that's why the",
    "start": "1663500",
    "end": "1670460"
  },
  {
    "text": "requests are getting getting rejected and customers were like hey what I have enough capacity on my table why is my",
    "start": "1670460",
    "end": "1676760"
  },
  {
    "text": "request getting rejected and this particular thing was called as throughput dilation and so the next",
    "start": "1676760",
    "end": "1682640"
  },
  {
    "text": "thing we had to do was solve throughput dilution and to solve throughput dilution what we",
    "start": "1682640",
    "end": "1688159"
  },
  {
    "text": "did was we launched Global admission control so dynamodb realized it would it would",
    "start": "1688159",
    "end": "1695419"
  },
  {
    "text": "be going to be beneficial to remove the admission control from partition level",
    "start": "1695419",
    "end": "1700600"
  },
  {
    "text": "and move it up to the request router layer and let all these partitions burst",
    "start": "1700600",
    "end": "1707960"
  },
  {
    "text": "still have a maximum capacity that a single partition can do for workload isolation but move the token buckets",
    "start": "1707960",
    "end": "1714500"
  },
  {
    "text": "from the partition to a global table level to convert it so in the new",
    "start": "1714500",
    "end": "1719840"
  },
  {
    "text": "architecture what happens is we introduce a new service called as GAC Global admission control as a service",
    "start": "1719840",
    "end": "1726919"
  },
  {
    "text": "it's built on the same idea of token buckets but the GAC service centrally tracks the total consumption of table",
    "start": "1726919",
    "end": "1734240"
  },
  {
    "text": "capacity again in terms of tokens and each request router it maintains a local",
    "start": "1734240",
    "end": "1739279"
  },
  {
    "text": "token bucket to make sure the admission decisions are made independently and",
    "start": "1739279",
    "end": "1745100"
  },
  {
    "text": "communicate with GAC to replenish the tokens at regular interval and GAC essentially maintains an affirmal State",
    "start": "1745100",
    "end": "1752600"
  },
  {
    "text": "Computing on the Fly from the client requests so going back to the 300 RCU example now",
    "start": "1752600",
    "end": "1760820"
  },
  {
    "text": "customers could drive that much traffic to even a single partition because we moved the token Bucket from the",
    "start": "1760820",
    "end": "1766940"
  },
  {
    "text": "partition level to a global level which is a table level bucket",
    "start": "1766940",
    "end": "1772580"
  },
  {
    "start": "1771000",
    "end": "1944000"
  },
  {
    "text": "with that no more throughput dilution a great win for customers right",
    "start": "1772580",
    "end": "1780380"
  },
  {
    "text": "well that particular solution was amazing so we had essentially launched launched",
    "start": "1780380",
    "end": "1786620"
  },
  {
    "text": "bursting and we did Global admission control",
    "start": "1786620",
    "end": "1791380"
  },
  {
    "text": "and it helped a lot of use cases in dynamodb but still you know there were",
    "start": "1791779",
    "end": "1797840"
  },
  {
    "text": "cases where a customer would still due to maybe non-uniform distribution over",
    "start": "1797840",
    "end": "1803419"
  },
  {
    "text": "time or space but it's it's still run into scenarios where traffic to a",
    "start": "1803419",
    "end": "1808640"
  },
  {
    "text": "specific partition is you know reaching its maximum so if a partition can do 3000 rcu's maximum",
    "start": "1808640",
    "end": "1815960"
  },
  {
    "text": "and customer wants to do more on that partition request greater than 3000 RCU would get",
    "start": "1815960",
    "end": "1822980"
  },
  {
    "text": "rejected and we wanted to solve that problem as well so what we did was as the traffic increases on the partition",
    "start": "1822980",
    "end": "1829760"
  },
  {
    "text": "we actually split the partitions instead of throttling the customer we started doing automatic splits",
    "start": "1829760",
    "end": "1836899"
  },
  {
    "text": "and the idea behind automatic splits was that to identify the right midpoint",
    "start": "1836899",
    "end": "1842299"
  },
  {
    "text": "which will actually help to redistribute the traffic between two new partitions and if customers and more traffic",
    "start": "1842299",
    "end": "1849520"
  },
  {
    "text": "to one of the partitions we would again further split that into smaller",
    "start": "1849520",
    "end": "1855140"
  },
  {
    "text": "partitions and Route the traffic to the new partitions so now you have",
    "start": "1855140",
    "end": "1862580"
  },
  {
    "text": "these partitions that are you know equally sized or they're balanced",
    "start": "1862580",
    "end": "1867740"
  },
  {
    "text": "essentially and you as a developer did not have to do any any single thing so",
    "start": "1867740",
    "end": "1874700"
  },
  {
    "text": "AWS literally is adjusting the service to fit your custom leads on the specific",
    "start": "1874700",
    "end": "1881600"
  },
  {
    "text": "usage pattern that you are generating for for the service and all this magic",
    "start": "1881600",
    "end": "1887179"
  },
  {
    "text": "happens to solve both the problems even if you have non-uniform traffic over time or",
    "start": "1887179",
    "end": "1893419"
  },
  {
    "text": "non uniform traffic over space and this is not something that we got right from day one as more and more customers built",
    "start": "1893419",
    "end": "1901520"
  },
  {
    "text": "on top of dynamodb we analyze their traffic understood the problems that they were facing and solve these",
    "start": "1901520",
    "end": "1908360"
  },
  {
    "text": "problems by you know introducing bursting split for consumption and Global",
    "start": "1908360",
    "end": "1914899"
  },
  {
    "text": "admission control as solutions for all these different problems so going back to the picture where if the",
    "start": "1914899",
    "end": "1922340"
  },
  {
    "text": "customer is driving 3000 requests per second to the green partition we would automatically split identify where",
    "start": "1922340",
    "end": "1928880"
  },
  {
    "text": "exactly is the right place to split that and split it so that 1500 rcu's 1500 rcu's the traffic splits between the two",
    "start": "1928880",
    "end": "1937460"
  },
  {
    "text": "this was again amazing we essentially did a bunch of heavy lifting on behalf",
    "start": "1937460",
    "end": "1943340"
  },
  {
    "text": "of the customers one thing we still were hearing from customers that hey",
    "start": "1943340",
    "end": "1948820"
  },
  {
    "start": "1944000",
    "end": "1998000"
  },
  {
    "text": "dynamodb has figured out a lot of things for us now for us coming from the world",
    "start": "1948820",
    "end": "1954740"
  },
  {
    "text": "where we are always have been thinking about servers now you have started",
    "start": "1954740",
    "end": "1960440"
  },
  {
    "text": "asking us to think in terms of read capacity units right capacity units can",
    "start": "1960440",
    "end": "1965600"
  },
  {
    "text": "you further simplify that so instead of asking customers to specify provisioning at the at the time of table creation",
    "start": "1965600",
    "end": "1972500"
  },
  {
    "text": "what we did was we launched on demand where you don't even have to specify that all the innovations that we did",
    "start": "1972500",
    "end": "1980120"
  },
  {
    "text": "around bursting step for consuming consumption Global admission control all",
    "start": "1980120",
    "end": "1985340"
  },
  {
    "text": "of those enabled us to launch something which is basically on demand mode on your tables where you just create a",
    "start": "1985340",
    "end": "1991580"
  },
  {
    "text": "table and start sending requests and you pay per request for and for for that",
    "start": "1991580",
    "end": "1996919"
  },
  {
    "text": "particular table so the key lesson here is that designing the system",
    "start": "1996919",
    "end": "2003580"
  },
  {
    "start": "1998000",
    "end": "2048000"
  },
  {
    "text": "which adapts to the customer traffic pattern is the best experience that you can provide to the customer who are",
    "start": "2003580",
    "end": "2010779"
  },
  {
    "text": "using the database and dynamodb Strikes for that um and we as I said before we did not",
    "start": "2010779",
    "end": "2017740"
  },
  {
    "text": "get this right in the first place we launched with the assumption that traffic will be uniformly distributed",
    "start": "2017740",
    "end": "2022779"
  },
  {
    "text": "but realized there are actually non-uniform traffic distribution based on time and space and then",
    "start": "2022779",
    "end": "2029860"
  },
  {
    "text": "analyzing those problems making educated guesses we evolved the service and",
    "start": "2029860",
    "end": "2034899"
  },
  {
    "text": "solved all these problems so that customers all the heavy lifting all the essential complexity is moved away from",
    "start": "2034899",
    "end": "2042340"
  },
  {
    "text": "the customer into the service and customers they just get a magical experience",
    "start": "2042340",
    "end": "2047860"
  },
  {
    "text": "now let's jump into another topic which is about availability I said dynamodb",
    "start": "2047860",
    "end": "2053378"
  },
  {
    "start": "2048000",
    "end": "2101000"
  },
  {
    "text": "provides High availability let's look at how Dynamo DB does that",
    "start": "2053379",
    "end": "2061260"
  },
  {
    "text": "now dynamodb has evolved and you know a lot of customers have moved their",
    "start": "2061839",
    "end": "2068800"
  },
  {
    "text": "mission critical workloads into dynamodb and AWS you know there are service",
    "start": "2068800",
    "end": "2075700"
  },
  {
    "text": "disruptions that happened 2015 Dynamo Levy also had a service disruption and",
    "start": "2075700",
    "end": "2080740"
  },
  {
    "text": "as I said in the beginning Whenever there is a service disruption that happens we try to learn from them and the goal",
    "start": "2080740",
    "end": "2087280"
  },
  {
    "text": "is to make sure that the impact that we saw doesn't repeat we want to make sure the system weaknesses are actually fixed",
    "start": "2087280",
    "end": "2094960"
  },
  {
    "text": "and we have a more [Music] um highly available service",
    "start": "2094960",
    "end": "2100300"
  },
  {
    "text": "so when this issue happened one of the learnings that we had from that particular Coe was that we identified a",
    "start": "2100300",
    "end": "2106000"
  },
  {
    "start": "2101000",
    "end": "2146000"
  },
  {
    "text": "weak Link in the system and that link was related to caches and these caches are essentially the metadata caches that",
    "start": "2106000",
    "end": "2113440"
  },
  {
    "text": "we had in the dynamodb system and",
    "start": "2113440",
    "end": "2119500"
  },
  {
    "text": "one thing about caches is that the caches are by model there are essentially two routes that a cash code",
    "start": "2119500",
    "end": "2127420"
  },
  {
    "text": "can take one is when there is a cash hit your requests are served from the cache",
    "start": "2127420",
    "end": "2132579"
  },
  {
    "text": "so in the case of metadata all the metadata that we request routers wanted it was being served from the cache and",
    "start": "2132579",
    "end": "2139359"
  },
  {
    "text": "then you have a cache missed case where all the requests actually go back to the database and that's what I meant by the",
    "start": "2139359",
    "end": "2145839"
  },
  {
    "text": "bimodal nature of the caches and bimodality in a distributed system is a volcano waiting to another",
    "start": "2145839",
    "end": "2152980"
  },
  {
    "text": "um why do I say that so this is the you know going back to our",
    "start": "2152980",
    "end": "2158260"
  },
  {
    "text": "put item request so whenever a customer made a request to dynamodb to put an",
    "start": "2158260",
    "end": "2163359"
  },
  {
    "text": "item or a get an item the request router is the first service where that request",
    "start": "2163359",
    "end": "2168700"
  },
  {
    "text": "plans now request router has to find out where to Route the request what are the storage nodes for that particular",
    "start": "2168700",
    "end": "2174700"
  },
  {
    "text": "customer uh table and partition so it will hit a metadata service now",
    "start": "2174700",
    "end": "2183780"
  },
  {
    "text": "to optimize that dynamodb also had a partition map cache in the request",
    "start": "2184260",
    "end": "2189820"
  },
  {
    "text": "routers the idea is that if a particular since the partition metadata doesn't",
    "start": "2189820",
    "end": "2194859"
  },
  {
    "text": "change that often it's it's a highly cachable workload and dynamodp we actually had about",
    "start": "2194859",
    "end": "2201960"
  },
  {
    "text": "99.75 percent cash it ratio from these caches which are on the request router so whenever a",
    "start": "2201960",
    "end": "2208240"
  },
  {
    "text": "request lands on a brand new request router it has to go and find out the metadata and instead of just asking the",
    "start": "2208240",
    "end": "2214480"
  },
  {
    "text": "metadata for a specific partition it would ask the metadata for the Full Table assuming that you know next time customer makes a request for a different",
    "start": "2214480",
    "end": "2220720"
  },
  {
    "text": "partition it already has that information right and you know",
    "start": "2220720",
    "end": "2226320"
  },
  {
    "text": "maximum 64 MB request you can get and we don't have just one request router as I",
    "start": "2226320",
    "end": "2231880"
  },
  {
    "text": "said we have like multiple request routers and if a customer creates a table with you know millions of",
    "start": "2231880",
    "end": "2237400"
  },
  {
    "text": "partitions and start sending that many requests they'll probably hit not just one",
    "start": "2237400",
    "end": "2243160"
  },
  {
    "text": "request order they'll hit multiple request routers and all those requests then would request routers would start",
    "start": "2243160",
    "end": "2249099"
  },
  {
    "text": "asking about the same metadata which means you have essentially reached a state System state where you have",
    "start": "2249099",
    "end": "2256660"
  },
  {
    "text": "a lot of a big Fleet talking to a small Fleet so the fundamental problem is that",
    "start": "2256660",
    "end": "2265079"
  },
  {
    "text": "whenever either there is nothing in the caches I.E the cash hit ratio is zero",
    "start": "2265079",
    "end": "2271359"
  },
  {
    "text": "you have a big Fleet driving so many requests to a small Fleet and a sudden",
    "start": "2271359",
    "end": "2278380"
  },
  {
    "text": "spike in traffic in steady state it was zero point you know two five percent",
    "start": "2278380",
    "end": "2284619"
  },
  {
    "text": "if the cache ratio becomes zero cash has become ill effective traffic jumps 200",
    "start": "2284619",
    "end": "2290260"
  },
  {
    "text": "which means 400x increase in traffic and that would further lead to you know",
    "start": "2290260",
    "end": "2295420"
  },
  {
    "text": "cascaling failures in the system so",
    "start": "2295420",
    "end": "2300000"
  },
  {
    "text": "the thing that we wanted to solve is remove this weak link from the system so",
    "start": "2300579",
    "end": "2305980"
  },
  {
    "text": "that the system can you know always operate in a stable manner",
    "start": "2305980",
    "end": "2311680"
  },
  {
    "text": "so how did we do that so what we did was two things one is",
    "start": "2311680",
    "end": "2317500"
  },
  {
    "text": "as I said previously in the previous version of dynamodb the first version request router would load whenever it",
    "start": "2317500",
    "end": "2323500"
  },
  {
    "text": "finds out that there is no information about the partition which it wants to talk to it would load the full partition",
    "start": "2323500",
    "end": "2329680"
  },
  {
    "text": "map for the table first change we did was instead of asking for the full partition map just ask for that",
    "start": "2329680",
    "end": "2336640"
  },
  {
    "text": "particular partition which you are interested in that was the one change that we did and it was a simpler change",
    "start": "2336640",
    "end": "2341800"
  },
  {
    "text": "we were able to do it faster and then we also secondly built a in-memory",
    "start": "2341800",
    "end": "2346960"
  },
  {
    "text": "distributed data store called MDS and memds it stores you know all the metadata",
    "start": "2346960",
    "end": "2352420"
  },
  {
    "text": "in memory think of it like an L2 cache all the data is stored in a highly",
    "start": "2352420",
    "end": "2358780"
  },
  {
    "text": "compressed manner the MDS processes on a load encapsulates essentially a",
    "start": "2358780",
    "end": "2366160"
  },
  {
    "text": "purple data structure um and it can answer questions like hey for this particular key which particular",
    "start": "2366160",
    "end": "2373060"
  },
  {
    "text": "partition it lands into so that you know MDS can respond back to the request router the information",
    "start": "2373060",
    "end": "2379780"
  },
  {
    "text": "and then request router can route the request to the corresponding storage node",
    "start": "2379780",
    "end": "2385839"
  },
  {
    "text": "now we still do not want to impact performance right so we do not want to make a off box call",
    "start": "2385839",
    "end": "2393640"
  },
  {
    "text": "for every request that customer is making so we still want to Cache results so we introduced a new cache which is",
    "start": "2393640",
    "end": "2401380"
  },
  {
    "text": "called the MDS cache on these request routers one thing which is different and",
    "start": "2401380",
    "end": "2407020"
  },
  {
    "text": "critical the most important thing which we did different here on these caches is that even though there is a cash hit",
    "start": "2407020",
    "end": "2415680"
  },
  {
    "text": "cache we would still send all the traffic to the MDS system asking for the",
    "start": "2415680",
    "end": "2422680"
  },
  {
    "text": "information even though we have a cash fit on these members MDS nodes what what",
    "start": "2422680",
    "end": "2428500"
  },
  {
    "text": "that is doing is essentially the system is generating always constant load to",
    "start": "2428500",
    "end": "2434500"
  },
  {
    "text": "the member system so there is not a case where suddenly caches become ineffective",
    "start": "2434500",
    "end": "2439900"
  },
  {
    "text": "your traffic suddenly Rises it is always acting like the caches are ineffective",
    "start": "2439900",
    "end": "2446079"
  },
  {
    "text": "right um and that's how essentially we solve the weak Link in the system of",
    "start": "2446079",
    "end": "2452380"
  },
  {
    "text": "metadata getting impacted by requests being like Landing from multiple request",
    "start": "2452380",
    "end": "2457599"
  },
  {
    "text": "routers onto the malaria nodes so overall the lesson here is that",
    "start": "2457599",
    "end": "2465099"
  },
  {
    "text": "designing systems for predictability over absolute efficiency improves the",
    "start": "2465099",
    "end": "2470619"
  },
  {
    "text": "stability of the system like while system like caches you know",
    "start": "2470619",
    "end": "2476079"
  },
  {
    "text": "can improve performance but do not allow them to hide the work that would be",
    "start": "2476079",
    "end": "2482380"
  },
  {
    "text": "performed in their absence ensuring that your system is always provisioned to",
    "start": "2482380",
    "end": "2488200"
  },
  {
    "text": "handle the unexpected load that can happen when the cash is become ineffective so with that",
    "start": "2488200",
    "end": "2494980"
  },
  {
    "start": "2494000",
    "end": "2626000"
  },
  {
    "text": "conclusions slash key takeaways we talked about the first and second the first one being",
    "start": "2494980",
    "end": "2501880"
  },
  {
    "text": "adapting two customer traffic patterns improve their experience and we looked at it with how different problems we",
    "start": "2501880",
    "end": "2510400"
  },
  {
    "text": "solved by introducing um Global admission control bursting and",
    "start": "2510400",
    "end": "2516940"
  },
  {
    "text": "and finally on demand second designing systems of",
    "start": "2516940",
    "end": "2522280"
  },
  {
    "text": "predictability over absolute efficiency improves system stability that's the one we just saw with caches caches the",
    "start": "2522280",
    "end": "2528520"
  },
  {
    "text": "bimodal how it's important to make sure that system",
    "start": "2528520",
    "end": "2534160"
  },
  {
    "text": "is doing predictable load and the failure scenarios are tamed by making",
    "start": "2534160",
    "end": "2541599"
  },
  {
    "text": "sure that your system uh is provisioned for you know the the maximum load that",
    "start": "2541599",
    "end": "2546820"
  },
  {
    "text": "you have to do third and fourth these are two more",
    "start": "2546820",
    "end": "2551980"
  },
  {
    "text": "things that we talk about in much detail in the paper the third being",
    "start": "2551980",
    "end": "2558240"
  },
  {
    "text": "dynamodb is a distributed system we have multiple storage nodes multiple request routers",
    "start": "2558240",
    "end": "2564579"
  },
  {
    "text": "performing continuous verification of data at rest is necessary and that's the",
    "start": "2564579",
    "end": "2570040"
  },
  {
    "text": "best way we have figured out to ensure we meet high durability goals last maintaining High variability as",
    "start": "2570040",
    "end": "2577660"
  },
  {
    "text": "system evolves it might mean that you have to build that you have to touch the",
    "start": "2577660",
    "end": "2583839"
  },
  {
    "text": "most complex part of your system and in dynamodb one of the most complicated most complex part is the pack Source",
    "start": "2583839",
    "end": "2592240"
  },
  {
    "text": "protocol the multi-paxos protocol and to improve availability we had to do some changes in that protocol layer and the",
    "start": "2592240",
    "end": "2600099"
  },
  {
    "text": "reason we were able to do those changes easily was because we had formal proof of these algorithms that are being",
    "start": "2600099",
    "end": "2606700"
  },
  {
    "text": "written there that that were written there right from you know original days of dynamodb and that gave us quite high",
    "start": "2606700",
    "end": "2613660"
  },
  {
    "text": "confidence since we had a proof of uh the working system we could tweak it and make sure that the new system still you",
    "start": "2613660",
    "end": "2620920"
  },
  {
    "text": "know uh and shorts correctness all the invariants are met with that",
    "start": "2620920",
    "end": "2627460"
  },
  {
    "text": "thanks a lot for listening tuning in and I am happy to",
    "start": "2627460",
    "end": "2633040"
  },
  {
    "text": "take any questions either you can ask me questions now meet me here or you can",
    "start": "2633040",
    "end": "2639160"
  },
  {
    "text": "tweet um using my using you can tweet directly to me at akshatwig with that thanks a",
    "start": "2639160",
    "end": "2646839"
  },
  {
    "text": "lot and I'm here to take any questions thank you",
    "start": "2646839",
    "end": "2652260"
  },
  {
    "text": "hello well actually this is a wonderful talk thank you again for building this",
    "start": "2655200",
    "end": "2660640"
  },
  {
    "text": "uh we have tons of questions lined up um and let's just go through some of the",
    "start": "2660640",
    "end": "2666099"
  },
  {
    "text": "questions that the uh the various folks had um I'll go in the order that I recorded",
    "start": "2666099",
    "end": "2672280"
  },
  {
    "text": "them and the way they were asked but um someone asked what storage system doesn't metadata storage use I think you",
    "start": "2672280",
    "end": "2679900"
  },
  {
    "text": "answered this but you can go into more detail here yeah so dynamodb uses dynamodb for its storage needs so just",
    "start": "2679900",
    "end": "2686380"
  },
  {
    "text": "think about it store the software that is running on the storage nodes that's the same software which is running on",
    "start": "2686380",
    "end": "2692079"
  },
  {
    "text": "the metadata nodes as well and as we have evolved as I talked about memds",
    "start": "2692079",
    "end": "2698560"
  },
  {
    "text": "being introduced in the later parts of the slide that's the that's the basically think of it like a cache an L2",
    "start": "2698560",
    "end": "2705160"
  },
  {
    "text": "cache on top of the metadata that is being used to serve like partition metadata for all the learnings that we",
    "start": "2705160",
    "end": "2712240"
  },
  {
    "text": "had in scaling bottlenecks with the metadata field got it so like the metadata nodes you're",
    "start": "2712240",
    "end": "2718359"
  },
  {
    "text": "saying it's just another Dynamo system or is it just a storage system like does it does a metadata service come with its",
    "start": "2718359",
    "end": "2723700"
  },
  {
    "text": "own request router and storage tiers or is it just yeah no it's just the storage node software request router basically",
    "start": "2723700",
    "end": "2731319"
  },
  {
    "text": "has a client which talks to the storage nodes it uses the same client to talk to",
    "start": "2731319",
    "end": "2737079"
  },
  {
    "text": "you know the metadata node so that whenever the request comes it's exactly same code that we run for like a",
    "start": "2737079",
    "end": "2743500"
  },
  {
    "text": "production customer who would be accessing dynamodb excellent okay",
    "start": "2743500",
    "end": "2749560"
  },
  {
    "text": "um Central on the topic of mmbs someone asked is MDS eventually consistent how",
    "start": "2749560",
    "end": "2756880"
  },
  {
    "text": "does it change to a cached value edition of a new value of one MBS node would get replicated to other nodes",
    "start": "2756880",
    "end": "2763599"
  },
  {
    "text": "yeah yeah MDS is built using I would say",
    "start": "2763599",
    "end": "2769720"
  },
  {
    "text": "um think of it like you have an L2 cache and that cache is updated not by not a",
    "start": "2769720",
    "end": "2775540"
  },
  {
    "text": "right through cash but a right around cash so whenever a write happens on the metadatan fleet which is like very low",
    "start": "2775540",
    "end": "2782079"
  },
  {
    "text": "throughput partitions they don't change often right so what happens is whenever a write happens on the partition",
    "start": "2782079",
    "end": "2789280"
  },
  {
    "text": "um table in the metadata store that those rights through streams they",
    "start": "2789280",
    "end": "2795040"
  },
  {
    "text": "are basically consumed by a central system which sends all the all the MDS",
    "start": "2795040",
    "end": "2801579"
  },
  {
    "text": "nodes to have the updated values uh and this whole process happens within like",
    "start": "2801579",
    "end": "2807099"
  },
  {
    "text": "milliseconds again um as as the change comes in it just gets replicated on all the boxes so the",
    "start": "2807099",
    "end": "2812980"
  },
  {
    "text": "guy which is in the center is responsible for making sure all the MDS nodes are getting the latest values",
    "start": "2812980",
    "end": "2819640"
  },
  {
    "text": "until it is acknowledged by all the nodes that system doesn't move forward so that's how that's how the the do you",
    "start": "2819640",
    "end": "2828040"
  },
  {
    "text": "employ partitioning in you know uh like I guess Master Slave sort of models",
    "start": "2828040",
    "end": "2833200"
  },
  {
    "text": "there or other agreement protocols like it says every member MDS don't have all",
    "start": "2833200",
    "end": "2839140"
  },
  {
    "text": "of the data MDS has all the metadata so every node has it's basically vertically scaled",
    "start": "2839140",
    "end": "2845020"
  },
  {
    "text": "every node has because the partition metadata as I said is like not that big",
    "start": "2845020",
    "end": "2850060"
  },
  {
    "text": "it's tiny the main thing there is the throughput that we have to support uh that's what is very very high that's why",
    "start": "2850060",
    "end": "2855700"
  },
  {
    "text": "we just keep on adding more read replicas there um and uh yeah so it's not the leader",
    "start": "2855700",
    "end": "2863020"
  },
  {
    "text": "follower configuration it's like every node is eventually consistent it's a cache and the the they are scaled for",
    "start": "2863020",
    "end": "2870640"
  },
  {
    "text": "specifically the reads that that metadata uh requests have to be served for whenever customers are sending",
    "start": "2870640",
    "end": "2876819"
  },
  {
    "text": "requests or any other system which wants to access the partition method so if I understand so so there's some",
    "start": "2876819",
    "end": "2882280"
  },
  {
    "text": "coordinator that's in the MDS system request is sent uh with a new write",
    "start": "2882280",
    "end": "2888460"
  },
  {
    "text": "request that coordinator is responsible for ensuring that all of the entire fleet gets that data yes exact",
    "start": "2888460",
    "end": "2895780"
  },
  {
    "text": "coordinator's job it's not each peer-to-peer kind of uh Gossip type protocol to link yeah so that",
    "start": "2895780",
    "end": "2902680"
  },
  {
    "text": "coordinator is kind of responsible for a durable State and consistent State yep",
    "start": "2902680",
    "end": "2908800"
  },
  {
    "text": "like as each node once they get it they're durable but but that's just for consistent State transfers yeah and",
    "start": "2908800",
    "end": "2915700"
  },
  {
    "text": "durable in the sense it's a cache so when it when it crashes it will just ask from the other node in the in the system",
    "start": "2915700",
    "end": "2921960"
  },
  {
    "text": "to to get up to speed and then start serving requests okay and uh oh that",
    "start": "2921960",
    "end": "2929980"
  },
  {
    "text": "makes sense and and that coordinator how do you make you know everyone always asks in distribution system how do you make that",
    "start": "2929980",
    "end": "2935319"
  },
  {
    "text": "reliable so all these rights are idempotent again so you could have like",
    "start": "2935319",
    "end": "2941560"
  },
  {
    "text": "any number of these these guys running in the system at any time and writing to",
    "start": "2941560",
    "end": "2946720"
  },
  {
    "text": "the the destination uh since the rights are important they always monotonically increase so never goes back so if a",
    "start": "2946720",
    "end": "2954760"
  },
  {
    "text": "partition has changed from P1 to P2 if you write again it will say hey I have the latest information I don't need this",
    "start": "2954760",
    "end": "2960280"
  },
  {
    "text": "anymore so we don't need a coordinator we try to avoid that uh so you don't have a coordinator",
    "start": "2960280",
    "end": "2967180"
  },
  {
    "text": "service that's writing to these nodes like we don't have the the leader configuration there leaders leader and",
    "start": "2967180",
    "end": "2972880"
  },
  {
    "text": "follower configuration there yeah but you do have a coordinator that all requests go to and that keeps track",
    "start": "2972880",
    "end": "2979780"
  },
  {
    "text": "of the the monoticity of the rights like a writer headlong so it's got some sort of",
    "start": "2979780",
    "end": "2986099"
  },
  {
    "text": "distributed reliable writer headlock and it's just somehow sending it would that",
    "start": "2986099",
    "end": "2992079"
  },
  {
    "text": "be a topic of another talk that you may get I mean Global cables is what I what",
    "start": "2992079",
    "end": "2998260"
  },
  {
    "text": "I hear a lot we did transactions yeah so I think we should probably figure out figure out another talk as a follow-up",
    "start": "2998260",
    "end": "3004380"
  },
  {
    "text": "at another coupon I'd be happy to come okay um cool so that was sort of the MDS",
    "start": "3004380",
    "end": "3010079"
  },
  {
    "text": "stuff uh were there any other items or I'm curious if there any other questions uh people have about an MBS while we're",
    "start": "3010079",
    "end": "3016260"
  },
  {
    "text": "on the topic I don't see oh uh someone did ask this question so",
    "start": "3016260",
    "end": "3024000"
  },
  {
    "text": "when the partition grows and splits how do you ensure the metadata memdes cash layers get updated consistently",
    "start": "3024000",
    "end": "3031680"
  },
  {
    "text": "yeah as I said it's eventually consistent and the workflows that are actually doing like partition splits and",
    "start": "3031680",
    "end": "3037920"
  },
  {
    "text": "all these things they wait until MDS MDS is updated before you know flipping the",
    "start": "3037920",
    "end": "3044579"
  },
  {
    "text": "information there so other thing is even if the data is not there in MDS storage nodes also have the protocol to respond",
    "start": "3044579",
    "end": "3051480"
  },
  {
    "text": "back to the request router saying that hey by the way I am updated I I don't host any I don't Host this partition",
    "start": "3051480",
    "end": "3059040"
  },
  {
    "text": "anymore but this is the hint that I have so maybe you can go talk to that guy so even if you know uh for like the the",
    "start": "3059040",
    "end": "3065940"
  },
  {
    "text": "edge cases where this might information might get delayed we still have mechanisms built in the protocol",
    "start": "3065940",
    "end": "3072599"
  },
  {
    "text": "to update the coordinator so so each partition also has some information of who else has this data",
    "start": "3072599",
    "end": "3078960"
  },
  {
    "text": "uh yes just just about mmds node yes yeah and also I guess it's a little",
    "start": "3078960",
    "end": "3086280"
  },
  {
    "text": "easier because this unlike dynamodb this can be eventually consistent dynamodb has people care about consistency over",
    "start": "3086280",
    "end": "3092940"
  },
  {
    "text": "uh I guess availability and other things okay so essentially we're saying that",
    "start": "3092940",
    "end": "3099420"
  },
  {
    "text": "that uh so when you when you split uh it doesn't have to be replicated everywhere",
    "start": "3099420",
    "end": "3105980"
  },
  {
    "text": "immediately it's actually I had a question about that uh",
    "start": "3105980",
    "end": "3112260"
  },
  {
    "text": "when you do a split are you rebalancing sorry sorry to interrupt we have we've",
    "start": "3112260",
    "end": "3117599"
  },
  {
    "text": "run out of time on this session and phrase it so we can continue the conversation in slack",
    "start": "3117599",
    "end": "3122819"
  },
  {
    "text": "um uh it's just people be coming in for the next we'll be leaving for the next session ah okay",
    "start": "3122819",
    "end": "3128940"
  },
  {
    "text": "um actually do you have all of these questions you can uh you want to go into the channel",
    "start": "3128940",
    "end": "3135078"
  },
  {
    "text": "people can join the uh what is it called track uh architecture you've always",
    "start": "3135200",
    "end": "3141720"
  },
  {
    "text": "wanted about people can ask questions here I can post this sort of question list I have",
    "start": "3141720",
    "end": "3148400"
  },
  {
    "text": "um if you want one Consolidated place to see it but yeah let's do that well Akshay thank you so much for your",
    "start": "3148980",
    "end": "3155160"
  },
  {
    "text": "answers I I have so many more questions but I think we'll take it to to slap for the rest sounds good thank you very much",
    "start": "3155160",
    "end": "3162300"
  },
  {
    "text": "thanks foreign [Music]",
    "start": "3162300",
    "end": "3172629"
  }
]