[
  {
    "text": "so here's the agenda for today uh two 45 minute sessions um the first session",
    "start": "6399",
    "end": "11440"
  },
  {
    "text": "will be kind of learning the basics of spark using data frames and SQL doing a bit of time in uh data analysis learning",
    "start": "11440",
    "end": "19400"
  },
  {
    "text": "about how to use memory in spark and some spark internals I would like to use",
    "start": "19400",
    "end": "24840"
  },
  {
    "text": "the second 45 minutes to give you the opportunity to actually download one of",
    "start": "24840",
    "end": "30240"
  },
  {
    "text": "the 400 data sets and do something with it there's a bunch of people here from data bricks who can help out as Tas um I",
    "start": "30240",
    "end": "38000"
  },
  {
    "text": "might also use the second half to kind of wrap up anything I wasn't able to get to in the beginning um okay so you know",
    "start": "38000",
    "end": "45680"
  },
  {
    "text": "I was uh thinking a lot about what data set to use and I found that actually maybe a a timely thing to do would be to",
    "start": "45680",
    "end": "53199"
  },
  {
    "text": "look at the uh 4th of July uh calls to",
    "start": "53199",
    "end": "58600"
  },
  {
    "text": "911 that happened like two days ago specifically requesting um the San",
    "start": "58600",
    "end": "64439"
  },
  {
    "text": "Francisco Fire Department uh so that's kind of what we'll kind of be doing and uh exploring",
    "start": "64439",
    "end": "70759"
  },
  {
    "text": "a couple of data sets around this so uh another thing you might want to do right",
    "start": "70759",
    "end": "75799"
  },
  {
    "text": "away is go to this link data. sf.gov uh sfgov.org",
    "start": "75799",
    "end": "81880"
  },
  {
    "text": "and you can see that you can click on these different icons to start browsing",
    "start": "81880",
    "end": "86920"
  },
  {
    "text": "the different data sets um let me start by just showing you what I did a couple of days ago when I found the data uh I",
    "start": "86920",
    "end": "93479"
  },
  {
    "text": "typed in fire incidents and if you do",
    "start": "93479",
    "end": "100159"
  },
  {
    "text": "that you get uh a couple of fire uh a few different actually fire data sets",
    "start": "100159",
    "end": "106759"
  },
  {
    "text": "the first one I got interested in is actually the third one down this one that says fire department calls for",
    "start": "106759",
    "end": "112759"
  },
  {
    "text": "service the reason I thought this was interesting is because it's um last updated the night before so I thought be",
    "start": "112759",
    "end": "120280"
  },
  {
    "text": "uh interesting to look at this data so if you click on this you then get this like UI that will",
    "start": "120280",
    "end": "128959"
  },
  {
    "text": "in a second show you uh the table and this table has like 30 columns you can",
    "start": "128959",
    "end": "135160"
  },
  {
    "text": "get a quick uh glance of what this data looks like there's like a call number an",
    "start": "135160",
    "end": "140560"
  },
  {
    "text": "incident number for the 911 call um a call type and there's all these like",
    "start": "140560",
    "end": "146120"
  },
  {
    "text": "time stamps here for the exact time the call came in",
    "start": "146120",
    "end": "151840"
  },
  {
    "text": "and zip code of the",
    "start": "153360",
    "end": "157200"
  },
  {
    "text": "incident and finally uh on the far right here it says the neighborhood District",
    "start": "159280",
    "end": "164640"
  },
  {
    "text": "that the call uh came in from okay so this data you can download to your",
    "start": "164640",
    "end": "170560"
  },
  {
    "text": "computer so don't do what I'm about to show you because I've already put this um 1.6 gab file on S3 so you don't want",
    "start": "170560",
    "end": "178480"
  },
  {
    "text": "to download this and uh kind of saturate our Wi-Fi but I'll show you what I did do two days ago I clicked on export up",
    "start": "178480",
    "end": "185120"
  },
  {
    "text": "here and then I just basically clicked on the CSV file downloaded the 1.6 gab",
    "start": "185120",
    "end": "191200"
  },
  {
    "text": "file dumped it on S3 and that's kind of where we're going to pick up in a second um you'll notice here you can actually",
    "start": "191200",
    "end": "197440"
  },
  {
    "text": "also get like Json versions of this data but so that's kind of what I did",
    "start": "197440",
    "end": "204959"
  },
  {
    "text": "um okay so I'm hoping that uh everyone has",
    "start": "205040",
    "end": "210480"
  },
  {
    "text": "uh successfully logged into Data brecks uh if you're viewing this on the internet uh later on then uh the way you",
    "start": "210480",
    "end": "217400"
  },
  {
    "text": "can check out this uh data and follow along with this lecture is go to datab bricks.com",
    "start": "217400",
    "end": "223840"
  },
  {
    "text": "try and then you're going to want to go to Community Edition on the right",
    "start": "223840",
    "end": "229400"
  },
  {
    "text": "here and this is basically uh going going to give you a free uh sign up for",
    "start": "229439",
    "end": "237200"
  },
  {
    "text": "data breaks there's no credit card required and the main reason we like to use this platform for talks like this is",
    "start": "237200",
    "end": "245200"
  },
  {
    "text": "because there's no setup involved on your laptop right this is a Apache spark cluster running in the Amazon Cloud and",
    "start": "245200",
    "end": "252840"
  },
  {
    "text": "all you need to do uh the Hands-On Workshop today is Chrome or Firefox so",
    "start": "252840",
    "end": "258639"
  },
  {
    "text": "like Internet Explorer is not as tested and um Safari might work but I highly recommend using Chrome or Firefox sign",
    "start": "258639",
    "end": "266199"
  },
  {
    "text": "up for this you should get an email within about 2 minutes uh log into your email and uh click on the link to get to",
    "start": "266199",
    "end": "273960"
  },
  {
    "text": "databas Community Edition once you do that by the way please also write down these two links",
    "start": "273960",
    "end": "280360"
  },
  {
    "text": "um bitly or bit. lsf openen laabs and bit.",
    "start": "280360",
    "end": "287160"
  },
  {
    "text": "lsf open readal go ahead and download those two as well and this down here",
    "start": "287160",
    "end": "292600"
  },
  {
    "text": "community. cloud. datab bricks.com is the actual login to datab Bricks um but",
    "start": "292600",
    "end": "298600"
  },
  {
    "text": "your email will probably have that link as well so this file is about a one",
    "start": "298600",
    "end": "305360"
  },
  {
    "text": "megabyte file the open Labs file this is the the file that has our actual um code for it today okay so I'm going to just",
    "start": "305360",
    "end": "313039"
  },
  {
    "text": "continue but if you are having trouble logging in raise your hand no one",
    "start": "313039",
    "end": "321000"
  },
  {
    "text": "amazing okay by the way throughout the next 30",
    "start": "321000",
    "end": "326840"
  },
  {
    "text": "45 minutes just raise your hand if you have any trouble um with anything there are several people here from datab",
    "start": "326840",
    "end": "332080"
  },
  {
    "text": "bricks who can help so what I'm going to do is I'm going to log into my own datab bricks um",
    "start": "332080",
    "end": "338720"
  },
  {
    "text": "account and uh I'll walk you through what you'll have to do to get to uh the",
    "start": "338720",
    "end": "344520"
  },
  {
    "text": "environment when you log into Data bricks this is what you'll see uh the first thing you'll want to do is click",
    "start": "344520",
    "end": "350280"
  },
  {
    "text": "on clusters down here and let's go ahead and launch a Apache spark 2.0 cluster so",
    "start": "350280",
    "end": "357800"
  },
  {
    "text": "I'm going to click on create cluster I'll call this one um you know Samir",
    "start": "357800",
    "end": "365199"
  },
  {
    "text": "demo and I'm G to change the default from spark 1.6 to all the way at the",
    "start": "365240",
    "end": "370759"
  },
  {
    "text": "bottom spark 2.0 uh which has not been released officially yet but it's in",
    "start": "370759",
    "end": "376080"
  },
  {
    "text": "release candidate 2 and it should be released very soon um all the code I'm going to show you in this uh demo is",
    "start": "376080",
    "end": "382160"
  },
  {
    "text": "going to be the new spark 2.0 um the new spark 2.0",
    "start": "382160",
    "end": "387520"
  },
  {
    "text": "API okay this is all I'm going to do I'm just going to uh click on create cluster",
    "start": "387520",
    "end": "395520"
  },
  {
    "text": "now okay so this is pending and it's uh provisioning a a spa instance in Amazon",
    "start": "399120",
    "end": "405039"
  },
  {
    "text": "for me while that's going on I'm not going to wait I'm just going to click on workspace",
    "start": "405039",
    "end": "410240"
  },
  {
    "text": "here and I'm going to rightclick on this white space and I won't go to create",
    "start": "410240",
    "end": "416319"
  },
  {
    "text": "I'll go to import instead um because I want to import that one megabyte lab document or",
    "start": "416319",
    "end": "422720"
  },
  {
    "text": "file okay so click on import and then um just leave that on file click this",
    "start": "422720",
    "end": "428960"
  },
  {
    "text": "button and that is the uh datab bricks Cloud file the DBC file that uh is the",
    "start": "428960",
    "end": "436560"
  },
  {
    "text": "labs file it's 100 kilobytes now the way you get that file one more time is this",
    "start": "436560",
    "end": "442840"
  },
  {
    "text": "top link Labs file that will give you that 100 kilobyte file this readalong is",
    "start": "442840",
    "end": "449919"
  },
  {
    "text": "basically a static HTML copy of the exact same notebook if you are having",
    "start": "449919",
    "end": "455319"
  },
  {
    "text": "trouble logging in or you kind of just want to browse through what I'm doing and not really type any code then feel",
    "start": "455319",
    "end": "461680"
  },
  {
    "text": "free to do the readal and you'll just get a static HTML copy of the notebook all right but everybody really",
    "start": "461680",
    "end": "467840"
  },
  {
    "text": "like I hope everybody actually uses the labs file and starts playing with this code okay so that's my labs file so let",
    "start": "467840",
    "end": "475039"
  },
  {
    "text": "me import it uh okay some I guess my cluster is having a bit of a trouble coming up Let",
    "start": "475039",
    "end": "481960"
  },
  {
    "text": "me refresh this okay there we go my cluster is up perfectly I I don't have to go to my",
    "start": "481960",
    "end": "487039"
  },
  {
    "text": "backup environment so you have to first make sure that you get this green button here that says running or the green icon",
    "start": "487039",
    "end": "494039"
  },
  {
    "text": "that took about 1 minute for me so I'm now actually ready to import my uh my",
    "start": "494039",
    "end": "499759"
  },
  {
    "text": "file one more time so I'll click on import and just click on that 100 kilobyte file one more",
    "start": "499759",
    "end": "505879"
  },
  {
    "text": "time and I'm good to go I have this folder now here called SF Meetup once",
    "start": "505879",
    "end": "511120"
  },
  {
    "text": "again I right clicked on this and I went to import and I brought in my file there are um a few different notebooks here so",
    "start": "511120",
    "end": "518719"
  },
  {
    "text": "first go to the data sets Mount notebook and then up here click on",
    "start": "518719",
    "end": "525240"
  },
  {
    "text": "detached attach it to your cluster I'll attach it to My Demo cluster and then put your cursor in this first cell and",
    "start": "525240",
    "end": "532959"
  },
  {
    "text": "hit shift enter to run that cell um and then move to this next cell and hit",
    "start": "532959",
    "end": "541680"
  },
  {
    "text": "shift enter on the keyboard and those two um cells should execute and finally",
    "start": "541680",
    "end": "547839"
  },
  {
    "text": "if you run this last cell you should successfully see a bunch of Mount points showing up one of those Mount points",
    "start": "547839",
    "end": "554160"
  },
  {
    "text": "you'll notice is um SF open data all right so all I did is I just attached to",
    "start": "554160",
    "end": "559959"
  },
  {
    "text": "my cluster up here and I ran the three cells okay then uh we'll click back on",
    "start": "559959",
    "end": "565959"
  },
  {
    "text": "workspace on the left and now um what I'm going to do see this little like um gray part here I'm going to",
    "start": "565959",
    "end": "573160"
  },
  {
    "text": "expand uh I'm going to expand this column so I can see that there's actually two notebooks here runme and",
    "start": "573160",
    "end": "579560"
  },
  {
    "text": "read me read me is a static notebook with all the cells already run it's not",
    "start": "579560",
    "end": "586200"
  },
  {
    "text": "what I would recommend doing um I recommend going to runme in runme the code is there but the cells are not",
    "start": "586200",
    "end": "592160"
  },
  {
    "text": "executed so everyone should just go ahead and mount the data set and jump into the runme version of The Notebook",
    "start": "592160",
    "end": "598120"
  },
  {
    "text": "and we can finally get into the material okay so uh what I'll do next is",
    "start": "598120",
    "end": "604160"
  },
  {
    "text": "um in the runme version of The Notebook uh I'll let it load for a second and",
    "start": "604160",
    "end": "609600"
  },
  {
    "text": "once it's Lo Done loading I'm going to click on this detach button and attach it to my cluster one more",
    "start": "609600",
    "end": "616079"
  },
  {
    "text": "time it says attached I'm am good to go who needs help because they're not",
    "start": "616079",
    "end": "622360"
  },
  {
    "text": "here yet just raise your hand and leave it up for a few seconds okay there's a few Tas who will start walking around",
    "start": "622360",
    "end": "628560"
  },
  {
    "text": "I'll ask you to do that again in like five more minutes also consider just asking your peer to the left or right to",
    "start": "628560",
    "end": "634560"
  },
  {
    "text": "help you out okay if you click on this very first cell here where you see this spark logo",
    "start": "634560",
    "end": "641240"
  },
  {
    "text": "double click on that and notice that this is actually a markdown cell uh let me make this a little bigger so the",
    "start": "641240",
    "end": "647160"
  },
  {
    "text": "people in the back can read um basically all I did is I double clicked on this cell the first cell and I'm just",
    "start": "647160",
    "end": "653440"
  },
  {
    "text": "pointing out that that's a markdown cell that is uh pointing to two PNG files",
    "start": "653440",
    "end": "661240"
  },
  {
    "text": "and uh all these text files here are actually markdown files or these text cells um to make you a little familiar",
    "start": "662160",
    "end": "669600"
  },
  {
    "text": "with this interface see like when you hover between the two cells I can hit this plus button and a",
    "start": "669600",
    "end": "676600"
  },
  {
    "text": "new cell opens up if you hit the plus button and then you can like write python code in here like 1+ one for",
    "start": "676600",
    "end": "683760"
  },
  {
    "text": "example and run the cell you can also delete a cell like this by hitting this",
    "start": "683760",
    "end": "690000"
  },
  {
    "text": "X and you can click on the keyboard icon button up here to see operating system",
    "start": "690000",
    "end": "695880"
  },
  {
    "text": "specific shortcuts the the really the only two important shortcuts for now that I want you to know are if I like",
    "start": "695880",
    "end": "703519"
  },
  {
    "text": "make a couple of cells here like you know xal 5 and then I make another cell here let me make this bigger I'll make",
    "start": "703519",
    "end": "710959"
  },
  {
    "text": "another cell here that says like x um+ one okay um here's the most important",
    "start": "710959",
    "end": "717639"
  },
  {
    "text": "two keyboard shortcuts if I move my cursor in this cell in Windows I can hit",
    "start": "717639",
    "end": "724040"
  },
  {
    "text": "shift enter or in mac and notice that it runs the cell and moves the cursor to the next cell sometimes you're going to",
    "start": "724040",
    "end": "730639"
  },
  {
    "text": "want to keep the cursor in this cell and keep running it over and over and over again without moving the cursor for that",
    "start": "730639",
    "end": "736920"
  },
  {
    "text": "on Windows I can do control enter so see if I run control enter the cursor stays",
    "start": "736920",
    "end": "742199"
  },
  {
    "text": "there and it's not moving to the next cell uh and sometimes I'm going to want to use that technique if you're on Mac",
    "start": "742199",
    "end": "748839"
  },
  {
    "text": "you can figure out the keyboard shortcut by just clicking on the keyboard shortcut area up here if you come from a",
    "start": "748839",
    "end": "756800"
  },
  {
    "text": "Jupiter or I python notebook background there is no command mode here so you can",
    "start": "756800",
    "end": "761920"
  },
  {
    "text": "like hit escape and get keyboard shortcuts there's really only one mode and those uh shortcuts in that one mode",
    "start": "761920",
    "end": "768959"
  },
  {
    "text": "can be run if you run uh any of these shortcuts right now okay let me set the",
    "start": "768959",
    "end": "774240"
  },
  {
    "text": "stage for what we're going to do um we will look at the uh some of the",
    "start": "774240",
    "end": "779800"
  },
  {
    "text": "Fourth of July data for the firefighters um incoming calls to",
    "start": "779800",
    "end": "785600"
  },
  {
    "text": "911 okay so really quick uh a little bit of information about spark um as Jules",
    "start": "785600",
    "end": "791800"
  },
  {
    "text": "was saying it started at UC Berkeley's amp lab in 2009 it's open source under Apache the latest major stable release",
    "start": "791800",
    "end": "799560"
  },
  {
    "text": "is indeed 1.6 but we're going to work on a preview release of 2.0 today uh most",
    "start": "799560",
    "end": "805720"
  },
  {
    "text": "of the lines of code in spark are written in Scala but there is is a um python API as",
    "start": "805720",
    "end": "811920"
  },
  {
    "text": "well okay so um there's many different libraries in spark um we will focus",
    "start": "811920",
    "end": "817959"
  },
  {
    "text": "almost exclusively today on SQL and data frames but there is a machine learning API a streaming API and a graph API",
    "start": "817959",
    "end": "825399"
  },
  {
    "text": "however the SQL and data frames API is really the foundation and the gateway to",
    "start": "825399",
    "end": "830600"
  },
  {
    "text": "get into any of these other apis so I hope you get a really solid foundation about what what you can do with spark",
    "start": "830600",
    "end": "837160"
  },
  {
    "text": "that will let you build upon doing super vised and unsupervised machine learning graph processing like nodes and vertices",
    "start": "837160",
    "end": "843920"
  },
  {
    "text": "and Page Rank and shortest path and also streaming um the way streaming works is",
    "start": "843920",
    "end": "849360"
  },
  {
    "text": "pretty cool by the way in 2.0 what you do is when you turn on structured streaming in 2.0 you basically start",
    "start": "849360",
    "end": "855839"
  },
  {
    "text": "building an infinite data frame so that your data frame or SQL query that you learn to use and write today will just",
    "start": "855839",
    "end": "864040"
  },
  {
    "text": "translate uh automatically into structured streaming and that infinite",
    "start": "864040",
    "end": "869240"
  },
  {
    "text": "data frame um you can apply your code right away to streaming basically",
    "start": "869240",
    "end": "875440"
  },
  {
    "text": "okay so um if you're new to spark here's like spark 101 a little bit more technical um there are uh many data",
    "start": "875440",
    "end": "883240"
  },
  {
    "text": "sources you can read from basically every major file system or database in 2016 you can read from uh spark runs in",
    "start": "883240",
    "end": "891079"
  },
  {
    "text": "almost every major environment uh Docker ec2 msos uh kubernetes yarn we will run",
    "start": "891079",
    "end": "898079"
  },
  {
    "text": "exclusively in Amazon EC today and we're going to be reading data um from S3 the",
    "start": "898079",
    "end": "903880"
  },
  {
    "text": "data we'll be reading is CSV data or paret data all right um one more thing if you're new to spark",
    "start": "903880",
    "end": "912000"
  },
  {
    "text": "please don't use the rdd API that's kind of like the internal API to spark now it's kind of older you want to stick",
    "start": "912000",
    "end": "918720"
  },
  {
    "text": "with this new API up here data frame SQL and data",
    "start": "918720",
    "end": "923639"
  },
  {
    "text": "sets so here's what a spark cluster looks like a distributed spark cluster has a bunch of jvms there's a driver jvm",
    "start": "923800",
    "end": "931839"
  },
  {
    "text": "and a bunch of executive jvms you can have hundreds of executive jvms but you'll always have just one driver jvm",
    "start": "931839",
    "end": "939279"
  },
  {
    "text": "the driver jvm is kind of like the head of your spark cluster it does scheduling",
    "start": "939279",
    "end": "944680"
  },
  {
    "text": "and um it uh if you actually look at this every executive here is running",
    "start": "944680",
    "end": "950120"
  },
  {
    "text": "slots and you'll notice that in this example there's two slots on every executive so what the driver does is the",
    "start": "950120",
    "end": "958000"
  },
  {
    "text": "driver tasks to run on the empty slots right so",
    "start": "958000",
    "end": "963440"
  },
  {
    "text": "the driver has auler in it it basically takes your code from The Notebook and it schedules it to run uh in a distributed",
    "start": "963440",
    "end": "970480"
  },
  {
    "text": "cluster however that is not what we're going to be doing today we're going to be using spark in local mode today um",
    "start": "970480",
    "end": "976959"
  },
  {
    "text": "local mode spark is used for prototyping and learning the spark API this is what",
    "start": "976959",
    "end": "982040"
  },
  {
    "text": "we're actually running right now your notebook meaning this notebook I'm looking at right now is uh connected to",
    "start": "982040",
    "end": "988680"
  },
  {
    "text": "an ec2 machine in Amazon in which there is a 6 gab container within which there",
    "start": "988680",
    "end": "995240"
  },
  {
    "text": "is a 3.7 GB jvm now this special jvm actually has instantiated the code for",
    "start": "995240",
    "end": "1002399"
  },
  {
    "text": "the driver and the executive classes so we've basically collapsed the driver and the executor in one jvm and this uh uh",
    "start": "1002399",
    "end": "1011120"
  },
  {
    "text": "jvm has three slots we'll come back to like why this is important later but",
    "start": "1011120",
    "end": "1016880"
  },
  {
    "text": "essentially what that means is you can run three tasks at a time if for whatever reason you need to run six",
    "start": "1016880",
    "end": "1022519"
  },
  {
    "text": "tasks that's doable um you can run six tasks but you're going to have to do three and then three more all right so",
    "start": "1022519",
    "end": "1029839"
  },
  {
    "text": "that's going to come into play in a little bit when we have to kind of re architect a few things for for",
    "start": "1029839",
    "end": "1035280"
  },
  {
    "text": "performance so I work at data bricks the company created by the founders of spark",
    "start": "1035280",
    "end": "1040918"
  },
  {
    "text": "um one fact I'll tell you about data bricks is that in 2015 the engineering team at data BR contributed 10 times",
    "start": "1040919",
    "end": "1047640"
  },
  {
    "text": "more source code to spar than any other organization okay so let's get to our",
    "start": "1047640",
    "end": "1055480"
  },
  {
    "text": "analysis if you put your cursor in this cell and hit control enter or shift",
    "start": "1055480",
    "end": "1061880"
  },
  {
    "text": "enter um I'll I'll hit control enter here because I want to I don't want to move the cursor notice what I'm doing is",
    "start": "1061880",
    "end": "1067760"
  },
  {
    "text": "I'm running the file system magic command in data breaks and I'm listing the contents of this directory and I see",
    "start": "1067760",
    "end": "1074880"
  },
  {
    "text": "a CSV file in here this is a CSV file I exported a couple of or actually",
    "start": "1074880",
    "end": "1080320"
  },
  {
    "text": "yesterday and um this file is a 1.6 GB file this is a file that we want to uh",
    "start": "1080320",
    "end": "1088000"
  },
  {
    "text": "analyze initially with spark so what we'll do is we'll use the",
    "start": "1088000",
    "end": "1093039"
  },
  {
    "text": "spark object this is a new object that uh uh came into existence only in spark",
    "start": "1093039",
    "end": "1098400"
  },
  {
    "text": "2.0 so basically if you just like type in spark in a cell and hit control enter or",
    "start": "1098400",
    "end": "1105720"
  },
  {
    "text": "just move to the cell and hit control enter you'll see that this is a handle for a spark session um the handle for",
    "start": "1105720",
    "end": "1111799"
  },
  {
    "text": "all functionality in spark basically this is the handle to get into spark streaming machine learning SQL data",
    "start": "1111799",
    "end": "1118720"
  },
  {
    "text": "frames and graph processing so this is what we'll do next we're we're going to use the spark",
    "start": "1118720",
    "end": "1126440"
  },
  {
    "text": "object to read the CSV file at that path I just showed",
    "start": "1126440",
    "end": "1132000"
  },
  {
    "text": "you and what I'm going to say here is a couple more extra par",
    "start": "1132000",
    "end": "1139240"
  },
  {
    "text": "there is a header in the file so header true and I want spark to infer the",
    "start": "1139240",
    "end": "1144520"
  },
  {
    "text": "schema um you saw how many columns there are like I don't want to be defining all the columns manually so I'm going to",
    "start": "1144520",
    "end": "1150280"
  },
  {
    "text": "take a Gamble and see if spark can figure out the the the data types for all the columns all right so put your",
    "start": "1150280",
    "end": "1156240"
  },
  {
    "text": "cursor in here and go ahead and uh do control enter or um shift enter so what spark is doing now is",
    "start": "1156240",
    "end": "1163760"
  },
  {
    "text": "actually kicking off a job that is doing a read of the data and it's sampling",
    "start": "1163760",
    "end": "1169840"
  },
  {
    "text": "some of the rows and columns to infer the data types and now if you do this in first",
    "start": "1169840",
    "end": "1178120"
  },
  {
    "text": "schema against terabytes of data this will do pretty much a full scan of all",
    "start": "1178120",
    "end": "1183520"
  },
  {
    "text": "your data which is maybe not optimal if that's going to take like 20 minutes",
    "start": "1183520",
    "end": "1189080"
  },
  {
    "text": "right so I'm going to show you a different technique notice this took about 20 um what is that 28 seconds or",
    "start": "1189080",
    "end": "1195280"
  },
  {
    "text": "so to go through like 1.6 gab of a CS v file um so before I even do anything",
    "start": "1195280",
    "end": "1202960"
  },
  {
    "text": "with this data frame this fire service calls data frame before I do anything with that I'll show you the alternative",
    "start": "1202960",
    "end": "1209679"
  },
  {
    "text": "approach of how to do this so you can import a bunch of Pi spark or python",
    "start": "1209679",
    "end": "1215840"
  },
  {
    "text": "spark specific data types and then here I've defined my schema manually notice",
    "start": "1215840",
    "end": "1221919"
  },
  {
    "text": "that for every column here I'm just basically giving it either a string type or integer type and I'm saying that all",
    "start": "1221919",
    "end": "1229039"
  },
  {
    "text": "of these columns can be null so true just means the column is nullable now",
    "start": "1229039",
    "end": "1236039"
  },
  {
    "text": "um uh one thing I'll do is I'll go ahead and run this",
    "start": "1236039",
    "end": "1241880"
  },
  {
    "text": "cell it took only a half a second to run and then look at what I'm doing now I'm",
    "start": "1242080",
    "end": "1247440"
  },
  {
    "text": "still using the spark object to read the CSV file but this time I'm saying schema is my fire schema from above notice if I",
    "start": "1247440",
    "end": "1255360"
  },
  {
    "text": "run this cell there's no spark job that kicked off it just used the schema I gave it um there is one more reason I'm",
    "start": "1255360",
    "end": "1262080"
  },
  {
    "text": "doing this um so another reason I'm actually manually putting in schema is because this data if you actually go to",
    "start": "1262080",
    "end": "1269159"
  },
  {
    "text": "the city of San Francisco's data uh on the website and if you look at the column names they have spaces in them so",
    "start": "1269159",
    "end": "1276039"
  },
  {
    "text": "the column names like have like spaces in here like this and this will cause",
    "start": "1276039",
    "end": "1282679"
  },
  {
    "text": "problems with spark so I am bypassing all those problems by giving it a manual",
    "start": "1282679",
    "end": "1287919"
  },
  {
    "text": "scheme and moving all the spaces um so those are two things you should know so",
    "start": "1287919",
    "end": "1294039"
  },
  {
    "text": "let's just continue on with our data analysis okay now I have this data frame from my infer uh from my static",
    "start": "1294039",
    "end": "1302919"
  },
  {
    "text": "schema um very often what I like to do when I have a data frame is just look at a first few uh Records in it what I'm",
    "start": "1302919",
    "end": "1309400"
  },
  {
    "text": "doing here is on the data frame I'm limiting it to five rows and then passing that into the display function",
    "start": "1309400",
    "end": "1315240"
  },
  {
    "text": "in data breaks to uh create a uh HTM table of the data and here I see it I do",
    "start": "1315240",
    "end": "1321520"
  },
  {
    "text": "see that the um column names don't have spaces in it that that's pretty good um and yeah I mean the data looks",
    "start": "1321520",
    "end": "1328840"
  },
  {
    "text": "fine uh you know it's all there there's a lot of columns actually but that looks kind of similar",
    "start": "1328840",
    "end": "1335760"
  },
  {
    "text": "to what I looked I saw on the internet on the website earlier okay my my data frame uh did load the data successfully",
    "start": "1335760",
    "end": "1343200"
  },
  {
    "text": "on my data frame I can call the columns uh method just to print out all the columns this gives you like a python",
    "start": "1343200",
    "end": "1349279"
  },
  {
    "text": "list back okay next I like to always run",
    "start": "1349279",
    "end": "1355400"
  },
  {
    "text": "count I like to do it for two reasons one to understand how many rows are in my data frame meaning how many like",
    "start": "1355400",
    "end": "1361320"
  },
  {
    "text": "lines are in the CSV file also I like to run count because count does a full scan",
    "start": "1361320",
    "end": "1367120"
  },
  {
    "text": "of the data which means that my jvm is having to read 1.6 gabt of the CSV file",
    "start": "1367120",
    "end": "1374880"
  },
  {
    "text": "remotely from S3 so this give me gives me just a sense of how long a full scan is going to take you know before I",
    "start": "1374880",
    "end": "1381320"
  },
  {
    "text": "started doing complicated SQL queries just a simple scan for benchmarking reasons 20 seconds okay so that sets the",
    "start": "1381320",
    "end": "1388840"
  },
  {
    "text": "stage the simplest full scan against S3 is 20 seconds um I have four um M four",
    "start": "1388840",
    "end": "1396200"
  },
  {
    "text": "million or so a little more than four million rows in my data frame okay cool go ahead and open up",
    "start": "1396200",
    "end": "1404600"
  },
  {
    "text": "these uh spark 2 documentation if you'd like to so just like you know right",
    "start": "1404600",
    "end": "1410440"
  },
  {
    "text": "click on these go to New Link um we usually don't give these links out often",
    "start": "1410440",
    "end": "1415640"
  },
  {
    "text": "because they're kind of preview documents but if you go to that first link and then click on the very bottom",
    "start": "1415640",
    "end": "1421039"
  },
  {
    "text": "link here you'll see that um this will give you access to the spark 2.o documentation I basically just opened up",
    "start": "1421039",
    "end": "1428080"
  },
  {
    "text": "this first link here in a new tab and I See Spark 2.0 here then go to",
    "start": "1428080",
    "end": "1433840"
  },
  {
    "text": "programming guides and see this uh right here data frames data sets and SQL this is going to be your um your source of",
    "start": "1433840",
    "end": "1442039"
  },
  {
    "text": "Truth for the API so click on that and then you know there's a lot of stuff",
    "start": "1442039",
    "end": "1448039"
  },
  {
    "text": "here just keep this tab open and maybe we'll come back to this",
    "start": "1448039",
    "end": "1453240"
  },
  {
    "text": "when we need to refer to something another thing to do is go to API docs here and right click on Python and open",
    "start": "1453240",
    "end": "1459480"
  },
  {
    "text": "this up as well all right this gives you access to the full API methods of py spark so just those are the two main",
    "start": "1459480",
    "end": "1466120"
  },
  {
    "text": "things you need to have open right now okay I have my documents open I'm ready",
    "start": "1466120",
    "end": "1472159"
  },
  {
    "text": "to start doing some uh data frame queries there are two fundamental operations in spark Transformations and",
    "start": "1472159",
    "end": "1479760"
  },
  {
    "text": "actions here's an example of some Transformations like select to pull out just a few columns out of my 30 columns",
    "start": "1479760",
    "end": "1487399"
  },
  {
    "text": "you can do distinct Group by Sum order by um Transformations essentially",
    "start": "1487399",
    "end": "1493039"
  },
  {
    "text": "essentially take the the First Data frame and give you a new data frame back",
    "start": "1493039",
    "end": "1499000"
  },
  {
    "text": "so that's what Transformations are doing they just basically take a data frame modify in some way give you a new data",
    "start": "1499000",
    "end": "1504960"
  },
  {
    "text": "frame back all right um actions are used to print",
    "start": "1504960",
    "end": "1511360"
  },
  {
    "text": "results Transformations are lazy actions trigger computation to happen um let me",
    "start": "1511360",
    "end": "1518120"
  },
  {
    "text": "show you what I mean by that so the first question um will answer is how many different types of",
    "start": "1518120",
    "end": "1525440"
  },
  {
    "text": "calls were made to the fire department of San Francisco so um on my data frame",
    "start": "1525440",
    "end": "1531000"
  },
  {
    "text": "I can call the select transformation yank out just the call type column and",
    "start": "1531000",
    "end": "1536679"
  },
  {
    "text": "then call this action show five um select is a transformation and show is an action and if I run this cell I",
    "start": "1536679",
    "end": "1543880"
  },
  {
    "text": "should see the um an asky print out of five of the um first call",
    "start": "1543880",
    "end": "1551679"
  },
  {
    "text": "types now notice something okay if I remove my action here I'm I'm deleting",
    "start": "1551679",
    "end": "1556919"
  },
  {
    "text": "the do show okay if delete that look at what's going to happen see the spark JW",
    "start": "1556919",
    "end": "1562000"
  },
  {
    "text": "that I ran earlier well if you just call Transformations no spark jobs",
    "start": "1562000",
    "end": "1568320"
  },
  {
    "text": "run um in order to trigger any computation to happen you have to call an action like show and then that",
    "start": "1568320",
    "end": "1575080"
  },
  {
    "text": "triggers the dag or or or basically the the um the graph of all the previous",
    "start": "1575080",
    "end": "1580880"
  },
  {
    "text": "transformations to run okay so it looks like those are some of the reasons",
    "start": "1580880",
    "end": "1586039"
  },
  {
    "text": "people call the San Francisco Fire Department but I want to understand all the reasons",
    "start": "1586039",
    "end": "1591120"
  },
  {
    "text": "so I'm going to call two different Transformations first out of all four million rows select out the call type",
    "start": "1591120",
    "end": "1599200"
  },
  {
    "text": "for every row so this this gives me um that gives me 4 million uh specific call",
    "start": "1599200",
    "end": "1605440"
  },
  {
    "text": "types but sometimes there's duplicates so I want to get rid of those and call the distinct transformation and then I'm",
    "start": "1605440",
    "end": "1611559"
  },
  {
    "text": "going to call show and I actually know that there's actually 35 specific types of call types so that's why I'm passing",
    "start": "1611559",
    "end": "1618559"
  },
  {
    "text": "in passing in show 35 and the reason I'm passing in false is because sometimes",
    "start": "1618559",
    "end": "1624240"
  },
  {
    "text": "the call type is pretty long of a string and I want I don't want to truncate",
    "start": "1624240",
    "end": "1630240"
  },
  {
    "text": "it so let me show you the results and you'll you'll see what this means 35 means show me uh",
    "start": "1630240",
    "end": "1637000"
  },
  {
    "text": "35 results uh 35 rows and false just means um keep the width here as long as",
    "start": "1637000",
    "end": "1645440"
  },
  {
    "text": "the longest item in here okay so these are the reasons people",
    "start": "1645440",
    "end": "1650960"
  },
  {
    "text": "called the city of San Francisco okay but what I really want to",
    "start": "1650960",
    "end": "1656399"
  },
  {
    "text": "know is which one of these um call types gets the most calls right like this is",
    "start": "1656399",
    "end": "1662200"
  },
  {
    "text": "kind of cool but I want to get a count for each one of these call types so",
    "start": "1662200",
    "end": "1667279"
  },
  {
    "text": "that's what I'm doing here I'm calling a few more Transformations actually I I call Select then I want to group by the",
    "start": "1667279",
    "end": "1673840"
  },
  {
    "text": "call type one of the 35 then I want to count how many are in that group and",
    "start": "1673840",
    "end": "1679360"
  },
  {
    "text": "then order by the count in ascending order fall so descending order and",
    "start": "1679360",
    "end": "1686200"
  },
  {
    "text": "finally I should get like my first interesting Insight um why are the people of San Francisco the residents",
    "start": "1686200",
    "end": "1692559"
  },
  {
    "text": "here or tourists um calling the city of San Francisco's fire department any",
    "start": "1692559",
    "end": "1699320"
  },
  {
    "text": "guesses um my roommate in Florida is actually a fire uh fighter so I knew it",
    "start": "1699320",
    "end": "1704679"
  },
  {
    "text": "would be medical incidents um this is by far the main reason uh firefighters get called um you can see here it's about",
    "start": "1704679",
    "end": "1712720"
  },
  {
    "text": "2.5 million times that happened this is massive we have 4 million rows 2.5 of",
    "start": "1712720",
    "end": "1720559"
  },
  {
    "text": "them refer to Medical incidents and then uh half a million are for structural fires alarms traffic collisions where um",
    "start": "1720559",
    "end": "1731399"
  },
  {
    "text": "158,000 other C citizen assist outside fire administrative it drops pretty",
    "start": "1731399",
    "end": "1737600"
  },
  {
    "text": "severely now one thing is um",
    "start": "1737600",
    "end": "1742799"
  },
  {
    "text": "that actually I'll get back to the graphing in a second let's just continue um I do want to point out this though",
    "start": "1744720",
    "end": "1750840"
  },
  {
    "text": "notice it took 14 seconds to run that query right this query right here to do",
    "start": "1750840",
    "end": "1756880"
  },
  {
    "text": "all of these Transformations took 14 seconds in a in a little bit we will learn how to cache data into memory so",
    "start": "1756880",
    "end": "1764720"
  },
  {
    "text": "that this will become like 50 times faster but let's hold off on that for a",
    "start": "1764720",
    "end": "1771519"
  },
  {
    "text": "second another thing I want to do before we take a break is I want to do some time in data analysis so recall that my",
    "start": "1771519",
    "end": "1779200"
  },
  {
    "text": "data frame schema um only has strings and integers and this is problematic",
    "start": "1779200",
    "end": "1784399"
  },
  {
    "text": "because you know these um columns are actually timestamps um but when I provided the",
    "start": "1784399",
    "end": "1791000"
  },
  {
    "text": "schema I just said it was a string because it's easiest to load the data that way but um let me show you how you",
    "start": "1791000",
    "end": "1797399"
  },
  {
    "text": "would cast The Columns that are actually incorrectly labeled as strings right now",
    "start": "1797399",
    "end": "1803279"
  },
  {
    "text": "into time stamps so you can do time based",
    "start": "1803279",
    "end": "1808000"
  },
  {
    "text": "queries and then we'll take a short break okay so you can look at the Unix",
    "start": "1809000",
    "end": "1814200"
  },
  {
    "text": "timestamp um uh docs here if you're interested but here's the the gist of it",
    "start": "1814200",
    "end": "1819799"
  },
  {
    "text": "you're basically going to do some imports to get the um to get basically",
    "start": "1819799",
    "end": "1825120"
  },
  {
    "text": "the Unix timestamp function and then um um let me just scroll up really quickly and remind you what the this data looks",
    "start": "1825120",
    "end": "1832279"
  },
  {
    "text": "like there's two different types of um timestamps I've notice there's this type",
    "start": "1832279",
    "end": "1837440"
  },
  {
    "text": "of time stamp which is really just a date it's month day and year and then um there's two columns like that and then",
    "start": "1837440",
    "end": "1844039"
  },
  {
    "text": "the other pattern that I noticed is like this it's the you know month day year and then this full time stamp and AM PM",
    "start": "1844039",
    "end": "1850640"
  },
  {
    "text": "and you see all of these time stamps are actually like that there's two patterns I need to convert all these strings into",
    "start": "1850640",
    "end": "1856360"
  },
  {
    "text": "um uh time stamps so that is what I'm doing down here I",
    "start": "1856360",
    "end": "1861639"
  },
  {
    "text": "kind of already gave you the code so you can uh quickly just uh review",
    "start": "1861639",
    "end": "1867679"
  },
  {
    "text": "it here's the two patterns that's the first pattern remember and this is the",
    "start": "1867919",
    "end": "1873440"
  },
  {
    "text": "second pattern what I'm going to do is for all these columns like this one I'm",
    "start": "1873440",
    "end": "1879480"
  },
  {
    "text": "going to say that this was of type pattern one cast it to timestamp um this was of type pattern two cast it to",
    "start": "1879480",
    "end": "1886080"
  },
  {
    "text": "timestamp and then um make a new column that is the same column name but then",
    "start": "1886080",
    "end": "1891120"
  },
  {
    "text": "just put TS at the end of it and then drop the old column right that's what I'm doing for like the seven timestamp",
    "start": "1891120",
    "end": "1897679"
  },
  {
    "text": "type of columns I'm just casting it to an actual Tim stamp then I'm dropping",
    "start": "1897679",
    "end": "1902720"
  },
  {
    "text": "the old column and I'm calling this new derived column um the exact same name",
    "start": "1902720",
    "end": "1908039"
  },
  {
    "text": "but with a TS at the end of it all right that's all I'm doing it's a simple",
    "start": "1908039",
    "end": "1914398"
  },
  {
    "text": "cast okay so let me run this cell and next on this new data frame that now",
    "start": "1914880",
    "end": "1923120"
  },
  {
    "text": "I tacked on CTS in there on this new data frame name let me print schema and I'm hoping i'll see time stamp",
    "start": "1923120",
    "end": "1930039"
  },
  {
    "text": "now um if I scroll down",
    "start": "1930039",
    "end": "1936519"
  },
  {
    "text": "actually did that happen lower down oh yeah all the time",
    "start": "1936519",
    "end": "1942960"
  },
  {
    "text": "stamp columns are at the bottom actually so there they are",
    "start": "1942960",
    "end": "1948440"
  },
  {
    "text": "okay you can also just do a limit five and and you know display the data frame this new data frame that it has TS in",
    "start": "1949360",
    "end": "1956799"
  },
  {
    "text": "its name you can do that here as well and uh the the main way you can know that this uh transformation happened is",
    "start": "1956799",
    "end": "1963200"
  },
  {
    "text": "if you go all the way to the end where the time stamps are and notice how they show up differently now so it's just",
    "start": "1963200",
    "end": "1969120"
  },
  {
    "text": "like it's formatted differently and that's a clue that the um conversion and the cast did",
    "start": "1969120",
    "end": "1975320"
  },
  {
    "text": "happen Okay so what will I want to do with this",
    "start": "1975320",
    "end": "1980639"
  },
  {
    "text": "now um let's run a couple of cells and we'll take a quick 10-minute break on",
    "start": "1980639",
    "end": "1986159"
  },
  {
    "text": "this new data frame uh Alias I'm going to select out the year from the call",
    "start": "1986159",
    "end": "1992559"
  },
  {
    "text": "date column there's a column that basically has the date the call came in it's just month year date from that I'm",
    "start": "1992559",
    "end": "2000039"
  },
  {
    "text": "going to use the year function to pull out just the year like 2014 or 2015 from",
    "start": "2000039",
    "end": "2005960"
  },
  {
    "text": "that I want to know distinctly like how many years of data do I have so uh you",
    "start": "2005960",
    "end": "2011559"
  },
  {
    "text": "know for all the four million rows um let's do distinct on that and then order by year and",
    "start": "2011559",
    "end": "2018080"
  },
  {
    "text": "show so how many years of data am I looking",
    "start": "2018080",
    "end": "2022639"
  },
  {
    "text": "at notice this is starting to take a bit of time to run now if I expand this",
    "start": "2026080",
    "end": "2031880"
  },
  {
    "text": "you'll notice that it's kicking off like a spark job 18 with two stages and it's",
    "start": "2031880",
    "end": "2036919"
  },
  {
    "text": "like running three tasks right now and let's go and minimize this because I'm I'm going to explain what's going on",
    "start": "2036919",
    "end": "2042480"
  },
  {
    "text": "here behind the scenes after the Break um I just want to introduce you to the API right now without getting into the",
    "start": "2042480",
    "end": "2049158"
  },
  {
    "text": "internals um well it looks like I have data from 200 until about",
    "start": "2049159",
    "end": "2054440"
  },
  {
    "text": "2016 so 16 years of data so in 16 years about 4 million",
    "start": "2054440",
    "end": "2061560"
  },
  {
    "text": "calls came came in to the firefighters of San Francisco and as I start doing",
    "start": "2061560",
    "end": "2066599"
  },
  {
    "text": "more and more complicated queries my query time is going up which is a little alarming now it's taking 25 seconds to",
    "start": "2066599",
    "end": "2072599"
  },
  {
    "text": "run this query so I definitely want to start using cash in a little bit uh and",
    "start": "2072599",
    "end": "2078480"
  },
  {
    "text": "start moving things to memory all right one last thing and we'll take a break how many service calls were logged in",
    "start": "2078480",
    "end": "2085440"
  },
  {
    "text": "just the last seven days um uh July 6th which is today is",
    "start": "2085440",
    "end": "2092240"
  },
  {
    "text": "the 18 87th day of the year so I basically want to know um",
    "start": "2092240",
    "end": "2098160"
  },
  {
    "text": "everything above day 180 all right so just skim through this code uh in my data frame I'm going to",
    "start": "2098160",
    "end": "2104880"
  },
  {
    "text": "filter down for year 2016 then do another filter",
    "start": "2104880",
    "end": "2110119"
  },
  {
    "text": "for uh keep only the data where the day of year is greater than 180 because 180",
    "start": "2110119",
    "end": "2117119"
  },
  {
    "text": "is basically 7 days ago day 180 of the year and then look at what I'm doing I'm",
    "start": "2117119",
    "end": "2123280"
  },
  {
    "text": "selecting out I'm selecting out the call Dat",
    "start": "2123280",
    "end": "2128800"
  },
  {
    "text": "column however from that I'm deriving the day of year so I'm actually getting",
    "start": "2128800",
    "end": "2134240"
  },
  {
    "text": "the day of year which is going to be like 180 or 181 or 182 I'm getting that and then I'm getting that distinctly and",
    "start": "2134240",
    "end": "2141400"
  },
  {
    "text": "then ordering by day of year let's run that and just look at",
    "start": "2141400",
    "end": "2146640"
  },
  {
    "text": "what comes back once again it's running a spark job",
    "start": "2146640",
    "end": "2153520"
  },
  {
    "text": "show is my action and everything before this are transformations filters a",
    "start": "2153520",
    "end": "2158640"
  },
  {
    "text": "transformation select is a transformation distinct and order by are all transformations um okay I do have",
    "start": "2158640",
    "end": "2165280"
  },
  {
    "text": "seven days of data here 180 to 186 these days were found in",
    "start": "2165280",
    "end": "2171000"
  },
  {
    "text": "the data set um that took 25 seconds to run look at this if I get rid of the",
    "start": "2171000",
    "end": "2177599"
  },
  {
    "text": "show this just like doesn't do anything it comes back in a tenth of a second or something you have to call an action",
    "start": "2177599",
    "end": "2184640"
  },
  {
    "text": "before computation happens okay um let's run this was this the same",
    "start": "2184640",
    "end": "2191800"
  },
  {
    "text": "exact cell oh this is doing a count now so",
    "start": "2191800",
    "end": "2199079"
  },
  {
    "text": "we're doing something similar we're filtering out for 2016 filter out for just the last s days and then count how",
    "start": "2199079",
    "end": "2206520"
  },
  {
    "text": "many calls came in in each of the last seven days and then order by um the day",
    "start": "2206520",
    "end": "2212000"
  },
  {
    "text": "of year this should tell us this should give us two columns it should give us on",
    "start": "2212000",
    "end": "2219280"
  },
  {
    "text": "the left a column for the day of year like 180 182 183 we should get seven of",
    "start": "2219280",
    "end": "2224599"
  },
  {
    "text": "those rows and then the second column should give us a count of how much how many calls came in on that day um by the",
    "start": "2224599",
    "end": "2232200"
  },
  {
    "text": "way July 4th was the 185th day of the year and I was curious to see if there would be a spike um the spike actually",
    "start": "2232200",
    "end": "2239200"
  },
  {
    "text": "happened after midnight um there were a thousand calls roughly that happened um on July 5th and",
    "start": "2239200",
    "end": "2247560"
  },
  {
    "text": "I'm assuming that's because of partying that was that was probably going on okay last thing before a break is if",
    "start": "2247560",
    "end": "2255920"
  },
  {
    "text": "you run this cell right here it's basically the same code AS up there except I've wrapped it in display",
    "start": "2255920",
    "end": "2263640"
  },
  {
    "text": "and this is going to give me a bar chart which is going to make it a little bit easier for me to visualize the last",
    "start": "2263640",
    "end": "2269319"
  },
  {
    "text": "seven days of call records to see if there was a spike on a specific day so notice what I did here is",
    "start": "2269319",
    "end": "2276520"
  },
  {
    "text": "basically I took this exact same code but I got",
    "start": "2276520",
    "end": "2282800"
  },
  {
    "text": "rid of the show okay I took this exact same code I copied and pasted it but I deleted the",
    "start": "2282800",
    "end": "2288319"
  },
  {
    "text": "show and I replaced the uh actually I just deleted the show that's it and then I wrapped the whole thing and",
    "start": "2288319",
    "end": "2295240"
  },
  {
    "text": "display and then um you get back this bar graph and this is showing me the days day 180 181 182 and then it shows",
    "start": "2295240",
    "end": "2302880"
  },
  {
    "text": "me how many calls came in on that day by the way you can see the table representation of this by clicking on",
    "start": "2302880",
    "end": "2309079"
  },
  {
    "text": "the table on the left see this table icon you can click on that and just see the table representation or click on",
    "start": "2309079",
    "end": "2316400"
  },
  {
    "text": "this and do another visualization like I don't know pie chart doesn't really make sense but it's pretty I guess or you can",
    "start": "2316400",
    "end": "2324040"
  },
  {
    "text": "go to bar and then you can expand it out by dragging this",
    "start": "2324040",
    "end": "2329359"
  },
  {
    "text": "over so this is built in datab bricks visualization um and uh we'll take a",
    "start": "2329359",
    "end": "2335160"
  },
  {
    "text": "break before we continue with the next section where um we'll take a 10-minute break in just a minute but I do want to warn you that",
    "start": "2335160",
    "end": "2341560"
  },
  {
    "text": "if you came here to kind of understand what happens with spark behind the scenes how to do optimizations how to",
    "start": "2341560",
    "end": "2347839"
  },
  {
    "text": "use the spark interface um to really understand what's going on that's what I'll do right after the Break um but",
    "start": "2347839",
    "end": "2354720"
  },
  {
    "text": "before that I'll leave you with one more thing as far as visualizations go if you go here to workspace and then go to data",
    "start": "2354720",
    "end": "2362000"
  },
  {
    "text": "Brick's guide on the left data Brick's guide is a manual to Apache Spark and",
    "start": "2362000",
    "end": "2367599"
  },
  {
    "text": "data breaks 90% of what's in here is just about Apache spark and then maybe like 5 or 10% of this is some specific",
    "start": "2367599",
    "end": "2375000"
  },
  {
    "text": "stuff about the implementation of spark that we have at data bricks what I wanted to show you here is just I'm",
    "start": "2375000",
    "end": "2380800"
  },
  {
    "text": "going to scroll down to visualizations we're not going to have much time to do fancy visualizations today however maybe",
    "start": "2380800",
    "end": "2387400"
  },
  {
    "text": "I want to do a D3 visualization so I'm going to click on D3 on the left and then if I scroll down here notice that",
    "start": "2387400",
    "end": "2394560"
  },
  {
    "text": "you can actually do do display HTML and embed any JavaScript based visualization",
    "start": "2394560",
    "end": "2400520"
  },
  {
    "text": "library that you want basically data Bri supports like D3 map plot live uh Google",
    "start": "2400520",
    "end": "2407040"
  },
  {
    "text": "visualization like Google charts caborn um anything JavaScript based notice at",
    "start": "2407040",
    "end": "2412839"
  },
  {
    "text": "the very bottom of this page is indeed a D3 visualization you can see it's really just display HTML and then some HTML but",
    "start": "2412839",
    "end": "2420720"
  },
  {
    "text": "if I scroll down here there's this and if I move around",
    "start": "2420720",
    "end": "2427760"
  },
  {
    "text": "it it looks cool I don't really know what this is but um it's a D3 visualization now you can bring this",
    "start": "2427760",
    "end": "2434680"
  },
  {
    "text": "notebook and make this notebook come alive I can't edit this code right now like you know I can't click on this it's",
    "start": "2434680",
    "end": "2440520"
  },
  {
    "text": "kind of annoying but what I can do is I can click import up here I can do this for any notebook and datab guide and I",
    "start": "2440520",
    "end": "2446400"
  },
  {
    "text": "can just like say I want to import to my that to my workspace here click clone",
    "start": "2446400",
    "end": "2451760"
  },
  {
    "text": "and then in my workspace oh there we go now this notebook is in the root of my workspace here",
    "start": "2451760",
    "end": "2457880"
  },
  {
    "text": "and now what I can do is pretty cool I can actually attach this to my cluster and uh now I can run all these cells and",
    "start": "2457880",
    "end": "2463920"
  },
  {
    "text": "I can start messing around with that D3 visualization I can like run this one and you can do this for all the two or",
    "start": "2463920",
    "end": "2470040"
  },
  {
    "text": "300 notebooks in data br's Guide and that'll be a really awesome way for you to continue learning",
    "start": "2470040",
    "end": "2476079"
  },
  {
    "text": "spark um here's where I'm going to like kind of go a little bit deeper around what's going on behind the",
    "start": "2476079",
    "end": "2481599"
  },
  {
    "text": "scenes um if this section is a little bit too",
    "start": "2481599",
    "end": "2488480"
  },
  {
    "text": "fast then um there's going to be a YouTube recording of this and the link",
    "start": "2488480",
    "end": "2494800"
  },
  {
    "text": "will be poster in a couple of days on the meetup.com page so basically just feel free to just even watch me do this",
    "start": "2494800",
    "end": "2501560"
  },
  {
    "text": "as a demo if it's like hard to keep up for some reason um you can review the",
    "start": "2501560",
    "end": "2506599"
  },
  {
    "text": "recording later another uh question that came up during the break is um does data",
    "start": "2506599",
    "end": "2511880"
  },
  {
    "text": "bricks Community Edition like just run forever for free uh what we do is actually we turn terminate that cluster",
    "start": "2511880",
    "end": "2518760"
  },
  {
    "text": "that you launched after 1 hour of inactivity um so that's how we keep this",
    "start": "2518760",
    "end": "2524000"
  },
  {
    "text": "affordable and free for everyone all right so you can run you can run as long as you need to but if you it's",
    "start": "2524000",
    "end": "2529520"
  },
  {
    "text": "inactivity for an hour it'll self-terminate okay to get back to the notebook I was in um what I'm going to",
    "start": "2529520",
    "end": "2535880"
  },
  {
    "text": "do is on the left I'm going to click on resent and I'm going to go to the fire",
    "start": "2535880",
    "end": "2541640"
  },
  {
    "text": "um incident exploration notebook and um I see it's still connected to my cluster and just to like",
    "start": "2541640",
    "end": "2549240"
  },
  {
    "text": "refresh your memory on what we did so far um we learned a little bit about",
    "start": "2549240",
    "end": "2558640"
  },
  {
    "text": "spark we learned that this is like what a distributed Spar cluster looks like with you know many different executors",
    "start": "2560680",
    "end": "2567240"
  },
  {
    "text": "running on many slave machines in a data center but I did clarify that that's not what we're using right now we're using",
    "start": "2567240",
    "end": "2574440"
  },
  {
    "text": "this local mode type of Spar cluster is a Proto for prototyping purposes it's a",
    "start": "2574440",
    "end": "2579960"
  },
  {
    "text": "3.7 GB jvm and uh you know on this ec2",
    "start": "2579960",
    "end": "2585280"
  },
  {
    "text": "machine there's actually about 10 different containers running and perhaps nine other peers from this Workshop are",
    "start": "2585280",
    "end": "2592079"
  },
  {
    "text": "also on the same ECT machine as you but in isolated containers um so basically what we did",
    "start": "2592079",
    "end": "2598400"
  },
  {
    "text": "so far is we like did a directory listing on S3 to just look at that CSV file uh I pointed out the spark session",
    "start": "2598400",
    "end": "2605760"
  },
  {
    "text": "object and that spark object is what we used um to read the same CSV file and we",
    "start": "2605760",
    "end": "2613400"
  },
  {
    "text": "said that the header is true and then we tried to infers schema initially and notice that it took like 25 seconds and",
    "start": "2613400",
    "end": "2620440"
  },
  {
    "text": "uh we got the data frame back but it had to do a scan and the infers schema um also brings in the column names the way",
    "start": "2620440",
    "end": "2627920"
  },
  {
    "text": "they are in the file with spaces in them and I don't want that because that's going to cause problems later on like",
    "start": "2627920",
    "end": "2633440"
  },
  {
    "text": "when I try to save a parquet file so for those two reasons because of the fact of",
    "start": "2633440",
    "end": "2638800"
  },
  {
    "text": "infer schema taking too long to run for the full scan and I want to manually",
    "start": "2638800",
    "end": "2643920"
  },
  {
    "text": "give my column names without spaces for those two reasons I then showed you um",
    "start": "2643920",
    "end": "2649520"
  },
  {
    "text": "how you can actually provide schema yourself manually this is basically me",
    "start": "2649520",
    "end": "2655800"
  },
  {
    "text": "manually making a schema type I can name these columns whatever I want and I just",
    "start": "2655800",
    "end": "2662160"
  },
  {
    "text": "named them very similar to the file uh except I removed all spaces and I only",
    "start": "2662160",
    "end": "2667559"
  },
  {
    "text": "used string and integer types here actually I used a Boolean type down here",
    "start": "2667559",
    "end": "2672880"
  },
  {
    "text": "also okay so then I provided a schema manually and I noticed that this time around when I made the data frame it did",
    "start": "2672880",
    "end": "2679680"
  },
  {
    "text": "not run a job at all um because I already knew the schema I didn't have to infer the schema and then I you know",
    "start": "2679680",
    "end": "2685640"
  },
  {
    "text": "started looking at the data frame uh limit five five rows um look at the",
    "start": "2685640",
    "end": "2691720"
  },
  {
    "text": "columns count 20 seconds to read all the data and four million rows uh there's",
    "start": "2691720",
    "end": "2697920"
  },
  {
    "text": "Transformations and actions in the data frame API but in just a second I'll unlock the SQL API so if you're more",
    "start": "2697920",
    "end": "2704680"
  },
  {
    "text": "comfortable with that I'll show you how you can run SQL uh queries and if if you don't want to use the dataframe API um",
    "start": "2704680",
    "end": "2711880"
  },
  {
    "text": "the data frame API and Spark tries to be as close to the Panda's data frame API",
    "start": "2711880",
    "end": "2717160"
  },
  {
    "text": "as possible so if you're comfortable with that you'll find the data frames API very very natural however in pandas",
    "start": "2717160",
    "end": "2724400"
  },
  {
    "text": "Panda data frames if you call a transformation that is eager it happens",
    "start": "2724400",
    "end": "2730160"
  },
  {
    "text": "right away the computation happens as soon as you call a transformation in spark Transformations are lazy so they",
    "start": "2730160",
    "end": "2736880"
  },
  {
    "text": "just get build up build up build up until you call an action and the action will collapse a bunch of Transformations",
    "start": "2736880",
    "end": "2743640"
  },
  {
    "text": "and do a bunch of optimizations and then run your computation so that's the main way Panda's data frames are different",
    "start": "2743640",
    "end": "2749920"
  },
  {
    "text": "from spark data frames data frame this is the first implementation of data frames that are lazy meaning you can",
    "start": "2749920",
    "end": "2756359"
  },
  {
    "text": "tack on a 100 Transformations and nothing will happen until you call an action to look at results and then it",
    "start": "2756359",
    "end": "2763240"
  },
  {
    "text": "will it will figure out the most efficient way to run all those 100 Transformations um okay and then we like",
    "start": "2763240",
    "end": "2770680"
  },
  {
    "text": "started looking at the call type this is a uh you know one of the columns we were looking at um we noticed that um there's",
    "start": "2770680",
    "end": "2778599"
  },
  {
    "text": "like 35 different types of call most of them are medical um if this datetime part was",
    "start": "2778599",
    "end": "2785000"
  },
  {
    "text": "confusing to you then really all you need to remember is that the original data frame had a whole bunch of strings",
    "start": "2785000",
    "end": "2791760"
  },
  {
    "text": "for columns that were actually you know time or date so what I was doing here in",
    "start": "2791760",
    "end": "2797599"
  },
  {
    "text": "this is I'm just defining these two patterns and then applying them to",
    "start": "2797599",
    "end": "2803319"
  },
  {
    "text": "convert those column types from string to timestamp and I'm not I don't want to cover this in depth this is just some",
    "start": "2803319",
    "end": "2809599"
  },
  {
    "text": "sample code I wanted to show you for how to do that and then uh now I see that these last few columns show up as",
    "start": "2809599",
    "end": "2815960"
  },
  {
    "text": "timestamp which then unlocks essentially the ability to do this year this would",
    "start": "2815960",
    "end": "2823400"
  },
  {
    "text": "not have worked before um I'll prove that to you by removing actually I deleted the the old column didn't I if I",
    "start": "2823400",
    "end": "2830839"
  },
  {
    "text": "run this on the earlier data frame this would not work because this",
    "start": "2830839",
    "end": "2837960"
  },
  {
    "text": "year function only works when the column type has a timestamp in it if this was a",
    "start": "2837960",
    "end": "2846160"
  },
  {
    "text": "string uh this year function wouldn't uh work on that column type at all this is a",
    "start": "2846160",
    "end": "2852400"
  },
  {
    "text": "timestamp column type therefore I can apply the year function I pull out the",
    "start": "2852400",
    "end": "2858319"
  },
  {
    "text": "year um for every row then I call distinct to just keep the years distinctly and then order by this column",
    "start": "2858319",
    "end": "2866319"
  },
  {
    "text": "this derived column order by the year column okay let's just jump on to the",
    "start": "2866319",
    "end": "2872119"
  },
  {
    "text": "next section memory and caching notice those earlier queries were taking like 20 seconds",
    "start": "2872119",
    "end": "2877880"
  },
  {
    "text": "okay here's where uh things get a little bit interesting on my data frame the last data frame that has the correctly",
    "start": "2877880",
    "end": "2884079"
  },
  {
    "text": "defined time stamp columns I'm going to convert that data frame to an rdd and get the number of partitions and I'll",
    "start": "2884079",
    "end": "2890200"
  },
  {
    "text": "see that's 13 so what does this mean well internally in spark data frames are",
    "start": "2890200",
    "end": "2896319"
  },
  {
    "text": "made of partitions um here's a visual way of thinking about it my data frame",
    "start": "2896319",
    "end": "2902359"
  },
  {
    "text": "fire service calls timestamp data frame is made of 13 partitions",
    "start": "2902359",
    "end": "2908400"
  },
  {
    "text": "so basically a 13th of the data is in each partition the reason these this is",
    "start": "2908400",
    "end": "2914599"
  },
  {
    "text": "all dashed is because technically this data frame doesn't exist in memory this",
    "start": "2914599",
    "end": "2920040"
  },
  {
    "text": "is just an idea of a data frame if I try to do something with this data frame",
    "start": "2920040",
    "end": "2925319"
  },
  {
    "text": "like a count action or on this idea of a data frame if I call five",
    "start": "2925319",
    "end": "2930760"
  },
  {
    "text": "Transformations and then an action then spark actually starts reading the first",
    "start": "2930760",
    "end": "2935839"
  },
  {
    "text": "CSV file from disk on S3 and it then materializes this data frame for some",
    "start": "2935839",
    "end": "2942839"
  },
  {
    "text": "period of time and then immediately transforms this data frame in memory into three or four or five other data",
    "start": "2942839",
    "end": "2950040"
  },
  {
    "text": "frames depending on how many Transformations I'm doing um",
    "start": "2950040",
    "end": "2955400"
  },
  {
    "text": "but uh here's the thing 13 is not optimal um if you really want to know",
    "start": "2955400",
    "end": "2961240"
  },
  {
    "text": "how to use spark correctly you will not go with this default of 13 instead",
    "start": "2961240",
    "end": "2967920"
  },
  {
    "text": "recall from way earlier the type of cluster that you're running right now is",
    "start": "2967920",
    "end": "2974119"
  },
  {
    "text": "actually a local mode cluster this one right and it has three slots you can run",
    "start": "2974119",
    "end": "2979440"
  },
  {
    "text": "up to three tasks simultaneously so the thing with spark is that you can go pretty far by just",
    "start": "2979440",
    "end": "2986760"
  },
  {
    "text": "learning the API like that's what we did so far however if you start aligning",
    "start": "2986760",
    "end": "2992559"
  },
  {
    "text": "your knowledge of the spark API to the actual cluster architecture you can get better performance and I'll show you how",
    "start": "2992559",
    "end": "2999680"
  },
  {
    "text": "ideally if I have three slots then maybe I want a data frame in memory that is",
    "start": "2999680",
    "end": "3005200"
  },
  {
    "text": "some multiple of three like maybe six or 9 or 12 um but I definitely don't want a",
    "start": "3005200",
    "end": "3011799"
  },
  {
    "text": "data frame in memory that's going to be one partition because if I do that the other two slots are going to remain",
    "start": "3011799",
    "end": "3017440"
  },
  {
    "text": "empty so um I'm going to actually take the data frame um idea the data frame",
    "start": "3017440",
    "end": "3024400"
  },
  {
    "text": "idea that says 13 partitions I'm going to repartition that into six and then",
    "start": "3024400",
    "end": "3030440"
  },
  {
    "text": "I'm going to Cache the data frame with six partitions into memory",
    "start": "3030440",
    "end": "3036200"
  },
  {
    "text": "permanently and then I'm going to start operating on that data frame in memory so let me show you how to do that sorry",
    "start": "3036200",
    "end": "3041839"
  },
  {
    "text": "let me just scroll back down to where I was okay this is my data frame right here with 13 partitions I'll repartition",
    "start": "3041839",
    "end": "3048760"
  },
  {
    "text": "it to six and then I'm going to make a view for that called fire service",
    "start": "3048760",
    "end": "3055319"
  },
  {
    "text": "view okay run that cell and then I'm going to say go ahead and cach the table okay before I run",
    "start": "3055319",
    "end": "3063000"
  },
  {
    "text": "this cell let me show you something I'm going to click this and then go to this",
    "start": "3063000",
    "end": "3068240"
  },
  {
    "text": "UI and open it up in a new tab so if you've never done seen the spark UI this",
    "start": "3068240",
    "end": "3073319"
  },
  {
    "text": "is really important um just kind of click this down arrow right click on this and go to UI notice a new um a",
    "start": "3073319",
    "end": "3081520"
  },
  {
    "text": "whole new uh tab is loading and there's going to be a lot of information overload for new people using spark here",
    "start": "3081520",
    "end": "3088640"
  },
  {
    "text": "I will explain at least the fundamentals of what's going on here but for now all",
    "start": "3088640",
    "end": "3093680"
  },
  {
    "text": "you really need to do is click on storage and notice that it's empty there is nothing showing up in storage because",
    "start": "3093680",
    "end": "3099960"
  },
  {
    "text": "nothing is cached storage just shows you the data frames are cached in memory and I didn't run the cache command yet but",
    "start": "3099960",
    "end": "3107280"
  },
  {
    "text": "I'll do it now okay so I'm going to run the cash command well it ran in like a tenth of a",
    "start": "3107280",
    "end": "3113440"
  },
  {
    "text": "second that's really suspicious because I know it takes 20 seconds to do a full scan of the file on disk so it didn't",
    "start": "3113440",
    "end": "3119280"
  },
  {
    "text": "even read the file um because cash is lazy it doesn't actually trigger the",
    "start": "3119280",
    "end": "3124559"
  },
  {
    "text": "cash to happen until I do a subsequent operation like count that actually does",
    "start": "3124559",
    "end": "3130040"
  },
  {
    "text": "need to read all of the data from disk now this count is special because now it",
    "start": "3130040",
    "end": "3135440"
  },
  {
    "text": "is going to do two things it's going to actually read all the data from dis and tell me that there's you know 4 million",
    "start": "3135440",
    "end": "3141440"
  },
  {
    "text": "rows in the file but this count is going to take much more than 20 seconds to run",
    "start": "3141440",
    "end": "3147119"
  },
  {
    "text": "because this time it's going to go ahead and materialize this cache as well",
    "start": "3147119",
    "end": "3152240"
  },
  {
    "text": "actually it's going to read the file from disk get a data frame temporarily in memory with 13 partitions then it",
    "start": "3152240",
    "end": "3159160"
  },
  {
    "text": "will repartition that into six and then it'll cach that in memory so that's",
    "start": "3159160",
    "end": "3164599"
  },
  {
    "text": "what's going on right now and this is going to take like a minute or or a minute and a half to run if I expand",
    "start": "3164599",
    "end": "3170240"
  },
  {
    "text": "spark jobs right now uh job 23 is running and I can see that the first",
    "start": "3170240",
    "end": "3175359"
  },
  {
    "text": "stage in job 23 is actually creating a data frame with 13 partitions um that",
    "start": "3175359",
    "end": "3183160"
  },
  {
    "text": "just has to do with the way spark reads from S3 it reads um 64 megabytes of the CSV file at a",
    "start": "3183160",
    "end": "3191559"
  },
  {
    "text": "time and that basically um ends up being about 13 partitions but notice what's",
    "start": "3191559",
    "end": "3196760"
  },
  {
    "text": "happening actually is as soon as it makes that data frame it recreates a new data frame with six partitions which is",
    "start": "3196760",
    "end": "3203640"
  },
  {
    "text": "about to happen right now there we go now that we're here um I will be able to",
    "start": "3203640",
    "end": "3209480"
  },
  {
    "text": "come to storage and I can probably in a second start seeing this data frame with six partitions materialize in memory",
    "start": "3209480",
    "end": "3217200"
  },
  {
    "text": "right now in just a second there we go the first three partitions of six 50%",
    "start": "3217200",
    "end": "3223040"
  },
  {
    "text": "have been materialized um it's taking up about 309 megabytes in memory if I click",
    "start": "3223040",
    "end": "3228880"
  },
  {
    "text": "storage one more time it's not all done yet uh there we go now it is six of six",
    "start": "3228880",
    "end": "3235880"
  },
  {
    "text": "partitions are in memory 100% it's taking up about 618 megabytes in memory",
    "start": "3235880",
    "end": "3242760"
  },
  {
    "text": "granted this is a 1.6 gigabyte file on disk however when you put a data frame",
    "start": "3242760",
    "end": "3249799"
  },
  {
    "text": "in memory spark uses the tungsten binary format to columnar compress this data in",
    "start": "3249799",
    "end": "3257079"
  },
  {
    "text": "memory so the short of why this is 600 megabytes in memory is because we're doing columnar compression and that",
    "start": "3257079",
    "end": "3264359"
  },
  {
    "text": "makes the storage and memory be far more efficient than the uh than the serialized data on disk and if I click",
    "start": "3264359",
    "end": "3272079"
  },
  {
    "text": "on this right here um I can see how large each one of the six partitions is see down here um each partition is 103",
    "start": "3272079",
    "end": "3282359"
  },
  {
    "text": "megabytes so this is my representation of the data in memory if I do anything with this data frame it's going to",
    "start": "3282799",
    "end": "3289559"
  },
  {
    "text": "require in the first stage six tasks of which only three can run first on my",
    "start": "3289559",
    "end": "3295440"
  },
  {
    "text": "three slots then three more and now I'm done analyzing the full data frame in memory okay also notice that if uh I",
    "start": "3295440",
    "end": "3305079"
  },
  {
    "text": "actually have about 1.3 gigabyt of data remaining that I can cach more uh I can",
    "start": "3305079",
    "end": "3311839"
  },
  {
    "text": "cach that much more data essentially okay let's go back uh to The Notebook notice that took 100 seconds to",
    "start": "3311839",
    "end": "3318920"
  },
  {
    "text": "run um what I'm GNA do here is I'm gonna pull out a data frame from this table so",
    "start": "3318920",
    "end": "3326720"
  },
  {
    "text": "that now this data frame is operating on the memory cached version version see",
    "start": "3326720",
    "end": "3333319"
  },
  {
    "text": "this name fire service DF anytime you see this it actually came from the",
    "start": "3333319",
    "end": "3338920"
  },
  {
    "text": "cached version in memory so what that means is if I do this like on this new",
    "start": "3338920",
    "end": "3344039"
  },
  {
    "text": "data frame name if I call count it'll run in like a tenth of a second and it's",
    "start": "3344039",
    "end": "3349520"
  },
  {
    "text": "going to use six tasks well first of all look it's like skipping the first stage",
    "start": "3349520",
    "end": "3355119"
  },
  {
    "text": "completely because it is isn't need to read from disk the CSV file in S3 it's instead reading directly from the",
    "start": "3355119",
    "end": "3361480"
  },
  {
    "text": "representation of the six partitioned data frame in memory so that's the stage it reads from and then it actually uh",
    "start": "3361480",
    "end": "3368640"
  },
  {
    "text": "launches this last stage here to do a count as well okay um is my table cached",
    "start": "3368640",
    "end": "3376559"
  },
  {
    "text": "true yes this is basically what I have in memory right now I have six partitions each about 104 megabytes",
    "start": "3376559",
    "end": "3384480"
  },
  {
    "text": "total data frame size 626 why did I use six I use six because I know that in",
    "start": "3384480",
    "end": "3391000"
  },
  {
    "text": "general when you're putting data frames in memory The Sweet Spot that you should aim for is each partition should be 50",
    "start": "3391000",
    "end": "3397640"
  },
  {
    "text": "megabytes to 200 megabytes in memory so don't have multi kilobyte size",
    "start": "3397640",
    "end": "3403400"
  },
  {
    "text": "partitions that are like tiny or multi- gigabyte size partitions that are large",
    "start": "3403400",
    "end": "3409039"
  },
  {
    "text": "aim for something like a 100 to 200 megabytes that's what I went with six",
    "start": "3409039",
    "end": "3414240"
  },
  {
    "text": "myself and um I already showed you this in uh in the UI as well okay um one more",
    "start": "3414240",
    "end": "3421839"
  },
  {
    "text": "thing we did a whole bunch of work with our data frame right we read the CSV file we converted a bunch of the columns",
    "start": "3421839",
    "end": "3428599"
  },
  {
    "text": "from string to timestamp and the thing is if I'm like going to need to take a break and I want to go you know um stop",
    "start": "3428599",
    "end": "3436920"
  },
  {
    "text": "doing some analysis for a minute here's what I would actually want to do right now um I would want to take my data",
    "start": "3436920",
    "end": "3443640"
  },
  {
    "text": "frame write it down as a parket file to S3 so that in the future if I want to",
    "start": "3443640",
    "end": "3451240"
  },
  {
    "text": "continue my work I'm going to read this paret file which is going to give me a",
    "start": "3451240",
    "end": "3456480"
  },
  {
    "text": "data frame with six partitions with all the columns properly formatted like some",
    "start": "3456480",
    "end": "3461720"
  },
  {
    "text": "of them will be timestamped so this is a very common operation right we just did the whole Loop of ETL we um extracted we",
    "start": "3461720",
    "end": "3470400"
  },
  {
    "text": "did like extraction of the data from CSV we transformed it we cast some of the strings into time stamp and now I'm",
    "start": "3470400",
    "end": "3477440"
  },
  {
    "text": "going to load it into a paret file um and that's what I'm doing right now I just basically say data frame save it as",
    "start": "3477440",
    "end": "3484640"
  },
  {
    "text": "par you can also save it as Json or CSV but par is a very efficient",
    "start": "3484640",
    "end": "3490200"
  },
  {
    "text": "format and this is going to let me read the file way faster than CSV recall that",
    "start": "3490200",
    "end": "3497240"
  },
  {
    "text": "there's no reduction in data right this is still 1.6 GB of data fundamentally",
    "start": "3497240",
    "end": "3502799"
  },
  {
    "text": "and 4 million rows I'm writing all 4 million row right now to this paret file",
    "start": "3502799",
    "end": "3508319"
  },
  {
    "text": "in this temporary location on S3 and uh this should just take a second",
    "start": "3508319",
    "end": "3514559"
  },
  {
    "text": "there uh it's running right",
    "start": "3514559",
    "end": "3518160"
  },
  {
    "text": "now and once that's done I'm going to just run a here LS here and show you",
    "start": "3520319",
    "end": "3526039"
  },
  {
    "text": "what this parquet file looks like see how it has six different gz compressed",
    "start": "3526039",
    "end": "3533599"
  },
  {
    "text": "parts so these are what just happened is that data frame in memory with six partitions each one of those six",
    "start": "3533599",
    "end": "3540720"
  },
  {
    "text": "partition partitions just dumped down the gz compressed version of that",
    "start": "3540720",
    "end": "3546599"
  },
  {
    "text": "partition as an individual parquet file so in the future imagine that like my",
    "start": "3546599",
    "end": "3551680"
  },
  {
    "text": "cluster self terminates in an hour however when I come back like tomorrow morning to continue doing data analysis",
    "start": "3551680",
    "end": "3558200"
  },
  {
    "text": "I can read that parquet file into like a temporary data frame or something and",
    "start": "3558200",
    "end": "3564160"
  },
  {
    "text": "then I can get back to my dat right away and this parquet file has in it embedded",
    "start": "3564160",
    "end": "3570000"
  },
  {
    "text": "all the correct column names without spaces and all the correct data types",
    "start": "3570000",
    "end": "3575160"
  },
  {
    "text": "like string integer Boolean or timestamp so now I want to point out one more",
    "start": "3575160",
    "end": "3580720"
  },
  {
    "text": "really important thing um if I take this temp data frame like check this out I",
    "start": "3580720",
    "end": "3587079"
  },
  {
    "text": "take this temp data frame and what I'm going to do is open up a new cell and I'm going to do a count on it which does",
    "start": "3587079",
    "end": "3594119"
  },
  {
    "text": "a full scan of the parket file on dis I just want you to see how long it's going to take in half a second parquet files are",
    "start": "3594119",
    "end": "3603079"
  },
  {
    "text": "really efficient to read from in spark 2.0 there's actually a 3X Improvement to",
    "start": "3603079",
    "end": "3609440"
  },
  {
    "text": "the throughput of reading paret files from just January 2016 in January 2016 6",
    "start": "3609440",
    "end": "3616240"
  },
  {
    "text": "months ago spark 1.6 was cutting edge since January of this year there has",
    "start": "3616240",
    "end": "3621720"
  },
  {
    "text": "been a 3X increase to reading paret files so basically Al I always want to",
    "start": "3621720",
    "end": "3627280"
  },
  {
    "text": "take CSV or Json that are nonoptimal do my ETL write it back down to par and",
    "start": "3627280",
    "end": "3633240"
  },
  {
    "text": "then from there I'm going to start doing more sophisticated analysis so let's do that in the last 10 or so minutes first",
    "start": "3633240",
    "end": "3640760"
  },
  {
    "text": "SQL queries um now that I have a view defined on my uh data frame in memory",
    "start": "3640760",
    "end": "3647000"
  },
  {
    "text": "you can just do like percent SQL in data bricks and this is a magic command um",
    "start": "3647000",
    "end": "3654280"
  },
  {
    "text": "and it lets you run SQL queries so you can do like select all from my uh table View and there's four million",
    "start": "3654280",
    "end": "3662559"
  },
  {
    "text": "rows by the way if you're running spark on premise like on your local laptop",
    "start": "3662559",
    "end": "3668160"
  },
  {
    "text": "this percent SQL will not work on Jupiter or Zeppelin or in the python shell you'll need to do something like",
    "start": "3668160",
    "end": "3676119"
  },
  {
    "text": "it's something like you know spark. SQL I'm doing this off the top of my head it's something like that and",
    "start": "3676119",
    "end": "3683520"
  },
  {
    "text": "then you like you know do something like that that and I don't know if that will run maybe it will oh yeah and then I want to",
    "start": "3683520",
    "end": "3691640"
  },
  {
    "text": "do oops this gives me back a data frame and then I'll do show on that oops that",
    "start": "3691640",
    "end": "3699400"
  },
  {
    "text": "also gives me back a data frame uh that should run yeah there we go so that's how you",
    "start": "3699400",
    "end": "3706640"
  },
  {
    "text": "run this like if you're running in Jupiter the percent SQL is just a magic command for data brecks okay let's do",
    "start": "3706640",
    "end": "3712760"
  },
  {
    "text": "something a little cooler oh yeah this is pretty cool because it's got colors it all right so what happened up here",
    "start": "3712760",
    "end": "3718480"
  },
  {
    "text": "when I did this count right the way this looked earlier recall um let me just get rid of this",
    "start": "3718480",
    "end": "3725319"
  },
  {
    "text": "complexity there we go I just did a count now if I expand this um it ran like two stages the first",
    "start": "3725319",
    "end": "3734119"
  },
  {
    "text": "stage was skipped it ran stage 59 with six tasks and then stage um 60 let me",
    "start": "3734119",
    "end": "3741520"
  },
  {
    "text": "show you this in the spark UI so if I go to the UI up here this was job 36 right",
    "start": "3741520",
    "end": "3747400"
  },
  {
    "text": "so let me go up here to I need to find click on jobs and I need to find job 36 oh well it happens to be the first one",
    "start": "3747400",
    "end": "3753599"
  },
  {
    "text": "here this is my job 36 and this is the job with two stages that ran 59 and 60",
    "start": "3753599",
    "end": "3760440"
  },
  {
    "text": "so notice if I just kind of scroll a little bit to the right on the same row I can see that um this kicked off two",
    "start": "3760440",
    "end": "3768520"
  },
  {
    "text": "stages uh a total of uh seven tasks Ran So here's what I'm going to do one thing",
    "start": "3768520",
    "end": "3774599"
  },
  {
    "text": "here one level of depth I'm going to click on this link under this the description column when I do that now",
    "start": "3774599",
    "end": "3781640"
  },
  {
    "text": "it's going to show me more details about the three stages the the one that skipped us down here it didn't run at",
    "start": "3781640",
    "end": "3788319"
  },
  {
    "text": "all these are the two that ran 59 and then 60 stage 59 is the first stage that",
    "start": "3788319",
    "end": "3794480"
  },
  {
    "text": "ran and notice this stage read 620 megabytes of data from memory and then",
    "start": "3794480",
    "end": "3802359"
  },
  {
    "text": "this this stage used six tasks to do that basically each task read the 103",
    "start": "3802359",
    "end": "3807920"
  },
  {
    "text": "megabyte partition of the six partitions and then this stage emitted 350 bytes of",
    "start": "3807920",
    "end": "3815640"
  },
  {
    "text": "data what basically happened is this these six tasks each wrote down a file",
    "start": "3815640",
    "end": "3822680"
  },
  {
    "text": "to S3 and those files those six files in total amount to 354 bytes then stage 59",
    "start": "3822680",
    "end": "3831039"
  },
  {
    "text": "was completed it finished and then stage 69 ran which launched only one task that",
    "start": "3831039",
    "end": "3837920"
  },
  {
    "text": "read the shuffle data from before so it read all six files and then it it gave",
    "start": "3837920",
    "end": "3843160"
  },
  {
    "text": "me the results of the count what actually happened here behind the scenes is if I drill deeper into Stage 59 with",
    "start": "3843160",
    "end": "3850359"
  },
  {
    "text": "six tasks I'm going to click this link here to understand what each one of those six tasks did just scroll all the",
    "start": "3850359",
    "end": "3856640"
  },
  {
    "text": "way to the bottom here are the six tasks the task ID notice each task",
    "start": "3856640",
    "end": "3863720"
  },
  {
    "text": "read3 megabytes of data and then each of the six tasks wrote down one record that",
    "start": "3863720",
    "end": "3870799"
  },
  {
    "text": "is 59 bytes to SSD what basically is going on here is each one of these six",
    "start": "3870799",
    "end": "3876520"
  },
  {
    "text": "tasks is reading the 100 Megs of data and then doing a a local local aggregated count like this guy's going",
    "start": "3876520",
    "end": "3883720"
  },
  {
    "text": "to say I see 800 million rows I see 750",
    "start": "3883720",
    "end": "3888880"
  },
  {
    "text": "million that number 800 million or 700 million is being uh or 700,000 is being",
    "start": "3888880",
    "end": "3895160"
  },
  {
    "text": "written down down here so each partition is basically or each task is just",
    "start": "3895160",
    "end": "3900279"
  },
  {
    "text": "writing down how many rows it saw in its3 megabyte partition so just one",
    "start": "3900279",
    "end": "3905960"
  },
  {
    "text": "number is being written down and this number is going to be if you actually open up that file on SSD it's going to",
    "start": "3905960",
    "end": "3911400"
  },
  {
    "text": "say something like 800,000 or 700,000 but the problem is we have six different",
    "start": "3911400",
    "end": "3917000"
  },
  {
    "text": "files each file is giving a aggregated count of just its local partition and we",
    "start": "3917000",
    "end": "3923720"
  },
  {
    "text": "need to sum all the numbers up all six of those numbers need to be summed up so we get the final count um that's exactly",
    "start": "3923720",
    "end": "3931920"
  },
  {
    "text": "what's going on in the next stage stage 60 right here is running only one task",
    "start": "3931920",
    "end": "3938440"
  },
  {
    "text": "but that one task notice all the way at the bottom here it's hard to see if you're in the back but this one task is",
    "start": "3938440",
    "end": "3944559"
  },
  {
    "text": "reading six records those are the six files and then all this guy does it runs",
    "start": "3944559",
    "end": "3950039"
  },
  {
    "text": "in 1 millisecond because all he has to do is sum up six numbers and then he says I see four 4 .1 million or what AR",
    "start": "3950039",
    "end": "3957079"
  },
  {
    "text": "Rose so that's a little bit about like what's going on behind the scenes and this UI is the best way you can start",
    "start": "3957079",
    "end": "3964640"
  },
  {
    "text": "diagnosing your performance and optimizations and debugging this is like",
    "start": "3964640",
    "end": "3970079"
  },
  {
    "text": "ground truth and notice here by the way if I go back to the earlier stage which",
    "start": "3970079",
    "end": "3975279"
  },
  {
    "text": "had six tasks see this one I'm going to show you one something kind of cool see over here says event timeline if I",
    "start": "3975279",
    "end": "3981480"
  },
  {
    "text": "expand that this shows me the six tasks in stage 59 the first stage and I'm not",
    "start": "3981480",
    "end": "3988000"
  },
  {
    "text": "going to cover this in depth I just want to want you to see that each one of these rectangles is is a task the first",
    "start": "3988000",
    "end": "3994079"
  },
  {
    "text": "three uh tasks run first and then as soon as a task completes another task",
    "start": "3994079",
    "end": "4000079"
  },
  {
    "text": "number four launches then task five and task six green is compute time of the",
    "start": "4000079",
    "end": "4005960"
  },
  {
    "text": "task um red is uh deserialization time and blue is",
    "start": "4005960",
    "end": "4011799"
  },
  {
    "text": "schedu or delay um these uh colors can be very useful in trying to figure out",
    "start": "4011799",
    "end": "4018559"
  },
  {
    "text": "where the bottleneck might be in your spark application these six tasks all",
    "start": "4018559",
    "end": "4024400"
  },
  {
    "text": "wrote down one file to SSD however it wrote it so fast see up there the yellow",
    "start": "4024400",
    "end": "4030079"
  },
  {
    "text": "for Shuffle right time you know see this yellow right here for Shuffle right time",
    "start": "4030079",
    "end": "4035559"
  },
  {
    "text": "technically you can't really see it but the end of this at the very end is a tiny pixel of yellow um because there it",
    "start": "4035559",
    "end": "4042039"
  },
  {
    "text": "did write something down but it just happened so fast if I was writing a lot of Shuffle data meaning there was no",
    "start": "4042039",
    "end": "4048599"
  },
  {
    "text": "reduction in data like I do something where there's um the shuffle WR has to",
    "start": "4048599",
    "end": "4054760"
  },
  {
    "text": "still write all 1.6 gigabyt of data down across six tasks then I would be seeing",
    "start": "4054760",
    "end": "4059920"
  },
  {
    "text": "a lot of yellow here okay that's it for this part let's",
    "start": "4059920",
    "end": "4065359"
  },
  {
    "text": "go back to the code all right that's a little bit about behind the scenes about how you can use",
    "start": "4065359",
    "end": "4070839"
  },
  {
    "text": "the spark UI um next I just want to do the SQL",
    "start": "4070839",
    "end": "4076279"
  },
  {
    "text": "um you guys all know squl so um you can see that I'm doing some neighborhood analysis here I'm basically saying I",
    "start": "4076279",
    "end": "4083079"
  },
  {
    "text": "want to know which neighborhood in SF generated the most calls in 2015 any",
    "start": "4083079",
    "end": "4088880"
  },
  {
    "text": "guesses I had a guess uh before I did this I'll be honest I thought it would be the tender line uh you know but let's see what",
    "start": "4088880",
    "end": "4096600"
  },
  {
    "text": "happens so uh I'm running a SQL query uh I want to get a column for neighborhood district and then another column for the",
    "start": "4096600",
    "end": "4103278"
  },
  {
    "text": "count and um you know I'm doing a a wear Clause keep only 2015 data Group by The",
    "start": "4103279",
    "end": "4110600"
  },
  {
    "text": "Neighborhood order by The Neighborhood count and descending order give me the top 15 indeed tenderloin last year in",
    "start": "4110600",
    "end": "4117600"
  },
  {
    "text": "2015 had 40,000 calls to the fire department and then uh South of Market",
    "start": "4117600",
    "end": "4123960"
  },
  {
    "text": "30,000 and if you go to some of the other neighborhoods it really drops like Chinatown or Pacific Heights last year",
    "start": "4123960",
    "end": "4130560"
  },
  {
    "text": "only made about 6,000 calls to the fire department okay um by the way uh you",
    "start": "4130560",
    "end": "4137960"
  },
  {
    "text": "know we will understand in just a second the last thing we're going to do is find out what type of calls the tender line",
    "start": "4137960",
    "end": "4143798"
  },
  {
    "text": "was making so I thought that would be kind of cool to do but one thing I want you to see here see the spark jobs right",
    "start": "4143799",
    "end": "4150238"
  },
  {
    "text": "here I'm going to expand that and there is something weird here why does this say",
    "start": "4150239",
    "end": "4156199"
  },
  {
    "text": "200 does anybody know raise your hand if you know what this means 200 uh the uh",
    "start": "4156199",
    "end": "4162838"
  },
  {
    "text": "senior director of training at data knows but that's cheating I'm going to",
    "start": "4162839",
    "end": "4168199"
  },
  {
    "text": "tell you something um you know this query ran it ran in look like like two",
    "start": "4168199",
    "end": "4173359"
  },
  {
    "text": "seconds but what happened actually is there is a static setting in spark and",
    "start": "4173359",
    "end": "4178798"
  },
  {
    "text": "that setting is set to 200 what that setting means is it means that anytime a",
    "start": "4178799",
    "end": "4185000"
  },
  {
    "text": "shuffle happens how many partitions should be in the data frame after the shuffle and unfortunately that's hard",
    "start": "4185000",
    "end": "4192120"
  },
  {
    "text": "set 200 is not ideal for my data set um I want to change that to six I",
    "start": "4192120",
    "end": "4198880"
  },
  {
    "text": "already told you why I like six because you know in this Shuffle by the way there was no reduction of data I went",
    "start": "4198880",
    "end": "4204880"
  },
  {
    "text": "from a data frame in uh memory with six partitions to a data frame in memory with 200 now this is basically like the",
    "start": "4204880",
    "end": "4212360"
  },
  {
    "text": "internals of spark I don't want to explain why this happened but this did happen and um like here if I click on",
    "start": "4212360",
    "end": "4219800"
  },
  {
    "text": "this I right here the I next to the six it's a shortcut to the UI and if if I go",
    "start": "4219800",
    "end": "4225800"
  },
  {
    "text": "and look at the six tasks that ran notice now that the shuffle right was",
    "start": "4225800",
    "end": "4231719"
  },
  {
    "text": "about 3 kilobytes of data the shuffle right was Tiny each task rode 42 files",
    "start": "4231719",
    "end": "4239560"
  },
  {
    "text": "so 6times 42 right 6 * 42 I think should be like 200",
    "start": "4239560",
    "end": "4246280"
  },
  {
    "text": "um 42 * 6 is 252 okay 252 if I scroll",
    "start": "4246280",
    "end": "4251640"
  },
  {
    "text": "down to the next task that ran uh oops next stage this stage with 200 what's going to happen here is oh my",
    "start": "4251640",
    "end": "4259480"
  },
  {
    "text": "God there's in this stage 200 tasks ran um they read a tiny tiny tiny amount of",
    "start": "4259480",
    "end": "4266480"
  },
  {
    "text": "data it's actually like barely even showing up this is nonoptimal I'm going to show you how to fix this I basically",
    "start": "4266480",
    "end": "4273040"
  },
  {
    "text": "never want a data frame in memory of 200 partitions so here's what I do uh see",
    "start": "4273040",
    "end": "4278400"
  },
  {
    "text": "this setting here is set to 200 I'm going to change it to six and and verify",
    "start": "4278400",
    "end": "4286199"
  },
  {
    "text": "that is set to six if you want to know more about this setting copy this name",
    "start": "4286199",
    "end": "4291560"
  },
  {
    "text": "and just go to the spark documentation right here remember I went here to data frames just go there search for that and",
    "start": "4291560",
    "end": "4299040"
  },
  {
    "text": "it explains what that means okay so I set it to 200 um the",
    "start": "4299040",
    "end": "4306239"
  },
  {
    "text": "last time I ran that query it took 1.2 seconds this little tiny",
    "start": "4306239",
    "end": "4311840"
  },
  {
    "text": "optimization is it going to make it any better well I don't know now it's running in 7 seconds on big data that",
    "start": "4311840",
    "end": "4320000"
  },
  {
    "text": "would have been actually a significant speed increase okay um by the way there's like",
    "start": "4320000",
    "end": "4326000"
  },
  {
    "text": "a bunch of you know um uh SQL commands you can run this is describing my table",
    "start": "4326000",
    "end": "4332120"
  },
  {
    "text": "okay we're almost done with the day I want to explain uh one more thing there's a catalyst Optimizer internally",
    "start": "4332120",
    "end": "4339960"
  },
  {
    "text": "in spark and what it does is it takes your SQL query or your data frame query",
    "start": "4339960",
    "end": "4345199"
  },
  {
    "text": "generates a logical plan which flows through this Catalyst Optimizer through",
    "start": "4345199",
    "end": "4351080"
  },
  {
    "text": "logical optimizations and physical optimizations and it finally generates some rdds and runs that code now I'm not",
    "start": "4351080",
    "end": "4357360"
  },
  {
    "text": "going to have time to cover all all of this today but I am going to show you something if you want to learn more",
    "start": "4357360",
    "end": "4364000"
  },
  {
    "text": "about what's going on behind the scenes here here's what you can do look this is",
    "start": "4364000",
    "end": "4369239"
  },
  {
    "text": "a SQL query the same one as before um if I run a SQL query it gives me back a",
    "start": "4369239",
    "end": "4375480"
  },
  {
    "text": "data frame okay just look at this code right here look at what I'm going to do",
    "start": "4375480",
    "end": "4380560"
  },
  {
    "text": "next though I'm going to tack something on at the end here I'm going to tack on explain",
    "start": "4380560",
    "end": "4386600"
  },
  {
    "text": "true if I run that look at this now it's saying it didn't it didn't run any code",
    "start": "4386600",
    "end": "4392639"
  },
  {
    "text": "it just it's just showing me the pared logical plan and then it's showing me",
    "start": "4392639",
    "end": "4398199"
  },
  {
    "text": "the analyzed logical plan and then it's showing me the probably uh like other",
    "start": "4398199",
    "end": "4404320"
  },
  {
    "text": "logical plans and and physical plans what are all of these you know here's the optimized logical plan well",
    "start": "4404320",
    "end": "4411199"
  },
  {
    "text": "this stuff right here is all the different phases of flowing through this",
    "start": "4411199",
    "end": "4416840"
  },
  {
    "text": "Catalyst Optimizer I'm only pointing this out because as you start learning more about spark this is what you're",
    "start": "4416840",
    "end": "4422880"
  },
  {
    "text": "going to want to do to debug your code and do this and then post your question on the spark mailing L and mailing list",
    "start": "4422880",
    "end": "4429920"
  },
  {
    "text": "and say Here's my all my plans it's not running or it's running really slow then they'll be able to help help you one",
    "start": "4429920",
    "end": "4436520"
  },
  {
    "text": "more thing that you can do to understand what's going on behind the scenes with this engine right here see the SQL query",
    "start": "4436520",
    "end": "4444520"
  },
  {
    "text": "right here um we ran it right here right it ran job um which job was this one I'm",
    "start": "4444520",
    "end": "4451520"
  },
  {
    "text": "going to run this one one more time there I just ran that and now it ran job",
    "start": "4451520",
    "end": "4456800"
  },
  {
    "text": "what is this 40 job 40 okay so check this out you can go to the spark UI I'm going to show you one new thing click on",
    "start": "4456800",
    "end": "4462560"
  },
  {
    "text": "SQL up here and notice that the the top here here's job 40 don't click on that",
    "start": "4462560",
    "end": "4470040"
  },
  {
    "text": "just notice that this is the job 40 that I just ran if you click on this it shows",
    "start": "4470040",
    "end": "4476120"
  },
  {
    "text": "you A visual representation of what just happened to do to do this SQL query",
    "start": "4476120",
    "end": "4482199"
  },
  {
    "text": "right here right here this SQL query translates into this physical plan",
    "start": "4482199",
    "end": "4489880"
  },
  {
    "text": "behind the scenes it reads data from memory and then it does a filter it does",
    "start": "4489880",
    "end": "4495040"
  },
  {
    "text": "does a project it doesn't aggregate um exchange means Shuffle so it does a",
    "start": "4495040",
    "end": "4501239"
  },
  {
    "text": "shuffle and then it runs a whole other stage all right so hopefully this will like help you down the line with your",
    "start": "4501239",
    "end": "4507400"
  },
  {
    "text": "debugging this if you expand details here it shows you the exact same logical",
    "start": "4507400",
    "end": "4513800"
  },
  {
    "text": "plans and physical plan that that explained true shows you all right so",
    "start": "4513800",
    "end": "4519199"
  },
  {
    "text": "this is just a little bit of depth if you want to know more about what's going on behind the scenes okay we're close to",
    "start": "4519199",
    "end": "4525440"
  },
  {
    "text": "wrapping up and opening up for Q&A however another uh really important",
    "start": "4525440",
    "end": "4530719"
  },
  {
    "text": "thing that you would probably need to do if you're going to use spark there's all the stuff I already covered is a",
    "start": "4530719",
    "end": "4537440"
  },
  {
    "text": "join this might be really important because um look there's another data set I found uh yesterday it was not fire",
    "start": "4537440",
    "end": "4545600"
  },
  {
    "text": "service calls but fire incidents um this was like it had an incident number like",
    "start": "4545600",
    "end": "4550920"
  },
  {
    "text": "this and then the thing I liked about this different data set is if you scroll",
    "start": "4550920",
    "end": "4556000"
  },
  {
    "text": "over here it has this thing that says why the PE why somebody called 911 see",
    "start": "4556000",
    "end": "4561360"
  },
  {
    "text": "the primary situation like building fire or I don't know outside rubbish fire or",
    "start": "4561360",
    "end": "4568440"
  },
  {
    "text": "overheated motor uh like all the this is interesting I was like wondering you",
    "start": "4568440",
    "end": "4573679"
  },
  {
    "text": "know why was the tenderloin calling and then perhaps why does a more richer part of San Francisco call so like that was",
    "start": "4573679",
    "end": "4580679"
  },
  {
    "text": "the next thing I was wondering so I basically loaded that file I just showed you into S3 um here it is let's load it up into a",
    "start": "4580679",
    "end": "4587360"
  },
  {
    "text": "data frame called incident data frame okay a totally different data set I'm going to stick with infers",
    "start": "4587360",
    "end": "4594520"
  },
  {
    "text": "schema here because it will be fine for my example let's print the schema here",
    "start": "4594520",
    "end": "4600480"
  },
  {
    "text": "this is the the data it it it doesn't translate the Tim stamp correctly but that's fine I don't",
    "start": "4600480",
    "end": "4607040"
  },
  {
    "text": "need the timestamp um how many rows are in this data um I think it's like 1 million this",
    "start": "4607040",
    "end": "4615159"
  },
  {
    "text": "file these incidents are all non-medical related incidents I think for private",
    "start": "4615159",
    "end": "4620679"
  },
  {
    "text": "privacy reasons like um they don't track incidents here if it's like medical",
    "start": "4620679",
    "end": "4625840"
  },
  {
    "text": "these are all medical non-medical incidents okay and there's um in the",
    "start": "4625840",
    "end": "4632120"
  },
  {
    "text": "incident data frame there's only 400,000 rows it's way smaller than 4",
    "start": "4632120",
    "end": "4638280"
  },
  {
    "text": "million um here's what it looks like but that's not what I what I really want to do is a join on the incident number",
    "start": "4638280",
    "end": "4646239"
  },
  {
    "text": "because my earlier data frame also had an incident number I want to do a join on that so notice when I made my data",
    "start": "4646239",
    "end": "4653040"
  },
  {
    "text": "frame I did something kind of tricky um I don't know if you noticed I made my data frame and then look at what I did I",
    "start": "4653040",
    "end": "4659199"
  },
  {
    "text": "said with column renamed take the incident space number and rename that",
    "start": "4659199",
    "end": "4665159"
  },
  {
    "text": "column to just incident number get rid of that space so I can do the join I",
    "start": "4665159",
    "end": "4671080"
  },
  {
    "text": "kept all the other column names with the space in there all right so the join is super simple to do here it is my fire",
    "start": "4671080",
    "end": "4678400"
  },
  {
    "text": "service data frame that is currently in memory and then I said join it on the",
    "start": "4678400",
    "end": "4683639"
  },
  {
    "text": "incidence data frame on this column on",
    "start": "4683639",
    "end": "4689320"
  },
  {
    "text": "both and by the way I also tacked on a cache at the end here so that when it",
    "start": "4689320",
    "end": "4695960"
  },
  {
    "text": "reads the incidence data frame it goes in and caches that in memory as well and then I will have both data frames in",
    "start": "4695960",
    "end": "4702520"
  },
  {
    "text": "memory making my join super efficient ient so that's what I'm doing here and I just basically called it join data frame",
    "start": "4702520",
    "end": "4709040"
  },
  {
    "text": "to keep it simple and then um here's now my massive join data frame in memory",
    "start": "4709040",
    "end": "4714320"
  },
  {
    "text": "with like probably 60 columns um here we go it's actually now",
    "start": "4714320",
    "end": "4721600"
  },
  {
    "text": "reading that second data frame um because that was the first time it actually needed to do that and here we",
    "start": "4721600",
    "end": "4727480"
  },
  {
    "text": "go this is my massive data frame so now I've joined on the incident ID and",
    "start": "4727480",
    "end": "4732960"
  },
  {
    "text": "finally I can do my query right right here filter for um only 2015 on my",
    "start": "4732960",
    "end": "4739880"
  },
  {
    "text": "joined data frame and also filter for the neighborhood District of tender line and how many do I even have in 2015 um",
    "start": "4739880",
    "end": "4747040"
  },
  {
    "text": "8,000 times somebody called 8,600 from the tenderloin and um here we",
    "start": "4747040",
    "end": "4753159"
  },
  {
    "text": "go what was the reason so here I'm grouping by primary reason and I found",
    "start": "4753159",
    "end": "4758760"
  },
  {
    "text": "this interesting uh false alarm most of the calls were false alarm",
    "start": "4758760",
    "end": "4765239"
  },
  {
    "text": "uh about a tenth of the 10% of the data uh the calls and then there were um like",
    "start": "4765239",
    "end": "4771239"
  },
  {
    "text": "malicious fire alarms that that doesn't sound very good like you know unintentional fire alarms oh my God uh",
    "start": "4771239",
    "end": "4778080"
  },
  {
    "text": "alarm system sounded due to malfunction service call um not all of",
    "start": "4778080",
    "end": "4783920"
  },
  {
    "text": "these would probably annoy a firefighter so my theory is that firefighters",
    "start": "4783920",
    "end": "4789080"
  },
  {
    "text": "probably don't like going to tenderloin but let's go to one of the other districts like um let's go up here to",
    "start": "4789080",
    "end": "4795480"
  },
  {
    "text": "like one of those districts that only called like you know like 6,000 times uh right here I don't know what's like a",
    "start": "4795480",
    "end": "4803440"
  },
  {
    "text": "district with Pacific Heights is that like a rich one let's try that one they",
    "start": "4803440",
    "end": "4809080"
  },
  {
    "text": "called 6,000 times what type of calls came out of the uh Pacific uh District",
    "start": "4809080",
    "end": "4816639"
  },
  {
    "text": "I'm just going to go back to the same query and replace the string",
    "start": "4816639",
    "end": "4822000"
  },
  {
    "text": "tenderloin with Pacific Heights how many times did they call for nonmedical",
    "start": "4822000",
    "end": "4827920"
  },
  {
    "text": "reasons 2,000 times Let's uh go ahead and see",
    "start": "4827920",
    "end": "4833520"
  },
  {
    "text": "why so Pacific Heights called okay also false alarm but only 6,000 times for",
    "start": "4834320",
    "end": "4840239"
  },
  {
    "text": "that smoke detector building fire that's pretty legit I guess so okay so you can",
    "start": "4840239",
    "end": "4846639"
  },
  {
    "text": "kind of play around with this uh and continue this analysis um oh yeah I did Russian Hill uh these guys were actually",
    "start": "4846639",
    "end": "4853560"
  },
  {
    "text": "pretty good users of their local 911 service main time reason they called",
    "start": "4853560",
    "end": "4859800"
  },
  {
    "text": "from Russian Hill is to do service call there's fake call false",
    "start": "4859800",
    "end": "4866120"
  },
  {
    "text": "alarm so okay I thought that was kind of interesting um we did a join um another",
    "start": "4866120",
    "end": "4871800"
  },
  {
    "text": "thing I just figured I'd show you that you can actually import pandas here and take your joined data frame this is a",
    "start": "4871800",
    "end": "4878199"
  },
  {
    "text": "spark data frame filter it down just to 2016 so it's tiny now you can actually",
    "start": "4878199",
    "end": "4884600"
  },
  {
    "text": "converted to a pandas data frame and now this pandas 2016 DF is a a python pandas",
    "start": "4884600",
    "end": "4891000"
  },
  {
    "text": "data frame so you know if you want to do something in pandas because it's a richer API and there's some trick in",
    "start": "4891000",
    "end": "4896440"
  },
  {
    "text": "there you want to use then this is uh an example of just doing some stuff this",
    "start": "4896440",
    "end": "4901600"
  },
  {
    "text": "none of these are spark operations these are just pandas data frame operations and that's it um I'll open up",
    "start": "4901600",
    "end": "4909400"
  },
  {
    "text": "for Q&A for the last few minutes um but I want to say one more thing about where you can go with this here's where I",
    "start": "4909400",
    "end": "4915560"
  },
  {
    "text": "recommend here's like what I would do next right what I was thinking about doing if I had more time was um using",
    "start": "4915560",
    "end": "4923080"
  },
  {
    "text": "the ml uh Library here the machine learning library to do decision trees I",
    "start": "4923080",
    "end": "4928239"
  },
  {
    "text": "thought that would be kind of cool one of the um one of the columns I noticed in the join data frame says whether or",
    "start": "4928239",
    "end": "4935080"
  },
  {
    "text": "not the call was for a lifethreatening emergency and that's a Boolean and you",
    "start": "4935080",
    "end": "4941000"
  },
  {
    "text": "can have said that to true or false and I thought it would be interesting to then integrate spark decision tree um uh",
    "start": "4941000",
    "end": "4948400"
  },
  {
    "text": "from the ml package and train it on all the data except for 2016 keep that for",
    "start": "4948400",
    "end": "4953800"
  },
  {
    "text": "you know testing data and try to see if you can train a model to use these 60 or",
    "start": "4953800",
    "end": "4960040"
  },
  {
    "text": "so columns to be able to predict with like hopefully 95% accuracy so as soon",
    "start": "4960040",
    "end": "4966360"
  },
  {
    "text": "as the Call Comes in based on the time stamp of the call the location of the call and what was going on uh uh like uh",
    "start": "4966360",
    "end": "4976520"
  },
  {
    "text": "at that time you should be able to then predict whether or not that would be a",
    "start": "4976520",
    "end": "4981560"
  },
  {
    "text": "life-threatening call and if it is a life-threatening call you might take that a little bit more seriously and",
    "start": "4981560",
    "end": "4987000"
  },
  {
    "text": "based on your metrics alert the fire department that this is probably not a fake call even if it's in the tenderloin",
    "start": "4987000",
    "end": "4993920"
  },
  {
    "text": "our metrics are telling us with 70% probability that this is a life-threatening call I don't think that",
    "start": "4993920",
    "end": "4999760"
  },
  {
    "text": "service exists but I hope that somebody builds that and uh like I think that's some of the cool things you can do",
    "start": "4999760",
    "end": "5005639"
  },
  {
    "text": "another thing you can do with streaming is if the fire department of San Francisco connects their incoming you",
    "start": "5005639",
    "end": "5013600"
  },
  {
    "text": "know call uh station to spark streaming you can actually run some ml lib",
    "start": "5013600",
    "end": "5019159"
  },
  {
    "text": "algorithms on the live stream like logistic regression linear regression K means clustering and you can uh apply ml",
    "start": "5019159",
    "end": "5028000"
  },
  {
    "text": "algorithms on the stream and start doing analysis on the stream of data coming in so that's where I would basically go",
    "start": "5028000",
    "end": "5033880"
  },
  {
    "text": "next with this data and if you had more time I'll would try to cover some of that okay that's all I have for now um",
    "start": "5033880",
    "end": "5041360"
  },
  {
    "text": "I'll go and open it for Q&A for the last like five or 10 minutes and I do have people from data bricks here who I might",
    "start": "5041360",
    "end": "5047520"
  },
  {
    "text": "call up on stage if I don't know the answer um cool well give a big hand to Samir",
    "start": "5047520",
    "end": "5054400"
  },
  {
    "text": "guys thank you I know I know there was loads of information but I I think this is this is a great place to get started",
    "start": "5056639",
    "end": "5061960"
  },
  {
    "text": "you actually saw the power and potential of what the they communic Edition can do and all the information that that that",
    "start": "5061960",
    "end": "5068159"
  },
  {
    "text": "today was was parlay to you can actually go home and deal with it do you guys have any",
    "start": "5068159",
    "end": "5074678"
  },
  {
    "text": "questions oh one to the back okay I'll go in the ascending order",
    "start": "5075199",
    "end": "5081719"
  },
  {
    "text": "here um so some of these things you can do in SQL or you could do it with data frames is there I mean you showed the",
    "start": "5082239",
    "end": "5088920"
  },
  {
    "text": "optimizer or you mean with pandas data frames um sorry um I'm talking spark oh",
    "start": "5088920",
    "end": "5095239"
  },
  {
    "text": "and Spark you can do things with SQL or with data frames you're were saying yeah is there benefit to go one there's absolutely no benefit at all really if",
    "start": "5095239",
    "end": "5102239"
  },
  {
    "text": "you look at this um remember this like Catalyst thing I showed you this like uh",
    "start": "5102239",
    "end": "5107280"
  },
  {
    "text": "at the very end notice something here okay um let me just scroll down to",
    "start": "5107280",
    "end": "5114280"
  },
  {
    "text": "Catalyst um right here somewhere yeah right here",
    "start": "5114280",
    "end": "5121199"
  },
  {
    "text": "here's uh something important regardless of whether e you write your query as a SQL query or a data frame query or the",
    "start": "5121199",
    "end": "5128119"
  },
  {
    "text": "newor data sets API once it flows through Catalyst the exact same physical",
    "start": "5128119",
    "end": "5133760"
  },
  {
    "text": "plan will be generated and run as rdd code so it doesn't matter you just pick",
    "start": "5133760",
    "end": "5139520"
  },
  {
    "text": "your API and what are you're more comfortable with and the exact same physical plan will come out so it's not",
    "start": "5139520",
    "end": "5146480"
  },
  {
    "text": "going to be any difference in performance and then with SQL can it handle things like nested queries it",
    "start": "5146480",
    "end": "5152440"
  },
  {
    "text": "should and a nested query sparked 2.0 supports the full spark uh the full SQL",
    "start": "5152440",
    "end": "5158440"
  },
  {
    "text": "NC 2013 standard so all of the SQL 2013 queries will run in spark 2.0 this is",
    "start": "5158440",
    "end": "5166920"
  },
  {
    "text": "kind of a milestone for the spark project um like spark 1.6 could run a lot of queries but not all of the SQL",
    "start": "5166920",
    "end": "5174840"
  },
  {
    "text": "2013 queries actually if you go to the data breakes blog site we just released",
    "start": "5174840",
    "end": "5180000"
  },
  {
    "text": "a a a a nested subquery command have actually sparked to does that and and it actually shows an",
    "start": "5180000",
    "end": "5186840"
  },
  {
    "text": "explanation of how this query plan is is executed so you might I want to try it it's called you know subqueries in spark",
    "start": "5186840",
    "end": "5192639"
  },
  {
    "text": "apach spark 2.0 go to the engineering blog and you'll actually see that and that's exactly what the question was about spark2 has kind of come of age I",
    "start": "5192639",
    "end": "5200560"
  },
  {
    "text": "think that it has finally become kind of a drop in replacement for like Oracle or Hive queries um your SQL query should",
    "start": "5200560",
    "end": "5208320"
  },
  {
    "text": "translate pretty cleanly from other engines to spark SQL any other questions any other",
    "start": "5208320",
    "end": "5214840"
  },
  {
    "text": "questions oh there was one in the back hi uh yeah I'm curious to know um",
    "start": "5214840",
    "end": "5220880"
  },
  {
    "text": "if you wanted to basically chain together multiple notebooks into some kind of pipeline yeah um is the best",
    "start": "5220880",
    "end": "5227920"
  },
  {
    "text": "workflow for that simply writing to dis and then reading off the dis in data",
    "start": "5227920",
    "end": "5234199"
  },
  {
    "text": "bricks we actually have a technique to do that um there are a lot of magic commands in these notebooks like percent",
    "start": "5234199",
    "end": "5240840"
  },
  {
    "text": "SQL percent FS I didn't tell you though about percent sh that lets you run like",
    "start": "5240840",
    "end": "5247280"
  },
  {
    "text": "bash you know commands I also didn't tell you about percent run if you do",
    "start": "5247280",
    "end": "5252600"
  },
  {
    "text": "percent run on a other notebook title it will if I do if I go to the very top of",
    "start": "5252600",
    "end": "5257920"
  },
  {
    "text": "this notebook and the first cell says percent run my datab brick Mount",
    "start": "5257920",
    "end": "5263440"
  },
  {
    "text": "notebook then if I run that cell it will actually trigger the other notebook to run which itself might be running many",
    "start": "5263440",
    "end": "5270360"
  },
  {
    "text": "other notebooks that are getting triggered to run and then what I can do and um data bricks like professional or",
    "start": "5270360",
    "end": "5276800"
  },
  {
    "text": "Enterprise Edition is I can go to jobs over here and then I can actually take my final notebook and productionize it",
    "start": "5276800",
    "end": "5284320"
  },
  {
    "text": "to run on a cron schedule like at midnight and then that will basically just launch a cluster only for one or",
    "start": "5284320",
    "end": "5290159"
  },
  {
    "text": "two hours run my analysis write the results back down to paret for the other Downstream consumers of my data uh",
    "start": "5290159",
    "end": "5297239"
  },
  {
    "text": "unfortunately the jobs feature is not turned on in Community Edition it is in professional edition of data bricks",
    "start": "5297239",
    "end": "5303520"
  },
  {
    "text": "which cost cost uh $100 a month to start out with and with the $100 a month you",
    "start": "5303520",
    "end": "5310480"
  },
  {
    "text": "can run as big of a spar cluster as you want unlimited size spark clusters however uh you pay 40 cents per node um",
    "start": "5310480",
    "end": "5318880"
  },
  {
    "text": "as of uh July 2016 if you're watching on YouTube the prices will probably drop um",
    "start": "5318880",
    "end": "5323920"
  },
  {
    "text": "in the next few months but essentially you pay $100 a month Run unlimited Spar clusters we just charge you like an",
    "start": "5323920",
    "end": "5329639"
  },
  {
    "text": "hourly fee it's a small fee on top of the ec2 cost but I think it will save",
    "start": "5329639",
    "end": "5336320"
  },
  {
    "text": "you a drastic amount of time as opposed to like running yourself an EMR or doing all the devops yourself any other",
    "start": "5336320",
    "end": "5343239"
  },
  {
    "text": "questions by the way this is Apache spark you can export these notebooks as Scala or Python and run all this code",
    "start": "5343239",
    "end": "5350320"
  },
  {
    "text": "yourself let me just show you that like we are very careful about not doing something like locking people into our",
    "start": "5350320",
    "end": "5356960"
  },
  {
    "text": "platform so here you can click on this notebook we've been using and go to export and Export it as a Jupiter or",
    "start": "5356960",
    "end": "5363280"
  },
  {
    "text": "IPython notebook and just start using it on your laptop or you can export it as a",
    "start": "5363280",
    "end": "5369440"
  },
  {
    "text": "py file as a source file and so basically you can prototype in data",
    "start": "5369440",
    "end": "5374520"
  },
  {
    "text": "bricks export as a pi file and then run thatp file in your production cluster",
    "start": "5374520",
    "end": "5380080"
  },
  {
    "text": "because you can't get the data off of um uh your data center right so that might",
    "start": "5380080",
    "end": "5385119"
  },
  {
    "text": "be kind of useful for you guys if as you play around more with these environments Samir",
    "start": "5385119",
    "end": "5392440"
  },
  {
    "text": "yes John um just to jump on the whole workflows thing there's a notebook workflows that's coming out that",
    "start": "5392440",
    "end": "5398600"
  },
  {
    "text": "basically allows you to do things in response to certain sort of like outputs from notebooks um and that's about to to",
    "start": "5398600",
    "end": "5406280"
  },
  {
    "text": "come out like in the next week so yeah you should check it out right uh let's",
    "start": "5406280",
    "end": "5411800"
  },
  {
    "text": "take like two more questions maybe Max and then I'll hang around here for a few more minutes if you guys have any",
    "start": "5411800",
    "end": "5417040"
  },
  {
    "text": "questions recall that um this is all on YouTube I'm sorry if I moved a little bit too fast um but hopefully uh you",
    "start": "5417040",
    "end": "5423800"
  },
  {
    "text": "know you'll be able to play around with this stuff later as well going back to the you know this being the 2.0 and it's",
    "start": "5423800",
    "end": "5431000"
  },
  {
    "text": "somewhat still being in beta what is your um kind of I guess your suggestion as to us working it if we want to work",
    "start": "5431000",
    "end": "5437960"
  },
  {
    "text": "on something that's going to be going into production how much longer might you go into production from now um let's",
    "start": "5437960",
    "end": "5443760"
  },
  {
    "text": "say like even like two to 3 months still I think if you're going to go into production 3 months from now you should",
    "start": "5443760",
    "end": "5449040"
  },
  {
    "text": "absolutely prototype on the release candidate 2 spark 2.0 um there's a lot",
    "start": "5449040",
    "end": "5455080"
  },
  {
    "text": "of reasons first of all spark has been going through a complete re architecture",
    "start": "5455080",
    "end": "5460320"
  },
  {
    "text": "for the past year and a half and that re architecture finally comes to end with",
    "start": "5460320",
    "end": "5466040"
  },
  {
    "text": "spark 2.0 you basically want to build all your prototypes right now in 2016 on",
    "start": "5466040",
    "end": "5471719"
  },
  {
    "text": "2.0 so your API is going to be the new API of spark that will remain stable for",
    "start": "5471719",
    "end": "5477239"
  },
  {
    "text": "the next few years if you write on 1.6 you might have to recode that in a few months to use the new 2 Apo in addition",
    "start": "5477239",
    "end": "5484960"
  },
  {
    "text": "to that the uh in 2.0 out of the box your 1.6 Spar code will run about 10x",
    "start": "5484960",
    "end": "5492440"
  },
  {
    "text": "faster there's a lot of optimization specifically with um things like whole",
    "start": "5492440",
    "end": "5498920"
  },
  {
    "text": "whole stage whole stage code generation and uh vectorization and um binary formats in",
    "start": "5498920",
    "end": "5507320"
  },
  {
    "text": "memory there's a lot of interesting internal Cutting Edge things that have come to fortition finally in 2.0 Reading",
    "start": "5507320",
    "end": "5513560"
  },
  {
    "text": "part k files is three times faster than before you want those optimizations and you want all the bug fixes that went",
    "start": "5513560",
    "end": "5520880"
  },
  {
    "text": "into um I think 2.0 so I would recommend prototyping on 2.0 right now anything",
    "start": "5520880",
    "end": "5526679"
  },
  {
    "text": "else all right guys",
    "start": "5526679",
    "end": "5530960"
  }
]