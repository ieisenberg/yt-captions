[
  {
    "start": "0",
    "end": "219000"
  },
  {
    "text": "it's great to see so many people in the room today I want to talk about today about parallelism and particularly about",
    "start": "4290",
    "end": "10440"
  },
  {
    "text": "how to think about parallel performance so the you know the Java class libraries",
    "start": "10440",
    "end": "15779"
  },
  {
    "text": "have supported the ability to do parallel decomposition for the fork/join library for quite a few years now but in",
    "start": "15779",
    "end": "21570"
  },
  {
    "text": "Java eight we added something that makes it a lot easier to use parallelism which is the streams library and I think most",
    "start": "21570",
    "end": "27269"
  },
  {
    "text": "of you have used streams before right great so what I'm going to talk about today is not what streams is but how to",
    "start": "27269",
    "end": "33510"
  },
  {
    "text": "think about parallel performance and whether parallelism is going to help you or hurt you streams makes it easy to go back and",
    "start": "33510",
    "end": "39870"
  },
  {
    "text": "forth between sequential and parallel execution but we still have to think about which we want and so that's basically the topic of today's talk",
    "start": "39870",
    "end": "45899"
  },
  {
    "text": "that's right so this is how you know I work at Oracle I have to give this disclaimer slide not an imposter real",
    "start": "45899",
    "end": "51719"
  },
  {
    "text": "Oracle employee C legal slide ok great so everyone's probably seen this graph",
    "start": "51719",
    "end": "57030"
  },
  {
    "text": "right so this is a graph that herbes that are put together for almost 10 years ago at this point and it shows a",
    "start": "57030",
    "end": "63179"
  },
  {
    "text": "number of trends in in processor hardware so the green line this is",
    "start": "63179",
    "end": "68939"
  },
  {
    "text": "plotted on a log scale so the green dots show number of transistors on a chip",
    "start": "68939",
    "end": "75509"
  },
  {
    "text": "over time right so you know what since this is a log scale a straight line means something that's exponentially",
    "start": "75509",
    "end": "81600"
  },
  {
    "text": "increasing and this essentially says yep Moore's Law works it's continued to work",
    "start": "81600",
    "end": "86880"
  },
  {
    "text": "over a relatively long period of time transistor counts are consistently doubling every 18 months but what you",
    "start": "86880",
    "end": "94320"
  },
  {
    "text": "know what changed and you can sort of see that from the the lower graphs is what we got out of those additional",
    "start": "94320",
    "end": "100350"
  },
  {
    "text": "transistors so around 2002 2003 something very unfortunate happened",
    "start": "100350",
    "end": "105869"
  },
  {
    "text": "which was hardware engineers were unable to plow that transistor bounty into",
    "start": "105869",
    "end": "111750"
  },
  {
    "text": "giving us just faster and faster chips so for the longest time we were able to make our programs faster just by dragging our feet and waiting to run it",
    "start": "111750",
    "end": "118200"
  },
  {
    "text": "on the next generation of hardware that was a great trick for like 25 years unfortunately that trick doesn't work",
    "start": "118200",
    "end": "123420"
  },
  {
    "text": "anymore and so we have to like actually do our jobs now and this is demonstrated",
    "start": "123420",
    "end": "129600"
  },
  {
    "text": "by these lower three lines the you know the the blue line represents clock speed",
    "start": "129600",
    "end": "134700"
  },
  {
    "text": "so we saw a clock speed increasing exponentially for years and years and years and then it leveled off and it's pretty much stayed",
    "start": "134700",
    "end": "140700"
  },
  {
    "text": "where it is the the lower two measures are other measures of performance the light blue",
    "start": "140700",
    "end": "146160"
  },
  {
    "text": "line is power consumption so as chips got faster they were consuming more",
    "start": "146160",
    "end": "151170"
  },
  {
    "text": "power and we basically hit the limit of how much power we're willing to feed into chips for you know for cooling",
    "start": "151170",
    "end": "157920"
  },
  {
    "text": "cooling reasons mostly and the bottom the bottom line the you know that the purple line is an interesting one it",
    "start": "157920",
    "end": "164160"
  },
  {
    "text": "shows a measure of parallelism inside the chip how much work were getting out of a chip for a clock cycle and again",
    "start": "164160",
    "end": "169470"
  },
  {
    "text": "hardware engineers were able to do really cool stuff in terms of using parallelism within the chip to give us",
    "start": "169470",
    "end": "174810"
  },
  {
    "text": "faster performance and that game played out you know in 2002-2003 timeframe as",
    "start": "174810",
    "end": "180840"
  },
  {
    "text": "well so all of these all of these forces basically conspired to say chips aren't",
    "start": "180840",
    "end": "186870"
  },
  {
    "text": "getting squiggly faster it took from 2003 to maybe two thousand seven or eight to get over the period of denial",
    "start": "186870",
    "end": "192840"
  },
  {
    "text": "that followed we were kind of all hoping the good times are coming back but they weren't coming back",
    "start": "192840",
    "end": "198990"
  },
  {
    "text": "but it doesn't mean that Moore's law has been repealed we're still getting more and more transistors every year so what",
    "start": "198990",
    "end": "204600"
  },
  {
    "text": "are we doing with them we can plow them into more cores on a chip rather than faster cores so we're still",
    "start": "204600",
    "end": "210720"
  },
  {
    "text": "getting better chips but we we have to work harder to use them because we have to figure out how to paralyze our",
    "start": "210720",
    "end": "216510"
  },
  {
    "text": "software to run on many core chips so if you look at the chips that the that the",
    "start": "216510",
    "end": "224400"
  },
  {
    "start": "219000",
    "end": "219000"
  },
  {
    "text": "electronics industry is actually delivered if if we went by what Moore's law said the number of cores we would",
    "start": "224400",
    "end": "231750"
  },
  {
    "text": "have would follow this red line but in reality you know the blue dots indicate",
    "start": "231750",
    "end": "238260"
  },
  {
    "text": "high-end ship core counts you know these are the Intel e^x you know Xeon ships",
    "start": "238260",
    "end": "244100"
  },
  {
    "text": "they are increasing but not nearly as fast as Moore's law says they can increase and the basic reason for that",
    "start": "244100",
    "end": "250080"
  },
  {
    "text": "is economic no one wants to buy million core chips because we don't know what to do with them",
    "start": "250080",
    "end": "255360"
  },
  {
    "text": "so or the set of applications that know what to do with them is small enough that there isn't really a market for it",
    "start": "255360",
    "end": "261209"
  },
  {
    "text": "so instead of seeing thousand core chips today we're seeing you know high-end",
    "start": "261209",
    "end": "266640"
  },
  {
    "text": "chips with 20 40 60 you know cores but now not thousand two thousand four thousand",
    "start": "266640",
    "end": "272820"
  },
  {
    "text": "quarters and of course you know consumer chips have even lower core counts than this so we you know the multi-core age",
    "start": "272820",
    "end": "279240"
  },
  {
    "text": "is here but not as dramatically here as it was predicted it would be you know",
    "start": "279240",
    "end": "284700"
  },
  {
    "text": "ten ten years ago and I think that's largely because of us there isn't demand for these chips we don't know how to use",
    "start": "284700",
    "end": "290250"
  },
  {
    "text": "them effectively okay so concurrency isn't a end in",
    "start": "290250",
    "end": "297000"
  },
  {
    "start": "292000",
    "end": "292000"
  },
  {
    "text": "itself it's a means to an end right as a means to better performance and in particular it's a means to better processor utilization now what that's",
    "start": "297000",
    "end": "303690"
  },
  {
    "text": "what that means has changed over time but for the most part we tend to use concurrency as a way of getting more",
    "start": "303690",
    "end": "309720"
  },
  {
    "text": "performance out of the hardware there are some programming models that take advances concurrency actively for structuring a program like actors but",
    "start": "309720",
    "end": "316770"
  },
  {
    "text": "that's still sort of in the fringes for the most part we view concurrency is a tool for squeezing more performance out",
    "start": "316770",
    "end": "322200"
  },
  {
    "text": "of the hardware and the strategy you know that we use changes over time as the hardware changes so back in the",
    "start": "322200",
    "end": "328380"
  },
  {
    "text": "single-core era we use concurrency to get asynchrony to do non-blocking operations do do useful work while we",
    "start": "328380",
    "end": "336090"
  },
  {
    "text": "were waiting for an IO to complete you know when ships had a few cores we",
    "start": "336090",
    "end": "341550"
  },
  {
    "text": "tended to focus on sort of coarse grain task based concurrency so we had thread pools where you'd be running you know a",
    "start": "341550",
    "end": "348720"
  },
  {
    "text": "handful or a few dozen tasks simultaneously and this was largely about improving throughput you know",
    "start": "348720",
    "end": "354990"
  },
  {
    "text": "processing more requests per second out of your you know fork or server box as core counts increase the another target",
    "start": "354990",
    "end": "362850"
  },
  {
    "text": "becomes available which is using parallelism not to do more work on the same box but to get to the answer faster",
    "start": "362850",
    "end": "369870"
  },
  {
    "text": "so throw more cores of the problem in order to get to the answer in in a",
    "start": "369870",
    "end": "375570"
  },
  {
    "text": "smaller amount of wall clock time because maybe having the answer faster has business value right so the",
    "start": "375570",
    "end": "382260"
  },
  {
    "text": "techniques we use sort of follow the you know the hardware trends and and and this makes sense right that you know the",
    "start": "382260",
    "end": "387770"
  },
  {
    "start": "385000",
    "end": "385000"
  },
  {
    "text": "hardware guys cook up some cool new hardware stuff and then the software guys have to figure out how to use it",
    "start": "387770",
    "end": "394110"
  },
  {
    "text": "effectively and that starts with the operating system and that works its way all the way up the stack you know",
    "start": "394110",
    "end": "399570"
  },
  {
    "text": "through the JVM through the language through the libraries through the programming model and so you know the the hardware is out",
    "start": "399570",
    "end": "406280"
  },
  {
    "text": "there on the leading edge and languages libraries and frameworks are kind of trailing and so you know in 1995 you",
    "start": "406280",
    "end": "412910"
  },
  {
    "text": "know it was a fairly you know fairly cool that Java had built in support for",
    "start": "412910",
    "end": "420110"
  },
  {
    "text": "threads and locks and conditions queues built into the JVM built into the language that was kind of a new thing at the time and that was relevant to the",
    "start": "420110",
    "end": "427190"
  },
  {
    "text": "concurrency that was available from the hardware of the day when we started to see multi-core systems that's when the",
    "start": "427190",
    "end": "433880"
  },
  {
    "text": "Java libraries acquired tools like thread pools and blocking queues and concurrent collections and when we",
    "start": "433880",
    "end": "439700"
  },
  {
    "text": "started to see more cores that's when the library is acquired you know tools for prefer test based",
    "start": "439700",
    "end": "445220"
  },
  {
    "text": "parallelism and you know as the as the hardware is continued to improve we try",
    "start": "445220",
    "end": "451070"
  },
  {
    "text": "to make it easier to access the power of the hardware okay so I want to spend a",
    "start": "451070",
    "end": "457160"
  },
  {
    "start": "454000",
    "end": "454000"
  },
  {
    "text": "second on terminology people often use the words concurrency and parallelism interchangeably they're not the same",
    "start": "457160",
    "end": "464330"
  },
  {
    "text": "thing and unfortunately the historical meanings of concurrency and parallelism",
    "start": "464330",
    "end": "469550"
  },
  {
    "text": "you know they're grown up in the literature I don't think are actually as useful today as they were when they are",
    "start": "469550",
    "end": "476090"
  },
  {
    "text": "originally coined and so a lot of modern curricula are sort of redefining these terms which makes even more confusing so",
    "start": "476090",
    "end": "482180"
  },
  {
    "text": "the historical meaning was that concurrency is a property of a program structure whether things could be",
    "start": "482180",
    "end": "488900"
  },
  {
    "text": "happening at the same time whether your program is structured as cooperating activities whereas parallelism was",
    "start": "488900",
    "end": "495380"
  },
  {
    "text": "historically framed as a property of the program's execution do things actually happen at the same time or are you just",
    "start": "495380",
    "end": "501770"
  },
  {
    "text": "doing fancy scheduling tricks so in the old model concurrency was the potential for parallelism and this was a useful",
    "start": "501770",
    "end": "509210"
  },
  {
    "text": "distinction in the older days when true concurrent execution was mostly a theoretical construct but I think it's",
    "start": "509210",
    "end": "515570"
  },
  {
    "text": "less of a useful distinction today so the distinction that people are you know and that educators are using today is to",
    "start": "515570",
    "end": "522260"
  },
  {
    "text": "say that concurrency is about correctly and efficiently controlling access to",
    "start": "522260",
    "end": "527720"
  },
  {
    "text": "shared resources so this is things like constructing thread safe data structures like concurrent hash map and the",
    "start": "527720",
    "end": "534110"
  },
  {
    "text": "primitives of concurrency are things like locks and semaphores cool routine software",
    "start": "534110",
    "end": "539490"
  },
  {
    "text": "transactional memory etc it's about coordinating access to shared shared data whereas parallelism is more about",
    "start": "539490",
    "end": "546559"
  },
  {
    "text": "using additional resources to get the answer faster I could do this with one core but if I had ten cores to throw at",
    "start": "546559",
    "end": "552779"
  },
  {
    "text": "it maybe I can get it faster I've had a hundred course to throw at it maybe I could get the answer even faster so all",
    "start": "552779",
    "end": "559079"
  },
  {
    "text": "right this may sound like an academic digression why should we care about this distinction well the reason the",
    "start": "559079",
    "end": "565170"
  },
  {
    "text": "distinction is relevant is it turns out that concurrency is usually pretty hard",
    "start": "565170",
    "end": "570529"
  },
  {
    "text": "that the tools that we have for concurrency like you know locks and threads requires complex reasoning",
    "start": "570529",
    "end": "576600"
  },
  {
    "text": "that's like easy to get wrong you know and even if you have like you know the secret wizard spell book it's still hard",
    "start": "576600",
    "end": "582720"
  },
  {
    "text": "you know to get it right whereas parallelism tends to be really easy at",
    "start": "582720",
    "end": "587730"
  },
  {
    "text": "least in terms of the way you think about it because parallelism that the standard trick and parallelism is partition the problem into two smaller",
    "start": "587730",
    "end": "594120"
  },
  {
    "text": "problems and then work on the problems independently and then when you're done combine the results and this is how you",
    "start": "594120",
    "end": "600779"
  },
  {
    "text": "know we effectively you know share work in the real world right if you've got a stack of papers to grade well you call",
    "start": "600779",
    "end": "606629"
  },
  {
    "text": "in your teaching assistants you know you give them each you know one ends of the other stack you go you know you go to",
    "start": "606629",
    "end": "611939"
  },
  {
    "text": "lunch and let them do the work right and they don't need to coordinate with each other while they're grading the papers",
    "start": "611939",
    "end": "617009"
  },
  {
    "text": "and then you know the end they just make a big pile and and you've got your work done very efficiently so it doesn't",
    "start": "617009",
    "end": "624509"
  },
  {
    "text": "require a lot of reasoning as long as you partition the data and partition the work into independent tasks so",
    "start": "624509",
    "end": "629850"
  },
  {
    "text": "parallelism tends to be much easier than concurrency people are scared by concurrency rightly so cause it's hard",
    "start": "629850",
    "end": "636120"
  },
  {
    "text": "parallelism doesn't have to be hard so it's worth noting that parallelism is",
    "start": "636120",
    "end": "642149"
  },
  {
    "text": "strictly an optimization it's about using more resources to get the answer faster if you don't have additional",
    "start": "642149",
    "end": "648600"
  },
  {
    "text": "resources you just have one core to work on no problem you can still compute the answer sequentially it just might be",
    "start": "648600",
    "end": "654420"
  },
  {
    "text": "slower now the flip side of parallelism being an optimization is that it's only useful if it is actually an optimization",
    "start": "654420",
    "end": "661860"
  },
  {
    "text": "which is to say it actually gets you the answer faster and it isn't necessarily the case that using more resources gets",
    "start": "661860",
    "end": "668160"
  },
  {
    "text": "you the answer faster or even as fast so there are some parallel computations that are slower",
    "start": "668160",
    "end": "673650"
  },
  {
    "text": "than the the obvious sequential alternative so that means we have to",
    "start": "673650",
    "end": "679080"
  },
  {
    "text": "think about it unfortunately but it's not hard you know we have tools we can you know we have analysis we can look",
    "start": "679080",
    "end": "685440"
  },
  {
    "text": "the problem think about the problem we can implement measure test repeat etc a",
    "start": "685440",
    "end": "690470"
  },
  {
    "text": "general rule of thumb this is one of the questions that like oh that comes up on Stack Overflow all the time is like ok",
    "start": "690470",
    "end": "696180"
  },
  {
    "text": "Java has parallel streams should I use parallelism everywhere know start with",
    "start": "696180",
    "end": "701700"
  },
  {
    "text": "sequential streams and then if you need to optimize your code because you're not",
    "start": "701700",
    "end": "707010"
  },
  {
    "text": "getting to the answer fast enough where fast enough decided defined by actual performance targets and business",
    "start": "707010",
    "end": "712950"
  },
  {
    "text": "requirements then you can think about whether making it parallel will actually get you a speed-up and the way we",
    "start": "712950",
    "end": "720210"
  },
  {
    "text": "measure the effectiveness of parallelism is by speed-up how much faster is the parallel execution or slower than the",
    "start": "720210",
    "end": "726330"
  },
  {
    "text": "sequential execution okay now it's worth keeping in mind that a parallel",
    "start": "726330",
    "end": "732120"
  },
  {
    "text": "computation is always going to do more work than the best sequential alternative because it still has to",
    "start": "732120",
    "end": "739350"
  },
  {
    "text": "solve the problem and it has to do extra stuff to manage the parallelism split up the work hand it out to workers wait for",
    "start": "739350",
    "end": "746160"
  },
  {
    "text": "the workers to finish combine the results you know that's extra overhead on top of actually solving the problem",
    "start": "746160",
    "end": "753300"
  },
  {
    "text": "so the parallel version always kind of starts out at a deficit against the",
    "start": "753300",
    "end": "758880"
  },
  {
    "text": "sequential version and you're hoping to make up for it in volume right so if you have you know 20 papers to grade going",
    "start": "758880",
    "end": "766560"
  },
  {
    "text": "out and finding 20 people to do the work talking them each into doing you know doing one paper probably takes longer",
    "start": "766560",
    "end": "772470"
  },
  {
    "text": "than grading them yourself you know with so you have to have enough enough volume",
    "start": "772470",
    "end": "777990"
  },
  {
    "text": "and you have to have a parallelizable problem and you have to have a good parallel implementation in order for",
    "start": "777990",
    "end": "783450"
  },
  {
    "text": "this to win right so what most of this talk is about is about looking at a problem and developing an intuition",
    "start": "783450",
    "end": "791040"
  },
  {
    "text": "about whether parallelism is likely to help you or not of course you still want",
    "start": "791040",
    "end": "796800"
  },
  {
    "text": "to measure on top of that but a lot of problems you can look at and say that's not going to paralyse because",
    "start": "796800",
    "end": "802700"
  },
  {
    "text": "not enough data there's you know an inherent bottleneck here etc also I'll go through all the examples of things",
    "start": "802700",
    "end": "809210"
  },
  {
    "text": "that uh you know that can deprive you of parallelism so as an example uh you know",
    "start": "809210",
    "end": "816950"
  },
  {
    "start": "814000",
    "end": "814000"
  },
  {
    "text": "to start with you have to have a problem that's parallelizable to begin with some problems are inherently sequential by",
    "start": "816950",
    "end": "822620"
  },
  {
    "text": "their definition so here's a pair of functions G of 0 you have some operation",
    "start": "822620",
    "end": "829610"
  },
  {
    "text": "F okay and then you're going to find G where you're saying G of 0 is just F of 0 and G of n is f of G of n minus 1 ok",
    "start": "829610",
    "end": "838540"
  },
  {
    "text": "so and similarly I have another you know another function whose definition looks",
    "start": "838540",
    "end": "845150"
  },
  {
    "text": "almost the same but actually has different parallelism characteristics and I'll go through it but it's worth",
    "start": "845150",
    "end": "850610"
  },
  {
    "text": "pointing out that even though they're both defined recursively they have very similar structure the left one is",
    "start": "850610",
    "end": "857210"
  },
  {
    "text": "inherently sequential and the right one is not and and and I'll show how that works ok so let's take the first one",
    "start": "857210",
    "end": "865660"
  },
  {
    "text": "parallelism is going to be a lost cause on this because if I rewrite my function you know G as an iterated function what",
    "start": "866260",
    "end": "874460"
  },
  {
    "text": "I realize is that G of n is basically F of F of F of F of F of 0 with n",
    "start": "874460",
    "end": "880450"
  },
  {
    "text": "applications of F and I can't start computing F of F of 0 until I've",
    "start": "880450",
    "end": "886040"
  },
  {
    "text": "computed F of 0 right the problem is fundamentally sequential and if I draw a dataflow dependency diagram well G of 0",
    "start": "886040",
    "end": "893270"
  },
  {
    "text": "depends on F of 0 G of 1 depends on G of 0 you look at that and you say that's just not going to paralyse right so some",
    "start": "893270",
    "end": "900590"
  },
  {
    "text": "problems just don't paralyze and so it's worth recognizing that if you're lucky",
    "start": "900590",
    "end": "905600"
  },
  {
    "text": "you try to implement this in parallel will be no slower than the sequential implementation that's the best case for",
    "start": "905600",
    "end": "911360"
  },
  {
    "text": "a non paralyzed bull problem all right so let's look at our other example which looks almost the same especially if you",
    "start": "911360",
    "end": "917750"
  },
  {
    "text": "don't like math and all you see your FS and G's and recursive definitions and all that this problem turns out to be",
    "start": "917750",
    "end": "926000"
  },
  {
    "text": "very parallelizable because if we draw the dataflow dependency diagram we see ok H of 0 depends on F of 0 H of 1 will",
    "start": "926000",
    "end": "933530"
  },
  {
    "text": "have depends on H of 0 and it also depends on F of 1 but can calculate F of 1 and F of 0 separately because they have no you know",
    "start": "933530",
    "end": "941090"
  },
  {
    "text": "they don't dependent anything and at each level of the each level of the tree there's I can get some parallelism",
    "start": "941090",
    "end": "947210"
  },
  {
    "text": "because I can be calculating all of these s simultaneously right so if you",
    "start": "947210",
    "end": "953390"
  },
  {
    "text": "if you draw out the data flow diagram it gives you a very quick hint as to whether the problem that you have is",
    "start": "953390",
    "end": "960380"
  },
  {
    "text": "going to be amenable to parallelization or not so if you rewrite H you realize",
    "start": "960380",
    "end": "966740"
  },
  {
    "text": "that H of n is just adding up a bunch of independent things and addition paralyzes nicely you know and and",
    "start": "966740",
    "end": "974630"
  },
  {
    "text": "computing those F of 1 F of 2 also paralyzes nicely right so there should be some ability to extract parallelism",
    "start": "974630",
    "end": "981170"
  },
  {
    "text": "from this problem in fact this is what we call an embarrassingly parallel problem but it is possible to write the",
    "start": "981170",
    "end": "987560"
  },
  {
    "text": "solution in such a way that we get in the way of extracting the parallelism so step one is you have to have a problem",
    "start": "987560",
    "end": "993650"
  },
  {
    "text": "it's parallelizable step two is that you uh you also have to have an approach for",
    "start": "993650",
    "end": "1001420"
  },
  {
    "text": "extracting the parallelism ok so like I",
    "start": "1001420",
    "end": "1008710"
  },
  {
    "start": "1006000",
    "end": "1006000"
  },
  {
    "text": "said these two functions look similar one was parallelizable and one was not so let's look at how we might actually",
    "start": "1008710",
    "end": "1015010"
  },
  {
    "text": "Express the computation of this two you know to exploit the power possible parallelism and the key the key takeaway",
    "start": "1015010",
    "end": "1022180"
  },
  {
    "text": "in this section is we all have some bad habits we have some bad habits from having written sequential code for 10 20",
    "start": "1022180",
    "end": "1029650"
  },
  {
    "text": "30 40 years depending on how long even programming and we have to unlearn those bad habits in order to effectively write",
    "start": "1029650",
    "end": "1037780"
  },
  {
    "text": "parallel code ok so again let's take a simple problem this is basically what",
    "start": "1037780",
    "end": "1042819"
  },
  {
    "start": "1039000",
    "end": "1039000"
  },
  {
    "text": "the H function is add up the numbers from 1 to n so you know what kind of data flow graph do you know what do we get out of that well let's let's write",
    "start": "1042819",
    "end": "1050170"
  },
  {
    "text": "up an implementation here's a sequential implementation where we have a you know some counter and a for loop where we you",
    "start": "1050170",
    "end": "1057070"
  },
  {
    "text": "know where we increment a you know our counter well if we if we write it out",
    "start": "1057070",
    "end": "1063070"
  },
  {
    "text": "this way we're not calculating the sum you know",
    "start": "1063070",
    "end": "1068230"
  },
  {
    "text": "we're essentially calculating the sum sequentially and so we end up with a computation tree that looks looks like",
    "start": "1068230",
    "end": "1074960"
  },
  {
    "text": "this you know something like this now that's not the computation tree we want",
    "start": "1074960",
    "end": "1079970"
  },
  {
    "text": "but that's the computation tree we get out of a typical typical sequential implementation the computation tree we",
    "start": "1079970",
    "end": "1086390"
  },
  {
    "text": "want is something that looks more like this where you do a bunch of additions in parallel and then the next level up",
    "start": "1086390",
    "end": "1092360"
  },
  {
    "text": "and the tree you do more additions in parallel and it's only when you get to that last operation where you have to",
    "start": "1092360",
    "end": "1097400"
  },
  {
    "text": "two pieces to add that you end up being sequential right so one of the patterns",
    "start": "1097400",
    "end": "1104120"
  },
  {
    "text": "you know that we have to unlearn that getsome gets in the way of writing parallel friendly code is this accumulator pattern right as soon as you",
    "start": "1104120",
    "end": "1111080"
  },
  {
    "text": "start with in sum equals zero you're already dead right because you've already created a bottleneck that every",
    "start": "1111080",
    "end": "1118220"
  },
  {
    "text": "task is going to have to contend for access to that sum so whenever you see code like this that should be a warning",
    "start": "1118220",
    "end": "1123620"
  },
  {
    "text": "sign this is the we call this the simulator anti-pattern you know this is",
    "start": "1123620",
    "end": "1129110"
  },
  {
    "text": "going to get in the way of parallelism so all right well how would we solve",
    "start": "1129110",
    "end": "1134720"
  },
  {
    "text": "this problem you know not sequentially well we could try it with concurrency you know here's a a broken example here",
    "start": "1134720",
    "end": "1143799"
  },
  {
    "text": "but it illustrates you know part of the challenge of having this accumulator variable where we say okay I'm going to",
    "start": "1143799",
    "end": "1150559"
  },
  {
    "text": "whack the array in two pieces so I have a left half in a right half and I have my sum accumulator and then somehow",
    "start": "1150559",
    "end": "1156110"
  },
  {
    "text": "concurrently magically wave you know sprinkle the magic concurrency dust I'm going to you know sum up the first half",
    "start": "1156110",
    "end": "1163850"
  },
  {
    "text": "and the second half of the array concurrently and they're both trying to update the",
    "start": "1163850",
    "end": "1169490"
  },
  {
    "text": "same accumulator variable what's going to happen here right this is broken right because we have a data race both",
    "start": "1169490",
    "end": "1176659"
  },
  {
    "text": "both of these concurrent activities are trying to both read and write this shared shared accumulator and you're",
    "start": "1176659",
    "end": "1184669"
  },
  {
    "text": "going to get the wrong answer okay well we know how to fix that right we can you",
    "start": "1184669",
    "end": "1189860"
  },
  {
    "text": "know make sure that we do this atomically and how do we do this atomically well we take a lock well that",
    "start": "1189860",
    "end": "1196820"
  },
  {
    "text": "kind of makes the code correct but our performance is going to be even worse than the sequential version because now",
    "start": "1196820",
    "end": "1204200"
  },
  {
    "text": "every time someone goes to add a number to a counter they have to take a lock and well it's almost guaranteed that the",
    "start": "1204200",
    "end": "1211460"
  },
  {
    "text": "other guy has the lock so they're going to spend a lot of time waiting for the other to give up the lock and it",
    "start": "1211460",
    "end": "1216619"
  },
  {
    "text": "actually takes a fair amount of time you know to to release a lock to another thread so you have this tiny tiny amount",
    "start": "1216619",
    "end": "1222710"
  },
  {
    "text": "of work to do you want to add two numbers together you take a lock which like Dwarfs the amount of work the",
    "start": "1222710",
    "end": "1228080"
  },
  {
    "text": "amount of time it takes to do the addition so we have a solution that's correct but is like actually worse than",
    "start": "1228080",
    "end": "1233509"
  },
  {
    "text": "the sequential version so all right so where are we well we had a problem that",
    "start": "1233509",
    "end": "1239419"
  },
  {
    "text": "was embarrassingly parallel but we tried a couple of ways to exploit the parallelism and we failed all right well",
    "start": "1239419",
    "end": "1245690"
  },
  {
    "text": "let's keep trying you know if at first you don't succeed right so but I'll step",
    "start": "1245690",
    "end": "1250940"
  },
  {
    "start": "1249000",
    "end": "1249000"
  },
  {
    "text": "back the fundamental problem you know with the the broken examples is shared state",
    "start": "1250940",
    "end": "1256070"
  },
  {
    "text": "the first version was broken because it didn't coordinate access to the shared state the second version sucks because",
    "start": "1256070",
    "end": "1261649"
  },
  {
    "text": "it coordinated access to the shared state but we lost all our performance so there's you know three ways to safely",
    "start": "1261649",
    "end": "1267830"
  },
  {
    "text": "handle shared state either coordinate access using walking don't share or",
    "start": "1267830",
    "end": "1274009"
  },
  {
    "text": "don't mutate generally the first two don't share or don't mutate are a lot",
    "start": "1274009",
    "end": "1279019"
  },
  {
    "text": "easier to get right then than the third so let's try the don't share approach let's do the same thing we did before we",
    "start": "1279019",
    "end": "1284720"
  },
  {
    "text": "partition the array into two chunks will operate on them separately but we won't have any shared state right so here's a",
    "start": "1284720",
    "end": "1291080"
  },
  {
    "text": "first cut at a parallel solution it looks almost like the other one but this one actually works because the first",
    "start": "1291080",
    "end": "1297440"
  },
  {
    "text": "task is busy incrementing the left some the second task is busy incrementing the",
    "start": "1297440",
    "end": "1302690"
  },
  {
    "text": "right some they're not they're not accessing a shared counter so there's there's no need for coordination there's",
    "start": "1302690",
    "end": "1309049"
  },
  {
    "text": "no contention we get the right answer and then when it's all done we add the two together and everything's good",
    "start": "1309049",
    "end": "1315379"
  },
  {
    "text": "so this is you know this is basically the game with parallelism where you want",
    "start": "1315379",
    "end": "1320419"
  },
  {
    "text": "to divide the problem into generally two pieces you want to have each piece work",
    "start": "1320419",
    "end": "1325549"
  },
  {
    "text": "on their part of the problem independently without having to access either shared inputs or shared outputs",
    "start": "1325549",
    "end": "1331599"
  },
  {
    "text": "and that way they can each work efficiently on their part and you get and get to the right answer so",
    "start": "1331599",
    "end": "1338309"
  },
  {
    "text": "decompose the problem into subproblems solve the subproblems combine the result hopefully with no sharing and no",
    "start": "1338309",
    "end": "1344220"
  },
  {
    "text": "contention so alright how's this going to perform well if we have enough data",
    "start": "1344220",
    "end": "1353370"
  },
  {
    "text": "then we'll overcome the overhead of like forking off these concurrent tasks waiting for them to finish and joining",
    "start": "1353370",
    "end": "1360030"
  },
  {
    "text": "them and we'll probably get a pretty good speed up you know if we have if we have two cores we probably won't lose",
    "start": "1360030",
    "end": "1367020"
  },
  {
    "text": "very big if we have one core it'll just run the tasks you know what you know one ahead of the other but the only thing we",
    "start": "1367020",
    "end": "1372960"
  },
  {
    "text": "lose is the fixed overhead of dividing it but given the structure of this program it's pretty obvious that if we",
    "start": "1372960",
    "end": "1378450"
  },
  {
    "text": "had 100 cores we're still only going to be able to use two of them effectively so we've made progress potentially we",
    "start": "1378450",
    "end": "1385860"
  },
  {
    "text": "could double our speed with enough data but we'd like this to be able to scale to mini course right so let's see how we",
    "start": "1385860",
    "end": "1392700"
  },
  {
    "text": "would you know how we would extend that so the basic game for parallel execution is you know the old track divide and",
    "start": "1392700",
    "end": "1400110"
  },
  {
    "start": "1394000",
    "end": "1394000"
  },
  {
    "text": "conquer you recursively divide your problem into smaller problems so you",
    "start": "1400110",
    "end": "1405270"
  },
  {
    "text": "take a big problem divided into smaller problems if those smaller problems are still kind of big you divide them again",
    "start": "1405270",
    "end": "1412650"
  },
  {
    "text": "into smaller problems and you keep doing them until they're small enough that the logical thing to do is to solve the",
    "start": "1412650",
    "end": "1418440"
  },
  {
    "text": "sub-problem sequentially right so if I'm asking you to add up two numbers you're not going to fork off two tasks you know",
    "start": "1418440",
    "end": "1425700"
  },
  {
    "text": "you're going to say adding adding these numbers there's so little work that I'll just do it sequentially so when you get",
    "start": "1425700",
    "end": "1430800"
  },
  {
    "text": "down to a small small data size it's generally more efficient to do it sequentially so recursively divided the",
    "start": "1430800",
    "end": "1439020"
  },
  {
    "text": "problem until the chunks are small enough solve the chunks concurrently hopefully whether you know they're",
    "start": "1439020",
    "end": "1444600"
  },
  {
    "text": "independent and then combine the results so this is the pseudo code is the problem small if so solve it",
    "start": "1444600",
    "end": "1450840"
  },
  {
    "text": "sequentially otherwise concurrently solve the left sub problem and the right sub problem and then combine the results",
    "start": "1450840",
    "end": "1457230"
  },
  {
    "text": "this this is basically all parallel programs have this behavior the nice",
    "start": "1457230",
    "end": "1464190"
  },
  {
    "text": "thing about this is it's simple right how many cores I had wasn't an input",
    "start": "1464190",
    "end": "1469679"
  },
  {
    "text": "into that code oh so it wasn't input into writing the code and then put into running the code and a lot of problems a",
    "start": "1469679",
    "end": "1479190"
  },
  {
    "text": "lot of data structures are already defined recursively right you know what trees are defined recursively so you're",
    "start": "1479190",
    "end": "1484200"
  },
  {
    "text": "already going to be operating on them recursively anyway so it you know it's a good match for a lot of data structures",
    "start": "1484200",
    "end": "1489960"
  },
  {
    "text": "anyway there's no shared mutable state because even though they're reading data out of the same array they're just",
    "start": "1489960",
    "end": "1495929"
  },
  {
    "text": "reading they're not right and we don't have to coordinate for that the intermediate results like those intermediate sums they just live on the",
    "start": "1495929",
    "end": "1502320"
  },
  {
    "text": "stack and you know so this has the potential to be very efficient of course",
    "start": "1502320",
    "end": "1509669"
  },
  {
    "text": "double in the details right you know how expensive is it to fork off a task how expensive it is to wait for you know for",
    "start": "1509669",
    "end": "1516150"
  },
  {
    "text": "a task these all affect you know what kind of speed up you're going to get generally because spinning up tasks",
    "start": "1516150",
    "end": "1523380"
  },
  {
    "text": "especially you know at the early stages of a program has some cost you want to",
    "start": "1523380",
    "end": "1528659"
  },
  {
    "text": "be able to start breaking off work early to start some worker working on some sub part of your problem while you're still",
    "start": "1528659",
    "end": "1534120"
  },
  {
    "text": "decomposing the rest of it but the decomposition should be dynamic and it can it can incorporate how many cores do",
    "start": "1534120",
    "end": "1541679"
  },
  {
    "text": "I have what's the load and that's exactly what the fork/join framework that we added in Java 7 does it manages",
    "start": "1541679",
    "end": "1549270"
  },
  {
    "text": "the you know the decomposition of tasks and the waiting for tasks and all of that so you only have to write the part",
    "start": "1549270",
    "end": "1556110"
  },
  {
    "text": "of your program that has what problem I trying to solve and the mechanics of creating tasks merging tasks handled by",
    "start": "1556110",
    "end": "1562409"
  },
  {
    "text": "the library for you so you know to draw a very simplistic picture you know if I",
    "start": "1562409",
    "end": "1569159"
  },
  {
    "start": "1564000",
    "end": "1564000"
  },
  {
    "text": "have an array of numbers I want to add them up I could add them up sequentially if I only had eight numbers that would be the logical thing to do but imagine",
    "start": "1569159",
    "end": "1575909"
  },
  {
    "text": "it's a you know much bigger array so what am I going to do I'm going to divide it into two I'm going to divide",
    "start": "1575909",
    "end": "1581070"
  },
  {
    "text": "that into two again and I'm not copying the data I'm just sharing sharing the reference because my tasks are only",
    "start": "1581070",
    "end": "1586080"
  },
  {
    "text": "going to work on their little part of the problem and then you know this turns",
    "start": "1586080",
    "end": "1591419"
  },
  {
    "text": "into you know a bunch of little add up one and two add up three and four add up five and six out of seven and eight and",
    "start": "1591419",
    "end": "1597870"
  },
  {
    "text": "because addition is associative I can do that and whatever I like and then I can you know compute",
    "start": "1597870",
    "end": "1603049"
  },
  {
    "text": "the partial sums and I work my way up the tree until I have my answer so it",
    "start": "1603049",
    "end": "1610580"
  },
  {
    "text": "actually does work like that right it's fairly straightforward and it's easy to reason about because you know everyone",
    "start": "1610580",
    "end": "1616970"
  },
  {
    "text": "works on their little subset of the data okay so this all sounds great how does it work in practice well",
    "start": "1616970",
    "end": "1624080"
  },
  {
    "start": "1619000",
    "end": "1619000"
  },
  {
    "text": "sometimes it works great in practice sometimes it doesn't how do we know whether it's going to work great or not well we could measure but before we",
    "start": "1624080",
    "end": "1631039"
  },
  {
    "text": "measure we could think about it so that's what we talked about what are the costs in doing that parallel",
    "start": "1631039",
    "end": "1637370"
  },
  {
    "text": "decomposition well sometimes has a cost of splitting the problem into some problems are much",
    "start": "1637370",
    "end": "1643520"
  },
  {
    "text": "cheaper to split than other problems sometimes splitting is so expensive it's easier to just do the things",
    "start": "1643520",
    "end": "1648980"
  },
  {
    "text": "sequentially you know if you have to copy a lot of data to just to split the problem that's you know that becomes an",
    "start": "1648980",
    "end": "1655520"
  },
  {
    "text": "impediment to parallelism there are the tasks based cost forking off tasks",
    "start": "1655520",
    "end": "1660980"
  },
  {
    "text": "joining with tasks you know you can you know you can do a lot of work in the time it takes to do a handoff across",
    "start": "1660980",
    "end": "1666650"
  },
  {
    "text": "threads so you don't want to spend all your time managing tasks you want to spend your time keeping those cores busy",
    "start": "1666650",
    "end": "1672740"
  },
  {
    "text": "doing like actual work on the other side there's some cost to combine the results",
    "start": "1672740",
    "end": "1679460"
  },
  {
    "text": "some combination operations are cheaper than others if you know in the example I gave where we're summing an array the",
    "start": "1679460",
    "end": "1685280"
  },
  {
    "text": "combination operator is take the sum of the left half and the sum of the right half and add them that's a very cheap",
    "start": "1685280",
    "end": "1691250"
  },
  {
    "text": "combination operation but what if the result of the subproblem with a set and i had to merge the sets that's a much",
    "start": "1691250",
    "end": "1697250"
  },
  {
    "text": "more expensive operation right so just like splitting costs combination cost",
    "start": "1697250",
    "end": "1702679"
  },
  {
    "text": "can also be an impediment to parallelism and then the elephant of the room is that pesky hardware a hardware locality",
    "start": "1702679",
    "end": "1710059"
  },
  {
    "text": "tends to be a significant factor in determining whether we get parallelism out of a you know out of a problem or",
    "start": "1710059",
    "end": "1715940"
  },
  {
    "text": "not and when I say hardware look memory locality I mean is the data that a given",
    "start": "1715940",
    "end": "1722030"
  },
  {
    "text": "task is working on located close to the other data that it's working on in memory because if it is it will be hot",
    "start": "1722030",
    "end": "1729200"
  },
  {
    "text": "in cache already and when the the task goes to get the next item to work on it not going to be waiting on the memory",
    "start": "1729200",
    "end": "1735830"
  },
  {
    "text": "subsystem to cough up data if you have a data structure that exhibits poor locality you may divide it up into a",
    "start": "1735830",
    "end": "1743029"
  },
  {
    "text": "whole bunch of worker threads but then what's likely to happen is the worker threads are going to spend all their time waiting on the cache subsystem to",
    "start": "1743029",
    "end": "1749779"
  },
  {
    "text": "deliver the next piece of data and operate on and most systems are memory bandwidth limited they're not CPU",
    "start": "1749779",
    "end": "1755809"
  },
  {
    "text": "limited and so if you've got a lot of cores all cash missing at once you're not going to get a lot of throughput out",
    "start": "1755809",
    "end": "1761360"
  },
  {
    "text": "of it so each of these factors yields away part of the potential for parallelism and you know you know so if",
    "start": "1761360",
    "end": "1770659"
  },
  {
    "text": "you analyze your problem and you say are any of these particularly bad you're likely to get poor parallelism out of it",
    "start": "1770659",
    "end": "1776720"
  },
  {
    "text": "and in general you need a fair amount of data for parallelism to work for you if",
    "start": "1776720",
    "end": "1781759"
  },
  {
    "text": "you're adding up a thousand numbers do it sequentially it's much faster but if you're adding up a billion numbers",
    "start": "1781759",
    "end": "1787450"
  },
  {
    "text": "parallelism going to give you a big advantage ok so like I said Java Java SE 7 adds the",
    "start": "1787450",
    "end": "1794929"
  },
  {
    "start": "1792000",
    "end": "1792000"
  },
  {
    "text": "fork/join framework this is a task management framework designed for fine-grained mostly CPU intensive tasks",
    "start": "1794929",
    "end": "1801710"
  },
  {
    "text": "doesn't mean you can't do use it with i/o but it's tuned for CPU intensive tasks the nice thing is it scales really",
    "start": "1801710",
    "end": "1808159"
  },
  {
    "text": "well over the range of cores that you're likely to encounter on a real system and",
    "start": "1808159",
    "end": "1813409"
  },
  {
    "text": "so you don't have to worry about the details of task management and it's basically optimized for the kind of",
    "start": "1813409",
    "end": "1818929"
  },
  {
    "text": "divide and conquer algorithms I've been showing so the two basic operations are fork off a task and wait for that task",
    "start": "1818929",
    "end": "1827119"
  },
  {
    "text": "to complete fork and join and that's an efficient implementation of that magic concurrent primitive that I was showing",
    "start": "1827119",
    "end": "1833629"
  },
  {
    "text": "in my earlier examples the overhead isn't zero but it's pretty good all",
    "start": "1833629",
    "end": "1840110"
  },
  {
    "text": "right so let's talk about screens you know we talked about how you know you've",
    "start": "1840110",
    "end": "1846139"
  },
  {
    "text": "got the stream framework it's pretty cool people like it it gives you almost free parallelism in terms of making it",
    "start": "1846139",
    "end": "1852919"
  },
  {
    "text": "really easy to express that I want this computation sequential or parallel and flip back and forth between them so",
    "start": "1852919",
    "end": "1859759"
  },
  {
    "text": "that's great you know and it encourages a sort of more declarative style that",
    "start": "1859759",
    "end": "1864830"
  },
  {
    "text": "you know programming that tends to be more readable and they're prone so that's great but it's",
    "start": "1864830",
    "end": "1870759"
  },
  {
    "text": "not magic parallelism dust and so you still have to know is this going to",
    "start": "1870759",
    "end": "1876789"
  },
  {
    "text": "effects acute efficiently in parallel otherwise you can easily say dot parallel and maybe it's not any faster",
    "start": "1876789",
    "end": "1883360"
  },
  {
    "text": "maybe it's even slower okay so you know you've all seen examples like this right",
    "start": "1883360",
    "end": "1889690"
  },
  {
    "text": "you know you have imperative code like this you can turn this into a screen pipeline you know uh and yeah the code",
    "start": "1889690",
    "end": "1895629"
  },
  {
    "text": "is smaller it's more compact it's easier to read but the important thing is it's it's declarative it's telling you what",
    "start": "1895629",
    "end": "1902139"
  },
  {
    "text": "answer you want rather than how to get to the answer and the benefit there is it's much easier for the library to say",
    "start": "1902139",
    "end": "1909789"
  },
  {
    "text": "I know how to do this in parallel I'm going to go decompose it for you there's basically no compiler that could like",
    "start": "1909789",
    "end": "1915609"
  },
  {
    "text": "paralyze that for that first example so you know but parallelism is the bonus it",
    "start": "1915609",
    "end": "1920980"
  },
  {
    "text": "wasn't the point of doing this the point of doing this was writing code that's easier to read less likely to be wrong",
    "start": "1920980",
    "end": "1926879"
  },
  {
    "text": "but it's a nice bonus that the the runtime can can optimize it for you as",
    "start": "1926879",
    "end": "1932169"
  },
  {
    "start": "1932000",
    "end": "1932000"
  },
  {
    "text": "well okay so before I gave a bunch of criteria of things that steal away your",
    "start": "1932169",
    "end": "1937840"
  },
  {
    "text": "your your parallel performance splitting combination past dispatch and locality",
    "start": "1937840",
    "end": "1944320"
  },
  {
    "text": "so each of those apply to a parallel stream the parallel streams work by splitting their data sources so the",
    "start": "1944320",
    "end": "1951399"
  },
  {
    "text": "first question is well what's the source for my stream is it an ArrayList or linked list is you know and and these",
    "start": "1951399",
    "end": "1956980"
  },
  {
    "text": "split very differently and ArrayList splits really nicely you just compute the midpoint and say you work on that",
    "start": "1956980",
    "end": "1963159"
  },
  {
    "text": "half you work on that half whereas the linked list doesn't split so nicely how do you split a linked list first and",
    "start": "1963159",
    "end": "1970330"
  },
  {
    "text": "rest well you could do that again recursively right you have first and then first of the rest and rest of the",
    "start": "1970330",
    "end": "1976539"
  },
  {
    "text": "rest and you can keep going and you actually could get some parallelism out of this if the task you're doing is",
    "start": "1976539",
    "end": "1982869"
  },
  {
    "text": "sufficiently expensive right if you're doing some really expensive you know cryptographic attack you actually could",
    "start": "1982869",
    "end": "1989409"
  },
  {
    "text": "get some parallelism out of a linked list but if you're just adding up you know numbers or searching you know searching a data structure for something",
    "start": "1989409",
    "end": "1995649"
  },
  {
    "text": "you're not going to get it right so splitting cost is a real concern and so you need to know where's",
    "start": "1995649",
    "end": "2001240"
  },
  {
    "text": "data coming from and how well is it likely to split tasks dispatched while that hats handled by the fork/join",
    "start": "2001240",
    "end": "2007270"
  },
  {
    "text": "framework you can't do too much about that the combination cost though is again part of your problem am i adding",
    "start": "2007270",
    "end": "2012940"
  },
  {
    "text": "up numbers or my merging sets and then locality goes back to the source what is",
    "start": "2012940",
    "end": "2018580"
  },
  {
    "text": "my source is an array arrays have great locality is it a linked list linked lists have almost pessimal locality",
    "start": "2018580",
    "end": "2024310"
  },
  {
    "text": "right so it you have to know where's my data and how to laid out in memory so",
    "start": "2024310",
    "end": "2031870"
  },
  {
    "start": "2031000",
    "end": "2031000"
  },
  {
    "text": "I'm going to wave my hands and give you a completely unscientific model for",
    "start": "2031870",
    "end": "2037030"
  },
  {
    "text": "thinking about parallel performance I see Martin wincing when I say this so",
    "start": "2037030",
    "end": "2042990"
  },
  {
    "text": "imagine a really simple model where you only have two parameters how much data do I have and how much work do I do per",
    "start": "2042990",
    "end": "2051280"
  },
  {
    "text": "data element and you know where work might be you know a basic arithmetic",
    "start": "2051280",
    "end": "2057310"
  },
  {
    "text": "operation or a bytecode or machine cycle if it doesn't really matter but you're asking you know am I just adding up",
    "start": "2057310",
    "end": "2064210"
  },
  {
    "text": "numbers or am I you know trying to factor large problems and you multiply these things together and you this needs",
    "start": "2064210",
    "end": "2070960"
  },
  {
    "text": "to be larger than some threshold so if you have a trivial little operation like adding up numbers you generally need on",
    "start": "2070960",
    "end": "2076929"
  },
  {
    "text": "the order of 10,000 elements in order to get a theta at a parallelism with you know with this framework which you know",
    "start": "2076930",
    "end": "2083169"
  },
  {
    "text": "isn't unrealistic we all have datasets that are much much larger than that so but a hundred numbers not going to",
    "start": "2083170",
    "end": "2088330"
  },
  {
    "text": "paralyze thousand number is not going to paralyze if your operation is more expensive then you don't need as much",
    "start": "2088330",
    "end": "2093879"
  },
  {
    "text": "data to get over this threshold but if you have a trivial small operation and a trivial amount of data you know just",
    "start": "2093880",
    "end": "2101650"
  },
  {
    "text": "stop sequential sperate just use that alright so let me work through these uh",
    "start": "2101650",
    "end": "2107320"
  },
  {
    "start": "2106000",
    "end": "2106000"
  },
  {
    "text": "you know these items one at a time source quitting some sources splitter than others and the cost of splitting a",
    "start": "2107320",
    "end": "2113890"
  },
  {
    "text": "source is what you have to compute what the split is and you also have to take",
    "start": "2113890",
    "end": "2119290"
  },
  {
    "text": "into account how even is this blip so an ArrayList you know or an array you can find the",
    "start": "2119290",
    "end": "2126250"
  },
  {
    "text": "midpoint really cheaply and you're going to split it into almost people's equal pieces that's great and as a bonus you",
    "start": "2126250",
    "end": "2132550"
  },
  {
    "text": "even know exactly how big the pieces are which helps you avoid copying in some operations so that's great so arrays are",
    "start": "2132550",
    "end": "2141640"
  },
  {
    "text": "great link lists have none of these properties they split terribly you know you you have to you know take a pointer",
    "start": "2141640",
    "end": "2150280"
  },
  {
    "text": "traversal to do the split you you come away and you come away with a terribly even uneven split one and n minus one if",
    "start": "2150280",
    "end": "2158320"
  },
  {
    "text": "your data is not already in a data structure but it's coming from some kind of generator there's an analog here so",
    "start": "2158320",
    "end": "2165400"
  },
  {
    "text": "iterative generators like like calling an iterator to get your next element that works exactly like a linked list",
    "start": "2165400",
    "end": "2171270"
  },
  {
    "text": "right splitting an iterator is just like splitting a linked list on the other hand if you have a stateless generator",
    "start": "2171270",
    "end": "2176860"
  },
  {
    "text": "that splits just like an array does so you know depending on whether your data",
    "start": "2176860",
    "end": "2182410"
  },
  {
    "text": "is coming from an already coming from a data structure or it's coming from a computation you can still at you know looking at the structure of it say is",
    "start": "2182410",
    "end": "2188920"
  },
  {
    "text": "this going to split well or not so you know and as a comparison both of these streams generate the same you're the",
    "start": "2188920",
    "end": "2196570"
  },
  {
    "text": "same sequence right in stream got iterate start with zero each time add add one to it and then and then stop",
    "start": "2196570",
    "end": "2204010"
  },
  {
    "text": "after a certain number that generates the you know the stream zero one two three four five so didn't scream range",
    "start": "2204010",
    "end": "2209740"
  },
  {
    "text": "but the first one paralyzes terribly and the second one paralyzes perfectly right",
    "start": "2209740",
    "end": "2216520"
  },
  {
    "text": "because the first one is a sequential sequential iterator and the second one is a stateless generator okay so it's",
    "start": "2216520",
    "end": "2223630"
  },
  {
    "text": "real you know they look the same you look at that and you say well they compute the same sequence but they paralyze very differently okay so like I",
    "start": "2223630",
    "end": "2231760"
  },
  {
    "start": "2230000",
    "end": "2230000"
  },
  {
    "text": "said before the elephant of the rooms locality parallelism wins only when we can keep the CPUs busy doing useful work",
    "start": "2231760",
    "end": "2238619"
  },
  {
    "text": "waiting for cache misses is not useful work and so if you if you have your data",
    "start": "2238619",
    "end": "2245200"
  },
  {
    "text": "an array that in an array that's great because everybody starts working on their little chunk of the array but when",
    "start": "2245200",
    "end": "2250869"
  },
  {
    "text": "you go pull in the first element it's going to pull like the first 15 elements into your cache line so fetching the",
    "start": "2250869",
    "end": "2257260"
  },
  {
    "text": "second third fourth fifth six element that's basically free and as you sequentially walk through an array the",
    "start": "2257260",
    "end": "2263770"
  },
  {
    "text": "hardware prefetcher kicks in and says you're probably going to need the next chunk of the array let me go start fetching that so you're not going to have to wait",
    "start": "2263770",
    "end": "2269960"
  },
  {
    "text": "for it you know for as long on the other hand you know if your data is scattered",
    "start": "2269960",
    "end": "2277190"
  },
  {
    "text": "all over memory you're going to be taking cache misses left and right and you're going to be spending a lot more time waiting for data and a lot less",
    "start": "2277190",
    "end": "2283400"
  },
  {
    "text": "time doing computation and you know the comparison is I have an array of int and",
    "start": "2283400",
    "end": "2288410"
  },
  {
    "text": "I turn that into a stream and I want to add them up that's great because I get great locality you know this one this",
    "start": "2288410",
    "end": "2294770"
  },
  {
    "text": "one this one this one I'm done whereas if I have a stream of boxed integers in",
    "start": "2294770",
    "end": "2300020"
  },
  {
    "text": "order to I get the number that I want to add up I have to take a pointer dereference to like go find the payload",
    "start": "2300020",
    "end": "2306020"
  },
  {
    "text": "of that that integer box and that's not likely to be in cache so I'm going to",
    "start": "2306020",
    "end": "2311300"
  },
  {
    "text": "have to wait for that right so the performance again these computes the same results but but the performance of",
    "start": "2311300",
    "end": "2317390"
  },
  {
    "text": "the one that works on primitives is better and the parallelism is going to be better as well because you're going",
    "start": "2317390",
    "end": "2322850"
  },
  {
    "text": "to be able to exploit look the locality that the hardware gives you and to illustrate this this is how these things",
    "start": "2322850",
    "end": "2328790"
  },
  {
    "text": "are laid out in memory if I have an array of int they're all sequentially in memory one after the other so you know",
    "start": "2328790",
    "end": "2335090"
  },
  {
    "text": "the cache is working for me the prefecture is working for me if I have an array of boxed integers every time I",
    "start": "2335090",
    "end": "2340130"
  },
  {
    "text": "go to traverse one of those pointers I risk taking a cache miss all right so locality is important the trouble with",
    "start": "2340130",
    "end": "2345410"
  },
  {
    "text": "locality is it doesn't really appear in your code right we don't have a locality key word in our language we just have to",
    "start": "2345410",
    "end": "2352520"
  },
  {
    "text": "have some knowledge about how our data structures laid out and how is that going to affect our performance and if",
    "start": "2352520",
    "end": "2358130"
  },
  {
    "text": "we care about performance all right a few other considerations some streams",
    "start": "2358130",
    "end": "2364490"
  },
  {
    "start": "2360000",
    "end": "2360000"
  },
  {
    "text": "have what's called an encounter order that means the the order in which the elements appear has some significance",
    "start": "2364490",
    "end": "2371150"
  },
  {
    "text": "arrays have encountered orders there's a first element of an array there the second element of array where assets",
    "start": "2371150",
    "end": "2377660"
  },
  {
    "text": "don't have a meaningful encounter order they just have elements which means the iterator is perfectly free to serve up",
    "start": "2377660",
    "end": "2383810"
  },
  {
    "text": "the elements in whatever order it finds convenient now some operations their",
    "start": "2383810",
    "end": "2391100"
  },
  {
    "text": "semantics are tied to encounter order operations like limit and skip and find",
    "start": "2391100",
    "end": "2396560"
  },
  {
    "text": "first fine first means find me the first element in the encounter order that means you have to",
    "start": "2396560",
    "end": "2402440"
  },
  {
    "text": "you know you can't just pick any element you have to pick the first one now sometimes in our in our problems the",
    "start": "2402440",
    "end": "2408710"
  },
  {
    "text": "encounter order actually has a meaning sometimes it doesn't sometimes you just care about any element that satisfies a",
    "start": "2408710",
    "end": "2415970"
  },
  {
    "text": "certain property so we can't automatically guess what the meaning is",
    "start": "2415970",
    "end": "2421819"
  },
  {
    "text": "right you know to you but you probably know as the programmer you have an idea what problem you're supposed to be",
    "start": "2421819",
    "end": "2426859"
  },
  {
    "text": "solving so if you've got a stream pipeline and you're using an operation like limit or skip or fine first think",
    "start": "2426859",
    "end": "2434210"
  },
  {
    "text": "about do I care that I'm limiting it to the first n elements or - I just want to",
    "start": "2434210",
    "end": "2439640"
  },
  {
    "text": "limit it to n elements and I don't care which and if you don't care which which is often the case then you can say",
    "start": "2439640",
    "end": "2445819"
  },
  {
    "text": "there's a stream operation called unordered which means I don't care if you thought this stream had a defined",
    "start": "2445819",
    "end": "2451520"
  },
  {
    "text": "encounter order it doesn't just ignore it and that will that will enable optimizations where limit and skips and",
    "start": "2451520",
    "end": "2458930"
  },
  {
    "text": "fine first will rather than picking the first end will pick any end so if you're",
    "start": "2458930",
    "end": "2466520"
  },
  {
    "text": "using operations that whose semantics are tied to encounter order that limits parallelism right when you say compute",
    "start": "2466520",
    "end": "2472369"
  },
  {
    "text": "the first n elements of this sequence you know I can't bring as much parallelism as to bear to there as if I",
    "start": "2472369",
    "end": "2479210"
  },
  {
    "text": "say compute any n elements just you know start processing on a whole bunch of cores and as soon as you have n we're",
    "start": "2479210",
    "end": "2486020"
  },
  {
    "text": "good we're done I can get a lot more parallelism out of that so beware of operations that are intrinsically tied",
    "start": "2486020",
    "end": "2491869"
  },
  {
    "text": "to encounter order ask yourself if it matters if it doesn't matter tell the system you don't care about that ok we",
    "start": "2491869",
    "end": "2500210"
  },
  {
    "text": "talked about splitting we talked about locality we talked about encounter order let's talk about a result combination in",
    "start": "2500210",
    "end": "2506030"
  },
  {
    "text": "some operations like if I'm trying to find the biggest number in a set or trying to add up the elements of a set",
    "start": "2506030",
    "end": "2511849"
  },
  {
    "text": "the merge operation is super cheap adding two numbers computing the Makah two numbers for other operations and I'm",
    "start": "2511849",
    "end": "2518690"
  },
  {
    "text": "doing a group by operation and I'm creating a hashmap merging two hash maps is really expensive it involves a lot of",
    "start": "2518690",
    "end": "2526490"
  },
  {
    "text": "copying it involves sequential iteration through the key set and not only once but you're probably going to do this at",
    "start": "2526490",
    "end": "2532730"
  },
  {
    "text": "every level up the tree right you know so if you split two say four levels you'll do merging of the hash set at the bottom level and then",
    "start": "2532730",
    "end": "2539110"
  },
  {
    "text": "the next level up and the next level up and then at the top level so that's a lot of work so you know you have to look",
    "start": "2539110",
    "end": "2545350"
  },
  {
    "text": "at this and say is this an expensive operation is this going to be an impediment to parallelism you know and",
    "start": "2545350",
    "end": "2551700"
  },
  {
    "text": "you know if you do an operation like collecting - the hash set and you try to",
    "start": "2551700",
    "end": "2557140"
  },
  {
    "text": "do that in parallel you'll often see a significant slowdown 3 X 5 X 10 X because all the work you're doing is",
    "start": "2557140",
    "end": "2565030"
  },
  {
    "text": "just combining you know combining sets now there's an alternative to that you",
    "start": "2565030",
    "end": "2571480"
  },
  {
    "text": "can either go sequential or you could use a concurrent data structure all of the you know the the operations that",
    "start": "2571480",
    "end": "2578080"
  },
  {
    "text": "produce sets and maps like group buy and such they have a concurrent you know analog where you can say put the results",
    "start": "2578080",
    "end": "2584890"
  },
  {
    "text": "in a concurrent hash map and just allow all the threads to you know fire elements into the into the map this is a",
    "start": "2584890",
    "end": "2592030"
  },
  {
    "text": "trade-off because you lose or during it's an intrinsically unordered operation but you also ditch the bottleneck of merging your result set so",
    "start": "2592030",
    "end": "2599680"
  },
  {
    "text": "you have to ask yourself you know what's my merge operation how expensive it is it and am I paying for more than I need",
    "start": "2599680",
    "end": "2606850"
  },
  {
    "text": "am I paying for a an order preserving merge when I don't need one right so",
    "start": "2606850",
    "end": "2611890"
  },
  {
    "text": "there's often a lot of ways to tweak your problem and say well I can get rid of some of these costs okay so you know",
    "start": "2611890",
    "end": "2621030"
  },
  {
    "start": "2619000",
    "end": "2619000"
  },
  {
    "text": "just just to illustrate if I have you know if I'm trying to you know if I'm",
    "start": "2621030",
    "end": "2626890"
  },
  {
    "text": "trying to collect the elements 1 through 8 I can create a little set at the bottom but then I have to merge them",
    "start": "2626890",
    "end": "2631960"
  },
  {
    "text": "into intermediate sets and then I have to merge them again at the end right and that's that's expensive ok so I rattled",
    "start": "2631960",
    "end": "2642460"
  },
  {
    "text": "off a list of factors a splitting cost merging cost locality sensitivity to",
    "start": "2642460",
    "end": "2648400"
  },
  {
    "text": "order each of these could undermine your speed-up and if any one of these is",
    "start": "2648400",
    "end": "2657160"
  },
  {
    "text": "really bad unless you're have you know really expensive operations you're doing on each element it's likely to be a",
    "start": "2657160",
    "end": "2663610"
  },
  {
    "text": "significant impediment to parallelism so you know the good news here is you can",
    "start": "2663610",
    "end": "2669040"
  },
  {
    "text": "look at a pipeline and say how's that going to paralyze and you can very often immediately look at it and say yeah it's",
    "start": "2669040",
    "end": "2675640"
  },
  {
    "text": "not going to paralyze well because of this which is great because you just saved yourself a whole bunch of",
    "start": "2675640",
    "end": "2680830"
  },
  {
    "text": "measurement time and you know experiment time you look the problem and you say it's just not going to paralyze well so",
    "start": "2680830",
    "end": "2688180"
  },
  {
    "text": "this should always be our you know our our second step our first step run it sequentially if it's fast enough you're done go solve a different problem",
    "start": "2688180",
    "end": "2694420"
  },
  {
    "text": "second step if you think it needs to be faster look at it and ask if parallelism is to help you and usually you can",
    "start": "2694420",
    "end": "2700420"
  },
  {
    "text": "figure it out pretty quickly by applying these criteria only then do you want to",
    "start": "2700420",
    "end": "2705610"
  },
  {
    "text": "move on to well let's try it in a parallel measure and see how it works so",
    "start": "2705610",
    "end": "2710730"
  },
  {
    "text": "summing up streams are cool yay parallelism is cool yay but parallelism",
    "start": "2710730",
    "end": "2716530"
  },
  {
    "text": "isn't magic performance test parallelism is hard it's easy to ask for but it's",
    "start": "2716530",
    "end": "2721930"
  },
  {
    "text": "hard to reason about we have to you know it so it's only an optimization we should only apply it if it actually",
    "start": "2721930",
    "end": "2727510"
  },
  {
    "text": "delivers an optimization and it may require analysis and measurement on in",
    "start": "2727510",
    "end": "2732580"
  },
  {
    "text": "order to figure that out now I'll give a little commercial about performance you know performance tuning but I always",
    "start": "2732580",
    "end": "2738250"
  },
  {
    "text": "give which is if you're going to do performance tuning first make sure that",
    "start": "2738250",
    "end": "2744040"
  },
  {
    "text": "your code isn't already fast enough what it's fast enough mean look at your business requirements if you don't have",
    "start": "2744040",
    "end": "2750790"
  },
  {
    "text": "business requirements performance your code is fast enough because if you",
    "start": "2750790",
    "end": "2756070"
  },
  {
    "text": "haven't invested in defining performance requirements and you haven't invested in defining performance metrics which isn't",
    "start": "2756070",
    "end": "2762310"
  },
  {
    "text": "easy and isn't cheap then your code is fast enough done stop optimizing so these techniques",
    "start": "2762310",
    "end": "2770890"
  },
  {
    "text": "are useful but there you know but apply them when you already have a good reason to believe you need to speed up your",
    "start": "2770890",
    "end": "2777070"
  },
  {
    "text": "code and when you think when you know the analysis tells you that parallelism might actually speed up your code and",
    "start": "2777070",
    "end": "2783460"
  },
  {
    "text": "when your measurement tells you it actually does alright so I think we have",
    "start": "2783460",
    "end": "2789190"
  },
  {
    "text": "a few few minutes left oops I think we have a few minutes left to take questions so what did we do the thing",
    "start": "2789190",
    "end": "2797560"
  },
  {
    "start": "2797000",
    "end": "2797000"
  },
  {
    "text": "with the microphone here questions anybody don't fire them all",
    "start": "2797560",
    "end": "2805029"
  },
  {
    "text": "off in parallel hi when you talked about",
    "start": "2805029",
    "end": "2810779"
  },
  {
    "text": "encounter order I I thought that was I thought that was interesting because",
    "start": "2810779",
    "end": "2816970"
  },
  {
    "text": "I've never thought of it that way I think probably badly in terms of",
    "start": "2816970",
    "end": "2823829"
  },
  {
    "text": "associativity and commutativity which I probably mix up quite often but I ask myself does ordering affect the result",
    "start": "2823829",
    "end": "2831400"
  },
  {
    "text": "of the computation and then not as consequence but why why did you talk about encounter order what why did I",
    "start": "2831400",
    "end": "2839049"
  },
  {
    "text": "talk to those other concepts as they're subtle difference or so I mean so I",
    "start": "2839049",
    "end": "2848440"
  },
  {
    "text": "think the question is correct correct me if I'm wrong so I would think parallelism is mostly about is my",
    "start": "2848440",
    "end": "2854289"
  },
  {
    "text": "combining operation associated and all of that but you talked about this weird thing encounter order that I never heard about why that your question yeah okay",
    "start": "2854289",
    "end": "2862029"
  },
  {
    "text": "so the answer is most people have heard about the notion of in order to",
    "start": "2862029",
    "end": "2867309"
  },
  {
    "text": "decompose a problem in parallel you need to have some efficient way of combining the operation most people haven't",
    "start": "2867309",
    "end": "2873520"
  },
  {
    "text": "thought about is there something intrinsic to the operations I'm doing in my program that",
    "start": "2873520",
    "end": "2878650"
  },
  {
    "text": "are going to that are going to undermine parallelism and you know parallelism works best when you can divide the",
    "start": "2878650",
    "end": "2885609"
  },
  {
    "text": "problem up into chunks that everybody is operating independently on encounter order undermines that independence",
    "start": "2885609",
    "end": "2891150"
  },
  {
    "text": "criteria right if some chunk is somehow semantically different to a different",
    "start": "2891150",
    "end": "2896410"
  },
  {
    "text": "you know another chunk because it comes earlier in the decomposition then that means they're not really independent and",
    "start": "2896410",
    "end": "2903010"
  },
  {
    "text": "that limits how much parallelism you could get and it's some of the worst examples that we've seen where people",
    "start": "2903010",
    "end": "2908170"
  },
  {
    "text": "you know say I tried to run this in you know parallel and I got a terrible result was when you combine something",
    "start": "2908170",
    "end": "2915970"
  },
  {
    "text": "like limit and fine first right so limited fine first are both encounter",
    "start": "2915970",
    "end": "2921369"
  },
  {
    "text": "sent encounter order sensitive operations and they actually interact really terribly but they work great",
    "start": "2921369",
    "end": "2927670"
  },
  {
    "text": "sequentially so we have all of the all of these years of you know sequential training",
    "start": "2927670",
    "end": "2933200"
  },
  {
    "text": "think all right well limiting really chief operation finding the first element really chief operation must",
    "start": "2933200",
    "end": "2938839"
  },
  {
    "text": "paralyze great actually paralyzes terribly right and so this is counterintuitive and surprising and so",
    "start": "2938839",
    "end": "2944720"
  },
  {
    "text": "I'm kind of here to talk about the things that are counter to intuitive and surprising because the other things are kind of obvious so we have time for one",
    "start": "2944720",
    "end": "2952220"
  },
  {
    "text": "more question maybe maybe not oh yes we have time for one more question question in the back",
    "start": "2952220",
    "end": "2957770"
  },
  {
    "text": "sir I was just wondering if you could spend a couple of minutes talking about the jvm warm-ups get best performance and streams and parallel streams yeah so",
    "start": "2957770",
    "end": "2964490"
  },
  {
    "text": "the question is how does JVM warm-up play into this and you know warm-up isn't a single thing warm-up is a lot of",
    "start": "2964490",
    "end": "2969740"
  },
  {
    "text": "things so warm-up involves clasp loading it involves compilation it involves",
    "start": "2969740",
    "end": "2975619"
  },
  {
    "text": "initializing JVM data structures and para and using parallelism also has an",
    "start": "2975619",
    "end": "2981290"
  },
  {
    "text": "element of warmup what threads is your TAS going to run in when you run a parallel stream well the",
    "start": "2981290",
    "end": "2987140"
  },
  {
    "text": "library spins up a a common fork/join tasks pool and that involves creating",
    "start": "2987140",
    "end": "2993170"
  },
  {
    "text": "some threads so the first time you do a parallel operation it's going to go and look to see is that is that pool started",
    "start": "2993170",
    "end": "3001119"
  },
  {
    "text": "and if not it'll go start it and it will start some threads that's going to be slower than if that pool is already up",
    "start": "3001119",
    "end": "3006640"
  },
  {
    "text": "and running so typically the threads in that pool will stick around for some amount of time you know a minute a few",
    "start": "3006640",
    "end": "3012280"
  },
  {
    "text": "minutes etc and if there's no work to do they'll tear themselves down so that they don't sit up you sit around waiting",
    "start": "3012280",
    "end": "3018369"
  },
  {
    "text": "up memory and so just like all the other aspects of warm-up there's a warming up",
    "start": "3018369",
    "end": "3024369"
  },
  {
    "text": "the you know the parallel subsystem that largely involves that largely involves",
    "start": "3024369",
    "end": "3029650"
  },
  {
    "text": "starting up threads similarly within each task there's a little bit of micro",
    "start": "3029650",
    "end": "3037210"
  },
  {
    "text": "warm-up of like getting your data into the cache right so if you get scheduled on one of these threads whose data is",
    "start": "3037210",
    "end": "3043810"
  },
  {
    "text": "going to be in the cache well the previous tasks hopefully you know you'll",
    "start": "3043810",
    "end": "3049030"
  },
  {
    "text": "you know your data exhibits good locality and you'll be able to benefit from you know you do a little bit of work to warm up the cache and then you",
    "start": "3049030",
    "end": "3055359"
  },
  {
    "text": "can you get good performance out of it but that depends on locality so is there's warm-up at many levels so I",
    "start": "3055359",
    "end": "3061869"
  },
  {
    "text": "think that takes up our time so thank you very much [Applause]",
    "start": "3061869",
    "end": "3070229"
  }
]