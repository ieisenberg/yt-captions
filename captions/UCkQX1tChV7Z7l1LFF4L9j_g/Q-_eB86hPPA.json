[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "hello everyone do you know that many software applications are under using hardware",
    "start": "4640",
    "end": "10000"
  },
  {
    "text": "resources that means that your application many software applications could run potentially faster while consuming",
    "start": "10000",
    "end": "16080"
  },
  {
    "text": "less energy and this is because today computing systems are including many type of hardware",
    "start": "16080",
    "end": "21920"
  },
  {
    "text": "like multi-core cpus gpus fpgas or even custom design chips like google's",
    "start": "21920",
    "end": "27760"
  },
  {
    "text": "that has designed like tensor processing units and you have all of this available for increasing performance and expert",
    "start": "27760",
    "end": "35120"
  },
  {
    "text": "programmers know this and what they do normally is they take existing applications",
    "start": "35120",
    "end": "40640"
  },
  {
    "text": "and they rerun portion of it into low-level languages like cuda opencl or even sequel the new standard",
    "start": "40640",
    "end": "48079"
  },
  {
    "text": "those are the languages that allows you to run on heterogeneous hardware but this is a very tedious process",
    "start": "48079",
    "end": "55760"
  },
  {
    "text": "okay first of all because we have now more than one type of hardware the user needs the developers need to",
    "start": "55760",
    "end": "62160"
  },
  {
    "text": "know which portion of the code correspond better to each hardware and this this is",
    "start": "62160",
    "end": "67600"
  },
  {
    "text": "because there is no a single hardware that better executes all type",
    "start": "67600",
    "end": "72880"
  },
  {
    "text": "of workloads so the programmer has to know which portion are best suitable for each one",
    "start": "72880",
    "end": "78159"
  },
  {
    "text": "then the programmer have to port you have to know architectural detail for example have to mess around the",
    "start": "78159",
    "end": "84880"
  },
  {
    "text": "thread scheduler you know the data partitioning and all of these tricks can help you to increase performance",
    "start": "84880",
    "end": "90960"
  },
  {
    "text": "plus if you want to increase even further performance you have to go deep into the architecture for example gpus",
    "start": "90960",
    "end": "97759"
  },
  {
    "text": "has different level of tier memory and you have to know that one level is l1 cache you can copy data",
    "start": "97759",
    "end": "104479"
  },
  {
    "text": "there if you want to but cash is not coherent so barriers are up to the programmer",
    "start": "104479",
    "end": "110079"
  },
  {
    "text": "and when the new generation of gpus comes along or fpgas or accelerator you",
    "start": "110079",
    "end": "115200"
  },
  {
    "text": "have to repeat this process again not from scratch but you have to change your code",
    "start": "115200",
    "end": "122159"
  },
  {
    "text": "this is a very tedious process i i hope you feel my pain so instead of doing that let's imagine",
    "start": "122159",
    "end": "129440"
  },
  {
    "text": "the following let's imagine that we have a software system that can take automatically a high level",
    "start": "129440",
    "end": "134959"
  },
  {
    "text": "program it doesn't mean only coc plus but especially in this community we know that programmers can",
    "start": "134959",
    "end": "140480"
  },
  {
    "text": "become also from other communities like java r ruby python and automatically executing",
    "start": "140480",
    "end": "146239"
  },
  {
    "text": "on the terror genius hardware wouldn't be that great and because we are in the streaming mode",
    "start": "146239",
    "end": "151360"
  },
  {
    "text": "let's also imagine that we can also perform task migration across devices that would be cool right guess what i",
    "start": "151360",
    "end": "158400"
  },
  {
    "text": "have just defined torrenovi and this is exactly what nuvian does so tornovm",
    "start": "158400",
    "end": "163599"
  },
  {
    "text": "is a plugin to open jdk and gral that allows you to run java r groovy python",
    "start": "163599",
    "end": "171280"
  },
  {
    "text": "scala programs on heterogeneous hardware without changing any code of line any line of code",
    "start": "171280",
    "end": "178560"
  },
  {
    "text": "even more with tornado vm we can perform dynamically task migration across devices without restarting application and",
    "start": "178560",
    "end": "184879"
  },
  {
    "text": "without any knowledge from the promised perspective about the actual hardware so",
    "start": "184879",
    "end": "190800"
  },
  {
    "text": "in my talk i will explain to you the tornado vm but before that i will explain some background so don't worry",
    "start": "190800",
    "end": "195920"
  },
  {
    "text": "if you haven't heard about gpus that much or the fpga is something new don't worry",
    "start": "195920",
    "end": "201120"
  },
  {
    "text": "at all i will explain to you the basics and i will explain to you how we use it from the ternary perspective",
    "start": "201120",
    "end": "206799"
  },
  {
    "text": "so you're asking the right talk later i will introduce to you how you can use tornado how you can execute it",
    "start": "206799",
    "end": "213200"
  },
  {
    "text": "and then i will show you some internal details so i'm a compiler engineer i would like to know everything inside and i show i would",
    "start": "213200",
    "end": "220159"
  },
  {
    "text": "like to show you this passion with you as well so i will show you how we can compile code at runtime so",
    "start": "220159",
    "end": "225920"
  },
  {
    "text": "basically internal details of the jit compilation and um i will show you also how can we",
    "start": "225920",
    "end": "231440"
  },
  {
    "text": "migrate executional runtime and stay with me i will also show you some demos so hopefully i can convince",
    "start": "231440",
    "end": "237200"
  },
  {
    "text": "you that this type of technology is useful for in general management time languages",
    "start": "237200",
    "end": "242480"
  },
  {
    "text": "before that just a quick workout a quick words about myself so this is my first time in this conference i'm juan fumero and a postdoc now at the",
    "start": "242480",
    "end": "249280"
  },
  {
    "text": "university of manchester and i'm currently the lead developer of tornado vm project",
    "start": "249280",
    "end": "255760"
  },
  {
    "text": "okay so let's get started question why should we care about",
    "start": "255760",
    "end": "261359"
  },
  {
    "text": "heterogeneous devices anyways is something important to motivate the answer i'll show you here",
    "start": "261359",
    "end": "267280"
  },
  {
    "text": "three different micro architectures an intel mic architecture on the left hand side a gpu",
    "start": "267280",
    "end": "273199"
  },
  {
    "text": "and an fpga and let's focus on the intel one on the left hand side this one is ice lake micro architecture",
    "start": "273199",
    "end": "279919"
  },
  {
    "text": "is one of the latest by intel and this one has a physical course plus",
    "start": "279919",
    "end": "285040"
  },
  {
    "text": "avx instructions plus it's interesting one it has a gpu that is inside it's called integrated gpu",
    "start": "285040",
    "end": "292639"
  },
  {
    "text": "if you run on this one and you use all of this available which is quite difficult you can get up to one teraflop",
    "start": "292639",
    "end": "299199"
  },
  {
    "text": "of performance now let's look at the gpu this one is pascal micro architecture it's already",
    "start": "299199",
    "end": "304960"
  },
  {
    "text": "two generations all from nvidia this one is 16 nanometers technology and this one",
    "start": "304960",
    "end": "310320"
  },
  {
    "text": "look at this instead of a physical cores we have 3500 physical cores that you can use",
    "start": "310320",
    "end": "318880"
  },
  {
    "text": "this gives you up to 10 teraflop of performance okay it's much much higher than a single",
    "start": "318880",
    "end": "323919"
  },
  {
    "text": "cpu and similar situation applies for fpgas this one by intel you can get up to 10",
    "start": "323919",
    "end": "329280"
  },
  {
    "text": "teraflops of performance so during my talk i will be talking about a lot about gpus and fpga so just",
    "start": "329280",
    "end": "335600"
  },
  {
    "text": "in case let's set up the terminology i guess many of you know this already",
    "start": "335600",
    "end": "341039"
  },
  {
    "start": "339000",
    "end": "339000"
  },
  {
    "text": "but gpus stands for graphics processing unit it's basically",
    "start": "341039",
    "end": "346080"
  },
  {
    "text": "used okay at the beginning it was mainly used for rendering and computer graphics",
    "start": "346080",
    "end": "351120"
  },
  {
    "text": "however a few years ago researchers realized that some of the stages to come to do the",
    "start": "351120",
    "end": "357360"
  },
  {
    "text": "rendering okay the gpu implement some stages like computing textures volumes vertices and",
    "start": "357360",
    "end": "362880"
  },
  {
    "text": "so on some of the stages can be used for general purpose computation and that's where cuda and opencl come",
    "start": "362880",
    "end": "369520"
  },
  {
    "text": "from so where we can use the gpu not only for computing graphics but also for general purpose computation",
    "start": "369520",
    "end": "376479"
  },
  {
    "text": "physics machine learning deep learning bitcoin this kind of stuff",
    "start": "376479",
    "end": "381520"
  },
  {
    "text": "um this one i highlighted before is um my pascal mic architecture i want to",
    "start": "381520",
    "end": "387120"
  },
  {
    "text": "highlight two things from here apart from the programming model which you have to learn it if you want to use",
    "start": "387120",
    "end": "392639"
  },
  {
    "text": "it could be opencl cuza or any other you have to know architecture details in order to use",
    "start": "392639",
    "end": "399280"
  },
  {
    "text": "them efficiently and that for many users could be you know handicapped you don't have to have in my opinion you",
    "start": "399280",
    "end": "406400"
  },
  {
    "text": "don't have to be an expert to use it could be a biologist a psychologist why not",
    "start": "406400",
    "end": "411520"
  },
  {
    "text": "they actually have the need to run on those devices so perhaps we need high level",
    "start": "411520",
    "end": "416639"
  },
  {
    "text": "abstractions let's define quickly what is an fpga",
    "start": "416639",
    "end": "421919"
  },
  {
    "start": "420000",
    "end": "420000"
  },
  {
    "text": "are you familiar with fpgas how many of you have heard about fpgas okay that's much more than i expect fine",
    "start": "421919",
    "end": "429199"
  },
  {
    "text": "if you don't know don't worry if you use tornado you don't have to know but i'm going to explain to you anyways",
    "start": "429199",
    "end": "435039"
  },
  {
    "text": "so an fpga stands for field programmable gate array and basically it's a piece of hardware",
    "start": "435039",
    "end": "440319"
  },
  {
    "text": "that is empty empty after manufacturing so it's up to the programmer what to run in there",
    "start": "440319",
    "end": "446240"
  },
  {
    "text": "in some sense it's like physically wiring your applications into hardware to do so the fpga provides with",
    "start": "446240",
    "end": "454319"
  },
  {
    "text": "logic slides like lookup tables flip flops program programmable memory and dsps dsp",
    "start": "454319",
    "end": "459840"
  },
  {
    "text": "specific functions to perform math operations and that could give you a lot of performance",
    "start": "459840",
    "end": "465759"
  },
  {
    "text": "and a lot of energy saving because you go you just run what you need basically however the",
    "start": "465759",
    "end": "472720"
  },
  {
    "text": "probability here is it's a big issue normally you're programming bhdl you know very low this kind of stuff",
    "start": "472720",
    "end": "479199"
  },
  {
    "text": "more recently main vendors provide you can program using opencl and i tell",
    "start": "479199",
    "end": "486160"
  },
  {
    "text": "you this because tornado targets fpgas and tornado targets fg at the method level which means",
    "start": "486160",
    "end": "492000"
  },
  {
    "text": "that we can physically wire the java methods into hardware",
    "start": "492000",
    "end": "497039"
  },
  {
    "text": "how cool is that okay so far i have been talking about few",
    "start": "497039",
    "end": "503360"
  },
  {
    "text": "hardware but we need a way to program them right",
    "start": "503360",
    "end": "508560"
  },
  {
    "text": "so if you want to use gpus and fpgas you might target something like code up",
    "start": "508560",
    "end": "514399"
  },
  {
    "text": "and cl c class plus something like that right but we know that that a lot of",
    "start": "514399",
    "end": "520399"
  },
  {
    "text": "developers if you want to use it from java for example for python for ruby you have to plug in right now an external library",
    "start": "520399",
    "end": "527279"
  },
  {
    "text": "but there is no such a virtual machine there is not such a thing that automatically you can get target a",
    "start": "527279",
    "end": "533760"
  },
  {
    "text": "java or python program and run it directly without any knowledge on the heterogeneous hardware and that's what we propose we that's",
    "start": "533760",
    "end": "541120"
  },
  {
    "text": "what we call a heterogeneous virtual machine so basically it's a synonym with that you can target java but also",
    "start": "541120",
    "end": "548399"
  },
  {
    "text": "other languages this is new we released a new version last week uh actually at the beginning",
    "start": "548399",
    "end": "554160"
  },
  {
    "text": "we only run java now we can run more than java i will show you a demo actually later on with this strategy you can run on any",
    "start": "554160",
    "end": "561360"
  },
  {
    "text": "type of hardware cool let me show you how and",
    "start": "561360",
    "end": "567279"
  },
  {
    "text": "let's start with a demo i want to show you the suite first and then i will show you the details so",
    "start": "567279",
    "end": "573680"
  },
  {
    "text": "this one is called kinect fusion you know the kinect the microsoft camera right so this one is a kinect this",
    "start": "573680",
    "end": "581040"
  },
  {
    "text": "recording area a room and the goal of application is to render in real time the whole application",
    "start": "581040",
    "end": "586640"
  },
  {
    "text": "what do we mean by real time so the human perceives real time around 30 frames 30 images per second that's",
    "start": "586640",
    "end": "592720"
  },
  {
    "text": "the quality of service the whole application is written in java and it's around seven thousand line of",
    "start": "592720",
    "end": "599600"
  },
  {
    "text": "java codes okay and let me show you by the way it's open source it's in this link and let me show you the application i",
    "start": "599600",
    "end": "607680"
  },
  {
    "text": "record a video just in case but okay let me explain first of all i'm going to run in pure java",
    "start": "607680",
    "end": "613760"
  },
  {
    "text": "okay there is no oscillation underneath it's just open jdk open jdk a actually on the left hand side you're",
    "start": "613760",
    "end": "620079"
  },
  {
    "text": "gonna see the inputs on the right hand side you're gonna see the output and there's the input and different",
    "start": "620079",
    "end": "625279"
  },
  {
    "text": "setup for the kinect fusion like death scene light scene the application trust me is already running it's around 1.5",
    "start": "625279",
    "end": "632079"
  },
  {
    "text": "frames per second it's extremely slow yeah so what i'm gonna do now i'm gonna",
    "start": "632079",
    "end": "638320"
  },
  {
    "text": "stop the application i'm gonna reset it and i'm gonna use tornado and tornado has different you",
    "start": "638320",
    "end": "644000"
  },
  {
    "text": "know you can run on different devices i'm going to set first to run on the multi-core when we choose",
    "start": "644000",
    "end": "650079"
  },
  {
    "text": "to run tornado on an intel or an ndcpu we run a multi-core configuration and",
    "start": "650079",
    "end": "656160"
  },
  {
    "text": "hopefully we can see that something is faster well it hard to see but actually it's around four frames per",
    "start": "656160",
    "end": "662480"
  },
  {
    "text": "second that was recorded on my laptop it's four core machine it's not that bad right cool so i'm",
    "start": "662480",
    "end": "668800"
  },
  {
    "text": "gonna do now i'm gonna go do switching um i'm gonna stop i'm gonna reset and",
    "start": "668800",
    "end": "673920"
  },
  {
    "text": "i'm gonna use now the nvidia gpu that is available on my laptop the 1050 and voila in a few seconds you get the",
    "start": "673920",
    "end": "681760"
  },
  {
    "text": "whole rendering okay of the whole room just it's just java okay we just offload the java code onto",
    "start": "681760",
    "end": "688160"
  },
  {
    "text": "the gpu right let me continue with the presentation",
    "start": "688160",
    "end": "694720"
  },
  {
    "text": "okay so now i'm gonna show you how this is done okay i'm gonna give you an overview of tornado vm",
    "start": "695839",
    "end": "703839"
  },
  {
    "start": "696000",
    "end": "696000"
  },
  {
    "text": "a lay architecture plus a micro cadent architecture we're talking about the software side on the top level we have an api you",
    "start": "705279",
    "end": "712160"
  },
  {
    "text": "might wonder why do we have an api well we what we do is we exploit",
    "start": "712160",
    "end": "717440"
  },
  {
    "text": "parallelism we don't detect parallelism detection is a very hard problem so we need a weight at",
    "start": "717440",
    "end": "723680"
  },
  {
    "text": "least to identify which core regions you want to offload this is done through what we call a task",
    "start": "723680",
    "end": "730320"
  },
  {
    "text": "based model each task corresponds to an existing java method okay and we can combine many tasks on a",
    "start": "730320",
    "end": "736480"
  },
  {
    "text": "single compilation unit and that's what we call a task schedule right and then we actually expose two",
    "start": "736480",
    "end": "742800"
  },
  {
    "text": "annotations at parallel and add reduce to just identify which loops you want to parallelize",
    "start": "742800",
    "end": "748880"
  },
  {
    "text": "however parallelization is what we call relaxed parallel semantics so that means that even if the user annotates the code",
    "start": "748880",
    "end": "756320"
  },
  {
    "text": "tornado will double check that that code can be parallelized otherwise it just bailouts and execute the sequential code",
    "start": "756320",
    "end": "763440"
  },
  {
    "text": "so we don't force parallelism uh then we have a runtime system that we first of all will optimize how",
    "start": "763440",
    "end": "771200"
  },
  {
    "text": "data is flowing across the tasks and this is because i mean many of you",
    "start": "771200",
    "end": "776560"
  },
  {
    "text": "know gpus and fgs already so you know that normally gpus and fpgas don't share memory",
    "start": "776560",
    "end": "782800"
  },
  {
    "text": "with the main cpu so we need to allocate first data over there and then do the data",
    "start": "782800",
    "end": "789279"
  },
  {
    "text": "transfer that takes time exactly going through pci express if we can",
    "start": "789279",
    "end": "794480"
  },
  {
    "text": "save data transfers we can get speed as overall and that's the goal of the data flow optimizer",
    "start": "794480",
    "end": "800399"
  },
  {
    "text": "once we have the data flow optimized we generate new byte codes and those are by code are executing on",
    "start": "800399",
    "end": "806320"
  },
  {
    "text": "top of java light codes okay don't worry this is very abstract i know this is just to give an overview i will focus later on step by step",
    "start": "806320",
    "end": "813600"
  },
  {
    "text": "then we because we have our own bytecode interpret sorry our own bytecodes we need a bytecode interpreted to run those",
    "start": "813600",
    "end": "819920"
  },
  {
    "text": "that's very simple actually how to orchestrate execution that's very simple process one of those bytecodes is launch learn",
    "start": "819920",
    "end": "827120"
  },
  {
    "text": "this method on this device the first time we launch we call the g-compiler say",
    "start": "827120",
    "end": "832480"
  },
  {
    "text": "now compile this method for that we extend gral the graduate compiler only to produce to",
    "start": "832480",
    "end": "839199"
  },
  {
    "text": "generate opencl okay with this strategy we can currently",
    "start": "839199",
    "end": "845440"
  },
  {
    "text": "target nvidia gpus amd gpus intel integrated graphics",
    "start": "845440",
    "end": "851839"
  },
  {
    "text": "fpgas by intel and silence we can run on top of opengdk8 and",
    "start": "851839",
    "end": "858639"
  },
  {
    "text": "gravel vn 19.3 okay okay so let's go deep i'm going to start",
    "start": "858639",
    "end": "865600"
  },
  {
    "start": "864000",
    "end": "864000"
  },
  {
    "text": "with api i'm going to start with an example i'll show you here a typical you know very easy matrix multiplication",
    "start": "865600",
    "end": "872639"
  },
  {
    "text": "a java class called compute and one method called mxm and then i show you here the sequential",
    "start": "872639",
    "end": "879519"
  },
  {
    "text": "code sequential code to run matrix multiplication and the first thing we do with tornado",
    "start": "879519",
    "end": "885680"
  },
  {
    "text": "is to annotate the sequential code with the annotation i'll show you before the add parallel",
    "start": "885680",
    "end": "891199"
  },
  {
    "text": "with this we are telling the user tells tornado these two loops might be parallel the fact that the user",
    "start": "891199",
    "end": "898160"
  },
  {
    "text": "as i said before annotates the code doesn't mean it's going to be paralyzed later on but just as this is just as a hint for",
    "start": "898160",
    "end": "904320"
  },
  {
    "text": "the compiler to where to go right that's the first thing we do the second thing is we build the task the schedule",
    "start": "904320",
    "end": "911120"
  },
  {
    "text": "i showed before for doing that we have an object called the schedule you pass a name could be any random name",
    "start": "911120",
    "end": "916160"
  },
  {
    "text": "full bar whatever we put that around time to change device for example i will show you an example",
    "start": "916160",
    "end": "922399"
  },
  {
    "text": "and then we call task.task.task each task is a reference to an existing java",
    "start": "922399",
    "end": "927519"
  },
  {
    "text": "method here we say class compute method mxm and then the rest is a normal parameter for our invocation call",
    "start": "927519",
    "end": "933920"
  },
  {
    "text": "okay then we have another call called stream out and this is because normally we don't share memory gpu has",
    "start": "933920",
    "end": "940959"
  },
  {
    "text": "some memory right so we need a way to synchronize data again we go we do that through the stream out operation",
    "start": "940959",
    "end": "947600"
  },
  {
    "text": "and then we call execute that's all and by the way we can add as many tasks we can in this case i showed",
    "start": "947600",
    "end": "953360"
  },
  {
    "text": "you just one but you can have you know 20 100 tasks whatever",
    "start": "953360",
    "end": "958800"
  },
  {
    "text": "okay so to run this we just type tornado under class in fact this because we are",
    "start": "958800",
    "end": "964880"
  },
  {
    "text": "lazy people in our team you just type actually internationally as to java plus all the",
    "start": "964880",
    "end": "970720"
  },
  {
    "text": "parameters on the tornado basically but it's just java okay so now",
    "start": "970720",
    "end": "977519"
  },
  {
    "start": "976000",
    "end": "976000"
  },
  {
    "text": "you have you know everything about tornado at the user level and i'm going to show",
    "start": "977519",
    "end": "982880"
  },
  {
    "text": "you a live demo now to run the matrix multiplication so",
    "start": "982880",
    "end": "988639"
  },
  {
    "text": "so let me explain what i'm doing i'm going to run with tornado command but i'm going to run first the sequential",
    "start": "988639",
    "end": "994320"
  },
  {
    "text": "code so i have a flag to indicate don't don't build it as a schedule just run the the code with",
    "start": "994320",
    "end": "1000720"
  },
  {
    "text": "open jdk and i run in this code multiple times when 100 times so don't take this as a",
    "start": "1000720",
    "end": "1006880"
  },
  {
    "text": "benchmark problem it's just to show you a quick demo and let's see the time i'm gonna track the time for each uh each",
    "start": "1006880",
    "end": "1014240"
  },
  {
    "text": "iteration i'm gonna run the multiplication 100 times okay so first of all the sequential this is",
    "start": "1014240",
    "end": "1019759"
  },
  {
    "text": "the size of the matrix and as you can see it's taking around 240 milliseconds per iteration",
    "start": "1019759",
    "end": "1027120"
  },
  {
    "text": "okay i'm not going to wait 100 iterations um okay let me go now i'm going to enable",
    "start": "1027120",
    "end": "1034319"
  },
  {
    "text": "tornado and the default device i will show you later is going to run on the gpu that i have here on my system",
    "start": "1034319",
    "end": "1041120"
  },
  {
    "text": "and if i run tornado boom each iteration takes around four milliseconds you can",
    "start": "1041120",
    "end": "1046640"
  },
  {
    "text": "say ah no no no i don't trust you okay far enough",
    "start": "1046640",
    "end": "1052720"
  },
  {
    "text": "uh you can tell tornado to um to tell uh to give me debug information",
    "start": "1052720",
    "end": "1059919"
  },
  {
    "text": "and that will tell me which device you're running okay so i run the debug info",
    "start": "1059919",
    "end": "1065039"
  },
  {
    "text": "and now he's telling me you're running on nvidia 1050 and videos i have my laptop",
    "start": "1065039",
    "end": "1070080"
  },
  {
    "text": "uh in fact we can change the device uh but before changing the device let me show you we generate opens here underneath for",
    "start": "1070080",
    "end": "1076080"
  },
  {
    "text": "you and you can enable this with print kernel",
    "start": "1076080",
    "end": "1082639"
  },
  {
    "text": "let me pipe this to see this and this is the opencl kernel we",
    "start": "1082880",
    "end": "1089520"
  },
  {
    "text": "generate for you it looks very ugly this is because we generate codes from grail graphics ssa representation",
    "start": "1089520",
    "end": "1095840"
  },
  {
    "text": "language and echo based on ssa but it's legal opencl code okay",
    "start": "1095840",
    "end": "1100880"
  },
  {
    "text": "you don't have to go that deep if you don't want to by the way if you want to tune the code afterwards you can",
    "start": "1100880",
    "end": "1107280"
  },
  {
    "text": "produce the code manually tune it and plug in later on that's actually the process we do when",
    "start": "1107280",
    "end": "1112320"
  },
  {
    "text": "we do the we build the compiler okay let me show you now",
    "start": "1112320",
    "end": "1118640"
  },
  {
    "text": "how we can run on a other device so if i run this command tornado devices the devices will tell me",
    "start": "1118640",
    "end": "1126400"
  },
  {
    "text": "i have four devices available the default one is the nvidia 1050 i have a multi-core i have another",
    "start": "1126400",
    "end": "1132080"
  },
  {
    "text": "multi-core but using different opencl driver and here i have an intel integrated graphics so let's do",
    "start": "1132080",
    "end": "1138320"
  },
  {
    "text": "that let me remove the pipe and i",
    "start": "1138320",
    "end": "1144000"
  },
  {
    "text": "remember that i get names to the task scheduler and that's what they are useful so i can say",
    "start": "1144000",
    "end": "1149039"
  },
  {
    "text": "this task with this name running on device 0 3 which is the device of the",
    "start": "1149039",
    "end": "1155600"
  },
  {
    "text": "interrelated graph and running also with the debug info yeah that's fine and now",
    "start": "1155600",
    "end": "1164000"
  },
  {
    "text": "boom are running on the intel integrated graphics if i have a fpga plugin here that would be cool but",
    "start": "1164000",
    "end": "1169440"
  },
  {
    "text": "for now there is no laptop with fpgas as far as i know i could even run it there okay",
    "start": "1169440",
    "end": "1176799"
  },
  {
    "text": "i hope i convinced you that this type of technology is useful let's go back to the presentation let's talk let's start talking about the",
    "start": "1176799",
    "end": "1182480"
  },
  {
    "text": "details okay so remember this is a very general overview you annotate the code you build",
    "start": "1182480",
    "end": "1188160"
  },
  {
    "text": "a task schedule there is a magic box there and then damage box will generate an",
    "start": "1188160",
    "end": "1193840"
  },
  {
    "text": "opencl code which means that we need the knowledge to compile later on to produ to generate the actual binary for this",
    "start": "1193840",
    "end": "1198960"
  },
  {
    "text": "and without the strategy you can run on any other device in fact we can plug in any other",
    "start": "1198960",
    "end": "1206320"
  },
  {
    "text": "other languages apart from java we can plug in node.js python r ruby",
    "start": "1206320",
    "end": "1213039"
  },
  {
    "text": "javascript scala to do so we go through graphvm okay so still we provide tornado is written",
    "start": "1213039",
    "end": "1220880"
  },
  {
    "text": "using java we have the dot classes here and there is a component in grail called",
    "start": "1220880",
    "end": "1227039"
  },
  {
    "text": "truffle that is it's a framework that allows you to write that other languages on top of graph",
    "start": "1227039",
    "end": "1232720"
  },
  {
    "text": "and this is a component on polyglot that is talking to other languages okay and that guy is gonna",
    "start": "1232720",
    "end": "1238880"
  },
  {
    "text": "tell ah some of the code here some of the classes here if you like are in java so talk to them but these",
    "start": "1238880",
    "end": "1246320"
  },
  {
    "text": "java classes are expressed with tornado so that's why we can go through node.js let's say to the gpu in fact i'm",
    "start": "1246320",
    "end": "1253520"
  },
  {
    "text": "going to show you an example now with node.js and the mandelbrot you know the mandelbrot",
    "start": "1253520",
    "end": "1258799"
  },
  {
    "start": "1257000",
    "end": "1257000"
  },
  {
    "text": "typical okay okay let me show that let me show you that so i'm going",
    "start": "1258799",
    "end": "1263840"
  },
  {
    "text": "to run that i'm going to actually run in that docker that's a command ah well let me show you",
    "start": "1263840",
    "end": "1269200"
  },
  {
    "text": "first the code actually um yes",
    "start": "1269200",
    "end": "1276000"
  },
  {
    "text": "is the class mandelbrot it's just java okay where we have the two annotation that's",
    "start": "1276000",
    "end": "1281840"
  },
  {
    "text": "the code to run java the demandable computation and we have one method called compute",
    "start": "1281840",
    "end": "1287200"
  },
  {
    "text": "that we we will build here the task schedule and then we have a method called sequential that will call just",
    "start": "1287200",
    "end": "1294000"
  },
  {
    "text": "the sequential code then we have a word server.js",
    "start": "1294000",
    "end": "1302400"
  },
  {
    "text": "okay and on the entry point and using the express module on the entry point we'll bring a",
    "start": "1302400",
    "end": "1308240"
  },
  {
    "text": "bunch of messages and the interesting part is here i'm calling a java type this is because i'm",
    "start": "1308240",
    "end": "1313840"
  },
  {
    "text": "using truffle and the polyglot engine i'm calling the java type mandelbrot dot compute which is the one that builds",
    "start": "1313840",
    "end": "1321280"
  },
  {
    "text": "the crop that's scheduled to run on the gpu potentially i'm printing the time and that code will actually generate an",
    "start": "1321280",
    "end": "1328320"
  },
  {
    "text": "image and i'm gonna print the image if i tap if i type slash java i'm going to do the same but i'm going to run the",
    "start": "1328320",
    "end": "1334159"
  },
  {
    "text": "sequential code so let's do that i already start the server and if guy go to that direction",
    "start": "1334159",
    "end": "1343520"
  },
  {
    "text": "boom the the motherboard already there it might not be that impressive to you",
    "start": "1343520",
    "end": "1348559"
  },
  {
    "text": "it takes around 1.3 seconds to compute okay compile and compute this",
    "start": "1348559",
    "end": "1355520"
  },
  {
    "text": "image if i uh refresh the browser now it's going down to 0.1 second",
    "start": "1355520",
    "end": "1362240"
  },
  {
    "text": "because once we get the code we just get it from the code cache now just out of curiosity let's run in",
    "start": "1362240",
    "end": "1369520"
  },
  {
    "text": "with the sequential one any guesses about the time i'm running now with open jdk by the way it's last java run stop in",
    "start": "1369520",
    "end": "1376080"
  },
  {
    "text": "jdk 5 seconds 10 seconds",
    "start": "1376080",
    "end": "1382000"
  },
  {
    "text": "20 i think you're close i think you're close still running still running",
    "start": "1382159",
    "end": "1389840"
  },
  {
    "text": "17 seconds okay and if i i can't refresh it you can get down",
    "start": "1390000",
    "end": "1395120"
  },
  {
    "text": "maybe the g compiler can get seen this is not going down 15 seconds okay",
    "start": "1395120",
    "end": "1400720"
  },
  {
    "text": "compared to 1.4 seconds including the compilation on the gpu and yeah perhaps you can plug in your",
    "start": "1400720",
    "end": "1407039"
  },
  {
    "text": "video game engine now i don't know that could be cool okay let's continue",
    "start": "1407039",
    "end": "1412559"
  },
  {
    "start": "1412000",
    "end": "1412000"
  },
  {
    "text": "so remember that we have this blue box all the magic happening here let's open the blue box",
    "start": "1412559",
    "end": "1418159"
  },
  {
    "text": "and that's what we find so we have our data flow analyzer or runtime and we have a big component here",
    "start": "1418159",
    "end": "1424400"
  },
  {
    "text": "it's not the biggest but okay it takes significant amount of time for us to build",
    "start": "1424400",
    "end": "1429440"
  },
  {
    "text": "is a jit compiler basically we extend grail i'm not going to talk about the details of coral but has different",
    "start": "1429440",
    "end": "1435919"
  },
  {
    "text": "ir presentations high level ir for architecture independent optimizations",
    "start": "1435919",
    "end": "1441039"
  },
  {
    "text": "memory optimizations and architecture dependent optimizations so basically what we do is we have a",
    "start": "1441039",
    "end": "1446080"
  },
  {
    "text": "graph a console flow graph uh and there are many nodes in there for loops",
    "start": "1446080",
    "end": "1451520"
  },
  {
    "text": "you know data dependencies and so on basically what we do is to do no replacement depending on the optimizations we want to do for example",
    "start": "1451520",
    "end": "1458960"
  },
  {
    "text": "you want to target the gpu you have a for loop remove the for loop introduce the get global id this kind of things",
    "start": "1458960",
    "end": "1465600"
  },
  {
    "text": "we actually have kind of between grad and tornado kind of 170 optimizations in",
    "start": "1465600",
    "end": "1471600"
  },
  {
    "text": "that process and at the end of the process we have the openclc code um that means that we need another round",
    "start": "1471600",
    "end": "1478799"
  },
  {
    "text": "another compiler afterwards and we we do this by just by calling the actual driver so you are",
    "start": "1478799",
    "end": "1484400"
  },
  {
    "text": "using nvidia gpu we just call the nvidia driver to get the ptx back if you are using the intel",
    "start": "1484400",
    "end": "1489760"
  },
  {
    "text": "stylings um sorry intel fga which is called the intel driver and will give us the bit stream",
    "start": "1489760",
    "end": "1495200"
  },
  {
    "text": "the configuration file back okay so one of the things we do and",
    "start": "1495200",
    "end": "1502240"
  },
  {
    "text": "actually that's one of the things that take us most most of our time is compiled specializations",
    "start": "1502240",
    "end": "1507360"
  },
  {
    "text": "and let me tell you why we need this so we generate opencl underneath opencl you know that is a standard right that means",
    "start": "1507360",
    "end": "1514320"
  },
  {
    "text": "that code is portable but opencl the performance is not portable which means that if we",
    "start": "1514320",
    "end": "1519520"
  },
  {
    "text": "don't change if we don't massage the code we generate we that we might not get the performance",
    "start": "1519520",
    "end": "1525039"
  },
  {
    "text": "we want and i show you here one type of specialization we do for loops uh",
    "start": "1525039",
    "end": "1530480"
  },
  {
    "text": "this is the the input code this is the graph form don't worry if you can't read it it's just an example and then we",
    "start": "1530480",
    "end": "1536480"
  },
  {
    "text": "say okay you're targeting gpus so each each loop is gonna",
    "start": "1536480",
    "end": "1543120"
  },
  {
    "text": "we transform the loop that says um we call like fine-grained parallelism basically each thread is gonna compute",
    "start": "1543120",
    "end": "1549520"
  },
  {
    "text": "its own element if we target multi-cores israel is gonna compute a range of elements",
    "start": "1549520",
    "end": "1555600"
  },
  {
    "text": "we do this specialization directly in the ir i show you for simplicity in java code but we do this information directly in there",
    "start": "1555600",
    "end": "1562880"
  },
  {
    "text": "and we have many optimization for that let me show you what we do for fpgas and actually",
    "start": "1562880",
    "end": "1568480"
  },
  {
    "start": "1566000",
    "end": "1566000"
  },
  {
    "text": "this is very important because if we don't specialize the code for cpus of gpus",
    "start": "1568480",
    "end": "1574880"
  },
  {
    "text": "well that's fine but you're going to get performance anyways not that good but still much much higher",
    "start": "1574880",
    "end": "1581200"
  },
  {
    "text": "than hotspot but if we don't do optimization for fpgas we most likely are gonna get slow down not",
    "start": "1581200",
    "end": "1587600"
  },
  {
    "text": "even speed up not even one x and let me show you what we do to get speed up so",
    "start": "1587600",
    "end": "1593600"
  },
  {
    "text": "that's the growl ir style don't worry about the details so one of the things we do we introduce",
    "start": "1593600",
    "end": "1599279"
  },
  {
    "text": "in the ir level thread scheduling which means that the ir has the knowledge of how many threads we want to execute for",
    "start": "1599279",
    "end": "1605600"
  },
  {
    "text": "example a block of 32 by 32 64 by 64 things like that",
    "start": "1605600",
    "end": "1610960"
  },
  {
    "text": "then we can tune the loop and roller grab has a good one but it's good for cpus for fpga is not that",
    "start": "1610960",
    "end": "1617360"
  },
  {
    "text": "good so we tune it and just by introducing a new loop and roller we can actually save some space physical space on the",
    "start": "1617360",
    "end": "1623760"
  },
  {
    "text": "pga and that can give you better speed we have a bunch of optimizations and just by doing that",
    "start": "1623760",
    "end": "1629120"
  },
  {
    "text": "we go from in our benchmark we have seen we go from slow down to 240x speed up",
    "start": "1629120",
    "end": "1636640"
  },
  {
    "text": "and the the server we execute was a four core machine huh now you might you might want to use",
    "start": "1636640",
    "end": "1641840"
  },
  {
    "text": "tornado vm for some workloads okay so i'm going to switch a bit the",
    "start": "1641840",
    "end": "1648399"
  },
  {
    "text": "context i want to prepare the background i need to explain you how we can perform live tiles migration which is the other",
    "start": "1648399",
    "end": "1654720"
  },
  {
    "text": "big part that tornado can do and many many times we define tornadoes",
    "start": "1654720",
    "end": "1659760"
  },
  {
    "text": "as a vm in a vm okay like like inception if you like so",
    "start": "1659760",
    "end": "1665600"
  },
  {
    "text": "if you execute your java program but you don't have your task skills defined you might get something open jdk or",
    "start": "1665600",
    "end": "1671840"
  },
  {
    "text": "graph that's fine if your code gets hot potentially you're going to reach the com",
    "start": "1671840",
    "end": "1677120"
  },
  {
    "text": "the specialized code the compile code for cpus if you hire if you have your task",
    "start": "1677120",
    "end": "1682559"
  },
  {
    "text": "schedules we have something like this we are going to trigger the tornado compiler as i showed you before",
    "start": "1682559",
    "end": "1688000"
  },
  {
    "text": "we have our data from analyzing optimizer don't worry about the details i'm going to explain in the next slide and then we generate new bytecodes",
    "start": "1688000",
    "end": "1695440"
  },
  {
    "text": "and then because we are in this mode we manage memory we manage execution we manage",
    "start": "1695440",
    "end": "1701679"
  },
  {
    "text": "compilation and we manage task migration that's what we say that we have a vm in a vm that's why we say",
    "start": "1701679",
    "end": "1709600"
  },
  {
    "text": "actually it's a plugin to open jdk right without a strategy we can do task migration across many",
    "start": "1709600",
    "end": "1716320"
  },
  {
    "text": "devices okay let me show you an example of this we have your class compute for example",
    "start": "1716320",
    "end": "1722960"
  },
  {
    "start": "1720000",
    "end": "1720000"
  },
  {
    "text": "and you have map and reduce and we build a task schedule in this case we have two tasks one pointing two",
    "start": "1722960",
    "end": "1729200"
  },
  {
    "text": "map methods the other pointing to the reduced methods and then i want to highlight that we",
    "start": "1729200",
    "end": "1735120"
  },
  {
    "text": "pass input output input output but the output of the first is the input of the second and we don't use it anymore",
    "start": "1735120",
    "end": "1741919"
  },
  {
    "text": "which means that we could potentially optimize it and that's what we do in the graph in",
    "start": "1741919",
    "end": "1747120"
  },
  {
    "text": "the in the graph analyzer when we do the graph analysis here of the data flow",
    "start": "1747120",
    "end": "1754399"
  },
  {
    "text": "basically we have the data coming into the first method then we produce a result we don't need",
    "start": "1754399",
    "end": "1760480"
  },
  {
    "text": "the result coming from here we just can keep it on the device we have the second method",
    "start": "1760480",
    "end": "1766000"
  },
  {
    "text": "and finally we synchronize with the host again we we send the results back",
    "start": "1766000",
    "end": "1772640"
  },
  {
    "text": "and once we have the graph optimized we generate byte codes as i showed you the very beginning and",
    "start": "1772640",
    "end": "1778320"
  },
  {
    "text": "the bytecodes are pretty simple by codes everything is in close between begin and end plus we pass an index",
    "start": "1778320",
    "end": "1784080"
  },
  {
    "text": "this guy is just the initial hint initial device index to run could be a",
    "start": "1784080",
    "end": "1790159"
  },
  {
    "text": "gpu but we can change this at runtime we by default we take a decision",
    "start": "1790159",
    "end": "1795760"
  },
  {
    "text": "and then we this is a simple process we just arrest the graph we say copy in the first variable",
    "start": "1795760",
    "end": "1800960"
  },
  {
    "text": "then allocate a space for a second we need to we don't need to copy anything then launch well many of the bad codes",
    "start": "1800960",
    "end": "1807600"
  },
  {
    "text": "are non-blocking which means we need a way to block okay until the trend the data transfer is finished",
    "start": "1807600",
    "end": "1813520"
  },
  {
    "text": "and then we launch the first method the first time we execute this bytecode we call gral",
    "start": "1813520",
    "end": "1820559"
  },
  {
    "text": "well tornado compiler and we compiled from by code to a pencil",
    "start": "1820559",
    "end": "1825600"
  },
  {
    "text": "and so on so forth until getting the final thing so the tornado by coach in my opinion",
    "start": "1825600",
    "end": "1832399"
  },
  {
    "text": "is a very very simple way to orchestrate execution on heterogeneous devices why because we can do more complex",
    "start": "1832399",
    "end": "1839200"
  },
  {
    "text": "things like let's imagine this this scenario and this is very typical by the way",
    "start": "1839200",
    "end": "1845440"
  },
  {
    "text": "we want to process 16 gigabyte of data on a gpu that only has one gigabyte this is very typical gpu has only a",
    "start": "1845440",
    "end": "1851520"
  },
  {
    "text": "limited amount of memory so the way we do that is through an api call called batch we can",
    "start": "1851520",
    "end": "1858720"
  },
  {
    "text": "batch execution we have three arrays we say batching in 300 megabytes each so 900",
    "start": "1858720",
    "end": "1865120"
  },
  {
    "text": "megabytes they fit into one gigabyte we can process it underneath what hap what's happening",
    "start": "1865120",
    "end": "1870880"
  },
  {
    "text": "is we unfold all the bytecodes in batches first batch coping execution",
    "start": "1870880",
    "end": "1876480"
  },
  {
    "text": "copy our second batch and so on so forth and so on so forth we haven't changed the compiler we haven't changed the runtime nothing",
    "start": "1876480",
    "end": "1883519"
  },
  {
    "text": "just how the byte codes are running that's all right and that's a very powerful capability",
    "start": "1883519",
    "end": "1890840"
  },
  {
    "text": "okay with that let me switch now to the task migration i have all the ingredients that i need to explain this",
    "start": "1890840",
    "end": "1898720"
  },
  {
    "text": "because we have the ability to compile and run for many devices what we're gonna what we're gonna do is",
    "start": "1898720",
    "end": "1906480"
  },
  {
    "text": "we are going to span a set of java threads each thread is going to target one",
    "start": "1906480",
    "end": "1911760"
  },
  {
    "text": "particular device and each thread is going to compile and run we also keep another thread to run the",
    "start": "1911760",
    "end": "1918720"
  },
  {
    "text": "sequential application this is because we want to switch only if the application runs faster on",
    "start": "1918720",
    "end": "1925600"
  },
  {
    "text": "the device only if it's not faster just keep with hotspot they do a very good job",
    "start": "1925600",
    "end": "1931200"
  },
  {
    "text": "so we keep one thread to run the sequential hotspot and then we have a component here to make the decision in fact what",
    "start": "1931200",
    "end": "1938320"
  },
  {
    "text": "each thread is running is an instance of the vm i just told you the by codes i just told you which is",
    "start": "1938320",
    "end": "1944240"
  },
  {
    "text": "copying execution copy out for all this uh for all the messages you have",
    "start": "1944240",
    "end": "1949919"
  },
  {
    "text": "and then the the vm will tell us i know you're running with the sequential thread but now if you switch",
    "start": "1949919",
    "end": "1956159"
  },
  {
    "text": "to this device at this time you're gonna run faster than it used to be",
    "start": "1956159",
    "end": "1961200"
  },
  {
    "text": "now how this decision logic made well we introduced policies",
    "start": "1961200",
    "end": "1966399"
  },
  {
    "text": "uh we have three for now we focus on performance and we have what we call end-to-end",
    "start": "1966399",
    "end": "1972000"
  },
  {
    "text": "performance peak performance and latency the end-to-end performance will encounter in the decision",
    "start": "1972000",
    "end": "1978159"
  },
  {
    "text": "did compilation and execution okay including data transfers pip performance will not encounter g g",
    "start": "1978159",
    "end": "1985039"
  },
  {
    "text": "compilation only execution and then we'll make the decision these two modes were for all the devices in",
    "start": "1985039",
    "end": "1990880"
  },
  {
    "text": "order to make a decision that could be very slow that's why we introduced a third component a third",
    "start": "1990880",
    "end": "1996000"
  },
  {
    "text": "policy called latency which means i spawn a set of java threads the first one to finish just cover it",
    "start": "1996000",
    "end": "2002840"
  },
  {
    "text": "okay okay in fact i have a demo to show you this so uh",
    "start": "2002840",
    "end": "2008480"
  },
  {
    "text": "this is hard to show we have another calling api that we can switch the device and that's what we're gonna show you but",
    "start": "2008480",
    "end": "2014960"
  },
  {
    "text": "the internal logic is the same so let me start with the code",
    "start": "2014960",
    "end": "2020480"
  },
  {
    "text": "i have a server client application dot java the server will open a socket",
    "start": "2020480",
    "end": "2028559"
  },
  {
    "text": "and the server has a one task scale we'll run we'll try to run one task on the gpu",
    "start": "2028559",
    "end": "2035679"
  },
  {
    "text": "it's a very simple just vector addition and the server the run method will wait for the client",
    "start": "2035679",
    "end": "2042240"
  },
  {
    "text": "to tell in which device you want to run and then the client can change the device at any",
    "start": "2042240",
    "end": "2048079"
  },
  {
    "text": "point at runtime so the server will still be open right by the way all the examples",
    "start": "2048079",
    "end": "2053839"
  },
  {
    "text": "all the examples i've showed you today are available on github you can easily reproduce them",
    "start": "2053839",
    "end": "2059839"
  },
  {
    "text": "read the code and let me show you that so on the left hand side this side i'm",
    "start": "2059839",
    "end": "2065679"
  },
  {
    "text": "gonna run the server okay that's the flux i use i'm gonna",
    "start": "2065679",
    "end": "2071599"
  },
  {
    "text": "print the kernel but i'm gonna run with debug just to tell me which device is running and on the right hand side i'm gonna run",
    "start": "2071599",
    "end": "2078320"
  },
  {
    "text": "the client and now i stop there to wait for",
    "start": "2078320",
    "end": "2083679"
  },
  {
    "text": "a device in which to run the task let's run and device zero important bit i select the gpu compile",
    "start": "2083679",
    "end": "2091839"
  },
  {
    "text": "and run okay now i select the second device",
    "start": "2091839",
    "end": "2098160"
  },
  {
    "text": "it's an intel cpu multicore then the next one is a multi-cop up with different opencl driver",
    "start": "2098160",
    "end": "2105280"
  },
  {
    "text": "compile and run for all of those for all of them and finally the integrated graphics",
    "start": "2105280",
    "end": "2110880"
  },
  {
    "text": "and i have been switching without restarting the application in fact now i can switch you know let's",
    "start": "2110880",
    "end": "2116320"
  },
  {
    "text": "run on zero again on the nvidia i didn't now compile i just run it the code is already in the code cache",
    "start": "2116320",
    "end": "2122800"
  },
  {
    "text": "obviously binaries are different because devices are different and now i can run this run on device one",
    "start": "2122800",
    "end": "2128400"
  },
  {
    "text": "let's run on zero again zero again let's run on the intel graphics result nvidia",
    "start": "2128400",
    "end": "2134560"
  },
  {
    "text": "let's run on the multi-core and so on so forth and i haven't restarted application okay",
    "start": "2134560",
    "end": "2141520"
  },
  {
    "text": "cool i hope i convinced you that this is a nice feature to have for manus languages in fact what we",
    "start": "2141520",
    "end": "2147280"
  },
  {
    "start": "2145000",
    "end": "2145000"
  },
  {
    "text": "propose is something like this so um are you familiar with hotspots some of the internals of how hotspot compile",
    "start": "2147280",
    "end": "2153520"
  },
  {
    "text": "codes so you might have that for example open jdk have a few compilers c1 and c2 but they",
    "start": "2153520",
    "end": "2158560"
  },
  {
    "text": "follow c2 or you might have the crowd compiler but as soon as you reach the maximum level",
    "start": "2158560",
    "end": "2164880"
  },
  {
    "text": "c2 or graph you cannot get faster that's not possible however if we plug",
    "start": "2164880",
    "end": "2171119"
  },
  {
    "text": "in the dynamic reconfiguration we might get something faster after all that's the",
    "start": "2171119",
    "end": "2176800"
  },
  {
    "text": "whole idea so tornado vm will say i know you're running with the code that c2 was",
    "start": "2176800",
    "end": "2182720"
  },
  {
    "text": "produced but if we switch to multi-core you're gonna get faster and then over again if",
    "start": "2182720",
    "end": "2187839"
  },
  {
    "text": "you switch to a gpu you're gonna get even faster why is this switching because you might run an expression with different",
    "start": "2187839",
    "end": "2194079"
  },
  {
    "text": "input sizes for example and depending on the input size you might see the device by the way don't run on a gpu you only",
    "start": "2194079",
    "end": "2200400"
  },
  {
    "text": "have a few elements to run so at least one thousand threads run on the gpu otherwise the gpu is",
    "start": "2200400",
    "end": "2205839"
  },
  {
    "text": "on holidays right okay um just to mention a few related",
    "start": "2205839",
    "end": "2211599"
  },
  {
    "text": "works in the context of java there are plenty out there um just to summarize well let me say",
    "start": "2211599",
    "end": "2218240"
  },
  {
    "text": "tell you this so you might have about aparappi project or ibm gni",
    "start": "2218240",
    "end": "2223359"
  },
  {
    "text": "they support only gpus aparappu support multicore as well because they target opencl",
    "start": "2223359",
    "end": "2229119"
  },
  {
    "text": "there are a few other projects that this one actually is my psd thesis in my opinion",
    "start": "2229119",
    "end": "2235599"
  },
  {
    "text": "tornado is the one that supports more devices because we can support gpus fpgas and multi-core we can perform task",
    "start": "2235599",
    "end": "2242000"
  },
  {
    "text": "migration and as far as i know there is no other one that can do that we can perform a specialization at",
    "start": "2242000",
    "end": "2248960"
  },
  {
    "text": "runtime and i'm not aware for the project windows doing that and we can target also dynamic languages by the way just",
    "start": "2248960",
    "end": "2255760"
  },
  {
    "text": "for reference because a lot of people ask me about that gr cuda i put it here but jar cuda is a framework on top of",
    "start": "2255760",
    "end": "2263359"
  },
  {
    "text": "truffle okay but the code you write is kuda not javascript or r it's cuda okay but i put",
    "start": "2263359",
    "end": "2271040"
  },
  {
    "text": "it for reference but there are a lot of differences between jr code and tornado with tornado you just write java which i",
    "start": "2271040",
    "end": "2277839"
  },
  {
    "text": "believe is much higher level okay so let me show your performance in a real setup",
    "start": "2277839",
    "end": "2284400"
  },
  {
    "text": "and i want to show you the dynamic reconfiguration in action in a in a server and how to read how to",
    "start": "2284400",
    "end": "2290640"
  },
  {
    "text": "read this graph don't worry so x axis shows input size y axis shows",
    "start": "2290640",
    "end": "2295839"
  },
  {
    "text": "speed up against hotspot i have two applications dft and embody and",
    "start": "2295839",
    "end": "2303359"
  },
  {
    "text": "the dots or the squares is i run tornado on that device without switching i don't",
    "start": "2303359",
    "end": "2308880"
  },
  {
    "text": "care just run on this no matter what and the line is iron tornado with the ability of doing",
    "start": "2308880",
    "end": "2316160"
  },
  {
    "text": "the task migration and let's focus on the dft end-to-end policy",
    "start": "2316160",
    "end": "2321280"
  },
  {
    "text": "tornado will say for small input sizes stay with hotspot they do a very good job as soon as the data increases",
    "start": "2321280",
    "end": "2328079"
  },
  {
    "text": "tornado vl will switch execution from hotspot to the gpu and will continue from there and",
    "start": "2328079",
    "end": "2333839"
  },
  {
    "text": "similar situation applies for other benchmarks okay um",
    "start": "2333839",
    "end": "2340560"
  },
  {
    "start": "2339000",
    "end": "2339000"
  },
  {
    "text": "wait we can run i show you through the demos but we can run on many devices in fact we can run also amd",
    "start": "2340560",
    "end": "2346240"
  },
  {
    "text": "gpus and one important thing i want to highlight tornado doesn't get performance for all",
    "start": "2346240",
    "end": "2352720"
  },
  {
    "text": "benchmarks okay in fact for example sakspy would you have a huge amount of data to copy",
    "start": "2352720",
    "end": "2357839"
  },
  {
    "text": "to the device one or two operations to do on the device and a huge amount of data to copy",
    "start": "2357839",
    "end": "2363200"
  },
  {
    "text": "back that that doesn't work for gpus or fpgas okay",
    "start": "2363200",
    "end": "2368240"
  },
  {
    "text": "and in fact you will get slowdown but for all the type of applications black shorts mandelbrot mbody for",
    "start": "2368240",
    "end": "2374320"
  },
  {
    "text": "physics multi-multiplication for deep learning dft stuff tornado is a very good suite for running",
    "start": "2374320",
    "end": "2381520"
  },
  {
    "text": "on heterogeneous devices if you're interested we have a bunch of papers",
    "start": "2381520",
    "end": "2388000"
  },
  {
    "text": "those are available in our github repository if you are interested and actually there are more coming okay",
    "start": "2388000",
    "end": "2395680"
  },
  {
    "text": "so obviously let's talk now about limitations we have limitations um and most of the limitations are bound",
    "start": "2395680",
    "end": "2404079"
  },
  {
    "text": "because of the programming model we use underneath uh we use opencl underneath an opencl doesn't support",
    "start": "2404079",
    "end": "2410000"
  },
  {
    "text": "for example recursion so we don't support a cushion on the device we don't support objects well kind of we",
    "start": "2410000",
    "end": "2417040"
  },
  {
    "text": "support some type of objects the objects we know the data layout",
    "start": "2417040",
    "end": "2422240"
  },
  {
    "text": "for example the matrix multiplication remember the matrix to the 3d those are objects we support those",
    "start": "2422480",
    "end": "2429520"
  },
  {
    "text": "we don't support dynamic memory allocation kind of in some cases we do why because we compile a runtime and",
    "start": "2430079",
    "end": "2437359"
  },
  {
    "text": "around time if we know the size of the ray we can kind of simulate dynamic allocation but",
    "start": "2437359",
    "end": "2442480"
  },
  {
    "text": "it's not right uh and we don't support exceptions we might support receptions in the future",
    "start": "2442480",
    "end": "2449040"
  },
  {
    "text": "and this is a bit controversial because in my opinion",
    "start": "2449040",
    "end": "2454240"
  },
  {
    "text": "supporting all these features on these devices these devices are not thought for running this kind of stuff so if we",
    "start": "2454240",
    "end": "2461040"
  },
  {
    "text": "want to force to force this we might not get the speed that we want the philosophy we we took with tornado",
    "start": "2461040",
    "end": "2468160"
  },
  {
    "text": "vm is run when the application when the overload makes sense to run in there otherwise just",
    "start": "2468160",
    "end": "2473839"
  },
  {
    "text": "stay with hotspot they do a very good job so rather than competing against hotspot is a complement",
    "start": "2473839",
    "end": "2480079"
  },
  {
    "text": "to hotspot or ground for future work we have many things in",
    "start": "2480079",
    "end": "2485599"
  },
  {
    "start": "2484000",
    "end": "2484000"
  },
  {
    "text": "progress um we are running we are trying to integrate uh more uh jit com specializations okay in",
    "start": "2485599",
    "end": "2493599"
  },
  {
    "text": "the g compiler so you know that gpus has different memory memory tiers like constant memory",
    "start": "2493599",
    "end": "2499359"
  },
  {
    "text": "local memory global memory private memory other projects like aparappi what they",
    "start": "2499359",
    "end": "2504800"
  },
  {
    "text": "do is they expose an api a set of api calls for each individual region what we're",
    "start": "2504800",
    "end": "2512319"
  },
  {
    "text": "trying to do we have a phd student working on that what he's doing is automatically in the",
    "start": "2512319",
    "end": "2518079"
  },
  {
    "text": "ir explore which levels to insert and automatically generated code for la and that could give you very good speed",
    "start": "2518079",
    "end": "2524720"
  },
  {
    "text": "ups in automatic manner other thing we want to investigate energy policies so we focus now on",
    "start": "2524720",
    "end": "2530160"
  },
  {
    "text": "performance but think about this if we can switch not because performance requirements but energy",
    "start": "2530160",
    "end": "2536960"
  },
  {
    "text": "requirements run this method on this device because you consume less energy",
    "start": "2536960",
    "end": "2542160"
  },
  {
    "text": "that would be cool so this is work in progress okay we're working on that another thing we're working is for",
    "start": "2542160",
    "end": "2547440"
  },
  {
    "text": "example we're plugging in a ptx backend underneath so instead of opencl run with",
    "start": "2547440",
    "end": "2552640"
  },
  {
    "text": "nvidia only but hopefully we can get slightly better performance",
    "start": "2552640",
    "end": "2558160"
  },
  {
    "text": "so i'm finalizing the presentation i want to give you a few slides a few things about how this",
    "start": "2558160",
    "end": "2565280"
  },
  {
    "text": "is currently being used by industry so turner vm is part of an european project called e2data",
    "start": "2565280",
    "end": "2571200"
  },
  {
    "text": "i will need another talk to explain the project but just a sentence what we do is to run apache flink map",
    "start": "2571200",
    "end": "2577200"
  },
  {
    "text": "reduce workloads on a heterogeneous cluster okay automatically and for that we plug in tornado on the",
    "start": "2577200",
    "end": "2584880"
  },
  {
    "text": "final node and the final node will contain a bunch of heterogeneous hardware so ideally you",
    "start": "2584880",
    "end": "2590400"
  },
  {
    "text": "will just type your mapreduce computation with link no changes in the primary model",
    "start": "2590400",
    "end": "2595760"
  },
  {
    "text": "no extensions and everything will run if we can't on it reduce devices",
    "start": "2595760",
    "end": "2601200"
  },
  {
    "text": "automatically so this is work in progress so if you like this project stay tuned",
    "start": "2601200",
    "end": "2606480"
  },
  {
    "text": "we are working we have a prototype actually working so but stay tuned feel free to ask me later for more details",
    "start": "2606480",
    "end": "2613359"
  },
  {
    "text": "then there is a company called lexus is here in london they what they do is they are using",
    "start": "2613359",
    "end": "2619200"
  },
  {
    "text": "tornado for machine learning the goal of application is they want to predict",
    "start": "2619200",
    "end": "2624880"
  },
  {
    "text": "the number of patients to be admitted in a particular hospital to predict resources to predict you know",
    "start": "2624880",
    "end": "2630960"
  },
  {
    "text": "how medical doctors and so on and for doing that what they did is they trained a machine",
    "start": "2630960",
    "end": "2637280"
  },
  {
    "text": "learning model in java and the sequential code runs around two thousand second seconds",
    "start": "2637280",
    "end": "2643760"
  },
  {
    "text": "and by using tornado they went down to 188 seconds okay so 40 times faster",
    "start": "2643760",
    "end": "2653280"
  },
  {
    "text": "14 times faster okay just to sum up um tornado is also open source",
    "start": "2653359",
    "end": "2660400"
  },
  {
    "text": "is on github it's fully available on github check it out we have last release last",
    "start": "2660400",
    "end": "2665839"
  },
  {
    "text": "week in that release we introduced these dynamic languages with raw vm",
    "start": "2665839",
    "end": "2671200"
  },
  {
    "text": "we also have docker images in fact the demo i show you with node.js is through docker",
    "start": "2671200",
    "end": "2676800"
  },
  {
    "text": "so i my my personal advice is if you want to give a try to tornado just use",
    "start": "2676800",
    "end": "2682160"
  },
  {
    "text": "the docker images because you don't have to mess around with the driver installation okay we have two types for gpus by",
    "start": "2682160",
    "end": "2688960"
  },
  {
    "text": "nvidia are for intel integrated graphics",
    "start": "2688960",
    "end": "2693280"
  },
  {
    "text": "um we have a team i'm i have to personally thank all members",
    "start": "2694640",
    "end": "2700160"
  },
  {
    "text": "of the team we have a small team but they are doing a great very good job mostly we are",
    "start": "2700160",
    "end": "2705200"
  },
  {
    "text": "research staff academic staff and psta student pst students they are working on different aspects of tornado",
    "start": "2705200",
    "end": "2713119"
  },
  {
    "text": "like the optimizations the the fling part and we have a bunch of students",
    "start": "2713119",
    "end": "2719359"
  },
  {
    "text": "recently working on how to accelerate deep learning with tornado and we are also looking for feedback and",
    "start": "2719359",
    "end": "2727920"
  },
  {
    "text": "collaboration so we want to hear to hear from you what you think about this if you are missing something",
    "start": "2727920",
    "end": "2734000"
  },
  {
    "text": "and yeah feel free to talk to us i will be the whole day here so yeah feel free to reach me",
    "start": "2734000",
    "end": "2741040"
  },
  {
    "text": "so just to summarize takeaways i have shown you that today's computing",
    "start": "2741040",
    "end": "2747839"
  },
  {
    "text": "devices are heterogeneous okay heterogeneous devices are everywhere",
    "start": "2747839",
    "end": "2752960"
  },
  {
    "text": "there is no way to escape the thing is how to program efficiently i show you one alternative to program by",
    "start": "2752960",
    "end": "2759119"
  },
  {
    "text": "using tornado with that you can target any high level program language by that i mean java",
    "start": "2759119",
    "end": "2764960"
  },
  {
    "text": "are ruby and run it transparently on heterogeneous hardware",
    "start": "2764960",
    "end": "2770160"
  },
  {
    "text": "i have shown you also a way to do task migration i believe this is really cool feature",
    "start": "2770160",
    "end": "2775920"
  },
  {
    "text": "i hope i convinced you that we can do that and you can get very high speed we're not talking about handy decks we're go",
    "start": "2775920",
    "end": "2782240"
  },
  {
    "text": "we're talking about 1000x okay 4000x as i showed you before and this is because we run with the",
    "start": "2782240",
    "end": "2788720"
  },
  {
    "text": "capabilities of the device okay and that's all from my presentation so if you have questions i'm happy to",
    "start": "2788720",
    "end": "2795119"
  },
  {
    "text": "take them thank you very much for your attention",
    "start": "2795119",
    "end": "2803839"
  },
  {
    "start": "2801000",
    "end": "2801000"
  },
  {
    "text": "yeah perhaps you need a mic",
    "start": "2806800",
    "end": "2817119"
  },
  {
    "text": "first of all very very interesting thank you thank you i noticed when you ran one of your examples that your computer said",
    "start": "2817119",
    "end": "2823119"
  },
  {
    "text": "your gpu memory is running low um sorry again i noticed that your computer was telling you your gpu memory",
    "start": "2823119",
    "end": "2829920"
  },
  {
    "text": "was running low when you ran one of your ah yeah yeah does it do any kind of automatic cleanup of i",
    "start": "2829920",
    "end": "2837040"
  },
  {
    "text": "mean you're copying code to the gpu right does it do cleanup good question so we're running",
    "start": "2837040",
    "end": "2842720"
  },
  {
    "text": "in isolation and obviously after run internet we do a cleanup phase obviously yes yes",
    "start": "2842720",
    "end": "2851359"
  },
  {
    "text": "yes another question for that no thank you first um but uh that",
    "start": "2853760",
    "end": "2860480"
  },
  {
    "text": "one one of your sides when you were you had the matrix comparing all of the different um",
    "start": "2860480",
    "end": "2866240"
  },
  {
    "text": "projects but yeah yeah yeah um and uh yeah it said that tornado wasn't",
    "start": "2866240",
    "end": "2872480"
  },
  {
    "text": "production ready um so not yet asterisk um yeah",
    "start": "2872480",
    "end": "2878480"
  },
  {
    "text": "what do you need to do so we are pushing for something else um i believe we support more things that",
    "start": "2878480",
    "end": "2885920"
  },
  {
    "text": "others but for now we are running as a part of the academic project so i guess what we",
    "start": "2885920",
    "end": "2891760"
  },
  {
    "text": "need to do is to get a use case from a company and make them useful one of those is sexuals",
    "start": "2891760",
    "end": "2898559"
  },
  {
    "text": "hopefully they won't they they can make it you know in production for now because we are running on the",
    "start": "2898559",
    "end": "2903599"
  },
  {
    "text": "european project for knowledge academic that's what i mean not yet for me i will say yes of course use it",
    "start": "2903599",
    "end": "2915838"
  },
  {
    "text": "any thoughts on using tpus yeah good point um if the tpu supports opencl yes",
    "start": "2916160",
    "end": "2922960"
  },
  {
    "text": "why not um we were thinking to introduce",
    "start": "2922960",
    "end": "2930960"
  },
  {
    "text": "llvmir and perhaps go through you know this project by google that can",
    "start": "2931359",
    "end": "2936880"
  },
  {
    "text": "target tpus from this ir but we don't have students working on that right now um",
    "start": "2936880",
    "end": "2942800"
  },
  {
    "text": "but that's one of the internal things where we're discussing to to get more back-ends one of the back-ends we're building",
    "start": "2942800",
    "end": "2948400"
  },
  {
    "text": "right now is the new one is a ptx",
    "start": "2948400",
    "end": "2952160"
  },
  {
    "text": "um well thanks very much um and do ask one questions directly um but yeah thanks very much um",
    "start": "2956240",
    "end": "2963280"
  },
  {
    "text": "thank you our next talk is on building webassembly compiler so please stay and learn more but um thanks very much for",
    "start": "2963280",
    "end": "2969760"
  },
  {
    "text": "that great talk thank you",
    "start": "2969760",
    "end": "2975680"
  }
]