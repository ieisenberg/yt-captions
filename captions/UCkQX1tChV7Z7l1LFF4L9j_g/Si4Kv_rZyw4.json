[
  {
    "text": "[Music]",
    "start": "1100",
    "end": "9719"
  },
  {
    "text": "my name is Andre uh I'm the founder of DC uh where we are building an alternative to kubernetes a container",
    "start": "9719",
    "end": "16520"
  },
  {
    "text": "orchestrator for managing eii infrastructure and since it's used by companies that develop AI models",
    "start": "16520",
    "end": "23920"
  },
  {
    "text": "especially large language models we quite passionate about open source LMS",
    "start": "23920",
    "end": "29640"
  },
  {
    "text": "and of course want to help more companies understand how to adopt them",
    "start": "29640",
    "end": "35719"
  },
  {
    "text": "without feeling overwhelmed and of course to see more value in this the",
    "start": "35719",
    "end": "41920"
  },
  {
    "text": "talk isn't really about covering all their best practices or dirty hacks for",
    "start": "41920",
    "end": "48480"
  },
  {
    "text": "deploying open source llms in production this would be hardly possible instead",
    "start": "48480",
    "end": "54760"
  },
  {
    "text": "the goal is to give you a rough sense of what to expect from using open source",
    "start": "54760",
    "end": "60719"
  },
  {
    "text": "lamps when and why they might be the right choice and what the development",
    "start": "60719",
    "end": "66520"
  },
  {
    "text": "process might feel like so if by the end of this talk you are more convinced of",
    "start": "66520",
    "end": "72880"
  },
  {
    "text": "the value of Open Source LMS and consider exploring them in your company",
    "start": "72880",
    "end": "77920"
  },
  {
    "text": "or learning more about them I would feel achieve my goal so the maturity of AI remains a",
    "start": "77920",
    "end": "86479"
  },
  {
    "text": "debated topic and with annual predictions from experts ranging from",
    "start": "86479",
    "end": "91720"
  },
  {
    "text": "skepticism about LMS utility Beyond Chet Bots to concerns about um AGI and",
    "start": "91720",
    "end": "99000"
  },
  {
    "text": "existential risks from it an open source AI is also subject to frequent speculations with ongoing",
    "start": "99000",
    "end": "106000"
  },
  {
    "text": "questions whether it can ever rival the quality of proprietary models such as open Ai and",
    "start": "106000",
    "end": "112200"
  },
  {
    "text": "entropic so CLM the founder of hugging phas um the leading platform for publishing open source models actually",
    "start": "112200",
    "end": "119479"
  },
  {
    "text": "predicted last year that open source models would match the equality of",
    "start": "119479",
    "end": "125039"
  },
  {
    "text": "closed Source ones by 2024 so now that we are uh in the",
    "start": "125039",
    "end": "132120"
  },
  {
    "text": "second half of 2024 uh please raise raise hands if you believe in that",
    "start": "132120",
    "end": "137200"
  },
  {
    "text": "production if you think that open source will catch",
    "start": "137200",
    "end": "142360"
  },
  {
    "text": "up okay so many Skeptics here um so you'll be surprised to learn",
    "start": "142360",
    "end": "149200"
  },
  {
    "text": "that in just 6 months meta the company behind Facebook released Lama 3.1 an",
    "start": "149200",
    "end": "156120"
  },
  {
    "text": "open source model that for the first time match the quality of close Source models so in this chart uh carefully",
    "start": "156120",
    "end": "164000"
  },
  {
    "text": "compiled by Maxim Labon andl engineer at liquid AI um it shows the release timelines of",
    "start": "164000",
    "end": "171959"
  },
  {
    "text": "both close source and open source models over the last two years so the the chart tracks",
    "start": "171959",
    "end": "179440"
  },
  {
    "text": "each moreel model score on the MML U Benchmark which is massive",
    "start": "179440",
    "end": "185840"
  },
  {
    "text": "multilanguage understanding uh Benchmark which is one of the most comprehensive measures of a model performance across",
    "start": "185840",
    "end": "192720"
  },
  {
    "text": "wide range of tasks and for open source models you'll notice that the model name",
    "start": "192720",
    "end": "197920"
  },
  {
    "text": "names here include the number of parameters uh the font is a bit small",
    "start": "197920",
    "end": "203360"
  },
  {
    "text": "but if you look uh um close you can see the names of the open source",
    "start": "203360",
    "end": "208760"
  },
  {
    "text": "models also had this uh number of U uh",
    "start": "208760",
    "end": "214000"
  },
  {
    "text": "parameters and generally the higher the The Benchmark score uh basically the",
    "start": "214000",
    "end": "220159"
  },
  {
    "text": "better the model is the larger the model means the more parameters the model has",
    "start": "220159",
    "end": "226200"
  },
  {
    "text": "so the model that finally achieved the parity with close Source models has 45 billion parameters which will talk",
    "start": "226200",
    "end": "234799"
  },
  {
    "text": "more about what what that really means so in this upper right hand and upper",
    "start": "234799",
    "end": "241040"
  },
  {
    "text": "right corner um of the chart um neur Lama 3.1 45 billion you can also sport",
    "start": "241040",
    "end": "248519"
  },
  {
    "text": "quen 2.5 72 billion uh which was released um soon after and despite this",
    "start": "248519",
    "end": "257519"
  },
  {
    "text": "um significantly smaller size it nearly matches the performance of Lama 3.1 um",
    "start": "257519",
    "end": "263800"
  },
  {
    "text": "45 billion the the largest one by meter and um this quen model was was released",
    "start": "263800",
    "end": "270160"
  },
  {
    "text": "by Alibaba Cloud the team behind uh the uh found well basically that trains",
    "start": "270160",
    "end": "276199"
  },
  {
    "text": "these foundational models there um and uh this is basically the current best",
    "start": "276199",
    "end": "282840"
  },
  {
    "text": "open source models um uh competing directly with the closed Source ones and",
    "start": "282840",
    "end": "287880"
  },
  {
    "text": "as you see despite early doubts open source models are keeping Pace with closed Source ones going head tohe head",
    "start": "287880",
    "end": "294280"
  },
  {
    "text": "in quality and just yesterday while preparing my slides met announced lamb",
    "start": "294280",
    "end": "300440"
  },
  {
    "text": "3.2 uh probably some of you uh already know a multimodal model which which",
    "start": "300440",
    "end": "306039"
  },
  {
    "text": "surpass the best close Source multimodel models uh in performance a multimodel uh",
    "start": "306039",
    "end": "311759"
  },
  {
    "text": "sorry multimodel model uh means that it it not only generates text but it also",
    "start": "311759",
    "end": "317600"
  },
  {
    "text": "understand pictures and then can generate pictures as well right so let's take a closer look at Lama 3.1 why it's",
    "start": "317600",
    "end": "325080"
  },
  {
    "text": "significant and how it performs on various benchmarks",
    "start": "325080",
    "end": "330160"
  },
  {
    "text": "so Lama 3.1 is available in three sizes 7 billion parameters 70 billion parameters and 45 billion parameters so",
    "start": "330160",
    "end": "338039"
  },
  {
    "text": "it supports eight languages offers a 127k token context window um which",
    "start": "338039",
    "end": "345240"
  },
  {
    "text": "includes the uh the length of text that it can accept uh plus the length of the",
    "start": "345240",
    "end": "351400"
  },
  {
    "text": "text that it can generate uh the the longer the context window the larger",
    "start": "351400",
    "end": "356800"
  },
  {
    "text": "text llm can understand and generate um so it um it has a know it it has a it",
    "start": "356800",
    "end": "365800"
  },
  {
    "text": "allows you to use the model commercially basically the license allows you to use uh Lama 3.1 uh in",
    "start": "365800",
    "end": "373120"
  },
  {
    "text": "commercial um projects and it's not only customizable but also can be used uh it",
    "start": "373120",
    "end": "379360"
  },
  {
    "text": "it's not only uh used for inference but also for fine-tuning it is capable of also generating synthetic data um and",
    "start": "379360",
    "end": "387680"
  },
  {
    "text": "doing uh all all sorts of knowledge distillation as well which we'll probably talk about in the further",
    "start": "387680",
    "end": "393680"
  },
  {
    "text": "slides so on the right hand you can see several sorry on the left hand you can",
    "start": "393680",
    "end": "398880"
  },
  {
    "text": "see the several benchmarks that assess the performance of each model on different tasks um this Benchmark I",
    "start": "398880",
    "end": "405199"
  },
  {
    "text": "already mentioned MML this is the one of the most common benchmarks it measures",
    "start": "405199",
    "end": "410520"
  },
  {
    "text": "model performance across um a range of language U um tasks and um there are two",
    "start": "410520",
    "end": "418400"
  },
  {
    "text": "types of this uh Benchmark one is the known as MML it focuses on General",
    "start": "418400",
    "end": "423639"
  },
  {
    "text": "Knowledge understanding and there's another one uh which is called MML Pro",
    "start": "423639",
    "end": "429120"
  },
  {
    "text": "also known as five shot it assesses also in addition to general knowledge also",
    "start": "429120",
    "end": "434160"
  },
  {
    "text": "reasoning capabilities um so another key Benchmark",
    "start": "434160",
    "end": "439319"
  },
  {
    "text": "uh here is this human eval U and this Benchmark evaluates model code",
    "start": "439319",
    "end": "444440"
  },
  {
    "text": "generation skills so code generation is a valuable use case for LMS",
    "start": "444440",
    "end": "450440"
  },
  {
    "text": "um and not only to be used in like code completion in your ID which you probably heard of uh but also it enables the use",
    "start": "450440",
    "end": "458160"
  },
  {
    "text": "of tools um and helps building automate automate like automated agents uh with",
    "start": "458160",
    "end": "463639"
  },
  {
    "text": "with LMS and as shown uh the the largest version of Lama 3.1 scores highly on on",
    "start": "463639",
    "end": "470240"
  },
  {
    "text": "both of these benchmarks if we compare to um to other models including those",
    "start": "470240",
    "end": "475520"
  },
  {
    "text": "proprietary ones like GPT 4 and then clo 3.5 so it's worth noting that Lama 3.1 is a",
    "start": "475520",
    "end": "482720"
  },
  {
    "text": "more than just a model it's basically a developer platform and meta actually refers to it as a llama stack basically",
    "start": "482720",
    "end": "491120"
  },
  {
    "text": "a developer stack which uh includes not only the mot itself but um in different",
    "start": "491120",
    "end": "497199"
  },
  {
    "text": "sizes but also numerous uh tools and Integrations so tools that help you um",
    "start": "497199",
    "end": "503720"
  },
  {
    "text": "make their model more safe or tools for building agents um doing evals and fine",
    "start": "503720",
    "end": "509680"
  },
  {
    "text": "tuning as well so it's it's a lot of different tools it's not just a model and here we see the Aquin 2.5",
    "start": "509680",
    "end": "518039"
  },
  {
    "text": "which I me mentioned when we looked at this chart of models uh this is the",
    "start": "518039",
    "end": "523080"
  },
  {
    "text": "newest version of quen so we we had quen 1.0 2.0 and now this is the most recent",
    "start": "523080",
    "end": "531160"
  },
  {
    "text": "One released a couple of months ago um as I said it's created by the team behind Alibaba Cloud that specialize in",
    "start": "531160",
    "end": "538279"
  },
  {
    "text": "foundational models and besides demonstrating strong capabilities in fundamental knowledge",
    "start": "538279",
    "end": "545079"
  },
  {
    "text": "like reasoning and code Generation Um it it also speaks 29 languages compared to",
    "start": "545079",
    "end": "551680"
  },
  {
    "text": "eight languages supported by lamath Lama 3.1 uh so it's it's it it it Nows more",
    "start": "551680",
    "end": "557519"
  },
  {
    "text": "more languages uh which is great and notably it delivers impressive performance while being five times",
    "start": "557519",
    "end": "563000"
  },
  {
    "text": "smaller so as you see the the name of the model is uh 72 billion uh uh",
    "start": "563000",
    "end": "569720"
  },
  {
    "text": "parameters which is more than five time times smaller than the largest Lama 3.1",
    "start": "569720",
    "end": "575519"
  },
  {
    "text": "model uh and still it comes really close to the Quality U of Lama 3.1 for 45",
    "start": "575519",
    "end": "582760"
  },
  {
    "text": "billion parameters um so quen quen models come in different sizes and the majority of",
    "start": "582760",
    "end": "590120"
  },
  {
    "text": "models are available under Apache 2.0 license which is the most permissive license uh and this is great so there",
    "start": "590120",
    "end": "596640"
  },
  {
    "text": "are basically no conditions um except probably the largest one 72 billion",
    "start": "596640",
    "end": "601839"
  },
  {
    "text": "perameters which which comes with a propietary license um however it's still",
    "start": "601839",
    "end": "606880"
  },
  {
    "text": "uh open weights model and it's still allowed to use it commercially um if uh",
    "start": "606880",
    "end": "613480"
  },
  {
    "text": "your number of monthly active users is less than one um um when then specific",
    "start": "613480",
    "end": "622160"
  },
  {
    "text": "number I don't remember the number but but it's pretty big number so if you are not Google you certainly don't want uh",
    "start": "622160",
    "end": "628160"
  },
  {
    "text": "don't have to uh be be concerned about this and you actually can use this qu model commercial commercial purposes um",
    "start": "628160",
    "end": "635440"
  },
  {
    "text": "and later in the presentation we'll of course examine how model size um greatly influences its um practical applications",
    "start": "635440",
    "end": "643760"
  },
  {
    "text": "uh because U uh we should understand that uh the smaller the model is the much easier it is to use the model",
    "start": "643760",
    "end": "651279"
  },
  {
    "text": "um right so uh after covering five slides we finally arrived to the key question when should we use open source",
    "start": "651279",
    "end": "658079"
  },
  {
    "text": "models and what are the reasons behind it and based on my uh numerous",
    "start": "658079",
    "end": "663200"
  },
  {
    "text": "conversations with people um what I see is that most companies tend to Strate",
    "start": "663200",
    "end": "668480"
  },
  {
    "text": "strategically uh underestimate the importance of Open Source models and fail actually to recognize why relying",
    "start": "668480",
    "end": "676040"
  },
  {
    "text": "on closed Source models is not a viable long-term option for the industry so pretty pretty strong",
    "start": "676040",
    "end": "682560"
  },
  {
    "text": "statement but I'd love to elaborate on this because I think this is important um and um uh as we as we witnessing",
    "start": "682560",
    "end": "690920"
  },
  {
    "text": "today much like the internet or computers before uh geni is becoming an",
    "start": "690920",
    "end": "696720"
  },
  {
    "text": "integral becom an integral uh nearly to uh every service product or human",
    "start": "696720",
    "end": "703600"
  },
  {
    "text": "computer interaction uh and it basically transforms how we work how we communicate how we interact and uh as we",
    "start": "703600",
    "end": "711160"
  },
  {
    "text": "um enter the geni era it is it significantly impacts both economics and",
    "start": "711160",
    "end": "716240"
  },
  {
    "text": "the distribution of competitive Advantage so before um gen companies",
    "start": "716240",
    "end": "721399"
  },
  {
    "text": "gained the Competitive Edge through their of course like Talent uh technical",
    "start": "721399",
    "end": "726720"
  },
  {
    "text": "talent and through the proprietary Technologies which the company um owns um and what we see that in the Gen era",
    "start": "726720",
    "end": "735279"
  },
  {
    "text": "uh this Advantage this competitive advantage of a company will increasingly Steam from how AI is leveraged and appli",
    "start": "735279",
    "end": "742000"
  },
  {
    "text": "to the specific use cases and data so we see that transition and or we yet to see",
    "start": "742000",
    "end": "747480"
  },
  {
    "text": "this transition from the uh from the tech talent and the technology the company has to how AI is applied in very",
    "start": "747480",
    "end": "755480"
  },
  {
    "text": "specific use cases um and if a company aims to maintain its Competitive Edge um",
    "start": "755480",
    "end": "762160"
  },
  {
    "text": "um in my personal view it must certainly take gen seriously and avoid Outsourcing",
    "start": "762160",
    "end": "767680"
  },
  {
    "text": "this to external companies just as they did previously with uh software",
    "start": "767680",
    "end": "773199"
  },
  {
    "text": "development right uh so now that we talked about Theory bit let's maybe look",
    "start": "773199",
    "end": "778760"
  },
  {
    "text": "a bit uh uh under the hood what it actually feels to to use open source models and do not expect me to go into",
    "start": "778760",
    "end": "786240"
  },
  {
    "text": "maybe specific um um applications of Frameworks but I'd rather spend more",
    "start": "786240",
    "end": "791880"
  },
  {
    "text": "time uh talking about most fundamental aspects of using LMS and if you are",
    "start": "791880",
    "end": "797639"
  },
  {
    "text": "already into llms let's say pretty deep you might not find uh some um uh",
    "start": "797639",
    "end": "803880"
  },
  {
    "text": "significant um uh Insight here but uh those people that are curious about the",
    "start": "803880",
    "end": "809320"
  },
  {
    "text": "adoption of op Source LMS and maybe not necessarily the actual research uh but",
    "start": "809320",
    "end": "814720"
  },
  {
    "text": "but the use of this uh uh the results of This research uh within the companies uh",
    "start": "814720",
    "end": "820120"
  },
  {
    "text": "that that might uh be uh super helpful because then uh you understand what are the main constraints so here here's the",
    "start": "820120",
    "end": "828000"
  },
  {
    "text": "slide with the hardware requirements so you've probably heard many times that um in order to use l LMS you need uh",
    "start": "828000",
    "end": "835279"
  },
  {
    "text": "basically gpus and that that that you need they are very expensive uh and you need a lot of them so we'll talk about",
    "start": "835279",
    "end": "841320"
  },
  {
    "text": "that um uh just just a bit so uh if we look at Lama 3.1 which comes in three",
    "start": "841320",
    "end": "847320"
  },
  {
    "text": "different sizes we see that the larger the mot is the more parameters it has",
    "start": "847320",
    "end": "853399"
  },
  {
    "text": "the uh the bigger this GB number uh which means the number of memory GPU",
    "start": "853399",
    "end": "860600"
  },
  {
    "text": "memory which you need in order to run inference here and uh the column is called fp16 means that float Point 16 is",
    "start": "860600",
    "end": "868920"
  },
  {
    "text": "the format which uh the morel is stored the weights of the model this um this U",
    "start": "868920",
    "end": "874959"
  },
  {
    "text": "tensors are stored in this is also known as half Precision uh half of 32 uh and",
    "start": "874959",
    "end": "881360"
  },
  {
    "text": "this is mostly how llms are stored um um once they are trained let's say um and",
    "start": "881360",
    "end": "889320"
  },
  {
    "text": "we can see that uh in order to run an inference of a smallest model we would need a 16 at least 16 gab and this uh",
    "start": "889320",
    "end": "898519"
  },
  {
    "text": "pretty close go to one of the smallest GPU so um if if we talk about for",
    "start": "898519",
    "end": "904880"
  },
  {
    "text": "example Nvidia gpus um most uh most uh used gpus today like a100 which is 40 GB",
    "start": "904880",
    "end": "912680"
  },
  {
    "text": "or 80 gb or h100 which is uh um 80 gb as",
    "start": "912680",
    "end": "918240"
  },
  {
    "text": "well well if you are not into this yet um and all all you need to to know right",
    "start": "918240",
    "end": "923839"
  },
  {
    "text": "now is that um the more gpus you have the more memory you in total have and",
    "start": "923839",
    "end": "929959"
  },
  {
    "text": "this is your constraint and and depending on that um uh it depends uh which which model you can actually run",
    "start": "929959",
    "end": "937519"
  },
  {
    "text": "uh for inference um however it's not as simple as that of course if you are for example using your uh model in",
    "start": "937519",
    "end": "944639"
  },
  {
    "text": "production and you have uh multiple users concurrently accessing your model and you need to scale um it means that",
    "start": "944639",
    "end": "952319"
  },
  {
    "text": "you would need uh to run more uh instances uh of your model concurrently",
    "start": "952319",
    "end": "958480"
  },
  {
    "text": "means that you would need to have more to use more GPU uh but this number is for example if you look at this largest",
    "start": "958480",
    "end": "964920"
  },
  {
    "text": "Motel here 400 b um one you see that it it requires um",
    "start": "964920",
    "end": "971040"
  },
  {
    "text": "810 GB uh which which which won't even fit",
    "start": "971040",
    "end": "976240"
  },
  {
    "text": "into one GPU and you already probably heard that this GPU is pretty expensive",
    "start": "976240",
    "end": "981839"
  },
  {
    "text": "um in order to use this model you would need um eight gpus the this most",
    "start": "981839",
    "end": "987720"
  },
  {
    "text": "expensive gpus and and then you would need two machines like that just to run this uh large uh lama lama 3.1 model so",
    "start": "987720",
    "end": "996920"
  },
  {
    "text": "the situations with fine tuning is a little bit more complex uh because um",
    "start": "996920",
    "end": "1002000"
  },
  {
    "text": "when you are running inference you are only um using your memory for the forward pass uh for basically generating",
    "start": "1002000",
    "end": "1009800"
  },
  {
    "text": "predictions uh when you are doing fine tuning you would also need memory for the backward um propagation um and then",
    "start": "1009800",
    "end": "1018399"
  },
  {
    "text": "for storing the entire badge because you are actually training in badge um um and",
    "start": "1018399",
    "end": "1024880"
  },
  {
    "text": "then also some memory we going to be used for the optimizer which is used and for other U ilitary purposes but",
    "start": "1024880",
    "end": "1030918"
  },
  {
    "text": "basically all of that means simply that you just need a lot more uh memory for",
    "start": "1030919",
    "end": "1036839"
  },
  {
    "text": "fine tuning so for example if we look at what amount of memory would be needed for full fine tuning of the largest lama",
    "start": "1036839",
    "end": "1044400"
  },
  {
    "text": "lama model you would see that basically it's going to be six nodes like that uh which",
    "start": "1044400",
    "end": "1049559"
  },
  {
    "text": "um uh brings us to this famous meme about how I can ever do this um and uh",
    "start": "1049559",
    "end": "1057280"
  },
  {
    "text": "on the right uh hand and left hand sides you actually see people that left hand",
    "start": "1057280",
    "end": "1062679"
  },
  {
    "text": "one is like non experts at all on the on the right hand side is like experts and",
    "start": "1062679",
    "end": "1068039"
  },
  {
    "text": "in the middle you have basically the majority the majority of of companies and and teams they actually do all kind",
    "start": "1068039",
    "end": "1073679"
  },
  {
    "text": "of optimizations in order to reduce this memory and we'll talk about that in a moment but if you look at the those",
    "start": "1073679",
    "end": "1079039"
  },
  {
    "text": "experts and known experts you're going to see that you simply have to buy more gpus that's the the hard CH that you",
    "start": "1079039",
    "end": "1084799"
  },
  {
    "text": "need to know right however um uh you don't have to be on those uh end of",
    "start": "1084799",
    "end": "1091200"
  },
  {
    "text": "spectrum uh there are a lot of optimizations but we're going to talk about two most important ones um so",
    "start": "1091200",
    "end": "1097120"
  },
  {
    "text": "first one is quantization as I told you a model uh basically consists of layers",
    "start": "1097120",
    "end": "1103159"
  },
  {
    "text": "um and each layer is basically just think of it as like a taner it's it's a it's a multi-dimensional metrix um",
    "start": "1103159",
    "end": "1111400"
  },
  {
    "text": "and um basically those are uh numbers that use this FP let's say 16 uh float",
    "start": "1111400",
    "end": "1119080"
  },
  {
    "text": "point2 half Precision format and you can understand that it takes memory and this",
    "start": "1119080",
    "end": "1125039"
  },
  {
    "text": "is exactly what memory is used for so in order to run inference or fine tuning you have to load your moral into the GPU",
    "start": "1125039",
    "end": "1131400"
  },
  {
    "text": "memory and that's how it works so the bigger the morrow is the more memory it takes so there are certain tricks that",
    "start": "1131400",
    "end": "1138120"
  },
  {
    "text": "can sign signicantly reduce the amount of memory which you need so and one of them is quantization um instead of",
    "start": "1138120",
    "end": "1145960"
  },
  {
    "text": "storing the uh full Precision you you converting this fla point to int and uh",
    "start": "1145960",
    "end": "1153480"
  },
  {
    "text": "by that you basically lowering the rank um and with this lower lower DEC um uh",
    "start": "1153480",
    "end": "1159799"
  },
  {
    "text": "Precision uh it takes less um um memory",
    "start": "1159799",
    "end": "1164880"
  },
  {
    "text": "uh for inference and for fine fine tuning as well um and and this is um a",
    "start": "1164880",
    "end": "1170360"
  },
  {
    "text": "pretty research topic um because you lowering the Precision uh there's a loss",
    "start": "1170360",
    "end": "1175679"
  },
  {
    "text": "in uh the quality of the prediction however this loss is not as um significant uh and in most cases you",
    "start": "1175679",
    "end": "1183520"
  },
  {
    "text": "actually can just dismiss that uh however of course there are cases when you you cannot do that but if you for",
    "start": "1183520",
    "end": "1189039"
  },
  {
    "text": "example look at the Lama 3.1 release you'll see that um they actually recommend you to use fp8 which is a",
    "start": "1189039",
    "end": "1196240"
  },
  {
    "text": "quantized version uh whiches doesn't have much loss at all so basically um",
    "start": "1196240",
    "end": "1203039"
  },
  {
    "text": "now if you apply that to these numbers you can see that if you basically cut",
    "start": "1203039",
    "end": "1208240"
  },
  {
    "text": "the Precision twice and you go from FP 16 to fp8 you linearly basically reduce",
    "start": "1208240",
    "end": "1214760"
  },
  {
    "text": "the amount of memory needed and if you go uh further and for example switch the model to in4 Precision then uh basically",
    "start": "1214760",
    "end": "1223039"
  },
  {
    "text": "it's uh significant uh drop uh in memory required so now if you look at uh let's",
    "start": "1223039",
    "end": "1229159"
  },
  {
    "text": "say 70 B uh billion moral um you would need just one GPU and it it will just",
    "start": "1229159",
    "end": "1236480"
  },
  {
    "text": "fit um so what about the fine tuning um there's another technique uh which",
    "start": "1236480",
    "end": "1242640"
  },
  {
    "text": "pretty useful uh especially in both inference and fine tuning which is called as low rank adaptation uh and",
    "start": "1242640",
    "end": "1250799"
  },
  {
    "text": "this is how it works uh so think of this um um um let's say",
    "start": "1250799",
    "end": "1257600"
  },
  {
    "text": "uh model weights uh set of tensors so",
    "start": "1257600",
    "end": "1263039"
  },
  {
    "text": "you divide the this this um weights into two parts um pre-trained weights and you",
    "start": "1263039",
    "end": "1270039"
  },
  {
    "text": "call them freezed Frozen weights and you're saying we're not going to use those um uh weights for training we're",
    "start": "1270039",
    "end": "1277520"
  },
  {
    "text": "going to only set a take a subset of this weights and only use this smaller",
    "start": "1277520",
    "end": "1282960"
  },
  {
    "text": "much smaller subset of weights for fine tuning and we only load those this uh",
    "start": "1282960",
    "end": "1288360"
  },
  {
    "text": "this smaller set of Weights um and this is how you reduce the amount of memory required for fine tuning instead of",
    "start": "1288360",
    "end": "1295320"
  },
  {
    "text": "loading the whole weights you load in only this what is called adapter weights",
    "start": "1295320",
    "end": "1300520"
  },
  {
    "text": "um and then you find in adapter weights um um and once you've trained the model",
    "start": "1300520",
    "end": "1306520"
  },
  {
    "text": "then you can merge them together uh and this is how you get a fine-tune version",
    "start": "1306520",
    "end": "1312000"
  },
  {
    "text": "of a model without using all the weights so uh for short it called uh Laura and",
    "start": "1312000",
    "end": "1317799"
  },
  {
    "text": "this technique is pretty notable not only because you can use it for fine tuning but also for uh inference imagine",
    "start": "1317799",
    "end": "1325320"
  },
  {
    "text": "that you'd like to um um actually use multiple models multiple fine tune",
    "start": "1325320",
    "end": "1331320"
  },
  {
    "text": "models on the same GPU um if you are not using Laura you would have to um load",
    "start": "1331320",
    "end": "1337240"
  },
  {
    "text": "each model every time all the weights however when you are using Laura you can uh load pre-trained weights once and",
    "start": "1337240",
    "end": "1344799"
  },
  {
    "text": "then switch adapters from different models uh and this way you can actually run inference for multiple models um",
    "start": "1344799",
    "end": "1351679"
  },
  {
    "text": "however if we look at how Laura uh is applied for fine tuning we're going to see that it significantly reduces the",
    "start": "1351679",
    "end": "1357799"
  },
  {
    "text": "amount of memory needed for tuning and there's a also a lot of like a big body",
    "start": "1357799",
    "end": "1363360"
  },
  {
    "text": "of research how this affects the quality um so now if we combine both techniques",
    "start": "1363360",
    "end": "1368440"
  },
  {
    "text": "we're going to see that um by using uh quantization and Laura uh we can go even",
    "start": "1368440",
    "end": "1376120"
  },
  {
    "text": "even even further and this is basically how you actually train and run inference without having a lot of",
    "start": "1376120",
    "end": "1382840"
  },
  {
    "text": "gpus um right now um let's um let's um",
    "start": "1382840",
    "end": "1388440"
  },
  {
    "text": "go into their actual development process like from start to end just to give get",
    "start": "1388440",
    "end": "1393760"
  },
  {
    "text": "a rough sense of what it like what it is to to actually get a model and we can um",
    "start": "1393760",
    "end": "1400000"
  },
  {
    "text": "um split the whole process into several parts first one is uh pre-processing",
    "start": "1400000",
    "end": "1406120"
  },
  {
    "text": "this is where you collect all the data and process this data prepared for the training then there is pre-training this",
    "start": "1406120",
    "end": "1412679"
  },
  {
    "text": "is where you take this bulk data you have and you just train your model",
    "start": "1412679",
    "end": "1418240"
  },
  {
    "text": "without any specific assistance here um and once the model learns the basic",
    "start": "1418240",
    "end": "1424520"
  },
  {
    "text": "knowledge from this data um then you can go to the Post trining phase where now",
    "start": "1424520",
    "end": "1431279"
  },
  {
    "text": "you can um educate your model or another way to say it would be to align your",
    "start": "1431279",
    "end": "1437360"
  },
  {
    "text": "model with specific t task and this is how you make the model work for very specific tasks to follow make it to",
    "start": "1437360",
    "end": "1443320"
  },
  {
    "text": "follow instructions at all um so this is a very complex area and there are so many different approaches to that if you",
    "start": "1443320",
    "end": "1450840"
  },
  {
    "text": "read uh if you want to learn more about this you would probably read some technical reports every model when when",
    "start": "1450840",
    "end": "1456360"
  },
  {
    "text": "somebody releases a new model there is typically a technical report which goes into the very specific process how this",
    "start": "1456360",
    "end": "1462960"
  },
  {
    "text": "model was uh how data was prepared how how it was pre pre-trained and then how",
    "start": "1462960",
    "end": "1468559"
  },
  {
    "text": "it was aligned um and um one example uh",
    "start": "1468559",
    "end": "1473600"
  },
  {
    "text": "that might be interesting to know is like super supervised fine tuning basically when you train a large model",
    "start": "1473600",
    "end": "1479640"
  },
  {
    "text": "you basically give it bug data like internet kind of data Reddit data um and",
    "start": "1479640",
    "end": "1485720"
  },
  {
    "text": "um and then you uh want to switch from like just pre-training to supervised uh",
    "start": "1485720",
    "end": "1492000"
  },
  {
    "text": "training where you give it very specific uh curated data sets so it starts to",
    "start": "1492000",
    "end": "1497880"
  },
  {
    "text": "learn how high quality um uh data and this is called uh Super Wise fine tuning",
    "start": "1497880",
    "end": "1503720"
  },
  {
    "text": "where you prepare this additional data sets um this is typically done by uh",
    "start": "1503720",
    "end": "1509399"
  },
  {
    "text": "done after the the base motor is trained um another interesting thing",
    "start": "1509399",
    "end": "1516679"
  },
  {
    "text": "here is uh and sometimes it can be used either in addition or instead of",
    "start": "1516679",
    "end": "1522120"
  },
  {
    "text": "supervis fine tuning is uh now that we have those large Motors we can actually use U very high high quality models for",
    "start": "1522120",
    "end": "1529520"
  },
  {
    "text": "generating the pre-trained data pre-training data and this is called synthetic synthetic data and uh for",
    "start": "1529520",
    "end": "1537279"
  },
  {
    "text": "example uh there are propietary models like clot which allow you to generate uh",
    "start": "1537279",
    "end": "1542880"
  },
  {
    "text": "data sets which you can later use for pre-training your model and if you for",
    "start": "1542880",
    "end": "1548919"
  },
  {
    "text": "example look at the technical report of Lama 3.1 or some other models you're going to basically see there that the",
    "start": "1548919",
    "end": "1555399"
  },
  {
    "text": "team actually generated a lot of synthetic data to train it on so it's",
    "start": "1555399",
    "end": "1560559"
  },
  {
    "text": "not only this lowquality internet data it is actually high quality generated data generated by most expensive",
    "start": "1560559",
    "end": "1568120"
  },
  {
    "text": "llms right and of course uh this would be um uh strange if we don't mention uh",
    "start": "1568120",
    "end": "1574880"
  },
  {
    "text": "rlf this is reinforcement learning human from Human feedback this is the main",
    "start": "1574880",
    "end": "1580080"
  },
  {
    "text": "technique how you uh make the model not only generate text but actually follow",
    "start": "1580080",
    "end": "1585480"
  },
  {
    "text": "instructions and um the main trick here is that instead of just giving it text",
    "start": "1585480",
    "end": "1591720"
  },
  {
    "text": "that it can learn to generate it also learns whether the generated text is",
    "start": "1591720",
    "end": "1597320"
  },
  {
    "text": "good or bad and then you basically come up with some labels like good generation",
    "start": "1597320",
    "end": "1602520"
  },
  {
    "text": "bad generation or a number like from0 to 10 how great their generation uh text",
    "start": "1602520",
    "end": "1610200"
  },
  {
    "text": "generation is so the model can learn from this feedback to to not generate",
    "start": "1610200",
    "end": "1615279"
  },
  {
    "text": "bad um results and always gener generate good ones and uh in general this works",
    "start": "1615279",
    "end": "1621880"
  },
  {
    "text": "through the reinforcement learning where you actually train a morel which is um",
    "start": "1621880",
    "end": "1627320"
  },
  {
    "text": "which um learns how to score like what what is good what is bad result and then",
    "start": "1627320",
    "end": "1633000"
  },
  {
    "text": "you've trained this model and mor learned that then the model is used uh when you actually like uh uh post Trin",
    "start": "1633000",
    "end": "1640600"
  },
  {
    "text": "the model uh this is a bit complicated process because you first have to train",
    "start": "1640600",
    "end": "1645840"
  },
  {
    "text": "this reward model and only then you actually use that reward model in an actual",
    "start": "1645840",
    "end": "1651559"
  },
  {
    "text": "training and um because this process is a bit complex there's an alternative to that known as DPO like direct preference",
    "start": "1651559",
    "end": "1659240"
  },
  {
    "text": "optimization where instead of training this intermediate reward model you uh",
    "start": "1659240",
    "end": "1664880"
  },
  {
    "text": "would directly uh provide um just provide this label data",
    "start": "1664880",
    "end": "1671720"
  },
  {
    "text": "and um and the the uh trainer Will Will just use it without training intermediate model so uh of course I'm",
    "start": "1671720",
    "end": "1679840"
  },
  {
    "text": "I'm just giving you an overview and you if you are interest interested in this you would you would go and and uh uh",
    "start": "1679840",
    "end": "1686240"
  },
  {
    "text": "read in more detail about how this works um wrapping it up um I would like also",
    "start": "1686240",
    "end": "1692519"
  },
  {
    "text": "to mention um Frameworks and tools uh that are very important and as I",
    "start": "1692519",
    "end": "1698200"
  },
  {
    "text": "previously said that um I'm not here to to provide Aller let's say uh hacks how",
    "start": "1698200",
    "end": "1704880"
  },
  {
    "text": "to leverage open source models in production but rather give you um um intuition of what it feels and what",
    "start": "1704880",
    "end": "1711679"
  },
  {
    "text": "tools you can use um so typically when you are into open source models um there",
    "start": "1711679",
    "end": "1718080"
  },
  {
    "text": "are different uh approaches one approach is when you actually want to go deeper",
    "start": "1718080",
    "end": "1723240"
  },
  {
    "text": "and that's why that's when you would need researchers and those researchers would go into the architecture of the",
    "start": "1723240",
    "end": "1729760"
  },
  {
    "text": "model and into very specific process and in order to understand how it is done you would need to go and read how other",
    "start": "1729760",
    "end": "1736799"
  },
  {
    "text": "models are trained and this is best to be done by reading those technical",
    "start": "1736799",
    "end": "1741840"
  },
  {
    "text": "reports however in most cases uh we don't have this uh that uh resources to",
    "start": "1741840",
    "end": "1749120"
  },
  {
    "text": "uh be very much involved into the research pre-training we would probably decide to um focus on um on less",
    "start": "1749120",
    "end": "1758159"
  },
  {
    "text": "expensive Parts um of their development process and rely on um base pre-train",
    "start": "1758159",
    "end": "1765840"
  },
  {
    "text": "models and rely on uh all kind of uh tools if we go back a little bit here",
    "start": "1765840",
    "end": "1771360"
  },
  {
    "text": "and we're going to see that um after we've done this post training there's this um stage uh called",
    "start": "1771360",
    "end": "1779559"
  },
  {
    "text": "optimization you already trained the model so now you want to uh use that model in production for example for",
    "start": "1779559",
    "end": "1785440"
  },
  {
    "text": "inference or further F training for example and um today there are enough",
    "start": "1785440",
    "end": "1791039"
  },
  {
    "text": "tools that help you that with that uh so first of all uh let me mention um Cuda",
    "start": "1791039",
    "end": "1798039"
  },
  {
    "text": "RM and xcla um when when you want to use open source",
    "start": "1798039",
    "end": "1803120"
  },
  {
    "text": "models you can use accelerators basically gpus however there are different kinds of them there is NVIDIA",
    "start": "1803120",
    "end": "1809799"
  },
  {
    "text": "uh then there is also MD uh um and they actually start to offer some very good",
    "start": "1809799",
    "end": "1816279"
  },
  {
    "text": "uh accelerators that start to compete with Nvidia um and then of course there",
    "start": "1816279",
    "end": "1821600"
  },
  {
    "text": "are other alternative um uh accelerators like Google for example uh offers uh TPU",
    "start": "1821600",
    "end": "1828600"
  },
  {
    "text": "uh which can be also an alternative and it's good to basically know about this Choice even though there's a lot of",
    "start": "1828600",
    "end": "1835480"
  },
  {
    "text": "Nvidia gpus sometimes you actually want if for example you'd like to use on Prem there might be cases when you can",
    "start": "1835480",
    "end": "1842440"
  },
  {
    "text": "consider using for example IMD or if you for example using Google there are",
    "start": "1842440",
    "end": "1848440"
  },
  {
    "text": "enough cases when it's very good to use TPU and those Cuda ocm and XEL those are",
    "start": "1848440",
    "end": "1854320"
  },
  {
    "text": "different basically drivers uh for NVIDIA you would use Cuda for MD you would use RM and for TPU for example you",
    "start": "1854320",
    "end": "1861480"
  },
  {
    "text": "would use xlaa um and uh simply because there's a team behind those drivers",
    "start": "1861480",
    "end": "1868120"
  },
  {
    "text": "they've done um enormous number of optimizations which you don't have to",
    "start": "1868120",
    "end": "1873559"
  },
  {
    "text": "care about there's a dedicate for example if you just take um um um xlaa",
    "start": "1873559",
    "end": "1879679"
  },
  {
    "text": "TPU there's a dedicated team that try to optimize the inference and fine tuning using pie torch uh and you don't even",
    "start": "1879679",
    "end": "1886880"
  },
  {
    "text": "have to think of it you just basically stand on the sh shers of these Giants um",
    "start": "1886880",
    "end": "1893360"
  },
  {
    "text": "without worrying about the low level optimizations things like optimizing carels at all even though of course you",
    "start": "1893360",
    "end": "1899360"
  },
  {
    "text": "can do that uh if you want then there is draw Frameworks for inference for example most known are VM TGI and and",
    "start": "1899360",
    "end": "1907840"
  },
  {
    "text": "and ni me n um what you really what you really need to know is that they are",
    "start": "1907840",
    "end": "1914279"
  },
  {
    "text": "they slightly different however they have a lot of commonality between each other and they offer pretty much",
    "start": "1914279",
    "end": "1921519"
  },
  {
    "text": "everything that you would need for inference and you might heard many different optimizations speculative",
    "start": "1921519",
    "end": "1926679"
  },
  {
    "text": "decoding batching um and other uh optimizations that they are already built in um so you",
    "start": "1926679",
    "end": "1933639"
  },
  {
    "text": "don't have to worry about this at all and bl mtgi they are crossplatform uh n",
    "start": "1933639",
    "end": "1939720"
  },
  {
    "text": "is only Nvidia when it comes to training um TRL",
    "start": "1939720",
    "end": "1945200"
  },
  {
    "text": "this is the most known uh let's say framework this is by hugging phas and um",
    "start": "1945200",
    "end": "1950880"
  },
  {
    "text": "it uh it helps you uh do RF I'm sorry",
    "start": "1950880",
    "end": "1956120"
  },
  {
    "text": "this reinforcement learning from Human feedback supervised fine tuning and DP and it has all sorts of optimizations",
    "start": "1956120",
    "end": "1962840"
  },
  {
    "text": "for fine tuning um um which you also don't have to uh worry about this is",
    "start": "1962840",
    "end": "1968120"
  },
  {
    "text": "very um good developer experience Library so I totally recommend if you want to find you in the most uh one of",
    "start": "1968120",
    "end": "1974880"
  },
  {
    "text": "the most recent llms you would just go to tal there is a lot of different tutorials examples how to use it it's",
    "start": "1974880",
    "end": "1980720"
  },
  {
    "text": "pretty easy to use and finally there's this um Axel it's a rer uh around uh",
    "start": "1980720",
    "end": "1987799"
  },
  {
    "text": "basically tools like TRL which makes it much simpler to fine tune it's basically",
    "start": "1987799",
    "end": "1993600"
  },
  {
    "text": "a framework for fine tuning and um uh most of the time if you want to do very classical typical uh fine tuning you",
    "start": "1993600",
    "end": "2001080"
  },
  {
    "text": "would just go with Axel atal so finally concluding this with this tag this tag is a is a uh project which my team is",
    "start": "2001080",
    "end": "2008399"
  },
  {
    "text": "working on uh and maybe just providing a few uh insight about this one so DC is a",
    "start": "2008399",
    "end": "2014480"
  },
  {
    "text": "container orchestrator which is vendor agnostic means that it can run on any accelerator on any Cloud on Prem think",
    "start": "2014480",
    "end": "2021639"
  },
  {
    "text": "of it as Docker except that it's interface is designed for AI researchers to simplify the development training and",
    "start": "2021639",
    "end": "2029039"
  },
  {
    "text": "deployment so you would probably just Define what you want as a simple yo file and then don't worry about like what",
    "start": "2029039",
    "end": "2036080"
  },
  {
    "text": "cloud provider you use uh or whether you are using on pram you would basically be able to run any any AI workload without",
    "start": "2036080",
    "end": "2044279"
  },
  {
    "text": "uh going into um into managing of the of the containers yourself so with that",
    "start": "2044279",
    "end": "2051240"
  },
  {
    "text": "happy to answer",
    "start": "2051240",
    "end": "2054118"
  },
  {
    "text": "[Applause]",
    "start": "2056750",
    "end": "2063040"
  },
  {
    "text": "questions do we have any questions",
    "start": "2063040",
    "end": "2067839"
  },
  {
    "text": "I thanks for a nice presentation so regarding the first slide about the necessity of the fine-tuned models so if",
    "start": "2069960",
    "end": "2077480"
  },
  {
    "text": "we would talk about the big llms I would say so that uh and not the high Lots",
    "start": "2077480",
    "end": "2087320"
  },
  {
    "text": "from the um in terms of the consumption of the models does it makes sense to",
    "start": "2087320",
    "end": "2093760"
  },
  {
    "text": "just go into this Rabbit Hole of fine tuning rather than just selecting the more",
    "start": "2093760",
    "end": "2100640"
  },
  {
    "text": "pricey model just spend a little bit more for",
    "start": "2100640",
    "end": "2106040"
  },
  {
    "text": "tokens and yeah and just don't do uh",
    "start": "2106040",
    "end": "2111800"
  },
  {
    "text": "that thing so what are criteria so maybe you just partially answered that on the first slide uh but still so what in is",
    "start": "2111800",
    "end": "2119320"
  },
  {
    "text": "in your opinion the criteria or what our criteria to just do the fine-tuning",
    "start": "2119320",
    "end": "2125880"
  },
  {
    "text": "rather than using maybe prop prety maybe open source so a bigger model hosted",
    "start": "2125880",
    "end": "2132000"
  },
  {
    "text": "somewhere I don't know Gro or something and pay just yeah thank you um thank you",
    "start": "2132000",
    "end": "2139640"
  },
  {
    "text": "thank you for the question maybe I'll just repeat that uh the and uh As I",
    "start": "2139640",
    "end": "2144880"
  },
  {
    "text": "understood it when do we need to fine tune a model",
    "start": "2144880",
    "end": "2150520"
  },
  {
    "text": "when should we use simply a bigger model when should we use uh instead of an open",
    "start": "2150520",
    "end": "2156839"
  },
  {
    "text": "source model when we should use a properti model correct um so yeah there",
    "start": "2156839",
    "end": "2162040"
  },
  {
    "text": "are two two two questions uh number one is when we need to F tune or we we can",
    "start": "2162040",
    "end": "2168680"
  },
  {
    "text": "use the um the the model which is already like a larger model let's say um",
    "start": "2168680",
    "end": "2175760"
  },
  {
    "text": "I mean uh I'm afraid that I'll give I I'll give maybe a bit of generic answer here but um if you if you ever face that",
    "start": "2175760",
    "end": "2182720"
  },
  {
    "text": "situation in the reality you would quickly understand that uh this highly",
    "start": "2182720",
    "end": "2187880"
  },
  {
    "text": "depends on the resources which you have at hand and then the necessity to reduce",
    "start": "2187880",
    "end": "2194000"
  },
  {
    "text": "uh the costs uh and uh most of the time",
    "start": "2194000",
    "end": "2199839"
  },
  {
    "text": "uh a team starts with a something that provides you kind of a baseline of the",
    "start": "2199839",
    "end": "2205359"
  },
  {
    "text": "quality and you see okay so this Motel actually does exactly what I want so now",
    "start": "2205359",
    "end": "2210920"
  },
  {
    "text": "let me think how to optimize it and this is also a chance to go into this premature",
    "start": "2210920",
    "end": "2216800"
  },
  {
    "text": "optimization topic but basically the idea is that um now that you know that this model uh works for you and",
    "start": "2216800",
    "end": "2225040"
  },
  {
    "text": "sometimes it might be even a proprietary model you basically take open AI you use it um and then you see that it works so",
    "start": "2225040",
    "end": "2232319"
  },
  {
    "text": "now you're basically thinking okay so how do I make it work given my resources and then you realize that the only way",
    "start": "2232319",
    "end": "2239280"
  },
  {
    "text": "to do it would be to fine tune and this is going to be experimentation anyway you would probably do several different",
    "start": "2239280",
    "end": "2245480"
  },
  {
    "text": "appro approaches and you would compare two options basically you try to fine tune and then you're going to see how",
    "start": "2245480",
    "end": "2252760"
  },
  {
    "text": "better performance you have and then you compare and basically then you just choose um between what what you have if",
    "start": "2252760",
    "end": "2259920"
  },
  {
    "text": "a smaller model which you fun yourself uh getting you where you want then of",
    "start": "2259920",
    "end": "2265960"
  },
  {
    "text": "course you would use it because uh it will um reduce the cost",
    "start": "2265960",
    "end": "2271040"
  },
  {
    "text": "uh um and for fine-tuning uh the fine tuning cost are a lot less",
    "start": "2271040",
    "end": "2278599"
  },
  {
    "text": "than the inference costs um especially if you are into fine-tuning fine tuning",
    "start": "2278599",
    "end": "2284440"
  },
  {
    "text": "is done um let's say um from time to time but inference is done every every",
    "start": "2284440",
    "end": "2291280"
  },
  {
    "text": "let's say second it depends of course on the scale um but uh you would always",
    "start": "2291280",
    "end": "2297240"
  },
  {
    "text": "optimize for inference um that means that sometimes you actually have to fine tune and",
    "start": "2297240",
    "end": "2303599"
  },
  {
    "text": "that's the the best way the more the better the fine tune in step going to be",
    "start": "2303599",
    "end": "2308760"
  },
  {
    "text": "of course the less cost going to be for the inference um and maybe final one uh answering about the how do I choose",
    "start": "2308760",
    "end": "2315960"
  },
  {
    "text": "between a properti model and open source model um it's not even about the quality",
    "start": "2315960",
    "end": "2321440"
  },
  {
    "text": "of the model it's always about whether you are allowed to use the proprietary",
    "start": "2321440",
    "end": "2327480"
  },
  {
    "text": "one or not so that that that's one and it's it's a separate question probably",
    "start": "2327480",
    "end": "2334079"
  },
  {
    "text": "there are other concerns um um if you don't if you can get where you want by",
    "start": "2334079",
    "end": "2340240"
  },
  {
    "text": "using the proprietary one you you should go there um hope hope that",
    "start": "2340240",
    "end": "2346640"
  },
  {
    "text": "helps um yeah uh so two different questions probably one around the use",
    "start": "2347680",
    "end": "2355000"
  },
  {
    "text": "cases in the market today in which um field and and what what are the most",
    "start": "2355000",
    "end": "2361640"
  },
  {
    "text": "used use cases today that the AI is used according to your uh observations",
    "start": "2361640",
    "end": "2370280"
  },
  {
    "text": "and the second question is what are the observability tools which you can we can",
    "start": "2370280",
    "end": "2375480"
  },
  {
    "text": "enable to measure how performant a model is are there any tools known to the",
    "start": "2375480",
    "end": "2381839"
  },
  {
    "text": "market so that it measures the performance for example of the response how the model so that we also think",
    "start": "2381839",
    "end": "2389079"
  },
  {
    "text": "about Autos scaling probably yeah um",
    "start": "2389079",
    "end": "2394160"
  },
  {
    "text": "thank you yeah uh two challenging questions actually first one about use cases um and for certain reason I really",
    "start": "2394160",
    "end": "2402280"
  },
  {
    "text": "find it hard to answer uh what are the use cases um I guess everybody is now",
    "start": "2402280",
    "end": "2408000"
  },
  {
    "text": "trying to figure this out uh and based on what I've seen there's no single",
    "start": "2408000",
    "end": "2413480"
  },
  {
    "text": "cluster of use cases uh it's basically everywhere uh there are companies that that use LMS to generate close closes",
    "start": "2413480",
    "end": "2421440"
  },
  {
    "text": "design uh there are companies that actually use it for for like food design",
    "start": "2421440",
    "end": "2428040"
  },
  {
    "text": "um and and if we if we make this list of course uh at the top we're going to have those chat Bots which everybody's",
    "start": "2428040",
    "end": "2434040"
  },
  {
    "text": "talking about and then all kind of C co-pilot and then if you take every industry like financial industry healthc",
    "start": "2434040",
    "end": "2440240"
  },
  {
    "text": "care industry um whatnot and then there will be always those chat Bots and",
    "start": "2440240",
    "end": "2445560"
  },
  {
    "text": "co-pilots but but then um at least what I can speculate about personally is that",
    "start": "2445560",
    "end": "2452200"
  },
  {
    "text": "um it's going to be a rabbit hole and we're going to see more and more use cases and whenever we we we look we're",
    "start": "2452200",
    "end": "2458280"
  },
  {
    "text": "going to see those use cases uh basically everywhere that's why I mentioned um when I was talking about",
    "start": "2458280",
    "end": "2465599"
  },
  {
    "text": "the impact of AI and why companies should really consider investing more into their competitive like make this",
    "start": "2465599",
    "end": "2472839"
  },
  {
    "text": "gen part of the competitive Advantage is that it's going to basically um affect uh all all their all",
    "start": "2472839",
    "end": "2481079"
  },
  {
    "text": "the use cases in a way that that that's why it's much easier might be to answer which use cases are not going to be",
    "start": "2481079",
    "end": "2488160"
  },
  {
    "text": "affected uh then we have more freedom let's say to think of it so and and then we can brainstorm okay so let's come up",
    "start": "2488160",
    "end": "2494839"
  },
  {
    "text": "with 10 10 use cases which geni not going to affect which will be much",
    "start": "2494839",
    "end": "2500079"
  },
  {
    "text": "easier to come up with rather than coming up with which use cases but of course there's a well",
    "start": "2500079",
    "end": "2506480"
  },
  {
    "text": "um um uh the problem with this is that uh the the the question is not of course",
    "start": "2506480",
    "end": "2511920"
  },
  {
    "text": "like which use cases um um going to be affected or not you probably would would like to know uh",
    "start": "2511920",
    "end": "2518640"
  },
  {
    "text": "which use cases I can already use LM for now right uh which is a totally different uh topic and this is why we",
    "start": "2518640",
    "end": "2525200"
  },
  {
    "text": "need R&D research and development that that's why you need to take your use case you need to take those llms and",
    "start": "2525200",
    "end": "2531000"
  },
  {
    "text": "then you need to do some research and experiments and without actually doing that you never know whether this",
    "start": "2531000",
    "end": "2537440"
  },
  {
    "text": "particular use case going to going to work um so now um again this is a bit of",
    "start": "2537440",
    "end": "2543079"
  },
  {
    "text": "generic answer but uh getting back to evaluations um again going to disappoint",
    "start": "2543079",
    "end": "2549599"
  },
  {
    "text": "you a bit um everybody asks about like obser observability and hey how can I",
    "start": "2549599",
    "end": "2556640"
  },
  {
    "text": "solve observability and they think that now somebody will tell them and then finally",
    "start": "2556640",
    "end": "2563880"
  },
  {
    "text": "they know and then they'll tell everybody else nobody knows but now this guy actually will tell and then",
    "start": "2563880",
    "end": "2570160"
  },
  {
    "text": "everything is clear now the everybody keeps on on on thinking of evaluations",
    "start": "2570160",
    "end": "2575240"
  },
  {
    "text": "because it's a hard problem it is a hard problem and we have um certain tools for",
    "start": "2575240",
    "end": "2581720"
  },
  {
    "text": "evolation but whenever you look eii researchers or developers AI developers",
    "start": "2581720",
    "end": "2587440"
  },
  {
    "text": "you're going to hear the same story uh we don't have enough uh good evaluation",
    "start": "2587440",
    "end": "2592920"
  },
  {
    "text": "tools that that's why we need to improve them and uh those benchmarks is a one",
    "start": "2592920",
    "end": "2598640"
  },
  {
    "text": "way another way sometimes so it depends on the use case but but sometimes you",
    "start": "2598640",
    "end": "2603880"
  },
  {
    "text": "actually can now leverage also LMS as judge as a judge for evaluating your",
    "start": "2603880",
    "end": "2611000"
  },
  {
    "text": "llm for example whenever you have an expensive llm and less expensive LM you",
    "start": "2611000",
    "end": "2616920"
  },
  {
    "text": "can always ask a more expensive LM to judge it um I mean ideal situation you would",
    "start": "2616920",
    "end": "2622720"
  },
  {
    "text": "involve human uh however you can save cost well as it turns out LMS are",
    "start": "2622720",
    "end": "2630200"
  },
  {
    "text": "cheaper than human um so that's why you can actually use LMS as well but but",
    "start": "2630200",
    "end": "2635960"
  },
  {
    "text": "finally of course there are so many observability tools right now in the market you you go and they somehow help",
    "start": "2635960",
    "end": "2642680"
  },
  {
    "text": "you track metric but in the end those are just metrics [Music]",
    "start": "2642680",
    "end": "2657369"
  }
]