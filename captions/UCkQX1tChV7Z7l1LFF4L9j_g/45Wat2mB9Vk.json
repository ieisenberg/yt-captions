[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "so my name is dan and I work on the Big Data Platform and today we're gonna be talking a little bit about our",
    "start": "3970",
    "end": "9010"
  },
  {
    "text": "cloud-based analytics infrastructure just to give you a quick overview what",
    "start": "9010",
    "end": "14200"
  },
  {
    "text": "we're gonna talk about first is just what kind of data we're talking about then we'll go over some of the metrics",
    "start": "14200",
    "end": "19750"
  },
  {
    "text": "around our scale walk through the architecture of our platform in the cloud and then the two main topics we'll",
    "start": "19750",
    "end": "26320"
  },
  {
    "text": "be covering is data warehousing and orchestration which is Jeannie and finally we'll have plenty of time",
    "start": "26320",
    "end": "32020"
  },
  {
    "text": "hopefully for Q&A so the first thing I like to say when I talk about data at",
    "start": "32020",
    "end": "37510"
  },
  {
    "start": "33000",
    "end": "168000"
  },
  {
    "text": "Netflix when we talk about analytics it's that actually not the data that most people think of when they think",
    "start": "37510",
    "end": "42760"
  },
  {
    "text": "about Netflix so you think of the streaming data the actual video or audio content and that's not what we're",
    "start": "42760",
    "end": "47800"
  },
  {
    "text": "talking about here but we're actually talking about is all of the micro services that we have out in the cloud the client devices marketing campaigns",
    "start": "47800",
    "end": "56020"
  },
  {
    "text": "all sorts of different resources that are funneling data back through our data pipeline to our back-end services so",
    "start": "56020",
    "end": "63370"
  },
  {
    "text": "it's not the video streaming data but it's a lot of other kinds of data and why do we do this well Netflix is",
    "start": "63370",
    "end": "69340"
  },
  {
    "text": "actually a very data-driven company we like to make decisions based on evidence so we don't want to make changes to the",
    "start": "69340",
    "end": "75340"
  },
  {
    "text": "platform that we can't substantiate will actually improve the experience for users and this can be somewhat",
    "start": "75340",
    "end": "81880"
  },
  {
    "text": "counterintuitive because a lot of times you have a feature you think oh everybody's gonna love this but it can actually make it more confusing it can",
    "start": "81880",
    "end": "88300"
  },
  {
    "text": "get in the way of people finding content that they actually want to watch and degrade the experience so we want to",
    "start": "88300",
    "end": "93430"
  },
  {
    "text": "make sure that we we actually do things to improve the platform so one of the",
    "start": "93430",
    "end": "98680"
  },
  {
    "text": "things that we do with this data is a lot of a be testing so we test lots of new features ways of you know navigating",
    "start": "98680",
    "end": "104710"
  },
  {
    "text": "the UI what we show users different algorithms and all of this is done with",
    "start": "104710",
    "end": "109930"
  },
  {
    "text": "different cells just like many companies do right now but the idea is that we can collect all of this data through our pipeline through many different tests",
    "start": "109930",
    "end": "116860"
  },
  {
    "text": "and then we have systems like this we call it our ignite platform but it services this data from our data",
    "start": "116860",
    "end": "122560"
  },
  {
    "text": "warehouse in ways that data scientists can actually evaluate the data and choose the right kind of pushes to make",
    "start": "122560",
    "end": "128200"
  },
  {
    "text": "to our platform another thing we do is we collect information about how well the video is actually streaming so we",
    "start": "128200",
    "end": "134710"
  },
  {
    "text": "publish the ISP speed index this gives you know people around the world a good understanding of how well their ISPs are",
    "start": "134710",
    "end": "141190"
  },
  {
    "text": "performing and what service you should be going to in order to get the best Netflix content another thing that we do",
    "start": "141190",
    "end": "148750"
  },
  {
    "text": "is recommendations so obviously we want to make sure people can find the content that they're going to be interested in",
    "start": "148750",
    "end": "154600"
  },
  {
    "text": "and there's a lot of the work that goes into this and a lot of data that you know is used across different cultures",
    "start": "154600",
    "end": "161560"
  },
  {
    "text": "different countries at this point to come up with the best recommendations for any individual",
    "start": "161560",
    "end": "166840"
  },
  {
    "text": "user so it's not surprising that we frequently say oh that our biggest challenge is scale the amount of data",
    "start": "166840",
    "end": "173920"
  },
  {
    "start": "168000",
    "end": "272000"
  },
  {
    "text": "that we collect grow super linearly with the number of users of our platform so just because we add some number of",
    "start": "173920",
    "end": "180070"
  },
  {
    "text": "millions of users it doesn't mean that we scale linearly we actually collect a lot more data we're doing a lot more a",
    "start": "180070",
    "end": "185590"
  },
  {
    "text": "be testing so we have to be able to scale beyond just what our user user base is scaling to so here are a couple",
    "start": "185590",
    "end": "192280"
  },
  {
    "text": "metrics of Netflix right now so we have more than 86 million users of the",
    "start": "192280",
    "end": "198100"
  },
  {
    "text": "platform we're effectively global we're in almost every country in the world so there's a lot of different cultures",
    "start": "198100",
    "end": "204400"
  },
  {
    "text": "there's a lot of different types of connectivity devices other things that",
    "start": "204400",
    "end": "209500"
  },
  {
    "text": "we have to account for and then we have over a thousand devices that are supported just about everybody in this",
    "start": "209500",
    "end": "215170"
  },
  {
    "text": "room probably has a device that they could pull out and watch netflix on right now so that's a lot of information that we collect along with the millions",
    "start": "215170",
    "end": "222070"
  },
  {
    "text": "of hours that we stream a video content that generates a lot of data that we use for that we collect in our analytics",
    "start": "222070",
    "end": "228790"
  },
  {
    "text": "platform so what is our platform actually look like from a metrics perspective we have more than 500",
    "start": "228790",
    "end": "233860"
  },
  {
    "text": "billion events coming through our kafka data pipeline every day and this comes to a centralized data warehouse our data",
    "start": "233860",
    "end": "241150"
  },
  {
    "text": "warehouse is actually in excess of 60 petabytes right now which is a lot of data but we go through this data very",
    "start": "241150",
    "end": "247450"
  },
  {
    "text": "quickly like the event data that we bring in we typically delete very quickly just because it doesn't have a",
    "start": "247450",
    "end": "252820"
  },
  {
    "text": "lot of meaning beyond say three months in a lot of cases so most of the data gets deleted very quickly but we ETL",
    "start": "252820",
    "end": "258700"
  },
  {
    "text": "that data we enhance it we enrich it and then store it in our data warehouse for analysts to use and it progressively",
    "start": "258700",
    "end": "264550"
  },
  {
    "text": "gets more meaningful so we read about three petabytes a day and we write about 500 tera",
    "start": "264550",
    "end": "272130"
  },
  {
    "start": "272000",
    "end": "463000"
  },
  {
    "text": "so what does our architecture look like so we have two pipelines for how we get",
    "start": "272170",
    "end": "278330"
  },
  {
    "text": "data into our warehouse one is the event data so these are all those you know little events that I was talking about",
    "start": "278330",
    "end": "283640"
  },
  {
    "text": "log messages things and that goes through our Kafka data pipeline coming from all the various services we have in",
    "start": "283640",
    "end": "289160"
  },
  {
    "text": "the cloud and many other sources and then we have a process called Ursula and what ERISA does is it just splits these",
    "start": "289160",
    "end": "294650"
  },
  {
    "text": "events and stages it within our warehouse and based on the event type and then also pulls together and merges",
    "start": "294650",
    "end": "300620"
  },
  {
    "text": "the files so they're big enough to do big data processing on top of the other pipeline is our dimension data so if",
    "start": "300620",
    "end": "306500"
  },
  {
    "text": "you're interacting with a device in Netflix you're likely talking to a Cassandra cluster at some point or another keeping track of you know where",
    "start": "306500",
    "end": "312500"
  },
  {
    "text": "you are in a show or serving new content so the Cassandra cluster holds the dimension data and what we do is we use",
    "start": "312500",
    "end": "319160"
  },
  {
    "text": "the backups from Cassandra and agathis which is an open source project we created to suck that data into our data",
    "start": "319160",
    "end": "325520"
  },
  {
    "text": "warehouse for the dimension data and then you have both your dimension and back data that you can do all sorts of analytics on top of so the actual",
    "start": "325520",
    "end": "334130"
  },
  {
    "text": "platform itself from the compute perspective we're going to walk through this so at the very lowest layer we have s3 which is where we have our source of",
    "start": "334130",
    "end": "341840"
  },
  {
    "text": "truth data for our warehouse and then we have Parque which is our predominant file format which we use for performance",
    "start": "341840",
    "end": "348400"
  },
  {
    "text": "we'll be talking about this more specifically when we talk about data warehousing on top of that sits all of",
    "start": "348400",
    "end": "355160"
  },
  {
    "text": "our compute layers so we run EMR clusters typically everything on top of yarn with the exception of you know",
    "start": "355160",
    "end": "360920"
  },
  {
    "text": "druid and presto which are their own you know separate clusters but we have a wide variety of compute tools on top of",
    "start": "360920",
    "end": "368000"
  },
  {
    "text": "that we have an orchestration layer services that we use to run all of the services across our big data platform",
    "start": "368000",
    "end": "374540"
  },
  {
    "text": "and tom is gonna be covering some of those topics then on top of that we have a whole slew of tools for doing things",
    "start": "374540",
    "end": "380390"
  },
  {
    "text": "like quality checks and visualizing the performance of jobs and understanding what kind of workflows you're actually",
    "start": "380390",
    "end": "386270"
  },
  {
    "text": "executing and finally at the very top for our data scientists our users of the",
    "start": "386270",
    "end": "391340"
  },
  {
    "text": "platform we have an interface so a web interface you can go to and launch queries track job status you know",
    "start": "391340",
    "end": "397760"
  },
  {
    "text": "schedule things as well as an API so anybody can execute this just with Python",
    "start": "397760",
    "end": "403490"
  },
  {
    "text": "don't have to have any of the actual clients installed on their local machines or in a container or anything like that they just need a Python API so",
    "start": "403490",
    "end": "411470"
  },
  {
    "text": "what do our clusters look like so right now we've got two large clusters our production cluster is about 2,300 d24",
    "start": "411470",
    "end": "418880"
  },
  {
    "text": "XLS so these are big instances in Amazon they've got a lot of disk storage there's 24 terabytes per instance on",
    "start": "418880",
    "end": "426229"
  },
  {
    "text": "these disks and this will be important as well when we talk about the warehousing the ad hoc cluster has 1,200",
    "start": "426229",
    "end": "432259"
  },
  {
    "text": "this is what people typically will develop different workflows on or if they're just doing some sort of one-off analytic they'll run on this cluster as",
    "start": "432259",
    "end": "439190"
  },
  {
    "text": "well as you know doing some backfill if they need to rerun some data we also have a whole bunch of other smaller",
    "start": "439190",
    "end": "444860"
  },
  {
    "text": "clusters that we use for testing or specific build clusters for certain types of connectivity or workloads and",
    "start": "444860",
    "end": "450650"
  },
  {
    "text": "then presto for our ad hoc use case so if people are just you know doing very",
    "start": "450650",
    "end": "455720"
  },
  {
    "text": "quick analytics iterating on a particular problem they can do that very very quickly with presto",
    "start": "455720",
    "end": "463000"
  },
  {
    "start": "463000",
    "end": "530000"
  },
  {
    "text": "so this launches us into the the first big section which is about data",
    "start": "463000",
    "end": "468169"
  },
  {
    "text": "warehousing on top of s3 and s3 is actually not the most common use for a",
    "start": "468169",
    "end": "474139"
  },
  {
    "text": "data warehouse within the Big Data or at least the open source Big Data community most people run on HDFS and you know",
    "start": "474139",
    "end": "480080"
  },
  {
    "text": "this goes back to the old adage of you know keep your compute and your storage together but we're actually separating",
    "start": "480080",
    "end": "486590"
  },
  {
    "text": "those two so why do we do this well there are a lot of reasons why we use s3 so one is they have lots of nines you",
    "start": "486590",
    "end": "493699"
  },
  {
    "text": "know they take care of the the durability the availability they also have a bunch of features that don't",
    "start": "493699",
    "end": "499940"
  },
  {
    "text": "exist within HDFS so you know HDFS doesn't have versioning they have a trash so it's a little easy to recover a",
    "start": "499940",
    "end": "506960"
  },
  {
    "text": "you know data that was deleted once but in s3 you actually have full vir versioning of all objects if you use",
    "start": "506960",
    "end": "512240"
  },
  {
    "text": "version buckets and so that really helps you know the users of the platform so they don't have to worry about tiptoeing",
    "start": "512240",
    "end": "518750"
  },
  {
    "text": "around data and making sure they don't make a mistake and delete a whole data set we can deal with that we can recover so their velocity increases they don't",
    "start": "518750",
    "end": "525589"
  },
  {
    "text": "have to worry about are they going to accidentally do something very bad and",
    "start": "525589",
    "end": "530800"
  },
  {
    "start": "530000",
    "end": "657000"
  },
  {
    "text": "most importantly that and this underlies our entire deployment is that we deke up",
    "start": "530800",
    "end": "536010"
  },
  {
    "text": "our storage and compute now this is mostly just for the warehouse data obviously within the cluster there's a",
    "start": "536010",
    "end": "541860"
  },
  {
    "text": "lot of locality there's a lot of execution that happens you know between stages of jobs or in shuffles and spark",
    "start": "541860",
    "end": "547949"
  },
  {
    "text": "locality still exists but our actual data warehouse is separate and there's a lot of good reasons for this so one good",
    "start": "547949",
    "end": "554040"
  },
  {
    "text": "reason for decoupling the scaling is our decoupling is is scaling so each one of",
    "start": "554040",
    "end": "559320"
  },
  {
    "text": "these boxes represents a petabyte of data so this is our data warehouse size now if you take a look at all of our",
    "start": "559320",
    "end": "566160"
  },
  {
    "text": "cluster resources all the discs across all of our clusters and you put that together and you account for the",
    "start": "566160",
    "end": "572550"
  },
  {
    "text": "three-factor replication that you would need probably as a minimum with HDFS especially in the cloud because you know",
    "start": "572550",
    "end": "577620"
  },
  {
    "text": "instances they can come and go you might have to terminate them and you don't allow for any buffer at all we don't",
    "start": "577620",
    "end": "584220"
  },
  {
    "text": "have enough capacity in our existing cluster to house our daya warehouse we would need something at least two or",
    "start": "584220",
    "end": "590519"
  },
  {
    "text": "three times the size of our current deployment as large as it is just to hold our data warehouse and then you",
    "start": "590519",
    "end": "595589"
  },
  {
    "text": "have all the complexity that comes along with keeping your storage and compute together and this is a problem that a lot of companies get into where you have",
    "start": "595589",
    "end": "602610"
  },
  {
    "text": "a big Hadoop deployment and all the data is in that cluster and all the jobs are running on that cluster now you need to",
    "start": "602610",
    "end": "607769"
  },
  {
    "text": "do an upgrade and you're trying to migrate people over to the new cluster and nobody wants to go because the data is not there and now you have to figure",
    "start": "607769",
    "end": "612870"
  },
  {
    "text": "out well how do I get the data over to the new cluster how do I sink this how do I make sure everybody's running off there the right data and it really",
    "start": "612870",
    "end": "619649"
  },
  {
    "text": "decreases the ability for the platform to evolve so we can move very very",
    "start": "619649",
    "end": "624690"
  },
  {
    "text": "rapidly we can have different versions of the same you know deployments out and running at the same time we can have",
    "start": "624690",
    "end": "630720"
  },
  {
    "text": "many different versions of the different compute engines running at the same time and this allows us to kind of evolve",
    "start": "630720",
    "end": "636870"
  },
  {
    "text": "independently of the data so that's part",
    "start": "636870",
    "end": "642690"
  },
  {
    "text": "of what I was talking about in terms of decoupling the computing storage each one of these clusters can still use the",
    "start": "642690",
    "end": "647850"
  },
  {
    "text": "production data so even if we're using a test cluster testing out new features we get a used production data for that we",
    "start": "647850",
    "end": "653069"
  },
  {
    "text": "don't have to silo this and keep them separate now of course if you use s3",
    "start": "653069",
    "end": "659010"
  },
  {
    "start": "657000",
    "end": "835000"
  },
  {
    "text": "it's not that it's always going to be better than HDFS there are a number of trade-offs and one of those is is",
    "start": "659010",
    "end": "664649"
  },
  {
    "text": "definitely performance so there's much more latency in",
    "start": "664649",
    "end": "669779"
  },
  {
    "text": "to s3 than there are in HDFS and this can affect jobs in their split calculation specifically so if you have",
    "start": "669779",
    "end": "676589"
  },
  {
    "text": "a lot of directories that you need a list to figure out how to break up your jobs and execute them in parallel this",
    "start": "676589",
    "end": "682589"
  },
  {
    "text": "can take a lot of time however this also runs off cluster so your job is doing most of this work and it's not impacting",
    "start": "682589",
    "end": "689279"
  },
  {
    "text": "the total throughput of the cluster but it is impacting possibly stages between different workflows so if you have a",
    "start": "689279",
    "end": "695279"
  },
  {
    "text": "hive job that's feeding into a pig job that's feeding into a spark job you're introducing latency between each one of",
    "start": "695279",
    "end": "701370"
  },
  {
    "text": "those another area is the actual table",
    "start": "701370",
    "end": "707730"
  },
  {
    "text": "scan so when you're doing some sort of table scan operator within the execution engine and you're using like park' file",
    "start": "707730",
    "end": "714269"
  },
  {
    "text": "it does a lot of seek operations and those again are much more expensive because there's a higher latency for every request that you have to s3",
    "start": "714269",
    "end": "721550"
  },
  {
    "text": "there's also some overhead in the actual read request itself just a protocol that's being used but what we find is",
    "start": "721550",
    "end": "729240"
  },
  {
    "text": "actually that the performance of these converge as the volume and the complexity increases so if you're doing",
    "start": "729240",
    "end": "735660"
  },
  {
    "text": "a if you're running a job that has some sort of heavy stage or parsing that",
    "start": "735660",
    "end": "740759"
  },
  {
    "text": "happens in the initial stage the complexity of that is forced over onto",
    "start": "740759",
    "end": "745800"
  },
  {
    "text": "the CPU side so you become CPU bound and your i/o matters less and less to the point that it's effectively the same as",
    "start": "745800",
    "end": "751949"
  },
  {
    "text": "running on top of HDFS so in cases where you have really short running jobs and possibly reading tiny tiny files you can",
    "start": "751949",
    "end": "758850"
  },
  {
    "text": "be impacted by just the difference in performance from s3 to HDFS but if you",
    "start": "758850",
    "end": "764519"
  },
  {
    "text": "are operating at scale with large jobs large input sizes that diminishes to the",
    "start": "764519",
    "end": "769949"
  },
  {
    "text": "point where it's almost non-existent so how do we see this so this is kind of a",
    "start": "769949",
    "end": "775709"
  },
  {
    "text": "simple overview of a job typically you have a section that happens in the client this is the the planning stage",
    "start": "775709",
    "end": "782250"
  },
  {
    "text": "the split calculation that can have some impact and it gets you know exacerbated by the number of like locations that",
    "start": "782250",
    "end": "788970"
  },
  {
    "text": "you're you're actually listing and how much data is being split in that in that",
    "start": "788970",
    "end": "794550"
  },
  {
    "text": "phase but then on the cluster side you really have two areas that are impacted by any performance difference in s3 one",
    "start": "794550",
    "end": "801509"
  },
  {
    "text": "is the initial table scan and then the other one is actually the store operator but inside that you have",
    "start": "801509",
    "end": "806760"
  },
  {
    "text": "many different stages and all of those are either using HDFS for like big hive MapReduce with spark it's gonna be using",
    "start": "806760",
    "end": "815070"
  },
  {
    "text": "local disk and shuffling and then in press so it's actually using in-memory so there are a lot of stages and what",
    "start": "815070",
    "end": "821520"
  },
  {
    "text": "happens is this cost is also amortized over the length of a job so if you have jobs that are many many stages just that",
    "start": "821520",
    "end": "827910"
  },
  {
    "text": "impact at the very ends is not going to be very significant compared to the whole operation okay so in addition so",
    "start": "827910",
    "end": "836520"
  },
  {
    "start": "835000",
    "end": "973000"
  },
  {
    "text": "we talked a little bit about the actual data and how we store it in s3 but it doesn't make a lot of sense unless you",
    "start": "836520",
    "end": "842610"
  },
  {
    "text": "can make you know you can get to the data that you actually want to so we have a metadata system that we call",
    "start": "842610",
    "end": "848310"
  },
  {
    "text": "medic add and it's a federated metadata system so you can think of it in terms of like the hive meta store but this is",
    "start": "848310",
    "end": "855150"
  },
  {
    "text": "kind of the hive meta store on steroids because it reaches across many different systems it can talk and you know describe RDS instances Teradata",
    "start": "855150",
    "end": "862740"
  },
  {
    "text": "deployments redshift deployments druid deployments and it allows us to get information across all these different",
    "start": "862740",
    "end": "868860"
  },
  {
    "text": "systems but it also exposes a high thrift interface for anything that we can run the big data workloads across so",
    "start": "868860",
    "end": "875550"
  },
  {
    "text": "this means that hive presto spark anybody can talk to it and get a better understanding or an accurate",
    "start": "875550",
    "end": "881850"
  },
  {
    "text": "understanding of where the data is located within these this large data warehouse and the most important thing",
    "start": "881850",
    "end": "887040"
  },
  {
    "text": "is it provides a logical abstraction over the actual data because nobody wants to remember s3 bucket some path",
    "start": "887040",
    "end": "893640"
  },
  {
    "text": "into a warehouse and then it's partitioned by some values this is all handled by the actual warehouse what does that look like well if you're",
    "start": "893640",
    "end": "899970"
  },
  {
    "text": "familiar with hive this is a very normal concept it sounds or is set up very",
    "start": "899970",
    "end": "905670"
  },
  {
    "text": "similar to a relational database so you have databases so right here we have you know like a data science and ETL",
    "start": "905670",
    "end": "911040"
  },
  {
    "text": "telemetry data a B test data and then within each of those you have different tables and you can select just by",
    "start": "911040",
    "end": "917670"
  },
  {
    "text": "calling out the name in sequel or in you know using spark or Pig which one you're looking for and then the added level of",
    "start": "917670",
    "end": "924930"
  },
  {
    "text": "indirection is the partitioning so this is how you actually structure it within s3 the nice thing about this is it",
    "start": "924930",
    "end": "930750"
  },
  {
    "text": "actually overlays very easily on top of s3 in the same way that it works on HDFS so that partition",
    "start": "930750",
    "end": "936660"
  },
  {
    "text": "location is actually just pointing to a path an s3 and all the data for that particular particular partition is",
    "start": "936660",
    "end": "942180"
  },
  {
    "text": "underneath that path so of course the most important thing is down selecting",
    "start": "942180",
    "end": "947940"
  },
  {
    "text": "how much data you're actually going to process and by structuring your warehouse and your tables appropriate",
    "start": "947940",
    "end": "953189"
  },
  {
    "text": "you can really limit how much data you're going to process now no job is gonna process all 60 petabytes but many",
    "start": "953189",
    "end": "959399"
  },
  {
    "text": "of them can carve down to just the data that they're actually interested in in order to answer a question and that has a big impact on the total performance of",
    "start": "959399",
    "end": "966120"
  },
  {
    "text": "the job so now we've talked about the warehouse all the way down to a specific location but how do we actually store",
    "start": "966120",
    "end": "972540"
  },
  {
    "text": "the data so I mentioned we use Park a very heavily and there's a lot of features in Park Hae that add to the",
    "start": "972540",
    "end": "978689"
  },
  {
    "start": "973000",
    "end": "1087000"
  },
  {
    "text": "performance especially on top of s3 so for those who don't know what Parque is it's a file format it's columnar you",
    "start": "978689",
    "end": "986639"
  },
  {
    "text": "know that means that it's going to take the data and instead of like a comma separated file or tab separated file",
    "start": "986639",
    "end": "991829"
  },
  {
    "text": "it's gonna store the column data together contiguously on disk and this has some benefits one you can improve",
    "start": "991829",
    "end": "998939"
  },
  {
    "text": "the compression because data in columns is going to be very similar across the entire column and so you get really high",
    "start": "998939",
    "end": "1006019"
  },
  {
    "text": "compression there you can also do column projection so if I've got a hundred columns in a table and all I care about",
    "start": "1006019",
    "end": "1011540"
  },
  {
    "text": "is maybe five of them I only have to read the data for those five and that's really helpful because it cuts down on",
    "start": "1011540",
    "end": "1016819"
  },
  {
    "text": "the total volume of data that you're processing in the job so that's nothing particularly new you know we've got",
    "start": "1016819",
    "end": "1022939"
  },
  {
    "text": "columnar file formats they've been around for a long time but one of the things that parkade does add is it adds",
    "start": "1022939",
    "end": "1029720"
  },
  {
    "text": "metadata about the the actual data that's in the file so at the very bottom this is the structure of a parquet file",
    "start": "1029720",
    "end": "1035329"
  },
  {
    "text": "so you have the data is actually broken up into row groups and that's maybe you know some size of data that you want to",
    "start": "1035329",
    "end": "1042079"
  },
  {
    "text": "hold contiguously together but within those row groups you've got column data and that is what is actually stored",
    "start": "1042079",
    "end": "1047329"
  },
  {
    "text": "contiguously on disk and what we see here is a file it has two row groups in",
    "start": "1047329",
    "end": "1052970"
  },
  {
    "text": "it but at the very bottom we have metadata and what it has is information about each specific column what is the min value what is the max value as well",
    "start": "1052970",
    "end": "1060620"
  },
  {
    "text": "as the areas highlighted in red which are dictionary pages so if it can be Dictionary encoded you have a full set",
    "start": "1060620",
    "end": "1066470"
  },
  {
    "text": "of the values even though you don't necessarily know how many times it's repeated or anything like that you know what is actually in that column",
    "start": "1066470",
    "end": "1071730"
  },
  {
    "text": "data and we can use that especially when you stage your data appropriate appropriately to take advantage of some",
    "start": "1071730",
    "end": "1080100"
  },
  {
    "text": "of the features in the processing engines to either skip or do certain types of operations like counts very",
    "start": "1080100",
    "end": "1086700"
  },
  {
    "text": "quickly so one thing you have to do is you actually have to stage your data at this point so one of the things that we",
    "start": "1086700",
    "end": "1092610"
  },
  {
    "start": "1087000",
    "end": "1188000"
  },
  {
    "text": "do is you partition by low cardinality fields so these are things like date or hour or maybe region or something like",
    "start": "1092610",
    "end": "1099720"
  },
  {
    "text": "that where you know you're gonna have very large buckets of data under each one of those partitions but then what you do is",
    "start": "1099720",
    "end": "1105840"
  },
  {
    "text": "you actually sort by the very high cardinality fields and a really good example of this is like telemetry data",
    "start": "1105840",
    "end": "1111120"
  },
  {
    "text": "so at Netflix we have a system called a Atlas and it keeps track of all the metrics for all the services at Netflix",
    "start": "1111120",
    "end": "1117029"
  },
  {
    "text": "and they have one column which is the actual metric name and that's typically what you're going to be looking for be",
    "start": "1117029",
    "end": "1123090"
  },
  {
    "text": "it latency or you know throughput or something like that but there are millions of these different metrics",
    "start": "1123090",
    "end": "1128880"
  },
  {
    "text": "across all the different micro services and you know ways that they slice and dice this so if you sort by that high",
    "start": "1128880",
    "end": "1134490"
  },
  {
    "text": "cardinality field you can reduce the amount of data that you actually read by a hundred x easily so what does that",
    "start": "1134490",
    "end": "1142380"
  },
  {
    "text": "actually look like so if we store our data you can see all if these are all data files and the data we're actually looking for is all the little red lines",
    "start": "1142380",
    "end": "1148950"
  },
  {
    "text": "that you see there if they're all sporadic throughout the entire data set but if we sort that you put all of that",
    "start": "1148950",
    "end": "1155279"
  },
  {
    "text": "data into just one file and then what we can do is as we go through that you can",
    "start": "1155279",
    "end": "1162179"
  },
  {
    "text": "you can filter out just the one file that your actual on a process yes you have to read the footer for every single",
    "start": "1162179",
    "end": "1168059"
  },
  {
    "text": "file but that's tiny you can put in comparison to reading the entire data set and processing the whole thing so we",
    "start": "1168059",
    "end": "1173909"
  },
  {
    "text": "take advantage of this with some of our biggest data sets like I mentioned our telemetry data set we do it with we also",
    "start": "1173909",
    "end": "1178980"
  },
  {
    "text": "have like UI events that we have a common logging so it means that we have lots of different event types and we use",
    "start": "1178980",
    "end": "1185909"
  },
  {
    "text": "the same approach there so if you want more information about how to take advantage of these there was a recent",
    "start": "1185909",
    "end": "1192149"
  },
  {
    "start": "1188000",
    "end": "1372000"
  },
  {
    "text": "release of park' 1.9 that has all of the the features that we've been working on and there's a SlideShare here with a",
    "start": "1192149",
    "end": "1198840"
  },
  {
    "text": "presentation that goes into great detail about the various properties and everything else so we'll be posted I believe after the",
    "start": "1198840",
    "end": "1204509"
  },
  {
    "text": "conference so if you're interested you can go take a look at that so with that I'm gonna hand you over to Tom and he's",
    "start": "1204509",
    "end": "1210179"
  },
  {
    "text": "gonna take you back up to the sack and talk a little bit about our service layer right thanks Dan okay so Dan went",
    "start": "1210179",
    "end": "1217620"
  },
  {
    "text": "really low I'm gonna come back up a little bit higher and talk about our Jeannie service which is an open source",
    "start": "1217620",
    "end": "1222960"
  },
  {
    "text": "service developed to solve a few problems and to kind of establish why we develop this service I'm going to walk",
    "start": "1222960",
    "end": "1228779"
  },
  {
    "text": "through kind of a story of how a data platform might evolve and some of you may be able to relate with this some of",
    "start": "1228779",
    "end": "1234210"
  },
  {
    "text": "you may not so you start out with a nascent data platform right you've got a few users they might log into one",
    "start": "1234210",
    "end": "1240450"
  },
  {
    "text": "machine and access your one Hadoop cluster or whatever processing cluster you have everybody's pretty happy it was",
    "start": "1240450",
    "end": "1246539"
  },
  {
    "text": "enough resources for everybody there's not much stuff going on pretty soon you realize that you need to start testing",
    "start": "1246539",
    "end": "1252690"
  },
  {
    "text": "some stuff and you need two versions of things so now you've got a test gateway you've got a prog eight-way you got two clusters it's not too bad people know",
    "start": "1252690",
    "end": "1259679"
  },
  {
    "text": "where to go people may need to replicate data like dan was mentioning from prod to test but you can still kind of handle",
    "start": "1259679",
    "end": "1265200"
  },
  {
    "text": "the situation pretty soon a company's growing organization is growing we have",
    "start": "1265200",
    "end": "1270629"
  },
  {
    "text": "more users we need more resources more client resources more cluster resources everything's growing doing deployments",
    "start": "1270629",
    "end": "1277230"
  },
  {
    "text": "becomes much more complicated you don't want to has much downtime you don't want to as you have more gateways you need to",
    "start": "1277230",
    "end": "1283379"
  },
  {
    "text": "deploy different binaries to these gateways it's becomes very difficult to start managing that pretty soon you",
    "start": "1283379",
    "end": "1289529"
  },
  {
    "text": "decide oh I need clusters for specific purposes so Dan mentioned at Netflix we have a lot of clusters we have ad hoc",
    "start": "1289529",
    "end": "1295350"
  },
  {
    "text": "clusters and presto we have lots of production who do clusters test-tube",
    "start": "1295350",
    "end": "1301860"
  },
  {
    "text": "clusters and you start scaling horizontally across all these different needs then your user base starts",
    "start": "1301860",
    "end": "1308460"
  },
  {
    "text": "maturing so your data scientists actually become you know kind of smarter than people running the data platform",
    "start": "1308460",
    "end": "1314399"
  },
  {
    "text": "they they want to run the new stuff SPARC to you know want to use R or Python or whatever it is they start",
    "start": "1314399",
    "end": "1321299"
  },
  {
    "text": "complaining about their job being slow because they're not getting enough resources or their co-workers starving them out and you as a system",
    "start": "1321299",
    "end": "1328139"
  },
  {
    "text": "administrator of their data platform need to respond to all these things quickly and dynamically and pretty soon",
    "start": "1328139",
    "end": "1333450"
  },
  {
    "text": "everybody starts looking kind of like this the administrators aren't happy the",
    "start": "1333450",
    "end": "1338910"
  },
  {
    "text": "users aren't happy everybody starts crying and you know people are stressed so a few years ago we started developing",
    "start": "1338910",
    "end": "1347370"
  },
  {
    "text": "geni to solve these problems we wanted something that from the administrator point of view helped us to administer",
    "start": "1347370",
    "end": "1353000"
  },
  {
    "text": "launching clusters maintaining binaries all that stuff and abstracting those details from the users well at the same",
    "start": "1353000",
    "end": "1359430"
  },
  {
    "text": "time allowing users to have a scalable set of resources to access these clusters and not really worry about",
    "start": "1359430",
    "end": "1365250"
  },
  {
    "text": "where the clusters were how to connect to them what to run on them all that kind of stuff or setting up a working",
    "start": "1365250",
    "end": "1371460"
  },
  {
    "text": "environment so I'm going to walk through a couple problems that we face at Netflix from administration and user",
    "start": "1371460",
    "end": "1378690"
  },
  {
    "start": "1372000",
    "end": "1457000"
  },
  {
    "text": "point of view for the administrators we've got a lot of moving parts and Dan",
    "start": "1378690",
    "end": "1384690"
  },
  {
    "text": "and my team we're kind of the administrators in this case because we do devops so we've got about 15 clusters",
    "start": "1384690",
    "end": "1390060"
  },
  {
    "text": "dan went walk through some of them at any given time we run about 45 different variations of the executables you can",
    "start": "1390060",
    "end": "1396300"
  },
  {
    "text": "run against those clusters whether it be different versions of SPARC different versions of you know hi different",
    "start": "1396300",
    "end": "1401850"
  },
  {
    "text": "versions of pig or presto whatever it may be and then we see a very heavy load of jobs we run about 45 to 50 thousand",
    "start": "1401850",
    "end": "1408770"
  },
  {
    "text": "individual jobs which could compose of actual a breakdown of those different",
    "start": "1408770",
    "end": "1414960"
  },
  {
    "text": "processing layers so one job could actually result in four or five actual MapReduce jobs or SPARC jobs resulting",
    "start": "1414960",
    "end": "1422070"
  },
  {
    "text": "on the cluster and we've got hundreds of users so in the talk before this if any of you were there they talked about how",
    "start": "1422070",
    "end": "1427530"
  },
  {
    "text": "you have 80 data scientists may be at stitch fix we've got well over 300 I",
    "start": "1427530",
    "end": "1432660"
  },
  {
    "text": "believe in Netflix now so between them and a lot of other people we're seeing a lot of users and for users they don't",
    "start": "1432660",
    "end": "1439560"
  },
  {
    "text": "really want to they don't really care about the details right they don't care where your clusters are what's running they just want to make sure they can",
    "start": "1439560",
    "end": "1445890"
  },
  {
    "text": "connect and get to their data we've got a lot of data they want to do it they want to do their jobs they want to be easy so how do we kind of merge those",
    "start": "1445890",
    "end": "1453240"
  },
  {
    "text": "problems together and have a solution that's kind of where geni comes in in our platform so I'm going to talk about",
    "start": "1453240",
    "end": "1459210"
  },
  {
    "start": "1457000",
    "end": "1520000"
  },
  {
    "text": "how it comes in and helps the platform administrator which is our team which is really all I care about so",
    "start": "1459210",
    "end": "1465720"
  },
  {
    "text": "the administrators we want a tool that basically simplifies configuration management deployment so when we launch",
    "start": "1465720",
    "end": "1471960"
  },
  {
    "text": "a new cluster we don't want to be copying around site XML files or configurations about what the cluster",
    "start": "1471960",
    "end": "1478259"
  },
  {
    "text": "looks like what version is running anything like that we want to minimize the impact of changes to users this is",
    "start": "1478259",
    "end": "1484529"
  },
  {
    "text": "really important Netflix is we have 300 data scientists they've got a lot of important work to",
    "start": "1484529",
    "end": "1490200"
  },
  {
    "text": "do we don't want when we change out a cluster version or anything like that we don't want them to have any impact in",
    "start": "1490200",
    "end": "1496019"
  },
  {
    "text": "terms of being able to do their job we want to be able to respond to any problems with the system quickly so we",
    "start": "1496019",
    "end": "1502049"
  },
  {
    "text": "want to know what clusters are running where they are how to shut them down how to do any of that kind of stuff and we",
    "start": "1502049",
    "end": "1507590"
  },
  {
    "text": "definitely want to be able to scale the client resources as load increases so we",
    "start": "1507590",
    "end": "1512730"
  },
  {
    "text": "don't want to be scaling out and sending out a new gateway every time that we need to we just want you know the system",
    "start": "1512730",
    "end": "1517860"
  },
  {
    "text": "to handle that automatically so from an administrative perspective there's I'm",
    "start": "1517860",
    "end": "1523500"
  },
  {
    "start": "1520000",
    "end": "1626000"
  },
  {
    "text": "gonna go into the Jeannie data model so it's pretty simple we've got something called a cluster it just basically",
    "start": "1523500",
    "end": "1529259"
  },
  {
    "text": "represents the metadata about the cluster which could include things like where the site XML files are located we",
    "start": "1529259",
    "end": "1534779"
  },
  {
    "text": "we personally load them to s3 women the cluster is launched on EMR then each",
    "start": "1534779",
    "end": "1540840"
  },
  {
    "text": "cluster it can be linked to many executables and we call these commands so basically those are what you would if",
    "start": "1540840",
    "end": "1546990"
  },
  {
    "text": "you were on a command line terminal you would execute hive you'd have SKU Pig that's what they execute the command would be and then each command can be",
    "start": "1546990",
    "end": "1554250"
  },
  {
    "text": "linked to many applications so the applications represent the dependencies so it would be a tarball of all your",
    "start": "1554250",
    "end": "1560970"
  },
  {
    "text": "binaries for hadoop at our ball of your spark application whatever it may be and jeanne will take care of downloading all",
    "start": "1560970",
    "end": "1567750"
  },
  {
    "text": "the necessary dependencies into the working directory for a given job and I'll get into that in a little bit it's",
    "start": "1567750",
    "end": "1574889"
  },
  {
    "text": "important to note that each of these resources we have in the data model can be tagged as specific tags and we use these tags a lot to uniquely identify a",
    "start": "1574889",
    "end": "1583980"
  },
  {
    "text": "cluster and allow users to just set certain tags to say I always want to run on say the SLA cluster or I always want",
    "start": "1583980",
    "end": "1591690"
  },
  {
    "text": "to run SPARC submit I don't care what version is just give me the default or give me I always want on the SLA cluster",
    "start": "1591690",
    "end": "1597600"
  },
  {
    "text": "so it allows us to move the bags around from cluster to cluster as the role of clusters or resources change",
    "start": "1597600",
    "end": "1603590"
  },
  {
    "text": "and I'll get into some of that too this is just a quick screenshot of the Gini",
    "start": "1603590",
    "end": "1609270"
  },
  {
    "text": "UI that shows how you can search resources it shows all the clusters that we currently have registered in Genie",
    "start": "1609270",
    "end": "1614640"
  },
  {
    "text": "and kind of their state you can filter on state like that so this is kind of how we as administrators can go and see",
    "start": "1614640",
    "end": "1620730"
  },
  {
    "text": "how anybody is launching clusters and where they live and how to connect to them and that kind of stuff so let's go",
    "start": "1620730",
    "end": "1627210"
  },
  {
    "start": "1626000",
    "end": "1803000"
  },
  {
    "text": "through some quick use cases that somebody might want to do in a Big Data Platform if you're an administrator so",
    "start": "1627210",
    "end": "1632730"
  },
  {
    "text": "let's say you want to update a cluster the first thing you would do in the cloud is kind of start up a new cluster you'd then register that cluster with",
    "start": "1632730",
    "end": "1639240"
  },
  {
    "text": "geni with a set of kind of default tags which generally means your ID and your name but none of those common tags like",
    "start": "1639240",
    "end": "1646559"
  },
  {
    "text": "SLA or anything that would be common across a bunch of clusters you then run whatever smoke test you want to make",
    "start": "1646559",
    "end": "1652559"
  },
  {
    "text": "sure the cluster is good let's say you're testing a new version of Hadoop you want to make sure all your previous tests still run fine you would then move",
    "start": "1652559",
    "end": "1660929"
  },
  {
    "text": "your tags over from the old cluster to the new cluster in geni so like that",
    "start": "1660929",
    "end": "1666270"
  },
  {
    "text": "schedule SLA tag I showed earlier you can move that to the new cluster and all the jobs would automatically load over",
    "start": "1666270",
    "end": "1672840"
  },
  {
    "text": "to the G to the new cluster so anybody submitting new jobs that wanted to go to the SLA cluster would go to the new one",
    "start": "1672840",
    "end": "1678809"
  },
  {
    "text": "they didn't have to change anything on their client code they'd have to change anything in there you see for code anything like that it just automatically",
    "start": "1678809",
    "end": "1684960"
  },
  {
    "text": "works and you can let the old cluster the old jobs finish on the old cluster behind the scenes users can still access",
    "start": "1684960",
    "end": "1691260"
  },
  {
    "text": "them because geni still knows where they're running you can still work through that and then you could shut down the old cluster when they're done",
    "start": "1691260",
    "end": "1697110"
  },
  {
    "text": "and there's no downtime to users in this C's case next let's say you want to load",
    "start": "1697110",
    "end": "1702540"
  },
  {
    "text": "balance between clusters let's say you in the middle of night you have different loads so for Netflix specifically you know people watch a lot",
    "start": "1702540",
    "end": "1709410"
  },
  {
    "text": "of TV late at night but then you know in the middle at night they're sleeping so our load goes way down we have a lot of resources available well we may want to",
    "start": "1709410",
    "end": "1716309"
  },
  {
    "text": "move some of our ETL jobs over to a different cluster so we take the tags that are in our production cluster and",
    "start": "1716309",
    "end": "1722520"
  },
  {
    "text": "copy them over to our ad hoc cluster because all data scientists are usually sleeping at night hopefully so we take",
    "start": "1722520",
    "end": "1728520"
  },
  {
    "text": "the resources that are on those ad hoc clusters and we put the same tag on both and now Jeannie will just start putting",
    "start": "1728520",
    "end": "1734280"
  },
  {
    "text": "jobs to both of them because they're really the same cluster it's just how we reference them then in the morning when",
    "start": "1734280",
    "end": "1740460"
  },
  {
    "text": "we say okay the data scientists are starting wake up we need the resources back we remove the tags from the from",
    "start": "1740460",
    "end": "1746460"
  },
  {
    "text": "the ad hoc cluster and jobs are stopped being submitted to than that should be on the production cluster and this is",
    "start": "1746460",
    "end": "1751710"
  },
  {
    "text": "transparent to all the clients they don't really know which cluster their jobs are going to they just know how to access them via geni so and then usually",
    "start": "1751710",
    "end": "1760560"
  },
  {
    "text": "if you're an application administrator you want to deploy a new version of SPARC let's say you have to find all the",
    "start": "1760560",
    "end": "1765600"
  },
  {
    "text": "clients that are running those versions write all the gateways any users that are upstream running SPARC or whatever",
    "start": "1765600",
    "end": "1771840"
  },
  {
    "text": "it may be but you need to update that without any downtime for geni since all",
    "start": "1771840",
    "end": "1777360"
  },
  {
    "text": "the jobs run on the Genii nodes we can load the new binaries to essential download location in our case that's s3",
    "start": "1777360",
    "end": "1783650"
  },
  {
    "text": "and we can then and then Ginny will automatically invalidate that's cache and it'll download the new ones on the",
    "start": "1783650",
    "end": "1789600"
  },
  {
    "text": "next run-through so this will be an instant change across the geni cluster as jobs are run so you don't have to",
    "start": "1789600",
    "end": "1795330"
  },
  {
    "text": "actually hunt down all the places SPARC or whatever it may be are deployed you just need to update Jeannie's",
    "start": "1795330",
    "end": "1800940"
  },
  {
    "text": "configuration and Ginny will take care of the rest okay so those are some use",
    "start": "1800940",
    "end": "1806430"
  },
  {
    "start": "1803000",
    "end": "1840000"
  },
  {
    "text": "cases that refer administrators hopefully most of you can relate with them in some way",
    "start": "1806430",
    "end": "1811520"
  },
  {
    "text": "so for users the data scientist they usually want to discover a cluster for a job to run on they want to run the job",
    "start": "1811520",
    "end": "1819150"
  },
  {
    "text": "client which could be hive SPARC whatever they want they don't care about what your dependencies are they don't",
    "start": "1819150",
    "end": "1824340"
  },
  {
    "text": "want to have to install SPARC they want to install hive they just want to run they want to be able to check on the",
    "start": "1824340",
    "end": "1829770"
  },
  {
    "text": "status of their job easily obviously they want to view a history of their jobs to see how their performance",
    "start": "1829770",
    "end": "1834840"
  },
  {
    "text": "changed over time and they obviously want to get their job results so I'm",
    "start": "1834840",
    "end": "1840870"
  },
  {
    "start": "1840000",
    "end": "2049000"
  },
  {
    "text": "gonna walk quickly through how Java submission to geni works just so that we're kind of on the same wavelength",
    "start": "1840870",
    "end": "1847020"
  },
  {
    "text": "here so you're a superstar data scientist and you have your boss wants",
    "start": "1847020",
    "end": "1853350"
  },
  {
    "text": "you to create a report or you need some sort of new report on an a/b test and you're like I need to run a job so you",
    "start": "1853350",
    "end": "1860100"
  },
  {
    "text": "submit a job to Jeannie and basically to the key field in any job submission are these cluster criteria and come",
    "start": "1860100",
    "end": "1865840"
  },
  {
    "text": "criteria which leverage those tags I discussed earlier in this case the user wants to run on a yarn cluster that's",
    "start": "1865840",
    "end": "1872350"
  },
  {
    "text": "the SLA roll so kind of a production cluster they don't care what it is they just want the production cluster and",
    "start": "1872350",
    "end": "1878440"
  },
  {
    "text": "they want to run SPARC version 1.6 that Java submission goes to geni through the",
    "start": "1878440",
    "end": "1884169"
  },
  {
    "text": "REST API and geni accesses its RDS instance in this case my sequel to get",
    "start": "1884169",
    "end": "1890440"
  },
  {
    "text": "the metadata about what's available currently in the system it finds that there's a bunch of clusters and a bunch",
    "start": "1890440",
    "end": "1897010"
  },
  {
    "text": "of commands that are currently available it'll then compare the tags that are on those resources to commands in the",
    "start": "1897010",
    "end": "1902919"
  },
  {
    "text": "clusters to match anything that could match what the user selected and it finds that there's a cluster that",
    "start": "1902919",
    "end": "1907929"
  },
  {
    "text": "matches and a spark command that matches it'll then download all the",
    "start": "1907929",
    "end": "1913600"
  },
  {
    "text": "configurations out of s3 this bucket is supposed to represent s3 my boss told me they would so that's us three so all the",
    "start": "1913600",
    "end": "1920860"
  },
  {
    "text": "files are downloaded onto the geni node at runtime and set up in the working directory so for that spark command and",
    "start": "1920860",
    "end": "1927640"
  },
  {
    "text": "for that hadoop cluster there all downloaded into the working directory which becomes effectively like if you were just sitting on the command line if",
    "start": "1927640",
    "end": "1933490"
  },
  {
    "text": "you executed SPARC that's your working directory Deenie will then submit the job to the cluster that was selected",
    "start": "1933490",
    "end": "1939640"
  },
  {
    "text": "it'll run your user can go into the genie UI monitor it see what's going on and wait for it to be done so hopefully",
    "start": "1939640",
    "end": "1949630"
  },
  {
    "text": "that's clear so the genie data model looks kind of like this from a user perspective their job is linked to the",
    "start": "1949630",
    "end": "1956169"
  },
  {
    "text": "cluster command and applications that ran with they can go and access this metadata anytime going forward",
    "start": "1956169",
    "end": "1962740"
  },
  {
    "text": "a job request it's kind of hard to see the scale but basically there's some important fields in here the cluster",
    "start": "1962740",
    "end": "1968320"
  },
  {
    "text": "criteria and command criteria you can see and any command arguments they don't want to add to their executable and tag",
    "start": "1968320",
    "end": "1975220"
  },
  {
    "text": "you can tag your job so you can search them later and that kind of stuff from a higher level we've created Python",
    "start": "1975220",
    "end": "1981640"
  },
  {
    "text": "clients Java clients that users can use to submit jobs to genie that abstract a lot of the details of those JSON",
    "start": "1981640",
    "end": "1987610"
  },
  {
    "text": "payloads so in this case this is a simple example using the OSS Python client from somebody who wanted to run a",
    "start": "1987610",
    "end": "1994690"
  },
  {
    "text": "presto job so they're just selecting you just say what script you want to run any parameters you want to submit in and",
    "start": "1994690",
    "end": "2000659"
  },
  {
    "text": "you can run the job and wait for it to be done just via these few lines of Python so you don't don't worry about",
    "start": "2000659",
    "end": "2006480"
  },
  {
    "text": "where the presto cluster is what the binary that's running is this is all through service based access once your",
    "start": "2006480",
    "end": "2014549"
  },
  {
    "text": "jobs are done users can go to this job history UI and look up jobs and go to the find the",
    "start": "2014549",
    "end": "2020190"
  },
  {
    "text": "job output directories so those little file folders if they click that for their job they're taking us something",
    "start": "2020190",
    "end": "2025890"
  },
  {
    "text": "looks like this and this is basically their working directory so all their scripts are loaded in there the output",
    "start": "2025890",
    "end": "2032100"
  },
  {
    "text": "is in standard output standard error obviously holds your error file so it's just like they have their own custom",
    "start": "2032100",
    "end": "2038100"
  },
  {
    "text": "working directory on the box so there's nothing that the user needed to do to set all this up do you need download it",
    "start": "2038100",
    "end": "2043530"
  },
  {
    "text": "at all out of the configurations and and ran the job and executed it on the cluster so that's kind of a brief",
    "start": "2043530",
    "end": "2050520"
  },
  {
    "start": "2049000",
    "end": "2078000"
  },
  {
    "text": "walkthrough through Jeannie's so wrapping up we've got our data warehouse",
    "start": "2050520",
    "end": "2055648"
  },
  {
    "text": "which Dan discussed we use s3 to scale out as a robust set of nines as Dan",
    "start": "2055649",
    "end": "2062490"
  },
  {
    "text": "mentioned we like to decouple the compute from the storage for all the",
    "start": "2062490",
    "end": "2067800"
  },
  {
    "text": "reasons that Dan discussed the performance and the ability to run clusters across all the data sets I'm",
    "start": "2067800",
    "end": "2073888"
  },
  {
    "text": "going to use Parque for speed a take advantage of the column or format on the genie side we're running the actual OSS",
    "start": "2073889",
    "end": "2081690"
  },
  {
    "start": "2078000",
    "end": "2105000"
  },
  {
    "text": "code in geni so if you want to run geni you can take the same code and run the same stuff we are we're running about",
    "start": "2081690",
    "end": "2088220"
  },
  {
    "text": "45,000 jobs a day on those geni nodes which are about a cluster of 25 I to for",
    "start": "2088220",
    "end": "2094679"
  },
  {
    "text": "excels in production and we keep about three months of jobs in the database and",
    "start": "2094679",
    "end": "2100050"
  },
  {
    "text": "the job history so that's about three million or so of history and if you want",
    "start": "2100050",
    "end": "2106320"
  },
  {
    "start": "2105000",
    "end": "2126000"
  },
  {
    "text": "to get see what Ginny is all about you can go to the github page we've actually created a demo using docker and docker",
    "start": "2106320",
    "end": "2113790"
  },
  {
    "text": "compose where you can stand up jeanny along with a couple of clusters on your localhost and submit jobs to those",
    "start": "2113790",
    "end": "2120000"
  },
  {
    "text": "clusters and you can see kind of how it works so with that I think we finished a",
    "start": "2120000",
    "end": "2125460"
  },
  {
    "text": "little bit early so we've got time for some questions if anybody has them",
    "start": "2125460",
    "end": "2130850"
  },
  {
    "start": "2126000",
    "end": "2189000"
  },
  {
    "text": "just don't piss us off cuz I don't know",
    "start": "2130850",
    "end": "2136580"
  },
  {
    "text": "it so do we oh we don't all for this yeah we don't have a mic so we're gonna yeah and we'll repeat the question okay",
    "start": "2136580",
    "end": "2144880"
  },
  {
    "text": "how do we handle EMR five master failure so there's it does happen all the time",
    "start": "2146440",
    "end": "2153380"
  },
  {
    "text": "well I'm not gonna say it happens all the time but it does happen and what we actually do is we actually use Jeannie",
    "start": "2153380",
    "end": "2159110"
  },
  {
    "text": "for this so one thing that you can do is you can either have a small standby cluster that just has a couple nodes in",
    "start": "2159110",
    "end": "2165050"
  },
  {
    "text": "at the sitting you know idle and using the Jeannie tags when we detect a master",
    "start": "2165050",
    "end": "2170480"
  },
  {
    "text": "failure we can just reset the tags to the new cluster and issue a resize and it will you know go up to size terminate",
    "start": "2170480",
    "end": "2176720"
  },
  {
    "text": "the old one very quickly and usually we can do that with very minimal downtime we also do this with just our upgrades",
    "start": "2176720",
    "end": "2182660"
  },
  {
    "text": "and everything else so we do a red-black deployment and we use the tagging and Jeannie in order to handle that but yes",
    "start": "2182660",
    "end": "2188450"
  },
  {
    "text": "it does happen next question way over",
    "start": "2188450",
    "end": "2196430"
  },
  {
    "start": "2189000",
    "end": "2273000"
  },
  {
    "text": "there in the corner how do you handle",
    "start": "2196430",
    "end": "2203859"
  },
  {
    "text": "two systems in different so we actually use we have all of our data house a data",
    "start": "2209530",
    "end": "2216109"
  },
  {
    "text": "warehouse in US East one so we bring all the data to one place because that's going to work best for the big data",
    "start": "2216109",
    "end": "2223250"
  },
  {
    "text": "compute if you're reaching across to a region in some on some other continent",
    "start": "2223250",
    "end": "2228950"
  },
  {
    "text": "you're gonna see some serious performance degradation is it is my expectation I've never actually tested it as far as the disaster recovery",
    "start": "2228950",
    "end": "2236480"
  },
  {
    "text": "you know we rely a lot on s3 for the the warehousing that goes to the eleven nines of durability or whatever they",
    "start": "2236480",
    "end": "2242930"
  },
  {
    "text": "they spec out so that's what we do from that perspective and then the clusters themselves we run in two different a zs",
    "start": "2242930",
    "end": "2249260"
  },
  {
    "text": "so we can always make sure that we have a sizable cluster in one easy or the other and using the tags if something",
    "start": "2249260",
    "end": "2256099"
  },
  {
    "text": "happened where we lost our production cluster we could always just retag our ad hoc cluster to be the production",
    "start": "2256099",
    "end": "2261800"
  },
  {
    "text": "cluster and it would start taking all the production load and while there would be some interruption because it's",
    "start": "2261800",
    "end": "2266990"
  },
  {
    "text": "not as large as the production cluster at least the the critical jobs and everything else would still be able to go through so so the question was how do",
    "start": "2266990",
    "end": "2283550"
  },
  {
    "start": "2273000",
    "end": "2365000"
  },
  {
    "text": "we handle access control and can we talk a little bit more about meta cat so meta cat again is a federated metadata",
    "start": "2283550",
    "end": "2289760"
  },
  {
    "text": "service and the idea behind this is that we have lots of different data systems and we want some central way of talking",
    "start": "2289760",
    "end": "2296270"
  },
  {
    "text": "to each one of them or getting information about the data that's stored in each one of them that doesn't mean that you can read data through meta cat",
    "start": "2296270",
    "end": "2302930"
  },
  {
    "text": "it's just describing all of the data sources so you may be able to say something like oh we have a druid",
    "start": "2302930",
    "end": "2308150"
  },
  {
    "text": "cluster and this is the data that we indexed into it and you know this is the configuration for it so any system that",
    "start": "2308150",
    "end": "2314060"
  },
  {
    "text": "needs that information has one central place to go to so we do that with Terra data we do that with redshift and it",
    "start": "2314060",
    "end": "2319849"
  },
  {
    "text": "makes it really easy for us to do things like you know we have libraries built around moving data from one place to",
    "start": "2319849",
    "end": "2325880"
  },
  {
    "text": "another while s3 is our source of truth we want to be able to say oh well take this s3 data or maybe this specific table and",
    "start": "2325880",
    "end": "2333260"
  },
  {
    "text": "these date ranges or some selection of that and move it over to Terra data or move it into redshift and so people can",
    "start": "2333260",
    "end": "2339589"
  },
  {
    "text": "just issue very small commands that just say move you know h2r hive to redshift and",
    "start": "2339589",
    "end": "2345270"
  },
  {
    "text": "it will take care of it for you so that's the the purpose of meta cat but one of the things it also does is it",
    "start": "2345270",
    "end": "2351630"
  },
  {
    "text": "abstracts something so you can actually run against a hive meta store so it exposes an interface to get metadata in",
    "start": "2351630",
    "end": "2358859"
  },
  {
    "text": "the same way that all the big data compute engines that are out in the open source typically expect to see it so I'm",
    "start": "2358859",
    "end": "2374819"
  },
  {
    "start": "2365000",
    "end": "2428000"
  },
  {
    "text": "not saying that we're not concerned about our data I mean there's always disaster cases or potentially malicious",
    "start": "2374819",
    "end": "2381329"
  },
  {
    "text": "cases where data will be lost what we do is we leverage the tools that they have",
    "start": "2381329",
    "end": "2386400"
  },
  {
    "text": "for both you know access protection and you know control over what users can actually do to the data and one of the",
    "start": "2386400",
    "end": "2392760"
  },
  {
    "text": "things that I mentioned is that we have version data so we let people soft elite we don't let people hard delete so there",
    "start": "2392760",
    "end": "2398190"
  },
  {
    "text": "are things that you can do to you know ensure that you're not going to accidentally lose data in those cases but you know there's obviously the case",
    "start": "2398190",
    "end": "2404849"
  },
  {
    "text": "where there's some giant disaster and you know there there are things that we're looking into for better ways of",
    "start": "2404849",
    "end": "2411660"
  },
  {
    "text": "backing up as much as we can of the data warehouse but replicating sixty petabytes is a lot of data to have a",
    "start": "2411660",
    "end": "2418049"
  },
  {
    "text": "second copy of expensive I saw some",
    "start": "2418049",
    "end": "2424680"
  },
  {
    "text": "other hands here did anybody else have a",
    "start": "2424680",
    "end": "2428150"
  },
  {
    "start": "2428000",
    "end": "2569000"
  },
  {
    "text": "[Music]",
    "start": "2446520",
    "end": "2449599"
  },
  {
    "text": "yes so I'll answer a couple of those at",
    "start": "2453079",
    "end": "2475049"
  },
  {
    "text": "kind of a high level and if you have more specific questions so the question was about performance with spark and",
    "start": "2475049",
    "end": "2480299"
  },
  {
    "text": "park' against s3 how do you do the the right size of files how do you set the right options some of the options are",
    "start": "2480299",
    "end": "2487289"
  },
  {
    "text": "set in in that talk from the that was on the slide there as far as the actual",
    "start": "2487289",
    "end": "2493859"
  },
  {
    "text": "file size block location doesn't matter in s3 because we don't have any information as to the block size or how",
    "start": "2493859",
    "end": "2499890"
  },
  {
    "text": "s3 is actually carving up the data in the back end so you don't necessarily want to try and align to certain block",
    "start": "2499890",
    "end": "2505920"
  },
  {
    "text": "sizes there are ways you can kind of fake it out with the file system for split calculations that may have some effect on how those get divided up SPARC",
    "start": "2505920",
    "end": "2514740"
  },
  {
    "text": "specifically we actually have a heavily modified internal version of SPARC to address some of those problems with the",
    "start": "2514740",
    "end": "2520410"
  },
  {
    "text": "read and write path we are working on trying to make an open-source version of our s3 output committer which takes care",
    "start": "2520410",
    "end": "2528029"
  },
  {
    "text": "of a lot of the problems that you know you see with the open source especially you know because it tries to treat s3",
    "start": "2528029",
    "end": "2533099"
  },
  {
    "text": "exactly like any other file system they'll try to stage its intermediate data there and then when it goes to",
    "start": "2533099",
    "end": "2538230"
  },
  {
    "text": "commit it'll copy it into the final location but s3 doesn't have a copy I mean it doesn't have a rename function",
    "start": "2538230",
    "end": "2544200"
  },
  {
    "text": "it only has copy so you effectively copied to s3 then re copy it to s3 and",
    "start": "2544200",
    "end": "2549599"
  },
  {
    "text": "then delete the temporary wants and sometimes that can happen multiple times so there are high latencies and we've",
    "start": "2549599",
    "end": "2555299"
  },
  {
    "text": "worked around that in certain ways so we use an s3 output committer that actually writes locally and then uploads to s3 is",
    "start": "2555299",
    "end": "2560579"
  },
  {
    "text": "the final commit and does that you know massively parallel so it's actually quite performant so any more specific ones we can take",
    "start": "2560579",
    "end": "2568049"
  },
  {
    "text": "offline",
    "start": "2568049",
    "end": "2570289"
  },
  {
    "text": "so I doesn't have support for Parque with vectorize read anyway I mean it's got a vectorized execution but the read",
    "start": "2594130",
    "end": "2600380"
  },
  {
    "text": "itself is not vectorized so I wouldn't worry about it too much with hive as far",
    "start": "2600380",
    "end": "2605810"
  },
  {
    "text": "as the vectorization I think that we'd have to look at that more closely because I'm not sure that it is quite",
    "start": "2605810",
    "end": "2612850"
  },
  {
    "text": "linked to actual actually where it's stored because a lot of these systems like with spark or presto the execution",
    "start": "2612850",
    "end": "2620240"
  },
  {
    "text": "that they do is not necessarily vectorize in the sense of like CPU vectorization it's vectorized in the",
    "start": "2620240",
    "end": "2625910"
  },
  {
    "text": "sense that they're doing an operation across a lot of data that's you know in one particular column or maybe sets of",
    "start": "2625910",
    "end": "2631640"
  },
  {
    "text": "columns at a time and the way the Presto does it it just bytecode compiles the entire stage or execution that it's",
    "start": "2631640",
    "end": "2637310"
  },
  {
    "text": "doing and like spark judo is doing that across all the difference you know against all the different operators within a stage as much as it can so it's",
    "start": "2637310",
    "end": "2644630"
  },
  {
    "text": "actually not vectorization in the terms of cpu right exactly right",
    "start": "2644630",
    "end": "2654490"
  },
  {
    "start": "2652000",
    "end": "2725000"
  },
  {
    "text": "using the summary files or the yeah so",
    "start": "2655900",
    "end": "2662619"
  },
  {
    "text": "we actually don't use summary files and part of the reason we don't use summary files is because we have a metadata service to do this for us so we entirely",
    "start": "2662619",
    "end": "2670299"
  },
  {
    "text": "do not rely on any of the the footer data for split calculation or the",
    "start": "2670299",
    "end": "2675309"
  },
  {
    "text": "summary files for describing the the schemas or anything like that all of the schema information is actually in the",
    "start": "2675309",
    "end": "2681549"
  },
  {
    "text": "meta store and across most of the engines there is a feature which is called column index accessed so if",
    "start": "2681549",
    "end": "2688720"
  },
  {
    "text": "you're typically reading park' files you're going to be reading by name so you're gonna be addressing columns by",
    "start": "2688720",
    "end": "2694660"
  },
  {
    "text": "name but any system that's working with a hive Metis or hive traditionally will reference things by their column ordinal",
    "start": "2694660",
    "end": "2701559"
  },
  {
    "text": "so what we do is there's a flag that you can flip to do ordinal base access and then what you're doing is you're",
    "start": "2701559",
    "end": "2706779"
  },
  {
    "text": "basically saying I don't really care about the file schema I only care about what the metadata schema is and that's",
    "start": "2706779",
    "end": "2711970"
  },
  {
    "text": "the way that we work any other questions",
    "start": "2711970",
    "end": "2719559"
  },
  {
    "text": "guess who'll let you go for the night thank you very much thanks [Applause]",
    "start": "2719559",
    "end": "2727479"
  }
]