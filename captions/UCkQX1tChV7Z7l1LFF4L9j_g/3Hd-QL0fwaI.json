[
  {
    "text": "hello guys Uh thank you so much for having me here today So I've called this uh navigating alum deployment tips",
    "start": "10160",
    "end": "16800"
  },
  {
    "text": "tricks and techniques 2.0 I gave this talk in London about six months ago and",
    "start": "16800",
    "end": "23279"
  },
  {
    "text": "I was kind of hoping I could just reuse the same presentation um because you know bit lazy like that but",
    "start": "23279",
    "end": "29279"
  },
  {
    "text": "unfortunately the space evolved so quickly I had to completely rewrite it So it's a completely different presentation more or less to what I did",
    "start": "29279",
    "end": "35120"
  },
  {
    "text": "in London I could also rename it how to deploy LLMs if you don't work at Meta uh",
    "start": "35120",
    "end": "42320"
  },
  {
    "text": "you know OpenAI Google Mistral or Anthropic If you do work at one of these places you can still stay but it might",
    "start": "42320",
    "end": "48000"
  },
  {
    "text": "be less interesting to you Um I'm specifically interested in how you deploy LMS if you're not serving it as a",
    "start": "48000",
    "end": "56160"
  },
  {
    "text": "business Um and instead you're serving it so you can build applications on top of it Um and you end up deploying it in",
    "start": "56160",
    "end": "62160"
  },
  {
    "text": "fairly different ways if you deploy if you work at a normal company versus one of these guys Out of interest hands up",
    "start": "62160",
    "end": "68159"
  },
  {
    "text": "if you don't work at one of these guys Okay cool So there's some relevant people here That's good",
    "start": "68159",
    "end": "74760"
  },
  {
    "text": "Um so hopefully you're going to get three things out of this session Firstly you're going to learn when self-hosting",
    "start": "74760",
    "end": "81600"
  },
  {
    "text": "is right for you because you're going to find out it can be a bit of a pain and it's something you should only do if you kind of really need",
    "start": "81600",
    "end": "87720"
  },
  {
    "text": "to Understanding the differences between your deployments and the deployments of AI labs And then also I'm going to give",
    "start": "87720",
    "end": "95520"
  },
  {
    "text": "some best practices tips tricks techniques a non-exhaustive list um for",
    "start": "95520",
    "end": "101119"
  },
  {
    "text": "how to deploy AI in corporate and enterprise environments So first this is me There's",
    "start": "101119",
    "end": "108000"
  },
  {
    "text": "my QR code to connect on LinkedIn I'll also show it again at the end Um essentially we build infrastructure for",
    "start": "108000",
    "end": "114560"
  },
  {
    "text": "serving alms So firstly um when should you",
    "start": "114560",
    "end": "119840"
  },
  {
    "text": "self-host to explain what that is I'll just clarify what I mean by self-hosting",
    "start": "119840",
    "end": "124880"
  },
  {
    "text": "Um I distinguish self-hosting um compared to interacting with LLMs through an API provider So this is how",
    "start": "124880",
    "end": "131440"
  },
  {
    "text": "you interact with LM through an API provider They do all of the serving and hosting for you It's deployed on their",
    "start": "131440",
    "end": "137360"
  },
  {
    "text": "GPUs not your GPUs They manage all of their infrastructure and what they expose is just an API that you can",
    "start": "137360",
    "end": "143200"
  },
  {
    "text": "interact with That's what an API hosted model is And all of those companies I",
    "start": "143200",
    "end": "148319"
  },
  {
    "text": "mentioned at the beginning host these models for you versus being self-hosted So when you",
    "start": "148319",
    "end": "155720"
  },
  {
    "text": "self-host you're in control of the GPUs You take a model from Hugging Face or",
    "start": "155720",
    "end": "160879"
  },
  {
    "text": "wherever you're taking that model from and you deploy it and you serve it to your end users This is kind of the broad",
    "start": "160879",
    "end": "167200"
  },
  {
    "text": "difference It's essentially a matter of who owns the GPUs um and who's responsible for that serving",
    "start": "167200",
    "end": "172480"
  },
  {
    "text": "infrastructure So why would you ever want to self-host because when people manage things for",
    "start": "172480",
    "end": "179680"
  },
  {
    "text": "you your life is a little bit easier But there are three main reasons why you'd want to self-host The first one is you",
    "start": "179680",
    "end": "184720"
  },
  {
    "text": "have decreased costs Well I should clarify that you have decreased costs",
    "start": "184720",
    "end": "190400"
  },
  {
    "text": "when you're starting to scale At the very very early stages of pinging and trying things out you don't have",
    "start": "190400",
    "end": "196480"
  },
  {
    "text": "decreased costs It's actually much much cheaper to use an API provider where you pay per token and the per token price is",
    "start": "196480",
    "end": "201920"
  },
  {
    "text": "very low But once you get to any kind of scale where you can fully utilize a GPU",
    "start": "201920",
    "end": "208400"
  },
  {
    "text": "um or close to it it actually becomes much more costefficient The second reason why",
    "start": "208400",
    "end": "213680"
  },
  {
    "text": "you'd want self-host is improved performance So this might sound counterintuitive because on all the",
    "start": "213680",
    "end": "219440"
  },
  {
    "text": "leading benchmarks um the you know GPT models and the claw models are",
    "start": "219440",
    "end": "224799"
  },
  {
    "text": "bestin-class for those benchmarks However if you know your domain and you",
    "start": "224799",
    "end": "229920"
  },
  {
    "text": "know your particular use case you can get much better uh performance when self-hosting This is especially true and",
    "start": "229920",
    "end": "236640"
  },
  {
    "text": "I'm going to talk about this a bit more later This is especially true for embedding models search models ranker",
    "start": "236640",
    "end": "241840"
  },
  {
    "text": "models The state-of-the-art for most of them is actually in open source not in LLMs So if you want the best of the best",
    "start": "241840",
    "end": "248480"
  },
  {
    "text": "breed models you'll have a combination of self-hosting for some models and using API providers for others don't you",
    "start": "248480",
    "end": "254319"
  },
  {
    "text": "can get much better performance based self hosting and some privacy and security Um",
    "start": "254319",
    "end": "260639"
  },
  {
    "text": "I'm from Europe and we really care about this Um but we also work with regulated",
    "start": "260639",
    "end": "266400"
  },
  {
    "text": "industries here in the US um where you have various reasons why you might want to deploy it within your own environment",
    "start": "266400",
    "end": "272400"
  },
  {
    "text": "Maybe you have a multi cloud environment or maybe you're still on prem And this kind of sits with the data",
    "start": "272400",
    "end": "280000"
  },
  {
    "text": "that A6Z collected that there's broadly three reasons why people self-host control customizability and cost So it",
    "start": "280000",
    "end": "287759"
  },
  {
    "text": "is something that a lot of people are thinking about So how do I know if I fall into",
    "start": "287759",
    "end": "294400"
  },
  {
    "text": "one of those uh buckets well broadly if I care about decreased cost that's",
    "start": "294400",
    "end": "300560"
  },
  {
    "text": "relevant to me if I'm deploying at scale or it's relevant to me if I'm able to use a smaller specialized model for my",
    "start": "300560",
    "end": "307280"
  },
  {
    "text": "task run than a very very big domain like very big general model like GPT4 Um if I care about uh performance I",
    "start": "307280",
    "end": "316240"
  },
  {
    "text": "will get improved performance If I'm running embedding or re-ranking workloads um or from operating in a",
    "start": "316240",
    "end": "321840"
  },
  {
    "text": "specialized domain that might benefit from some fine-tuning or I have very clearly defined task requirements I can",
    "start": "321840",
    "end": "327600"
  },
  {
    "text": "often do better if I'm self-hosting rather than using these very generic models And finally on the privacy and",
    "start": "327600",
    "end": "334240"
  },
  {
    "text": "security things um if you you might have legal restrictions you'll obviously then have to self-host Um potentially you",
    "start": "334240",
    "end": "341680"
  },
  {
    "text": "might have region specific uh deployment requirements So we work with a couple clients who because of the various AWS",
    "start": "341680",
    "end": "348160"
  },
  {
    "text": "regions and Azure regions they have to self-host to make sure they're maintaining that sovereignty in their",
    "start": "348160",
    "end": "353720"
  },
  {
    "text": "deployments And then finally if you have multicloud or hybrid infrastructure that's normally a good sign that you need to self-host",
    "start": "353720",
    "end": "361759"
  },
  {
    "text": "A lot of people fall into those buckets Uh which is why the vast majority of enterprises are looking into building up",
    "start": "361759",
    "end": "367840"
  },
  {
    "text": "some kind of self-hosting infrastructure Not necessarily for all of their use cases but it's good to have um as a",
    "start": "367840",
    "end": "374319"
  },
  {
    "text": "sovereignty play I'm going to make a quick detour and I also felt very cringy making a",
    "start": "374319",
    "end": "381600"
  },
  {
    "text": "slide which had a screenshot of my own LinkedIn post on it Um but I thought my LinkedIn post was quite good in this case Um quick public service",
    "start": "381600",
    "end": "388560"
  },
  {
    "text": "announcement that I mentioned embedding models and I mentioned that the state-of-the-art for embedding models is",
    "start": "388560",
    "end": "395840"
  },
  {
    "text": "actually in the open source realm or they're very very very good There's another reason why you should almost",
    "start": "395840",
    "end": "402080"
  },
  {
    "text": "always self-host your embedding models And the reason why is because you use your embedding models to create you know",
    "start": "402080",
    "end": "407919"
  },
  {
    "text": "your vex database and you've indexed vast vast vast amount vast amounts of data If that embedding model uh that",
    "start": "407919",
    "end": "415840"
  },
  {
    "text": "you're using through an API provider ever goes down or ever is depreciated you have to reindex your whole vector",
    "start": "415840",
    "end": "421680"
  },
  {
    "text": "database that is a massive pain You shouldn't do that They're very very very cheap to host as well Always self-host",
    "start": "421680",
    "end": "427520"
  },
  {
    "text": "your embedding models That's a quick announcement Uh back to our regular scheduled programming So when should I",
    "start": "427520",
    "end": "435039"
  },
  {
    "text": "self-host versus when should I not i've been a little bit cheeky here Um good reasons to self-host You're building for",
    "start": "435039",
    "end": "440720"
  },
  {
    "text": "scale You're deploying in your own environment You're using embedding models or ranker models You have domain",
    "start": "440720",
    "end": "446479"
  },
  {
    "text": "specific use cases or my favorite one you kind of have trust issues Someone hurt you in the past and you want to be",
    "start": "446479",
    "end": "452319"
  },
  {
    "text": "able to control your own uh control your own infrastructure That's also a valid reason Bad reasons to self-host is",
    "start": "452319",
    "end": "458319"
  },
  {
    "text": "because a you thought it was going to be easy It's not easier necessarily than using API providers or someone told you",
    "start": "458319",
    "end": "465759"
  },
  {
    "text": "it was cool It is kind of cool but that's not a good reason So that is why that's how you should",
    "start": "465759",
    "end": "472400"
  },
  {
    "text": "evaluate whether self-hosting is right for you If you fall into one of these on the left you should self host If not you",
    "start": "472400",
    "end": "479080"
  },
  {
    "text": "shouldn't So understanding your the difference between your deployments and the deployments in AP in AI labs",
    "start": "479080",
    "end": "487319"
  },
  {
    "text": "So if I'm a for example an open AI and I'm serving these language models I'm",
    "start": "487319",
    "end": "493280"
  },
  {
    "text": "not just serving one use case I'm serving literally millions and millions and millions of different use cases And",
    "start": "493280",
    "end": "500160"
  },
  {
    "text": "that means I end up building my serving stack very very differently You let's say I'm hosting with an enterprise or",
    "start": "500160",
    "end": "507360"
  },
  {
    "text": "corporate environment Maybe I'm serving I don't know 20 use cases like in a more",
    "start": "507360",
    "end": "513279"
  },
  {
    "text": "mature enterprise Um maybe I'm serving now like just a couple and because I",
    "start": "513279",
    "end": "518640"
  },
  {
    "text": "have that difference I'm able to make different design decisions when it comes to my infrastructure",
    "start": "518640",
    "end": "523959"
  },
  {
    "text": "So here are a couple reasons why your self-hosting regime will be very different to the OpenAI self-hosting",
    "start": "523959",
    "end": "530120"
  },
  {
    "text": "regime First one is they have lots and lots and lots of H100s and lots of cache",
    "start": "530120",
    "end": "535200"
  },
  {
    "text": "My guess is the majority of you guys don't have lots of H100s and are probably you know renting them via um",
    "start": "535200",
    "end": "542320"
  },
  {
    "text": "AWS They're more likely to be computebound because they're using the GPUs like",
    "start": "542320",
    "end": "548320"
  },
  {
    "text": "H100s uh rather than you know things like um A10s They have very little information",
    "start": "548320",
    "end": "555040"
  },
  {
    "text": "about your end workload So they are just kind of going on a we're just trying to stream tokens out for arbitrary",
    "start": "555040",
    "end": "560480"
  },
  {
    "text": "workloads Um you have a lot more information about your workload and they're optimizing for one or two models which means they can do things that just",
    "start": "560480",
    "end": "566560"
  },
  {
    "text": "don't scale for regular people self-hosting If I'm deploying just GPT4",
    "start": "566560",
    "end": "572560"
  },
  {
    "text": "then I'm able to make very very very specific optimizations that only work for that model which wouldn't work um",
    "start": "572560",
    "end": "578320"
  },
  {
    "text": "anywhere else So if you're not self-hosting you're likely using uh",
    "start": "578320",
    "end": "583360"
  },
  {
    "text": "cheaper smaller GPUs Um you're probably using also a range of GPUs so not just",
    "start": "583360",
    "end": "588959"
  },
  {
    "text": "one type Maybe you're using a couple different types You're more likely to be memory bound than compute",
    "start": "588959",
    "end": "594279"
  },
  {
    "text": "bound You have lots of information about your workload So this is something that is very exciting that for most",
    "start": "594279",
    "end": "600240"
  },
  {
    "text": "enterprises the workflows actually look kind of similar which is normally some kind of like long form rag or maybe",
    "start": "600240",
    "end": "606320"
  },
  {
    "text": "extraction task and you can make decisions based on that and you have to deal with dozens of model types which is",
    "start": "606320",
    "end": "613200"
  },
  {
    "text": "a luxury um you don't just don't have the luxury of those AI labs So here are some differences between the serving",
    "start": "613200",
    "end": "620079"
  },
  {
    "text": "that you'll do and the serving that your AI labs will have to do",
    "start": "620079",
    "end": "626000"
  },
  {
    "text": "So best practices There are literally an infinite infinite number of best",
    "start": "626000",
    "end": "632720"
  },
  {
    "text": "practices that I could give but I've tried to boil them down to I think it's now six cuz I think I added one this",
    "start": "632720",
    "end": "638240"
  },
  {
    "text": "morning Um six non-exhaustive tips for self-hosting LLMs Um this isn't a",
    "start": "638240",
    "end": "643519"
  },
  {
    "text": "complete guide to self-hosting but hopefully there are some useful things um that we've learned over the last",
    "start": "643519",
    "end": "648880"
  },
  {
    "text": "couple years that might be useful to you Um the first one is know your deployment boundaries and work backwards",
    "start": "648880",
    "end": "655920"
  },
  {
    "text": "Quantized models are your friend Getting batching right really really matters Optimize for your workload So",
    "start": "655920",
    "end": "663600"
  },
  {
    "text": "that goes back to what I said earlier about you can do optimizations that they just can't Be sensible about the models",
    "start": "663600",
    "end": "669680"
  },
  {
    "text": "you use And then finally consolidate infrastructure within your org These are the top tips that I'm going to talk",
    "start": "669680",
    "end": "676800"
  },
  {
    "text": "through now for the rest of the session So let's start with deployment",
    "start": "676800",
    "end": "682839"
  },
  {
    "text": "boundaries Assuming you don't have unlimited compute and to be honest even the AI",
    "start": "682839",
    "end": "688240"
  },
  {
    "text": "labs don't have unlimited compute It's a really really scarce resource at the moment you need to be very very aware of",
    "start": "688240",
    "end": "696320"
  },
  {
    "text": "what your deployment requirements are and then work backwards from what you know about them So if you know you only",
    "start": "696320",
    "end": "704760"
  },
  {
    "text": "have a certain available hardware maybe you're for example just CPU bound and you haven't you're deploying completely",
    "start": "704760",
    "end": "711040"
  },
  {
    "text": "on prem and you don't have GPUs then you probably shouldn't be looking into deploying a llama 40 or 5 billion So",
    "start": "711040",
    "end": "716959"
  },
  {
    "text": "knowing that kind of uh boundary is important from the get-go You should have an idea of your target latency as",
    "start": "716959",
    "end": "722560"
  },
  {
    "text": "well and you should have an idea of your expected load If you have all of these",
    "start": "722560",
    "end": "727839"
  },
  {
    "text": "together and you can construct some kind of sentence of I would like to deploy on an instance cheaper than X um which will",
    "start": "727839",
    "end": "736160"
  },
  {
    "text": "serve Y concurrent users in a average latency of of I don't know less than zed",
    "start": "736160",
    "end": "741360"
  },
  {
    "text": "If you can form that kind of sentence it makes everything else that you'll have to do much easier and you won't kind of have that bottleneck of being like oh I",
    "start": "741360",
    "end": "747839"
  },
  {
    "text": "built this great application I just have no idea how to deploy it",
    "start": "747839",
    "end": "753279"
  },
  {
    "text": "I can use that kind of information to then work backwards and figure out what kind of models I should be looking at um",
    "start": "753279",
    "end": "759120"
  },
  {
    "text": "and how much effort I should be putting into things like uh prompting and really refining my search techniques rather",
    "start": "759120",
    "end": "765200"
  },
  {
    "text": "than upgrading to bigger models Which leads me on to my second",
    "start": "765200",
    "end": "770240"
  },
  {
    "text": "tip which is you should pretty much always be using quantized models",
    "start": "770240",
    "end": "776079"
  },
  {
    "text": "um for a couple different reasons The first reason I'm going to say is that you should always use quantized models",
    "start": "776079",
    "end": "781760"
  },
  {
    "text": "more or less is because the accuracy is pretty much always better than a model",
    "start": "781760",
    "end": "787440"
  },
  {
    "text": "of that same uh memory requirement that you've quantized it down to So you retain a lot of that accuracy And the",
    "start": "787440",
    "end": "793519"
  },
  {
    "text": "second reason that you should pretty much always quantize is because actually the accuracy loss isn't that different from the original model And I'll",
    "start": "793519",
    "end": "799760"
  },
  {
    "text": "reference two pieces of uh research that show this So there was this paper that",
    "start": "799760",
    "end": "805440"
  },
  {
    "text": "came out I think it was last year Yeah in 2023 I believe uh by Tim Tim Detmas",
    "start": "805440",
    "end": "811440"
  },
  {
    "text": "um amongst others who was a bit of a legend in the field and it's called the case for four bit uh four bit uh",
    "start": "811440",
    "end": "817959"
  },
  {
    "text": "precision and what he showed in this paper I can show you the kind of highlight figure is that for a fixed",
    "start": "817959",
    "end": "826320"
  },
  {
    "text": "model bit size the accuracy of the model is um is far far higher if you if you're",
    "start": "826320",
    "end": "835040"
  },
  {
    "text": "using a quantized model So given that we know when we have a model with more and more parameters um as I scale up the",
    "start": "835040",
    "end": "841760"
  },
  {
    "text": "parameters the accuracy of the model goes up and up and up But what's interesting is if I take one of those large models and quantize it down to a",
    "start": "841760",
    "end": "848880"
  },
  {
    "text": "natively smaller size it retains a lot of that accuracy that we had in the beginning which is very very good So I'm",
    "start": "848880",
    "end": "854800"
  },
  {
    "text": "pretty much always going to have better performance using a quantized 70 billion parameter model than I will of a model",
    "start": "854800",
    "end": "860560"
  },
  {
    "text": "of a native size This goes on to some great research that neuromagic did on this So they showed",
    "start": "860560",
    "end": "868240"
  },
  {
    "text": "that firstly if I uh quantize models So",
    "start": "868240",
    "end": "873360"
  },
  {
    "text": "here they have um the accuracy recovery So this is when they quantize um take",
    "start": "873360",
    "end": "879120"
  },
  {
    "text": "original bit precision models and then quantize them It pretty much retains all of the accuracy from the original model",
    "start": "879120",
    "end": "886079"
  },
  {
    "text": "which is great So I mean you get like 99% accuracy um maintenance which is",
    "start": "886079",
    "end": "892040"
  },
  {
    "text": "awesome and you can see that even though there are slight dips in the in the",
    "start": "892040",
    "end": "899040"
  },
  {
    "text": "performance so for example if I look at this 405 here you know with one of the quantized variants it's a slight dip in",
    "start": "899040",
    "end": "905680"
  },
  {
    "text": "the original like it's like a couple of basis points it's still far far far",
    "start": "905680",
    "end": "910880"
  },
  {
    "text": "higher than the 70 billion parameter or the 8 billion parameter model So it retains much of that accuracy that you",
    "start": "910880",
    "end": "916639"
  },
  {
    "text": "were looking for So if you have um if you know your deployment uh",
    "start": "916639",
    "end": "923519"
  },
  {
    "text": "boundaries and you don't have like unlimited amounts of compute to call from using that quantized model using",
    "start": "923519",
    "end": "929279"
  },
  {
    "text": "the best model that will fit within that is a great uh piece of advice So if I know that this is the piece of",
    "start": "929279",
    "end": "935279"
  },
  {
    "text": "infrastructure I'm working with this is the GPU I'm working with I can then say okay which is the biggest model that",
    "start": "935279",
    "end": "941519"
  },
  {
    "text": "when quantized down uh to 4bit is going to perform the best So in this case and",
    "start": "941519",
    "end": "946639"
  },
  {
    "text": "this is a couple months old mix is not really a thing anymore but you get the idea that I would rather use the mix",
    "start": "946639",
    "end": "952480"
  },
  {
    "text": "4bit which is in 22 gig than the llama 13B because it's retained a lot of that",
    "start": "952480",
    "end": "957920"
  },
  {
    "text": "that performance And I put at the bottom places that you can find quantized models Um we retain",
    "start": "957920",
    "end": "964959"
  },
  {
    "text": "an open source bank of quantiz models that you can check out Um the bloke also used to be a great source of this as",
    "start": "964959",
    "end": "970000"
  },
  {
    "text": "well although he's not doing it as much recently The third thing I'm going to",
    "start": "970000",
    "end": "975920"
  },
  {
    "text": "talk about is batching strategies Uh this is something that's very very important to get right Um and the reason",
    "start": "975920",
    "end": "982079"
  },
  {
    "text": "it's very important to get right is because it's a very easy way to waste GPU resources",
    "start": "982079",
    "end": "987440"
  },
  {
    "text": "So when you're first deploying you're probably not using any batching Um which",
    "start": "987440",
    "end": "993360"
  },
  {
    "text": "means you end up with GPU utilization coming like this It's not great We then see people going straight",
    "start": "993360",
    "end": "1000399"
  },
  {
    "text": "to dynamic batching So what this means is I have a set batch size So let's say",
    "start": "1000399",
    "end": "1006160"
  },
  {
    "text": "my batch size is 16 I will wait until 16 requests have come in to process or I'll",
    "start": "1006160",
    "end": "1011279"
  },
  {
    "text": "wait a fixed amount of time and you end up with this kind of spiky um GPU utilization which is still not great",
    "start": "1011279",
    "end": "1018600"
  },
  {
    "text": "either What's far far better if if you're deploying generative models so very big piece of advice is to use",
    "start": "1018600",
    "end": "1025120"
  },
  {
    "text": "something like continuous batching where you can get consistently high GPU uh",
    "start": "1025120",
    "end": "1030199"
  },
  {
    "text": "utilization It's a state-of-the-art technique designed for batching of generative models and it allows requests",
    "start": "1030199",
    "end": "1036160"
  },
  {
    "text": "to um interrupt long inflight requests And this batching kind of happens at the token level rather than happening at the",
    "start": "1036160",
    "end": "1042240"
  },
  {
    "text": "happening at the request level So I could for example have my model generate the you know 65th token of one response",
    "start": "1042240",
    "end": "1049840"
  },
  {
    "text": "and then the fifth uh token of another response and I end up with a utilization that is far far far more",
    "start": "1049840",
    "end": "1055720"
  },
  {
    "text": "even This is just one of the examples of like inference optimizations that you can do um that make really big",
    "start": "1055720",
    "end": "1062080"
  },
  {
    "text": "difference So here I think I've gone from a what's that like a 10% utilization to what near enough 80 So",
    "start": "1062080",
    "end": "1069280"
  },
  {
    "text": "really significant improvements and if you're in a regime where you don't have much compute environment or compute",
    "start": "1069280",
    "end": "1075120"
  },
  {
    "text": "resource this is a very very valuable thing to do Okay going to talk about workload",
    "start": "1075120",
    "end": "1081120"
  },
  {
    "text": "optimizations Now this is something I didn't have when I did the London talk Um and it's something that we've been",
    "start": "1081120",
    "end": "1086960"
  },
  {
    "text": "researching a lot actively and we think is really promising So you know something that the AI labs",
    "start": "1086960",
    "end": "1093200"
  },
  {
    "text": "don't know Um which is you know what your workload looks like Um and what that means is you can",
    "start": "1093200",
    "end": "1099520"
  },
  {
    "text": "make a lot of decisions based on what your workload looks like that it would just never ever make sense for them to",
    "start": "1099520",
    "end": "1105840"
  },
  {
    "text": "make So I'm going to give you a couple examples of techniques that you can use",
    "start": "1105840",
    "end": "1113200"
  },
  {
    "text": "that make sense if you know what your workload looks like um that don't make sense if you're serving like big",
    "start": "1113200",
    "end": "1119440"
  },
  {
    "text": "multi-tenant um environments into multiple user groups So one of them is prefix caching Uh this is probably one",
    "start": "1119440",
    "end": "1126880"
  },
  {
    "text": "of my most uh one of the things I'm most excited by that happened this year",
    "start": "1126880",
    "end": "1133280"
  },
  {
    "text": "So one of the most common use cases that we see from our clients is situations where they have really really long",
    "start": "1133280",
    "end": "1138880"
  },
  {
    "text": "prompts and often these prompts are shared between requests So maybe what I have is I",
    "start": "1138880",
    "end": "1145720"
  },
  {
    "text": "have a very long context rag on like a set document that maybe I just pass",
    "start": "1145720",
    "end": "1151760"
  },
  {
    "text": "through the whole document and I'm asking questions of it or maybe I have a situation where um I have a very very",
    "start": "1151760",
    "end": "1159039"
  },
  {
    "text": "long set of um user instructions to the to the LLM So these are instances where",
    "start": "1159039",
    "end": "1164880"
  },
  {
    "text": "I have very very long prompts and traditionally what I would have to do is I'd have to reprocess that prompt every",
    "start": "1164880",
    "end": "1170559"
  },
  {
    "text": "single request which is super inefficient But what we can do if you know what your workload looks like and",
    "start": "1170559",
    "end": "1175919"
  },
  {
    "text": "you know um that you're in a regime where you have a lot of shared prompts or very very long shared prompts you can",
    "start": "1175919",
    "end": "1182080"
  },
  {
    "text": "use something called prefix caching um which is essentially when you premputee the KB cache of text and you reuse that",
    "start": "1182080",
    "end": "1189200"
  },
  {
    "text": "um when the context is reused um when you do your next generation So my LLM doesn't need to reprocess that long",
    "start": "1189200",
    "end": "1195840"
  },
  {
    "text": "prompt every single time They can process just the difference between them So if I have a very very long shared uh",
    "start": "1195840",
    "end": "1201919"
  },
  {
    "text": "prompt and then slightly different things each time it can just process that difference and then",
    "start": "1201919",
    "end": "1207080"
  },
  {
    "text": "return So um on the right I have um some results that are fresh off the press So",
    "start": "1207080",
    "end": "1213679"
  },
  {
    "text": "um my team sent me them yesterday Um what we have here is our server with the",
    "start": "1213679",
    "end": "1220000"
  },
  {
    "text": "prefix caching turned on and turned off So in the green and the green lines we",
    "start": "1220000",
    "end": "1225280"
  },
  {
    "text": "have it with turned off The light green line is with two GPUs and the dark green line is with one",
    "start": "1225280",
    "end": "1231559"
  },
  {
    "text": "GPU and with the blue line we have it with the prefix caching turned on And what you can see is we have very very",
    "start": "1231559",
    "end": "1238320"
  },
  {
    "text": "significant throughput improvements So it's throughput on the Y and then batch size on the X About 7x higher throughput",
    "start": "1238320",
    "end": "1245039"
  },
  {
    "text": "um which is very very significant It means that you can um process many many",
    "start": "1245039",
    "end": "1250080"
  },
  {
    "text": "more requests or you can process those requests much much cheaper This has almost well has like no impact if you",
    "start": "1250080",
    "end": "1257919"
  },
  {
    "text": "didn't know that you had a long shared prompt It doesn't really have an impact if I'm serving um multiple dozens of",
    "start": "1257919",
    "end": "1264559"
  },
  {
    "text": "users at any one time um this only really works if I know what my use case looks",
    "start": "1264559",
    "end": "1270120"
  },
  {
    "text": "like Another example um of caching that we're really excited by So we call this",
    "start": "1270120",
    "end": "1275360"
  },
  {
    "text": "internally um SSD which is a form of spective decoding but you can also think of it like caching which only makes",
    "start": "1275360",
    "end": "1281919"
  },
  {
    "text": "sense if you know what your workload looks like So this is a use case where if your",
    "start": "1281919",
    "end": "1289360"
  },
  {
    "text": "model might predict similar tokens between tasks So let's say I'm doing a",
    "start": "1289360",
    "end": "1295440"
  },
  {
    "text": "task where I have phrases that are very very frequently repeated What I can do",
    "start": "1295440",
    "end": "1301120"
  },
  {
    "text": "is I can essentially cache those very frequent um those very very frequently",
    "start": "1301120",
    "end": "1306240"
  },
  {
    "text": "repeated uh phrases and instead of computing the whole phrase every single time token by token I can just you know",
    "start": "1306240",
    "end": "1312159"
  },
  {
    "text": "pull a cache and and uh inference on that So we did benchmarking on this last week",
    "start": "1312159",
    "end": "1318960"
  },
  {
    "text": "Um and for a workload it was a JSON extractive workload Uh we got about a 2.5 latency decrease um on this which is",
    "start": "1318960",
    "end": "1327039"
  },
  {
    "text": "pretty significant Again would have literally no impact if I'm using the OpenAI API because there's no way they",
    "start": "1327039",
    "end": "1333440"
  },
  {
    "text": "could cache your responses and you know 75 million other people's responses So it only really makes sense if you're",
    "start": "1333440",
    "end": "1339200"
  },
  {
    "text": "deploying in your own environment Cool So I have two more that",
    "start": "1339200",
    "end": "1345919"
  },
  {
    "text": "I'm going to go through Uh this one is model selection I think everyone knows",
    "start": "1345919",
    "end": "1351120"
  },
  {
    "text": "this by now but the only real reason I put it up is because um last time people really liked the uh like like the image",
    "start": "1351120",
    "end": "1357200"
  },
  {
    "text": "Um this is something that I still see people getting wrong which is they get their large language models which are",
    "start": "1357200",
    "end": "1363120"
  },
  {
    "text": "the most difficult things to deploy and the most expensive things to deploy to do pretty much all of the workload Um",
    "start": "1363120",
    "end": "1370240"
  },
  {
    "text": "and that's not a good idea And the reason it's not a good idea is because you then have more of this difficult",
    "start": "1370240",
    "end": "1376000"
  },
  {
    "text": "expensive thing to manage There are a lot of pi parts to an enterprise rag pipeline Um in fact the most important",
    "start": "1376000",
    "end": "1382880"
  },
  {
    "text": "part of your enterprise rag pipeline is actually not the LLM at all The most important part is your embedding and",
    "start": "1382880",
    "end": "1388240"
  },
  {
    "text": "retrieval part by far Um and they don't need GPT4 level",
    "start": "1388240",
    "end": "1394159"
  },
  {
    "text": "reasoning obviously Um and if you have really good uh search and retrieval pipelines you can actually use much much",
    "start": "1394159",
    "end": "1400720"
  },
  {
    "text": "much smaller models to get the same kind of results So we've seen clients move from GPT4 type setups to move to things",
    "start": "1400720",
    "end": "1408880"
  },
  {
    "text": "as small as uh the Llama 2 billion models if they get their retrieval part right which means you can run um the",
    "start": "1408880",
    "end": "1415280"
  },
  {
    "text": "whole um the whole application very very cheaply Um I would advise people to check out",
    "start": "1415280",
    "end": "1422080"
  },
  {
    "text": "the Gemma and the small uh llama models for this We think they're very very good very good on extractive workloads as",
    "start": "1422080",
    "end": "1428240"
  },
  {
    "text": "well especially if you're using a JSON um adjacent structured format like um",
    "start": "1428240",
    "end": "1433280"
  },
  {
    "text": "outlines or LMF So pick your application design",
    "start": "1433280",
    "end": "1438880"
  },
  {
    "text": "carefully and only use big models when you really need to use big models Finally I'm going to talk about",
    "start": "1438880",
    "end": "1445200"
  },
  {
    "text": "infrastructure consolidation Um in traditional ML most teams that actually work in silos still",
    "start": "1445200",
    "end": "1452240"
  },
  {
    "text": "unfortunately which is they deploy their own model and they're responsible for deploying their own model Um in genai it",
    "start": "1452240",
    "end": "1458960"
  },
  {
    "text": "does not make sense to do that and that's why these API providers are making an absolute killing because it",
    "start": "1458960",
    "end": "1464159"
  },
  {
    "text": "does make sense to you know deploy once manage centrally and then provide it as a shared resource within the",
    "start": "1464159",
    "end": "1469880"
  },
  {
    "text": "organization Um and it and if you do this it enables your end developers to",
    "start": "1469880",
    "end": "1475919"
  },
  {
    "text": "focus on building this application level rather than building the infrastructure So what I almost when I speak to my",
    "start": "1475919",
    "end": "1481679"
  },
  {
    "text": "clients sometimes we talk about like building an internal version of the OpenAI API That's kind of the experience",
    "start": "1481679",
    "end": "1488000"
  },
  {
    "text": "you want to be able to provide to your uh developers because otherwise they'll have to learn what prefix caching is",
    "start": "1488000",
    "end": "1493279"
  },
  {
    "text": "every single time they want to deploy a model which is just not an efficient use of time So I think this kind of",
    "start": "1493279",
    "end": "1498960"
  },
  {
    "text": "centralization is a really exciting opportunity for MLOps teams um to take that kind of ownership within the",
    "start": "1498960",
    "end": "1504919"
  },
  {
    "text": "org This still means you know you can understand the kind of workloads you work with and optimize for for example",
    "start": "1504919",
    "end": "1511360"
  },
  {
    "text": "long context rag situations that's still something that you can do and so you can actually end up making a cheaper and",
    "start": "1511360",
    "end": "1517520"
  },
  {
    "text": "better version of something like the open AAI API Um so serving is hard Don't",
    "start": "1517520",
    "end": "1523520"
  },
  {
    "text": "do it twice or even three or we've seen clients who do it over and over again So you'll have a team deploying with O Lama",
    "start": "1523520",
    "end": "1529120"
  },
  {
    "text": "another team deploying with BLM and you know trying to do it much better to have a central team manage",
    "start": "1529120",
    "end": "1535159"
  },
  {
    "text": "that and GPUs are very expensive Don't waste them So I can talk about what this would",
    "start": "1535159",
    "end": "1541440"
  },
  {
    "text": "actually look like in practice um this central compute uh this central",
    "start": "1541440",
    "end": "1546559"
  },
  {
    "text": "consolidation of infrastructure you can kind of think of it as having like an internal bedrock or an internal uh open",
    "start": "1546559",
    "end": "1553039"
  },
  {
    "text": "AI API or something like that and individual ML teams can work from these APIs and then build applications on top",
    "start": "1553039",
    "end": "1560400"
  },
  {
    "text": "of them using techniques like Laura so that's a form of um uh fine-tuning or",
    "start": "1560400",
    "end": "1566080"
  },
  {
    "text": "use things like rag um where they're plugging into it in the same way they use the open AI",
    "start": "1566080",
    "end": "1571320"
  },
  {
    "text": "API we uh a quick case study So a client of ours uh did this So they started off",
    "start": "1571320",
    "end": "1576880"
  },
  {
    "text": "with a bunch of different um models Each app was essentially attached to its own model and attached to its own GPU setup",
    "start": "1576880",
    "end": "1584320"
  },
  {
    "text": "Uh this obviously not ideal because I'm deploying for example at the time it was a mix We were deploying that multiple",
    "start": "1584320",
    "end": "1589760"
  },
  {
    "text": "times over and over again and wasting GPU resources Instead what we did is we just kind of like pulled it into one um",
    "start": "1589760",
    "end": "1597600"
  },
  {
    "text": "and deployed that mixture on a centrally um hosted infrastructure which meant we could use much much less GPU resource",
    "start": "1597600",
    "end": "1603919"
  },
  {
    "text": "than we were using otherwise So something that I like the",
    "start": "1603919",
    "end": "1610400"
  },
  {
    "text": "pattern of is giving your team so if you're doing this central deployment and central hosting is give your team access",
    "start": "1610400",
    "end": "1617600"
  },
  {
    "text": "to a couple different models of different sizes Give them optionality So don't say something like you can only",
    "start": "1617600",
    "end": "1623840"
  },
  {
    "text": "use this particular model because you know they want to be able to try things but deploying like a large medium and",
    "start": "1623840",
    "end": "1630159"
  },
  {
    "text": "small model um that you guys have checked out and that you're happy with is a really good place to start with a",
    "start": "1630159",
    "end": "1635840"
  },
  {
    "text": "bunch of you know maybe those auxiliary models so table passes embedding models reank models etc etc Um if you do that",
    "start": "1635840",
    "end": "1642880"
  },
  {
    "text": "you give your team optionality and you still maintain the benefit of being able to consolidate that infrastructure and",
    "start": "1642880",
    "end": "1649039"
  },
  {
    "text": "make those optimizations So there's my six",
    "start": "1649039",
    "end": "1655279"
  },
  {
    "text": "non-exhaustive tips for self-hosting LLMs Know your deployment boundaries and work backwards If you do know this",
    "start": "1655279",
    "end": "1661840"
  },
  {
    "text": "everything else becomes much easier Quantized models are your friends um they might seem scary because they",
    "start": "1661840",
    "end": "1668159"
  },
  {
    "text": "affect the model but they're actually on the whole much better Getting batching right really matters for your GPU",
    "start": "1668159",
    "end": "1674679"
  },
  {
    "text": "utilization Optimize for your particular workload You have a superpower that the API providers do not which is you know",
    "start": "1674679",
    "end": "1681120"
  },
  {
    "text": "what your workload will look like Be sensible about the models you use And then finally consolidate infrastructure",
    "start": "1681120",
    "end": "1686880"
  },
  {
    "text": "within your organization So today we've gone through why you should self-host Um and",
    "start": "1686880",
    "end": "1694159"
  },
  {
    "text": "there's a bunch of self um there's a bunch of infrastructural reasons why self-hosting is a great idea Um",
    "start": "1694159",
    "end": "1699760"
  },
  {
    "text": "deploying at scale um if you want to use embedding models reanker models if you",
    "start": "1699760",
    "end": "1705039"
  },
  {
    "text": "um for example are using domain specific models all very good reasons to self-host your needs in deploying will",
    "start": "1705039",
    "end": "1711840"
  },
  {
    "text": "be different to the needs of AI labs and mass AI um AI API providers because you're using different types of hardware",
    "start": "1711840",
    "end": "1718399"
  },
  {
    "text": "and your workloads are different And I've also given six tips best practices",
    "start": "1718399",
    "end": "1724399"
  },
  {
    "text": "that we've learned over the last couple years for deploying LLMs in this kind of environment So thank you very much And",
    "start": "1724399",
    "end": "1731200"
  },
  {
    "text": "any questions [Applause]",
    "start": "1731200",
    "end": "1739420"
  },
  {
    "text": "as usual there's lots of great tips there So uh if you have any questions raise your hand We'll give the mic to",
    "start": "1739440",
    "end": "1745360"
  },
  {
    "text": "you If not let me ask started a question Um",
    "start": "1745360",
    "end": "1751360"
  },
  {
    "text": "for those tips that you gave if I have a bursty workload um are any of those tips",
    "start": "1751360",
    "end": "1757919"
  },
  {
    "text": "more relevant than others that's a great question So if you have a bursty",
    "start": "1757919",
    "end": "1763480"
  },
  {
    "text": "workload that's kind of it's kind of rough right because um there's always going to be",
    "start": "1763480",
    "end": "1770240"
  },
  {
    "text": "some kind of cold start problem when you're deploying these So for bursty workloads they're slightly less well",
    "start": "1770240",
    "end": "1777279"
  },
  {
    "text": "suited for self-hosting unless you're deploying smaller models So if you're deploying smaller models the scaling up",
    "start": "1777279",
    "end": "1782480"
  },
  {
    "text": "and scaling down is much much easier But if you're deploying bigger models I mean consider whether you do need to actually",
    "start": "1782480",
    "end": "1787760"
  },
  {
    "text": "self-host or whether you can use other providers I mean we have uh within our infrastructure obviously built like you",
    "start": "1787760",
    "end": "1794640"
  },
  {
    "text": "know Kubernetes resources that can do that scaling up and scaling down Um but if you have bursty workloads with very",
    "start": "1794640",
    "end": "1800559"
  },
  {
    "text": "very low latency requirements that is just challenging Um what you could do um and what we have seen people do is if",
    "start": "1800559",
    "end": "1807679"
  },
  {
    "text": "they have a bursty workload migrating to smaller and cheaper models during the periods of like high burst um and then",
    "start": "1807679",
    "end": "1814799"
  },
  {
    "text": "going back to the higher performance models when you know things are a bit more chill So you could imagine a regime",
    "start": "1814799",
    "end": "1821360"
  },
  {
    "text": "where you had a 70 billion parameter model um servicing most of the requests",
    "start": "1821360",
    "end": "1827039"
  },
  {
    "text": "and then you know let's say you get to periods of very very high load um while you're waiting to scale up you move to",
    "start": "1827039",
    "end": "1832480"
  },
  {
    "text": "something like an 8 billion parameter model or something slightly smaller you take a small accuracy hit um and then go",
    "start": "1832480",
    "end": "1837760"
  },
  {
    "text": "from there So that's something that we've seen work as well Any other questions so there's hands",
    "start": "1837760",
    "end": "1843760"
  },
  {
    "text": "there [Music]",
    "start": "1843760",
    "end": "1850429"
  },
  {
    "text": "Hi thank you Great talk Um one question would be how would you architect kind of",
    "start": "1854320",
    "end": "1860559"
  },
  {
    "text": "a central compute infra if each downstream application requires different fine-tuning where you can't",
    "start": "1860559",
    "end": "1867200"
  },
  {
    "text": "really have an internal open AI API like serving infrastructure that's an awesome",
    "start": "1867200",
    "end": "1872240"
  },
  {
    "text": "question Um let's go back to him So um that's very difficult if you want to do",
    "start": "1872240",
    "end": "1878240"
  },
  {
    "text": "something like full fine-tuning which we've been used to over the last couple years which essentially you fine-tune the whole model because what that means",
    "start": "1878240",
    "end": "1884080"
  },
  {
    "text": "is every single time you fine-tune the model you have to deploy a separate instance of it Um fortunately over the",
    "start": "1884080",
    "end": "1889760"
  },
  {
    "text": "last I guess like two years um there's been advancements in PET methods um",
    "start": "1889760",
    "end": "1896000"
  },
  {
    "text": "which is essentially a way that you can fine-tune but you don't have to fine-tune the whole model you can just fine-tune a small subsection of it So",
    "start": "1896000",
    "end": "1903840"
  },
  {
    "text": "let's say I have a situation where I'm centrally hosting and I have 10",
    "start": "1903840",
    "end": "1909200"
  },
  {
    "text": "different teams and every single team has done a fine-tuned version of a llama model for example What I used to have to",
    "start": "1909200",
    "end": "1916399"
  },
  {
    "text": "do is go in and deploy each of those on like their own GPUs or shared GPUs What I can do now instead is just deploy a",
    "start": "1916399",
    "end": "1923039"
  },
  {
    "text": "llama model like a centrally hosted llama models and then deploy on the same GPU still in my same server all of these",
    "start": "1923039",
    "end": "1929679"
  },
  {
    "text": "different Laura fine-tuned versions So this like these small adapters that we add to all of the models and on",
    "start": "1929679",
    "end": "1936000"
  },
  {
    "text": "inference time we can hot swap them in and out So it's like you're calling separate models but actually it's one",
    "start": "1936000",
    "end": "1941840"
  },
  {
    "text": "base models and these different adapters and that's really really exciting because it means we can deploy hundreds if not thousands of fine-tuned domain",
    "start": "1941840",
    "end": "1949120"
  },
  {
    "text": "specific models with the exact same infrastructure resources that we needed to deploy one So that's the way I would",
    "start": "1949120",
    "end": "1954399"
  },
  {
    "text": "go about it PEFT methods I mean we prefer Lauras as a method but you know there are other other available on the",
    "start": "1954399",
    "end": "1960080"
  },
  {
    "text": "market",
    "start": "1960080",
    "end": "1962799"
  },
  {
    "text": "Uh yeah one quick question So you're speaking a lot about the models themselves which is awesome but in your",
    "start": "1968320",
    "end": "1973760"
  },
  {
    "text": "diagrams when you're uh referring to GPUs and you know all the work that you did is should we assume most of this is",
    "start": "1973760",
    "end": "1980640"
  },
  {
    "text": "based on HB100 Black well Nvidia or did you actually go across gaudy and uh",
    "start": "1980640",
    "end": "1985679"
  },
  {
    "text": "Instinct from AMD and and so on and so forth um I'm just curious because we're looking to build something and you know",
    "start": "1985679",
    "end": "1992080"
  },
  {
    "text": "support for all models is not ubiquous right now So yeah where did you stop and start with how far your suggestions go",
    "start": "1992080",
    "end": "1997919"
  },
  {
    "text": "here on some of that hardware level it's a great question So um we're kind we",
    "start": "1997919",
    "end": "2004799"
  },
  {
    "text": "very rarely see clients deploying on like H100s or B100s The kinds of clients that we work with are like enterprise",
    "start": "2004799",
    "end": "2011200"
  },
  {
    "text": "and corporates which might have access to either the previous generations of GPUs or maybe slightly cheaper GPUs Um",
    "start": "2011200",
    "end": "2018559"
  },
  {
    "text": "so we tend to do most of our benchmarking on those kinds of of GPUs",
    "start": "2018559",
    "end": "2023600"
  },
  {
    "text": "Um and we support uh Nvidia and AMD and",
    "start": "2023600",
    "end": "2029200"
  },
  {
    "text": "we're rolling out support for Intel as well Um we have seen good results on AMD",
    "start": "2029200",
    "end": "2035200"
  },
  {
    "text": "We think they're a really interesting option Um and then for Intel we're kind of waiting for the software support to",
    "start": "2035200",
    "end": "2040880"
  },
  {
    "text": "to catch up there Um and we also know that like inferentia are doing interesting things and the TPUs are",
    "start": "2040880",
    "end": "2046320"
  },
  {
    "text": "doing interesting things as well So there's definitely more out there than the B100s and H100s that you see And for",
    "start": "2046320",
    "end": "2052960"
  },
  {
    "text": "most enterprises you really really don't need them Um and that you're probably fine with previous generations of Nvidia",
    "start": "2052960",
    "end": "2059679"
  },
  {
    "text": "hardware My recommendation would be to stick with Nvidia mainly because the",
    "start": "2059679",
    "end": "2064878"
  },
  {
    "text": "software stack is a bit more evolved Um but I would build your infrastructure",
    "start": "2064879",
    "end": "2070079"
  },
  {
    "text": "and applications such that you can move to AMD or you can move to other providers in a couple years because my",
    "start": "2070079",
    "end": "2076158"
  },
  {
    "text": "guess is that gap is going to close um pretty quickly So you don't want to tie yourself to like an Nvidia native",
    "start": "2076159",
    "end": "2082040"
  },
  {
    "text": "everything Out of interest what are you guys using we're using OpenAI off of Azure So we're",
    "start": "2082040",
    "end": "2089358"
  },
  {
    "text": "using Nvidia but I'm looking to build something out locally because we'll balance loads between cloud and onrem",
    "start": "2089359",
    "end": "2094878"
  },
  {
    "text": "for development or even just spilling over extra load So keeping mine open to everything But great talk Thank you Yeah",
    "start": "2094879",
    "end": "2101760"
  },
  {
    "text": "Yeah We we have something funny um which is one of our you know selling points is that we are um hardware agnostic So we",
    "start": "2101760",
    "end": "2109359"
  },
  {
    "text": "can deploy on not just Nvidia but others as well And the conversation always goes something like can I deploy on AMD and",
    "start": "2109359",
    "end": "2116000"
  },
  {
    "text": "we're like yes Do you use AMD no but I might want to Um so it's a good thing to keep your options open there",
    "start": "2116000",
    "end": "2123880"
  },
  {
    "text": "Nice one Couple It's very cool",
    "start": "2138599",
    "end": "2144640"
  },
  {
    "text": "Yeah Thanks for the presentation Uh could you elaborate on some of the batching techniques yeah",
    "start": "2144640",
    "end": "2153160"
  },
  {
    "text": "Um so if I go back to batching Um so let's assume we're not",
    "start": "2153160",
    "end": "2160640"
  },
  {
    "text": "going to do no batching and we're going to do some kind of batching So if I'm doing dynamic batching what's happening",
    "start": "2160640",
    "end": "2165680"
  },
  {
    "text": "is this is batching on request level So let's say my batch size is 16 I'm going",
    "start": "2165680",
    "end": "2172000"
  },
  {
    "text": "to wait for 16 requests to come in and then I'm going to process them all Um if I am in a period of very very low",
    "start": "2172000",
    "end": "2179599"
  },
  {
    "text": "traffic I'll also have some kind of time as well So maybe either 16 or a couple seconds and then and then I'll process",
    "start": "2179599",
    "end": "2185839"
  },
  {
    "text": "that batch What that means is I get kind of this spiky workload where I'm waiting",
    "start": "2185839",
    "end": "2190960"
  },
  {
    "text": "for long requests to finish and stuff like that What we can do instead is this continuous batching So um what this",
    "start": "2190960",
    "end": "2198480"
  },
  {
    "text": "allows you to do is it allows you to interrupt very very long u uh long",
    "start": "2198480",
    "end": "2204400"
  },
  {
    "text": "responses with other responses as well So this means that we can do essentially",
    "start": "2204400",
    "end": "2209520"
  },
  {
    "text": "like token level processing rather than like request level So I might have a situation where I have a very long um a",
    "start": "2209520",
    "end": "2216720"
  },
  {
    "text": "very long response and I can interrupt that response every so often to process um shorter responses as well And it",
    "start": "2216720",
    "end": "2223680"
  },
  {
    "text": "means that my GPU is always at very very high utilization because I'm constantly feeding it new things to do So we end up",
    "start": "2223680",
    "end": "2229920"
  },
  {
    "text": "with continuous batching which is much better GPU utilization",
    "start": "2229920",
    "end": "2235400"
  },
  {
    "text": "Just token level Yeah From there Okay Cool Any other questions",
    "start": "2237040",
    "end": "2244359"
  },
  {
    "text": "apologies I consider myself a bit of a novice in the space but I I just maybe just me but how how does interrupting",
    "start": "2251839",
    "end": "2259520"
  },
  {
    "text": "and giving it a different task improves GPU utilization why is not just",
    "start": "2259520",
    "end": "2264960"
  },
  {
    "text": "continuing with the current task uh gives you the the peak utilization at all times",
    "start": "2264960",
    "end": "2271640"
  },
  {
    "text": "um it's a good so essentially it's because otherwise you have to wait for",
    "start": "2271640",
    "end": "2278320"
  },
  {
    "text": "that whole request to finish and the GPU is able to process things in parallel So it doesn't actually affect the quality",
    "start": "2278320",
    "end": "2284240"
  },
  {
    "text": "of the output at all Um it just means that for each pass I can just um have it",
    "start": "2284240",
    "end": "2289520"
  },
  {
    "text": "look predict different tokens from different requests So it's constantly being fed with new new things rather",
    "start": "2289520",
    "end": "2295040"
  },
  {
    "text": "than you know waiting for the longest response to finish and it's only working on that one one response while you know",
    "start": "2295040",
    "end": "2300800"
  },
  {
    "text": "we're waiting for a new batch Um so you're suggesting I can put multiple requests in at the same time in a single",
    "start": "2300800",
    "end": "2306800"
  },
  {
    "text": "request to the GPU Exactly So what you end up with is your latency on average",
    "start": "2306800",
    "end": "2312000"
  },
  {
    "text": "is a little bit higher but my throughput is much much is much better So if I batch these requests together and",
    "start": "2312000",
    "end": "2317280"
  },
  {
    "text": "process them together I end up getting much better throughput than if I was just to process them one by one Okay And",
    "start": "2317280",
    "end": "2323200"
  },
  {
    "text": "and apologies for the silly question but when when I do give it a long running request what is GPU doing if it's not",
    "start": "2323200",
    "end": "2329680"
  },
  {
    "text": "being utilized 100% What is taking that long sorry what",
    "start": "2329680",
    "end": "2334720"
  },
  {
    "text": "do you mean what why is the request taking that long while GPU is not being utilized what is going on in in in that",
    "start": "2334720",
    "end": "2340880"
  },
  {
    "text": "in that setup when the GPU is not being utilized yeah But you're saying I'm putting on a long run request So it is",
    "start": "2340880",
    "end": "2346400"
  },
  {
    "text": "running Yeah What is running if the GPU is not being utilized at the same time to maximum capacity yeah So if it's",
    "start": "2346400",
    "end": "2354240"
  },
  {
    "text": "essentially doing each request one by one by one So I have to wait until the next one Um but we can like afterwards I",
    "start": "2354240",
    "end": "2360960"
  },
  {
    "text": "have a fairly good article um I think base 10 wrote that I'll share with you which also was very very good in this and I can also pull up some examples I",
    "start": "2360960",
    "end": "2367520"
  },
  {
    "text": "have on my phone and that Thank you Thanks Cool Thank you Any other ones before we call",
    "start": "2367520",
    "end": "2374359"
  },
  {
    "text": "it all right If not let's give another hands for Miriam Thank you",
    "start": "2374359",
    "end": "2380680"
  }
]