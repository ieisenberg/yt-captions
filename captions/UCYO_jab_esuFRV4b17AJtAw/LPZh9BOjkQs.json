[
  {
    "text": "Imagine you happen across a short movie script that",
    "start": "1140",
    "end": "3975"
  },
  {
    "text": "describes a scene between a person and their AI assistant.",
    "start": "3976",
    "end": "7140"
  },
  {
    "text": "The script has what the person asks the AI, but the AI's response has been torn off.",
    "start": "7480",
    "end": "13059"
  },
  {
    "text": "Suppose you also have this powerful magical machine that can take",
    "start": "13060",
    "end": "16980"
  },
  {
    "text": "any text and provide a sensible prediction of what word comes next.",
    "start": "16980",
    "end": "20960"
  },
  {
    "text": "You could then finish the script by feeding in what you have to the machine,",
    "start": "21500",
    "end": "25506"
  },
  {
    "text": "seeing what it would predict to start the AI's answer,",
    "start": "25506",
    "end": "28368"
  },
  {
    "text": "and then repeating this over and over with a growing script completing the dialogue.",
    "start": "28368",
    "end": "32740"
  },
  {
    "text": "When you interact with a chatbot, this is exactly what's happening.",
    "start": "33380",
    "end": "36480"
  },
  {
    "text": "A large language model is a sophisticated mathematical function",
    "start": "37020",
    "end": "40701"
  },
  {
    "text": "that predicts what word comes next for any piece of text.",
    "start": "40701",
    "end": "43980"
  },
  {
    "text": "Instead of predicting one word with certainty, though,",
    "start": "44380",
    "end": "47402"
  },
  {
    "text": "what it does is assign a probability to all possible next words.",
    "start": "47402",
    "end": "50920"
  },
  {
    "text": "To build a chatbot, you lay out some text that describes an interaction between a user",
    "start": "51620",
    "end": "56800"
  },
  {
    "text": "and a hypothetical AI assistant, add on whatever the user types in as the first part of",
    "start": "56800",
    "end": "62040"
  },
  {
    "text": "the interaction, and then have the model repeatedly predict the next word that such a",
    "start": "62040",
    "end": "67160"
  },
  {
    "text": "hypothetical AI assistant would say in response, and that's what's presented to the user.",
    "start": "67160",
    "end": "72460"
  },
  {
    "text": "In doing this, the output tends to look a lot more natural if",
    "start": "73080",
    "end": "76214"
  },
  {
    "text": "you allow it to select less likely words along the way at random.",
    "start": "76214",
    "end": "79500"
  },
  {
    "text": "So what this means is even though the model itself is deterministic,",
    "start": "80140",
    "end": "83620"
  },
  {
    "text": "a given prompt typically gives a different answer each time it's run.",
    "start": "83620",
    "end": "87100"
  },
  {
    "text": "Models learn how to make these predictions by processing an enormous amount of text,",
    "start": "88040",
    "end": "92332"
  },
  {
    "text": "typically pulled from the internet.",
    "start": "92332",
    "end": "94100"
  },
  {
    "text": "For a standard human to read the amount of text that was used to train GPT-3,",
    "start": "94100",
    "end": "99471"
  },
  {
    "text": "for example, if they read non-stop 24-7, it would take over 2600 years.",
    "start": "99471",
    "end": "104360"
  },
  {
    "text": "Larger models since then train on much, much more.",
    "start": "104720",
    "end": "107340"
  },
  {
    "text": "You can think of training a little bit like tuning the dials on a big machine.",
    "start": "108200",
    "end": "111780"
  },
  {
    "text": "The way that a language model behaves is entirely determined by these",
    "start": "112280",
    "end": "116300"
  },
  {
    "text": "many different continuous values, usually called parameters or weights.",
    "start": "116301",
    "end": "120380"
  },
  {
    "text": "Changing those parameters will change the probabilities",
    "start": "121020",
    "end": "124099"
  },
  {
    "text": "that the model gives for the next word on a given input.",
    "start": "124099",
    "end": "127180"
  },
  {
    "text": "What puts the large in large language model is how",
    "start": "127860",
    "end": "130727"
  },
  {
    "text": "they can have hundreds of billions of these parameters.",
    "start": "130727",
    "end": "133820"
  },
  {
    "text": "No human ever deliberately sets those parameters.",
    "start": "135200",
    "end": "138040"
  },
  {
    "text": "Instead, they begin at random, meaning the model just outputs gibberish,",
    "start": "138440",
    "end": "142643"
  },
  {
    "text": "but they're repeatedly refined based on many example pieces of text.",
    "start": "142643",
    "end": "146560"
  },
  {
    "text": "One of these training examples could be just a handful of words,",
    "start": "147140",
    "end": "150656"
  },
  {
    "text": "or it could be thousands, but in either case, the way this works is to",
    "start": "150656",
    "end": "154496"
  },
  {
    "text": "pass in all but the last word from that example into the model and",
    "start": "154496",
    "end": "158120"
  },
  {
    "text": "compare the prediction that it makes with the true last word from the example.",
    "start": "158120",
    "end": "162340"
  },
  {
    "text": "An algorithm called backpropagation is used to tweak all of the parameters",
    "start": "163260",
    "end": "167392"
  },
  {
    "text": "in such a way that it makes the model a little more likely to choose",
    "start": "167393",
    "end": "171196"
  },
  {
    "text": "the true last word and a little less likely to choose all the others.",
    "start": "171196",
    "end": "175000"
  },
  {
    "text": "When you do this for many, many trillions of examples,",
    "start": "175740",
    "end": "178750"
  },
  {
    "text": "not only does the model start to give more accurate predictions on the training data,",
    "start": "178750",
    "end": "183458"
  },
  {
    "text": "but it also starts to make more reasonable predictions on text that it's never",
    "start": "183458",
    "end": "187782"
  },
  {
    "text": "seen before.",
    "start": "187783",
    "end": "188440"
  },
  {
    "text": "Given the huge number of parameters and the enormous amount of training data,",
    "start": "189420",
    "end": "193918"
  },
  {
    "text": "the scale of computation involved in training a large language model is mind-boggling.",
    "start": "193919",
    "end": "198880"
  },
  {
    "text": "To illustrate, imagine that you could perform one",
    "start": "199600",
    "end": "202285"
  },
  {
    "text": "billion additions and multiplications every single second.",
    "start": "202285",
    "end": "205400"
  },
  {
    "text": "How long do you think it would take for you to do all of the",
    "start": "206060",
    "end": "209326"
  },
  {
    "text": "operations involved in training the largest language models?",
    "start": "209326",
    "end": "212540"
  },
  {
    "text": "Do you think it would take a year?",
    "start": "213460",
    "end": "215039"
  },
  {
    "text": "Maybe something like 10,000 years?",
    "start": "216039",
    "end": "217960"
  },
  {
    "text": "The answer is actually much more than that.",
    "start": "219020",
    "end": "220800"
  },
  {
    "text": "It's well over 100 million years.",
    "start": "221120",
    "end": "223900"
  },
  {
    "text": "This is only part of the story, though.",
    "start": "225520",
    "end": "227360"
  },
  {
    "text": "This whole process is called pre-training.",
    "start": "227540",
    "end": "229219"
  },
  {
    "text": "The goal of auto-completing a random passage of text from the",
    "start": "229500",
    "end": "232646"
  },
  {
    "text": "internet is very different from the goal of being a good AI assistant.",
    "start": "232646",
    "end": "236200"
  },
  {
    "text": "To address this, chatbots undergo another type of training,",
    "start": "236880",
    "end": "240080"
  },
  {
    "text": "just as important, called reinforcement learning with human feedback.",
    "start": "240080",
    "end": "243760"
  },
  {
    "text": "Workers flag unhelpful or problematic predictions,",
    "start": "244480",
    "end": "247498"
  },
  {
    "text": "and their corrections further change the model's parameters,",
    "start": "247498",
    "end": "251109"
  },
  {
    "text": "making them more likely to give predictions that users prefer.",
    "start": "251109",
    "end": "254780"
  },
  {
    "text": "Looking back at the pre-training, though, this staggering amount of",
    "start": "254780",
    "end": "258859"
  },
  {
    "text": "computation is only made possible by using special computer chips that",
    "start": "258860",
    "end": "263120"
  },
  {
    "text": "are optimized for running many operations in parallel, known as GPUs.",
    "start": "263120",
    "end": "267260"
  },
  {
    "text": "However, not all language models can be easily parallelized.",
    "start": "268120",
    "end": "271620"
  },
  {
    "text": "Prior to 2017, most language models would process text one word at a time,",
    "start": "272080",
    "end": "276817"
  },
  {
    "text": "but then a team of researchers at Google introduced a new model known as the transformer.",
    "start": "276817",
    "end": "282439"
  },
  {
    "text": "Transformers don't read text from the start to the finish,",
    "start": "283300",
    "end": "286745"
  },
  {
    "text": "they soak it all in at once, in parallel.",
    "start": "286745",
    "end": "289139"
  },
  {
    "text": "The very first step inside a transformer, and most other language models for that matter,",
    "start": "289900",
    "end": "294600"
  },
  {
    "text": "is to associate each word with a long list of numbers.",
    "start": "294600",
    "end": "297420"
  },
  {
    "text": "The reason for this is that the training process only works with continuous values,",
    "start": "297860",
    "end": "302396"
  },
  {
    "text": "so you have to somehow encode language using numbers,",
    "start": "302396",
    "end": "305311"
  },
  {
    "text": "and each of these lists of numbers may somehow encode the meaning of the",
    "start": "305312",
    "end": "309254"
  },
  {
    "text": "corresponding word.",
    "start": "309254",
    "end": "310280"
  },
  {
    "text": "What makes transformers unique is their reliance",
    "start": "310280",
    "end": "313360"
  },
  {
    "text": "on a special operation known as attention.",
    "start": "313360",
    "end": "316000"
  },
  {
    "text": "This operation gives all of these lists of numbers a chance to talk to one another",
    "start": "316980",
    "end": "321684"
  },
  {
    "text": "and refine the meanings they encode based on the context around, all done in parallel.",
    "start": "321684",
    "end": "326560"
  },
  {
    "text": "For example, the numbers encoding the word bank might be changed based on the",
    "start": "327400",
    "end": "331707"
  },
  {
    "text": "context surrounding it to somehow encode the more specific notion of a riverbank.",
    "start": "331707",
    "end": "336180"
  },
  {
    "text": "Transformers typically also include a second type of operation known",
    "start": "337280",
    "end": "341029"
  },
  {
    "text": "as a feed-forward neural network, and this gives the model extra",
    "start": "341029",
    "end": "344561"
  },
  {
    "text": "capacity to store more patterns about language learned during training.",
    "start": "344561",
    "end": "348420"
  },
  {
    "text": "All of this data repeatedly flows through many different iterations of",
    "start": "349280",
    "end": "353401"
  },
  {
    "text": "these two fundamental operations, and as it does so,",
    "start": "353401",
    "end": "356478"
  },
  {
    "text": "the hope is that each list of numbers is enriched to encode whatever",
    "start": "356478",
    "end": "360484"
  },
  {
    "text": "information might be needed to make an accurate prediction of what word",
    "start": "360484",
    "end": "364664"
  },
  {
    "text": "follows in the passage.",
    "start": "364664",
    "end": "366000"
  },
  {
    "text": "At the end, one final function is performed on the last vector in this sequence,",
    "start": "367000",
    "end": "371534"
  },
  {
    "text": "which now has had a chance to be influenced by all the other context from the input text,",
    "start": "371534",
    "end": "376573"
  },
  {
    "text": "as well as everything the model learned during training,",
    "start": "376573",
    "end": "379764"
  },
  {
    "text": "to produce a prediction of the next word.",
    "start": "379764",
    "end": "382060"
  },
  {
    "text": "Again, the model's prediction looks like a probability for every possible next word.",
    "start": "382480",
    "end": "387360"
  },
  {
    "text": "Although researchers design the framework for how each of these steps work,",
    "start": "388560",
    "end": "392794"
  },
  {
    "text": "it's important to understand that the specific behavior is an emergent phenomenon",
    "start": "392794",
    "end": "397362"
  },
  {
    "text": "based on how those hundreds of billions of parameters are tuned during training.",
    "start": "397362",
    "end": "401820"
  },
  {
    "text": "This makes it incredibly challenging to determine",
    "start": "402480",
    "end": "405070"
  },
  {
    "text": "why the model makes the exact predictions that it does.",
    "start": "405070",
    "end": "407920"
  },
  {
    "text": "What you can see is that when you use large language model predictions to autocomplete",
    "start": "408440",
    "end": "413778"
  },
  {
    "text": "a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.",
    "start": "413778",
    "end": "419240"
  },
  {
    "text": "If you're a new viewer and you're curious about more details on how",
    "start": "425719",
    "end": "428826"
  },
  {
    "text": "transformers and attention work, boy do I have some material for you.",
    "start": "428826",
    "end": "431979"
  },
  {
    "text": "One option is to jump into a series I made about deep learning,",
    "start": "432399",
    "end": "436080"
  },
  {
    "text": "where we visualize and motivate the details of attention and all the other steps",
    "start": "436080",
    "end": "440740"
  },
  {
    "text": "in a transformer.",
    "start": "440740",
    "end": "441719"
  },
  {
    "text": "Also, on my second channel I just posted a talk I gave a couple",
    "start": "442099",
    "end": "445529"
  },
  {
    "text": "months ago about this topic for the company TNG in Munich.",
    "start": "445529",
    "end": "448638"
  },
  {
    "text": "Sometimes I actually prefer the content I make as a casual talk rather than a produced",
    "start": "449079",
    "end": "453101"
  },
  {
    "text": "video, but I leave it up to you which one of these feels like the better follow-on.",
    "start": "453101",
    "end": "456939"
  }
]