[
  {
    "start": "0",
    "end": "130000"
  },
  {
    "text": "The basic function underlying a normal distribution, ",
    "start": "0",
    "end": "3243"
  },
  {
    "text": "aka a Gaussian, is e to the negative x squared.",
    "start": "3243",
    "end": "6120"
  },
  {
    "text": "But you might wonder, why this function?",
    "start": "6640",
    "end": "8340"
  },
  {
    "text": "Of all the expressions we could dream up that give you some symmetric smooth ",
    "start": "8720",
    "end": "12464"
  },
  {
    "text": "graph with mass concentrated towards the middle, ",
    "start": "12464",
    "end": "14847"
  },
  {
    "text": "why is it that the theory of probability seems to have a special place in its ",
    "start": "14847",
    "end": "18640"
  },
  {
    "text": "heart for this particular expression?",
    "start": "18640",
    "end": "20439"
  },
  {
    "text": "For the last many videos I've been hinting at an answer to this question, ",
    "start": "21380",
    "end": "24662"
  },
  {
    "text": "and here we'll finally arrive at something like a satisfying answer.",
    "start": "24663",
    "end": "27680"
  },
  {
    "text": "As a quick refresher on where we are, a couple videos ago we talked about ",
    "start": "27680",
    "end": "31645"
  },
  {
    "text": "the central limit theorem, which describes how as you add multiple copies ",
    "start": "31645",
    "end": "35610"
  },
  {
    "text": "of a random variable, for example rolling a weighted die many different times, ",
    "start": "35610",
    "end": "39843"
  },
  {
    "text": "or letting a ball bounce off of a peg repeatedly, ",
    "start": "39843",
    "end": "42522"
  },
  {
    "text": "then the distribution describing that sum tends to look approximately like ",
    "start": "42522",
    "end": "46541"
  },
  {
    "text": "a normal distribution.",
    "start": "46541",
    "end": "47720"
  },
  {
    "text": "What the central limit theorem says is as you make that sum bigger and bigger, ",
    "start": "48440",
    "end": "52142"
  },
  {
    "text": "under appropriate conditions, that approximation to a normal becomes better and better.",
    "start": "52142",
    "end": "56219"
  },
  {
    "text": "But I never explained why this theorem is actually true.",
    "start": "56940",
    "end": "60180"
  },
  {
    "text": "We only talked about what it's claiming.",
    "start": "60220",
    "end": "61980"
  },
  {
    "text": "In the last video we started talking about the ",
    "start": "63080",
    "end": "65531"
  },
  {
    "text": "math involved in adding two random variables.",
    "start": "65532",
    "end": "67880"
  },
  {
    "text": "If you have two random variables, each following some distribution, ",
    "start": "68260",
    "end": "71844"
  },
  {
    "text": "then to find the distribution describing the sum of those variables, ",
    "start": "71844",
    "end": "75482"
  },
  {
    "text": "you compute something known as a convolution between the two original functions.",
    "start": "75482",
    "end": "79700"
  },
  {
    "text": "And we spent a lot of time building up two distinct ways ",
    "start": "79880",
    "end": "82964"
  },
  {
    "text": "to visualize what this convolution operation really is.",
    "start": "82964",
    "end": "85939"
  },
  {
    "text": "Today our basic job is to work through a particular example, ",
    "start": "85940",
    "end": "89505"
  },
  {
    "text": "which is to ask what happens when you add two normally distributed random variables, ",
    "start": "89505",
    "end": "94473"
  },
  {
    "text": "which, as you know by now, is the same as asking what do you get if ",
    "start": "94473",
    "end": "98448"
  },
  {
    "text": "you compute a convolution between two Gaussian functions.",
    "start": "98448",
    "end": "101780"
  },
  {
    "text": "I'd like to share an especially pleasing visual way that you can think ",
    "start": "102520",
    "end": "105830"
  },
  {
    "text": "about this calculation, which hopefully offers some sense of what makes ",
    "start": "105831",
    "end": "109188"
  },
  {
    "text": "the e to the negative x squared function special in the first place.",
    "start": "109188",
    "end": "112360"
  },
  {
    "text": "After we walk through it, we'll talk about how this calculation ",
    "start": "112360",
    "end": "115254"
  },
  {
    "text": "is one of the steps involved in proving the central limit theorem.",
    "start": "115254",
    "end": "118240"
  },
  {
    "text": "It's the step that answers the question of why a ",
    "start": "118320",
    "end": "120837"
  },
  {
    "text": "Gaussian and not something else is the central limit.",
    "start": "120837",
    "end": "123560"
  },
  {
    "text": "But first, let's dive in.",
    "start": "124200",
    "end": "125840"
  },
  {
    "text": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
    "start": "129780",
    "end": "134440"
  },
  {
    "start": "130000",
    "end": "218000"
  },
  {
    "text": "The exponent is typically written as negative one half times x divided by sigma squared, ",
    "start": "134820",
    "end": "139483"
  },
  {
    "text": "where sigma describes the spread of the distribution, specifically the standard deviation.",
    "start": "139483",
    "end": "144200"
  },
  {
    "text": "All of this needs to be multiplied by a fraction on the front, ",
    "start": "144680",
    "end": "147881"
  },
  {
    "text": "which is there to make sure that the area under the curve is one, ",
    "start": "147881",
    "end": "151235"
  },
  {
    "text": "making it a valid probability distribution.",
    "start": "151235",
    "end": "153420"
  },
  {
    "text": "And if you want to consider distributions that aren't necessarily centered at zero, ",
    "start": "154020",
    "end": "157875"
  },
  {
    "text": "you would also throw another parameter, mu, into the exponent like this.",
    "start": "157875",
    "end": "161180"
  },
  {
    "text": "Although for everything we'll be doing here, we just consider centered distributions.",
    "start": "161540",
    "end": "165120"
  },
  {
    "text": "Now if you look at our central goal for today, ",
    "start": "165800",
    "end": "168405"
  },
  {
    "text": "which is to compute a convolution between two Gaussian functions, ",
    "start": "168405",
    "end": "172063"
  },
  {
    "text": "the direct way to do this would be to take the definition of a convolution, ",
    "start": "172063",
    "end": "176276"
  },
  {
    "text": "this integral expression we built up last video, ",
    "start": "176276",
    "end": "178992"
  },
  {
    "text": "and then to plug in for each one of the functions involved the formula for a Gaussian.",
    "start": "178992",
    "end": "183760"
  },
  {
    "text": "It's kind of a lot of symbols when you throw it all together, ",
    "start": "184220",
    "end": "186760"
  },
  {
    "text": "but more than anything, working this out is an exercise in completing the square.",
    "start": "186760",
    "end": "190080"
  },
  {
    "text": "And there's nothing wrong with that.",
    "start": "190560",
    "end": "191580"
  },
  {
    "text": "That will get you the answer that you want.",
    "start": "191720",
    "end": "193220"
  },
  {
    "text": "But of course, you know me, I'm a sucker for visual intuition, and in this case, ",
    "start": "193760",
    "end": "197504"
  },
  {
    "text": "there's another way to think about it that I haven't seen written about before ",
    "start": "197504",
    "end": "201155"
  },
  {
    "text": "that offers a very nice connection to other aspects of this distribution, ",
    "start": "201156",
    "end": "204577"
  },
  {
    "text": "like the presence of pi and certain ways to derive where it comes from.",
    "start": "204577",
    "end": "207860"
  },
  {
    "text": "And the way I'd like to do this is by first peeling away all of the ",
    "start": "208200",
    "end": "211437"
  },
  {
    "text": "constants associated with the actual distribution, ",
    "start": "211437",
    "end": "213864"
  },
  {
    "text": "and just showing the computation for the simplified form, e to the negative x squared.",
    "start": "213865",
    "end": "217960"
  },
  {
    "text": "The essence of what we want to compute is what the ",
    "start": "217960",
    "end": "220797"
  },
  {
    "start": "218000",
    "end": "507000"
  },
  {
    "text": "convolution between two copies of this function looks like.",
    "start": "220797",
    "end": "224080"
  },
  {
    "text": "If you'll remember, in the last video we had two different ways to visualize ",
    "start": "224460",
    "end": "228360"
  },
  {
    "text": "convolutions, and the one we'll be using here is the second one involving diagonal slices.",
    "start": "228360",
    "end": "232920"
  },
  {
    "text": "And as a quick reminder of the way that worked, ",
    "start": "233280",
    "end": "235902"
  },
  {
    "text": "if you have two different distributions that are described by two different functions, ",
    "start": "235902",
    "end": "240655"
  },
  {
    "text": "f and g, then every possible pair of values that you might get when you ",
    "start": "240655",
    "end": "244588"
  },
  {
    "text": "sample from these two distributions can be thought of as individual points ",
    "start": "244588",
    "end": "248685"
  },
  {
    "text": "on the xy-plane.",
    "start": "248685",
    "end": "249560"
  },
  {
    "text": "And the probability density of landing on one such point, ",
    "start": "250360",
    "end": "254067"
  },
  {
    "text": "assuming independence, looks like f of x times g of y.",
    "start": "254067",
    "end": "257519"
  },
  {
    "text": "So what we do is we look at a graph of that expression as a two-variable ",
    "start": "258000",
    "end": "262039"
  },
  {
    "text": "function of x and y, which is a way of showing the distribution of all ",
    "start": "262039",
    "end": "265968"
  },
  {
    "text": "possible outcomes when we sample from the two different variables.",
    "start": "265968",
    "end": "269620"
  },
  {
    "text": "To interpret the convolution of f and g evaluated on some input s, ",
    "start": "270560",
    "end": "274904"
  },
  {
    "text": "which is a way of saying how likely are you to get a pair of samples that ",
    "start": "274904",
    "end": "279703"
  },
  {
    "text": "adds up to this sum s, what you do is you look at a slice of this graph ",
    "start": "279703",
    "end": "284371"
  },
  {
    "text": "over the line x plus y equals s, and you consider the area under that slice.",
    "start": "284371",
    "end": "289300"
  },
  {
    "text": "This area is almost, but not quite, the value of the convolution at s.",
    "start": "291100",
    "end": "296320"
  },
  {
    "text": "For a mildly technical reason, you need to divide by the square root of 2.",
    "start": "296800",
    "end": "300159"
  },
  {
    "text": "Still, this area is the key feature to focus on.",
    "start": "300840",
    "end": "303440"
  },
  {
    "text": "You can think of it as a way to combine together all the probability ",
    "start": "303440",
    "end": "307412"
  },
  {
    "text": "densities for all of the outcomes corresponding to a given sum.",
    "start": "307412",
    "end": "311039"
  },
  {
    "text": "In the specific case where these two functions look like e to ",
    "start": "313300",
    "end": "316646"
  },
  {
    "text": "the negative x squared and e to the negative y squared, ",
    "start": "316646",
    "end": "319668"
  },
  {
    "text": "the resulting 3D graph has a really nice property that you can exploit.",
    "start": "319668",
    "end": "323500"
  },
  {
    "text": "It's rotationally symmetric.",
    "start": "323720",
    "end": "325680"
  },
  {
    "text": "You can see this by combining the terms and noticing that it's entirely ",
    "start": "326880",
    "end": "330812"
  },
  {
    "text": "a function of x squared plus y squared, and this term describes the ",
    "start": "330812",
    "end": "334527"
  },
  {
    "text": "square of the distance between any point on the xy plane and the origin.",
    "start": "334527",
    "end": "338460"
  },
  {
    "text": "So in other words, the expression is purely a function of the distance from the origin.",
    "start": "339200",
    "end": "343160"
  },
  {
    "text": "And by the way, this would not be true for any other distribution.",
    "start": "344560",
    "end": "347919"
  },
  {
    "text": "It's a property that uniquely characterizes bell curves.",
    "start": "348100",
    "end": "351280"
  },
  {
    "text": "So for most other pairs of functions, these diagonal slices will be some complicated ",
    "start": "353160",
    "end": "357319"
  },
  {
    "text": "shape that's hard to think about, and honestly, ",
    "start": "357319",
    "end": "359668"
  },
  {
    "text": "calculating the area would just amount to computing the original integral that defines ",
    "start": "359668",
    "end": "363925"
  },
  {
    "text": "a convolution in the first place.",
    "start": "363925",
    "end": "365539"
  },
  {
    "text": "So in most cases, the visual intuition doesn't really buy you anything.",
    "start": "365940",
    "end": "369360"
  },
  {
    "text": "But in the case of bell curves, you can leverage that rotational symmetry.",
    "start": "370360",
    "end": "373919"
  },
  {
    "text": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
    "start": "374800",
    "end": "380479"
  },
  {
    "text": "And remember, the convolution that we're trying to compute is a function of s.",
    "start": "381300",
    "end": "385840"
  },
  {
    "text": "The thing that you want is an expression of s that tells you the area under this slice.",
    "start": "385840",
    "end": "391100"
  },
  {
    "text": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis ",
    "start": "391700",
    "end": "396392"
  },
  {
    "text": "at zero s, and a little bit of Pythagoras will show you that the straight line ",
    "start": "396392",
    "end": "400913"
  },
  {
    "text": "distance from the origin to this line is s divided by the square root of two.",
    "start": "400913",
    "end": "405320"
  },
  {
    "text": "Now, because of the symmetry, this slice is identical to one ",
    "start": "405860",
    "end": "409360"
  },
  {
    "text": "that you get rotating 45 degrees where you'd find something ",
    "start": "409360",
    "end": "412801"
  },
  {
    "text": "parallel to the y-axis the same distance away from the origin.",
    "start": "412802",
    "end": "416360"
  },
  {
    "text": "The key is that computing this other area of a slice parallel to the y-axis is much, ",
    "start": "417640",
    "end": "422366"
  },
  {
    "text": "much easier than slices in other directions because it only ",
    "start": "422366",
    "end": "425702"
  },
  {
    "text": "involves taking an integral with respect to y.",
    "start": "425702",
    "end": "428260"
  },
  {
    "text": "The value of x on this slice is a constant.",
    "start": "428740",
    "end": "431440"
  },
  {
    "text": "Specifically, it would be the constant s divided by the square root of two.",
    "start": "431620",
    "end": "434760"
  },
  {
    "text": "So when you're computing the integral, finding this area, ",
    "start": "434760",
    "end": "438231"
  },
  {
    "text": "all of this term here behaves like it was just some number, and you can factor it out.",
    "start": "438231",
    "end": "443379"
  },
  {
    "text": "This is the important point.",
    "start": "443880",
    "end": "444940"
  },
  {
    "text": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
    "start": "445280",
    "end": "450200"
  },
  {
    "text": "This remaining integral is a little bit tricky.",
    "start": "450820",
    "end": "453000"
  },
  {
    "text": "I did a whole video on it, it's actually quite famous.",
    "start": "453080",
    "end": "455199"
  },
  {
    "text": "But you almost don't really care.",
    "start": "455500",
    "end": "456900"
  },
  {
    "text": "The point is that it's just some number.",
    "start": "457240",
    "end": "459000"
  },
  {
    "text": "That number happens to be the square root of pi, ",
    "start": "459000",
    "end": "461645"
  },
  {
    "text": "but what really matters is that it's something with no dependence on s.",
    "start": "461645",
    "end": "465479"
  },
  {
    "text": "And essentially this is our answer.",
    "start": "466880",
    "end": "468480"
  },
  {
    "text": "We were looking for an expression for the area of these slices as a function of s, ",
    "start": "468780",
    "end": "473255"
  },
  {
    "text": "and now we have it.",
    "start": "473255",
    "end": "474280"
  },
  {
    "text": "It looks like e to the negative s squared divided by two, scaled by some constant.",
    "start": "474380",
    "end": "478840"
  },
  {
    "text": "In other words, it's also a bell curve, another Gaussian, ",
    "start": "479300",
    "end": "482209"
  },
  {
    "text": "just stretched out a little bit because of this two in the exponent.",
    "start": "482209",
    "end": "485620"
  },
  {
    "text": "As I said earlier, the convolution evaluated at s is not quite this area.",
    "start": "485620",
    "end": "490860"
  },
  {
    "text": "Technically it's this area divided by the square root of two.",
    "start": "491340",
    "end": "494160"
  },
  {
    "text": "We talked about it in the last video, but it doesn't ",
    "start": "494800",
    "end": "496901"
  },
  {
    "text": "really matter because it just gets baked into the constant.",
    "start": "496901",
    "end": "499240"
  },
  {
    "text": "What really matters is the conclusion that a convolution ",
    "start": "499680",
    "end": "502906"
  },
  {
    "text": "between two Gaussians is itself another Gaussian.",
    "start": "502906",
    "end": "505680"
  },
  {
    "start": "507000",
    "end": "750000"
  },
  {
    "text": "If you were to go back and reintroduce all of the constants for a normal distribution ",
    "start": "507560",
    "end": "511930"
  },
  {
    "text": "with a mean zero and an arbitrary standard deviation sigma, ",
    "start": "511930",
    "end": "514979"
  },
  {
    "text": "essentially identical reasoning will lead to the same square root of two factor that ",
    "start": "514980",
    "end": "519300"
  },
  {
    "text": "shows up in the exponent and out front, and it leads to the conclusion that the ",
    "start": "519300",
    "end": "523365"
  },
  {
    "text": "convolution between two such normal distributions is another normal distribution with a ",
    "start": "523366",
    "end": "527838"
  },
  {
    "text": "standard deviation square root of two times sigma.",
    "start": "527838",
    "end": "530380"
  },
  {
    "text": "If you haven't computed a lot of convolutions before, ",
    "start": "530980",
    "end": "533543"
  },
  {
    "text": "it's worth emphasizing this is a very special result.",
    "start": "533543",
    "end": "536060"
  },
  {
    "text": "Almost always you end up with a completely different kind of function, ",
    "start": "536380",
    "end": "539912"
  },
  {
    "text": "but here there's a sort of stability to the process.",
    "start": "539912",
    "end": "542500"
  },
  {
    "text": "Also, for those of you who enjoy exercises, I'll leave one up on the screen ",
    "start": "543260",
    "end": "546455"
  },
  {
    "text": "for how you would handle the case of two different standard deviations.",
    "start": "546455",
    "end": "549440"
  },
  {
    "text": "Still, some of you might be raising your hands and saying, what's the big deal?",
    "start": "550420",
    "end": "553940"
  },
  {
    "text": "I mean, when you first heard the question, what do you get when you ",
    "start": "554480",
    "end": "557743"
  },
  {
    "text": "add two normally distributed random variables, ",
    "start": "557744",
    "end": "560000"
  },
  {
    "text": "you probably even guessed that the answer should be another normally distributed variable.",
    "start": "560000",
    "end": "564320"
  },
  {
    "text": "After all, what else is it going to be?",
    "start": "564760",
    "end": "566360"
  },
  {
    "text": "Normal distributions are supposedly quite common, so why not?",
    "start": "566860",
    "end": "570240"
  },
  {
    "text": "You could even say that this should follow from the central limit theorem.",
    "start": "570240",
    "end": "573339"
  },
  {
    "text": "But that would have it all backwards.",
    "start": "573860",
    "end": "575480"
  },
  {
    "text": "First of all, the supposed ubiquity of normal distributions is often a little ",
    "start": "576180",
    "end": "580003"
  },
  {
    "text": "exaggerated, but to the extent that they do come up, ",
    "start": "580003",
    "end": "582600"
  },
  {
    "text": "it is because of the central limit theorem, but it would be cheating to say the central ",
    "start": "582600",
    "end": "586914"
  },
  {
    "text": "limit theorem implies this result because this computation we just did is the reason ",
    "start": "586914",
    "end": "591080"
  },
  {
    "text": "that the function at the heart of the central limit theorem is a Gaussian in the first ",
    "start": "591080",
    "end": "595344"
  },
  {
    "text": "place, and not some other function.",
    "start": "595344",
    "end": "597060"
  },
  {
    "text": "We've talked all about the central limit theorem before, ",
    "start": "597060",
    "end": "600300"
  },
  {
    "text": "but essentially it says if you repeatedly add copies of a random variable to itself, ",
    "start": "600300",
    "end": "605131"
  },
  {
    "text": "which mathematically looks like repeatedly computing convolutions against a given ",
    "start": "605131",
    "end": "609792"
  },
  {
    "text": "distribution, then after appropriate shifting and rescaling, ",
    "start": "609792",
    "end": "613260"
  },
  {
    "text": "the tendency is always to approach a normal distribution.",
    "start": "613260",
    "end": "616500"
  },
  {
    "text": "Technically, there's a small assumption the distribution you start with ",
    "start": "616980",
    "end": "620188"
  },
  {
    "text": "can't have infinite variance, but it's a relatively soft assumption.",
    "start": "620189",
    "end": "623220"
  },
  {
    "text": "The magic is that for a huge category of initial distributions, ",
    "start": "623220",
    "end": "626875"
  },
  {
    "text": "this process of adding a whole bunch of random variables drawn from ",
    "start": "626875",
    "end": "630759"
  },
  {
    "text": "that distribution always tends towards this one universal shape, a Gaussian.",
    "start": "630759",
    "end": "635100"
  },
  {
    "text": "One common approach to proving this theorem involves two separate steps.",
    "start": "635820",
    "end": "639300"
  },
  {
    "text": "The first step is to show that for all the different finite variance ",
    "start": "639600",
    "end": "643188"
  },
  {
    "text": "distributions you might start with, there exists a single universal ",
    "start": "643188",
    "end": "646724"
  },
  {
    "text": "shape that this process of repeated convolutions tends towards.",
    "start": "646724",
    "end": "650000"
  },
  {
    "text": "This step is actually pretty technical, it goes ",
    "start": "650000",
    "end": "652142"
  },
  {
    "text": "a little beyond what I want to talk about here.",
    "start": "652142",
    "end": "654240"
  },
  {
    "text": "You often use these objects called moment generating functions that gives you ",
    "start": "654520",
    "end": "658348"
  },
  {
    "text": "a very abstract argument that there must be some universal shape, ",
    "start": "658348",
    "end": "661587"
  },
  {
    "text": "but it doesn't make any claim about what that particular shape is, ",
    "start": "661587",
    "end": "664875"
  },
  {
    "text": "just that everything in this big family is tending towards a single point in ",
    "start": "664875",
    "end": "668654"
  },
  {
    "text": "the space of distributions.",
    "start": "668654",
    "end": "669980"
  },
  {
    "text": "So then step number two is what we just showed in this video, ",
    "start": "670620",
    "end": "673878"
  },
  {
    "text": "prove that the convolution of two Gaussians gives another Gaussian.",
    "start": "673878",
    "end": "677399"
  },
  {
    "text": "What that means is that as you apply this process of repeated convolutions, ",
    "start": "677400",
    "end": "681517"
  },
  {
    "text": "a Gaussian doesn't change, it's a fixed point, ",
    "start": "681517",
    "end": "684063"
  },
  {
    "text": "so the only thing it can approach is itself, and since it's one member in this ",
    "start": "684063",
    "end": "688342"
  },
  {
    "text": "big family of distributions, all of which must be tending towards a single universal ",
    "start": "688342",
    "end": "692946"
  },
  {
    "text": "shape, it must be that universal shape.",
    "start": "692947",
    "end": "695060"
  },
  {
    "text": "I mentioned at the start how this calculation, step two, ",
    "start": "695580",
    "end": "698562"
  },
  {
    "text": "is something that you can do directly, just symbolically with the definitions, ",
    "start": "698562",
    "end": "702696"
  },
  {
    "text": "but one of the reasons I'm so charmed by a geometric argument that leverages the ",
    "start": "702696",
    "end": "706935"
  },
  {
    "text": "rotational symmetry of this graph is that it directly connects to a few things that ",
    "start": "706935",
    "end": "711330"
  },
  {
    "text": "we've talked about on this channel before, for example, ",
    "start": "711330",
    "end": "714260"
  },
  {
    "text": "the Herschel-Maxwell derivation of a Gaussian, ",
    "start": "714260",
    "end": "716720"
  },
  {
    "text": "which essentially says that you can view this rotational symmetry as the defining ",
    "start": "716720",
    "end": "721011"
  },
  {
    "text": "feature of the distribution, that it locks you into this e to the negative x squared ",
    "start": "721011",
    "end": "725458"
  },
  {
    "text": "form, and also as an added bonus, it connects to the classic proof for why pi shows up ",
    "start": "725458",
    "end": "730011"
  },
  {
    "text": "in the formula, meaning we now have a direct line between the presence and mystery of ",
    "start": "730011",
    "end": "734511"
  },
  {
    "text": "that pi and the central limit theorem.",
    "start": "734511",
    "end": "736500"
  },
  {
    "text": "Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter ",
    "start": "737060",
    "end": "740357"
  },
  {
    "text": "brought my attention to a completely different approach I hadn't seen before, ",
    "start": "740357",
    "end": "743832"
  },
  {
    "text": "which leverages the use of entropy, and again, for the theoretically curious among you, ",
    "start": "743832",
    "end": "747753"
  },
  {
    "text": "I'll leave some links in the description.",
    "start": "747753",
    "end": "749580"
  },
  {
    "start": "750000",
    "end": "796000"
  },
  {
    "text": "By the way, if you want to stay up to date with new videos, ",
    "start": "750960",
    "end": "753585"
  },
  {
    "text": "and also any other projects that I put out there, like the Summer of Math Exposition, ",
    "start": "753585",
    "end": "757349"
  },
  {
    "text": "there is a mailing list.",
    "start": "757349",
    "end": "758400"
  },
  {
    "text": "It's relatively new, and I'm pretty sparing about ",
    "start": "758720",
    "end": "760878"
  },
  {
    "text": "only posting what I think people will enjoy.",
    "start": "760879",
    "end": "762780"
  },
  {
    "text": "Usually I try not to be too promotional at the end of videos these days, ",
    "start": "763220",
    "end": "775660"
  },
  {
    "text": "but if you are interested in following the work that I do, ",
    "start": "775661",
    "end": "785716"
  },
  {
    "text": "this is probably one of the most enduring ways to do so.",
    "start": "785716",
    "end": "795260"
  }
]