[
  {
    "text": "Eigenvectors and eigenvalues is one of those topics ",
    "start": "19920",
    "end": "22812"
  },
  {
    "text": "that a lot of students find particularly unintuitive.",
    "start": "22812",
    "end": "25760"
  },
  {
    "text": "Questions like, why are we doing this and what does this actually mean, ",
    "start": "25760",
    "end": "29433"
  },
  {
    "text": "are too often left just floating away in an unanswered sea of computations.",
    "start": "29433",
    "end": "33260"
  },
  {
    "text": "And as I've put out the videos of this series, ",
    "start": "33920",
    "end": "36025"
  },
  {
    "text": "a lot of you have commented about looking forward to visualizing this topic in particular.",
    "start": "36026",
    "end": "40060"
  },
  {
    "text": "I suspect that the reason for this is not so much that ",
    "start": "40680",
    "end": "43373"
  },
  {
    "text": "eigenthings are particularly complicated or poorly explained.",
    "start": "43373",
    "end": "46360"
  },
  {
    "text": "In fact, it's comparatively straightforward, and ",
    "start": "46860",
    "end": "49065"
  },
  {
    "text": "I think most books do a fine job explaining it.",
    "start": "49065",
    "end": "51180"
  },
  {
    "text": "The issue is that it only really makes sense if you have a ",
    "start": "51520",
    "end": "54805"
  },
  {
    "text": "solid visual understanding for many of the topics that precede it.",
    "start": "54805",
    "end": "58480"
  },
  {
    "text": "Most important here is that you know how to think about matrices as ",
    "start": "59060",
    "end": "62616"
  },
  {
    "text": "linear transformations, but you also need to be comfortable with things ",
    "start": "62616",
    "end": "66383"
  },
  {
    "text": "like determinants, linear systems of equations, and change of basis.",
    "start": "66383",
    "end": "69940"
  },
  {
    "text": "Confusion about eigenstuffs usually has more to do with a shaky foundation in ",
    "start": "70720",
    "end": "74979"
  },
  {
    "text": "one of these topics than it does with eigenvectors and eigenvalues themselves.",
    "start": "74979",
    "end": "79240"
  },
  {
    "text": "To start, consider some linear transformation in two dimensions, like the one shown here.",
    "start": "79980",
    "end": "84840"
  },
  {
    "text": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
    "start": "85460",
    "end": "91040"
  },
  {
    "text": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
    "start": "91780",
    "end": "95640"
  },
  {
    "text": "Focus in on what it does to one particular vector, ",
    "start": "96600",
    "end": "99353"
  },
  {
    "text": "and think about the span of that vector, the line passing through its origin and its tip.",
    "start": "99353",
    "end": "104159"
  },
  {
    "text": "Most vectors are going to get knocked off their span during the transformation.",
    "start": "104920",
    "end": "108380"
  },
  {
    "text": "I mean, it would seem pretty coincidental if the place where ",
    "start": "108780",
    "end": "112049"
  },
  {
    "text": "the vector landed also happened to be somewhere on that line.",
    "start": "112050",
    "end": "115320"
  },
  {
    "text": "But some special vectors do remain on their own span, ",
    "start": "117400",
    "end": "120653"
  },
  {
    "text": "meaning the effect that the matrix has on such a vector is just to stretch it or ",
    "start": "120653",
    "end": "125533"
  },
  {
    "text": "squish it, like a scalar.",
    "start": "125533",
    "end": "127039"
  },
  {
    "text": "For this specific example, the basis vector i-hat is one such special vector.",
    "start": "129460",
    "end": "134100"
  },
  {
    "text": "The span of i-hat is the x-axis, and from the first column of the matrix, ",
    "start": "134640",
    "end": "139412"
  },
  {
    "text": "we can see that i-hat moves over to 3 times itself, still on that x-axis.",
    "start": "139412",
    "end": "144120"
  },
  {
    "text": "What's more, because of the way linear transformations work, ",
    "start": "146320",
    "end": "150031"
  },
  {
    "text": "any other vector on the x-axis is also just stretched by a factor of 3, ",
    "start": "150031",
    "end": "154411"
  },
  {
    "text": "and hence remains on its own span.",
    "start": "154411",
    "end": "156480"
  },
  {
    "text": "A slightly sneakier vector that remains on its own ",
    "start": "158500",
    "end": "161325"
  },
  {
    "text": "span during this transformation is negative 1, 1.",
    "start": "161325",
    "end": "164040"
  },
  {
    "text": "It ends up getting stretched by a factor of 2.",
    "start": "164660",
    "end": "167140"
  },
  {
    "text": "And again, linearity is going to imply that any other vector on the diagonal ",
    "start": "169000",
    "end": "173610"
  },
  {
    "text": "line spanned by this guy is just going to get stretched out by a factor of 2.",
    "start": "173610",
    "end": "178220"
  },
  {
    "text": "And for this transformation, those are all the vectors ",
    "start": "179820",
    "end": "182575"
  },
  {
    "text": "with this special property of staying on their span.",
    "start": "182575",
    "end": "185180"
  },
  {
    "text": "Those on the x-axis getting stretched out by a factor of 3, ",
    "start": "185620",
    "end": "188624"
  },
  {
    "text": "and those on this diagonal line getting stretched by a factor of 2.",
    "start": "188624",
    "end": "191980"
  },
  {
    "text": "Any other vector is going to get rotated somewhat during the transformation, ",
    "start": "192760",
    "end": "196417"
  },
  {
    "text": "knocked off the line that it spans.",
    "start": "196417",
    "end": "198080"
  },
  {
    "text": "As you might have guessed by now, these special vectors are called the eigenvectors of ",
    "start": "202520",
    "end": "207362"
  },
  {
    "text": "the transformation, and each eigenvector has associated with it what's called an ",
    "start": "207362",
    "end": "211870"
  },
  {
    "text": "eigenvalue, which is just the factor by which it's stretched or squished during the ",
    "start": "211870",
    "end": "216545"
  },
  {
    "text": "transformation.",
    "start": "216545",
    "end": "217380"
  },
  {
    "text": "Of course, there's nothing special about stretching versus squishing, ",
    "start": "220280",
    "end": "223399"
  },
  {
    "text": "or the fact that these eigenvalues happen to be positive.",
    "start": "223399",
    "end": "225940"
  },
  {
    "text": "In another example, you could have an eigenvector with eigenvalue negative 1 half, ",
    "start": "226380",
    "end": "231060"
  },
  {
    "text": "meaning that the vector gets flipped and squished by a factor of 1 half.",
    "start": "231060",
    "end": "235120"
  },
  {
    "text": "But the important part here is that it stays on the ",
    "start": "236980",
    "end": "239737"
  },
  {
    "text": "line that it spans out without getting rotated off of it.",
    "start": "239737",
    "end": "242760"
  },
  {
    "text": "For a glimpse of why this might be a useful thing to think about, ",
    "start": "244460",
    "end": "247753"
  },
  {
    "text": "consider some three-dimensional rotation.",
    "start": "247753",
    "end": "249800"
  },
  {
    "text": "If you can find an eigenvector for that rotation, ",
    "start": "251660",
    "end": "254983"
  },
  {
    "text": "a vector that remains on its own span, what you have found is the axis of rotation.",
    "start": "254983",
    "end": "260500"
  },
  {
    "text": "And it's much easier to think about a 3D rotation in terms of some ",
    "start": "262600",
    "end": "266587"
  },
  {
    "text": "axis of rotation and an angle by which it's rotating, ",
    "start": "266587",
    "end": "269800"
  },
  {
    "text": "rather than thinking about the full 3x3 matrix associated with that transformation.",
    "start": "269800",
    "end": "274740"
  },
  {
    "text": "In this case, by the way, the corresponding eigenvalue would have to be 1, ",
    "start": "277000",
    "end": "280797"
  },
  {
    "text": "since rotations never stretch or squish anything, ",
    "start": "280797",
    "end": "283328"
  },
  {
    "text": "so the length of the vector would remain the same.",
    "start": "283328",
    "end": "285860"
  },
  {
    "text": "This pattern shows up a lot in linear algebra.",
    "start": "288080",
    "end": "290020"
  },
  {
    "text": "With any linear transformation described by a matrix, ",
    "start": "290440",
    "end": "293253"
  },
  {
    "text": "you could understand what it's doing by reading off the columns of this matrix as the ",
    "start": "293253",
    "end": "297733"
  },
  {
    "text": "landing spots for basis vectors.",
    "start": "297733",
    "end": "299400"
  },
  {
    "text": "But often, a better way to get at the heart of what the linear ",
    "start": "300020",
    "end": "303600"
  },
  {
    "text": "transformation actually does, less dependent on your particular coordinate system, ",
    "start": "303601",
    "end": "308318"
  },
  {
    "text": "is to find the eigenvectors and eigenvalues.",
    "start": "308318",
    "end": "310820"
  },
  {
    "text": "I won't cover the full details on methods for computing eigenvectors ",
    "start": "315460",
    "end": "318996"
  },
  {
    "text": "and eigenvalues here, but I'll try to give an overview of the ",
    "start": "318997",
    "end": "322175"
  },
  {
    "text": "computational ideas that are most important for a conceptual understanding.",
    "start": "322175",
    "end": "326020"
  },
  {
    "text": "Symbolically, here's what the idea of an eigenvector looks like.",
    "start": "327180",
    "end": "330479"
  },
  {
    "text": "A is the matrix representing some transformation, with v as the eigenvector, ",
    "start": "331040",
    "end": "335929"
  },
  {
    "text": "and lambda is a number, namely the corresponding eigenvalue.",
    "start": "335929",
    "end": "339740"
  },
  {
    "text": "What this expression is saying is that the matrix-vector product, A times v, ",
    "start": "340680",
    "end": "345289"
  },
  {
    "text": "gives the same result as just scaling the eigenvector v by some value lambda.",
    "start": "345289",
    "end": "349900"
  },
  {
    "text": "So finding the eigenvectors and their eigenvalues of a matrix A comes ",
    "start": "351000",
    "end": "355423"
  },
  {
    "text": "down to finding the values of v and lambda that make this expression true.",
    "start": "355423",
    "end": "360100"
  },
  {
    "text": "It's a little awkward to work with at first, because that left-hand side represents ",
    "start": "361920",
    "end": "366057"
  },
  {
    "text": "matrix-vector multiplication, but the right-hand side here is scalar-vector ",
    "start": "366057",
    "end": "369801"
  },
  {
    "text": "multiplication.",
    "start": "369801",
    "end": "370540"
  },
  {
    "text": "So let's start by rewriting that right-hand side as some kind of matrix-vector ",
    "start": "371120",
    "end": "375408"
  },
  {
    "text": "multiplication, using a matrix which has the effect of scaling any vector by a factor ",
    "start": "375408",
    "end": "380077"
  },
  {
    "text": "of lambda.",
    "start": "380077",
    "end": "380620"
  },
  {
    "text": "The columns of such a matrix will represent what happens to each basis vector, ",
    "start": "381680",
    "end": "386177"
  },
  {
    "text": "and each basis vector is simply multiplied by lambda, ",
    "start": "386178",
    "end": "389252"
  },
  {
    "text": "so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
    "start": "389252",
    "end": "394320"
  },
  {
    "text": "The common way to write this guy is to factor that lambda out and write it ",
    "start": "396180",
    "end": "400491"
  },
  {
    "text": "as lambda times i, where i is the identity matrix with 1s down the diagonal.",
    "start": "400491",
    "end": "404860"
  },
  {
    "text": "With both sides looking like matrix-vector multiplication, ",
    "start": "405860",
    "end": "408784"
  },
  {
    "text": "we can subtract off that right-hand side and factor out the v.",
    "start": "408785",
    "end": "411860"
  },
  {
    "text": "So what we now have is a new matrix, A minus lambda times the identity, ",
    "start": "414160",
    "end": "418971"
  },
  {
    "text": "and we're looking for a vector v such that this new matrix times v gives the zero vector.",
    "start": "418971",
    "end": "424920"
  },
  {
    "text": "Now, this will always be true if v itself is the zero vector, but that's boring.",
    "start": "426380",
    "end": "431100"
  },
  {
    "text": "What we want is a non-zero eigenvector.",
    "start": "431340",
    "end": "433639"
  },
  {
    "text": "And if you watch chapter 5 and 6, you'll know that the only way it's possible ",
    "start": "434420",
    "end": "438934"
  },
  {
    "text": "for the product of a matrix with a non-zero vector to become zero is if the ",
    "start": "438934",
    "end": "443332"
  },
  {
    "text": "transformation associated with that matrix squishes space into a lower dimension.",
    "start": "443332",
    "end": "448020"
  },
  {
    "text": "And that squishification corresponds to a zero determinant for the matrix.",
    "start": "449300",
    "end": "454220"
  },
  {
    "text": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, ",
    "start": "455480",
    "end": "459934"
  },
  {
    "text": "and think about subtracting off a variable amount, lambda, from each diagonal entry.",
    "start": "459934",
    "end": "465520"
  },
  {
    "text": "Now imagine tweaking lambda, turning a knob to change its value.",
    "start": "466480",
    "end": "470280"
  },
  {
    "text": "As that value of lambda changes, the matrix itself changes, ",
    "start": "470940",
    "end": "474539"
  },
  {
    "text": "and so the determinant of the matrix changes.",
    "start": "474539",
    "end": "477240"
  },
  {
    "text": "The goal here is to find a value of lambda that will make this determinant zero, ",
    "start": "478220",
    "end": "482964"
  },
  {
    "text": "meaning the tweaked transformation squishes space into a lower dimension.",
    "start": "482964",
    "end": "487240"
  },
  {
    "text": "In this case, the sweet spot comes when lambda equals 1.",
    "start": "488160",
    "end": "491160"
  },
  {
    "text": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
    "start": "492180",
    "end": "496120"
  },
  {
    "text": "The sweet spot might be hit at some other value of lambda.",
    "start": "496240",
    "end": "498599"
  },
  {
    "text": "So this is kind of a lot, but let's unravel what this is saying.",
    "start": "500080",
    "end": "502960"
  },
  {
    "text": "When lambda equals 1, the matrix A minus lambda ",
    "start": "502960",
    "end": "506330"
  },
  {
    "text": "times the identity squishes space onto a line.",
    "start": "506330",
    "end": "509560"
  },
  {
    "text": "That means there's a non-zero vector v such that A minus ",
    "start": "510440",
    "end": "514500"
  },
  {
    "text": "lambda times the identity times v equals the zero vector.",
    "start": "514500",
    "end": "518559"
  },
  {
    "text": "And remember, the reason we care about that is because it means A times v ",
    "start": "520480",
    "end": "526029"
  },
  {
    "text": "equals lambda times v, which you can read off as saying that the vector v ",
    "start": "526030",
    "end": "531579"
  },
  {
    "text": "is an eigenvector of A, staying on its own span during the transformation A.",
    "start": "531579",
    "end": "537279"
  },
  {
    "text": "In this example, the corresponding eigenvalue is 1, ",
    "start": "538320",
    "end": "541375"
  },
  {
    "text": "so v would actually just stay fixed in place.",
    "start": "541375",
    "end": "544020"
  },
  {
    "text": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
    "start": "546220",
    "end": "549500"
  },
  {
    "text": "This is the kind of thing I mentioned in the introduction.",
    "start": "553380",
    "end": "555640"
  },
  {
    "text": "If you didn't have a solid grasp of determinants and why they ",
    "start": "556220",
    "end": "559526"
  },
  {
    "text": "relate to linear systems of equations having non-zero solutions, ",
    "start": "559526",
    "end": "562993"
  },
  {
    "text": "an expression like this would feel completely out of the blue.",
    "start": "562993",
    "end": "566300"
  },
  {
    "text": "To see this in action, let's revisit the example from the start, ",
    "start": "568320",
    "end": "571962"
  },
  {
    "text": "with a matrix whose columns are 3, 0 and 1, 2.",
    "start": "571962",
    "end": "574540"
  },
  {
    "text": "To find if a value lambda is an eigenvalue, subtract it from ",
    "start": "575350",
    "end": "579511"
  },
  {
    "text": "the diagonals of this matrix and compute the determinant.",
    "start": "579511",
    "end": "583399"
  },
  {
    "text": "Doing this, we get a certain quadratic polynomial in lambda, ",
    "start": "590580",
    "end": "594441"
  },
  {
    "text": "3 minus lambda times 2 minus lambda.",
    "start": "594441",
    "end": "596720"
  },
  {
    "text": "Since lambda can only be an eigenvalue if this determinant happens to be zero, ",
    "start": "597800",
    "end": "602899"
  },
  {
    "text": "you can conclude that the only possible eigenvalues are lambda equals 2 and lambda ",
    "start": "602900",
    "end": "608258"
  },
  {
    "text": "equals 3.",
    "start": "608258",
    "end": "608839"
  },
  {
    "text": "To figure out what the eigenvectors are that actually have one of these eigenvalues, ",
    "start": "609640",
    "end": "614979"
  },
  {
    "text": "say lambda equals 2, plug in that value of lambda to the matrix and then ",
    "start": "614979",
    "end": "619565"
  },
  {
    "text": "solve for which vectors this diagonally altered matrix sends to zero.",
    "start": "619565",
    "end": "623900"
  },
  {
    "text": "If you computed this the way you would any other linear system, ",
    "start": "624940",
    "end": "628707"
  },
  {
    "text": "you'd see that the solutions are all the vectors on the diagonal line spanned ",
    "start": "628707",
    "end": "633298"
  },
  {
    "text": "by negative 1, 1.",
    "start": "633299",
    "end": "634300"
  },
  {
    "text": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, ",
    "start": "635220",
    "end": "639277"
  },
  {
    "text": "2, has the effect of stretching all those vectors by a factor of 2.",
    "start": "639277",
    "end": "643459"
  },
  {
    "text": "Now, a 2D transformation doesn't have to have eigenvectors.",
    "start": "646320",
    "end": "650200"
  },
  {
    "text": "For example, consider a rotation by 90 degrees.",
    "start": "650720",
    "end": "653399"
  },
  {
    "text": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
    "start": "653660",
    "end": "658199"
  },
  {
    "text": "If you actually try computing the eigenvalues of a rotation like this, ",
    "start": "660800",
    "end": "664513"
  },
  {
    "text": "notice what happens.",
    "start": "664513",
    "end": "665560"
  },
  {
    "text": "Its matrix has columns 0, 1 and negative 1, 0.",
    "start": "666300",
    "end": "670140"
  },
  {
    "text": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
    "start": "671100",
    "end": "675800"
  },
  {
    "text": "In this case, you get the polynomial lambda squared plus 1.",
    "start": "678140",
    "end": "681940"
  },
  {
    "text": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
    "start": "682680",
    "end": "687920"
  },
  {
    "text": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
    "start": "688840",
    "end": "693600"
  },
  {
    "text": "Another pretty interesting example worth holding in the back of your mind is a shear.",
    "start": "695540",
    "end": "699820"
  },
  {
    "text": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
    "start": "700560",
    "end": "707840"
  },
  {
    "text": "All of the vectors on the x-axis are eigenvectors ",
    "start": "708740",
    "end": "711610"
  },
  {
    "text": "with eigenvalue 1 since they remain fixed in place.",
    "start": "711611",
    "end": "714540"
  },
  {
    "text": "In fact, these are the only eigenvectors.",
    "start": "715680",
    "end": "717820"
  },
  {
    "text": "When you subtract off lambda from the diagonals and compute the determinant, ",
    "start": "718760",
    "end": "723924"
  },
  {
    "text": "what you get is 1 minus lambda squared.",
    "start": "723924",
    "end": "726540"
  },
  {
    "text": "And the only root of this expression is lambda equals 1.",
    "start": "729320",
    "end": "732860"
  },
  {
    "text": "This lines up with what we see geometrically, ",
    "start": "734560",
    "end": "737112"
  },
  {
    "text": "that all of the eigenvectors have eigenvalue 1.",
    "start": "737112",
    "end": "739720"
  },
  {
    "text": "Keep in mind though, it's also possible to have just one eigenvalue, ",
    "start": "741080",
    "end": "745037"
  },
  {
    "text": "but with more than just a line full of eigenvectors.",
    "start": "745037",
    "end": "748019"
  },
  {
    "text": "A simple example is a matrix that scales everything by 2.",
    "start": "749900",
    "end": "753180"
  },
  {
    "text": "The only eigenvalue is 2, but every vector in the ",
    "start": "753900",
    "end": "757200"
  },
  {
    "text": "plane gets to be an eigenvector with that eigenvalue.",
    "start": "757200",
    "end": "760700"
  },
  {
    "text": "Now is another good time to pause and ponder some ",
    "start": "762000",
    "end": "764666"
  },
  {
    "text": "of this before I move on to the last topic.",
    "start": "764666",
    "end": "766960"
  },
  {
    "text": "I want to finish off here with the idea of an eigenbasis, ",
    "start": "783540",
    "end": "786944"
  },
  {
    "text": "which relies heavily on ideas from the last video.",
    "start": "786944",
    "end": "789879"
  },
  {
    "text": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
    "start": "791480",
    "end": "796380"
  },
  {
    "text": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
    "start": "797120",
    "end": "802380"
  },
  {
    "text": "Writing their new coordinates as the columns of a matrix, ",
    "start": "803420",
    "end": "807014"
  },
  {
    "text": "notice that those scalar multiples, negative 1 and 2, ",
    "start": "807014",
    "end": "810360"
  },
  {
    "text": "which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, ",
    "start": "810361",
    "end": "815382"
  },
  {
    "text": "and every other entry is a 0.",
    "start": "815382",
    "end": "817180"
  },
  {
    "text": "Any time a matrix has zeros everywhere other than the diagonal, ",
    "start": "818880",
    "end": "822551"
  },
  {
    "text": "it's called, reasonably enough, a diagonal matrix.",
    "start": "822551",
    "end": "825420"
  },
  {
    "text": "And the way to interpret this is that all the basis vectors are eigenvectors, ",
    "start": "825840",
    "end": "830509"
  },
  {
    "text": "with the diagonal entries of this matrix being their eigenvalues.",
    "start": "830509",
    "end": "834399"
  },
  {
    "text": "There are a lot of things that make diagonal matrices much nicer to work with.",
    "start": "837100",
    "end": "841060"
  },
  {
    "text": "One big one is that it's easier to compute what will happen ",
    "start": "841780",
    "end": "845032"
  },
  {
    "text": "if you multiply this matrix by itself a whole bunch of times.",
    "start": "845032",
    "end": "848339"
  },
  {
    "text": "Since all one of these matrices does is scale each basis vector by some eigenvalue, ",
    "start": "849420",
    "end": "854733"
  },
  {
    "text": "applying that matrix many times, say 100 times, ",
    "start": "854733",
    "end": "857769"
  },
  {
    "text": "is just going to correspond to scaling each basis vector by the 100th power of ",
    "start": "857769",
    "end": "862764"
  },
  {
    "text": "the corresponding eigenvalue.",
    "start": "862765",
    "end": "864600"
  },
  {
    "text": "In contrast, try computing the 100th power of a non-diagonal matrix.",
    "start": "865700",
    "end": "869680"
  },
  {
    "text": "Really, try it for a moment.",
    "start": "869680",
    "end": "871320"
  },
  {
    "text": "It's a nightmare.",
    "start": "871740",
    "end": "872440"
  },
  {
    "text": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
    "start": "876080",
    "end": "881260"
  },
  {
    "text": "But if your transformation has a lot of eigenvectors, ",
    "start": "882040",
    "end": "885110"
  },
  {
    "text": "like the one from the start of this video, enough so that you can choose a set that ",
    "start": "885110",
    "end": "889886"
  },
  {
    "text": "spans the full space, then you could change your coordinate system so that these ",
    "start": "889887",
    "end": "894492"
  },
  {
    "text": "eigenvectors are your basis vectors.",
    "start": "894492",
    "end": "896540"
  },
  {
    "text": "I talked about change of basis last video, but I'll go through ",
    "start": "897140",
    "end": "900371"
  },
  {
    "text": "a super quick reminder here of how to express a transformation ",
    "start": "900371",
    "end": "903602"
  },
  {
    "text": "currently written in our coordinate system into a different system.",
    "start": "903603",
    "end": "907040"
  },
  {
    "text": "Take the coordinates of the vectors that you want to use as a new basis, ",
    "start": "908440",
    "end": "912282"
  },
  {
    "text": "which in this case means our two eigenvectors, ",
    "start": "912282",
    "end": "914755"
  },
  {
    "text": "then make those coordinates the columns of a matrix, known as the change of basis matrix.",
    "start": "914755",
    "end": "919440"
  },
  {
    "text": "When you sandwich the original transformation, ",
    "start": "920180",
    "end": "922834"
  },
  {
    "text": "putting the change of basis matrix on its right and the inverse of the ",
    "start": "922834",
    "end": "926842"
  },
  {
    "text": "change of basis matrix on its left, the result will be a matrix representing ",
    "start": "926843",
    "end": "931191"
  },
  {
    "text": "that same transformation, but from the perspective of the new basis ",
    "start": "931191",
    "end": "935031"
  },
  {
    "text": "vectors coordinate system.",
    "start": "935031",
    "end": "936500"
  },
  {
    "text": "The whole point of doing this with eigenvectors is that this new matrix is ",
    "start": "937440",
    "end": "941910"
  },
  {
    "text": "guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
    "start": "941910",
    "end": "946680"
  },
  {
    "text": "This is because it represents working in a coordinate system where what ",
    "start": "946860",
    "end": "950417"
  },
  {
    "text": "happens to the basis vectors is that they get scaled during the transformation.",
    "start": "950417",
    "end": "954320"
  },
  {
    "text": "A set of basis vectors which are also eigenvectors is called, ",
    "start": "955800",
    "end": "959300"
  },
  {
    "text": "again, reasonably enough, an eigenbasis.",
    "start": "959301",
    "end": "961560"
  },
  {
    "text": "So if, for example, you needed to compute the 100th power of this matrix, ",
    "start": "962340",
    "end": "967108"
  },
  {
    "text": "it would be much easier to change to an eigenbasis, ",
    "start": "967108",
    "end": "970459"
  },
  {
    "text": "compute the 100th power in that system, then convert back to our standard system.",
    "start": "970460",
    "end": "975680"
  },
  {
    "text": "You can't do this with all transformations.",
    "start": "976620",
    "end": "978320"
  },
  {
    "text": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
    "start": "978320",
    "end": "982980"
  },
  {
    "text": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
    "start": "983460",
    "end": "988160"
  },
  {
    "text": "For those of you willing to work through a pretty neat puzzle to ",
    "start": "989120",
    "end": "991771"
  },
  {
    "text": "see what this looks like in action and how it can be used to produce ",
    "start": "991771",
    "end": "994586"
  },
  {
    "text": "some surprising results, I'll leave up a prompt here on the screen.",
    "start": "994586",
    "end": "997320"
  },
  {
    "text": "It takes a bit of work, but I think you'll enjoy it.",
    "start": "997600",
    "end": "1000279"
  },
  {
    "text": "The next and final video of this series is going to be on abstract vector spaces. ",
    "start": "1000840",
    "end": "1005397"
  },
  {
    "text": "See you then!",
    "start": "1005397",
    "end": "1006120"
  }
]