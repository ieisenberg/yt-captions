[
  {
    "start": "0",
    "end": "99000"
  },
  {
    "text": "In the last chapter, you and I started to step",
    "start": "0",
    "end": "1967"
  },
  {
    "text": "through the internal workings of a transformer.",
    "start": "1967",
    "end": "4020"
  },
  {
    "text": "This is one of the key pieces of technology inside large language models,",
    "start": "4560",
    "end": "7880"
  },
  {
    "text": "and a lot of other tools in the modern wave of AI.",
    "start": "7880",
    "end": "10200"
  },
  {
    "text": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need,",
    "start": "10980",
    "end": "15519"
  },
  {
    "text": "and in this chapter you and I will dig into what this attention mechanism is,",
    "start": "15520",
    "end": "19786"
  },
  {
    "text": "visualizing how it processes data.",
    "start": "19786",
    "end": "21699"
  },
  {
    "text": "As a quick recap, here's the important context I want you to have in mind.",
    "start": "26140",
    "end": "29539"
  },
  {
    "text": "The goal of the model that you and I are studying is to",
    "start": "30000",
    "end": "32950"
  },
  {
    "text": "take in a piece of text and predict what word comes next.",
    "start": "32950",
    "end": "36060"
  },
  {
    "text": "The input text is broken up into little pieces that we call tokens,",
    "start": "36860",
    "end": "40337"
  },
  {
    "text": "and these are very often words or pieces of words,",
    "start": "40337",
    "end": "42983"
  },
  {
    "text": "but just to make the examples in this video easier for you and me to think about,",
    "start": "42983",
    "end": "47239"
  },
  {
    "text": "let's simplify by pretending that tokens are always just words.",
    "start": "47239",
    "end": "50559"
  },
  {
    "text": "The first step in a transformer is to associate each token",
    "start": "51480",
    "end": "54537"
  },
  {
    "text": "with a high-dimensional vector, what we call its embedding.",
    "start": "54537",
    "end": "57699"
  },
  {
    "text": "The most important idea I want you to have in mind is how directions in this",
    "start": "57700",
    "end": "62010"
  },
  {
    "text": "high-dimensional space of all possible embeddings can correspond with semantic meaning.",
    "start": "62010",
    "end": "67000"
  },
  {
    "text": "In the last chapter we saw an example for how direction can correspond to gender,",
    "start": "67680",
    "end": "71716"
  },
  {
    "text": "in the sense that adding a certain step in this space can take you from the",
    "start": "71716",
    "end": "75504"
  },
  {
    "text": "embedding of a masculine noun to the embedding of the corresponding feminine noun.",
    "start": "75504",
    "end": "79640"
  },
  {
    "text": "That's just one example you could imagine how many other directions in this",
    "start": "80160",
    "end": "83595"
  },
  {
    "text": "high-dimensional space could correspond to numerous other aspects of a word's meaning.",
    "start": "83595",
    "end": "87580"
  },
  {
    "text": "The aim of a transformer is to progressively adjust these embeddings",
    "start": "88800",
    "end": "92535"
  },
  {
    "text": "so that they don't merely encode an individual word,",
    "start": "92535",
    "end": "95445"
  },
  {
    "text": "but instead they bake in some much, much richer contextual meaning.",
    "start": "95445",
    "end": "99180"
  },
  {
    "start": "99000",
    "end": "269000"
  },
  {
    "text": "I should say up front that a lot of people find the attention mechanism,",
    "start": "100140",
    "end": "103656"
  },
  {
    "text": "this key piece in a transformer, very confusing,",
    "start": "103656",
    "end": "106049"
  },
  {
    "text": "so don't worry if it takes some time for things to sink in.",
    "start": "106050",
    "end": "108980"
  },
  {
    "text": "I think that before we dive into the computational details and all",
    "start": "109440",
    "end": "112696"
  },
  {
    "text": "the matrix multiplications, it's worth thinking about a couple",
    "start": "112696",
    "end": "115805"
  },
  {
    "text": "examples for the kind of behavior that we want attention to enable.",
    "start": "115805",
    "end": "119160"
  },
  {
    "text": "Consider the phrases American shrew mole, one mole of carbon dioxide,",
    "start": "120140",
    "end": "124335"
  },
  {
    "text": "and take a biopsy of the mole.",
    "start": "124335",
    "end": "126220"
  },
  {
    "text": "You and I know that the word mole has different meanings in each one of these,",
    "start": "126700",
    "end": "129976"
  },
  {
    "text": "based on the context.",
    "start": "129976",
    "end": "130899"
  },
  {
    "text": "But after the first step of a transformer, the one that breaks up the text",
    "start": "131360",
    "end": "135075"
  },
  {
    "text": "and associates each token with a vector, the vector that's associated with",
    "start": "135075",
    "end": "138840"
  },
  {
    "text": "mole would be the same in all of these cases,",
    "start": "138840",
    "end": "141150"
  },
  {
    "text": "because this initial token embedding is effectively a lookup table with no",
    "start": "141150",
    "end": "144915"
  },
  {
    "text": "reference to the context.",
    "start": "144915",
    "end": "146219"
  },
  {
    "text": "It's only in the next step of the transformer that the surrounding",
    "start": "146620",
    "end": "149961"
  },
  {
    "text": "embeddings have the chance to pass information into this one.",
    "start": "149961",
    "end": "153100"
  },
  {
    "text": "The picture you might have in mind is that there are multiple distinct directions in",
    "start": "153820",
    "end": "158288"
  },
  {
    "text": "this embedding space encoding the multiple distinct meanings of the word mole,",
    "start": "158288",
    "end": "162491"
  },
  {
    "text": "and that a well-trained attention block calculates what you need to add to the generic",
    "start": "162491",
    "end": "167119"
  },
  {
    "text": "embedding to move it to one of these specific directions, as a function of the context.",
    "start": "167119",
    "end": "171800"
  },
  {
    "text": "To take another example, consider the embedding of the word tower.",
    "start": "173300",
    "end": "176180"
  },
  {
    "text": "This is presumably some very generic, non-specific direction in the space,",
    "start": "177060",
    "end": "181067"
  },
  {
    "text": "associated with lots of other large, tall nouns.",
    "start": "181067",
    "end": "183720"
  },
  {
    "text": "If this word was immediately preceded by Eiffel,",
    "start": "184020",
    "end": "186589"
  },
  {
    "text": "you could imagine wanting the mechanism to update this vector so that",
    "start": "186589",
    "end": "190336"
  },
  {
    "text": "it points in a direction that more specifically encodes the Eiffel tower,",
    "start": "190336",
    "end": "194296"
  },
  {
    "text": "maybe correlated with vectors associated with Paris and France and things made of steel.",
    "start": "194296",
    "end": "199060"
  },
  {
    "text": "If it was also preceded by the word miniature,",
    "start": "199920",
    "end": "202229"
  },
  {
    "text": "then the vector should be updated even further,",
    "start": "202229",
    "end": "204639"
  },
  {
    "text": "so that it no longer correlates with large, tall things.",
    "start": "204639",
    "end": "207500"
  },
  {
    "text": "More generally than just refining the meaning of a word,",
    "start": "209480",
    "end": "212274"
  },
  {
    "text": "the attention block allows the model to move information encoded in",
    "start": "212274",
    "end": "215667"
  },
  {
    "text": "one embedding to that of another, potentially ones that are quite far away,",
    "start": "215667",
    "end": "219458"
  },
  {
    "text": "and potentially with information that's much richer than just a single word.",
    "start": "219458",
    "end": "223299"
  },
  {
    "text": "What we saw in the last chapter was how after all of the vectors flow through the",
    "start": "223300",
    "end": "227931"
  },
  {
    "text": "network, including many different attention blocks,",
    "start": "227931",
    "end": "230904"
  },
  {
    "text": "the computation you perform to produce a prediction of the next token is entirely a",
    "start": "230904",
    "end": "235707"
  },
  {
    "text": "function of the last vector in the sequence.",
    "start": "235707",
    "end": "238280"
  },
  {
    "text": "Imagine, for example, that the text you input is most of an entire mystery novel,",
    "start": "239100",
    "end": "243450"
  },
  {
    "text": "all the way up to a point near the end, which reads, therefore the murderer was.",
    "start": "243450",
    "end": "247800"
  },
  {
    "text": "If the model is going to accurately predict the next word,",
    "start": "248400",
    "end": "251457"
  },
  {
    "text": "that final vector in the sequence, which began its life simply embedding the word was,",
    "start": "251457",
    "end": "256043"
  },
  {
    "text": "will have to have been updated by all of the attention blocks to represent much,",
    "start": "256043",
    "end": "260312"
  },
  {
    "text": "much more than any individual word, somehow encoding all of the information",
    "start": "260313",
    "end": "264319"
  },
  {
    "text": "from the full context window that's relevant to predicting the next word.",
    "start": "264319",
    "end": "268220"
  },
  {
    "start": "269000",
    "end": "668000"
  },
  {
    "text": "To step through the computations, though, let's take a much simpler example.",
    "start": "269500",
    "end": "272580"
  },
  {
    "text": "Imagine that the input includes the phrase, a",
    "start": "272980",
    "end": "275390"
  },
  {
    "text": "fluffy blue creature roamed the verdant forest.",
    "start": "275390",
    "end": "277960"
  },
  {
    "text": "And for the moment, suppose that the only type of update that we care about",
    "start": "278460",
    "end": "282620"
  },
  {
    "text": "is having the adjectives adjust the meanings of their corresponding nouns.",
    "start": "282620",
    "end": "286780"
  },
  {
    "text": "What I'm about to describe is what we would call a single head of attention,",
    "start": "287000",
    "end": "290720"
  },
  {
    "text": "and later we will see how the attention block consists of many different heads run in",
    "start": "290720",
    "end": "294930"
  },
  {
    "text": "parallel.",
    "start": "294930",
    "end": "295419"
  },
  {
    "text": "Again, the initial embedding for each word is some high dimensional vector",
    "start": "296140",
    "end": "299835"
  },
  {
    "text": "that only encodes the meaning of that particular word with no context.",
    "start": "299835",
    "end": "303380"
  },
  {
    "text": "Actually, that's not quite true.",
    "start": "304000",
    "end": "305220"
  },
  {
    "text": "They also encode the position of the word.",
    "start": "305380",
    "end": "307640"
  },
  {
    "text": "There's a lot more to say about the specific way that positions are encoded,",
    "start": "307980",
    "end": "311620"
  },
  {
    "text": "but right now, all you need to know is that the entries of this vector are",
    "start": "311620",
    "end": "315212"
  },
  {
    "text": "enough to tell you both what the word is and where it exists in the context.",
    "start": "315212",
    "end": "318900"
  },
  {
    "text": "Let's go ahead and denote these embeddings with the letter e.",
    "start": "319500",
    "end": "321660"
  },
  {
    "text": "The goal is to have a series of computations produce a new refined",
    "start": "322420",
    "end": "326050"
  },
  {
    "text": "set of embeddings where, for example, those corresponding to the",
    "start": "326050",
    "end": "329625"
  },
  {
    "text": "nouns have ingested the meaning from their corresponding adjectives.",
    "start": "329625",
    "end": "333420"
  },
  {
    "text": "And playing the deep learning game, we want most of the computations",
    "start": "333900",
    "end": "337149"
  },
  {
    "text": "involved to look like matrix-vector products,",
    "start": "337149",
    "end": "339345"
  },
  {
    "text": "where the matrices are full of tuneable weights,",
    "start": "339346",
    "end": "341687"
  },
  {
    "text": "things that the model will learn based on data.",
    "start": "341687",
    "end": "343980"
  },
  {
    "text": "To be clear, I'm making up this example of adjectives updating nouns just to",
    "start": "344660",
    "end": "348363"
  },
  {
    "text": "illustrate the type of behavior that you could imagine an attention head doing.",
    "start": "348363",
    "end": "352260"
  },
  {
    "text": "As with so much deep learning, the true behavior is much harder to parse because it's",
    "start": "352860",
    "end": "357003"
  },
  {
    "text": "based on tweaking and tuning a huge number of parameters to minimize some cost function.",
    "start": "357003",
    "end": "361340"
  },
  {
    "text": "It's just that as we step through all of different matrices filled with parameters",
    "start": "361680",
    "end": "365557"
  },
  {
    "text": "that are involved in this process, I think it's really helpful to have an imagined",
    "start": "365558",
    "end": "369484"
  },
  {
    "text": "example of something that it could be doing to help keep it all more concrete.",
    "start": "369484",
    "end": "373220"
  },
  {
    "text": "For the first step of this process, you might imagine each noun, like creature,",
    "start": "374140",
    "end": "378152"
  },
  {
    "text": "asking the question, hey, are there any adjectives sitting in front of me?",
    "start": "378152",
    "end": "381960"
  },
  {
    "text": "And for the words fluffy and blue, to each be able to answer,",
    "start": "382160",
    "end": "385376"
  },
  {
    "text": "yeah, I'm an adjective and I'm in that position.",
    "start": "385376",
    "end": "387960"
  },
  {
    "text": "That question is somehow encoded as yet another vector,",
    "start": "388960",
    "end": "392259"
  },
  {
    "text": "another list of numbers, which we call the query for this word.",
    "start": "392260",
    "end": "396100"
  },
  {
    "text": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
    "start": "396980",
    "end": "402020"
  },
  {
    "text": "Computing this query looks like taking a certain matrix,",
    "start": "402940",
    "end": "406300"
  },
  {
    "text": "which I'll label wq, and multiplying it by the embedding.",
    "start": "406300",
    "end": "409780"
  },
  {
    "text": "Compressing things a bit, let's write that query vector as q,",
    "start": "410960",
    "end": "414169"
  },
  {
    "text": "and then anytime you see me put a matrix next to an arrow like this one,",
    "start": "414170",
    "end": "418012"
  },
  {
    "text": "it's meant to represent that multiplying this matrix by the vector at the arrow's start",
    "start": "418012",
    "end": "422642"
  },
  {
    "text": "gives you the vector at the arrow's end.",
    "start": "422642",
    "end": "424800"
  },
  {
    "text": "In this case, you multiply this matrix by all of the embeddings in the context,",
    "start": "425860",
    "end": "430211"
  },
  {
    "text": "producing one query vector for each token.",
    "start": "430211",
    "end": "432580"
  },
  {
    "text": "The entries of this matrix are parameters of the model,",
    "start": "433740",
    "end": "436380"
  },
  {
    "text": "which means the true behavior is learned from data, and in practice,",
    "start": "436381",
    "end": "439694"
  },
  {
    "text": "what this matrix does in a particular attention head is challenging to parse.",
    "start": "439694",
    "end": "443440"
  },
  {
    "text": "But for our sake, imagining an example that we might hope that it would learn,",
    "start": "443900",
    "end": "447896"
  },
  {
    "text": "we'll suppose that this query matrix maps the embeddings of nouns to",
    "start": "447896",
    "end": "451431"
  },
  {
    "text": "certain directions in this smaller query space that somehow encodes",
    "start": "451431",
    "end": "454915"
  },
  {
    "text": "the notion of looking for adjectives in preceding positions.",
    "start": "454915",
    "end": "458040"
  },
  {
    "text": "As to what it does to other embeddings, who knows?",
    "start": "458780",
    "end": "461440"
  },
  {
    "text": "Maybe it simultaneously tries to accomplish some other goal with those.",
    "start": "461720",
    "end": "464340"
  },
  {
    "text": "Right now, we're laser focused on the nouns.",
    "start": "464540",
    "end": "467160"
  },
  {
    "text": "At the same time, associated with this is a second matrix called the key matrix,",
    "start": "467280",
    "end": "471598"
  },
  {
    "text": "which you also multiply by every one of the embeddings.",
    "start": "471598",
    "end": "474620"
  },
  {
    "text": "This produces a second sequence of vectors that we call the keys.",
    "start": "475280",
    "end": "478500"
  },
  {
    "text": "Conceptually, you want to think of the keys as potentially answering the queries.",
    "start": "479420",
    "end": "483140"
  },
  {
    "text": "This key matrix is also full of tuneable parameters, and just like the query matrix,",
    "start": "483840",
    "end": "487964"
  },
  {
    "text": "it maps the embedding vectors to that same smaller dimensional space.",
    "start": "487964",
    "end": "491400"
  },
  {
    "text": "You think of the keys as matching the queries whenever they closely align with each other.",
    "start": "492200",
    "end": "497020"
  },
  {
    "text": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and",
    "start": "497460",
    "end": "502153"
  },
  {
    "text": "blue to vectors that are closely aligned with the query produced by the word creature.",
    "start": "502153",
    "end": "506740"
  },
  {
    "text": "To measure how well each key matches each query,",
    "start": "507200",
    "end": "510114"
  },
  {
    "text": "you compute a dot product between each possible key-query pair.",
    "start": "510114",
    "end": "514000"
  },
  {
    "text": "I like to visualize a grid full of a bunch of dots,",
    "start": "514480",
    "end": "517104"
  },
  {
    "text": "where the bigger dots correspond to the larger dot products,",
    "start": "517105",
    "end": "520243"
  },
  {
    "text": "the places where the keys and queries align.",
    "start": "520244",
    "end": "522560"
  },
  {
    "text": "For our adjective noun example, that would look a little more like this,",
    "start": "523280",
    "end": "527477"
  },
  {
    "text": "where if the keys produced by fluffy and blue really do align closely with the query",
    "start": "527477",
    "end": "532432"
  },
  {
    "text": "produced by creature, then the dot products in these two spots would be some large",
    "start": "532432",
    "end": "537271"
  },
  {
    "text": "positive numbers.",
    "start": "537271",
    "end": "538320"
  },
  {
    "text": "In the lingo, machine learning people would say that this means the",
    "start": "539100",
    "end": "542259"
  },
  {
    "text": "embeddings of fluffy and blue attend to the embedding of creature.",
    "start": "542260",
    "end": "545420"
  },
  {
    "text": "By contrast to the dot product between the key for some other",
    "start": "546040",
    "end": "549466"
  },
  {
    "text": "word like the and the query for creature would be some small",
    "start": "549466",
    "end": "552893"
  },
  {
    "text": "or negative value that reflects that are unrelated to each other.",
    "start": "552893",
    "end": "556600"
  },
  {
    "text": "So we have this grid of values that can be any real number from",
    "start": "557700",
    "end": "561332"
  },
  {
    "text": "negative infinity to infinity, giving us a score for how relevant",
    "start": "561332",
    "end": "565136"
  },
  {
    "text": "each word is to updating the meaning of every other word.",
    "start": "565136",
    "end": "568480"
  },
  {
    "text": "The way we're about to use these scores is to take a certain",
    "start": "569200",
    "end": "572518"
  },
  {
    "text": "weighted sum along each column, weighted by the relevance.",
    "start": "572518",
    "end": "575780"
  },
  {
    "text": "So instead of having values range from negative infinity to infinity,",
    "start": "576520",
    "end": "580160"
  },
  {
    "text": "what we want is for the numbers in these columns to be between 0 and 1,",
    "start": "580160",
    "end": "583959"
  },
  {
    "text": "and for each column to add up to 1, as if they were a probability distribution.",
    "start": "583959",
    "end": "588180"
  },
  {
    "text": "If you're coming in from the last chapter, you know what we need to do then.",
    "start": "589280",
    "end": "592220"
  },
  {
    "text": "We compute a softmax along each one of these columns to normalize the values.",
    "start": "592620",
    "end": "597300"
  },
  {
    "text": "In our picture, after you apply softmax to all of the columns,",
    "start": "600060",
    "end": "603187"
  },
  {
    "text": "we'll fill in the grid with these normalized values.",
    "start": "603187",
    "end": "605860"
  },
  {
    "text": "At this point you're safe to think about each column as giving weights according",
    "start": "606780",
    "end": "610705"
  },
  {
    "text": "to how relevant the word on the left is to the corresponding value at the top.",
    "start": "610705",
    "end": "614580"
  },
  {
    "text": "We call this grid an attention pattern.",
    "start": "615080",
    "end": "616840"
  },
  {
    "text": "Now if you look at the original transformer paper,",
    "start": "618080",
    "end": "620235"
  },
  {
    "text": "there's a really compact way that they write this all down.",
    "start": "620235",
    "end": "622820"
  },
  {
    "text": "Here the variables q and k represent the full arrays of query",
    "start": "623880",
    "end": "627428"
  },
  {
    "text": "and key vectors respectively, those little vectors you get by",
    "start": "627428",
    "end": "631034"
  },
  {
    "text": "multiplying the embeddings by the query and the key matrices.",
    "start": "631034",
    "end": "634640"
  },
  {
    "text": "This expression up in the numerator is a really compact way to represent",
    "start": "635160",
    "end": "639063"
  },
  {
    "text": "the grid of all possible dot products between pairs of keys and queries.",
    "start": "639063",
    "end": "643019"
  },
  {
    "text": "A small technical detail that I didn't mention is that for numerical stability,",
    "start": "644000",
    "end": "648035"
  },
  {
    "text": "it happens to be helpful to divide all of these values by the",
    "start": "648035",
    "end": "651201"
  },
  {
    "text": "square root of the dimension in that key query space.",
    "start": "651202",
    "end": "653960"
  },
  {
    "text": "Then this softmax that's wrapped around the full expression",
    "start": "654480",
    "end": "657808"
  },
  {
    "text": "is meant to be understood to apply column by column.",
    "start": "657809",
    "end": "660800"
  },
  {
    "text": "As to that v term, we'll talk about it in just a second.",
    "start": "661640",
    "end": "664700"
  },
  {
    "text": "Before that, there's one other technical detail that so far I've skipped.",
    "start": "665020",
    "end": "668460"
  },
  {
    "start": "668000",
    "end": "762000"
  },
  {
    "text": "During the training process, when you run this model on a given text example,",
    "start": "669040",
    "end": "672998"
  },
  {
    "text": "and all of the weights are slightly adjusted and tuned to either reward or punish it",
    "start": "672999",
    "end": "677369"
  },
  {
    "text": "based on how high a probability it assigns to the true next word in the passage,",
    "start": "677369",
    "end": "681534"
  },
  {
    "text": "it turns out to make the whole training process a lot more efficient if you",
    "start": "681534",
    "end": "685442"
  },
  {
    "text": "simultaneously have it predict every possible next token following each initial",
    "start": "685442",
    "end": "689555"
  },
  {
    "text": "subsequence of tokens in this passage.",
    "start": "689555",
    "end": "691560"
  },
  {
    "text": "For example, with the phrase that we've been focusing on,",
    "start": "691940",
    "end": "694876"
  },
  {
    "text": "it might also be predicting what words follow creature and what words follow the.",
    "start": "694876",
    "end": "699100"
  },
  {
    "text": "This is really nice, because it means what would otherwise",
    "start": "699940",
    "end": "702825"
  },
  {
    "text": "be a single training example effectively acts as many.",
    "start": "702825",
    "end": "705560"
  },
  {
    "text": "For the purposes of our attention pattern, it means that you never",
    "start": "706100",
    "end": "709430"
  },
  {
    "text": "want to allow later words to influence earlier words,",
    "start": "709430",
    "end": "712154"
  },
  {
    "text": "since otherwise they could kind of give away the answer for what comes next.",
    "start": "712155",
    "end": "716040"
  },
  {
    "text": "What this means is that we want all of these spots here,",
    "start": "716560",
    "end": "719562"
  },
  {
    "text": "the ones representing later tokens influencing earlier ones,",
    "start": "719562",
    "end": "722831"
  },
  {
    "text": "to somehow be forced to be zero.",
    "start": "722831",
    "end": "724600"
  },
  {
    "text": "The simplest thing you might think to do is to set them equal to zero,",
    "start": "725920",
    "end": "728711"
  },
  {
    "text": "but if you did that the columns wouldn't add up to one anymore,",
    "start": "728711",
    "end": "731264"
  },
  {
    "text": "they wouldn't be normalized.",
    "start": "731264",
    "end": "732420"
  },
  {
    "text": "So instead, a common way to do this is that before applying softmax,",
    "start": "733120",
    "end": "736409"
  },
  {
    "text": "you set all of those entries to be negative infinity.",
    "start": "736409",
    "end": "739020"
  },
  {
    "text": "If you do that, then after applying softmax, all of those get turned into zero,",
    "start": "739680",
    "end": "743559"
  },
  {
    "text": "but the columns stay normalized.",
    "start": "743559",
    "end": "745180"
  },
  {
    "text": "This process is called masking.",
    "start": "746000",
    "end": "747540"
  },
  {
    "text": "There are versions of attention where you don't apply it, but in our GPT example,",
    "start": "747540",
    "end": "751298"
  },
  {
    "text": "even though this is more relevant during the training phase than it would be,",
    "start": "751298",
    "end": "754918"
  },
  {
    "text": "say, running it as a chatbot or something like that,",
    "start": "754918",
    "end": "757377"
  },
  {
    "text": "you do always apply this masking to prevent later tokens from influencing earlier ones.",
    "start": "757377",
    "end": "761460"
  },
  {
    "start": "762000",
    "end": "790000"
  },
  {
    "text": "Another fact that's worth reflecting on about this attention",
    "start": "762480",
    "end": "765771"
  },
  {
    "text": "pattern is how its size is equal to the square of the context size.",
    "start": "765771",
    "end": "769500"
  },
  {
    "text": "So this is why context size can be a really huge bottleneck for large language models,",
    "start": "769900",
    "end": "773999"
  },
  {
    "text": "and scaling it up is non-trivial.",
    "start": "773999",
    "end": "775620"
  },
  {
    "text": "As you imagine, motivated by a desire for bigger and bigger context windows,",
    "start": "776300",
    "end": "780075"
  },
  {
    "text": "recent years have seen some variations to the attention mechanism aimed at making",
    "start": "780075",
    "end": "784148"
  },
  {
    "text": "context more scalable, but right here, you and I are staying focused on the basics.",
    "start": "784148",
    "end": "788320"
  },
  {
    "start": "790000",
    "end": "944000"
  },
  {
    "text": "Okay, great, computing this pattern lets the model",
    "start": "790560",
    "end": "792925"
  },
  {
    "text": "deduce which words are relevant to which other words.",
    "start": "792925",
    "end": "795480"
  },
  {
    "text": "Now you need to actually update the embeddings,",
    "start": "796020",
    "end": "798510"
  },
  {
    "text": "allowing words to pass information to whichever other words they're relevant to.",
    "start": "798510",
    "end": "802800"
  },
  {
    "text": "For example, you want the embedding of Fluffy to somehow cause a change",
    "start": "802800",
    "end": "806761"
  },
  {
    "text": "to Creature that moves it to a different part of this 12,000-dimensional",
    "start": "806762",
    "end": "810837"
  },
  {
    "text": "embedding space that more specifically encodes a Fluffy creature.",
    "start": "810837",
    "end": "814519"
  },
  {
    "text": "What I'm going to do here is first show you the most straightforward",
    "start": "815460",
    "end": "818323"
  },
  {
    "text": "way that you could do this, though there's a slight way that",
    "start": "818323",
    "end": "820891"
  },
  {
    "text": "this gets modified in the context of multi-headed attention.",
    "start": "820892",
    "end": "823460"
  },
  {
    "text": "This most straightforward way would be to use a third matrix,",
    "start": "824080",
    "end": "827115"
  },
  {
    "text": "what we call the value matrix, which you multiply by the embedding of that first word,",
    "start": "827115",
    "end": "831445"
  },
  {
    "text": "for example Fluffy.",
    "start": "831445",
    "end": "832440"
  },
  {
    "text": "The result of this is what you would call a value vector,",
    "start": "833300",
    "end": "835886"
  },
  {
    "text": "and this is something that you add to the embedding of the second word,",
    "start": "835886",
    "end": "839153"
  },
  {
    "text": "in this case something you add to the embedding of Creature.",
    "start": "839153",
    "end": "841920"
  },
  {
    "text": "So this value vector lives in the same very high-dimensional space as the embeddings.",
    "start": "842600",
    "end": "847000"
  },
  {
    "text": "When you multiply this value matrix by the embedding of a word,",
    "start": "847460",
    "end": "850780"
  },
  {
    "text": "you might think of it as saying, if this word is relevant to adjusting the meaning of",
    "start": "850780",
    "end": "855311"
  },
  {
    "text": "something else, what exactly should be added to the embedding of that something else",
    "start": "855311",
    "end": "859790"
  },
  {
    "text": "in order to reflect this?",
    "start": "859790",
    "end": "861160"
  },
  {
    "text": "Looking back in our diagram, let's set aside all of the keys and the queries,",
    "start": "862140",
    "end": "865968"
  },
  {
    "text": "since after you compute the attention pattern you're done with those,",
    "start": "865968",
    "end": "869448"
  },
  {
    "text": "then you're going to take this value matrix and multiply it by every",
    "start": "869448",
    "end": "872878"
  },
  {
    "text": "one of those embeddings to produce a sequence of value vectors.",
    "start": "872878",
    "end": "876060"
  },
  {
    "text": "You might think of these value vectors as being",
    "start": "877120",
    "end": "879099"
  },
  {
    "text": "kind of associated with the corresponding keys.",
    "start": "879099",
    "end": "881120"
  },
  {
    "text": "For each column in this diagram, you multiply each of the",
    "start": "882320",
    "end": "885750"
  },
  {
    "text": "value vectors by the corresponding weight in that column.",
    "start": "885750",
    "end": "889240"
  },
  {
    "text": "For example here, under the embedding of Creature,",
    "start": "890080",
    "end": "892762"
  },
  {
    "text": "you would be adding large proportions of the value vectors for Fluffy and Blue,",
    "start": "892762",
    "end": "897054"
  },
  {
    "text": "while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
    "start": "897054",
    "end": "901560"
  },
  {
    "text": "And then finally, the way to actually update the embedding associated with this column,",
    "start": "902120",
    "end": "906751"
  },
  {
    "text": "previously encoding some context-free meaning of Creature,",
    "start": "906751",
    "end": "909891"
  },
  {
    "text": "you add together all of these rescaled values in the column,",
    "start": "909892",
    "end": "913139"
  },
  {
    "text": "producing a change that you want to add, that I'll label delta-e,",
    "start": "913139",
    "end": "916652"
  },
  {
    "text": "and then you add that to the original embedding.",
    "start": "916652",
    "end": "919259"
  },
  {
    "text": "Hopefully what results is a more refined vector encoding the more",
    "start": "919680",
    "end": "923115"
  },
  {
    "text": "contextually rich meaning, like that of a fluffy blue creature.",
    "start": "923116",
    "end": "926500"
  },
  {
    "text": "And of course you don't just do this to one embedding,",
    "start": "927380",
    "end": "930172"
  },
  {
    "text": "you apply the same weighted sum across all of the columns in this picture,",
    "start": "930172",
    "end": "934050"
  },
  {
    "text": "producing a sequence of changes, adding all of those changes to the corresponding",
    "start": "934050",
    "end": "938290"
  },
  {
    "text": "embeddings, produces a full sequence of more refined embeddings popping out",
    "start": "938290",
    "end": "942219"
  },
  {
    "text": "of the attention block.",
    "start": "942219",
    "end": "943460"
  },
  {
    "start": "944000",
    "end": "1101000"
  },
  {
    "text": "Zooming out, this whole process is what you would describe as a single head of attention.",
    "start": "944860",
    "end": "949100"
  },
  {
    "text": "As I've described things so far, this process is parameterized by three distinct",
    "start": "949600",
    "end": "954240"
  },
  {
    "text": "matrices, all filled with tunable parameters, the key, the query, and the value.",
    "start": "954241",
    "end": "958940"
  },
  {
    "text": "I want to take a moment to continue what we started in the last chapter,",
    "start": "959500",
    "end": "962935"
  },
  {
    "text": "with the scorekeeping where we count up the total number of model parameters using the",
    "start": "962935",
    "end": "967086"
  },
  {
    "text": "numbers from GPT-3.",
    "start": "967086",
    "end": "968040"
  },
  {
    "text": "These key and query matrices each have 12,288 columns, matching the embedding dimension,",
    "start": "969300",
    "end": "975037"
  },
  {
    "text": "and 128 rows, matching the dimension of that smaller key query space.",
    "start": "975037",
    "end": "979600"
  },
  {
    "text": "This gives us an additional 1.5 million or so parameters for each one.",
    "start": "980260",
    "end": "984220"
  },
  {
    "text": "If you look at that value matrix by contrast, the way I've described things so",
    "start": "984860",
    "end": "990123"
  },
  {
    "text": "far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows,",
    "start": "990123",
    "end": "995859"
  },
  {
    "text": "since both its inputs and outputs live in this very large embedding space.",
    "start": "995859",
    "end": "1000920"
  },
  {
    "text": "If true, that would mean about 150 million added parameters.",
    "start": "1001500",
    "end": "1005140"
  },
  {
    "text": "And to be clear, you could do that.",
    "start": "1005660",
    "end": "1007300"
  },
  {
    "text": "You could devote orders of magnitude more parameters",
    "start": "1007420",
    "end": "1009760"
  },
  {
    "text": "to the value map than to the key and query.",
    "start": "1009760",
    "end": "1011740"
  },
  {
    "text": "But in practice, it is much more efficient if instead you make",
    "start": "1012060",
    "end": "1014992"
  },
  {
    "text": "it so that the number of parameters devoted to this value map",
    "start": "1014992",
    "end": "1017923"
  },
  {
    "text": "is the same as the number devoted to the key and the query.",
    "start": "1017923",
    "end": "1020759"
  },
  {
    "text": "This is especially relevant in the setting of",
    "start": "1021460",
    "end": "1023290"
  },
  {
    "text": "running multiple attention heads in parallel.",
    "start": "1023290",
    "end": "1025160"
  },
  {
    "text": "The way this looks is that the value map is factored as a product of two smaller matrices.",
    "start": "1026240",
    "end": "1030100"
  },
  {
    "text": "Conceptually, I would still encourage you to think about the overall linear map,",
    "start": "1031180",
    "end": "1035334"
  },
  {
    "text": "one with inputs and outputs, both in this larger embedding space,",
    "start": "1035335",
    "end": "1038761"
  },
  {
    "text": "for example taking the embedding of blue to this blueness direction that you would",
    "start": "1038762",
    "end": "1043073"
  },
  {
    "text": "add to nouns.",
    "start": "1043073",
    "end": "1043800"
  },
  {
    "text": "It's just that it's a smaller number of rows,",
    "start": "1047040",
    "end": "1049808"
  },
  {
    "text": "typically the same size as the key query space.",
    "start": "1049808",
    "end": "1052760"
  },
  {
    "text": "What this means is you can think of it as mapping the",
    "start": "1053100",
    "end": "1055745"
  },
  {
    "text": "large embedding vectors down to a much smaller space.",
    "start": "1055745",
    "end": "1058440"
  },
  {
    "text": "This is not the conventional naming, but I'm going to call this the value down matrix.",
    "start": "1059040",
    "end": "1062700"
  },
  {
    "text": "The second matrix maps from this smaller space back up to the embedding space,",
    "start": "1063400",
    "end": "1067372"
  },
  {
    "text": "producing the vectors that you use to make the actual updates.",
    "start": "1067372",
    "end": "1070580"
  },
  {
    "text": "I'm going to call this one the value up matrix, which again is not conventional.",
    "start": "1071000",
    "end": "1074740"
  },
  {
    "text": "The way that you would see this written in most papers looks a little different.",
    "start": "1075160",
    "end": "1078080"
  },
  {
    "text": "I'll talk about it in a minute.",
    "start": "1078380",
    "end": "1079520"
  },
  {
    "text": "In my opinion, it tends to make things a little more conceptually confusing.",
    "start": "1079700",
    "end": "1082539"
  },
  {
    "text": "To throw in linear algebra jargon here, what we're basically doing is",
    "start": "1083260",
    "end": "1086826"
  },
  {
    "text": "constraining the overall value map to be a low rank transformation.",
    "start": "1086826",
    "end": "1090340"
  },
  {
    "text": "Turning back to the parameter count, all four of these matrices have the same size,",
    "start": "1091420",
    "end": "1096100"
  },
  {
    "text": "and adding them all up we get about 6.3 million parameters for one attention head.",
    "start": "1096100",
    "end": "1100780"
  },
  {
    "start": "1101000",
    "end": "1159000"
  },
  {
    "text": "As a quick side note, to be a little more accurate,",
    "start": "1102040",
    "end": "1104194"
  },
  {
    "text": "everything described so far is what people would call a self-attention head,",
    "start": "1104194",
    "end": "1107445"
  },
  {
    "text": "to distinguish it from a variation that comes up in other models that's",
    "start": "1107446",
    "end": "1110486"
  },
  {
    "text": "called cross-attention.",
    "start": "1110486",
    "end": "1111500"
  },
  {
    "text": "This isn't relevant to our GPT example, but if you're curious,",
    "start": "1112300",
    "end": "1115732"
  },
  {
    "text": "cross-attention involves models that process two distinct types of data,",
    "start": "1115732",
    "end": "1119774"
  },
  {
    "text": "like text in one language and text in another language that's part of an",
    "start": "1119774",
    "end": "1123815"
  },
  {
    "text": "ongoing generation of a translation, or maybe audio input of speech and an",
    "start": "1123815",
    "end": "1127966"
  },
  {
    "text": "ongoing transcription.",
    "start": "1127967",
    "end": "1129240"
  },
  {
    "text": "A cross-attention head looks almost identical.",
    "start": "1130400",
    "end": "1132700"
  },
  {
    "text": "The only difference is that the key and query maps act on different data sets.",
    "start": "1132980",
    "end": "1137400"
  },
  {
    "text": "In a model doing translation, for example, the keys might come from one language,",
    "start": "1137840",
    "end": "1142058"
  },
  {
    "text": "while the queries come from another, and the attention pattern could describe",
    "start": "1142058",
    "end": "1146119"
  },
  {
    "text": "which words from one language correspond to which words in another.",
    "start": "1146119",
    "end": "1149660"
  },
  {
    "text": "And in this setting there would typically be no masking,",
    "start": "1150340",
    "end": "1152885"
  },
  {
    "text": "since there's not really any notion of later tokens affecting earlier ones.",
    "start": "1152885",
    "end": "1156340"
  },
  {
    "text": "Staying focused on self-attention though, if you understood everything so far,",
    "start": "1157180",
    "end": "1160766"
  },
  {
    "start": "1159000",
    "end": "1336000"
  },
  {
    "text": "and if you were to stop here, you would come away with the essence of what attention",
    "start": "1160766",
    "end": "1164674"
  },
  {
    "text": "really is.",
    "start": "1164674",
    "end": "1165179"
  },
  {
    "text": "All that's really left to us is to lay out the sense",
    "start": "1165760",
    "end": "1168714"
  },
  {
    "text": "in which you do this many many different times.",
    "start": "1168714",
    "end": "1171440"
  },
  {
    "text": "In our central example we focused on adjectives updating nouns,",
    "start": "1172100",
    "end": "1175132"
  },
  {
    "text": "but of course there are lots of different ways that context can influence the",
    "start": "1175132",
    "end": "1178886"
  },
  {
    "text": "meaning of a word.",
    "start": "1178886",
    "end": "1179799"
  },
  {
    "text": "If the words they crashed the preceded the word car,",
    "start": "1180360",
    "end": "1183195"
  },
  {
    "text": "it has implications for the shape and structure of that car.",
    "start": "1183195",
    "end": "1186519"
  },
  {
    "text": "And a lot of associations might be less grammatical.",
    "start": "1187200",
    "end": "1189279"
  },
  {
    "text": "If the word wizard is anywhere in the same passage as Harry,",
    "start": "1189760",
    "end": "1192883"
  },
  {
    "text": "it suggests that this might be referring to Harry Potter,",
    "start": "1192883",
    "end": "1195903"
  },
  {
    "text": "whereas if instead the words Queen, Sussex, and William were in that passage,",
    "start": "1195903",
    "end": "1199963"
  },
  {
    "text": "then perhaps the embedding of Harry should instead be updated to refer to the prince.",
    "start": "1199963",
    "end": "1204440"
  },
  {
    "text": "For every different type of contextual updating that you might imagine,",
    "start": "1205040",
    "end": "1208540"
  },
  {
    "text": "the parameters of these key and query matrices would be different to",
    "start": "1208540",
    "end": "1211942"
  },
  {
    "text": "capture the different attention patterns, and the parameters of our",
    "start": "1211942",
    "end": "1215295"
  },
  {
    "text": "value map would be different based on what should be added to the embeddings.",
    "start": "1215295",
    "end": "1219140"
  },
  {
    "text": "And again, in practice the true behavior of these maps is much more",
    "start": "1219980",
    "end": "1223117"
  },
  {
    "text": "difficult to interpret, where the weights are set to do whatever the",
    "start": "1223117",
    "end": "1226347"
  },
  {
    "text": "model needs them to do to best accomplish its goal of predicting the next token.",
    "start": "1226348",
    "end": "1230140"
  },
  {
    "text": "As I said before, everything we described is a single head of attention,",
    "start": "1231400",
    "end": "1235188"
  },
  {
    "text": "and a full attention block inside a transformer consists of what's",
    "start": "1235188",
    "end": "1238713"
  },
  {
    "text": "called multi-headed attention, where you run a lot of these operations in parallel,",
    "start": "1238713",
    "end": "1243132"
  },
  {
    "text": "each with its own distinct key query and value maps.",
    "start": "1243132",
    "end": "1245919"
  },
  {
    "text": "GPT-3 for example uses 96 attention heads inside each block.",
    "start": "1247420",
    "end": "1251700"
  },
  {
    "text": "Considering that each one is already a bit confusing,",
    "start": "1252020",
    "end": "1254471"
  },
  {
    "text": "it's certainly a lot to hold in your head.",
    "start": "1254471",
    "end": "1256460"
  },
  {
    "text": "Just to spell it all out very explicitly, this means you have 96 distinct",
    "start": "1256760",
    "end": "1261119"
  },
  {
    "text": "key and query matrices producing 96 distinct attention patterns.",
    "start": "1261119",
    "end": "1265000"
  },
  {
    "text": "Then each head has its own distinct value matrices",
    "start": "1265440",
    "end": "1268914"
  },
  {
    "text": "used to produce 96 sequences of value vectors.",
    "start": "1268914",
    "end": "1272179"
  },
  {
    "text": "These are all added together using the corresponding attention patterns as weights.",
    "start": "1272460",
    "end": "1276679"
  },
  {
    "text": "What this means is that for each position in the context, each token,",
    "start": "1277480",
    "end": "1281398"
  },
  {
    "text": "every one of these heads produces a proposed change to be added to the embedding in",
    "start": "1281398",
    "end": "1286168"
  },
  {
    "text": "that position.",
    "start": "1286168",
    "end": "1287020"
  },
  {
    "text": "So what you do is you sum together all of those proposed changes, one for each head,",
    "start": "1287660",
    "end": "1292010"
  },
  {
    "text": "and you add the result to the original embedding of that position.",
    "start": "1292010",
    "end": "1295480"
  },
  {
    "text": "This entire sum here would be one slice of what's outputted from this multi-headed",
    "start": "1296660",
    "end": "1301721"
  },
  {
    "text": "attention block, a single one of those refined embeddings that pops out the other end",
    "start": "1301721",
    "end": "1307028"
  },
  {
    "text": "of it.",
    "start": "1307028",
    "end": "1307460"
  },
  {
    "text": "Again, this is a lot to think about, so don't",
    "start": "1308320",
    "end": "1310188"
  },
  {
    "text": "worry at all if it takes some time to sink in.",
    "start": "1310188",
    "end": "1312140"
  },
  {
    "text": "The overall idea is that by running many distinct heads in parallel,",
    "start": "1312380",
    "end": "1316318"
  },
  {
    "text": "you're giving the model the capacity to learn many distinct ways that context",
    "start": "1316318",
    "end": "1320835"
  },
  {
    "text": "changes meaning.",
    "start": "1320835",
    "end": "1321820"
  },
  {
    "text": "Pulling up our running tally for parameter count with 96 heads,",
    "start": "1323700",
    "end": "1327267"
  },
  {
    "text": "each including its own variation of these four matrices,",
    "start": "1327267",
    "end": "1330494"
  },
  {
    "text": "each block of multi-headed attention ends up with around 600 million parameters.",
    "start": "1330494",
    "end": "1335080"
  },
  {
    "start": "1336000",
    "end": "1399000"
  },
  {
    "text": "There's one added slightly annoying thing that I should really",
    "start": "1336420",
    "end": "1339026"
  },
  {
    "text": "mention for any of you who go on to read more about transformers.",
    "start": "1339026",
    "end": "1341800"
  },
  {
    "text": "You remember how I said that the value map is factored out into these two",
    "start": "1342080",
    "end": "1345591"
  },
  {
    "text": "distinct matrices, which I labeled as the value down and the value up matrices.",
    "start": "1345592",
    "end": "1349440"
  },
  {
    "text": "The way that I framed things would suggest that you see this pair of matrices",
    "start": "1349960",
    "end": "1354227"
  },
  {
    "text": "inside each attention head, and you could absolutely implement it this way.",
    "start": "1354228",
    "end": "1358440"
  },
  {
    "text": "That would be a valid design.",
    "start": "1358640",
    "end": "1359920"
  },
  {
    "text": "But the way that you see this written in papers and the way",
    "start": "1360260",
    "end": "1362570"
  },
  {
    "text": "that it's implemented in practice looks a little different.",
    "start": "1362570",
    "end": "1364919"
  },
  {
    "text": "All of these value up matrices for each head appear stapled together in one giant matrix",
    "start": "1365340",
    "end": "1370829"
  },
  {
    "text": "that we call the output matrix, associated with the entire multi-headed attention block.",
    "start": "1370829",
    "end": "1376380"
  },
  {
    "text": "And when you see people refer to the value matrix for a given attention head,",
    "start": "1376820",
    "end": "1380586"
  },
  {
    "text": "they're typically only referring to this first step,",
    "start": "1380586",
    "end": "1383178"
  },
  {
    "text": "the one that I was labeling as the value down projection into the smaller space.",
    "start": "1383178",
    "end": "1387140"
  },
  {
    "text": "For the curious among you, I've left an on-screen note about it.",
    "start": "1388340",
    "end": "1391039"
  },
  {
    "text": "It's one of those details that runs the risk of distracting",
    "start": "1391260",
    "end": "1393594"
  },
  {
    "text": "from the main conceptual points, but I do want to call it out",
    "start": "1393594",
    "end": "1396047"
  },
  {
    "text": "just so that you know if you read about this in other sources.",
    "start": "1396047",
    "end": "1398540"
  },
  {
    "start": "1399000",
    "end": "1494000"
  },
  {
    "text": "Setting aside all the technical nuances, in the preview from the last chapter we saw how",
    "start": "1399240",
    "end": "1403665"
  },
  {
    "text": "data flowing through a transformer doesn't just flow through a single attention block.",
    "start": "1403665",
    "end": "1408040"
  },
  {
    "text": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
    "start": "1408640",
    "end": "1412700"
  },
  {
    "text": "We'll talk more about those in the next chapter.",
    "start": "1413120",
    "end": "1414880"
  },
  {
    "text": "And then it repeatedly goes through many many copies of both of these operations.",
    "start": "1415180",
    "end": "1419320"
  },
  {
    "text": "What this means is that after a given word imbibes some of its context,",
    "start": "1419980",
    "end": "1423905"
  },
  {
    "text": "there are many more chances for this more nuanced embedding",
    "start": "1423905",
    "end": "1427221"
  },
  {
    "text": "to be influenced by its more nuanced surroundings.",
    "start": "1427221",
    "end": "1430039"
  },
  {
    "text": "The further down the network you go, with each embedding taking in more and more",
    "start": "1430940",
    "end": "1434946"
  },
  {
    "text": "meaning from all the other embeddings, which themselves are getting more and more",
    "start": "1434947",
    "end": "1439055"
  },
  {
    "text": "nuanced, the hope is that there's the capacity to encode higher level and more",
    "start": "1439055",
    "end": "1443012"
  },
  {
    "text": "abstract ideas about a given input beyond just descriptors and grammatical structure.",
    "start": "1443012",
    "end": "1447320"
  },
  {
    "text": "Things like sentiment and tone and whether it's a poem and what underlying",
    "start": "1447880",
    "end": "1451712"
  },
  {
    "text": "scientific truths are relevant to the piece and things like that.",
    "start": "1451712",
    "end": "1455130"
  },
  {
    "text": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers,",
    "start": "1456700",
    "end": "1461987"
  },
  {
    "text": "so the total number of key query and value parameters is multiplied by another 96,",
    "start": "1461988",
    "end": "1467341"
  },
  {
    "text": "which brings the total sum to just under 58 billion distinct parameters",
    "start": "1467341",
    "end": "1471985"
  },
  {
    "text": "devoted to all of the attention heads.",
    "start": "1471985",
    "end": "1474500"
  },
  {
    "text": "That is a lot to be sure, but it's only about a third",
    "start": "1474980",
    "end": "1477960"
  },
  {
    "text": "of the 175 billion that are in the network in total.",
    "start": "1477960",
    "end": "1480940"
  },
  {
    "text": "So even though attention gets all of the attention,",
    "start": "1481520",
    "end": "1484097"
  },
  {
    "text": "the majority of parameters come from the blocks sitting in between these steps.",
    "start": "1484097",
    "end": "1488140"
  },
  {
    "text": "In the next chapter, you and I will talk more about those",
    "start": "1488560",
    "end": "1490975"
  },
  {
    "text": "other blocks and also a lot more about the training process.",
    "start": "1490975",
    "end": "1493559"
  },
  {
    "start": "1494000",
    "end": "1570000"
  },
  {
    "text": "A big part of the story for the success of the attention mechanism is not so much any",
    "start": "1494120",
    "end": "1498764"
  },
  {
    "text": "specific kind of behaviour that it enables, but the fact that it's extremely",
    "start": "1498764",
    "end": "1502971"
  },
  {
    "text": "parallelizable, meaning that you can run a huge number of computations in a short time",
    "start": "1502971",
    "end": "1507724"
  },
  {
    "text": "using GPUs.",
    "start": "1507724",
    "end": "1508380"
  },
  {
    "text": "Given that one of the big lessons about deep learning in the last decade or two has",
    "start": "1509460",
    "end": "1513310"
  },
  {
    "text": "been that scale alone seems to give huge qualitative improvements in model performance,",
    "start": "1513311",
    "end": "1517394"
  },
  {
    "text": "there's a huge advantage to parallelizable architectures that let you do this.",
    "start": "1517394",
    "end": "1521060"
  },
  {
    "text": "If you want to learn more about this stuff, I've left lots of links in the description.",
    "start": "1522040",
    "end": "1525340"
  },
  {
    "text": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
    "start": "1525920",
    "end": "1530040"
  },
  {
    "text": "In this video, I wanted to just jump into attention in its current form,",
    "start": "1530560",
    "end": "1533720"
  },
  {
    "text": "but if you're curious about more of the history for how we got here",
    "start": "1533720",
    "end": "1536704"
  },
  {
    "text": "and how you might reinvent this idea for yourself,",
    "start": "1536704",
    "end": "1538942"
  },
  {
    "text": "my friend Vivek just put up a couple videos giving a lot more of that motivation.",
    "start": "1538942",
    "end": "1542540"
  },
  {
    "text": "Also, Britt Cruz from the channel The Art of the Problem has a",
    "start": "1543120",
    "end": "1545790"
  },
  {
    "text": "really nice video about the history of large language models.",
    "start": "1545790",
    "end": "1548460"
  },
  {
    "text": "Thank you.",
    "start": "1564960",
    "end": "1569200"
  }
]