[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "Last video I laid out the structure of a neural network.",
    "start": "4180",
    "end": "7279"
  },
  {
    "text": "I'll give a quick recap here so that it's fresh in our minds,",
    "start": "7680",
    "end": "10459"
  },
  {
    "text": "and then I have two main goals for this video.",
    "start": "10459",
    "end": "12599"
  },
  {
    "text": "The first is to introduce the idea of gradient descent,",
    "start": "13100",
    "end": "15646"
  },
  {
    "text": "which underlies not only how neural networks learn,",
    "start": "15646",
    "end": "18054"
  },
  {
    "text": "but how a lot of other machine learning works as well.",
    "start": "18054",
    "end": "20600"
  },
  {
    "text": "Then after that we'll dig in a little more into how this particular network performs,",
    "start": "21120",
    "end": "25118"
  },
  {
    "text": "and what those hidden layers of neurons end up looking for.",
    "start": "25118",
    "end": "27940"
  },
  {
    "text": "As a reminder, our goal here is the classic example of handwritten digit recognition,",
    "start": "28980",
    "end": "34066"
  },
  {
    "start": "30000",
    "end": "109000"
  },
  {
    "text": "the hello world of neural networks.",
    "start": "34066",
    "end": "36220"
  },
  {
    "text": "These digits are rendered on a 28x28 pixel grid,",
    "start": "37020",
    "end": "40032"
  },
  {
    "text": "each pixel with some grayscale value between 0 and 1.",
    "start": "40032",
    "end": "43420"
  },
  {
    "text": "Those are what determine the activations of 784 neurons in the input layer of the network.",
    "start": "43820",
    "end": "50040"
  },
  {
    "text": "And then the activation for each neuron in the following layers is based on a weighted",
    "start": "51180",
    "end": "55890"
  },
  {
    "text": "sum of all the activations in the previous layer, plus some special number called a bias.",
    "start": "55890",
    "end": "60820"
  },
  {
    "text": "Then you compose that sum with some other function,",
    "start": "62160",
    "end": "64760"
  },
  {
    "text": "like the sigmoid squishification, or a relu, the way I walked through last video.",
    "start": "64760",
    "end": "68940"
  },
  {
    "text": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each,",
    "start": "69480",
    "end": "75107"
  },
  {
    "text": "the network has about 13,000 weights and biases that we can adjust,",
    "start": "75107",
    "end": "79456"
  },
  {
    "text": "and it's these values that determine what exactly the network actually does.",
    "start": "79456",
    "end": "84380"
  },
  {
    "text": "Then what we mean when we say that this network classifies a given digit is that",
    "start": "84880",
    "end": "89090"
  },
  {
    "text": "the brightest of those 10 neurons in the final layer corresponds to that digit.",
    "start": "89090",
    "end": "93299"
  },
  {
    "text": "And remember, the motivation we had in mind here for the layered structure",
    "start": "94100",
    "end": "97971"
  },
  {
    "text": "was that maybe the second layer could pick up on the edges,",
    "start": "97971",
    "end": "101110"
  },
  {
    "text": "and the third layer might pick up on patterns like loops and lines,",
    "start": "101110",
    "end": "104667"
  },
  {
    "text": "and the last one could just piece together those patterns to recognize digits.",
    "start": "104667",
    "end": "108800"
  },
  {
    "start": "109000",
    "end": "181000"
  },
  {
    "text": "So here, we learn how the network learns.",
    "start": "109800",
    "end": "112240"
  },
  {
    "text": "What we want is an algorithm where you can show this network a whole bunch of",
    "start": "112640",
    "end": "116781"
  },
  {
    "text": "training data, which comes in the form of a bunch of different images of handwritten",
    "start": "116781",
    "end": "121353"
  },
  {
    "text": "digits, along with labels for what they're supposed to be,",
    "start": "121353",
    "end": "124525"
  },
  {
    "text": "and it'll adjust those 13,000 weights and biases so as to improve its performance",
    "start": "124526",
    "end": "128937"
  },
  {
    "text": "on the training data.",
    "start": "128937",
    "end": "130119"
  },
  {
    "text": "Hopefully, this layered structure will mean that what it",
    "start": "130720",
    "end": "133790"
  },
  {
    "text": "learns generalizes to images beyond that training data.",
    "start": "133790",
    "end": "136860"
  },
  {
    "text": "The way we test that is that after you train the network,",
    "start": "137640",
    "end": "140591"
  },
  {
    "text": "you show it more labeled data that it's never seen before,",
    "start": "140591",
    "end": "143645"
  },
  {
    "text": "and you see how accurately it classifies those new images.",
    "start": "143645",
    "end": "146700"
  },
  {
    "text": "Fortunately for us, and what makes this such a common example to start with,",
    "start": "151120",
    "end": "154871"
  },
  {
    "text": "is that the good people behind the MNIST database have put together a collection of tens",
    "start": "154871",
    "end": "159264"
  },
  {
    "text": "of thousands of handwritten digit images, each one labeled with the numbers they're",
    "start": "159264",
    "end": "163410"
  },
  {
    "text": "supposed to be.",
    "start": "163410",
    "end": "164200"
  },
  {
    "text": "And as provocative as it is to describe a machine as learning,",
    "start": "164900",
    "end": "168504"
  },
  {
    "text": "once you see how it works, it feels a lot less like some crazy sci-fi premise,",
    "start": "168504",
    "end": "173097"
  },
  {
    "text": "and a lot more like a calculus exercise.",
    "start": "173097",
    "end": "175480"
  },
  {
    "text": "I mean, basically it comes down to finding the minimum of a certain function.",
    "start": "176200",
    "end": "179959"
  },
  {
    "start": "181000",
    "end": "415000"
  },
  {
    "text": "Remember, conceptually, we're thinking of each neuron as being connected to all",
    "start": "181940",
    "end": "186222"
  },
  {
    "text": "the neurons in the previous layer, and the weights in the weighted sum defining",
    "start": "186222",
    "end": "190558"
  },
  {
    "text": "its activation are kind of like the strengths of those connections,",
    "start": "190558",
    "end": "194244"
  },
  {
    "text": "and the bias is some indication of whether that neuron tends to be active or inactive.",
    "start": "194244",
    "end": "198960"
  },
  {
    "text": "And to start things off, we're just going to initialize",
    "start": "199720",
    "end": "202171"
  },
  {
    "text": "all of those weights and biases totally randomly.",
    "start": "202171",
    "end": "204400"
  },
  {
    "text": "Needless to say, this network is going to perform pretty horribly on",
    "start": "204940",
    "end": "207873"
  },
  {
    "text": "a given training example, since it's just doing something random.",
    "start": "207873",
    "end": "210720"
  },
  {
    "text": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
    "start": "211040",
    "end": "216019"
  },
  {
    "text": "So what you do is define a cost function, a way of telling the computer,",
    "start": "216600",
    "end": "221409"
  },
  {
    "text": "no, bad computer, that output should have activations which are 0 for most neurons,",
    "start": "221409",
    "end": "227019"
  },
  {
    "text": "but 1 for this neuron, what you gave me is utter trash.",
    "start": "227020",
    "end": "230760"
  },
  {
    "text": "To say that a little more mathematically, you add up the squares of the differences",
    "start": "231720",
    "end": "236438"
  },
  {
    "text": "between each of those trash output activations and the value you want them to have,",
    "start": "236438",
    "end": "241212"
  },
  {
    "text": "and this is what we'll call the cost of a single training example.",
    "start": "241212",
    "end": "245020"
  },
  {
    "text": "Notice this sum is small when the network confidently classifies the image correctly,",
    "start": "245960",
    "end": "251472"
  },
  {
    "text": "but it's large when the network seems like it doesn't know what it's doing.",
    "start": "251472",
    "end": "256400"
  },
  {
    "text": "So then what you do is consider the average cost over all of",
    "start": "258640",
    "end": "262012"
  },
  {
    "text": "the tens of thousands of training examples at your disposal.",
    "start": "262012",
    "end": "265440"
  },
  {
    "text": "This average cost is our measure for how lousy the network is,",
    "start": "267040",
    "end": "270574"
  },
  {
    "text": "and how bad the computer should feel.",
    "start": "270574",
    "end": "272740"
  },
  {
    "text": "And that's a complicated thing.",
    "start": "273420",
    "end": "274600"
  },
  {
    "text": "Remember how the network itself was basically a function,",
    "start": "275040",
    "end": "278557"
  },
  {
    "text": "one that takes in 784 numbers as inputs, the pixel values,",
    "start": "278557",
    "end": "282198"
  },
  {
    "text": "and spits out 10 numbers as its output, and in a sense it's parameterized",
    "start": "282198",
    "end": "286764"
  },
  {
    "text": "by all these weights and biases?",
    "start": "286764",
    "end": "288800"
  },
  {
    "text": "Well the cost function is a layer of complexity on top of that.",
    "start": "289500",
    "end": "292820"
  },
  {
    "text": "It takes as its input those 13,000 or so weights and biases,",
    "start": "293100",
    "end": "296789"
  },
  {
    "text": "and spits out a single number describing how bad those weights and biases are,",
    "start": "296789",
    "end": "301646"
  },
  {
    "text": "and the way it's defined depends on the network's behavior over all the tens of",
    "start": "301646",
    "end": "306564"
  },
  {
    "text": "thousands of pieces of training data.",
    "start": "306564",
    "end": "308900"
  },
  {
    "text": "That's a lot to think about.",
    "start": "309520",
    "end": "311000"
  },
  {
    "text": "But just telling the computer what a crappy job it's doing isn't very helpful.",
    "start": "312400",
    "end": "315820"
  },
  {
    "text": "You want to tell it how to change those weights and biases so that it gets better.",
    "start": "316220",
    "end": "320060"
  },
  {
    "text": "To make it easier, rather than struggling to imagine a function with 13,000 inputs,",
    "start": "320780",
    "end": "325381"
  },
  {
    "text": "just imagine a simple function that has one number as an input and one number as an",
    "start": "325381",
    "end": "330037"
  },
  {
    "text": "output.",
    "start": "330037",
    "end": "330479"
  },
  {
    "text": "How do you find an input that minimizes the value of this function?",
    "start": "331480",
    "end": "335300"
  },
  {
    "text": "Calculus students will know that you can sometimes figure out that minimum explicitly,",
    "start": "336460",
    "end": "341169"
  },
  {
    "text": "but that's not always feasible for really complicated functions,",
    "start": "341169",
    "end": "344728"
  },
  {
    "text": "certainly not in the 13,000 input version of this situation for our crazy complicated",
    "start": "344728",
    "end": "349437"
  },
  {
    "text": "neural network cost function.",
    "start": "349437",
    "end": "351080"
  },
  {
    "text": "A more flexible tactic is to start at any input,",
    "start": "351580",
    "end": "354577"
  },
  {
    "text": "and figure out which direction you should step to make that output lower.",
    "start": "354578",
    "end": "359200"
  },
  {
    "text": "Specifically, if you can figure out the slope of the function where you are,",
    "start": "360080",
    "end": "364091"
  },
  {
    "text": "then shift to the left if that slope is positive,",
    "start": "364092",
    "end": "366732"
  },
  {
    "text": "and shift the input to the right if that slope is negative.",
    "start": "366732",
    "end": "369900"
  },
  {
    "text": "If you do this repeatedly, at each point checking the new slope and taking the",
    "start": "371960",
    "end": "375875"
  },
  {
    "text": "appropriate step, you're going to approach some local minimum of the function.",
    "start": "375875",
    "end": "379840"
  },
  {
    "text": "The image you might have in mind here is a ball rolling down a hill.",
    "start": "380640",
    "end": "383800"
  },
  {
    "text": "Notice, even for this really simplified single input function,",
    "start": "384620",
    "end": "387791"
  },
  {
    "text": "there are many possible valleys that you might land in,",
    "start": "387791",
    "end": "390655"
  },
  {
    "text": "depending on which random input you start at,",
    "start": "390655",
    "end": "393007"
  },
  {
    "text": "and there's no guarantee that the local minimum you land in is going to",
    "start": "393007",
    "end": "396689"
  },
  {
    "text": "be the smallest possible value of the cost function.",
    "start": "396689",
    "end": "399400"
  },
  {
    "text": "That will carry over to our neural network case as well.",
    "start": "400220",
    "end": "402620"
  },
  {
    "text": "And I also want you to notice how if you make your step sizes proportional to the slope,",
    "start": "403180",
    "end": "407568"
  },
  {
    "text": "then when the slope is flattening out towards the minimum,",
    "start": "407568",
    "end": "410511"
  },
  {
    "text": "your steps get smaller and smaller, and that kind of helps you from overshooting.",
    "start": "410511",
    "end": "414599"
  },
  {
    "start": "415000",
    "end": "678000"
  },
  {
    "text": "Bumping up the complexity a bit, imagine instead",
    "start": "415940",
    "end": "418598"
  },
  {
    "text": "a function with two inputs and one output.",
    "start": "418598",
    "end": "420979"
  },
  {
    "text": "You might think of the input space as the xy-plane,",
    "start": "421500",
    "end": "424497"
  },
  {
    "text": "and the cost function as being graphed as a surface above it.",
    "start": "424497",
    "end": "428140"
  },
  {
    "text": "Now instead of asking about the slope of the function,",
    "start": "428760",
    "end": "431803"
  },
  {
    "text": "you have to ask which direction you should step in this input",
    "start": "431803",
    "end": "435297"
  },
  {
    "text": "space so as to decrease the output of the function most quickly.",
    "start": "435297",
    "end": "438960"
  },
  {
    "text": "In other words, what's the downhill direction?",
    "start": "439720",
    "end": "441760"
  },
  {
    "text": "Again, it's helpful to think of a ball rolling down that hill.",
    "start": "442380",
    "end": "445560"
  },
  {
    "text": "Those of you familiar with multivariable calculus will know that the",
    "start": "446660",
    "end": "450661"
  },
  {
    "text": "gradient of a function gives you the direction of steepest ascent,",
    "start": "450661",
    "end": "454603"
  },
  {
    "text": "which direction should you step to increase the function most quickly.",
    "start": "454603",
    "end": "458780"
  },
  {
    "text": "Naturally enough, taking the negative of that gradient gives you",
    "start": "459560",
    "end": "462800"
  },
  {
    "text": "the direction to step that decreases the function most quickly.",
    "start": "462800",
    "end": "466039"
  },
  {
    "text": "Even more than that, the length of this gradient vector is",
    "start": "467240",
    "end": "470569"
  },
  {
    "text": "an indication for just how steep that steepest slope is.",
    "start": "470569",
    "end": "473840"
  },
  {
    "text": "If you're unfamiliar with multivariable calculus and want to learn more,",
    "start": "474540",
    "end": "477611"
  },
  {
    "text": "check out some of the work I did for Khan Academy on the topic.",
    "start": "477611",
    "end": "480340"
  },
  {
    "text": "Honestly though, all that matters for you and me right now is that",
    "start": "480860",
    "end": "484485"
  },
  {
    "text": "in principle there exists a way to compute this vector,",
    "start": "484485",
    "end": "487560"
  },
  {
    "text": "this vector that tells you what the downhill direction is and how steep it is.",
    "start": "487561",
    "end": "491900"
  },
  {
    "text": "You'll be okay if that's all you know and you're not rock solid on the details.",
    "start": "492400",
    "end": "496120"
  },
  {
    "text": "Cause If you can get that, the algorithm for minimizing the function is to compute this",
    "start": "497200",
    "end": "502054"
  },
  {
    "text": "gradient direction, then take a small step downhill, and repeat that over and over.",
    "start": "502054",
    "end": "506740"
  },
  {
    "text": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
    "start": "507700",
    "end": "512820"
  },
  {
    "text": "Imagine organizing all 13,000 weights and biases",
    "start": "513400",
    "end": "516596"
  },
  {
    "text": "of our network into a giant column vector.",
    "start": "516596",
    "end": "519460"
  },
  {
    "text": "The negative gradient of the cost function is just a vector,",
    "start": "520140",
    "end": "523919"
  },
  {
    "text": "it's some direction inside this insanely huge input space that tells you which",
    "start": "523919",
    "end": "528896"
  },
  {
    "text": "nudges to all of those numbers is going to cause the most rapid decrease to",
    "start": "528896",
    "end": "533683"
  },
  {
    "text": "the cost function.",
    "start": "533683",
    "end": "534880"
  },
  {
    "text": "And of course, with our specially designed cost function,",
    "start": "535640",
    "end": "538833"
  },
  {
    "text": "changing the weights and biases to decrease it means making the",
    "start": "538833",
    "end": "542418"
  },
  {
    "text": "output of the network on each piece of training data look less like",
    "start": "542418",
    "end": "546227"
  },
  {
    "text": "a random array of 10 values, and more like an actual decision we want it to make.",
    "start": "546227",
    "end": "550820"
  },
  {
    "text": "It's important to remember, this cost function involves an average over all of the",
    "start": "551440",
    "end": "555877"
  },
  {
    "text": "training data, so if you minimize it, it means it's a better performance on all of those",
    "start": "555877",
    "end": "560693"
  },
  {
    "text": "samples.",
    "start": "560693",
    "end": "561180"
  },
  {
    "text": "The algorithm for computing this gradient efficiently,",
    "start": "563820",
    "end": "566563"
  },
  {
    "text": "which is effectively the heart of how a neural network learns,",
    "start": "566563",
    "end": "569764"
  },
  {
    "text": "is called backpropagation, and it's what I'm going to be talking about next video.",
    "start": "569764",
    "end": "573980"
  },
  {
    "text": "There, I really want to take the time to walk through what exactly happens to",
    "start": "574660",
    "end": "578668"
  },
  {
    "text": "each weight and bias for a given piece of training data,",
    "start": "578668",
    "end": "581635"
  },
  {
    "text": "trying to give an intuitive feel for what's happening beyond the pile of relevant",
    "start": "581635",
    "end": "585903"
  },
  {
    "text": "calculus and formulas.",
    "start": "585903",
    "end": "587100"
  },
  {
    "text": "Right here, right now, the main thing I want you to know,",
    "start": "587780",
    "end": "590780"
  },
  {
    "text": "independent of implementation details, is that what we mean when we",
    "start": "590780",
    "end": "594360"
  },
  {
    "text": "talk about a network learning is that it's just minimizing a cost function.",
    "start": "594360",
    "end": "598360"
  },
  {
    "text": "And notice, one consequence of that is that it's important for this cost function to have",
    "start": "599300",
    "end": "603651"
  },
  {
    "text": "a nice smooth output, so that we can find a local minimum by taking little steps",
    "start": "603651",
    "end": "607611"
  },
  {
    "text": "downhill.",
    "start": "607611",
    "end": "608100"
  },
  {
    "text": "This is why, by the way, artificial neurons have continuously ranging activations,",
    "start": "609260",
    "end": "613889"
  },
  {
    "text": "rather than simply being active or inactive in a binary way,",
    "start": "613889",
    "end": "617332"
  },
  {
    "text": "the way biological neurons are.",
    "start": "617333",
    "end": "619140"
  },
  {
    "text": "This process of repeatedly nudging an input of a function by some",
    "start": "620220",
    "end": "623566"
  },
  {
    "text": "multiple of the negative gradient is called gradient descent.",
    "start": "623567",
    "end": "626760"
  },
  {
    "text": "It's a way to converge towards some local minimum of a cost function,",
    "start": "627300",
    "end": "630837"
  },
  {
    "text": "basically a valley in this graph.",
    "start": "630837",
    "end": "632579"
  },
  {
    "text": "I'm still showing the picture of a function with two inputs, of course,",
    "start": "633440",
    "end": "636885"
  },
  {
    "text": "because nudges in a 13,000 dimensional input space are a little hard to",
    "start": "636885",
    "end": "640378"
  },
  {
    "text": "wrap your mind around, but there is a nice non-spatial way to think about this.",
    "start": "640378",
    "end": "644260"
  },
  {
    "text": "Each component of the negative gradient tells us two things.",
    "start": "645080",
    "end": "648440"
  },
  {
    "text": "The sign, of course, tells us whether the corresponding",
    "start": "649060",
    "end": "651993"
  },
  {
    "text": "component of the input vector should be nudged up or down.",
    "start": "651993",
    "end": "655139"
  },
  {
    "text": "But importantly, the relative magnitudes of all these",
    "start": "655800",
    "end": "659165"
  },
  {
    "text": "components kind of tells you which changes matter more.",
    "start": "659165",
    "end": "662720"
  },
  {
    "text": "You see, in our network, an adjustment to one of the weights might have a much",
    "start": "665220",
    "end": "669129"
  },
  {
    "text": "greater impact on the cost function than the adjustment to some other weight.",
    "start": "669130",
    "end": "673040"
  },
  {
    "text": "Some of these connections just matter more for our training data.",
    "start": "674800",
    "end": "678200"
  },
  {
    "start": "678000",
    "end": "739000"
  },
  {
    "text": "So a way you can think about this gradient vector of our mind-warpingly massive",
    "start": "679320",
    "end": "683626"
  },
  {
    "text": "cost function is that it encodes the relative importance of each weight and bias,",
    "start": "683626",
    "end": "688095"
  },
  {
    "text": "that is, which of these changes is going to carry the most bang for your buck.",
    "start": "688095",
    "end": "692399"
  },
  {
    "text": "This really is just another way of thinking about direction.",
    "start": "693620",
    "end": "696640"
  },
  {
    "text": "To take a simpler example, if you have some function with two variables as an input,",
    "start": "697100",
    "end": "701785"
  },
  {
    "text": "and you compute that its gradient at some particular point comes out as 3,1,",
    "start": "701786",
    "end": "706082"
  },
  {
    "text": "then on the one hand you can interpret that as saying that when you're",
    "start": "706082",
    "end": "710043"
  },
  {
    "text": "standing at that input, moving along this direction increases the function most quickly,",
    "start": "710043",
    "end": "715008"
  },
  {
    "text": "that when you graph the function above the plane of input points,",
    "start": "715008",
    "end": "718690"
  },
  {
    "text": "that vector is what's giving you the straight uphill direction.",
    "start": "718690",
    "end": "722260"
  },
  {
    "text": "But another way to read that is to say that changes to this first variable have 3",
    "start": "722860",
    "end": "727355"
  },
  {
    "text": "times the importance as changes to the second variable,",
    "start": "727355",
    "end": "730462"
  },
  {
    "text": "that at least in the neighborhood of the relevant input,",
    "start": "730463",
    "end": "733626"
  },
  {
    "text": "nudging the x-value carries a lot more bang for your buck.",
    "start": "733626",
    "end": "736899"
  },
  {
    "start": "739000",
    "end": "781000"
  },
  {
    "text": "Let's zoom out and sum up where we are so far.",
    "start": "739880",
    "end": "742340"
  },
  {
    "text": "The network itself is this function with 784 inputs and 10 outputs,",
    "start": "742840",
    "end": "747147"
  },
  {
    "text": "defined in terms of all these weighted sums.",
    "start": "747147",
    "end": "750040"
  },
  {
    "text": "The cost function is a layer of complexity on top of that.",
    "start": "750640",
    "end": "753680"
  },
  {
    "text": "It takes the 13,000 weights and biases as inputs and spits out",
    "start": "753980",
    "end": "757850"
  },
  {
    "text": "a single measure of lousiness based on the training examples.",
    "start": "757850",
    "end": "761720"
  },
  {
    "text": "And the gradient of the cost function is one more layer of complexity still.",
    "start": "762440",
    "end": "766900"
  },
  {
    "text": "It tells us what nudges to all these weights and biases cause the",
    "start": "767360",
    "end": "770795"
  },
  {
    "text": "fastest change to the value of the cost function,",
    "start": "770796",
    "end": "773439"
  },
  {
    "text": "which you might interpret as saying which changes to which weights matter the most.",
    "start": "773439",
    "end": "777880"
  },
  {
    "start": "781000",
    "end": "997000"
  },
  {
    "text": "So, when you initialize the network with random weights and biases,",
    "start": "782560",
    "end": "786089"
  },
  {
    "text": "and adjust them many times based on this gradient descent process,",
    "start": "786089",
    "end": "789618"
  },
  {
    "text": "how well does it actually perform on images it's never seen before?",
    "start": "789618",
    "end": "793199"
  },
  {
    "text": "The one I've described here, with the two hidden layers of 16 neurons each,",
    "start": "794100",
    "end": "798961"
  },
  {
    "text": "chosen mostly for aesthetic reasons, is not bad,",
    "start": "798961",
    "end": "802136"
  },
  {
    "text": "classifying about 96% of the new images it sees correctly.",
    "start": "802136",
    "end": "805959"
  },
  {
    "text": "And honestly, if you look at some of the examples it messes up on,",
    "start": "806680",
    "end": "810164"
  },
  {
    "text": "you feel compelled to cut it a little slack.",
    "start": "810164",
    "end": "812540"
  },
  {
    "text": "Now if you play around with the hidden layer structure and make a couple tweaks,",
    "start": "816220",
    "end": "820324"
  },
  {
    "text": "you can get this up to 98%.",
    "start": "820324",
    "end": "821759"
  },
  {
    "text": "And that's pretty good!",
    "start": "821760",
    "end": "822720"
  },
  {
    "text": "It's not the best, you can certainly get better performance by getting more sophisticated",
    "start": "823020",
    "end": "827851"
  },
  {
    "text": "than this plain vanilla network, but given how daunting the initial task is,",
    "start": "827851",
    "end": "832030"
  },
  {
    "text": "I think there's something incredible about any network doing this well on images it's",
    "start": "832030",
    "end": "836698"
  },
  {
    "text": "never seen before, given that we never specifically told it what patterns to look for.",
    "start": "836698",
    "end": "841420"
  },
  {
    "text": "Originally, the way I motivated this structure was by describing a hope we might have,",
    "start": "842560",
    "end": "846881"
  },
  {
    "text": "that the second layer might pick up on little edges,",
    "start": "846881",
    "end": "849543"
  },
  {
    "text": "that the third layer would piece together those edges to recognize loops",
    "start": "849543",
    "end": "853211"
  },
  {
    "text": "and longer lines, and that those might be pieced together to recognize digits.",
    "start": "853211",
    "end": "857180"
  },
  {
    "text": "So is this what our network is actually doing?",
    "start": "857960",
    "end": "860400"
  },
  {
    "text": "Well, for this one at least, not at all.",
    "start": "861080",
    "end": "864400"
  },
  {
    "text": "Remember how last video we looked at how the weights of the connections from all",
    "start": "864820",
    "end": "868917"
  },
  {
    "text": "the neurons in the first layer to a given neuron in the second layer can be",
    "start": "868917",
    "end": "872809"
  },
  {
    "text": "visualized as a given pixel pattern that the second layer neuron is picking up on?",
    "start": "872809",
    "end": "877060"
  },
  {
    "text": "Well, when we actually do that for the weights associated with these transitions,",
    "start": "877780",
    "end": "882622"
  },
  {
    "text": "from the first layer to the next, instead of picking up on isolated little edges here",
    "start": "882622",
    "end": "887762"
  },
  {
    "text": "and there, they look, well, almost random, just with some very loose patterns in the",
    "start": "887762",
    "end": "892843"
  },
  {
    "text": "middle there.",
    "start": "892843",
    "end": "893680"
  },
  {
    "text": "It would seem that in the unfathomably large 13,000 dimensional space",
    "start": "893760",
    "end": "897616"
  },
  {
    "text": "of possible weights and biases, our network found itself a happy",
    "start": "897616",
    "end": "901248"
  },
  {
    "text": "little local minimum that, despite successfully classifying most images,",
    "start": "901248",
    "end": "905328"
  },
  {
    "text": "doesn't exactly pick up on the patterns we might have hoped for.",
    "start": "905328",
    "end": "908960"
  },
  {
    "text": "And to really drive this point home, watch what happens when you input a random image.",
    "start": "909780",
    "end": "913820"
  },
  {
    "text": "If the system was smart, you might expect it to feel uncertain,",
    "start": "914320",
    "end": "918326"
  },
  {
    "text": "maybe not really activating any of those 10 output neurons or activating them",
    "start": "918326",
    "end": "923286"
  },
  {
    "text": "all evenly, but instead it confidently gives you some nonsense answer,",
    "start": "923286",
    "end": "927801"
  },
  {
    "text": "as if it feels as sure that this random noise is a 5 as it does that an actual",
    "start": "927801",
    "end": "932825"
  },
  {
    "text": "image of a 5 is a 5.",
    "start": "932825",
    "end": "934160"
  },
  {
    "text": "Phrased differently, even if this network can recognize digits pretty well,",
    "start": "934540",
    "end": "938818"
  },
  {
    "text": "it has no idea how to draw them.",
    "start": "938818",
    "end": "940700"
  },
  {
    "text": "A lot of this is because it's such a tightly constrained training setup.",
    "start": "941420",
    "end": "945240"
  },
  {
    "text": "I mean, put yourself in the network's shoes here.",
    "start": "945880",
    "end": "947740"
  },
  {
    "text": "From its point of view, the entire universe consists of nothing but clearly",
    "start": "948140",
    "end": "952378"
  },
  {
    "text": "defined unmoving digits centered in a tiny grid,",
    "start": "952378",
    "end": "955147"
  },
  {
    "text": "and its cost function never gave it any incentive to be anything but utterly",
    "start": "955147",
    "end": "959498"
  },
  {
    "text": "confident in its decisions.",
    "start": "959498",
    "end": "961079"
  },
  {
    "text": "So with this as the image of what those second layer neurons are really doing,",
    "start": "962120",
    "end": "965373"
  },
  {
    "text": "you might wonder why I would introduce this network with the",
    "start": "965373",
    "end": "967918"
  },
  {
    "text": "motivation of picking up on edges and patterns.",
    "start": "967918",
    "end": "969920"
  },
  {
    "text": "I mean, that's just not at all what it ends up doing.",
    "start": "969920",
    "end": "972300"
  },
  {
    "text": "Well, this is not meant to be our end goal, but instead a starting point.",
    "start": "973380",
    "end": "977180"
  },
  {
    "text": "Frankly, this is old technology, the kind researched in the 80s and 90s,",
    "start": "977640",
    "end": "981417"
  },
  {
    "text": "and you do need to understand it before you can understand more detailed modern",
    "start": "981417",
    "end": "985613"
  },
  {
    "text": "variants, and it clearly is capable of solving some interesting problems,",
    "start": "985613",
    "end": "989495"
  },
  {
    "text": "but the more you dig into what those hidden layers are really doing,",
    "start": "989495",
    "end": "993114"
  },
  {
    "text": "the less intelligent it seems.",
    "start": "993114",
    "end": "994740"
  },
  {
    "start": "997000",
    "end": "1058000"
  },
  {
    "text": "Shifting the focus for a moment from how networks learn to how you learn,",
    "start": "998480",
    "end": "1002337"
  },
  {
    "text": "that'll only happen if you engage actively with the material here somehow.",
    "start": "1002337",
    "end": "1006300"
  },
  {
    "text": "One pretty simple thing I want you to do is just pause right now and think deeply",
    "start": "1007060",
    "end": "1011705"
  },
  {
    "text": "for a moment about what changes you might make to this system and how it perceives",
    "start": "1011705",
    "end": "1016464"
  },
  {
    "text": "images if you wanted it to better pick up on things like edges and patterns.",
    "start": "1016464",
    "end": "1020880"
  },
  {
    "text": "But better than that, to actually engage with the material,",
    "start": "1021480",
    "end": "1024601"
  },
  {
    "text": "I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
    "start": "1024602",
    "end": "1029100"
  },
  {
    "text": "In it, you can find the code and the data to download and play with for this exact",
    "start": "1029680",
    "end": "1034020"
  },
  {
    "text": "example, and the book will walk you through step by step what that code is doing.",
    "start": "1034020",
    "end": "1038360"
  },
  {
    "text": "What's awesome is that this book is free and publicly available,",
    "start": "1039300",
    "end": "1042447"
  },
  {
    "text": "so if you do get something out of it, consider joining me in making a donation towards",
    "start": "1042447",
    "end": "1046725"
  },
  {
    "text": "Nielsen's efforts.",
    "start": "1046726",
    "end": "1047659"
  },
  {
    "text": "I've also linked a couple other resources I like a lot in the description,",
    "start": "1047660",
    "end": "1051625"
  },
  {
    "text": "including the phenomenal and beautiful blog post by Chris Ola and the articles in",
    "start": "1051625",
    "end": "1056018"
  },
  {
    "text": "Distill.",
    "start": "1056018",
    "end": "1056500"
  },
  {
    "start": "1058000",
    "end": "1198000"
  },
  {
    "text": "To close things off here for the last few minutes,",
    "start": "1058280",
    "end": "1060520"
  },
  {
    "text": "I want to jump back into a snippet of the interview I had with Leisha Lee.",
    "start": "1060520",
    "end": "1063880"
  },
  {
    "text": "You might remember her from the last video, she did her PhD work in deep learning.",
    "start": "1064300",
    "end": "1067720"
  },
  {
    "text": "In this little snippet she talks about two recent papers that really dig into",
    "start": "1068300",
    "end": "1072016"
  },
  {
    "text": "how some of the more modern image recognition networks are actually learning.",
    "start": "1072016",
    "end": "1075780"
  },
  {
    "text": "Just to set up where we were in the conversation,",
    "start": "1076120",
    "end": "1078462"
  },
  {
    "text": "the first paper took one of these particularly deep neural networks that's really good",
    "start": "1078462",
    "end": "1082621"
  },
  {
    "text": "at image recognition, and instead of training it on a properly labeled dataset,",
    "start": "1082621",
    "end": "1086445"
  },
  {
    "text": "shuffled all the labels around before training.",
    "start": "1086445",
    "end": "1088740"
  },
  {
    "text": "Obviously the testing accuracy here was going to be no better than random,",
    "start": "1089480",
    "end": "1093315"
  },
  {
    "text": "since everything's just randomly labeled. But it was still able to achieve",
    "start": "1093315",
    "end": "1097200"
  },
  {
    "text": "the same training accuracy as you would on a properly labeled dataset.",
    "start": "1097201",
    "end": "1100880"
  },
  {
    "text": "Basically, the millions of weights for this particular network were",
    "start": "1101600",
    "end": "1105259"
  },
  {
    "text": "enough for it to just memorize the random data,",
    "start": "1105259",
    "end": "1107880"
  },
  {
    "text": "which raises the question for whether minimizing this cost function",
    "start": "1107880",
    "end": "1111594"
  },
  {
    "text": "actually corresponds to any sort of structure in the image, or is it just memorization?",
    "start": "1111594",
    "end": "1116400"
  },
  {
    "text": "...to memorize the entire dataset of what the correct classification is.",
    "start": "1131440",
    "end": "1134397"
  },
  {
    "text": "And so half a year later at ICML this year, there was not exactly rebuttal paper,",
    "start": "1134397",
    "end": "1137765"
  },
  {
    "text": "but paper that addressed some aspects of like, hey,",
    "start": "1137765",
    "end": "1139901"
  },
  {
    "text": "actually these networks are doing something a little bit smarter than that.",
    "start": "1139901",
    "end": "1143022"
  },
  {
    "text": "If you look at that accuracy curve if you were just training on a random data set",
    "start": "1143022",
    "end": "1146431"
  },
  {
    "text": "that curve went down very slowly, almost in a linear fashion.",
    "start": "1146431",
    "end": "1148977"
  },
  {
    "text": "So youâ€™re really struggling to find that local minimum of the right weights.",
    "start": "1148977",
    "end": "1152140"
  },
  {
    "text": "Whereas if you're actually training on a structured dataset,",
    "start": "1152240",
    "end": "1155765"
  },
  {
    "text": "one that has the right labels, you fiddle around a little bit in the beginning,",
    "start": "1155765",
    "end": "1160465"
  },
  {
    "text": "but then you kind of dropped very fast to get to that accuracy level,",
    "start": "1160465",
    "end": "1164577"
  },
  {
    "text": "and so in some sense it was easier to find that local maxima.",
    "start": "1164577",
    "end": "1168220"
  },
  {
    "text": "And so what was also interesting about that is it brings into light another paper from",
    "start": "1168540",
    "end": "1173556"
  },
  {
    "text": "actually a couple of years ago, which has a lot more simplifications about the network",
    "start": "1173556",
    "end": "1178630"
  },
  {
    "text": "layers, but one of the results was saying how if you look at the optimization landscape,",
    "start": "1178630",
    "end": "1183821"
  },
  {
    "text": "the local minima that these networks tend to learn are actually of equal quality,",
    "start": "1183821",
    "end": "1188604"
  },
  {
    "text": "so in some sense if your dataset is structured,",
    "start": "1188604",
    "end": "1191404"
  },
  {
    "text": "you should be able to find that much more easily.",
    "start": "1191404",
    "end": "1194320"
  },
  {
    "start": "1198000",
    "end": "1233000"
  },
  {
    "text": "My thanks, as always, to those of you supporting on Patreon.",
    "start": "1198160",
    "end": "1201180"
  },
  {
    "text": "I've said before just what a game changer Patreon is,",
    "start": "1201520",
    "end": "1204019"
  },
  {
    "text": "but these videos really would not be possible without you.",
    "start": "1204019",
    "end": "1206800"
  },
  {
    "text": "I also want to give a special thanks to the VC firm Amplify Partners",
    "start": "1207460",
    "end": "1210120"
  },
  {
    "text": "and their support of these initial videos in the series. Thank you.",
    "start": "1210120",
    "end": "1212780"
  }
]