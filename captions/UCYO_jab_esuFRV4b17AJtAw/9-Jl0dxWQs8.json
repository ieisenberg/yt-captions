[
  {
    "start": "0",
    "end": "135000"
  },
  {
    "text": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank,",
    "start": "0",
    "end": "5038"
  },
  {
    "text": "and you have it predict what comes next, and it correctly predicts basketball,",
    "start": "5038",
    "end": "9559"
  },
  {
    "text": "this would suggest that somewhere, inside its hundreds of billions of parameters,",
    "start": "9560",
    "end": "14255"
  },
  {
    "text": "it's baked in knowledge about a specific person and his specific sport.",
    "start": "14255",
    "end": "18320"
  },
  {
    "text": "And I think in general, anyone who's played around with one of these",
    "start": "18940",
    "end": "22146"
  },
  {
    "text": "models has the clear sense that it's memorized tons and tons of facts.",
    "start": "22146",
    "end": "25400"
  },
  {
    "text": "So a reasonable question you could ask is, how exactly does that work?",
    "start": "25700",
    "end": "29160"
  },
  {
    "text": "And where do those facts live?",
    "start": "29160",
    "end": "31039"
  },
  {
    "text": "Last December, a few researchers from Google DeepMind posted about work on this question,",
    "start": "35720",
    "end": "40385"
  },
  {
    "text": "and they were using this specific example of matching athletes to their sports.",
    "start": "40385",
    "end": "44480"
  },
  {
    "text": "And although a full mechanistic understanding of how facts are stored remains unsolved,",
    "start": "44900",
    "end": "49824"
  },
  {
    "text": "they had some interesting partial results, including the very general high-level",
    "start": "49824",
    "end": "54357"
  },
  {
    "text": "conclusion that the facts seem to live inside a specific part of these networks,",
    "start": "54357",
    "end": "58890"
  },
  {
    "text": "known fancifully as the multi-layer perceptrons, or MLPs for short.",
    "start": "58890",
    "end": "62640"
  },
  {
    "text": "In the last couple of chapters, you and I have been digging into",
    "start": "63120",
    "end": "66262"
  },
  {
    "text": "the details behind transformers, the architecture underlying large language models,",
    "start": "66262",
    "end": "70324"
  },
  {
    "text": "and also underlying a lot of other modern AI.",
    "start": "70324",
    "end": "72500"
  },
  {
    "text": "In the most recent chapter, we were focusing on a piece called Attention.",
    "start": "73060",
    "end": "76200"
  },
  {
    "text": "And the next step for you and me is to dig into the details of what happens inside",
    "start": "76840",
    "end": "80964"
  },
  {
    "text": "these multi-layer perceptrons, which make up the other big portion of the network.",
    "start": "80964",
    "end": "85039"
  },
  {
    "text": "The computation here is actually relatively simple,",
    "start": "85680",
    "end": "88074"
  },
  {
    "text": "especially when you compare it to attention.",
    "start": "88074",
    "end": "90100"
  },
  {
    "text": "It boils down essentially to a pair of matrix",
    "start": "90560",
    "end": "92656"
  },
  {
    "text": "multiplications with a simple something in between.",
    "start": "92656",
    "end": "94979"
  },
  {
    "text": "However, interpreting what these computations are doing is exceedingly challenging.",
    "start": "95720",
    "end": "100460"
  },
  {
    "text": "Our main goal here is to step through the computations and make them memorable,",
    "start": "101560",
    "end": "105666"
  },
  {
    "text": "but I'd like to do it in the context of showing a specific example of how",
    "start": "105666",
    "end": "109464"
  },
  {
    "text": "one of these blocks could, at least in principle, store a concrete fact.",
    "start": "109464",
    "end": "113159"
  },
  {
    "text": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
    "start": "113580",
    "end": "117080"
  },
  {
    "text": "I should mention the layout here is inspired by a conversation",
    "start": "118080",
    "end": "120768"
  },
  {
    "text": "I had with one of those DeepMind researchers, Neil Nanda.",
    "start": "120768",
    "end": "123200"
  },
  {
    "text": "For the most part, I will assume that you've either watched the last two chapters,",
    "start": "124060",
    "end": "128037"
  },
  {
    "text": "or otherwise you have a basic sense for what a transformer is,",
    "start": "128038",
    "end": "131056"
  },
  {
    "text": "but refreshers never hurt, so here's the quick reminder of the overall flow.",
    "start": "131057",
    "end": "134700"
  },
  {
    "start": "135000",
    "end": "279000"
  },
  {
    "text": "You and I have been studying a model that's trained",
    "start": "135340",
    "end": "138246"
  },
  {
    "text": "to take in a piece of text and predict what comes next.",
    "start": "138246",
    "end": "141320"
  },
  {
    "text": "That input text is first broken into a bunch of tokens,",
    "start": "141720",
    "end": "144965"
  },
  {
    "text": "which means little chunks that are typically words or little pieces of words,",
    "start": "144965",
    "end": "149485"
  },
  {
    "text": "and each token is associated with a high-dimensional vector,",
    "start": "149485",
    "end": "153020"
  },
  {
    "text": "which is to say a long list of numbers.",
    "start": "153020",
    "end": "155280"
  },
  {
    "text": "This sequence of vectors then repeatedly passes through two kinds of operation,",
    "start": "155840",
    "end": "160318"
  },
  {
    "text": "attention, which allows the vectors to pass information between one another,",
    "start": "160318",
    "end": "164629"
  },
  {
    "text": "and then the multilayer perceptrons, the thing that we're gonna dig into today,",
    "start": "164629",
    "end": "169108"
  },
  {
    "text": "and also there's a certain normalization step in between.",
    "start": "169108",
    "end": "172300"
  },
  {
    "text": "After the sequence of vectors has flowed through many,",
    "start": "173300",
    "end": "176431"
  },
  {
    "text": "many different iterations of both of these blocks, by the end,",
    "start": "176431",
    "end": "180019"
  },
  {
    "text": "the hope is that each vector has soaked up enough information, both from the context,",
    "start": "180019",
    "end": "184916"
  },
  {
    "text": "all of the other words in the input, and also from the general knowledge that",
    "start": "184916",
    "end": "189357"
  },
  {
    "text": "was baked into the model weights through training,",
    "start": "189357",
    "end": "192261"
  },
  {
    "text": "that it can be used to make a prediction of what token comes next.",
    "start": "192261",
    "end": "196020"
  },
  {
    "text": "One of the key ideas that I want you to have in your mind is that all of",
    "start": "196860",
    "end": "200682"
  },
  {
    "text": "these vectors live in a very, very high-dimensional space,",
    "start": "200682",
    "end": "203772"
  },
  {
    "text": "and when you think about that space, different directions can encode different",
    "start": "203772",
    "end": "207909"
  },
  {
    "text": "kinds of meaning.",
    "start": "207909",
    "end": "208800"
  },
  {
    "text": "So a very classic example that I like to refer back to is how if you look",
    "start": "210120",
    "end": "214096"
  },
  {
    "text": "at the embedding of woman and subtract the embedding of man,",
    "start": "214096",
    "end": "217374"
  },
  {
    "text": "and you take that little step and you add it to another masculine noun,",
    "start": "217374",
    "end": "221241"
  },
  {
    "text": "something like uncle, you land somewhere very,",
    "start": "221242",
    "end": "223768"
  },
  {
    "text": "very close to the corresponding feminine noun.",
    "start": "223768",
    "end": "226240"
  },
  {
    "text": "In this sense, this particular direction encodes gender information.",
    "start": "226440",
    "end": "230880"
  },
  {
    "text": "The idea is that many other distinct directions in this super high-dimensional",
    "start": "231640",
    "end": "235614"
  },
  {
    "text": "space could correspond to other features that the model might want to represent.",
    "start": "235614",
    "end": "239640"
  },
  {
    "text": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
    "start": "241400",
    "end": "246180"
  },
  {
    "text": "As they flow through the network, they imbibe a much richer meaning based",
    "start": "246680",
    "end": "250987"
  },
  {
    "text": "on all the context around them, and also based on the model's knowledge.",
    "start": "250988",
    "end": "255180"
  },
  {
    "text": "Ultimately, each one needs to encode something far,",
    "start": "255880",
    "end": "258506"
  },
  {
    "text": "far beyond the meaning of a single word, since it needs to be sufficient to",
    "start": "258506",
    "end": "262345"
  },
  {
    "text": "predict what will come next.",
    "start": "262345",
    "end": "263760"
  },
  {
    "text": "We've already seen how attention blocks let you incorporate context,",
    "start": "264560",
    "end": "268480"
  },
  {
    "text": "but a majority of the model parameters actually live inside the MLP blocks,",
    "start": "268480",
    "end": "272798"
  },
  {
    "text": "and one thought for what they might be doing is that they offer extra capacity",
    "start": "272798",
    "end": "277287"
  },
  {
    "text": "to store facts.",
    "start": "277287",
    "end": "278139"
  },
  {
    "text": "Like I said, the lesson here is gonna center on the concrete toy example",
    "start": "278720",
    "end": "282345"
  },
  {
    "start": "279000",
    "end": "367000"
  },
  {
    "text": "of how exactly it could store the fact that Michael Jordan plays basketball.",
    "start": "282345",
    "end": "286120"
  },
  {
    "text": "Now, this toy example is gonna require that you and I make",
    "start": "287120",
    "end": "289530"
  },
  {
    "text": "a couple of assumptions about that high-dimensional space.",
    "start": "289530",
    "end": "291900"
  },
  {
    "text": "First, we'll suppose that one of the directions represents the idea of a first name",
    "start": "292360",
    "end": "296991"
  },
  {
    "text": "Michael, and then another nearly perpendicular direction represents the idea of the",
    "start": "296991",
    "end": "301623"
  },
  {
    "text": "last name Jordan, and then yet a third direction will represent the idea of basketball.",
    "start": "301623",
    "end": "306419"
  },
  {
    "text": "So specifically, what I mean by this is if you look in the network and",
    "start": "307400",
    "end": "311121"
  },
  {
    "text": "you pluck out one of the vectors being processed,",
    "start": "311121",
    "end": "313742"
  },
  {
    "text": "if its dot product with this first name Michael direction is one,",
    "start": "313742",
    "end": "317202"
  },
  {
    "text": "that's what it would mean for the vector to be encoding the idea of a",
    "start": "317202",
    "end": "320872"
  },
  {
    "text": "person with that first name.",
    "start": "320872",
    "end": "322340"
  },
  {
    "text": "Otherwise, that dot product would be zero or negative,",
    "start": "323800",
    "end": "326142"
  },
  {
    "text": "meaning the vector doesn't really align with that direction.",
    "start": "326143",
    "end": "328700"
  },
  {
    "text": "And for simplicity, let's completely ignore the very reasonable",
    "start": "329420",
    "end": "332217"
  },
  {
    "text": "question of what it might mean if that dot product was bigger than one.",
    "start": "332217",
    "end": "335320"
  },
  {
    "text": "Similarly, its dot product with these other directions would",
    "start": "336200",
    "end": "339831"
  },
  {
    "text": "tell you whether it represents the last name Jordan or basketball.",
    "start": "339831",
    "end": "343759"
  },
  {
    "text": "So let's say a vector is meant to represent the full name, Michael Jordan,",
    "start": "344740",
    "end": "348791"
  },
  {
    "text": "then its dot product with both of these directions would have to be one.",
    "start": "348791",
    "end": "352680"
  },
  {
    "text": "Since the text Michael Jordan spans two different tokens,",
    "start": "353480",
    "end": "356658"
  },
  {
    "text": "this would also mean we have to assume that an earlier attention block has successfully",
    "start": "356658",
    "end": "361479"
  },
  {
    "text": "passed information to the second of these two vectors so as to ensure that it can",
    "start": "361480",
    "end": "365973"
  },
  {
    "text": "encode both names.",
    "start": "365973",
    "end": "366960"
  },
  {
    "start": "367000",
    "end": "938000"
  },
  {
    "text": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
    "start": "367940",
    "end": "371480"
  },
  {
    "text": "What happens inside a multilayer perceptron?",
    "start": "371880",
    "end": "374980"
  },
  {
    "text": "You might think of this sequence of vectors flowing into the block, and remember,",
    "start": "377100",
    "end": "381366"
  },
  {
    "text": "each vector was originally associated with one of the tokens from the input text.",
    "start": "381366",
    "end": "385580"
  },
  {
    "text": "What's gonna happen is that each individual vector from that sequence",
    "start": "386080",
    "end": "389442"
  },
  {
    "text": "goes through a short series of operations, we'll unpack them in just a moment,",
    "start": "389442",
    "end": "393237"
  },
  {
    "text": "and at the end, we'll get another vector with the same dimension.",
    "start": "393237",
    "end": "396360"
  },
  {
    "text": "That other vector is gonna get added to the original one that flowed in,",
    "start": "396880",
    "end": "400999"
  },
  {
    "text": "and that sum is the result flowing out.",
    "start": "400999",
    "end": "403199"
  },
  {
    "text": "This sequence of operations is something you apply to every vector in the sequence,",
    "start": "403720",
    "end": "407946"
  },
  {
    "text": "associated with every token in the input, and it all happens in parallel.",
    "start": "407946",
    "end": "411620"
  },
  {
    "text": "In particular, the vectors don't talk to each other in this step,",
    "start": "412100",
    "end": "414605"
  },
  {
    "text": "they're all kind of doing their own thing.",
    "start": "414605",
    "end": "416200"
  },
  {
    "text": "And for you and me, that actually makes it a lot simpler,",
    "start": "416720",
    "end": "419349"
  },
  {
    "text": "because it means if we understand what happens to just one of the",
    "start": "419349",
    "end": "422342"
  },
  {
    "text": "vectors through this block, we effectively understand what happens to all of them.",
    "start": "422342",
    "end": "426060"
  },
  {
    "text": "When I say this block is gonna encode the fact that Michael Jordan plays basketball,",
    "start": "427100",
    "end": "431380"
  },
  {
    "text": "what I mean is that if a vector flows in that encodes first name Michael and last",
    "start": "431380",
    "end": "435509"
  },
  {
    "text": "name Jordan, then this sequence of computations will produce something that includes",
    "start": "435509",
    "end": "439789"
  },
  {
    "text": "that direction basketball, which is what will add on to the vector in that position.",
    "start": "439789",
    "end": "444020"
  },
  {
    "text": "The first step of this process looks like multiplying that vector by a very big matrix.",
    "start": "445600",
    "end": "449700"
  },
  {
    "text": "No surprises there, this is deep learning.",
    "start": "450040",
    "end": "451980"
  },
  {
    "text": "And this matrix, like all of the other ones we've seen,",
    "start": "452680",
    "end": "455235"
  },
  {
    "text": "is filled with model parameters that are learned from data,",
    "start": "455235",
    "end": "457973"
  },
  {
    "text": "which you might think of as a bunch of knobs and dials that get tweaked and",
    "start": "457973",
    "end": "461440"
  },
  {
    "text": "tuned to determine what the model behavior is.",
    "start": "461441",
    "end": "463540"
  },
  {
    "text": "Now, one nice way to think about matrix multiplication is to imagine each row of",
    "start": "464500",
    "end": "468678"
  },
  {
    "text": "that matrix as being its own vector, and taking a bunch of dot products between",
    "start": "468678",
    "end": "472804"
  },
  {
    "text": "those rows and the vector being processed, which I'll label as E for embedding.",
    "start": "472804",
    "end": "476879"
  },
  {
    "text": "For example, suppose that very first row happened to equal",
    "start": "477280",
    "end": "480576"
  },
  {
    "text": "this first name Michael direction that we're presuming exists.",
    "start": "480576",
    "end": "484039"
  },
  {
    "text": "That would mean that the first component in this output, this dot product right here,",
    "start": "484320",
    "end": "489411"
  },
  {
    "text": "would be one if that vector encodes the first name Michael,",
    "start": "489411",
    "end": "492964"
  },
  {
    "text": "and zero or negative otherwise.",
    "start": "492964",
    "end": "494800"
  },
  {
    "text": "Even more fun, take a moment to think about what it would mean if that",
    "start": "495880",
    "end": "499505"
  },
  {
    "text": "first row was this first name Michael plus last name Jordan direction.",
    "start": "499505",
    "end": "503080"
  },
  {
    "text": "And for simplicity, let me go ahead and write that down as M plus J.",
    "start": "503700",
    "end": "507420"
  },
  {
    "text": "Then, taking a dot product with this embedding E,",
    "start": "508080",
    "end": "510931"
  },
  {
    "text": "things distribute really nicely, so it looks like M dot E plus J dot E.",
    "start": "510931",
    "end": "514979"
  },
  {
    "text": "And notice how that means the ultimate value would be two if the vector encodes the",
    "start": "514980",
    "end": "519781"
  },
  {
    "text": "full name Michael Jordan, and otherwise it would be one or something smaller than one.",
    "start": "519782",
    "end": "524700"
  },
  {
    "text": "And that's just one row in this matrix.",
    "start": "525340",
    "end": "527260"
  },
  {
    "text": "You might think of all of the other rows as in parallel asking some other kinds of",
    "start": "527600",
    "end": "531871"
  },
  {
    "text": "questions, probing at some other sorts of features of the vector being processed.",
    "start": "531871",
    "end": "536040"
  },
  {
    "text": "Very often this step also involves adding another vector to the output,",
    "start": "536700",
    "end": "539916"
  },
  {
    "text": "which is full of model parameters learned from data.",
    "start": "539916",
    "end": "542240"
  },
  {
    "text": "This other vector is known as the bias.",
    "start": "542240",
    "end": "544560"
  },
  {
    "text": "For our example, I want you to imagine that the value of this",
    "start": "545180",
    "end": "548567"
  },
  {
    "text": "bias in that very first component is negative one,",
    "start": "548567",
    "end": "551353"
  },
  {
    "text": "meaning our final output looks like that relevant dot product, but minus one.",
    "start": "551353",
    "end": "555560"
  },
  {
    "text": "You might very reasonably ask why I would want you to assume that the",
    "start": "556120",
    "end": "559990"
  },
  {
    "text": "model has learned this, and in a moment you'll see why it's very clean",
    "start": "559991",
    "end": "563918"
  },
  {
    "text": "and nice if we have a value here which is positive if and only if a vector",
    "start": "563918",
    "end": "568067"
  },
  {
    "text": "encodes the full name Michael Jordan, and otherwise it's zero or negative.",
    "start": "568067",
    "end": "572160"
  },
  {
    "text": "The total number of rows in this matrix, which is something",
    "start": "573040",
    "end": "576268"
  },
  {
    "text": "like the number of questions being asked, in the case of GPT-3,",
    "start": "576268",
    "end": "579712"
  },
  {
    "text": "whose numbers we've been following, is just under 50,000.",
    "start": "579712",
    "end": "582779"
  },
  {
    "text": "In fact, it's exactly four times the number of dimensions in this embedding space.",
    "start": "583100",
    "end": "586639"
  },
  {
    "text": "That's a design choice.",
    "start": "586920",
    "end": "587899"
  },
  {
    "text": "You could make it more, you could make it less,",
    "start": "587940",
    "end": "589816"
  },
  {
    "text": "but having a clean multiple tends to be friendly for hardware.",
    "start": "589816",
    "end": "592240"
  },
  {
    "text": "Since this matrix full of weights maps us into a higher dimensional space,",
    "start": "592740",
    "end": "596945"
  },
  {
    "text": "I'm gonna give it the shorthand W up.",
    "start": "596945",
    "end": "599020"
  },
  {
    "text": "I'll continue labeling the vector we're processing as E,",
    "start": "599020",
    "end": "602334"
  },
  {
    "text": "and let's label this bias vector as B up and put that all back down in the diagram.",
    "start": "602334",
    "end": "607160"
  },
  {
    "text": "At this point, a problem is that this operation is purely linear,",
    "start": "609180",
    "end": "612956"
  },
  {
    "text": "but language is a very non-linear process.",
    "start": "612956",
    "end": "615360"
  },
  {
    "text": "If the entry that we're measuring is high for Michael plus Jordan,",
    "start": "615880",
    "end": "619778"
  },
  {
    "text": "it would also necessarily be somewhat triggered by Michael plus Phelps",
    "start": "619778",
    "end": "623910"
  },
  {
    "text": "and also Alexis plus Jordan, despite those being unrelated conceptually.",
    "start": "623910",
    "end": "628100"
  },
  {
    "text": "What you really want is a simple yes or no for the full name.",
    "start": "628540",
    "end": "632000"
  },
  {
    "text": "So the next step is to pass this large intermediate",
    "start": "632900",
    "end": "635443"
  },
  {
    "text": "vector through a very simple non-linear function.",
    "start": "635443",
    "end": "637839"
  },
  {
    "text": "A common choice is one that takes all of the negative values and",
    "start": "638360",
    "end": "641803"
  },
  {
    "text": "maps them to zero and leaves all of the positive values unchanged.",
    "start": "641803",
    "end": "645300"
  },
  {
    "text": "And continuing with the deep learning tradition of overly fancy names,",
    "start": "646440",
    "end": "650744"
  },
  {
    "text": "this very simple function is often called the rectified linear unit, or ReLU for short.",
    "start": "650744",
    "end": "656020"
  },
  {
    "text": "Here's what the graph looks like.",
    "start": "656020",
    "end": "657880"
  },
  {
    "text": "So taking our imagined example where this first entry of the intermediate vector is one,",
    "start": "658300",
    "end": "663372"
  },
  {
    "text": "if and only if the full name is Michael Jordan and zero or negative otherwise,",
    "start": "663372",
    "end": "667874"
  },
  {
    "text": "after you pass it through the ReLU, you end up with a very clean value where",
    "start": "667874",
    "end": "672263"
  },
  {
    "text": "all of the zero and negative values just get clipped to zero.",
    "start": "672263",
    "end": "675740"
  },
  {
    "text": "So this output would be one for the full name Michael Jordan and zero otherwise.",
    "start": "676100",
    "end": "679779"
  },
  {
    "text": "In other words, it very directly mimics the behavior of an AND gate.",
    "start": "680560",
    "end": "684120"
  },
  {
    "text": "Often models will use a slightly modified function that's called the GELU,",
    "start": "685660",
    "end": "689251"
  },
  {
    "text": "which has the same basic shape, it's just a bit smoother.",
    "start": "689252",
    "end": "692020"
  },
  {
    "text": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
    "start": "692500",
    "end": "695720"
  },
  {
    "text": "Also, when you hear people refer to the neurons of a transformer,",
    "start": "696740",
    "end": "700146"
  },
  {
    "text": "they're talking about these values right here.",
    "start": "700146",
    "end": "702520"
  },
  {
    "text": "Whenever you see that common neural network picture with a layer of dots and a",
    "start": "702900",
    "end": "707390"
  },
  {
    "text": "bunch of lines connecting to the previous layer, which we had earlier in this series,",
    "start": "707390",
    "end": "712278"
  },
  {
    "text": "that's typically meant to convey this combination of a linear step,",
    "start": "712278",
    "end": "716144"
  },
  {
    "text": "a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
    "start": "716144",
    "end": "721260"
  },
  {
    "text": "You would say that this neuron is active whenever this value",
    "start": "722500",
    "end": "725818"
  },
  {
    "text": "is positive and that it's inactive if that value is zero.",
    "start": "725818",
    "end": "728920"
  },
  {
    "text": "The next step looks very similar to the first one.",
    "start": "730120",
    "end": "732380"
  },
  {
    "text": "You multiply by a very large matrix and you add on a certain bias term.",
    "start": "732560",
    "end": "736580"
  },
  {
    "text": "In this case, the number of dimensions in the output is back down to the size of",
    "start": "736980",
    "end": "741147"
  },
  {
    "text": "that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
    "start": "741147",
    "end": "745519"
  },
  {
    "text": "And this time, instead of thinking of things row by row,",
    "start": "746220",
    "end": "748907"
  },
  {
    "text": "it's actually nicer to think of it column by column.",
    "start": "748907",
    "end": "751360"
  },
  {
    "text": "You see, another way that you can hold matrix multiplication in your head is to",
    "start": "751860",
    "end": "756252"
  },
  {
    "text": "imagine taking each column of the matrix and multiplying it by the corresponding",
    "start": "756252",
    "end": "760698"
  },
  {
    "text": "term in the vector that it's processing and adding together all of those rescaled columns.",
    "start": "760698",
    "end": "765640"
  },
  {
    "text": "The reason it's nicer to think about this way is because here the columns have the same",
    "start": "766840",
    "end": "771361"
  },
  {
    "text": "dimension as the embedding space, so we can think of them as directions in that space.",
    "start": "771361",
    "end": "775780"
  },
  {
    "text": "For instance, we will imagine that the model has learned to make that",
    "start": "776140",
    "end": "779685"
  },
  {
    "text": "first column into this basketball direction that we suppose exists.",
    "start": "779685",
    "end": "783080"
  },
  {
    "text": "What that would mean is that when the relevant neuron in that first position is active,",
    "start": "784180",
    "end": "788450"
  },
  {
    "text": "we'll be adding this column to the final result.",
    "start": "788450",
    "end": "790780"
  },
  {
    "text": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
    "start": "791140",
    "end": "795780"
  },
  {
    "text": "And it doesn't just have to be basketball.",
    "start": "796500",
    "end": "798060"
  },
  {
    "text": "The model could also bake into this column and many other features that",
    "start": "798220",
    "end": "801638"
  },
  {
    "text": "it wants to associate with something that has the full name Michael Jordan.",
    "start": "801638",
    "end": "805200"
  },
  {
    "text": "And at the same time, all of the other columns in this matrix are telling you",
    "start": "806980",
    "end": "811850"
  },
  {
    "text": "what will be added to the final result if the corresponding neuron is active.",
    "start": "811851",
    "end": "816660"
  },
  {
    "text": "And if you have a bias in this case, it's something that you're",
    "start": "817360",
    "end": "820454"
  },
  {
    "text": "just adding every single time, regardless of the neuron values.",
    "start": "820454",
    "end": "823500"
  },
  {
    "text": "You might wonder what's that doing.",
    "start": "824060",
    "end": "825279"
  },
  {
    "text": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
    "start": "825540",
    "end": "829320"
  },
  {
    "text": "Maybe there's some bookkeeping that the network needs to do,",
    "start": "829320",
    "end": "832287"
  },
  {
    "text": "but you can feel free to ignore it for now.",
    "start": "832287",
    "end": "834380"
  },
  {
    "text": "Making our notation a little more compact again,",
    "start": "834860",
    "end": "837738"
  },
  {
    "text": "I'll call this big matrix W down and similarly call that bias vector B down and",
    "start": "837738",
    "end": "842438"
  },
  {
    "text": "put that back into our diagram.",
    "start": "842438",
    "end": "844260"
  },
  {
    "text": "Like I previewed earlier, what you do with this final result is add it to the vector",
    "start": "844740",
    "end": "849118"
  },
  {
    "text": "that flowed into the block at that position and that gets you this final result.",
    "start": "849118",
    "end": "853240"
  },
  {
    "text": "So for example, if the vector flowing in encoded both first name Michael and last name",
    "start": "853820",
    "end": "859060"
  },
  {
    "text": "Jordan, then because this sequence of operations will trigger that AND gate,",
    "start": "859060",
    "end": "863698"
  },
  {
    "text": "it will add on the basketball direction, so what pops out will encode all of those",
    "start": "863698",
    "end": "868697"
  },
  {
    "text": "together.",
    "start": "868697",
    "end": "869240"
  },
  {
    "text": "And remember, this is a process happening to every one of those vectors in parallel.",
    "start": "869820",
    "end": "874200"
  },
  {
    "text": "In particular, taking the GPT-3 numbers, it means that this block doesn't just",
    "start": "874800",
    "end": "879766"
  },
  {
    "text": "have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
    "start": "879767",
    "end": "884860"
  },
  {
    "text": "So that is the entire operation, two matrix products,",
    "start": "888180",
    "end": "891356"
  },
  {
    "text": "each with a bias added and a simple clipping function in between.",
    "start": "891356",
    "end": "895180"
  },
  {
    "text": "Any of you who watched the earlier videos of the series will recognize this",
    "start": "896080",
    "end": "899415"
  },
  {
    "text": "structure as the most basic kind of neural network that we studied there.",
    "start": "899415",
    "end": "902620"
  },
  {
    "text": "In that example, it was trained to recognize handwritten digits.",
    "start": "903080",
    "end": "906100"
  },
  {
    "text": "Over here, in the context of a transformer for a large language model,",
    "start": "906580",
    "end": "910804"
  },
  {
    "text": "this is one piece in a larger architecture and any attempt to interpret",
    "start": "910804",
    "end": "915088"
  },
  {
    "text": "what exactly it's doing is heavily intertwined with the idea of encoding",
    "start": "915088",
    "end": "919431"
  },
  {
    "text": "information into vectors of a high-dimensional embedding space.",
    "start": "919431",
    "end": "923180"
  },
  {
    "text": "That is the core lesson, but I do wanna step back and reflect on two different things,",
    "start": "924260",
    "end": "928600"
  },
  {
    "text": "the first of which is a kind of bookkeeping, and the second of which",
    "start": "928600",
    "end": "932043"
  },
  {
    "text": "involves a very thought-provoking fact about higher dimensions that",
    "start": "932043",
    "end": "935435"
  },
  {
    "text": "I actually didn't know until I dug into transformers.",
    "start": "935435",
    "end": "938080"
  },
  {
    "start": "938000",
    "end": "1024000"
  },
  {
    "text": "In the last two chapters, you and I started counting up the total number of parameters",
    "start": "941080",
    "end": "945947"
  },
  {
    "text": "in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
    "start": "945947",
    "end": "950760"
  },
  {
    "text": "I already mentioned how this up projection matrix has just under 50,000 rows and",
    "start": "951400",
    "end": "956790"
  },
  {
    "text": "that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
    "start": "956790",
    "end": "962180"
  },
  {
    "text": "Multiplying those together, it gives us 604 million parameters just for that matrix,",
    "start": "963240",
    "end": "968516"
  },
  {
    "text": "and the down projection has the same number of parameters just with a transposed shape.",
    "start": "968517",
    "end": "973920"
  },
  {
    "text": "So together, they give about 1.2 billion parameters.",
    "start": "974500",
    "end": "977400"
  },
  {
    "text": "The bias vector also accounts for a couple more parameters,",
    "start": "978280",
    "end": "980885"
  },
  {
    "text": "but it's a trivial proportion of the total, so I'm not even gonna show it.",
    "start": "980885",
    "end": "984100"
  },
  {
    "text": "In GPT-3, this sequence of embedding vectors flows through not one,",
    "start": "984660",
    "end": "989612"
  },
  {
    "text": "but 96 distinct MLPs, so the total number of parameters devoted",
    "start": "989612",
    "end": "994273"
  },
  {
    "text": "to all of these blocks adds up to about 116 billion.",
    "start": "994273",
    "end": "998060"
  },
  {
    "text": "This is around 2 thirds of the total parameters in the network,",
    "start": "998820",
    "end": "1002177"
  },
  {
    "text": "and when you add it to everything that we had before, for the attention blocks,",
    "start": "1002177",
    "end": "1006374"
  },
  {
    "text": "the embedding, and the unembedding, you do indeed get that grand total of 175",
    "start": "1006374",
    "end": "1010465"
  },
  {
    "text": "billion as advertised.",
    "start": "1010465",
    "end": "1011620"
  },
  {
    "text": "It's probably worth mentioning there's another set of parameters associated",
    "start": "1013060",
    "end": "1016637"
  },
  {
    "text": "with those normalization steps that this explanation has skipped over,",
    "start": "1016637",
    "end": "1019979"
  },
  {
    "text": "but like the bias vector, they account for a very trivial proportion of the total.",
    "start": "1019979",
    "end": "1023840"
  },
  {
    "start": "1024000",
    "end": "1297000"
  },
  {
    "text": "As to that second point of reflection, you might be wondering if",
    "start": "1025900",
    "end": "1029160"
  },
  {
    "text": "this central toy example we've been spending so much time on",
    "start": "1029160",
    "end": "1032219"
  },
  {
    "text": "reflects how facts are actually stored in real large language models.",
    "start": "1032219",
    "end": "1035680"
  },
  {
    "text": "It is true that the rows of that first matrix can be thought of as",
    "start": "1036319",
    "end": "1039767"
  },
  {
    "text": "directions in this embedding space, and that means the activation of each",
    "start": "1039768",
    "end": "1043575"
  },
  {
    "text": "neuron tells you how much a given vector aligns with some specific direction.",
    "start": "1043576",
    "end": "1047540"
  },
  {
    "text": "It's also true that the columns of that second matrix tell",
    "start": "1047760",
    "end": "1050968"
  },
  {
    "text": "you what will be added to the result if that neuron is active.",
    "start": "1050968",
    "end": "1054340"
  },
  {
    "text": "Both of those are just mathematical facts.",
    "start": "1054640",
    "end": "1056800"
  },
  {
    "text": "However, the evidence does suggest that individual neurons very rarely",
    "start": "1057740",
    "end": "1061806"
  },
  {
    "text": "represent a single clean feature like Michael Jordan,",
    "start": "1061806",
    "end": "1064899"
  },
  {
    "text": "and there may actually be a very good reason this is the case,",
    "start": "1064899",
    "end": "1068507"
  },
  {
    "text": "related to an idea floating around interpretability researchers these",
    "start": "1068507",
    "end": "1072516"
  },
  {
    "text": "days known as superposition.",
    "start": "1072516",
    "end": "1074120"
  },
  {
    "text": "This is a hypothesis that might help to explain both why the models are",
    "start": "1074640",
    "end": "1078557"
  },
  {
    "text": "especially hard to interpret and also why they scale surprisingly well.",
    "start": "1078557",
    "end": "1082420"
  },
  {
    "text": "The basic idea is that if you have an n-dimensional space and you wanna",
    "start": "1083500",
    "end": "1087386"
  },
  {
    "text": "represent a bunch of different features using directions that are all",
    "start": "1087386",
    "end": "1091165"
  },
  {
    "text": "perpendicular to one another in that space, you know,",
    "start": "1091165",
    "end": "1094080"
  },
  {
    "text": "that way if you add a component in one direction,",
    "start": "1094080",
    "end": "1096779"
  },
  {
    "text": "it doesn't influence any of the other directions,",
    "start": "1096780",
    "end": "1099479"
  },
  {
    "text": "then the maximum number of vectors you can fit is only n, the number of dimensions.",
    "start": "1099479",
    "end": "1103960"
  },
  {
    "text": "To a mathematician, actually, this is the definition of dimension.",
    "start": "1104600",
    "end": "1107620"
  },
  {
    "text": "But where it gets interesting is if you relax that",
    "start": "1108220",
    "end": "1110872"
  },
  {
    "text": "constraint a little bit and you tolerate some noise.",
    "start": "1110873",
    "end": "1113580"
  },
  {
    "text": "Say you allow those features to be represented by vectors that aren't exactly",
    "start": "1114180",
    "end": "1118709"
  },
  {
    "text": "perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
    "start": "1118709",
    "end": "1123820"
  },
  {
    "text": "If we were in two or three dimensions, this makes no difference.",
    "start": "1124820",
    "end": "1128019"
  },
  {
    "text": "That gives you hardly any extra wiggle room to fit more vectors in,",
    "start": "1128260",
    "end": "1131608"
  },
  {
    "text": "which makes it all the more counterintuitive that for higher dimensions,",
    "start": "1131608",
    "end": "1135204"
  },
  {
    "text": "the answer changes dramatically.",
    "start": "1135204",
    "end": "1136780"
  },
  {
    "text": "I can give you a really quick and dirty illustration of this using some",
    "start": "1137660",
    "end": "1141845"
  },
  {
    "text": "scrappy Python that's going to create a list of 100-dimensional vectors,",
    "start": "1141845",
    "end": "1146088"
  },
  {
    "text": "each one initialized randomly, and this list is going to contain 10,000 distinct vectors,",
    "start": "1146088",
    "end": "1151319"
  },
  {
    "text": "so 100 times as many vectors as there are dimensions.",
    "start": "1151319",
    "end": "1154400"
  },
  {
    "text": "This plot right here shows the distribution of angles between pairs of these vectors.",
    "start": "1155320",
    "end": "1159899"
  },
  {
    "text": "So because they started at random, those angles could be anything from 0 to 180 degrees,",
    "start": "1160680",
    "end": "1165393"
  },
  {
    "text": "but you'll notice that already, even just for random vectors,",
    "start": "1165393",
    "end": "1168675"
  },
  {
    "text": "there's this heavy bias for things to be closer to 90 degrees.",
    "start": "1168676",
    "end": "1171960"
  },
  {
    "text": "Then what I'm going to do is run a certain optimization process that iteratively nudges",
    "start": "1172500",
    "end": "1177169"
  },
  {
    "text": "all of these vectors so that they try to become more perpendicular to one another.",
    "start": "1177169",
    "end": "1181519"
  },
  {
    "text": "After repeating this many different times, here's",
    "start": "1182060",
    "end": "1184533"
  },
  {
    "text": "what the distribution of angles looks like.",
    "start": "1184533",
    "end": "1186659"
  },
  {
    "text": "We have to actually zoom in on it here because all of the possible angles",
    "start": "1187120",
    "end": "1191819"
  },
  {
    "text": "between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
    "start": "1191819",
    "end": "1196900"
  },
  {
    "text": "In general, a consequence of something known as the Johnson-Lindenstrauss",
    "start": "1198020",
    "end": "1202217"
  },
  {
    "text": "lemma is that the number of vectors you can cram into a space that are nearly",
    "start": "1202217",
    "end": "1206642"
  },
  {
    "text": "perpendicular like this grows exponentially with the number of dimensions.",
    "start": "1206642",
    "end": "1210840"
  },
  {
    "text": "This is very significant for large language models,",
    "start": "1211960",
    "end": "1214820"
  },
  {
    "text": "which might benefit from associating independent ideas with nearly",
    "start": "1214820",
    "end": "1218505"
  },
  {
    "text": "perpendicular directions.",
    "start": "1218505",
    "end": "1219880"
  },
  {
    "text": "It means that it's possible for it to store many,",
    "start": "1220000",
    "end": "1222596"
  },
  {
    "text": "many more ideas than there are dimensions in the space that it's allotted.",
    "start": "1222596",
    "end": "1226440"
  },
  {
    "text": "This might partially explain why model performance seems to scale so well with size.",
    "start": "1227320",
    "end": "1231740"
  },
  {
    "text": "A space that has 10 times as many dimensions can store way,",
    "start": "1232540",
    "end": "1236316"
  },
  {
    "text": "way more than 10 times as many independent ideas.",
    "start": "1236316",
    "end": "1239400"
  },
  {
    "text": "And this is relevant not just to that embedding space where the vectors",
    "start": "1240420",
    "end": "1243871"
  },
  {
    "text": "flowing through the model live, but also to that vector full of neurons",
    "start": "1243871",
    "end": "1247323"
  },
  {
    "text": "in the middle of that multilayer perceptron that we just studied.",
    "start": "1247323",
    "end": "1250440"
  },
  {
    "text": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features,",
    "start": "1250960",
    "end": "1256112"
  },
  {
    "text": "but if it instead leveraged this enormous added capacity by using",
    "start": "1256113",
    "end": "1259978"
  },
  {
    "text": "nearly perpendicular directions of the space, it could be probing at many,",
    "start": "1259978",
    "end": "1264370"
  },
  {
    "text": "many more features of the vector being processed.",
    "start": "1264370",
    "end": "1267240"
  },
  {
    "text": "But if it was doing that, what it means is that individual",
    "start": "1267780",
    "end": "1270926"
  },
  {
    "text": "features aren't gonna be visible as a single neuron lighting up.",
    "start": "1270926",
    "end": "1274340"
  },
  {
    "text": "It would have to look like some specific combination of neurons instead, a superposition.",
    "start": "1274660",
    "end": "1279380"
  },
  {
    "text": "For any of you curious to learn more, a key relevant search term here is sparse",
    "start": "1280400",
    "end": "1284315"
  },
  {
    "text": "autoencoder, which is a tool that some of the interpretability people use to try to",
    "start": "1284315",
    "end": "1288425"
  },
  {
    "text": "extract what the true features are, even if they're very superimposed on all these",
    "start": "1288426",
    "end": "1292488"
  },
  {
    "text": "neurons.",
    "start": "1292488",
    "end": "1292880"
  },
  {
    "text": "I'll link to a couple really great anthropic posts all about this.",
    "start": "1293540",
    "end": "1296800"
  },
  {
    "start": "1297000",
    "end": "1363000"
  },
  {
    "text": "At this point, we haven't touched every detail of a transformer,",
    "start": "1297880",
    "end": "1300970"
  },
  {
    "text": "but you and I have hit the most important points.",
    "start": "1300970",
    "end": "1303299"
  },
  {
    "text": "The main thing that I wanna cover in a next chapter is the training process.",
    "start": "1303520",
    "end": "1307640"
  },
  {
    "text": "On the one hand, the short answer for how training works is that it's all",
    "start": "1308460",
    "end": "1311929"
  },
  {
    "text": "backpropagation, and we covered backpropagation in a separate context with earlier",
    "start": "1311929",
    "end": "1315820"
  },
  {
    "text": "chapters in the series.",
    "start": "1315821",
    "end": "1316900"
  },
  {
    "text": "But there is more to discuss, like the specific cost function used for language models,",
    "start": "1317220",
    "end": "1322033"
  },
  {
    "text": "the idea of fine-tuning using reinforcement learning with human feedback,",
    "start": "1322034",
    "end": "1326083"
  },
  {
    "text": "and the notion of scaling laws.",
    "start": "1326083",
    "end": "1327780"
  },
  {
    "text": "Quick note for the active followers among you,",
    "start": "1328960",
    "end": "1331112"
  },
  {
    "text": "there are a number of non-machine learning-related videos that I'm excited to",
    "start": "1331113",
    "end": "1334686"
  },
  {
    "text": "sink my teeth into before I make that next chapter, so it might be a while,",
    "start": "1334686",
    "end": "1338167"
  },
  {
    "text": "but I do promise it'll come in due time.",
    "start": "1338167",
    "end": "1340000"
  },
  {
    "text": "Thank you.",
    "start": "1355640",
    "end": "1357920"
  }
]