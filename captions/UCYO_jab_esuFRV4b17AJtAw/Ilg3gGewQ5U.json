[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
    "start": "4060",
    "end": "8880"
  },
  {
    "text": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough",
    "start": "9400",
    "end": "13379"
  },
  {
    "text": "for what the algorithm is actually doing, without any reference to the formulas.",
    "start": "13379",
    "end": "17000"
  },
  {
    "text": "Then, for those of you who do want to dive into the math,",
    "start": "17660",
    "end": "20294"
  },
  {
    "text": "the next video goes into the calculus underlying all this.",
    "start": "20294",
    "end": "23020"
  },
  {
    "start": "23000",
    "end": "187000"
  },
  {
    "text": "If you watched the last two videos, or if you're just jumping in with the appropriate",
    "start": "23820",
    "end": "27410"
  },
  {
    "text": "background, you know what a neural network is, and how it feeds forward information.",
    "start": "27410",
    "end": "31000"
  },
  {
    "text": "Here, we're doing the classic example of recognizing handwritten digits whose pixel",
    "start": "31680",
    "end": "36007"
  },
  {
    "text": "values get fed into the first layer of the network with 784 neurons,",
    "start": "36007",
    "end": "39604"
  },
  {
    "text": "and I've been showing a network with two hidden layers having just 16 neurons each,",
    "start": "39604",
    "end": "43983"
  },
  {
    "text": "and an output layer of 10 neurons, indicating which digit the network is choosing",
    "start": "43983",
    "end": "48258"
  },
  {
    "text": "as its answer.",
    "start": "48258",
    "end": "49040"
  },
  {
    "text": "I'm also expecting you to understand gradient descent,",
    "start": "50040",
    "end": "53039"
  },
  {
    "text": "as described in the last video, and how what we mean by learning is",
    "start": "53039",
    "end": "56816"
  },
  {
    "text": "that we want to find which weights and biases minimize a certain cost function.",
    "start": "56816",
    "end": "61260"
  },
  {
    "text": "As a quick reminder, for the cost of a single training example,",
    "start": "62040",
    "end": "65755"
  },
  {
    "text": "you take the output the network gives, along with the output you wanted it to give,",
    "start": "65755",
    "end": "70708"
  },
  {
    "text": "and add up the squares of the differences between each component.",
    "start": "70708",
    "end": "74600"
  },
  {
    "text": "Doing this for all of your tens of thousands of training examples and",
    "start": "75380",
    "end": "78790"
  },
  {
    "text": "averaging the results, this gives you the total cost of the network.",
    "start": "78790",
    "end": "82200"
  },
  {
    "text": "And as if that's not enough to think about, as described in the last video,",
    "start": "82200",
    "end": "86271"
  },
  {
    "text": "the thing that we're looking for is the negative gradient of this cost function,",
    "start": "86271",
    "end": "90667"
  },
  {
    "text": "which tells you how you need to change all of the weights and biases,",
    "start": "90667",
    "end": "94466"
  },
  {
    "text": "all of these connections, so as to most efficiently decrease the cost.",
    "start": "94466",
    "end": "98320"
  },
  {
    "text": "Backpropagation, the topic of this video, is an",
    "start": "103260",
    "end": "105664"
  },
  {
    "text": "algorithm for computing that crazy complicated gradient.",
    "start": "105664",
    "end": "108580"
  },
  {
    "text": "And the one idea from the last video that I really want you to hold firmly",
    "start": "109140",
    "end": "112876"
  },
  {
    "text": "in your mind right now is that because thinking of the gradient vector",
    "start": "112876",
    "end": "116461"
  },
  {
    "text": "as a direction in 13,000 dimensions is, to put it lightly,",
    "start": "116461",
    "end": "119440"
  },
  {
    "text": "beyond the scope of our imaginations, there's another way you can think about it.",
    "start": "119440",
    "end": "123580"
  },
  {
    "text": "The magnitude of each component here is telling you how",
    "start": "124600",
    "end": "127741"
  },
  {
    "text": "sensitive the cost function is to each weight and bias.",
    "start": "127741",
    "end": "130940"
  },
  {
    "text": "For example, let's say you go through the process I'm about to describe,",
    "start": "131800",
    "end": "135744"
  },
  {
    "text": "and you compute the negative gradient, and the component associated with the weight on",
    "start": "135744",
    "end": "140509"
  },
  {
    "text": "this edge here comes out to be 3.2, while the component associated with this edge here",
    "start": "140509",
    "end": "145274"
  },
  {
    "text": "comes out as 0.1.",
    "start": "145274",
    "end": "146260"
  },
  {
    "text": "The way you would interpret that is that the cost of the function is 32 times more",
    "start": "146820",
    "end": "150969"
  },
  {
    "text": "sensitive to changes in that first weight, so if you were to wiggle that value",
    "start": "150969",
    "end": "154965"
  },
  {
    "text": "just a little bit, it's going to cause some change to the cost,",
    "start": "154965",
    "end": "158203"
  },
  {
    "text": "and that change is 32 times greater than what the same wiggle to that second",
    "start": "158203",
    "end": "162099"
  },
  {
    "text": "weight would give.",
    "start": "162099",
    "end": "163060"
  },
  {
    "text": "Personally, when I was first learning about backpropagation,",
    "start": "168420",
    "end": "171368"
  },
  {
    "text": "I think the most confusing aspect was just the notation and the index chasing of it all.",
    "start": "171368",
    "end": "175739"
  },
  {
    "text": "But once you unwrap what each part of this algorithm is really doing,",
    "start": "176220",
    "end": "179444"
  },
  {
    "text": "each individual effect it's having is actually pretty intuitive,",
    "start": "179444",
    "end": "182481"
  },
  {
    "text": "it's just that there's a lot of little adjustments getting layered on top of each other.",
    "start": "182481",
    "end": "186640"
  },
  {
    "start": "187000",
    "end": "573000"
  },
  {
    "text": "So I'm going to start things off here with a complete disregard for the notation,",
    "start": "187740",
    "end": "191780"
  },
  {
    "text": "and just step through the effects each training example has on the weights and biases.",
    "start": "191780",
    "end": "196120"
  },
  {
    "text": "Because the cost function involves averaging a certain cost per example over all",
    "start": "197020",
    "end": "201733"
  },
  {
    "text": "the tens of thousands of training examples, the way we adjust the weights and",
    "start": "201733",
    "end": "206327"
  },
  {
    "text": "biases for a single gradient descent step also depends on every single example.",
    "start": "206327",
    "end": "211040"
  },
  {
    "text": "Or rather, in principle it should, but for computational efficiency we'll do a little",
    "start": "211680",
    "end": "215530"
  },
  {
    "text": "trick later to keep you from needing to hit every single example for every step.",
    "start": "215531",
    "end": "219200"
  },
  {
    "text": "In other cases, right now, all we're going to do is focus",
    "start": "219200",
    "end": "222610"
  },
  {
    "text": "our attention on one single example, this image of a 2.",
    "start": "222610",
    "end": "225960"
  },
  {
    "text": "What effect should this one training example have",
    "start": "226720",
    "end": "229228"
  },
  {
    "text": "on how the weights and biases get adjusted?",
    "start": "229228",
    "end": "231480"
  },
  {
    "text": "Let's say we're at a point where the network is not well trained yet,",
    "start": "232680",
    "end": "236213"
  },
  {
    "text": "so the activations in the output are going to look pretty random,",
    "start": "236213",
    "end": "239593"
  },
  {
    "text": "maybe something like 0.5, 0.8, 0.2, on and on.",
    "start": "239593",
    "end": "242000"
  },
  {
    "text": "We can't directly change those activations, we",
    "start": "242520",
    "end": "244815"
  },
  {
    "text": "only have influence on the weights and biases.",
    "start": "244815",
    "end": "247160"
  },
  {
    "text": "But it's helpful to keep track of which adjustments",
    "start": "247160",
    "end": "249952"
  },
  {
    "text": "we wish should take place to that output layer.",
    "start": "249952",
    "end": "252580"
  },
  {
    "text": "And since we want it to classify the image as a 2,",
    "start": "253360",
    "end": "256398"
  },
  {
    "text": "we want that third value to get nudged up while all the others get nudged down.",
    "start": "256398",
    "end": "261260"
  },
  {
    "text": "Moreover, the sizes of these nudges should be proportional",
    "start": "262060",
    "end": "265695"
  },
  {
    "text": "to how far away each current value is from its target value.",
    "start": "265696",
    "end": "269520"
  },
  {
    "text": "For example, the increase to that number 2 neuron's activation",
    "start": "270220",
    "end": "273780"
  },
  {
    "text": "is in a sense more important than the decrease to the number 8 neuron,",
    "start": "273780",
    "end": "277857"
  },
  {
    "text": "which is already pretty close to where it should be.",
    "start": "277857",
    "end": "280900"
  },
  {
    "text": "So zooming in further, let's focus just on this one neuron,",
    "start": "282040",
    "end": "284984"
  },
  {
    "text": "the one whose activation we wish to increase.",
    "start": "284984",
    "end": "287280"
  },
  {
    "text": "Remember, that activation is defined as a certain weighted sum of all the",
    "start": "288180",
    "end": "292526"
  },
  {
    "text": "activations in the previous layer, plus a bias,",
    "start": "292526",
    "end": "295384"
  },
  {
    "text": "which is all then plugged into something like the sigmoid squishification function,",
    "start": "295384",
    "end": "300385"
  },
  {
    "text": "or a ReLU.",
    "start": "300385",
    "end": "301040"
  },
  {
    "text": "So there are three different avenues that can team",
    "start": "301640",
    "end": "304442"
  },
  {
    "text": "up together to help increase that activation.",
    "start": "304442",
    "end": "307020"
  },
  {
    "text": "You can increase the bias, you can increase the weights,",
    "start": "307440",
    "end": "310626"
  },
  {
    "text": "and you can change the activations from the previous layer.",
    "start": "310626",
    "end": "314040"
  },
  {
    "text": "Focusing on how the weights should be adjusted,",
    "start": "314940",
    "end": "317359"
  },
  {
    "text": "notice how the weights actually have differing levels of influence.",
    "start": "317359",
    "end": "320860"
  },
  {
    "text": "The connections with the brightest neurons from the preceding layer have the",
    "start": "321440",
    "end": "325196"
  },
  {
    "text": "biggest effect since those weights are multiplied by larger activation values.",
    "start": "325196",
    "end": "329100"
  },
  {
    "text": "So if you were to increase one of those weights,",
    "start": "331460",
    "end": "333884"
  },
  {
    "text": "it actually has a stronger influence on the ultimate cost function than increasing",
    "start": "333884",
    "end": "338076"
  },
  {
    "text": "the weights of connections with dimmer neurons,",
    "start": "338076",
    "end": "340500"
  },
  {
    "text": "at least as far as this one training example is concerned.",
    "start": "340500",
    "end": "343480"
  },
  {
    "text": "Remember, when we talk about gradient descent,",
    "start": "344420",
    "end": "346585"
  },
  {
    "text": "we don't just care about whether each component should get nudged up or down,",
    "start": "346585",
    "end": "350255"
  },
  {
    "text": "we care about which ones give you the most bang for your buck.",
    "start": "350255",
    "end": "353220"
  },
  {
    "text": "This, by the way, is at least somewhat reminiscent of a theory in",
    "start": "355020",
    "end": "358511"
  },
  {
    "text": "neuroscience for how biological networks of neurons learn, Hebbian theory,",
    "start": "358511",
    "end": "362539"
  },
  {
    "text": "often summed up in the phrase, neurons that fire together wire together.",
    "start": "362539",
    "end": "366460"
  },
  {
    "text": "Here, the biggest increases to weights, the biggest strengthening of connections,",
    "start": "367260",
    "end": "371719"
  },
  {
    "text": "happens between neurons which are the most active,",
    "start": "371719",
    "end": "374527"
  },
  {
    "text": "and the ones which we wish to become more active.",
    "start": "374527",
    "end": "377280"
  },
  {
    "text": "In a sense, the neurons that are firing while seeing a 2 get",
    "start": "377940",
    "end": "381156"
  },
  {
    "text": "more strongly linked to those firing when thinking about a 2.",
    "start": "381156",
    "end": "384480"
  },
  {
    "text": "To be clear, I'm not in a position to make statements one way or another about",
    "start": "385400",
    "end": "389356"
  },
  {
    "text": "whether artificial networks of neurons behave anything like biological brains,",
    "start": "389356",
    "end": "393362"
  },
  {
    "text": "and this fires together wire together idea comes with a couple meaningful asterisks,",
    "start": "393362",
    "end": "397673"
  },
  {
    "text": "but taken as a very loose analogy, I find it interesting to note.",
    "start": "397673",
    "end": "401020"
  },
  {
    "text": "Anyway, the third way we can help increase this neuron's activation",
    "start": "401940",
    "end": "405746"
  },
  {
    "text": "is by changing all the activations in the previous layer.",
    "start": "405746",
    "end": "409040"
  },
  {
    "text": "Namely, if everything connected to that digit 2 neuron with a positive",
    "start": "409040",
    "end": "413015"
  },
  {
    "text": "weight got brighter, and if everything connected with a negative weight got dimmer,",
    "start": "413015",
    "end": "417784"
  },
  {
    "text": "then that digit 2 neuron would become more active.",
    "start": "417784",
    "end": "420680"
  },
  {
    "text": "And similar to the weight changes, you're going to get the most bang for your buck",
    "start": "422540",
    "end": "426387"
  },
  {
    "text": "by seeking changes that are proportional to the size of the corresponding weights.",
    "start": "426387",
    "end": "430280"
  },
  {
    "text": "Now of course, we cannot directly influence those activations,",
    "start": "432140",
    "end": "435096"
  },
  {
    "text": "we only have control over the weights and biases.",
    "start": "435096",
    "end": "437480"
  },
  {
    "text": "But just as with the last layer, it's helpful to",
    "start": "437480",
    "end": "440835"
  },
  {
    "text": "keep a note of what those desired changes are.",
    "start": "440835",
    "end": "444120"
  },
  {
    "text": "But keep in mind, zooming out one step here, this",
    "start": "444580",
    "end": "446938"
  },
  {
    "text": "is only what that digit 2 output neuron wants.",
    "start": "446938",
    "end": "449199"
  },
  {
    "text": "Remember, we also want all the other neurons in the last layer to become less active,",
    "start": "449760",
    "end": "453942"
  },
  {
    "text": "and each of those other output neurons has its own thoughts about",
    "start": "453942",
    "end": "457189"
  },
  {
    "text": "what should happen to that second to last layer.",
    "start": "457189",
    "end": "459600"
  },
  {
    "text": "So, the desire of this digit 2 neuron is added together with the desires",
    "start": "462700",
    "end": "467401"
  },
  {
    "text": "of all the other output neurons for what should happen to this second to last layer,",
    "start": "467401",
    "end": "472951"
  },
  {
    "text": "again in proportion to the corresponding weights,",
    "start": "472951",
    "end": "476215"
  },
  {
    "text": "and in proportion to how much each of those neurons needs to change.",
    "start": "476215",
    "end": "480720"
  },
  {
    "text": "This right here is where the idea of propagating backwards comes in.",
    "start": "481600",
    "end": "485480"
  },
  {
    "text": "By adding together all these desired effects, you basically get a",
    "start": "485820",
    "end": "489477"
  },
  {
    "text": "list of nudges that you want to happen to this second to last layer.",
    "start": "489477",
    "end": "493360"
  },
  {
    "text": "And once you have those, you can recursively apply the same process to the",
    "start": "494220",
    "end": "497847"
  },
  {
    "text": "relevant weights and biases that determine those values,",
    "start": "497847",
    "end": "500639"
  },
  {
    "text": "repeating the same process I just walked through and moving backwards",
    "start": "500640",
    "end": "504071"
  },
  {
    "text": "through the network.",
    "start": "504071",
    "end": "505100"
  },
  {
    "text": "And zooming out a bit further, remember that this is all just how a single",
    "start": "508960",
    "end": "513063"
  },
  {
    "text": "training example wishes to nudge each one of those weights and biases.",
    "start": "513063",
    "end": "517000"
  },
  {
    "text": "If we only listened to what that 2 wanted, the network would",
    "start": "517480",
    "end": "520279"
  },
  {
    "text": "ultimately be incentivized just to classify all images as a 2.",
    "start": "520280",
    "end": "523220"
  },
  {
    "text": "So what you do is go through this same backprop routine for every other training example,",
    "start": "524060",
    "end": "529244"
  },
  {
    "text": "recording how each of them would like to change the weights and biases,",
    "start": "529244",
    "end": "533437"
  },
  {
    "text": "and average together those desired changes.",
    "start": "533437",
    "end": "536000"
  },
  {
    "text": "This collection here of the averaged nudges to each weight and bias is,",
    "start": "541720",
    "end": "545883"
  },
  {
    "text": "loosely speaking, the negative gradient of the cost function referenced",
    "start": "545883",
    "end": "550104"
  },
  {
    "text": "in the last video, or at least something proportional to it.",
    "start": "550104",
    "end": "553680"
  },
  {
    "text": "I say loosely speaking only because I have yet to get quantitatively precise",
    "start": "554380",
    "end": "558390"
  },
  {
    "text": "about those nudges, but if you understood every change I just referenced,",
    "start": "558390",
    "end": "562294"
  },
  {
    "text": "why some are proportionally bigger than others,",
    "start": "562294",
    "end": "564827"
  },
  {
    "text": "and how they all need to be added together, you understand the mechanics for",
    "start": "564827",
    "end": "568890"
  },
  {
    "text": "what backpropagation is actually doing.",
    "start": "568890",
    "end": "571000"
  },
  {
    "start": "573000",
    "end": "748000"
  },
  {
    "text": "By the way, in practice, it takes computers an extremely long time to add",
    "start": "573960",
    "end": "578229"
  },
  {
    "text": "up the influence of every training example every gradient descent step.",
    "start": "578229",
    "end": "582440"
  },
  {
    "text": "So here's what's commonly done instead.",
    "start": "583140",
    "end": "584820"
  },
  {
    "text": "You randomly shuffle your training data and then divide it into a whole",
    "start": "585480",
    "end": "588925"
  },
  {
    "text": "bunch of mini-batches, let's say each one having 100 training examples.",
    "start": "588926",
    "end": "592420"
  },
  {
    "text": "Then you compute a step according to the mini-batch.",
    "start": "592940",
    "end": "596200"
  },
  {
    "text": "It's not going to be the actual gradient of the cost function,",
    "start": "596960",
    "end": "600012"
  },
  {
    "text": "which depends on all of the training data, not this tiny subset,",
    "start": "600012",
    "end": "603211"
  },
  {
    "text": "so it's not the most efficient step downhill,",
    "start": "603211",
    "end": "605475"
  },
  {
    "text": "but each mini-batch does give you a pretty good approximation, and more importantly,",
    "start": "605475",
    "end": "609659"
  },
  {
    "text": "it gives you a significant computational speedup.",
    "start": "609659",
    "end": "612120"
  },
  {
    "text": "If you were to plot the trajectory of your network under the relevant cost surface,",
    "start": "612820",
    "end": "617078"
  },
  {
    "text": "it would be a little more like a drunk man stumbling aimlessly down a hill but taking",
    "start": "617078",
    "end": "621490"
  },
  {
    "text": "quick steps, rather than a carefully calculating man determining the exact downhill",
    "start": "621490",
    "end": "625799"
  },
  {
    "text": "direction of each step before taking a very slow and careful step in that direction.",
    "start": "625799",
    "end": "630160"
  },
  {
    "text": "This technique is referred to as stochastic gradient descent.",
    "start": "631540",
    "end": "634660"
  },
  {
    "text": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
    "start": "635960",
    "end": "639620"
  },
  {
    "text": "Backpropagation is the algorithm for determining how a single training",
    "start": "640440",
    "end": "644220"
  },
  {
    "text": "example would like to nudge the weights and biases,",
    "start": "644220",
    "end": "647027"
  },
  {
    "text": "not just in terms of whether they should go up or down,",
    "start": "647028",
    "end": "650052"
  },
  {
    "text": "but in terms of what relative proportions to those changes cause the",
    "start": "650052",
    "end": "653778"
  },
  {
    "text": "most rapid decrease to the cost.",
    "start": "653778",
    "end": "655560"
  },
  {
    "text": "A true gradient descent step would involve doing this for all your tens of",
    "start": "656260",
    "end": "660230"
  },
  {
    "text": "thousands of training examples and averaging the desired changes you get.",
    "start": "660230",
    "end": "664199"
  },
  {
    "text": "But that's computationally slow, so instead you randomly subdivide the",
    "start": "664860",
    "end": "668906"
  },
  {
    "text": "data into mini-batches and compute each step with respect to a mini-batch.",
    "start": "668906",
    "end": "673240"
  },
  {
    "text": "Repeatedly going through all of the mini-batches and making these adjustments,",
    "start": "674000",
    "end": "677863"
  },
  {
    "text": "you will converge towards a local minimum of the cost function,",
    "start": "677863",
    "end": "681032"
  },
  {
    "text": "which is to say your network will end up doing a really good job on the training",
    "start": "681033",
    "end": "685045"
  },
  {
    "text": "examples.",
    "start": "685045",
    "end": "685540"
  },
  {
    "text": "So with all of that said, every line of code that would go into implementing backprop",
    "start": "687240",
    "end": "692036"
  },
  {
    "text": "actually corresponds with something you have now seen, at least in informal terms.",
    "start": "692036",
    "end": "696720"
  },
  {
    "text": "But sometimes knowing what the math does is only half the battle,",
    "start": "697560",
    "end": "700481"
  },
  {
    "text": "and just representing the damn thing is where it gets all muddled and confusing.",
    "start": "700481",
    "end": "704120"
  },
  {
    "text": "So for those of you who do want to go deeper, the next video goes through the same",
    "start": "704860",
    "end": "708576"
  },
  {
    "text": "ideas that were just presented here, but in terms of the underlying calculus,",
    "start": "708577",
    "end": "712113"
  },
  {
    "text": "which should hopefully make it a little more familiar as you see the topic in other",
    "start": "712113",
    "end": "715920"
  },
  {
    "text": "resources.",
    "start": "715921",
    "end": "716420"
  },
  {
    "text": "Before that, one thing worth emphasizing is that for this algorithm to work,",
    "start": "717340",
    "end": "720838"
  },
  {
    "text": "and this goes for all sorts of machine learning beyond just neural networks,",
    "start": "720838",
    "end": "724381"
  },
  {
    "text": "you need a lot of training data.",
    "start": "724381",
    "end": "725899"
  },
  {
    "text": "In our case, one thing that makes handwritten digits such a nice example is that there",
    "start": "726420",
    "end": "730654"
  },
  {
    "text": "exists the MNIST database, with so many examples that have been labeled by humans.",
    "start": "730654",
    "end": "734740"
  },
  {
    "text": "So a common challenge that those of you working in machine learning will be familiar with",
    "start": "735300",
    "end": "739204"
  },
  {
    "text": "is just getting the labeled training data you actually need,",
    "start": "739204",
    "end": "741880"
  },
  {
    "text": "whether that's having people label tens of thousands of images,",
    "start": "741880",
    "end": "744687"
  },
  {
    "text": "or whatever other data type you might be dealing with.",
    "start": "744687",
    "end": "747100"
  }
]