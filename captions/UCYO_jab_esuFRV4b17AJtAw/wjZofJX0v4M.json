[
  {
    "start": "0",
    "end": "183000"
  },
  {
    "text": "The initials GPT stand for Generative Pretrained Transformer.",
    "start": "0",
    "end": "4560"
  },
  {
    "text": "So that first word is straightforward enough, these are bots that generate new text.",
    "start": "5220",
    "end": "9019"
  },
  {
    "text": "Pretrained refers to how the model went through a process of learning",
    "start": "9800",
    "end": "13181"
  },
  {
    "text": "from a massive amount of data, and the prefix insinuates that there's",
    "start": "13181",
    "end": "16610"
  },
  {
    "text": "more room to fine-tune it on specific tasks with additional training.",
    "start": "16610",
    "end": "20040"
  },
  {
    "text": "But the last word, that's the real key piece.",
    "start": "20720",
    "end": "22900"
  },
  {
    "text": "A transformer is a specific kind of neural network, a machine learning model,",
    "start": "23380",
    "end": "27571"
  },
  {
    "text": "and it's the core invention underlying the current boom in AI.",
    "start": "27571",
    "end": "31000"
  },
  {
    "text": "What I want to do with this video and the following chapters is go through a",
    "start": "31740",
    "end": "35430"
  },
  {
    "text": "visually-driven explanation for what actually happens inside a transformer.",
    "start": "35430",
    "end": "39120"
  },
  {
    "text": "We're going to follow the data that flows through it and go step by step.",
    "start": "39700",
    "end": "42820"
  },
  {
    "text": "There are many different kinds of models that you can build using transformers.",
    "start": "43440",
    "end": "47379"
  },
  {
    "text": "Some models take in audio and produce a transcript.",
    "start": "47800",
    "end": "50800"
  },
  {
    "text": "This sentence comes from a model going the other way around,",
    "start": "51340",
    "end": "54183"
  },
  {
    "text": "producing synthetic speech just from text.",
    "start": "54183",
    "end": "56219"
  },
  {
    "text": "All those tools that took the world by storm in 2022 like DALL-E and Midjourney",
    "start": "56660",
    "end": "61062"
  },
  {
    "text": "that take in a text description and produce an image are based on transformers.",
    "start": "61062",
    "end": "65520"
  },
  {
    "text": "Even if I can't quite get it to understand what a pi creature is supposed to be,",
    "start": "66000",
    "end": "69737"
  },
  {
    "text": "I'm still blown away that this kind of thing is even remotely possible.",
    "start": "69737",
    "end": "73100"
  },
  {
    "text": "And the original transformer introduced in 2017 by Google was invented for",
    "start": "73900",
    "end": "78000"
  },
  {
    "text": "the specific use case of translating text from one language into another.",
    "start": "78000",
    "end": "82100"
  },
  {
    "text": "But the variant that you and I will focus on, which is the type that",
    "start": "82660",
    "end": "86395"
  },
  {
    "text": "underlies tools like ChatGPT, will be a model that's trained to take in a piece of text,",
    "start": "86395",
    "end": "91284"
  },
  {
    "text": "maybe even with some surrounding images or sound accompanying it,",
    "start": "91284",
    "end": "94909"
  },
  {
    "text": "and produce a prediction for what comes next in the passage.",
    "start": "94909",
    "end": "98259"
  },
  {
    "text": "That prediction takes the form of a probability distribution",
    "start": "98600",
    "end": "101337"
  },
  {
    "text": "over many different chunks of text that might follow.",
    "start": "101337",
    "end": "103799"
  },
  {
    "text": "At first glance, you might think that predicting the next word",
    "start": "105040",
    "end": "107551"
  },
  {
    "text": "feels like a very different goal from generating new text.",
    "start": "107551",
    "end": "109940"
  },
  {
    "text": "But once you have a prediction model like this,",
    "start": "110180",
    "end": "112519"
  },
  {
    "text": "a simple thing you could try to make it generate, a longer piece of text,",
    "start": "112519",
    "end": "116201"
  },
  {
    "text": "is to give it an initial snippet to work with,",
    "start": "116202",
    "end": "118541"
  },
  {
    "text": "have it take a random sample from the distribution it just generated,",
    "start": "118541",
    "end": "122025"
  },
  {
    "text": "append that sample to the text, and then run the whole process again to make",
    "start": "122025",
    "end": "125857"
  },
  {
    "text": "a new prediction based on all the new text, including what it just added.",
    "start": "125857",
    "end": "129540"
  },
  {
    "text": "I don't know about you, but it really doesn't feel like this should actually work.",
    "start": "130100",
    "end": "133000"
  },
  {
    "text": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly",
    "start": "133420",
    "end": "137946"
  },
  {
    "text": "predict and sample the next chunk of text to generate a story based on the seed text.",
    "start": "137946",
    "end": "142420"
  },
  {
    "text": "The story just doesn't actually really make that much sense.",
    "start": "142420",
    "end": "146120"
  },
  {
    "text": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model,",
    "start": "146500",
    "end": "151293"
  },
  {
    "text": "just much bigger, suddenly almost magically we do get a sensible story,",
    "start": "151293",
    "end": "155451"
  },
  {
    "text": "one that even seems to infer that a pi creature would live in a land of math and",
    "start": "155451",
    "end": "160129"
  },
  {
    "text": "computation.",
    "start": "160129",
    "end": "160880"
  },
  {
    "text": "This process here of repeated prediction and sampling is essentially",
    "start": "161580",
    "end": "164931"
  },
  {
    "text": "what's happening when you interact with ChatGPT,",
    "start": "164931",
    "end": "167346"
  },
  {
    "text": "or any of these other large language models, and you see them producing",
    "start": "167346",
    "end": "170894"
  },
  {
    "text": "one word at a time.",
    "start": "170894",
    "end": "171879"
  },
  {
    "text": "In fact, one feature that I would very much enjoy is the ability to",
    "start": "172480",
    "end": "175849"
  },
  {
    "text": "see the underlying distribution for each new word that it chooses.",
    "start": "175850",
    "end": "179220"
  },
  {
    "start": "183000",
    "end": "396000"
  },
  {
    "text": "Let's kick things off with a very high level preview",
    "start": "183820",
    "end": "186258"
  },
  {
    "text": "of how data flows through a transformer.",
    "start": "186258",
    "end": "188180"
  },
  {
    "text": "We will spend much more time motivating and interpreting and expanding",
    "start": "188640",
    "end": "191964"
  },
  {
    "text": "on the details of each step, but in broad strokes,",
    "start": "191964",
    "end": "194386"
  },
  {
    "text": "when one of these chatbots generates a given word, here's what's going on under the hood.",
    "start": "194386",
    "end": "198660"
  },
  {
    "text": "First, the input is broken up into a bunch of little pieces.",
    "start": "199080",
    "end": "202040"
  },
  {
    "text": "These pieces are called tokens, and in the case of text these tend to be",
    "start": "202620",
    "end": "206220"
  },
  {
    "text": "words or little pieces of words or other common character combinations.",
    "start": "206220",
    "end": "209820"
  },
  {
    "text": "If images or sound are involved, then tokens could be little",
    "start": "210740",
    "end": "214077"
  },
  {
    "text": "patches of that image or little chunks of that sound.",
    "start": "214077",
    "end": "217080"
  },
  {
    "text": "Each one of these tokens is then associated with a vector, meaning some list of numbers,",
    "start": "217580",
    "end": "222206"
  },
  {
    "text": "which is meant to somehow encode the meaning of that piece.",
    "start": "222206",
    "end": "225360"
  },
  {
    "text": "If you think of these vectors as giving coordinates in some very high dimensional space,",
    "start": "225880",
    "end": "230089"
  },
  {
    "text": "words with similar meanings tend to land on vectors that are",
    "start": "230089",
    "end": "233006"
  },
  {
    "text": "close to each other in that space.",
    "start": "233006",
    "end": "234680"
  },
  {
    "text": "This sequence of vectors then passes through an operation that's",
    "start": "235280",
    "end": "238187"
  },
  {
    "text": "known as an attention block, and this allows the vectors to talk to",
    "start": "238187",
    "end": "241275"
  },
  {
    "text": "each other and pass information back and forth to update their values.",
    "start": "241275",
    "end": "244500"
  },
  {
    "text": "For example, the meaning of the word model in the phrase \"a machine learning",
    "start": "244880",
    "end": "248482"
  },
  {
    "text": "model\" is different from its meaning in the phrase \"a fashion model\".",
    "start": "248482",
    "end": "251799"
  },
  {
    "text": "The attention block is what's responsible for figuring out which",
    "start": "252260",
    "end": "255510"
  },
  {
    "text": "words in context are relevant to updating the meanings of which other words,",
    "start": "255510",
    "end": "259420"
  },
  {
    "text": "and how exactly those meanings should be updated.",
    "start": "259421",
    "end": "261959"
  },
  {
    "text": "And again, whenever I use the word meaning, this is",
    "start": "262500",
    "end": "265092"
  },
  {
    "text": "somehow entirely encoded in the entries of those vectors.",
    "start": "265092",
    "end": "268040"
  },
  {
    "text": "After that, these vectors pass through a different kind of operation,",
    "start": "269180",
    "end": "272276"
  },
  {
    "text": "and depending on the source that you're reading this will be referred",
    "start": "272276",
    "end": "275418"
  },
  {
    "text": "to as a multi-layer perceptron or maybe a feed-forward layer.",
    "start": "275418",
    "end": "278200"
  },
  {
    "text": "And here the vectors don't talk to each other,",
    "start": "278580",
    "end": "280495"
  },
  {
    "text": "they all go through the same operation in parallel.",
    "start": "280495",
    "end": "282660"
  },
  {
    "text": "And while this block is a little bit harder to interpret,",
    "start": "283060",
    "end": "285748"
  },
  {
    "text": "later on we'll talk about how the step is a little bit like asking a long list",
    "start": "285748",
    "end": "289473"
  },
  {
    "text": "of questions about each vector, and then updating them based on the answers",
    "start": "289473",
    "end": "293057"
  },
  {
    "text": "to those questions.",
    "start": "293057",
    "end": "294000"
  },
  {
    "text": "All of the operations in both of these blocks look like a",
    "start": "294900",
    "end": "298181"
  },
  {
    "text": "giant pile of matrix multiplications, and our primary job is",
    "start": "298181",
    "end": "301693"
  },
  {
    "text": "going to be to understand how to read the underlying matrices.",
    "start": "301693",
    "end": "305320"
  },
  {
    "text": "I'm glossing over some details about some normalization steps that happen in between,",
    "start": "306980",
    "end": "310933"
  },
  {
    "text": "but this is after all a high-level preview.",
    "start": "310933",
    "end": "312980"
  },
  {
    "text": "After that, the process essentially repeats, you go back and forth",
    "start": "313680",
    "end": "317237"
  },
  {
    "text": "between attention blocks and multi-layer perceptron blocks,",
    "start": "317237",
    "end": "320470"
  },
  {
    "text": "until at the very end the hope is that all of the essential meaning",
    "start": "320470",
    "end": "324135"
  },
  {
    "text": "of the passage has somehow been baked into the very last vector in the sequence.",
    "start": "324135",
    "end": "328500"
  },
  {
    "text": "We then perform a certain operation on that last vector that produces a probability",
    "start": "328920",
    "end": "333325"
  },
  {
    "text": "distribution over all possible tokens, all possible little chunks of text that might",
    "start": "333325",
    "end": "337836"
  },
  {
    "text": "come next.",
    "start": "337836",
    "end": "338420"
  },
  {
    "text": "And like I said, once you have a tool that predicts what comes next",
    "start": "338980",
    "end": "342318"
  },
  {
    "text": "given a snippet of text, you can feed it a little bit of seed text and",
    "start": "342318",
    "end": "345856"
  },
  {
    "text": "have it repeatedly play this game of predicting what comes next,",
    "start": "345856",
    "end": "349094"
  },
  {
    "text": "sampling from the distribution, appending it, and then repeating over and over.",
    "start": "349094",
    "end": "353080"
  },
  {
    "text": "Some of you in the know may remember how long before ChatGPT came into the scene,",
    "start": "353640",
    "end": "357944"
  },
  {
    "text": "this is what early demos of GPT-3 looked like,",
    "start": "357944",
    "end": "360442"
  },
  {
    "text": "you would have it autocomplete stories and essays based on an initial snippet.",
    "start": "360442",
    "end": "364639"
  },
  {
    "text": "To make a tool like this into a chatbot, the easiest starting point is to have a",
    "start": "365580",
    "end": "369819"
  },
  {
    "text": "little bit of text that establishes the setting of a user interacting with a",
    "start": "369820",
    "end": "373901"
  },
  {
    "text": "helpful AI assistant, what you would call the system prompt,",
    "start": "373901",
    "end": "377135"
  },
  {
    "text": "and then you would use the user's initial question or prompt as the first bit of",
    "start": "377135",
    "end": "381428"
  },
  {
    "text": "dialogue, and then you have it start predicting what such a helpful AI assistant",
    "start": "381428",
    "end": "385721"
  },
  {
    "text": "would say in response.",
    "start": "385721",
    "end": "386940"
  },
  {
    "text": "There is more to say about an added step of training that's required",
    "start": "387720",
    "end": "390974"
  },
  {
    "text": "to make this work well, but at a high level this is the idea.",
    "start": "390974",
    "end": "393940"
  },
  {
    "text": "In this chapter, you and I are going to expand on the details of what happens at the very",
    "start": "395720",
    "end": "399964"
  },
  {
    "start": "396000",
    "end": "440000"
  },
  {
    "text": "beginning of the network, at the very end of the network,",
    "start": "399964",
    "end": "402729"
  },
  {
    "text": "and I also want to spend a lot of time reviewing some important bits of background",
    "start": "402729",
    "end": "406687"
  },
  {
    "text": "knowledge, things that would have been second nature to any machine learning engineer by",
    "start": "406687",
    "end": "410931"
  },
  {
    "text": "the time transformers came around.",
    "start": "410931",
    "end": "412600"
  },
  {
    "text": "If you're comfortable with that background knowledge and a little impatient,",
    "start": "413060",
    "end": "416217"
  },
  {
    "text": "you could probably feel free to skip to the next chapter,",
    "start": "416217",
    "end": "418626"
  },
  {
    "text": "which is going to focus on the attention blocks,",
    "start": "418626",
    "end": "420662"
  },
  {
    "text": "generally considered the heart of the transformer.",
    "start": "420662",
    "end": "422780"
  },
  {
    "text": "After that, I want to talk more about these multi-layer perceptron blocks,",
    "start": "423360",
    "end": "426982"
  },
  {
    "text": "how training works, and a number of other details that will have been skipped up to",
    "start": "426982",
    "end": "431093"
  },
  {
    "text": "that point.",
    "start": "431093",
    "end": "431680"
  },
  {
    "text": "For broader context, these videos are additions to a mini-series about deep learning,",
    "start": "432180",
    "end": "436206"
  },
  {
    "text": "and it's okay if you haven't watched the previous ones,",
    "start": "436206",
    "end": "438858"
  },
  {
    "text": "I think you can do it out of order, but before diving into transformers specifically,",
    "start": "438858",
    "end": "442931"
  },
  {
    "start": "440000",
    "end": "747000"
  },
  {
    "text": "I do think it's worth making sure that we're on the same page about the basic premise",
    "start": "442931",
    "end": "447004"
  },
  {
    "text": "and structure of deep learning.",
    "start": "447004",
    "end": "448520"
  },
  {
    "text": "At the risk of stating the obvious, this is one approach to machine learning,",
    "start": "449020",
    "end": "453248"
  },
  {
    "text": "which describes any model where you're using data to somehow determine how a model",
    "start": "453248",
    "end": "457806"
  },
  {
    "text": "behaves.",
    "start": "457806",
    "end": "458300"
  },
  {
    "text": "What I mean by that is, let's say you want a function that takes in",
    "start": "459140",
    "end": "462475"
  },
  {
    "text": "an image and it produces a label describing it,",
    "start": "462475",
    "end": "464865"
  },
  {
    "text": "or our example of predicting the next word given a passage of text,",
    "start": "464865",
    "end": "468250"
  },
  {
    "text": "or any other task that seems to require some element of intuition",
    "start": "468250",
    "end": "471535"
  },
  {
    "text": "and pattern recognition.",
    "start": "471535",
    "end": "472780"
  },
  {
    "text": "We almost take this for granted these days, but the idea with machine learning is that",
    "start": "473200",
    "end": "477695"
  },
  {
    "text": "rather than trying to explicitly define a procedure for how to do that task in code,",
    "start": "477695",
    "end": "482138"
  },
  {
    "text": "which is what people would have done in the earliest days of AI,",
    "start": "482138",
    "end": "485535"
  },
  {
    "text": "instead you set up a very flexible structure with tunable parameters,",
    "start": "485535",
    "end": "489194"
  },
  {
    "text": "like a bunch of knobs and dials, and then, somehow,",
    "start": "489194",
    "end": "491912"
  },
  {
    "text": "you use many examples of what the output should look like for a given input to tweak",
    "start": "491912",
    "end": "496354"
  },
  {
    "text": "and tune the values of those parameters to mimic this behavior.",
    "start": "496355",
    "end": "499700"
  },
  {
    "text": "For example, maybe the simplest form of machine learning is linear regression,",
    "start": "499700",
    "end": "504117"
  },
  {
    "text": "where your inputs and outputs are each single numbers,",
    "start": "504117",
    "end": "507231"
  },
  {
    "text": "something like the square footage of a house and its price,",
    "start": "507231",
    "end": "510628"
  },
  {
    "text": "and what you want is to find a line of best fit through this data, you know,",
    "start": "510628",
    "end": "514987"
  },
  {
    "text": "to predict future house prices.",
    "start": "514988",
    "end": "516800"
  },
  {
    "text": "That line is described by two continuous parameters,",
    "start": "517440",
    "end": "520520"
  },
  {
    "text": "say the slope and the y-intercept, and the goal of linear",
    "start": "520520",
    "end": "523955"
  },
  {
    "text": "regression is to determine those parameters to closely match the data.",
    "start": "523955",
    "end": "528160"
  },
  {
    "text": "Needless to say, deep learning models get much more complicated.",
    "start": "528880",
    "end": "532100"
  },
  {
    "text": "GPT-3, for example, has not two, but 175 billion parameters.",
    "start": "532620",
    "end": "537660"
  },
  {
    "text": "But here's the thing, it's not a given that you can create some giant",
    "start": "538120",
    "end": "541951"
  },
  {
    "text": "model with a huge number of parameters without it either grossly",
    "start": "541952",
    "end": "545562"
  },
  {
    "text": "overfitting the training data or being completely intractable to train.",
    "start": "545562",
    "end": "549560"
  },
  {
    "text": "Deep learning describes a class of models that in the",
    "start": "550260",
    "end": "553087"
  },
  {
    "text": "last couple decades have proven to scale remarkably well.",
    "start": "553087",
    "end": "556180"
  },
  {
    "text": "What unifies them is that they all use the same training algorithm,",
    "start": "556480",
    "end": "559657"
  },
  {
    "text": "it's called backpropagation, we talked about it in previous chapters,",
    "start": "559658",
    "end": "562979"
  },
  {
    "text": "and the context that I want you to have as we go in is that in order for this",
    "start": "562979",
    "end": "566679"
  },
  {
    "text": "training algorithm to work well at scale, these models have to follow a certain",
    "start": "566679",
    "end": "570474"
  },
  {
    "text": "specific format.",
    "start": "570474",
    "end": "571280"
  },
  {
    "text": "And if you know this format going in, it helps to explain many of the choices for how a",
    "start": "571800",
    "end": "576050"
  },
  {
    "text": "transformer processes language, which otherwise run the risk of feeling kinda arbitrary.",
    "start": "576051",
    "end": "580400"
  },
  {
    "text": "First, whatever kind of model you're making, the",
    "start": "581440",
    "end": "583910"
  },
  {
    "text": "input has to be formatted as an array of real numbers.",
    "start": "583910",
    "end": "586740"
  },
  {
    "text": "This could simply mean a list of numbers, it could be a two-dimensional array,",
    "start": "586740",
    "end": "590938"
  },
  {
    "text": "or very often you deal with higher dimensional arrays,",
    "start": "590939",
    "end": "593900"
  },
  {
    "text": "where the general term used is tensor.",
    "start": "593900",
    "end": "596000"
  },
  {
    "text": "You often think of that input data as being progressively transformed into many",
    "start": "596560",
    "end": "600516"
  },
  {
    "text": "distinct layers, where again, each layer is always structured as some kind of",
    "start": "600517",
    "end": "604423"
  },
  {
    "text": "array of real numbers, until you get to a final layer which you consider the output.",
    "start": "604423",
    "end": "608680"
  },
  {
    "text": "For example, the final layer in our text processing model is a list of numbers",
    "start": "609280",
    "end": "613326"
  },
  {
    "text": "representing the probability distribution for all possible next tokens.",
    "start": "613326",
    "end": "617060"
  },
  {
    "text": "In deep learning, these model parameters are almost always referred to as weights,",
    "start": "617820",
    "end": "622035"
  },
  {
    "text": "and this is because a key feature of these models is that the only way these",
    "start": "622035",
    "end": "625993"
  },
  {
    "text": "parameters interact with the data being processed is through weighted sums.",
    "start": "625993",
    "end": "629899"
  },
  {
    "text": "You also sprinkle some non-linear functions throughout,",
    "start": "630340",
    "end": "632743"
  },
  {
    "text": "but they won't depend on parameters.",
    "start": "632743",
    "end": "634360"
  },
  {
    "text": "Typically, though, instead of seeing the weighted sums all naked",
    "start": "635200",
    "end": "638620"
  },
  {
    "text": "and written out explicitly like this, you'll instead find them",
    "start": "638620",
    "end": "641986"
  },
  {
    "text": "packaged together as various components in a matrix vector product.",
    "start": "641986",
    "end": "645620"
  },
  {
    "text": "It amounts to saying the same thing, if you think back to how matrix vector",
    "start": "646740",
    "end": "650416"
  },
  {
    "text": "multiplication works, each component in the output looks like a weighted sum.",
    "start": "650416",
    "end": "654240"
  },
  {
    "text": "It's just often conceptually cleaner for you and me to think",
    "start": "654780",
    "end": "658250"
  },
  {
    "text": "about matrices that are filled with tunable parameters that",
    "start": "658250",
    "end": "661719"
  },
  {
    "text": "transform vectors that are drawn from the data being processed.",
    "start": "661719",
    "end": "665420"
  },
  {
    "text": "For example, those 175 billion weights in GPT-3 are",
    "start": "666340",
    "end": "670212"
  },
  {
    "text": "organized into just under 28,000 distinct matrices.",
    "start": "670212",
    "end": "674160"
  },
  {
    "text": "Those matrices in turn fall into eight different categories,",
    "start": "674660",
    "end": "677417"
  },
  {
    "text": "and what you and I are going to do is step through each one of those categories to",
    "start": "677417",
    "end": "681230"
  },
  {
    "text": "understand what that type does.",
    "start": "681230",
    "end": "682699"
  },
  {
    "text": "As we go through, I think it's kind of fun to reference the specific",
    "start": "683160",
    "end": "687087"
  },
  {
    "text": "numbers from GPT-3 to count up exactly where those 175 billion come from.",
    "start": "687087",
    "end": "691360"
  },
  {
    "text": "Even if nowadays there are bigger and better models,",
    "start": "691880",
    "end": "694411"
  },
  {
    "text": "this one has a certain charm as the first large-language",
    "start": "694411",
    "end": "697186"
  },
  {
    "text": "model to really capture the world's attention outside of ML communities.",
    "start": "697186",
    "end": "700740"
  },
  {
    "text": "Also, practically speaking, companies tend to keep much tighter",
    "start": "701440",
    "end": "704177"
  },
  {
    "text": "lips around the specific numbers for more modern networks.",
    "start": "704177",
    "end": "706740"
  },
  {
    "text": "I just want to set the scene going in, that as you peek under the",
    "start": "707360",
    "end": "710703"
  },
  {
    "text": "hood to see what happens inside a tool like ChatGPT,",
    "start": "710703",
    "end": "713429"
  },
  {
    "text": "almost all of the actual computation looks like matrix vector multiplication.",
    "start": "713429",
    "end": "717440"
  },
  {
    "text": "There's a little bit of a risk getting lost in the sea of billions of numbers,",
    "start": "717900",
    "end": "721883"
  },
  {
    "text": "but you should draw a very sharp distinction in your mind between",
    "start": "721883",
    "end": "725253"
  },
  {
    "text": "the weights of the model, which I'll always color in blue or red,",
    "start": "725253",
    "end": "728623"
  },
  {
    "text": "and the data being processed, which I'll always color in gray.",
    "start": "728623",
    "end": "731839"
  },
  {
    "text": "The weights are the actual brains, they are the things learned during training,",
    "start": "732180",
    "end": "736158"
  },
  {
    "text": "and they determine how it behaves.",
    "start": "736158",
    "end": "737920"
  },
  {
    "text": "The data being processed simply encodes whatever specific input is",
    "start": "738280",
    "end": "742299"
  },
  {
    "text": "fed into the model for a given run, like an example snippet of text.",
    "start": "742299",
    "end": "746500"
  },
  {
    "start": "747000",
    "end": "1105000"
  },
  {
    "text": "With all of that as foundation, let's dig into the first step of this text processing",
    "start": "747480",
    "end": "751701"
  },
  {
    "text": "example, which is to break up the input into little chunks and turn those chunks into",
    "start": "751702",
    "end": "755973"
  },
  {
    "text": "vectors.",
    "start": "755973",
    "end": "756420"
  },
  {
    "text": "I mentioned how those chunks are called tokens,",
    "start": "757020",
    "end": "759261"
  },
  {
    "text": "which might be pieces of words or punctuation,",
    "start": "759261",
    "end": "761501"
  },
  {
    "text": "but every now and then in this chapter and especially in the next one,",
    "start": "761501",
    "end": "764886"
  },
  {
    "text": "I'd like to just pretend that it's broken more cleanly into words.",
    "start": "764886",
    "end": "768079"
  },
  {
    "text": "Because we humans think in words, this will just make it much",
    "start": "768600",
    "end": "771386"
  },
  {
    "text": "easier to reference little examples and clarify each step.",
    "start": "771386",
    "end": "774079"
  },
  {
    "text": "The model has a predefined vocabulary, some list of all possible words,",
    "start": "775260",
    "end": "779420"
  },
  {
    "text": "say 50,000 of them, and the first matrix that we'll encounter,",
    "start": "779420",
    "end": "783112"
  },
  {
    "text": "known as the embedding matrix, has a single column for each one of these words.",
    "start": "783112",
    "end": "787800"
  },
  {
    "text": "These columns are what determines what vector each word turns into in that first step.",
    "start": "788940",
    "end": "793760"
  },
  {
    "text": "We label it We, and like all the matrices we see,",
    "start": "795100",
    "end": "798040"
  },
  {
    "text": "its values begin random, but they're going to be learned based on data.",
    "start": "798040",
    "end": "802360"
  },
  {
    "text": "Turning words into vectors was common practice in machine learning long before",
    "start": "803620",
    "end": "807378"
  },
  {
    "text": "transformers, but it's a little weird if you've never seen it before,",
    "start": "807378",
    "end": "810750"
  },
  {
    "text": "and it sets the foundation for everything that follows,",
    "start": "810750",
    "end": "813448"
  },
  {
    "text": "so let's take a moment to get familiar with it.",
    "start": "813448",
    "end": "815760"
  },
  {
    "text": "We often call this embedding a word, which invites you to think of these",
    "start": "816040",
    "end": "819910"
  },
  {
    "text": "vectors very geometrically as points in some high dimensional space.",
    "start": "819911",
    "end": "823620"
  },
  {
    "text": "Visualizing a list of three numbers as coordinates for points in 3D space would",
    "start": "824180",
    "end": "828054"
  },
  {
    "text": "be no problem, but word embeddings tend to be much much higher dimensional.",
    "start": "828054",
    "end": "831780"
  },
  {
    "text": "In GPT-3 they have 12,288 dimensions, and as you'll see,",
    "start": "832280",
    "end": "835936"
  },
  {
    "text": "it matters to work in a space that has a lot of distinct directions.",
    "start": "835936",
    "end": "840440"
  },
  {
    "text": "In the same way that you could take a two-dimensional slice through a 3D space",
    "start": "841180",
    "end": "845060"
  },
  {
    "text": "and project all the points onto that slice, for the sake of animating word",
    "start": "845060",
    "end": "848791"
  },
  {
    "text": "embeddings that a simple model is giving me, I'm going to do an analogous",
    "start": "848791",
    "end": "852471"
  },
  {
    "text": "thing by choosing a three-dimensional slice through this very high dimensional space,",
    "start": "852471",
    "end": "856749"
  },
  {
    "text": "and projecting the word vectors down onto that and displaying the results.",
    "start": "856749",
    "end": "860480"
  },
  {
    "text": "The big idea here is that as a model tweaks and tunes its weights to determine",
    "start": "861280",
    "end": "865522"
  },
  {
    "text": "how exactly words get embedded as vectors during training,",
    "start": "865522",
    "end": "868730"
  },
  {
    "text": "it tends to settle on a set of embeddings where directions in the space have a",
    "start": "868730",
    "end": "873026"
  },
  {
    "text": "kind of semantic meaning.",
    "start": "873026",
    "end": "874440"
  },
  {
    "text": "For the simple word-to-vector model I'm running here,",
    "start": "874980",
    "end": "877790"
  },
  {
    "text": "if I run a search for all the words whose embeddings are closest to that of tower,",
    "start": "877790",
    "end": "882188"
  },
  {
    "text": "you'll notice how they all seem to give very similar tower-ish vibes.",
    "start": "882189",
    "end": "885900"
  },
  {
    "text": "And if you want to pull up some Python and play along at home,",
    "start": "886340",
    "end": "888781"
  },
  {
    "text": "this is the specific model that I'm using to make the animations.",
    "start": "888781",
    "end": "891380"
  },
  {
    "text": "It's not a transformer, but it's enough to illustrate the",
    "start": "891620",
    "end": "894484"
  },
  {
    "text": "idea that directions in the space can carry semantic meaning.",
    "start": "894484",
    "end": "897600"
  },
  {
    "text": "A very classic example of this is how if you take the difference between",
    "start": "898300",
    "end": "902187"
  },
  {
    "text": "the vectors for woman and man, something you would visualize as a",
    "start": "902187",
    "end": "905750"
  },
  {
    "text": "little vector in the space connecting the tip of one to the tip of the other,",
    "start": "905750",
    "end": "909961"
  },
  {
    "text": "it's very similar to the difference between king and queen.",
    "start": "909961",
    "end": "913200"
  },
  {
    "text": "So let's say you didn't know the word for a female monarch,",
    "start": "915080",
    "end": "918355"
  },
  {
    "text": "you could find it by taking king, adding this woman minus man direction,",
    "start": "918355",
    "end": "922407"
  },
  {
    "text": "and searching for the embedding closest to that point.",
    "start": "922407",
    "end": "925459"
  },
  {
    "text": "At least, kind of.",
    "start": "927000",
    "end": "928200"
  },
  {
    "text": "Despite this being a classic example for the model I'm playing with,",
    "start": "928480",
    "end": "931773"
  },
  {
    "text": "the true embedding of queen is actually a little farther off than this would suggest,",
    "start": "931773",
    "end": "935937"
  },
  {
    "text": "presumably because the way queen is used in training data is not merely a feminine",
    "start": "935937",
    "end": "939957"
  },
  {
    "text": "version of king.",
    "start": "939957",
    "end": "940779"
  },
  {
    "text": "When I played around, family relations seemed to illustrate the idea much better.",
    "start": "941620",
    "end": "945260"
  },
  {
    "text": "The point is, it looks like during training the model found it advantageous to",
    "start": "946340",
    "end": "950461"
  },
  {
    "text": "choose embeddings such that one direction in this space encodes gender information.",
    "start": "950461",
    "end": "954900"
  },
  {
    "text": "Another example is that if you take the embedding of Italy,",
    "start": "956800",
    "end": "960081"
  },
  {
    "text": "and you subtract the embedding of Germany, and add that to the embedding of Hitler,",
    "start": "960081",
    "end": "964753"
  },
  {
    "text": "you get something very close to the embedding of Mussolini.",
    "start": "964753",
    "end": "968089"
  },
  {
    "text": "It's as if the model learned to associate some directions with Italian-ness,",
    "start": "968570",
    "end": "973431"
  },
  {
    "text": "and others with WWII axis leaders.",
    "start": "973431",
    "end": "975670"
  },
  {
    "text": "Maybe my favorite example in this vein is how in some models,",
    "start": "976470",
    "end": "979931"
  },
  {
    "text": "if you take the difference between Germany and Japan, and add it to sushi,",
    "start": "979931",
    "end": "984187"
  },
  {
    "text": "you end up very close to bratwurst.",
    "start": "984187",
    "end": "986230"
  },
  {
    "text": "Also in playing this game of finding nearest neighbors,",
    "start": "987350",
    "end": "990186"
  },
  {
    "text": "I was very pleased to see how close cat was to both beast and monster.",
    "start": "990187",
    "end": "993850"
  },
  {
    "text": "One bit of mathematical intuition that's helpful to have in mind,",
    "start": "994690",
    "end": "997743"
  },
  {
    "text": "especially for the next chapter, is how the dot product of two",
    "start": "997743",
    "end": "1000703"
  },
  {
    "text": "vectors can be thought of as a way to measure how well they align.",
    "start": "1000703",
    "end": "1003850"
  },
  {
    "text": "Computationally, dot products involve multiplying all the",
    "start": "1004870",
    "end": "1007693"
  },
  {
    "text": "corresponding components and then adding the results, which is good,",
    "start": "1007693",
    "end": "1011111"
  },
  {
    "text": "since so much of our computation has to look like weighted sums.",
    "start": "1011111",
    "end": "1014330"
  },
  {
    "text": "Geometrically, the dot product is positive when vectors point in similar directions,",
    "start": "1015190",
    "end": "1019999"
  },
  {
    "text": "it's zero if they're perpendicular, and it's negative whenever",
    "start": "1019999",
    "end": "1023606"
  },
  {
    "text": "they point in opposite directions.",
    "start": "1023606",
    "end": "1025610"
  },
  {
    "text": "For example, let's say you were playing with this model,",
    "start": "1026550",
    "end": "1029916"
  },
  {
    "text": "and you hypothesize that the embedding of cats minus cat might represent a sort of",
    "start": "1029916",
    "end": "1034906"
  },
  {
    "text": "plurality direction in this space.",
    "start": "1034906",
    "end": "1037010"
  },
  {
    "text": "To test this, I'm going to take this vector and compute its dot",
    "start": "1037430",
    "end": "1040569"
  },
  {
    "text": "product against the embeddings of certain singular nouns,",
    "start": "1040570",
    "end": "1043460"
  },
  {
    "text": "and compare it to the dot products with the corresponding plural nouns.",
    "start": "1043461",
    "end": "1047050"
  },
  {
    "text": "If you play around with this, you'll notice that the plural ones",
    "start": "1047270",
    "end": "1050219"
  },
  {
    "text": "do indeed seem to consistently give higher values than the singular ones,",
    "start": "1050219",
    "end": "1053628"
  },
  {
    "text": "indicating that they align more with this direction.",
    "start": "1053628",
    "end": "1056070"
  },
  {
    "text": "It's also fun how if you take this dot product with the embeddings of the words one,",
    "start": "1057070",
    "end": "1061616"
  },
  {
    "text": "two, three, and so on, they give increasing values,",
    "start": "1061616",
    "end": "1064429"
  },
  {
    "text": "so it's as if we can quantitatively measure how plural the model finds a given word.",
    "start": "1064430",
    "end": "1069030"
  },
  {
    "text": "Again, the specifics for how words get embedded is learned using data.",
    "start": "1070250",
    "end": "1073570"
  },
  {
    "text": "This embedding matrix, whose columns tell us what happens to each word,",
    "start": "1074050",
    "end": "1077475"
  },
  {
    "text": "is the first pile of weights in our model.",
    "start": "1077475",
    "end": "1079549"
  },
  {
    "text": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257,",
    "start": "1080030",
    "end": "1084727"
  },
  {
    "text": "and again, technically this consists not of words per se, but of tokens.",
    "start": "1084727",
    "end": "1089770"
  },
  {
    "text": "The embedding dimension is 12,288, and multiplying those",
    "start": "1090630",
    "end": "1094309"
  },
  {
    "text": "tells us this consists of about 617 million weights.",
    "start": "1094309",
    "end": "1097789"
  },
  {
    "text": "Let's go ahead and add this to a running tally,",
    "start": "1098250",
    "end": "1100626"
  },
  {
    "text": "remembering that by the end we should count up to 175 billion.",
    "start": "1100626",
    "end": "1103809"
  },
  {
    "start": "1105000",
    "end": "1222000"
  },
  {
    "text": "In the case of transformers, you really want to think of the vectors",
    "start": "1105430",
    "end": "1108756"
  },
  {
    "text": "in this embedding space as not merely representing individual words.",
    "start": "1108756",
    "end": "1112130"
  },
  {
    "text": "For one thing, they also encode information about the position of that word,",
    "start": "1112550",
    "end": "1116513"
  },
  {
    "text": "which we'll talk about later, but more importantly,",
    "start": "1116513",
    "end": "1119224"
  },
  {
    "text": "you should think of them as having the capacity to soak in context.",
    "start": "1119224",
    "end": "1122770"
  },
  {
    "text": "A vector that started its life as the embedding of the word king, for example,",
    "start": "1123350",
    "end": "1127408"
  },
  {
    "text": "might progressively get tugged and pulled by various blocks in this network,",
    "start": "1127408",
    "end": "1131413"
  },
  {
    "text": "so that by the end it points in a much more specific and nuanced direction that",
    "start": "1131413",
    "end": "1135575"
  },
  {
    "text": "somehow encodes that it was a king who lived in Scotland,",
    "start": "1135575",
    "end": "1138591"
  },
  {
    "text": "and who had achieved his post after murdering the previous king,",
    "start": "1138592",
    "end": "1141973"
  },
  {
    "text": "and who's being described in Shakespearean language.",
    "start": "1141973",
    "end": "1144730"
  },
  {
    "text": "Think about your own understanding of a given word.",
    "start": "1145210",
    "end": "1147789"
  },
  {
    "text": "The meaning of that word is clearly informed by the surroundings,",
    "start": "1148250",
    "end": "1151727"
  },
  {
    "text": "and sometimes this includes context from a long distance away,",
    "start": "1151727",
    "end": "1155098"
  },
  {
    "text": "so in putting together a model that has the ability to predict what word comes next,",
    "start": "1155098",
    "end": "1159645"
  },
  {
    "text": "the goal is to somehow empower it to incorporate context efficiently.",
    "start": "1159645",
    "end": "1163390"
  },
  {
    "text": "To be clear, in that very first step, when you create the array of",
    "start": "1164050",
    "end": "1167136"
  },
  {
    "text": "vectors based on the input text, each one of those is simply plucked",
    "start": "1167136",
    "end": "1170363"
  },
  {
    "text": "out of the embedding matrix, so initially each one can only encode",
    "start": "1170363",
    "end": "1173496"
  },
  {
    "text": "the meaning of a single word without any input from its surroundings.",
    "start": "1173496",
    "end": "1176770"
  },
  {
    "text": "But you should think of the primary goal of this network that it flows through",
    "start": "1177710",
    "end": "1181562"
  },
  {
    "text": "as being to enable each one of those vectors to soak up a meaning that's much",
    "start": "1181562",
    "end": "1185414"
  },
  {
    "text": "more rich and specific than what mere individual words could represent.",
    "start": "1185414",
    "end": "1188970"
  },
  {
    "text": "The network can only process a fixed number of vectors at a time,",
    "start": "1189510",
    "end": "1192802"
  },
  {
    "text": "known as its context size.",
    "start": "1192802",
    "end": "1194170"
  },
  {
    "text": "For GPT-3 it was trained with a context size of 2048,",
    "start": "1194510",
    "end": "1197672"
  },
  {
    "text": "so the data flowing through the network always looks like this array of 2048 columns,",
    "start": "1197672",
    "end": "1202803"
  },
  {
    "text": "each of which has 12,000 dimensions.",
    "start": "1202803",
    "end": "1205010"
  },
  {
    "text": "This context size limits how much text the transformer can",
    "start": "1205590",
    "end": "1208657"
  },
  {
    "text": "incorporate when it's making a prediction of the next word.",
    "start": "1208657",
    "end": "1211830"
  },
  {
    "text": "This is why long conversations with certain chatbots,",
    "start": "1212370",
    "end": "1215042"
  },
  {
    "text": "like the early versions of ChatGPT, often gave the feeling of",
    "start": "1215042",
    "end": "1218168"
  },
  {
    "text": "the bot kind of losing the thread of conversation as you continued too long.",
    "start": "1218168",
    "end": "1222049"
  },
  {
    "start": "1222000",
    "end": "1342000"
  },
  {
    "text": "We'll go into the details of attention in due time,",
    "start": "1223030",
    "end": "1225230"
  },
  {
    "text": "but skipping ahead I want to talk for a minute about what happens at the very end.",
    "start": "1225230",
    "end": "1228809"
  },
  {
    "text": "Remember, the desired output is a probability",
    "start": "1229450",
    "end": "1231991"
  },
  {
    "text": "distribution over all tokens that might come next.",
    "start": "1231991",
    "end": "1234870"
  },
  {
    "text": "For example, if the very last word is Professor,",
    "start": "1235170",
    "end": "1237759"
  },
  {
    "text": "and the context includes words like Harry Potter,",
    "start": "1237759",
    "end": "1240456"
  },
  {
    "text": "and immediately preceding we see least favorite teacher,",
    "start": "1240456",
    "end": "1243531"
  },
  {
    "text": "and also if you give me some leeway by letting me pretend that tokens simply",
    "start": "1243531",
    "end": "1247685"
  },
  {
    "text": "look like full words, then a well-trained network that had built up knowledge",
    "start": "1247685",
    "end": "1251892"
  },
  {
    "text": "of Harry Potter would presumably assign a high number to the word Snape.",
    "start": "1251892",
    "end": "1255830"
  },
  {
    "text": "This involves two different steps.",
    "start": "1256510",
    "end": "1257970"
  },
  {
    "text": "The first one is to use another matrix that maps the very last vector in that",
    "start": "1258310",
    "end": "1263052"
  },
  {
    "text": "context to a list of 50,000 values, one for each token in the vocabulary.",
    "start": "1263052",
    "end": "1267610"
  },
  {
    "text": "Then there's a function that normalizes this into a probability distribution,",
    "start": "1268170",
    "end": "1272173"
  },
  {
    "text": "it's called softmax and we'll talk more about it in just a second,",
    "start": "1272173",
    "end": "1275657"
  },
  {
    "text": "but before that it might seem a little bit weird to only use this last embedding",
    "start": "1275657",
    "end": "1279868"
  },
  {
    "text": "to make a prediction, when after all in that last step there are thousands of",
    "start": "1279868",
    "end": "1283923"
  },
  {
    "text": "other vectors in the layer just sitting there with their own context-rich meanings.",
    "start": "1283923",
    "end": "1288289"
  },
  {
    "text": "This has to do with the fact that in the training process it turns out to be",
    "start": "1288930",
    "end": "1292677"
  },
  {
    "text": "much more efficient if you use each one of those vectors in the final layer",
    "start": "1292677",
    "end": "1296424"
  },
  {
    "text": "to simultaneously make a prediction for what would come immediately after it.",
    "start": "1296424",
    "end": "1300269"
  },
  {
    "text": "There's a lot more to be said about training later on,",
    "start": "1300970",
    "end": "1303240"
  },
  {
    "text": "but I just want to call that out right now.",
    "start": "1303240",
    "end": "1305090"
  },
  {
    "text": "This matrix is called the Unembedding matrix and we give it the label WU.",
    "start": "1305730",
    "end": "1309690"
  },
  {
    "text": "Again, like all the weight matrices we see, its entries begin at random,",
    "start": "1310210",
    "end": "1313574"
  },
  {
    "text": "but they are learned during the training process.",
    "start": "1313574",
    "end": "1315910"
  },
  {
    "text": "Keeping score on our total parameter count, this Unembedding",
    "start": "1316470",
    "end": "1319447"
  },
  {
    "text": "matrix has one row for each word in the vocabulary,",
    "start": "1319447",
    "end": "1322028"
  },
  {
    "text": "and each row has the same number of elements as the embedding dimension.",
    "start": "1322028",
    "end": "1325649"
  },
  {
    "text": "It's very similar to the embedding matrix, just with the order swapped,",
    "start": "1326410",
    "end": "1330381"
  },
  {
    "text": "so it adds another 617 million parameters to the network,",
    "start": "1330381",
    "end": "1333625"
  },
  {
    "text": "meaning our count so far is a little over a billion,",
    "start": "1333625",
    "end": "1336589"
  },
  {
    "text": "a small but not wholly insignificant fraction of the 175 billion",
    "start": "1336589",
    "end": "1340224"
  },
  {
    "text": "we'll end up with in total.",
    "start": "1340224",
    "end": "1341790"
  },
  {
    "start": "1342000",
    "end": "1563000"
  },
  {
    "text": "As the very last mini-lesson for this chapter,",
    "start": "1342550",
    "end": "1344657"
  },
  {
    "text": "I want to talk more about this softmax function,",
    "start": "1344657",
    "end": "1346901"
  },
  {
    "text": "since it makes another appearance for us once we dive into the attention blocks.",
    "start": "1346901",
    "end": "1350610"
  },
  {
    "text": "The idea is that if you want a sequence of numbers to act as a probability distribution,",
    "start": "1351430",
    "end": "1356554"
  },
  {
    "text": "say a distribution over all possible next words,",
    "start": "1356554",
    "end": "1359408"
  },
  {
    "text": "then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
    "start": "1359408",
    "end": "1364590"
  },
  {
    "text": "However, if you're playing the deep learning game where everything you do looks like",
    "start": "1365250",
    "end": "1369892"
  },
  {
    "text": "matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
    "start": "1369892",
    "end": "1374809"
  },
  {
    "text": "The values are often negative, or much bigger than 1,",
    "start": "1375330",
    "end": "1377784"
  },
  {
    "text": "and they almost certainly don't add up to 1.",
    "start": "1377785",
    "end": "1379870"
  },
  {
    "text": "Softmax is the standard way to turn an arbitrary list of numbers",
    "start": "1380510",
    "end": "1384030"
  },
  {
    "text": "into a valid distribution in such a way that the largest values end up closest to 1,",
    "start": "1384030",
    "end": "1388705"
  },
  {
    "text": "and the smaller values end up very close to 0.",
    "start": "1388705",
    "end": "1391289"
  },
  {
    "text": "That's all you really need to know.",
    "start": "1391830",
    "end": "1393070"
  },
  {
    "text": "But if you're curious, the way it works is to first raise e to the power",
    "start": "1393090",
    "end": "1397128"
  },
  {
    "text": "of each of the numbers, which means you now have a list of positive values,",
    "start": "1397129",
    "end": "1401392"
  },
  {
    "text": "and then you can take the sum of all those positive values and divide each",
    "start": "1401392",
    "end": "1405599"
  },
  {
    "text": "term by that sum, which normalizes it into a list that adds up to 1.",
    "start": "1405599",
    "end": "1409470"
  },
  {
    "text": "You'll notice that if one of the numbers in the input is meaningfully bigger than the",
    "start": "1410170",
    "end": "1414286"
  },
  {
    "text": "rest, then in the output the corresponding term dominates the distribution,",
    "start": "1414286",
    "end": "1417966"
  },
  {
    "text": "so if you were sampling from it you'd almost certainly just be picking the maximizing",
    "start": "1417966",
    "end": "1422131"
  },
  {
    "text": "input.",
    "start": "1422131",
    "end": "1422470"
  },
  {
    "text": "But it's softer than just picking the max in the sense that when other values",
    "start": "1422990",
    "end": "1426998"
  },
  {
    "text": "are similarly large, they also get meaningful weight in the distribution,",
    "start": "1426998",
    "end": "1430850"
  },
  {
    "text": "and everything changes continuously as you continuously vary the inputs.",
    "start": "1430850",
    "end": "1434650"
  },
  {
    "text": "In some situations, like when ChatGPT is using this distribution to create a next word,",
    "start": "1435130",
    "end": "1439984"
  },
  {
    "text": "there's room for a little bit of extra fun by adding a little extra spice into this",
    "start": "1439984",
    "end": "1444670"
  },
  {
    "text": "function, with a constant T thrown into the denominator of those exponents.",
    "start": "1444670",
    "end": "1448910"
  },
  {
    "text": "We call it the temperature, since it vaguely resembles the role of temperature in",
    "start": "1449550",
    "end": "1454000"
  },
  {
    "text": "certain thermodynamics equations, and the effect is that when T is larger,",
    "start": "1454000",
    "end": "1458121"
  },
  {
    "text": "you give more weight to the lower values, meaning the distribution is a little bit",
    "start": "1458121",
    "end": "1462681"
  },
  {
    "text": "more uniform, and if T is smaller, then the bigger values will dominate more",
    "start": "1462681",
    "end": "1466911"
  },
  {
    "text": "aggressively, where in the extreme, setting T equal to zero means all of the weight",
    "start": "1466911",
    "end": "1471526"
  },
  {
    "text": "goes to maximum value.",
    "start": "1471526",
    "end": "1472790"
  },
  {
    "text": "For example, I'll have GPT-3 generate a story with the seed text,",
    "start": "1473470",
    "end": "1477662"
  },
  {
    "text": "\"once upon a time there was A\", but I'll use different temperatures in each case.",
    "start": "1477662",
    "end": "1482950"
  },
  {
    "text": "Temperature zero means that it always goes with the most predictable word,",
    "start": "1483630",
    "end": "1488283"
  },
  {
    "text": "and what you get ends up being a trite derivative of Goldilocks.",
    "start": "1488283",
    "end": "1492370"
  },
  {
    "text": "A higher temperature gives it a chance to choose less likely words,",
    "start": "1493010",
    "end": "1496540"
  },
  {
    "text": "but it comes with a risk.",
    "start": "1496540",
    "end": "1497910"
  },
  {
    "text": "In this case, the story starts out more originally,",
    "start": "1498230",
    "end": "1501148"
  },
  {
    "text": "about a young web artist from South Korea, but it quickly degenerates into nonsense.",
    "start": "1501148",
    "end": "1506010"
  },
  {
    "text": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
    "start": "1506950",
    "end": "1510830"
  },
  {
    "text": "There's no mathematical reason for this, it's just an arbitrary constraint imposed",
    "start": "1511170",
    "end": "1515336"
  },
  {
    "text": "to keep their tool from being seen generating things that are too nonsensical.",
    "start": "1515336",
    "end": "1519350"
  },
  {
    "text": "So if you're curious, the way this animation is actually working is I'm taking the",
    "start": "1519870",
    "end": "1524272"
  },
  {
    "text": "20 most probable next tokens that GPT-3 generates,",
    "start": "1524272",
    "end": "1527011"
  },
  {
    "text": "which seems to be the maximum they'll give me,",
    "start": "1527011",
    "end": "1529533"
  },
  {
    "text": "and then I tweak the probabilities based on an exponent of 1/5.",
    "start": "1529534",
    "end": "1532970"
  },
  {
    "text": "As another bit of jargon, in the same way that you might call the components of",
    "start": "1533130",
    "end": "1537434"
  },
  {
    "text": "the output of this function probabilities, people often refer to the inputs as logits,",
    "start": "1537434",
    "end": "1542173"
  },
  {
    "text": "or some people say logits, some people say logits, I'm gonna say logits.",
    "start": "1542173",
    "end": "1546150"
  },
  {
    "text": "So for instance, when you feed in some text, you have all these word embeddings",
    "start": "1546530",
    "end": "1550417"
  },
  {
    "text": "flow through the network, and you do this final multiplication with the",
    "start": "1550417",
    "end": "1553960"
  },
  {
    "text": "unembedding matrix, machine learning people would refer to the components in that raw,",
    "start": "1553960",
    "end": "1558241"
  },
  {
    "text": "unnormalized output as the logits for the next word prediction.",
    "start": "1558241",
    "end": "1561390"
  },
  {
    "start": "1563000",
    "end": "1634000"
  },
  {
    "text": "A lot of the goal with this chapter was to lay the foundations for",
    "start": "1563330",
    "end": "1566696"
  },
  {
    "text": "understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
    "start": "1566697",
    "end": "1570370"
  },
  {
    "text": "You see, if you have a strong intuition for word embeddings, for softmax,",
    "start": "1570850",
    "end": "1574890"
  },
  {
    "text": "for how dot products measure similarity, and also the underlying premise that",
    "start": "1574890",
    "end": "1579206"
  },
  {
    "text": "most of the calculations have to look like matrix multiplication with matrices",
    "start": "1579206",
    "end": "1583577"
  },
  {
    "text": "full of tunable parameters, then understanding the attention mechanism,",
    "start": "1583577",
    "end": "1587562"
  },
  {
    "text": "this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
    "start": "1587562",
    "end": "1592210"
  },
  {
    "text": "For that, come join me in the next chapter.",
    "start": "1592650",
    "end": "1594510"
  },
  {
    "text": "As I'm publishing this, a draft of that next chapter",
    "start": "1596390",
    "end": "1598922"
  },
  {
    "text": "is available for review by Patreon supporters.",
    "start": "1598922",
    "end": "1601210"
  },
  {
    "text": "A final version should be up in public in a week or two,",
    "start": "1601770",
    "end": "1604239"
  },
  {
    "text": "it usually depends on how much I end up changing based on that review.",
    "start": "1604239",
    "end": "1607370"
  },
  {
    "text": "In the meantime, if you want to dive into attention,",
    "start": "1607810",
    "end": "1609708"
  },
  {
    "text": "and if you want to help the channel out a little bit, it's there waiting.",
    "start": "1609708",
    "end": "1612410"
  }
]