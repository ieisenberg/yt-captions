[
  {
    "start": "0",
    "end": "67000"
  },
  {
    "text": "This is a 3.",
    "start": "4220",
    "end": "5399"
  },
  {
    "text": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,",
    "start": "6060",
    "end": "10712"
  },
  {
    "text": "but your brain has no trouble recognizing it as a 3.",
    "start": "10713",
    "end": "13720"
  },
  {
    "text": "And I want you to take a moment to appreciate how",
    "start": "14340",
    "end": "16559"
  },
  {
    "text": "crazy it is that brains can do this so effortlessly.",
    "start": "16559",
    "end": "18960"
  },
  {
    "text": "I mean, this, this and this are also recognizable as 3s,",
    "start": "19700",
    "end": "22962"
  },
  {
    "text": "even though the specific values of each pixel is very different from one",
    "start": "22962",
    "end": "27213"
  },
  {
    "text": "image to the next.",
    "start": "27213",
    "end": "28320"
  },
  {
    "text": "The particular light-sensitive cells in your eye that are firing when you",
    "start": "28900",
    "end": "32948"
  },
  {
    "text": "see this 3 are very different from the ones firing when you see this 3.",
    "start": "32948",
    "end": "36940"
  },
  {
    "text": "But something in that crazy-smart visual cortex of yours resolves these as representing",
    "start": "37520",
    "end": "42740"
  },
  {
    "text": "the same idea, while at the same time recognizing other images as their own distinct",
    "start": "42740",
    "end": "47840"
  },
  {
    "text": "ideas.",
    "start": "47840",
    "end": "48340"
  },
  {
    "text": "But if I told you, hey, sit down and write for me a program that takes in a grid of",
    "start": "49220",
    "end": "54634"
  },
  {
    "text": "28x28 pixels like this and outputs a single number between 0 and 10,",
    "start": "54634",
    "end": "59135"
  },
  {
    "text": "telling you what it thinks the digit is, well the task goes from comically trivial to",
    "start": "59135",
    "end": "64744"
  },
  {
    "text": "dauntingly difficult.",
    "start": "64745",
    "end": "66180"
  },
  {
    "start": "67000",
    "end": "162000"
  },
  {
    "text": "Unless you've been living under a rock, I think I hardly need to motivate the relevance",
    "start": "67160",
    "end": "70857"
  },
  {
    "text": "and importance of machine learning and neural networks to the present and to the future.",
    "start": "70857",
    "end": "74640"
  },
  {
    "text": "But what I want to do here is show you what a neural network actually is,",
    "start": "75120",
    "end": "78950"
  },
  {
    "text": "assuming no background, and to help visualize what it's doing,",
    "start": "78950",
    "end": "82256"
  },
  {
    "text": "not as a buzzword but as a piece of math.",
    "start": "82256",
    "end": "84460"
  },
  {
    "text": "My hope is that you come away feeling like the structure itself is motivated,",
    "start": "85020",
    "end": "88776"
  },
  {
    "text": "and to feel like you know what it means when you read,",
    "start": "88777",
    "end": "91461"
  },
  {
    "text": "or you hear about a neural network quote-unquote learning.",
    "start": "91461",
    "end": "94340"
  },
  {
    "text": "This video is just going to be devoted to the structure component of that,",
    "start": "95360",
    "end": "98261"
  },
  {
    "text": "and the following one is going to tackle learning.",
    "start": "98261",
    "end": "100260"
  },
  {
    "text": "What we're going to do is put together a neural",
    "start": "100960",
    "end": "103278"
  },
  {
    "text": "network that can learn to recognize handwritten digits.",
    "start": "103278",
    "end": "106040"
  },
  {
    "text": "This is a somewhat classic example for introducing the topic,",
    "start": "109360",
    "end": "112060"
  },
  {
    "text": "and I'm happy to stick with the status quo here,",
    "start": "112060",
    "end": "114228"
  },
  {
    "text": "because at the end of the two videos I want to point you to a couple good",
    "start": "114228",
    "end": "117503"
  },
  {
    "text": "resources where you can learn more, and where you can download the code that",
    "start": "117503",
    "end": "120910"
  },
  {
    "text": "does this and play with it on your own computer.",
    "start": "120911",
    "end": "123080"
  },
  {
    "text": "There are many many variants of neural networks,",
    "start": "125040",
    "end": "127661"
  },
  {
    "text": "and in recent years there's been sort of a boom in research towards these variants,",
    "start": "127661",
    "end": "132246"
  },
  {
    "text": "but in these two introductory videos you and I are just going to look at the simplest",
    "start": "132246",
    "end": "136942"
  },
  {
    "text": "plain vanilla form with no added frills.",
    "start": "136942",
    "end": "139180"
  },
  {
    "text": "This is kind of a necessary prerequisite for understanding any of the more powerful",
    "start": "139860",
    "end": "143890"
  },
  {
    "text": "modern variants, and trust me it still has plenty of complexity for us to wrap our minds",
    "start": "143890",
    "end": "148212"
  },
  {
    "text": "around.",
    "start": "148212",
    "end": "148712"
  },
  {
    "text": "But even in this simplest form it can learn to recognize handwritten digits,",
    "start": "149120",
    "end": "153194"
  },
  {
    "text": "which is a pretty cool thing for a computer to be able to do.",
    "start": "153195",
    "end": "156520"
  },
  {
    "text": "And at the same time you'll see how it does fall",
    "start": "157480",
    "end": "159806"
  },
  {
    "text": "short of a couple hopes that we might have for it.",
    "start": "159807",
    "end": "162280"
  },
  {
    "start": "162000",
    "end": "215000"
  },
  {
    "text": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
    "start": "163380",
    "end": "168500"
  },
  {
    "text": "What are the neurons, and in what sense are they linked together?",
    "start": "168520",
    "end": "171660"
  },
  {
    "text": "Right now when I say neuron all I want you to think about is a thing that holds a number,",
    "start": "172500",
    "end": "178021"
  },
  {
    "text": "specifically a number between 0 and 1.",
    "start": "178021",
    "end": "180440"
  },
  {
    "text": "It's really not more than that.",
    "start": "180680",
    "end": "182560"
  },
  {
    "text": "For example the network starts with a bunch of neurons corresponding to",
    "start": "183780",
    "end": "188821"
  },
  {
    "text": "each of the 28x28 pixels of the input image, which is 784 neurons in total.",
    "start": "188822",
    "end": "194220"
  },
  {
    "text": "Each one of these holds a number that represents the grayscale value of the",
    "start": "194700",
    "end": "199414"
  },
  {
    "text": "corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
    "start": "199414",
    "end": "204380"
  },
  {
    "text": "This number inside the neuron is called its activation,",
    "start": "205300",
    "end": "208253"
  },
  {
    "text": "and the image you might have in mind here is that each neuron is lit up when its",
    "start": "208253",
    "end": "212603"
  },
  {
    "text": "activation is a high number.",
    "start": "212603",
    "end": "214160"
  },
  {
    "start": "215000",
    "end": "331000"
  },
  {
    "text": "So all of these 784 neurons make up the first layer of our network.",
    "start": "216720",
    "end": "221860"
  },
  {
    "text": "Now jumping over to the last layer, this has 10 neurons,",
    "start": "226500",
    "end": "229426"
  },
  {
    "text": "each representing one of the digits.",
    "start": "229426",
    "end": "231360"
  },
  {
    "text": "The activation in these neurons, again some number that's between 0 and 1,",
    "start": "232040",
    "end": "236616"
  },
  {
    "text": "represents how much the system thinks that a given image corresponds with a given digit.",
    "start": "236616",
    "end": "242120"
  },
  {
    "text": "There's also a couple layers in between called the hidden layers,",
    "start": "243040",
    "end": "246420"
  },
  {
    "text": "which for the time being should just be a giant question mark for",
    "start": "246421",
    "end": "249855"
  },
  {
    "text": "how on earth this process of recognizing digits is going to be handled.",
    "start": "249855",
    "end": "253599"
  },
  {
    "text": "In this network I chose two hidden layers, each one with 16 neurons,",
    "start": "254260",
    "end": "257859"
  },
  {
    "text": "and admittedly that's kind of an arbitrary choice.",
    "start": "257860",
    "end": "260560"
  },
  {
    "text": "To be honest I chose two layers based on how I want to motivate the structure in",
    "start": "261020",
    "end": "264655"
  },
  {
    "text": "just a moment, and 16, well that was just a nice number to fit on the screen.",
    "start": "264655",
    "end": "268200"
  },
  {
    "text": "In practice there is a lot of room for experiment with a specific structure here.",
    "start": "268780",
    "end": "272340"
  },
  {
    "text": "The way the network operates, activations in one",
    "start": "273020",
    "end": "275667"
  },
  {
    "text": "layer determine the activations of the next layer.",
    "start": "275667",
    "end": "278479"
  },
  {
    "text": "And of course the heart of the network as an information processing mechanism comes down",
    "start": "279200",
    "end": "283811"
  },
  {
    "text": "to exactly how those activations from one layer bring about activations in the next",
    "start": "283811",
    "end": "288213"
  },
  {
    "text": "layer.",
    "start": "288213",
    "end": "288713"
  },
  {
    "text": "It's meant to be loosely analogous to how in biological networks of neurons,",
    "start": "289140",
    "end": "293633"
  },
  {
    "text": "some groups of neurons firing cause certain others to fire.",
    "start": "293633",
    "end": "297180"
  },
  {
    "text": "Now the network I'm showing here has already been trained to recognize digits,",
    "start": "298120",
    "end": "301581"
  },
  {
    "text": "and let me show you what I mean by that.",
    "start": "301581",
    "end": "303400"
  },
  {
    "text": "It means if you feed in an image, lighting up all 784 neurons of the input layer",
    "start": "303640",
    "end": "308294"
  },
  {
    "text": "according to the brightness of each pixel in the image,",
    "start": "308294",
    "end": "311551"
  },
  {
    "text": "that pattern of activations causes some very specific pattern in the next layer",
    "start": "311551",
    "end": "316205"
  },
  {
    "text": "which causes some pattern in the one after it,",
    "start": "316205",
    "end": "318939"
  },
  {
    "text": "which finally gives some pattern in the output layer.",
    "start": "318939",
    "end": "322080"
  },
  {
    "text": "And the brightest neuron of that output layer is the network's choice,",
    "start": "322560",
    "end": "326517"
  },
  {
    "text": "so to speak, for what digit this image represents.",
    "start": "326517",
    "end": "329400"
  },
  {
    "start": "331000",
    "end": "518000"
  },
  {
    "text": "And before jumping into the math for how one layer influences the next,",
    "start": "332560",
    "end": "336337"
  },
  {
    "text": "or how training works, let's just talk about why it's even reasonable",
    "start": "336337",
    "end": "340062"
  },
  {
    "text": "to expect a layered structure like this to behave intelligently.",
    "start": "340062",
    "end": "343520"
  },
  {
    "text": "What are we expecting here?",
    "start": "344060",
    "end": "345220"
  },
  {
    "text": "What is the best hope for what those middle layers might be doing?",
    "start": "345400",
    "end": "347600"
  },
  {
    "text": "Well, when you or I recognize digits, we piece together various components.",
    "start": "348920",
    "end": "353520"
  },
  {
    "text": "A 9 has a loop up top and a line on the right.",
    "start": "354200",
    "end": "356820"
  },
  {
    "text": "An 8 also has a loop up top, but it's paired with another loop down low.",
    "start": "357380",
    "end": "361180"
  },
  {
    "text": "A 4 basically breaks down into three specific lines, and things like that.",
    "start": "361980",
    "end": "366820"
  },
  {
    "text": "Now in a perfect world, we might hope that each neuron in the second",
    "start": "367600",
    "end": "371558"
  },
  {
    "text": "to last layer corresponds with one of these subcomponents,",
    "start": "371558",
    "end": "374992"
  },
  {
    "text": "that anytime you feed in an image with, say, a loop up top,",
    "start": "374992",
    "end": "378484"
  },
  {
    "text": "like a 9 or an 8, there's some specific neuron whose activation is",
    "start": "378484",
    "end": "382383"
  },
  {
    "text": "going to be close to 1.",
    "start": "382383",
    "end": "383780"
  },
  {
    "text": "And I don't mean this specific loop of pixels,",
    "start": "384500",
    "end": "386906"
  },
  {
    "text": "the hope would be that any generally loopy pattern towards the top sets off this neuron.",
    "start": "386906",
    "end": "391560"
  },
  {
    "text": "That way, going from the third layer to the last one just requires",
    "start": "392440",
    "end": "396049"
  },
  {
    "text": "learning which combination of subcomponents corresponds to which digits.",
    "start": "396049",
    "end": "400040"
  },
  {
    "text": "Of course, that just kicks the problem down the road,",
    "start": "401000",
    "end": "403199"
  },
  {
    "text": "because how would you recognize these subcomponents,",
    "start": "403199",
    "end": "405399"
  },
  {
    "text": "or even learn what the right subcomponents should be?",
    "start": "405399",
    "end": "407639"
  },
  {
    "text": "And I still haven't even talked about how one layer influences the next,",
    "start": "408060",
    "end": "411218"
  },
  {
    "text": "but run with me on this one for a moment.",
    "start": "411218",
    "end": "413060"
  },
  {
    "text": "Recognizing a loop can also break down into subproblems.",
    "start": "413680",
    "end": "416680"
  },
  {
    "text": "One reasonable way to do this would be to first",
    "start": "417280",
    "end": "419891"
  },
  {
    "text": "recognize the various little edges that make it up.",
    "start": "419891",
    "end": "422780"
  },
  {
    "text": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7,",
    "start": "423780",
    "end": "428399"
  },
  {
    "text": "is really just a long edge, or maybe you think of it as a certain pattern of several",
    "start": "428399",
    "end": "433431"
  },
  {
    "text": "smaller edges.",
    "start": "433432",
    "end": "434320"
  },
  {
    "text": "So maybe our hope is that each neuron in the second layer of",
    "start": "435140",
    "end": "438808"
  },
  {
    "text": "the network corresponds with the various relevant little edges.",
    "start": "438808",
    "end": "442720"
  },
  {
    "text": "Maybe when an image like this one comes in, it lights up all of the",
    "start": "443540",
    "end": "447511"
  },
  {
    "text": "neurons associated with around 8 to 10 specific little edges,",
    "start": "447511",
    "end": "451185"
  },
  {
    "text": "which in turn lights up the neurons associated with the upper loop",
    "start": "451185",
    "end": "455156"
  },
  {
    "text": "and a long vertical line, and those light up the neuron associated with a 9.",
    "start": "455156",
    "end": "459720"
  },
  {
    "text": "Whether or not this is what our final network actually does is another question,",
    "start": "460680",
    "end": "464682"
  },
  {
    "text": "one that I'll come back to once we see how to train the network,",
    "start": "464683",
    "end": "467936"
  },
  {
    "text": "but this is a hope that we might have, a sort of goal with the layered structure",
    "start": "467936",
    "end": "471990"
  },
  {
    "text": "like this.",
    "start": "471990",
    "end": "472539"
  },
  {
    "text": "Moreover, you can imagine how being able to detect edges and patterns",
    "start": "473160",
    "end": "476756"
  },
  {
    "text": "like this would be really useful for other image recognition tasks.",
    "start": "476756",
    "end": "480300"
  },
  {
    "text": "And even beyond image recognition, there are all sorts of intelligent",
    "start": "480880",
    "end": "484012"
  },
  {
    "text": "things you might want to do that break down into layers of abstraction.",
    "start": "484012",
    "end": "487280"
  },
  {
    "text": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds,",
    "start": "488040",
    "end": "492729"
  },
  {
    "text": "which combine to make certain syllables, which combine to form words,",
    "start": "492729",
    "end": "496503"
  },
  {
    "text": "which combine to make up phrases and more abstract thoughts, etc.",
    "start": "496503",
    "end": "500060"
  },
  {
    "text": "But getting back to how any of this actually works,",
    "start": "501100",
    "end": "503685"
  },
  {
    "text": "picture yourself right now designing how exactly the activations in one layer might",
    "start": "503685",
    "end": "507943"
  },
  {
    "text": "determine the activations in the next.",
    "start": "507943",
    "end": "509919"
  },
  {
    "text": "The goal is to have some mechanism that could conceivably combine pixels into edges,",
    "start": "510860",
    "end": "515987"
  },
  {
    "text": "or edges into patterns, or patterns into digits.",
    "start": "515988",
    "end": "518979"
  },
  {
    "start": "518000",
    "end": "694000"
  },
  {
    "text": "And to zoom in on one very specific example, let's say the hope",
    "start": "519440",
    "end": "523268"
  },
  {
    "text": "is for one particular neuron in the second layer to pick up",
    "start": "523268",
    "end": "526914"
  },
  {
    "text": "on whether or not the image has an edge in this region here.",
    "start": "526914",
    "end": "530620"
  },
  {
    "text": "The question at hand is what parameters should the network have?",
    "start": "531440",
    "end": "535100"
  },
  {
    "text": "What dials and knobs should you be able to tweak so that it's expressive",
    "start": "535640",
    "end": "539650"
  },
  {
    "text": "enough to potentially capture this pattern, or any other pixel pattern,",
    "start": "539650",
    "end": "543659"
  },
  {
    "text": "or the pattern that several edges can make a loop, and other such things?",
    "start": "543659",
    "end": "547779"
  },
  {
    "text": "Well, what we'll do is assign a weight to each one of the",
    "start": "548720",
    "end": "551813"
  },
  {
    "text": "connections between our neuron and the neurons from the first layer.",
    "start": "551814",
    "end": "555560"
  },
  {
    "text": "These weights are just numbers.",
    "start": "556320",
    "end": "557700"
  },
  {
    "text": "Then take all of those activations from the first layer",
    "start": "558540",
    "end": "561897"
  },
  {
    "text": "and compute their weighted sum according to these weights.",
    "start": "561898",
    "end": "565500"
  },
  {
    "text": "I find it helpful to think of these weights as being organized into a",
    "start": "567700",
    "end": "571097"
  },
  {
    "text": "little grid of their own, and I'm going to use green pixels to indicate",
    "start": "571097",
    "end": "574641"
  },
  {
    "text": "positive weights, and red pixels to indicate negative weights,",
    "start": "574642",
    "end": "577743"
  },
  {
    "text": "where the brightness of that pixel is some loose depiction of the weight's value.",
    "start": "577743",
    "end": "581779"
  },
  {
    "text": "Now if we made the weights associated with almost all of the pixels zero",
    "start": "582780",
    "end": "586527"
  },
  {
    "text": "except for some positive weights in this region that we care about,",
    "start": "586527",
    "end": "590066"
  },
  {
    "text": "then taking the weighted sum of all the pixel values really just amounts",
    "start": "590066",
    "end": "593865"
  },
  {
    "text": "to adding up the values of the pixel just in the region that we care about.",
    "start": "593865",
    "end": "597820"
  },
  {
    "text": "And if you really wanted to pick up on whether there's an edge here,",
    "start": "599140",
    "end": "602392"
  },
  {
    "text": "what you might do is have some negative weights associated with the surrounding pixels.",
    "start": "602392",
    "end": "606600"
  },
  {
    "text": "Then the sum is largest when those middle pixels",
    "start": "607480",
    "end": "610037"
  },
  {
    "text": "are bright but the surrounding pixels are darker.",
    "start": "610037",
    "end": "612700"
  },
  {
    "text": "When you compute a weighted sum like this, you might come out with any number,",
    "start": "614260",
    "end": "618647"
  },
  {
    "text": "but for this network what we want is for activations to be some value between 0 and 1.",
    "start": "618647",
    "end": "623540"
  },
  {
    "text": "So a common thing to do is to pump this weighted sum into some function",
    "start": "624120",
    "end": "628246"
  },
  {
    "text": "that squishes the real number line into the range between 0 and 1.",
    "start": "628246",
    "end": "632139"
  },
  {
    "text": "And a common function that does this is called the sigmoid function,",
    "start": "632460",
    "end": "635833"
  },
  {
    "text": "also known as a logistic curve.",
    "start": "635833",
    "end": "637420"
  },
  {
    "text": "Basically very negative inputs end up close to 0, positive inputs end up close to 1,",
    "start": "638000",
    "end": "643351"
  },
  {
    "text": "and it just steadily increases around the input 0.",
    "start": "643351",
    "end": "646600"
  },
  {
    "text": "So the activation of the neuron here is basically a",
    "start": "649120",
    "end": "652637"
  },
  {
    "text": "measure of how positive the relevant weighted sum is.",
    "start": "652637",
    "end": "656360"
  },
  {
    "text": "But maybe it's not that you want the neuron to",
    "start": "657540",
    "end": "659641"
  },
  {
    "text": "light up when the weighted sum is bigger than 0.",
    "start": "659641",
    "end": "661880"
  },
  {
    "text": "Maybe you only want it to be active when the sum is bigger than say 10.",
    "start": "662280",
    "end": "666360"
  },
  {
    "text": "That is, you want some bias for it to be inactive.",
    "start": "666840",
    "end": "670260"
  },
  {
    "text": "What we'll do then is just add in some other number like negative 10 to this",
    "start": "671380",
    "end": "675466"
  },
  {
    "text": "weighted sum before plugging it through the sigmoid squishification function.",
    "start": "675466",
    "end": "679660"
  },
  {
    "text": "That additional number is called the bias.",
    "start": "680580",
    "end": "682440"
  },
  {
    "text": "So the weights tell you what pixel pattern this neuron in the second",
    "start": "683460",
    "end": "687310"
  },
  {
    "text": "layer is picking up on, and the bias tells you how high the weighted",
    "start": "687310",
    "end": "691217"
  },
  {
    "text": "sum needs to be before the neuron starts getting meaningfully active.",
    "start": "691217",
    "end": "695180"
  },
  {
    "start": "694000",
    "end": "750000"
  },
  {
    "text": "And that is just one neuron.",
    "start": "696120",
    "end": "697680"
  },
  {
    "text": "Every other neuron in this layer is going to be connected to",
    "start": "698280",
    "end": "702477"
  },
  {
    "text": "all 784 pixel neurons from the first layer, and each one of",
    "start": "702477",
    "end": "706673"
  },
  {
    "text": "those 784 connections has its own weight associated with it.",
    "start": "706673",
    "end": "710940"
  },
  {
    "text": "Also, each one has some bias, some other number that you add",
    "start": "711600",
    "end": "714574"
  },
  {
    "text": "on to the weighted sum before squishing it with the sigmoid.",
    "start": "714575",
    "end": "717600"
  },
  {
    "text": "And that's a lot to think about!",
    "start": "718110",
    "end": "719540"
  },
  {
    "text": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights,",
    "start": "719960",
    "end": "726198"
  },
  {
    "text": "along with 16 biases.",
    "start": "726198",
    "end": "727980"
  },
  {
    "text": "And all of that is just the connections from the first layer to the second.",
    "start": "728840",
    "end": "731940"
  },
  {
    "text": "The connections between the other layers also have",
    "start": "732520",
    "end": "734883"
  },
  {
    "text": "a bunch of weights and biases associated with them.",
    "start": "734883",
    "end": "737339"
  },
  {
    "text": "All said and done, this network has almost exactly 13,000 total weights and biases.",
    "start": "738340",
    "end": "743800"
  },
  {
    "text": "13,000 knobs and dials that can be tweaked and turned",
    "start": "743800",
    "end": "747065"
  },
  {
    "text": "to make this network behave in different ways.",
    "start": "747065",
    "end": "749960"
  },
  {
    "start": "750000",
    "end": "806000"
  },
  {
    "text": "So when we talk about learning, what that's referring to is",
    "start": "751040",
    "end": "754261"
  },
  {
    "text": "getting the computer to find a valid setting for all of these",
    "start": "754262",
    "end": "757647"
  },
  {
    "text": "many many numbers so that it'll actually solve the problem at hand.",
    "start": "757647",
    "end": "761360"
  },
  {
    "text": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting",
    "start": "762620",
    "end": "767186"
  },
  {
    "text": "down and setting all of these weights and biases by hand,",
    "start": "767186",
    "end": "770230"
  },
  {
    "text": "purposefully tweaking the numbers so that the second layer picks up on edges,",
    "start": "770230",
    "end": "774323"
  },
  {
    "text": "the third layer picks up on patterns, etc.",
    "start": "774323",
    "end": "776580"
  },
  {
    "text": "I personally find this satisfying rather than just treating the network as a total black",
    "start": "776980",
    "end": "781342"
  },
  {
    "text": "box, because when the network doesn't perform the way you anticipate,",
    "start": "781342",
    "end": "784811"
  },
  {
    "text": "if you've built up a little bit of a relationship with what those weights and biases",
    "start": "784812",
    "end": "789025"
  },
  {
    "text": "actually mean, you have a starting place for experimenting with how to change the",
    "start": "789025",
    "end": "793090"
  },
  {
    "text": "structure to improve.",
    "start": "793090",
    "end": "794180"
  },
  {
    "text": "Or when the network does work but not for the reasons you might expect,",
    "start": "794960",
    "end": "798433"
  },
  {
    "text": "digging into what the weights and biases are doing is a good way to challenge",
    "start": "798433",
    "end": "802248"
  },
  {
    "text": "your assumptions and really expose the full space of possible solutions.",
    "start": "802249",
    "end": "805820"
  },
  {
    "start": "806000",
    "end": "917000"
  },
  {
    "text": "By the way, the actual function here is a little cumbersome to write down,",
    "start": "806840",
    "end": "809963"
  },
  {
    "text": "don't you think?",
    "start": "809963",
    "end": "810680"
  },
  {
    "text": "So let me show you a more notationally compact way that these connections are represented.",
    "start": "812500",
    "end": "817140"
  },
  {
    "text": "This is how you'd see it if you choose to read up more about neural networks.",
    "start": "817660",
    "end": "820519"
  },
  {
    "text": "Organize all of the activations from one layer into a column as a vector.",
    "start": "820520",
    "end": "828023"
  },
  {
    "text": "Then organize all of the weights as a matrix, where each row of that matrix corresponds",
    "start": "828357",
    "end": "830038"
  },
  {
    "text": "to the connections between one layer and a particular neuron in the next layer.",
    "start": "830038",
    "end": "838000"
  },
  {
    "text": "What that means is that taking the weighted sum of the activations in",
    "start": "838540",
    "end": "842214"
  },
  {
    "text": "the first layer according to these weights corresponds to one of the",
    "start": "842214",
    "end": "845886"
  },
  {
    "text": "terms in the matrix vector product of everything we have on the left here.",
    "start": "845887",
    "end": "849880"
  },
  {
    "text": "By the way, so much of machine learning just comes down to having a good",
    "start": "854000",
    "end": "857714"
  },
  {
    "text": "grasp of linear algebra, so for any of you who want a nice visual",
    "start": "857714",
    "end": "861119"
  },
  {
    "text": "understanding for matrices and what matrix vector multiplication means,",
    "start": "861119",
    "end": "864834"
  },
  {
    "text": "take a look at the series I did on linear algebra, especially chapter 3.",
    "start": "864834",
    "end": "868600"
  },
  {
    "text": "Back to our expression, instead of talking about adding the bias to each one of",
    "start": "869240",
    "end": "873593"
  },
  {
    "text": "these values independently, we represent it by organizing all those biases into",
    "start": "873593",
    "end": "878002"
  },
  {
    "text": "a vector, and adding the entire vector to the previous matrix vector product.",
    "start": "878002",
    "end": "882300"
  },
  {
    "text": "Then as a final step, I'll wrap a sigmoid around the outside here,",
    "start": "883280",
    "end": "886814"
  },
  {
    "text": "and what that's supposed to represent is that you're going to apply the",
    "start": "886814",
    "end": "890670"
  },
  {
    "text": "sigmoid function to each specific component of the resulting vector inside.",
    "start": "890670",
    "end": "894740"
  },
  {
    "text": "So once you write down this weight matrix and these vectors as their own symbols,",
    "start": "895940",
    "end": "900478"
  },
  {
    "text": "you can communicate the full transition of activations from one layer to the next in an",
    "start": "900478",
    "end": "905408"
  },
  {
    "text": "extremely tight and neat little expression, and this makes the relevant code both a lot",
    "start": "905408",
    "end": "910338"
  },
  {
    "text": "simpler and a lot faster, since many libraries optimize the heck out of matrix",
    "start": "910338",
    "end": "914764"
  },
  {
    "text": "multiplication.",
    "start": "914764",
    "end": "915660"
  },
  {
    "start": "917000",
    "end": "987000"
  },
  {
    "text": "Remember how earlier I said these neurons are simply things that hold numbers?",
    "start": "917820",
    "end": "921460"
  },
  {
    "text": "Well of course the specific numbers that they hold depends on the image you feed in,",
    "start": "922220",
    "end": "927329"
  },
  {
    "text": "so it's actually more accurate to think of each neuron as a function,",
    "start": "927330",
    "end": "931588"
  },
  {
    "text": "one that takes in the outputs of all the neurons in the previous layer and spits out a",
    "start": "931588",
    "end": "936880"
  },
  {
    "text": "number between 0 and 1.",
    "start": "936880",
    "end": "938340"
  },
  {
    "text": "Really the entire network is just a function, one that takes in",
    "start": "939200",
    "end": "943130"
  },
  {
    "text": "784 numbers as an input and spits out 10 numbers as an output.",
    "start": "943130",
    "end": "947060"
  },
  {
    "text": "It's an absurdly complicated function, one that involves 13,000 parameters",
    "start": "947560",
    "end": "951462"
  },
  {
    "text": "in the forms of these weights and biases that pick up on certain patterns,",
    "start": "951462",
    "end": "955415"
  },
  {
    "text": "and which involves iterating many matrix vector products and the sigmoid",
    "start": "955416",
    "end": "959265"
  },
  {
    "text": "squishification function, but it's just a function nonetheless.",
    "start": "959265",
    "end": "962640"
  },
  {
    "text": "And in a way it's kind of reassuring that it looks complicated.",
    "start": "963400",
    "end": "966660"
  },
  {
    "text": "I mean if it were any simpler, what hope would we have",
    "start": "967340",
    "end": "969701"
  },
  {
    "text": "that it could take on the challenge of recognizing digits?",
    "start": "969701",
    "end": "972279"
  },
  {
    "text": "And how does it take on that challenge?",
    "start": "973340",
    "end": "974700"
  },
  {
    "text": "How does this network learn the appropriate weights and biases just by looking at data?",
    "start": "975080",
    "end": "979360"
  },
  {
    "text": "Well that's what I'll show in the next video, and I'll also dig a little",
    "start": "980140",
    "end": "983194"
  },
  {
    "text": "more into what this particular network we're seeing is really doing.",
    "start": "983194",
    "end": "986120"
  },
  {
    "start": "987000",
    "end": "1023000"
  },
  {
    "text": "Now is the point I suppose I should say subscribe to stay notified",
    "start": "987580",
    "end": "990748"
  },
  {
    "text": "about when that video or any new videos come out,",
    "start": "990748",
    "end": "993148"
  },
  {
    "text": "but realistically most of you don't actually receive notifications from YouTube, do you?",
    "start": "993148",
    "end": "997420"
  },
  {
    "text": "Maybe more honestly I should say subscribe so that the neural networks",
    "start": "998020",
    "end": "1001276"
  },
  {
    "text": "that underlie YouTube's recommendation algorithm are primed to believe",
    "start": "1001276",
    "end": "1004578"
  },
  {
    "text": "that you want to see content from this channel get recommended to you.",
    "start": "1004578",
    "end": "1007880"
  },
  {
    "text": "Anyway, stay posted for more.",
    "start": "1008560",
    "end": "1009940"
  },
  {
    "text": "Thank you very much to everyone supporting these videos on Patreon.",
    "start": "1010760",
    "end": "1013500"
  },
  {
    "text": "I've been a little slow to progress in the probability series this summer,",
    "start": "1014000",
    "end": "1017439"
  },
  {
    "text": "but I'm jumping back into it after this project,",
    "start": "1017439",
    "end": "1019716"
  },
  {
    "text": "so patrons you can look out for updates there.",
    "start": "1019716",
    "end": "1021900"
  },
  {
    "start": "1023000",
    "end": "1120000"
  },
  {
    "text": "To close things off here I have with me Lisha Li who did her PhD work on the",
    "start": "1023600",
    "end": "1027089"
  },
  {
    "text": "theoretical side of deep learning and who currently works at a venture capital",
    "start": "1027090",
    "end": "1030716"
  },
  {
    "text": "firm called Amplify Partners who kindly provided some of the funding for this video.",
    "start": "1030717",
    "end": "1034620"
  },
  {
    "text": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
    "start": "1035460",
    "end": "1039120"
  },
  {
    "text": "As I understand it early networks use this to squish the relevant weighted",
    "start": "1039700",
    "end": "1043157"
  },
  {
    "text": "sum into that interval between zero and one, you know kind of motivated",
    "start": "1043158",
    "end": "1046522"
  },
  {
    "text": "by this biological analogy of neurons either being inactive or active.",
    "start": "1046522",
    "end": "1049840"
  },
  {
    "text": "Exactly.\nBut relatively few modern networks actually use sigmoid anymore.",
    "start": "1050280",
    "end": "1054040"
  },
  {
    "text": "Yeah.\nIt's kind of old school right?",
    "start": "1054320",
    "end": "1055759"
  },
  {
    "text": "Yeah or rather ReLU seems to be much easier to train.",
    "start": "1055760",
    "end": "1058980"
  },
  {
    "text": "And ReLU, ReLU stands for rectified linear unit?",
    "start": "1059400",
    "end": "1062340"
  },
  {
    "text": "Yes it's this kind of function where you're just taking a max of zero",
    "start": "1062680",
    "end": "1067401"
  },
  {
    "text": "and a where a is given by what you were explaining in the video and",
    "start": "1067401",
    "end": "1072054"
  },
  {
    "text": "what this was sort of motivated from I think was a partially by a",
    "start": "1072054",
    "end": "1076570"
  },
  {
    "text": "biological analogy with how neurons would either be activated or not.",
    "start": "1076570",
    "end": "1081360"
  },
  {
    "text": "And so if it passes a certain threshold it would be the identity function but if it did",
    "start": "1081360",
    "end": "1086020"
  },
  {
    "text": "not then it would just not be activated so it'd be zero so it's kind of a simplification.",
    "start": "1086020",
    "end": "1090840"
  },
  {
    "text": "Using sigmoids didn't help training or it was very difficult to",
    "start": "1091160",
    "end": "1095695"
  },
  {
    "text": "train at some point and people just tried ReLU and it happened",
    "start": "1095695",
    "end": "1100229"
  },
  {
    "text": "to work very well for these incredibly deep neural networks.",
    "start": "1100229",
    "end": "1104620"
  },
  {
    "text": "All right thank you Lisha.",
    "start": "1105100",
    "end": "1105640"
  }
]