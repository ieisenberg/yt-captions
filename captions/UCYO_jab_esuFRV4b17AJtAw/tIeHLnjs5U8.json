[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "The hard assumption here is that you've watched part 3,",
    "start": "4020",
    "end": "6701"
  },
  {
    "text": "giving an intuitive walkthrough of the backpropagation algorithm.",
    "start": "6702",
    "end": "9920"
  },
  {
    "text": "Here we get a little more formal and dive into the relevant calculus.",
    "start": "11040",
    "end": "14220"
  },
  {
    "text": "It's normal for this to be at least a little confusing,",
    "start": "14820",
    "end": "17265"
  },
  {
    "text": "so the mantra to regularly pause and ponder certainly applies as much here",
    "start": "17265",
    "end": "20599"
  },
  {
    "text": "as anywhere else.",
    "start": "20600",
    "end": "21400"
  },
  {
    "text": "Our main goal is to show how people in machine learning commonly think about",
    "start": "21940",
    "end": "25875"
  },
  {
    "text": "the chain rule from calculus in the context of networks,",
    "start": "25875",
    "end": "28825"
  },
  {
    "text": "which has a different feel from how most introductory calculus courses",
    "start": "28825",
    "end": "32501"
  },
  {
    "text": "approach the subject.",
    "start": "32501",
    "end": "33640"
  },
  {
    "text": "For those of you uncomfortable with the relevant calculus,",
    "start": "34340",
    "end": "36971"
  },
  {
    "text": "I do have a whole series on the topic.",
    "start": "36971",
    "end": "38740"
  },
  {
    "start": "38000",
    "end": "236000"
  },
  {
    "text": "Let's start off with an extremely simple network,",
    "start": "39960",
    "end": "43021"
  },
  {
    "text": "one where each layer has a single neuron in it.",
    "start": "43021",
    "end": "46020"
  },
  {
    "text": "This network is determined by three weights and three biases,",
    "start": "46320",
    "end": "49896"
  },
  {
    "text": "and our goal is to understand how sensitive the cost function is to these variables.",
    "start": "49896",
    "end": "54880"
  },
  {
    "text": "That way, we know which adjustments to those terms will",
    "start": "55420",
    "end": "58095"
  },
  {
    "text": "cause the most efficient decrease to the cost function.",
    "start": "58096",
    "end": "60820"
  },
  {
    "text": "And we're just going to focus on the connection between the last two neurons.",
    "start": "61960",
    "end": "64839"
  },
  {
    "text": "Let's label the activation of that last neuron with a superscript L,",
    "start": "65980",
    "end": "70266"
  },
  {
    "text": "indicating which layer it's in, so the activation of the previous neuron is a(L-1).",
    "start": "70266",
    "end": "75560"
  },
  {
    "text": "These are not exponents, they're just a way of indexing what we're talking about,",
    "start": "76360",
    "end": "80092"
  },
  {
    "text": "since I want to save subscripts for different indices later on.",
    "start": "80092",
    "end": "83040"
  },
  {
    "text": "Let's say that the value we want this last activation to be for",
    "start": "83720",
    "end": "87950"
  },
  {
    "text": "a given training example is y, for example, y might be 0 or 1.",
    "start": "87950",
    "end": "92180"
  },
  {
    "text": "So the cost of this network for a single training example is (a(L) - y) squared.",
    "start": "92840",
    "end": "99240"
  },
  {
    "text": "We'll denote the cost of that one training example as C0.",
    "start": "100260",
    "end": "104380"
  },
  {
    "text": "As a reminder, this last activation is determined by a weight,",
    "start": "105900",
    "end": "109840"
  },
  {
    "text": "which I'm going to call w(L), times the previous neuron's activation plus some bias,",
    "start": "109840",
    "end": "115242"
  },
  {
    "text": "which I'll call b(L).",
    "start": "115242",
    "end": "116640"
  },
  {
    "text": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
    "start": "117420",
    "end": "121320"
  },
  {
    "text": "It's actually going to make things easier for us if we give a special name to",
    "start": "121800",
    "end": "125442"
  },
  {
    "text": "this weighted sum, like z, with the same superscript as the relevant activations.",
    "start": "125442",
    "end": "129320"
  },
  {
    "text": "This is a lot of terms, and a way you might conceptualize it is that the weight,",
    "start": "130380",
    "end": "135331"
  },
  {
    "text": "previous action and the bias all together are used to compute z,",
    "start": "135331",
    "end": "139353"
  },
  {
    "text": "which in turn lets us compute a, which finally, along with a constant y,",
    "start": "139353",
    "end": "143871"
  },
  {
    "text": "lets us compute the cost.",
    "start": "143871",
    "end": "145480"
  },
  {
    "text": "And of course a(L-1) is influenced by its own weight and bias and such,",
    "start": "147340",
    "end": "151946"
  },
  {
    "text": "but we're not going to focus on that right now.",
    "start": "151946",
    "end": "155060"
  },
  {
    "text": "All of these are just numbers, right?",
    "start": "155700",
    "end": "157620"
  },
  {
    "text": "And it can be nice to think of each one as having its own little number line.",
    "start": "158060",
    "end": "161040"
  },
  {
    "text": "Our first goal is to understand how sensitive the",
    "start": "161720",
    "end": "165183"
  },
  {
    "text": "cost function is to small changes in our weight w(L).",
    "start": "165183",
    "end": "169000"
  },
  {
    "text": "Or phrase differently, what is the derivative of C with respect to w(L)?",
    "start": "169540",
    "end": "174860"
  },
  {
    "text": "When you see this del w term, think of it as meaning some tiny nudge to W,",
    "start": "175600",
    "end": "180666"
  },
  {
    "text": "like a change by 0.01, and think of this del C term as meaning",
    "start": "180666",
    "end": "184979"
  },
  {
    "text": "whatever the resulting nudge to the cost is.",
    "start": "184979",
    "end": "188060"
  },
  {
    "text": "What we want is their ratio.",
    "start": "188060",
    "end": "190220"
  },
  {
    "text": "Conceptually, this tiny nudge to w(L) causes some nudge to z(L),",
    "start": "191260",
    "end": "195790"
  },
  {
    "text": "which in turn causes some nudge to a(L), which directly influences the cost.",
    "start": "195790",
    "end": "201239"
  },
  {
    "text": "So we break things up by first looking at the ratio of a tiny change to z(L)",
    "start": "203120",
    "end": "208127"
  },
  {
    "text": "to this tiny change q, that is, the derivative of z(L) with respect to w(L).",
    "start": "208127",
    "end": "213200"
  },
  {
    "text": "Likewise, you then consider the ratio of the change to a(L) to",
    "start": "213200",
    "end": "216959"
  },
  {
    "text": "the tiny change in z(L) that caused it, as well as the ratio",
    "start": "216959",
    "end": "220658"
  },
  {
    "text": "between the final nudge to C and this intermediate nudge to a(L).",
    "start": "220658",
    "end": "224660"
  },
  {
    "text": "This right here is the chain rule, where multiplying together these",
    "start": "225740",
    "end": "230371"
  },
  {
    "text": "three ratios gives us the sensitivity of C to small changes in w(L).",
    "start": "230371",
    "end": "235140"
  },
  {
    "start": "236000",
    "end": "285000"
  },
  {
    "text": "So on screen right now, there's a lot of symbols,",
    "start": "236880",
    "end": "239562"
  },
  {
    "text": "and take a moment to make sure it's clear what they all are,",
    "start": "239562",
    "end": "242901"
  },
  {
    "text": "because now we're going to compute the relevant derivatives.",
    "start": "242901",
    "end": "246240"
  },
  {
    "text": "The derivative of C with respect to a(L) works out to be 2(a(L)-y).",
    "start": "247440",
    "end": "253160"
  },
  {
    "text": "Notice this means its size is proportional to the difference between the network's",
    "start": "253980",
    "end": "258592"
  },
  {
    "text": "output and the thing we want it to be, so if that output was very different,",
    "start": "258592",
    "end": "262922"
  },
  {
    "text": "even slight changes stand to have a big impact on the final cost function.",
    "start": "262922",
    "end": "267139"
  },
  {
    "text": "The derivative of a(L) with respect to z(L) is just the derivative",
    "start": "267840",
    "end": "271917"
  },
  {
    "text": "of our sigmoid function, or whatever nonlinearity you choose to use.",
    "start": "271917",
    "end": "276180"
  },
  {
    "text": "And the derivative of z(L) with respect to w(L) comes out to be a(L-1).",
    "start": "277220",
    "end": "284660"
  },
  {
    "start": "285000",
    "end": "339000"
  },
  {
    "text": "Now I don't know about you, but I think it's easy to get stuck head down in the",
    "start": "285760",
    "end": "289384"
  },
  {
    "text": "formulas without taking a moment to sit back and remind yourself of what they all mean.",
    "start": "289384",
    "end": "293420"
  },
  {
    "text": "In the case of this last derivative, the amount that the small nudge to the",
    "start": "293920",
    "end": "298254"
  },
  {
    "text": "weight influenced the last layer depends on how strong the previous neuron is.",
    "start": "298254",
    "end": "302820"
  },
  {
    "text": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
    "start": "303380",
    "end": "308280"
  },
  {
    "text": "And all of this is the derivative with respect to w(L)",
    "start": "309200",
    "end": "312372"
  },
  {
    "text": "only of the cost for a specific single training example.",
    "start": "312372",
    "end": "315720"
  },
  {
    "text": "Since the full cost function involves averaging together all",
    "start": "316440",
    "end": "319902"
  },
  {
    "text": "those costs across many different training examples,",
    "start": "319902",
    "end": "322960"
  },
  {
    "text": "its derivative requires averaging this expression over all training examples.",
    "start": "322960",
    "end": "327460"
  },
  {
    "text": "And of course, that is just one component of the gradient vector,",
    "start": "328380",
    "end": "331833"
  },
  {
    "text": "which itself is built up from the partial derivatives of the",
    "start": "331833",
    "end": "335073"
  },
  {
    "text": "cost function with respect to all those weights and biases.",
    "start": "335073",
    "end": "338260"
  },
  {
    "start": "339000",
    "end": "402000"
  },
  {
    "text": "But even though that's just one of the many partial derivatives we need,",
    "start": "340640",
    "end": "343838"
  },
  {
    "text": "it's more than 50% of the work.",
    "start": "343838",
    "end": "345260"
  },
  {
    "text": "The sensitivity to the bias, for example, is almost identical.",
    "start": "346340",
    "end": "349720"
  },
  {
    "text": "We just need to change out this del z del w term for a del z del b.",
    "start": "350040",
    "end": "355020"
  },
  {
    "text": "And if you look at the relevant formula, that derivative comes out to be 1.",
    "start": "358420",
    "end": "362400"
  },
  {
    "text": "Also, and this is where the idea of propagating backwards comes in,",
    "start": "366140",
    "end": "370263"
  },
  {
    "text": "you can see how sensitive this cost function is to the activation of the previous layer.",
    "start": "370263",
    "end": "375740"
  },
  {
    "text": "Namely, this initial derivative in the chain rule expression,",
    "start": "375740",
    "end": "379972"
  },
  {
    "text": "the sensitivity of z to the previous activation, comes out to be the weight w(L).",
    "start": "379972",
    "end": "385659"
  },
  {
    "text": "And again, even though we're not going to be able to directly influence",
    "start": "386640",
    "end": "390482"
  },
  {
    "text": "that previous layer activation, it's helpful to keep track of,",
    "start": "390482",
    "end": "393891"
  },
  {
    "text": "because now we can just keep iterating this same chain rule idea backwards",
    "start": "393891",
    "end": "397949"
  },
  {
    "text": "to see how sensitive the cost function is to previous weights and previous biases.",
    "start": "397949",
    "end": "402440"
  },
  {
    "start": "402000",
    "end": "553000"
  },
  {
    "text": "And you might think this is an overly simple example, since all layers have one neuron,",
    "start": "403180",
    "end": "407289"
  },
  {
    "text": "and things are going to get exponentially more complicated for a real network.",
    "start": "407289",
    "end": "411020"
  },
  {
    "text": "But honestly, not that much changes when we give the layers multiple neurons,",
    "start": "411700",
    "end": "415909"
  },
  {
    "text": "really it's just a few more indices to keep track of.",
    "start": "415909",
    "end": "418860"
  },
  {
    "text": "Rather than the activation of a given layer simply being a(L),",
    "start": "419380",
    "end": "422753"
  },
  {
    "text": "it's also going to have a subscript indicating which neuron of that layer it is.",
    "start": "422753",
    "end": "427160"
  },
  {
    "text": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
    "start": "427160",
    "end": "434420"
  },
  {
    "text": "For the cost, again we look at what the desired output is,",
    "start": "435260",
    "end": "438567"
  },
  {
    "text": "but this time we add up the squares of the differences between these last layer",
    "start": "438567",
    "end": "443128"
  },
  {
    "text": "activations and the desired output.",
    "start": "443128",
    "end": "445180"
  },
  {
    "text": "That is, you take a sum over a(L)j minus yj squared.",
    "start": "446080",
    "end": "450840"
  },
  {
    "text": "Since there's a lot more weights, each one has to have a couple",
    "start": "453040",
    "end": "456839"
  },
  {
    "text": "more indices to keep track of where it is, so let's call the weight",
    "start": "456839",
    "end": "460940"
  },
  {
    "text": "of the edge connecting this kth neuron to the jth neuron, w(L)jk.",
    "start": "460940",
    "end": "464920"
  },
  {
    "text": "Those indices might feel a little backwards at first,",
    "start": "465620",
    "end": "468380"
  },
  {
    "text": "but it lines up with how you'd index the weight matrix I talked about in",
    "start": "468380",
    "end": "472183"
  },
  {
    "text": "the part 1 video.",
    "start": "472183",
    "end": "473120"
  },
  {
    "text": "Just as before, it's still nice to give a name to the relevant weighted sum,",
    "start": "473620",
    "end": "477880"
  },
  {
    "text": "like z, so that the activation of the last layer is just your special function,",
    "start": "477881",
    "end": "482366"
  },
  {
    "text": "like the sigmoid, applied to z.",
    "start": "482366",
    "end": "484160"
  },
  {
    "text": "You can see what I mean, where all of these are essentially the same equations we had",
    "start": "484660",
    "end": "488992"
  },
  {
    "text": "before in the one-neuron-per-layer case, it's just that it looks a little more",
    "start": "488992",
    "end": "493018"
  },
  {
    "text": "complicated.",
    "start": "493018",
    "end": "493680"
  },
  {
    "text": "And indeed, the chain-ruled derivative expression describing how",
    "start": "495440",
    "end": "499223"
  },
  {
    "text": "sensitive the cost is to a specific weight looks essentially the same.",
    "start": "499223",
    "end": "503419"
  },
  {
    "text": "I'll leave it to you to pause and think about each of those terms if you want.",
    "start": "503920",
    "end": "506840"
  },
  {
    "text": "What does change here, though, is the derivative of the cost",
    "start": "508980",
    "end": "512918"
  },
  {
    "text": "with respect to one of the activations in the layer L-1.",
    "start": "512918",
    "end": "516660"
  },
  {
    "text": "In this case, the difference is that the neuron influences",
    "start": "517780",
    "end": "520469"
  },
  {
    "text": "the cost function through multiple different paths.",
    "start": "520469",
    "end": "522880"
  },
  {
    "text": "That is, on the one hand, it influences a(L)0, which plays a role in the cost function,",
    "start": "524680",
    "end": "530219"
  },
  {
    "text": "but it also has an influence on a(L)1, which also plays a role in the cost function,",
    "start": "530219",
    "end": "535630"
  },
  {
    "text": "and you have to add those up.",
    "start": "535630",
    "end": "537540"
  },
  {
    "text": "And that, well, that's pretty much it.",
    "start": "539820",
    "end": "543040"
  },
  {
    "text": "Once you know how sensitive the cost function is to the",
    "start": "543500",
    "end": "546283"
  },
  {
    "text": "activations in this second-to-last layer, you can just repeat",
    "start": "546283",
    "end": "549420"
  },
  {
    "text": "the process for all the weights and biases feeding into that layer.",
    "start": "549420",
    "end": "552860"
  },
  {
    "start": "553000",
    "end": "618000"
  },
  {
    "text": "So pat yourself on the back!",
    "start": "553900",
    "end": "554960"
  },
  {
    "text": "If all of this makes sense, you have now looked deep into the heart of backpropagation,",
    "start": "555300",
    "end": "560056"
  },
  {
    "text": "the workhorse behind how neural networks learn.",
    "start": "560056",
    "end": "562680"
  },
  {
    "text": "These chain rule expressions give you the derivatives that determine each component in",
    "start": "563300",
    "end": "568186"
  },
  {
    "text": "the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
    "start": "568186",
    "end": "573300"
  },
  {
    "text": "If you sit back and think about all that, this is a lot of layers of complexity to",
    "start": "574300",
    "end": "578394"
  },
  {
    "text": "wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
    "start": "578395",
    "end": "582740"
  }
]