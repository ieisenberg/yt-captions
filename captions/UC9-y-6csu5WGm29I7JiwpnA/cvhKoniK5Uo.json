[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "today we're going to be talking about",
    "start": "80",
    "end": "1760"
  },
  {
    "text": "mapreduce which is kind of a programming",
    "start": "1760",
    "end": "4240"
  },
  {
    "text": "paradigm for",
    "start": "4240",
    "end": "5680"
  },
  {
    "text": "doing large-scale computations across",
    "start": "5680",
    "end": "9280"
  },
  {
    "text": "a computing cluster",
    "start": "9280",
    "end": "12240"
  },
  {
    "text": "google originally came up with the whole",
    "start": "12800",
    "end": "14920"
  },
  {
    "start": "13000",
    "end": "33000"
  },
  {
    "text": "mapreduce idea and um",
    "start": "14920",
    "end": "17440"
  },
  {
    "text": "yeah that way of thinking about doing",
    "start": "17440",
    "end": "19439"
  },
  {
    "text": "large scale computations",
    "start": "19439",
    "end": "21439"
  },
  {
    "text": "and then it got popularized through an",
    "start": "21439",
    "end": "23279"
  },
  {
    "text": "open source implementation",
    "start": "23279",
    "end": "25840"
  },
  {
    "text": "called apache hadoop and then that",
    "start": "25840",
    "end": "29119"
  },
  {
    "text": "was then quite popular um in kind of",
    "start": "29119",
    "end": "31599"
  },
  {
    "text": "processing these very large volumes of",
    "start": "31599",
    "end": "33200"
  },
  {
    "text": "data so we can do an example of a",
    "start": "33200",
    "end": "35120"
  },
  {
    "text": "mapreduce computation so the first thing",
    "start": "35120",
    "end": "36960"
  },
  {
    "text": "with mapreduce is that your the job you",
    "start": "36960",
    "end": "39920"
  },
  {
    "text": "want to do has got to be able to be",
    "start": "39920",
    "end": "41280"
  },
  {
    "text": "split down into a map stage in a reduced",
    "start": "41280",
    "end": "43040"
  },
  {
    "text": "phase so the map phase",
    "start": "43040",
    "end": "45520"
  },
  {
    "text": "needs to be you want to do the same",
    "start": "45520",
    "end": "46559"
  },
  {
    "text": "computation across all your data items",
    "start": "46559",
    "end": "49039"
  },
  {
    "text": "at the same time",
    "start": "49039",
    "end": "50480"
  },
  {
    "text": "and so you do that all your data is",
    "start": "50480",
    "end": "52000"
  },
  {
    "text": "distributed across the cluster",
    "start": "52000",
    "end": "53920"
  },
  {
    "text": "now stored on the different nodes and",
    "start": "53920",
    "end": "55600"
  },
  {
    "text": "each node does the computation on its",
    "start": "55600",
    "end": "57280"
  },
  {
    "text": "own data",
    "start": "57280",
    "end": "59120"
  },
  {
    "text": "and then",
    "start": "59120",
    "end": "60239"
  },
  {
    "text": "the reduced stage would then take the",
    "start": "60239",
    "end": "62160"
  },
  {
    "text": "results of the map phase reduce it down",
    "start": "62160",
    "end": "64559"
  },
  {
    "text": "into kind of a single value and then",
    "start": "64559",
    "end": "66640"
  },
  {
    "text": "send that back as the result to the",
    "start": "66640",
    "end": "68080"
  },
  {
    "start": "68000",
    "end": "81000"
  },
  {
    "text": "computation a classic example would be",
    "start": "68080",
    "end": "70720"
  },
  {
    "text": "a word count so say we've got a massive",
    "start": "70720",
    "end": "72720"
  },
  {
    "text": "text file and we've got it in a",
    "start": "72720",
    "end": "74000"
  },
  {
    "text": "distributed file system across a cluster",
    "start": "74000",
    "end": "76640"
  },
  {
    "text": "and that means that each node of the",
    "start": "76640",
    "end": "78799"
  },
  {
    "text": "cluster will be doing the computation on",
    "start": "78799",
    "end": "80240"
  },
  {
    "text": "its own data and we want to count the",
    "start": "80240",
    "end": "82720"
  },
  {
    "start": "81000",
    "end": "238000"
  },
  {
    "text": "number of occurrences of each word in",
    "start": "82720",
    "end": "84240"
  },
  {
    "text": "that text well so for simplicity's sake",
    "start": "84240",
    "end": "86960"
  },
  {
    "text": "we've got a file which first line is aba",
    "start": "86960",
    "end": "89920"
  },
  {
    "text": "and then abc and then cd and this would",
    "start": "89920",
    "end": "93360"
  },
  {
    "text": "be distributed so let's say aba is on",
    "start": "93360",
    "end": "95759"
  },
  {
    "text": "the first node abc is on the second node",
    "start": "95759",
    "end": "98400"
  },
  {
    "text": "and then cd on the last node so the map",
    "start": "98400",
    "end": "101759"
  },
  {
    "text": "stage would take this and it would put",
    "start": "101759",
    "end": "105360"
  },
  {
    "text": "this into what's called a key value pair",
    "start": "105360",
    "end": "106880"
  },
  {
    "text": "so we're going to take each word as the",
    "start": "106880",
    "end": "108880"
  },
  {
    "text": "key so for each word within this we'll",
    "start": "108880",
    "end": "112000"
  },
  {
    "text": "map it so that the word's the key and",
    "start": "112000",
    "end": "113360"
  },
  {
    "text": "then we put the number one next to it is",
    "start": "113360",
    "end": "114799"
  },
  {
    "text": "the value we go a1 b1 a1 a1 b1 c1 and",
    "start": "114799",
    "end": "120560"
  },
  {
    "text": "then c1",
    "start": "120560",
    "end": "122320"
  },
  {
    "text": "d1 at the end of the map stage which is",
    "start": "122320",
    "end": "124560"
  },
  {
    "text": "here we've got all the keys and values",
    "start": "124560",
    "end": "127439"
  },
  {
    "text": "what happens then is a shuffle phase",
    "start": "127439",
    "end": "129119"
  },
  {
    "text": "which the programmer doesn't need to",
    "start": "129119",
    "end": "130399"
  },
  {
    "text": "know about it just kind of happens",
    "start": "130399",
    "end": "132560"
  },
  {
    "text": "in between and that basically groups",
    "start": "132560",
    "end": "134080"
  },
  {
    "text": "these on nodes based on the key so that",
    "start": "134080",
    "end": "137360"
  },
  {
    "text": "all data items with the same key are",
    "start": "137360",
    "end": "139599"
  },
  {
    "text": "stored in the same node we'll go to a1",
    "start": "139599",
    "end": "144400"
  },
  {
    "text": "and then on the second node we'll have b",
    "start": "144400",
    "end": "147280"
  },
  {
    "text": "one one and on the last node we might",
    "start": "147280",
    "end": "149120"
  },
  {
    "text": "have c one one and d one and the reduced",
    "start": "149120",
    "end": "153120"
  },
  {
    "text": "phase is going to return a single value",
    "start": "153120",
    "end": "155599"
  },
  {
    "text": "for each one of these keys so it's about",
    "start": "155599",
    "end": "157680"
  },
  {
    "text": "combining all of the values associated",
    "start": "157680",
    "end": "159360"
  },
  {
    "text": "with that key into one single value so",
    "start": "159360",
    "end": "161840"
  },
  {
    "text": "for this we want to count the number of",
    "start": "161840",
    "end": "163040"
  },
  {
    "text": "occurrences of",
    "start": "163040",
    "end": "164640"
  },
  {
    "text": "like the word a and so we're going to",
    "start": "164640",
    "end": "166640"
  },
  {
    "text": "reduce it",
    "start": "166640",
    "end": "167920"
  },
  {
    "text": "down and we're just going to use a plus",
    "start": "167920",
    "end": "169040"
  },
  {
    "text": "operator to do that in the reduce",
    "start": "169040",
    "end": "170400"
  },
  {
    "text": "function so",
    "start": "170400",
    "end": "172239"
  },
  {
    "text": "it then finally comes to a3 b2 c2 and d1",
    "start": "172239",
    "end": "177840"
  },
  {
    "text": "and if you imagine that we're doing this",
    "start": "177840",
    "end": "180000"
  },
  {
    "text": "on huge huge volumes of data so over",
    "start": "180000",
    "end": "182720"
  },
  {
    "text": "very very large files this kind of",
    "start": "182720",
    "end": "184800"
  },
  {
    "text": "computation is a lot more efficient if",
    "start": "184800",
    "end": "186560"
  },
  {
    "text": "you can distribute that because doing",
    "start": "186560",
    "end": "188319"
  },
  {
    "text": "this map phase of saying okay this is",
    "start": "188319",
    "end": "190159"
  },
  {
    "text": "one occurrence of the letter a that's",
    "start": "190159",
    "end": "192080"
  },
  {
    "text": "independent of anything else and so you",
    "start": "192080",
    "end": "194000"
  },
  {
    "text": "can split that across individual nodes",
    "start": "194000",
    "end": "195680"
  },
  {
    "text": "they can do that part of the computation",
    "start": "195680",
    "end": "197280"
  },
  {
    "text": "individually and then later on we do the",
    "start": "197280",
    "end": "199280"
  },
  {
    "text": "shuffle and reduce it all back to the",
    "start": "199280",
    "end": "200720"
  },
  {
    "text": "single value so are they physically",
    "start": "200720",
    "end": "202400"
  },
  {
    "text": "moving data or just moving the",
    "start": "202400",
    "end": "203599"
  },
  {
    "text": "computation saying right you're",
    "start": "203599",
    "end": "204879"
  },
  {
    "text": "responsible for that computation",
    "start": "204879",
    "end": "207040"
  },
  {
    "text": "um",
    "start": "207040",
    "end": "208159"
  },
  {
    "text": "they'll be moving the data this all",
    "start": "208159",
    "end": "209840"
  },
  {
    "text": "happens in one node this happens in one",
    "start": "209840",
    "end": "211519"
  },
  {
    "text": "node and then this bit would be in one",
    "start": "211519",
    "end": "212879"
  },
  {
    "text": "node because you'd need to know the key",
    "start": "212879",
    "end": "214879"
  },
  {
    "text": "and then all the values that are",
    "start": "214879",
    "end": "216080"
  },
  {
    "text": "associated with it in order to to do",
    "start": "216080",
    "end": "218400"
  },
  {
    "text": "that computation because the other point",
    "start": "218400",
    "end": "220239"
  },
  {
    "text": "about mapreduce is data locality so",
    "start": "220239",
    "end": "222319"
  },
  {
    "text": "doing the computation",
    "start": "222319",
    "end": "224080"
  },
  {
    "text": "close to where the data is stored so you",
    "start": "224080",
    "end": "225680"
  },
  {
    "text": "want to minimize the amount you're",
    "start": "225680",
    "end": "227120"
  },
  {
    "text": "moving data around",
    "start": "227120",
    "end": "229280"
  },
  {
    "text": "um because that obviously takes time so",
    "start": "229280",
    "end": "232000"
  },
  {
    "text": "you want to",
    "start": "232000",
    "end": "232879"
  },
  {
    "text": "move data around the cluster as little",
    "start": "232879",
    "end": "235120"
  },
  {
    "text": "as possible and do the computations",
    "start": "235120",
    "end": "236959"
  },
  {
    "text": "close to where it's stored so that is",
    "start": "236959",
    "end": "238799"
  },
  {
    "start": "238000",
    "end": "264000"
  },
  {
    "text": "the end of the mapreduce process yeah",
    "start": "238799",
    "end": "241519"
  },
  {
    "text": "you've done the reduce that's it what",
    "start": "241519",
    "end": "243120"
  },
  {
    "text": "you then do with that data is up to you",
    "start": "243120",
    "end": "244879"
  },
  {
    "text": "so",
    "start": "244879",
    "end": "245599"
  },
  {
    "text": "in like a business use case you could be",
    "start": "245599",
    "end": "248000"
  },
  {
    "text": "using it to go over you know millions of",
    "start": "248000",
    "end": "250000"
  },
  {
    "text": "customer records and get some",
    "start": "250000",
    "end": "252400"
  },
  {
    "text": "kind of statistics out of it",
    "start": "252400",
    "end": "254640"
  },
  {
    "text": "um you could then save that back to the",
    "start": "254640",
    "end": "256560"
  },
  {
    "text": "distributed file system",
    "start": "256560",
    "end": "258400"
  },
  {
    "text": "which is probably what they'd want to do",
    "start": "258400",
    "end": "260079"
  },
  {
    "text": "for later use",
    "start": "260079",
    "end": "261440"
  },
  {
    "text": "uh yeah after that stage it's up to your",
    "start": "261440",
    "end": "263520"
  },
  {
    "text": "use case is this still used",
    "start": "263520",
    "end": "266320"
  },
  {
    "start": "264000",
    "end": "292000"
  },
  {
    "text": "and so it would still be used in some",
    "start": "266320",
    "end": "268320"
  },
  {
    "text": "cases because this is very good for kind",
    "start": "268320",
    "end": "270080"
  },
  {
    "text": "of doing it like a single batch job",
    "start": "270080",
    "end": "272320"
  },
  {
    "text": "you've got one computation you want to",
    "start": "272320",
    "end": "273600"
  },
  {
    "text": "do over the data and get a single result",
    "start": "273600",
    "end": "276080"
  },
  {
    "text": "out of it",
    "start": "276080",
    "end": "277680"
  },
  {
    "text": "but then it's not a very flexible way of",
    "start": "277680",
    "end": "279759"
  },
  {
    "text": "doing it",
    "start": "279759",
    "end": "281199"
  },
  {
    "text": "so for example",
    "start": "281199",
    "end": "283360"
  },
  {
    "text": "you've got to be able to fit your",
    "start": "283360",
    "end": "284400"
  },
  {
    "text": "computation into this map stage in this",
    "start": "284400",
    "end": "285919"
  },
  {
    "text": "reduced space and you've got to be",
    "start": "285919",
    "end": "287600"
  },
  {
    "text": "working with key value pairs and this is",
    "start": "287600",
    "end": "289680"
  },
  {
    "text": "in the apache hadoop version anyway it",
    "start": "289680",
    "end": "292479"
  },
  {
    "start": "292000",
    "end": "338000"
  },
  {
    "text": "can be quite painful to put something",
    "start": "292479",
    "end": "294400"
  },
  {
    "text": "into that framework or just",
    "start": "294400",
    "end": "296800"
  },
  {
    "text": "too difficult to do",
    "start": "296800",
    "end": "298960"
  },
  {
    "text": "and then secondly",
    "start": "298960",
    "end": "300560"
  },
  {
    "text": "because this does basically get the data",
    "start": "300560",
    "end": "302960"
  },
  {
    "text": "do map do the reduce and then it just",
    "start": "302960",
    "end": "306000"
  },
  {
    "text": "writes it back to disk it's not very",
    "start": "306000",
    "end": "307840"
  },
  {
    "text": "good at reusing that same data across",
    "start": "307840",
    "end": "309440"
  },
  {
    "text": "multiple computations because you're",
    "start": "309440",
    "end": "310800"
  },
  {
    "text": "constantly having to write stuff back to",
    "start": "310800",
    "end": "312080"
  },
  {
    "text": "disk reload it",
    "start": "312080",
    "end": "314800"
  },
  {
    "text": "so it's not good for like iterative",
    "start": "314800",
    "end": "316960"
  },
  {
    "text": "algorithms",
    "start": "316960",
    "end": "318400"
  },
  {
    "text": "so a lot of data mining stuff such as",
    "start": "318400",
    "end": "320800"
  },
  {
    "text": "k-means clustering that would be going",
    "start": "320800",
    "end": "322880"
  },
  {
    "text": "over the data again and again and again",
    "start": "322880",
    "end": "325120"
  },
  {
    "text": "which mapreduce is not very good for",
    "start": "325120",
    "end": "328560"
  },
  {
    "text": "um so then this there are then",
    "start": "328560",
    "end": "330880"
  },
  {
    "text": "more recent big data processing",
    "start": "330880",
    "end": "332639"
  },
  {
    "text": "frameworks such as apache spark um that",
    "start": "332639",
    "end": "334720"
  },
  {
    "text": "are kind of designed to alleviate those",
    "start": "334720",
    "end": "337199"
  },
  {
    "text": "issues having a massive file of text or",
    "start": "337199",
    "end": "340320"
  },
  {
    "start": "338000",
    "end": "379000"
  },
  {
    "text": "anything and then having to move bits of",
    "start": "340320",
    "end": "342320"
  },
  {
    "text": "it around that feels like it's a bit",
    "start": "342320",
    "end": "344000"
  },
  {
    "text": "clunky as well is that the case as well",
    "start": "344000",
    "end": "345759"
  },
  {
    "text": "do you think having the distributed file",
    "start": "345759",
    "end": "347840"
  },
  {
    "text": "you'd use a distributed file system so a",
    "start": "347840",
    "end": "349840"
  },
  {
    "text": "lot of this would be",
    "start": "349840",
    "end": "351199"
  },
  {
    "text": "sorted out for you um so for example the",
    "start": "351199",
    "end": "353600"
  },
  {
    "text": "hadoop distributed file system",
    "start": "353600",
    "end": "355759"
  },
  {
    "text": "if you have like a huge file it's like",
    "start": "355759",
    "end": "357759"
  },
  {
    "text": "terabytes then it kind of splits it up",
    "start": "357759",
    "end": "360080"
  },
  {
    "text": "into chunks and puts single chunks on",
    "start": "360080",
    "end": "362400"
  },
  {
    "text": "like individual nodes in your cluster",
    "start": "362400",
    "end": "364319"
  },
  {
    "text": "then when you're doing the processing",
    "start": "364319",
    "end": "366240"
  },
  {
    "text": "kind of the point of these frameworks is",
    "start": "366240",
    "end": "367919"
  },
  {
    "text": "that you don't have to think about it",
    "start": "367919",
    "end": "370479"
  },
  {
    "text": "so that bits kind of hidden from the",
    "start": "370479",
    "end": "372240"
  },
  {
    "text": "programmer so for example in mapreduce",
    "start": "372240",
    "end": "373840"
  },
  {
    "text": "the programmer doesn't have to worry too",
    "start": "373840",
    "end": "375039"
  },
  {
    "text": "much about the shuffle phase so that's",
    "start": "375039",
    "end": "376560"
  },
  {
    "text": "done automatically they just have to do",
    "start": "376560",
    "end": "377919"
  },
  {
    "text": "the map and the reduce you don't want to",
    "start": "377919",
    "end": "379840"
  },
  {
    "start": "379000",
    "end": "400000"
  },
  {
    "text": "be moving data around is the point",
    "start": "379840",
    "end": "381440"
  },
  {
    "text": "because that's taking up time you want",
    "start": "381440",
    "end": "383120"
  },
  {
    "text": "to be keeping everything local on a",
    "start": "383120",
    "end": "384639"
  },
  {
    "text": "single node as much as possible",
    "start": "384639",
    "end": "387120"
  },
  {
    "text": "train for a long time and let's not let",
    "start": "387120",
    "end": "389039"
  },
  {
    "text": "steve off a hook right there steve over",
    "start": "389039",
    "end": "390639"
  },
  {
    "text": "here high value of two high value of one",
    "start": "390639",
    "end": "392319"
  },
  {
    "text": "whatever that means the interesting",
    "start": "392319",
    "end": "393600"
  },
  {
    "text": "thing about this is we're not performing",
    "start": "393600",
    "end": "394960"
  },
  {
    "text": "a classification little endian systems",
    "start": "394960",
    "end": "397440"
  },
  {
    "text": "so that's why we call it endianness it",
    "start": "397440",
    "end": "399440"
  },
  {
    "text": "all traces back to",
    "start": "399440",
    "end": "402720"
  }
]