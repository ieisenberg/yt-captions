[
  {
    "start": "0",
    "end": "61000"
  },
  {
    "text": "cSo I wanted to make a video about",
    "start": "89",
    "end": "2089"
  },
  {
    "text": "GPT - 2",
    "start": "2439",
    "end": "3489"
  },
  {
    "text": "Because it's been in the news recently",
    "start": "3490",
    "end": "5109"
  },
  {
    "text": "this very powerful language model from open AI and I thought it would make sense to start by just doing a video about",
    "start": "5109",
    "end": "10499"
  },
  {
    "text": "transformers and language models in general because",
    "start": "11920",
    "end": "14429"
  },
  {
    "text": "GPT 2 is a very large",
    "start": "15549",
    "end": "17549"
  },
  {
    "text": "Language model implemented as a transformer, but you have a previous video about generating YouTube comments, which is the same kind of task, right?",
    "start": "17800",
    "end": "23880"
  },
  {
    "text": "That's a language modeling task from language processing to generate new samples for cooling of the most complex or magnetic",
    "start": "23880",
    "end": "30150"
  },
  {
    "text": "Consistent brackets like a computer to expect found in creating organizations",
    "start": "30369",
    "end": "34679"
  },
  {
    "text": "I believe that video was made October 2017 and this paper came out December 2017, which has kind of",
    "start": "34800",
    "end": "41640"
  },
  {
    "text": "Revolutionized the way that people carry out that kind of task. That's not the GPT -  2 that's something before that, right?",
    "start": "42160",
    "end": "47849"
  },
  {
    "text": "That's the transformer, which is a new realm. Yeah relatively new",
    "start": "47910",
    "end": "52199"
  },
  {
    "text": "architecture",
    "start": "52840",
    "end": "54370"
  },
  {
    "text": "for neural networks, that can do actually all kinds of tasks, but they're especially good at this kind of",
    "start": "54370",
    "end": "59189"
  },
  {
    "text": "language modeling task",
    "start": "59920",
    "end": "61920"
  },
  {
    "start": "61000",
    "end": "310000"
  },
  {
    "text": "a language model is a probability distribution over like sequences of",
    "start": "64489",
    "end": "68929"
  },
  {
    "text": "Tokens or symbols or words or whatever in a language?",
    "start": "70920",
    "end": "73489"
  },
  {
    "text": "So for any given like sequence of tokens, it can tell you how likely that is",
    "start": "73490",
    "end": "77540"
  },
  {
    "text": "So if you have a good language model of English",
    "start": "77880",
    "end": "80719"
  },
  {
    "text": "It can look at a sequence of you know words or characters or whatever and say how likely that is to occur in English",
    "start": "81060",
    "end": "87618"
  },
  {
    "text": "How likely that is to be an English phrase or sentence or whatever",
    "start": "88680",
    "end": "91909"
  },
  {
    "text": "And when you have that you can use that for a lot of different tasks. So",
    "start": "92460",
    "end": "97429"
  },
  {
    "text": "If you want to generate text, then you can you can just sort of sample from that distribution and keep giving it",
    "start": "98550",
    "end": "106369"
  },
  {
    "text": "its own output",
    "start": "107039",
    "end": "109039"
  },
  {
    "text": "so you you sample a word and then you say",
    "start": "109110",
    "end": "111800"
  },
  {
    "text": "And to be clear sampling from a distribution means you're just taking",
    "start": "112950",
    "end": "115909"
  },
  {
    "text": "Your you're sort of rolling the dice on that probability distribution and taking whichever one comes out. So",
    "start": "116729",
    "end": "122927"
  },
  {
    "text": "so you can like sample a word and then",
    "start": "124229",
    "end": "126709"
  },
  {
    "text": "And then say okay conditioning on that given that the first word of this sentence is V",
    "start": "127530",
    "end": "133399"
  },
  {
    "text": "What does the probability distribution look like for the second word?",
    "start": "133890",
    "end": "136549"
  },
  {
    "text": "And then you sample from that distribution and then it's you know",
    "start": "136549",
    "end": "139039"
  },
  {
    "text": "with a cat and you say given that it's the cat what's likely to come next and so on so you can you can build",
    "start": "139290",
    "end": "144228"
  },
  {
    "text": "up a",
    "start": "144230",
    "end": "145890"
  },
  {
    "text": "string of text by sampling from",
    "start": "145890",
    "end": "147890"
  },
  {
    "text": "Your distribution that's one of the things you could use it for",
    "start": "148170",
    "end": "150500"
  },
  {
    "text": "most of us kind of have an example of this sort of in our pockets of",
    "start": "150930",
    "end": "154159"
  },
  {
    "text": "Its actual absolutely right and that's like that's the that's the way that most people interact with a language model",
    "start": "154620",
    "end": "161360"
  },
  {
    "text": "I guess this is how I often start a sentence",
    "start": "161360",
    "end": "165320"
  },
  {
    "text": "apparently with I I am not sure if you have any questions or concerns, please visit the",
    "start": "165870",
    "end": "173299"
  },
  {
    "text": "Plugin settings so I can do it for the first time in the future of that's no good",
    "start": "173820",
    "end": "180559"
  },
  {
    "text": "Here's a different option. Let's just see what this way. Maybe the same",
    "start": "180660",
    "end": "183649"
  },
  {
    "text": "I am in the morning",
    "start": "183650",
    "end": "185900"
  },
  {
    "text": "But I can't find it on the phone screen from the phone screen on the phone screen on the phone screen on the phone screen",
    "start": "185970",
    "end": "193790"
  },
  {
    "text": "On the phone screen. I don't actually know how this is implemented",
    "start": "195030",
    "end": "197899"
  },
  {
    "text": "it might be a neural network, but my guess is that it's some kind of",
    "start": "197970",
    "end": "201859"
  },
  {
    "text": "like Markov model Markov chain type setup where you just",
    "start": "202800",
    "end": "206539"
  },
  {
    "text": "for each word in your language you look at your data set and you see how often a particular",
    "start": "207060",
    "end": "212750"
  },
  {
    "text": "how often each other word is",
    "start": "213360",
    "end": "215360"
  },
  {
    "text": "Following that word and then that's how you build your distribution",
    "start": "216120",
    "end": "219379"
  },
  {
    "text": "So like for the word \"I\" the most common word to follow that is \"am\" and there are a few others, you know",
    "start": "219380",
    "end": "225469"
  },
  {
    "text": "so this is like a very simple model and",
    "start": "225470",
    "end": "227470"
  },
  {
    "text": "This sentence on the phone screen on the phone screen on the phone screen on the phone screen on the phone screen",
    "start": "227850",
    "end": "231889"
  },
  {
    "text": "He's actually very unlikely, right?",
    "start": "232020",
    "end": "234409"
  },
  {
    "text": "This is the super low probability sentence where I would somebody type this and the thing is it's like myopic",
    "start": "234410",
    "end": "240919"
  },
  {
    "text": "It's only I'm not sure I even it's probably only looking at the previous word",
    "start": "240959",
    "end": "245539"
  },
  {
    "text": "It might be looking at like the previous two words, but the problem is to look back. It becomes extremely expensive",
    "start": "245540",
    "end": "250790"
  },
  {
    "text": "Computationally expensive right?",
    "start": "251760",
    "end": "253549"
  },
  {
    "text": "Like you've got I don't know 50,000 words that you might be looking at and so then it so you're you're you're remembering",
    "start": "253550",
    "end": "260088"
  },
  {
    "text": "50,000 probability distributions or",
    "start": "260430",
    "end": "262430"
  },
  {
    "text": "50,000 top three words",
    "start": "262890",
    "end": "264260"
  },
  {
    "text": "but you know then if you want to do",
    "start": "264260",
    "end": "266269"
  },
  {
    "text": "2, that's 50,000 squared right and if you want to go back three words",
    "start": "266610",
    "end": "269750"
  },
  {
    "text": "You have to cube it. So you like raising it to the power of the number of words back you want to go which is",
    "start": "269750",
    "end": "274850"
  },
  {
    "text": "Which means that this type of model?",
    "start": "275520",
    "end": "277520"
  },
  {
    "text": "Basically doesn't look back by the time we're saying on the it's already forgotten the previous time",
    "start": "278070",
    "end": "283159"
  },
  {
    "text": "It said on the it doesn't realize that it's repeating itself and there are slightly better things you can do in this general area",
    "start": "283160",
    "end": "289309"
  },
  {
    "text": "But like fundamentally if you don't remember you're not going to be able to make good sentences",
    "start": "289380",
    "end": "293899"
  },
  {
    "text": "If you can't remember the beginning of the sentence by the time you're at the end of it, right?",
    "start": "293900",
    "end": "297859"
  },
  {
    "text": "and",
    "start": "298590",
    "end": "299790"
  },
  {
    "text": "so",
    "start": "299790",
    "end": "301169"
  },
  {
    "text": "One of the big areas of progress in language models is handling long term dependencies",
    "start": "301169",
    "end": "306559"
  },
  {
    "text": "I mean handling dependencies of any kind but especially long term dependencies",
    "start": "306740",
    "end": "310490"
  },
  {
    "start": "310000",
    "end": "483000"
  },
  {
    "text": "You've got a sentence that's like Shawn came to the hack space to record a video and I talked to",
    "start": "310490",
    "end": "316819"
  },
  {
    "text": "Blank right in that situation if your model is good",
    "start": "317460",
    "end": "321979"
  },
  {
    "text": "you're expecting like a pronoun probably so it's it's she they",
    "start": "321979",
    "end": "327219"
  },
  {
    "text": "You know them whatever and but the relevant piece of information is the words short",
    "start": "328070",
    "end": "332469"
  },
  {
    "text": "Which is like all the way at the beginning of the sentence",
    "start": "333050",
    "end": "335228"
  },
  {
    "text": "so your model needs to be able to say oh, okay, you know Shawn that's",
    "start": "335230",
    "end": "338800"
  },
  {
    "text": "Usually associated with male pronouns, so we'll put the male pronoun in there. And if your model doesn't have that ability to look back",
    "start": "339410",
    "end": "346179"
  },
  {
    "text": "Or to just remember what it's just said then",
    "start": "346880",
    "end": "349989"
  },
  {
    "text": "You end up with these sentences that?",
    "start": "350900",
    "end": "352900"
  },
  {
    "text": "Like go nowhere",
    "start": "353000",
    "end": "354100"
  },
  {
    "text": "It's just a slight like it might make a guess",
    "start": "354100",
    "end": "356100"
  },
  {
    "text": "just a random guess at a pronoun and might get it wrong or it might just",
    "start": "356120",
    "end": "359798"
  },
  {
    "text": "and I talked to and then just be like",
    "start": "360500",
    "end": "362649"
  },
  {
    "text": "Frank, you know just like introduced a new name because it's guessing at what's likely to come there and it's completely forgotten that sure was",
    "start": "363290",
    "end": "370480"
  },
  {
    "text": "Ever like a thing. So yeah, these kind of dependencies are a big issue with things that you would want to language model to do",
    "start": "370480",
    "end": "377349"
  },
  {
    "text": "But we've only so far talked about",
    "start": "378500",
    "end": "380500"
  },
  {
    "text": "Language models for generating text in this way, but you can also use them for all kinds of different things. So like",
    "start": "381410",
    "end": "388450"
  },
  {
    "text": "people use language models for translation",
    "start": "389750",
    "end": "391779"
  },
  {
    "text": "Obviously you have some input sequence that's like in English and you want to output a sequence in French or something like that",
    "start": "391880",
    "end": "399010"
  },
  {
    "text": "Having a good language model is really important so that you end up with something. That makes sense",
    "start": "399410",
    "end": "403299"
  },
  {
    "text": "Summarization is a task that people often want",
    "start": "403940",
    "end": "406029"
  },
  {
    "text": "Where you read in a long piece of text and then you generate a short piece of text. That's like a summary of that",
    "start": "406130",
    "end": "411159"
  },
  {
    "text": "that's the kind of thing that you would use a language model for or",
    "start": "411590",
    "end": "414790"
  },
  {
    "text": "reading a piece of text and then answering questions about that text or",
    "start": "416210",
    "end": "419620"
  },
  {
    "text": "If you want to write like a chatbot that's going to converse with people having a language model as good like basically almost all",
    "start": "419900",
    "end": "425410"
  },
  {
    "text": "like natural language processing",
    "start": "425690",
    "end": "427690"
  },
  {
    "text": "right is it's useful to have this the other thing is",
    "start": "427910",
    "end": "430689"
  },
  {
    "text": "You can use it to enhance",
    "start": "431450",
    "end": "433450"
  },
  {
    "text": "Enhance a lot of other language related tasks",
    "start": "435380",
    "end": "438159"
  },
  {
    "text": "So if you're doing like speech recognition then having a good language model",
    "start": "438160",
    "end": "442660"
  },
  {
    "text": "Like there's a lot of things people can say that sound very similar and to get the right one",
    "start": "442660",
    "end": "446799"
  },
  {
    "text": "You need to be like, oh, well, this actually makes sense, you know",
    "start": "447260",
    "end": "450640"
  },
  {
    "text": "This word. That sounds very similar",
    "start": "451370",
    "end": "453370"
  },
  {
    "text": "Would be incoherent in this sentence. It's a very low probability",
    "start": "454070",
    "end": "456609"
  },
  {
    "text": "It's much more likely that they this thing which is like would flow in the language",
    "start": "456680",
    "end": "460750"
  },
  {
    "text": "And human beings do this all the time same thing",
    "start": "461210",
    "end": "464108"
  },
  {
    "text": "With recognizing text from images, you know",
    "start": "465500",
    "end": "468760"
  },
  {
    "text": "You've got two words that look similar or there's some ambiguity or whatever and to resolve that you need",
    "start": "468760",
    "end": "473349"
  },
  {
    "text": "an",
    "start": "473690",
    "end": "474350"
  },
  {
    "text": "understanding of what word would make sense there what word would fit if you're trying to use a neural network to do the kind of",
    "start": "474350",
    "end": "479200"
  },
  {
    "text": "thing we were talking about before, of having a phone, you know autocorrect based on the previous word or two",
    "start": "479200",
    "end": "483980"
  },
  {
    "start": "483000",
    "end": "820000"
  },
  {
    "text": "Suppose you've got a sequence of two words going in you've got \"so\" and then \"I\" and you put",
    "start": "484020",
    "end": "489419"
  },
  {
    "text": "both of these into your network and it will then output, you know",
    "start": "489560",
    "end": "494350"
  },
  {
    "text": "like \"said\" for example as like a sensible next word and then what you do is you throw away or so and you then",
    "start": "494350",
    "end": "499989"
  },
  {
    "text": "Bring your set around and you make a new",
    "start": "500390",
    "end": "502390"
  },
  {
    "text": "Sequence which is I said and then put that into your network and it will put out",
    "start": "503120",
    "end": "507699"
  },
  {
    "text": "like I said - for example would make sense and so on and you keep going around, but the problem is",
    "start": "507700",
    "end": "513369"
  },
  {
    "text": "This length is really short you try and make this long enough to contain an entire",
    "start": "513620",
    "end": "518409"
  },
  {
    "text": "Sentence just an ordinary length sentence and this problem starts to become really really hard",
    "start": "519169",
    "end": "524018"
  },
  {
    "text": "And networks have a hard time learning it and you don't get very good performance",
    "start": "524690",
    "end": "528760"
  },
  {
    "text": "and even then",
    "start": "529580",
    "end": "531580"
  },
  {
    "text": "You're still like have this absolute hard limit on how long a thing you you have to just pick a number",
    "start": "531620",
    "end": "537099"
  },
  {
    "text": "That's like how far back am I looking a better thing to do you say recurring neural network? Where you",
    "start": "537100",
    "end": "542350"
  },
  {
    "text": "You give the thing. Let's like divide that up",
    "start": "542720",
    "end": "544720"
  },
  {
    "text": "So in this case, then you have a network you give it this vector?",
    "start": "544850",
    "end": "548139"
  },
  {
    "text": "You just like have a bunch of numbers which is gonna be like the memory",
    "start": "548140",
    "end": "552969"
  },
  {
    "text": "for that network is the idea like the problem is it's forgotten in the beginning of the sentence by the time it gets to the",
    "start": "553310",
    "end": "557500"
  },
  {
    "text": "end so we've got to give it some way of remembering and",
    "start": "557500",
    "end": "559780"
  },
  {
    "text": "rather than feeding it the entire sentence every time you give it this vector and",
    "start": "560300",
    "end": "564880"
  },
  {
    "text": "you give it to just one word at a time of your inputs and",
    "start": "565730",
    "end": "568779"
  },
  {
    "text": "This vector, which you initialize I guess with zeros. I want to be clear",
    "start": "569510",
    "end": "573129"
  },
  {
    "text": "This is not something that I've studied in a huge amount of detail",
    "start": "573130",
    "end": "576010"
  },
  {
    "text": "I'm just like giving the overall like structure of the thing. But the point is you give it this vector and the word and",
    "start": "576010",
    "end": "581200"
  },
  {
    "text": "it outputs its guess for the next word and also a",
    "start": "581900",
    "end": "585759"
  },
  {
    "text": "Modified version of that vector that you then for the next thing you give it",
    "start": "586400",
    "end": "591039"
  },
  {
    "text": "where did it spit out or the sequence that it spit out and",
    "start": "591580",
    "end": "593770"
  },
  {
    "text": "Its own modified version of the vector every cycle that goes around. It's modifying this memory",
    "start": "594110",
    "end": "599409"
  },
  {
    "text": "Once this system is like trained very well",
    "start": "599450",
    "end": "602049"
  },
  {
    "text": "If you give it if you give it the first word Shawn then part of this vector is going to contain some",
    "start": "602050",
    "end": "607870"
  },
  {
    "text": "information that's like this subject of this sentence is the word short and",
    "start": "608390",
    "end": "612009"
  },
  {
    "text": "some other part will probably keep track of like",
    "start": "612560",
    "end": "614679"
  },
  {
    "text": "We expect to use a male pronoun for this sentence and that kind of thing",
    "start": "615470",
    "end": "620199"
  },
  {
    "text": "So you take this and give it to that and these are just two instances of the same network, and then it keeps going",
    "start": "620200",
    "end": "626920"
  },
  {
    "text": "every time",
    "start": "627410",
    "end": "628519"
  },
  {
    "text": "So it spits out like this is I so then the AI also comes around to here you might then put outside and so on",
    "start": "628520",
    "end": "634660"
  },
  {
    "text": "But it's got this continuous thread of",
    "start": "634670",
    "end": "637659"
  },
  {
    "text": "of memory effectively going through because it keeps passing the thing through in principle if it figures out something important at the beginning of",
    "start": "638990",
    "end": "645549"
  },
  {
    "text": "You know",
    "start": "645860",
    "end": "647450"
  },
  {
    "text": "The complete works of Shakespeare that it's generating. There's nothing",
    "start": "647450",
    "end": "650830"
  },
  {
    "text": "Strictly speaking stopping that from persisting from being passed through",
    "start": "651470",
    "end": "655389"
  },
  {
    "text": "From from iteration to iteration to iteration every time",
    "start": "656150",
    "end": "659530"
  },
  {
    "text": "In practice, it doesn't work that way because in practice",
    "start": "660440",
    "end": "664270"
  },
  {
    "text": "The whole thing is being messed with by the network on every step and so in in the training process it's going to learn",
    "start": "664970",
    "end": "672339"
  },
  {
    "text": "That it performs best when it leaves most of it alone and it doesn't just randomly change the whole thing",
    "start": "673100",
    "end": "678850"
  },
  {
    "text": "But by the time you're on the fiftieth word of your sentence",
    "start": "679010",
    "end": "682059"
  },
  {
    "text": "whatever the network decided to do on the first word of the sentence is a",
    "start": "682760",
    "end": "687069"
  },
  {
    "text": "photocopy of a photocopy of a photocopy of a photocopy and so",
    "start": "687290",
    "end": "690368"
  },
  {
    "text": "things have a tendency to",
    "start": "691370",
    "end": "693370"
  },
  {
    "text": "Fade out to nothing. It has to be successfully remembered at every step of this process",
    "start": "693620",
    "end": "698410"
  },
  {
    "text": "and if at any point it gets overwritten with something else or just",
    "start": "698410",
    "end": "700899"
  },
  {
    "text": "It did its best to remember it but it's actually remembering 99% of it each time point nine",
    "start": "701420",
    "end": "706089"
  },
  {
    "text": "Nine to the fifty is like actually not that big of a number",
    "start": "706090",
    "end": "708910"
  },
  {
    "text": "So these things work pretty well, but they still get the performance like really quickly drops off once the sentences start to get long",
    "start": "708910",
    "end": "716319"
  },
  {
    "text": "So this is a recurrent neural network",
    "start": "716320",
    "end": "718689"
  },
  {
    "text": "rnl because all of these boxes",
    "start": "719300",
    "end": "722139"
  },
  {
    "text": "Are really the same box because this is the same network at different time steps. It's really a loop like this",
    "start": "723500",
    "end": "729429"
  },
  {
    "text": "You're giving the output of the network back as input every time so this works better and then people have tried all kinds of interesting",
    "start": "729429",
    "end": "735129"
  },
  {
    "text": "Things things like LS TMS. There's all kinds of variants on this general like recurrent Network",
    "start": "735129",
    "end": "740199"
  },
  {
    "text": "LS TM is the thing. That might use isn't it? Right right long short-term memory, which is kind of surreal",
    "start": "740720",
    "end": "746949"
  },
  {
    "text": "But yeah, so the idea of that is it's a lot more complicated inside these networks",
    "start": "746949",
    "end": "750488"
  },
  {
    "text": "There's actually kind of sub networks that make specific decisions about gating things. So",
    "start": "750489",
    "end": "756219"
  },
  {
    "text": "Rather than having to have this system learn that it ought to pass most things on it's sort of more in the architecture that passes",
    "start": "756799",
    "end": "763358"
  },
  {
    "text": "most things on and then there's a there's a sub there's like part of the learning is",
    "start": "763359",
    "end": "767589"
  },
  {
    "text": "Deciding what to forget",
    "start": "768619",
    "end": "770029"
  },
  {
    "text": "At each step and like deciding what to change and what to put it in what parcel and so on and they perform better",
    "start": "770029",
    "end": "775178"
  },
  {
    "text": "They can hang on to the information the relevant information for longer",
    "start": "775249",
    "end": "778389"
  },
  {
    "text": "But the other thing that people often build into these kinds of systems is something called attention",
    "start": "780499",
    "end": "785738"
  },
  {
    "text": "Which is actually a pretty good metaphor",
    "start": "786709",
    "end": "790298"
  },
  {
    "text": "Where in the same way that you would have?",
    "start": "791089",
    "end": "793089"
  },
  {
    "text": "networks that decide which parts of your hidden state to hang on to or which starts to forget or",
    "start": "793549",
    "end": "798819"
  },
  {
    "text": "Those kinds of decisions like gating and stuff",
    "start": "800239",
    "end": "802478"
  },
  {
    "text": "You have a system which is deciding which parts of the input to pay attention to which parts to use in",
    "start": "803929",
    "end": "811329"
  },
  {
    "text": "The in the calculation and which parts to ignore and this turns out to be actually very powerful. So there was this paper",
    "start": "811669",
    "end": "818769"
  },
  {
    "text": "When was this?",
    "start": "819679",
    "end": "820999"
  },
  {
    "start": "820000",
    "end": "1038000"
  },
  {
    "text": "2000",
    "start": "820999",
    "end": "822049"
  },
  {
    "text": "2017. Yeah, so this is funny because this came out the same year as",
    "start": "822049",
    "end": "826299"
  },
  {
    "text": "The video you have about generating YouTube comments. This is in December. I think that video was October ancient history now",
    "start": "826879",
    "end": "833108"
  },
  {
    "text": "Alright, we're talking two years ago. The idea of this is as its called attention is all you need. They developed this system. Whereby",
    "start": "833109",
    "end": "839679"
  },
  {
    "text": "it's actually as",
    "start": "840259",
    "end": "842149"
  },
  {
    "text": "it's a lot simpler as a",
    "start": "842149",
    "end": "844298"
  },
  {
    "text": "As a network you can see on the diagram here if you compare this to the diagram for an LS TM or",
    "start": "845869",
    "end": "850928"
  },
  {
    "text": "Any of those kind of variants? It's relatively simple and it's just kind of using attention to do everything",
    "start": "851539",
    "end": "857679"
  },
  {
    "text": "So when made that video the ASTM type stuff was like state-of-the-art and that was until a couple of months later",
    "start": "858049",
    "end": "864419"
  },
  {
    "text": "I guess when this paper came out the idea of this is that attention is all you need of it like this stuff about",
    "start": "864420",
    "end": "871440"
  },
  {
    "text": "having gates for forgetting things and",
    "start": "872050",
    "end": "874680"
  },
  {
    "text": "All of that all of that kind of stuff in fact your whole recurrence like architecture",
    "start": "876160",
    "end": "880529"
  },
  {
    "text": "you can do away with it and just use attention attention is powerful enough to",
    "start": "881500",
    "end": "885239"
  },
  {
    "text": "do everything that you need at its base attention is about actively deciding in the same way that",
    "start": "885940",
    "end": "891659"
  },
  {
    "text": "the LS TM is actively deciding what to forget and so on this is deciding which parts of",
    "start": "893139",
    "end": "900358"
  },
  {
    "text": "some other part of the data it's going to",
    "start": "901000",
    "end": "903419"
  },
  {
    "text": "take into account which parts it's going to look at like it can be very dangerous in AI to",
    "start": "903550",
    "end": "908579"
  },
  {
    "text": "use words for things that are words that people already use",
    "start": "909639",
    "end": "913858"
  },
  {
    "text": "For the way that humans do things. It makes it very easy transform for more finds and just",
    "start": "914380",
    "end": "918989"
  },
  {
    "text": "make, you know get confused because the abstraction doesn't quite work but I think attention is a pretty decent thing because it is",
    "start": "919690",
    "end": "926549"
  },
  {
    "text": "It does make sense",
    "start": "927519",
    "end": "928560"
  },
  {
    "text": "It sort of draws the relationships between things so you can have attention from the output to the input",
    "start": "928560",
    "end": "932819"
  },
  {
    "text": "Which is what that would be you can also have attention from the output to other parts of the output",
    "start": "933040",
    "end": "938040"
  },
  {
    "text": "so for example when I'm generating in that sentence like",
    "start": "938560",
    "end": "942419"
  },
  {
    "text": "Shawn came to record a video or whatever by the time I get to generating the word him",
    "start": "943000",
    "end": "946949"
  },
  {
    "text": "I don't need to be thinking about the entire sentence",
    "start": "946949",
    "end": "949469"
  },
  {
    "text": "I can just focus my attention on where I remember",
    "start": "949470",
    "end": "952620"
  },
  {
    "text": "The name was so the attention goes to Shawn and then I can make the decision for to use the word him based on",
    "start": "952930",
    "end": "959010"
  },
  {
    "text": "that",
    "start": "959560",
    "end": "960670"
  },
  {
    "text": "so",
    "start": "960670",
    "end": "962649"
  },
  {
    "text": "so rather than having to hang on to a huge amount of memory you",
    "start": "962649",
    "end": "965878"
  },
  {
    "text": "Can just selectively look at the things that are actually relevant and the system learns",
    "start": "967810",
    "end": "973079"
  },
  {
    "text": "Where to look where to pay attention to and that's really cool like you can do it",
    "start": "973480",
    "end": "978509"
  },
  {
    "text": "There's attention based systems for all kinds of things like not just text you can do",
    "start": "978540",
    "end": "982889"
  },
  {
    "text": "Like suppose you have your input is like an image and you want to caption it",
    "start": "984279",
    "end": "988469"
  },
  {
    "text": "You can actually look at when it was outputting the sequence you can say when you generated the word dog",
    "start": "988470",
    "end": "993050"
  },
  {
    "text": "What was your you can get like an attention heat map and it will highlight the dog",
    "start": "993420",
    "end": "997370"
  },
  {
    "text": "Because that's the part of the image that it was paying attention to when it generated that output",
    "start": "997440",
    "end": "1001690"
  },
  {
    "text": "It makes your system more interpretable because you can see what it was thinking and sometimes you can catch problems that way as well",
    "start": "1001690",
    "end": "1008049"
  },
  {
    "text": "which is kind of fun like",
    "start": "1008050",
    "end": "1009560"
  },
  {
    "text": "It generates the output that's like a man is lifting a dumbbell or something like that and you look at it",
    "start": "1009560",
    "end": "1015878"
  },
  {
    "text": "And it's not actually correct. It's like its owner trots and I go he's drinking some tea out of a mug, right and",
    "start": "1015879",
    "end": "1021219"
  },
  {
    "text": "what you find is then when you look at your",
    "start": "1022100",
    "end": "1024100"
  },
  {
    "text": "Outputs where it says dumbbell you look at the attention and the attention is like mostly looking at the arms. That's usually somebody muscular",
    "start": "1024199",
    "end": "1031089"
  },
  {
    "text": "Who's lifting the dumbbell in your photos?",
    "start": "1031089",
    "end": "1032558"
  },
  {
    "text": "It's and so it it's overriding the fact that this kind of looks like a mug because it was looking at the arms",
    "start": "1032559",
    "end": "1037537"
  },
  {
    "text": "So the idea is this system which is called a transformer is a type of neural network",
    "start": "1037539",
    "end": "1042728"
  },
  {
    "start": "1038000",
    "end": "1144000"
  },
  {
    "text": "which just relies very heavily on attention to",
    "start": "1042730",
    "end": "1045880"
  },
  {
    "text": "Produce like state-of-the-art performance and if you train them on a large",
    "start": "1046429",
    "end": "1050949"
  },
  {
    "text": "corpus of natural language they can learn",
    "start": "1051860",
    "end": "1054669"
  },
  {
    "text": "They can learn to do very well, right they give you they can be very powerful language models",
    "start": "1056539",
    "end": "1061569"
  },
  {
    "text": "We had the example of a language model on your phone",
    "start": "1061570",
    "end": "1064059"
  },
  {
    "text": "That's like a very very basic and then trying to do this with neural networks and the problems with remembering",
    "start": "1064059",
    "end": "1069699"
  },
  {
    "text": "And so you have like recurrent systems that keep track of they allow you to pass memory along so that you can remember the beginning",
    "start": "1070340",
    "end": "1076449"
  },
  {
    "text": "of the sentence at least by the end of it and",
    "start": "1076450",
    "end": "1078760"
  },
  {
    "text": "Things like LSTMs there is all these different varieties that people try different things",
    "start": "1079520",
    "end": "1083889"
  },
  {
    "text": "That are better and hanging on to memory so that they can do better it they can have longer term",
    "start": "1085130",
    "end": "1089949"
  },
  {
    "text": "Dependencies, which allows you to have more coherent",
    "start": "1090080",
    "end": "1092980"
  },
  {
    "text": "outputs",
    "start": "1094400",
    "end": "1095539"
  },
  {
    "text": "in just generally better performance, and then the transformer is",
    "start": "1095539",
    "end": "1099518"
  },
  {
    "text": "Is a variant on that?",
    "start": "1100280",
    "end": "1102280"
  },
  {
    "text": "Well is a different way of doing things where you really focus on attention. And so these are actually not recurrent which is an",
    "start": "1102409",
    "end": "1110409"
  },
  {
    "text": "important distinction to make we don't have this thing of like",
    "start": "1111049",
    "end": "1113979"
  },
  {
    "text": "Taking the output and feeding that back as the input and so on every time",
    "start": "1114169",
    "end": "1117368"
  },
  {
    "text": "Because we have attention. We don't need to keep a big memory",
    "start": "1117890",
    "end": "1122319"
  },
  {
    "text": "That we run through every time when the system wants to know something it can use its attention to look back to that part",
    "start": "1123350",
    "end": "1130640"
  },
  {
    "text": "It's not like memorizing the text as it goes. It's",
    "start": "1130980",
    "end": "1134419"
  },
  {
    "text": "paying attention to different bits of the text as",
    "start": "1134970",
    "end": "1137360"
  },
  {
    "text": "they as it thinks that they're relevant to the bit that it's looking at now and",
    "start": "1138059",
    "end": "1141498"
  },
  {
    "text": "The thing about that is when you have this recurrent thing",
    "start": "1142740",
    "end": "1145459"
  },
  {
    "text": "It's kind of inherently serial",
    "start": "1145890",
    "end": "1147890"
  },
  {
    "text": "most of the calculations for this you can't do them until you have",
    "start": "1147900",
    "end": "1150979"
  },
  {
    "text": "The inputs and the inputs are the output of the previous network. And so",
    "start": "1151410",
    "end": "1155659"
  },
  {
    "text": "You can't do the thing that people like to do now, which is run it on a million computers",
    "start": "1156270",
    "end": "1160729"
  },
  {
    "text": "And get lightning-fast performance because you have to go through them in order right? It's like inherently serial",
    "start": "1161309",
    "end": "1166219"
  },
  {
    "text": "Where as transformers are much more parallelizable, which means you get better computational performance out of them as well?",
    "start": "1167130",
    "end": "1172760"
  },
  {
    "text": "Which is another",
    "start": "1174120",
    "end": "1176120"
  },
  {
    "text": "Selling point so they they work better and they run faster. So they're they're really a",
    "start": "1176130",
    "end": "1180829"
  },
  {
    "text": "Step up. So transformers. Are this really powerful",
    "start": "1181740",
    "end": "1184370"
  },
  {
    "text": "architecture.  They seem to give really good performance on this kind of sort of language modeling type tasks and",
    "start": "1185460",
    "end": "1191658"
  },
  {
    "text": "we",
    "start": "1192840",
    "end": "1193890"
  },
  {
    "text": "But what we didn't know really was how far you can push them or how how good they can get",
    "start": "1193890",
    "end": "1198439"
  },
  {
    "text": "What happens if you take this architecture and you give it a bigger data set than any of them has ever been given and more?",
    "start": "1198720",
    "end": "1206059"
  },
  {
    "text": "Compute to train with, you know, a larger model with more parameters and more data",
    "start": "1206370",
    "end": "1211010"
  },
  {
    "text": "How good can these things get how how good a language model?",
    "start": "1211320",
    "end": "1215299"
  },
  {
    "text": "Can you actually make and that's what opening I was doing with GPT 2?",
    "start": "1215299",
    "end": "1218778"
  },
  {
    "text": "So an executable binary the net effect of slotting that T diagram against here slightly downwards is to show you",
    "start": "1219780",
    "end": "1227089"
  },
  {
    "text": "That the C you've written gets converted into binary and the net output from this",
    "start": "1228450",
    "end": "1234169"
  },
  {
    "text": "process it produces out a program that you probably store in a",
    "start": "1234990",
    "end": "1238819"
  }
]