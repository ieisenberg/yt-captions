[
  {
    "text": "so last time we looked at Markoff decision processes which are a way of modeling decision-making problems",
    "start": "40",
    "end": "5120"
  },
  {
    "text": "particularly ones under uncertainty but we never got around to talking about how you actually solve them so we just looked at them as a modeling Tool uh and",
    "start": "5120",
    "end": "11320"
  },
  {
    "text": "today we're going to look at a algorithm called value iteration which is how you produce action decisions from those",
    "start": "11320",
    "end": "17039"
  },
  {
    "text": "decision processes so we're going to go back and look I think to start with it maybe",
    "start": "17039",
    "end": "23320"
  },
  {
    "text": "recap what a mark of decision process is and when I say that I'm just going to say mdp so that's the that's the",
    "start": "23320",
    "end": "29480"
  },
  {
    "text": "abbreviation so I will say mdp from now on and we've got this so this is the model we had from last time so this was",
    "start": "29480",
    "end": "36399"
  },
  {
    "text": "the the sort of Transport or getting to work mdp if you're thinking about just the average cost then everything gets a",
    "start": "36399",
    "end": "42719"
  },
  {
    "text": "lot easier I made a slightly fancier version um this time so you start at home your goal is to get to work and",
    "start": "42719",
    "end": "49840"
  },
  {
    "text": "you've got effectively three action choices you can choose to take the train you can choose to take a car or you can",
    "start": "49840",
    "end": "56280"
  },
  {
    "text": "choose to to go on your bike and this is a way now we can start to think about the more formal way of of thinking about",
    "start": "56280",
    "end": "62559"
  },
  {
    "text": "the mdp so mdp has a set of states which describe it in in in this",
    "start": "62559",
    "end": "69159"
  },
  {
    "text": "model they're kind of the locations you can be in but in general they're sort of like a snapshot of the world and we we",
    "start": "69159",
    "end": "74880"
  },
  {
    "text": "think of these uh we typically write big S capital S for for a state and we say",
    "start": "74880",
    "end": "80320"
  },
  {
    "text": "the lowercase some state is an element of this this set in this example our states will be things like uh at home in",
    "start": "80320",
    "end": "87280"
  },
  {
    "text": "the waiting room on a train in the car in different traffic levels or on a in",
    "start": "87280",
    "end": "92560"
  },
  {
    "text": "fact or or at work um and then we've got actions so the actions are the choices",
    "start": "92560",
    "end": "98320"
  },
  {
    "text": "we can make uh and we we usually label them a out of some big set there and here the",
    "start": "98320",
    "end": "104520"
  },
  {
    "text": "actions are things like take the car take the railway cycle we've got a drive",
    "start": "104520",
    "end": "109840"
  },
  {
    "text": "action relax wait and go home so these are Al will be in the previous video and and explained it a bit uh and then the",
    "start": "109840",
    "end": "116680"
  },
  {
    "text": "the kind of interesting things that we're modeling we've got we've got costs sometimes people would use rewards",
    "start": "116680",
    "end": "121880"
  },
  {
    "text": "interchangeably uh and the costs are going to be how much it costs to execute a particular action and we're going to",
    "start": "121880",
    "end": "128399"
  },
  {
    "text": "have a what we call a cost function which is going to be C SAA so I think",
    "start": "128399",
    "end": "133840"
  },
  {
    "text": "about the cost of executing some action a in a state s and then I have um my",
    "start": "133840",
    "end": "139760"
  },
  {
    "text": "transition function which is going to give me the probability of ending up in a particular state after executing a",
    "start": "139760",
    "end": "146560"
  },
  {
    "text": "particular action so you would write that typically with a t s a s Prime and",
    "start": "146560",
    "end": "153040"
  },
  {
    "text": "the S Prime is the successor State the state you get to and so if we look at um",
    "start": "153040",
    "end": "158640"
  },
  {
    "text": "say the the model here we've got our our home state our action car and then we",
    "start": "158640",
    "end": "165440"
  },
  {
    "text": "have three different S Prime so three different states we could reach so this is light traffic medium traffic and",
    "start": "165440",
    "end": "171040"
  },
  {
    "text": "heavy traffic those are all different states um so the states are the circles right in that picture so this is our mdp",
    "start": "171040",
    "end": "177280"
  },
  {
    "text": "we kind of assume we know this model we assume we know our transition function you could also think about this transition function actually as a sort",
    "start": "177280",
    "end": "182879"
  },
  {
    "text": "of conditional probability so typically that's going to be the probability of of being an S Prime given that I started in",
    "start": "182879",
    "end": "189760"
  },
  {
    "text": "in state s and I execute action so this is kind of an equivalent way of of thinking about it so this is on our",
    "start": "189760",
    "end": "195799"
  },
  {
    "text": "edges these are the the numbers after this Dot and then our costs are the things we've got after these colons so",
    "start": "195799",
    "end": "202360"
  },
  {
    "text": "that that tells you that's kind of the the pictorial pictorial version of this right clear so far yeah I think so yeah",
    "start": "202360",
    "end": "208760"
  },
  {
    "text": "so just as a kind of if I took the car there's a possibility that the traffic is heavy and therefore the time that it",
    "start": "208760",
    "end": "215200"
  },
  {
    "text": "takes is our cost in that instance it's going to be longer yeah but the way the way we've modeled it here is when I get",
    "start": "215200",
    "end": "220760"
  },
  {
    "text": "in the car this is just kind of a quirk of how you have to model things in an mdp it's not always obvious the right",
    "start": "220760",
    "end": "225920"
  },
  {
    "text": "way right place to put the costs so here we have this intermediate state where the state is kind of effectively being",
    "start": "225920",
    "end": "232400"
  },
  {
    "text": "on the road in light traffic or being on the road in medium traffic and then we",
    "start": "232400",
    "end": "237799"
  },
  {
    "text": "we have one action drive but I can EX it in in any of those States yes so it's the same action but when I execute it in",
    "start": "237799",
    "end": "244799"
  },
  {
    "text": "different states it costs me different things so when I execute the drive action in the medium traffic state it",
    "start": "244799",
    "end": "250560"
  },
  {
    "text": "cost me 30 and that's why if you look back to this this function it's both kind of an S and an A in the cost",
    "start": "250560",
    "end": "256280"
  },
  {
    "text": "function it's like okay I'm executing this action which might be drive and then this could be light traffic LT this",
    "start": "256280",
    "end": "262639"
  },
  {
    "text": "is a decision-making model what we want is action so the reason you've got an mdp is because ultimately you want to",
    "start": "262639",
    "end": "268320"
  },
  {
    "text": "get uh some action choices uh and way we describe this or",
    "start": "268320",
    "end": "273960"
  },
  {
    "text": "the way we kind of encode the actions is in a policy so policy is the output when I when I say solve an mdp people talk",
    "start": "273960",
    "end": "280160"
  },
  {
    "text": "about solving mdp they're solving it to produce a policy so a policy we're going to represent with pi or P if I put in a",
    "start": "280160",
    "end": "287639"
  },
  {
    "text": "state it's going to give me an action back so you can think about this as a lookup table or a map the input is the",
    "start": "287639",
    "end": "294520"
  },
  {
    "text": "state the output is the action and when we solve our mdp we get a policy",
    "start": "294520",
    "end": "301160"
  },
  {
    "text": "uh and so that's really the key thing uh and we we'll probably want well in this",
    "start": "301160",
    "end": "306440"
  },
  {
    "text": "case we're going to want a policy that covers all of the states in our problem so then the question is what",
    "start": "306440",
    "end": "313600"
  },
  {
    "text": "policy do we produce or how do we think about this policy in the research that I do we think about the policy as being",
    "start": "313600",
    "end": "319479"
  },
  {
    "text": "produced to meet some kind of specification which is kind of some set of requirements you've got for solving",
    "start": "319479",
    "end": "325560"
  },
  {
    "text": "the mdp in our case for this example what we want to do is minimize cost right we want to",
    "start": "325560",
    "end": "332199"
  },
  {
    "text": "minimize the travel time to work and therefore we're going to minimize this",
    "start": "332199",
    "end": "337720"
  },
  {
    "text": "cost function in fact more specifically we're going to minimize the cost function to reach the goal typically we",
    "start": "337720",
    "end": "342919"
  },
  {
    "text": "might have some State that's an element of a set G and G would be our set of goal States and that can be more than",
    "start": "342919",
    "end": "348080"
  },
  {
    "text": "one state and for for this kind of mdp and and the work that I do we think about typically achieving goals because",
    "start": "348080",
    "end": "354560"
  },
  {
    "text": "that's an interesting way to think about the world uh but there are other solution mechanisms and other people",
    "start": "354560",
    "end": "360440"
  },
  {
    "text": "that think about mdps that think about things like reward and they might do a reward maximization rather than cost",
    "start": "360440",
    "end": "366599"
  },
  {
    "text": "minimization and often in in things like reinforcement learning people think about infinite Horizon discounted reward",
    "start": "366599",
    "end": "373319"
  },
  {
    "text": "maximization which is like your system's going to run forever and I just want to kind of maximize the amount of reward I",
    "start": "373319",
    "end": "378560"
  },
  {
    "text": "get for the next five or six steps but doing that over the an infinite Horizon so I keep kind of pushing forward",
    "start": "378560",
    "end": "384120"
  },
  {
    "text": "continuing and continuing yeah okay um and there there's other things as well so you can think about specification",
    "start": "384120",
    "end": "389880"
  },
  {
    "text": "such as uh we could represent a specification as something called temporal logic that instead of just",
    "start": "389880",
    "end": "395479"
  },
  {
    "text": "describing a set of states it might describe a sequence of states or a conditional sequence of States so one of",
    "start": "395479",
    "end": "401440"
  },
  {
    "text": "my students loves this um carpet puddle example where you say I'm I'm a robot in",
    "start": "401440",
    "end": "406880"
  },
  {
    "text": "a home and if I drive through a puddle uh then I have to go over the carpet to drive my wheels before I go into my",
    "start": "406880",
    "end": "413759"
  },
  {
    "text": "charging station and then you can also think about specifications that encode constraints say I want to reach work in",
    "start": "413759",
    "end": "419879"
  },
  {
    "text": "a maximum of 60 minutes that's going to change the set of options that I've got",
    "start": "419879",
    "end": "425240"
  },
  {
    "text": "and that constraint you can think of as a specification so I want to produce a policy to meet my specification and",
    "start": "425240",
    "end": "431440"
  },
  {
    "text": "there's a whole amount of richness that you can put into into the the specification and that will change both",
    "start": "431440",
    "end": "437599"
  },
  {
    "text": "the kind of algorithms you need to use to to produce the policy and also the kind of behavior you get at the end we're going to in this case minimize the",
    "start": "437599",
    "end": "445680"
  },
  {
    "text": "expected cost so cost we know this is this C that we had earlier cost is a",
    "start": "445680",
    "end": "452319"
  },
  {
    "text": "random variable so this is a mark of decision process there there are probabilistic outcomes to our actions if",
    "start": "452319",
    "end": "458599"
  },
  {
    "text": "we go back to this car example when I get in the car I have different probabilities of reaching these",
    "start": "458599",
    "end": "464080"
  },
  {
    "text": "different states traffic levels and then each traffic level yields a different cost when I execute the drive",
    "start": "464080",
    "end": "470599"
  },
  {
    "text": "action so if I was to say what is the cost of of driving there isn't one",
    "start": "470599",
    "end": "477120"
  },
  {
    "text": "answer there's a distribution of answers and each answer each cost has a different probability of occurring and",
    "start": "477120",
    "end": "483360"
  },
  {
    "text": "the expectation you can think of as like the average given the probabilities so",
    "start": "483360",
    "end": "488560"
  },
  {
    "text": "the expectation of a random variable is the sum over its outcomes so its costs",
    "start": "488560",
    "end": "494520"
  },
  {
    "text": "where each outcome is then multiplied by its probability if I say like what's the expected cost of I'm not going to be too",
    "start": "494520",
    "end": "501280"
  },
  {
    "text": "too tight on the symbols what's the expected cost of of effectively taking the car of the drive action well it's",
    "start": "501280",
    "end": "508720"
  },
  {
    "text": "the probability of ending up in the light",
    "start": "508720",
    "end": "513919"
  },
  {
    "text": "traffic times by the cost of driving after the light traffic plus the probability of uh",
    "start": "513919",
    "end": "522760"
  },
  {
    "text": "ending up in medium traffic times its cost times the",
    "start": "522760",
    "end": "528800"
  },
  {
    "text": "and which equals 32 I'm not a professor of basic mathematics my kids could do",
    "start": "528800",
    "end": "535120"
  },
  {
    "text": "that um but I can't so that's the that's the expected cost of of the car action",
    "start": "535120",
    "end": "540560"
  },
  {
    "text": "uh and there's some interesting properties about this model in particular which in this case the expected cost is kind of close to the",
    "start": "540560",
    "end": "547279"
  },
  {
    "text": "the most common value uh but like the worst case cost is a lot higher but I",
    "start": "547279",
    "end": "552519"
  },
  {
    "text": "think today we won't go into that but that that's an interesting property of using the expectation that yeah because I think you said before you know if you",
    "start": "552519",
    "end": "558279"
  },
  {
    "text": "had to get there within an hour then the even the finest minutest chance of it taking longer than hour is a problem",
    "start": "558279",
    "end": "564519"
  },
  {
    "text": "yeah exactly we wouldn't want to use the expected cost as the specification we'd want to use some kind of bounded cost",
    "start": "564519",
    "end": "570680"
  },
  {
    "text": "specification that prevented these outcomes from ever being considered but for the for the algorithm we're going to",
    "start": "570680",
    "end": "576240"
  },
  {
    "text": "look at today we're going to we're going to stick with expected cost and we're going to be looking at minimizing these kind of values or explicit these kind of",
    "start": "576240",
    "end": "582839"
  },
  {
    "text": "values this is kind of the basic in some sense that's that's kind of the core mass that we're going to need to think about and now we need two more steps we",
    "start": "582839",
    "end": "589240"
  },
  {
    "text": "need to think about how we write down the value uh or how we generalize this",
    "start": "589240",
    "end": "596399"
  },
  {
    "text": "kind of approach to a policy because the policy is the thing we're trying toach produce and then we need the algorithm",
    "start": "596399",
    "end": "602120"
  },
  {
    "text": "that's actually going to create the policy to minimize minimize these kind of values next step is to to think about",
    "start": "602120",
    "end": "609480"
  },
  {
    "text": "how we take that expected cost and write it for a policy the way we think about this is we think about what's called the",
    "start": "609480",
    "end": "615920"
  },
  {
    "text": "the value and I'm going to use V for value and we can think about this is generally is called the state value",
    "start": "615920",
    "end": "621720"
  },
  {
    "text": "which tells me effectively how good or how bad is it to be in a particular state in my",
    "start": "621720",
    "end": "627600"
  },
  {
    "text": "mdp and it only really makes sense to think about that when I've got a policy so I'm going to say what is the value of",
    "start": "627600",
    "end": "634000"
  },
  {
    "text": "a state under a policy and I'll write that as VP uh of s and so that's going",
    "start": "634000",
    "end": "640320"
  },
  {
    "text": "to say if I'm executing this policy or this I've committed to using this policy how good is it to be in a particular",
    "start": "640320",
    "end": "645880"
  },
  {
    "text": "State and this is going to be the number of this is going to correspond to the expected cost of following that policy",
    "start": "645880",
    "end": "652720"
  },
  {
    "text": "from that state so it's saying okay I'm in this state if I follow this policy what's the expected cost to reach the",
    "start": "652720",
    "end": "658720"
  },
  {
    "text": "goal if if I'm already in my goal what is the expected cost to reach the goal uh zero oh you're good you're good what",
    "start": "658720",
    "end": "665320"
  },
  {
    "text": "we'll do is we'll Define this function uh and it's zero if the state is an",
    "start": "665320",
    "end": "670440"
  },
  {
    "text": "element of that goal set that I wrote earlier and that kind of gives us a nice fix point to to think about when we when",
    "start": "670440",
    "end": "675760"
  },
  {
    "text": "we when we do the math later on we're going to work backwards from there effectively and then if I'm not in that",
    "start": "675760",
    "end": "682560"
  },
  {
    "text": "state then I need to choose an action effectively that minimizes that cost and the way we write the cost of an action",
    "start": "682560",
    "end": "689639"
  },
  {
    "text": "is something called The Q function and the Q function is is the",
    "start": "689639",
    "end": "695440"
  },
  {
    "text": "sort of call it the action cost or the state action cost this is going to sort of come back to the expected cost of",
    "start": "695440",
    "end": "701800"
  },
  {
    "text": "executing that action we talked about previously and just to check because I know you mentioned the pi symbol before",
    "start": "701800",
    "end": "707519"
  },
  {
    "text": "but this is nothing to do with pi as we know with circles and everything no no nothing to do with circles Pi is our",
    "start": "707519",
    "end": "713000"
  },
  {
    "text": "policy yeah and because policy starts with p we could just write P there but",
    "start": "713000",
    "end": "719440"
  },
  {
    "text": "PI right computer scientists like to be fancy um so that's why it's Pi so we'll write the Q function for S of a and the",
    "start": "719440",
    "end": "727639"
  },
  {
    "text": "value here because we're trying to minimize the the expected cost we're going to perform a minimization over a",
    "start": "727639",
    "end": "735000"
  },
  {
    "text": "so the the value of the state s under policy Pi is either Zero from in the",
    "start": "735000",
    "end": "741120"
  },
  {
    "text": "goal otherwise it's what happen it's the value I get if I choose the best action",
    "start": "741120",
    "end": "746639"
  },
  {
    "text": "in that state or the best action that I know about according to this policy so all we have left to do now is Define",
    "start": "746639",
    "end": "754639"
  },
  {
    "text": "q so Q sa equals the cost of of the that first",
    "start": "754639",
    "end": "762600"
  },
  {
    "text": "action so whenever I execute that action I always have to pay this cost so whenever I get in the car on this model",
    "start": "762600",
    "end": "769040"
  },
  {
    "text": "it cost me one or whenever I cycle it cost me 45 but that's only the kind of",
    "start": "769040",
    "end": "774480"
  },
  {
    "text": "immediate action cost MH we also need because this is the policy and it's the",
    "start": "774480",
    "end": "779959"
  },
  {
    "text": "policy to reach the goal I have to also include the the the the subsequent steps",
    "start": "779959",
    "end": "785639"
  },
  {
    "text": "yeah okay and the way I do that if we look at our car example there are three different states I can reach with",
    "start": "785639",
    "end": "792040"
  },
  {
    "text": "different probabilities so this is where I effectively put the expected cost of",
    "start": "792040",
    "end": "797079"
  },
  {
    "text": "following the policy from this state that has to involve a sum over the",
    "start": "797079",
    "end": "803160"
  },
  {
    "text": "possible success Estates so this will be RS Prime and it's the probability of",
    "start": "803160",
    "end": "809399"
  },
  {
    "text": "being in that state so again we think back to that expected value statement it's the probability of being there and in our mdp language that's T which is",
    "start": "809399",
    "end": "816880"
  },
  {
    "text": "our transition function s a s Prime and then we multiply it by the value of",
    "start": "816880",
    "end": "824839"
  },
  {
    "text": "being in that successor state so the value of that successor state is V it tells me how good it is to be in that",
    "start": "824839",
    "end": "831839"
  },
  {
    "text": "particular state but now this is for the subsequent state so if I'm in home and I execute car S Prime might be light",
    "start": "831839",
    "end": "838639"
  },
  {
    "text": "traffic and if it's light traffic then I'm M it's 2 multipli by 20 and I'm summing it",
    "start": "838639",
    "end": "844800"
  },
  {
    "text": "up across those just like I did for the example with the expected cost so now I'm doing this sum so this is our little",
    "start": "844800",
    "end": "850759"
  },
  {
    "text": "sum symbol doing a sum over all the successor States and this is it so these are called the Bellman optimality",
    "start": "850759",
    "end": "859519"
  },
  {
    "text": "equations and actually the reason they're called Bellman optimality equations is typically we think about the optimal Q values and the optimal V",
    "start": "859519",
    "end": "866680"
  },
  {
    "text": "State values and we'd put a little star at the top to say they're optimal but here we're just sort of talking generally about the values under a",
    "start": "866680",
    "end": "873880"
  },
  {
    "text": "policy right so that gives us the language to understand um State values and state action values",
    "start": "873880",
    "end": "881560"
  },
  {
    "text": "given a policy the remaining question is how do we find the optimal policy how do",
    "start": "881560",
    "end": "888120"
  },
  {
    "text": "we find the best policy and the best policy or the optimal policy for us is the one that's going to minimize the",
    "start": "888120",
    "end": "893759"
  },
  {
    "text": "expected cost to goal we can think about that as our expected cost of following the policy and our expected cost to goal",
    "start": "893759",
    "end": "899560"
  },
  {
    "text": "is just going to be this minimization step right we're going to find the policy where for each state we minimize",
    "start": "899560",
    "end": "905199"
  },
  {
    "text": "the the value and we choose the best action the policy is going to be a mapping from the state to the lowest Q",
    "start": "905199",
    "end": "910920"
  },
  {
    "text": "valued action so how do we do that well the interesting thing I don't want to mix up your pens uh well okay maybe go",
    "start": "910920",
    "end": "917839"
  },
  {
    "text": "on then I'm another color somebody stole my black but otherwise you can use any color you like so we have an interesting",
    "start": "917839",
    "end": "923480"
  },
  {
    "text": "problem the interesting problem is Q is defi or V is defined in terms of Q Q uh",
    "start": "923480",
    "end": "930440"
  },
  {
    "text": "and Q is defined in terms of V so like these things rely on each other there's",
    "start": "930440",
    "end": "936199"
  },
  {
    "text": "kind of a recursive definition so the way we solve this is by something called",
    "start": "936199",
    "end": "942319"
  },
  {
    "text": "dynamic programming so dynamic programming is kind of an iterative technique that allows you to",
    "start": "942319",
    "end": "947759"
  },
  {
    "text": "compute kind of one set of values and then you run the algorithm again using",
    "start": "947759",
    "end": "953120"
  },
  {
    "text": "the values from that previous iteration in the next iteration uh and there's",
    "start": "953120",
    "end": "958399"
  },
  {
    "text": "lots of different dynamically programming algorithms for different problems the the sort of dynamic programming approach for the Bellman",
    "start": "958399",
    "end": "966120"
  },
  {
    "text": "equations for mdps that approach is called value iteration and value iteration is an algorithm that simply",
    "start": "966120",
    "end": "972160"
  },
  {
    "text": "computes the values of the states at one iteration and then repeats that and",
    "start": "972160",
    "end": "979360"
  },
  {
    "text": "repeats that and repeats that until you found the optimal policy so you got do create a graph or something and have a",
    "start": "979360",
    "end": "984560"
  },
  {
    "text": "look no no graphs actually that's interest so there are generally you can be more effic efficient if you start",
    "start": "984560",
    "end": "989600"
  },
  {
    "text": "thinking about graphs and connectivity but they all of the algorithms that use graphs",
    "start": "989600",
    "end": "995079"
  },
  {
    "text": "typically that what you'd call approximations they're an approximate version of value iteration they tend to",
    "start": "995079",
    "end": "1001199"
  },
  {
    "text": "only focus on like a subset of the problem here we're going to do like the full mathematical optimal exhaustive",
    "start": "1001199",
    "end": "1007000"
  },
  {
    "text": "version that gives us the true uh values and the true optimal policy but in",
    "start": "1007000",
    "end": "1013000"
  },
  {
    "text": "reality it it works really well for small problems but doesn't scale particularly well right so Force almost",
    "start": "1013000",
    "end": "1019880"
  },
  {
    "text": "yeah yeah it's absolutely it's the Brute Force version of this it's it's not particularly smart but it's mathematically correct so the the kind",
    "start": "1019880",
    "end": "1026360"
  },
  {
    "text": "of key thing that we're going to do in value iteration is we're going to compute a set of values at VN so when",
    "start": "1026360",
    "end": "1033480"
  },
  {
    "text": "I'm here I'm going to I'm sort of mixing and matching my symbols a little bit previous I had a policy here uh now this",
    "start": "1033480",
    "end": "1038959"
  },
  {
    "text": "is n is going to be the iteration of the algorithm so the number of times we've gone through the loop and we're going to",
    "start": "1038959",
    "end": "1044079"
  },
  {
    "text": "use the values we've computed at VN in the calculation for the values at",
    "start": "1044079",
    "end": "1049559"
  },
  {
    "text": "VN + one so and this is going to be the way that the dynamic programming kind of",
    "start": "1049559",
    "end": "1054640"
  },
  {
    "text": "process works it takes the previous values and uses them at the next time you could write this in a single line that makes a bit clear but I think the",
    "start": "1054640",
    "end": "1060400"
  },
  {
    "text": "algorithm will I hope the algorithm will be clear enough um so how does this work",
    "start": "1060400",
    "end": "1065880"
  },
  {
    "text": "well we start at v0 and we're going to effectively we're",
    "start": "1065880",
    "end": "1071640"
  },
  {
    "text": "going to initialize initialize kind of with arbitrary",
    "start": "1071640",
    "end": "1077559"
  },
  {
    "text": "values which is is effectively we're going to give random values to all our States",
    "start": "1077559",
    "end": "1083640"
  },
  {
    "text": "because we don't know them yet um typically because zero we know is the goal state if the states are in our goal",
    "start": "1083640",
    "end": "1090320"
  },
  {
    "text": "set which we know we're going to initialize them to zero and for this example if our states",
    "start": "1090320",
    "end": "1096919"
  },
  {
    "text": "are not in our goal State we're going to initialize them to 100 um because 100 is a big number and it's bigger than any",
    "start": "1096919",
    "end": "1104159"
  },
  {
    "text": "number in this and it makes the mass a bit easier I'll write the algorithm out and then I'll come back and do the mass so we might remember we'll come back to",
    "start": "1104159",
    "end": "1109520"
  },
  {
    "text": "those numbers in a minute we basically just repeat the algorithm I'm going to see if I can fit this in let's see what we do is we compute for s in s so across",
    "start": "1109520",
    "end": "1118200"
  },
  {
    "text": "all our states we're going to compute VN of s here what I'm going to do is we",
    "start": "1118200",
    "end": "1124120"
  },
  {
    "text": "could sort of write the kind of whole Q thing here we're going to minimize a and this is going to be over c s of a uh",
    "start": "1124120",
    "end": "1131320"
  },
  {
    "text": "plus the sum over S Prime t s a s Prime",
    "start": "1131320",
    "end": "1137360"
  },
  {
    "text": "multipli by and this is the in thing V of n minus one of s I've kind of rolled",
    "start": "1137360",
    "end": "1143120"
  },
  {
    "text": "the two equations if we go back in here I had two I had two things that's why I kind of hesitated a little bit on that we when we compute our value it's either",
    "start": "1143120",
    "end": "1150000"
  },
  {
    "text": "zero if it's the goal state or we do this minimization over q and Q is like this um I'm just going to be lazy and",
    "start": "1150000",
    "end": "1156360"
  },
  {
    "text": "write that all in kind of one line here so we're going to compute the the values uh if this if State isn't the goal we're",
    "start": "1156360",
    "end": "1162320"
  },
  {
    "text": "going to do this if States the goal is zero we'll ignore that in the algorithm so your your viewers shouldn't type what",
    "start": "1162320",
    "end": "1168080"
  },
  {
    "text": "I'm writing here here into a computer they should think about it a bit more before they before they do that",
    "start": "1168080",
    "end": "1174480"
  },
  {
    "text": "uh so we comput the the values but the key thing really is in this this this bit here where we're using um",
    "start": "1174480",
    "end": "1182600"
  },
  {
    "text": "n and we're calculating it based on N minus one so we're using the values of n",
    "start": "1182600",
    "end": "1188600"
  },
  {
    "text": "from the previous uh time and we might even if we want to up here we can start",
    "start": "1188600",
    "end": "1194760"
  },
  {
    "text": "with n equals z uh and then in this loop we're just",
    "start": "1194760",
    "end": "1201080"
  },
  {
    "text": "doing Nal n + one so each time we go through this n is going to increase um",
    "start": "1201080",
    "end": "1206200"
  },
  {
    "text": "and so the first time Round We compute the the kind of V the first that would be one of s using zero and zero is going",
    "start": "1206200",
    "end": "1213200"
  },
  {
    "text": "to be our our v0 that we initialized before so we do that we do that overall our states that gives us the state value",
    "start": "1213200",
    "end": "1220320"
  },
  {
    "text": "actions that we want so we we compute our value and then we also compute something called the residual okay",
    "start": "1220320",
    "end": "1226520"
  },
  {
    "text": "something called the residual the residual is the the size or the absolute value of the difference",
    "start": "1226520",
    "end": "1232640"
  },
  {
    "text": "between uh v n of s minus V nus one of s",
    "start": "1232640",
    "end": "1239200"
  },
  {
    "text": "so this residual tells us how much the value has changed in each iteration so we we compute our value and then we look",
    "start": "1239200",
    "end": "1245080"
  },
  {
    "text": "how much is our value changed and so this is going to be the residual of s we do this for all our states and we repeat",
    "start": "1245080",
    "end": "1253159"
  },
  {
    "text": "we're kind of repeating while the max of all the residuals we've calculated",
    "start": "1253159",
    "end": "1259520"
  },
  {
    "text": "is greater than some Epsilon so we're basically we're going to run this Loop until the difference between values at",
    "start": "1259520",
    "end": "1266840"
  },
  {
    "text": "each iteration is smaller than some some small value and Epsilon kind of it get",
    "start": "1266840",
    "end": "1272559"
  },
  {
    "text": "it's problem dependent much like the initialization but that might be I don't know 10 the minus5 or something we might",
    "start": "1272559",
    "end": "1278679"
  },
  {
    "text": "want a small number something arbitrary and this is all we need so the kind of core bit is the mass that we saw before",
    "start": "1278679",
    "end": "1284720"
  },
  {
    "text": "which is Computing the value of this state we look at how much the state the value hasang Chang to each iteration and",
    "start": "1284720",
    "end": "1290679"
  },
  {
    "text": "then we run that until the change the changes across all our states goes below some small threshold so I think the",
    "start": "1290679",
    "end": "1296480"
  },
  {
    "text": "thing to do now is just to work I can just kind of go through that for this example like one or two iterations I'm not going to I'm not going to run",
    "start": "1296480",
    "end": "1302799"
  },
  {
    "text": "forever um and then I think I think that's kind of enough I might use two pages at once let's do that so let's uh",
    "start": "1302799",
    "end": "1311600"
  },
  {
    "text": "have uh v0 and maybe up here I'll keep the values and down here I'll do my very",
    "start": "1311600",
    "end": "1317000"
  },
  {
    "text": "bad mental arithmetic or or not even mental arithmetic full stop and try and",
    "start": "1317000",
    "end": "1322400"
  },
  {
    "text": "work out I'm going to write all our states and I'm just going to abbreviate them so we've got home so those are the states in the problem I said i' we'll",
    "start": "1322400",
    "end": "1328720"
  },
  {
    "text": "initialize all our all our state values to to something kind of arbitary so work",
    "start": "1328720",
    "end": "1334120"
  },
  {
    "text": "we initialize to zero and everything else will make a 100 because I don't know what it is we'll do explicitly the",
    "start": "1334120",
    "end": "1341159"
  },
  {
    "text": "kind of minimization Step that we had in the middle it's sort of a bit easier to work bottom up and it's it's easier at",
    "start": "1341159",
    "end": "1349039"
  },
  {
    "text": "least mathema to start with by thinking about the the actions that are kind of deterministic so we'll start with um the",
    "start": "1349039",
    "end": "1356480"
  },
  {
    "text": "action of cycling so I can only execute cycle in home so I'm going to say what's",
    "start": "1356480",
    "end": "1362039"
  },
  {
    "text": "the Q value of being at home and",
    "start": "1362039",
    "end": "1367919"
  },
  {
    "text": "cycling well it's the cost of cycling which is 45 plus this expected value of following",
    "start": "1367919",
    "end": "1375360"
  },
  {
    "text": "the policy after that action is complete but cycling takes me with probability one to the goal so there's nothing to",
    "start": "1375360",
    "end": "1381760"
  },
  {
    "text": "add on there so I am just going to write 45 so",
    "start": "1381760",
    "end": "1387159"
  },
  {
    "text": "I know the Q value of cycling from home is is 45 well we've also got relax so relax is similar so relax uh is",
    "start": "1387159",
    "end": "1396400"
  },
  {
    "text": "deterministic it takes us to the goal it costs 35 so I might just write the Q",
    "start": "1396400",
    "end": "1401559"
  },
  {
    "text": "value of here I'm on the train and if I relax that's going to cost me 30 5",
    "start": "1401559",
    "end": "1409240"
  },
  {
    "text": "that's reasonably easy so remind me of relax was that you just get straight on the no that's when you're on the train",
    "start": "1409240",
    "end": "1415159"
  },
  {
    "text": "there's nothing to do there's no more decisions to make you're happy you just sit there and it takes you takes you to work uh the railway the train side is",
    "start": "1415159",
    "end": "1422000"
  },
  {
    "text": "interesting because you you might not you might have to wait to catch the train yeah when you get to the train",
    "start": "1422000",
    "end": "1427159"
  },
  {
    "text": "station you either get straight on the train with probability point. n or you go to the waiting room and in the",
    "start": "1427159",
    "end": "1432480"
  },
  {
    "text": "waiting room you can either choose to carry on waiting or you can choose to go home let's go to",
    "start": "1432480",
    "end": "1438960"
  },
  {
    "text": "let's do the waiting room so the waiting room I have two options I can either",
    "start": "1438960",
    "end": "1444200"
  },
  {
    "text": "choose to wait or I can choose to go home so let's start with let's look at",
    "start": "1444200",
    "end": "1450880"
  },
  {
    "text": "the go home action so if I say the Q of being in the waiting room and choose the",
    "start": "1450880",
    "end": "1457880"
  },
  {
    "text": "go home action well okay go home cost me two to",
    "start": "1457880",
    "end": "1465440"
  },
  {
    "text": "execute and then I've one outcome state which is being at",
    "start": "1465440",
    "end": "1472559"
  },
  {
    "text": "home so this is this is going to be our kind of Q of in in in iteration one so I",
    "start": "1472559",
    "end": "1479600"
  },
  {
    "text": "need to add on the value of iteration zero so we got this",
    "start": "1479600",
    "end": "1486080"
  },
  {
    "text": "sum but there's only one state which is the state of being at home there's only one outcome state from that action so we",
    "start": "1486080",
    "end": "1491960"
  },
  {
    "text": "don't really need the sum the value of zero of My Success state which is being",
    "start": "1491960",
    "end": "1497720"
  },
  {
    "text": "at home or just H from the thing so the value",
    "start": "1497720",
    "end": "1503000"
  },
  {
    "text": "here from zero I go back to this table I say okay this is 100 so the Q value of being in the waiting room and executing",
    "start": "1503000",
    "end": "1509240"
  },
  {
    "text": "go home is 102 I've got this other option if I'm in the waiting room I can",
    "start": "1509240",
    "end": "1515279"
  },
  {
    "text": "choose to wait and this is where things get a bit more interesting and I probably have to do some math although I",
    "start": "1515279",
    "end": "1520640"
  },
  {
    "text": "think I know the answer I I did do the mass earlier so so it looked like I noted it off the top of my head and that's making it harder because now I'm",
    "start": "1520640",
    "end": "1526640"
  },
  {
    "text": "like oh which bit was which which bit was which I don't know so I'm in the waiting room and I can choose to wait",
    "start": "1526640",
    "end": "1532360"
  },
  {
    "text": "now so the cost of waiting is three but I need to add to that so I now I'm going to do this sun bit so what I've got is",
    "start": "1532360",
    "end": "1538279"
  },
  {
    "text": "I've got the probability of there not being a train which lands me back in the waiting room so I multiply that by the",
    "start": "1538279",
    "end": "1545360"
  },
  {
    "text": "value of being in the waiting room from the previous iteration which is 100 and I also add on the",
    "start": "1545360",
    "end": "1552559"
  },
  {
    "text": "probability of that being being on a train so the train value uh uh here from",
    "start": "1552559",
    "end": "1559399"
  },
  {
    "text": "the same previous situation is 100 so that should give me 103 because",
    "start": "1559399",
    "end": "1564520"
  },
  {
    "text": "those two have the same outcome value 103 now I can minimize because I've got",
    "start": "1564520",
    "end": "1569960"
  },
  {
    "text": "my two actions on my waiting room I can either go home or I can wait the waiting action costs 103 the go",
    "start": "1569960",
    "end": "1577480"
  },
  {
    "text": "home action costs 102 so now if we go up here we can start filling",
    "start": "1577480",
    "end": "1582919"
  },
  {
    "text": "in the next iteration uh so being value of being in",
    "start": "1582919",
    "end": "1588399"
  },
  {
    "text": "waiting room has gone up and our residual will be three the difference between those two so where else can we",
    "start": "1588399",
    "end": "1594480"
  },
  {
    "text": "look at I think now we can maybe make sense to think about the railway action if I'm at home and I choose Railway so",
    "start": "1594480",
    "end": "1600679"
  },
  {
    "text": "commit to the railway that's going to cost two because it cost me two here and actually I have the same Choice as the",
    "start": "1600679",
    "end": "1606120"
  },
  {
    "text": "weight action so with 0.1 I'm in the waiting room 0.9 I'm in the train so",
    "start": "1606120",
    "end": "1611360"
  },
  {
    "text": "we've already done this Mass up here so it's the same as that and then at the end it's going to be",
    "start": "1611360",
    "end": "1616640"
  },
  {
    "text": "102 because I only cost two two to to the cost of the actual action is only two so then I've got the the value of",
    "start": "1616640",
    "end": "1623080"
  },
  {
    "text": "being rail cycle so that's two out of my three home actions so let's look at our",
    "start": "1623080",
    "end": "1628960"
  },
  {
    "text": "final home action that's applicable in in the home state which is to take the",
    "start": "1628960",
    "end": "1634880"
  },
  {
    "text": "car so the car is cost me one I guess CU it's sat on my drive it's easy to get to",
    "start": "1634880",
    "end": "1641440"
  },
  {
    "text": "and then it's 0.2 times uh the value of being in light",
    "start": "1641440",
    "end": "1647559"
  },
  {
    "text": "traffic which again from our previous step is",
    "start": "1647559",
    "end": "1654399"
  },
  {
    "text": "100 plus .7 of being in medium plus",
    "start": "1654399",
    "end": "1661279"
  },
  {
    "text": "0.1 of being in uh L so again actually this is fine",
    "start": "1661279",
    "end": "1668240"
  },
  {
    "text": "because this looks at this will be 101 because all all of the these will sum to",
    "start": "1668240",
    "end": "1673360"
  },
  {
    "text": "to 100 and we got 101 so now I've got three actions for",
    "start": "1673360",
    "end": "1678559"
  },
  {
    "text": "for home I've got getting in the car 101 I've got getting the train which",
    "start": "1678559",
    "end": "1685159"
  },
  {
    "text": "is 102 and I've got cycling which is 45 so we now we've can minimize over",
    "start": "1685159",
    "end": "1693240"
  },
  {
    "text": "those which is the lowest cycling excellent uh and so we can say the value",
    "start": "1693240",
    "end": "1700000"
  },
  {
    "text": "of being in home and this is for this iteration MH is 45 so we've calculated",
    "start": "1700000",
    "end": "1705760"
  },
  {
    "text": "the value for that M uh uh what else do we need so the rest we can kind of fill",
    "start": "1705760",
    "end": "1712080"
  },
  {
    "text": "in more or less directly I think from the answers from what we know from the model so weight the goal is always",
    "start": "1712080",
    "end": "1718960"
  },
  {
    "text": "zero uh and all of these other ones where there's just one deterministic action so one action with no no",
    "start": "1718960",
    "end": "1724720"
  },
  {
    "text": "distribution of the outcomes it's just the cost it's just the immediate cost so it's going to be",
    "start": "1724720",
    "end": "1731279"
  },
  {
    "text": "70 uh 30 20 and 35",
    "start": "1731279",
    "end": "1739559"
  },
  {
    "text": "that's kind of one iteration of value iteration probably not the most riveting video watching a grow man do math on a",
    "start": "1739559",
    "end": "1746279"
  },
  {
    "text": "bit of paper but um it kind of gives you an idea the key thing was that I we started with these arbitrary values and",
    "start": "1746279",
    "end": "1752799"
  },
  {
    "text": "now already some of these values are correct and kind of the final value of being in that state in fact anything",
    "start": "1752799",
    "end": "1758559"
  },
  {
    "text": "from here on down where there is a deterministic route from that state to",
    "start": "1758559",
    "end": "1763960"
  },
  {
    "text": "the goal we've actually converged in one step but we have still have quite a big",
    "start": "1763960",
    "end": "1769840"
  },
  {
    "text": "residual so the biggest residual I think we've got is is here so the max residual is",
    "start": "1769840",
    "end": "1776679"
  },
  {
    "text": "80 so we can't stop we would run another iteration but now on if we went on to",
    "start": "1776679",
    "end": "1783159"
  },
  {
    "text": "doing this for for v2 right the the mass would start to be",
    "start": "1783159",
    "end": "1788320"
  },
  {
    "text": "a bit different because we're using these values here before we calculate the next thing uh right so uh we made a",
    "start": "1788320",
    "end": "1795080"
  },
  {
    "text": "mistake we made a mistake I made a mistake uh in the for waiting room here",
    "start": "1795080",
    "end": "1800200"
  },
  {
    "text": "at 103 but that's I should have been minimizing over both these two which is",
    "start": "1800200",
    "end": "1805600"
  },
  {
    "text": "102 so that's going to be a two so we can look at the the next iteration just",
    "start": "1805600",
    "end": "1810960"
  },
  {
    "text": "to get a sense of how things evolve and the important thing really",
    "start": "1810960",
    "end": "1816399"
  },
  {
    "text": "for this problem is that at um The Next Step there's kind of two interesting",
    "start": "1816399",
    "end": "1822279"
  },
  {
    "text": "actions so everything from train on down is just is kind of boring now it's all it's all converged but for the next",
    "start": "1822279",
    "end": "1828720"
  },
  {
    "text": "iteration it's going to be these two two values that change and they change because the the kind of the things they",
    "start": "1828720",
    "end": "1835320"
  },
  {
    "text": "depend on which is this the sort of lower down States in this in this mdp the things they depend on have changed",
    "start": "1835320",
    "end": "1842360"
  },
  {
    "text": "in the previous iteration so those changes now kind of propagate through so this is the value iteration or the the",
    "start": "1842360",
    "end": "1847440"
  },
  {
    "text": "dynamic programming in work at work we've changed these things so the things they depend on will then change",
    "start": "1847440",
    "end": "1853200"
  },
  {
    "text": "subsequently and I'm just going to I'm just going to scribble on top here but now we know that um so if we're going to",
    "start": "1853200",
    "end": "1859639"
  },
  {
    "text": "move this to now kind of Q2 so for v2 uh these values are no longer 100 so this",
    "start": "1859639",
    "end": "1866840"
  },
  {
    "text": "was light traffic which is now this is going to become 20 this is going to become 30 and this is going to become 70",
    "start": "1866840",
    "end": "1875480"
  },
  {
    "text": "and then this will become 33 I believe uh and so that means that up here we",
    "start": "1875480",
    "end": "1881960"
  },
  {
    "text": "will get 33 because 33 will be better so the rail action now I'm reading this off another",
    "start": "1881960",
    "end": "1888519"
  },
  {
    "text": "bit of paper this becomes 44.7 and ultimately will converge to something a bit lower but 33 is the",
    "start": "1888519",
    "end": "1895480"
  },
  {
    "text": "cheaper one so we're going to end up with with 33 here and everything is",
    "start": "1895480",
    "end": "1900799"
  },
  {
    "text": "going to effectively after this point start to get get cheaper and cheaper um so in",
    "start": "1900799",
    "end": "1907360"
  },
  {
    "text": "fact at some point it becomes better to go home I think at this point it probably becomes better to go home so I",
    "start": "1907360",
    "end": "1912519"
  },
  {
    "text": "I I don't know the exact number here and I won't work it out but in the final policy the best action for waiting room",
    "start": "1912519",
    "end": "1919440"
  },
  {
    "text": "is to go home so the optional action is to go home because once you're home you then",
    "start": "1919440",
    "end": "1926840"
  },
  {
    "text": "take the car so we can also look at what the optimal policy is and the optimal",
    "start": "1926840",
    "end": "1933559"
  },
  {
    "text": "policy is the is the action choice that gives us the minimal values here so although I've not shown it the optimal",
    "start": "1933559",
    "end": "1939960"
  },
  {
    "text": "policy the optimal action choice for the waiting room is to go home and then the optimal choice in the home state is to",
    "start": "1939960",
    "end": "1946919"
  },
  {
    "text": "take the car so in the end we would say that the kind of sort of a bit messy across here that",
    "start": "1946919",
    "end": "1954080"
  },
  {
    "text": "we've got the the optimal value so we'd put a star the optimal value being at home is",
    "start": "1954080",
    "end": "1960120"
  },
  {
    "text": "33 and we'd keep run we'd run this I think you maybe run it two more times then the residuals go to zero and then",
    "start": "1960120",
    "end": "1966600"
  },
  {
    "text": "what we do to get the policy is we pick the actions with the minimal Q values and residual just to remind me is the",
    "start": "1966600",
    "end": "1972399"
  },
  {
    "text": "difference between one iteration to the next yeah yeah so so here when we went from zero to one of the iterations we",
    "start": "1972399",
    "end": "1979360"
  },
  {
    "text": "look at the absolute difference between the two columns and then we take all of those residuals that we calculate for",
    "start": "1979360",
    "end": "1985519"
  },
  {
    "text": "all the differences have to be below some threshold which and you're basically culminate in an answer that can't change anymore is that right yeah",
    "start": "1985519",
    "end": "1991480"
  },
  {
    "text": "exactly so the the the kind of mass that underpins this the dynamic programming math says that there's a kind of a fixed",
    "start": "1991480",
    "end": "1996919"
  },
  {
    "text": "point to this this this set of equations and value iteration is the algorithm that gets you to that fixed point and",
    "start": "1996919",
    "end": "2003240"
  },
  {
    "text": "for big complex systems with with lots of of stochasticity lots of kind of probabilistic comes uh it might take a",
    "start": "2003240",
    "end": "2009679"
  },
  {
    "text": "long time for those things to converge uh and you might not care about the exact values of those things so you",
    "start": "2009679",
    "end": "2015559"
  },
  {
    "text": "might not have a very tight threshold but yeah in time those things will value iteration will con converge to the fix",
    "start": "2015559",
    "end": "2022120"
  },
  {
    "text": "point of that of the Bellman equations and they should always pretty much get to a certain point or can they go crazy",
    "start": "2022120",
    "end": "2028080"
  },
  {
    "text": "and never do that no absolutely it's guaranteed well okay so for depending on the",
    "start": "2028080",
    "end": "2034360"
  },
  {
    "text": "model it depends slightly so in this case for what we're doing is expect",
    "start": "2034360",
    "end": "2040720"
  },
  {
    "text": "minimizing expected cost to reach goal we're solving something called a stochastic shortest path",
    "start": "2040720",
    "end": "2046200"
  },
  {
    "text": "mdp and when we do that we have to have some a certain set of uh requirements on",
    "start": "2046200",
    "end": "2051960"
  },
  {
    "text": "the model otherwise value iteration won't converge it will give us some some weird results so those are that there",
    "start": "2051960",
    "end": "2058000"
  },
  {
    "text": "are no zero cost Loops so it can't be possible for the for the for me to",
    "start": "2058000",
    "end": "2063480"
  },
  {
    "text": "choose an action that changes my state but costs zero",
    "start": "2063480",
    "end": "2068638"
  },
  {
    "text": "and I'm not allowed to then go back for zero cost otherwise there's a chance that I could basically keep choosing those actions get stuck in a loop I get",
    "start": "2068639",
    "end": "2074599"
  },
  {
    "text": "stuck in a loop so like the algorithm doesn't get stuck in Loop but the policy wouldn't be valid the policy wouldn't reach the goal and the kind of the other",
    "start": "2074599",
    "end": "2081358"
  },
  {
    "text": "the other requirement is that you must be able to reach the goal with probability one from any state in your",
    "start": "2081359",
    "end": "2086720"
  },
  {
    "text": "model for that to be a valid stochastic shortage path and for us to be able to use value iteration so under the right",
    "start": "2086720",
    "end": "2094158"
  },
  {
    "text": "conditions the algorithm always converges um and largely for a lot of problems so I work in robotics a lot of",
    "start": "2094159",
    "end": "2100400"
  },
  {
    "text": "Robotics problems we think about have that property well let's say some robotics problems have that property or",
    "start": "2100400",
    "end": "2105880"
  },
  {
    "text": "you can convert the problems into problems that have that property if they don't have that property then you have",
    "start": "2105880",
    "end": "2111839"
  },
  {
    "text": "to do other things you have to to process the model or you use a different algorithm and really in practice people",
    "start": "2111839",
    "end": "2119760"
  },
  {
    "text": "I mentioned earlier people use these use value iteration for for smallish",
    "start": "2119760",
    "end": "2125280"
  },
  {
    "text": "problems um but then small might orders of I don't know hundreds of thousands",
    "start": "2125280",
    "end": "2130960"
  },
  {
    "text": "maybe millions of states still so very big problems in terms of the kind of things we're looking at here but beyond",
    "start": "2130960",
    "end": "2138280"
  },
  {
    "text": "that things get very very difficult to solve and they get difficult to solve",
    "start": "2138280",
    "end": "2143680"
  },
  {
    "text": "um I mean maybe it's worth just just trying to illustrate that a little bit so if I I can think about the computational",
    "start": "2143680",
    "end": "2149920"
  },
  {
    "text": "complexity of one iteration through value iteration so what did I have to do",
    "start": "2149920",
    "end": "2155400"
  },
  {
    "text": "well I had to look at every state so if we go back to our previous notation I had capital",
    "start": "2155400",
    "end": "2160880"
  },
  {
    "text": "S um and I actually have to look at in in these kind of if I look back here",
    "start": "2160880",
    "end": "2166760"
  },
  {
    "text": "I've got this basically within this for Loop over every state I need to look at every successor State somehow so there's",
    "start": "2166760",
    "end": "2172960"
  },
  {
    "text": "this kind of basically a quadratic thing going on here saying I basically have to look at for every state I have to look at every other state um and so that",
    "start": "2172960",
    "end": "2180480"
  },
  {
    "text": "means the kind of complexity is the the number of states so the size of that set squared and then",
    "start": "2180480",
    "end": "2187280"
  },
  {
    "text": "I have to do that for every action so I kind of went through every action in a bit of a random order here so I need to",
    "start": "2187280",
    "end": "2193440"
  },
  {
    "text": "multiply that by the the size of the action set and so the the computational complexity so the kind of Big O notation",
    "start": "2193440",
    "end": "2200680"
  },
  {
    "text": "and that's one iteration so that's just one Loop through through this algorithm and you",
    "start": "2200680",
    "end": "2207800"
  },
  {
    "text": "can't tell in advance how many iterations you're going to need and so this as your States and action spaces",
    "start": "2207800",
    "end": "2213240"
  },
  {
    "text": "grow this becomes um quite costly the other issue is that what I have to do is",
    "start": "2213240",
    "end": "2218839"
  },
  {
    "text": "I have to build this table so I have to enumerate write down every single state",
    "start": "2218839",
    "end": "2224960"
  },
  {
    "text": "and so if if s is is Mill hundreds of thousands or millions and these tables get very big and typically I have to",
    "start": "2224960",
    "end": "2230599"
  },
  {
    "text": "produce matrices to encode these things that have the number of states on each Dimension so it's 2D 2D uh Matrix and",
    "start": "2230599",
    "end": "2238359"
  },
  {
    "text": "that again gets very very big so often it's the cost of kind of writing down",
    "start": "2238359",
    "end": "2243920"
  },
  {
    "text": "and then iterating through these states that is the problem the algorithm is kind of simple it's just linear algebra it's maths but like the the cost of",
    "start": "2243920",
    "end": "2251119"
  },
  {
    "text": "writing everything down enumerating the states and then storing all in memory is what typically drags these algorithms",
    "start": "2251119",
    "end": "2258760"
  },
  {
    "text": "down so now car is going to take one this is me leaving the house and going to to my parking space and finding what",
    "start": "2263680",
    "end": "2269440"
  },
  {
    "text": "car Railway maybe I have to walk a bit further to get to the railway so that'll be a cost of two and then we can start",
    "start": "2269440",
    "end": "2274520"
  },
  {
    "text": "to put these these um actions in so actually these arrows should be dots and I can put in the the action that we had",
    "start": "2274520",
    "end": "2280400"
  },
  {
    "text": "before",
    "start": "2280400",
    "end": "2283160"
  }
]