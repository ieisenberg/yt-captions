[
  {
    "text": "So Apache spark is another kind of framework for doing big data processing distributed across clusters like MapReduce",
    "start": "60",
    "end": "6029"
  },
  {
    "text": "The differences is kind of come in how those computations have done. So for example with spark",
    "start": "7270",
    "end": "15210"
  },
  {
    "text": "You've got a lot more flexibility in the computations",
    "start": "16900",
    "end": "19230"
  },
  {
    "text": "So with MapReduce you've got to do map and then reduce like there's no way of getting around it in spark",
    "start": "19230",
    "end": "24180"
  },
  {
    "text": "They provide like a load of different operations that you can do on the data such as joins between different data structures",
    "start": "24180",
    "end": "29909"
  },
  {
    "text": "Why would a you spark this purpose is to process large volumes of data",
    "start": "32529",
    "end": "36598"
  },
  {
    "text": "So it's mainly data that's not going to fit on a single node",
    "start": "36850",
    "end": "40199"
  },
  {
    "text": "There's also computations that over a large volume of data. You don't want to go through and sequentially data",
    "start": "41290",
    "end": "45929"
  },
  {
    "text": "And if you've got parts of your computation that are independent of each other and so you can do it on the data items",
    "start": "45930",
    "end": "51899"
  },
  {
    "text": "Individually, you can split that data across the cluster",
    "start": "52030",
    "end": "54448"
  },
  {
    "text": "and then",
    "start": "55390",
    "end": "56739"
  },
  {
    "text": "Do the computations on that single node exactly like with MapReduce you there's the data locality prints?",
    "start": "56739",
    "end": "62279"
  },
  {
    "text": "but again do the computations on the nodes where the data is stored and",
    "start": "62280",
    "end": "65188"
  },
  {
    "text": "Then you reduce those results down to what you want",
    "start": "65379",
    "end": "67768"
  },
  {
    "text": "The main programming structure that you're going to be dealing with is called a resilient distributed datasets",
    "start": "67770",
    "end": "73110"
  },
  {
    "text": "Which is usually shorter than two RDD",
    "start": "73180",
    "end": "75209"
  },
  {
    "text": "Which is kind of a collection of objects that spread across a cluster",
    "start": "75400",
    "end": "80759"
  },
  {
    "text": "But as a programmer when you're dealing with that, you're kind of just interacting with it as if it's on a single node",
    "start": "80830",
    "end": "86729"
  },
  {
    "text": "So it kind of hidden from you this it's distributed in a spark cluster",
    "start": "86729",
    "end": "90148"
  },
  {
    "text": "You'll have a driver node",
    "start": "90150",
    "end": "91770"
  },
  {
    "text": "and then several worker nodes and the driver node is running the main program where kind of has all of the",
    "start": "91770",
    "end": "97110"
  },
  {
    "text": "Transformations that you want to do to your data and then these kind of get sent out to the worker nodes who then operate that",
    "start": "97240",
    "end": "102570"
  },
  {
    "text": "On their chunks of data that they have. In fact, the transformations can be similar to MapReduce",
    "start": "102570",
    "end": "106619"
  },
  {
    "text": "So it still provides the same map function and reduce functions, but then you have additional stuff on top of that. So they like",
    "start": "106619",
    "end": "114089"
  },
  {
    "text": "Give you a filter operation directly",
    "start": "114759",
    "end": "116710"
  },
  {
    "text": "so you can do dental to implement that you can just",
    "start": "116710",
    "end": "118798"
  },
  {
    "text": "Call the filter function on an RDD and say I only want to return objects to which this is true",
    "start": "118869",
    "end": "123599"
  },
  {
    "text": "so here we've just got a very very simple spark example of just loading in a text file from the local file system and",
    "start": "124420",
    "end": "131190"
  },
  {
    "text": "We're just going to go through and count the number of occurrences of each word",
    "start": "131890",
    "end": "136669"
  },
  {
    "text": "This is exactly the same as the MapReduce example. We looked at last time but we're doing this in spark this time",
    "start": "136950",
    "end": "142759"
  },
  {
    "text": "Okay, so at the start we set off a spark config",
    "start": "142890",
    "end": "146028"
  },
  {
    "text": "So we just set the app name which allows us to seize the which of our jobs is currently running within the web UI",
    "start": "146030",
    "end": "151280"
  },
  {
    "text": "We then set the spark master. So because we're running this locally on a single computer. That's just local",
    "start": "151409",
    "end": "157338"
  },
  {
    "text": "We then set up a spark context which gives us access to like the spark functions for dealing with rdd's",
    "start": "157340",
    "end": "163609"
  },
  {
    "text": "We first of all need to load our data into an RDD",
    "start": "163639",
    "end": "165979"
  },
  {
    "text": "so we do this using need text file function and that puts the contents of that text file into an RDD the RDD you can",
    "start": "165980",
    "end": "172789"
  },
  {
    "text": "Kind of just view it as like an array if you want to it's been like an array",
    "start": "173519",
    "end": "177709"
  },
  {
    "text": "distributed across the cluster",
    "start": "178319",
    "end": "179909"
  },
  {
    "text": "So here we've got our lines RDD each element in the RDD is a single line from the text file",
    "start": "179909",
    "end": "184819"
  },
  {
    "text": "We then go through and split each line",
    "start": "184819",
    "end": "187819"
  },
  {
    "text": "Using the flat map function so that map's a single function over every single item in the dataset. So every line",
    "start": "188400",
    "end": "194840"
  },
  {
    "text": "We go through is split it up into words and then because we're using flat map that then takes that from an RDD of arrays",
    "start": "195450",
    "end": "202369"
  },
  {
    "text": "to an RDD of strings again we",
    "start": "202680",
    "end": "205668"
  },
  {
    "text": "Then go through and exactly the same as in the Map Reduce example",
    "start": "206250",
    "end": "209509"
  },
  {
    "text": "We use the map function",
    "start": "209510",
    "end": "210478"
  },
  {
    "text": "Sum up each word to a key value pair where the key is the word and then the value is the value 1 so indicating",
    "start": "210479",
    "end": "216199"
  },
  {
    "text": "We've got one instance of that word at that point",
    "start": "216199",
    "end": "218329"
  },
  {
    "text": "That then gives us a new RTD and for that one we go reduce by key",
    "start": "218549",
    "end": "222228"
  },
  {
    "text": "instead of it's a Map Reduce that would just be reduced but here in SPARC if we just did reduce it would give us a",
    "start": "222479",
    "end": "228199"
  },
  {
    "text": "single value for the entire RDD back at the driver reduced by key takes",
    "start": "228199",
    "end": "232939"
  },
  {
    "text": "an ID D of key value pairs and",
    "start": "233639",
    "end": "236149"
  },
  {
    "text": "For each key you give it a function to apply to those values for how you want it to be combined",
    "start": "236699",
    "end": "241399"
  },
  {
    "text": "So for us we want to add up the number of those instances of that word that we have",
    "start": "241400",
    "end": "246019"
  },
  {
    "text": "so we use just a simple + to",
    "start": "246090",
    "end": "249679"
  },
  {
    "text": "Aggregate those values so that finally gives us our word count RDD which contains key value pairs of words and a number of instances",
    "start": "250379",
    "end": "257569"
  },
  {
    "text": "Of those words. And so we then called the collect function which will bring that back to the",
    "start": "257570",
    "end": "262909"
  },
  {
    "text": "Driver node, and then for each one of those lines we've in them out. So we right now the counts all those words",
    "start": "263580",
    "end": "269740"
  },
  {
    "text": "So this at the moment that code is written for something that might be on your own computer",
    "start": "269740",
    "end": "274059"
  },
  {
    "text": "How would it differ if it was on a cluster and I'm server farm or a massive data center or something like that?",
    "start": "274780",
    "end": "279760"
  },
  {
    "text": "How would that vary?",
    "start": "279910",
    "end": "281210"
  },
  {
    "text": "And so if you're running this on an actual cluster and not just on your local computer",
    "start": "281210",
    "end": "286479"
  },
  {
    "text": "then",
    "start": "287870",
    "end": "289040"
  },
  {
    "text": "Rather than setting master within your code and setting it to run locally",
    "start": "289040",
    "end": "293110"
  },
  {
    "text": "What you do is you would have SPARC running on the cluster and you would use something called spark submit submit your spark jobs to",
    "start": "293390",
    "end": "300159"
  },
  {
    "text": "Spark to then be run",
    "start": "300160",
    "end": "301580"
  },
  {
    "text": "So it would it's just a different way of running them",
    "start": "301580",
    "end": "304119"
  },
  {
    "text": "Basically rather than hard coding it within your program even though you're their money on the cluster",
    "start": "304130",
    "end": "309460"
  },
  {
    "text": "The rest of the code would be the same",
    "start": "309460",
    "end": "311419"
  },
  {
    "text": "so the work that I've done with spark has been kind of using it to analyze large volumes of telematics data coming off of",
    "start": "311420",
    "end": "318670"
  },
  {
    "text": "Lorries as they're driving around and using the data from that to identify",
    "start": "318980",
    "end": "322869"
  },
  {
    "text": "Locations where incidents are occurring such as if they're cornering harshly or breaking harshly",
    "start": "323420",
    "end": "328719"
  },
  {
    "text": "Outside of research what sorts of things is spark we use. Yes",
    "start": "329450",
    "end": "332890"
  },
  {
    "text": "So Sparky is used quite a lot in the real world",
    "start": "332890",
    "end": "336039"
  },
  {
    "text": "like",
    "start": "336040",
    "end": "336440"
  },
  {
    "text": "You would find a lot of companies will be using it to kind of do large-scale jobs and all of the data that they have",
    "start": "336440",
    "end": "342760"
  },
  {
    "text": "and it can be used for like analysis or simply just processing that data and putting it into storage the good thing about the",
    "start": "343070",
    "end": "350379"
  },
  {
    "text": "Distributed computing in clusters is that if you want to scale the program more you just add",
    "start": "350630",
    "end": "355779"
  },
  {
    "text": "more",
    "start": "356600",
    "end": "357980"
  },
  {
    "text": "Like nodes to the cluster. So the point is if you want to increase your processing power you don't have to",
    "start": "357980",
    "end": "363610"
  },
  {
    "text": "Buy new hardware in terms of replacing your hardware",
    "start": "364250",
    "end": "367600"
  },
  {
    "text": "You keep your old hardware",
    "start": "367600",
    "end": "368540"
  },
  {
    "text": "You buy new nodes and just stick them on the end and you've instantly increased how much processing power you have",
    "start": "368540",
    "end": "373629"
  },
  {
    "text": "So if you suddenly say get a load more data that you need to be posting. You think Oh miss current cluster size",
    "start": "373820",
    "end": "379390"
  },
  {
    "text": "That's not great. We can",
    "start": "379610",
    "end": "381610"
  },
  {
    "text": "Then expand that and then just add a few more nodes",
    "start": "381680",
    "end": "384940"
  },
  {
    "text": "so the SPARC program would then just scale automatically going back to RDDs these are immutable data structures so that",
    "start": "384940",
    "end": "392140"
  },
  {
    "text": "Immutable means they can't be changed. Right? Is that right? Yes. Yeah. So yeah, and they're immutable they cannot be changed once they're created",
    "start": "392930",
    "end": "399130"
  },
  {
    "text": "You can pass them to other functions, but you can't change the contents of that single RDD",
    "start": "399680",
    "end": "404199"
  },
  {
    "text": "so what the spark program ends up being is kind of a chain of",
    "start": "404419",
    "end": "407259"
  },
  {
    "text": "Transformations with each one creating a new RDD and passing it on to the next function",
    "start": "407570",
    "end": "412779"
  },
  {
    "text": "The advantage of the RDDs is that they can be persisted in memory",
    "start": "412820",
    "end": "417999"
  },
  {
    "text": "Which means that then it's more efficient to reuse them later in the computation. So one of the disadvantages of hadoop mapreduce, is that you",
    "start": "418000",
    "end": "425529"
  },
  {
    "text": "It's every time you're writing and stuff to disk basically after your MapReduce computation if you want to reuse it",
    "start": "426169",
    "end": "433329"
  },
  {
    "text": "You've then got to go and get it from disk again",
    "start": "433330",
    "end": "435668"
  },
  {
    "text": "Whereas with SPARC you can just persist the rdd's in memory",
    "start": "436639",
    "end": "439449"
  },
  {
    "text": "If you want to come back to them later, then you can do it really easily",
    "start": "439450",
    "end": "442569"
  },
  {
    "text": "You're saying large amount volumes of data. Can we put some numbers on this? Well, what are we looking at here?",
    "start": "444050",
    "end": "448720"
  },
  {
    "text": "See the volumes of data we're talking about can vary I guess depending on company. It's probably ranging gigabytes to terabytes and then the biggest",
    "start": "449000",
    "end": "456519"
  },
  {
    "text": "We then just keep going up basically",
    "start": "457100",
    "end": "459369"
  }
]