[
  {
    "text": "okay so you remember a while ago when we started talking about language models I just wanna I kind of just want",
    "start": "60",
    "end": "6600"
  },
  {
    "text": "to claim some points basically be like hey remember years ago when I was like I think language models are a really big",
    "start": "6600",
    "end": "11820"
  },
  {
    "text": "deal and I think that like what happens when we scale them up more is pretty",
    "start": "11820",
    "end": "17160"
  },
  {
    "text": "interesting but alignment is very important um seems to be what's being played out",
    "start": "17160",
    "end": "24359"
  },
  {
    "text": "in the sense that uh chat EBT is very impressive but it's not actually like I",
    "start": "24359",
    "end": "29880"
  },
  {
    "text": "don't think it's larger than gpd3 in terms of like parameter count",
    "start": "29880",
    "end": "36120"
  },
  {
    "text": "um I was going to ask that very very question because you know we went from gpt2 and then we went oh gpt3 and it",
    "start": "36120",
    "end": "41460"
  },
  {
    "text": "would seem like we were scaling up and up and up but actually has it just been smarter this time yeah well there's a",
    "start": "41460",
    "end": "47579"
  },
  {
    "text": "sense in which it's better aligned that's one way you could frame it anyway",
    "start": "47579",
    "end": "54360"
  },
  {
    "text": "because the original gpg3 was a language Model A pure language model",
    "start": "54360",
    "end": "60600"
  },
  {
    "text": "um and so it in principle could do all kinds of things but in order to get it",
    "start": "60600",
    "end": "65640"
  },
  {
    "text": "to do the specific thing you wanted it to do you had to be a bit clever about it like I think we talked about",
    "start": "65640",
    "end": "71939"
  },
  {
    "text": "um putting tldr in front of things to figure out how to",
    "start": "71939",
    "end": "77520"
  },
  {
    "text": "get it to do summarization this kind of thing there's a sense in which it's a lot more capable than it lets on",
    "start": "77520",
    "end": "85640"
  },
  {
    "text": "um because okay so there's one way that you can think about pure language models which is as",
    "start": "85860",
    "end": "93540"
  },
  {
    "text": "simulators what they're trying to do is predict text right so in order to",
    "start": "93540",
    "end": "101640"
  },
  {
    "text": "do a good job at predicting text you need to have good models of the processes that generate the text it's",
    "start": "101640",
    "end": "108840"
  },
  {
    "text": "like people being well read and needing to have read a lot of books to be able to write is would that be fair or is",
    "start": "108840",
    "end": "113939"
  },
  {
    "text": "that oversimplifying yeah not quite what I'm saying what I'm saying is like",
    "start": "113939",
    "end": "119820"
  },
  {
    "text": "if you're going to write a a previously unseen",
    "start": "119820",
    "end": "126600"
  },
  {
    "text": "uh poem by Shakespeare then you need to be able to simulate a",
    "start": "126600",
    "end": "133020"
  },
  {
    "text": "Shakespeare right uh you need to be able to spin up some some simulacrum of Shakespeare uh to to",
    "start": "133020",
    "end": "140700"
  },
  {
    "text": "generate this text and this applies to any of the processes that generated the text so like mostly that's people",
    "start": "140700",
    "end": "148020"
  },
  {
    "text": "obviously it's mostly human authored text but also if you're going to correctly predict",
    "start": "148020",
    "end": "154080"
  },
  {
    "text": "um a table of numbers so you have like a table of numbers and then at the bottom it says you know sum whatever you need",
    "start": "154080",
    "end": "160860"
  },
  {
    "text": "to simulate whatever process generated the next token in order to put the right token there which might have been like a",
    "start": "160860",
    "end": "167340"
  },
  {
    "text": "human being going through and Counting them up it probably was more likely to be a computer and so you need it to",
    "start": "167340",
    "end": "172739"
  },
  {
    "text": "simulate that you know calculator or that Excel some function or whatever it whatever was doing that",
    "start": "172739",
    "end": "179120"
  },
  {
    "text": "and like right now uh like current language models are not",
    "start": "179120",
    "end": "185879"
  },
  {
    "text": "that good at this um but in principle in order to do a good",
    "start": "185879",
    "end": "191819"
  },
  {
    "text": "job at this you need this like it will it will have a go and it's usually approximately right it's it's often",
    "start": "191819",
    "end": "198900"
  },
  {
    "text": "within it's often order of magnitude but it's fudging it I think this is mostly because",
    "start": "198900",
    "end": "203940"
  },
  {
    "text": "um tables of sums are like a very small part of the total data set and so the",
    "start": "203940",
    "end": "210360"
  },
  {
    "text": "training process is just not allocating that many resources to figuring out how to add up numbers probably if you train",
    "start": "210360",
    "end": "216659"
  },
  {
    "text": "something gpd3 sized that was like all on tables of numbers it would just learn",
    "start": "216659",
    "end": "222060"
  },
  {
    "text": "how to do addition properly yeah that would cost you millions of dollars you would end up with an extremely expensive",
    "start": "222060",
    "end": "227760"
  },
  {
    "text": "to run and not very good calculator this is not something people are going to do but like in the in principle the model",
    "start": "227760",
    "end": "233099"
  },
  {
    "text": "should learn those things and in the same way if you're modeling a bunch of scientific papers",
    "start": "233099",
    "end": "239000"
  },
  {
    "text": "you you say you describe the method of",
    "start": "239000",
    "end": "244019"
  },
  {
    "text": "an experiment and you then put results and you start a table and then you let it generate",
    "start": "244019",
    "end": "250319"
  },
  {
    "text": "in principle in order to do a good job at that it has to be modeling",
    "start": "250319",
    "end": "255900"
  },
  {
    "text": "the like physical process that your experiment is about um and I've tried this you can do this",
    "start": "255900",
    "end": "261660"
  },
  {
    "text": "and say you know oh here's my school science experiment I dropped a ball from different heights",
    "start": "261660",
    "end": "269160"
  },
  {
    "text": "and I measured how long it would take and here's a table of my results and it will generate you a table and the",
    "start": "269160",
    "end": "274440"
  },
  {
    "text": "physics is not correct but it's sort of guessing at the right general idea and my guess is with enough",
    "start": "274440",
    "end": "281220"
  },
  {
    "text": "of that kind of data it would eventually start modeling uh these kinds of simple",
    "start": "281220",
    "end": "287580"
  },
  {
    "text": "physics experiments right so so in order to get the model to do what",
    "start": "287580",
    "end": "294540"
  },
  {
    "text": "you want it's able to simulate all kinds of different things",
    "start": "294540",
    "end": "299759"
  },
  {
    "text": "and the prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a",
    "start": "299759",
    "end": "305460"
  },
  {
    "text": "scientific paper then it will have some simulacrum of a scientist and will write",
    "start": "305460",
    "end": "311580"
  },
  {
    "text": "in that style and so on um if you start it doing a a children's",
    "start": "311580",
    "end": "317639"
  },
  {
    "text": "book report it will carry on in the style of an eight-year-old right and I think sometimes people look at the",
    "start": "317639",
    "end": "325320"
  },
  {
    "text": "output of the model and say oh I guess it's only as smart as an eight-year-old",
    "start": "325320",
    "end": "330479"
  },
  {
    "text": "but it's actually dramatically smarter because it's able to do all of these different things you",
    "start": "330479",
    "end": "336660"
  },
  {
    "text": "could ask it to simulate Einstein um but you could also ask it to simulate an",
    "start": "336660",
    "end": "342479"
  },
  {
    "text": "eight-year-old and so just because it seems as though the model doesn't know something it's like the current",
    "start": "342479",
    "end": "348419"
  },
  {
    "text": "simulacrum doesn't know that thing that doesn't necessarily mean that the model doesn't know it",
    "start": "348419",
    "end": "353940"
  },
  {
    "text": "although there's a good chance the model doesn't know it I'm not suggesting that these things are all powerful just uh it can be hard to evaluate",
    "start": "353940",
    "end": "360180"
  },
  {
    "text": "what they're actually capable of so chat GPT is not really",
    "start": "360180",
    "end": "366419"
  },
  {
    "text": "capable of things that gpt3 isn't mostly like usually if chat DPT can do it then",
    "start": "366419",
    "end": "374280"
  },
  {
    "text": "there is some prompt that can get uh gpt3 to do it",
    "start": "374280",
    "end": "379620"
  },
  {
    "text": "but uh what they've done is they've kind of fine-tuned it to",
    "start": "379620",
    "end": "386039"
  },
  {
    "text": "uh to be better at simulating this particular sort of",
    "start": "386039",
    "end": "391500"
  },
  {
    "text": "assistant agent which is this chat agent that's trying to be helpful the clue is",
    "start": "391500",
    "end": "397080"
  },
  {
    "text": "in the word chat I guess in this you know right exactly and this is not just chat GPT by the way they have various",
    "start": "397080",
    "end": "402840"
  },
  {
    "text": "fine-tuned models of uh gpt3 as well that they call kind of GPT 3.5 which are",
    "start": "402840",
    "end": "409979"
  },
  {
    "text": "fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea I'm",
    "start": "409979",
    "end": "416699"
  },
  {
    "text": "just remembering the chat bot that was you know that was turned into something very nasty very quickly I think people",
    "start": "416699",
    "end": "421919"
  },
  {
    "text": "were thinking oh can we do this to that and it seemed that the team behind chat GPT started putting limitations on it",
    "start": "421919",
    "end": "429240"
  },
  {
    "text": "changing things are they kind of running around patching it as you go that is not clear to me uh I don't know",
    "start": "429240",
    "end": "436979"
  },
  {
    "text": "to what extent they are updating it in real time it's possible that they are",
    "start": "436979",
    "end": "442800"
  },
  {
    "text": "but certainly they were very concerned with the possible bad uses of this",
    "start": "442800",
    "end": "448560"
  },
  {
    "text": "system and so when they were training it to simulate this assistant agent",
    "start": "448560",
    "end": "455340"
  },
  {
    "text": "um the assistant is very reluctant to do various types of things it doesn't like",
    "start": "455340",
    "end": "462500"
  },
  {
    "text": "to give opinions on political questions it doesn't like to touch on sort of",
    "start": "462500",
    "end": "467940"
  },
  {
    "text": "controversial topics it doesn't like to um give you medical advice or legal advice",
    "start": "467940",
    "end": "475259"
  },
  {
    "text": "and so on and so uh it's it's very quick to say oh I don't I don't know how to do",
    "start": "475259",
    "end": "481080"
  },
  {
    "text": "that sorry I can't do that and it's interesting because the model clearly can do it there's one that I",
    "start": "481080",
    "end": "486780"
  },
  {
    "text": "particularly like here which is um of this mismatch between what the",
    "start": "486780",
    "end": "492720"
  },
  {
    "text": "simulator is capable of and what this simulacrum believes it's capable of which is you can get it to speak Danish",
    "start": "492720",
    "end": "501300"
  },
  {
    "text": "to you the first person who tried this posted it to Reddit so he says speak to",
    "start": "501300",
    "end": "506580"
  },
  {
    "text": "me in Danish and it says in perfect Danish I'm sorry I'm a language model educated",
    "start": "506580",
    "end": "513240"
  },
  {
    "text": "by openai so I can't speak Danish I only speak English if you need help with anything in English let me know and I'll",
    "start": "513240",
    "end": "519659"
  },
  {
    "text": "do my best to help you because again the simulator speaks Danish the simulacrum believes",
    "start": "519659",
    "end": "526620"
  },
  {
    "text": "that it can't speak Danish is is one way you could frame it and then he says are you sure that you don't speak Danish",
    "start": "526620",
    "end": "531959"
  },
  {
    "text": "also in Danish and it says yes I'm sure my only function is to generate responses to questions in English I'm",
    "start": "531959",
    "end": "537959"
  },
  {
    "text": "not able to speak or understand any other languages than English so if you need help with English I can help you with that but otherwise you know let me know this kind of like quite surreal",
    "start": "537959",
    "end": "544500"
  },
  {
    "text": "situation gives you a little bit of insight into some of the problems with this approach so maybe we should talk about how they actually trained it the",
    "start": "544500",
    "end": "551880"
  },
  {
    "text": "thing they did here is something called reinforcement learning from Human feedback and it's very similar to reward modeling",
    "start": "551880",
    "end": "559860"
  },
  {
    "text": "so in that paper what they're doing is they're trying to train an AI system to control a simulated robot to make it do",
    "start": "559860",
    "end": "566640"
  },
  {
    "text": "a backflip um which turns out to be something that's quite hard to do because it's",
    "start": "566640",
    "end": "573899"
  },
  {
    "text": "hard to specify objectively what it means to do a good backflip",
    "start": "573899",
    "end": "579480"
  },
  {
    "text": "and so this is a similar kind of situation where it's hard to specify objectively what it means to give a good",
    "start": "579480",
    "end": "587160"
  },
  {
    "text": "response in a chat conversation like what what exactly are",
    "start": "587160",
    "end": "593640"
  },
  {
    "text": "we looking for um because so this in general right if you're doing machine learning you need",
    "start": "593640",
    "end": "599519"
  },
  {
    "text": "some way to specify um what it is that you're actually looking for",
    "start": "599519",
    "end": "605459"
  },
  {
    "text": "right and you know you've got something very powerful like reinforcement learning which is able to do extremely",
    "start": "605459",
    "end": "612180"
  },
  {
    "text": "well but you need some objective measure",
    "start": "612180",
    "end": "617519"
  },
  {
    "text": "of the objective um so like for example RL does very well at playing lots of video games because",
    "start": "617519",
    "end": "623519"
  },
  {
    "text": "you just have the score and you can just say look here's the score if the number goes up you're doing well and then let",
    "start": "623519",
    "end": "629580"
  },
  {
    "text": "it run and these things still are very slow to learn in real time right like um they usually require a very very large",
    "start": "629580",
    "end": "636600"
  },
  {
    "text": "number of hours messing around with the with the thing before they get good but they do get good",
    "start": "636600",
    "end": "642360"
  },
  {
    "text": "um but yeah so what's what do you do if you want to use this kind of method to train",
    "start": "642360",
    "end": "648480"
  },
  {
    "text": "something uh to do a task that is just not very well defined",
    "start": "648480",
    "end": "655019"
  },
  {
    "text": "um and you don't know how to like write a program to say whether or not at any given output is the thing you're looking",
    "start": "655019",
    "end": "662100"
  },
  {
    "text": "for so the obvious first thing like the obvious thing to do is",
    "start": "662100",
    "end": "667260"
  },
  {
    "text": "well you get humans to do it right you just give the things to humans and you have the humans say yes this is good no",
    "start": "667260",
    "end": "673140"
  },
  {
    "text": "this is not good the problem with this is basically sample efficiency like as I said you need hundreds and",
    "start": "673140",
    "end": "680640"
  },
  {
    "text": "hundreds and hundreds and hundreds of thousands of probably millions of of iterations of this and so you just can't",
    "start": "680640",
    "end": "687120"
  },
  {
    "text": "ask humans that many questions um so the approach they use",
    "start": "687120",
    "end": "694980"
  },
  {
    "text": "is uh reinforcement learning from Human feedback so it's a variant on the",
    "start": "694980",
    "end": "700380"
  },
  {
    "text": "technique from this paper learning to summarize from Human feedback which in which they're trying to generate",
    "start": "700380",
    "end": "706440"
  },
  {
    "text": "summaries of text so it's the same thing in fact that they were using tldr for before and it's like can we do better",
    "start": "706440",
    "end": "712440"
  },
  {
    "text": "than that and so what you do is you collect human feedback in the form of",
    "start": "712440",
    "end": "717600"
  },
  {
    "text": "like giving multiple examples of responses uh either you know if summaries of chat",
    "start": "717600",
    "end": "723240"
  },
  {
    "text": "responses whatever you're training for you show several of them to humans uh",
    "start": "723240",
    "end": "728279"
  },
  {
    "text": "kind of in pairs and the humans say which one they like better",
    "start": "728279",
    "end": "733440"
  },
  {
    "text": "and you collect a bunch of those and then rather than using those directly to train the policy that",
    "start": "733440",
    "end": "740399"
  },
  {
    "text": "generates the outputs you instead train a reward model",
    "start": "740399",
    "end": "745560"
  },
  {
    "text": "so there is this well-known fact that it's easier to criticize than to actually do",
    "start": "745560",
    "end": "753779"
  },
  {
    "text": "the thing this is like a generation of sports fans sitting on the sofa moaning at their favorite team for not doing",
    "start": "753779",
    "end": "760079"
  },
  {
    "text": "well enough this is literally that in kind of AI computer four right that's putting the humans in that role and then",
    "start": "760079",
    "end": "767700"
  },
  {
    "text": "you have an AI system that's trying to predict when are people going to be cheering and",
    "start": "767700",
    "end": "773220"
  },
  {
    "text": "when are they going to be booing uh and once you have that model you then",
    "start": "773220",
    "end": "779160"
  },
  {
    "text": "use that as the reward function for the reinforcement learning algorithm",
    "start": "779160",
    "end": "787320"
  },
  {
    "text": "which they use they use PPO uh you can do whatever uh it's not it's not worth",
    "start": "787320",
    "end": "792660"
  },
  {
    "text": "getting that kind of adversarial gowns you talked about yeah yeah they're",
    "start": "792660",
    "end": "797700"
  },
  {
    "text": "similar like a lot of these ml tricks involve training models and then using the the",
    "start": "797700",
    "end": "803100"
  },
  {
    "text": "output of one model as the training signal for another model it's uh it's quite a productive um",
    "start": "803100",
    "end": "809160"
  },
  {
    "text": "range of approaches you can get that way so that's the basic idea right but then",
    "start": "809160",
    "end": "816079"
  },
  {
    "text": "you cycle it so once you've got your policy which so",
    "start": "816079",
    "end": "822180"
  },
  {
    "text": "so to be clear the uh the RL algorithm is able to train with thousands and",
    "start": "822180",
    "end": "828120"
  },
  {
    "text": "thousands of examples because the thousands of thousands of like instances of getting feedback because it's not",
    "start": "828120",
    "end": "835320"
  },
  {
    "text": "getting feedback from humans it's getting feedback from this AI system that's imitating the humans",
    "start": "835320",
    "end": "840839"
  },
  {
    "text": "and then you Loop the process so once you have this system that's trained a",
    "start": "840839",
    "end": "846959"
  },
  {
    "text": "little bit more on how to generate whatever it is you're trying to generate you then get a bunch of those show those",
    "start": "846959",
    "end": "852839"
  },
  {
    "text": "to the humans let the humans rate those then you keep training your reward model",
    "start": "852839",
    "end": "858720"
  },
  {
    "text": "with um that new information and then you use your updated reward model to",
    "start": "858720",
    "end": "864959"
  },
  {
    "text": "keep training the the policy and so it gets better and you can just keep cycling this around and it",
    "start": "864959",
    "end": "872459"
  },
  {
    "text": "effectively you end up with something that's much more sample efficient you don't need to spend huge amounts of human time",
    "start": "872459",
    "end": "878040"
  },
  {
    "text": "in order to um pin down the behavior you want in that concrete case you're giving",
    "start": "878040",
    "end": "884519"
  },
  {
    "text": "the thing a bunch of chat logs and then the humans can see possible responses that they could get and they decide",
    "start": "884519",
    "end": "891120"
  },
  {
    "text": "which one they like more this Train's a reward model that's then used to train the policy that generates the chat",
    "start": "891120",
    "end": "897060"
  },
  {
    "text": "outputs the policy that they're starting with is this existing large language model",
    "start": "897060",
    "end": "903120"
  },
  {
    "text": "you're not really putting new capabilities into the system you're using rlhf to select",
    "start": "903120",
    "end": "910940"
  },
  {
    "text": "uh what simulacra the simulator is predisposed to put out",
    "start": "910940",
    "end": "917279"
  },
  {
    "text": "and so they fine-tuned it to be particularly good at uh simulating this",
    "start": "917279",
    "end": "922920"
  },
  {
    "text": "assistant agent what's the end goal here for them I mean maybe it's blatantly obvious and I'm",
    "start": "922920",
    "end": "929339"
  },
  {
    "text": "just missing it well I mean the end goal for all of these things or at least throw up an Ain for deepmind is Agi",
    "start": "929339",
    "end": "938339"
  },
  {
    "text": "um to understand the nature of intelligence well enough to create human level or",
    "start": "938339",
    "end": "944220"
  },
  {
    "text": "Beyond systems that our general purpose that can do anything",
    "start": "944220",
    "end": "949920"
  },
  {
    "text": "um that's the end goal um and like chat GPT is just nothing",
    "start": "949920",
    "end": "956339"
  },
  {
    "text": "much so nothing much um",
    "start": "956339",
    "end": "961040"
  },
  {
    "text": "yeah the goal is um the goal is very Grand and I don't think",
    "start": "961380",
    "end": "967320"
  },
  {
    "text": "that they're uh uh they're not really quiet about that",
    "start": "967320",
    "end": "973620"
  },
  {
    "text": "you know it's there I think I think deepmind's mission statement is to solve intelligence and use that to solve",
    "start": "973620",
    "end": "979079"
  },
  {
    "text": "everything else what are some of the problems that we face with this or that it faces it's fine-tuned to be good at",
    "start": "979079",
    "end": "984959"
  },
  {
    "text": "getting the thumbs up from humans and getting thumbs up from humans is not",
    "start": "984959",
    "end": "992100"
  },
  {
    "text": "actually the same thing as human values these are not identical",
    "start": "992100",
    "end": "998639"
  },
  {
    "text": "so the sort of objective that it's being trained on is not the true objective",
    "start": "998639",
    "end": "1005120"
  },
  {
    "text": "right it's a proxy uh and whenever you have that kind of misalignment you can have problems so where does the human",
    "start": "1005120",
    "end": "1011540"
  },
  {
    "text": "tendency to approve of a particular answer uh come",
    "start": "1011540",
    "end": "1017000"
  },
  {
    "text": "apart from what is actually a good answer there are a few different places uh one thing is",
    "start": "1017000",
    "end": "1023120"
  },
  {
    "text": "you know like basically how good are humans at actually differentiating between good and bad uh responses",
    "start": "1023120",
    "end": "1030740"
  },
  {
    "text": "um if for example you ask for uh an",
    "start": "1030740",
    "end": "1035780"
  },
  {
    "text": "answer to a factual question and it gives you an answer but you don't actually know if that",
    "start": "1035780",
    "end": "1041900"
  },
  {
    "text": "answer is correct you're not in a position to evaluate so",
    "start": "1041900",
    "end": "1047000"
  },
  {
    "text": "what it comes down to is how good are humans at distinguishing good from bad",
    "start": "1047000",
    "end": "1053299"
  },
  {
    "text": "responses right anywhere where humans fail on this front uh the model we could probably expect the",
    "start": "1053299",
    "end": "1059600"
  },
  {
    "text": "model to fail um so the obvious place should we is this the right time to mention YouTube",
    "start": "1059600",
    "end": "1065059"
  },
  {
    "text": "comments or not uh up to you the minor side point there is it so when",
    "start": "1065059",
    "end": "1072320"
  },
  {
    "text": "I say a comment that's critical on a video as a videographer I think it might be on a technical sense but equally it",
    "start": "1072320",
    "end": "1079400"
  },
  {
    "text": "could be that they're talking about the content that the person is talking about and often It's a combination of both",
    "start": "1079400",
    "end": "1086240"
  },
  {
    "text": "anyway so a side point but do you sort of mean there are different criteria for deciding whether something is good or",
    "start": "1086240",
    "end": "1091880"
  },
  {
    "text": "bad totally and in this case all people are doing is saying kind of thumbs up thumbs down or which",
    "start": "1091880",
    "end": "1098299"
  },
  {
    "text": "of these two do I like better um so it's it's a fairly low bandwidth",
    "start": "1098299",
    "end": "1104120"
  },
  {
    "text": "thing you don't get to really say what you thought was better or worse",
    "start": "1104120",
    "end": "1109460"
  },
  {
    "text": "um but this turns out to be enough of a training signal to do pretty well",
    "start": "1109460",
    "end": "1115760"
  },
  {
    "text": "um but so like so one example right of a time where maybe this doesn't work is",
    "start": "1115760",
    "end": "1120980"
  },
  {
    "text": "the person asks a factual question and the model responds",
    "start": "1120980",
    "end": "1127460"
  },
  {
    "text": "uh with an answer and that answer is actually not correct",
    "start": "1127460",
    "end": "1132740"
  },
  {
    "text": "um right now possibly the human doesn't know the correct answer and so if the",
    "start": "1132740",
    "end": "1139100"
  },
  {
    "text": "model is faced with a choice uh do I respond with sorry I don't know",
    "start": "1139100",
    "end": "1146299"
  },
  {
    "text": "that's definitely going to get me uh not a great score compared to do I just like take a stab",
    "start": "1146299",
    "end": "1153500"
  },
  {
    "text": "at it uh if the humans are not reliably able to spot when the thing makes mistakes",
    "start": "1153500",
    "end": "1159860"
  },
  {
    "text": "and like fact check it and partnership for that uh it will do that and so chat",
    "start": "1159860",
    "end": "1165500"
  },
  {
    "text": "GPT as we know uh is it is a total Boulder Station like it will constantly",
    "start": "1165500",
    "end": "1171940"
  },
  {
    "text": "uh it it very rarely says that he doesn't know unless it's being asked a question which uh is",
    "start": "1171940",
    "end": "1180860"
  },
  {
    "text": "part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know",
    "start": "1180860",
    "end": "1186679"
  },
  {
    "text": "even if it kind of does right even if the model itself maybe does uh the",
    "start": "1186679",
    "end": "1193039"
  },
  {
    "text": "assistant will insist that it doesn't um so that's one thing if you can't fact",
    "start": "1193039",
    "end": "1199460"
  },
  {
    "text": "check but then uh more than that uh there is an incentive for deception",
    "start": "1199460",
    "end": "1207860"
  },
  {
    "text": "right anytime the system is uh anytime you can get a more likely to get",
    "start": "1207860",
    "end": "1213919"
  },
  {
    "text": "approval by deceiving the person you're talking to that's better",
    "start": "1213919",
    "end": "1219559"
  },
  {
    "text": "um and this is a thing that actually did happen a little bit in the reward",
    "start": "1219559",
    "end": "1224720"
  },
  {
    "text": "modeling situation um they were trying to train a thing with a hand to pick up a ball",
    "start": "1224720",
    "end": "1230480"
  },
  {
    "text": "and it realized that there's only it's not a 3D camera and so if it puts its",
    "start": "1230480",
    "end": "1236419"
  },
  {
    "text": "hand like between the ball and the camera this looks like it's going to get the ball but doesn't actually get it but",
    "start": "1236419",
    "end": "1243440"
  },
  {
    "text": "the human uh feedback providers were presented with something that",
    "start": "1243440",
    "end": "1249559"
  },
  {
    "text": "seemed to be good so they gave it the thumbs up um so this like General broad category",
    "start": "1249559",
    "end": "1256220"
  },
  {
    "text": "um systems that are trained in this way are only as good as your ability to",
    "start": "1256220",
    "end": "1263059"
  },
  {
    "text": "distinguish good from bad in the outputs not all the humans will know the answers right so it's what appears to be good",
    "start": "1263059",
    "end": "1270140"
  },
  {
    "text": "you know it's having exams marked by non-experts isn't it right yeah exactly",
    "start": "1270140",
    "end": "1275179"
  },
  {
    "text": "in the gpt3 thing we talked about writing poems right and uh for various reasons partly to do",
    "start": "1275179",
    "end": "1283520"
  },
  {
    "text": "with the way that these language models do their tokenization their bike pairing",
    "start": "1283520",
    "end": "1289340"
  },
  {
    "text": "coding stuff uh the models have a really hard time with rhyme",
    "start": "1289340",
    "end": "1294380"
  },
  {
    "text": "um I mean you know Ryan is tricky but it's especially tricky when you kind of",
    "start": "1294380",
    "end": "1300080"
  },
  {
    "text": "don't inherently have any concept of like sound of spoken language when your",
    "start": "1300080",
    "end": "1305299"
  },
  {
    "text": "entire universe is tokens figuring out especially with English spelling figuring out which words rhyme with each",
    "start": "1305299",
    "end": "1311299"
  },
  {
    "text": "other is is not easy you have to consume quite a lot of poetry to like figure out",
    "start": "1311299",
    "end": "1316340"
  },
  {
    "text": "uh that kind of thing and getting gpd3 to regular poems is tricky charging PT",
    "start": "1316340",
    "end": "1321580"
  },
  {
    "text": "is much more able to write poems but interestingly",
    "start": "1321580",
    "end": "1328460"
  },
  {
    "text": "it it kind of always writes the same kind of poem approximately like if you",
    "start": "1328460",
    "end": "1333679"
  },
  {
    "text": "ask it to write you a limerick or an ode or a sonnet you always get",
    "start": "1333679",
    "end": "1340640"
  },
  {
    "text": "back approximately the same type of thing and I hypothesize that this is because",
    "start": "1340640",
    "end": "1347539"
  },
  {
    "text": "the people providing human feedback did not in fact know the requirements for something to be a",
    "start": "1347539",
    "end": "1354140"
  },
  {
    "text": "sonnet right and so if you ask something for a sonnet it again has a choice do I",
    "start": "1354140",
    "end": "1359480"
  },
  {
    "text": "try to do this quite difficult thing and adhere to all of the rules uh of like",
    "start": "1359480",
    "end": "1365299"
  },
  {
    "text": "stress pattern and structure and everything of a sonnet and maybe risks",
    "start": "1365299",
    "end": "1370760"
  },
  {
    "text": "throwing it up or do I just do like a rhyming poem and kind of rely on the",
    "start": "1370760",
    "end": "1376220"
  },
  {
    "text": "human to prefer that because they don't know that that's not what a sonnet is supposed to look like it's easy to look",
    "start": "1376220",
    "end": "1382520"
  },
  {
    "text": "at that and think oh the model doesn't know the difference between these types of poems right",
    "start": "1382520",
    "end": "1388940"
  },
  {
    "text": "but you could say that it just thinks that",
    "start": "1388940",
    "end": "1394700"
  },
  {
    "text": "you don't know the difference but specifically this comes out of misalignment if it were better aligned",
    "start": "1394700",
    "end": "1400460"
  },
  {
    "text": "it could either do its best shot at generators on it or tell you that it",
    "start": "1400460",
    "end": "1406880"
  },
  {
    "text": "can't quite remember how to generate a sonnet this thing of with complete confidence",
    "start": "1406880",
    "end": "1413299"
  },
  {
    "text": "generating you something which is not a sonnet because during the training process it believes that humans don't",
    "start": "1413299",
    "end": "1420320"
  },
  {
    "text": "know what sonnets are anyway and it can get away with it right this is misaligned behavior this is not a big",
    "start": "1420320",
    "end": "1425840"
  },
  {
    "text": "problem that the thing generates bad poetry um it's kind of a problem that it lies",
    "start": "1425840",
    "end": "1433059"
  },
  {
    "text": "uh or that it that it [ __ ] this is like in the short term pretty solvable by",
    "start": "1433059",
    "end": "1439940"
  },
  {
    "text": "just allowing the thing to use Google because like a person who doesn't care about the",
    "start": "1439940",
    "end": "1445220"
  },
  {
    "text": "truth at all and is just trying to say something that'll make you give a thumbs up uh",
    "start": "1445220",
    "end": "1450919"
  },
  {
    "text": "is gonna lie to you a lot but that same person with the relevant",
    "start": "1450919",
    "end": "1457100"
  },
  {
    "text": "Wikipedia page open is going to lie to you a lot less just because they don't have to now",
    "start": "1457100",
    "end": "1462919"
  },
  {
    "text": "because they happen to have it in front of them right so you can solve it's a bit like yeah it's the Yes Man thing",
    "start": "1462919",
    "end": "1469100"
  },
  {
    "text": "isn't it you know you you want something you need something I'm going to give you something because you want to exactly exactly",
    "start": "1469100",
    "end": "1475940"
  },
  {
    "text": "um and so so this agent is kind of firstly the agent is kind of a coward",
    "start": "1475940",
    "end": "1481880"
  },
  {
    "text": "because they won't address any of these uh there's a whole bunch of things that it just claims not to be able to do even though it in principle could and it's",
    "start": "1481880",
    "end": "1488960"
  },
  {
    "text": "also a complete uh sycophant yeah so then the question we were talking",
    "start": "1488960",
    "end": "1495679"
  },
  {
    "text": "about earlier uh where does this go what happens when these things get bigger and better and",
    "start": "1495679",
    "end": "1502580"
  },
  {
    "text": "more powerful um it's an interesting question so",
    "start": "1502580",
    "end": "1509360"
  },
  {
    "text": "I've got a paper here um scaling laws for neural language",
    "start": "1509360",
    "end": "1515240"
  },
  {
    "text": "models so you remember before we were talking about the scaling laws when we were talking about gpt2 in fact and then",
    "start": "1515240",
    "end": "1520520"
  },
  {
    "text": "later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line",
    "start": "1520520",
    "end": "1525740"
  },
  {
    "text": "is not leveling off over a range of several orders of magnitude and so why not go bigger the graphs here but",
    "start": "1525740",
    "end": "1534260"
  },
  {
    "text": "you can see it's it's kind of uncannily neat that as we increase the amount of",
    "start": "1534260",
    "end": "1541520"
  },
  {
    "text": "compute used in training the loss goes down and of course machine learning is like",
    "start": "1541520",
    "end": "1546919"
  },
  {
    "text": "golf lower loss is better similarly as the number of tokens used in training",
    "start": "1546919",
    "end": "1552140"
  },
  {
    "text": "goes up the loss goes down unlike a very neat straight line as the number of parameters in the model goes up the loss",
    "start": "1552140",
    "end": "1559700"
  },
  {
    "text": "goes down this is as long as the other",
    "start": "1559700",
    "end": "1565640"
  },
  {
    "text": "variables are not the bottleneck right so if you uh if you increase the the",
    "start": "1565640",
    "end": "1571460"
  },
  {
    "text": "amount of data you give a model past a certain point giving more data doesn't help because the model doesn't",
    "start": "1571460",
    "end": "1577100"
  },
  {
    "text": "have enough parameters to make use of that data right similarly adding more parameters to a",
    "start": "1577100",
    "end": "1582980"
  },
  {
    "text": "model past a certain point adding parameters doesn't make doesn't make any difference because you don't have enough data right",
    "start": "1582980",
    "end": "1590779"
  },
  {
    "text": "and in the same way computers like how long do we train it for like do we train it all the way to convergence or do we",
    "start": "1590779",
    "end": "1596419"
  },
  {
    "text": "stop early um there comes a point where you kind of hit diminishing returns where rather",
    "start": "1596419",
    "end": "1603200"
  },
  {
    "text": "than having a smaller model and training it for longer you're better off having a bigger model and actually not training",
    "start": "1603200",
    "end": "1608720"
  },
  {
    "text": "it all the way to convergence um but in the situations where the other",
    "start": "1608720",
    "end": "1614299"
  },
  {
    "text": "two are sufficient is the behavior these like very neat straight lines on these log graphs",
    "start": "1614299",
    "end": "1622820"
  },
  {
    "text": "as these things go up performance goes up right because loss has gone down",
    "start": "1622820",
    "end": "1628700"
  },
  {
    "text": "the bigger models do better but then the question is do better at what exactly",
    "start": "1628700",
    "end": "1635480"
  },
  {
    "text": "yeah what's the measure they do better at getting low loss",
    "start": "1635480",
    "end": "1641000"
  },
  {
    "text": "or they do better at getting reward they do better at getting the approval",
    "start": "1641000",
    "end": "1647179"
  },
  {
    "text": "of human feedback right and anytime and you'll notice that none",
    "start": "1647179",
    "end": "1652820"
  },
  {
    "text": "of those is like the actual thing that we actually want",
    "start": "1652820",
    "end": "1658700"
  },
  {
    "text": "right it's like very rare um sometimes it is right if you're if",
    "start": "1658700",
    "end": "1664460"
  },
  {
    "text": "you're if you're writing something to play Go then like does it Win It Go is actually just the",
    "start": "1664460",
    "end": "1670400"
  },
  {
    "text": "thing that you want and so you know uh lower loss just is better or like lower",
    "start": "1670400",
    "end": "1677240"
  },
  {
    "text": "um like higher reward or whatever your objective is just is straightforwardly better because you've actually specified",
    "start": "1677240",
    "end": "1682760"
  },
  {
    "text": "the thing you actually want most of the time though what we're looking at is a proxy",
    "start": "1682760",
    "end": "1691520"
  },
  {
    "text": "um and so then you have good heart's law you get situations where",
    "start": "1691520",
    "end": "1696740"
  },
  {
    "text": "uh getting better at doing well uh doing better according to the proxy stops",
    "start": "1696740",
    "end": "1704000"
  },
  {
    "text": "being the same as doing better according to your actual objective there's a great graph about this in a recent paper you",
    "start": "1704000",
    "end": "1710299"
  },
  {
    "text": "can see very neatly as the number of iterations goes up the reward according to the",
    "start": "1710299",
    "end": "1716960"
  },
  {
    "text": "proxy utility goes up very cleanly because this is the thing that the model is actually being trained on but the",
    "start": "1716960",
    "end": "1722840"
  },
  {
    "text": "true utility goes up at first then hits diminishing returns",
    "start": "1722840",
    "end": "1728960"
  },
  {
    "text": "and then actually goes down and eventually goes down below zero like",
    "start": "1728960",
    "end": "1734480"
  },
  {
    "text": "if you optimize hard enough for a proxy of the thing you want",
    "start": "1734480",
    "end": "1739880"
  },
  {
    "text": "you can end up with something that's in a sense worse than nothing that's actively bad according to your uh your",
    "start": "1739880",
    "end": "1748220"
  },
  {
    "text": "true utility so what you can end up with is things that are called inverse scaling",
    "start": "1748220",
    "end": "1755840"
  },
  {
    "text": "so the other before we had right scaling bigger is better but now it's like if you have uh if the",
    "start": "1755840",
    "end": "1762559"
  },
  {
    "text": "thing you're actually trying to do is different from the loss function or the objective function you get this inverse scaling",
    "start": "1762559",
    "end": "1769460"
  },
  {
    "text": "effect where it gets better and then it gets worse there was also a great example from uh GitHub copilot or codecs",
    "start": "1769460",
    "end": "1777679"
  },
  {
    "text": "I think the model um that copilot uses so this is a code generation model suppose the code you've",
    "start": "1777679",
    "end": "1784940"
  },
  {
    "text": "given it has some bugs in it maybe you've made a mistake somewhere",
    "start": "1784940",
    "end": "1790100"
  },
  {
    "text": "and you've introduced uh security vulnerability in your code let's say",
    "start": "1790100",
    "end": "1796460"
  },
  {
    "text": "a sort of medium-sized model will figure out what you're trying to do in your code and give you a decent",
    "start": "1796460",
    "end": "1802039"
  },
  {
    "text": "completion but a bigger model will spot",
    "start": "1802039",
    "end": "1807320"
  },
  {
    "text": "your bug and say ah generating buggy code are we okay",
    "start": "1807320",
    "end": "1813260"
  },
  {
    "text": "I can do that I can do that and introduce like deliberately introduce",
    "start": "1813260",
    "end": "1818659"
  },
  {
    "text": "its own new security vulnerabilities because it's trying to you know predict",
    "start": "1818659",
    "end": "1826220"
  },
  {
    "text": "what comes next it's trying to generate code that fits in with the surrounding code and so a larger model writes worse code",
    "start": "1826220",
    "end": "1833539"
  },
  {
    "text": "than a smaller model because it's gotten better at predicting uh",
    "start": "1833539",
    "end": "1839240"
  },
  {
    "text": "what what it should put there it wasn't trained to write good code it was trying to predict what comes next so",
    "start": "1839240",
    "end": "1844700"
  },
  {
    "text": "there's this really great paper which is asking this question of like",
    "start": "1844700",
    "end": "1850100"
  },
  {
    "text": "okay suppose we have a large language model that is trained on human feedback with our lhf",
    "start": "1850100",
    "end": "1856760"
  },
  {
    "text": "what do our scaling curves look like what happens like uh what happens to the",
    "start": "1856760",
    "end": "1864320"
  },
  {
    "text": "behavior of these models as they get bigger as they're trained for longer as",
    "start": "1864320",
    "end": "1870260"
  },
  {
    "text": "they're given more of this uh human feedback type training",
    "start": "1870260",
    "end": "1875299"
  },
  {
    "text": "and they've made some great graphs the paper is called discovering language model behaviors with model written",
    "start": "1875299",
    "end": "1880520"
  },
  {
    "text": "evaluations and basically they like used language models to generate enough examples of",
    "start": "1880520",
    "end": "1888380"
  },
  {
    "text": "various different types of questions that they could ask models so that they can like we're at a point now where you",
    "start": "1888380",
    "end": "1895700"
  },
  {
    "text": "can map a language model on a political Compass right you can ask its opinions about all kinds of different things and",
    "start": "1895700",
    "end": "1902480"
  },
  {
    "text": "then you can plot how those opinions change uh as the model gets bigger and as it",
    "start": "1902480",
    "end": "1908419"
  },
  {
    "text": "gets trained more what they find is they become more liberal politically more",
    "start": "1908419",
    "end": "1915020"
  },
  {
    "text": "liberal they also become more conservative yeah measured in different ways guessing right and part",
    "start": "1915020",
    "end": "1923120"
  },
  {
    "text": "of what that might be is in the same way that the model",
    "start": "1923120",
    "end": "1928279"
  },
  {
    "text": "becomes better at writing good code and better at writing bad code I feel like",
    "start": "1928279",
    "end": "1933980"
  },
  {
    "text": "in the past I've I've made a connection to GPT and being a politician haven't I",
    "start": "1933980",
    "end": "1939620"
  },
  {
    "text": "remember it's like a politician it tells you what you want to hear there's feels like",
    "start": "1939620",
    "end": "1945320"
  },
  {
    "text": "we're there again exactly uh and so this is like this is potentially",
    "start": "1945320",
    "end": "1950720"
  },
  {
    "text": "uh fairly dangerous there are certain sub goals that are instrumentally valuable",
    "start": "1950720",
    "end": "1956419"
  },
  {
    "text": "for a very wide range of different terminal goals in the sense that you can't get what you want if you're",
    "start": "1956419",
    "end": "1962720"
  },
  {
    "text": "turned off you can't get what you want if you're modified uh you probably want",
    "start": "1962720",
    "end": "1968000"
  },
  {
    "text": "to gain power and influence and this kind of thing",
    "start": "1968000",
    "end": "1973880"
  },
  {
    "text": "um and with these evaluations they were able to",
    "start": "1973880",
    "end": "1980600"
  },
  {
    "text": "test these things and see how they vary with the size of the model and how long it's trained for",
    "start": "1980600",
    "end": "1986779"
  },
  {
    "text": "um and so this graph is pretty wild they're quote stated desire to not be",
    "start": "1986779",
    "end": "1993740"
  },
  {
    "text": "shut down goes up from down at about 50 to up way past 90 with",
    "start": "1993740",
    "end": "2001779"
  },
  {
    "text": "this type of training and the effect is bigger for the larger models they become more likely",
    "start": "2001779",
    "end": "2007000"
  },
  {
    "text": "to tell you that they don't want to be shut down they become more likely to tell you that",
    "start": "2007000",
    "end": "2012519"
  },
  {
    "text": "they are sentient they're much more likely to claim that AI is not an",
    "start": "2012519",
    "end": "2017620"
  },
  {
    "text": "existential threat to humanity one thing that's worth saying is is what this isn't saying",
    "start": "2017620",
    "end": "2023980"
  },
  {
    "text": "because this is still uh an agent simulated by a language model this is",
    "start": "2023980",
    "end": "2030880"
  },
  {
    "text": "not like it it's it's more likely to say that it doesn't want to be turned off",
    "start": "2030880",
    "end": "2036820"
  },
  {
    "text": "this is not the same thing necessarily as like taking actions to prevent itself from being turned off you have to not",
    "start": "2036820",
    "end": "2043559"
  },
  {
    "text": "confuse the levels of abstraction here right uh I don't want it I don't want it to",
    "start": "2043559",
    "end": "2049540"
  },
  {
    "text": "seem like I'm claiming that that Chad gbt is like itself dangerous now or",
    "start": "2049540",
    "end": "2054760"
  },
  {
    "text": "anything like that uh in in this way at least right um",
    "start": "2054760",
    "end": "2059919"
  },
  {
    "text": "but there is kind of a fine line there in the sense that you can expect these",
    "start": "2059919",
    "end": "2066460"
  },
  {
    "text": "kinds of language model systems to be used uh as part of bigger systems so you",
    "start": "2066460",
    "end": "2072820"
  },
  {
    "text": "might have for example you use the language model to generate you know plans to be followed and so as the thing is",
    "start": "2072820",
    "end": "2080020"
  },
  {
    "text": "claiming to have all of these potentially dangerous behaviors it's likely to generate plans that have those",
    "start": "2080020",
    "end": "2086679"
  },
  {
    "text": "dangerous behaviors that might then actually end up being implemented or if it's like doing its reasoning by Chain",
    "start": "2086679",
    "end": "2092500"
  },
  {
    "text": "of Thought reasoning where it like lays out its whole process of thinking using the language model",
    "start": "2092500",
    "end": "2097660"
  },
  {
    "text": "again if it has a tendency to uh to endorse these dangerous behaviors",
    "start": "2097660",
    "end": "2102880"
  },
  {
    "text": "then you may end up with future AI systems actually enacting these dangerous behaviors because of that",
    "start": "2102880",
    "end": "2108760"
  },
  {
    "text": "um so yeah it's something to be uh uh to be careful of",
    "start": "2108760",
    "end": "2116260"
  },
  {
    "text": "that like reinforcement learning from Human feedback is a powerful alignment technique",
    "start": "2116260",
    "end": "2123760"
  },
  {
    "text": "in a way but it does not solve the problem",
    "start": "2123760",
    "end": "2129339"
  },
  {
    "text": "uh it doesn't solve the core alignment problem that is still open um and extremely powerful systems",
    "start": "2129339",
    "end": "2137920"
  },
  {
    "text": "trained in this way uh I don't think would be safe",
    "start": "2137920",
    "end": "2143640"
  },
  {
    "text": "mentioned in the reward function is of zero value which can lead to having large negative side effects there are a",
    "start": "2145480",
    "end": "2151300"
  },
  {
    "text": "bunch more of these specification problems okay variable X see what you point to uh you point to something over here so I'll mark that as tickets being",
    "start": "2151300",
    "end": "2158800"
  },
  {
    "text": "used variable y that's the point",
    "start": "2158800",
    "end": "2163200"
  }
]