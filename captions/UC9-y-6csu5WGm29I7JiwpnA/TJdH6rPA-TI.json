[
  {
    "text": "Principal component analysis is perhaps the most widely used data reduction technique on the planet",
    "start": "99",
    "end": "5399"
  },
  {
    "text": "Everyone uses it but here's the thing. It doesn't actually do data reduction",
    "start": "5710",
    "end": "9510"
  },
  {
    "text": "Principal component analysis is the idea of trying to find a different view for our data in which we can separate it better",
    "start": "13179",
    "end": "19469"
  },
  {
    "text": "And I'll show an example piece of paper",
    "start": "20410",
    "end": "22240"
  },
  {
    "text": "And the idea is that what we want to try and do is reframe our data",
    "start": "22240",
    "end": "25320"
  },
  {
    "text": "Maybe move it around so that we can better separate things out better cluster things perhaps it's better for machine learning",
    "start": "25320",
    "end": "30719"
  },
  {
    "text": "Now as a side effect of this",
    "start": "31119",
    "end": "33119"
  },
  {
    "text": "in PCA, we also order our",
    "start": "33760",
    "end": "36359"
  },
  {
    "text": "Axes by the most to least useful in some sense. So then we can perform a separate data reduction",
    "start": "37120",
    "end": "43739"
  },
  {
    "text": "Technique later by taking the slightly less useful axes away by or in this case",
    "start": "44289",
    "end": "48989"
  },
  {
    "text": "Dimensions or attributes of our data PCA is commonly pitched as a data reduction technique. Actually. It's a data transformation technique",
    "start": "49780",
    "end": "56340"
  },
  {
    "text": "It just makes our data and meaning able to production later",
    "start": "56340",
    "end": "59999"
  },
  {
    "text": "So let's imagine we have some attributes and we we know that some are correlated some are not correlated",
    "start": "60070",
    "end": "64889"
  },
  {
    "text": "The problem is that maybe we don't want to just delete some of the attributes may be",
    "start": "65530",
    "end": "68820"
  },
  {
    "text": "0.65 correlation is I mean",
    "start": "69460",
    "end": "71430"
  },
  {
    "text": "It's it's good",
    "start": "71430",
    "end": "71970"
  },
  {
    "text": "But it's not it doesn't mean we definitely want to delete attribute two and keep only attribute one on the other hand maybe",
    "start": "71970",
    "end": "78509"
  },
  {
    "text": "We do need to reduce some of the number of dimensions we've got or maybe we just want to try and make our data",
    "start": "79030",
    "end": "84150"
  },
  {
    "text": "More amenable to things like clustering. So let's look at a quick example",
    "start": "84280",
    "end": "87509"
  },
  {
    "text": "Typically PCA is done over many dimensions when we've got lots and lots of attributes",
    "start": "87580",
    "end": "92399"
  },
  {
    "text": "I'm just going to show two because obviously I starts to break down when I try and draw that many on the page",
    "start": "92400",
    "end": "96839"
  },
  {
    "text": "So if we have two attributes",
    "start": "97000",
    "end": "98590"
  },
  {
    "text": "And what we want to try and do is work out what the contribution of each of these is to our data set",
    "start": "98590",
    "end": "102810"
  },
  {
    "text": "Which of these is useful which of these is not useful and now obviously if we had many dimensions, you know",
    "start": "102810",
    "end": "107970"
  },
  {
    "text": "Seven hundred ten thousand we can still apply the same technique. So maybe we have some data that's like this",
    "start": "107970",
    "end": "113279"
  },
  {
    "text": "We have some datasets over here and perhaps we have a little gap and maybe some data over here and in general",
    "start": "113710",
    "end": "119489"
  },
  {
    "text": "Our data is kind of increasing like this. So this means that attribute one and attribute two are positively correlated to some extent",
    "start": "119950",
    "end": "127828"
  },
  {
    "text": "but maybe the correlation is not so strong that we just wanted elite attribute to what we want to try and do is",
    "start": "128560",
    "end": "134459"
  },
  {
    "text": "Transform our data into a way where these are more useful imagine that you've got some data but looks a bit like this",
    "start": "134800",
    "end": "140249"
  },
  {
    "text": "But if we rotate our data",
    "start": "140349",
    "end": "141840"
  },
  {
    "text": "We take a different view we can see there's actually two",
    "start": "141840",
    "end": "144090"
  },
  {
    "text": "objects and then we can separate them out and maybe if you were to take them again you could see there was four objects and",
    "start": "144310",
    "end": "149190"
  },
  {
    "text": "So on this is the idea what PCA is going to do is find new axes for this data",
    "start": "149190",
    "end": "153959"
  },
  {
    "text": "That separate it better for PCA to work. What we would start by doing is standardizing our data",
    "start": "154239",
    "end": "159299"
  },
  {
    "text": "So all of our dimensions attribute one attribute to all of the attributes are going to be centered around zero",
    "start": "159300",
    "end": "164369"
  },
  {
    "text": "And they're going to have a standard deviation of one",
    "start": "164500",
    "end": "166800"
  },
  {
    "text": "PCA will not work really at all. If you have widely different scales for your data",
    "start": "167410",
    "end": "172019"
  },
  {
    "text": "So what we want to try and do is find a direction or an axis through these two attributes",
    "start": "172180",
    "end": "176189"
  },
  {
    "text": "That separates out our data better than individual attributes",
    "start": "176560",
    "end": "179459"
  },
  {
    "text": "Do let's see how this data looks just from attribute one",
    "start": "179459",
    "end": "183089"
  },
  {
    "text": "If we trace down this way",
    "start": "183090",
    "end": "184260"
  },
  {
    "text": "you can see that it's sort of got this amount of spread in actually Bute one and they kind of",
    "start": "184260",
    "end": "188819"
  },
  {
    "text": "Dotted around like this and they sort of should go all the way along like this so you can't really see anything on here",
    "start": "189010",
    "end": "194879"
  },
  {
    "text": "Meaningful about these two groups, right?",
    "start": "195730",
    "end": "197759"
  },
  {
    "text": "And of course the more dimensions you have the more this could be a problem",
    "start": "197760",
    "end": "200549"
  },
  {
    "text": "Similarly about tribute to if we trace along here it goes from this range to this range",
    "start": "200799",
    "end": "205108"
  },
  {
    "text": "This is the variance of attribute - like the range and we can see that roughly speaking",
    "start": "205109",
    "end": "210058"
  },
  {
    "text": "The data is as spread out in attribute one as it is in attribute to that spread is about the same and both of them",
    "start": "210370",
    "end": "216149"
  },
  {
    "text": "are kind of useful for looking of a data but not really because again,",
    "start": "216150",
    "end": "219329"
  },
  {
    "text": "We have an equal distribution of point all the way along here. So that's not hugely useful",
    "start": "219329",
    "end": "223469"
  },
  {
    "text": "All right. So if we look at just attribute one, that's not hugely helpful",
    "start": "223600",
    "end": "226619"
  },
  {
    "text": "If we look at just attribute two, that's not hugely helpful either. So what can we do?",
    "start": "226620",
    "end": "231060"
  },
  {
    "text": "well",
    "start": "231070",
    "end": "231579"
  },
  {
    "text": "what we want to try and do is find a new axis like some new attribute that fits through this data like this and can",
    "start": "231579",
    "end": "238799"
  },
  {
    "text": "Really separate everything out because the spread of this data is actually diagonally in some sense not this way or this way",
    "start": "238799",
    "end": "245278"
  },
  {
    "text": "So what principal component analysis is going to do is find this principal component miss axis through our data like this",
    "start": "245410",
    "end": "252149"
  },
  {
    "text": "Such that when we look at the spread of a data, it's maximized, right?",
    "start": "253120",
    "end": "257789"
  },
  {
    "text": "So the data is as spread out as we can find it",
    "start": "257789",
    "end": "260069"
  },
  {
    "text": "And this is going to happen over any number of attributes",
    "start": "260590",
    "end": "262830"
  },
  {
    "text": "So actually one here",
    "start": "263340",
    "end": "265030"
  },
  {
    "text": "attribute to attribute three attribute for all the way to attribute n when we've got maybe 700 or 800 or",
    "start": "265030",
    "end": "271189"
  },
  {
    "text": "$1000 so at the moment which is fitting one principal component, this is one line through our two-dimensional data",
    "start": "272879",
    "end": "279379"
  },
  {
    "text": "There's going to be more principal components later, right?",
    "start": "279379",
    "end": "282169"
  },
  {
    "text": "But what we want to do is we want to pick the direction through this data",
    "start": "282169",
    "end": "285679"
  },
  {
    "text": "However, many attributes it has that has the most spread. So how do we measure this?",
    "start": "285680",
    "end": "290060"
  },
  {
    "text": "There's really two goals which were exactly the same one is to maximize the variance",
    "start": "290060",
    "end": "293629"
  },
  {
    "text": "So we find a direction for this line such that these points at the very edge are farthest apart",
    "start": "293629",
    "end": "298729"
  },
  {
    "text": "The other one is that we minimize the error",
    "start": "298740",
    "end": "300710"
  },
  {
    "text": "so we take this error from here this distance this distance from all these points to our new axes and we minimize it so",
    "start": "300710",
    "end": "308388"
  },
  {
    "text": "You can imagine if we do this for all our points we can get the sum of the squared",
    "start": "308729",
    "end": "313369"
  },
  {
    "text": "distances from these points to this line and then as we move this line around",
    "start": "313469",
    "end": "317449"
  },
  {
    "text": "Sometimes if it's going to be better",
    "start": "318270",
    "end": "319770"
  },
  {
    "text": "Sometimes it's not if we have a line that goes like this",
    "start": "319770",
    "end": "322189"
  },
  {
    "text": "Some of these lines are going to be very large like this and that's going to be a higher amount of error",
    "start": "322590",
    "end": "328340"
  },
  {
    "text": "So what we'll find is that if we do this our first principal component will sit through whichever direction in the data",
    "start": "328340",
    "end": "333859"
  },
  {
    "text": "minimizes these distances and by definition",
    "start": "334860",
    "end": "337879"
  },
  {
    "text": "Maximizes this spread which makes this axis super useful if we use this axis now as our new X and we rotate this whole page",
    "start": "338759",
    "end": "345739"
  },
  {
    "text": "All our data is lovely and separated. And actually we have two distinct clusters in this data set, right?",
    "start": "346020",
    "end": "352159"
  },
  {
    "text": "So that's what we're going to do",
    "start": "352259",
    "end": "353509"
  },
  {
    "text": "Now as I mentioned PCA doesn't typically reduce the number of attributes from two to one just like that",
    "start": "353509",
    "end": "359359"
  },
  {
    "text": "We're going to have another principal component which represents the second amount of most variance orthogonal e so at ninety degrees",
    "start": "359400",
    "end": "366379"
  },
  {
    "text": "So that's going to be this one here",
    "start": "366449",
    "end": "368959"
  },
  {
    "text": "We find the first principal component which maximizes variance and then we find the next one along that maximizes events in the next direction",
    "start": "368969",
    "end": "375919"
  },
  {
    "text": "Now if there were multiple dimensions we'd keep applying this process",
    "start": "376110",
    "end": "379638"
  },
  {
    "text": "We keep finding new axes for our data that systematically show more and more of a spread of our data",
    "start": "379639",
    "end": "385848"
  },
  {
    "text": "But we're crucially we're ordering this by the amount of variance that they represent",
    "start": "386400",
    "end": "391518"
  },
  {
    "text": "So this is PC one or principal component one. This is principal component two and",
    "start": "391520",
    "end": "396168"
  },
  {
    "text": "Principal component one is always going to have the most",
    "start": "397259",
    "end": "399799"
  },
  {
    "text": "Varied data in it principal component to the next most three the next most all the way to the end with the least",
    "start": "400409",
    "end": "406709"
  },
  {
    "text": "so a natural",
    "start": "406810",
    "end": "408370"
  },
  {
    "text": "Side-effect of this process is that we're going to have new axes through our data",
    "start": "408370",
    "end": "411629"
  },
  {
    "text": "Which and we're going to have a same number of axes as there are original dimensions in our data",
    "start": "412330",
    "end": "417270"
  },
  {
    "text": "But they're going to get less and less useful in terms of the variance of our data as we go forward",
    "start": "417460",
    "end": "422548"
  },
  {
    "text": "So PC one is going to be the most important",
    "start": "422550",
    "end": "424500"
  },
  {
    "text": "most of our data is spread out across",
    "start": "424500",
    "end": "426280"
  },
  {
    "text": "Pc-1 pc2 a little bit less spread out PC three a little bit less still all the way down to PC n all the way",
    "start": "426280",
    "end": "432000"
  },
  {
    "text": "Down here if you wanted to perform dimensionality reduction because you felt you had too many dimensions to your data",
    "start": "432000",
    "end": "438210"
  },
  {
    "text": "you could just for example keep the first 10 principal components project your data into that space and",
    "start": "438340",
    "end": "443849"
  },
  {
    "text": "Still retain most of the information",
    "start": "444550",
    "end": "446530"
  },
  {
    "text": "we won't go into the mathematics of how to calculate these principal components because you can find out very easily online and all has a",
    "start": "446530",
    "end": "453179"
  },
  {
    "text": "Lovely function to do it for us",
    "start": "453180",
    "end": "454350"
  },
  {
    "text": "I wanted to focus on intuitively what PCA does but how we will actually project these points onto these new",
    "start": "454350",
    "end": "459659"
  },
  {
    "text": "axes and rotate the whole thing is",
    "start": "459880",
    "end": "461880"
  },
  {
    "text": "Each of these principal components is going to be a weighted sum of all the attributes",
    "start": "462370",
    "end": "466350"
  },
  {
    "text": "So for example PC one is going to be some amount of attribute one",
    "start": "466420",
    "end": "469650"
  },
  {
    "text": "Added to some amount of attribute two now in this case because it sort of goes off at sort of 45 degrees",
    "start": "470170",
    "end": "474840"
  },
  {
    "text": "It's going to be about the same but you could imagine if your data was like this",
    "start": "474840",
    "end": "477780"
  },
  {
    "text": "It'll be mostly attribute one and a little bit of attribute two if it was like this",
    "start": "477910",
    "end": "481770"
  },
  {
    "text": "It'll be mostly attribute to a little bit attribute one",
    "start": "482080",
    "end": "485249"
  },
  {
    "text": "All right. Now, of course the n-dimensional data or we have many more dimensions that I can't draw on the page",
    "start": "485250",
    "end": "490050"
  },
  {
    "text": "The principle is exactly the same some amount of attribute one attribute to attribute three and so on all the way to the end",
    "start": "490120",
    "end": "496710"
  },
  {
    "text": "Right and that's going to project our points straight onto this line through that data",
    "start": "496810",
    "end": "501330"
  },
  {
    "text": "So when we talk about minimizing the error you can imagine",
    "start": "501490",
    "end": "504960"
  },
  {
    "text": "Rotating this about the center of these points here like this",
    "start": "505270",
    "end": "507900"
  },
  {
    "text": "And as you do this these red lines are going to change in length",
    "start": "507960",
    "end": "510569"
  },
  {
    "text": "And it's going to settle on the very center line where these weights are minimized",
    "start": "510570",
    "end": "514919"
  },
  {
    "text": "Right and as it happens that also maximizes the variance of these points here because of the fact that this mathematics is based around",
    "start": "514930",
    "end": "521580"
  },
  {
    "text": "eigenvectors and eigenvalues",
    "start": "522130",
    "end": "523840"
  },
  {
    "text": "Pc2 is always going to come out or foggin all or in this case at 90 degrees to pc one now",
    "start": "523840",
    "end": "528660"
  },
  {
    "text": "This is true of however many dimensions. You've got every single new axis that appears or new vector",
    "start": "528660",
    "end": "534389"
  },
  {
    "text": "A new principal component is going to come out or foggin all to the ones before",
    "start": "534930",
    "end": "538289"
  },
  {
    "text": "Until you run out of dimensions and you can't do it anymore",
    "start": "538540",
    "end": "540868"
  },
  {
    "text": "We've already reached the most we can fit in on this two-dimensional plane",
    "start": "541060",
    "end": "544290"
  },
  {
    "text": "We've got one here and we've got another one orthogonal to it",
    "start": "544290",
    "end": "546930"
  },
  {
    "text": "There is no other lines I can draw for that to be true",
    "start": "546930",
    "end": "549809"
  },
  {
    "text": "Right, but obviously if we had more attributes, that would be the case",
    "start": "549850",
    "end": "552930"
  },
  {
    "text": "so the reason that it's so important to scare your data appropriately is that you're trying to find the direction for your data that",
    "start": "552970",
    "end": "559319"
  },
  {
    "text": "Maximizes the variance now, if one of your dimensions is much much bigger than the other of course",
    "start": "559510",
    "end": "563670"
  },
  {
    "text": "That one is the one that's going to maximize the variance",
    "start": "563670",
    "end": "565800"
  },
  {
    "text": "if you've got salary that's between naught and",
    "start": "565800",
    "end": "568289"
  },
  {
    "text": "10,000 and all your others are between naught and 1",
    "start": "568570",
    "end": "570749"
  },
  {
    "text": "Then your first principal component is going to be predominately salary because that's the most important thing as far as it know",
    "start": "570850",
    "end": "575969"
  },
  {
    "text": "If as it knows this is why it's so important to standardize your data first",
    "start": "575970",
    "end": "579688"
  },
  {
    "text": "We're going to continue to use our music data set for this video now for those of you",
    "start": "579880",
    "end": "583860"
  },
  {
    "text": "Forgotten this data set is a set of music files that are freely available online",
    "start": "584440",
    "end": "589200"
  },
  {
    "text": "Where we've got the metadata of a genres or titles for different tracks and then for those tracks",
    "start": "589360",
    "end": "594329"
  },
  {
    "text": "We've also calculated some features about the actual audio for example",
    "start": "594330",
    "end": "598259"
  },
  {
    "text": "Temporal features how loud they are?",
    "start": "599020",
    "end": "601020"
  },
  {
    "text": "How fast the music is how upbeat it is whether you could dance to it this kind of thing",
    "start": "601180",
    "end": "605998"
  },
  {
    "text": "Apparently dance ability is a measurable trait",
    "start": "606030",
    "end": "608030"
  },
  {
    "text": "Apparently these teachers have been generated by two different libraries once called Lib rosa",
    "start": "608290",
    "end": "611790"
  },
  {
    "text": "Which is freely available online and the other ways echo nests",
    "start": "611790",
    "end": "614639"
  },
  {
    "text": "Which are the features that a core of Spotify and how it does its music recommender system and its playlists",
    "start": "614640",
    "end": "620189"
  },
  {
    "text": "So let's load the data set",
    "start": "620190",
    "end": "622190"
  },
  {
    "text": "So I'm going to read it. It takes quite a long time to load",
    "start": "622510",
    "end": "624749"
  },
  {
    "text": "It'll probably be faster if it wasn't in a CSV, you've got to remember if your files are in CSV",
    "start": "631260",
    "end": "635330"
  },
  {
    "text": "You've got to actually pass the more than workout, whether they're numerical or text, you know for every cell. Okay, so we've got",
    "start": "635330",
    "end": "641059"
  },
  {
    "text": "13,000 instances or rows in our data and we've got",
    "start": "641730",
    "end": "644808"
  },
  {
    "text": "751 attributes or dimensions to our data? So these are going to include features from both liberals",
    "start": "645480",
    "end": "650748"
  },
  {
    "text": "ER and echo nest and the other metadata of these tracks",
    "start": "650750",
    "end": "654320"
  },
  {
    "text": "So we're going to select just the echo nest features for this part",
    "start": "654330",
    "end": "657470"
  },
  {
    "text": "Just be it's a little bit easier to have fewer dimensions to look at this would work just as well",
    "start": "657660",
    "end": "661519"
  },
  {
    "text": "On all the other features as long as they're numeric",
    "start": "661700",
    "end": "663710"
  },
  {
    "text": "So we're gonna select echo nest is equal to the music data frame all of the rows and just the echo nests columns",
    "start": "664020",
    "end": "671449"
  },
  {
    "text": "Which are 528 to the end and then we're going to standardize all this data now",
    "start": "671450",
    "end": "675350"
  },
  {
    "text": "So we're going to Center it around 0 a mean of 0 and a standard deviation of 1 using the scale function",
    "start": "675350",
    "end": "681019"
  },
  {
    "text": "now take a minute to finish and then we can just check to make sure that our",
    "start": "681360",
    "end": "685100"
  },
  {
    "text": "Variance and our mean or what we expected. So we're going to apply over dimension two",
    "start": "685500",
    "end": "690260"
  },
  {
    "text": "so that's over all the columns the",
    "start": "690260",
    "end": "692390"
  },
  {
    "text": "Variance function and find out what the variances are and you can see they're all one, which is exactly what we want",
    "start": "692790",
    "end": "697819"
  },
  {
    "text": "So let's have a look at the mean the mean should be centered about 0",
    "start": "697820",
    "end": "700790"
  },
  {
    "text": "It won't be exactly 0 just because of you know, floating point errors and so on. So there we go. So 1.5",
    "start": "700790",
    "end": "706579"
  },
  {
    "text": "10 to the minus 17 very very small right close enough to 0 perfectly fine",
    "start": "707130",
    "end": "711439"
  },
  {
    "text": "So the function we're going to use is the PPR comp function in R",
    "start": "711440",
    "end": "715700"
  },
  {
    "text": "This is going to perform principal component analysis for those of you who are interested in learning more",
    "start": "715700",
    "end": "719749"
  },
  {
    "text": "What it's going to do is create a covariance matrix, and then it's going to use singular value decomposition",
    "start": "719750",
    "end": "723980"
  },
  {
    "text": "To find the eigenvectors and the eigenvalues and those are the things that actually we want from a PCA",
    "start": "724590",
    "end": "730550"
  },
  {
    "text": "So we're gonna run that now it doesn't take too long. But this is still quite a live data set",
    "start": "730620",
    "end": "734900"
  },
  {
    "text": "This will slow down quite a lot if you had a very very large data set, but it still might be worthwhile",
    "start": "734900",
    "end": "739429"
  },
  {
    "text": "What it's done is it's found the directions through our data that maximize the variance and it's projected our data into that",
    "start": "740010",
    "end": "746359"
  },
  {
    "text": "Space or transformed our data into that space at the moment. The dimensionality of our data is exactly the same and completely unchanged. No",
    "start": "746790",
    "end": "753949"
  },
  {
    "text": "Dimensionality reduction has happened yet. So let's perform a quick summary",
    "start": "754470",
    "end": "757519"
  },
  {
    "text": "There'll be a lot of the stuff on the screen, but I'll point towards what's important",
    "start": "757680",
    "end": "761060"
  },
  {
    "text": "So what it's doing is it's showing us the list of all the compiled",
    "start": "761060",
    "end": "764210"
  },
  {
    "text": "They're standard deviations service spread in that direction",
    "start": "764750",
    "end": "766880"
  },
  {
    "text": "and",
    "start": "767010",
    "end": "767640"
  },
  {
    "text": "also how much of the variance it accounts for",
    "start": "767640",
    "end": "770480"
  },
  {
    "text": "You can imagine that",
    "start": "770550",
    "end": "771649"
  },
  {
    "text": "Let's imagine a spread of your data and all the different dimensions is this much but in one direction, it's just this much",
    "start": "771650",
    "end": "776869"
  },
  {
    "text": "What was the percentage of the spread or the variance that that principle component accounts for right?",
    "start": "777000",
    "end": "783920"
  },
  {
    "text": "This is very easily quantified so you can see in here",
    "start": "783920",
    "end": "786289"
  },
  {
    "text": "We've got the proportion of variance for PC one is naught point one one six nine",
    "start": "786290",
    "end": "790099"
  },
  {
    "text": "which is about eleven point six percent so out of all the",
    "start": "790100",
    "end": "793459"
  },
  {
    "text": "224 echo nest features",
    "start": "794370",
    "end": "796370"
  },
  {
    "text": "This weighted sum in principal component one or vistit direction through our data",
    "start": "796980",
    "end": "801498"
  },
  {
    "text": "Represents eleven percent of us bread, which it's not too bad. Actually, I think that's pretty good",
    "start": "802050",
    "end": "806239"
  },
  {
    "text": "Why principal component two that's another eight percent",
    "start": "806390",
    "end": "809119"
  },
  {
    "text": "So the cumulative proportion of these two principal components is going to be twenty percent and at printed component three twenty-five percent and so on",
    "start": "809120",
    "end": "815839"
  },
  {
    "text": "So what we're saying is by the time we get to principal component three if we represent our data",
    "start": "816060",
    "end": "820279"
  },
  {
    "text": "Is this three dimensional space around these axes pc-1 pc2 PC three?",
    "start": "820280",
    "end": "825259"
  },
  {
    "text": "We're getting 25 percent of a spread of the data",
    "start": "825420",
    "end": "828050"
  },
  {
    "text": "That we had before but that's three dimensions instead of two hundred and twenty four dimensions. So that's not too bad",
    "start": "828290",
    "end": "833238"
  },
  {
    "text": "Now one important thing to look out for is where our spread starts to get towards a hundred percent",
    "start": "833910",
    "end": "838128"
  },
  {
    "text": "Where is it in our data set that we can say, you know?",
    "start": "838590",
    "end": "841100"
  },
  {
    "text": "what these later dimensions these later principal components are not really adding anything to our",
    "start": "841100",
    "end": "845899"
  },
  {
    "text": "Data set so we scroll down and we'll find here at 95 percent",
    "start": "846150",
    "end": "849650"
  },
  {
    "text": "scroll down a bit further 98 percent 98 percent and",
    "start": "850290",
    "end": "853849"
  },
  {
    "text": "Here we go. Principal component",
    "start": "854610",
    "end": "856910"
  },
  {
    "text": "133 the cumulative proportion of variance explained by all of those ones from 1 all the way to",
    "start": "858000",
    "end": "863840"
  },
  {
    "text": "133 is 99 percent if you're going to perform dimensionality reduction",
    "start": "864150",
    "end": "867559"
  },
  {
    "text": "Stopping at 99 percent of the variance is very common",
    "start": "868200",
    "end": "870859"
  },
  {
    "text": "What we're saying is we can delete any of our data from principal component",
    "start": "870900",
    "end": "875150"
  },
  {
    "text": "134 all the way to the end and we're still getting 99 percent of a spread or",
    "start": "875520",
    "end": "880160"
  },
  {
    "text": "Information from our dataset if you want to use PCA for data reduction",
    "start": "880380",
    "end": "883700"
  },
  {
    "text": "Then what you're going to have to do is decide what your cutoff is going to be now 99 percent is a good",
    "start": "883700",
    "end": "888140"
  },
  {
    "text": "Number to use what does that actually mean?",
    "start": "888450",
    "end": "890720"
  },
  {
    "text": "What it means is if we plotted the different principal components going this way and the amount of barrier. It's",
    "start": "890880",
    "end": "898160"
  },
  {
    "text": "That they explain like the amount of a spread of a data that they're responsible for they're going to decrease like this",
    "start": "898950",
    "end": "904400"
  },
  {
    "text": "I mean this is going to be a bar chart actually",
    "start": "904400",
    "end": "906050"
  },
  {
    "text": "Right so like this so principal component one is always going to be the most variance explained because that's how the mathematics works",
    "start": "906050",
    "end": "911029"
  },
  {
    "text": "These are ordered in that way principal component 2 is less three is less four is less and so on",
    "start": "911030",
    "end": "915889"
  },
  {
    "text": "we're going to keep going down until",
    "start": "915990",
    "end": "917990"
  },
  {
    "text": "99% of the variance has been explained in some band and we can remove everything else. That's what we're going to do",
    "start": "918030",
    "end": "923240"
  },
  {
    "text": "So 99% is one option ninety-five percent something like this",
    "start": "923240",
    "end": "926359"
  },
  {
    "text": "Any number of principal components that you remove is going to delete some of your data equivalent to the moving dimensions",
    "start": "926550",
    "end": "932779"
  },
  {
    "text": "But because they're ordered in this way from the most useful to the least useful",
    "start": "932970",
    "end": "937100"
  },
  {
    "text": "It just makes that job a little bit easier instead of saying it was tempo or feature five that we didn't want",
    "start": "937140",
    "end": "942740"
  },
  {
    "text": "actually",
    "start": "943110",
    "end": "943610"
  },
  {
    "text": "We're saying in this axis principal component one and principal component to its",
    "start": "943610",
    "end": "948169"
  },
  {
    "text": "Principal component one hundred and thirty-four that it's not that useful to us",
    "start": "948270",
    "end": "951170"
  },
  {
    "text": "Let's have a look at one of our principal components and see what it is. So we're going to type PCA",
    "start": "951360",
    "end": "955610"
  },
  {
    "text": "Followed rotation and we're going to select just the first one because otherwise it's going to be too much information",
    "start": "956250",
    "end": "961039"
  },
  {
    "text": "so this is going to be how much of each of our",
    "start": "961040",
    "end": "963290"
  },
  {
    "text": "224 dimensions does pc1 need to create this weighted sum and project our data so you can see for example",
    "start": "963930",
    "end": "970310"
  },
  {
    "text": "It's - naught point naught one of tempo or two - two - naught point naught two of tempo or two - three",
    "start": "970310",
    "end": "976579"
  },
  {
    "text": "one thing to remember about these is these are now",
    "start": "976740",
    "end": "978889"
  },
  {
    "text": "Arbitrary axes through some massively dimensional space very difficult to know exactly what this means, right?",
    "start": "979620",
    "end": "985039"
  },
  {
    "text": "You can start to look into based on these weights, which of these features is more useful, but that's kind of a second",
    "start": "985040",
    "end": "991219"
  },
  {
    "text": "second step you can use so for example tempo or feature naught is no point naught - so we're going to take",
    "start": "992160",
    "end": "997579"
  },
  {
    "text": "Naught point naught to eight times tempo or not",
    "start": "997740",
    "end": "1000250"
  },
  {
    "text": "Whatever that value is times like this much of the next one times by this much of the next one",
    "start": "1000250",
    "end": "1004450"
  },
  {
    "text": "I'm gonna add them all up and that is a projection of our data point into this new space",
    "start": "1004450",
    "end": "1008799"
  },
  {
    "text": "So we can do this for our entire data set as it happens",
    "start": "1009020",
    "end": "1012129"
  },
  {
    "text": "Our calculates is forced. But you could calculate this using a matrix multiplication if you wanted so these are all our points",
    "start": "1012470",
    "end": "1017920"
  },
  {
    "text": "Transformed into this new space. So hopefully we can see them better",
    "start": "1018470",
    "end": "1021430"
  },
  {
    "text": "Then we're going to start plotting",
    "start": "1021650",
    "end": "1023650"
  },
  {
    "text": "Different genres of music in these principal components to see if if the separation is any better than it was before",
    "start": "1024020",
    "end": "1029170"
  },
  {
    "text": "So let's have a quick look. So this is a scatter plot a principal component 1 vs",
    "start": "1029170",
    "end": "1034369"
  },
  {
    "text": "principal component 2 and every single song in our data set and you can see it's a bit of a higgledy-piggledy mess and it would",
    "start": "1034370",
    "end": "1039859"
  },
  {
    "text": "Be because there's some 13,000 songs here",
    "start": "1039860",
    "end": "1042229"
  },
  {
    "text": "But you can see that maybe some of these songs are bigger over here and some of these over here",
    "start": "1042230",
    "end": "1046668"
  },
  {
    "text": "Maybe let's just look at a few genres to sort of narrow it down and make our figure a little bit clearer",
    "start": "1046740",
    "end": "1051740"
  },
  {
    "text": "So I'm going to select just a rock electronic and classical genres. I don't know they seem like they'd be slightly different",
    "start": "1051740",
    "end": "1057769"
  },
  {
    "text": "So let's run that so we're going to take just those genres",
    "start": "1057929",
    "end": "1061278"
  },
  {
    "text": "We're gonna plot them in the same scatter plot and see where they are in this space",
    "start": "1061380",
    "end": "1065179"
  },
  {
    "text": "So how did this data get into this form? What happened was for every individual track",
    "start": "1065179",
    "end": "1069709"
  },
  {
    "text": "We had a number of features in our 224 dimensional space each of these",
    "start": "1069710",
    "end": "1074268"
  },
  {
    "text": "Principal components is a weighted sum. So for example for let's say track 516 we'd have taken tempo feature 1",
    "start": "1074970",
    "end": "1081709"
  },
  {
    "text": "multiplied it by part of principal component 1 the loadings",
    "start": "1082409",
    "end": "1085278"
  },
  {
    "text": "Added that to the next bit to the next bit to the next bit and worked out where it sits in terms of principal component",
    "start": "1085529",
    "end": "1090679"
  },
  {
    "text": "1 this new axis, we'd have done the same for principal component 2 and that puts them down here",
    "start": "1090679",
    "end": "1095658"
  },
  {
    "text": "Now there's quite a lot of overlap",
    "start": "1096299",
    "end": "1097549"
  },
  {
    "text": "But you can start to see we're teasing apart the electronic music from the rock music the rock music's sitting over here on the right",
    "start": "1097549",
    "end": "1103368"
  },
  {
    "text": "The electronic music sitting on the lower left and the classical music has up the top here",
    "start": "1103889",
    "end": "1107869"
  },
  {
    "text": "Now these axes don't mean you know",
    "start": "1107870",
    "end": "1110750"
  },
  {
    "text": "That musics faster or slower or more or less upbeat because without looking into the waiting's and below dings for these",
    "start": "1110750",
    "end": "1117260"
  },
  {
    "text": "Principal components. It's impossible to say for sure",
    "start": "1117779",
    "end": "1120018"
  },
  {
    "text": "but what we can say is that they're starting to come apart and there are some differences in our data set the fact that they",
    "start": "1120149",
    "end": "1125569"
  },
  {
    "text": "Still overlap means that probably two dimensions is not enough to satisfactorily",
    "start": "1125570",
    "end": "1130518"
  },
  {
    "text": "Separate out all these things if you wanted to pass this projected and transform data into a machine learning algorithm",
    "start": "1131010",
    "end": "1136939"
  },
  {
    "text": "You'd probably need to pass in more than two dimensions",
    "start": "1136950",
    "end": "1139158"
  },
  {
    "text": "And in this case given that 90% or 99% of the variance is explained after principal compare",
    "start": "1139260",
    "end": "1145999"
  },
  {
    "text": "133 those",
    "start": "1146460",
    "end": "1148350"
  },
  {
    "text": "133 dimensions are probably what you'd use",
    "start": "1148350",
    "end": "1150028"
  },
  {
    "text": "You can actually use the entire output a PCA the same number of dimensions you have before to just show a better",
    "start": "1150029",
    "end": "1155569"
  },
  {
    "text": "Rotated version of your data to a machine learning algorithm",
    "start": "1156000",
    "end": "1158959"
  },
  {
    "text": "You don't have to remove any dimensions if you don't want to but because the dimensions are ordered",
    "start": "1158960",
    "end": "1164120"
  },
  {
    "text": "In from most variants to least you can kind of get a good gauge for where you should cut off",
    "start": "1164210",
    "end": "1168829"
  },
  {
    "text": "And remove data that way this kind of data reduction along with the ones we looked at before are going to form part of this",
    "start": "1169139",
    "end": "1175189"
  },
  {
    "text": "data",
    "start": "1175350",
    "end": "1175980"
  },
  {
    "text": "cleaning data",
    "start": "1175980",
    "end": "1177360"
  },
  {
    "text": "transformation and data reduction approach that we're going to iterate through until our data is as small as we can get it and it's it's",
    "start": "1177360",
    "end": "1183289"
  },
  {
    "text": "We can extract as much knowledge as possible in the easiest way",
    "start": "1183330",
    "end": "1186378"
  },
  {
    "text": "once we're done with this our date will be ready for clustering for machine learning for",
    "start": "1186419",
    "end": "1190819"
  },
  {
    "text": "Classification for aggression for anything else that we want to do?",
    "start": "1190980",
    "end": "1194240"
  },
  {
    "text": "Today we're going to talk about clustering",
    "start": "1194970",
    "end": "1196499"
  },
  {
    "text": "Do you ever find when you're on YouTube you'll watch a video on something and then suddenly you're being recommended a load of other videos",
    "start": "1196499",
    "end": "1202789"
  },
  {
    "text": "That you hadn't even heard of that are actually kind of similar. This happens to me. I watch some video",
    "start": "1203129",
    "end": "1208279"
  }
]