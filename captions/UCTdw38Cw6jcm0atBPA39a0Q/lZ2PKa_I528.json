[
  {
    "text": "hey good afternoon everybody we're gonna talk about integrating Hadoop with sequel server so my name is Kevin feasel",
    "start": "9599",
    "end": "16830"
  },
  {
    "text": "I am a manager of a predictive analytics team in Durham North Carolina also a Data Platform MVP and I've got a blog",
    "start": "16830",
    "end": "24060"
  },
  {
    "text": "curated sequel curated sequel is all about trying to find interesting blog posts five to ten interesting blog posts",
    "start": "24060",
    "end": "31230"
  },
  {
    "text": "per day in the space of database development database administration Hadoop power bi our Python this security",
    "start": "31230",
    "end": "38790"
  },
  {
    "text": "this ever broadening ever-expanding data platform space so it's curated SQL comm",
    "start": "38790",
    "end": "45350"
  },
  {
    "text": "I'm going to assume for the purposes of this talk that you have a Hadoop cluster if you don't that's no problem I'm just",
    "start": "45350",
    "end": "52920"
  },
  {
    "text": "not gonna show you how to set it up I'm also going to assume that you're more familiar with T sequel or with c-sharp",
    "start": "52920",
    "end": "60329"
  },
  {
    "text": "or some dotnet language than you are with the natural languages of Hadoop like Java Scala and even Python and",
    "start": "60329",
    "end": "69020"
  },
  {
    "text": "finally I'll assume that you want to work with Hadoop so those are the three basic of some assumptions I'm making",
    "start": "69020",
    "end": "75570"
  },
  {
    "text": "they're not as strict as you may think but yeah that's my audience",
    "start": "75570",
    "end": "83390"
  },
  {
    "text": "the reason integrate comes from a book that's 241 years old now it's basically",
    "start": "83390",
    "end": "90659"
  },
  {
    "text": "this is my my sneaky way of trying to fit economics into this discussion if I want to talk about it from a product",
    "start": "90659",
    "end": "97979"
  },
  {
    "text": "standpoint think of it this way if you have a small company then you can",
    "start": "97979",
    "end": "104009"
  },
  {
    "text": "probably fit all of your data in one relational database and it works then",
    "start": "104009",
    "end": "109290"
  },
  {
    "text": "eventually the company grows you have additional things you have additional resources some of those things don't",
    "start": "109290",
    "end": "115950"
  },
  {
    "text": "necessarily fit nicely into the mantra of what a relational database is and so",
    "start": "115950",
    "end": "121350"
  },
  {
    "text": "you expand out you have other teams working on other things maybe you eventually hit a point where",
    "start": "121350",
    "end": "126960"
  },
  {
    "text": "you need a Hadoop cluster and you go out and you put one together but there's",
    "start": "126960",
    "end": "132690"
  },
  {
    "text": "still a reason to tie the data back together the specialization and division of labor both so we specialize by",
    "start": "132690",
    "end": "139680"
  },
  {
    "text": "putting certain data in sequel server versus in Hadoop that stuff that fits really well in",
    "start": "139680",
    "end": "145410"
  },
  {
    "text": "sequel server is the stuff that if it's wrong you're fired we can take advantage of properties like",
    "start": "145410",
    "end": "152450"
  },
  {
    "text": "consistency that don't necessarily exist in a lot of other data platforms we also",
    "start": "152450",
    "end": "158970"
  },
  {
    "text": "have decades of experience we have a lot of people who know how to use this model we have ways of getting that data into",
    "start": "158970",
    "end": "166200"
  },
  {
    "text": "applications very quickly we have a lot of people who can tune those queries so there are some good reasons to put",
    "start": "166200",
    "end": "173160"
  },
  {
    "text": "things into a relational database by contrast there are also really good reasons to put things into a Hadoop",
    "start": "173160",
    "end": "179819"
  },
  {
    "text": "cluster if I Magra gating a huge amount of data that's kind of the classic",
    "start": "179819",
    "end": "185610"
  },
  {
    "text": "reason for Hadoop the newer reason for Hadoop is almost the opposite I'm",
    "start": "185610",
    "end": "192330"
  },
  {
    "text": "streaming a lot of tiny data so I have millions of messages that I'm sending",
    "start": "192330",
    "end": "197730"
  },
  {
    "text": "out each message is is itself very small but it's just a sheer throughput of messages that drives a larger solution",
    "start": "197730",
    "end": "205850"
  },
  {
    "text": "then when it comes to things like warehousing on my sequel server side the",
    "start": "205850",
    "end": "211700"
  },
  {
    "text": "warehousing solutions tend to be tell me what I need to know so in other words I",
    "start": "211700",
    "end": "217590"
  },
  {
    "text": "know that I have to fill out these reports and give them to the sec every quarter I need to fill out these reports",
    "start": "217590",
    "end": "224489"
  },
  {
    "text": "and give them to different regulators I know the CEO needs to see these reports the board needs to see these reports my",
    "start": "224489",
    "end": "231180"
  },
  {
    "text": "manager needs to see these reports so we build the warehouse with those reports in mind we build it with an end goal in",
    "start": "231180",
    "end": "237780"
  },
  {
    "text": "mind we are answering known questions the Hadoop cluster is about spelunking it's about trying to find the unknown so",
    "start": "237780",
    "end": "245850"
  },
  {
    "text": "we're digging into a data set that we may not necessarily know what's in there",
    "start": "245850",
    "end": "251549"
  },
  {
    "text": "we just expect that there is an answer there so it's two different use cases",
    "start": "251549",
    "end": "257910"
  },
  {
    "text": "and yet we're going to want to tie these together we're going to want to take the data that lives over here take the data",
    "start": "257910",
    "end": "264690"
  },
  {
    "text": "that lives over here integrate that data together because from both of these we",
    "start": "264690",
    "end": "270450"
  },
  {
    "text": "can get the full answer so I'm going to show really five methods",
    "start": "270450",
    "end": "276820"
  },
  {
    "text": "of tying together Hadoop and sequel server scoop is a method that I will briefly discuss but again I'm assuming",
    "start": "276820",
    "end": "283360"
  },
  {
    "text": "that we're more interested in the dotnet side of things so we'll start with",
    "start": "283360",
    "end": "288520"
  },
  {
    "text": "Ambari and barri is just a web framework for managing a Hadoop cluster this comes",
    "start": "288520",
    "end": "297190"
  },
  {
    "text": "by default with the Hortonworks distribution and I should probably note that I am using the Hortonworks distribution I have my portable Hadoop",
    "start": "297190",
    "end": "304720"
  },
  {
    "text": "cluster right over here and I have my network router that will inevitably let",
    "start": "304720",
    "end": "309940"
  },
  {
    "text": "me down right here and I'm going to be working with those today so I have",
    "start": "309940",
    "end": "316240"
  },
  {
    "text": "Hortonworks Hadoop installed on that thing a couple other important distributions there's cloud era which",
    "start": "316240",
    "end": "322330"
  },
  {
    "text": "was the original distribution and there's map bar which is the third distribution they have their own",
    "start": "322330",
    "end": "330580"
  },
  {
    "text": "management interfaces if you happen to have a cloud error cluster use its",
    "start": "330580",
    "end": "336310"
  },
  {
    "text": "management interface it's basically the same there will be little differences here and there but you know nothing",
    "start": "336310",
    "end": "341979"
  },
  {
    "text": "nothing too big to get worked up about this is perfect if you have one file one",
    "start": "341979",
    "end": "348640"
  },
  {
    "text": "time that you need to move over somewhere or I have to pull a file once a month and I'm willing to do that",
    "start": "348640",
    "end": "354490"
  },
  {
    "text": "instead of automating it so let's take a quick look at that I have an bar set up",
    "start": "354490",
    "end": "363760"
  },
  {
    "text": "right here so this is my Hadoop cluster and it's looking on port 8000 so that's",
    "start": "363760",
    "end": "371110"
  },
  {
    "text": "my embargo and barri gives you a quick overview of the health of the",
    "start": "371110",
    "end": "377020"
  },
  {
    "text": "environment you can see I've got a bunch of stuff that's currently down that's because I rebooted the machine before I",
    "start": "377020",
    "end": "382690"
  },
  {
    "text": "came down here over here I have a menu and inside that menu I get to see things",
    "start": "382690",
    "end": "391120"
  },
  {
    "text": "like the files view so this is where I'm going to be able to look at HDFS the",
    "start": "391120",
    "end": "397120"
  },
  {
    "text": "Hadoop distributed file system if I select files view it will take a moment",
    "start": "397120",
    "end": "403500"
  },
  {
    "text": "because the cluster just started acting up and responding really slowly",
    "start": "403500",
    "end": "409090"
  },
  {
    "text": "to all of my requests so it knows that it's on right now it knows that you're",
    "start": "409090",
    "end": "414310"
  },
  {
    "text": "looking at it it's very it's very shy inside here I have a temp folder inside",
    "start": "414310",
    "end": "422169"
  },
  {
    "text": "this temp folder I've got other stuff I can create a new folder if I want and I",
    "start": "422169",
    "end": "429789"
  },
  {
    "text": "have now an indc folder notice that it has unix-style permissions owner group and the the CH Ahmad 777 set up",
    "start": "429789",
    "end": "439779"
  },
  {
    "text": "speaking of Tremonti seven of course we're gonna give everybody read write and execute on it because who cares",
    "start": "439779",
    "end": "447789"
  },
  {
    "text": "about security right so now anybody can go in here and modify this file I can",
    "start": "447789",
    "end": "454840"
  },
  {
    "text": "hit refresh and I get to see yeah everybody's got full access to this directory now everybody can do stuff in here eventually we sober up and realize",
    "start": "454840",
    "end": "462610"
  },
  {
    "text": "that was a bad idea so I'm just gonna delete it so it's gone let's go into here OTP inside here I've got some files",
    "start": "462610",
    "end": "470560"
  },
  {
    "text": "already I'm actually going to delete this one because we're gonna recreate it later there we go and I'm going to",
    "start": "470560",
    "end": "478120"
  },
  {
    "text": "upload some batting ratings so all of my data today is baseball and I have a",
    "start": "478120",
    "end": "486550"
  },
  {
    "text": "batting ratings file it's a very small data set but it serves the job so we're",
    "start": "486550",
    "end": "493089"
  },
  {
    "text": "going to upload this data set as it slowly uploads eventually we're going to",
    "start": "493089",
    "end": "499779"
  },
  {
    "text": "see it there we go now we have the data",
    "start": "499779",
    "end": "506259"
  },
  {
    "text": "set I can hit open and that will give me a brief view of the data its comma",
    "start": "506259",
    "end": "513490"
  },
  {
    "text": "delimited data where the first row is a header and then we have some basic",
    "start": "513490",
    "end": "519399"
  },
  {
    "text": "information about a bunch of baseball players so now that I have that data I",
    "start": "519399",
    "end": "527079"
  },
  {
    "text": "can go in to hive hive is sequel on Hadoop inside this hive you I have it",
    "start": "527079",
    "end": "534520"
  },
  {
    "text": "open over here on this other tab I have a database",
    "start": "534520",
    "end": "540430"
  },
  {
    "text": "hive is a set of databases and they don't have schemas as such it's",
    "start": "540430",
    "end": "546920"
  },
  {
    "text": "basically database table I can create a new database but I'm gonna use default",
    "start": "546920",
    "end": "551930"
  },
  {
    "text": "because I'm lazy let's take a script this script here will create a batting",
    "start": "551930",
    "end": "559610"
  },
  {
    "text": "ratings table so we are dropping a table if it already exists and if it doesn't",
    "start": "559610",
    "end": "565280"
  },
  {
    "text": "exist we're gonna create it with one column it's called placeholder it's a string we're gonna load the data from",
    "start": "565280",
    "end": "571580"
  },
  {
    "text": "batting rating CSV into the table batting ratings so let's execute this",
    "start": "571580",
    "end": "577340"
  },
  {
    "text": "and I'm gonna cross my fingers that it will actually work here because hive was",
    "start": "577340",
    "end": "582860"
  },
  {
    "text": "giving me problems just a moment ago so this there all right it worked I go over",
    "start": "582860",
    "end": "590810"
  },
  {
    "text": "to default and you know we can see down here that the query did in fact succeed life is good",
    "start": "590810",
    "end": "596480"
  },
  {
    "text": "I can go to batting ratings and hit the menu option right here and that's going",
    "start": "596480",
    "end": "602960"
  },
  {
    "text": "to select the top hundred rows from the batting ratings table I can click on the",
    "start": "602960",
    "end": "608360"
  },
  {
    "text": "batting ratings table itself and it will show me a list of the columns but I'll",
    "start": "608360",
    "end": "616100"
  },
  {
    "text": "be able to see them soon enough over here batting ratings dot placeholder and",
    "start": "616100",
    "end": "622610"
  },
  {
    "text": "in fact these are the individual rows so I have two problems here problem the",
    "start": "622610",
    "end": "629810"
  },
  {
    "text": "first my headers are in the data problem the second my data is not really",
    "start": "629810",
    "end": "639080"
  },
  {
    "text": "separated nicely for Hadoop that's okay this is semi structured I'm saying dump",
    "start": "639080",
    "end": "645920"
  },
  {
    "text": "the data however and I'll define that data when it comes time to query it so I",
    "start": "645920",
    "end": "651050"
  },
  {
    "text": "can write functions that will explode out this text it'll split it on comas",
    "start": "651050",
    "end": "656420"
  },
  {
    "text": "like maybe I want to get all of the first baseman who are younger than 25 years old so I can go find what the",
    "start": "656420",
    "end": "664570"
  },
  {
    "text": "position is for first base I can find the position for age I can break out",
    "start": "664570",
    "end": "669800"
  },
  {
    "text": "that array and do a an if statement and check that out or I could do it the right way and the",
    "start": "669800",
    "end": "677790"
  },
  {
    "text": "right way is by creating a table where we've actually defined the data types so",
    "start": "677790",
    "end": "685440"
  },
  {
    "text": "here I've already created this table I don't need to run it again but I've defined all of the individual data types",
    "start": "685440",
    "end": "691170"
  },
  {
    "text": "in my data set this came from a structured data source somewhere probably and so I can take advantage of",
    "start": "691170",
    "end": "698700"
  },
  {
    "text": "that structure when I load it into hive if it came from a strictly unstructured data source I'm not able to do this I",
    "start": "698700",
    "end": "705630"
  },
  {
    "text": "could create different views of the data using hive and like look for things",
    "start": "705630",
    "end": "713130"
  },
  {
    "text": "where the structure looks like this or I may end up having to create functions and explode out the data set and do the",
    "start": "713130",
    "end": "720900"
  },
  {
    "text": "nasty work that I'm trying to avoid anyhow I create the table here it's",
    "start": "720900",
    "end": "727620"
  },
  {
    "text": "fields terminated by comma lines separated by a new line stored as text input and it has a particular output",
    "start": "727620",
    "end": "735960"
  },
  {
    "text": "format but what's important is skip header line count equals 1 so that first",
    "start": "735960",
    "end": "742200"
  },
  {
    "text": "row is a header throw it away and I can",
    "start": "742200",
    "end": "747390"
  },
  {
    "text": "see the end result of that up here in batting rating is perm so I run this it's going to pull back a hundred",
    "start": "747390",
    "end": "754260"
  },
  {
    "text": "records and those records will have appropriate names so now I can write a",
    "start": "754260",
    "end": "759570"
  },
  {
    "text": "simple select query that says select star from batting ratings perm where age",
    "start": "759570",
    "end": "764910"
  },
  {
    "text": "less than 25 and position equals first base and I get all of those players",
    "start": "764910",
    "end": "771680"
  },
  {
    "text": "another example of what I might be able to do is to write out some right",
    "start": "772550",
    "end": "778470"
  },
  {
    "text": "fielders so run this and say 1000 TP",
    "start": "778470",
    "end": "785910"
  },
  {
    "text": "output I'd better make sure that directory exists I'm going to terminate fields with a comma and I say give me",
    "start": "785910",
    "end": "792690"
  },
  {
    "text": "all of the right fielders so first let's make sure temp OTP output exists so",
    "start": "792690",
    "end": "800550"
  },
  {
    "text": "we're back up here in an Baris files view and I see that there is no folder for that so let's do",
    "start": "800550",
    "end": "807360"
  },
  {
    "text": "Oh TP output and I think I can leave",
    "start": "807360",
    "end": "814170"
  },
  {
    "text": "this as current permissions if we get an error then I'm just gonna grant it",
    "start": "814170",
    "end": "820700"
  },
  {
    "text": "Jamaat 777 because again security not my problem I'm a developer that's only like",
    "start": "820700",
    "end": "832410"
  },
  {
    "text": "40% facetious okay so it succeeded and I",
    "start": "832410",
    "end": "840180"
  },
  {
    "text": "have an output file output file looks like this it's just a bunch of zeros",
    "start": "840180",
    "end": "845839"
  },
  {
    "text": "that's because each of the individual nodes of my Hadoop cluster can generate",
    "start": "845839",
    "end": "853079"
  },
  {
    "text": "its own output file and then it's my job at the end to stitch them all together I can open this output file and see the",
    "start": "853079",
    "end": "861300"
  },
  {
    "text": "results where yeah these are my right fielders from the data set so with that",
    "start": "861300",
    "end": "867570"
  },
  {
    "text": "I've got some data I've got some files I can also do one last thing that will",
    "start": "867570",
    "end": "876959"
  },
  {
    "text": "help me out a little bit later in the talk and that is I'm going to create a table called second basement so this is",
    "start": "876959",
    "end": "885089"
  },
  {
    "text": "a new table off of an existing table create table if it doesn't exist as a",
    "start": "885089",
    "end": "890610"
  },
  {
    "text": "select statement so I run this and it's",
    "start": "890610",
    "end": "895860"
  },
  {
    "text": "going to give me back 777 results in my second basement table so while that's",
    "start": "895860",
    "end": "903510"
  },
  {
    "text": "running not a big deal let's talk about scoop scoop is a console application it",
    "start": "903510",
    "end": "912600"
  },
  {
    "text": "has two major benefits or it has two major use cases use case number one is",
    "start": "912600",
    "end": "919560"
  },
  {
    "text": "to retrieve data from a relational database and put it into a Hadoop cluster use case number two is to take",
    "start": "919560",
    "end": "925769"
  },
  {
    "text": "data from a Hadoop cluster and wait for it put it into a staging table in a",
    "start": "925769",
    "end": "932070"
  },
  {
    "text": "relational database so I had to specify staging table because scoop does not",
    "start": "932070",
    "end": "937079"
  },
  {
    "text": "like to play nicely with existing data if it has data here and if you want to insert all new data",
    "start": "937079",
    "end": "943980"
  },
  {
    "text": "it's happy with that if you want to update all existing data its it's ok with that if you want to do come a",
    "start": "943980",
    "end": "950850"
  },
  {
    "text": "combination of inserting some rows and updating rows that already exist whoa does not like that at all so my",
    "start": "950850",
    "end": "957960"
  },
  {
    "text": "recommendation if you are using scoop take data from Hadoop put it into a staging table and then write your own",
    "start": "957960",
    "end": "964500"
  },
  {
    "text": "code to merge the staging table into your real tables also if you do use",
    "start": "964500",
    "end": "970680"
  },
  {
    "text": "scoop you'll have to install the Microsoft JDBC driver if you want to talk to",
    "start": "970680",
    "end": "976230"
  },
  {
    "text": "sequel server that is because sequel server has concepts of things like schema that don't exist in hive or in",
    "start": "976230",
    "end": "983340"
  },
  {
    "text": "Oracle at least not in the same way if",
    "start": "983340",
    "end": "990000"
  },
  {
    "text": "you do want to check out scoop I have a whole set of scripts on what you can do",
    "start": "990000",
    "end": "995790"
  },
  {
    "text": "with scoop and it will include things like listing all the databases listing",
    "start": "995790",
    "end": "1001520"
  },
  {
    "text": "all of the tables we can take a table as is and just dump it right into Hadoop we",
    "start": "1001520",
    "end": "1007580"
  },
  {
    "text": "can write queries to join together data and dump that data into Hadoop and then",
    "start": "1007580",
    "end": "1012950"
  },
  {
    "text": "vice versa we can take data from HDFS so just some text files from HDFS and dump",
    "start": "1012950",
    "end": "1019250"
  },
  {
    "text": "them over here or we can query hive data dump it over here there's also some",
    "start": "1019250",
    "end": "1025880"
  },
  {
    "text": "pretty good security options in here where I can store credentials securely",
    "start": "1025880",
    "end": "1031670"
  },
  {
    "text": "within my Hadoop cluster so I don't have to specify a username and password and I",
    "start": "1031670",
    "end": "1039079"
  },
  {
    "text": "think I'm hesitating on this because I never actually got it to work but you're",
    "start": "1039080",
    "end": "1045230"
  },
  {
    "text": "supposed to be able to use Active Directory to connect to a sequel server from a Hadoop and completely bypass",
    "start": "1045230",
    "end": "1052670"
  },
  {
    "text": "credentials I've not gotten that to work but it's supposed to be possible all",
    "start": "1052670",
    "end": "1060200"
  },
  {
    "text": "right so let's talk about some of the fun stuff let's write some dotnet code if you install a particular nougat",
    "start": "1060200",
    "end": "1066680"
  },
  {
    "text": "package microsoft hadoop mapreduce then you get the ability to perform file",
    "start": "1066680",
    "end": "1072260"
  },
  {
    "text": "maintenance this sounds like a really boring super power but it's actually pretty",
    "start": "1072260",
    "end": "1077450"
  },
  {
    "text": "cool we're going to take some data from Hadoop pull it down in a sequel server",
    "start": "1077450",
    "end": "1082999"
  },
  {
    "text": "and then we're going to have some dye net code write that data into a database I would use this method if you're",
    "start": "1082999",
    "end": "1092179"
  },
  {
    "text": "already comfortable with writing ETL in dotnet if you don't use other tooling for this or if you have some very",
    "start": "1092179",
    "end": "1099350"
  },
  {
    "text": "complex work that you need to do and also if you're not very comfortable writing Java so let's go check that out",
    "start": "1099350",
    "end": "1110950"
  },
  {
    "text": "first step I have to talk about NuGet packages I have two interesting new get",
    "start": "1110950",
    "end": "1118190"
  },
  {
    "text": "packages first one is Microsoft hadoop mapreduce that is a dotnet API notice",
    "start": "1118190",
    "end": "1124850"
  },
  {
    "text": "that says MapReduce functionality so originally back in 2013 it did MapReduce",
    "start": "1124850",
    "end": "1130909"
  },
  {
    "text": "because Microsoft had this cockamamie idea that they would put Hadoop on Windows thankfully that died so if",
    "start": "1130909",
    "end": "1139340"
  },
  {
    "text": "you're using hdinsight it actually still has an option for Windows please don't do that",
    "start": "1139340",
    "end": "1144350"
  },
  {
    "text": "never do that but back in 2013 they had this idea that all right you're gonna",
    "start": "1144350",
    "end": "1149720"
  },
  {
    "text": "write some c-sharp code to write map and root reduce jobs just as the world was",
    "start": "1149720",
    "end": "1155090"
  },
  {
    "text": "leaving map and reduced jobs because writing hive queries Pig queries was a lot better using spark way better anyhow",
    "start": "1155090",
    "end": "1163899"
  },
  {
    "text": "ignore the map introduced this is all about file maintenance it will install Newton soft JSON because every project",
    "start": "1163899",
    "end": "1170929"
  },
  {
    "text": "must have Newton's off die JSON and I installed F sharp not dated a sequel client which is a type provider that",
    "start": "1170929",
    "end": "1177860"
  },
  {
    "text": "will give you a micro arm in F sharp so yeah this is an example that uses F",
    "start": "1177860",
    "end": "1184759"
  },
  {
    "text": "sharp if you're not familiar with the language don't worry about it the syntax here is gonna be really similar to C",
    "start": "1184759",
    "end": "1190789"
  },
  {
    "text": "sharp and we're gonna walk through it step by step so what I want to do is load my",
    "start": "1190789",
    "end": "1196759"
  },
  {
    "text": "libraries and I create a connection string to my database the database is",
    "start": "1196759",
    "end": "1203330"
  },
  {
    "text": "OTP and I'm using windows authentication I then a statement here to insert into the",
    "start": "1203330",
    "end": "1211160"
  },
  {
    "text": "player that second baseman table and I have my columns that I need and I have the values so I'm going to build a an",
    "start": "1211160",
    "end": "1219650"
  },
  {
    "text": "insert statement and I'm going to pass in some values and it's going to insert a row into that table so pretty easy so",
    "start": "1219650",
    "end": "1228140"
  },
  {
    "text": "far by the way let's show that I have nothing up my sleeve I'm going to drop",
    "start": "1228140",
    "end": "1234080"
  },
  {
    "text": "the table if it exists and recreate it this is syntax that exists in sequel",
    "start": "1234080",
    "end": "1239720"
  },
  {
    "text": "Server 2016 this drop if exists if you don't have 2016 then you have to do a",
    "start": "1239720",
    "end": "1244880"
  },
  {
    "text": "check to see if the table exists yourself fortunately I have at least 2016 so we have an empty table I can say",
    "start": "1244880",
    "end": "1252170"
  },
  {
    "text": "selects star player that second basement I run this I get 0 results back because",
    "start": "1252170",
    "end": "1261320"
  },
  {
    "text": "I just dropped the table in my main",
    "start": "1261320",
    "end": "1266480"
  },
  {
    "text": "function I'm going to connect to my Hadoop cluster on port 5 0 0 7 0 this is",
    "start": "1266480",
    "end": "1274580"
  },
  {
    "text": "the web HDFS port so this is a web protocol that will allow me to work with",
    "start": "1274580",
    "end": "1281210"
  },
  {
    "text": "the Hadoop distributed file system I can add files I can retrieve files perform",
    "start": "1281210",
    "end": "1287360"
  },
  {
    "text": "that file maintenance I'm gonna connect to it with a username of admin and a",
    "start": "1287360",
    "end": "1292670"
  },
  {
    "text": "password of null because every time I go and do something secure I then go and",
    "start": "1292670",
    "end": "1297740"
  },
  {
    "text": "mess it up in a real cluster you would have a real password so let's say hey",
    "start": "1297740",
    "end": "1306680"
  },
  {
    "text": "who do cluster now that I'm talking to you now that we're in speaking terms I would like to copy a file from you to",
    "start": "1306680",
    "end": "1314870"
  },
  {
    "text": "me specifically I want to copy this file second baseman CSV to a local path in my",
    "start": "1314870",
    "end": "1321830"
  },
  {
    "text": "temp directory then I'm going to read",
    "start": "1321830",
    "end": "1328220"
  },
  {
    "text": "the file now this is a very small file so I can get away with using read all",
    "start": "1328220",
    "end": "1333380"
  },
  {
    "text": "lines that's going to read everything into memory if I can't do that then I'm going to open up a file stream",
    "start": "1333380",
    "end": "1339080"
  },
  {
    "text": "and I'm gonna string the file in which is what I would do with a data set that was",
    "start": "1339080",
    "end": "1344390"
  },
  {
    "text": "larger than 777 rows second basemen you can see is a string array so each",
    "start": "1344390",
    "end": "1351920"
  },
  {
    "text": "element of the array is a line in the file I open up my connection to sequel",
    "start": "1351920",
    "end": "1358070"
  },
  {
    "text": "server and then I begin a transaction and I say let's take that file and first",
    "start": "1358070",
    "end": "1364910"
  },
  {
    "text": "throw away any lines that have a length of zero there's a new line at the very",
    "start": "1364910",
    "end": "1370340"
  },
  {
    "text": "end let's not confuse anybody as just throw it away then for every row in this file I want",
    "start": "1370340",
    "end": "1378920"
  },
  {
    "text": "to take that element of the array and I want to run split against it so I split",
    "start": "1378920",
    "end": "1384590"
  },
  {
    "text": "on commas so now I have a new array this thing is called s so this is my string",
    "start": "1384590",
    "end": "1390380"
  },
  {
    "text": "array that's going to have five data points it's going to have the first name",
    "start": "1390380",
    "end": "1396410"
  },
  {
    "text": "last name age bats and throws then I'm",
    "start": "1396410",
    "end": "1401960"
  },
  {
    "text": "going to create an instance of that insert statement using my micro arm I'm",
    "start": "1401960",
    "end": "1410000"
  },
  {
    "text": "going to execute that statement and notice I never created an execute method",
    "start": "1410000",
    "end": "1416030"
  },
  {
    "text": "that was generated for me it also uses sequel server metadata to go look up",
    "start": "1416030",
    "end": "1421880"
  },
  {
    "text": "what those data types should be so age is actually an integer in my database so",
    "start": "1421880",
    "end": "1427130"
  },
  {
    "text": "I need to pass in an int so it correctly figured out that I need to give it an int which means that if I did not give",
    "start": "1427130",
    "end": "1433850"
  },
  {
    "text": "it an int I would get a compiler error right here and it says that yeah you",
    "start": "1433850",
    "end": "1440720"
  },
  {
    "text": "should pass at an integer here so let's make the compiler happy and I'm gonna do",
    "start": "1440720",
    "end": "1446750"
  },
  {
    "text": "that for each line create a new object to insert run the insert statement once",
    "start": "1446750",
    "end": "1454130"
  },
  {
    "text": "I'm done I'll close the transaction then let's take a file locally so hey cluster",
    "start": "1454130",
    "end": "1462380"
  },
  {
    "text": "I got something for you let's copy it from local up to HDFS it's called",
    "start": "1462380",
    "end": "1469490"
  },
  {
    "text": "pitching ratings so we're going to set this as the default project because I didn't and",
    "start": "1469490",
    "end": "1479120"
  },
  {
    "text": "then we'll run this so it says that it",
    "start": "1479120",
    "end": "1484520"
  },
  {
    "text": "loaded the table and in a moment it's going to say that it successfully uploaded the new file while that's going",
    "start": "1484520",
    "end": "1491179"
  },
  {
    "text": "on let's confirm that it did in fact download second basement CSV 816 at 1:25",
    "start": "1491179",
    "end": "1499400"
  },
  {
    "text": "a.m. yah mahn Eastern Time eastern US time so it's 1:25 a.m. right now",
    "start": "1499400",
    "end": "1505539"
  },
  {
    "text": "hey it uploaded the file ok so we can go back to here and",
    "start": "1505539",
    "end": "1511340"
  },
  {
    "text": "double-check and let's make sure that in OTP pitching ratings is as of 1:25 a.m.",
    "start": "1511340",
    "end": "1519950"
  },
  {
    "text": "which it is so we've got the new files",
    "start": "1519950",
    "end": "1524950"
  },
  {
    "text": "one thing that we can also check let's delete those lines so I don't",
    "start": "1525159",
    "end": "1530330"
  },
  {
    "text": "accidentally run it select star from player dot second baseman we have 777",
    "start": "1530330",
    "end": "1535370"
  },
  {
    "text": "rows as I expected we have all of our second baseman in the data set so great",
    "start": "1535370",
    "end": "1542510"
  },
  {
    "text": "at this point in 50 lines of code I took a file from here I loaded it into sequel",
    "start": "1542510",
    "end": "1549470"
  },
  {
    "text": "server I took another file and I loaded it up here I've just created a fairly",
    "start": "1549470",
    "end": "1554780"
  },
  {
    "text": "basic ETL engine you can add a lot more to it you can start moving around more",
    "start": "1554780",
    "end": "1559880"
  },
  {
    "text": "generic sets of data but this is the core this is the basics so I'm going to",
    "start": "1559880",
    "end": "1567830"
  },
  {
    "text": "hit enter and close that so this is the",
    "start": "1567830",
    "end": "1573429"
  },
  {
    "text": "meat of you know what I would do if I my dotnet developer and I'm working with",
    "start": "1573429",
    "end": "1578960"
  },
  {
    "text": "both sequel server and Hadoop this is probably one of the two methods that I would be most inclined to use now",
    "start": "1578960",
    "end": "1588070"
  },
  {
    "text": "suppose that you are a big fan of sequel server integration services suppose",
    "start": "1588070",
    "end": "1593330"
  },
  {
    "text": "further that you have sequel server 2016 and Visual Studio 2015 or later in that",
    "start": "1593330",
    "end": "1600890"
  },
  {
    "text": "case you have some components that will help you in your quest with Hadoop because you will be able to",
    "start": "1600890",
    "end": "1606750"
  },
  {
    "text": "connect to the Hadoop distributed file system as a source or a destination in integration services if you're using",
    "start": "1606750",
    "end": "1614429"
  },
  {
    "text": "older versions you're very limited access I wouldn't even really talk about",
    "start": "1614429",
    "end": "1621000"
  },
  {
    "text": "it in that case because here you can do one thing and one thing only it's only",
    "start": "1621000",
    "end": "1626760"
  },
  {
    "text": "how if you normally write integration services packages for sequel server then",
    "start": "1626760",
    "end": "1633380"
  },
  {
    "text": "2016 you can do this with SSIS let's",
    "start": "1633380",
    "end": "1641490"
  },
  {
    "text": "walk through an example what I have here are a couple of Hadoop connection",
    "start": "1641490",
    "end": "1648269"
  },
  {
    "text": "managers we're only going to look at connection manager number one the other one was a test I created that by saying",
    "start": "1648269",
    "end": "1654169"
  },
  {
    "text": "right click on connection managers and say new connection manager so I do that",
    "start": "1654169",
    "end": "1659730"
  },
  {
    "text": "and as of SSIS 2016 I now have a hadoop",
    "start": "1659730",
    "end": "1666419"
  },
  {
    "text": "connection manager so I can select that and it will give me a screen that looks",
    "start": "1666419",
    "end": "1673080"
  },
  {
    "text": "roughly like this that's actually connection manager one that was my test",
    "start": "1673080",
    "end": "1678899"
  },
  {
    "text": "that was going to add your HD insight so it looks like this there are two",
    "start": "1678899",
    "end": "1685320"
  },
  {
    "text": "connection methods with integration services 2016 web h-cat and web HDFS we",
    "start": "1685320",
    "end": "1692760"
  },
  {
    "text": "talked about web HDFS already web h-cat is the hive catalog so I create hive",
    "start": "1692760",
    "end": "1700769"
  },
  {
    "text": "tables and then I can expose that data on the network but I don't want to move",
    "start": "1700769",
    "end": "1707970"
  },
  {
    "text": "around hive tables I want to move around data files so I'm going to connect my hosts on my web HDFS port I'm going to",
    "start": "1707970",
    "end": "1715139"
  },
  {
    "text": "give it a user name and I don't have a password for it because my",
    "start": "1715139",
    "end": "1721710"
  },
  {
    "text": "authentication is so basic you can use Kerberos and actually have serious",
    "start": "1721710",
    "end": "1727500"
  },
  {
    "text": "security but this is a demo so I create a data flow task inside this",
    "start": "1727500",
    "end": "1735830"
  },
  {
    "text": "data flow task I have an L a DB source this data source is my OTP database",
    "start": "1735830",
    "end": "1741679"
  },
  {
    "text": "again and it is top salary by age so this is a very small database table that",
    "start": "1741679",
    "end": "1749690"
  },
  {
    "text": "has two values an integer and a numeric value it's a decimal value so very small",
    "start": "1749690",
    "end": "1757340"
  },
  {
    "text": "table but that's okay I can still write it to HDFS this is one of the problems I",
    "start": "1757340",
    "end": "1763309"
  },
  {
    "text": "have with integration services with high resolution monitors it's not supposed to",
    "start": "1763309",
    "end": "1770720"
  },
  {
    "text": "look like that in fact it gets a little bit worse than this because there are",
    "start": "1770720",
    "end": "1777529"
  },
  {
    "text": "some values under here that you have to set so if you have a high res monitor like like this or if you have a 4k",
    "start": "1777529",
    "end": "1784370"
  },
  {
    "text": "monitor well it doesn't work that well if you're still on 1080p then you'll",
    "start": "1784370",
    "end": "1790580"
  },
  {
    "text": "actually see all of the values and one of one of the things that you're missing is you have a data type well this is a",
    "start": "1790580",
    "end": "1796700"
  },
  {
    "text": "flat file this is a text file cool what's the separator oh it's a comma okay coming back here I can tell you",
    "start": "1796700",
    "end": "1805190"
  },
  {
    "text": "yeah it's a text format what's your separator somewhere down here",
    "start": "1805190",
    "end": "1811570"
  },
  {
    "text": "fortunately I did this before I migrated to a high res monitor that said once you",
    "start": "1811629",
    "end": "1819230"
  },
  {
    "text": "have the data loaded what's nice about integration services is sources and",
    "start": "1819230",
    "end": "1825169"
  },
  {
    "text": "destinations are just sources and destinations they're all abstracted out to be the same general principle I don't",
    "start": "1825169",
    "end": "1830659"
  },
  {
    "text": "care if the destination is a sequel server or a flat file or if it's HDFS or if it's Oracle or if it's anywhere else",
    "start": "1830659",
    "end": "1837049"
  },
  {
    "text": "if I have a destination component it's all going to work the same way I have input columns and destination",
    "start": "1837049",
    "end": "1844909"
  },
  {
    "text": "columns and I can map them appropriately so I can write this data and as I",
    "start": "1844909",
    "end": "1851899"
  },
  {
    "text": "execute this package it will take the",
    "start": "1851899",
    "end": "1857570"
  },
  {
    "text": "gigantic 22 rows of data and it will write it to my I do cluster meanwhile my",
    "start": "1857570",
    "end": "1864679"
  },
  {
    "text": "Hadoop cluster is saying you you realize that the minimum block size in HDFS is set to 64 megabytes right and",
    "start": "1864679",
    "end": "1870589"
  },
  {
    "text": "I say yes and I'm still going to give you 1/3 of a kilobyte this is big data",
    "start": "1870589",
    "end": "1877819"
  },
  {
    "text": "people so let's let's look at the top file we're gonna open it up and it's",
    "start": "1877819",
    "end": "1885769"
  },
  {
    "text": "going to show us that we do in fact as soon as it actually opens we do in fact have all of our data in here I'm not",
    "start": "1885769",
    "end": "1893599"
  },
  {
    "text": "going to wait for it to open it's being sassy with me I will sass it right back so let's continue on to retrieve from",
    "start": "1893599",
    "end": "1901939"
  },
  {
    "text": "HDFS and just trust that the data is up there all third of a kilobyte of it we",
    "start": "1901939",
    "end": "1907999"
  },
  {
    "text": "have our data flow and we can take data from HDFS once again not worrying about",
    "start": "1907999",
    "end": "1916669"
  },
  {
    "text": "the screen resolution issues I select a file path pitching ratings this is again a text file that is comma delimited so",
    "start": "1916669",
    "end": "1924739"
  },
  {
    "text": "in integration services you hit columns and it will show you the columns and when you connect to a flat file or to a",
    "start": "1924739",
    "end": "1931399"
  },
  {
    "text": "sequel server database or like an Oracle database oftentimes this metadata is",
    "start": "1931399",
    "end": "1937729"
  },
  {
    "text": "filled out for you but it's not filled",
    "start": "1937729",
    "end": "1944059"
  },
  {
    "text": "out for you here the reason is we're using web HDFS and web HDFS when it",
    "start": "1944059",
    "end": "1951199"
  },
  {
    "text": "retrieves a file does not have any type of method of saying hey here's what the",
    "start": "1951199",
    "end": "1958009"
  },
  {
    "text": "columns actually represent here's what the data actually means so integration",
    "start": "1958009",
    "end": "1963259"
  },
  {
    "text": "services is just taking a file and it's saying I know that you have let's find",
    "start": "1963259",
    "end": "1968479"
  },
  {
    "text": "out how many columns it has 61 columns in here so you have 61 columns in here I",
    "start": "1968479",
    "end": "1975679"
  },
  {
    "text": "don't know what they mean I just know that you have them and so it gives you automate it default names if you want to",
    "start": "1975679",
    "end": "1984049"
  },
  {
    "text": "clean up the data it's pretty easy to create your own transformations and I",
    "start": "1984049",
    "end": "1989179"
  },
  {
    "text": "decided to add some new columns those columns are named first name last name",
    "start": "1989179",
    "end": "1994789"
  },
  {
    "text": "age vats and throws and they are unicode strings including",
    "start": "1994789",
    "end": "2002330"
  },
  {
    "text": "age which we know is an integer that's pretty suspicious but I've put on a",
    "start": "2002330",
    "end": "2008840"
  },
  {
    "text": "little watch icon to help us explain why",
    "start": "2008840",
    "end": "2014239"
  },
  {
    "text": "I did it this way no points for spoiling it so we're going to connect a Hadoop",
    "start": "2014239",
    "end": "2022820"
  },
  {
    "text": "maybe there we go okay yes it retrieved",
    "start": "2022820",
    "end": "2028190"
  },
  {
    "text": "the data and if I scroll all the way over to the right we have our data we",
    "start": "2028190",
    "end": "2033980"
  },
  {
    "text": "have our columns and then we have our lines of data that include the headers",
    "start": "2033980",
    "end": "2039190"
  },
  {
    "text": "so pull down the headers unfortunately integration services for Hadoop does not",
    "start": "2039190",
    "end": "2045919"
  },
  {
    "text": "understand the idea of a header row with a flat file you can say use the first",
    "start": "2045919",
    "end": "2051980"
  },
  {
    "text": "row as a header and it will take those values and it will replace the headers with those values and then if I could do",
    "start": "2051980",
    "end": "2058520"
  },
  {
    "text": "that life would be good I could make this an integer and we would have a pretty decent data set but I can't do it",
    "start": "2058520",
    "end": "2065868"
  },
  {
    "text": "here so that is a limitation that currently exists in integration services",
    "start": "2065869",
    "end": "2073429"
  },
  {
    "text": "when dealing with Hadoop given the set of limitations I'm not sure that I would",
    "start": "2073429",
    "end": "2079908"
  },
  {
    "text": "recommend using SSIS if you're moving data back and forth with hoop I mean I",
    "start": "2079909",
    "end": "2085940"
  },
  {
    "text": "guess maybe if that's all you do then it's an option but I would probably just write some net code it's not that",
    "start": "2085940",
    "end": "2093049"
  },
  {
    "text": "difficult so up to this point we have looked at",
    "start": "2093049",
    "end": "2099280"
  },
  {
    "text": "ways of integrating data indirectly I've been shuffling files around and",
    "start": "2099280",
    "end": "2105380"
  },
  {
    "text": "I've been assuming that people down here will understand what to do with this file people up here will understand what",
    "start": "2105380",
    "end": "2111170"
  },
  {
    "text": "to do with a file but now I want to say over here in the world of sequel server",
    "start": "2111170",
    "end": "2116270"
  },
  {
    "text": "how do I actually get data from somewhere in Hadoop and do something",
    "start": "2116270",
    "end": "2122240"
  },
  {
    "text": "with it and the first method is using a linked server link servers have been",
    "start": "2122240",
    "end": "2127790"
  },
  {
    "text": "around for decades and they allow to connect external resources to a",
    "start": "2127790",
    "end": "2133680"
  },
  {
    "text": "sequel server so that external resource it could be another sequel server it could be Oracle it could be an Access",
    "start": "2133680",
    "end": "2139589"
  },
  {
    "text": "database it could be an Excel spreadsheet could be a flat file it could be a hive table this is going to",
    "start": "2139589",
    "end": "2147989"
  },
  {
    "text": "be your main option prior to sequel Server 2016 and let's go check out a",
    "start": "2147989",
    "end": "2154680"
  },
  {
    "text": "demo in order for this to work I must have the Microsoft hive ODBC driver",
    "start": "2154680",
    "end": "2163009"
  },
  {
    "text": "installed on my on the windows side on the side that has sequel server on it so",
    "start": "2163009",
    "end": "2169890"
  },
  {
    "text": "the Microsoft hive ODBC driver and I have a link to that let's configure this",
    "start": "2169890",
    "end": "2179190"
  },
  {
    "text": "driver this says please connect to my",
    "start": "2179190",
    "end": "2184380"
  },
  {
    "text": "cluster on ports 10,000 this is another configurable port that's the hive",
    "start": "2184380",
    "end": "2189680"
  },
  {
    "text": "integration port I actually have a password here so for once I did it right",
    "start": "2189680",
    "end": "2196769"
  },
  {
    "text": "ish so then I also have to hit Advanced",
    "start": "2196769",
    "end": "2201809"
  },
  {
    "text": "Options by default the default string column name length will be 32 K well",
    "start": "2201809",
    "end": "2210959"
  },
  {
    "text": "what happens here is that sequel server and the the hive o it uses the hive ODBC",
    "start": "2210959",
    "end": "2216569"
  },
  {
    "text": "driver to take string data and it says how do I translate this okay well I'm",
    "start": "2216569",
    "end": "2221880"
  },
  {
    "text": "just gonna take this value and make it a varchar' of that size so if it's set to",
    "start": "2221880",
    "end": "2227059"
  },
  {
    "text": "32,000 blah blah blah then sequel server will say I don't know of this 32,000",
    "start": "2227059",
    "end": "2234420"
  },
  {
    "text": "character varchar' that you're talking about that's an invalid data type error so you do have to set it to a value",
    "start": "2234420",
    "end": "2241579"
  },
  {
    "text": "unfortunately there's no such thing as a max or a 0 or a negative 1 or anything to indicate I want a max column meaning",
    "start": "2241579",
    "end": "2250440"
  },
  {
    "text": "that you're limited to 8,000 characters per column when you're using this hive",
    "start": "2250440",
    "end": "2256349"
  },
  {
    "text": "driver a couple things to think about there so",
    "start": "2256349",
    "end": "2262200"
  },
  {
    "text": "I have my linked server I can or excuse",
    "start": "2262200",
    "end": "2268140"
  },
  {
    "text": "me I have my ODBC driver and I can create a link server pretty easily I say my server name is",
    "start": "2268140",
    "end": "2276810"
  },
  {
    "text": "called cluster Ino it is going to read a database called",
    "start": "2276810",
    "end": "2282390"
  },
  {
    "text": "hive and I just named that myself you can name it something else it will use",
    "start": "2282390",
    "end": "2288380"
  },
  {
    "text": "da SQL providers that's pretty default the data source is my hive ODBC",
    "start": "2288380",
    "end": "2297330"
  },
  {
    "text": "connection the one that I created and I just showed you user ID and password",
    "start": "2297330",
    "end": "2302940"
  },
  {
    "text": "fill in your own real password don't use mine that's not my real password it's admin one no so then anyhow we then",
    "start": "2302940",
    "end": "2311580"
  },
  {
    "text": "create a link server login and I can make sure that the query actually works",
    "start": "2311580",
    "end": "2316650"
  },
  {
    "text": "I've already created this link server as we can see in server objects link",
    "start": "2316650",
    "end": "2322680"
  },
  {
    "text": "servers I can see it right here that creates a hive database and",
    "start": "2322680",
    "end": "2328400"
  },
  {
    "text": "remember in hive I said that the database was called default and the",
    "start": "2328400",
    "end": "2333540"
  },
  {
    "text": "table was called second baseman or batting gradings perm so in sequel",
    "start": "2333540",
    "end": "2338550"
  },
  {
    "text": "server it takes the hive database and turns that into the schema and I defined",
    "start": "2338550",
    "end": "2346500"
  },
  {
    "text": "what this hive database was so took a few seconds gave me 777 rows they are",
    "start": "2346500",
    "end": "2356010"
  },
  {
    "text": "the 777 rows I'm expecting the execution plan says this is a remote query all of",
    "start": "2356010",
    "end": "2363570"
  },
  {
    "text": "the work is happening outside of sequel server so it has relatively little",
    "start": "2363570",
    "end": "2368790"
  },
  {
    "text": "information to share with you except that it thought you were going to send it 10,000 rows why 10,000 because that",
    "start": "2368790",
    "end": "2376980"
  },
  {
    "text": "was a number that somebody plugged in and is the default for sequel server 2017 for 2016 and before I think it was",
    "start": "2376980",
    "end": "2385140"
  },
  {
    "text": "like one row so basically database engine is saying I have no clue how many",
    "start": "2385140",
    "end": "2391500"
  },
  {
    "text": "rows coming in let me give you a number 10,000 that sounds like a good number and it takes in all of the data and it",
    "start": "2391500",
    "end": "2401080"
  },
  {
    "text": "does this little compute scaler thing what that does is I'm taking the columns that came in first name last name age",
    "start": "2401080",
    "end": "2409000"
  },
  {
    "text": "bats throws and I am converting them to sequel server data types so that's what",
    "start": "2409000",
    "end": "2415720"
  },
  {
    "text": "this is doing and then returning those results so that took a few seconds but",
    "start": "2415720",
    "end": "2422490"
  },
  {
    "text": "what we can do once we have that link server in effect is we can join sequel",
    "start": "2422490",
    "end": "2431950"
  },
  {
    "text": "server tables with hive tables so this is a sample query and I'm selecting the",
    "start": "2431950",
    "end": "2439060"
  },
  {
    "text": "top 50 records from my second basement table and then I'm doing something that",
    "start": "2439060",
    "end": "2445810"
  },
  {
    "text": "does not exist in hive I'm using the cross apply operator so this is in",
    "start": "2445810",
    "end": "2451240"
  },
  {
    "text": "sequel server T sequel this is not in hives hql and what I'm saying is okay",
    "start": "2451240",
    "end": "2457359"
  },
  {
    "text": "for the second basement if you are more than 30 years old you are officially old in my data set this is a cruel cruel thing to tell",
    "start": "2457359",
    "end": "2464530"
  },
  {
    "text": "people but we're gonna modify it a little bit by joining to this top salary",
    "start": "2464530",
    "end": "2469869"
  },
  {
    "text": "by age and what that is is what how much money did the highest-paid second",
    "start": "2469869",
    "end": "2477100"
  },
  {
    "text": "baseman of your age make in the year I think this was 2016 so I'm going to",
    "start": "2477100",
    "end": "2485460"
  },
  {
    "text": "point out that this table is in sequel server this table is in hive we're",
    "start": "2485460",
    "end": "2491530"
  },
  {
    "text": "joining the data together and we're going to get some results and it's going",
    "start": "2491530",
    "end": "2497290"
  },
  {
    "text": "to take a little while first of all Hadoop clusters being shy",
    "start": "2497290",
    "end": "2502330"
  },
  {
    "text": "second of all lint servers suck I'll get into more about why link servers suck as",
    "start": "2502330",
    "end": "2508930"
  },
  {
    "text": "we go on but I just want to point out that Ben Zobrist is old but I would",
    "start": "2508930",
    "end": "2514030"
  },
  {
    "text": "happily be called old if I got me if I got to make seven-and-a-half million dollars this year that's a trade-off I",
    "start": "2514030",
    "end": "2520570"
  },
  {
    "text": "am willing to take so this is link servers prior to 2016",
    "start": "2520570",
    "end": "2528059"
  },
  {
    "text": "it's probably your best option outside of shuffling around using.net code with 2016",
    "start": "2528059",
    "end": "2535650"
  },
  {
    "text": "Microsoft has poly base poly base is the latest offering it was originally",
    "start": "2535650",
    "end": "2541140"
  },
  {
    "text": "introduced in 2010 in a product called",
    "start": "2541140",
    "end": "2546150"
  },
  {
    "text": "parallel data warehouse in 2012 they improved upon it later on they renamed",
    "start": "2546150",
    "end": "2552720"
  },
  {
    "text": "parallel data warehouse to ApS analytics processing system I believe basically",
    "start": "2552720",
    "end": "2558900"
  },
  {
    "text": "this is a really really expensive appliance that is a combination of a Hadoop cluster and a sequel server",
    "start": "2558900",
    "end": "2564509"
  },
  {
    "text": "instance jammed together really interesting but base price I think is",
    "start": "2564509",
    "end": "2570539"
  },
  {
    "text": "upper six digits lower seven digits so a lot of companies just they're not gonna",
    "start": "2570539",
    "end": "2575759"
  },
  {
    "text": "buy this thing poly base was a key component in that in that process well",
    "start": "2575759",
    "end": "2583740"
  },
  {
    "text": "in sequel Server 2016 Microsoft released it to the masses by the masses I mean anybody who can afford",
    "start": "2583740",
    "end": "2590369"
  },
  {
    "text": "Enterprise Edition so not really the masses but compared to PDW or ApS as a",
    "start": "2590369",
    "end": "2597509"
  },
  {
    "text": "much broader audience so if you have Enterprise Edition you can take advantage of this sequel Server 2016",
    "start": "2597509",
    "end": "2605700"
  },
  {
    "text": "Service Pack 1 changed a lot with regard to what you were allowed to do many",
    "start": "2605700",
    "end": "2611849"
  },
  {
    "text": "things that were previously Enterprise only now we're available in standard edition in Express Edition so you know",
    "start": "2611849",
    "end": "2619680"
  },
  {
    "text": "down to the free version poly base is not really one of those it's still an",
    "start": "2619680",
    "end": "2625559"
  },
  {
    "text": "enterprise only feature with a caveat and we'll talk about that caveat poly",
    "start": "2625559",
    "end": "2633569"
  },
  {
    "text": "base has three big advantages over Lync servers number one is the ability to",
    "start": "2633569",
    "end": "2640890"
  },
  {
    "text": "push down predicates I can take a filter like part of a where clause or maybe a",
    "start": "2640890",
    "end": "2646410"
  },
  {
    "text": "grouping operation and push that down and make the Hadoop cluster do that work my sequel servers are very expensive my",
    "start": "2646410",
    "end": "2654089"
  },
  {
    "text": "Hadoop cluster nodes nearly as expensive so if I can take work over here and force it down that",
    "start": "2654089",
    "end": "2660239"
  },
  {
    "text": "way I'm making the less expensive stuff do more work and it's gonna return back to my pretty expensive database instance",
    "start": "2660239",
    "end": "2667170"
  },
  {
    "text": "and I'll return that to the end-user the way that we figure out whether or not",
    "start": "2667170",
    "end": "2674430"
  },
  {
    "text": "it's worth pushing down and having Hadoop do the job is with statistics in",
    "start": "2674430",
    "end": "2679890"
  },
  {
    "text": "the linked server case I showed you the estimate was 10,000 that number is pulled out of a hat it's just the",
    "start": "2679890",
    "end": "2686160"
  },
  {
    "text": "default with Polly base we can actually collect stats on how many rows are in",
    "start": "2686160",
    "end": "2692279"
  },
  {
    "text": "that table what's the distribution of values in that table just like a regular sequel server table and that will give",
    "start": "2692279",
    "end": "2699569"
  },
  {
    "text": "you an idea that well if I'm filtering down and I'm only gonna get back one row from here maybe I should have the Hadoop",
    "start": "2699569",
    "end": "2705239"
  },
  {
    "text": "side do that filter give me back one row but if I'm getting back 95 percent of",
    "start": "2705239",
    "end": "2711299"
  },
  {
    "text": "the rows just stream that data to me and let me do the filtering the third big",
    "start": "2711299",
    "end": "2719640"
  },
  {
    "text": "thing and this I think is more of a big thing for the future parallelized sequel",
    "start": "2719640",
    "end": "2725759"
  },
  {
    "text": "server so let's draw out we have a Hadoop cluster we have a name node on",
    "start": "2725759",
    "end": "2733469"
  },
  {
    "text": "the Hadoop cluster and the name node is basically in control of a bunch of data",
    "start": "2733469",
    "end": "2740309"
  },
  {
    "text": "nodes so each one of these data nodes is responsible for holding data it's",
    "start": "2740309",
    "end": "2747150"
  },
  {
    "text": "responsible for running code that the name notice signs to it on the sequel",
    "start": "2747150",
    "end": "2755069"
  },
  {
    "text": "server side you know historically we just had a sequel server but with poly",
    "start": "2755069",
    "end": "2760769"
  },
  {
    "text": "base and specifically for poly based queries we have this head node and the",
    "start": "2760769",
    "end": "2766950"
  },
  {
    "text": "head node can farm out work to what it",
    "start": "2766950",
    "end": "2773279"
  },
  {
    "text": "calls compute nodes",
    "start": "2773279",
    "end": "2776839"
  },
  {
    "text": "so I have these smaller databases down here these are my compute nodes these",
    "start": "2778670",
    "end": "2784700"
  },
  {
    "text": "can be standard edition head node must be Enterprise Edition what this allows",
    "start": "2784700",
    "end": "2791900"
  },
  {
    "text": "me to do is say ok I have a pretty big query that I know is going to be",
    "start": "2791900",
    "end": "2798890"
  },
  {
    "text": "distributed out I have three compute",
    "start": "2798890",
    "end": "2804589"
  },
  {
    "text": "nodes that are available to me there are some number of data nodes over here just",
    "start": "2804589",
    "end": "2810019"
  },
  {
    "text": "for simplicity sake let's also say three as part of the poly based engine there's",
    "start": "2810019",
    "end": "2817160"
  },
  {
    "text": "actually direct communication between my compute node sequel servers and the",
    "start": "2817160",
    "end": "2825079"
  },
  {
    "text": "Hadoop data nodes what that lets me do is send data back into these smaller",
    "start": "2825079",
    "end": "2832039"
  },
  {
    "text": "compute nodes pull it into temp tables basically do as much of the processing as it can here and then send up the",
    "start": "2832039",
    "end": "2839210"
  },
  {
    "text": "small data set up into my head node where I finish processing where I do all",
    "start": "2839210",
    "end": "2845210"
  },
  {
    "text": "final work and I send that data out to the user in other words this is a",
    "start": "2845210",
    "end": "2851480"
  },
  {
    "text": "massive parallel processing for sequel server it only works when you're working",
    "start": "2851480",
    "end": "2856519"
  },
  {
    "text": "with a Hadoop cluster but cross my fingers I'm hoping that the poly based",
    "start": "2856519",
    "end": "2863960"
  },
  {
    "text": "team is able to get this to work with other queries so that sequel server becomes a massive parallel processing",
    "start": "2863960",
    "end": "2870680"
  },
  {
    "text": "system just like a new cluster would be there are a lot of problems that that",
    "start": "2870680",
    "end": "2875990"
  },
  {
    "text": "help solve but it's not there today this only works when you're working with",
    "start": "2875990",
    "end": "2881000"
  },
  {
    "text": "a Hadoop cluster poly base is not perfect this is something that was designed in 2010 that means that it has",
    "start": "2881000",
    "end": "2889940"
  },
  {
    "text": "the limitations of something that was designed in 2010 the stuff that has happened into Hadoop world since 2010",
    "start": "2889940",
    "end": "2896410"
  },
  {
    "text": "poly based kind of ignores so it's MapReduce only there's no tez tez is a",
    "start": "2896410",
    "end": "2905619"
  },
  {
    "text": "directed a cyclical graph generator that helps reduce this the",
    "start": "2905619",
    "end": "2911020"
  },
  {
    "text": "sighs of mapreduce operations basically it it reduces the number of times you",
    "start": "2911020",
    "end": "2916150"
  },
  {
    "text": "write to disk and that makes things faster it's almost a free improvement it's built into hive now but is not",
    "start": "2916150",
    "end": "2922900"
  },
  {
    "text": "available in poly base there's no spark spark is to me it is the current",
    "start": "2922900",
    "end": "2929980"
  },
  {
    "text": "database in or the current data engine for Hadoop that has replaced MapReduce this is the default now because it is so",
    "start": "2929980",
    "end": "2937060"
  },
  {
    "text": "much faster than MapReduce no spark support that said poly a base is still",
    "start": "2937060",
    "end": "2945610"
  },
  {
    "text": "really cool and this is my preferred method of working with a Hadoop cluster",
    "start": "2945610",
    "end": "2951550"
  },
  {
    "text": "when working also with sequel server so let's take a quick look at that in the",
    "start": "2951550",
    "end": "2956860"
  },
  {
    "text": "last few minutes here I am going to want",
    "start": "2956860",
    "end": "2962380"
  },
  {
    "text": "to create a poly based table in order to do that I've got to do some prep work first the first thing that I need to do",
    "start": "2962380",
    "end": "2969580"
  },
  {
    "text": "is create an external data source that's saying where does the data live data is",
    "start": "2969580",
    "end": "2976240"
  },
  {
    "text": "not gonna live in sequel server sequel server is gonna have a pointer to the data but it won't actually host the data",
    "start": "2976240",
    "end": "2981960"
  },
  {
    "text": "the data will be hosted in a Hadoop cluster and the location will be this",
    "start": "2981960",
    "end": "2989370"
  },
  {
    "text": "now I left this in this is the version that's available when you grab the",
    "start": "2989370",
    "end": "2994510"
  },
  {
    "text": "slides and codes and demos because I'm assuming that you download a Hortonworks sandbox and they by default hard code",
    "start": "2994510",
    "end": "3003450"
  },
  {
    "text": "that URL sandbox Hortonworks com in reality mine is cluster II no it's on",
    "start": "3003450",
    "end": "3009480"
  },
  {
    "text": "port 8000 that is the MapReduce port it's a configurable port but that's",
    "start": "3009480",
    "end": "3014840"
  },
  {
    "text": "default it's a standard with Hadoop the resource manager is necessary for",
    "start": "3014840",
    "end": "3020760"
  },
  {
    "text": "predicate push down see what predicates push down does essentially is it decides",
    "start": "3020760",
    "end": "3026520"
  },
  {
    "text": "I need to push down this predicate in other words I need to generate a MapReduce job over here the alternative is I don't push a",
    "start": "3026520",
    "end": "3034080"
  },
  {
    "text": "predicate I just stream the data back this way well in order to create MapReduce jobs",
    "start": "3034080",
    "end": "3040910"
  },
  {
    "text": "the poly based engine needs to communicate with a resource manager yarn so it does so typically",
    "start": "3040910",
    "end": "3048630"
  },
  {
    "text": "that's the default port eight zero five zero the resource manager location is not required but if you do not include",
    "start": "3048630",
    "end": "3055860"
  },
  {
    "text": "it you will never be able to do MapReduce jobs you will only be able to pull data I have a data source I need a",
    "start": "3055860",
    "end": "3065160"
  },
  {
    "text": "file format as well this is how I'm going to start defining structure comma",
    "start": "3065160",
    "end": "3071610"
  },
  {
    "text": "delimited and it's I'm just calling it a text file format so delimited text",
    "start": "3071610",
    "end": "3077430"
  },
  {
    "text": "delimiter is a comma I have a location I have a format I can create a table an",
    "start": "3077430",
    "end": "3084930"
  },
  {
    "text": "external table the first half of the statement up till about that point",
    "start": "3084930",
    "end": "3090810"
  },
  {
    "text": "except for the word external it just looks like a normal table so dbo that second baseman this is not a remote",
    "start": "3090810",
    "end": "3098490"
  },
  {
    "text": "server this is just another table and I could put it into a schema like Hadoop",
    "start": "3098490",
    "end": "3104100"
  },
  {
    "text": "cluster dot second baseman or I can just make it a regular dbo table here's where",
    "start": "3104100",
    "end": "3114270"
  },
  {
    "text": "it gets weird the location is my poly base file or",
    "start": "3114270",
    "end": "3122520"
  },
  {
    "text": "folder so I'm going to connect to HDFS and I'm going to look for that file",
    "start": "3122520",
    "end": "3128730"
  },
  {
    "text": "called second baseman dot CSV now I can scratch this out and just say tempo OTP",
    "start": "3128730",
    "end": "3136170"
  },
  {
    "text": "and in that case I would look for everything in that folder typically I'm",
    "start": "3136170",
    "end": "3142080"
  },
  {
    "text": "gonna look for everything in the folder because then I can have processes that will create new files regularly and I",
    "start": "3142080",
    "end": "3147450"
  },
  {
    "text": "don't have to update my table statement I define my datasource and file format",
    "start": "3147450",
    "end": "3152940"
  },
  {
    "text": "which we already know the last bit is rejection this Hadoop cluster",
    "start": "3152940",
    "end": "3159600"
  },
  {
    "text": "semi-structured the data that's in there doesn't necessarily fit the structure",
    "start": "3159600",
    "end": "3165810"
  },
  {
    "text": "that I'm trying to apply down here in sequel server so I need to be able to have a safety valve let's say that a row",
    "start": "3165810",
    "end": "3172860"
  },
  {
    "text": "is bad do I want to stop the job the answer could be yes it could be I need every row",
    "start": "3172860",
    "end": "3178230"
  },
  {
    "text": "to be correct or else we fail in that case I would set the reject value to",
    "start": "3178230",
    "end": "3183950"
  },
  {
    "text": "zero but realistically I probably want",
    "start": "3183950",
    "end": "3193800"
  },
  {
    "text": "to have some slack I probably want to give it a few values so I'll let five",
    "start": "3193800",
    "end": "3199710"
  },
  {
    "text": "Records fail before it gives me an error now that there is a trade-off here because let's say that I pick one record",
    "start": "3199710",
    "end": "3208460"
  },
  {
    "text": "then I'm probably gonna get a lot of failures because oh there's two rows that are bad but let's say that I pick a",
    "start": "3208460",
    "end": "3216390"
  },
  {
    "text": "million rows million rows have to be bad before this thing gives me an error",
    "start": "3216390",
    "end": "3221630"
  },
  {
    "text": "probably not gonna get an error very frequently but if I write a query where like every row is bad for some reason",
    "start": "3221630",
    "end": "3228410"
  },
  {
    "text": "then it's going to have to read a million rows before it gives me an error message that says hey dummy",
    "start": "3228410",
    "end": "3234119"
  },
  {
    "text": "you probably didn't mean this the worst case is let's say I pick a million and I",
    "start": "3234119",
    "end": "3241530"
  },
  {
    "text": "have a hundred billion rows and it turns out that the one millionth error is on row number one hundred billion it is",
    "start": "3241530",
    "end": "3248940"
  },
  {
    "text": "going to go through the entire data set it is going to try to process everything you're gonna see things and then it will",
    "start": "3248940",
    "end": "3256320"
  },
  {
    "text": "give you an error message it says oh we've hit the threshold sorry something went wrong let's just discard all of",
    "start": "3256320",
    "end": "3264090"
  },
  {
    "text": "these results that have been piling up on your screen so that's kind of",
    "start": "3264090",
    "end": "3270270"
  },
  {
    "text": "disappointing because typically the error message does not tell you why the values were bad it usually just says",
    "start": "3270270",
    "end": "3276540"
  },
  {
    "text": "well good luck with that you got a hundred billion row file you can use",
    "start": "3276540",
    "end": "3282090"
  },
  {
    "text": "grep or something right okay so we've",
    "start": "3282090",
    "end": "3288030"
  },
  {
    "text": "got that table we've created the table and I'm gonna show you here that I already have it in place in the OTP",
    "start": "3288030",
    "end": "3295730"
  },
  {
    "text": "database there's this external tables folder so you need management studio",
    "start": "3295730",
    "end": "3302160"
  },
  {
    "text": "2016 or later to view this but their external tables here's D biota second",
    "start": "3302160",
    "end": "3307530"
  },
  {
    "text": "baseman that was the table that I created I can right-click on it select it and it'll take a couple",
    "start": "3307530",
    "end": "3319349"
  },
  {
    "text": "seconds to spin up but it gave me back all of my records 777 of them so let's",
    "start": "3319349",
    "end": "3328500"
  },
  {
    "text": "run a couple comparisons I'm going to very quickly compare the sequel server",
    "start": "3328500",
    "end": "3336089"
  },
  {
    "text": "version using poly base to the sequel server version using linked server query",
    "start": "3336089",
    "end": "3344240"
  },
  {
    "text": "I'm going to turn on actual execution plans and I'm going to run these while",
    "start": "3344240",
    "end": "3350940"
  },
  {
    "text": "I'm running this let me note that this query is just a rewriting of the linked",
    "start": "3350940",
    "end": "3356039"
  },
  {
    "text": "server query that I showed you and what I'm trying to get at with this rewrite is trying to tell the optimizer that you",
    "start": "3356039",
    "end": "3364440"
  },
  {
    "text": "really want to push down on second",
    "start": "3364440",
    "end": "3370529"
  },
  {
    "text": "baseman you really want to filter to include just 50 records you really want",
    "start": "3370529",
    "end": "3376349"
  },
  {
    "text": "to do as much as you can against the remote resource and then I do the same",
    "start": "3376349",
    "end": "3386099"
  },
  {
    "text": "thing down here with dbo that second baseman so again that's connecting to a",
    "start": "3386099",
    "end": "3391680"
  },
  {
    "text": "poly based server or excuse me it's connecting using poly base to my Hadoop",
    "start": "3391680",
    "end": "3397920"
  },
  {
    "text": "server I get back the same number of records 50 for each I get back the fact",
    "start": "3397920",
    "end": "3403289"
  },
  {
    "text": "that Ben Zobrist is old and that he's probably making a big chunk of change",
    "start": "3403289",
    "end": "3410420"
  },
  {
    "text": "but what I what's most interesting to me",
    "start": "3410420",
    "end": "3415670"
  },
  {
    "text": "query cost 99% versus 1% and the major",
    "start": "3415819",
    "end": "3423240"
  },
  {
    "text": "chunk up from that is I have to pull all of the records from my link server and",
    "start": "3423240",
    "end": "3431089"
  },
  {
    "text": "then do all of the work in sequel server and then sort and then join to my top",
    "start": "3431089",
    "end": "3438210"
  },
  {
    "text": "salary by age table so notice the relative thickness of the lines fat line",
    "start": "3438210",
    "end": "3443579"
  },
  {
    "text": "fat line in line these are 700 some rows that's",
    "start": "3443579",
    "end": "3449670"
  },
  {
    "text": "50 rows by contrast down here my remote query I'm already filtering",
    "start": "3449670",
    "end": "3456960"
  },
  {
    "text": "out all but the 50 rows so technically I did not push down the predicates",
    "start": "3456960",
    "end": "3462630"
  },
  {
    "text": "technically I did not run a MapReduce job but even then I was still able to",
    "start": "3462630",
    "end": "3468810"
  },
  {
    "text": "take the data stream it down filter it down to the 50 rows before running the",
    "start": "3468810",
    "end": "3474270"
  },
  {
    "text": "sort and then joining to my table so I was still able to get a pretty big",
    "start": "3474270",
    "end": "3479970"
  },
  {
    "text": "benefit just from doing that and from also taking advantage of the better",
    "start": "3479970",
    "end": "3485070"
  },
  {
    "text": "statistics where hey we don't have 10,000 rows coming in we have 50 rows so",
    "start": "3485070",
    "end": "3494250"
  },
  {
    "text": "I have a quick comparison table if you're interested want to figure out which to use there are other methods as",
    "start": "3494250",
    "end": "3500430"
  },
  {
    "text": "well one method that I've really grown on is using Kafka which is a message",
    "start": "3500430",
    "end": "3506160"
  },
  {
    "text": "broker think of it like a service broker or Microsoft message queue or a rabbitmq",
    "start": "3506160",
    "end": "3511170"
  },
  {
    "text": "celery 0 mq one of literally dozens of other options but you can take that and",
    "start": "3511170",
    "end": "3517730"
  },
  {
    "text": "push messages individual messages from one source and draw them into the other",
    "start": "3517730",
    "end": "3524420"
  },
  {
    "text": "actually I have a whole talk on that there's a link to it in my slides so",
    "start": "3524420",
    "end": "3529680"
  },
  {
    "text": "wrapping everything up plenty of ways to integrate Hadoop with sequel server I recommend that you mix and match however",
    "start": "3529680",
    "end": "3536460"
  },
  {
    "text": "suits your needs figure out what works best for you and go with it if you want to grab the slides the demos the code",
    "start": "3536460",
    "end": "3543810"
  },
  {
    "text": "links to additional resources there at CS more dot info slash on slash Hadoop SQL if you have any questions at all",
    "start": "3543810",
    "end": "3551040"
  },
  {
    "text": "please feel free to reach out to me my email address and twitter twitter handle are up here and i'm gonna be here all week so thanks everybody",
    "start": "3551040",
    "end": "3559460"
  },
  {
    "text": "you [Applause]",
    "start": "3560400",
    "end": "3564189"
  }
]