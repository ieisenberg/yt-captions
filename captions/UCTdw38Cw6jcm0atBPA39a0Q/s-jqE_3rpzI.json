[
  {
    "text": "okay so it started a little disclaimer first I know the abstract said node and",
    "start": "2550",
    "end": "9240"
  },
  {
    "text": "poster has the technique that I'm just going to be talking about is generally applicable to any our DBMS and there's",
    "start": "9240",
    "end": "15750"
  },
  {
    "text": "only a few points that are actually node specific here so with that out of the",
    "start": "15750",
    "end": "20760"
  },
  {
    "text": "way the situation that we developed this technique in is that were small business",
    "start": "20760",
    "end": "28050"
  },
  {
    "text": "to business enterprise web accessibility shop so we do a lot of testing and",
    "start": "28050",
    "end": "33780"
  },
  {
    "text": "remediation for clients who are much much larger than we are so in order to",
    "start": "33780",
    "end": "39570"
  },
  {
    "text": "sort of keep up with the needs of our clients we want to try to minimize our infrastructure and streamline our",
    "start": "39570",
    "end": "45149"
  },
  {
    "text": "processes as much as possible to do for example a multi-tenant trial server that we can later break people",
    "start": "45149",
    "end": "51329"
  },
  {
    "text": "out of and move them to full instances possibly to some analytics to see how people are actually using our applications but our problem is that",
    "start": "51329",
    "end": "58800"
  },
  {
    "text": "since our tenants are so much bigger than we are and have their own needs in their own wishes for actually managing",
    "start": "58800",
    "end": "66270"
  },
  {
    "text": "the pace of upgrades we can't do two software-as-a-service so that is out because one tenant may",
    "start": "66270",
    "end": "73740"
  },
  {
    "text": "want to work with an older version for as long as they want to and we're just trying to keep up and release new stuff",
    "start": "73740",
    "end": "79439"
  },
  {
    "text": "so the problem tries is sort of how do we have our cake and eat it too with regard to having a multi-tenant capable",
    "start": "79439",
    "end": "86630"
  },
  {
    "text": "setups that we can later pull data sets out of so fundamentally our problem is",
    "start": "86630",
    "end": "95039"
  },
  {
    "text": "we've got too many databases floating around out there at one database for tenant we have dozens of clients those",
    "start": "95039",
    "end": "103200"
  },
  {
    "text": "databases it's just it's a lot to manage and so any way that we can reduce this and bring these together and consolidate",
    "start": "103200",
    "end": "110130"
  },
  {
    "text": "as much as possible that's just less to have floating around it's less to deal with a trust this matters because of",
    "start": "110130",
    "end": "117450"
  },
  {
    "text": "time money and effort we are a pretty small company especially relative to our",
    "start": "117450",
    "end": "122819"
  },
  {
    "text": "clients so the less time we spend juggling databases configuring things",
    "start": "122819",
    "end": "128130"
  },
  {
    "text": "Rhian dexing doing maintenance the more time we have to actually develop software",
    "start": "128130",
    "end": "133710"
  },
  {
    "text": "money I'm perfectly happy to spin up and in four large and just leave it out there for coupla months but other people",
    "start": "133710",
    "end": "139570"
  },
  {
    "text": "seem to object to it so if we can just pull these together have fewer databases",
    "start": "139570",
    "end": "145120"
  },
  {
    "text": "we have less maintenance costs less server costs and it's just it's a lot to",
    "start": "145120",
    "end": "150700"
  },
  {
    "text": "deal with - just juggle all these remember where everybody is and be able to pull stuff",
    "start": "150700",
    "end": "156580"
  },
  {
    "text": "in and out of their databases and manage separate versions so we can have a",
    "start": "156580",
    "end": "162610"
  },
  {
    "text": "multi-tenant application with one of three possible database architectures we",
    "start": "162610",
    "end": "169360"
  },
  {
    "text": "can do separate databases one per tenant which from a data perspective this is",
    "start": "169360",
    "end": "175000"
  },
  {
    "text": "not an improvement it's the status quo we want to be able to do better than that if at all possible it's sort of our",
    "start": "175000",
    "end": "181090"
  },
  {
    "text": "last resort next option is a little bit less overhead is to have a single",
    "start": "181090",
    "end": "186940"
  },
  {
    "text": "database and separate schemas per Kennet this is better for us because we have",
    "start": "186940",
    "end": "192630"
  },
  {
    "text": "fewer things floating around but sorry another thing is that it does allow version independence you can have one",
    "start": "192630",
    "end": "199480"
  },
  {
    "text": "schema app version 1 1 once game version 1 2 and so on and so forth but it's got",
    "start": "199480",
    "end": "205050"
  },
  {
    "text": "still a lot of application security overhead you have to have different users and manage access privileges to",
    "start": "205050",
    "end": "211060"
  },
  {
    "text": "different schemas just like you would have for the databases you have to juggle connection pools which is bit of",
    "start": "211060",
    "end": "216730"
  },
  {
    "text": "pain and analytics are still pretty difficult if we want to do those because you have to stitch together result set",
    "start": "216730",
    "end": "222220"
  },
  {
    "text": "of separate queries for separate schemas the third option is partitioned tables",
    "start": "222220",
    "end": "227770"
  },
  {
    "text": "which is it's actually pretty great it's got the lowest server overhead security",
    "start": "227770",
    "end": "233380"
  },
  {
    "text": "and access becomes strictly an application concern there's only a single connection analytics everything's",
    "start": "233380",
    "end": "239680"
  },
  {
    "text": "just right there so if at all possible we want to do this so if you get started",
    "start": "239680",
    "end": "246010"
  },
  {
    "text": "designing a schema for a partition table structure this is sort of a reduced",
    "start": "246010",
    "end": "251410"
  },
  {
    "text": "version of one of our accessibility testing products so we have a tenant",
    "start": "251410",
    "end": "257049"
  },
  {
    "text": "table where we have a single broker ten-inch that we're looking at each",
    "start": "257049",
    "end": "262060"
  },
  {
    "text": "tenant have users and tests which are related by a junction table users have devices which they used to",
    "start": "262060",
    "end": "269460"
  },
  {
    "text": "perform the tests and raise issues so the problem for us with this schema is",
    "start": "269460",
    "end": "276870"
  },
  {
    "text": "that how do we work at the level of individual tenant datasets so the tenant",
    "start": "276870",
    "end": "282510"
  },
  {
    "text": "row and their related roads in these other tables so we have tenants in a",
    "start": "282510",
    "end": "287850"
  },
  {
    "text": "shared database if one of them buys a standalone license like we had and insurance industry client moved to",
    "start": "287850",
    "end": "294060"
  },
  {
    "text": "standalone server recently so we need to be able to pull their data out and just give it to them in a tiny little package",
    "start": "294060",
    "end": "300210"
  },
  {
    "text": "without everybody else's stuff coming along for the ride because they probably don't want anybody else to see just what",
    "start": "300210",
    "end": "306750"
  },
  {
    "text": "sort of shape their sites are in so most of you probably know sequel server this",
    "start": "306750",
    "end": "313200"
  },
  {
    "text": "is a Postgres dump script this is a fairly standard way to throw data around",
    "start": "313200",
    "end": "319970"
  },
  {
    "text": "Postgres has PD dump my sequel has my sequel dump sequel server you got to go",
    "start": "319970",
    "end": "325260"
  },
  {
    "text": "through a couple of context menus and wizards and there's the generate scripts option Oracle is the exception here it's",
    "start": "325260",
    "end": "331080"
  },
  {
    "text": "got a binary dump format but what it's doing here is saying to copy into this",
    "start": "331080",
    "end": "336510"
  },
  {
    "text": "table these these rows which are tab separated and it goes field by field so",
    "start": "336510",
    "end": "344250"
  },
  {
    "text": "if we have something like this for our entire database can we generate that",
    "start": "344250",
    "end": "350300"
  },
  {
    "text": "something it only has the data for the tenant that we're interested in so ways",
    "start": "350300",
    "end": "358050"
  },
  {
    "text": "to do this the first thing we can do is just attack it with a blunt instrument and just go for brute force just pull",
    "start": "358050",
    "end": "365130"
  },
  {
    "text": "our data out as we can especially that",
    "start": "365130",
    "end": "371790"
  },
  {
    "text": "we've given the schema we just take it from the top you go to the tenant table",
    "start": "371790",
    "end": "377610"
  },
  {
    "text": "because that's going to be our entry point we pull out the row that we're interested in and we just pop it out",
    "start": "377610",
    "end": "383790"
  },
  {
    "text": "into our output moving on from there we go to the users table pull out the users",
    "start": "383790",
    "end": "392070"
  },
  {
    "text": "related to that tenant dump them out to the output from their devices and we start to see a",
    "start": "392070",
    "end": "400279"
  },
  {
    "text": "pattern here where the further you get from the entry point the more complex your queries are so your exporter",
    "start": "400279",
    "end": "407149"
  },
  {
    "text": "complexity is going to be essentially tracking schema complexity the more complex your schema the more complex",
    "start": "407149",
    "end": "412550"
  },
  {
    "text": "your exporter which becomes also especially problematic if you have something very complex where you have",
    "start": "412550",
    "end": "419659"
  },
  {
    "text": "converging schemas for instance if devices can be accessed through another table say tests you have to get not just",
    "start": "419659",
    "end": "426499"
  },
  {
    "text": "all devices which are linked to tests but all devices which are owned by users and you have to do a lot of singing to",
    "start": "426499",
    "end": "432860"
  },
  {
    "text": "figure out okay how do I get all the devices for this tenant through any or all paths that we could have them you",
    "start": "432860",
    "end": "441289"
  },
  {
    "text": "can also track intermediary IDs and use in lists to pull out these data that",
    "start": "441289",
    "end": "446719"
  },
  {
    "text": "it's not really much better another major problem is that it's very fragile if you make any major changes to your",
    "start": "446719",
    "end": "452899"
  },
  {
    "text": "schema you have to come back in here and do some maintenance and update your exporter you can ameliorate this to some",
    "start": "452899",
    "end": "458599"
  },
  {
    "text": "extent by automating the columns part of",
    "start": "458599",
    "end": "463879"
  },
  {
    "text": "it by pulling out those from the result data set but if you add a table drop a table rename something juggle",
    "start": "463879",
    "end": "470569"
  },
  {
    "text": "relationships a bit you're going to be back in here doing work and it's thankless and crappy unfortunately we",
    "start": "470569",
    "end": "481519"
  },
  {
    "text": "can approach this a little bit better in a better way if we go back a little bit and have a sort of discrete math",
    "start": "481519",
    "end": "487699"
  },
  {
    "text": "refresher so a graph is fundamentally a set of vertices connected by edges",
    "start": "487699",
    "end": "493990"
  },
  {
    "text": "definition of a vertex or an edge is abstract to greater or lesser degree but",
    "start": "493990",
    "end": "499309"
  },
  {
    "text": "it should be consistent and that you should be talking about the same sort of thing when you talk about a vertex or an",
    "start": "499309",
    "end": "504439"
  },
  {
    "text": "edge there's lots of things that are represented as graphs in the wild",
    "start": "504439",
    "end": "509619"
  },
  {
    "text": "everything from hiking trails could be a graph where the path is an edge and stations belong at our vertices network",
    "start": "509619",
    "end": "517550"
  },
  {
    "text": "topology diagrams our graphs pair straight up where your hardware your routers switches were stations those are",
    "start": "517550",
    "end": "525620"
  },
  {
    "text": "vertices communications going back and forth between them or edges also if you shopped on Amazon or Zappos",
    "start": "525620",
    "end": "532130"
  },
  {
    "text": "you can frequently see sort of people who purchase this product also bought this that the other behind the scenes",
    "start": "532130",
    "end": "539300"
  },
  {
    "text": "that's a graph products are vertices and the act of purchasing or having multiple purchases in somebody's account is an",
    "start": "539300",
    "end": "546170"
  },
  {
    "text": "edge there's a specific sort of graph that were interested in though which is",
    "start": "546170",
    "end": "551509"
  },
  {
    "text": "a digraph or directed graph the only difference here is that edges have directionality so for here there's an",
    "start": "551509",
    "end": "558709"
  },
  {
    "text": "edge going from A to B there is no corresponding edge going from B to a in the other direction",
    "start": "558709",
    "end": "564279"
  },
  {
    "text": "so some examples here would be stuff like social media follows we follow somebody on Twitter they don't",
    "start": "564279",
    "end": "570050"
  },
  {
    "text": "necessarily follow you back so that is constitutes a directed graph between you and that other user package dependencies",
    "start": "570050",
    "end": "578300"
  },
  {
    "text": "are represented as digraphs in everything from NPM to maven you get and",
    "start": "578300",
    "end": "587500"
  },
  {
    "text": "stuff like rpm or the arch repository those generally come in as trees which",
    "start": "587500",
    "end": "594709"
  },
  {
    "text": "is sort of a special case of a digraph where you don't have convergence so B to D and C to D if you pull those out you",
    "start": "594709",
    "end": "601220"
  },
  {
    "text": "have a tree with the route a it's that's",
    "start": "601220",
    "end": "608480"
  },
  {
    "text": "a better so but crucially for us the",
    "start": "608480",
    "end": "613519"
  },
  {
    "text": "another example of a diagraph in the wild is for T's a data bases relationships make up a directed graph",
    "start": "613519",
    "end": "621220"
  },
  {
    "text": "so if we can somehow represent our schema as a digraph we can start to",
    "start": "621220",
    "end": "627260"
  },
  {
    "text": "manipulate it and do things with it to start pulling out related data so if we",
    "start": "627260",
    "end": "634069"
  },
  {
    "text": "go we have to first define what our vertices and what our edges are going to be our vertices is pretty obvious it's a",
    "start": "634069",
    "end": "640370"
  },
  {
    "text": "table or more specifically a set of data within a table or set of rows so for us",
    "start": "640370",
    "end": "645920"
  },
  {
    "text": "we're just going to say that every table has to have a primary key constraint so that we can query it consistently it's includes junction tables which are",
    "start": "645920",
    "end": "653870"
  },
  {
    "text": "going to be vertices in their own right even though in E RDS they're frequently elided you just have one edge going",
    "start": "653870",
    "end": "659660"
  },
  {
    "text": "between the two tables that are related by the menu to relationship so our vertex is going to",
    "start": "659660",
    "end": "666500"
  },
  {
    "text": "be a table name and a collection of primary Q values our edges are going to",
    "start": "666500",
    "end": "672380"
  },
  {
    "text": "be the relationship you might note that this is actually a little bit backwards the source is the reference column in",
    "start": "672380",
    "end": "679279"
  },
  {
    "text": "the relationship and the target is the referencing one you have to think about it in terms of dependency rather than",
    "start": "679279",
    "end": "685520"
  },
  {
    "text": "who owns which columns we also have to store the actual columns that make up",
    "start": "685520",
    "end": "690860"
  },
  {
    "text": "the key for example if it's possible to have multiple relationships between the same tables if our tests can have user",
    "start": "690860",
    "end": "698690"
  },
  {
    "text": "who created the test and user who's currently assigned to it we have to track both of those relationships in",
    "start": "698690",
    "end": "704810"
  },
  {
    "text": "order to guarantee that we have an entire picture of the data set so this",
    "start": "704810",
    "end": "713960"
  },
  {
    "text": "is a representation of our schema as a digraph so if we can start traversing",
    "start": "713960",
    "end": "718970"
  },
  {
    "text": "these relationships and just bounce around between them and just cover the entire graph this is not going to be any",
    "start": "718970",
    "end": "724880"
  },
  {
    "text": "kind of fancy pathfinding thing it's just let's visit everywhere and pull up everything that we possibly can if you",
    "start": "724880",
    "end": "731930"
  },
  {
    "text": "can get a connected subgraph so some rows from this table some herbs from this table summers from that table all",
    "start": "731930",
    "end": "739100"
  },
  {
    "text": "related to that gives us a single tenants entire data set in isolation that we can work with can actually get",
    "start": "739100",
    "end": "746990"
  },
  {
    "text": "this we need to start creating the metadata of our database the ansi",
    "start": "746990",
    "end": "752029"
  },
  {
    "text": "standard defines an information schema which allows you to query your structure",
    "start": "752029",
    "end": "757700"
  },
  {
    "text": "in your metadata just as you query your other tables and your actual tables and",
    "start": "757700",
    "end": "762709"
  },
  {
    "text": "views everybody implements it to varying degrees of conformance these are often",
    "start": "762709",
    "end": "768380"
  },
  {
    "text": "views on implementation specific tables postgrads has the p.g catalog sequel server has a system catalog where you",
    "start": "768380",
    "end": "774740"
  },
  {
    "text": "got the civils or whatever sorry but",
    "start": "774740",
    "end": "780440"
  },
  {
    "text": "press there's three specific items were interested in table constraints gives us",
    "start": "780440",
    "end": "785540"
  },
  {
    "text": "all the primary keys in the entire database referential constraints to the same for foreign keys and finally key",
    "start": "785540",
    "end": "792320"
  },
  {
    "text": "column usage tells us for a certain key which columns actually get up in a given table so the next step",
    "start": "792320",
    "end": "801740"
  },
  {
    "text": "for us is going to be actually building our sub graph but you're going to have to do through a fairly involved recursive process so we start obviously",
    "start": "801740",
    "end": "812180"
  },
  {
    "text": "at the entry point to the side out here we could start anywhere we just need a",
    "start": "812180",
    "end": "819170"
  },
  {
    "text": "table name and a primary key value and we can pull out any data that's somehow",
    "start": "819170",
    "end": "825530"
  },
  {
    "text": "somewhere related to that row in the table that is easiest because we know the tenants that's going to be sort of",
    "start": "825530",
    "end": "831650"
  },
  {
    "text": "our obvious entry point just go as a tenant ID and our process is going to be",
    "start": "831650",
    "end": "837860"
  },
  {
    "text": "we add first the primary key information to the vertex then we start scanning for foreign keys that involve",
    "start": "837860",
    "end": "843950"
  },
  {
    "text": "that vertex trust there's two of them there's users and tests both have a",
    "start": "843950",
    "end": "849350"
  },
  {
    "text": "tenant ID the order in which we take these is completely arbitrary it's easiest for us to go just alphabetical",
    "start": "849350",
    "end": "855980"
  },
  {
    "text": "by source for consistency sake so we start creating an edge but we don't know",
    "start": "855980",
    "end": "862430"
  },
  {
    "text": "what the primary key values are on the other side of it so we have to actually",
    "start": "862430",
    "end": "868550"
  },
  {
    "text": "recurse along the edge as we're building it to get the vertex information to the other side and complete the edge so we",
    "start": "868550",
    "end": "875570"
  },
  {
    "text": "do that we go to tests add the test priority information to that vertex we finish the edge and we start do the same",
    "start": "875570",
    "end": "883670"
  },
  {
    "text": "thing all over again scan for foreign keys involving tests the first one for us is going to be issues with our scheme",
    "start": "883670",
    "end": "891130"
  },
  {
    "text": "so we do the same thing start building the edge we recurse we've finished the edge by adding the primary keys to the",
    "start": "891130",
    "end": "897530"
  },
  {
    "text": "issues vertex and then there's only the foreign key back up to tests and this is",
    "start": "897530",
    "end": "902720"
  },
  {
    "text": "the first time that we hit our recursive base case which is we're not adding any new information at all because the edge",
    "start": "902720",
    "end": "908780"
  },
  {
    "text": "direction guarantees that any issues they're going to belong to tests that we already covered so instead of recursing",
    "start": "908780",
    "end": "916070"
  },
  {
    "text": "where you roll back up to tests and then the next foreign key is actually going to be device ID from tests so this is",
    "start": "916070",
    "end": "924440"
  },
  {
    "text": "the first edge that we Traverse from the target that works exactly the same press we process the vertex",
    "start": "924440",
    "end": "930350"
  },
  {
    "text": "the same way the only real difference is that because technically because test",
    "start": "930350",
    "end": "935660"
  },
  {
    "text": "has device ID in it we already know the primary keys but it's really easiest just to treat it exactly the same way",
    "start": "935660",
    "end": "943600"
  },
  {
    "text": "so after devices we recurse to users and same deal we add the vertex at the",
    "start": "946540",
    "end": "953960"
  },
  {
    "text": "primary keys but scanning the primary cues does reveal that we could have incomplete information because we came",
    "start": "953960",
    "end": "960920"
  },
  {
    "text": "to devices from tests there could be devices that users have that they haven't started testing this",
    "start": "960920",
    "end": "966230"
  },
  {
    "text": "at this point so we don't necessarily have all the devices for that tenant",
    "start": "966230",
    "end": "972020"
  },
  {
    "text": "even though we have all of them for that tenants test so we actually window for",
    "start": "972020",
    "end": "977060"
  },
  {
    "text": "cursing back to devices which brings us to another sort of edge case here pun",
    "start": "977060",
    "end": "984020"
  },
  {
    "text": "not intended or you can have multiple vertices existing for the same table if",
    "start": "984020",
    "end": "990290"
  },
  {
    "text": "we go back to devices because I'm going to have devices that overlap of users",
    "start": "990290",
    "end": "996800"
  },
  {
    "text": "have them seven test some Naughton tests so we'll just consolidate those in order to make sure that we don't have the same",
    "start": "996800",
    "end": "1003250"
  },
  {
    "text": "row in devices appearing multiple times in the directed graph or multiple",
    "start": "1003250",
    "end": "1008350"
  },
  {
    "text": "vertices which would then cause them to appear multiple times in our output because if we do that we try to insert",
    "start": "1008350",
    "end": "1014890"
  },
  {
    "text": "the same thing multiple times crashes and burns and it's useless to us so in",
    "start": "1014890",
    "end": "1021400"
  },
  {
    "text": "complex schemas however you could have rows that to get to four multiple paths that do not overlap in that case you can",
    "start": "1021400",
    "end": "1028630"
  },
  {
    "text": "have multiple vertices this is looks a little bit weird if you have the same",
    "start": "1028630",
    "end": "1034079"
  },
  {
    "text": "block of inserts or copy statements for the same table multiple times but it's",
    "start": "1034080",
    "end": "1039459"
  },
  {
    "text": "harmless because the same data does not appear in both so all users devices does",
    "start": "1039460",
    "end": "1046300"
  },
  {
    "text": "overlap so we merge the new entries into the existing vertex and then we get the base case again which is that there",
    "start": "1046300",
    "end": "1052240"
  },
  {
    "text": "aren't going to be any new tests because the only devices that we've added aren't being used in tests so we roll back up",
    "start": "1052240",
    "end": "1059110"
  },
  {
    "text": "to users and then just like with devices where initially only got those that are",
    "start": "1059110",
    "end": "1064330"
  },
  {
    "text": "used in tests since we're only looking at the current vertex and its neighbors the entry point is really it's just",
    "start": "1064330",
    "end": "1070419"
  },
  {
    "text": "another table so we could theoretically have another tenant connected to some of the users that we just picked up",
    "start": "1070419",
    "end": "1075880"
  },
  {
    "text": "we shouldn't be really really terrible if we did because that would mean we",
    "start": "1075880",
    "end": "1081130"
  },
  {
    "text": "have some sort of pollution going between individual tenants data so it's connected we don't want that but we have",
    "start": "1081130",
    "end": "1087700"
  },
  {
    "text": "to just go back up and check but also you could have users that don't have",
    "start": "1087700",
    "end": "1093220"
  },
  {
    "text": "devices so we need to go back double back up here and pull out all the users related to that tenant and just make",
    "start": "1093220",
    "end": "1100360"
  },
  {
    "text": "sure that we visited every row in that vertex so if we do that and blast table",
    "start": "1100360",
    "end": "1108190"
  },
  {
    "text": "that we haven't visited is the junction table so we go in there even though",
    "start": "1108190",
    "end": "1114130"
  },
  {
    "text": "there's the compound key it's the same deal for us we are already just building",
    "start": "1114130",
    "end": "1119230"
  },
  {
    "text": "the edge getting the full primary key of the junction table out adding that to",
    "start": "1119230",
    "end": "1124510"
  },
  {
    "text": "the vertex do many-to-many relationship does imply that it's possible to have a test that is not related to a tenant so",
    "start": "1124510",
    "end": "1131950"
  },
  {
    "text": "we have to go back there just to make sure and at that point we mapped out the entire connected subgraph we have all",
    "start": "1131950",
    "end": "1138639"
  },
  {
    "text": "the rows present for that tenant in our vertices if they write note that we",
    "start": "1138639",
    "end": "1146350"
  },
  {
    "text": "haven't base case all the way back up to tenants and recurse to users there's no need for that because we've actually",
    "start": "1146350",
    "end": "1151870"
  },
  {
    "text": "already visited users from tenants so we have all the data that we need there is",
    "start": "1151870",
    "end": "1157600"
  },
  {
    "text": "one caveat here these will mess you up if you have for instance the JIRA style",
    "start": "1157600",
    "end": "1163799"
  },
  {
    "text": "scheme where impact can be such trivial moderate minor serious critical blocker",
    "start": "1163799",
    "end": "1172230"
  },
  {
    "text": "you have a lookup table is common pattern for approaching this where you",
    "start": "1172230",
    "end": "1177789"
  },
  {
    "text": "have just a table this Keys mapped to the values for the impact or any other",
    "start": "1177789",
    "end": "1183549"
  },
  {
    "text": "situation where you have sort of an enumeration if you do that you can start",
    "start": "1183549",
    "end": "1189039"
  },
  {
    "text": "coming in from one tenant curse into the issue impacts table and then you start",
    "start": "1189039",
    "end": "1195159"
  },
  {
    "text": "looking for any issues that have packs with those primary keys and suddenly you've got your entire data set",
    "start": "1195159",
    "end": "1200680"
  },
  {
    "text": "out there and you don't want that in Postgres we've set for decides that the",
    "start": "1200680",
    "end": "1207340"
  },
  {
    "text": "question entirely we use enumeration types which are a custom type that you can create that converts a string",
    "start": "1207340",
    "end": "1214800"
  },
  {
    "text": "visible value into a number in the back end so it's more efficient for indexing",
    "start": "1214800",
    "end": "1219900"
  },
  {
    "text": "but for other DBMS is that don't offer that functionality you would have to",
    "start": "1219900",
    "end": "1225520"
  },
  {
    "text": "look at defining some sort of stopped able to say if you reach this table",
    "start": "1225520",
    "end": "1230950"
  },
  {
    "text": "don't recurse from it just assume you've hit the base case and pull back out so",
    "start": "1230950",
    "end": "1237250"
  },
  {
    "text": "at this point we have the data set how do we actually convert that into a",
    "start": "1237250",
    "end": "1242680"
  },
  {
    "text": "script that has the single tenant data that we can pull out into somebody else's database so type a sort is sort",
    "start": "1242680",
    "end": "1253750"
  },
  {
    "text": "of a graph very function where you order vertices by dependency this is how",
    "start": "1253750",
    "end": "1259390"
  },
  {
    "text": "package managers decide what order to install things in for example the way it",
    "start": "1259390",
    "end": "1267640"
  },
  {
    "text": "works is that you start by looking at the vertices which are not targeted by",
    "start": "1267640",
    "end": "1272950"
  },
  {
    "text": "any edges so we're going to start with tenants we're going to query the full rows from tenants prevented scripts",
    "start": "1272950",
    "end": "1280480"
  },
  {
    "text": "those out to our output which stress is going to be standard out if you use",
    "start": "1280480",
    "end": "1286660"
  },
  {
    "text": "Visual Studio the console because from a command-line application what we can do is we can take standard out pipe it out",
    "start": "1286660",
    "end": "1293860"
  },
  {
    "text": "into a file or put it directly into the input of another command so poll tenants",
    "start": "1293860",
    "end": "1301450"
  },
  {
    "text": "ouch we remove the vertices and edges that we just looked at and then we just repeat",
    "start": "1301450",
    "end": "1308140"
  },
  {
    "text": "that same process until we have nothing left in the graph at all there's one",
    "start": "1308140",
    "end": "1314440"
  },
  {
    "text": "possible scenario could hit which is that if you have a cycle anywhere in your graph you're basically screwed the",
    "start": "1314440",
    "end": "1321250"
  },
  {
    "text": "good news is you have to really really try to get this specifically you have to at some point stop enforcing your",
    "start": "1321250",
    "end": "1327100"
  },
  {
    "text": "foreign key constraints because the only way to insert or the owner to generate a cycle is to insert data before data it depends on so",
    "start": "1327100",
    "end": "1335290"
  },
  {
    "text": "you can insert one row here and another row and then turn on foreign key constraints and suddenly you have a",
    "start": "1335290",
    "end": "1342640"
  },
  {
    "text": "referential integrity problem because you have a cycle going between the two and you don't know which is the source",
    "start": "1342640",
    "end": "1348460"
  },
  {
    "text": "and which is the target let's know to get started with the turbo",
    "start": "1348460",
    "end": "1353620"
  },
  {
    "text": "sort we take our tenants if you just generate that copy statement and push it",
    "start": "1353620",
    "end": "1360280"
  },
  {
    "text": "to our output that precept users which we treat the same way pull it out type",
    "start": "1360280",
    "end": "1368290"
  },
  {
    "text": "it at the output the devices work similarly the one that frees up for us",
    "start": "1368290",
    "end": "1378010"
  },
  {
    "text": "the tests which you might notice that this is not a JavaScript date it's a post-grad date Azra polling this",
    "start": "1378010",
    "end": "1385900"
  },
  {
    "text": "information out the driver that we're using and this is the one specific thing for node the PG driver includes column",
    "start": "1385900",
    "end": "1394270"
  },
  {
    "text": "type information so we can use that to pass our results out to a formatter and",
    "start": "1394270",
    "end": "1400050"
  },
  {
    "text": "pipes and through there on the way to the output if you don't have the type metadata and your results you can use",
    "start": "1400050",
    "end": "1407110"
  },
  {
    "text": "information schema columns table or dia and that includes type information you can just correlate with your output and",
    "start": "1407110",
    "end": "1414540"
  },
  {
    "text": "make sure that you have your results coming out in the format that your import will expect so the last two",
    "start": "1414540",
    "end": "1422710"
  },
  {
    "text": "tables for us with all our edges and other vertices gone our issues and user tests the ordering doesn't matter here",
    "start": "1422710",
    "end": "1428440"
  },
  {
    "text": "at the same level so we just type those out in whichever order so and great",
    "start": "1428440",
    "end": "1438130"
  },
  {
    "text": "talking about it but we actually try it out I've created the example schema that",
    "start": "1438130",
    "end": "1444160"
  },
  {
    "text": "I've been using I have a dataset generated for that so we can actually",
    "start": "1444160",
    "end": "1449820"
  },
  {
    "text": "pull that out",
    "start": "1449820",
    "end": "1453149"
  },
  {
    "text": "it's kind of retyping on the back screen so all the databases that we have",
    "start": "1455980",
    "end": "1462130"
  },
  {
    "text": "Arachne is the name of the application it's after okay it's after the Greek",
    "start": "1462130",
    "end": "1468230"
  },
  {
    "text": "Weaver in mythology who had the bad idea to challenge Athena to contest I got turned into a spider for troubles um",
    "start": "1468230",
    "end": "1476299"
  },
  {
    "text": "so please and psql is the command-line client for postgrads that will let us",
    "start": "1476299",
    "end": "1483139"
  },
  {
    "text": "actually query the data and look at what we actually have in there so here's all",
    "start": "1483139",
    "end": "1491120"
  },
  {
    "text": "the tables how you look familiar got the same set i've got data for three tenants in there so Acme's list the",
    "start": "1491120",
    "end": "1501590"
  },
  {
    "text": "tasks like to place and they've all got users you've got users for every single",
    "start": "1501590",
    "end": "1508580"
  },
  {
    "text": "one actually came out more or less the same order there's a bit of a mix-up there which good so exit out of there",
    "start": "1508580",
    "end": "1517940"
  },
  {
    "text": "and actually run or acne and it tells us",
    "start": "1517940",
    "end": "1523759"
  },
  {
    "text": "how to use it pass in connection options we give it the name of an entry point",
    "start": "1523759",
    "end": "1528860"
  },
  {
    "text": "table and a primary key value",
    "start": "1528860",
    "end": "1532870"
  },
  {
    "text": "like so turns through a little bit and",
    "start": "1538240",
    "end": "1543530"
  },
  {
    "text": "then generates all of our copy statements in the order we expect so it",
    "start": "1543530",
    "end": "1548690"
  },
  {
    "text": "starts with tenants move on to users and devices pulls everything out in the",
    "start": "1548690",
    "end": "1556400"
  },
  {
    "text": "dependency order that we were looking at with TOFA sergeant so if we pass it a",
    "start": "1556400",
    "end": "1565640"
  },
  {
    "text": "different primary key we got another tenants data we get Cespedes here so I",
    "start": "1565640",
    "end": "1574340"
  },
  {
    "text": "make sure I dropped that first so now",
    "start": "1574340",
    "end": "1581480"
  },
  {
    "text": "given this output if I create a new database and apply the schema because this is a data only export I can I have",
    "start": "1581480",
    "end": "1593240"
  },
  {
    "text": "the schema generation statements already in a file that I can just run against this database as we'll create to set up",
    "start": "1593240",
    "end": "1599180"
  },
  {
    "text": "that we're looking at and then",
    "start": "1599180",
    "end": "1606340"
  },
  {
    "text": "I can run Arachne again and because it's outputting to standard out I can use the",
    "start": "1609050",
    "end": "1615050"
  },
  {
    "text": "pipe to just hook the output of Arachne into the input of psql and it will run",
    "start": "1615050",
    "end": "1621130"
  },
  {
    "text": "the statements in PDP database I can see",
    "start": "1621130",
    "end": "1627290"
  },
  {
    "text": "they're just copied I'm standing away it copied everything out that we're looking",
    "start": "1627290",
    "end": "1632960"
  },
  {
    "text": "for and if we go back into P SQL into the new database again so if you only",
    "start": "1632960",
    "end": "1641510"
  },
  {
    "text": "have the single tenant now and single tenants users should show up so there",
    "start": "1641510",
    "end": "1651170"
  },
  {
    "text": "you have that and for the mouse come",
    "start": "1651170",
    "end": "1660950"
  },
  {
    "text": "back here so this is on github I'll get to that a little bit but tress what this",
    "start": "1660950",
    "end": "1667160"
  },
  {
    "text": "is allowed us to do is to treat you such-and-such data set as a logical unit that we can work with so we can backup",
    "start": "1667160",
    "end": "1673870"
  },
  {
    "text": "individual tenants and this is the thing that's made it possible for us to run this the partition table setups as",
    "start": "1673870",
    "end": "1680120"
  },
  {
    "text": "opposed to a multiple schema set up which makes things just makes my life a lot easier which I'm always in favor of",
    "start": "1680120",
    "end": "1688180"
  },
  {
    "text": "so we could have a situation where we have all three tenants in a single",
    "start": "1688180",
    "end": "1694010"
  },
  {
    "text": "database and turn it into that I just pull out extract a single data set put",
    "start": "1694010",
    "end": "1701030"
  },
  {
    "text": "it anywhere we want so we can restore them we can back them up on independent schedules we can keep independent",
    "start": "1701030",
    "end": "1707240"
  },
  {
    "text": "backups so this has ramifications for removing tenants for data retention policies we're really just scratching",
    "start": "1707240",
    "end": "1714620"
  },
  {
    "text": "the surface of what we can do with this so far other thing other big selling",
    "start": "1714620",
    "end": "1720500"
  },
  {
    "text": "point here conforming to constraints is a big deal PD dumps orders by dependency",
    "start": "1720500",
    "end": "1726620"
  },
  {
    "text": "by default my sequel dump doesn't if you have a my sequel dump script it will",
    "start": "1726620",
    "end": "1733310"
  },
  {
    "text": "actually turn off your foreign key constraints run all the inserts and turn the constraints back on this is actually",
    "start": "1733310",
    "end": "1740780"
  },
  {
    "text": "kind of dangerous because if something goes wrong along then you're left in an inconsistent state with your foreign key",
    "start": "1740780",
    "end": "1746630"
  },
  {
    "text": "constraints off but ordering by dependency lets us take this scenario",
    "start": "1746630",
    "end": "1754250"
  },
  {
    "text": "that we had just from pulling PGP out and Regan for everybody else into that",
    "start": "1754250",
    "end": "1760160"
  },
  {
    "text": "database without downtime this is honestly more of a cool hypothetical than something we actually had to do yet",
    "start": "1760160",
    "end": "1766660"
  },
  {
    "text": "but being able to apply the output of Arachne into another live database means",
    "start": "1766660",
    "end": "1773930"
  },
  {
    "text": "we can offer things like teardown grades for shared servers there is another sort",
    "start": "1773930",
    "end": "1780020"
  },
  {
    "text": "of bonus usage which is as a schema validator if you dump out your database and restore a single tenant and you have",
    "start": "1780020",
    "end": "1787880"
  },
  {
    "text": "data missing then you know your foreign key constraints are not complete and you know more or less where to look it's",
    "start": "1787880",
    "end": "1793100"
  },
  {
    "text": "where you don't see any data so being",
    "start": "1793100",
    "end": "1801770"
  },
  {
    "text": "able to handle arbitrary schemas saves us from having to do export or maintenance we can take any data set",
    "start": "1801770",
    "end": "1809240"
  },
  {
    "text": "from anywhere we could use alternate entry points you know if for whatever reason we wanted to start with a",
    "start": "1809240",
    "end": "1815270"
  },
  {
    "text": "specific user and just pull out their entire entire data set we would wind up with exactly the same output in the same",
    "start": "1815270",
    "end": "1822350"
  },
  {
    "text": "order because we would visit the graph go back all the way up to tenants and then that would be the first one that",
    "start": "1822350",
    "end": "1828020"
  },
  {
    "text": "gets Topa sorted out so something else does let us do it is another developer",
    "start": "1828020",
    "end": "1835400"
  },
  {
    "text": "and I @bq have been working on a fairly major change recently it's been I want to say probably a couple months actually",
    "start": "1835400",
    "end": "1842050"
  },
  {
    "text": "we got rid of our users table everything about users was now supplied by key",
    "start": "1842050",
    "end": "1847610"
  },
  {
    "text": "cloak a single sign-on provider so this has been an absolutely huge code change for the application so we've been",
    "start": "1847610",
    "end": "1855260"
  },
  {
    "text": "tearing stuff out revising tests or revising the entire infrastructure to",
    "start": "1855260",
    "end": "1860660"
  },
  {
    "text": "handle users coming from somewhere else but this didn't have to change at all it",
    "start": "1860660",
    "end": "1866420"
  },
  {
    "text": "worked exactly the same way we had to do no maintenance at all so it's been great",
    "start": "1866420",
    "end": "1871910"
  },
  {
    "text": "because I would probably forgotten about this until the last minute and she would've been horrible so one of",
    "start": "1871910",
    "end": "1878940"
  },
  {
    "text": "the cool things about working at DQ is they let us get away with open sourcing or tooling so I put this up on github",
    "start": "1878940",
    "end": "1884580"
  },
  {
    "text": "it's sort of a reference implementation for the technique I want to say this is sort of a final note this doesn't make",
    "start": "1884580",
    "end": "1890369"
  },
  {
    "text": "the partition table strategy the automatic solution you could have requirements for medical or financial",
    "start": "1890369",
    "end": "1896369"
  },
  {
    "text": "records where you are required by law to segregate your tenant data or if you",
    "start": "1896369",
    "end": "1901469"
  },
  {
    "text": "have architectures where tenants need to be able to interact with each other then you're kind of stuck this is not going",
    "start": "1901469",
    "end": "1907259"
  },
  {
    "text": "to help you with those but for what we do where we have tenants that are completely apart and just living in the",
    "start": "1907259",
    "end": "1914070"
  },
  {
    "text": "same space without interacting or sharing any data means we can ignore the major pain points of the partition table",
    "start": "1914070",
    "end": "1920009"
  },
  {
    "text": "strategy almost entirely so it's me that a lot easier for us and that's it",
    "start": "1920009",
    "end": "1927950"
  },
  {
    "text": "[Applause]",
    "start": "1929060",
    "end": "1931990"
  }
]