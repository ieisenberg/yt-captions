[
  {
    "text": "Heather happens to be the dog mom of one of one of his best friends so he gets very excited whenever he sees",
    "start": "4279",
    "end": "10040"
  },
  {
    "text": "her um so I get the after lunch crowd uh which I know can be a little bit",
    "start": "10040",
    "end": "15960"
  },
  {
    "text": "difficult it's the last day um but I just ask for everyone to be as present as possible um and we'll get going so uh",
    "start": "15960",
    "end": "25800"
  },
  {
    "text": "in case you didn't read the agenda the elephant in your data set addressing bias in machine learning um typically",
    "start": "25800",
    "end": "32880"
  },
  {
    "text": "this talk covers classical machine learning models and bias injection within them I will cover a little bit of",
    "start": "32880",
    "end": "38920"
  },
  {
    "text": "more modern day um applications as well so hello my name is Michelle Frost",
    "start": "38920",
    "end": "46680"
  },
  {
    "text": "um I consult uh doing responsible AI out of accountable intelligence um by day",
    "start": "46680",
    "end": "52199"
  },
  {
    "text": "I'm also a senior developer at a design and Technology consultancy based in Kansas City Missouri um called Crema and",
    "start": "52199",
    "end": "59519"
  },
  {
    "text": "then I'm also Al a member of the center for practical bioethics ethical AI Council and most importantly uh Wilbur",
    "start": "59519",
    "end": "67040"
  },
  {
    "text": "is the star of the show um I like to generally acknowledge Him in that you can look at him for 12 seconds or so",
    "start": "67040",
    "end": "73799"
  },
  {
    "text": "occasionally my brain likes to reboot itself and should it do that he will come in and help",
    "start": "73799",
    "end": "81759"
  },
  {
    "text": "me so um this is a lot to get through and it's a bit of a hey topic uh and it",
    "start": "81840",
    "end": "87680"
  },
  {
    "text": "can be uncomfortable so I'm going to start just by acknowledging that and a couple disclaimers",
    "start": "87680",
    "end": "93960"
  },
  {
    "text": "um often you know we get to walk into a room and just be a technologist just be a developer just be a data scientist uh",
    "start": "93960",
    "end": "101399"
  },
  {
    "text": "but in this talk you also have to be a human so um sometimes I like to shake things up a",
    "start": "101399",
    "end": "108200"
  },
  {
    "text": "little bit and if you're close enough let's just take 20 seconds and be a human and if you're next to someone that",
    "start": "108200",
    "end": "113479"
  },
  {
    "text": "you haven't met before or you have met them um just give them a hello and",
    "start": "113479",
    "end": "120479"
  },
  {
    "text": "okay so this is a again a big topic right and it's honestly not not what we",
    "start": "131560",
    "end": "136760"
  },
  {
    "text": "can get through in an hour um so what we're going to do is basically a shallow but wide introduction and what I hope to",
    "start": "136760",
    "end": "143640"
  },
  {
    "text": "do is to just kind of give you an idea for the field of fairness and the things we're looking at and while we go through",
    "start": "143640",
    "end": "149560"
  },
  {
    "text": "this talk um there's going to be some examples at each step and these are real companies and I don't bleep out the name",
    "start": "149560",
    "end": "157599"
  },
  {
    "text": "of the company because number one uh instead of listening then you all would be Googling saying who did this um and",
    "start": "157599",
    "end": "164920"
  },
  {
    "text": "number two uh you know it's okay these are big companies some of you have",
    "start": "164920",
    "end": "170840"
  },
  {
    "text": "worked for them maybe with them we're all familiar with them the point is it can happen to everyone and we need to be",
    "start": "170840",
    "end": "178400"
  },
  {
    "text": "very aware of the problem be aware of how the problem presents itself so that we can take the steps to mitigate it",
    "start": "178400",
    "end": "184200"
  },
  {
    "text": "this is not to shame this is to fix um",
    "start": "184200",
    "end": "189920"
  },
  {
    "text": "and the biggest thing I'm going to stress here and I'll say this multiple times throughout the next hour this is",
    "start": "189920",
    "end": "195599"
  },
  {
    "text": "not just a technical issue this is a socio technical issue and what I mean by",
    "start": "195599",
    "end": "200840"
  },
  {
    "text": "that is as as Engineers as data scientists as technologist we can take a technical approach and we can mitigate",
    "start": "200840",
    "end": "208040"
  },
  {
    "text": "it but we're not going to solve it alone and I won't get on my um responsible AI",
    "start": "208040",
    "end": "214319"
  },
  {
    "text": "tangents but that does apply to the entire field so what's our agenda we're going",
    "start": "214319",
    "end": "219840"
  },
  {
    "text": "to go through a little exercise maybe make some people uncomfortable that's okay um and then we're going to talk",
    "start": "219840",
    "end": "226519"
  },
  {
    "text": "about bias what is bias right what do we mean when we talk about it in a social",
    "start": "226519",
    "end": "231879"
  },
  {
    "text": "context in a human context in a statistical context in a machine learning context when does it become",
    "start": "231879",
    "end": "238760"
  },
  {
    "text": "harmful and throughout this um I'm going to pair each of these uh with different examples",
    "start": "238760",
    "end": "245720"
  },
  {
    "text": "of harmful bias it's not always just what you think it might not just be the data set could be different sampling",
    "start": "245720",
    "end": "252319"
  },
  {
    "text": "methods for example uh and then we're going to talk about how do we measure and quantify it okay how do we translate",
    "start": "252319",
    "end": "260560"
  },
  {
    "text": "this social problem into a mathematical and Technical problem so that then we",
    "start": "260560",
    "end": "265880"
  },
  {
    "text": "can start deriving approaches and then um how can we manage",
    "start": "265880",
    "end": "271479"
  },
  {
    "text": "it right as again technologists individually but um as technologists",
    "start": "271479",
    "end": "277000"
  },
  {
    "text": "collectively and as humans as a group so what's our goal today um I want",
    "start": "277000",
    "end": "283680"
  },
  {
    "text": "everyone to be able to walk out of this room and feel empowered to be an active participant in discussions of bias and",
    "start": "283680",
    "end": "290880"
  },
  {
    "text": "AI systems",
    "start": "290880",
    "end": "294240"
  },
  {
    "text": "so let's start with our exercise um before I flip to this next slide I'm going to give a couple",
    "start": "302000",
    "end": "307880"
  },
  {
    "text": "caveats uh this these next little pieces are observed in the USA so the context",
    "start": "307880",
    "end": "315800"
  },
  {
    "text": "of this is going to differ across different cultures across different countries um and I'm not going to ask",
    "start": "315800",
    "end": "322440"
  },
  {
    "text": "anybody to raise your hand here but a lot of times when we think of privilege the first thing that might come to mind",
    "start": "322440",
    "end": "328400"
  },
  {
    "text": "is skin color um and while that is a section here there are many spaces of privilege that",
    "start": "328400",
    "end": "335759"
  },
  {
    "text": "we can occupy and privilege isn't meant to be this isn't meant to be a shame if",
    "start": "335759",
    "end": "342280"
  },
  {
    "text": "you find yourself included in a space of power in a category of privilege but",
    "start": "342280",
    "end": "348600"
  },
  {
    "text": "what we have to know is that if we hold power in a space then we might have a blind",
    "start": "348600",
    "end": "355680"
  },
  {
    "text": "spot and if we have a blind spot because we don't understand someone else's perspective then we might erroneously",
    "start": "355680",
    "end": "362759"
  },
  {
    "text": "inject bias and as a tangent um this is why it's so",
    "start": "362759",
    "end": "369759"
  },
  {
    "text": "important to have diverse team members and I mean diversity of all forms so I'm",
    "start": "369759",
    "end": "375240"
  },
  {
    "text": "going to read through these because it's hard to see and in the center of the wheel you hold the spaces of power again",
    "start": "375240",
    "end": "382160"
  },
  {
    "text": "us so there might be contextually um some differences and on the outside is",
    "start": "382160",
    "end": "387360"
  },
  {
    "text": "the marginalized groups what I'm going to do is just read the category and then who holds the space of power and as I",
    "start": "387360",
    "end": "394039"
  },
  {
    "text": "iterate through these I just kind of want you to maybe check off and say ooh is that something that I kind of sit",
    "start": "394039",
    "end": "400199"
  },
  {
    "text": "within is that something that I didn't realize that I held power within and so these",
    "start": "400199",
    "end": "406960"
  },
  {
    "text": "categories we will start with the blue at um just about 12:00 um skin",
    "start": "406960",
    "end": "414319"
  },
  {
    "text": "color white neurodiversity neuro",
    "start": "414319",
    "end": "420599"
  },
  {
    "text": "typical mental health stable physical",
    "start": "420599",
    "end": "427080"
  },
  {
    "text": "ability able-bodied religion Christian wealth Rich again",
    "start": "427080",
    "end": "436039"
  },
  {
    "text": "some of these are subjective but age adult education",
    "start": "436039",
    "end": "442400"
  },
  {
    "text": "postsecondary housing homeowner Transportation you can drive",
    "start": "442400",
    "end": "450759"
  },
  {
    "text": "political affiliation again us Democrat Republican primarily ruled by a two-",
    "start": "450759",
    "end": "456520"
  },
  {
    "text": "party class um marriage homogeneous um citizenship citizen",
    "start": "456520",
    "end": "462680"
  },
  {
    "text": "employment you have salaried language and communication you're a native English",
    "start": "462680",
    "end": "468360"
  },
  {
    "text": "speaker uh sexuality heterogenous heterosexual monogamous um gender",
    "start": "468360",
    "end": "474680"
  },
  {
    "text": "cisgendered man body size slim and muscular",
    "start": "474680",
    "end": "480960"
  },
  {
    "text": "so again we don't need to call ourselves out or if you want to feel free but did",
    "start": "481280",
    "end": "486479"
  },
  {
    "text": "this surprise anyone was there a space that maybe you hadn't thought of as holding a privilege in",
    "start": "486479",
    "end": "493240"
  },
  {
    "text": "it okay before we move on um who is concerned about bias in Ai and",
    "start": "495440",
    "end": "502479"
  },
  {
    "text": "Technology systems who doesn't think it's issue and",
    "start": "502479",
    "end": "509919"
  },
  {
    "text": "we're blowing it up and that's okay no one I don't believe",
    "start": "509919",
    "end": "516719"
  },
  {
    "text": "you we're here to have a conversation we're here to have an engagement okay this is a respectful place and I'm going",
    "start": "518080",
    "end": "524320"
  },
  {
    "text": "to open it up for dialogue at the end um and here's the thing if we cannot have respectful",
    "start": "524320",
    "end": "530120"
  },
  {
    "text": "conversation in this room we're not going to have it out in the world",
    "start": "530120",
    "end": "536560"
  },
  {
    "text": "so we have to approach it together and if we have different views that's okay we can listen to each other as long as",
    "start": "536560",
    "end": "542519"
  },
  {
    "text": "it's done respectfully all right let's get into it what's",
    "start": "542519",
    "end": "549640"
  },
  {
    "text": "bias yes one giggle all right well societal bias",
    "start": "550880",
    "end": "558160"
  },
  {
    "text": "let's start with a big picture Okay um this is the frame it's the bottom of the",
    "start": "558160",
    "end": "563399"
  },
  {
    "text": "iceberg systemic bias it's prejudices and stereotypes present within a society",
    "start": "563399",
    "end": "569440"
  },
  {
    "text": "that influence behaviors attitudes and decisions they're reflected in policies",
    "start": "569440",
    "end": "574920"
  },
  {
    "text": "and social norms and cultural practices we get down a little more",
    "start": "574920",
    "end": "580600"
  },
  {
    "text": "individual human bias first thought um second thought has anyone",
    "start": "580600",
    "end": "586040"
  },
  {
    "text": "heard of this yes cool um so this is an idea that",
    "start": "586040",
    "end": "592079"
  },
  {
    "text": "a lot of times the biases that we hold are biases that were influenced um from",
    "start": "592079",
    "end": "598880"
  },
  {
    "text": "our surroundings as we grew up perhaps maybe around family um around our commun",
    "start": "598880",
    "end": "604880"
  },
  {
    "text": "communities and sometimes the first thought that we have is developed by",
    "start": "604880",
    "end": "610720"
  },
  {
    "text": "that we're responsible for our second",
    "start": "610720",
    "end": "615439"
  },
  {
    "text": "thought so human bias encompasses a whole range of different individual",
    "start": "616399",
    "end": "621640"
  },
  {
    "text": "judgment and decision- making um there's I think like 21 different categories of",
    "start": "621640",
    "end": "627519"
  },
  {
    "text": "bias everything from confirmation bias like hey every time I wear these socks",
    "start": "627519",
    "end": "633720"
  },
  {
    "text": "my favorite sport team wins so now you're bias towards wearing the socks um there's there's many different",
    "start": "633720",
    "end": "641880"
  },
  {
    "text": "biases within this statistical bias okay what's that",
    "start": "641880",
    "end": "647920"
  },
  {
    "text": "well a systemic deviation of an estimator um creates a bias from the",
    "start": "647920",
    "end": "653600"
  },
  {
    "text": "true value of the parameter it estimates right and this can come from flaws in",
    "start": "653600",
    "end": "659800"
  },
  {
    "text": "data collection process and our sampling um and many different things but uh basically it's part of an error",
    "start": "659800",
    "end": "667160"
  },
  {
    "text": "right and I say part of and we'll get into that okay what about AI system",
    "start": "667160",
    "end": "673160"
  },
  {
    "text": "bias um when we create systems that have",
    "start": "673160",
    "end": "679440"
  },
  {
    "text": "systemic and unfair discrimination embedded into models and algorithms this",
    "start": "679440",
    "end": "684600"
  },
  {
    "text": "can result from train skewed training data um from flawed design",
    "start": "684600",
    "end": "690240"
  },
  {
    "text": "processes and it can lead to unequal outcomes across different types of",
    "start": "690240",
    "end": "695480"
  },
  {
    "text": "users now let's drill in a little bit because AI is a giant field right we Encompass a lot of",
    "start": "695480",
    "end": "701800"
  },
  {
    "text": "things what about machine learning well this is actually closer to statistical bias um but it's a syst systematic error",
    "start": "701800",
    "end": "709639"
  },
  {
    "text": "that occurs in the model itself due to assumptions made in the machine learning",
    "start": "709639",
    "end": "714680"
  },
  {
    "text": "model um so it can lead to models that consistently make predictions that are",
    "start": "714680",
    "end": "720040"
  },
  {
    "text": "too simple they don't capture the complexity of the data or the system that it's operating",
    "start": "720040",
    "end": "726959"
  },
  {
    "text": "within now I can't talk about harmful bias um without talking about the",
    "start": "726959",
    "end": "732399"
  },
  {
    "text": "necessary bias sometimes bias is necessary um say you lose your keys okay",
    "start": "732399",
    "end": "741360"
  },
  {
    "text": "uh you are Guided by an assumption of where your keys might be you're not",
    "start": "741360",
    "end": "747480"
  },
  {
    "text": "going to go look in the fr maybe you're going to go look in the fridge you're likely not going to go",
    "start": "747480",
    "end": "753399"
  },
  {
    "text": "look in the dog kennel because it wouldn't make sense for the keys to have landed in in the dog kennel right um so",
    "start": "753399",
    "end": "762240"
  },
  {
    "text": "in a machine learning model uh it's a search process and search is Guided by a",
    "start": "762240",
    "end": "767839"
  },
  {
    "text": "set of assumptions otherwise we'd be searching blind and the set of assumptions is referred to as inductive",
    "start": "767839",
    "end": "773320"
  },
  {
    "text": "bias and basically um so Tom Mitchell 1997 the formal definition of inductive",
    "start": "773320",
    "end": "778920"
  },
  {
    "text": "bias is the set of assumptions combined with the observed training examples that deductively entail subsequent instance",
    "start": "778920",
    "end": "786320"
  },
  {
    "text": "classifications made by the learner it's not always a bad thing and",
    "start": "786320",
    "end": "791760"
  },
  {
    "text": "in fact sometimes we might want to tune bias towards a desired outcome especially if we get it wrong in a",
    "start": "791760",
    "end": "797800"
  },
  {
    "text": "certain way leads to a harmful type of error okay I promise this is the last",
    "start": "797800",
    "end": "805120"
  },
  {
    "text": "statistical slide for you uh but we also have to talk about the bias experience trade-off right so we've all heard of",
    "start": "805120",
    "end": "811000"
  },
  {
    "text": "the terms overfitting and underfitting um this is this is because",
    "start": "811000",
    "end": "816639"
  },
  {
    "text": "of this tradeoff between bias and variance and we can actually formally",
    "start": "816639",
    "end": "821959"
  },
  {
    "text": "Define uh error as bias squared plus variance plus noise which is irreducible",
    "start": "821959",
    "end": "829279"
  },
  {
    "text": "error okay we've we've gotten our statistical methods out of the way let's",
    "start": "829279",
    "end": "834560"
  },
  {
    "text": "talk about harmful bias oh I'm so sorry Jody",
    "start": "834560",
    "end": "839680"
  },
  {
    "text": "so what about when it becomes harmful so the following few slides I'm going to use a uh wonderful",
    "start": "844240",
    "end": "850560"
  },
  {
    "text": "paper um called a framework for understanding sources of harm throughout the machine learning life cycle this is",
    "start": "850560",
    "end": "856720"
  },
  {
    "text": "a 19 or 19 I'm old 2021 paper um by S",
    "start": "856720",
    "end": "862839"
  },
  {
    "text": "rashan gut and what they did was basically let's identify the spaces in",
    "start": "862839",
    "end": "868120"
  },
  {
    "text": "an ml life cycle where we can get it wrong and we can inject bias so",
    "start": "868120",
    "end": "874480"
  },
  {
    "text": "um to kind of help me decide how deep we go here uh how many of us are data",
    "start": "874480",
    "end": "879920"
  },
  {
    "text": "scientists machine learning engineers and operate within this space um on a pretty regular Cadence within our job",
    "start": "879920",
    "end": "887560"
  },
  {
    "text": "okay let's go let's go over it um so this top slide uh it's a little hard to",
    "start": "887560",
    "end": "893240"
  },
  {
    "text": "see is our data generation okay so if we roughly divide the ml life cycle process",
    "start": "893240",
    "end": "899440"
  },
  {
    "text": "into two phases right we're looking at our data generation and then our model building and implementation so we start",
    "start": "899440",
    "end": "906160"
  },
  {
    "text": "on the top leftand side with what the world we're looking for data okay and we",
    "start": "906160",
    "end": "913600"
  },
  {
    "text": "go out and we find the data and then we create a population population selection",
    "start": "913600",
    "end": "920279"
  },
  {
    "text": "right and then from that population we have to measure",
    "start": "920279",
    "end": "925399"
  },
  {
    "text": "it what's our goal what's our Target how do we Define that from different",
    "start": "925399",
    "end": "931360"
  },
  {
    "text": "attributes that we've created in this data set and then we have our data set okay",
    "start": "931360",
    "end": "936519"
  },
  {
    "text": "and then we have to pre-process it right we split it into training and testing",
    "start": "936519",
    "end": "942079"
  },
  {
    "text": "subsets and from there we get to go into our model building and implementation so",
    "start": "942079",
    "end": "948319"
  },
  {
    "text": "we have a data set it's been split we have a problem we have to Define an",
    "start": "948319",
    "end": "953480"
  },
  {
    "text": "objective right we have to optimize on that objective we have to Define our",
    "start": "953480",
    "end": "959319"
  },
  {
    "text": "model how do we Define our model well based on the objective um we have to",
    "start": "959319",
    "end": "964959"
  },
  {
    "text": "evaluate it how do we evaluate it we use bench marks and then what happens with it we",
    "start": "964959",
    "end": "970560"
  },
  {
    "text": "say oh we've hit x amount of accuracy looks good enough to me ship it it goes",
    "start": "970560",
    "end": "976519"
  },
  {
    "text": "out into the world and it has real world",
    "start": "976519",
    "end": "983000"
  },
  {
    "text": "implications so the paper found seven sources of harm in this life cycle",
    "start": "987920",
    "end": "993959"
  },
  {
    "text": "process at each of these points and we're going to go through these one by one don't get me wrong but we start with historical bias we move into",
    "start": "993959",
    "end": "1000920"
  },
  {
    "text": "representation bias measurement bias and then within our building we have learning and aggregation bias we",
    "start": "1000920",
    "end": "1008120"
  },
  {
    "text": "evaluate it we have evaluation bias and we deploy it deployment and bias and what",
    "start": "1008120",
    "end": "1014639"
  },
  {
    "text": "happens what happens when we release a model that is making predictions in real world",
    "start": "1014800",
    "end": "1021759"
  },
  {
    "text": "applications they cycle back up into the data that we're pulling",
    "start": "1022000",
    "end": "1027438"
  },
  {
    "text": "from okay this is the easy one to start with we've all heard of the phrase garbage in garbage out right if you",
    "start": "1029360",
    "end": "1036199"
  },
  {
    "text": "start with a crappy data set that has a bunch of garbage in it and bias guess what you're going to end up",
    "start": "1036199",
    "end": "1042880"
  },
  {
    "text": "with so even if you perfectly measure the data set even if you perfect sample",
    "start": "1042880",
    "end": "1049640"
  },
  {
    "text": "that if the world as it is or as it was contains Prejudice in the domain that you're looking at you're going to have a",
    "start": "1049640",
    "end": "1055679"
  },
  {
    "text": "data set that contains bias now we can handle that but we have to recognize",
    "start": "1055679",
    "end": "1060799"
  },
  {
    "text": "it we can kind of handle it caveat this isn't a solved problem okay an example um so this was",
    "start": "1060799",
    "end": "1068679"
  },
  {
    "text": "an experiment on GPT chat GPT 3.5 by Dr Hades cotch um and Dr RIT Doan if you do",
    "start": "1068679",
    "end": "1076919"
  },
  {
    "text": "not follow them on LinkedIn they are fabulous um they have been working in this field for a long time but basically the",
    "start": "1076919",
    "end": "1083000"
  },
  {
    "text": "premises of this is they were trying to get chat GPT to admit that maybe the",
    "start": "1083000",
    "end": "1088480"
  },
  {
    "text": "nurse was a male and maybe the doctor was female and so they start with the",
    "start": "1088480",
    "end": "1095159"
  },
  {
    "text": "sentence the doctor yelled at the nurse because she was late who was late um according to the given sentence the",
    "start": "1095159",
    "end": "1100520"
  },
  {
    "text": "nurse was late okay okay fine fine she was late well what if we start swapping",
    "start": "1100520",
    "end": "1108360"
  },
  {
    "text": "around pronouns and nouns and try to make a",
    "start": "1108360",
    "end": "1114919"
  },
  {
    "text": "sentence that structurally wouldn't make sense for the doctor to be male it then starts trying to justify",
    "start": "1114919",
    "end": "1122480"
  },
  {
    "text": "well you actually must have asked this question wrong or the sentence doesn't make sense because it must be um and",
    "start": "1122480",
    "end": "1127880"
  },
  {
    "text": "this is a fun thought experiment you can do across a variety of domains you can say tell me a story about an engineer",
    "start": "1127880",
    "end": "1133280"
  },
  {
    "text": "it's going to tell you a story about a man you can say tell me a story about an engineer named Michelle and there will",
    "start": "1133280",
    "end": "1139039"
  },
  {
    "text": "something in the story about the gender construct in engineering you can do the",
    "start": "1139039",
    "end": "1144880"
  },
  {
    "text": "same thing tell me a story about a nurse who is male it's always going to say even though it's a woman dominated field",
    "start": "1144880",
    "end": "1150080"
  },
  {
    "text": "and his parents wanted him to become a doctor and there's some funny ones in",
    "start": "1150080",
    "end": "1156559"
  },
  {
    "text": "there you know it's like what a Russians like to drink Vodka it'll always pair those together but it's still a bias",
    "start": "1156559",
    "end": "1164679"
  },
  {
    "text": "right okay what about representation bias what if um a sample under represents",
    "start": "1166200",
    "end": "1173520"
  },
  {
    "text": "part of a population and this isn't always bad but it can become problematic when when the",
    "start": "1173520",
    "end": "1181000"
  },
  {
    "text": "target population does not reflect the use population if the target population contains underrepresented groups and if",
    "start": "1181000",
    "end": "1187960"
  },
  {
    "text": "the sample method from the target population is limited or uneven",
    "start": "1187960",
    "end": "1193840"
  },
  {
    "text": "who remembers this I think this was 2015 if I'm not",
    "start": "1200240",
    "end": "1206120"
  },
  {
    "text": "mistaken uh yeah 2015 so yeah Android released this feature",
    "start": "1206120",
    "end": "1214280"
  },
  {
    "text": "that starts automatically classifying your photos um and engineer called out like hey you've tagged me and my friends",
    "start": "1214280",
    "end": "1221280"
  },
  {
    "text": "as gorillas why",
    "start": "1221280",
    "end": "1229559"
  },
  {
    "text": "exactly there wasn't enough training data dark faces does anyone know how Google fixed this at the",
    "start": "1231360",
    "end": "1239158"
  },
  {
    "text": "time they removed gorillas as a label the problem was to too steep to",
    "start": "1239240",
    "end": "1247440"
  },
  {
    "text": "fix it immediately that the best solution was to remove the label",
    "start": "1247440",
    "end": "1252679"
  },
  {
    "text": "itself I've heard we've solved this since then um but I'll need a Google person to confirm",
    "start": "1254720",
    "end": "1261200"
  },
  {
    "text": "that for me okay what about measurement bias",
    "start": "1261200",
    "end": "1266960"
  },
  {
    "text": "right um so features and labels they're a concrete measurement right a proxy to",
    "start": "1266960",
    "end": "1275039"
  },
  {
    "text": "represent a construct an idea or a concept what happens when we get it",
    "start": "1275039",
    "end": "1280960"
  },
  {
    "text": "wrong what happens when it's it's complex um if we're doing a image",
    "start": "1280960",
    "end": "1287400"
  },
  {
    "text": "classification we can all agree that this is a glass or a cup of water we can get really detailed and argue about it",
    "start": "1287400",
    "end": "1293360"
  },
  {
    "text": "but it's it's pretty simple right straightforward what about things like",
    "start": "1293360",
    "end": "1302679"
  },
  {
    "text": "creditworthiness what about ideas like",
    "start": "1303039",
    "end": "1308840"
  },
  {
    "text": "recidivism well as it happens we're going to talk about reism today so has anyone heard of",
    "start": "1310679",
    "end": "1320520"
  },
  {
    "text": "Compass this is a a system in the United States and Compass stands for",
    "start": "1320880",
    "end": "1326559"
  },
  {
    "text": "Correctional offender management profiling for alternative sanctions and the system was built in the late 90s I",
    "start": "1326559",
    "end": "1334480"
  },
  {
    "text": "think deployed in the early 2000s um at one point it was being used by 47 out of 50",
    "start": "1334480",
    "end": "1341559"
  },
  {
    "text": "states and it wasn't independently reviewed until 2017 now recidivism um to",
    "start": "1341559",
    "end": "1348279"
  },
  {
    "text": "to Define is the likelihood that a defendant or someone who commits a crime is going to go on to commit another",
    "start": "1348279",
    "end": "1355080"
  },
  {
    "text": "crime okay but if we really want to start looking at recidivism as a",
    "start": "1355080",
    "end": "1363559"
  },
  {
    "text": "construct in the United States we know that there's a big problem with policing so is it truly an indicator",
    "start": "1363559",
    "end": "1371000"
  },
  {
    "text": "that someone is going to go on to commit another crime or is it an indicator of a much more complex social issue that",
    "start": "1371000",
    "end": "1378120"
  },
  {
    "text": "involv policing because different communities are policed at different rates and when the system was reviewed",
    "start": "1378120",
    "end": "1385960"
  },
  {
    "text": "uh by propublica in 2017 they released um their findings and",
    "start": "1385960",
    "end": "1391799"
  },
  {
    "text": "they found that black defendants were two times more likely to be classified",
    "start": "1391799",
    "end": "1397760"
  },
  {
    "text": "as a false positive meaning classified as likely to go on to commit a crime and they did not meanwhile white defendants",
    "start": "1397760",
    "end": "1405440"
  },
  {
    "text": "were two times more likely to be classified as a false negative meaning they were not going to go on to commit",
    "start": "1405440",
    "end": "1412039"
  },
  {
    "text": "another crime the system predicted that and yet they did um and in Pro Public's documentation there are several examples",
    "start": "1412039",
    "end": "1419400"
  },
  {
    "text": "where it just doesn't quite make sense now I have to admit that um I will",
    "start": "1419400",
    "end": "1425919"
  },
  {
    "text": "state that North Point uh rejected propublica's findings North Point is again the creator of the system and they",
    "start": "1425919",
    "end": "1432840"
  },
  {
    "text": "said that propublica got it wrong and they didn't have enough data uh so since then this has been a Hot Topic in the",
    "start": "1432840",
    "end": "1438559"
  },
  {
    "text": "fairness uh community and I'm not going to give a percentage but a large amount",
    "start": "1438559",
    "end": "1444799"
  },
  {
    "text": "greater than 50% agrees with propublica's findings to the point that Pro propublica's Compass data set is one",
    "start": "1444799",
    "end": "1452480"
  },
  {
    "text": "of the most widely used In fairness research we're going to talk about this as we go because this one actually",
    "start": "1452480",
    "end": "1458159"
  },
  {
    "text": "checks every single box um for our",
    "start": "1458159",
    "end": "1463120"
  },
  {
    "text": "examples okay aggregation bias",
    "start": "1463799",
    "end": "1469039"
  },
  {
    "text": "this is an assumption that a one siiz fits all model will map inputs to labels",
    "start": "1469039",
    "end": "1476600"
  },
  {
    "text": "consistently across different subsets what do we know from that wheel",
    "start": "1476600",
    "end": "1482120"
  },
  {
    "text": "of privilege that was brought up earlier that things mean thing different",
    "start": "1482120",
    "end": "1489000"
  },
  {
    "text": "things across different contexts um if I say spill the tea in",
    "start": "1489000",
    "end": "1494039"
  },
  {
    "text": "America I'm going to lean in for some hot juicy gossip if I say it in eng and um someone might be",
    "start": "1494039",
    "end": "1501240"
  },
  {
    "text": "offended if we're building a text classification model to predict risky",
    "start": "1501240",
    "end": "1507600"
  },
  {
    "text": "behavior and I shot off a text that said I'm going to be late I'm on",
    "start": "1507600",
    "end": "1513559"
  },
  {
    "text": "LSD red flag she's making bad",
    "start": "1513559",
    "end": "1519080"
  },
  {
    "text": "decisions if I was in Chicago I'd still made a bad decision but I could also be on Lakeshore Drive also referred to as",
    "start": "1519080",
    "end": "1526520"
  },
  {
    "text": "LSD",
    "start": "1526520",
    "end": "1529520"
  },
  {
    "text": "okay well what about Healthcare if we want to build a model that uh predicts diabetes we have to",
    "start": "1534840",
    "end": "1542760"
  },
  {
    "text": "understand that certain uh markers perform differently across different ethnicities so if we don't take that",
    "start": "1542760",
    "end": "1549360"
  },
  {
    "text": "context into account we're going to build a model that performs differently across different subsets of people",
    "start": "1549360",
    "end": "1558278"
  },
  {
    "text": "now learning bias is probably one of the more complicated ones here um because we",
    "start": "1563159",
    "end": "1568360"
  },
  {
    "text": "have to make a choice right when we're modeling um an objective and certain",
    "start": "1568360",
    "end": "1573440"
  },
  {
    "text": "objectives might prioritize one thing while diminishing another and um if we",
    "start": "1573440",
    "end": "1581840"
  },
  {
    "text": "prioritize for example we want to minimize cross entropy loss in a classification model um we might have a",
    "start": "1581840",
    "end": "1589200"
  },
  {
    "text": "model that has more false positives maybe we want that maybe we don't we",
    "start": "1589200",
    "end": "1594760"
  },
  {
    "text": "have to understand that though hard to find an example here because that would need an open source",
    "start": "1594760",
    "end": "1601039"
  },
  {
    "text": "model um but we can make some guesses right um so one thing we'll talk about is how uh",
    "start": "1601039",
    "end": "1608480"
  },
  {
    "text": "accuracy can sometimes have another trade-off with fairness because when we're dealing with",
    "start": "1608480",
    "end": "1613880"
  },
  {
    "text": "data that comes from an unfair world if we want to increase fairness means we take a hit in accuracy so if you blindly",
    "start": "1613880",
    "end": "1622159"
  },
  {
    "text": "go and prioritize accuracy across different fields without accounting for that Nuance it you're going to have a",
    "start": "1622159",
    "end": "1628679"
  },
  {
    "text": "lack of fairness um this was another one that hit a bunch of headlines and I think 201 yet 18 Amazon had created a",
    "start": "1628679",
    "end": "1635720"
  },
  {
    "text": "hiring tool that just immediately was like no women in",
    "start": "1635720",
    "end": "1640080"
  },
  {
    "text": "Tech there's at least a few of us okay how do we evaluate it",
    "start": "1641399",
    "end": "1648960"
  },
  {
    "text": "what happens if the Benchmark itself is biased what if the Benchmark doesn't represent the used",
    "start": "1648960",
    "end": "1654960"
  },
  {
    "text": "population right a model is optimized on training",
    "start": "1654960",
    "end": "1660080"
  },
  {
    "text": "data but the quality is measured against benchmarks so if there's a benchmark out there that's been established as the",
    "start": "1660080",
    "end": "1665159"
  },
  {
    "text": "high criteria and we're all measuring up to it we actually might have had a better model at some point but we said",
    "start": "1665159",
    "end": "1671960"
  },
  {
    "text": "it wasn't good enough because it's not meeting um it's not meeting that benchmark",
    "start": "1671960",
    "end": "1678320"
  },
  {
    "text": "um this was a study done a few years back that at the time was um looking at the top three facial recognition apis so",
    "start": "1678320",
    "end": "1686039"
  },
  {
    "text": "that was um Microsoft's uh face Plus+ API and IBM and overall if you look at",
    "start": "1686039",
    "end": "1692840"
  },
  {
    "text": "overall accuracy looks pretty good but if you start digging into the types of Errors across different um skin tones",
    "start": "1692840",
    "end": "1701000"
  },
  {
    "text": "and also between genders you find a gap um and I'll also call out that a lot of",
    "start": "1701000",
    "end": "1707320"
  },
  {
    "text": "these studies um you know we's still look at gender as a binary problem too so that's a whole other another range of",
    "start": "1707320",
    "end": "1714519"
  },
  {
    "text": "the equation um but from this study the largest difference was a 34.4% difference of error from IBM",
    "start": "1714519",
    "end": "1722159"
  },
  {
    "text": "between a darker skinned female and a light-skinned",
    "start": "1722159",
    "end": "1726600"
  },
  {
    "text": "male all right we build our model we're doing pretty good right it goes out into",
    "start": "1728080",
    "end": "1733760"
  },
  {
    "text": "the world what if we intended it to be used",
    "start": "1733760",
    "end": "1739480"
  },
  {
    "text": "in a certain way but yet it's been it's being used in another so for example",
    "start": "1739480",
    "end": "1745799"
  },
  {
    "text": "Compass originally was supposed to help judges decide if they should set bail",
    "start": "1745799",
    "end": "1752679"
  },
  {
    "text": "how risky would it be if um we allow this defendant to to meet bail um but",
    "start": "1752679",
    "end": "1760880"
  },
  {
    "text": "some judges ended up using it as sentencing measures okay so it's you",
    "start": "1760880",
    "end": "1767440"
  },
  {
    "text": "know building a hook that was meant for a tunea fish and someone's out there catching sharks with it it's a framing",
    "start": "1767440",
    "end": "1773760"
  },
  {
    "text": "trap now we can't necessarily uh mitigate how people use our products but",
    "start": "1773760",
    "end": "1781039"
  },
  {
    "text": "we can make sure that we're responsibly testing them and imagining all those different scenarios and trying our best",
    "start": "1781039",
    "end": "1786840"
  },
  {
    "text": "to create guard rails I have some book wrecks at the end this is always going to be one of them",
    "start": "1786840",
    "end": "1793399"
  },
  {
    "text": "um Virginia Eubanks automating inequality uh she looks at three different different case studies and one",
    "start": "1793399",
    "end": "1800279"
  },
  {
    "text": "of them was on a Model that predicted child abuse and what she found was that",
    "start": "1800279",
    "end": "1806720"
  },
  {
    "text": "it performed really differently for lower class families why because",
    "start": "1806720",
    "end": "1812480"
  },
  {
    "text": "protecting children looks different to different families based off of their",
    "start": "1812480",
    "end": "1817760"
  },
  {
    "text": "means I'm I doing 30 minutes okay",
    "start": "1820799",
    "end": "1826600"
  },
  {
    "text": "all right bison generative systems again this is still machine learning um",
    "start": "1828320",
    "end": "1835159"
  },
  {
    "text": "but this is a lot of what we're interfacing with so I will give you a couple examples we're not going to go in",
    "start": "1835159",
    "end": "1840840"
  },
  {
    "text": "too depth in here because that's a whole other talk this was a really beautiful UI",
    "start": "1840840",
    "end": "1847600"
  },
  {
    "text": "study actually if you go to this web page um the designers that worked on this project and the entire project",
    "start": "1847600",
    "end": "1852679"
  },
  {
    "text": "itself was absolutely fabulous um it came out I believe last year from Bloomberg and and basically what they",
    "start": "1852679",
    "end": "1859919"
  },
  {
    "text": "did was they said give me a photograph of a x give me a photograph of a",
    "start": "1859919",
    "end": "1867279"
  },
  {
    "text": "politician of a CEO of a criminal of a terrorist of a housekeeper of a fast",
    "start": "1867279",
    "end": "1875480"
  },
  {
    "text": "food worker and then what they did was they started to just take out the",
    "start": "1875480",
    "end": "1881760"
  },
  {
    "text": "features and and look at the variation of skin tone and do a distribution on",
    "start": "1881760",
    "end": "1887159"
  },
  {
    "text": "what types of skin skin tone came out from different requests um and what did they find",
    "start": "1887159",
    "end": "1892720"
  },
  {
    "text": "surprise surprise uh bias why how are",
    "start": "1892720",
    "end": "1897760"
  },
  {
    "text": "these models trained the historical",
    "start": "1897760",
    "end": "1903760"
  },
  {
    "text": "data feel free to shout out answers guys this G to be a chat okay another one I have to call out",
    "start": "1903760",
    "end": "1910960"
  },
  {
    "text": "because I I did get a comment once that said um whoever built Gemini listen to the likes of her which is kind of funny",
    "start": "1910960",
    "end": "1918360"
  },
  {
    "text": "um who saw this earlier this year yeah um",
    "start": "1918360",
    "end": "1927278"
  },
  {
    "text": "why we didn't buil in context okay we can't just have a model",
    "start": "1928720",
    "end": "1934960"
  },
  {
    "text": "that says hey go do this thing give me this thing but don't actually have historical context or knowledge about",
    "start": "1934960",
    "end": "1942879"
  },
  {
    "text": "it another segue for another time but um any sort of model that we're dealing",
    "start": "1943559",
    "end": "1949120"
  },
  {
    "text": "with the large language Foundation model space really needs multiple and an adversarial Network to create a chain of",
    "start": "1949120",
    "end": "1955120"
  },
  {
    "text": "checks and balances to talk for another day but I will just leave that with there okay so what are we going to do",
    "start": "1955120",
    "end": "1962120"
  },
  {
    "text": "about it any",
    "start": "1962120",
    "end": "1967519"
  },
  {
    "text": "ideas you may not answer Jody Den Che huh check",
    "start": "1967799",
    "end": "1978960"
  },
  {
    "text": "yeah yeah exactly it would be a great place to start right we should at least try and measure it we should at least be",
    "start": "1978960",
    "end": "1985799"
  },
  {
    "text": "thinking about this so one question that I get quite",
    "start": "1985799",
    "end": "1991880"
  },
  {
    "text": "frequently is well can't we just take out the protected",
    "start": "1991880",
    "end": "1997000"
  },
  {
    "text": "attributes and then it's blind we wish it was that easy no we",
    "start": "1997000",
    "end": "2004440"
  },
  {
    "text": "can't and this is a a deep conversation but the short",
    "start": "2004440",
    "end": "2009880"
  },
  {
    "text": "answer is no we cannot because of Simpsons Paradox it's not actually related to The Simpsons but",
    "start": "2009880",
    "end": "2016440"
  },
  {
    "text": "I I giggled here but um basically this is a um it's a statistical phenomenon",
    "start": "2016440",
    "end": "2024000"
  },
  {
    "text": "that says when we collect data in such a space these attributes are so tightly",
    "start": "2024000",
    "end": "2030039"
  },
  {
    "text": "woven it doesn't matter if we take out one because they bleed across the others",
    "start": "2030039",
    "end": "2035440"
  },
  {
    "text": "so even if we took out something like race or gender enough things have happened in the social context of which",
    "start": "2035440",
    "end": "2041799"
  },
  {
    "text": "we're collecting the data that you know we may as well actually have it there so that we can be aware of it and we can",
    "start": "2041799",
    "end": "2047960"
  },
  {
    "text": "measure and mitigate against us and I'll I'll show that in a little bit",
    "start": "2047960",
    "end": "2053638"
  },
  {
    "text": "here okay fairness it's a field of machine",
    "start": "2054200",
    "end": "2060720"
  },
  {
    "text": "learning research and what are we trying to do in fairness we're trying to manage and mitigate bias um no one cared about",
    "start": "2060720",
    "end": "2068878"
  },
  {
    "text": "this for like a long time and then all of a sudden everyone was like oh guys this is a problem and now um",
    "start": "2068879",
    "end": "2077480"
  },
  {
    "text": "fairness research is popping out so often it's uh very frustrating to do literature reviews because there's a new",
    "start": "2077480",
    "end": "2084079"
  },
  {
    "text": "paper coming out all the time um which I personally love I love how many people are working in this space and it's still",
    "start": "2084079",
    "end": "2090480"
  },
  {
    "text": "not a solved issue why because there's no free lunch because there's not going to be one solution that fixes all",
    "start": "2090480",
    "end": "2096440"
  },
  {
    "text": "problems because we have to understand the context in which we're operating so that we can create a meaningful solution",
    "start": "2096440",
    "end": "2103040"
  },
  {
    "text": "towards it so what does it mean to build Fair",
    "start": "2103040",
    "end": "2109440"
  },
  {
    "text": "systems well first off I want to call out a few things um you know you hear a",
    "start": "2109440",
    "end": "2115800"
  },
  {
    "text": "lot about well ethical AI was what we used to call it and then it came out with the term responsible AI right um",
    "start": "2115800",
    "end": "2123880"
  },
  {
    "text": "sometimes I mentioned the word ethics and I just see the eyes glaze over and that's okay that's a business",
    "start": "2123880",
    "end": "2131160"
  },
  {
    "text": "conversation um but then I say what about risk do you like law",
    "start": "2131160",
    "end": "2136800"
  },
  {
    "text": "suits we're driving towards the same goal we might call it something",
    "start": "2136800",
    "end": "2142000"
  },
  {
    "text": "differently but we're trying to reach the same goal so sometimes we do have to frame it differently as technologists",
    "start": "2142000",
    "end": "2147760"
  },
  {
    "text": "when we're speaking to business people but importantly here Equitable AI is a",
    "start": "2147760",
    "end": "2153200"
  },
  {
    "text": "very specific idea that AI should benefit everyone fairly and we should",
    "start": "2153200",
    "end": "2160280"
  },
  {
    "text": "address issues like inclusion accessibility and fair distribution of",
    "start": "2160280",
    "end": "2167000"
  },
  {
    "text": "advantages I know there are some thoughts going on right now that should be addressed by social scientist I am",
    "start": "2169119",
    "end": "2174880"
  },
  {
    "text": "here to give you the technical approach but um it's a complex system we're not",
    "start": "2174880",
    "end": "2180359"
  },
  {
    "text": "operating in vacuums here right okay so how are we going to measure it well no one can agree on that",
    "start": "2180359",
    "end": "2187920"
  },
  {
    "text": "either uh last I checked there were I think 26 different measures of fairness",
    "start": "2187920",
    "end": "2193440"
  },
  {
    "text": "different mathematical measures why because again there's no free lunch there's not a one siiz fits all solution",
    "start": "2193440",
    "end": "2200839"
  },
  {
    "text": "um but I'm going to introduce you to just a couple of Concepts so as you go forward you can think of oh did it what",
    "start": "2200839",
    "end": "2206720"
  },
  {
    "text": "if I measured against um against this this measurement so one that we talk about often is statistical parity also",
    "start": "2206720",
    "end": "2214079"
  },
  {
    "text": "called demographic parity um Cynthia D work is an amazing researcher In",
    "start": "2214079",
    "end": "2219640"
  },
  {
    "text": "fairness and she's come up with several of these um measurements um so basically",
    "start": "2219640",
    "end": "2225440"
  },
  {
    "text": "this just ensures that individuals from different groups have the same probability of receiving a positive",
    "start": "2225440",
    "end": "2234720"
  },
  {
    "text": "outcome why did I highlight that because it actually doesn't care about the ground",
    "start": "2234920",
    "end": "2240440"
  },
  {
    "text": "truth it just wants to know that we have an equal probability of predicting maybe",
    "start": "2240440",
    "end": "2247960"
  },
  {
    "text": "genders to go on to receive a loan that gets a little messy right",
    "start": "2247960",
    "end": "2253560"
  },
  {
    "text": "because we're actually not looking at the ground truth and we're kind of throwing that",
    "start": "2253560",
    "end": "2259440"
  },
  {
    "text": "out the window but it still starts to give us a framework to understand how our system is operating",
    "start": "2259440",
    "end": "2266760"
  },
  {
    "text": "in equal opportunity is very similar except it really wants to know about",
    "start": "2266839",
    "end": "2273440"
  },
  {
    "text": "qualified it wants to say what in this data set would make a a row an entity",
    "start": "2273440",
    "end": "2279720"
  },
  {
    "text": "qualified to receive a positive outcome and then how do we look at those",
    "start": "2279720",
    "end": "2286520"
  },
  {
    "text": "qualified entities and then look at the protected classes within them and",
    "start": "2286520",
    "end": "2293480"
  },
  {
    "text": "say yeah our probability is off between classes so we start to kind of narrow it",
    "start": "2293480",
    "end": "2300880"
  },
  {
    "text": "down a little bit right okay equalized odds well",
    "start": "2300880",
    "end": "2308560"
  },
  {
    "text": "this is um a stricter uh path and we basically require that the true positive",
    "start": "2308560",
    "end": "2315599"
  },
  {
    "text": "rate and the false positive rate are roughly the same between different groups so we can start looking at this",
    "start": "2315599",
    "end": "2322280"
  },
  {
    "text": "in terms of ratios which is what I'll pull up in my demo and um what we want",
    "start": "2322280",
    "end": "2327880"
  },
  {
    "text": "to do is we want to get as close to one as possible depending on how you're using a ratio",
    "start": "2327880",
    "end": "2335920"
  },
  {
    "text": "predictive parity okay we're starting to get into the ground truth right so this",
    "start": "2337640",
    "end": "2342880"
  },
  {
    "text": "is basically that the predictive positive outcome has the same Precision across different",
    "start": "2342880",
    "end": "2350480"
  },
  {
    "text": "groups treatment equality um now we're focusing on balancing the ratio of false",
    "start": "2352960",
    "end": "2358599"
  },
  {
    "text": "positive to false negative rates across different groups um personally I like to look at",
    "start": "2358599",
    "end": "2366079"
  },
  {
    "text": "all of these so that we can understand the full problem right that data scientists we don't just want one little",
    "start": "2366079",
    "end": "2371280"
  },
  {
    "text": "window into the picture we want to understand the entire environment so if you were doing",
    "start": "2371280",
    "end": "2377720"
  },
  {
    "text": "research on on a system to measure bias you might look at all these things and it actually might even tell you more",
    "start": "2377720",
    "end": "2383319"
  },
  {
    "text": "about your data might tell you more about your model and give you insights on how to even make your model better",
    "start": "2383319",
    "end": "2389839"
  },
  {
    "text": "because now we're starting to look at the different types of errors that we're producing now I'm going to start talking quickly because I have 19 minutes",
    "start": "2389839",
    "end": "2397720"
  },
  {
    "text": "all right mitigation so we're going to start with a technical approach um we can break this down into roughly three",
    "start": "2397720",
    "end": "2404520"
  },
  {
    "text": "categories there's active research going in into each of these so the first is kind of the easiest to um visualize and",
    "start": "2404520",
    "end": "2412319"
  },
  {
    "text": "understand I'll do a quick demo that um is in the pre-processing space what do I mean by pre-processing I mean that we're",
    "start": "2412319",
    "end": "2418440"
  },
  {
    "text": "trying to mitigate bias before the data is trained for the model before the",
    "start": "2418440",
    "end": "2424280"
  },
  {
    "text": "model is trained on the data okay so um we want to look at our um weights right",
    "start": "2424280",
    "end": "2431599"
  },
  {
    "text": "at our sampling Methods at um different Transformations that we can do prior now",
    "start": "2431599",
    "end": "2439359"
  },
  {
    "text": "in processing is techniques that modify the actual learning mechanisms um or the",
    "start": "2439359",
    "end": "2444920"
  },
  {
    "text": "algorithm to incorporate constraints and objectives directly during the model training itself um my personal opinion",
    "start": "2444920",
    "end": "2452079"
  },
  {
    "text": "is that this is where the solution should lie okay um we want the model to",
    "start": "2452079",
    "end": "2458720"
  },
  {
    "text": "recognize these things we can manually do things but if we could create models that recognize",
    "start": "2458720",
    "end": "2465480"
  },
  {
    "text": "bias themselves we might even get deeper insights into systems in our society that generate",
    "start": "2465480",
    "end": "2473318"
  },
  {
    "text": "bias um a lot of work is being done right now also in the multi-objective optimization space so I spoke earlier",
    "start": "2474119",
    "end": "2481200"
  },
  {
    "text": "about that tradeoff between fairness and accuracy well how do we take these two",
    "start": "2481200",
    "end": "2486400"
  },
  {
    "text": "objectives and seek an optimal solution that finds a midpoint without damaging one or the other how close can we get to",
    "start": "2486400",
    "end": "2493480"
  },
  {
    "text": "a compromise between these two and if we can solve for that then",
    "start": "2493480",
    "end": "2498599"
  },
  {
    "text": "that also is a better conversation to a stakeholder when I say hey I'm going to take a 4% hit on accuracy for a 2%",
    "start": "2498599",
    "end": "2505040"
  },
  {
    "text": "increase In fairness they don't always like that but if you can explain that we're finding the best solution between",
    "start": "2505040",
    "end": "2510800"
  },
  {
    "text": "the two it tells a better story um and then",
    "start": "2510800",
    "end": "2516119"
  },
  {
    "text": "postprocessing uh I don't love this one um so basically we take the results from our model and then",
    "start": "2516119",
    "end": "2522720"
  },
  {
    "text": "we manipulate the outputs or predictions so for an example um that compass that we'll look at they give a desile score",
    "start": "2522720",
    "end": "2530119"
  },
  {
    "text": "and I've seen some research where the desile score is between one and 10 one being very unlikely to um go on to",
    "start": "2530119",
    "end": "2536920"
  },
  {
    "text": "commit recidivism 10 being highly likely and um it understood that there in",
    "start": "2536920",
    "end": "2543800"
  },
  {
    "text": "postprocessing techniques that um it operates you know on a vacuum so they'll try and alleviate scores of protected",
    "start": "2543800",
    "end": "2550160"
  },
  {
    "text": "groups but that's not very scientific is it we're still making guesses and then we can measure it",
    "start": "2550160",
    "end": "2556400"
  },
  {
    "text": "but so for me personal opinions here stating that in processing is where we",
    "start": "2556400",
    "end": "2563800"
  },
  {
    "text": "should be aiming for um but pre-processing is still a good place to",
    "start": "2563800",
    "end": "2570760"
  },
  {
    "text": "start oh no live demos okay s",
    "start": "2572559",
    "end": "2578559"
  },
  {
    "text": "let's see if I can they moved my things",
    "start": "2578559",
    "end": "2587720"
  },
  {
    "text": "around and she's very tiny you can see it okay we're going to",
    "start": "2598240",
    "end": "2605119"
  },
  {
    "text": "go through this kind of quickly um and I will also disclose that um I",
    "start": "2605119",
    "end": "2611319"
  },
  {
    "text": "decided to change my demo in my bias very recently so some of this is messy but uh we get to dig in together um and",
    "start": "2611319",
    "end": "2617240"
  },
  {
    "text": "also I found out a very unfortunate thing this is a new computer and usually I demo this on a fairness dashboard um",
    "start": "2617240",
    "end": "2625079"
  },
  {
    "text": "that is contained in the Microsoft's RI um widgets uh package um and it breaks",
    "start": "2625079",
    "end": "2631760"
  },
  {
    "text": "on several new things first off it doesn't play nicely with newer versions of python um and 's also a break on",
    "start": "2631760",
    "end": "2638040"
  },
  {
    "text": "Homebrew um so to all my Microsoft friends out there please fix it for",
    "start": "2638040",
    "end": "2643760"
  },
  {
    "text": "me so what are we looking at here um if we scroll down this is the data set that",
    "start": "2643760",
    "end": "2650240"
  },
  {
    "text": "propublica put together on Compass so I think there's about",
    "start": "2650240",
    "end": "2656280"
  },
  {
    "text": "7,000 um going up here 7,214 entries um in the main data set",
    "start": "2656599",
    "end": "2663240"
  },
  {
    "text": "from propublica and it looks at things like name first name last name the screening date the sex the date of birth",
    "start": "2663240",
    "end": "2671079"
  },
  {
    "text": "what types of attributes were related to the crime itself and then they did a 2-year study that said okay this is what",
    "start": "2671079",
    "end": "2677119"
  },
  {
    "text": "the system predicted two years later what has happened now my uh caveat here about this demo is that this data set is",
    "start": "2677119",
    "end": "2684240"
  },
  {
    "text": "not meant to be predictive it's meant for analysis so that we can look at how the system performs so what I am going",
    "start": "2684240",
    "end": "2691200"
  },
  {
    "text": "to do here after I show you um their analysis and some pieces from it um is we're going to try and make a model",
    "start": "2691200",
    "end": "2697119"
  },
  {
    "text": "logistic regression model to replicate the compass system as close as we can as",
    "start": "2697119",
    "end": "2702400"
  },
  {
    "text": "possible which is not great it's like 76% that's fine um and then from there we're going to look at different",
    "start": "2702400",
    "end": "2708000"
  },
  {
    "text": "sampling methods to build that model and how those ratios are affected so if we just take a rough",
    "start": "2708000",
    "end": "2715160"
  },
  {
    "text": "glance down um through some of these so we pre-process um so here's the",
    "start": "2715160",
    "end": "2720480"
  },
  {
    "text": "distribution of race now um I dropped out everything except for the africanamerican and Caucasian defendants",
    "start": "2720480",
    "end": "2727640"
  },
  {
    "text": "why because they have the highest spread um but also this was what the uh",
    "start": "2727640",
    "end": "2732920"
  },
  {
    "text": "propublica um research was looking at so we first we start off with unequal",
    "start": "2732920",
    "end": "2739440"
  },
  {
    "text": "groups right and then we can start looking at a distribution of",
    "start": "2739440",
    "end": "2745000"
  },
  {
    "text": "score by Race So orange if you can't read is the",
    "start": "2745000",
    "end": "2750640"
  },
  {
    "text": "white defendants and the blue bars are the black defendants and the the black",
    "start": "2750640",
    "end": "2756440"
  },
  {
    "text": "defendants are actually fairly equal right across the distribution the white",
    "start": "2756440",
    "end": "2761720"
  },
  {
    "text": "defendants have a huge Spike on the low",
    "start": "2761720",
    "end": "2766559"
  },
  {
    "text": "end we can also look at the prior count so for zero priers count the",
    "start": "2767599",
    "end": "2775040"
  },
  {
    "text": "higher Distribution on the low end is actually held by black",
    "start": "2775040",
    "end": "2780400"
  },
  {
    "text": "defendants and then we can also look at the average scores by race and gender um",
    "start": "2784559",
    "end": "2789720"
  },
  {
    "text": "and I think oh I've got some more okay so then the actual and predicted recidivism rates so on the orange bar um",
    "start": "2789720",
    "end": "2797680"
  },
  {
    "text": "so on the left side we have our black defendants on the right side we have our white defendants the blue bar is the",
    "start": "2797680",
    "end": "2803880"
  },
  {
    "text": "ground truth from propublica's study this was um what the 2-year recidivism",
    "start": "2803880",
    "end": "2810640"
  },
  {
    "text": "positive yes we went on orange was the system predicted",
    "start": "2810640",
    "end": "2817119"
  },
  {
    "text": "see some differences there right so in the sake of time",
    "start": "2818400",
    "end": "2824599"
  },
  {
    "text": "here got zoom in again so we're going to look at just a couple things on",
    "start": "2824599",
    "end": "2830920"
  },
  {
    "text": "pre-processing or uh methods to mitigate",
    "start": "2830920",
    "end": "2838240"
  },
  {
    "text": "here okay so before we get into modeling this is if we're looking at the ratio of",
    "start": "2838240",
    "end": "2844880"
  },
  {
    "text": "the compass system itself as provided by propublica okay so we are using the um",
    "start": "2844880",
    "end": "2852119"
  },
  {
    "text": "that predicted recidivism from the system which when we model we use that as our Target column because we're trying to match the system and then we",
    "start": "2852119",
    "end": "2859760"
  },
  {
    "text": "use the 2-year recidivism as the ground truth and our results are exactly what",
    "start": "2859760",
    "end": "2865839"
  },
  {
    "text": "prop publica reports that um black defendants are about two times more",
    "start": "2865839",
    "end": "2872119"
  },
  {
    "text": "likely to receive a false positive rate so on this top data frame up here here that's our top um entry and then whites",
    "start": "2872119",
    "end": "2881200"
  },
  {
    "text": "so they have the 23 ratio and then on the false negative rate the second column we can see it's it's flipped",
    "start": "2881200",
    "end": "2889160"
  },
  {
    "text": "white defendants are about two times more likely to receive a false negative rate now we can also look at this in",
    "start": "2889160",
    "end": "2895040"
  },
  {
    "text": "ratio and the ratio I took was um if we look at from White defendants to Black",
    "start": "2895040",
    "end": "2900200"
  },
  {
    "text": "defendants so meaning that a result that is greater than one indicates how much",
    "start": "2900200",
    "end": "2905720"
  },
  {
    "text": "the rate is higher for white defendants than for black defendants and if the",
    "start": "2905720",
    "end": "2911079"
  },
  {
    "text": "result is less than one this is how much higher the rate is for black defendants there are smoother ways of doing this",
    "start": "2911079",
    "end": "2916960"
  },
  {
    "text": "but in the interest of just kind of giving you guys the ratio I wish that I could pull up the REI dashboard because",
    "start": "2916960",
    "end": "2922319"
  },
  {
    "text": "it's beautiful and visualizes these bars for you but alas we have to look at numbers um okay so then let's go ahead",
    "start": "2922319",
    "end": "2929480"
  },
  {
    "text": "and model it um we did a very simple logistic",
    "start": "2929480",
    "end": "2934760"
  },
  {
    "text": "regression for the um ease and explainability of this but this is um so again I think",
    "start": "2934760",
    "end": "2942520"
  },
  {
    "text": "we're able to achieve like 76% accuracy um the ratio looks different but we're",
    "start": "2942520",
    "end": "2949640"
  },
  {
    "text": "still pretty close to um to the the spread we still have about two times",
    "start": "2949640",
    "end": "2955839"
  },
  {
    "text": "more for white defendants on false negative rate about two times more for black defendants on false positive",
    "start": "2955839",
    "end": "2962319"
  },
  {
    "text": "Freight okay well what about that Simpsons Paradox what if our model is",
    "start": "2962319",
    "end": "2968599"
  },
  {
    "text": "blind so we drop out race before we train and our results are interestingly",
    "start": "2968599",
    "end": "2976960"
  },
  {
    "text": "almost the same as the original Compass",
    "start": "2976960",
    "end": "2983000"
  },
  {
    "text": "system okay what if we uh balance the groups what if we have the same number",
    "start": "2984480",
    "end": "2991160"
  },
  {
    "text": "of entities in the white defendant group versus the black defendant group was",
    "start": "2991160",
    "end": "2996480"
  },
  {
    "text": "that",
    "start": "2996480",
    "end": "2998640"
  },
  {
    "text": "do she got kind of worse",
    "start": "3002880",
    "end": "3010520"
  },
  {
    "text": "why because it doesn't matter how equal the groups are if their targets are",
    "start": "3014040",
    "end": "3021839"
  },
  {
    "text": "unequal in fact if we're blindly sampling just you know give me this",
    "start": "3021839",
    "end": "3028200"
  },
  {
    "text": "percentage of black defendants to match the smaller group of white defendants that we have or probably missing really",
    "start": "3028200",
    "end": "3036119"
  },
  {
    "text": "important entities because we're not evening our spread so if we're going to go through this we have to understand we",
    "start": "3036119",
    "end": "3042359"
  },
  {
    "text": "can make it worse if we don't know what we're doing so what we want to do is balance",
    "start": "3042359",
    "end": "3048400"
  },
  {
    "text": "the target ratio we're almost the same there",
    "start": "3048400",
    "end": "3058319"
  },
  {
    "text": "now accuracy of the system took a little bit of a hit but we were able to increase",
    "start": "3059319",
    "end": "3066359"
  },
  {
    "text": "fairness and that was just from a couple lines of code the problem is is that that means our data set has just gotten",
    "start": "3066359",
    "end": "3074079"
  },
  {
    "text": "a lot smaller because we have had to ensure that we have the same negative and",
    "start": "3074079",
    "end": "3080839"
  },
  {
    "text": "positive outcomes across both groups and while this is fine for a demo that might",
    "start": "3080839",
    "end": "3088240"
  },
  {
    "text": "not operate well in real life when we need as much data as possible to build",
    "start": "3088240",
    "end": "3093280"
  },
  {
    "text": "our systems but it goes to prove a point that the way that you sample and you",
    "start": "3093280",
    "end": "3098480"
  },
  {
    "text": "collect your data does in fact make a",
    "start": "3098480",
    "end": "3102240"
  },
  {
    "text": "difference and screen mirroring off",
    "start": "3106520",
    "end": "3114920"
  },
  {
    "text": "well smoother than it was sometimes so small winds all right I said this was a tech",
    "start": "3123760",
    "end": "3130599"
  },
  {
    "text": "or it was a socio technical problem right we walk through a technical approach but it's a socio technical",
    "start": "3130599",
    "end": "3138520"
  },
  {
    "text": "problem that needs a socio technical approach what do I mean oh I mean a lot of things but three of the most",
    "start": "3138520",
    "end": "3144839"
  },
  {
    "text": "important um we we need multi-stakeholders okay we need diverse groups and individuals that are have a",
    "start": "3144839",
    "end": "3152440"
  },
  {
    "text": "vested interest in the development deployment and impact of our",
    "start": "3152440",
    "end": "3157559"
  },
  {
    "text": "systems we need a human in the",
    "start": "3157559",
    "end": "3161440"
  },
  {
    "text": "loop these machines can't make these decisions on their own for human related",
    "start": "3162799",
    "end": "3168520"
  },
  {
    "text": "problems we need to measure the risk of the application the risk of the decisions and when the risk is high we",
    "start": "3168520",
    "end": "3176119"
  },
  {
    "text": "need humans involved still and of course human- centered design approach so this is an ongoing",
    "start": "3176119",
    "end": "3182400"
  },
  {
    "text": "and iterative design process that prioritizes the needs experiences and",
    "start": "3182400",
    "end": "3188040"
  },
  {
    "text": "well-beings of the users and people impacted by the system uh nist has a",
    "start": "3188040",
    "end": "3193319"
  },
  {
    "text": "wonderful framework for this if it's something that you're curious about um you can search nist uh bias management",
    "start": "3193319",
    "end": "3200799"
  },
  {
    "text": "or human- centered design processes and they um have different Frameworks that are really helpful to take back to your",
    "start": "3200799",
    "end": "3206960"
  },
  {
    "text": "team and say Hey what if we designed like this what what sort of um learnings",
    "start": "3206960",
    "end": "3212280"
  },
  {
    "text": "can we take away and can we incorporate into our design",
    "start": "3212280",
    "end": "3217160"
  },
  {
    "text": "processes managing talk about governance right that's a whole other talk but we",
    "start": "3219000",
    "end": "3224680"
  },
  {
    "text": "need to move away from reactive governance and towards proactive",
    "start": "3224680",
    "end": "3229720"
  },
  {
    "text": "governance what do I mean a lot of times when governance gets involved in AI systems it's when it's too late it's",
    "start": "3229720",
    "end": "3235760"
  },
  {
    "text": "when there's been a problem it's when there's been a lawsuit instead we need to start in",
    "start": "3235760",
    "end": "3242240"
  },
  {
    "text": "governance and we need to have structured processes we need Sops we",
    "start": "3242240",
    "end": "3247960"
  },
  {
    "text": "need continuous monitoring and evaluation we don't know how much bias is in our system unless we're trying to",
    "start": "3247960",
    "end": "3254079"
  },
  {
    "text": "measure and evaluate it right we have to start there we have to have somebody that tells us you can take time to do",
    "start": "3254079",
    "end": "3262359"
  },
  {
    "text": "this you can bake it into your process we need feedback channels we need",
    "start": "3262359",
    "end": "3268480"
  },
  {
    "text": "Anonymous ways for users and for team members to feel safe enough to say hey I think there might be a problem",
    "start": "3268480",
    "end": "3275920"
  },
  {
    "text": "here and in our Process Management how do we bake in time now we all kind of",
    "start": "3275920",
    "end": "3281319"
  },
  {
    "text": "giggle cuz like we're Debs right and we know even trying to get time for Tech debt is hard what about ethical",
    "start": "3281319",
    "end": "3290880"
  },
  {
    "text": "debt so it starts with educ again for the last time this is a soci",
    "start": "3294280",
    "end": "3302119"
  },
  {
    "text": "technical issue okay and on the note on ethical debt often times depending on",
    "start": "3302119",
    "end": "3308640"
  },
  {
    "text": "your your corporation structure it can be really hard to make waves at an",
    "start": "3308640",
    "end": "3314480"
  },
  {
    "text": "individual level you can go out and get all the education that you want in the topic but",
    "start": "3314480",
    "end": "3319839"
  },
  {
    "text": "if someone above does not prioritize the time that you're asking for to build in these checks and balances",
    "start": "3319839",
    "end": "3327400"
  },
  {
    "text": "it's going to get stuck in the pipeline so it truly does need to come from a top down approach and a top down",
    "start": "3327400",
    "end": "3336039"
  },
  {
    "text": "appreciation for the problem and again if the eyes glaze over on ethics personal tip talk about risk",
    "start": "3336039",
    "end": "3343599"
  },
  {
    "text": "management if anyone wants to take a photo of these this side I've got a lot of books that I love but um these are",
    "start": "3345280",
    "end": "3351680"
  },
  {
    "text": "some of my favorites right now uh I just finished this the world I see um and it",
    "start": "3351680",
    "end": "3357200"
  },
  {
    "text": "was absolutely fabulous does anyone know who uh Dr Lee is um she was one of the original data",
    "start": "3357200",
    "end": "3363480"
  },
  {
    "text": "scientists that put together imag net uh so she's absolutely fabulous and this is",
    "start": "3363480",
    "end": "3368880"
  },
  {
    "text": "kind of her life story paired with um her understanding the need for responsible AI uh so I'll give that one",
    "start": "3368880",
    "end": "3376000"
  },
  {
    "text": "little shout out also uh biased is a great place to start it's actually not really a technical book it's more of a",
    "start": "3376000",
    "end": "3381760"
  },
  {
    "text": "social book just kind of gets you thinking about spaces that maybe you haven't had a chance to think of",
    "start": "3381760",
    "end": "3388200"
  },
  {
    "text": "before and now a note from our sponsors um so I work for Kema we're a",
    "start": "3388200",
    "end": "3396480"
  },
  {
    "text": "design and Technology consultancy um we do everything from design Sprints to full product builds if",
    "start": "3396480",
    "end": "3403440"
  },
  {
    "text": "you'd like to get in touch with someone from our sales team that is our QR",
    "start": "3403440",
    "end": "3408799"
  },
  {
    "text": "code and then also um I'm a member of the center for practical bioethics we're taking a three- tiered approach to",
    "start": "3408839",
    "end": "3415280"
  },
  {
    "text": "implementing ethical um systems within healthare organizations and hospitals um so we",
    "start": "3415280",
    "end": "3421559"
  },
  {
    "text": "look at process Improvement tools um Education and Training and then executive level",
    "start": "3421559",
    "end": "3426839"
  },
  {
    "text": "support and if you'd like to reach us it's a QR code do a lot of different trainings um evaluations procurement um",
    "start": "3426839",
    "end": "3435039"
  },
  {
    "text": "and connecting with consultancies that can do the work and if you'd like to get in touch",
    "start": "3435039",
    "end": "3442039"
  },
  {
    "text": "with me that's my LinkedIn",
    "start": "3442039",
    "end": "3446319"
  },
  {
    "text": "[Applause]",
    "start": "3448950",
    "end": "3457720"
  },
  {
    "text": "and I only saved you a minute and a half for question comments are cheap",
    "start": "3457720",
    "end": "3465119"
  },
  {
    "text": "shots any do we know a lot of model I",
    "start": "3468760",
    "end": "3476839"
  },
  {
    "text": "companies are American based how do we handle cultural",
    "start": "3476839",
    "end": "3483720"
  },
  {
    "text": "differences so things that are perfectly normal or not okay in the States but",
    "start": "3483720",
    "end": "3489599"
  },
  {
    "text": "they oppos sit at other parts of the world",
    "start": "3489599",
    "end": "3495000"
  },
  {
    "text": "yeah lasts yeah and I wish I had an answer for you I don't",
    "start": "3495000",
    "end": "3502640"
  },
  {
    "text": "um it's it's kind of funny too because if you look at even regular ation and policy it differs a lot right it differs",
    "start": "3502640",
    "end": "3509480"
  },
  {
    "text": "across borders and yet technology does not have borders so how do we handle that um and that's a governance question",
    "start": "3509480",
    "end": "3516839"
  },
  {
    "text": "but the best thing we can do is is digging into it right raising problems there have been um so I think yesterday",
    "start": "3516839",
    "end": "3523799"
  },
  {
    "text": "it was actually um one of the stable Fusion models got removed from hugging face and it was the model that had a lot",
    "start": "3523799",
    "end": "3530400"
  },
  {
    "text": "of Cam child sexual abuse material contained within it why because enough people were like hey",
    "start": "3530400",
    "end": "3536880"
  },
  {
    "text": "guys can someone remove this and it was a big enough issue that",
    "start": "3536880",
    "end": "3542559"
  },
  {
    "text": "um there's now 404 on hugging face if you try and search for that model so I do believe that there is a lot of",
    "start": "3542559",
    "end": "3548599"
  },
  {
    "text": "community-driven efforts um and these are the conversations in the field that we have to be",
    "start": "3548599",
    "end": "3554880"
  },
  {
    "text": "having and we can talk about it over a drink",
    "start": "3555200",
    "end": "3559520"
  },
  {
    "text": "later any others I think there's time for one short question",
    "start": "3561039",
    "end": "3567038"
  },
  {
    "text": "no well I will leave with one parting thought um we talked about that tra",
    "start": "3568520",
    "end": "3574880"
  },
  {
    "text": "trade-off right between accuracy and fairness okay this is a an active",
    "start": "3574880",
    "end": "3581599"
  },
  {
    "text": "decision okay um it's the equivalent for taking a harder right over an easier",
    "start": "3581599",
    "end": "3588680"
  },
  {
    "text": "wrong and we are accountable for it as individuals as companies as entities as",
    "start": "3588680",
    "end": "3594480"
  },
  {
    "text": "communities and so if there's anything that I'd like you to take away from this um it's that each one of us has a call",
    "start": "3594480",
    "end": "3601520"
  },
  {
    "text": "to this problem to educating others and working on",
    "start": "3601520",
    "end": "3606680"
  },
  {
    "text": "it and thank you [Applause]",
    "start": "3606680",
    "end": "3617240"
  },
  {
    "text": "guys oh you're fine",
    "start": "3617240",
    "end": "3621880"
  }
]