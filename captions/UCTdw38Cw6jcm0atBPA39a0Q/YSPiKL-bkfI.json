[
  {
    "text": "hi everyone I'm Joe Albahari over from",
    "start": "9500",
    "end": "15320"
  },
  {
    "text": "in Australia I'm the author of c-sharp in a nutshell and if does anyone here",
    "start": "15320",
    "end": "21650"
  },
  {
    "text": "use link pad I hope that's good good show of hands is actually and Norway is the second highest per capita user of",
    "start": "21650",
    "end": "29779"
  },
  {
    "text": "link pad in the world it's one of your",
    "start": "29779",
    "end": "34790"
  },
  {
    "text": "neighbors is their highest per capita so I want to ask who remembers doing this",
    "start": "34790",
    "end": "41900"
  },
  {
    "text": "as a as a kid long multiplication is",
    "start": "41900",
    "end": "47780"
  },
  {
    "text": "something our parents or teachers taught us to do and they programmed us to do",
    "start": "47780",
    "end": "54739"
  },
  {
    "text": "this in the same way that we program computers right so they pretty much fed us an algorithm you multiply this number",
    "start": "54739",
    "end": "61699"
  },
  {
    "text": "by this number and you put this digit here and you add these things up together right that's a an algorithm and",
    "start": "61699",
    "end": "67340"
  },
  {
    "text": "if we were to program a computer to do this and that's not such a silly question because the first computer I",
    "start": "67340",
    "end": "74179"
  },
  {
    "text": "ever had I programmed in machine code and there was no there was it was no multiplication so I ended up doing",
    "start": "74179",
    "end": "80570"
  },
  {
    "text": "something kind of like this to multiply numbers and you're pretty much going through the same kind of algorithm you",
    "start": "80570",
    "end": "86299"
  },
  {
    "text": "would go through the way we were taught to do this but we take it for granted that when we learn to do this we already",
    "start": "86299",
    "end": "92990"
  },
  {
    "text": "knew how to recognize that that was a for right and how did how is it that we",
    "start": "92990",
    "end": "98840"
  },
  {
    "text": "knew that we our parents didn't program us like a computer with an algorithm to",
    "start": "98840",
    "end": "104689"
  },
  {
    "text": "do this we learn through example just by seeing lots of these digits we figured it out right now if you want to get a",
    "start": "104689",
    "end": "112670"
  },
  {
    "text": "computer to recognize handwritten digits and you would have come up with an algorithm that's really difficult to do",
    "start": "112670",
    "end": "119000"
  },
  {
    "text": "right so why can't we get the computer to learn in the same way and learn by",
    "start": "119000",
    "end": "124159"
  },
  {
    "text": "example and that's exactly what machine learning is right with machine learning we have lots of input and output and it",
    "start": "124159",
    "end": "130459"
  },
  {
    "text": "works out the algorithm gradually ters from looking at lots of samples and a",
    "start": "130459",
    "end": "135530"
  },
  {
    "text": "particular kind of machine learning algorithm is own artificial neural network as with well West with nearly",
    "start": "135530",
    "end": "143720"
  },
  {
    "text": "all machine learning architectures you can kind of look at it like this there's",
    "start": "143720",
    "end": "148730"
  },
  {
    "text": "an algorithm there whose behavior depends on a whole series of internal constants",
    "start": "148730",
    "end": "156110"
  },
  {
    "text": "these are really just floating point numbers and then as we train it by",
    "start": "156110",
    "end": "161900"
  },
  {
    "text": "feeding in lots of samples it works out the optimum values of all of those constants it's effectively how it works",
    "start": "161900",
    "end": "168920"
  },
  {
    "text": "so a machine a an artificial neural network they're arranged like this so",
    "start": "168920",
    "end": "174050"
  },
  {
    "text": "here's an example on the Left these these white circles these represent the inputs and they might correspond to all",
    "start": "174050",
    "end": "180860"
  },
  {
    "text": "of the pixels in an image so you might have values from 0 to 255 to represent",
    "start": "180860",
    "end": "186440"
  },
  {
    "text": "the level of gray the grayscale then that connects to another layer of neurons which in connects to another",
    "start": "186440",
    "end": "193400"
  },
  {
    "text": "layer which is where we get the output from will go into this architecture in more detail later but the important",
    "start": "193400",
    "end": "200030"
  },
  {
    "text": "thing for now is that those internal constants are represented by the sensitivity of the neurons we call that",
    "start": "200030",
    "end": "207260"
  },
  {
    "text": "the bias of the neurons and the strength of the interconnections between the",
    "start": "207260",
    "end": "212630"
  },
  {
    "text": "neurons we call the weights and if we represent those by colors this is what the training looks like we start out by",
    "start": "212630",
    "end": "219170"
  },
  {
    "text": "initializing those weights and biases to small random values and that's why they",
    "start": "219170",
    "end": "226100"
  },
  {
    "text": "look great at the moment because we're going to think that the color is going to represent the the weights of biases then we feed one of these numbers into",
    "start": "226100",
    "end": "232910"
  },
  {
    "text": "this like a for now because it's untrained it's gonna get that wrong you might think it's 7 so then we ask the",
    "start": "232910",
    "end": "241520"
  },
  {
    "text": "question what adjustments do we need to make sure those weights and biases to",
    "start": "241520",
    "end": "246680"
  },
  {
    "text": "make it a little bit less wrong we just make some really small adjustments to it then we do the same with another sample",
    "start": "246680",
    "end": "253790"
  },
  {
    "text": "and then we put a good 50,000 samples through that and that's called an epoch",
    "start": "253790",
    "end": "259070"
  },
  {
    "text": "when we're finished we shuffle those up into a different order and repeat it again and after several epochs gradually",
    "start": "259070",
    "end": "266360"
  },
  {
    "text": "that the weights and biases start to take shape it starts to settle down into",
    "start": "266360",
    "end": "272300"
  },
  {
    "text": "a set of values that lets us correctly infer the characters that's essentially",
    "start": "272300",
    "end": "278870"
  },
  {
    "text": "a nutshell how it works so if you want to do this most people we'll go out and use a library because",
    "start": "278870",
    "end": "284270"
  },
  {
    "text": "there are numerous libraries to do this and these libraries are the product of years of R&D and they do things you",
    "start": "284270",
    "end": "289940"
  },
  {
    "text": "can't easily do yourself like make use of the graphics processing units in your computer problem is when you start using",
    "start": "289940",
    "end": "296389"
  },
  {
    "text": "these that they're very powerful but they're all black boxes it doesn't really help you understand how they work",
    "start": "296389",
    "end": "302810"
  },
  {
    "text": "it's not particularly interesting you guys in it's all hidden away and it's rather like in if you've ever played",
    "start": "302810",
    "end": "308930"
  },
  {
    "text": "with a flight simulator and you start out with something like that and it's it's really cool you follow the",
    "start": "308930",
    "end": "314150"
  },
  {
    "text": "instructions and hopefully if you've got all the configuration correct you can take the thing off and fly it you get",
    "start": "314150",
    "end": "320120"
  },
  {
    "text": "one little thing wrong and it crashes or doesn't get off the ground and it's not very satisfying you don't know what you're doing and you come back a week",
    "start": "320120",
    "end": "326690"
  },
  {
    "text": "later you've forgotten everything all right because you never really understood what was going on it's much better after a while you go back to this",
    "start": "326690",
    "end": "332930"
  },
  {
    "text": "you start to learn how to fly that and you really get to understand the principles of flying and when you come back to this everything makes a lot more",
    "start": "332930",
    "end": "339260"
  },
  {
    "text": "sense it's a lot more fun in this session we're not just going to fly this we're going to build it from scratch",
    "start": "339260",
    "end": "344979"
  },
  {
    "text": "right so starting from the lowest unit which is in Euron so a neuron has one or",
    "start": "344979",
    "end": "353180"
  },
  {
    "text": "more inputs it needs usually stew to be useful and all these inputs and weights",
    "start": "353180",
    "end": "359750"
  },
  {
    "text": "everything these all real numbers has one or more inputs and it also has a bias so when we fly the neuron we apply",
    "start": "359750",
    "end": "367220"
  },
  {
    "text": "some numbers to those to the inputs and we multiply them by the weights and add up the bias that gives us a total input",
    "start": "367220",
    "end": "374419"
  },
  {
    "text": "simp simple linear arithmetic then we put it through a nonlinear activation",
    "start": "374419",
    "end": "379430"
  },
  {
    "text": "function and we get the output that's essentially what a neuron is now the simplest kind of neuron is called a",
    "start": "379430",
    "end": "386120"
  },
  {
    "text": "perceptron which uses for the activation function a comparator it asks a very",
    "start": "386120",
    "end": "391460"
  },
  {
    "text": "simple question is the total input greater than zero if so it outputs a 1",
    "start": "391460",
    "end": "396919"
  },
  {
    "text": "otherwise it outputs a zero so let's give an example we're going to create a",
    "start": "396919",
    "end": "402770"
  },
  {
    "text": "neuron and we're going to initialize the bias to 3 and each of the input weights to minus 2 and I'm going to put some",
    "start": "402770",
    "end": "410479"
  },
  {
    "text": "numbers in and to make it real simple we're just going to put binary numbers but of course we could put anything in",
    "start": "410479",
    "end": "415639"
  },
  {
    "text": "here we can put large small positive negative floating-point numbers so let's go through the mass with this so we",
    "start": "415639",
    "end": "422220"
  },
  {
    "text": "multiply each of those inputs by the weights and then we add in the bias so",
    "start": "422220",
    "end": "427410"
  },
  {
    "text": "that gives us a total input of 1 now 1 is greater than 0 therefore we output a",
    "start": "427410",
    "end": "433349"
  },
  {
    "text": "1 now if we just swap those two numbers around we're going to get the same",
    "start": "433349",
    "end": "438780"
  },
  {
    "text": "result because these two weights are the same right so we could 0 1 or 1 or 0",
    "start": "438780",
    "end": "444090"
  },
  {
    "text": "we're going to get the same result as before let's put two zeros in there right now when we do the maths the total",
    "start": "444090",
    "end": "452069"
  },
  {
    "text": "input is 3 which is still greater than 1 so we output a 1 and let's now put two",
    "start": "452069",
    "end": "458520"
  },
  {
    "text": "ones in there the total input is now a minus 1 right that's less than 0 so we",
    "start": "458520",
    "end": "464669"
  },
  {
    "text": "output a 0 so what we've done here is we have created a NAND gate a logic gate",
    "start": "464669",
    "end": "472319"
  },
  {
    "text": "now with an and gate Asst you can build any other kind of logic gate from the",
    "start": "472319",
    "end": "477509"
  },
  {
    "text": "NAND gate by combining them together so we can do anything with this this is infinitely powerful however we have kind",
    "start": "477509",
    "end": "483930"
  },
  {
    "text": "of cheated because we pre-programmed the weights and biases remember what makes us interesting is this can learn this",
    "start": "483930",
    "end": "491219"
  },
  {
    "text": "can figure out the weights and biases through example so let's play that out and see what happens",
    "start": "491219",
    "end": "496280"
  },
  {
    "text": "we're going to now initialize these now to small random numbers these are",
    "start": "496280",
    "end": "501539"
  },
  {
    "text": "actually quite large for but for the example this will work fine so I've initialized them now and now we're going",
    "start": "501539",
    "end": "507210"
  },
  {
    "text": "to put a couple of numbers in 0 & 1 and we're going to play that through and we're going to get an output of 0 in the",
    "start": "507210",
    "end": "514589"
  },
  {
    "text": "end because the combined inputs minus point for that's less than 0 so the",
    "start": "514589",
    "end": "519599"
  },
  {
    "text": "output 0 the correct answer is a 1 so we've got this wrong so what we ask is",
    "start": "519599",
    "end": "526350"
  },
  {
    "text": "what small adjustments do we need to make to the weights and biases to make",
    "start": "526350",
    "end": "531810"
  },
  {
    "text": "this a bit less wrong problem is with the way we've done it here is if we make",
    "start": "531810",
    "end": "537600"
  },
  {
    "text": "a any small change we make to the weights or biases we're going to get exactly the same answer because we're",
    "start": "537600",
    "end": "544050"
  },
  {
    "text": "either right or wrong there's no nothing in between there's no gray scale this is not conducive to learning right",
    "start": "544050",
    "end": "550110"
  },
  {
    "text": "either off or on it so if we look at that comparative function and we plot it it's like that right if we want to make",
    "start": "550110",
    "end": "556260"
  },
  {
    "text": "it conducive to learning we need to make it more like that so now this is kind of intermediate area this gray area so if",
    "start": "556260",
    "end": "562230"
  },
  {
    "text": "we don't get it right we can see the direction we need to go into in order to get to where we want to go this is",
    "start": "562230",
    "end": "568529"
  },
  {
    "text": "called a logistic sigmoid and this is a popular function without official neural",
    "start": "568529",
    "end": "573810"
  },
  {
    "text": "networks but the problem with it is if you look at the formula for that function that's quite expensive to",
    "start": "573810",
    "end": "581070"
  },
  {
    "text": "compute now sometimes we're going to be using 50,000 samples there are some neural networks that we're working with",
    "start": "581070",
    "end": "587100"
  },
  {
    "text": "Millions right and and with many over many epochs that's very wasteful so over the years",
    "start": "587100",
    "end": "592649"
  },
  {
    "text": "people have experimented with other functions and what they found is that there's something that works really well",
    "start": "592649",
    "end": "598589"
  },
  {
    "text": "that's really cheap to calculate it's more complicated to pronounce than it is to calculate and it's called a rail",
    "start": "598589",
    "end": "605399"
  },
  {
    "text": "you--or rectified linear unit and it simply x equals y if X is positive or 0",
    "start": "605399",
    "end": "612540"
  },
  {
    "text": "if X is negative variation on that which works in most cases slightly better is a",
    "start": "612540",
    "end": "618390"
  },
  {
    "text": "leaky le where FX is negative we divide it by a hundred that gives us a little",
    "start": "618390",
    "end": "624060"
  },
  {
    "text": "bit of a slope on that side right so if we if we now plug in a leaky aurelio or",
    "start": "624060",
    "end": "630089"
  },
  {
    "text": "a logistic sigmoid into our our neuron we can now train it the only thing we",
    "start": "630089",
    "end": "636480"
  },
  {
    "text": "have to be careful of now is in the end of the day we still need to interpret the answer as a 1 or a 0 because we're",
    "start": "636480",
    "end": "642690"
  },
  {
    "text": "trying to write a NAND gate and so it's pretty simple to convert this this continuous output into a discrete output",
    "start": "642690",
    "end": "650339"
  },
  {
    "text": "we just say we want a 0 or a 1 what's it closer to so if it outputs a",
    "start": "650339",
    "end": "655589"
  },
  {
    "text": "point 2 that's closer to a zero we'd say we'll treat that as a zero anything greater than point five we'll treat that",
    "start": "655589",
    "end": "661560"
  },
  {
    "text": "as a one alright but what we've gained is the ability to measure the error if",
    "start": "661560",
    "end": "667110"
  },
  {
    "text": "it gets it wrong we know how long it is and what direction it needs to go to be more to be less wrong so in this case",
    "start": "667110",
    "end": "674310"
  },
  {
    "text": "let's suppose the desired output is zero the actual output is point two we can",
    "start": "674310",
    "end": "681360"
  },
  {
    "text": "say the error is zero point two we define the error as the actual output",
    "start": "681360",
    "end": "686410"
  },
  {
    "text": "minus the desired output nots the error can be positive or negative now it would",
    "start": "686410",
    "end": "693010"
  },
  {
    "text": "seem reasonable that we our goal is to minimize the total error across all",
    "start": "693010",
    "end": "700390"
  },
  {
    "text": "training samples right problem is that won't actually work very well at all",
    "start": "700390",
    "end": "706589"
  },
  {
    "text": "right and to see why and go through an example we imagine across this is one",
    "start": "706589",
    "end": "713829"
  },
  {
    "text": "training sample imagine our other three training samples we get exactly the same output we get zero point two for all of",
    "start": "713829",
    "end": "721450"
  },
  {
    "text": "them and imagine that the correct answer is zero for all of them so it got all of them correct all of them we have an",
    "start": "721450",
    "end": "727870"
  },
  {
    "text": "error of point two right if the error is less than point five we'll go back a slide if the error is less than point",
    "start": "727870",
    "end": "733899"
  },
  {
    "text": "five we get it right errors more than point five we get it wrong so in this case four of them",
    "start": "733899",
    "end": "739990"
  },
  {
    "text": "errors point two and all of them have got the more correct imagine another scenario now the first three the error",
    "start": "739990",
    "end": "747220"
  },
  {
    "text": "is zero but the last one the error is point six so now it's got one of them",
    "start": "747220",
    "end": "752350"
  },
  {
    "text": "wrong but look at the total error the total error is lower in the second case so if we program the machine to minimize",
    "start": "752350",
    "end": "760839"
  },
  {
    "text": "the total error across all training samples it's going to favor that second outcome over the first outcome right it",
    "start": "760839",
    "end": "768220"
  },
  {
    "text": "does exactly what we tell it so what we need to do instead is we need some way",
    "start": "768220",
    "end": "773410"
  },
  {
    "text": "to punish the big errors more than the small errors the easiest way to do that",
    "start": "773410",
    "end": "780100"
  },
  {
    "text": "and by far the cheapest is to is to square the error first all right so if",
    "start": "780100",
    "end": "785440"
  },
  {
    "text": "we square all those errors before adding them up the first case we get point one six the second case we get point three",
    "start": "785440",
    "end": "791770"
  },
  {
    "text": "six so now it will favor the first case so now we're no longer minimizing the",
    "start": "791770",
    "end": "798640"
  },
  {
    "text": "total error per minute we need another term to describe what we're minimizing and the term that we use is called the",
    "start": "798640",
    "end": "804820"
  },
  {
    "text": "loss right or the cost that's a synonym right the loss is whatever we're trying to minimize we can define that loss",
    "start": "804820",
    "end": "811600"
  },
  {
    "text": "function however we like but the most popular loss function is the error squared",
    "start": "811600",
    "end": "817170"
  },
  {
    "text": "simple and easy to calculate right a few nuances rather than defining losses to",
    "start": "817170",
    "end": "823379"
  },
  {
    "text": "some of the error squared we usually define it as half the sum of the error square that makes no difference",
    "start": "823379",
    "end": "829739"
  },
  {
    "text": "if whether you minimize X or you minimize half of X you can end up in the same place the difference is it creates",
    "start": "829739",
    "end": "836339"
  },
  {
    "text": "a nice mathematical simplicity and symmetry that we'll get to later another",
    "start": "836339",
    "end": "841949"
  },
  {
    "text": "nuance is we could also instead of calculating that the the total the sum of the error squared we could calculate",
    "start": "841949",
    "end": "847619"
  },
  {
    "text": "the mean of the error squared again exactly the same result you'll sometimes see the term MSE mean squared error",
    "start": "847619",
    "end": "854910"
  },
  {
    "text": "that's exactly the same thing another interesting thing is that we don't",
    "start": "854910",
    "end": "860129"
  },
  {
    "text": "usually calculate the loss right how many of you here drive that's a lot of",
    "start": "860129",
    "end": "866639"
  },
  {
    "text": "hands I guess because the taxis are so expensive here right all right now most of you will probably consider yourselves",
    "start": "866639",
    "end": "872790"
  },
  {
    "text": "good drivers and yet if I were to ask you what is the probability you're going",
    "start": "872790",
    "end": "878009"
  },
  {
    "text": "to have an accident on the way here you're not going to be able to give me a number right and yet you could be",
    "start": "878009",
    "end": "883649"
  },
  {
    "text": "constantly doing things every day to monitor your your safety and improving your safety you could you could be it",
    "start": "883649",
    "end": "889679"
  },
  {
    "text": "you can minimize something and improve it without actually measuring it right so in the same way if you look for the",
    "start": "889679",
    "end": "895949"
  },
  {
    "text": "code that measures the loss in this you won't necessarily find it because we don't actually need to calculate it in",
    "start": "895949",
    "end": "901709"
  },
  {
    "text": "order to minimize it so let's get back to our example and we'll plug a leak here Lu in now to our neuron so we're",
    "start": "901709",
    "end": "910649"
  },
  {
    "text": "gonna little eyes it again small random values for the weights and biases I'm gonna put a 1 in a 0 in there the",
    "start": "910649",
    "end": "916980"
  },
  {
    "text": "combined inputs now 0.3 now we plug it into that leaky earlier now what should",
    "start": "916980",
    "end": "922679"
  },
  {
    "text": "the output be if the inputs 0.3 you can plug it through a leak here Lu what would the output be yeah that's it point",
    "start": "922679",
    "end": "933329"
  },
  {
    "text": "three it's greater than zero anything greater than zero we just take the x equals y so it's seven point three now",
    "start": "933329",
    "end": "941069"
  },
  {
    "text": "the correct answer of course is one right one and zero is one so the actual",
    "start": "941069",
    "end": "946379"
  },
  {
    "text": "output is point three we can now quantify the error as minus 0.7 so we can see now that what we need",
    "start": "946379",
    "end": "956040"
  },
  {
    "text": "to do is increase the output how do we know because we always want to go in the",
    "start": "956040",
    "end": "961920"
  },
  {
    "text": "opposite direction of the error right so we want less of the arrow not more of it right so we slip the sign we want to",
    "start": "961920",
    "end": "968250"
  },
  {
    "text": "increase this and maybe if we couldn't make this point 301 that would be a good",
    "start": "968250",
    "end": "973320"
  },
  {
    "text": "thing right it's just a wine back a step we want to adjust these weights and biases right to",
    "start": "973320",
    "end": "979740"
  },
  {
    "text": "make this a bit less wrong but what we need to know first how do we adjust them to do that we start from the right when",
    "start": "979740",
    "end": "986070"
  },
  {
    "text": "we work backwards work from right to left so we start with this and we say well the error is negative that means we",
    "start": "986070",
    "end": "992880"
  },
  {
    "text": "need to increase this slightly so be good if we could make this point 301 we can't control this but we can control we",
    "start": "992880",
    "end": "1000110"
  },
  {
    "text": "can work backwards so we can say what do we need to do to this to this combined input over here and because we're on the",
    "start": "1000110",
    "end": "1007130"
  },
  {
    "text": "positive side of the relia when the x equals y it's exactly the same thing so if we increase the output that means we",
    "start": "1007130",
    "end": "1013580"
  },
  {
    "text": "also need to increase the input if we're on the leaky side of the rally right",
    "start": "1013580",
    "end": "1018590"
  },
  {
    "text": "then we will be dividing by a hundred right so it would be it would be a bit different so we need to increase the",
    "start": "1018590",
    "end": "1024949"
  },
  {
    "text": "output so remember this is for one training sample we've got three others and then we're going to go over them",
    "start": "1024950",
    "end": "1031130"
  },
  {
    "text": "again and again and again right so we've got to take into account the wishes of the other training samples as well so",
    "start": "1031130",
    "end": "1037040"
  },
  {
    "text": "one way in which we could do this is we could repeat this exercise for the other three training samples and take votes",
    "start": "1037040",
    "end": "1043610"
  },
  {
    "text": "saying who wants to increase the bias who wants to decrease this weight and then whichever gets the most votes wins",
    "start": "1043610",
    "end": "1049880"
  },
  {
    "text": "and then we make an adjustment and continue but problem with doing that is we would be minimizing the total error",
    "start": "1049880",
    "end": "1057350"
  },
  {
    "text": "not the total loss if everyone gets one vote then it doesn't matter whether you really need to make an adjustment or not",
    "start": "1057350",
    "end": "1063710"
  },
  {
    "text": "right you're all getting the same say and so we need to be a bit smarter in",
    "start": "1063710",
    "end": "1068720"
  },
  {
    "text": "how we do it so what we do instead is we give a variable number of votes to each",
    "start": "1068720",
    "end": "1076280"
  },
  {
    "text": "training instance and that vote is simply the negative error so this one",
    "start": "1076280",
    "end": "1082040"
  },
  {
    "text": "gets zero point seven votes because it's out by points 'even if it was owned if the error was minus 0.1 it would only get 0.1 votes so",
    "start": "1082040",
    "end": "1090380"
  },
  {
    "text": "that way we take much bigger action when there's a bigger necessity right when this when this are a bigger error right",
    "start": "1090380",
    "end": "1097370"
  },
  {
    "text": "so if the output gets point 7 votes the input gets point 7 on the positive side of the rally if it was on the negative",
    "start": "1097370",
    "end": "1103970"
  },
  {
    "text": "side of the rally the input would get zero point 0:07 votes who would divide by a hundred",
    "start": "1103970",
    "end": "1111400"
  },
  {
    "text": "that would mean that would that our we would be doing almost nothing right we've been making almost no changes",
    "start": "1111400",
    "end": "1117620"
  },
  {
    "text": "which is what we want right it's very when you're training these things you want to be very careful because if you",
    "start": "1117620",
    "end": "1123470"
  },
  {
    "text": "make anything at any change you make can break it for other training samples so if it's not going to make any difference",
    "start": "1123470",
    "end": "1129440"
  },
  {
    "text": "for you you don't to break it for anyone else okay so the input gets point seven votes now we need to make we could now",
    "start": "1129440",
    "end": "1136760"
  },
  {
    "text": "go to all the other training samples and collect up the votes but we don't need to there's an easier way to do it right",
    "start": "1136760",
    "end": "1143330"
  },
  {
    "text": "if we were to do that it's called batched learning where we collate it for all of the samples but they say there's",
    "start": "1143330",
    "end": "1148640"
  },
  {
    "text": "a more popular and more efficient way called stochastic learning where what we do is we make a small adjustment after",
    "start": "1148640",
    "end": "1154580"
  },
  {
    "text": "every training sample and move on and the end result is similar to what we get if we added them all together and made",
    "start": "1154580",
    "end": "1161330"
  },
  {
    "text": "one adjustment so in this case the step size is called the learning rate at",
    "start": "1161330",
    "end": "1166340"
  },
  {
    "text": "learning rate of point oh one multiply that by the input votes of point seven the adjustments point double-oh-seven so",
    "start": "1166340",
    "end": "1173810"
  },
  {
    "text": "now what we do is we take that bias of point one that now becomes put 107 we",
    "start": "1173810",
    "end": "1179240"
  },
  {
    "text": "add it to that and we also add it to this weight here which becomes a now point 207 the second weight however you",
    "start": "1179240",
    "end": "1186260"
  },
  {
    "text": "notice I've not made any adjustment to that at all why not why have not adjusted that that weight",
    "start": "1186260",
    "end": "1193390"
  },
  {
    "text": "luckily it doesn't change the output and why doesn't it change the output because this is zero it gets multiplied by zero",
    "start": "1200250",
    "end": "1206670"
  },
  {
    "text": "remember we don't make adjustments unless is actually going to help us because all will do is mug it up for the",
    "start": "1206670",
    "end": "1212160"
  },
  {
    "text": "other training samples so we've done that training sample we've made and some",
    "start": "1212160",
    "end": "1217260"
  },
  {
    "text": "changes to the to the weights and biases we just keep repeating that for all of",
    "start": "1217260",
    "end": "1223020"
  },
  {
    "text": "the training samples over and over and that's all there is to training a neuron we've covered the whole algorithm for",
    "start": "1223020",
    "end": "1231120"
  },
  {
    "text": "stochastic gradient descent all right so before we code it up though I'm going to",
    "start": "1231120",
    "end": "1237780"
  },
  {
    "text": "go in a bit more detail on a couple of things right just for the sake of mathematical curiosity one is to explain",
    "start": "1237780",
    "end": "1245100"
  },
  {
    "text": "how it is that were actually minimizing the square of the error even though we didn't calculate it and the other thing",
    "start": "1245100",
    "end": "1251640"
  },
  {
    "text": "is to explain why it is how we deal with this rally why is it that we multiplied by what is effectively the slope of this",
    "start": "1251640",
    "end": "1260400"
  },
  {
    "text": "function right we were multiplying right well the output got 0.7 votes the input",
    "start": "1260400",
    "end": "1268080"
  },
  {
    "text": "got point seven we multiplied by the slope of this function wherever it is now and explained that let's switch to a",
    "start": "1268080",
    "end": "1276660"
  },
  {
    "text": "logistic sigmoid right if we used that activation function the black line shows you what the function looks like and",
    "start": "1276660",
    "end": "1283050"
  },
  {
    "text": "this gray line shows you the slope at any point so when it's right in this point we're switching off and on it's",
    "start": "1283050",
    "end": "1289920"
  },
  {
    "text": "got the steepest slope so it's going to get the most activity so the input the output any changes in the output will",
    "start": "1289920",
    "end": "1296820"
  },
  {
    "text": "affect the input significantly but on either side of it's over here or it's over here it's kind of switching off",
    "start": "1296820",
    "end": "1302850"
  },
  {
    "text": "it's not the and it changes in the output are not going to make much changes to the input so explain how this",
    "start": "1302850",
    "end": "1307980"
  },
  {
    "text": "works imagine you're a politician and you'll try to decide on a policy for",
    "start": "1307980",
    "end": "1315450"
  },
  {
    "text": "whether Norway joins the EU because apparently there's a vacancy coming up",
    "start": "1315450",
    "end": "1320640"
  },
  {
    "text": "soon so you go from constituency to constituency visiting all your",
    "start": "1320640",
    "end": "1326880"
  },
  {
    "text": "electorates gauging the public opinion",
    "start": "1326880",
    "end": "1331950"
  },
  {
    "text": "on their gap gathering votes but you're not going to treat all electorates equally right there's some",
    "start": "1331950",
    "end": "1338160"
  },
  {
    "text": "people out on the streets demonstrating you know you are you he other words no II you know are you and you go to",
    "start": "1338160",
    "end": "1344220"
  },
  {
    "text": "another electorate and then don't care one way or the other right the strength of the sentiment it's very important so",
    "start": "1344220",
    "end": "1351540"
  },
  {
    "text": "that will be equivalent to the output votes is the strength of the sentiment as a politician you're in Iran your",
    "start": "1351540",
    "end": "1357840"
  },
  {
    "text": "learning is you go from electorate to electorate each time you go you adjust based on the on the output votes so do",
    "start": "1357840",
    "end": "1365400"
  },
  {
    "text": "do politicians have have an activation function right today do they what what you doing here when you're multiplying",
    "start": "1365400",
    "end": "1371220"
  },
  {
    "text": "by the slope of dysfunction you're applying the chain rule in calculus right if you didn't understand the chain",
    "start": "1371220",
    "end": "1377160"
  },
  {
    "text": "rule before you you know it now so do politicians do this do they apply the chain rule when that when they're trying",
    "start": "1377160",
    "end": "1384660"
  },
  {
    "text": "to decide policy they most certainly do right politicians focus their efforts",
    "start": "1384660",
    "end": "1389760"
  },
  {
    "text": "into where they get makes the most difference right where that slope is a",
    "start": "1389760",
    "end": "1395520"
  },
  {
    "text": "steepest right that's where they make that what that that's where it has the",
    "start": "1395520",
    "end": "1400650"
  },
  {
    "text": "greatest say so often they'll decide policies maybe this is in Australia I'm sure it's not like this in Norway but",
    "start": "1400650",
    "end": "1407760"
  },
  {
    "text": "often they'll decide policies to please the marginal lecturers right even at the expense of upsetting",
    "start": "1407760",
    "end": "1414270"
  },
  {
    "text": "some save seats or unwinnable seats right because that that's how they can win the election so that's pretty much",
    "start": "1414270",
    "end": "1419790"
  },
  {
    "text": "how it works in terms of the deactivation function now in terms of how is it that we're calculating that",
    "start": "1419790",
    "end": "1425700"
  },
  {
    "text": "the square that the loss the minimizing that the loss rather than the error I've",
    "start": "1425700",
    "end": "1431220"
  },
  {
    "text": "plotted here a graph showing you the error in green and on the the x-axis is",
    "start": "1431220",
    "end": "1438210"
  },
  {
    "text": "the input so when the input was 0.3 the error was minus 0.7 and you can see that",
    "start": "1438210",
    "end": "1444840"
  },
  {
    "text": "if we shift the input to the right that's going to improve the error now let's plot the loss function on here",
    "start": "1444840",
    "end": "1451830"
  },
  {
    "text": "remember loss is half of error squared that's what it looks like now it just so",
    "start": "1451830",
    "end": "1457650"
  },
  {
    "text": "happens very nice coincidence that every point on that graph the slope of the",
    "start": "1457650",
    "end": "1464070"
  },
  {
    "text": "loss function is exactly the error right because when you differentiate half white cause half x squared to get y",
    "start": "1464070",
    "end": "1471630"
  },
  {
    "text": "equals x so what we're doing is with when we take a step to the right in proportion to the error we're taking a",
    "start": "1471630",
    "end": "1479520"
  },
  {
    "text": "step in proportion to the slope of the loss function so you imagine that you're",
    "start": "1479520",
    "end": "1485040"
  },
  {
    "text": "sitting that's a slide you're sitting on that on that slide you're going to go down at a speed proportional to its",
    "start": "1485040",
    "end": "1492660"
  },
  {
    "text": "slope right and eventually you'll go down further and further and then you'll slow down as you get close to the bottom",
    "start": "1492660",
    "end": "1498150"
  },
  {
    "text": "cuz a lot of steep and you'll settle down where that loss is a minimum so you're doing a gradient descent on the",
    "start": "1498150",
    "end": "1505260"
  },
  {
    "text": "loss even though we haven't actually calculated it that's what we're doing so",
    "start": "1505260",
    "end": "1511070"
  },
  {
    "text": "because we're doing stochastic gradient descent we only do we only make one step before changing over to a different",
    "start": "1511070",
    "end": "1518430"
  },
  {
    "text": "curve right different sample different training datum once we repeat the same thing we might now go the errand our",
    "start": "1518430",
    "end": "1524850"
  },
  {
    "text": "might be 0.35 so we now take a step to the left it's only half as much now because the slopes only half as much as",
    "start": "1524850",
    "end": "1531480"
  },
  {
    "text": "we had before okay so that the end result that we get is the same as if we",
    "start": "1531480",
    "end": "1538020"
  },
  {
    "text": "were doing a gradient descent on the combined loss function for all of the samples together so that's the mass",
    "start": "1538020",
    "end": "1545820"
  },
  {
    "text": "behind it then without any without too many symbols so to summarize for each training sample",
    "start": "1545820",
    "end": "1551790"
  },
  {
    "text": "we determine the slope of the loss function at the current position right remember this is really easy in practice",
    "start": "1551790",
    "end": "1557340"
  },
  {
    "text": "because the loss function is simply the error it's minus the error right and we step by the following amount the slope",
    "start": "1557340",
    "end": "1563550"
  },
  {
    "text": "times our learning rate and that's it so I think it's time to code something up and we're going to do this in link pad 6",
    "start": "1563550",
    "end": "1571380"
  },
  {
    "text": "which is I've just I've released the first preview a few days ago this runs",
    "start": "1571380",
    "end": "1577260"
  },
  {
    "text": "on dotnet call three so on preview six so even but there might be it might be a little bit unstable so bear with me I've",
    "start": "1577260",
    "end": "1583830"
  },
  {
    "text": "got link pants five here in case it all crashes and burns so we could go to that we'll see how it goes so first thing",
    "start": "1583830",
    "end": "1590370"
  },
  {
    "text": "we're gonna train this NAND gate I'm gonna have a look at the clock of critic classes for this hope is actually pretty",
    "start": "1590370",
    "end": "1595500"
  },
  {
    "text": "good for for writing this in in terms of creating code that's fairly easy to follow",
    "start": "1595500",
    "end": "1600940"
  },
  {
    "text": "so I've hard coded two weights onions gonna have to do weights it's gonna have",
    "start": "1600940",
    "end": "1606039"
  },
  {
    "text": "a bias and I initialize the weights and biases to small random numbers now when",
    "start": "1606039",
    "end": "1614289"
  },
  {
    "text": "you're doing low P because part of the feature of our PS as contrast to",
    "start": "1614289",
    "end": "1619629"
  },
  {
    "text": "functional programming is we have state and we mutate it right and we do that proudly because it's good for",
    "start": "1619629",
    "end": "1625330"
  },
  {
    "text": "performance reasons in this case but when you're doing it you want some hygiene over your states so it's very good to separate long term state from",
    "start": "1625330",
    "end": "1633039"
  },
  {
    "text": "short-term or transient state by putting them in separate classes it's good practice I mean in this case the the",
    "start": "1633039",
    "end": "1640239"
  },
  {
    "text": "long term state is the weights and biases right this changes very slowly as we trained it and then what we've",
    "start": "1640239",
    "end": "1646629"
  },
  {
    "text": "trained it they're kind of become constants we can save them to a file and load them back up again and they don't",
    "start": "1646629",
    "end": "1652149"
  },
  {
    "text": "change while we're actually using it for inference the transient state is that",
    "start": "1652149",
    "end": "1657159"
  },
  {
    "text": "they're all these temporary things like the total input right that's a that's a transient that's a transient state so",
    "start": "1657159",
    "end": "1662590"
  },
  {
    "text": "I've created another class called firing neuron which captures that all the transient State for inferring for firing",
    "start": "1662590",
    "end": "1671229"
  },
  {
    "text": "and for learning so to fire we multiply input one by the weight of the first",
    "start": "1671229",
    "end": "1677590"
  },
  {
    "text": "neuron add together input 2 by the 2nd weights add up the bias that's all there is to it all right then we apply the rel",
    "start": "1677590",
    "end": "1684369"
  },
  {
    "text": "you so if the input is this is the activation function is greater than 0 it's a total input if it otherwise it's",
    "start": "1684369",
    "end": "1692499"
  },
  {
    "text": "the total input divided by a hundred if it's negative now the here's a here's",
    "start": "1692499",
    "end": "1699429"
  },
  {
    "text": "the more interesting but here's how it learns so it so they're given two inputs and an expected output and the learning",
    "start": "1699429",
    "end": "1706539"
  },
  {
    "text": "rate how we do it it's the first thing we need to find a neuron so that we know what the output is the second thing is",
    "start": "1706539",
    "end": "1713739"
  },
  {
    "text": "to work out the output votes remember all it is it's the negative of",
    "start": "1713739",
    "end": "1718899"
  },
  {
    "text": "the error right the error is the output minus the expected output so the output votes we just flip it the other way",
    "start": "1718899",
    "end": "1725049"
  },
  {
    "text": "around right that's equivalent to taking two deriving the loss function right the",
    "start": "1725049",
    "end": "1730119"
  },
  {
    "text": "slope of the loss function then we apply the chain rule so we multiply it by the slow of the rally right so if the total input",
    "start": "1730119",
    "end": "1737660"
  },
  {
    "text": "is greater than zero we multiply it by one otherwise we multiply it by 0.01 and",
    "start": "1737660",
    "end": "1743080"
  },
  {
    "text": "now for the input votes we multiply that",
    "start": "1743080",
    "end": "1748100"
  },
  {
    "text": "slope of the rel u by the output votes right so that this is well we're applying the the margin electorate test",
    "start": "1748100",
    "end": "1754160"
  },
  {
    "text": "right so if it's if the slow if you output is greater than zero it's like a marginal extra otherwise it's safe C to",
    "start": "1754160",
    "end": "1760700"
  },
  {
    "text": "unwinnable C so now we can work out the adjustment is the input votes times the learning rate that's that small figure",
    "start": "1760700",
    "end": "1766970"
  },
  {
    "text": "and we add that to the bias and we also add it to each of the weights but we",
    "start": "1766970",
    "end": "1772460"
  },
  {
    "text": "remember we always multiply by the inputs so if the input zero it's not having any effects so we don't want to",
    "start": "1772460",
    "end": "1777890"
  },
  {
    "text": "make any adjustments and if the input is negative we want to adjust it the opposite direction so that's it that's",
    "start": "1777890",
    "end": "1784430"
  },
  {
    "text": "all the code there is to it so all of this stuff you can download this so if you go to link pet go to samples there's",
    "start": "1784430",
    "end": "1791150"
  },
  {
    "text": "a link here called download import more samples you click on that link and then scroll to the bottom and then it will",
    "start": "1791150",
    "end": "1798190"
  },
  {
    "text": "they'll be at they'll be a sample you can click on to download this so we're",
    "start": "1798190",
    "end": "1803510"
  },
  {
    "text": "gonna most of the result of the Cote is just creating test at data and testing it so to manufacture data it's very easy",
    "start": "1803510",
    "end": "1810350"
  },
  {
    "text": "we can we don't have to download the data we can make it up we just create whole lot of random pairs of zeros and",
    "start": "1810350",
    "end": "1816050"
  },
  {
    "text": "ones and then we know what the desired output is by applying the NAND function so let's just dump the sample data first",
    "start": "1816050",
    "end": "1823190"
  },
  {
    "text": "out to see what it's like and we'll run that so you can see what the sample data",
    "start": "1823190",
    "end": "1828680"
  },
  {
    "text": "is so here we go got lots of random inputs and we know what that's the NAND as a desired output okay so that's how",
    "start": "1828680",
    "end": "1835790"
  },
  {
    "text": "we start out the next thing we need to do is we need to separate it into",
    "start": "1835790",
    "end": "1841910"
  },
  {
    "text": "training and test out it because it's no good if we use all our data to train it",
    "start": "1841910",
    "end": "1847250"
  },
  {
    "text": "because then we won't know how well it performs in the real world because it might be that it works really well with",
    "start": "1847250",
    "end": "1853550"
  },
  {
    "text": "the data that we've trained it with but when we give it data it's never seen before it screws up so we didn't we need",
    "start": "1853550",
    "end": "1860540"
  },
  {
    "text": "to know that it can generalize to stuff it hasn't seen so we take usually 20% of the data we lock it away that's our test",
    "start": "1860540",
    "end": "1867350"
  },
  {
    "text": "data and the the data is our training data were going to use 80% for training so basically",
    "start": "1867350",
    "end": "1873080"
  },
  {
    "text": "we're splitting up to the training and test data we're training it now we're calling the learn method on each of",
    "start": "1873080",
    "end": "1878900"
  },
  {
    "text": "these and then we doing a report on how accurate it was so let's run this now",
    "start": "1878900",
    "end": "1885050"
  },
  {
    "text": "and see it and see what the result we get it's pretty quick this tells us after 200 right we're trained it was 200",
    "start": "1885050",
    "end": "1891710"
  },
  {
    "text": "let's go up to the top total samples we fed a thousand samples we unit a learning rate of 102 and it",
    "start": "1891710",
    "end": "1898309"
  },
  {
    "text": "got of all the 200 which remember we took 20% away for the for the test data",
    "start": "1898309",
    "end": "1905360"
  },
  {
    "text": "of all of those that got them correct these are the final weights and biases",
    "start": "1905360",
    "end": "1910429"
  },
  {
    "text": "bias 1.17 and the weights were negative points or something so that worked",
    "start": "1910429",
    "end": "1915710"
  },
  {
    "text": "pretty well now because we use small random weights and biases each time you",
    "start": "1915710",
    "end": "1920720"
  },
  {
    "text": "run it it's gonna be slightly different right but each time we run it it always gets it correct we can have a bit of fun",
    "start": "1920720",
    "end": "1927170"
  },
  {
    "text": "with it save what we reduce the total samples to a hundred what happens then does that work it doesn't in this case",
    "start": "1927170",
    "end": "1934280"
  },
  {
    "text": "it's got six correct so it's got 14 correct and 6 6 wrong so not enough we",
    "start": "1934280",
    "end": "1942110"
  },
  {
    "text": "need more than at more than a hundred samples we can also try increasing the learning rate 2.5 see what happens then",
    "start": "1942110",
    "end": "1948140"
  },
  {
    "text": "the learning rates too high again this screwed up because the learning rate is too high it will oscillate so you",
    "start": "1948140",
    "end": "1953480"
  },
  {
    "text": "imagine you're on that slide and you're going down you have too much momentum you're going to go up to the other side again you need to keep oscillating so",
    "start": "1953480",
    "end": "1959870"
  },
  {
    "text": "that learning rates too high we can also try seeing we can change the function on",
    "start": "1959870",
    "end": "1964940"
  },
  {
    "text": "here and see can we teach it to do NORs we change it to an all in sort of an and we can train it to to be inaugurated and",
    "start": "1964940",
    "end": "1972790"
  },
  {
    "text": "again that works perfectly the nor gate",
    "start": "1972790",
    "end": "1978320"
  },
  {
    "text": "what about if we can train it to be an XOR gate let's try that it's failed it's",
    "start": "1978320",
    "end": "1984470"
  },
  {
    "text": "got hundred and roughly the same number right and wrong train that again it can't do a nor gate so what I saw an XOR",
    "start": "1984470",
    "end": "1992059"
  },
  {
    "text": "gate we can't train it to do an XOR gate why not",
    "start": "1992059",
    "end": "1996550"
  },
  {
    "text": "right that what to do an xor gate it needs more than one neuron right we need to feed the output of this neuron to the",
    "start": "2000580",
    "end": "2007669"
  },
  {
    "text": "input of another neuron right to do an XOR which which leads us to the next part of the session because I did",
    "start": "2007669",
    "end": "2013999"
  },
  {
    "text": "promise you a Cessna but so far were kind of got a hang glider right so we're",
    "start": "2013999",
    "end": "2020840"
  },
  {
    "text": "going to go back to the back to the presentation just so that we can see what the here we",
    "start": "2020840",
    "end": "2029659"
  },
  {
    "text": "are so this is this is this is now a a network of neurons so what we do when we connect them together it's a pretty",
    "start": "2029659",
    "end": "2035659"
  },
  {
    "text": "simple architecture so all our inputs on the Left they're not neurons at all they're just inputs they just hold a",
    "start": "2035659",
    "end": "2040849"
  },
  {
    "text": "number each of these holds that the pixel value in one of these in this case",
    "start": "2040849",
    "end": "2046279"
  },
  {
    "text": "and then we organize them into layers on the right hand side is the output layer",
    "start": "2046279",
    "end": "2051289"
  },
  {
    "text": "so we have one of these per possible answer so in this case I've got ten",
    "start": "2051289",
    "end": "2056539"
  },
  {
    "text": "digits we'll have ten output neurons so whichever one gives us the highest",
    "start": "2056539",
    "end": "2061579"
  },
  {
    "text": "number that's the one it thinks it is right and everything in the middle are",
    "start": "2061579",
    "end": "2066829"
  },
  {
    "text": "called hidden neurons or hidden layers and we organize them into layers so the input for this neuron here every input",
    "start": "2066829",
    "end": "2074089"
  },
  {
    "text": "every neuron in the hidden layer gets its inputs from every pixel every of the",
    "start": "2074089",
    "end": "2080779"
  },
  {
    "text": "input layer and similarly on the app on this next layer here on the output layer",
    "start": "2080779",
    "end": "2086299"
  },
  {
    "text": "every one of these neurons derives its input from the outputs of these neurons here so it's a pretty simple",
    "start": "2086299",
    "end": "2091940"
  },
  {
    "text": "configuration now to fire it we go from left to right right it's called",
    "start": "2091940",
    "end": "2097730"
  },
  {
    "text": "feed-forward when we're firing it so we start by firing all of the hidden the",
    "start": "2097730",
    "end": "2103130"
  },
  {
    "text": "first layer of hidden neurons and this can be done in parallel right it's very conducive to parallelization so then",
    "start": "2103130",
    "end": "2109670"
  },
  {
    "text": "we'll end up firing all of these now once we've done that we can now fire the next layer across so we can fire all the",
    "start": "2109670",
    "end": "2114829"
  },
  {
    "text": "output neurons and now if we do that we should get ho little numbers if we feed one of them in there and in this case",
    "start": "2114829",
    "end": "2121190"
  },
  {
    "text": "the number eight is the highest so it thinks it's an eight if there was",
    "start": "2121190",
    "end": "2126529"
  },
  {
    "text": "another one there that had say point four it wouldn't really matter too much right",
    "start": "2126529",
    "end": "2132200"
  },
  {
    "text": "so just as before big errors are much",
    "start": "2132200",
    "end": "2137780"
  },
  {
    "text": "worse and smaller or small errors don't matter right as long as it the highest one is the one at which it's supposed to",
    "start": "2137780",
    "end": "2143270"
  },
  {
    "text": "be that we've done pretty well so we're going to use the same loss function for character recognition we're going to use",
    "start": "2143270",
    "end": "2149960"
  },
  {
    "text": "the the square of the error as a loss function so when it comes to training",
    "start": "2149960",
    "end": "2155869"
  },
  {
    "text": "this again it's pretty much the same as what we're doing before but now we work backwards we work from right to left so",
    "start": "2155869",
    "end": "2162950"
  },
  {
    "text": "we start by training all of these output neurons and exactly what we did before the same process we did find the same",
    "start": "2162950",
    "end": "2170359"
  },
  {
    "text": "loss function we know what the desired output is we know what the actual output is we trained each of them in exactly",
    "start": "2170359",
    "end": "2175670"
  },
  {
    "text": "the same way and now we need to move to the next layer over here in this case we've got one hidden layer and there's",
    "start": "2175670",
    "end": "2181250"
  },
  {
    "text": "one piece of the puzzle we need which is how do we work out what the output votes",
    "start": "2181250",
    "end": "2186770"
  },
  {
    "text": "is for each of these hidden neurons right because we don't have an expected output anymore there's no desired output",
    "start": "2186770",
    "end": "2192349"
  },
  {
    "text": "and it's the simplest possible thing that can work you take all of the",
    "start": "2192349",
    "end": "2197809"
  },
  {
    "text": "neurons that it connects to here all of their input votes and you do a weighted",
    "start": "2197809",
    "end": "2204079"
  },
  {
    "text": "sum rights the weighted sum of all of these interconnections that's all it is",
    "start": "2204079",
    "end": "2209390"
  },
  {
    "text": "anything that was simpler than that will be leaving out something we didn't waste it that would be a bit weird and if we",
    "start": "2209390",
    "end": "2214940"
  },
  {
    "text": "ignore some of the neurons that be a bit weird so it splits a simplest possible thing that can work so what can I go",
    "start": "2214940",
    "end": "2221119"
  },
  {
    "text": "ahead now and I've coded all that up and I'll show you what looks like now the",
    "start": "2221119",
    "end": "2228319"
  },
  {
    "text": "neurons got a bit more complicated the only extra complexity there there's a lot more code in here but it's all",
    "start": "2228319",
    "end": "2234020"
  },
  {
    "text": "plumbing it's just wiring things together there's very little in terms of neuron logic in here but I'll show you",
    "start": "2234020",
    "end": "2239660"
  },
  {
    "text": "which bits are different we've now got an array of input weights instead of just having two and what the bias I've",
    "start": "2239660",
    "end": "2245869"
  },
  {
    "text": "also created a pluggable activation function so instead of hard-coding",
    "start": "2245869",
    "end": "2251119"
  },
  {
    "text": "the rail you I've created an abstract class called activator and you can subclass it so I've got all the popular",
    "start": "2251119",
    "end": "2257210"
  },
  {
    "text": "activation functions are there for you to play with this is like a playground from your network so with all white",
    "start": "2257210",
    "end": "2262940"
  },
  {
    "text": "boxes and there are no libraries in here absolutely no library's no conflict files for us all here so I'll show you now also what the",
    "start": "2262940",
    "end": "2271650"
  },
  {
    "text": "that's our neural network which contains layers of neurons that's our firing neuron now and again",
    "start": "2271650",
    "end": "2277140"
  },
  {
    "text": "compute function to compute the total input same as similar to what we had before we just we do a weighted sum this",
    "start": "2277140",
    "end": "2286800"
  },
  {
    "text": "is the method to adjust the weights and biases this is part of learning now one thing you can do in C sharp is",
    "start": "2286800",
    "end": "2292440"
  },
  {
    "text": "great in terms of you can really easily paralyze something and we want this to use all available course to do that all",
    "start": "2292440",
    "end": "2299430"
  },
  {
    "text": "you have to do is parallel dot for right however there's that there's a possibility of creating contention and a",
    "start": "2299430",
    "end": "2305520"
  },
  {
    "text": "hotspot because we have to update the weights and biases we're updating shared variables right these weights and biases",
    "start": "2305520",
    "end": "2311340"
  },
  {
    "text": "so we need to have a lock so we want to minimize the time spent in that lock so there's another nice feature of c-sharp",
    "start": "2311340",
    "end": "2316890"
  },
  {
    "text": "you can use pointers when you really need this kind of performance advantage this speeds it up about 30% by changing",
    "start": "2316890",
    "end": "2323460"
  },
  {
    "text": "from you know from using the arrays to using pointers that's our firing net",
    "start": "2323460",
    "end": "2329460"
  },
  {
    "text": "that's our feed forward and we just go from layer to layer firing all the neurons",
    "start": "2329460",
    "end": "2335150"
  },
  {
    "text": "can we call compute outputs and here is the one thing when it comes to the learning this is the one extra bit of",
    "start": "2335150",
    "end": "2341760"
  },
  {
    "text": "logic that's related to the neurons rather than to the plumbing so for the",
    "start": "2341760",
    "end": "2347640"
  },
  {
    "text": "hidden neurons we need to work out the how many output votes to give it we take the weighted sum of the next layer to",
    "start": "2347640",
    "end": "2355619"
  },
  {
    "text": "the writes input slopes right all the input votes and this is where we're doing it we're using the sum of all of",
    "start": "2355619",
    "end": "2361560"
  },
  {
    "text": "the next layers input votes times the input weights so that's pretty much it",
    "start": "2361560",
    "end": "2366810"
  },
  {
    "text": "so that's that's the logic in terms of the there's few more minor things I made little adjustments to the learning rate",
    "start": "2366810",
    "end": "2373230"
  },
  {
    "text": "for each layer I found that improve stir that the rate at which it it learns so",
    "start": "2373230",
    "end": "2378560"
  },
  {
    "text": "we need some training data now fortunately there's a public domain a really good public domain data set",
    "start": "2378560",
    "end": "2384540"
  },
  {
    "text": "called EMA nest this code automatically downloads it for you if you don't already have know we downloaded it and",
    "start": "2384540",
    "end": "2390330"
  },
  {
    "text": "it's nicely divided to 50,000 training images and 10,000 testing images right",
    "start": "2390330",
    "end": "2396690"
  },
  {
    "text": "so this creates the neural net this is the number of input inputs it",
    "start": "2396690",
    "end": "2402040"
  },
  {
    "text": "has then uncle neurons the number of inputs because of their square images it's the image width height squared this",
    "start": "2402040",
    "end": "2408970"
  },
  {
    "text": "is the number of neurons in the first hidden layer xx and because we've only",
    "start": "2408970",
    "end": "2414310"
  },
  {
    "text": "got one hidden layer for now this is the number of neurons in the output layer 10 so this is pretty dumb it's only got 30",
    "start": "2414310",
    "end": "2421240"
  },
  {
    "text": "brain cells in total but we'll see how well we can do with 30 brain cells we're going to trade it with a learning rate",
    "start": "2421240",
    "end": "2427240"
  },
  {
    "text": "of point o 1 and 10 epochs we're going to go over the whole thing 10 times and let's run it in the removable report ok",
    "start": "2427240",
    "end": "2438400"
  },
  {
    "text": "so the first epoch we've got a training accuracy of 93% it's got to 95 in the",
    "start": "2438400",
    "end": "2444340"
  },
  {
    "text": "second epoch up to 96% now this is pretty good for 30 brain cells right now",
    "start": "2444340",
    "end": "2451750"
  },
  {
    "text": "a final after the 10 epochs our final training accuracy is 96.5% but what",
    "start": "2451750",
    "end": "2458200"
  },
  {
    "text": "really matters is the test accuracy because remember how well can it generalize to stuff it hasn't seen yet",
    "start": "2458200",
    "end": "2464350"
  },
  {
    "text": "right and it's actually done pretty well it's still 95.6% accurate now this shows us the failures",
    "start": "2464350",
    "end": "2472180"
  },
  {
    "text": "at the highest loss this is the things that had the most trouble with right and you can see that's a problem note that",
    "start": "2472180",
    "end": "2478870"
  },
  {
    "text": "it supposed to be right that's but that was supposed to be a 2 that was supposed",
    "start": "2478870",
    "end": "2484300"
  },
  {
    "text": "to be a 6 I thought it was a 5 that was supposed to be a 3 well no no what was",
    "start": "2484300",
    "end": "2490420"
  },
  {
    "text": "that you're supposed to be a 3 so on so now one of the things you can do in in",
    "start": "2490420",
    "end": "2497620"
  },
  {
    "text": "link pad is you can like visualize this so you can enough written to visualize that which for this so what I call this",
    "start": "2497620",
    "end": "2503980"
  },
  {
    "text": "what it will do is it will show us all the states of the the weights and biases",
    "start": "2503980",
    "end": "2509200"
  },
  {
    "text": "as it's running so when you dump this",
    "start": "2509200",
    "end": "2514960"
  },
  {
    "text": "out you can actually watch it learning now so you can see it's starting to take",
    "start": "2514960",
    "end": "2521530"
  },
  {
    "text": "shape now the color right now this is only show this is not showing you the inputs it's showing you the hidden",
    "start": "2521530",
    "end": "2527890"
  },
  {
    "text": "layers because there's too many inputs to show you it's showing you the hidden layers and it's showing you the output",
    "start": "2527890",
    "end": "2534900"
  },
  {
    "text": "now on the right hand side we've got a canvas that we can draw on because it's one thing to run it against the test",
    "start": "2540420",
    "end": "2547540"
  },
  {
    "text": "data but it's something else to actually you know draw something on here so wish me luck I'm gonna draw something and see",
    "start": "2547540",
    "end": "2554140"
  },
  {
    "text": "how I connect how it goes so put a 1 in there it got that right",
    "start": "2554140",
    "end": "2559180"
  },
  {
    "text": "which is kind of surprising he's gonna either go I like to almost a 7 all right",
    "start": "2559180",
    "end": "2566250"
  },
  {
    "text": "got that right so that's a for all the 4",
    "start": "2566250",
    "end": "2572950"
  },
  {
    "text": "it's not doing terribly well as in now there's a problem here it's that this is something or interesting because it's",
    "start": "2572950",
    "end": "2578859"
  },
  {
    "text": "getting about 40 30 percent accuracy how did it do so well with with the training",
    "start": "2578859",
    "end": "2584200"
  },
  {
    "text": "data and things so poorly and you don't have to look for configuration files or you've used the library wrong so your",
    "start": "2584200",
    "end": "2590290"
  },
  {
    "text": "code it's all there right so there's got to be something you've done wrong um and this is really when you learn when",
    "start": "2590290",
    "end": "2595750"
  },
  {
    "text": "you're under this kind of problem this is really when you learn how this stuff works and when you look carefully at the samples what you can see is that they're",
    "start": "2595750",
    "end": "2603070"
  },
  {
    "text": "all been really nicely centered that was part of the scanning processes that they scented them so this is being completely",
    "start": "2603070",
    "end": "2611020"
  },
  {
    "text": "confused by the fact it's not perfectly centered right so there's no danger of this taking over our jobs so there's two",
    "start": "2611020",
    "end": "2619480"
  },
  {
    "text": "ways we can solve this one of them is what we can do what's called data augmentation we take all those samples",
    "start": "2619480",
    "end": "2626859"
  },
  {
    "text": "and we shift them and we rotate them and put them up and down left and right and then next we get it to teach it how to",
    "start": "2626859",
    "end": "2633099"
  },
  {
    "text": "deal with that like this one way there's another way which is easier in this case is which we can simply Center this",
    "start": "2633099",
    "end": "2639160"
  },
  {
    "text": "sample before feeding into the neural network which is what I've done here so I've already done I just need to",
    "start": "2639160",
    "end": "2645339"
  },
  {
    "text": "uncomment the code so I'm gonna take this off and run it again ah a CH",
    "start": "2645339",
    "end": "2654099"
  },
  {
    "text": "exception external component this is a bug in dotnet call three preview six I'm",
    "start": "2654099",
    "end": "2659500"
  },
  {
    "text": "not blaming it on link pad you just have a quick look at that look away I'm gonna",
    "start": "2659500",
    "end": "2664570"
  },
  {
    "text": "see what satisfactory ssam there are you that's in the wind formstack alright okay so let's have a",
    "start": "2664570",
    "end": "2673030"
  },
  {
    "text": "look now again and see how well this works so it's done it's done that any box and it's got it right let's try",
    "start": "2673030",
    "end": "2682240"
  },
  {
    "text": "something a bit harder it's got that right I haven't even finished the five",
    "start": "2682240",
    "end": "2692500"
  },
  {
    "text": "it's got it right this is doing pretty well now just with that century now one",
    "start": "2692500",
    "end": "2697510"
  },
  {
    "text": "interesting thing is if I draw I may draw seven I'm gonna really carefully like a typewriter and do it really",
    "start": "2697510",
    "end": "2704440"
  },
  {
    "text": "nicely it can't get that because people",
    "start": "2704440",
    "end": "2711490"
  },
  {
    "text": "don't write like that it's never seen anything like that before that's completely confused it right so",
    "start": "2711490",
    "end": "2716890"
  },
  {
    "text": "it's this is interesting how it works because obviously we could train it with that kind of stuff so we can actually",
    "start": "2716890",
    "end": "2723010"
  },
  {
    "text": "improve it though because we've got a final training score of 96 percent and there's a number of ways we can improve",
    "start": "2723010",
    "end": "2728380"
  },
  {
    "text": "it one is we can create more layers so in there somewhere I'm gonna put Creek",
    "start": "2728380",
    "end": "2734020"
  },
  {
    "text": "increase up to 30 but I'm gonna add another hidden layers two layers in there now I'm gonna train that forget",
    "start": "2734020",
    "end": "2740770"
  },
  {
    "text": "about that exception yeah we can see see",
    "start": "2740770",
    "end": "2751960"
  },
  {
    "text": "that extra layer in there now",
    "start": "2751960",
    "end": "2755190"
  },
  {
    "text": "this is using all the cause so it's getting the fans starting up now on their on the on the laptop it's quite a",
    "start": "2762420",
    "end": "2771250"
  },
  {
    "text": "bit faster if you put a new hex core or a m8 call machine in there runs quite a bit faster now you see this is going up",
    "start": "2771250",
    "end": "2780400"
  },
  {
    "text": "the the training schools going up or up",
    "start": "2780400",
    "end": "2787420"
  },
  {
    "text": "to 98% now when you get more than than",
    "start": "2787420",
    "end": "2793589"
  },
  {
    "text": "what two layers we're now doing deep learning right that's way cooler than",
    "start": "2793589",
    "end": "2799390"
  },
  {
    "text": "shallow learning okay we've got a final",
    "start": "2799390",
    "end": "2805420"
  },
  {
    "text": "test accuracy of ninety six point eight we've gone up a little bit but to be",
    "start": "2805420",
    "end": "2810550"
  },
  {
    "text": "honest you could with this particular kind of problem you can get the best result just by by increasing the number",
    "start": "2810550",
    "end": "2816010"
  },
  {
    "text": "of Naraku layers rather than three and just increasing the number of neurons in",
    "start": "2816010",
    "end": "2822160"
  },
  {
    "text": "that in that middle layer just having one hidden layer but something else we can do because I told you we had our pluggable activation functions what we",
    "start": "2822160",
    "end": "2829540"
  },
  {
    "text": "can do is I can uncreate a new activation function in here so I'm to change the activation function on that",
    "start": "2829540",
    "end": "2835270"
  },
  {
    "text": "middle hidden layer to a an exotic one",
    "start": "2835270",
    "end": "2841080"
  },
  {
    "text": "softmax activator with cross-entropy loss right now that sounds really fancy",
    "start": "2841080",
    "end": "2847150"
  },
  {
    "text": "and it is but it what this actually does is not it's not just changing the activation function it also changes the",
    "start": "2847150",
    "end": "2852609"
  },
  {
    "text": "loss function it gives us a more elaborate one we're going to train that see how that goes and what's doing that",
    "start": "2852609",
    "end": "2861720"
  },
  {
    "text": "I'll cover a couple of other things which is this gradient descent that we",
    "start": "2861720",
    "end": "2869140"
  },
  {
    "text": "talked about before that's that that's nice curve applies to a single training sample but when we take all of them into",
    "start": "2869140",
    "end": "2875770"
  },
  {
    "text": "account we end up with something way more complex and you get this phenomenon called local minima where you get you",
    "start": "2875770",
    "end": "2881710"
  },
  {
    "text": "know it can end up getting stuck in there and this is one of the reasons why we initialize the weights and biases to",
    "start": "2881710",
    "end": "2887170"
  },
  {
    "text": "small round and valleys so just by re running it over and over again you'll end up in different spots so you choose",
    "start": "2887170",
    "end": "2893470"
  },
  {
    "text": "that you know which gives you the best final result but one way you can help avoid that problem is",
    "start": "2893470",
    "end": "2898480"
  },
  {
    "text": "if you keep it simpler so if you have fewer layers for your complexity in your function what you end up with is you",
    "start": "2898480",
    "end": "2904480"
  },
  {
    "text": "still get the local minima but they're all quite close together right so it turns into a less of a problem and what",
    "start": "2904480",
    "end": "2910000"
  },
  {
    "text": "we're doing here is pretty simple so we don't get too many of these problems see how this is doing now it's kind of slow",
    "start": "2910000",
    "end": "2916180"
  },
  {
    "text": "isn't it that's running so slowly I'm",
    "start": "2916180",
    "end": "2927460"
  },
  {
    "text": "going to do I'm gonna stop this I'll put a woman just gonna reduce this to 15",
    "start": "2927460",
    "end": "2932530"
  },
  {
    "text": "Euron's that is a much slower activation function this is one of the things you have to at a trade-off you're making and",
    "start": "2932530",
    "end": "2938530"
  },
  {
    "text": "doesn't matter whether using this or using a big library like tens of flow is that sometimes it's better to use an",
    "start": "2938530",
    "end": "2944950"
  },
  {
    "text": "inferior activation function and loss function if it's quicker because you can get way more iterations done you'll end",
    "start": "2944950",
    "end": "2951820"
  },
  {
    "text": "up getting our better results sooner anyway so sometimes it's not always better to have the better function it's",
    "start": "2951820",
    "end": "2957490"
  },
  {
    "text": "training quicker now and what you find",
    "start": "2957490",
    "end": "2963490"
  },
  {
    "text": "with this is that with with this activation function we can get right up to 99% 99.5% training school on there",
    "start": "2963490",
    "end": "2974410"
  },
  {
    "text": "but the problem is when it comes to the actual test score it's no better than",
    "start": "2974410",
    "end": "2980080"
  },
  {
    "text": "what we had before and this phenomenon is called overfit and what it means it's done a really",
    "start": "2980080",
    "end": "2985870"
  },
  {
    "text": "good job of fitting all of those training samples but it can't generalize",
    "start": "2985870",
    "end": "2991150"
  },
  {
    "text": "the stuff it hasn't seen as well as you'd like and that's overfit and that's",
    "start": "2991150",
    "end": "2996370"
  },
  {
    "text": "what's what happens tends to happen with this exotic activation function and loss",
    "start": "2996370",
    "end": "3002160"
  },
  {
    "text": "function right well 99.4% accurate on our training but the test accuracy is",
    "start": "3002160",
    "end": "3009480"
  },
  {
    "text": "only 97.8% so we can actually get the best result for the final test accuracy by going for",
    "start": "3009480",
    "end": "3018170"
  },
  {
    "text": "anything up to I'm going to stop at 200 but we can up to a thousand you've got a",
    "start": "3018170",
    "end": "3024630"
  },
  {
    "text": "thousand neurons in that middle layer and then that will that will get you about ninety eight point something",
    "start": "3024630",
    "end": "3032040"
  },
  {
    "text": "percent 98.5 percent accuracy which I'm given the simplicity of this is is quite",
    "start": "3032040",
    "end": "3038520"
  },
  {
    "text": "quite good so while this is going there's something else I'd is it still",
    "start": "3038520",
    "end": "3044520"
  },
  {
    "text": "going well as the SCH SCH exception finally got the better of it it is going",
    "start": "3044520",
    "end": "3050480"
  },
  {
    "text": "okay so something else I want to do which is how what do we do if we want to get even better than that we want to",
    "start": "3050480",
    "end": "3056190"
  },
  {
    "text": "solve more difficult problems like you know recognizing what kind of object",
    "start": "3056190",
    "end": "3061349"
  },
  {
    "text": "whether a so fluffy cat or a dog in the picture and the way these more advanced",
    "start": "3061349",
    "end": "3066540"
  },
  {
    "text": "neural networks operate is unlike this neural network which is a general-purpose you can use this kind of",
    "start": "3066540",
    "end": "3074160"
  },
  {
    "text": "network to solve a whole variety of different problems but what you can do is we can desert we can make this so",
    "start": "3074160",
    "end": "3081030"
  },
  {
    "text": "that it's only designed for image processing and nothing else and when you do that you can end up with something",
    "start": "3081030",
    "end": "3086130"
  },
  {
    "text": "which works a lot better for image processing for instance you could start by running rather than reading feeding",
    "start": "3086130",
    "end": "3091920"
  },
  {
    "text": "the raw pixels into the neural network you can first run it through some",
    "start": "3091920",
    "end": "3098099"
  },
  {
    "text": "filters and do things like edge detection and line detection in circle detection and then you feed the output",
    "start": "3098099",
    "end": "3104690"
  },
  {
    "text": "of those filters into the neural network and that gives you quite a bit better",
    "start": "3104690",
    "end": "3110790"
  },
  {
    "text": "result now what's really clever about that about that architecture is that you",
    "start": "3110790",
    "end": "3116819"
  },
  {
    "text": "don't have to figure out you know how to design those filters all you have to decide is how many filters you want",
    "start": "3116819",
    "end": "3123750"
  },
  {
    "text": "because when you write these filters the way they work is you use matrices which all it is is a bunch of numbers you can",
    "start": "3123750",
    "end": "3130770"
  },
  {
    "text": "describe a filter which does something like edge detection just with a bunch of",
    "start": "3130770",
    "end": "3135780"
  },
  {
    "text": "numbers basically what you put into a matrix now once you've got a bunch of numbers in there you can allow the",
    "start": "3135780",
    "end": "3141869"
  },
  {
    "text": "neural network to figure out what value is to put into those numbers through",
    "start": "3141869",
    "end": "3147240"
  },
  {
    "text": "back propagation exactly the same mechanism we did before the back propagation you can get it to tune the",
    "start": "3147240",
    "end": "3153480"
  },
  {
    "text": "field say decides on the filters you only decide I want ten filters and and initialize those filters to small random",
    "start": "3153480",
    "end": "3159839"
  },
  {
    "text": "values it will tune them and that's called a convolutional neural network and that's the basis of a lot of",
    "start": "3159839",
    "end": "3165680"
  },
  {
    "text": "the deep learning networks today that do image recognition something else I'll",
    "start": "3165680",
    "end": "3173690"
  },
  {
    "text": "hold on to describe which is what to do if you don't have much training data because we've got 50,000 in this case",
    "start": "3173690",
    "end": "3179480"
  },
  {
    "text": "and sometimes you don't have that amount and there is a court a technique called pre-training you can do or transfer",
    "start": "3179480",
    "end": "3186170"
  },
  {
    "text": "learning and that is that that imagine that we wanted to now train train it to",
    "start": "3186170",
    "end": "3191839"
  },
  {
    "text": "to to do letters well let's say we wanted to train it to do numbers but we didn't have any numbers of samples but",
    "start": "3191839",
    "end": "3197900"
  },
  {
    "text": "we've got lots of letters what you can do is you can train it first reduce letters to a similar kind of thing and then once you've optimally trained it",
    "start": "3197900",
    "end": "3205130"
  },
  {
    "text": "you can save all those weights and biases off to a file right so now it can now do letters and now when you want to",
    "start": "3205130",
    "end": "3211490"
  },
  {
    "text": "train it to do numbers when you've only got a small set of numbers what you can do is rather than starting out with random weights and biases you start out",
    "start": "3211490",
    "end": "3219559"
  },
  {
    "text": "with that pre trained network you got that was capable of doing letters and you find that it actually does a much",
    "start": "3219559",
    "end": "3225349"
  },
  {
    "text": "better job because there's similarities between the letters and numbers so it's kind of learned something in there",
    "start": "3225349",
    "end": "3230630"
  },
  {
    "text": "that's kind of encoded how to do some recognition and it's now applying that to a slightly different task so it's",
    "start": "3230630",
    "end": "3237079"
  },
  {
    "text": "called pre training or all transfer learning probably pre trains a better term for that and that that's that's",
    "start": "3237079",
    "end": "3243530"
  },
  {
    "text": "what's done it also extensively with the the deep learning networks that you can",
    "start": "3243530",
    "end": "3248990"
  },
  {
    "text": "play with such as tensorflow is if you want to do an image recognition task you can download",
    "start": "3248990",
    "end": "3254020"
  },
  {
    "text": "something which is already being trained with millions of images on the internet",
    "start": "3254020",
    "end": "3259150"
  },
  {
    "text": "let's go back to that and have a look see how it's doing it's getting very hot",
    "start": "3259150",
    "end": "3267290"
  },
  {
    "text": "I'll tell you that so while that's going",
    "start": "3267290",
    "end": "3273410"
  },
  {
    "text": "any questions we've got five minutes",
    "start": "3273410",
    "end": "3277030"
  },
  {
    "text": "okay the different colors in the lines is represents the on the on the neurons",
    "start": "3285239",
    "end": "3292799"
  },
  {
    "text": "the color represents the final bias of those of the bias of that so that's",
    "start": "3292799",
    "end": "3297999"
  },
  {
    "text": "constantly changing so that's the bias and the color of the it's power to tell",
    "start": "3297999",
    "end": "3304239"
  },
  {
    "text": "the so many of them but the color of these lines is the waste represents the weight so blue is negative and red is",
    "start": "3304239",
    "end": "3310269"
  },
  {
    "text": "positive any more questions",
    "start": "3310269",
    "end": "3318900"
  },
  {
    "text": "sad to see here with the lights",
    "start": "3322579",
    "end": "3326539"
  },
  {
    "text": "okay so we've got a final test accuracy of a now 98.3% with and this is with two",
    "start": "3334920",
    "end": "3342280"
  },
  {
    "text": "hundred two hundred and Euron's on the hidden layer and ten on the output layer",
    "start": "3342280",
    "end": "3348930"
  },
  {
    "text": "so we'll give it another go with",
    "start": "3352470",
    "end": "3356670"
  },
  {
    "text": "fat-fingered that one but that's worked",
    "start": "3357720",
    "end": "3361859"
  },
  {
    "text": "yes it's doing a pretty good job now so this this is similar to what you'll get even with some of the commercial ones",
    "start": "3367349",
    "end": "3374339"
  },
  {
    "text": "it's only slightly better than this so yes of that I'll be I'll be hanging",
    "start": "3374339",
    "end": "3380290"
  },
  {
    "text": "around so if you want to talk to me afterwards about this um you're welcome to come and see me so thanks everyone",
    "start": "3380290",
    "end": "3385599"
  },
  {
    "text": "for you for coming [Applause]",
    "start": "3385599",
    "end": "3395979"
  }
]