[
  {
    "start": "0",
    "end": "86000"
  },
  {
    "text": "so hey everyone thanks for coming to my talk auditing data and answering that lifelong question is",
    "start": "8080",
    "end": "14240"
  },
  {
    "text": "at the end of the day yet now if you're not familiar with that question don't worry about it because we're going to go",
    "start": "14240",
    "end": "19359"
  },
  {
    "text": "deep into it in the next 60 minutes so what are we going to talk about in the next 60 minutes um we're going to",
    "start": "19359",
    "end": "26640"
  },
  {
    "text": "start with nielsen's architecture and we'll really see how it had little fires",
    "start": "26640",
    "end": "32160"
  },
  {
    "text": "everywhere and i'm going to convince you why we have that need for a data auditing system",
    "start": "32160",
    "end": "37440"
  },
  {
    "text": "we'll then go into designing that system and talking about how even data auditing data looks like",
    "start": "37440",
    "end": "45120"
  },
  {
    "text": "how to store it how to query it then we'll finally maybe get an answer",
    "start": "45120",
    "end": "50320"
  },
  {
    "text": "to that question as yesterday over and then some alerts and add-ons because",
    "start": "50320",
    "end": "55440"
  },
  {
    "text": "that's always fun but first the most important thing who am i um so who am i",
    "start": "55440",
    "end": "62320"
  },
  {
    "text": "i'm simona i'm 29 years old i work at adoc as a big data engineer",
    "start": "62320",
    "end": "67760"
  },
  {
    "text": "but i did work at nielsen up to a few months ago so this is totally based on a true story",
    "start": "67760",
    "end": "74080"
  },
  {
    "text": "i've been dealing with data for the past 10 years now which feels like a lot i love music the weirder the better i",
    "start": "74080",
    "end": "80880"
  },
  {
    "text": "love japan and i love traveling so this is basically me in a picture",
    "start": "80880",
    "end": "87320"
  },
  {
    "start": "86000",
    "end": "86000"
  },
  {
    "text": "so the first thing to keep in mind when you look at nielsen's architecture is that this architecture receives and",
    "start": "87759",
    "end": "94240"
  },
  {
    "text": "processes more than 50 terabytes of data per day we're talking about billions of events",
    "start": "94240",
    "end": "99680"
  },
  {
    "text": "that we collect from various devices so that includes cell phones tvs computers",
    "start": "99680",
    "end": "105920"
  },
  {
    "text": "and all of these events they get eventually here's my laser",
    "start": "105920",
    "end": "111119"
  },
  {
    "text": "they go straight into the data serving servers and those servers they're included in that front-end layer",
    "start": "111119",
    "end": "117600"
  },
  {
    "text": "what's important to keep in mind about those are they're written in java slas are very very important to them",
    "start": "117600",
    "end": "124399"
  },
  {
    "text": "and another point is that the entire front-end layer is regional so at the",
    "start": "124399",
    "end": "129679"
  },
  {
    "text": "time that we've designed this architecture just like a area a year and a half ago",
    "start": "129679",
    "end": "134800"
  },
  {
    "text": "we had several data centers all across the world and each one of these data centers was",
    "start": "134800",
    "end": "140480"
  },
  {
    "text": "running its own front-end layer so the data serving servers they receive all",
    "start": "140480",
    "end": "145680"
  },
  {
    "text": "that data they enrich it and then they produce it to its own uh to their own local regional kafka from there the data",
    "start": "145680",
    "end": "154080"
  },
  {
    "text": "gets replicated using your eu replicator into that one central kafka and aws",
    "start": "154080",
    "end": "159920"
  },
  {
    "text": "and we're talking about kafka that's 50 brokers big from there",
    "start": "159920",
    "end": "165440"
  },
  {
    "text": "we have spark streaming applications that we personally call the data loaders",
    "start": "165440",
    "end": "170959"
  },
  {
    "text": "they consume that data they deserialize it and they spill it straight into rdr now rdr stands for",
    "start": "170959",
    "end": "178400"
  },
  {
    "text": "raw data repository and it's basically our data lake on s3",
    "start": "178400",
    "end": "183440"
  },
  {
    "text": "from there we have our spark jobs they run periodically they again consume",
    "start": "183440",
    "end": "189040"
  },
  {
    "text": "that data enrich it aggregate it and eventually store it in all of these different data stores right so we have",
    "start": "189040",
    "end": "195599"
  },
  {
    "text": "my sql we have postgres we have druid and even back into rdr and each one of these data stores is serving an entirely",
    "start": "195599",
    "end": "203200"
  },
  {
    "text": "different need an entirely different application and if we look at the application layer each one of these",
    "start": "203200",
    "end": "209440"
  },
  {
    "text": "applications can consume that data straight from that said data source or altogether through",
    "start": "209440",
    "end": "216000"
  },
  {
    "text": "hive and so this architecture really had little fires going on everywhere",
    "start": "216000",
    "end": "222319"
  },
  {
    "text": "especially in data latency data latency wise",
    "start": "222319",
    "end": "227599"
  },
  {
    "text": "and really at this point let's start discussing all of these data arrival",
    "start": "227599",
    "end": "232720"
  },
  {
    "text": "pain points so the first thing i want to do is really zoom into that connection in",
    "start": "232720",
    "end": "238560"
  },
  {
    "start": "235000",
    "end": "235000"
  },
  {
    "text": "between the regional kafka and the one central kafka and aws so if you're not familiar with the eu replicator um it's",
    "start": "238560",
    "end": "246080"
  },
  {
    "text": "a project by uber it's based off of kafka's own mirror maker and it's essentially it essentially",
    "start": "246080",
    "end": "252319"
  },
  {
    "text": "suffers from the exact same set of problems because of the way that the mirror maker was implemented so",
    "start": "252319",
    "end": "260000"
  },
  {
    "text": "what kind of set of problems am i talking about so the most straightforward problem would be the eu",
    "start": "260000",
    "end": "266240"
  },
  {
    "text": "replicator crashing right so the eu replicator crashes then data does not flow data is not being replicated",
    "start": "266240",
    "end": "273280"
  },
  {
    "text": "anymore from that regional kafka into the kafka aws but there are problems that are a lot",
    "start": "273280",
    "end": "279120"
  },
  {
    "text": "worse so let's say that you have a pretty big kafka topic um and i'm talking about",
    "start": "279120",
    "end": "284880"
  },
  {
    "text": "topics that are 1500 or 2000 partitions big so what would happen",
    "start": "284880",
    "end": "291120"
  },
  {
    "text": "is that the euro applicator would stop replicating but not on all of the partitions",
    "start": "291120",
    "end": "296479"
  },
  {
    "text": "so it would stop replicating on some 10 or 50 or 100 partitions so most of your data",
    "start": "296479",
    "end": "303919"
  },
  {
    "text": "is flowing and you're essentially not even aware that there's a chunk of that data that's",
    "start": "303919",
    "end": "310560"
  },
  {
    "text": "actually not being replicated so next talk let's next let's talk about",
    "start": "310560",
    "end": "316240"
  },
  {
    "text": "what happens actually after the data arrives into the kafka aws so what could",
    "start": "316240",
    "end": "321840"
  },
  {
    "text": "go wrong here well let's say that one of the kafka brokers started crashing or the entire",
    "start": "321840",
    "end": "328720"
  },
  {
    "text": "kafka cluster is unavailable or the data loaders are crashing alerts are popping",
    "start": "328720",
    "end": "333840"
  },
  {
    "text": "up we have to go and check what's wrong with the data loaders essentially data is not arriving into rdr it's again",
    "start": "333840",
    "end": "340880"
  },
  {
    "text": "being late so these were essentially our pain points in this architecture but the",
    "start": "340880",
    "end": "347360"
  },
  {
    "start": "343000",
    "end": "343000"
  },
  {
    "text": "thing that i didn't actually discuss is how do we recover from these failures",
    "start": "347360",
    "end": "353360"
  },
  {
    "text": "what would be our criteria to well okay something crashed data is not arriving",
    "start": "353360",
    "end": "358400"
  },
  {
    "text": "anymore but things are okay now so if we go back again to this connection between the regional",
    "start": "358400",
    "end": "365360"
  },
  {
    "text": "kafka and the kafka and aws the eu replicator crash is not working anymore",
    "start": "365360",
    "end": "370720"
  },
  {
    "text": "we started it back up again data flow data is flowing again well then that's great problem solved",
    "start": "370720",
    "end": "379360"
  },
  {
    "text": "and if if we look at this point right so kafka brokers crashing or the entire kafka cluster is not available anymore",
    "start": "379360",
    "end": "386319"
  },
  {
    "text": "our data loaders are crashing we just started them up again data is flowing in again um",
    "start": "386319",
    "end": "392639"
  },
  {
    "text": "so problem solved and at this point we had no actual information in regards to the",
    "start": "392639",
    "end": "400000"
  },
  {
    "text": "implications of all of these crashes on our data integrity um and nielsen is",
    "start": "400000",
    "end": "405840"
  },
  {
    "text": "a data company data is very very important to us data integrity is very very important to us",
    "start": "405840",
    "end": "411199"
  },
  {
    "text": "and we don't really have all of that information in regards to way that our",
    "start": "411199",
    "end": "416560"
  },
  {
    "text": "data may make through all of these points in this waterfall and the implications of these crashes on our",
    "start": "416560",
    "end": "422960"
  },
  {
    "text": "data on data loss and data duplication and data latency",
    "start": "422960",
    "end": "428720"
  },
  {
    "text": "and at this point i really want to start discussing this question the end of the day question",
    "start": "428720",
    "end": "434479"
  },
  {
    "text": "and to understand the importance of it we really need to talk about the use of",
    "start": "434479",
    "end": "439599"
  },
  {
    "text": "data in the media world so what interests us in the media world in nielsen's world",
    "start": "439599",
    "end": "446400"
  },
  {
    "text": "is daily data because we want to count users unique users",
    "start": "446400",
    "end": "451680"
  },
  {
    "text": "on a daily basis so when we run our processes and processors and i told you that we run",
    "start": "451680",
    "end": "458319"
  },
  {
    "text": "our spark jobs periodically we essentially want to run them on one",
    "start": "458319",
    "end": "463360"
  },
  {
    "text": "day's worth of data on yesterday so if data is late to arrive",
    "start": "463360",
    "end": "470319"
  },
  {
    "text": "and you run your processor on a partial amount of data then you'll have to rerun the processor again right and that would",
    "start": "470319",
    "end": "477919"
  },
  {
    "text": "cost you quite a lot of money so we tried solving this problem at",
    "start": "477919",
    "end": "483360"
  },
  {
    "start": "483000",
    "end": "483000"
  },
  {
    "text": "first with some legacy solutions we said okay we use airflow as our scheduler",
    "start": "483360",
    "end": "490840"
  },
  {
    "text": "um when's yesterday over let's trigger our job then all right so when would",
    "start": "490840",
    "end": "496960"
  },
  {
    "text": "yesterday be over for sure let's say 11 am 11 a.m sounds really good",
    "start": "496960",
    "end": "503360"
  },
  {
    "text": "so we set our spark jobs to run at 11 am and then we had this day with a peak of",
    "start": "503360",
    "end": "510240"
  },
  {
    "text": "data the data loaders were not really set up to the job of",
    "start": "510240",
    "end": "515279"
  },
  {
    "text": "consuming all that data and so we had a data latency problem data was not arriving in time into rdr and not the",
    "start": "515279",
    "end": "522640"
  },
  {
    "text": "entire day of data was did arrive actually by 11 am so the day was not actually over",
    "start": "522640",
    "end": "530240"
  },
  {
    "text": "we said okay so the fixed time solution is not a good enough solution we need a better indicator what kind of indicator",
    "start": "530240",
    "end": "536880"
  },
  {
    "text": "we can use well let's do awss 3ls because you know let's let's see if data",
    "start": "536880",
    "end": "543279"
  },
  {
    "text": "was not written to that day's bucket for like two hours then then yesterday's over the day's",
    "start": "543279",
    "end": "549120"
  },
  {
    "text": "over that's fine and we can run our spark jobs um but then we had a really bad kafka",
    "start": "549120",
    "end": "555040"
  },
  {
    "text": "outage it was out for hours and so the data processor started",
    "start": "555040",
    "end": "560320"
  },
  {
    "text": "running again again on a partial chunk of data so all of these solutions were not good",
    "start": "560320",
    "end": "566720"
  },
  {
    "text": "enough and if we talk about all these pain points again in the context of this",
    "start": "566720",
    "end": "572000"
  },
  {
    "text": "architecture i think you'll really see what i'm talking about so kafka's crashing or kafka is not",
    "start": "572000",
    "end": "578560"
  },
  {
    "text": "available at all the data loaders crashing essentially data is being laid arriving into rdr the",
    "start": "578560",
    "end": "585519"
  },
  {
    "text": "data processors are not aware they start running they process that partial data",
    "start": "585519",
    "end": "590800"
  },
  {
    "text": "they store it in all of these different data stores and so we have to reprocess that data",
    "start": "590800",
    "end": "595839"
  },
  {
    "text": "right but the problem is not over yet because if we talk about the application",
    "start": "595839",
    "end": "600959"
  },
  {
    "text": "layer then all of these applications that consume that data they not only consume it some of them create their own",
    "start": "600959",
    "end": "608160"
  },
  {
    "text": "data sets so they as well have to again reprocess run again",
    "start": "608160",
    "end": "615440"
  },
  {
    "text": "and so if we look at this architecture again i think you'll really see that we",
    "start": "615440",
    "end": "620480"
  },
  {
    "text": "have little fires going on everywhere so for all of these reasons for all of",
    "start": "620480",
    "end": "626640"
  },
  {
    "text": "our data arrival pain points and also for solving the end of the day question",
    "start": "626640",
    "end": "633839"
  },
  {
    "text": "we decided to design our auditing system and so what we'll do now is actually",
    "start": "633839",
    "end": "639839"
  },
  {
    "text": "just start designing our data understand how it looks like understand how we're going to produce and consume it",
    "start": "639839",
    "end": "647360"
  },
  {
    "text": "but first i really want to go through some points with you guys and these points are really important to keep in mind while while we go through",
    "start": "647360",
    "end": "654399"
  },
  {
    "text": "that process so what are these points that we should keep in mind the first point is going to",
    "start": "654399",
    "end": "660000"
  },
  {
    "text": "be that we do not have only one kafka topic we have we had several of them um",
    "start": "660000",
    "end": "666079"
  },
  {
    "text": "and not only that we had several kafka topics this number was not fixed either so if tomorrow there's a new need and a",
    "start": "666079",
    "end": "672800"
  },
  {
    "text": "new application we're probably going to define a new kafka topic",
    "start": "672800",
    "end": "678320"
  },
  {
    "text": "the next set of points to keep in mind really have to do with the data data serving servers and i've actually",
    "start": "678320",
    "end": "684160"
  },
  {
    "text": "mentioned some of them before so the first would be",
    "start": "684160",
    "end": "689600"
  },
  {
    "text": "in the second bullet is that they're implemented in java and only that they",
    "start": "689600",
    "end": "694880"
  },
  {
    "text": "implemented in java each one of these servers is running two jvms and each one of these jvms is working",
    "start": "694880",
    "end": "701839"
  },
  {
    "text": "against a set of topics so they get data they don't really care",
    "start": "701839",
    "end": "707440"
  },
  {
    "text": "what data they're getting they're enriching it and then they're shipping it to the relevant topic",
    "start": "707440",
    "end": "713600"
  },
  {
    "text": "the second point that i want to talk about is that we were using our old nielsen kafka producer",
    "start": "713600",
    "end": "719920"
  },
  {
    "text": "which is a kafka producer with some nielsen add-ons and we've decided to implement this",
    "start": "719920",
    "end": "726000"
  },
  {
    "text": "auditing system on the nielsen kafka producer and if we think again about the",
    "start": "726000",
    "end": "732240"
  },
  {
    "text": "fact that slas were very very important to us then we really had to implement this",
    "start": "732240",
    "end": "738959"
  },
  {
    "text": "code in an optimized way so not to interfere with data servings",
    "start": "738959",
    "end": "744240"
  },
  {
    "text": "slas the last point to keep in mind is that we use avro so we use avro for",
    "start": "744240",
    "end": "750240"
  },
  {
    "text": "serialization and this will actually make our job easier so what's the word window why is it",
    "start": "750240",
    "end": "758000"
  },
  {
    "text": "written on my screen right now so",
    "start": "758000",
    "end": "763360"
  },
  {
    "start": "761000",
    "end": "761000"
  },
  {
    "text": "if you think about all the points i told you to keep in mind and look at this auditing window which is essentially our",
    "start": "763360",
    "end": "771200"
  },
  {
    "text": "auditing metadata object you'll really see how they all make sense",
    "start": "771200",
    "end": "776560"
  },
  {
    "text": "so we have our topic or server and process right so for every server for every process for every topic we're",
    "start": "776560",
    "end": "783600"
  },
  {
    "text": "going to count the amount of messages that are being produced so what is the word window",
    "start": "783600",
    "end": "789839"
  },
  {
    "text": "well essentially you can't really audit forever so you have to really define a time",
    "start": "789839",
    "end": "795600"
  },
  {
    "text": "frame we've defined it to five minutes why well one minute that sounds like",
    "start": "795600",
    "end": "803120"
  },
  {
    "text": "not a lot and one hour sounds like too much so we said okay let's go with five",
    "start": "803120",
    "end": "808160"
  },
  {
    "text": "minutes and we're going to implement it in such a way that you can really just reconfigure it later on we never did",
    "start": "808160",
    "end": "814880"
  },
  {
    "text": "five minutes was great and what you see before you right now is also the avro definition of that",
    "start": "814880",
    "end": "821199"
  },
  {
    "text": "metadata object of the audit window object",
    "start": "821199",
    "end": "826320"
  },
  {
    "text": "cool um so what we need to do now now that we actually have that counter we",
    "start": "826320",
    "end": "831440"
  },
  {
    "text": "have that metadata object is just connected to our data so what we're going to do is we're going",
    "start": "831440",
    "end": "837839"
  },
  {
    "text": "to create an audit header so the audit header will have all that complementary information that our data does not have",
    "start": "837839",
    "end": "845600"
  },
  {
    "text": "so what do we do have we obviously have the topic because we're consuming the data off of some topic",
    "start": "845600",
    "end": "852800"
  },
  {
    "text": "we do not have the server name and the window timestamp if you're",
    "start": "852800",
    "end": "857839"
  },
  {
    "text": "wondering about a sequence id that's a bit of a spoiler and we're going to talk about it later on",
    "start": "857839",
    "end": "863600"
  },
  {
    "text": "now the question is basically how we're going to inject that audit header into that",
    "start": "863600",
    "end": "869040"
  },
  {
    "text": "into our data and that's what we're going to discuss right now so if we look at an anonymized version",
    "start": "869040",
    "end": "876560"
  },
  {
    "text": "of an avro definition of a topic that we had you can see several fields right and",
    "start": "876560",
    "end": "881920"
  },
  {
    "text": "you can see that the second line now includes audit header so we've just added that audit header object to all of",
    "start": "881920",
    "end": "889040"
  },
  {
    "text": "our avro schemas so now all that we have to do is just run over that field in our avro",
    "start": "889040",
    "end": "896560"
  },
  {
    "text": "definition in our code so what's going on here",
    "start": "896560",
    "end": "904560"
  },
  {
    "start": "901000",
    "end": "901000"
  },
  {
    "text": "okay so what's happening here is that we're receiving some sort of data",
    "start": "904959",
    "end": "910160"
  },
  {
    "text": "we're not really sure if auditing is enabled on it or not so the first thing",
    "start": "910160",
    "end": "915199"
  },
  {
    "text": "that we're going to do is just check if it includes the audit header if it does then that's great um we're",
    "start": "915199",
    "end": "922320"
  },
  {
    "text": "just going to run it over with an object that we've created and that way if our auditing data does",
    "start": "922320",
    "end": "930560"
  },
  {
    "text": "include an audit header it will now include all of that information that we need",
    "start": "930560",
    "end": "936639"
  },
  {
    "text": "so now that we have all of that information we just need to produce it somehow so that we can consume it on the",
    "start": "936639",
    "end": "943279"
  },
  {
    "text": "other on the other side so with our data that's pretty straightforward right our data is just",
    "start": "943279",
    "end": "949600"
  },
  {
    "text": "it just keeps flowing through the system now with an audit header so it's going to go through kafka just as it used to",
    "start": "949600",
    "end": "957120"
  },
  {
    "text": "and what we did with the audit metadata object we started producing it to its",
    "start": "957120",
    "end": "962320"
  },
  {
    "text": "own audit metadata topic cool so i think you've noticed that we",
    "start": "962320",
    "end": "968560"
  },
  {
    "text": "have a bit of a problem here right because we're actually auditing kafka",
    "start": "968560",
    "end": "974079"
  },
  {
    "text": "with kafka at this point and we've noticed a problem as well so what we decided to do",
    "start": "974079",
    "end": "980720"
  },
  {
    "text": "is just implement a simple rush service using amazon api gateway that's going to",
    "start": "980720",
    "end": "988160"
  },
  {
    "text": "receive that audit information the audit metadata information straight from the server",
    "start": "988160",
    "end": "994240"
  },
  {
    "text": "but then we had to choose we we said okay we already have that kafka topic implemented and now we also have that",
    "start": "994240",
    "end": "1001199"
  },
  {
    "text": "api gateway implemented and we decided to stick with both and if you're asking yourself why",
    "start": "1001199",
    "end": "1008240"
  },
  {
    "text": "it's a because the kafka solution was already working it was ready there",
    "start": "1008240",
    "end": "1014480"
  },
  {
    "text": "and then we also felt that just using both of these ways to ship our audi metadata",
    "start": "1014480",
    "end": "1021120"
  },
  {
    "text": "data is just gonna keep us covered so if there's a problem with kafka we're still",
    "start": "1021120",
    "end": "1026640"
  },
  {
    "text": "getting all of our windows through the api gateway uh if there's for some reason a problem with api gateway that",
    "start": "1026640",
    "end": "1033600"
  },
  {
    "text": "happens um then we of course have our kafka and so if we look at it again this is",
    "start": "1033600",
    "end": "1040558"
  },
  {
    "text": "just everything we've discussed we're going to produce that audit information through kafka but our metadata objects",
    "start": "1040559",
    "end": "1048079"
  },
  {
    "text": "are going to go through the london through the amazon api gateway as well",
    "start": "1048079",
    "end": "1053760"
  },
  {
    "text": "cool so now we have all of that information we're producing it we need to consume it",
    "start": "1053760",
    "end": "1059360"
  },
  {
    "text": "at this point and the audit window we're going to consume",
    "start": "1059360",
    "end": "1064559"
  },
  {
    "text": "using a simple kafka consumer and of course if it goes through the api we",
    "start": "1064559",
    "end": "1069919"
  },
  {
    "text": "have a lambda that parses it and stores it in a data audit table that we're",
    "start": "1069919",
    "end": "1075039"
  },
  {
    "text": "going to discuss in just a few minutes now the rest of the data that's arriving",
    "start": "1075039",
    "end": "1080400"
  },
  {
    "text": "into rdr that that's already being um consumed right so",
    "start": "1080400",
    "end": "1085679"
  },
  {
    "text": "it's already in s3 we created a simple consumer that's going to consume all of that data off of s3 and again start",
    "start": "1085679",
    "end": "1094080"
  },
  {
    "text": "store it in the same audit table and if we've implemented all of these consumers on a single emr",
    "start": "1094080",
    "end": "1100960"
  },
  {
    "text": "if you're not familiar with emr emr is a service by amazon that's called elastic mapreduce it's essentially a cluster a",
    "start": "1100960",
    "end": "1108000"
  },
  {
    "text": "bunch of servers running together and it allows you to run several applications in parallel so that's",
    "start": "1108000",
    "end": "1114640"
  },
  {
    "text": "exactly what we've decided to do and then again in context this is",
    "start": "1114640",
    "end": "1121919"
  },
  {
    "text": "everything we've discussed this is the entire data auditing architecture we're if we talk about our",
    "start": "1121919",
    "end": "1128640"
  },
  {
    "text": "data it's still being produced through kafka it still arrives into rdr and then we have our data auditing",
    "start": "1128640",
    "end": "1135039"
  },
  {
    "text": "consumers and then if we talk about the audit metadata it's being produced",
    "start": "1135039",
    "end": "1140320"
  },
  {
    "text": "through the kafka and then to a kafka consumer and then also straight through the api into the data audit table",
    "start": "1140320",
    "end": "1149039"
  },
  {
    "text": "awesome so now we have that information we have it in a table um let's start",
    "start": "1149039",
    "end": "1154720"
  },
  {
    "text": "discussing about how we decided to store that table and how we actually designed it",
    "start": "1154720",
    "end": "1160000"
  },
  {
    "text": "and i think when we talk about data auditing information you'll notice that it's it's pretty structured right",
    "start": "1160000",
    "end": "1167600"
  },
  {
    "text": "like it is it is structured so we've just decided to store it in an rds table",
    "start": "1167600",
    "end": "1172880"
  },
  {
    "text": "using postgres so when designing our table we first",
    "start": "1172880",
    "end": "1178960"
  },
  {
    "start": "1176000",
    "end": "1176000"
  },
  {
    "text": "need to ask ourselves what are the questions that we need answered",
    "start": "1178960",
    "end": "1184000"
  },
  {
    "text": "so let's start by talking about levels of granularity we'd probably want to answer all of these questions in regards",
    "start": "1184000",
    "end": "1190640"
  },
  {
    "text": "to a server right in regards to a process in regards to a topic that's that's really straightforward",
    "start": "1190640",
    "end": "1197120"
  },
  {
    "text": "these are all the points that we've discussed uh what could be an interesting add-on",
    "start": "1197120",
    "end": "1203120"
  },
  {
    "text": "so we could also answer all of these questions on a regional basis right we said that all of our front-end layers",
    "start": "1203120",
    "end": "1209120"
  },
  {
    "text": "are running on a regional um on data centers across the world so they're regional we'd want to answer these",
    "start": "1209120",
    "end": "1216480"
  },
  {
    "text": "questions on a regional granularity as well what else what",
    "start": "1216480",
    "end": "1221600"
  },
  {
    "text": "what are other questions that we want answered the first one is pretty straightforward so arrival rate is just for a certain",
    "start": "1221600",
    "end": "1229280"
  },
  {
    "text": "time frame out of the data that i've produced how much of it arrived how much of it ended",
    "start": "1229280",
    "end": "1235440"
  },
  {
    "text": "up arriving the second question there is a bit less straightforward so it's arrival latency",
    "start": "1235440",
    "end": "1242320"
  },
  {
    "text": "so how much of the data that i've produced arrived within a certain time frame how much of the data i've produced",
    "start": "1242320",
    "end": "1249360"
  },
  {
    "text": "arrived within an hour or within three hours or within five hours",
    "start": "1249360",
    "end": "1254480"
  },
  {
    "text": "these are the questions that we wanted answered these are the questions that we thought that would help us with our pain points",
    "start": "1254480",
    "end": "1262320"
  },
  {
    "text": "so let's let's look at our table so we'll have everything that we've",
    "start": "1262320",
    "end": "1268400"
  },
  {
    "text": "discussed we obviously will have all of these data points that we talked about in our auditing information the audit",
    "start": "1268400",
    "end": "1275600"
  },
  {
    "text": "time stamp the topic the server the process um what else well we're going to have the event count right that's",
    "start": "1275600",
    "end": "1282240"
  },
  {
    "text": "obviously our value just like what's the number uh what's the amount of messages that was produced for a certain time",
    "start": "1282240",
    "end": "1288640"
  },
  {
    "text": "frame what about add-ons so yeah we've added a region column and we've also added an",
    "start": "1288640",
    "end": "1295039"
  },
  {
    "text": "answer time why insert time answer time will be the thing that will help us just calculate",
    "start": "1295039",
    "end": "1300720"
  },
  {
    "text": "arrival latency if you deduct audit timestamp from insert time you're going to get that time interval that it took",
    "start": "1300720",
    "end": "1307840"
  },
  {
    "text": "the data to arrive from the moment it was produced and up until the moment that the data auditing information was",
    "start": "1307840",
    "end": "1314159"
  },
  {
    "text": "written into the audit table um and then also i have that location column there",
    "start": "1314159",
    "end": "1320240"
  },
  {
    "text": "what is it doing there well we've decided to store all of our data in the same table",
    "start": "1320240",
    "end": "1326320"
  },
  {
    "text": "and so to differentiate if it's the metadata or the actual data we've decided to use the",
    "start": "1326320",
    "end": "1332880"
  },
  {
    "text": "location column and so if we look at an again an anonymized version of our data",
    "start": "1332880",
    "end": "1340960"
  },
  {
    "text": "let's just look at something so if we look at insert time and window timestamp i",
    "start": "1340960",
    "end": "1346400"
  },
  {
    "text": "really want to draw our attention to the fact that they're all in utc that was especially important to us because that",
    "start": "1346400",
    "end": "1352400"
  },
  {
    "text": "data comes from different regions we have teams sitting in new york in the united states we also have a pretty big",
    "start": "1352400",
    "end": "1359520"
  },
  {
    "text": "r d in israel so working in utc was very very important to us we have topic name",
    "start": "1359520",
    "end": "1366159"
  },
  {
    "text": "we have the process id we the region we were producing out of the",
    "start": "1366159",
    "end": "1372000"
  },
  {
    "text": "server names that is not best practice but it was the easiest way to do it",
    "start": "1372000",
    "end": "1378320"
  },
  {
    "text": "another interesting thing that i really want to draw your attention to is the event count so you can definitely sit see different",
    "start": "1378320",
    "end": "1385440"
  },
  {
    "text": "event counts there like different numbers right but the interesting thing to notice is the zero",
    "start": "1385440",
    "end": "1391919"
  },
  {
    "text": "how is it possible that there's a zero there isn't something wrong with the auditing system",
    "start": "1391919",
    "end": "1397520"
  },
  {
    "text": "um the answer is no so the way that we've implemented that auditing subsystem the way that we",
    "start": "1397520",
    "end": "1405200"
  },
  {
    "text": "were creating these auditing metadata objects even if no data is arriving into",
    "start": "1405200",
    "end": "1411280"
  },
  {
    "text": "that server the audit metadata object will be created periodically every five",
    "start": "1411280",
    "end": "1416400"
  },
  {
    "text": "minutes so if no data arrived into the server the event count will be zero and",
    "start": "1416400",
    "end": "1422240"
  },
  {
    "text": "then still that audit metadata object will be produced will arrive and we'll",
    "start": "1422240",
    "end": "1428000"
  },
  {
    "text": "have an event count of zero and that's awesome because",
    "start": "1428000",
    "end": "1433360"
  },
  {
    "text": "a lot of the times an event count of zero can be completely normal maybe it's a dev server maybe we had um lower",
    "start": "1433360",
    "end": "1440880"
  },
  {
    "text": "lower data data rates that day but sometimes it can actually indicate a",
    "start": "1440880",
    "end": "1446880"
  },
  {
    "text": "problem cool so what you see before you now is just",
    "start": "1446880",
    "end": "1452880"
  },
  {
    "text": "the logo for apache superset and that's a bi tool that we use at nielsen israel",
    "start": "1452880",
    "end": "1458400"
  },
  {
    "text": "for for all of our bi needs and so we decided to use it for our auditing subsystem as well",
    "start": "1458400",
    "end": "1465039"
  },
  {
    "text": "so what i have before you now is just um the data arrival rate so it's a data",
    "start": "1465039",
    "end": "1470080"
  },
  {
    "text": "arrival rate for within three hours for the last month and what's interesting to notice um is that yeah it is above 100",
    "start": "1470080",
    "end": "1478799"
  },
  {
    "text": "it's 100 and 0.22 so there was definitely some data duplication going on there and i'm i'm",
    "start": "1478799",
    "end": "1486000"
  },
  {
    "text": "aware that that's not really like in the big data world when we're dealing with such huge volumes of data some data",
    "start": "1486000",
    "end": "1492960"
  },
  {
    "text": "duplication some data lost that's totally fine but when you see that number going above 150 or 200",
    "start": "1492960",
    "end": "1501440"
  },
  {
    "text": "you know for sure that you have a problem some more simple ones so we have just",
    "start": "1501440",
    "end": "1509120"
  },
  {
    "text": "like the straightforward data arrival for the last one hour or last 24 hours",
    "start": "1509120",
    "end": "1515840"
  },
  {
    "text": "and then also i have the exact query that these charts was produced",
    "start": "1515840",
    "end": "1521520"
  },
  {
    "text": "with for you so you can see that for arrival latency we just did just",
    "start": "1521520",
    "end": "1529120"
  },
  {
    "text": "deduct the um window timestamp for for insert time and it's not really that complex of the",
    "start": "1529120",
    "end": "1535760"
  },
  {
    "text": "core of a query the moment you understand what's the data you're dealing with and how you're actually",
    "start": "1535760",
    "end": "1542000"
  },
  {
    "text": "storing it so what you see before you now is again",
    "start": "1542000",
    "end": "1547200"
  },
  {
    "text": "the logo for apache superset but it's also the logo that superset use when",
    "start": "1547200",
    "end": "1552960"
  },
  {
    "text": "they're running your query and loading data and we were seeing that logo for",
    "start": "1552960",
    "end": "1558320"
  },
  {
    "text": "like a lot a lot of times um and so we definitely had a problem",
    "start": "1558320",
    "end": "1564320"
  },
  {
    "text": "and at this point i just want to do a small shout out to my dad my dad who's been a dba and a programmer",
    "start": "1564320",
    "end": "1570559"
  },
  {
    "text": "for um some 30 years more than me and so when i came to my dad with a problem i came to him with a query that was",
    "start": "1570559",
    "end": "1577120"
  },
  {
    "text": "running for one and a half minutes he didn't give up and he made it run for 10 seconds",
    "start": "1577120",
    "end": "1583360"
  },
  {
    "text": "so thanks dad so how did we do it how did me and my dad optimize password sql um to run",
    "start": "1583360",
    "end": "1591840"
  },
  {
    "start": "1585000",
    "end": "1585000"
  },
  {
    "text": "auditing data and to actually be able to quote to query audit data so the first",
    "start": "1591840",
    "end": "1597760"
  },
  {
    "text": "couple of things we did are pretty straightforward we decided to weekly partition our data",
    "start": "1597760",
    "end": "1603760"
  },
  {
    "text": "and we also decided to offload that data periodically to history",
    "start": "1603760",
    "end": "1608960"
  },
  {
    "text": "why well if you think about auditing data and log data usually users don't",
    "start": "1608960",
    "end": "1614960"
  },
  {
    "text": "really want to look back more than let's say a month so you really need only that first month in hot",
    "start": "1614960",
    "end": "1622320"
  },
  {
    "text": "storage the second thing we did was also pretty basic right so we've defined some unique",
    "start": "1622320",
    "end": "1628880"
  },
  {
    "text": "and complementary indexes on the auditing table looking at the predicates that are most used um for the queries",
    "start": "1628880",
    "end": "1636480"
  },
  {
    "text": "that we were running what else well if you're using um postgres on rds i really encourage you",
    "start": "1636480",
    "end": "1643679"
  },
  {
    "text": "to look at the at these three factors that i have on my screen right now so these are not enabled by default by",
    "start": "1643679",
    "end": "1651600"
  },
  {
    "text": "amazon on rds and so you'll really be amazed and what",
    "start": "1651600",
    "end": "1657440"
  },
  {
    "text": "can 10 workers do to your query instead of two so",
    "start": "1657440",
    "end": "1662799"
  },
  {
    "text": "if you need the if you have that need to run your queries in parallel and have better performance",
    "start": "1662799",
    "end": "1669360"
  },
  {
    "text": "look at these three configuration variables and last but not least",
    "start": "1669360",
    "end": "1674960"
  },
  {
    "text": "i know that we all think that the optimizer of the database that we're using is the best and knows best",
    "start": "1674960",
    "end": "1682320"
  },
  {
    "text": "but sometimes just tricking it will really improve the performance of your query and the best example for that",
    "start": "1682320",
    "end": "1689120"
  },
  {
    "text": "would be the in predicate so if you use it in predicate and you have several let's say have several strings inside",
    "start": "1689120",
    "end": "1696559"
  },
  {
    "text": "the in it would be actually better to use a union with several selects",
    "start": "1696559",
    "end": "1702380"
  },
  {
    "text": "[Music] it will actually improve the performance of your query",
    "start": "1702380",
    "end": "1708639"
  },
  {
    "text": "cool so just to mention i've mentioned that we decided to manage our partitions on a",
    "start": "1709039",
    "end": "1714640"
  },
  {
    "text": "weekly basis if you're wondering if postgres11 had that um",
    "start": "1714640",
    "end": "1719679"
  },
  {
    "text": "has that possibility of actually just creating partitions when the data arrives",
    "start": "1719679",
    "end": "1726399"
  },
  {
    "text": "you know it's a capability a capability that a lot of relational databases do have today",
    "start": "1726399",
    "end": "1732880"
  },
  {
    "text": "the answer is no they do not and so we actually had to create an entire system",
    "start": "1732880",
    "end": "1738720"
  },
  {
    "text": "based on airflow that would manage this these partitions for us um",
    "start": "1738720",
    "end": "1743760"
  },
  {
    "text": "we did it by using standardized partition names uh we had the week the",
    "start": "1743760",
    "end": "1749679"
  },
  {
    "text": "date of um of the date the date of the week begins with we had it in the partition name and",
    "start": "1749679",
    "end": "1756080"
  },
  {
    "text": "so we used these two very simple queries um and played with some dates uh just in",
    "start": "1756080",
    "end": "1762159"
  },
  {
    "text": "order to keep creating these weekly partitions",
    "start": "1762159",
    "end": "1767440"
  },
  {
    "text": "and of course offloading data to history we've created a spark job that would again run periodically once a week just",
    "start": "1767440",
    "end": "1775200"
  },
  {
    "text": "offload the old information that we do not need anymore calculate some statistics that we",
    "start": "1775200",
    "end": "1781520"
  },
  {
    "text": "thought were cool and might be useful in the future and store them in a data",
    "start": "1781520",
    "end": "1786880"
  },
  {
    "text": "auditing history table so now that we're done with that let's",
    "start": "1786880",
    "end": "1793200"
  },
  {
    "text": "go back to the end of the day question and see if we actually managed to answer",
    "start": "1793200",
    "end": "1798480"
  },
  {
    "text": "it see if actually the day's over um and and yeah yeah we did we definitely did",
    "start": "1798480",
    "end": "1804640"
  },
  {
    "text": "and we'll i'll start off by explaining how you can actually get an answer",
    "start": "1804640",
    "end": "1810240"
  },
  {
    "text": "we've created an api our developers can use we created alumda that would",
    "start": "1810240",
    "end": "1815760"
  },
  {
    "text": "actually just calculate make a calculation that we're going to go into in just half a minute um and that lambda",
    "start": "1815760",
    "end": "1823360"
  },
  {
    "text": "will give them back that answer is it the day end of the day yet can i run my",
    "start": "1823360",
    "end": "1828720"
  },
  {
    "text": "processors yet so here i have for you the pseudocode of",
    "start": "1828720",
    "end": "1834399"
  },
  {
    "text": "that entire lambda but don't worry we're not going to really go into that pseudo code we'll",
    "start": "1834399",
    "end": "1840799"
  },
  {
    "text": "just gonna talk about the actual questions that we need answered and these are the questions that we need",
    "start": "1840799",
    "end": "1847039"
  },
  {
    "text": "answered in order to say if is over yet so the first question is the data",
    "start": "1847039",
    "end": "1854159"
  },
  {
    "text": "arrival rate for the entire scope because if if we want a data arrival rate of more",
    "start": "1854159",
    "end": "1860640"
  },
  {
    "text": "than 99 and it's 95 or 90 then yeah the the day's not over yet i'm",
    "start": "1860640",
    "end": "1867440"
  },
  {
    "text": "sorry um but let's say it is let's say it's 100 we're great we're doing great so",
    "start": "1867440",
    "end": "1873279"
  },
  {
    "text": "data arrival rate um check what is the next question that we need answered the next thing that we're going",
    "start": "1873279",
    "end": "1880240"
  },
  {
    "text": "to uh to do is just check what's the number of audit windows that arrived for",
    "start": "1880240",
    "end": "1885440"
  },
  {
    "text": "that entire scope we're doing that um as a check for our auditing system because",
    "start": "1885440",
    "end": "1890799"
  },
  {
    "text": "if our auditing system has stopped working then we'll have a lower amount of auditing windows",
    "start": "1890799",
    "end": "1897760"
  },
  {
    "text": "and then essentially we can't really trust it to give us a correct answer",
    "start": "1897760",
    "end": "1903200"
  },
  {
    "text": "if the day's over yet okay so we haven't a high enough arrival rate we also have",
    "start": "1903200",
    "end": "1909039"
  },
  {
    "text": "a high enough amount of windows we're good to go um but no we're going to",
    "start": "1909039",
    "end": "1914480"
  },
  {
    "text": "answer another question um so the last answer that we need is just the",
    "start": "1914480",
    "end": "1920480"
  },
  {
    "text": "arrival rate for the last window um so when we talk about the last window it's of course 11 55 pm",
    "start": "1920480",
    "end": "1928080"
  },
  {
    "text": "why why do we need that answer so if i can take you back into our charts when we saw numbers that are higher than 100.",
    "start": "1928080",
    "end": "1936480"
  },
  {
    "text": "if we think about it let's say that at 2pm we had a really bad peak",
    "start": "1936480",
    "end": "1942120"
  },
  {
    "text": "[Music] we had a peak of like 150 or 200 percent um then essentially",
    "start": "1942120",
    "end": "1951039"
  },
  {
    "text": "um our calculation of data arrival by the end of the day will be skewed and",
    "start": "1951039",
    "end": "1956799"
  },
  {
    "text": "maybe we might get an okay answer for that um so we're just going to use that",
    "start": "1956799",
    "end": "1962880"
  },
  {
    "text": "last window as an indicator and see that yeah okay um if the data arrival rate",
    "start": "1962880",
    "end": "1970080"
  },
  {
    "text": "for the entire scope is fine is the number of windows for the entire scope is high enough if the arrival rate for",
    "start": "1970080",
    "end": "1977679"
  },
  {
    "text": "that very last window is also just fine then yeah okay",
    "start": "1977679",
    "end": "1983919"
  },
  {
    "text": "yesterday's over we're fine it is the end of the day",
    "start": "1983919",
    "end": "1990159"
  },
  {
    "start": "1989000",
    "end": "1989000"
  },
  {
    "text": "and so i'm really at the end of my presentation so i hope that you guys are holding up",
    "start": "1990159",
    "end": "1995600"
  },
  {
    "text": "so let's talk about some alerts and add-ons using all of that information we've managed to create an alert system um on",
    "start": "1995600",
    "end": "2003679"
  },
  {
    "text": "each one of the levels of granularity we wanted and it really helped us pinpoint a lot of the problems that we discussed",
    "start": "2003679",
    "end": "2010559"
  },
  {
    "text": "in the beginning so what would that be if you have an alert popping up on an",
    "start": "2010559",
    "end": "2016080"
  },
  {
    "text": "entire region then the problem is probably not with your aws kafka or with your data loaders it's probably with",
    "start": "2016080",
    "end": "2023360"
  },
  {
    "text": "your view replicators and if you have an alert on a topic then probably the problem is",
    "start": "2023360",
    "end": "2031279"
  },
  {
    "text": "not with your view replicator it's probably with your data loaders and so these alerts would look something",
    "start": "2031279",
    "end": "2037600"
  },
  {
    "text": "like this you can just actually run the query on your data and then",
    "start": "2037600",
    "end": "2042960"
  },
  {
    "text": "straightforward send an alert to slack and and this way these alerts would",
    "start": "2042960",
    "end": "2048320"
  },
  {
    "text": "actually arrive before any of our other alerts before an alert that the your applicator has crashed before an alert",
    "start": "2048320",
    "end": "2055358"
  },
  {
    "text": "that the spark driver is not responding anymore you will get this alert that the data auditing system has actually found",
    "start": "2055359",
    "end": "2062480"
  },
  {
    "text": "a problem what else well remember how i talked",
    "start": "2062480",
    "end": "2068158"
  },
  {
    "text": "about sequence id and i said yeah that's that's a bit of a spoiler what was it doing there well at first when we",
    "start": "2068159",
    "end": "2074960"
  },
  {
    "text": "started getting this high these high arrival rates and we said wait a duplication that cannot be maybe it's",
    "start": "2074960",
    "end": "2082000"
  },
  {
    "text": "a problem with our data auditing system so we decided that just to add a sequence id into our audit header",
    "start": "2082000",
    "end": "2090079"
  },
  {
    "text": "that way if you just query your audit headers straight through straight from your file",
    "start": "2090079",
    "end": "2096240"
  },
  {
    "text": "system and just group by that audit header you can see if you had a duplication if you group by your key",
    "start": "2096240",
    "end": "2103760"
  },
  {
    "text": "using that sequence id and you have an amount you have groups with a number higher than one",
    "start": "2103760",
    "end": "2110079"
  },
  {
    "text": "then you essentially have a data duplication and last but not least the location",
    "start": "2110079",
    "end": "2116560"
  },
  {
    "text": "column um so i talked about the location column and i i said that we've used it only for",
    "start": "2116560",
    "end": "2122560"
  },
  {
    "text": "two purposes right just like the purpose of differentiating between our data and our auditing data",
    "start": "2122560",
    "end": "2128000"
  },
  {
    "text": "but we also always thought about just adding more locations because our primary location was of course our kafka",
    "start": "2128000",
    "end": "2135599"
  },
  {
    "text": "and then the second location was s3 but just adding more locations more data",
    "start": "2135599",
    "end": "2141680"
  },
  {
    "text": "points would just provide us with more visibility about the way that our data makes through this entire waterfall",
    "start": "2141680",
    "end": "2149040"
  },
  {
    "text": "when is it late and just give us more visibility and a better way to handle all of these",
    "start": "2149040",
    "end": "2156240"
  },
  {
    "text": "problems do you have any questions",
    "start": "2156839",
    "end": "2161200"
  },
  {
    "text": "okay um so feel free to approach me on whatever platform you feel like you",
    "start": "2166000",
    "end": "2172640"
  },
  {
    "text": "wanna linkedin face to face and thanks for coming and listening",
    "start": "2172640",
    "end": "2180040"
  },
  {
    "text": "[Applause]",
    "start": "2182960",
    "end": "2186630"
  },
  {
    "text": "you",
    "start": "2192640",
    "end": "2194720"
  }
]